{
    "paper_title": "More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models",
    "authors": [
        "Xinyu Tian",
        "Shu Zou",
        "Zhaoyuan Yang",
        "Mengqi He",
        "Fabian Waschkowski",
        "Lukas Wesemann",
        "Peter Tu",
        "Jing Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/"
        },
        {
            "title": "Start",
            "content": "MORE THOUGHT, LESS ACCURACY? ON THE DUAL NATURE OF REASONING IN VISION-LANGUAGE MODELS Xinyu Tian, Shu Zou, Zhaoyuan Yang*, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu*, Jing Zhang, Australian National University firstname.lastname@anu.edu.au, firstname.lastname@ge.com University of Melbourne *GE Research Maincode 5 2 0 S 0 3 ] . [ 1 8 4 8 5 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reasoning has emerged as pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose VISION-ANCHORED POLICY OPTIMIZATION (VAPO), simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the models reliance on visual information and achieves new state-of-the-art results on wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reasoning has long been recognized as an essential capability of Large Language Models (LLMs). Early approaches, such as chain-of-thought (Wei et al., 2022; Kojima et al., 2022), encourage models to produce step-by-step explanations before arriving at an answer, whereas more recent advances have introduced aha moments (Guo et al., 2025; Muennighoff et al., 2025), characterized by cognitive behaviors such as self-reflection and verification. These developments have enabled models to address increasingly complex problems, e.g., mathematics and coding. Consequently, the community has recently sought to integrate reasoning into Vision-Language Models (VLMs). By leveraging Reinforcement Learning (RL) such as Group Relative Policy Optimization (GRPO) (Shao et al., 2024), researchers have trained models to think while tackling challenging multimodal tasks, e.g., geometry or navigation (Yang et al., 2025c; Tian et al., 2025b). Remarkably, this integration has also demonstrated promising results on traditional visual tasks, e.g., classification and detection (Liu et al., 2025; Shen et al., 2025), while yielding stronger generalization (Chu et al., 2025). Building on these successful practices, reasoning has emerged as powerful and seemingly universal strategy for addressing wide range of visual tasks. However, its limitations remain largely unexplored. Early doubts are raised by Li et al. (2025), which find that in certain scenarios, models trained with explicit reasoning result in only marginal gains compared to direct answering. More recently, Xia et al. (2025) observe counter-intuitive trend: unlike in text domains, RL in multimodal settings often exhibits collapse. As accuracy improves during training, the reasoning process gradually shortens, suggesting that models tend to favor brief, less structured responses. These findings lead us to ask: Is reasoning truly consistent performance booster for vision-language models? Motivated by this question, we perform zoom-in analysis of how the reasoning process impacts accuracy of existing VLMs. Beyond the standard settings of direct answering and full-step reasoning, we introduce an early decision mode, where the model is prompted to halt reasoning at selected log-"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The examples where more reasoning yields less accuracy. We select Vision-R1 (Huang et al., 2025) as representitive multimodal reasoning model and evaluate on established benchmarks. Given the ground truth (GT) answer, we examine three settings, i.e., direct (DT) answering, full reasoning, and early decision. Correct and incorrect responses are highlighted in green and red, respectively, while designed prompts for early decision are indicated in blue. ical breakpoints and produce an immediate answer. This setup allows us to measure the contribution of each reasoning segment to the final outcome. Interestingly, our finding reveals that while earlystage reasoning may offer substantial gains, its advantages tend to saturate and may even reverse in later steps. To understand this phenomenon, we conduct an error analysis of the reasoning content in failure cases and find that perception errors, where the model incorrectly interprets visual details in the image, constitute the dominant category, accounting for over 50%. Moreover, the majority of these errors can be rectified with shorter reasoning via early decision, indicating that the model is initially capable of producing the correct answer but is ultimately misled by extended reasoning. For instance, as shown in Fig. 1, the early stage of reasoning paths, i.e., early decision, enables the model to arrive at the correct answer, whereas prolonged thoughts, i.e., full reasoning, often lead to errors such as chart misreading or hallucination. This indicates the dual nature of reasoning: while it strengthens logical inference and facilitates complex problem-solving, it may gradually weaken perceptual grounding, leading to susceptibility on otherwise straightforward visual questions. This decline in perceptual capability constitutes major bottleneck for multimodal reasoning. To further investigate its underlying causes, we examine how attention to visual tokens evolves throughout the reasoning process. Our analysis reveals marked decrease in visual attention as reasoning progresses, suggesting the emergence of reasoning may inadvertently reduce the models reliance on visual input, phenomenon we refer to as visual forgetting. To validate this hypothesis, we propose two interventions: 1) visual replay, which reintroduces the input image to models at regular intervals during reasoning; 2) focus prompt, which prompts models to attend to the visual input at selected steps. Both methods are proven to alleviate performance degradation induced by reasoning, providing evidence that visual forgetting is central factor constraining reasoning potentials. While the two aforementioned remedies can partially alleviate visual forgetting, they incur substantial computational overhead during inference and fail to rectify the underlying behavioral deficiencies of existing reasoning models. To address this, we propose VISION-ANCHORED POLICY OPTIMIZATION (VAPO), simple yet effective policy gradient algorithm that explicitly guides the reasoning process toward visually grounded trajectory. The key idea is to embed sequence of visual anchors throughout the reasoning path. At each anchor, the models perceptual capability is assessed by evaluating its responses to set of primitive visual claims. Beyond standard outcome-based rewards such as accuracy and format, we introduce perception reward, which quantifies the models overall perceptual grounding during reasoning by aggregating scores across all anchor points. Ex-"
        },
        {
            "title": "Preprint",
            "content": "perimental results demonstrate that our result model, VAPO-Thinker-7B, significantly enhances the models reliance on visual input during reasoning and achieves average gains of 2 4% over strong baselines on established benchmarks, setting new state of the art. In summary, our contributions are as follows: Despite the promise of multimodal reasoning, we identify pronounced dual nature, where reasoning enhances logical inference capability at the expense of perceptual accuracy, constituting major bottleneck to the overall effectiveness. We further examine this side effect of reasoning and empirically demonstrate that it stems from visual forgetting, where prolonged reasoning gradually reduces the models reliance on visual input, leading to substantial increase in perceptual failures. We propose VAPO, straightforward yet effective policy gradient algorithm designed to strengthen the models dependence on visual information during reasoning, and demonstrate its effectiveness through experimental validation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reasoning in Large Language Models. Conventional research has posited trade-off between interpretability and accuracy (Wang et al., 2020; Koh et al., 2020). The advent of reasoning, however, has emerged as notable exception to the paradigm. Early works demonstrate that prompting LLMs (Wei et al., 2022; Yao et al., 2023a; Tian et al., 2024; 2025a; Zou et al., 2025) or applying Supervised Fine-Tuning (SFT) (Yang et al., 2025a; Cai et al., 2024) to encourage step-by-step explanations prior to answering could lead to substantial gains. These findings underscore the critical role of constructing large-scale, high-quality chain-of-thought trajectories. More recently, the emergence of RL-based methods such as GRPO (Shao et al., 2024) has diminished the reliance on manually crafted reasoning paths. With only limited number of examples, models are now capable of autonomously discovering optimal reasoning strategies and even exhibiting advanced behaviors such as self-reflection and verification (Yu et al., 2025; Zheng et al., 2025). Consequently, Test-Time Scaling (TTS) (Muennighoff et al., 2025; Snell et al., 2025; Qu et al., 2025; Yao et al., 2025) has become standard practice in LLMs, driven by the belief that longer or more elaborate reasoning consistently yields better performance. However, our empirical analysis challenges the universality of this assumption, revealing that TTS does not necessarily generalize to multimodal settings, particularly regarding its implications for VLMs. Reasoning in Vision-Language Models. Building on the success of reasoning in LLMs, growing line of research has sought to extend this capability to VLMs. Two predominant approaches have emerged: one leverages limited high-quality trajectories for cold-start via SFT, followed by RL (Peng et al., 2025; Tan et al., 2025; Yang et al., 2025b; Feng et al., 2025), while the other bypasses SFT entirely, employing RL alone to discover optimal reasoning paths (Liu et al., 2025; Wang et al., 2025c; Meng et al., 2025). Both approaches have yielded promising results on various visual tasks, particularly those requiring complex logical inference. However, recent findings indicate that, in certain scenarios, models trained with explicit reasoning offer little or no gains over direct answering (Li et al., 2025). Moreover, training dynamics frequently reveal that as accuracy increases, the length of generated reasoning tends to diminish, implying growing preference for shorter and more concise responses (Xia et al., 2025). These preliminary findings indicate that reasoning may not be free lunch in the development of VLMs. To the best of our knowledge, our work presents the first systematic investigation into the double-edged nature of reasoning in VLMs, identifying visual forgetting as key limitation and proposing effective solutions to mitigate this issue. Forgetting in Vision-Language Models. In fact, visual forgetting has long been well-known yet insufficiently addressed issue. Since the emergence of VLMs, researchers have identified pronounced text bias in these LLM-backboned models, where responses are predominantly guided by textual input while visual cues are largely ignored (Chen et al., 2024a; Fu et al., 2024; Yao et al., 2023b). At the time, this modality imbalance is less apparent, as early VLMs typically generate short outputs and lack sophisticated reasoning capabilities. Consequently, most prior efforts focus on test-time remedies such as contrastive decoding (Leng et al., 2024; Wang et al., 2024b) or attention reallocation (Tu et al., 2025; Gong et al., 2024) to circumvent visual forgetting. However, with the recent rapid progress in multimodal reasoning, VLMs have increasingly shifted from gen-"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: The dual nature analysis of reasoning. In (A), we depict how accuracy evolves throughout the reasoning process. In (B), we show the distribution of error categories under full reasoning (inner ring), alongside the proportion recoverable via early decision (outer ring). In (C), we further present the initial ratio of perception errors on different benchmarks (left bar), as well as the remaining errors that persist after recovery (right bar). It is important to note that recoverable ratio is loose metric based on series of early decisions along the reasoning trajectory rather than single evaluation. erating short answers to producing extended, step-by-step explanations. This shift has inadvertently amplified the forgetting issue, limiting the benefits of reasoning in multimodal contexts. To address this, we propose VAPO, training-based method that explicitly reinforces the models perceptual grounding, effectively alleviating visual forgetting, and achieving new state-of-the-art results."
        },
        {
            "title": "THE DUAL NATURE OF REASONING",
            "content": "3.1 NO-FREE-LUNCH DILEMMA: LOGIC VS. PERCEPTION We first explore how accuracy evolves throughout the reasoning process in existing multimodal reasoning models. Specifically, we select three representitve VLMs, i.e., R1-OneVision (Yang et al., 2025c), VLAA-Thinker (Chen et al., 2025) and Vision-R1 (Huang et al., 2025), spanning both SFTthen-RL and RL-only training pipelines. We evaluate models on reasoning-centric benchmarks encompassing MathVerse (Zhang et al., 2024) and LogicVista (Xiao et al., 2024), and vision-intensive benchmarks including MMStar (Chen et al., 2024b) and HallusionBench (Guan et al., 2024). To enable fine-grained analysis, we introduce an early decision mode, in which the model is prompted to terminate reasoning at intermediate logical boundaries such as commas, periods or line breaks, and provide an immediate answer, as illustrated in Fig. 1. This setting allows us to quantify the contribution of each reasoning segment to the overall task performance. Longer reasoning does not guarantee better performance. As shown in Fig. 2 (A), we leverage early decision to control the reasoning length and monitor accuracy trend while reasoning progresses from 0%, i.e., direct answering to 100%, i.e., full reasoning. In the early stages, reasoning yields substantial gains across benchmarks, consistent with prior works. However, as reasoning continues, these gains tend to plateau and even begin to reverse, leading to decline in accuracy. This dual effect is particularly pronounced on benchmarks such as MMStar and HallusionBench, where accuracy drops by over 2% from the peak, almost offsetting the original benefits of reasoning. The above analysis suggests that, while reasoning is overall effective, its potential is constrained by hidden factor that acts as significant bottleneck. To further examine this limitation, we perform comprehensive error analysis. In particular, we classify incorrect responses of Vision-R1 under full reasoning into three categories, based on the characteristics of their reasoning trajectories: 1) Perception Error. Reasoning demonstrates notable deficiencies in the understanding and recognition of visual details such as chart misreading or hallucination exemplified in Fig. 1. 2) Logical Error. Models exhibit symbolic reasoning errors such as arithmetic inaccuracies, logically invalid inferences, or incoherence across intermediate reasoning steps. 3) Knowledge Error. There are commonsense violations or factual errors that may stem from outdated knowledge and inherent limitations in the pre-training phase. We employ GPT-5 (Singh et al., 2025) to identify error categories and quantify their distribution, with details provided in Appendix A.3. Beyond this, we further investigate the impact of reasoning"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: The analysis of visual forgetting. In (A), we randomly select an example from MathVerse and visualize the evolution of attention ratio to visual tokens from Vision-R1 across generation steps. We consider vanilla reasoning as well as two variants, i.e., visual replay and focus prompt. In (B), we report the average accuracy across established benchmarks and models, and track its trend throughout the reasoning process under the aforementioned three settings. The average cutoff positions across examples to insert images or instructions are indicated in vertical dash lines. progress on these failure cases. Specifically, we apply early decision, where an error case is considered recoverable if the model can produce correct answer at any earlier point along the reasoning trajectory, and report the corresponding proportion of such cases. These recoverable instances suggest that the model is initially on the right path but is ultimately misled by prolonged reasoning. The harder the model thinks, the worse the model sees. As shown in Fig. 2 (B), despite the complexity of these reasoning benchmarks, the majority of errors made by VLMs stem not from logical failures (33.05%), but rather from basic perception errors (55.23%). Remarkably, substantial portion of these perceptual errors (32.35%) may be recovered by terminating the reasoning process in advance through early decision. This counterintuitive result suggests that as reasoning progresses, the models perceptual capability gradually weakens. Together with the finding in Fig. 2 (A), these degradation in visual perception caused by prolonged thoughts is most likely key contributor to the performance decline observed in the later stages of reasoning. The harms of reasoning are most evident in vision-heavy tasks. In Fig. 2 (C), we further examine the impact of reasoning progression on perception errors across different benchmarks. Notably, in MMStar and HallusionBench, which feature high-resolution real-world images and perceptually elusive content, the proportion of perception errors arising during reasoning is substantially higher. Moreover, the share of recoverable instances via early decision in these benchmarks is markedly greater than that observed in others, such as MathVerse and LogicVista. This observation suggests that, in contrast to tasks with simple visual structures, e.g., geometry, the shortcomings of reasoning become more pronounced when applied to vision-intensive problems. 3.2 SIDE EFFECT OF REASONING: VISUAL FORGETTING The above findings highlight severe trade-off between logical capability and perceptual grounding in multimodal reasoning, which has emerged as major bottleneck in the development of VLMs. Therefore, to further probe the underlying causes, we track the evolution of attention ratio assigned to visual tokens at each generation step, which provides proxy of how visual information contributes to the reasoning process. As shown in Fig. 3 (A), under vanilla reasoning, the attention of visual tokens exhibits marked decline as reasoning progresses, eventually reaching negligible levels. This pattern indicates the models decision-making becomes increasingly driven by its own historical thoughts rather than the visual cues, phenomenon termed visual forgetting. To validate this hypothesis, we design two straightforward inference-level remedies: 1) Visual Replay. Instead of presenting the visual input only at the beginning, we reintroduce the image to models at regular intervals throughout the reasoning process. 2) Focus Prompt. Similarly, at regular intervals, we explicitly prompt models to revisit the input image with instructions such as need to see the image or have to look back. Encouraging models to look more often boosts reasoning. Back to Fig. 3 (A), we insert either the image or instruction at four positions along the reasoning trajectory, which are approximately evenly spaced and aligned with logical boundaries; see Appendix A.5 for more details. Notably, both approaches trigger sharp and immediate increase in visual attention upon insertion. As shown in"
        },
        {
            "title": "Preprint",
            "content": "Fig. 3 (B), this reinforcement of visual information alleviates the performance degradation observed during the reasoning process, with visual replay yielding particularly pronounced gains, ultimately outperforming vanilla reasoning by around 1.5%. This also empirically shows that visual forgetting is the fundamental cause preventing reasoning from realizing its full potential in multimodal tasks."
        },
        {
            "title": "THE PROPOSED METHOD",
            "content": "4."
        },
        {
            "title": "PRELIMINARY",
            "content": "To introduce our method, we first revisit the standard RL-based algorithm, typically Group Relative Policy Optimization (GRPO), which is simplified variant of Proximal Policy Optimization (PPO) that eliminates the critic model. In the multimodal context, consider dataset D, where each instance consists of question q, ground truth answer y, and visual input I. The objective of GRPO is to encourage high-quality responses while penalizing inferior ones by comparing set of generated candidates. Formally, given policy model πθ, we aim to maximize the following: JGRPO(θ) = (q,y,I)D,{oi}G i=1πθold (q,I) (cid:18) min (cid:16) (cid:16) ri,t(θ) ˆAi,t, clip ri,t(θ), 1 ϵ, 1 + ϵ (cid:17) (cid:17) ˆAi,t λDKL (πθ πref ) (cid:19) 1 (cid:88) i=1 1 oi oi (cid:88) t=1 with ri,t(θ) = πθ(oi,t q, I, oi,<t) πθold (oi,t q, I, oi,<t) . (1) denotes the number of sequences oi sampled from the old policy πold. ϵ and λ are hyperparameters that constrain the clipping bounds to ensure on-policy training and penalize deviations from the reference policy πref to stabilize optimization. The estimated token-level advantage ˆAi,t is derived by broadcasting the normalized sequence-level reward Ri, which is defined as follows: Ri mean (R) std (R) = 1, , G, ˆAi,t = (2) , where = {R1, R2, ..., RG} indicates the reward of the sequence group. The design of reward functions offers considerable flexibility. widely adopted approach involves the use of verifiable rewards, which evaluate model responses by comparing them to ground-truth answers to derive accuracy-based and format-consistency signals. 4.2 VISION-ANCHORED POLICY OPTIMIZATION Although GRPO has become common practice for training reasoning models, our findings in Section 3 reveal that existing VLMs suffer from visual forgetting, constituting major bottleneck in multimodal reasoning. To address this, we propose VISION-ANCHORED POLICY OPTIMIZATION (VAPO), simple yet effective approach as multimodal replacement for GRPO that steers the reasoning process toward visually grounded trajectories. As shown in Fig. 4, VAPO first generates set of correct and incorrect claims that describe the visual details in the images. These claims are strategically inserted as visual anchors along the reasoning path, where the model is prompted to judge their veracity, thereby enabling ongoing evaluation of models perceptual capability throughout the reasoning process. Consequently, we introduce perception reward, aggregated over all anchor points, which explicitly incentivizes the model to retain and utilize visual cues during reasoning. Generating Visual Claims. We use visual claims as proxy to evaluate the models perceptual capability at various reasoning stages. To serve this purpose, the claims need to satisfy two criteria: 1) balanced, i.e., the set should contain an equal number of correct and wrong claims to avoid biased evaluation and 2) independent, i.e., the model judgment of each claim must rely solely on visual input, rather than historical reasoning. Specifically, for each example, we employ GPT-5 to generate diverse set of claims with balanced correctness. To enforce independence from reasoning contexts, we only provide visual input without revealing the corresponding question, thereby ensuring claim verification is grounded purely in perceptual understanding. See Appendix A.6 for more details."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The overview of VAPO. On the left, we first employ GPT to generate set of claims about visual input, each of which may be either factually correct or not. These claims are then treated as anchors and inserted into the models reasoning process. Specifically, on the right, for each anchor, we randomly sample prefix from the reasoning trajectory, append the claim to this truncated context, and probe the model judgment regarding the claims validity. Inserting Visual Anchors. Upon obtaining these claims, we set up series of visual anchors within the models reasoning process, functioning as intermediate checkpoints. Once reaching the anchor, the model is queried with selected claim to assess its perceptual capability at that stage of reasoning. Formally, given trajectory oi = (oi,1, oi,2, , oi,T ), we define set of anchors Ai = {a1, a2, , aK} with ak [1, ] denoting the position randomly distributed throughout the trajectory. At each anchor ak, claim ck Ci is sampled from the corresponding claim pool and appended to the prefix reasoning context, yielding binary score over the judgement: (cid:20) (cid:21) sk = 1 arg max j{yes,no} πθ(j q, I, oi,<ak , ck) = lk , (3) where lk denotes the ground truth of the claim. In other words, we evaluate the models perceptual capability by probing its binary decision on the claim and scoring it against the reference label. Perception Reward. Building on the anchors above, we introduce perception reward that quantifies the models overall perceptual capability throughout the reasoning process. Formally, given the per-anchor scores {sk}K k=1, we design late-emphasis weighted aggregation: Rperc = (cid:80)K k=1 wksk (cid:80)K k=1 wk with wk = exp (cid:16) β (cid:17) , ak (4) where wk indicates the weight assigned to the corresponding anchor, and β is hyperparameter that controls the degree of emphasis of later anchors. This design is motivated by the observation that the models perceptual capability tends to decline as reasoning progresses. Consequently, greater weight is assigned to anchors in the latter stages to precisely target the model weakness. In summary, the final reward for sampled sequence oi is computed as: Ri = Racc + Rfmt + γ 1 [Racc = 1] Rperc, (5) where Racc and Rfmt denote accuracy and format reward, γ is the weight of perception reward. In practice, we impose an accuracy condition on perception reward to guard against reward hacking, thereby preventing models from simply boosting perceptual capability by producing trivially short reasoning paths. Our training procedure follows the objective of GRPO as defined in Section 4.1."
        },
        {
            "title": "EXPERIMENT AND RESULTS",
            "content": "Implementation Details. Unless otherwise specified, all models are trained on ViRL39K (Wang et al., 2025a), high-quality and comprehensive dataset, for 2 epochs with learning rate of 5e6. We adopt Qwen2.5-VL (Bai et al., 2025) with 3B and 7B parameters as base models. For group reward computation, we follow previous works (Huang et al., 2025; Chen et al., 2025) by generating 5 responses per example with sampling temperature of 1.0. For VAPO, we set the default number of anchors to = 20, the late-emphasis weight to β = 1.5, and reward weight γ = 0.1. Benchmarks. We denote the models trained with our method as VAPO-Thinker, and evaluate on ten benchmarks covering diverse task types: mathmatical benchmarks such as MathVerse (Zhang"
        },
        {
            "title": "Preprint",
            "content": "Models MathVerse MathVista MathVision LogicVista WeMath Geometry3k Avg. Close-source models GPT-5-Thinking* Gemini-2.5-Pro* Open-source models Qwen2.5-VL-7B InternVL2.5-8B* R1-OneVision-7B VLAA-Thinker-7B Vision-R1-7B Our models VAPO-Thinker-3B VAPO-Thinker-7B 81.2 76. 40.7 34.5 46.4 48.2 52.4 35.8 53.3 81.9 80.9 62.3 68.2 64.1 68.0 73.5 67.1 75.6 72.0 69. 23.2 25.6 29.9 26.4 28.2 23.9 31.9 70.0 73.8 42.6 38.3 45.6 48.5 49.7 39.7 50.9 71.1 78. 33.1 38.6 44.6 41.5 41.6 35.4 43.6 79.9 77.2 38.5 44.8 46.1 50.6 49.0 44.2 51.3 76.1 75. 40.1 41.7 46.1 47.2 49.1 41.0 51.1 Table 1: The evaluation of our proposed method on various mathematical benchmarks. We report results of existing open-source multi-modal reasoning models, as well as proprietary models for reference. Note that * indicates baseline results referenced from the OpenCompass leaderboard. Models MMMU MMStar Hall MMVet Avg. Models WeMath Geo3k MMStar Hall Avg. Qwen-VL R1-OV VLAA V-R1 VAPO 52.7 54.3 59.1 57.6 60.2 54.9 54.1 49.7 61.4 63.0 50.0 52.5 54.7 49.5 57.4 64.8 65.2 70.0 71.1 71.9 55.6 56.5 58.4 59.9 63.1 VLAAFP VLAAVR V-R1FP V-R1VR VAPO 41.8 42.3 42.1 42.5 43.6 50.7 51.1 49.7 50.5 51.3 51.1 52.9 61.8 62.1 63.0 55.2 49.7 56.2 50.6 50.5 51.0 51.8 51.7 57.4 53.8 Table 2: The evaluation of our proposed method on general-purpose benchmarks. All baselines considered in this evaluation are of the 7B scale. Table 3: Comparison with strong baselines augmented with test-time remedies. FP and VR denote focus prompt and visual replay, respectively. et al., 2024), MathVista (Lu et al., 2023), MathVision (Wang et al., 2024a), LogicVista (Xiao et al., 2024), WeMath (Qiao et al., 2024) and Geometry3k (Lu et al., 2021), as well as general-purpose ones including MMMU (Yue et al., 2024), MMStar (Chen et al., 2024b), HallusionBench (Guan et al., 2024) and MMVet (Yu et al., 2023). Further evaluation results are provided in Appendix B. Baseline Methods. To verify the effectiveness of our approach, we compare VAPO-Thinker with range of existing strong reasoning models, including proprietary ones such as GPT-5Thinking (Singh et al., 2025) and Gemini-2.5-Pro (Comanici et al., 2025), as well as open-source counterparts encompassing InternVL2.5 (Chen et al., 2024d), R1-OneVision (Yang et al., 2025c), VLAA-Thinker (Chen et al., 2025), and Vision-R1 (Huang et al., 2025). The baseline results are primarily referenced from the corresponding papers, secondarily from the OpenCompass leaderboard, and reproduced when neither source is available. See Appendix A.7 for more configuration details. 5.1 MAIN RESULTS VAPO consistently improves accuracy across diverse benchmarks. As shown in Table 1, VAPO-Thinker-7B outperforms recent reasoning models of the same scale on mathematical problems, achieving an average improvement of 2% (49.1% 51.1%). The advantage is more pronounced on general-purpose tasks, as shown in Table 2, where our method surpasses previous best results by 3.2% (59.9% 63.1%), thereby establishing new state of the art. This suggests that compared to logic-heavy problems such as math, VAPO is particularly effective on vision-intensive tasks like MMStar and HallusionBench, demonstrating stronger visually grounded reasoning. VAPO offers principled solution beyond inference-level remedies. In Section 3.2, we introduce two remedies, i.e., visual replay and focus prompt, as preliminary exploration to verify the visual forgetting issue. Here we compare VAPO against these test-time strategies. As shown in Table 3, even when strong baselines are equipped with visual replay or focus prompt, VAPO still achieves substantially higher performance, suggesting that inference-level remedies alone are insufficient to address visual forgetting, whereas VAPO fundamentally rectifies this reasoning deficiency."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Zoom-in analysis of our method. In (A), we compare the evolution of visual attention across generation steps between VAPO and the baseline on the selected example. In (B), we track the average accuracy across ten benchmarks throughout the reasoning process via early decision. Figure 6: The learning curves of accuracy and perception reward during the training stage. Figure 7: The ablation study on the effects of anchor number and late-emphasis weight β. 5.2 FURTHER DISCUSSION VAPO fully releases the potentials of reasoning. Here we analyze the evolution of visual attention and accuracy over the course of reasoning process. In Fig. 5 (A), compared with the baseline, our model demonstrates more gentle decline in the attention ratio, sustaining consistently higher level in the later stages, indicating that VAPO effectively strengthens the contribution of visual cues to model decisions. The benefit brought by this is directly reflected in accuracy, as shown in Fig. 5 (B), where in contrast to the baseline which exhibits sharp performance decline during later steps, our method achieves steadily increasing accuracy, thereby fully leveraging the advantages of reasoning. VAPO greatly enhances perceptual capability during reasoning. In Fig. 6, we plot the training curves of accuracy and perception reward. The results reveal that perception reward exhibits steady upward trend in parallel with accuracy, suggesting that the model progressively allocates greater attention to visual input and thereby improves its ability to verify visual claims. Interestingly, an sharp decline in perception reward is observed during the early stage, which may be attributed to the initial dominance of accuracy and format alignment, causing the model to temporarily disregard visual information. This underscores fundamental limitation of prior standard training paradigms. 5.3 ABLATION STUDY More anchors yield better visually grounded reasoning. We examine the impact of visual anchor number on average accuracy across ten benchmarks. As shown on the left of Fig. 7, we vary from 0, which degrades to vanilla GRPO training, to 20, which is adopted as our default configuration. The results indicate that the average accuracy improves rapidly as increases and approaches saturation at around = 20. This behavior is intuitive, as setting up denser anchors during reasoning provides more reliable assessment of the models perceptual capability. Later anchors contribute to greater gains. In VAPO, the parameter β controls the relative emphasis placed on later anchors to target longer reasoning. As shown on the right of Fig. 7, we evaluate the effect of varying β on average accuracy. When β = 0, all anchors are assigned equal weights, while larger values assign greater importance to later anchors. The results show that performance peaks at β = 1.5, where around 50% of the total weight is concentrated on the last 30% of anchors. More results such as ablation of reward weight γ and limitation analysis are provided in Appendix B."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this study, we conduct comprehensive investigation into the capabilities and limitations of multimodal reasoning. We first reveal the dual nature of reasoning: while it enhances logical inference and proves beneficial for challenging problems, it may gradually impair perceptual grounding, leading to recognition deficiencies on otherwise basic visual questions. Then we further identify this phenomenon and attribute it to visual forgetting, where prolonged reasoning causes the model to increasingly disregard visual information. To address this, we propose VISION-ANCHORED POLICY OPTIMIZATION (VAPO), simple yet effective method that explicitly steers the reasoning process along trajectories anchored in visual evidence. Our result model, VAPO-Thinker-7B, effectively mitigates visual forgetting and establishes new state of the art across wide range of benchmarks."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This research was, in part, funded by the U.S. Government DARPA TIAMAT HR00112490421. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. We also gratefully acknowledge Lambda GPU Cloud and Maincode for their generous provision of computational resources."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In ECCV, pp. 1935. Springer, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? NeurIPS, 37:2705627087, 2024b. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. ACL, 2024c. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024d. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACM MM, pp. 1119811201, 2024."
        },
        {
            "title": "Preprint",
            "content": "Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, pp. 148166. Springer, 2024. Xuan Gong, Tianshi Ming, Xinpeng Wang, and Zhihua Wei. Damro: Dive into the attention mechanism of lvlm to reduce object hallucination. arXiv preprint arXiv:2410.04514, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CVPR, pp. 1437514385, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. Concept bottleneck models. In ICML, pp. 53385348. PMLR, 2020. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 35:2219922213, 2022. Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, pp. 1387213882, 2024. Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, and Kaipeng Zhang. Think or not think: study of explicit thinking in rule-based visual reinforcement finetuning. arXiv preprint arXiv:2503.16188, 2025. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. ACL, 2021. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. ICLR, 2023. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Yi Peng, Peiyu Wang, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, et al. Skywork r1v: Pioneering multimodal reasoning with chain-ofthought. arXiv preprint arXiv:2504.05599, 2025. Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? ACL, 2024."
        },
        {
            "title": "Preprint",
            "content": "Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement finetuning. ICML, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Aaditya Singh, Adam Fry, Adam Perelman, et al. Gpt-5 system card, 2025. URL https:// openai.com/index/gpt-5-system-card/. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ICLR, 2025. Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Argue: Attribute-guided prompt tuning for vision-language models. In CVPR, pp. 2857828587, 2024. Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, and Jing Zhang. Black sheep in the herd: Playing with spuriously correlated attributes for vision-language recognition. arXiv preprint arXiv:2502.15809, 2025a. Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Identifying and mitigating position bias of multi-image vision-language models. In CVPR, pp. 1059910609, 2025b. Chongjun Tu, Peng Ye, Dongzhan Zhou, Lei Bai, Gang Yu, Tao Chen, and Wanli Ouyang. Attention reallocation: Towards zero-cost and controllable hallucination mitigation of mllms. arXiv preprint arXiv:2503.08342, 2025. Haohan Wang, Xindi Wu, Zeyi Huang, and Eric Xing. High-frequency component helps explain the generalization of convolutional neural networks. In CVPR, pp. 86848694, 2020. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. NeurIPS, 37: 9509595169, 2024a. Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating multimodal math reasoning in multi-visual contexts. In CVPR, pp. 1954119551, 2025b. Xintong Wang, Jingheng Pan, Liang Ding, and Chris Biemann. Mitigating hallucinations in large vision-language models with instruction contrastive decoding. ACL Findings, 2024b. Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35: 2482424837, 2022."
        },
        {
            "title": "Preprint",
            "content": "Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025a. Shuo Yang, Yuwei Niu, Yuyang Liu, Yang Ye, Bin Lin, and Li Yuan. Look-back: Implicit visual re-focusing in mllm reasoning. arXiv preprint arXiv:2507.03019, 2025b. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025c. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. NeurIPS, 36:1180911822, 2023a. Yue Yao, Xinyu Tian, Zheng Tang, Sujit Biswas, Huan Lei, Tom Gedeon, and Liang Zheng. Training with product digital twins for autoretail checkout. arXiv preprint arXiv:2308.09708, 2023b. Yue Yao, Zelin Wen, Yan Tong, Xinyu Tian, Xuqing Li, Xiao Ma, Dongliang Xu, and Tom Gedeon. Simple radiology vllm test-time scaling with thought graph traversal. arXiv preprint arXiv:2506.11989, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. ICML, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, pp. 95569567, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In ECCV, pp. 169186. Springer, 2024. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Shu Zou, Xinyu Tian, Qinyu Zhao, Zhaoyuan Yang, and Jing Zhang. Simlabel: Consistency-guided ood detection with pretrained vision-language models. arXiv preprint arXiv:2501.11485, 2025."
        },
        {
            "title": "CONTENTS",
            "content": "A EXPERIMENTAL DETAILS A.1 BENCHMARK STATISTICS . A.2 BASELINE SETTINGS . A."
        },
        {
            "title": "ERROR ANALYSIS",
            "content": ". . . . . . . . . . . . A.4 ATTENTION VISUALIZATION . . . . . A. INFERENCE-LEVEL REMEDIES . A.6 VISUAL CLAIM GENERATION . A.7 TRAINING CONFIGURATION . . . . . . . . . . . . . . . . MORE EVALUATION B.1 ERROR CATEGORY DISTRIBUTION . B.2 VISUAL ANCHOR SCORE . . . . B.3 PERCEPTION REWARD WEIGHT . . . . . . . . . . . . . . . . . . . . . . . . B.4 ABLATION ON DATA AUGMENTATION . B. EFFECT OF VISUAL CLAIMS . B.6 KL PENALTY COEFFICIENT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.7 VAPO WITH INFERENCE-LEVEL REMEDIES . B.8 ATTENTION-BASED REWARD . . . . . . . B.9 COMPUTATIONAL EFFICENCY AND COST B.10 LIMITATION ANALYSIS . . . . . . . . . C. BROADER SOCIETAL IMPACTS D. SUPPLEMENTARY RESULTS D.1 FULL NUMERICAL MAIN RESULTS . D.2 NUMERICAL RESULTS OF . D.3 NUMERICAL RESULTS OF β . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 FULL RESULTS OF AUGMENTED BASELINES . D. FULL RESULTS OF 3B PARAMETER SCALE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 16 17 17 18 19 19 19 20 20 21 22 22 22 23 24 24 24 24"
        },
        {
            "title": "A EXPERIMENTAL DETAILS",
            "content": "A.1 BENCHMARK STATISTICS In the main paper, we provide high-level overview of the selected benchmarks. Here, we present detailed information for clarity and ease of reproducibility, including dataset types, sizes, and splits. All evaluation scripts are implemented using the VLMEvalKit framework (Duan et al., 2024). 1) MathVerse is benchmark comprising 2,612 high-quality mathematical questions. Each example provides varying levels of multimodal information. Following prior work, we evaluate on the Test Mini split using the Vision Only setting, which includes approximately 700 samples. 2) MathVista is comprehensive mathematical benchmark that evaluates range of skills including puzzle solving, algebraic reasoning, and scientific understanding, and comprises 6,141 examples. For our evaluation, we use the Test Mini split, which contains approximately 1,000 examples. 3) MathVision comprises 3,040 high-quality mathematical problems sourced from real-world math competitions, spanning 16 distinct disciplines and five levels of difficulty, providing comprehensive and challenging benchmark for evaluating VLMs. We conduct our evaluation using the full test set. 4) LogicVista assesses the fundamental logical reasoning capabilities of VLMs, covering range of reasoning types including spatial, deductive, inductive, numeric, and mechanical. The benchmark comprises 448 visual multiple-choice questions. We conduct our evaluation on the full test set. 5) WeMath comprises 6.5k visual math questions structured around 67 hierarchical knowledge concepts across five levels of granularity. We evaluate on the Test Mini split, which contains approximately 1,740 examples, and report the strict score as the primary evaluation metric. 6) Geometry3k consists of 3,002 geometry problems with dense annotations in formal language, requiring abstract problem-solving and symbolic reasoning based on axiomatic knowledge. For evaluation, we combine the validation and test splits, resulting in approximately 900 examples. 7) MMMU is challenging benchmark that covers broad range of disciplines, requiring collegelevel subject knowledge and reasoning. It contains 11.5k curated multimodal questions sourced from college exams, quizzes, and textbooks. We perform our evaluation on the validation split. 8) MMStar is vision-indispensable multimodal benchmark specifically designed to ensure that each sample exhibits strong visual dependency and requires advanced multimodal reasoning capabilities. It comprises 1,500 samples for offline evaluation. Here we evaluate on the full test set. 9) HallusionBench is designed to challenge advanced VLMs by emphasizing fine-grained understanding and interpretation of visual information. It consists of 346 manually curated images paired with 1,129 questions. In this study, we conduct our evaluation on the full test split. 10) MMVet comprises 218 examples and defines six core vision-language capabilities, focusing on their integration to evaluate the synergy among different skills. It is designed to assess the overall competence of generalist models. We perform our evaluation on the full test split. A.2 BASELINE SETTINGS In the main text, we compare our method against several popular baselines to validate its effectiveness. For models such as GPT-5-Thinking, Gemini-2.5-Pro, and InternVL-2.5, we directly report results from the OpenCompass leaderboard. For other models, we provide reproduced results when official numbers are not available in the original papers. Below, we detail the settings used for these reproduced baseline models for reference. All results are reproduced using greedy decoding. 1) R1-OneVision includes both 3B and 7B variants, which are first trained via SFT followed by RL, with particular focus on mathematical reasoning tasks. In our experiments, we adopt the publicly available 7B checkpoint based on Qwen2.5-VL-7B. 2) VLAA-Thinker is an RL-only model trained on high-quality and challenging dataset, and is the first to demonstrate that RL outperforms SFT in multimodal settings. In our experiments, we use the VLAA-Thinker-7B variant, which is also based on Qwen2.5-VL-7B."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: The designed prompt to instruct GPT-5 to assist in categorizing the different error types. 3) Vision-R1 is powerful reasoning model trained at scale, with the combined amount of RL and SFT data being approximately five times larger than that used in our setting. In our experiments, we use Vision-R1-7B, which represents the strongest variant reported by the authors. A.3 ERROR ANALYSIS In Section 3.1, we conduct comprehensive error analysis by leveraging GPT-5 to categorize different types of errors and quantify their distributions, aiming to assess the impact of reasoning on failure cases. The prompt used for GPT-5 input is shown in Fig. 8. As illustrated, we provide GPT-5 with four options: three predefined error categories, along with an Others category to account for small number of outliers, such as cases with uninterpretable content. We then compute the proportion of each error type to support our analysis. For completeness, in addition to the perception error examples as shown in Fig. 1, here we also provide representative cases of logical errors and knowledge errors, as illustrated in Fig. 9. Figure 9: The representative examples of logical and knowledge errors."
        },
        {
            "title": "Preprint",
            "content": "A.4 ATTENTION VISUALIZATION In the main paper, we visualize the attention over image tokens to reflect the contribution of visual information to the models reasoning process. Here, we provide additional details on the implementation. For each generation step, we compute the sum of attention scores after softmax assigned by the output token to all preceding image tokens. This yields ratio between 0 and 1, which is then averaged across all layers to produce the final attention value. Additionally, in Fig. 3 (A), since inserting images, i.e., visual replay, or instructions, i.e., focus prompt, may alter the final reasoning trajectory and result in slightly different sequence lengths, we normalize the comparison by truncating all outputs to the first 250 tokens, which basically covers the full response. This strategy is also applied in Fig. 5 (A) when comparing the visual attention between Vision-R1 and VAPO-Thinker. A.5 INFERENCE-LEVEL REMEDIES In the main text, we introduce two inference-level remedies, i.e., visual replay and focus prompt, as preliminary strategies to demonstrate the negative impact of visual forgetting on reasoning. For completeness, we provide detailed description of both approaches here. In the visual replay strategy, the input image is reintroduced periodically during the models reasoning process at regular intervals. In practice, to improve efficiency and prevent exceeding the models context length, the reinserted image is downsampled to lower resolution. Furthermore, the insertion points are selected to satisfy two criteria: (1) uniform segmentation of the reasoning trajectory, and (2) alignment with logical boundaries such as commas, periods, or line breaks to avoid interrupting syntactic or semantic units. For the focus prompt strategy, at each insertion point, we randomly sample one prompt from set of three manually designed instructions including need to see the image, have to look back and Let me verify against the visual input, to ensure robustness against prompt variability. For alignment, we adopt the nearby insertion positions as used in visual replay, facilitating consistent comparison between the two methods. It is worth noting that in visual replay, the image is reintroFigure 10: The user prompt for GPT-5 to generate visual claims for VAPO."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: The examples of the GPT-generated visual claims. duced as part of the user prompt in dialogue format, whereas in focus prompt, the instruction is directly injected into the assistants response. A.6 VISUAL CLAIM GENERATION Our method encourages the model to rely more effectively on visual input by introducing perception reward, which is derived from the models ability to evaluate set of visual claims during the reasoning process. This evaluation serves as proxy for assessing the models perceptual capability at varying reasoning stages. To support this, we prompt GPT-5 to generate specified number of visual claims conditioned on the input image. An example of the user prompt used for this purpose is shown in Fig. 10. We require the generated visual claims to be concise factual statements, free from hedging terms such as possibly or appears. Each set of claims must contain an equal number of correct and incorrect statements. Furthermore, the claims are constructed to be independent of the corresponding example question, ensuring that the model must refer to the visual input rather than relying on prior outputs when evaluating their validity. While several generated visual claims have already been presented in Fig. 4, we provide more illustrative examples in Fig. 11 for reference. A.7 TRAINING CONFIGURATION In Section 5, we briefly outline the training setup. Here, we provide more detailed description. By default, we adopt ViRL39K as our training dataset, which is refined collection derived from multiple existing sources such as MM-Eureka (Meng et al., 2025), MV-Math (Wang et al., 2025b), and M3CoT (Chen et al., 2024c). The dataset covers wide spectrum of domains, including STEM subjects, social topics, chart reasoning, and spatial relations. It is worth noting that prior baselines rely on substantially larger datasets that combine SFT and RL, for example, 155k samples for R1OneVision and 210k samples for Vision-R1. Although this comparison is not fair for us in terms of training data scale, it further highlights the effectiveness of our proposed method. During the rollout phase, we sample 5 responses per example with temperature of 1.0, and employ vLLM as the backend to accelerate decoding. We then insert visual anchors into these generated responses to evaluate the models perceptual capability. For each anchor, we randomly sample prefix of the reasoning content, append visual claim, and instruct the model to judge its correctness. To ensure binary decision-making, the decoding process is constrained to produce either yes or no, with the temperature fixed at 0.0. Importantly, throughout this stage, we adopt the default system prompt of Qwen2.5-VL without explicitly introducing the presence or meaning of anchors. This design avoids altering the models inherent behavior and keeps anchors fully transparent, serving as an implicit reward mechanism that encourages the model to rely more heavily on visual cues."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: The error ratio of Vision-R1 as well as the correction rate achieved by our method across benchmarks. In the bar chart, the full height of each bar represents the overall error rate of VisionR1, with the light-colored segment indicating the proportion of errors successfully corrected by our method, and the dark-colored segment corresponding to the remaining uncorrected errors. For training, we employ 8 NVIDIA A100-80G GPUs. The policy loss follows the default configuration of GRPO, where the clip ratio ϵ is set to 0.2, and the KL penalty coefficient λ is fixed at 1e2. For VAPO, we set the anchor number = 20, the late-emphasis weight β = 1.5, and the perception reward weight γ = 0.1. The entire training process is conducted using verl (Sheng et al., 2025)."
        },
        {
            "title": "B MORE EVALUATION",
            "content": "B.1 ERROR CATEGORY DISTRIBUTION In the main text, we analyze the error cases of the representative baseline Vision-R1 and observe that perception errors constitute the majority, underscoring the detrimental impact of reasoning on visual information utilization. To assess the effectiveness of our approach in addressing this issue, we further examine the correction rate of our method across the different error categories of the baseline model, as illustrated in Fig. 12. We observe that, for cases where the baseline model fails, our method substantially corrects significant portion of perception errors, with improvements reaching up to 20% on vision-intensive benchmarks. This perceptual correction capability constitutes major source of the performance gains achieved by our approach. B.2 VISUAL ANCHOR SCORE Our method assesses the models overall perceptual capability by setting up visual anchors at different stages of the reasoning process. As shown in Fig. 6, the perception reward exhibits clear Figure 13: The anchor score across varying anchor positions, where higher index corresponds to later stage in the reasoning process. Scores are averaged across all examples within the benchmark."
        },
        {
            "title": "Preprint",
            "content": "γ MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. 0.00 0.05 0.10 0.15 0.20 44.2 47.7 48.9 48.4 47.9 44.4 46.3 47.3 46.7 46.6 44.5 48.2 47.7 48.1 47.4 53.6 55.5 56.7 56.2 55. 55.2 58.6 59.1 59.6 59.4 51.2 53.7 55.5 56.7 54.2 49.0 51.7 52.5 52.4 51.9 Table 4: The benchmark accuracy with varying perception reward weight γ. upward trend throughout training, indicating that the model becomes increasingly proficient at evaluating visual claims and progressively relies more on visual information. To further investigate the role of individual visual anchors, we visualize the distribution of anchor scores before and after training, i.e., comparing the base model with the model after applying our method. As shown in Fig. 13, the base model, i.e., Qwen2.5-VL-7B, exhibits clear downward trend in anchor scores as reasoning progresses, eventually approaching the level of random guessing (around 0.5). In contrast, our trained model significantly improves anchor scores, maintaining stable level around 0.9 throughout the reasoning process. This indicates substantial enhancement in perceptual capability as result of our method. B. PERCEPTION REWARD WEIGHT In the main text, we have conducted ablation studies on the anchor number and the late-emphasis weight β to identify their optimal settings. Here, we further investigate the impact of perception reward weight γ on model performance. For computational efficiency, in this experiment we do not train on the full dataset but instead randomly sample 5000 examples from the full 39k training set. As shown in Table 4, setting γ = 0 reduces the training procedure to vanilla GRPO without perceptual supervision. As γ increases, the average accuracy across benchmarks improves significantly, peaking around γ = 0.1. Notably, mathematical tasks tend to favor smaller values of γ, which is intuitive given the relatively simple visual structures of these tasks. In contrast, vision-intensive benchmarks such as MMStar and HallusionBench benefit from more aggressive settings, reflecting their greater reliance on strong perceptual capability. B.4 ABLATION ON DATA AUGMENTATION In our approach, visual claims play central role by providing reward signals that explicitly encourage the model to rely more heavily on visual inputs. This mechanism helps mitigate visual forgetting and improves accuracy across benchmarks. However, since these visual claims are generated by GPT and accompanied by correctness labels, they may be viewed as new synthetic examples augmented to the original training data. This raises critical question: are the performance gains brought by VAPO primarily attributable to the reward-driven promotion of visually grounded reasoning, or are they simply result of data augmentation through the inclusion of these synthetic visual examples? To investigate this question, we introduce new baseline in which the generated visual claims are directly transformed into additional training examples and integrated into the original dataset. For computational efficiency, we randomly sample 5000 examples from the full dataset, as described in Appendix B.3. For each example, we generate 5 visual claims, resulting in fivefold expansion of the training set to 30k examples. This setup allows us to isolate the performance gains attributable purely to data augmentation with synthetic visual claims. The baseline is trained using the standard GRPO algorithm without any additional modifications. In contrast, under the VAPO framework, Method MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. GRPO GRPOaug VAPO 44.2 44.7 45.9 44.4 45.2 46.1 44.5 43.9 46.7 53.6 54.2 55.2 55.2 54.8 57. 52.2 53.1 54.3 49.0 49.3 51.0 Table 5: The comparison with VAPO and GRPO augmented with visual claim examples."
        },
        {
            "title": "Preprint",
            "content": "Method MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. GRPO NonVisualClaim QAlignedClaim VisClaim 44.2 45.7 44.9 48.9 44.4 44.9 45.8 47.3 44.5 44.3 46.1 47. 53.6 54.8 55.3 56.7 55.2 57.2 57.8 59.1 51.2 51.9 53.5 55.5 49.0 49.8 50.6 52.5 Table 6: The comparison with non-visually-dependent claims and question-aligned claims. these same visual claims are used as anchors during training, with the anchor number set to = 5 to match the available number of claims per instance. As shown in Table 5, augmenting vanilla GRPO with additional examples constructed from visual claims yields only marginal improvement over the original dataset (49.0 49.3). In contrast, our method achieves substantial performance gain of 2% (49.0 51.0). This discrepancy may be attributed to two key factors: 1) The visual claims are generally short, simple, and strongly dependent on visual information, often solvable without long reasoning trajectories, thus offering limited learning capacity; 2) The augmented examples are derived from small number of unique images, i.e., five examples share the same image, significantly reducing data diversity and increasing the risk of overfitting. These results suggest that interpreting the role of visual claims purely as form of data augmentation is not appropriate. Rather, the performance gains of VAPO are primarily driven by its promotion of visually grounded reasoning. B.5 EFFECT OF VISUAL CLAIMS In our method, visual claims serve as critical proxy for measuring models perceptual capability. To better understand their influence, we investigate how different types of visual claims affect model performance. In addition to our default setting, we consider two alternative variants: 1) non-visuallydependent claims: although these are factual statements about the image, they emphasize external knowledge or logical reasoning rather than concrete visual details; 2) question-aligned claims: these are closely related to the specific question associated with the image, potentially allowing the model to assess claim correctness based on its own historical reasoning outputs rather than pure visual grounding. To generate these claims, we prompt GPT accordingly: for the first variant, we explicitly instruct GPT to produce claims that are not visually dependent; for the second, we provide both the image and the corresponding question, asking it to output claims relevant to the question content. For efficiency, we conduct the analysis using 5000 examples sampled from the full training set. As shown in Table 6, both the non-visually-dependent and question-aligned variants perform substantially worse than our default visual claim setup. This is likely due to the fact that non-visuallydependent claims can often be judged correctly without requiring strong perceptual capability, thereby diminishing the core purpose of VAPO. On the other hand, question-aligned claims are highly correlated with the question content, allowing the model to infer their correctness from its own reasoning traces without relying on visual input. These results highlight that the design of visual claims is crucial to the effectiveness of the VAPO framework. B.6 KL PENALTY COEFFICIENT In the previous sections, we have analyzed the impact of VAPO-specific hyperparameters, including the anchor number K, the late-emphasis weight β, and the perception reward weight γ. For the underlying GRPO framework, we adopt the default setting for the KL penalty coefficient λ = 1e2. λ (1e2) MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. 0 1 2 5 48.1 48.9 48.3 48.1 47.1 47.3 47.0 46.2 47.3 47.7 47.9 48. 57.1 56.7 57.2 57.1 59.3 59.1 58.4 59.8 55.3 55.5 54.9 54.3 52.4 52.5 52.3 52.3 Table 7: The impact of KL penalty coefficient λ to the benchmark accuracy."
        },
        {
            "title": "Preprint",
            "content": "Method MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. VAPO VAPOFP VAPOVR 53.3 53.1 53.7 50.9 51.2 50.4 51.3 50.7 51.8 60.2 60.5 57. 63.0 63.4 62.7 57.4 57.9 58.2 56.0 56.1 55.8 Table 8: The impact of VAPO augmented with inference-level remedies. For completeness, here we also examine the effect of varying λ on the performance of VAPO. To maintain computational efficiency, we conduct this analysis on randomly sampled subset of 5000 examples from the full training set. As shown in Table 7, VAPO exhibits minimal sensitivity to the choice of λ. Regardless of whether the KL penalty is applied or varied in magnitude, the average accuracy fluctuates by no more than 0.2%. B.7 VAPO WITH INFERENCE-LEVEL REMEDIES To further compare the effectiveness of our method with inference-level remedies, i.e., visual replay and focus prompt, we augment VAPO by incorporating these two straightforward approaches. We follow the same insertion strategy as in Fig. 3, where interventions are applied at four points throughout the reasoning process. As shown in Table 8, augmenting our model with inference-level remedies for visual forgetting results in minimal impact, with performance fluctuations within approximately 0.2%. This suggests that our method has already substantially enhanced the models reliance on visual input, effectively subsuming the benefits provided by these additional strategies. B.8 ATTENTION-BASED REWARD In the main text, we use the evolution of visual attention throughout the reasoning process as proxy to examine whether our method mitigates visual forgetting. Therefore, natural baseline to consider is more trivial alternative: directly using the attention scores of image tokens as reward signal to guide training. To explore this possibility, we introduce an attention reward, which quantifies the models overall visual attention ratio across layers during the reasoning process, as detailed in Appendix A.4. We replace the perception reward with this attention-based reward while keeping all other experimental configurations consistent with the main setup, termed as VAPOattn, whereas our original method using perception reward is referred to as VAPOperc. As shown in Table 9, naively maximizing visual attention provides little to no performance gain and even leads to noticeable degradation on certain tasks. We hypothesize that this is due to two key factors: 1) Although visual attention can serve as an indicator of the contribution of visual inputs to the models decision-making, it does not directly reflect whether the model is effectively utilizing visual features. Blindly encouraging higher attention may disrupt the base models learned distribution; 2) higher visual attention ratio is not inherently better. As observed in earlier experiments, its values typically lie between 0 and 0.3. Unconstrained maximization of this ratio may lead to instability or training collapse in later stages. B.9 COMPUTATIONAL EFFICENCY AND COST In this section, we analyze the efficiency and computational cost of our method, and compare it against other policy gradient algorithms in terms of training time and performance gains. We consider two representative reinforcement learning baselines: GRPO and DAPO (Yu et al., 2025). The latter is widely adopted variant of GRPO that incorporates several advanced techniques, including Method MathVerse LogicVista Geometry3k MMMU MMStar HallBench Avg. GRPO VAPOattn VAPOperc 48.2 47.6 53.3 45.5 45.8 50.9 47.3 46.4 51.3 56.6 57.3 60. 58.9 59.4 63.0 53.2 54.8 57.4 51.6 51.8 56.0 Table 9: The comparison with attention reward which directly maximizes visual attention ratio."
        },
        {
            "title": "Preprint",
            "content": "Method Epoch Time Accuracy Gain GRPO DAPO VAPO 2 2 2 18h46m 25h11m 19h14m 51.54 53.17 55.91 +1.63 +4.37 Table 10: The time costs and gains for different policy gradient algorithms. dynamic sampling and higher clipping ratio. As shown in Table 10, under the same data budget and training epochs, DAPO requires approximately 6 additional hours of training time compared to GRPO, whereas our method incurs only marginal increase of around 30 minutes. More importantly, this modest computational overhead yields substantial performance gain: our method improves accuracy by 4.37% over GRPO, significantly surpassing the benefits provided by DAPO. This efficiency is largely attributable to the nature of our perceptual supervision. The key difference of our method from GRPO lies in the anchor scoring process, where the model is asked to evaluate visual claims. Unlike standard rollouts, which require generating full reasoning traces, our method only requires binary judgment, i.e., yes or no for each claim, essentially single-token output. Consequently, this process is highly efficient. Moreover, our gradient update procedure remains identical to that of GRPO. These results suggest that our approach introduces minimal additional computational cost while achieving notably greater performance improvements. B. LIMITATION ANALYSIS Visual Claim Quality. One factor influencing the effectiveness of VAPO is the quality of the generated visual claims. This includes whether the claims are strongly vision-dependent, clearly verifiable from the image, and correctly labeled. As such, the claim generation model, i.e., GPT-5 in our current setup, may become limiting factor for overall performance. However, this also suggests that VAPO retains great potential for further improvement: leveraging higher-quality visual claims, either generated by more capable models or annotated by human experts, could potentially enhance the effectiveness and further improve the models performance ceiling. Hyperparameter Sensitivity. Our method introduces several new hyperparameters including the anchor number K, the late-emphasis weight β, and the perception reward weight γ, all of which require careful tuning to achieve optimal performance. Moreover, the optimal settings for these hyperparameters may vary across task types: vision-intensive tasks tend to benefit from larger values of β and γ, whereas logic-heavy tasks such as mathematical reasoning often prefer smaller values. Designing an adaptive vision-anchored policy that dynamically adjusts these parameters based on task characteristics remains an important direction for future work. Single Image Setting. In the current work, we focus exclusively on single-image tasks for simplicity. However, VAPO can be readily extended to multi-image or even video-based tasks by generating visual claims that reference multiple frames or views. Exploring the benefits of vision-anchored training in such settings, particularly in multi-image reasoning or temporal video understanding presents an interesting and promising direction for future research. C. BROADER SOCIETAL IMPACTS Our work carries several positive societal implications. Our findings reveal that existing VLMs often produce reasoning that is not visually grounded, which can degrade performance and reliability. Such ungrounded reasoning may introduce misleading or hallucinated content, thereby posing risks to model safety and trustworthiness. Moreover, since these reasoning traces often rely heavily on statistical patterns learned from training data, they may inadvertently expose sensitive information, raising concerns about data privacy and potential leakage. In contrast, the proposed VAPO framework steers the models reasoning process to remain anchored in visual evidence. This grounding mechanism helps ensure that model outputs are more faithful to the input image and less likely to include irrelevant or speculative content, thus promoting both factual accuracy and privacy preservation. At present, we have not identified negative societal impacts associated with this work. How-"
        },
        {
            "title": "Preprint",
            "content": "ever, due to external factors such as the availability of datasets and baseline implementations, further assessment may be necessary in the future. D."
        },
        {
            "title": "SUPPLEMENTARY RESULTS",
            "content": "D."
        },
        {
            "title": "FULL NUMERICAL MAIN RESULTS",
            "content": "Due to space limitations in the main text, some baseline results are omitted. Here, we provide the full numerical results of our method and all baselines across the ten established benchmarks in Table 11, corresponding to Table 1 and Table 2. D.2 NUMERICAL RESULTS OF Here we report the numerical results of the ablation study on the anchor number in Table 12, corresponding to the visualization in Fig. 7 (A). D.3 NUMERICAL RESULTS OF β Similarly, we present the numerical results of the ablation study on the impact of β in Table 13, corresponding to Fig. 7 (B). D. FULL RESULTS OF AUGMENTED BASELINES Here we provide the full numerical results of our method compared with baselines augmented with inference-level remedies, i.e., visual replay and focus prompt, in Table 14, corresponding to Table 3 in the main paper. D.5 FULL RESULTS OF 3B PARAMETER SCALE Here for completeness, we report the evaluation results of our proposed method and baseline models at the 3B scale. To ensure fairness, we select baselines with comparable model sizes and exclude those without publicly available 3B-scale checkpoints. The results are presented in Table 15."
        },
        {
            "title": "Preprint",
            "content": "Models MathVerse MathVista MathVision LogicVista WeMath Geo3k MMMU MMStar HallBench MMVet Avg. Close-source Models GPT-5-Thinking Gemini-2.5-Pro Open-source Models Qwen2.5-VL-7B InternVL2.5-8B R1-OneVision-7B VLAA-Thinker-7B Vision-R1-7B Our Models VAPO-Thinker-3B VAPO-Thinker-7B 81.2 76.9 40.7 34. 46.4 48.2 52.4 35.8 53.3 81. 80.9 62.3 68.2 64.1 68.0 73. 67.1 75.6 72.0 69.1 23.2 25. 29.9 26.4 28.2 23.9 31.9 70. 73.8 42.6 38.3 45.6 48.5 49. 39.7 50.9 71.1 78.0 33.1 38. 44.6 41.5 41.6 35.4 43.6 79. 77.2 38.5 44.8 46.1 50.6 49. 44.2 51.3 81.8 74.7 52.7 56. 54.3 59.1 57.6 55.6 60.2 75. 73.6 54.9 63.2 54.1 49.7 61. 59.4 63.0 65.2 64.1 50.0 49. 52.5 54.7 49.5 49.5 57.4 77. 83.3 75.6 75.2 64.8 62.8 65. 70.0 71.1 46.3 48.1 50.3 51. 53.4 64.6 71.9 47.5 55.9 Table 11: The full numerical results of main experiments, corresponding to Table 1 and Table 2."
        },
        {
            "title": "Preprint",
            "content": "K MathVerse MathVista MathVision LogicVista WeMath Geo3k MMMU MMStar HallBench MMVet Avg. 0 5 10 15 48.2 50.9 52.8 53.2 53.3 70. 73.1 74.8 75.8 75.6 26.1 28. 30.7 31.5 31.9 45.5 47.8 49. 50.4 50.9 39.1 41.2 42.5 43. 43.6 47.3 50.5 51.2 51.0 51. 56.6 58.4 59.9 60.4 60.2 58. 61.3 62.2 62.7 63.0 53.2 55. 56.7 57.0 57.4 69.9 70.4 71. 71.6 71.9 51.5 53.8 55.1 55. 55.9 Table 12: The numerical results of ablation study of K, corresponding to Fig. 7 (A)."
        },
        {
            "title": "Preprint",
            "content": "β MathVerse MathVista MathVision LogicVista WeMath Geo3k MMMU MMStar HallBench MMVet Avg 0.0 0.5 1.0 1.5 2. 2.5 51.7 52.7 53.1 53.3 53. 51.8 73.8 74.8 75.7 75.6 75. 74.8 28.4 30.2 31.5 31.9 31. 30.4 48.6 49.6 51.3 50.9 50. 50.0 42.0 42.8 43.4 43.6 43. 42.6 49.9 50.7 51.4 51.3 51. 50.5 58.9 59.5 60.0 60.2 60. 59.8 60.2 62.0 62.5 63.0 63. 62.2 53.8 55.4 56.8 57.4 56. 55.3 70.3 71.3 72.0 71.9 71. 70.2 53.8 54.9 55.8 55.9 55. 54.7 Table 13: The numerical results of ablation study of β, corresponding to Fig. 7 (B)."
        },
        {
            "title": "Preprint",
            "content": "Models MathVerse MathVista MathVision LogicVista WeMath Geo3k MMMU MMStar HallBench MMVet Avg VLAA-Thinker-7B + Focus Prompt + Visual Replay Vision-R1-7B + Focus Prompt + Visual Replay VAPO-Thinker-7B 48.2 48.8 49.8 52. 52.8 53.4 53.3 68.0 68.4 69. 73.5 73.8 75.2 75.6 26.4 27. 27.9 28.2 29.3 29.9 31.9 48. 49.2 49.9 49.7 50.2 50.7 50. 41.5 41.8 42.3 41.6 42.1 42. 43.6 50.6 50.7 51.1 49.0 49. 50.5 51.3 59.1 59.7 60.5 57. 58.7 59.4 60.2 49.7 51.1 52. 61.4 61.8 62.1 63.0 54.7 55. 56.2 49.5 50.5 51.8 57.4 70. 70.7 71.7 71.1 71.4 71.9 71. 51.7 52.3 53.2 53.4 54.0 54. 55.9 Table 14: The full results of baselines augmented with test-time remedies, corresponding to Table 3."
        },
        {
            "title": "Preprint",
            "content": "Models MathVerse MathVista MathVision LogicVista WeMath Geo3k MMMU MMStar HallBench MMVet Avg. Qwen2.5-VL-3B InternVL2.5-2B R1-OneVision-3B VLAA-Thinker-3B VAPO-Thinker-3B 30.5 22.3 35.5 36.4 38.2 60. 51.1 61.0 64.5 19.8 14. 23.7 24.4 27.3 37.6 27.3 38.5 41.4 20.4 8.0 33. 35.3 31.7 36.1 42.7 44. 48.6 43.2 50.5 51.6 51. 54.3 56.2 59.4 44.0 42. 45.1 49.5 58.4 62.6 63.9 64.6 40.3 36.1 45. 47.6 Table 15: The results of our 3B model and baselines of comparable scale. indicates unavailable results due to unreleased checkpoints. Vision-R1 is excluded as it does not consider 3B scale."
        }
    ],
    "affiliations": [
        "Australian National University",
        "GE Research",
        "University of Melbourne"
    ]
}