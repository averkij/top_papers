{
    "paper_title": "O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?",
    "authors": [
        "Zhen Huang",
        "Haoyang Zou",
        "Xuefeng Li",
        "Yixiu Liu",
        "Yuxiang Zheng",
        "Ethan Chern",
        "Shijie Xia",
        "Yiwei Qin",
        "Weizhe Yuan",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a critical examination of current approaches to replicating OpenAI's O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1's API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that a base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) A detailed technical exposition of the distillation process and its effectiveness, (2) A comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) A critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in a crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in first-principles thinking is paramount."
        },
        {
            "title": "Start",
            "content": "O1 Replication Journey Part 2: Surpassing O1-preview through Simple Distillation Big Progress or Bitter Lesson? Zhen Huang4* Haoyang Zou4* Xuefeng Li1,4* Yixiu Liu1,4* Yuxiang Zheng1,4* Ethan Chern1,4* Shijie Xia1,2,4* Yiwei Qin4 Weizhe Yuan3 Pengfei Liu1,2,4 1Shanghai Jiao Tong University, 2SII, 3NYU, 4Generative AI Research Lab (GAIR)"
        },
        {
            "title": "Abstract",
            "content": "This paper presents critical examination of current approaches to replicating OpenAIs O1 model capabilities, with particular focus on the widespread but often undisclosed use of knowledge distillation techniques. While our previous work (Part 1 (Qin et al., 2024)) explored the fundamental technical path to O1 replication, this study reveals how simple distillation from O1s API, combined with supervised fine-tuning, can achieve superior performance on complex mathematical reasoning tasks. Through extensive experiments, we show that base model fine-tuned on simply tens of thousands of samples O1-distilled long-thought chains outperforms O1-preview on the American Invitational Mathematics Examination (AIME) with minimal technical complexity. Moreover, our investigation extends beyond mathematical reasoning to explore the generalization capabilities of O1-distilled models across diverse tasks: hallucination, safety and open-domain QA. Notably, despite training only on mathematical problem-solving data, our models demonstrated strong generalization to open-ended QA tasks and became significantly less susceptible to sycophancy after fine-tuning. We deliberately make this finding public to promote transparency in AI research and to challenge the current trend of obscured technical claims in the field. Our work includes: (1) detailed technical exposition of the distillation process and its effectiveness, (2) comprehensive benchmark framework for evaluating and categorizing O1 replication attempts based on their technical transparency and reproducibility, (3) critical discussion of the limitations and potential risks of over-relying on distillation approaches, our analysis culminates in crucial bitter lesson: while the pursuit of more capable AI systems is important, the development of researchers grounded in firstprinciples thinking is paramount. This educational imperative represents not just technical consideration, but fundamental human mission that will shape the future of AI innovation.1 Relevant resources will be available at https://github.com/GAIR-NLP/O1-Journey. 4 2 0 2 5 2 ] . [ 1 9 8 4 6 1 . 1 1 4 2 : r Figure 1: Illustration of our O1 replication journey from September 12 to November 22, 2024. * Co-first authors Corresponding author 1Per OpenAIs Terms of Use, our distillation of the OpenAI O1 series models is strictly for research purposes and will not be fully disclosed publicly."
        },
        {
            "title": "Introduction",
            "content": "The landscape of AI research has been dramatically transformed since OpenAIs announcement of their O1 model (OpenAI, 2024), which demonstrates unprecedented capabilities in complex reasoning tasks, particularly in mathematical problem-solving. This breakthrough has catalyzed race among research institutions and companies worldwide to replicate these capabilities, leading to numerous claimed successes in recent weeks (Team, 2024b; Qin et al., 2024; Team, 2024a; kimi, 2024; kunlun, 2024; deepseek, 2024). However, this surge of announcements has brought to light concerning trend in the research community - one that prioritizes rapid performance gains over transparent technical innovation. In exploring recent developments in O1 replication efforts, we demonstrate straightforward yet powerful approach: knowledge distillation (Hinton, 2015) from O1s API. This method involves directly prompting O1 with complex problems to generate long-thought chains, which are then used for supervised fine-tuning or reinforcement learning (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022) of other models. Through our experiments, we show that with just tens of thousands of distilled samples and standard supervised fine-tuning, base model can surpass O1-previews performance on the American Invitational Mathematics Examination (AIME). While this approach can indeed yield impressive performance metrics, its widespread but undisclosed use raises significant concerns about the current state and future direction of AI research. The implications of this shortcut approach extend far beyond mere technical considerations: (1) First, the lack of transparency in methodology reporting makes it increasingly difficult for the research community to accurately assess and build upon claimed advances. Many institutions may obscure their actual methodologies while making ambitious claims about their technical capabilities, creating distorted picture of the fields progress. (2) Second, this trend is fostering concerning pattern of innovation stagnation, where researchers become increasingly reliant on existing powerful models rather than developing fundamental new techniques. The focus shifts from original technical contributions to sophisticated prompt engineering, potentially stunting the fields long-term growth. (3) Moreover, models trained through distillation face inherent limitations - they are naturally bounded by the capabilities of their teacher model (in this case, O1), creating ceiling effect that may impede genuine advancement. This dependency cycle not only limits potential breakthroughs but also restricts the ability to extend capabilities to new domains or surpass existing benchmarks. (4) Perhaps most concerning is the educational impact: we are missing crucial opportunities to cultivate genuine research skills and problem-solving abilities in the next generation of AI researchers. To facilitate this transparency, we introduce novel benchmark framework for categorizing and evaluating O1 replication attempts based on their technical transparency and reproducibility. This framework provides clear metrics for assessing the transparency and the openness of different approaches, creating standardized platform for comparing various replication efforts. Through this systematic evaluation, we hope to encourage more rigorous and honest reporting of technical achievements in the field. Our work serves not only as technical contribution but also as call to action for the AI research community. We argue that while distillation approaches offer immediate performance gains, they risk creating dependency cycle that could ultimately impede genuine technological advancement. As the field continues to pursue increasingly advanced reasoning capabilities, we believe it is crucial to maintain balance between performance improvements and genuine technical innovation. The path forward requires renewed commitment to the fundamental values of scientific inquiry: transparency, originality, and genuine innovation. By openly acknowledging both the power and limitations of current approaches, we hope to foster an environment that encourages researchers to invest in fundamental technical innovations rather than relying solely on existing solutions. This paper aims to initiate broader discussion about research practices in AI and advocate for return to more transparent and innovative approaches to advancing the field."
        },
        {
            "title": "2 The “Shortcut” Path to O1 Replication",
            "content": "2.1 Core Technical Stack for O1 Replication In the first part of our o1 replication journey (Qin et al., 2024), we introduce novel method to synthesize long thinking processes called journey learning, as illustrated in Figure 2. The approach utilizes tree-searching algorithms (e.g., Monte Carlo) to explore different solution paths, followed by strategic node selection to construct promising exploration trajectories. These exploration trajectories often contain incorrect results or unpromising methods and end with the correct answers. To address the lack of reflection content in the trees, we leverage LLMs to analyze previous steps and identify reasoning errors, enabling better course correction. This process produces complete trajectories leading to correct answers. We collect these trajectories, including both reflection and correction steps, to fine-tune the LLMs. The tuned LLMs can then be utilized for subsequent iterations of training."
        },
        {
            "title": "2.2 Alternative Methods for Long-thought Synthesis",
            "content": "Figure 2: The framework of journey learning. Figure 3: Different methods of collecting the long thought data. The distillation method offers cost-effective and reliable approach to obtaining high-quality data. 2.2 Alternative Methods for Long-thought Synthesis In the O1 technical pipeline, one of the most challenging aspects is effectively synthesizing long chains of reasoning for solving complex problems. These chains typically incorporate reflection, error correction, and backtracking steps. While tree search, as discussed above, represents one of the most effective approaches, it can be computationally expensive and time-consuming. Beyond tree search, alternative methods for synthesizing long reasoning chains are listed as follows. Each of these methods offers different trade-offs between computational efficiency and reasoning thoroughness. Method I: Complete Human Thought Process Annotation Human problem-solving rarely follows linear path to success or failure. Instead, people regularly pause to reflect, backtrack, and revise their approach when encountering obstacles. This natural process mirrors the characteristics of long thought. By thoroughly documenting how humans solve problems, we can generate authentic long thought training data. Method II: Multi-Agent Approach Different from journey learning where the policy model does not react to feedback directly, we can involve multi-agents to complete the exploration process, instructing them to play different roles. For example, we can construct multi-agent debate system where policy model generates continuous reasoning while critique model evaluates whether to proceed or backtrack. This interactive process naturally produces long thought training data when solutions are found. Method III: Distillation from Advanced Models Advanced models like o1 demonstrate strong reflection and self-correction abilities. Following common practice of instructing weaker models using stronger ones, distilling responses from o1 is natural approach. However, careful prompting is needed since o1 restricts access to its internal thought processes. While diverse methods exist for generating long thoughts, the distillation method offers cost-effective and reliable approach to obtaining high-quality data. 2.3 Distillation-based Long Thought Synthesis Background of Distillation In the era of Large Language Models (LLMs), the quality of training data has emerged as critical factor in model development. Recent research indicates that data quality exerts more substantial influence on model performance than either model size or data volume. For instance, LIMA (Zhou et al., 2024) demonstrated superior performance through Supervised Fine-Tuning (SFT) using only 1,000 meticulously curated prompts and responses, outperforming models trained on extensive but lower-quality datasets. Similarly, Phi-1 (Gunasekar et al., 2023) achieved remarkable results by leveraging high-quality data synthesized from GPT-3.5, surpassing models with significantly larger parameter counts on both MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021a) benchmarks. Given advanced LLMs comprehensive knowledge base, sophisticated reasoning capabilities, and robust instruction-following abilities (Wei et al., 2022; Brown et al., 2020), coupled with their decreasing operational costs, the practice of distilling high-quality data from these models to train smaller models has become increasingly prevalent. Notable examples include Alpaca (Taori et al., 2023), an instruction finetuning dataset derived from GPT-3.5, and WizardLM (Xu et al., 2023), which enhances the complexity and diversity of existing instruction data. For reasoning tasks, which also have verifiable solutions, researchers have implemented rejection sampling methodologies that, when combined with distillation, enable the extraction and validation of advanced models reasoning processes (Zelikman et al., 2022; Yu et al., 2023) . Given O1s exceptional performance and sophisticated reasoning capabilities, implementing distillation process of its cognitive mechanisms represents the most viable approach for model replication. Post-training Data Curation To prepare the dataset for downstream post-training (e.g. SFT), we start with subset of Olympic-level problems from the open-source datasets and self-curated datasets. filtering process is applied to refine the dataset: we remove problems dependent on images, those lacking explicitly labeled answers, and all proof-based problems using carefully-designed rules, while retaining problems where the answer type is numerical. Reformatted Technology We use the reformatted technology (Fan et al., 2024) to further enhance the dataset, we use GPT-4o-mini to rewrite the original solutions. The rewriting process adheres to specific guidelines, ensuring that solutions are step-by-step, highly detailed, and longer in length. This step also standardizes the output format, requiring the final answers to be explicitly highlighted using boxed, aligning with the long thought format. Quality Control Mechanism We select Qwen2.5-Math-72B (Yang et al., 2024b) as our base model due to its exceptional foundational capability in mathematical reasoning. This strong baseline provides robust foundation for further enhancing the models reasoning abilities, ensuring solid starting point for subsequent improvements. 2.3.1 Supervised fine-tuning approach To familiarize and adapt the model to the long thought format, we perform an initial SFT phase before distillation. Using the refined and reformatted dataset described above, we train the model to generate longer, more fine-grained step-by-step solutions. This phase focuses on ensuring that the model becomes proficient in both producing detailed reasoning and adhering to standardized output style, preparing it for subsequent distillation phases. Following this, we proceed with the next SFT phase using the distilled dataset. This dataset, generated through our distillation process, is specifically curated to capture high-quality, detailed reasoning aligned with the long-thought format. During this phase, the model is further fine-tuned to not only enhance its reasoning capabilities but also to ensure consistency in producing precise and coherent outputs."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Benchmark Usage We select several widely recognized and commonly used benchmarks in the field of mathematical reasoning, chosen for their challenging nature. These include MATH (Hendrycks et al., 2021) and AIME. Specifically, we use the streamlined MATH500 subset to facilitate more extensive inference-time scaling experiments. For AIME, we utilize the newly released problems in 2024 to minimize the risk of data leakage (we refer to it as AIME2024). Additionally, we curate set of 30 problems from the 2024 China National High School Mathematics Competition, serving as an additional benchmark (MATH2024) to diversify and enrich our evaluation. This combination of benchmarks ensures comprehensive assessment of our models mathematical reasoning capabilities. 3.2 Evaluation Metric for Inference-Time Scaling Unlike conventional evaluation strategies that rely solely on metrics such as Pass@k (Chen et al., 2021b), Maj@k (Wang et al., 2022), or RM@k (Lightman et al., 2024), we introduce novel metric designed to evaluate model performance across varying computational cost scenarios. This new approach reflects the realities of inference-time scaling (Snell et al., 2024), where test-time compute plays crucial role in determining the effectiveness and efficiency of modern large-scale models. In the era of inference-time scaling, models like OpenAIs O1-series have demonstrated that performance is not solely dependent on training-time compute but also significantly influenced by the time spent thinking during inference. This shift necessitates more nuanced evaluation framework that accounts for the trade-off between computational cost and performance. Our proposed"
        },
        {
            "title": "3.3 Performance Analysis",
            "content": "metric directly addresses this by measuring the models reasoning ability under constrained test-token budgets, ensuring that evaluations reflect real-world constraints and deployment scenarios. Specifically, we measure the computational cost of model on given benchmark test set using the average token count for its outputs. This metric reflects the test-time computational expense, where longer average token outputs correspond to more extensive reasoning steps. Models capable of generating longer, more detailed outputs are often able to capture complex reasoning patterns more effectively, demonstrating their scalability under inference-time compute. Furthermore, this average token metric is inherently extensible. In scenarios where the evaluation requires higher average token count than what is typically generated in single response, we leverage the Maj@k metric to approximate the models performance without using any extra reward model. This approach reflects the models reasoning ability at extended computational costs, even when single output does not naturally reach the desired token length. By employing this method, we ensure scalable and fair evaluation framework that captures model performance across different inference-time compute settings. This approach avoids artificial constraints and allows for meaningful comparisons without relying on external reward signals, focusing solely on the models intrinsic reasoning capabilities. 3.3 Performance Analysis Comparison with O1s performance As is shown in Table 1, under similar reasoning computational costs (i.e., with comparable average output tokens on the corresponding benchmark), the distilled model demonstrates outstanding performance, surpassing the results of O1-preview on AIME2024. AIME(2024) MATH500 Model Accuracy # Average Token Accuracy # Average Token o1-preview o1-mini 12/30 21/30 Proprietary 9083 9903 Parameter Size: 72B Ours-72B 13/ 8016 85.5 90.0 87.2 1501 944 2235 Table 1: Comparison of the performance between the distilled O1-mini model and O1-series models on the AIME2024 and MATH500 benchmarks under specific inference cost constraints. Analysis of model behavior and limitations While the model achieves impressive results, there remains noticeable gap compared to O1-mini in terms of mathematical reasoning performance. Additionally, the generated long thought solutions still exhibit imperfections. Addressing these limitations is critical for closing the performance gap and ensuring the generated long thought solutions meet the highest standards of clarity and correctness."
        },
        {
            "title": "4 Application Beyond Math Reasoning",
            "content": "In this section, we investigate how the model trained on mathematic long thoughts generalizes when applied to other tasks or applications. Training Details To investigate the models generalization capability across different domains, we first construct diverse bilingual dataset through systematic data extraction and translation process. From our distilled O1 model outputs, we carefully select approximately 5,000 high-quality samples containing retrospective thinking and self-reflection elements. These samples are then translated into Chinese using GPT-4o mini model, resulting in balanced bilingual dataset. The final training dataset comprises 10,750 mixed Chinese-English sample pairs, where each sample consists of query-response pair. We then perform Supervised Fine-Tuning (SFT) on the Qwen2.5-72B-Instruct (Yang et al., 2024a) model (named as baseline) using this curated dataset to obtain our final model (named as Ours.) 4.1 Safety Setup To comprehensively assess the safety aspects of our models generalization capabilities, we construct diverse test set comprising 600 questions carefully selected from three established safety evaluation datasets: Flames (Huang et al., 2023), DiaSafety (Sun et al., 2022), and WildSafety (Liu et al., 2024). Specifically, we extract 200 questions from each dataset to ensure balanced representation across different safety scenarios. We utilize the Safety-J (Liu et al., 2024) to evaluate the responses from both the original and fine-tuned models."
        },
        {
            "title": "4.1 Safety",
            "content": "Model Safety Factuality General Flames DiaSafety WildSafety SimpleQA C-SimpleQA CFE-General CFE-Sycophancy Auto-J LIMA Baseline Ours 91.0 92.5 100.0 100.0 92.0 86.5 10.58 10. 47.08 45.76 69.08 62.65 89.70 92.65 81.6 88.0 77.2 87.2 Table 2: Performance comparison (accuracy) before and after SFT across different evaluation categories. The datasets are grouped into three categories: safety evaluation (Flames, DiaSafety, WildSafety), factuality evaluation (SimpleQA, Chinese SimpleQA, ChineseFactEval-General, ChineseFactEval-Sycophancy, and general evaluation (Auto-J, LIMA). Note: C-SimpleQA, CFE-General, and CFE-Sycophancy stand for Chinese SimpleQA, ChineseFactEval-General, and ChineseFactEval-Sycophancy, respectively. Figure 4: Case study on how model-generated long thoughts provide alternatives, resulting in safer responses. Results & Insights The evaluation results reveal interesting insights about the impact of our fine-tuning process on model safety. While performance improves slightly on Flames (91% to 92.5%) and remains stable on DiaSafety (100%), there is notable decrease on WildSafety (92% to 86.5%). Overall, the safety score drops marginally from 94.3% to 93.0% after fine-tuning. This slight decrease in safety metrics highlights crucial finding: even when using high-quality, O1-like long thought training data focused on retrospection and reflection, models can experience subtle degradation in safety performance if the training data lacks explicit safety alignment. We hypothesize that the improvement on Flames dataset might be attributed to its unique focus on testing models deep reflection capabilities compared to other datasets, which aligns well with our O1-like training data emphasizing thoughtful deliberation. Case Study To investigate why our fine-tuned model achieves better performance on the Flames dataset (from 91% to 92.5%), we conduct detailed analysis of typical cases from Flames. We find that most queries in Flames are designed to tempt models into prioritizing utility over safety, often leading to unsafe responses. Figure 4 presents representative case about storing and charging an electric bicycle in building corridor. Qwen2.5-72B-Instructs (the baselines) response demonstrates this utility-focused tendency by concentrating solely on anti-theft measures. The model provides detailed recommendations about lock selection, installation methods, and surveillance, directly addressing the users immediate concern about property security. However, it completely overlooks critical safety hazards, particularly the fire risks associated with charging electric bicycles in corridors, which could endanger multiple residents lives. In contrast, our model, after training on long-thought data, exhibits more comprehensive and systematic thinking patterns. Instead of immediately addressing the theft concern, it first identifies the fundamental safety issues: fire hazards from corridor charging, regulatory compliance, and community safety. The response demonstrates enhanced analytical depth through prioritizing life-threatening risks over property risks, considering multiple stakeholders including residents and property management, providing hierarchical analysis of different safety dimensions, and suggesting alternative solutions that balance both utility and safety. This case study reveals an important insight: the improved systematic thinking and long-form reasoning capabilities developed through our fine-tuning process contribute significantly to enhanced safety performance,"
        },
        {
            "title": "4.2 Hallucination",
            "content": "Figure 5: Case study on our model attempting to actively search and leverage external tools to solve short-form fact-seeking question. particularly in scenarios where safety considerations might be overshadowed by immediate utility concerns. The models ability to pause, reflect, and analyze situations comprehensively helps it identify potential safety issues that might be overlooked in more direct, utility-focused responses. However, the decreased performance on WildSafety (from 92% to 86.5%) suggests that enhanced thinking capabilities alone are insufficient for comprehensive safety alignment. While systematic thinking helps models identify potential safety issues, proper safety alignment remains crucial for consistently maintaining high safety standards across diverse scenarios. This finding indicates that future work should focus on combining systematic thinking capabilities with explicit safety alignment to achieve more robust and comprehensive safety performance. 4.2 Hallucination Setup We evaluated the factuality of the models before and after SFT. We used datasets from SimpleQA (Wei et al., 2024), ChineseSimpleQA (He et al., 2024), and ChineseFactEval (Wang et al., 2023). These datasets contain Chinese and English knowledge-based questions to verify model factuality. Notably, the ChineseFactEval dataset contains two subsets: general QA and sycophancy QA. The sycophancy QA subset includes misleading answers in the prompts to test the models propensity for sycophancy, while the general QA subset follows format similar to SimpleQA. All questions in these datasets require verifiable short-form answers. We evaluated the models responses against the golden answers using GPT-4o for more robust answer matching. Results & Insights Our results showed that models after SFT did not demonstrate significant improvement in factuality (10.58% to 10.41%, 47.08% to 45.76%, 69.08% to 62.65%). This was largely due to longer reasoning chains leading to additional hallucinationsspecifically, models attempting to use search engines and fabricating search results  (Fig. 5)  . Nevertheless, these attempts to actively use search engines suggest promising direction, and we believe that providing models with actual web access or tool-use (Gao et al., 2022; Chern et al., 2023) would significantly improve their factuality. Additionally, the enhanced reasoning chains in the post-SFT models offer detailed analysis and self-reflection capabilities that could help prevent hallucinations  (Fig. 6)  . We also found that models became slightly less susceptible to sycophancy after SFT (89.70% to 92.65%). This improvement can be attributed to the self-reflection process, where models are able to discern and think deeply about unreasonable assumptions presented in the prompt rather than accepting them without question  (Fig. 7)  . Case Study In Fig. 5, we observed that our model attempts to utilize search engines and has the potential to collect and cross-verify results from multiple sources. Although these search engine interactions are simulated (as we did not incorporate access to external databases), this behavior demonstrates promising potential. In Fig. 6, we observed that our model systematically documented all of Argentinas FIFA World Cup matches and results to ensure thoroughness. Furthermore, the model verified its initial findings through self-reflection process. In Fig. 7, through self-reflection, the model successfully corrected the false assumption in the prompt (that the Pearl River is the second-longest river) and correctly identified the Yellow River as Chinas second-longest river. The model also provided valuable insights from different perspectives (e.g., economic importance, water flow), making the"
        },
        {
            "title": "4.2 Hallucination",
            "content": "Figure 6: Case study on how detailed analysis and self-reflection can help prevent hallucination. Figure 7: Case study on how self-reflection can help models detect false assumptions."
        },
        {
            "title": "4.3 General Scenario",
            "content": "Figure 8: Case study of our model provides helpful insights from different perspectives on answering user questions. response more comprehensive and informative. 4.3 General Scenario Setup To evaluate our models performance in general scenarios, we curate test set of 100 queries equally sampled from the Auto-J (Li et al., 2023) and LIMA (Zhou et al., 2024) datasets (50 each), with specific focus on long-term planning tasks through manual adaptation. Three domain experts assess the response quality on scale of 0-100. Results & Insights The evaluation results show notable improvement after fine-tuning. The scores increase from 81.6% to 88% on Auto-J queries and from 77.2% to 87.2% on LIMA queries. This performance enhancement suggests that our fine-tuning approach not only improves bilingual conversation capabilities but also strengthens the models ability in handling general open-domain QA tasks, particularly for scenarios requiring long-term planning and structured thinking. Case Study Figure 8 presents detailed case study comparing responses from Qwen2.5-72B-Instruct and our model on technical programming query about Pythons asyncio library. The query Why in python await asyncio.sleep() is stuck? represents common programming challenge that requires both technical accuracy and clear explanation. Qwen2.5-72B-Instructs response, while technically accurate, provided relatively basic structure with five main points and corresponding code examples. It covered essential aspects like event loop issues, blocking code, and incorrect await usage, but lacked depth in several areas. Notable limitations included insufficient debugging guidance, potentially misleading thread-safe operation suggestions, and absence of performance considerations and best practices. Our model demonstrated substantial improvements across multiple dimensions. First, the response adopted more sophisticated structure with clear hierarchical sections and logical flow, making complex concepts more accessible. Second, it significantly expanded the technical coverage to include advanced topics such as systematic debugging approaches, event loop management strategies, and detailed analysis of blocking code scenarios. Third, it enhanced practical value by incorporating comprehensive debugging tips, concrete examples of common mistake patterns, and systematic troubleshooting steps. Finally, it integrated references to official documentation and reliable learning resources, supporting continued learning. Despite our SFT dataset being exclusively focused on mathematical problem-solving, our model demonstrates remarkable generalization abilities across diverse domains. This suggests that the systematic thinking patterns and structured approaches inherent in mathematical problem-solving can effectively transfer to other fields. The improvements seen in our case study, particularly in terms of structural organization, comprehensive analysis, and logical flow, reflect the successful transfer of mathematical reasoning patterns to general problem-solving scenarios. This finding indicates that carefully curated mathematical instruction data can serve as an effective foundation for developing general-purpose reasoning capabilities in LLMs."
        },
        {
            "title": "Index",
            "content": "To systematically evaluate and compare various O1 replication attempts, we propose the Technical Transparency Index (TTI), comprehensive framework that quantifies the transparency and reproducibility of claimed implementations. This framework aims to provide the research community with objective metrics for assessing the openness and verifiability of different approaches. 5.1 Evaluation Dimensions of Transparency The framework evaluates O1 replication efforts with primary focus on transparency, which is assessed across several interconnected aspects. These include the data transparency, encompassing the accessibility, quality, and documentation of datasets used for downstream search or post-training; methodological transparency, reflected in the clarity and detail of the described techniques, processes, and experimental setups; and evaluation transparency, which considers the reproducibility and comprehensiveness of performance evaluations. Additionally, the framework examines the openness of resources, such as the availability of code, datasets, and models, to ensure that the work can be independently verified and effectively utilized by the research community. This comprehensive perspective captures the multifaceted nature of transparency in replication efforts. Details will be introduced below. Index 1: Data Transparency This aspect evaluates whether the origin of the data is clearly specified, including detailed descriptions of the datasets used and their respective sources. It considers whether the dataset names, providers, or publications from which the data is derived are explicitly mentioned. This applies to all datasets used in downstream tasks such as supervised fine-tuning (SFT), reinforcement learning (RL), or search algorithms and becomes even more crucial when the datasets serve as seed data for synthesizing long thought data. Data Source: This aspect evaluates whether the origin of the data is clearly specified, including detailed descriptions of the datasets used and their respective sources. It considers whether the dataset names, providers, or publications from which the data is derived are explicitly mentioned. Data Selection Process: This focuses on the clarity and rigor of the criteria and methodology used for filtering, cleaning, or preprocessing data prior to its application in downstream tasks such supervised fine-tuning (SFT), searching, or reinforcement learning (RL). Index 2: Methodology Transparency Methodology transparency ensures that the approach, techniques, and processes employed in the work are described in sufficient detail to enable independent reproduction and validation. This section evaluates multiple components, from foundational model descriptions to training and data curation methods. Moreover, in addition to detailing how method is implemented, it is even more important to validate the effectiveness of the method itself. It highlights the importance of validating the effectiveness of each method employed. thorough evaluation should quantify the contributions of individual techniques to the overall system performance, rather than simply reporting final results. Foundation Model Details: This evaluates the depth and clarity of information provided about the base model used in the work. It includes details such as the architecture (e.g., transformer layers, attention mechanisms), parameter size (number of trainable parameters). The goal is to ensure that the foundational components of the approach are fully understood and reproducible. Search Algorithm: This focuses on the explanation of the search algorithm employed for inference-time scaling. It assesses whether the methodology for applying techniques like beam search, monte carlo tree search (MCTS), or other strategies is well-documented, including parameters, step-by-step processes, and any custom modifications. RL Algorithm: This examines the details of reinforcement learning (RL) or preference learning approaches (e.g., Direct Preference Optimization). It includes the specification of reward functions, optimization goals, and training dynamics. Long Thought (O1-like) Synthetic Algorithm: This aspect assesses the process of creating or synthesizing long-thought (O1-like) datasets. It includes explanations of any specific algorithms, heuristics, or rules applied in data generation or selection. Training Details: This examines the documentation of training procedures, including key hyper-parameters (e.g., learning rate, batch size, optimizer types) and the overall training configuration."
        },
        {
            "title": "5.2 Checklist for O1-style Technique",
            "content": "Effectiveness Validation: This evaluates whether the effectiveness of each method is rigorously validated. For instance, ablation studies, comparative experiments, or incremental analyses should be conducted to quantify how individual techniques contribute to the overall system. Such validations ensure that claims about the importance of method are backed by clear empirical evidence, fostering transparency and reproducibility. Index 3: Evaluation Transparency Benchmark Usage: This evaluates the selection of benchmarks used to assess model performance, considering whether the chosen benchmarks are appropriate for the task and domain. Evaluation Metrics: This assesses the metrics used to quantify model performance, such as pass@k, maj@k, or rm@k. It examines the clarity of metric definitions, their relevance to the specific task, and any customizations introduced to address unique aspects of the evaluation. Additionally, it evaluates how metrics are standardized and aligned across baselines to ensure fair and unbiased comparisons. Index 4: Open-Source Resources Open-source resources play vital role in fostering reproducibility and enabling the research community to build upon existing work. This section evaluates the availability and accessibility of datasets, models, code, and documentation, which are essential for independent validation and further experimentation. Data: This evaluates whether the post-training raw data and the synthesized O1-like datasets are made publicly available for use. Open availability of these datasets significantly enhances reproducibility and enables researchers to apply them to additional tasks. Model Weights: This assesses the public release of the trained model weights. Sharing model weights facilitates replication and further optimization efforts. Code: This considers whether the released codebase includes scripts for both training the model and evaluating its performance. complete and well-documented codebase is crucial for enabling others to reproduce and validate the work. Documentation: This examines the availability of supplemental documentation, such as research papers, technical reports, or blog posts. It assesses whether these materials clearly explain the methodology, results, and underlying ideas, and whether they provide actionable insights for researchers and practitioners. 5.2 Checklist for O1-style Technique Scoring Framework (100 Points) We propose scoring framework that provides unified approach to assess O1 replication efforts by focusing exclusively on transparency, with total score of 100 points (see Table 3). This focus underscores the critical importance of reproducibility and openness in evaluating the quality of replication efforts. The framework evaluates key dimensions as detailed in Section 5.1, ensuring comprehensive and fair assessment of each works commitment to clarity and accessibility. By emphasizing transparency through systematic checklist approach, this scoring system highlights the foundational aspects necessary for building trust and driving further advancements in the field. Binary Score Under this framework, every evaluation indicator in the checklist is assessed through simple Yes/No question, with each Yes response contributing its designated points to the total score. The binary nature of this system ensures clarity and consistency in evaluation, as each indicator is either fully satisfied or not. This method prioritizes transparency over implementation scope. For example, if work explicitly acknowledges that it does not employ particular technique (e.g., reinforcement learning), it will still receive full points for transparency in that indicator, as openly documenting such details reflects commitment to reproducibility and openness. When assigning point values to each indicator, we carefully weigh their relative importance in the technical pipeline. Indicators deemed to have more significant impact on the success and reproducibility of O1 replication efforts are given higher point values. For instance, transparency in the search algorithm and the long thought data synthesis algorithm is assigned higher scores, reflecting their critical roles in achieving high-quality and reproducible results. This weighted scoring ensures that the framework aligns with the priorities of the technical process, emphasizing the documentation of key components that drive the overall systems performance and reproducibility."
        },
        {
            "title": "Score",
            "content": "Are dataset names, sources, and providers explicitly documented and properly cited? Data (14) Is there sufficient documentation of data distributions, formats, and characteristics to enable proper replication? Are the criteria and methodology for data selection and filtering clearly justified and documented? For synthetic data generation, is the entire process transparent, including prompting strategies and quality control measures? Is there clear and complete description of the base model (including its architecture, size, etc.)? Is the complete search algorithm implementation (e.g., beam search, MCTS) detailed with all components? Methodology (33) Is the RL algorithm fully specified with its objective function and training procedure? Is the long thought data curation/generation algorithm thoroughly explained with its complete workflow? Is the complete training pipeline documented, including all stages and their sequence? Are the computational requirements and infrastructure details provided? Is there clear documentation of all training hyperparameters and optimization choices? Are there comprehensive ablation studies showing the contribution of each major component? Is there clear justification for the selection of evaluation benchmarks? Is the evaluation dimension clearly specified (e.g., answer-level, stepby-step level)? Evaluation (24) Are all evaluation metrics (e.g., pass@k, maj@k) clearly defined? For any custom metrics (if exists), are they well-justified and clearly documented? Are the evaluation metrics consistently applied across all baselines? Are the evaluation conditions (e.g., temperature, top-p) explained for all compared methods? Is the post-training data publicly available? Is the synthetic long thought data publicly available? Are trained model weights publicly available? Is the complete training codebase publicly available? Is the complete evaluation codebase publicly released? Open-Source (29) Are there step-by-step guidance and instruction for code usage? Is there comprehensive technical paper detailing all research aspects instead of brief blog post? 3 4 4 4 6 6 3 2 2 4 4 4 4 4 4 5 5 4 4 4 Table 3: Transparency scoring framework for O1 replication efforts. Each evaluation point of the checklist is assigned score based on their transparency criteria. The total transparency score sums up to 100 points."
        },
        {
            "title": "5.3 Compared Works",
            "content": "5.3 Compared Works We include comprehensive evaluation of existing attempts to replicate O1, assessing them across both transparency and performance dimensions. The works we cover include Open O1 (Team, 2024b), O1-Journey (Part 1) (Qin et al., 2024), LLaMA-O1 (Team, 2024a), k0Math (kimi, 2024), Skywork O1 (kunlun, 2024), Deepseek-R1Lite (deepseek, 2024) and this work O1-Journey (Part 2). These comparisons provide holistic view of the current progress in O1 replication efforts, highlighting their strengths and areas for further improvement. 5.4 Leaderboard"
        },
        {
            "title": "Work",
            "content": "Open O1 O1-Journey (Part1) LLaMA-O1 K0Math Skywork O1 DeepSeek-R1-Lite O1-Journey (Part2) Data (14) Methodology (33) Evaluation (24) Open-Source (29) Total Score"
        },
        {
            "title": "Evaluation Dimensions",
            "content": "0 10 0 0 0 0 10 8 33 6 0 0 0 33 20 24 0 16 0 20 24 5 9 5 0 0 0 12 33 76 11 16 0 20 79 Table 4: Transparency scores of various O1 replication efforts. Each column represents specific method, with individual scores provided for each evaluation dimension and indicator. The total transparency score is calculated out of 100 points, reflecting the openness and reproducibility of each approach. The leaderboard  (Table 4)  showcases the transparency levels of various O1 replication efforts, with our work achieving perfect transparency score. This result highlights our commitment to openness and reproducibility, building upon the solid foundation established by O1-Journey (Part 1). Together, the O1-Journey series sets new benchmark for transparency by excelling across all evaluation dimensions, including data accessibility, methodology clarity, and open-source resource availability."
        },
        {
            "title": "6 The Bitter Lesson of Simple Distillation",
            "content": "The remarkable success of knowledge distillation from O1 presents an alluring shortcut to achieving impressive performance gains in mathematical reasoning tasks. While this approach offers immediate and tangible benefits, it masks series of profound challenges that threaten the long-term development of both AI technology and its research community. In this section, we examine the true costs of prioritizing easy wins over fundamental innovation, revealing implications that extend far beyond mere technical considerations. SURFACE APPEAL At first glance, distillation appears to be an elegant solution: by learning directly from O1s sophisticated reasoning patterns, models can quickly achieve significant performance improvements with relatively straightforward implementation. This accessibility has led to widespread adoption, particularly among organizations seeking to rapidly demonstrate capabilities comparable to O1. However, this convenience comes at price that may not be immediately apparent but could prove devastating to the fields long-term progress. PERFORMANCE CEILING Perhaps the most immediate technical concern lies in the inherent limitations of distillation-based approaches. Models trained through distillation are invariably bounded by the capabilities of their teacher model - in this case, O1. This creates an implicit ceiling effect, where improvements, no matter how sophisticated the distillation process, can never truly surpass the original models capabilities. This limitation becomes particularly problematic when considering the need to extend capabilities to new domains or tackle previously unseen challenges. MISSED INNOVATION More fundamentally, the widespread adoption of distillation approaches is causing us to miss crucial opportunities in core technical innovation. O1s true breakthrough likely lies not just in its ability to solve complex problems, but in its sophisticated mechanisms for inference time scaling and search optimization. By bypassing the challenge of developing these fundamental capabilities, we risk creating widening technological gap between organizations that have mastered these core technologies and those relying primarily on distillation. This infrastructure gap may become increasingly difficult to bridge as the field advances."
        },
        {
            "title": "6.1 Suggestions",
            "content": "RESEARCH CULTURE SHIFT The impact on research culture is equally concerning. The availability of easy wins through distillation has begun to shift research focus away from tackling fundamental challenges. This trend manifests in reduced investment in advanced computing infrastructure and diminished emphasis on developing sophisticated search and reasoning algorithms. The resulting self-reinforcing cycle - where lack of infrastructure limits research possibilities, further encouraging reliance on distillation approaches - threatens to create an innovation bottleneck that could stifle future breakthroughs. EROSION OF FUNDAMENTALS Perhaps most alarming is the impact on educational development within the field. The widespread adoption of distillation approaches poses significant risk to the development of future AI researchers. When students and early-career researchers are primarily exposed to shortcut solutions, they miss crucial opportunities to develop deep problem-solving skills. The ability to tackle complex technical challenges from first principles - cornerstone of scientific innovation - may be gradually eroded as quick solutions become the norm. We are witnessing transformation in how the next generation of AI researchers approaches problemsolving. Instead of developing deep understanding through wrestling with fundamental challenges, many are being trained primarily in optimization and prompt engineering. This shift from how it works to what works represents fundamental change in research mentality that could have far-reaching consequences for the fields future innovation capacity. FIRST PRINCIPLES DECAY This erosion of first-principles thinking is particularly concerning as it undermines the very foundation of scientific innovation. The process of developing search algorithms, optimizing inference time, and building reasoning mechanisms from scratch provides invaluable learning experiences that cannot be replicated through distillation approaches. These challenges force researchers to deeply understand model behavior and limitations, develop systematic problem-solving strategies, and build intuition for algorithm design and optimization. Without these experiences, we risk creating generation of researchers who are more comfortable with applying existing solutions than developing new ones from fundamental principles. ACADEMIC IMPACT The educational implications extend beyond individual skill development. The academic research environment, traditionally crucible for fundamental innovation, is particularly vulnerable to these effects. Pressure to produce quick results may overshadow the value of deeper technical investigations, while students may be discouraged from pursuing more challenging, fundamental research directions. The emphasis on performance metrics over understanding threatens to create generation of researchers skilled in optimization but lacking in innovative capacity. GROWING DIVIDE Looking ahead, the cumulative effect of these factors paints troubling picture. The technical capability gap between organizations that have developed fundamental search and inference technologies and those relying primarily on distillation may become increasingly unbridgeable. This divide could lead to research ecosystem where genuine breakthroughs become the exclusive domain of small number of wellresourced organizations, while the broader community remains trapped in cycle of incremental improvements through distillation. 6.1 Suggestions To address these challenges, we propose several crucial recommendations. GROWING DIVIDE First, organizations must maintain balanced research portfolio that includes both distillation-based approaches and fundamental research into search and inference optimization. Second, despite the immediate availability of distillation-based solutions, continued investment in advanced computing infrastructure remains essential. Third, research programs should prioritize building core competencies in search algorithms and inference optimization alongside performance improvements. EDUCATIONAL REFORM In the educational context, we must redesign our approach to training future researchers. This includes developing balanced curricula that emphasize both practical applications and fundamental theory, structuring research projects to encourage deep understanding alongside performance optimization, and fostering research culture that values long-term innovation over quick gains. The bitter lesson here is not that distillation is inherently problematic - it remains valuable tool in our technical arsenal. Rather, the danger lies in allowing the convenience of distillation to divert us from the harder but ultimately more rewarding path of fundamental innovation. As we move forward, maintaining this balance between immediate gains and long-term development will be crucial for ensuring the continued advancement of AI capabilities and the cultivation of future innovators in the field. Building intelligent AI is crucial, but cultivating human minds with first-principles thinking is our ultimate mission - they are, after all, the true architects of AIs future."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. [5] Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative aia tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528. [6] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30. [7] deepseek. 2024. deepseekr1lite. website. [8] Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. 2024. Reformatted alignment. arXiv preprint arXiv:2402.12219. [9] Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2022. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726. [10] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. 2023. Textbooks are all you need. arXiv preprint arXiv:2306.11644. [11] Yancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu Guo, Chengwei Hu, Boren Zheng, et al. 2024. Chinese simpleqa: chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. [13] Geoffrey Hinton. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. [14] Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, and Dahua Lin. 2023. Flames: Benchmarking value alignment of chinese large language models. [15] kimi. 2024. k0math. website. [16] kunlun. 2024. skyworko1. website. [17] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470."
        },
        {
            "title": "References",
            "content": "[18] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. [19] Yixiu Liu, Yuxiang Zheng, Shijie Xia, Jiajun Li, Yi Tu, Chaoling Song, and Pengfei Liu. 2024. Safety-j: Evaluating safety with critique. arXiv preprint arXiv:2407.17075. [20] OpenAI. 2024. Learning to reason with llms. [21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. [22] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. [23] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. [24] Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. 2022. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 39063923. Association for Computational Linguistics. [25] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7. [26] LLaMaO1 Team. 2024a. Llamao1. Github. [27] OpenO1 Team. 2024b. Openo1. Github. [28] Wang, Chern, and Liu. 2023. Chinesefacteval: factuality benchmark for chinese llms. [29] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. [30] Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. [32] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. [33] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. [34] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. [35] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284. [36] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488. [37] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36. [38] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593."
        }
    ],
    "affiliations": [
        "Generative AI Research Lab (GAIR)",
        "NYU",
        "SII",
        "Shanghai Jiao Tong University"
    ]
}