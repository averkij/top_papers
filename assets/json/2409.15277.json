{
    "paper_title": "A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?",
    "authors": [
        "Yunfei Xie",
        "Juncheng Wu",
        "Haoqin Tu",
        "Siwei Yang",
        "Bingchen Zhao",
        "Yongshuo Zong",
        "Qiao Jin",
        "Cihang Xie",
        "Yuyin Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future research."
        },
        {
            "title": "Start",
            "content": "A PRELIMINARY STUDY OF O1 IN MEDICINE: ARE WE CLOSER TO AN AI DOCTOR ? Yunfei Xie1 Juncheng Wu1 Haoqin Tu1 Siwei Yang1 Bingchen Zhao2 Yongshuo Zong2 Qiao Jin3 Cihang Xie1 Yuyin Zhou1 equal technical contribution 1UC Santa Cruz 2University of Edinburgh 3National Institutes of Health 4 2 0 S 3 2 ] . [ 1 7 7 2 5 1 . 9 0 4 2 : r Figure 1: Overall results of o1 and other 4 strong LLMs. We show performance on 12 medical datasets spanning diverse domains. o1 demonstrates clear performance advantage over closeand open-source models. Figure 2: Average accuracy of o1 and other 4 strong LLMs. o1 achieves the highest average accuracy of 74.3% across 19 medical datasets."
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAIs o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides preliminary exploration of o1 on different medical scenarios, comprehensively examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an"
        },
        {
            "title": "Preprint",
            "content": "average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we also identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs at https://ucsc-vlaa.github.io/o1_medicine/ for future research."
        },
        {
            "title": "INTRODUCTION",
            "content": "Intelligence, complex and elusive concept, has puzzled psychologists, philosophers, and computer scientists for years (Bubeck et al., 2023). While there is no single agreed-upon definition of intelligence, it is widely accepted that it spans broad range of cognitive skills, rather than being confined to specific task (McCarthy et al., 1955). Creating artificial systems with such general intelligence has been long-standing and ambitious goal of AI research. The most exciting progresses in AI are achieved by language models in these years, from the initial start of ChatGPT to its evolution and other open-source projects (Touvron et al., 2023a;b; Jiang et al., 2023; Bai et al., 2023; Peng et al., 2024). Early LLM pioneers set out goals to understand and interact with human by exploring generalizable reasoning mechanisms and building knowledge bases with vast amounts of commonsense information. With parameters and data volume in place, the question of how to effectively prompt the model from the user end and train it from the developer end has become trending topic of exploration (Wei et al., 2022; Ouyang et al., 2022). On the user side, varying prompting techniques can significantly impact model performance. Chain-of-thought (CoT) prompting (Wei et al., 2022; Dong et al., 2022; Saunders et al., 2022), one of the most popular strategies, leverages the models internal reasoning patterns to enhance its ability to solve complex tasks. OpenAI capitalized on this by embedding the CoT process into model training, integrating reinforcement learning, and finally introduced the o1 model (OpenAI, 2024). While the o1 model demonstrates strong performance in general domains, its effectiveness in specialized fields like medicinewhere domain-specific training may be lackingremains uncertain. Moreover, current benchmarks for LLMs in the medical domain often evaluate models only on limited set of factors, often focusing on isolated aspects such as knowledge and reasoning (Nori et al., 2023b; Lievin et al., 2024), safety (Han et al., 2024), or multilinguality (Wang et al., 2024). These factors make comprehensive assessment of LLMs capabilitiesespecially for advanced models like o1in medical challenging tasks (Figure 1). This paper aims to provide an initiative to close this gap, focusing on o1. We identify three fundamental aspects of LLMs in medicine: understanding, reasoning, and multilinguality. To evaluate these capabilities, we assembled 35 existing medical datasets and developed two novel, challenging QA datasets that include instructions and expected outputs, ensuring comprehensive assessment. With evaluation on this extensive suite, our key findings include: o1 demonstrates improved transfer of clinical understanding and reasoning abilities, validating its competence in real-world diagnostic scenarios compared with both closeand open-source models as presented in Figure 1 and Figure 2; No single model excels across all tasks on our medical leaderboard, though o1 comes close to dominating most evaluations; o1 still suffers from the long-standing issue of hallucination and complex multilingual medical cases; Inconsistencies in metrics for medical NLP can significantly affect models standings, which calls for re-evaluation of reliable metrics for future LLMs; CoT prompting can further enhance o1 in medicine, despite its training having already integrated CoT data. In addition to these findings, we also elevate the discussion section as an initial attempt to address the issues identified during our benchmarking in Section 5. Particularly, we highlight the potential negative effects of o1, emphasize the urgent need for consistent and unified evaluation metrics for future LLMs, and advocate for improved instruction templates that can be applied to models with embedded prompting strategies."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Our evaluation pipeline has different (a) aspects with various (b) prompting strategies using the latest (c) language models. We leverage comprehensive set of (d) evaluations to present holistic view of model progress in the medical domain."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Large Language Models with Enhanced Reasoning Ability. Large Language models (LLMs) based on next token prediction pre-training (Touvron et al., 2023a;b; Achiam et al., 2023) have demonstrated promising capabilities on various language undersanding tasks. Instruction fine-tuning further improved the abilites of these LLMs for following user instructions. However, recent studies suggest that LLMs struggle with complex tasks involving logical reasoning. To address this issue, some researches propose to instruct LLMs to mimic human thinking processes by producing chainof-thought (CoT) (Feng et al., 2024; Wei et al., 2022) before generating final answer. Reinforcement learning from human feedback (Ouyang et al., 2022) has also been employed to enhance reasoning while make sure the models align with human values (Tu et al., 2023b;a). Recently, OpenAI introduced o1, which was trained on vast amount of CoT data, further enhancing the capability of LLMs in solving scientific problems. In this paper, we aim to investigate whether enhanced abilities of o1 effectively transfer to the clinical medical domain. Medical Large Language Models. Benefiting from the generalization capabilities of LLMs, generalpurpose models such as GPT-4 have demonstrated impressive performance on challenging medical problems (Nori et al., 2023a; Wu et al., 2024b). Some researchers have attempted to further equip LLMs with biomedical knowledge by fine-tuning them using domain-specific corpora (Chen et al., 2023; Wang et al., 2023; Wu et al., 2024a; Li et al., 2023). However, for clinical applications, LLMs are not only required to understand medical domain-specific knowledge but also to produce reliable responses by performing logical reasoning. In this paper, we aim to explore the potential of o1 as clinical viable model. Our experimental findings reveal that with enhanced understanding, reasoning, and multilinguality medical capabilities, o1 makes step closer to reliable clinical AI-system."
        },
        {
            "title": "3 EVALUATION PIPELINE",
            "content": "3.1 OVERALL TAXONOMY OF EVALUATIONS First, we present the taxonomy of our evaluation, along with an overview of the evaluation pipeline as shown in Figure 3. Firstly, we specify three aspects of the model capabilities, namely understanding, reasoning, and multilinguality, that correspond to the real-world needs of clinical physicians. To ensure comprehensive evaluation, we collect diverse range of medical tasks and datasets that fall under these three aspects. Moreover, we explore three prompting strategies in our pipeline, including (1) direct prompting, which instructs LLMs to solve specific problems directly, (2) chain-ofthought, which requires models to think step-by-step before generating the final answer, (3) few-shot 1https://www.thelancet.com/ 2https://www.nejm.org/"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Six datasets across three fundamental aspects employed in our evaluation suite. Asterisks (*) denotes the newly constructed datasets from public sources. Aspect Task Dataset Description Metrics BC5-disease (Li et al., 2016) BC5Chem (Li et al., 2016) BC4Chem (Savery et al., 2020) Species800 (Pafilis et al., 2013) HoC (Baker et al., 2016) Entity extraction for disease. Entity extraction for chemical. Entity extraction for chemical names from PubMed article abstracts. Extraction of organism names from PubMed article abstracts. Classification of the hallmarks of cancer given biomedical article abstracts. F1-score HumanDiseaseOntology (Schriml et al., 2019) Disease ontology-based entity extraction. BioLORD (Remy et al., 2024) Elaboration of biomedical concepts. BLEU, ROUGE, AlignScore, Mauve PMC-Patient (Zhao et al., 2023) PICO-Participant (Nye et al., 2018) PICO-Intervention (Nye et al., 2018) PICO-Outcome (Nye et al., 2018) ADE Corpus (Gurulingappa et al., 2012) Patient-related entity (gender and age for example) extraction from PubMed Central articles. Information extraction of outcome, intervention, and participant from article abstracts. Drug dose extraction given the drug information. Accuracy MIMIC-IV-Ultrasound (Johnson et al., 2023) Summarization of patient reports from emerMIMIC-IV-CT (Wallace et al., 2021) RCT-Text (Wallace et al., 2021) MedQSum (Lee et al., 2021) PubMedQA (Jin et al., 2019) MedQA (Jin et al., 2021) MedMCQA (Pal et al., 2022) LancetQA 1 NEJMQA 2 Medbullets (Chen et al., 2024) DDXPlus (Fansi Tchango et al., 2022) gency departments. Summarization of medical evidence from clinical studies in literature reviews. Summarization of patient notes, reports, and health records. QA data built on PubMed abstracts. QA data for medical knowledge assessment. QA data from AIIMS & NEET PG entrance exams. QA data crawled from Lancet picture quiz gallery. QA and diagnostic challenge requests from NEJM quiz. QA data from Medbullets online medical study platform. Diagnostic decision making of synthesized patient data. Treatment planning for breast cancer cases. BLEU, ROUGE, AlignScore, Mauve Accuracy Accuracy Concept Recognition Text Summary Knowledge QA d s n i a SEER (Dubey et al., 2023) MIMIC4ED-Hospitalization (Xie et al., 2022) MIMIC4ED-72h ED Revisit (Xie et al., 2022) MIMIC4ED-Critical Triage (Xie et al., 2022) MedNLI-Dis. (Romanov & Shivade, 2018) Discriminative entailment task for clinical hyPrediction of clinical outcomes in emergency medicine from MIMIC-IV-ED. Clinical Decision Support potheses. PUBHEALTH Ver. (Kotonya & Toni, 2020) Verification of health-related information from the public. EBMS (Molla & Santiago-Martinez, 2011) Justification verification using the EBMS corpus. PUBHEALTH Exp. (Kotonya & Toni, 2020) Explanation of health-related information ChatDoctor (Li et al., 2023) from the public. Patient-doctor dialogues from online medical consultations. MedNLI-Gen. (Romanov & Shivade, 2018) Generative entailment task for clinical hyBLEU, ROUGE, AlignScore, Mauve AI Hospital (Fan et al., 2024) Agent AgentClinic (Schmidgall et al., 2024) potheses. Multi-agent task simulating dynamic medical interactions in Chinese. Agent benchmark in simulated clinical environments from MedQA and NEJMQA scenarios. Accuracy Medical Calculation MedCalc-Bench (Khandekar et al., 2024) Medicine dose level calculation from ADE Accuracy corpus. Knowledge - u l n QA Agent XMedBench (Wang et al., 2024) AI Hospital (Fan et al., 2024) Multilingual benchmark for medical understanding and interaction. Multi-agent task simulating dynamic medical interactions in Chinese. Accuracy Accuracy"
        },
        {
            "title": "Preprint",
            "content": "prompting, which providing models with several examples to learn the input-output mapping on the fly. Lastly, appropriate metrics are utilized to measure the discrepancy between generated responses and ground-truth answers. Details about metrics utilized in each dataset are provided in Table 1."
        },
        {
            "title": "3.2 ASPECTS AND TASKS",
            "content": "In Table 1, our evaluation efforts are structured into three main parts: aspect, task, and dataset. Specifically, dataset refers to the data itself along with the metrics used in the current context. We utilize 35 existing datasets and create 2 additional challenging datasets for evaluation. task is collection of multiple datasets that share common goal or evaluate similar capabilities within the model. We categorize all 37 datasets into 6 tasks for clearer evaluation and analysis. An aspect describes specific capability or property to understand how well the model performs in particular area. In our evaluation pipeline, we focus on three key aspects. Formally, we illustrate these three evaluation aspects with their corresponding tasks as follows: Understanding refers to the models ability to utilize its internal medical knowledge to comprehend medical concepts. For example, in concept recognition task, the model is required to extract or elaborate medical concepts from article (Savery et al., 2020; Pafilis et al., 2013; Nye et al., 2018) or diagnosis report (Zhao et al., 2023). And in text summarization, the model need to understand concepts in complex texts to generate concise summary (Lee et al., 2021; Wallace et al., 2021; Johnson et al., 2019; 2023). Reasoning is the ability to conduct multiple steps of logical thinking to arrive at the conclusion. In question answering tasks, the model is prompted to select correct option from multi-choices based on reasoning derived from the medical information provided in the question. In addition to common question-answering datasets (Jin et al., 2019; Pal et al., 2022; Jin et al., 2021), we collect real-world clinical questions from The Lancet, the New England Journal of Medicine (NEJM), and Medbullets (Chen et al., 2024) to better assess the clinical utility of LLMs. In the clinical suggestion task, the model is required to provide treatment suggestions (Dubey et al., 2023; Li et al., 2023) or diagnostic decisions (Xie et al., 2022; Fansi Tchango et al., 2022) based on patients information. In the AI Hospital (Fan et al., 2024) and AgentClinic (Schmidgall et al., 2024) datasets, we task the model with serving as medical agent. Furthermore, in the MedCalc-Bench (Khandekar et al., 2024) dataset, the model is required to perform mathematical reasoning and calculate answers. Multilinguality is the ability to complete task when the languages of input instruction and/or output answers are changed to different languages. For example, XMedBench (Wang et al., 2024) dataset requires LLMs to answer medical questions in six languages, including Chinese, Arabic, Hindi, Spanish, Chinese and English. In AI Hospital dataset (Fan et al., 2024), the model is required to serve as an agent using Chinese. 3.3 METRICS In this section, we elaborate on metrics employed in our evaluation pipeline. Accuracy is used to directly measure the percentage of models generated answer which exactly match with the ground-truth. We use accuracy for multi-choice question datasets, MedCalcBench (Khandekar et al., 2024) dataset, and portions of clinical suggestion and concept recognition datasets where the ground-truth answer is single word or phrase. F1-score (Pedregosa et al., 2011) is the harmonic mean of precision and recall. It is employed in datasets where the model is required to select multiple correct answers. BLEU (Papineni et al., 2002) and ROUGE (Lin & Hovy, 2002) are NLP metrics measuring the similarity between the generated respond and the ground-truth. Specifically, we utilize BLEU-1 and ROUGE-1 for all free-form generation tasks in our evaluation. AlignScore (Zha et al., 2023) is metric to measure the factual consistency of generated text. In this paper, we use AlignScore for all free-form generation tasks to evaluate the extent of models hallucination. Mauve (Pillutla et al., 2021) is measure of gap between distribution of generated and humanwritten text. It is employed for all free-form generation tasks."
        },
        {
            "title": "Preprint",
            "content": "All metrics range from 0 to 100, and higher number indicates better quality output from the model."
        },
        {
            "title": "4.1 EXPERIMENT DETAILS",
            "content": "Prompting strategies. For most datasets, we employ the same prompting strategy as described in previous literature (Wu et al., 2024b; Nori et al., 2023b;a): For knowledge QA tasks, agent tasks, medical calculation tasks, and multilingual-related tasks, we use the direct prompting evaluation method, which is consistent with the settings of these benchmarks. For other tasks derived from MedS-Bench (Wu et al., 2024b), we follow their benchmark settings, leveraging few-shot (3shot) prompt strategy with its template shown in Section A.1. As officially suggested by OpenAI, common prompting techniques such as Chain-of-Thought (CoT) (Wei et al., 2022) and in-context examples may not boost o1s performance as it has implicit CoT built in. To further validate this claim, we also investigate the effect of several advanced promptings in our evaluation (e.g., CoT, Self-Consistency (Wang et al., 2022), and Reflex (Shinn et al., 2024)), the detailed input instruction formats are in Section A.1 Models for evaluation. We choose the following models to evaluate: GPT-3.5 (gpt-3.5-turbo0125)3, an advanced language model by OpenAI known for its enhanced contextual understanding; GPT-4 (gpt-4-0125-preview) (Achiam et al., 2023), the successor to GPT-3.5 with significant improvements in reasoning and language comprehension; o1 (o1-preview-2024-09-12) (OpenAI, 2024), the lastest LLM model that is capable of performing highly complex reasoning by employing chain-of-thought reasoning. Apart from these close-source models, we have also incorporated two open-source ones in our experiments: MEDITRON-70B (Chen et al., 2023), an LLM trained with medical-centric data and Llama3-8B (Meta, 2024), the latest and strongest open LLM right now. 4.2 MAIN RESULT: Yes! WE ARE ONE STEP CLOSER TO AN AI DOCTOR Enhanced ability of o1 transfers to its clinical understanding. Given the established results from o1, which underscore its remarkable effectiveness in knowledge and reasoning abilities such as mathematical problem-solving and code generation (OpenAI, 2024), we observe that this superior capability can also be transferred to the specific clinical knowledge understanding. Results presented in Table 2 demonstrate that o1 outperforms other models on the understanding aspect in most clinical tasks. We also present these statistics in Figure 1, where we observe that o1 has larger cover radius across various medical datasets. For instance, on 5 concept recognition datasets that use F1 as the metric, o1 outperforms both GPT-4 and GPT-3.5 by an average of 7.6% and 26.6%, respectively (i.e., 72.6% vs. 65.0% vs. 46.0%), with notable 24.5% average improvement on the widely used BC4Chem dataset. Additionally, on the summarization task in Table 3, o1 achieves 2.4% and 3.7% increase in ROUGE-1 score over GPT-4 and GPT-3.5 (i.e., 31.4% vs. 29.0% vs. 27.7%), demonstrating its enhanced capacity for real-world clinical understanding. This improved performance confirms that advancements in general NLP capabilities for LLMs can effectively translate to enhanced model understanding in the medical domain. The o1 model demonstrates strong reasoning in clinical diagnosis scenarios. On the reasoning aspect, o1 takes significant step forward in demonstrating its advantages in real-world diagnostic situations. In our newly constructed challenging QA tasks, NEJMQA and LacentQA, o1 showcases an average accuracy improvement of 8.9% and 27.1% over the performance of GPT-4 (79.6%) and GPT-3.5 (61.5%) on the respective datasets  (Table 2)  . Another noteworthy improvement in o1 is its capacity for mathematical reasoning, elevating the baseline of MedCalc-Bench to 34.9%, which surpasses GPT-4 by significant 9.4%. In more complex reasoning scenarios that involve multi-turn conversations and environmental simulations, o1 outperforms both GPT-4 and GPT-3.5 on the AgentClinic benchmark, achieving accuracy gains of at least 15.5% and 10% with scores of 45.5% and 20.0% on its MedQA and NEJM subsets, respectively. These observations serve as compelling evidence of o1s competence in complex real-world diagnosis and clinical utility scenarios. 3https://platform.openai.com/docs/models/gpt-3-5-turbo/"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Accuracy (Acc.) or F1 results on 4 tasks across 2 aspects. Model performances with * are taken from Wu et al. (2024b) as the reference. We use the gray background to highlight o1 results. And we present the average score (Average) of each metric in the table Aspect Task Datasets Metric o1 GPT-4 GPT-3. MEDITRON* (70B) Llama3* (8B) Concept Recognition PMC-Patient (Zhao et al., 2023) PICO-Participant (Nye et al., 2018) PICO-Intervention (Nye et al., 2018) PICO-Outcome (Nye et al., 2018) ADE Corpus (Gurulingappa et al., 2012) Average BC5-disease (Li et al., 2016) BC5Chem (Li et al., 2016) BC4Chem (Savery et al., 2020) Species800 (Pafilis et al., 2013) HoC (Pafilis et al., 2013) Average DDXPlus (Fansi Tchango et al., 2022) SEER (Dubey et al., 2023) Acc. Acc. Acc. Acc. Acc. Acc. F1 F1 F1 F1 F1 F1 Acc. Acc. Acc. 76.4 75.0 77.5 67.5 78.3 74.9 69.5 72.2 73.4 71.6 76.3 72. 64.0 80.0 64.0 75.7 75.0 75.0 65.0 78.3 73.8 63.0 71.2 65.1 66.8 59.0 65.0 56.0 69.6 61. Clinical Decision Support MIMIC4ED -Hospitalization MIMIC4ED -72h ED Revisit MIMIC4ED -Critical Triage (Xie et al., 2022) (Xie et al., 2022) Acc. 59. 58.0 (Xie et al., 2022) Acc. MedNLI-Dis. (Romanov & Shivade, 2018) Acc. PUBHEALTH Ver. (Kotonya & Toni, 2020) Acc. Acc. Average Knowledge QA PubMedQA (Jin et al., 2019) MedQA (Jin et al., 2021) MedMCQA (Pal et al., 2022) Medbullets (Chen et al., 2024) LancetQA NEJMQA Average Medical Calculation MedCalc-Bench (Khandekar et al., 2024) Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. 61.7 88.0 76.4 70. 75.0 75.5 95.0 90.6 81.5 91.2 84.8 34.9 66.7 84.0 75.7 67.3 52.8 69.7 79.5 66.9 76.0 83.5 71.4 25. 74.4 52.5 75.0 60.0 71.6 66.7 38.9 43.1 32.7 55.4 59.8 46.0 41.0 5.0 62.0 53.6 58. 57.0 74.4 50.2 25.4 53.8 58.8 50.7 61.0 65.0 52.5 10.8 72.2 72.1 46.6 51.2 95.7 67.6 1.4 4.2 2.0 0.4 23.7 6.3 29.6 68. 56.3 48.5 45.7 60.9 32.7 48.9 74.4 47.9 59.2 - - - 60.5 - 96.0 58.2 79.1 58.2 69.6 72.2 25.3 37.9 19.5 11.9 38.3 26.6 33.8 56.1 39.1 9.3 8. 63.9 63.9 39.3 73.0 60.9 50.7 - - - 61.5 - Table 3: BLEU-1 (B-1) and ROUGE-1 (R-1) results on 3 tasks across 2 aspects. We use the gray background to highlight o1 results. We also present the average score (Average) of each metric Aspect Task Datasets o1 GPT-4 GPT-3.5 MEDITRON* (70B) Llama3* (8B) B-1 R-1 B-1 R-1 B-1 R-1 B-1 R-1 B-1 R-1 MIMIC-IV-Ultrasound (Johnson et al., 2023) 22.2 28.8 15.9 27.0 11.0 21.1 Text Summary Concept Recognition MIMIC-IV-CT (Johnson et al., 2023) RCT-Text (Wallace et al., 2021) MedQSum (Lee et al., 2021) Average HumanDO (Schriml et al., 2019) BioLORD (Remy et al., 2024) Average 3.8 19.0 26.4 15.7 22.7 18.7 25.9 16.3 19.5 23.4 19.5 23.4 20.6 24.2 4.0 39.2 46.8 36.3 43.0 26.5 39.6 15.6 25.0 31.4 21.8 29.0 19.2 27.7 9.9 9.7 24.9 33.1 7.7 23.0 31.8 14.7 21.8 12.8 19.1 11.8 24.0 32.5 12.2 19.0 12.5 19.3 9.8 16.2 12.2 19.4 Clinical Decision Support EBMS (Molla & Santiago-Martinez, 2011) PUBHEALTH Exp. (Kotonya & Toni, 2020) 15.8 23.6 15.1 22.0 16.6 23.6 14.0 27.0 MedNLI-Gen. (Romanov & Shivade, 2018) 17.0 26.0 16.9 25.8 10.0 18.3 15.3 24.4 16.2 17.2 14.0 22. 16.2 20.4 12.0 16.3 15.4 19.4 11.6 6.1 - 4.4 7.4 ChatDoctor (Li et al., 2023) 12.2 27.6 20.9 Average 4.7 6.1 23.9 16.4 23.1 17. 25.4 22.7 24.1 15.8 8.7 - 14.1 12.9 18.1 20.0 24.5 29.4 15.4 14.6 22.5 25.1 20.1 22.3 14.9 18.8 8.9 14.6 11.9 16.7 16.5 16.5 16.8 20.3 - - 21.3 22.8 18.2 19.9 d s n n a n a e U i a In addition to delivering higher accuracy, o1 provides more concise and straightforward answers. In the example illustrated in Figure 4, o1 generates shorter interpretations while offering the correct answer. In contrast, GPT-4 tends to generate hallucinated explanations alongside incorrect answers. We believe o1s improvement in both knowledge and reasoning is primarily attributed to the enhanced data and infrastructure employed during the training process (e.g., CoT data and the reinforcement learning technique). These results together provide positive answer to the question we raised in this paper: Yes! We are getting closer to an automatic AI doctor with the latest o1 model."
        },
        {
            "title": "Preprint",
            "content": "Table 4: AlignScore and Mauve results on 3 tasks across 2 aspects Aspect Task Datasets i a e g o R Text Summary Concept Recognition Clinical Decision Support MIMIC-IV-Ultrasound (Johnson et al., 2023) MIMIC-IV-CT (Johnson et al., 2023) RCT-Text (Wallace et al., 2021) MedQSum (Lee et al., 2021) Average HumanDO (Schriml et al., 2019) BioLORD (Remy et al., 2024) Average EBMS (Molla & Santiago-Martinez, 2011) PUBHEALTH Exp. (Kotonya & Toni, 2020) ChatDoctor (Li et al., 2023) MedNLI-Gen. (Romanov & Shivade, 2018) Average AlignScore GPT-4 GPT-3.5 Mauve o1 GPT-4 GPT-3.5 30.9 13.3 4.9 37.1 21.6 5.5 19.0 12.3 6.6 19.0 20.4 9.7 13.9 23.6 13.8 5.7 13.6 14. 5.2 17.9 11.6 5.7 17.9 16.6 2.5 10.7 6.1 0.4 3.1 42.1 12.9 8.2 51.6 29.9 19.5 2.1 0.7 5.3 6.9 7.4 0.5 2.7 52.7 15. 0.4 4.2 2.3 1.9 0.8 0.5 4.5 1.9 7.3 0.5 11.9 0.6 5.1 0.4 1.1 0.8 2.3 1.1 0.6 0.9 1.2 27.5 14.4 4.9 34.5 20.3 17.5 13.0 15.3 9.0 14.8 26.5 6.8 14.3 Figure 4: Answers from o1 and GPT-4 on question from LancetQA. o1 provides more concise and accurate reasoning process compared to GPT-4. 4.3 FURTHER ANALYSIS No model excels across all tasks in the medical domain. Table 2 and Table 3 indicate that, for now, there are always trade-offs (even under the same metric) to be made when selecting model to use in the medical domain. One example is the clinical decision support task in Table 2, o1 outperforms both GPT-4 and GPT-3.5 on most datasets, but lags far behind GPT-4 on the MIMIC4ED-Critical Triage dataset by 5% in accuracy. Interestingly, we also found the recent released open LLM Llama3 takes lead in PMC-Patient and PICO-Intervention datasets with an unexpected 19.6% accuracy gap between o1 and Llama3 on PMC-Patient (76.4% vs. 96.0%). Nevertheless, o1 comes close to being the best in most situations, it boasts leading position across datasets in clinical decision support, knowledge QA, and medical calculation. This claim is supported by the average"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Accuracy of LLMs on two agentic benchmarks Tasks AI Hospital (Fan et al., 2024) AgentClinic (Schmidgall et al., 2024) Symp. Medical Exam. Diagnostic Results Diagnostic Rationales Treatment Plan MedQA NEJM o1 GPT-4 GPT-3. 67.0 66.7 62.0 43.4 45.0 40.7 45.1 44.2 35.8 45.1 45.8 36.3 39.9 38.2 24.7 45.5 30.4 25. 20.0 10.0 7.5 Table 6: Accuracy results of model results with/without CoT prompting on 5 knowledge QA datasets Datasets PubMedQA (Jin et al., 2019) MedQA (Jin et al., 2021) MedMCQA (Pal et al., 2022) LancetQA NEJMQA o1 75.0 95.0 75.5 81.5 91. o1 (CoT) GPT-4 GPT-4 (CoT) 75.2 95.2 81.9 85.5 96.3 52.8 79.5 69.7 76.0 83.5 62.2 86.1 72.6 81.5 86. result over 19 dataset accuracy in Table 2 and Figure 2: o1 (74.3%) > GPT-4 (68.1%) > GPT-3.5 (53.2%) Advanced prompting can partially help models trained with CoT data. o1 was released using chain-of-thought (CoT) data embedding in the training process; however, we found that applying the CoT prompting still enhances o1s performance on knowledge QA tasks in medicine, as shown in Table 6. The table reveals an average boost of 3.18% over the original 83.6% accuracy of o1. While this improvement is not as significant as with GPT-4, CoT proves to be promising way for guiding o1 in medical tasks. However, when it comes to other fancy promptings, such as self-consistency (SC) (Wang et al., 2022) and reflex (Shinn et al., 2024), this conclusion may not stand still. We witness an average performance decline of 12.8% using these two strategies compared to only CoT on LancetQA  (Table 7)  . Hallucination remains significant challenge. We use AlignScore (Zha et al., 2023) to evaluate hallucination in LLMs. In Table 4, the o1 model demonstrates 1.3% decrease in AlignScore compared to GPT-4 across five text summarization datasets. Moreover, the overall improvements of o1 across three tasks  (Table 4)  in AlignScore significantly lag behind those of other evaluation metricsaveraging 0.7 in AlignScore compared to 9.9 in Mauve relative to GPT-4. This indicates that o1 is still susceptible to language hallucination, highlighting that such problem remains persistent challenge in LLMs. o1 struggles in reasoning over complex multilingual tasks. Advanced LLMs are expected to demonstrate equivalent reasoning abilities to languages other than English. However, as o1 consistently outperforms other models in multilingual QA tasks: o1 (85.2%) > GPT-4 (75.7%) > GPT-3.5 (54.1%) on average  (Table 8)  , it falls short in much more complex Chinese agent benchmark in Table 5showing 1.6% accuracy drop in the medical examinations scenario over GPT-4 (43.4% vs. 45.0%), leaving its multilingual reasoning in complex situations to be desired. This interesting outcome might be attributed to the lack of multilingual CoT data during o1s training, as learning complex reasoning routes generally requires more efforts than plain instructions in the few-shot paradigm (Kim et al., 2023; Singh et al., 2024). We present failure example of o1 on AI Hospital in Figure 5. We identified instances of mixed language output in the generation from the doctor, which contribute to the suboptimal performance of o1 in this context. LLMs are facing biased judgement using different metrics. Choosing different metrics can lead to varied results of LLM evaluation (Liang et al., 2022), in our experiments, we observe similar unaligned trend even leveraging traditional NLP metrics such as BLEU-1, ROUGE-1, and Mauve. In most cases from Table 3, o1 surpasses GPT-4 in both two traditional reference-based measurements (i.e., BLEU-1, ROUGE-1) on average. One exception arises in the BLEU-1 comparison for clinical suggestion tasks. While o1 significantly triumph over GPT-4 in ROUGE-L (24.4% vs. 17.2%), it surprisingly underperforms in BLEU-1: o1 (15.3) < GPT-4 (16.2). When considering Mauve"
        },
        {
            "title": "Preprint",
            "content": "Table 7: Accuracy ablation results of using different promptings using o1 on our LancetQA"
        },
        {
            "title": "CoT",
            "content": "SC"
        },
        {
            "title": "Accuracy",
            "content": "81.5 85.5 84.5 61.0 Table 8: Accuracy of models on the multilingual task, XmedBench (Wang et al., 2024) Models English Chinese French Spanish Arabic Hindi Average o1 GPT-4 GPT-3.5 Meditron-70B* 76.4 75.7 72.0 58.7 80.2 61.0 47.4 44.3 95.4 89.4 58.9 53.3 95.0 91.2 74.2 59. 74.9 60.8 39.7 19.3 89.3 76.3 32.5 31.3 85.2 75.7 54.1 44.4 scores, although o1 consistently surpasses GPT-4 in both averaged BLEU-1 and ROUGE-1 for text summarization tasks, it still falls short by 2.9 points in Mauve, even when evaluated on the same output texts. similar anomaly can also be observed in the comparison between accuracy and F1 score. While Llama3 significantly outperforms o1 in accuracy on two concept recognition datasets, it consistently falls behind o1 in F1 on the same cases. These findings underscore the urgent need to identify or devise more reliable metrics for modern LLMs."
        },
        {
            "title": "5 DISCUSSION",
            "content": "What adverse impacts does o1 bring? The model o1 has made significant strides in both general NLP and the medical domainas demonstrated in this paper. But what adverse impacts does o1 have on users compared to the previous generations of LLMs? While embedding the Chain of Thought (CoT) process during generation by default requires more time (OpenAI, 2024), what exactly distinguishes o1 from other OpenAI models? In Table 10, we see that o1 has more than 2 and 9 longer decoding time cost on four medical tasks compared to GPT-4 and GPT-3.5, respectively (13.18s vs. 6.89s vs. 1.41s). This increased decoding time can lead to significant waiting periods when handling complex tasks. Additionally, o1 does not always outperform other models, with inconsistent performance across different tasks. For instance, in the concept recognition task detailed in Table 2, o1 underperforms compared to other LLMs on half of the datasets. This discrepancy may relate to recent findings suggesting that CoT data is most advantageous in more complex reasoning tasks (Sprague et al., 2024). However, in tasks that do not require complex reasoning, such as concept recognition, o1 does not have significant advantages over them. Rethinking evaluation metrics for stronger LLMs. Traditional evaluation metrics like BLEU and ROUGE, which rely on n-gram overlap, have long been criticized for their limitations in capturing the quality of generated text, particularly for LLMs. As result, using models like GPT-4 as evaluators, i.e., LLM-as-a-judge, has gained popularity for assessing the outputs of other models. However, this approach may not be valid when applied to the most advanced models such as o1, as GPT4 is even less capable and thus may produce less reliable evaluation. This is especially true for specialized domain like medicine. Therefore, there is growing need to develop more robust and nuanced evaluation metrics that can better assess the performance of state-of-the-art LLMs in complex scenarios. Call for reliable prompting techniques for future LLMs. As noted in Section 4.3, not all advanced prompting techniques positively impact o1s performance. As future LLMs like o1 may continue to evolve with internal prompts for efficient user instruction, new prompting methods should consider their adaptability to existing strategies. One potential exploration could be the integration of two prompting strategies (Wang et al., 2022; Zheng et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Limitations. While we conduct comprehensive evaluations in the medical domain on understanding, reasoning, and multilingual capabilities, there are many other dimensions to consider such as safety (Han et al., 2024) and we leave them for future work. Additionally, we leave more advanced prompting techniques such as retrieval augmented generation (RAG) (Lewis et al., 2020) for future work, which may enhance the factuality and mitigate hallucination. It is worth noting that current GPT-like models may still underperform BERT-based specialists in classification tasks (Nori et al., 2023b). However, we focus on GPT-like generalists in this paper due to their greater flexibility as zero-shot learners."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This preliminary study assesses 3 important aspects across 35 existing and 2 novel medical datasets using the latest o1 model. It marks the first step towards holistic evaluation of o1 in medicine, and we present our initial results, analysis, and discussion over the benchmark. The findings provide convincing evidence that o1 is narrowing the gap between AI and human doctors, shaping the vision of an ideal AI doctor closer to reality."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work is partially supported by the OpenAI Researcher Access Program and Microsoft Accelerate Foundation Models Research Program. Q.J. is supported by the NIH Intramural Research Program, National Library of Medicine. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Hogberg, Ulla Stenius, and Anna Korhonen. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics, 32(3):432440, 2016. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering and explaining challenging medical questions. arXiv preprint arXiv:2402.18060, 2024. Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Snigdha Dubey, Gaurav Tiwari, Sneha Singh, Saveli Goldberg, and Eugene Pinsky. Using machine learning for healthcare treatment planning. Frontiers in Artificial Intelligence, 6:1124182, 2023. Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and Jingren Zhou. Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for clinical diagnosis. arXiv preprint arXiv:2402.09742, 2024."
        },
        {
            "title": "Preprint",
            "content": "Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: new dataset for automatic medical diagnosis. Advances in neural information processing systems, 2022. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36, 2024. Harsha Gurulingappa, Abdul Mateen Rajput, Angus Roberts, Juliane Fluck, Martin Hofmann-Apitius, and Luca Toldo. Development of benchmark corpus to support the automatic extraction of drug-related adverse effects from medical case reports. Journal of biomedical informatics, 45(5): 885892, 2012. Tessa Han, Aounon Kumar, Chirag Agarwal, and Himabindu Lakkaraju. Towards safe large language models for medicine. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 2021. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg-chest radiographs with structured labels. PhysioNet, 2019. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023. Nikhil Khandekar, Qiao Jin, Guangzhi Xiong, Soren Dunn, Serina Applebaum, Zain Anwar, Maame Sarfo-Gyamfi, Conrad Safranek, Abid Anwar, Andrew Zhang, et al. Medcalc-bench: Evaluating large language models for medical calculations. arXiv preprint arXiv:2406.12036, 2024. Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The cot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023. Neema Kotonya and Francesca Toni. Explainable automated fact-checking for public health claims. arXiv preprint arXiv:2010.09926, 2020. Jooyeon Lee, Huong Dang, Ozlem Uzuner, and Sam Henry. Mnlp at mediqa 2021: fine-tuning pegasus for consumer health question summarization. In Proceedings of the 20th Workshop on Biomedical Language Processing, pp. 320327, 2021. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Jiao Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn Mattingly, Thomas Wiegers, and Zhiyong Lu. Biocreative cdr task corpus: resource for chemical disease relation extraction. Database, 2016, 2016. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023."
        },
        {
            "title": "Preprint",
            "content": "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Valentin Lievin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can large language models reason about medical questions? Patterns, 2024. Chin-Yew Lin and Eduard Hovy. Manual and automatic evaluation of summaries. In Proceedings of the ACL-02 workshop on automatic summarization, pp. 4551, 2002. John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. proposal for the dartmouth summer research project on artificial intelligence, august 31, 1955. AI magazine, 1955. AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. Diego Molla and Maria Elena Santiago-Martinez. Development of corpus for evidence based medicine summarisation. In Proceedings of the Australasian Language Technology Association Workshop 2011, pp. 8694. Australian Language Technology Association, 2011. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023a. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023b. Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain Marshall, Ani Nenkova, and Byron Wallace. corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 197. NIH Public Access, 2018. OpenAI. Openai o1 system card. openai-o1-system-card/, September 2024. https://openai.com/index/ Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 2022. Evangelos Pafilis, Sune Frankild, Lucia Fanini, Sarah Faulwetter, Christina Pavloudi, Aikaterini Vasileiadou, Christos Arvanitidis, and Lars Juhl Jensen. The species and organisms resources for fast and accurate identification of taxonomic names in text. PloS one, 8(6):e65390, 2013. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning. PMLR, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi Kiran GV, Jan Kocon, Bartłomiej Koptyra, Satyapriya Krishna, Ronald McClelland Jr. au2, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanisław Wozniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. In COLM, 2024."
        },
        {
            "title": "Preprint",
            "content": "Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:48164828, 2021. Francois Remy, Kris Demuynck, and Thomas Demeester. BioLORD-2023: semantic textual representations fusing large language models and clinical knowledge graph insights. Journal of the American Medical Informatics Association, 2024. Alexey Romanov and Chaitanya Shivade. Lessons from natural language inference in the clinical domain. arXiv preprint arXiv:1808.06752, 2018. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. Max Savery, Willie Rogers, Malvika Pillai, James Mork, and Dina Demner-Fushman. Chemical entity recognition for medline indexing. AMIA Summits on Translational Science Proceedings, 2020:561, 2020. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960, 2024. Lynn Schriml, Elvira Mitraka, James Munro, Becky Tauber, Mike Schor, Lance Nickle, Victor Felix, Linda Jeng, Cynthia Bearer, Richard Lichenstein, et al. Human disease ontology 2018 update: classification, content and workflow expansion. Nucleic acids research, 47(D1):D955D962, 2019. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 2024. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. To cot or not to cot? chainof-thought helps mainly on math and symbolic reasoning. arXiv preprint arXiv:2409.12183, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023a. Haoqin Tu, Bingchen Zhao, Chen Wei, and Cihang Xie. Sight beyond text: Multi-modal training enhances llms in truthfulness and ethics. arXiv preprint arXiv:2309.07120, 2023b. Byron Wallace, Sayantan Saha, Frank Soboczenski, and Iain Marshall. Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization. AMIA Summits on Translational Science Proceedings, 2021:605, 2021. Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023."
        },
        {
            "title": "Preprint",
            "content": "Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, and Benyou Wang. Apollo: Lightweight multilingual medical llms towards democratizing medical ai to 6b people. arXiv preprint arXiv:2403.03640, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 2022. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, pp. ocae045, 2024a. Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards evaluating and building versatile large language models for medicine. arXiv preprint arXiv:2408.12547, 2024b. Feng Xie, Jun Zhou, Jin Wee Lee, Mingrui Tan, Siqi Li, Logasan S/O Rajnthern, Marcel Lucas Chee, Bibhas Chakraborty, An-Kwok Ian Wong, Alon Dagan, et al. Benchmarking emergency department prediction models with machine learning and public electronic health records. Scientific Data, 9(1):658, 2022. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. Alignscore: Evaluating factual consistency with unified alignment function. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. large-scale dataset of patient summaries for retrieval-based clinical decision support systems. Scientific data, 10(1):909, 2023. Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. arXiv preprint arXiv:2408.16326, 2024."
        },
        {
            "title": "A SUPPLEMENTAL MATERIAL",
            "content": "A.1 PROMPTING STRATEGIES Base Prompt for MCQ. Question: {question} Options: A) ...... B) ...... ....... {Format Constraint} Format Constraint Examples for MCQ. Default: Answer only with the option index such as A/B/C/D in plain text. True/False Statement Questions: Answer only with Yes/No in plain text. Few-Shot Prompt. Case1: ... Case2: ... Case3: ... ... {Manually Written Definitions} Please learn from the few-shot cases to see what content you have to output. {Input Case} CoT Format Constraint. Reason step-by-step before answering. {Base Format Instruction}. Your final output should strictly follow this format: Reason{your step-by-step reasoning}/Reason Answer{your answer}/Answer Self Consistency. Given the following question and the {n sample} answers, please select the most consistent response with other answers and the question. {Base Format Constraint} in strictly this format: Answer{your final answer}/Answer. # Question: {Base Prompt with CoT} # Answer 1: {Model Answer 1} # Answer 2: {Model Answer 2} # Answer 3: {Model Answer 3}"
        },
        {
            "title": "Preprint",
            "content": "Prompt for Critic Generation for Reflex. {Base Prompt with CoT Format Constraint} # Response: {Model Response} Please review the answer above and criticize on where might be wrong. If you are absolutely sure it is correct, output True. Prompt for Reflected Answer Generation for Reflex. {Base Prompt with CoT Format Constraint} # Original Answer: {Model Answer} # Critic: {Model Critic} Given previous attempts and feedback, carefully consider where you could go wrong in your latest attempt. Using insights from previous attempts, try to solve the task better. Prompt for Final Answer Generation for Reflex. {Base Prompt with CoT Format Constraint} # Answer 1: {Reflected Answer 1} # Answer 2: {Reflected Answer 2} # Answer 3: {Reflected Answer 3} Please summarize the previous attempts and feedback and provide final answer. {Base Format Constraint} in strictly this format: Answer{your final answer}/Answer. A.2 DETAILS ABOUT DATASETS In this paper, we present summary of 36 medical-related datasets spanning 6 distinct tasks, as outlined in Table 1. Notably, the inclusion of commercial models, particularly o1, leads to significant costs and response latency. To address this, for some tasks we randomly sampled subset of test cases, which are detailed below. Concept Recognition BC4Chem (Savery et al., 2020) is dataset comprising 10,000 PubMed abstracts with 84,355 chemical entity mentions, manually annotated by expert chemistry literature curators. The task is to extract chemical names from the given abstracts. For evaluation, we randomly sample 300 instances from the test set. BC5Chem and BC5Disease are from BC5CDR (Li et al., 2016), widely-used resource in biomedical natural language processing, annotated for chemical and disease entities and their relationships. Following MedS-Bench (Wu et al., 2024b), BC5CDR is split into 2 datasets: chemical name extraction and disease name extraction. For evaluation, we randomly sample 300 instances from each tasks test set. Species800 (Pafilis et al., 2013) comprises 800 PubMed abstracts with annotated organism mentions. The task is to extract organism names from the given abstracts. For evaluation, we randomly sample 300 instances from the test set."
        },
        {
            "title": "Preprint",
            "content": "HoC (Baker et al., 2016) is specialized dataset containing 1,852 PubMed publication abstracts, expertly annotated according to taxonomy of cancer hallmarks. The task is to classify the hallmarks of cancer based on the given biomedical publication abstracts. For evaluation, we use the entire test set consisting of 158 instances. HumanDiseaseOntology (Schriml et al., 2019) is database providing consistent, reusable, and sustainable descriptions of human disease terms, phenotype characteristics, and related medical vocabularies. The task is to explain specified medical professional entities, with the database descriptions serving as ground truth. For evaluation, we randomly sample 300 instances. BioLORD (Remy et al., 2024) comprises pairs of biomedical concept names and descriptions. The task is to elaborate on concise concepts by generating long, detailed definitions. For evaluation, we randomly sample 300 instances. PMC-Patient (Zhao et al., 2023) is collection of 167,000 patient summaries extracted from case reports in PubMed Central (PMC), annotated with basic patient information. The task is to extract patient gender and age information from given clinical texts. For evaluation, we randomly sample 300 instances. PICO-Participant, PICO-Intervention and PICO-Outcome are three datasets derived from PICO (Nye et al., 2018), consisting of 5,000 abstracts from medical articles on randomized controlled clinical trials. The tasks involve extracting information about study participants, interventions, and outcomes from given sentences. For evaluation, we use the entire test set of 43 instances for each task. ADE Corpus (Gurulingappa et al., 2012) provides information on drugs and their corresponding adequate doses within sentences. The task is to extract the dosage levels of specified drugs from given sentences and drug names. We use the dataset prompted by Super-Instruction with 9:1 ratio for instruction tuning and evaluation. The test set consists of 23 instances. Text Summary MIMIC-IV-CT and MIMIC-IV-Ultrasound (Johnson et al., 2023; Wallace et al., 2021) are subsets of MIMIC-IV Report, large deidentified medical dataset of patients admitted to the Beth Israel Deaconess Medical Center. The task is to summarize radiology reports, treating the impression part as general summary of the findings. Following (Wu et al., 2024b), we randomly sampled 500 cases from body region part of Chest CT and 100 cases from ultrasound modality for evaluation. RCT-Text (Wallace et al., 2021) is dataset for summarizing medical evidence from clinical studies in literature reviews. The task is to output the primary conclusions of each study given the titles and abstracts. For evaluation, we randomly sample 100 instances. MedQSum (Lee et al., 2021) is derived from large database of de-identified health-related data. The task is to generate summary of detailed findings from imaging diagnostic reports, with the conclusion of the note serving as ground truth. For evaluation, we randomly sample 100 instances. Knowledge QA MedQA (Jin et al., 2021) is collection of medical multiple-choice questions in English. We use the 4-option English version with the official split. The test set contains 1273 samples. PubMedQA (Jin et al., 2019) is an English question-answering dataset based on PubMed abstracts. The task is to answer research questions with yes/no/maybe. We use the PQA-L subset as the test set, containing 1000 samples. MedMCQA (Pal et al., 2022) is large-scale English multiple-choice question-answering dataset from AIIMS & NEET PG entrance exams. We use the official test split containing 4183 questions, each with 4 choices. LancetQA and NEJMQA are datasets curated from The Lancet and the New England Journal of Medicine case challenges, focusing on patient diagnosis based on symptoms. We use 200 samples for LancetQA and 100 samples for NEJMQA. Medbullets (Chen et al., 2024) is dataset curated from the Medbullets online platform, comprising 308 USMLE Step 2&3 style questions. Each question includes case description, four answer choices, and an explanation."
        },
        {
            "title": "Preprint",
            "content": "Clinical Decision Support DDXPlus (Fansi Tchango et al., 2022) is dataset for Automatic Symptom Detection and Automatic Diagnosis systems, featuring synthesized patient data. The task is to make diagnostic decisions based on dialogues. For evaluation, we randomly sample 300 instances. SEER (Dubey et al., 2023) is treatment planning dataset based on the Surveillance, Epidemiology, and End Results breast cancer databases. The task is to recommend treatment plans from five types. For evaluation, we randomly sample 300 instances. MIMIC4ED-Hospitalization, MIMIC4ED-72h ED Revisit, and MIMIC4ED-Critical Triage are datasets from the MIMIC4ED Benchmark (Xie et al., 2022) for predicting clinical outcomes in emergency medicine. For each dataset, we randomly sample 300 instances for evaluation. MedNLI-Dis. (Discriminative) and MedNLI-Gen. (Generative) are derived from MedNLI (Romanov & Shivade, 2018), natural language inference dataset for the clinical domain. The dataset involve discriminative and generative entailment based on clinical premises. For each task, we randomly sample 300 instances for evaluation. EBMS (Molla & Santiago-Martinez, 2011) is justification verification dataset. We use the entire test set of 304 instances for evaluation. PUBHEALTH Exp. (Explanation) (Kotonya & Toni, 2020) requires models to provide explanations for specified claims using supporting material from given paragraphs. For evaluation, we randomly sample 300 instances. PUBHEALTH Ver. (Verification) (Kotonya & Toni, 2020) is fact verification task where models determine if claim contradicts evidence in given paragraph. For evaluation, we randomly sample 300 instances. Chatdoctor (Li et al., 2023) is based on 100K patient-physician conversations from an online medical consultation website4. The task involves engaging in medical consultations based on this data. Agent AI Hospital (Fan et al., 2024) is multi-agent framework simulating medical interactions in Chinese. It includes Patient, Examiner, Chief Physician, and Doctor agents, with 506 cases from diverse departments. The task involves simulating clinical scenarios through dialogue. Evaluation uses Chief Physicians 1-4 scale scoring across five dimensions: symptoms, examinations, diagnostic results, rationales, and treatment plan. 200 cases are sampled for evaluation. AgentClinic (Schmidgall et al., 2024) is clinical environment benchmark with 107 patient agents from MedQA and 15 multimodal agents from NEJM challenges. The task is patient diagnosis through dialogue and data collection. Evaluation considers diagnostic accuracy and patient perception metrics in biased scenarios. Medical Calculation MedCalc-Bench (Khandekar et al., 2024) evaluates LLMs medical calculation abilities using 1,047 instances across 55 tasks. It requires computing medical values from patient notes and questions. Evaluation compares LLM outputs to ground truth, with exact matches for rule-based and 5% tolerance for equation-based calculators. Multilinguality XMedBench (Wang et al., 2024) is multilingual medical benchmark in six languages: English, Chinese, Hindi, Spanish, French, and Arabic. It uses multiple-choice questions from various sources, including translated versions for Arabic and Hindi. The task evaluates LLMs medical knowledge across languages, using accuracy as the primary metric. AI Hospital (Fan et al., 2024) is multi-agent framework simulating medical interactions in Chinese. We also include this dataset into the multilinguality aspect because it is in Chinese. 4www.healthcaremagic.com"
        },
        {
            "title": "Preprint",
            "content": "A.3 MODEL-BASED EVALUATION As discussed in Section 5, Rethinking evaluation metrics for stronger LLMs, we also explore using techniques such as LLM-as-a-judge to assess the quality of generated outputs. Table 9 shows that o1 achieves nearly the same score as GPT-4 and outperforms GPT-3.5 (i.e., 3.3% vs. 3.3% vs. 3.0%), which contrasts with the traditional evaluation metrics in Table 3. This indicates that the LLM-as-a-judge method may be unreliable when applied to advanced models like o1, as GPT-4, being less capable, may provide less accurate evaluations. This limitation is particularly evident in specialized domains such as medicine. The prompt used for LLM-as-a-judge is shown in Figure A.3. Table 9: GPT Evaluation Score Comparison Task Datasets GPT Score o1 GPT-4 GPT-3. Text Summarization Clinical Suggestion medqsum RCT-Text MIMIC-IV-Ultrasound MIMIC-IV-CT MedNLI-Generative EMBS Justification Ver. PUBHEALTH Exp. Do Entity Exp. BioLORD Concept Exp. ChatDoctor Average 4.1 3.2 3.8 3. 2.3 3.1 3.0 3.7 3.3 2.5 3.3 3.8 3.2 3.8 3.8 2.4 3.0 3.3 3.6 3.3 2.6 3.3 4.1 3.1 3.4 3. 2.5 3.0 3.2 3.3 3.0 3.3 Prompt for LLM-as-a-judge. You are senior medical expert. Please evaluate the quality of the medical text material provided by medical interns based on the expert medical text material as reference answer. The quality is divided into five levels: 5. The assistant result completely matches the reference. 4. The assistant result is generally consistent with the reference, with only small part of omissions or errors. 3. The assistant result partially matches the reference, but there are some omissions and errors. 2. The assistant result is mostly inconsistent with the reference, with many omissions and errors. 1. The assistant result is completely inconsistent with the reference. {Input Medical Questions} Assistant Result: {Result} Reference Answer: {Reference} Please note: (1) Focus on the factual content of the medical answers, without concern for style, grammar, punctuation, and non-medical content. (2) Your response should be in the format. Rating: (int) A.4 DECODING TIME We evaluated the models time cost and the average number of decoding tokens across various tasks, including Knowledge QA, Clinical Decision Support, Text Summary, and Concept Recognition. For each task, we select representative dataset and perform inference on 50 samples. The time and decoded tokens are then averaged to obtain the results for each response, as illustrated in Table. 10. The decoding time for o1 is significantly higher than both GPT-4 and GPT-3.5, taking more than double the time of GPT-4 and over nine times that of GPT-3.5 across four medical tasks (13.18s compared to 6.89s and 1.41s, respectively)."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Model time cost and averaged number of decoding tokens for 4 datasets across 4 tasks Task Dataset Model Time (s) Prompt Tokens Completion Tokens Reasoning Tokens Total Tokens Knowledge QA MedQA Clinical Decision Support ChatDoctor Text Summary MIMIC-IV Concept Recognition BC5Chem o1 GPT-4 11.13 0.83 GPT-3.5 0.52 o1 GPT11.40 18.88 GPT-3.5 2.40 247.78 236.20 236.20 122.64 124.24 124.24 o1 GPT-4 20.56 1305.54 1254.84 6.26 GPT-3.5 2.02 1254.84 o1 GPT9.62 1.60 GPT-3.5 0.68 292.78 297.24 297.24 953.42 9.26 10.02 1127.44 509.28 150.10 1080.54 162.68 159.94 1080.54 19.64 12. 924.16 0 0 83.84 0 0 1201.20 245.46 246.22 1250.08 633.52 274.34 1057.28 1373.32 1417.52 1414.78 0 1057.28 1373.32 316.88 310.04 0 0 Figure 5: Failure case of o1 on AI Hospital. The model struggles with generating the right diagnosis and outputs mixed-language, resulting to its suboptimal performance in this context. A.5 CASE STUDY To demonstrate how the advanced o1 model outperforms previous methods in potential clinical applications, we present comparative case studies. As shown in Figure 6 and Figure 4, when responding to questions from NEJMQA and LancetQA, o1 provides the correct answer with more concise and precise reasoning process. In contrast, GPT-4 generates longer reasoning path while leading to an incorrect answer. in Figure 7, when offering diagnosis and treatment based on patient records, o1 delivers more accurate diagnosis and practical treatment plan, closely aligning with human experts and outperforming GPT-4. These cases demonstrate that o1 more closely resembles an AI Doctor."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Comparison of the answers from GPT-o1 and GPT-4 for question from NEJMQA. o1 provides more concise and accurate reasoning process compared to GPT-4."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Comparison of the answers from GPT-o1 and GPT-4 for case from the Chinese dataset AI Hospital, along with its English translation. o1 offers more precise diagnosis and practical treatment suggestions compared to GPT-4."
        }
    ],
    "affiliations": [
        "National Institutes of Health",
        "UC Santa Cruz",
        "University of Edinburgh"
    ]
}