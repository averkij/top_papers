{
    "paper_title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models",
    "authors": [
        "Shufan Li",
        "Jiuxiang Gu",
        "Kangning Liu",
        "Zhe Lin",
        "Zijun Wei",
        "Aditya Grover",
        "Jason Kuen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality."
        },
        {
            "title": "Start",
            "content": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models Shufan Li1,2,, Jiuxiang Gu1, Kangning Liu1, Zhe Lin1 Zijun Wei1, Aditya Grover2, Jason Kuen1 1Adobe 2UCLA * Work done primarily during internship at Adobe Research 5 2 0 2 6 1 ] . [ 1 8 0 0 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to 2 speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality. 1. Introduction Improving visual understanding and generation capabilities has been major focus of artificial intelligence research. common paradigm is to employ autoregressive (AR) visionlanguage models (VLMs) for visual understanding tasks such as question answering, and diffusion models for visual generation tasks such as text-to-image generation and image editing. Recently, there has been rising interest in building unified multimodal models capable of both understanding and generation [12, 40, 63, 76]. These models often surpass the performance of single-task systems because they enable understanding and generation capabilities to mutually benefit from shared representations under unified framework, which is especially advantageous for tasks requiring both abilities, such as image editing. Early works such as Transfusion [76] and BAGEL [12] build unified multimodal models by combining AR VLMs Figure 1. We propose Sparse-LaViDa, novel modeling technique for unified multimodal masked discrete diffusion models. Sparse-LaViDa achieves substantial speedup across wide range of tasks, including text-to-image generation, image editing, and visual math reasoning, compared with the baseline LaViDa-O. with continuous diffusion models to handle understanding and generation tasks respectively. More recently, Masked Diffusion Models (MDMs) have emerged as promising alternative, offering unified modeling framework for both text and image generation [32, 33, 54, 66, 71]. Concretely, MDMs represent both text and images as sequences of discrete tokens. Given such sequence, the forward diffusion process gradually converts it into fully masked sequence. An MDM learns the reverse process by predicting the distribution of original tokens at the masked positions. To sample from MDMs, we start with an all-mask sequence and iteratively unmask tokens to obtain clean sequence. This formulation brings several advantages over AR models, such as faster inference via parallel decoding, controllable generation, and bidirectional context, etc. (Figure 2 Left) Notably, the unified MDM LaViDa-O [32] achieves strong performance across wide range of image understanding and generation tasks. Despite supporting parallel decoding, existing MDMs still face major efficiency limitations. First, they rely on 1 Figure 2. Overall design of Sparse-LaViDa. Left: Vanilla MDMs materialize all masked tokens and support arbitrary-order decoding (top-down). Unlike AR models, they have bidirectional context and naturally support tasks such as image generation and inpainting. Middle: Block Diffusion truncates redundant masked tokens from the right but imposes left-to-right generation order using blockcausal attention mask, losing many benefits of MDMs. Right: Sparse-LaViDa is an alternative parameterization of vanilla MDMs. It preserves all benefits of standard MDMs while achieving the efficiency gains of Block Diffusion by allowing mask truncation at arbitrary positions. Special register tokens serve as compressed representations of truncated tokens. full attention instead of the causal attention used by AR models. While this enables bidirectional context and naturally supports tasks such as text infilling and image inpainting, it prevents the use of KV-cache acceleration during inference. Second, they must repeatedly process the full sequence, including many redundant masked tokens, at every sampling step. For example, if an image is represented by 1024 tokens, the model process all 1024 tokens at each diffusion step, even though only small subset is unmasked at time. Recently, Block Diffusion [1] was proposed to improve MDM efficiency by constraining parallel decoding to left-to-right block-causal order (Figure 2, Middle). While effective for language modeling, this leftto-right generation scheme is poorly suited for image generation and editing tasks, where tokens do not follow natural ordering. Furthermore, its block-causal attention mask eliminates bidirectional context,a key advantage of MDMs, making tasks such as inpainting difficult. To address these limitations, we propose Sparse-LaViDa, novel modeling framework that improves the efficiency of MDMs by supporting KV-cache usage and enabling truncation of arbitrary subsets of redundant tokens during inference. Concretely, Sparse-LaViDa introduces three key innovations. First, we propose sparse parameterization of MDMs that represents partially masked sequences without materializing all masked tokens. At each sampling step, Sparse-LaViDa takes only the prompt tokens, previously generated tokens, and selected subset of masked tokens that need to be decoded, in contrast to vanilla MDMs which always materialize all masks. Second, we introduce special register tokens that serve as compressed representations of truncated tokens and help recover modeling capacity lost due to truncation. Finally, we design step-causal attention mask that enables KV-cache support during inference while allowing efficient training. Unlike the block-causal mask used in Block Diffusion, our step-causal attention mask preserves the bidirectional context essential for image generation, editing, and inpainting. These components are illustrated in Figure 2 (Right). To validate the effectiveness of Sparse-LaViDa, we conduct extensive experiments across image generation, understanding, and editing tasks. Sparse-LaViDa achieves significant efficiency gains, including 1.96 speedup on text-to-image generation, 2.80 speedup on image editing, and 2.84 speedup on visual math reasoning, while maintaining generation quality comparable to the unified MDM LaViDa-O [32] (Figure 1). Notably, these improvements are achieved on top of many optimizations already employed by LaViDa-O, such as token compression. 2. Related Work 2.1. Masked Diffusion Models Early works on masked modeling, such as BERT [13] and MAE [19], focus on learning semantically rich representations through masked autoencoding. MaskGIT [7] and Muse [8] were among the first to apply masked modeling to image generation, using VQGAN [14] to convert images into sequences of discrete tokens. Despite some success, these early approaches lacked strong theoreti2 (a) Inference with Sparse Parameterization (b) Inference Attention Mask of Sparse-LaViDa Figure 3. Inference pipeline of Sparse-LaViDa. (a) At specific decoding step (top-down), the input of Sparse-LaViDa consists of four types of tokens: (1) previously decoded tokens stored in the KV cache; (2) newly decoded tokens from the previous step, which will be added to the cache at this step; (3) masked tokens to be decoded at the current step; and (4) register tokens. (b) During sampling, we apply specialized attention mask such that tokens of type (2) cannot attend to tokens of type (3) or (4). In this example, tokens at positions 2 and 7 are of type (2) (newly decoded), while tokens at positions 1 and 10 correspond to types (3) and (4) (mask and register), respectively. cal foundation and relied heavily on heuristics. Recently, MDMs [2, 38, 51] have established principled theoretical framework by formulating the masking and unmasking processes as forward and reverse discrete diffusion processes. This provides unified and mathematically grounded approach for training and sampling in masked generative models. Building on this foundation, several works such as Mercury [24], LLaDA [45] and Dream [67] have successfully scaled MDMs to large-scale language modeling, achieving performance comparable to autoregressive counterparts while offering advantages such as bidirectional context and parallel decoding. Further extensions, including LaViDa, LaViDa-O, and MMaDa [32, 33, 54, 66, 71], have expanded MDMs to multimodal understanding and generation tasks, achieving impressive results. 0 , . . . , Formally, given sequence of discrete tokens X0 = [X 1 0 , 2 0 ], where denotes the sequence length, the forward masked diffusion process q(XtXs) progressively replaces clean tokens with masked tokens over the time interval [0, 1], with 1 0. At = 1, the sequence X1 = [M, M, . . . , ] consists entirely of masked tokens. For intermediate steps where 0 < < 1, Xt contains mixture of clean and masked tokens. neural network pθ is trained to model the reverse process p(XsXt). The masked diffusion objective is defined as: LMDM = Et,X0,Xt (cid:21) log pθ(X0Xt) , (cid:20) 1 (1) where pθ(X0Xt) is factorized as (cid:81)L 0Xt) under standard independence assumptions [51]. At inference, we begin with fully masked sequence X1 and iteratively apply i=1 pθ(X the learned reverse process log pθ(X0Xt) to progressively unmask tokens until clean sequence X0 is obtained. Most existing MDMs adopt dense parameterization. At intermediate steps where 0 < < 1, all tokens in Xt, both masked and unmasked, are passed to the neural network pθ, which outputs dense tensor RLV , where is the vocabulary size. Each y[i] RV represents the logits corresponding to log pθ(X 0Xt). This design is computationally inefficient because all tokens must be processed even when predictions are only needed for small subset. The key contribution of Sparse-LaViDa is sparse parameterization that directly addresses this inefficiency. 2.2. Acceleration Methods for MDMs MDMs are known to suffer from inefficiencies since they do not natively support KV caching. Several approaches, such as Fast-dLLM [60], dKV-Cache [41], and SparsedLLM [55], attempt to incorporate KV caching into MDMs in training-free manner through heuristic modifications. However, these methods mostly focus on diffusion large language models (dLLMs) and assume left-to-right, block-wise, semi-AR sampling scheme. Moreover, while training-free, these approaches often result in unpredictable performance degradation that varies across tasks. Training-based acceleration methods have also been proposed for MDMs. Prominent examples include Block Diffusion [1], SDAR [11], and D2F [57], which interpolate between autoregressive and diffusion modeling by introducing block-causal attention masks. Compared with heuristic, training-free methods, these approaches natively support KV caching without inference-time performance degradation and achieve greater speedups by truncating redundant Figure 4. Illustration of Sparse Representation of Masked Sequence. Instead of materializing all masked tokens, partially masked sequence can be uniquely represented by non-mask tokens, their locations, and the number of total tokens in the original sequence. tokens. However, similar to the previous category, they are mostly designed for language modeling and assume leftto-right semi-AR decoding order. Furthermore, they sacrifice the bidirectional context of MDMs, which is crucial for tasks such as image generation and inpainting. Sparse-LaViDa is the first method to support both KV caching and token truncation without assuming left-toright decoding order or sacrificing bidirectional context. It implements the standard MDM formulation described in Sec. 2.1 efficiently and faithfully, preserving all desirable properties of MDMs without any qualitycompromise or traininginference gap. 3. Method 3.1. Sparse Parameterization of Masked Sequence 0Xt). i=1 pθ(X The core idea of Sparse-LaViDa is based on the independence of assumption described in Sec. 2.1, where pθ(X0Xt) is factorized into (cid:81)L Since pθ(X 0Xt) are optimized in the training and sampled at inference independently, if we are only interested in the models prediction at subset of locations C, there is no reason to predict pθ(X 0 Xt) where / C. However, this observation alone does not allow us to truncate any input tokens, since is still part of Xt which are needed to compute pθ(X 0 Xt). Recall that Xt is partially masked sequence containing both clean tokens and mask tokens. Critically, masked tokens carry no substantive information beyond indicating that position was masked, making compression possible. Consider seven-token sequence have [m] dog [m] [m] [m]. Rather than representing this with 7 independent tokens, we can equivalently encode it using: (1) clean tokens with their positional embeddings (I at position 1, have at position 2, dog at position 4), and (2) special token indicating the total sequence length (e.g., 7). The positions of masked tokens are then implicitly determined, as they occupy all positions not taken by clean tokens. In other words, specifying the sequence length is sufficient to represent which positions are masked. Register Tokens. In practice, we find that using single special token is not sufficient and leads to considerable performance drop on image generation quality. There are two reasons. First, truncating tokens may leads to considerable drop in model capacity. For example, an 10241024 image is represented by 4096 tokens. In the first few sampling steps, we only sample less than 100 tokens. Although aggressively reducing the token count improves efficiency, it does so at the expense of the models capacity. Second, previous works[29] using special tokens for text-to-image generation show that sufficient number of special tokens is needed to meaningfully impact the generation process through attention mechanism. In Sparse-LaViDa , we use 64 special register tokens in our final design, whose position ids are consecutively located at the end of the sequence (in the above example, it will be 8-51). This number remains constant throughout the inference process and is small in relation to the total sequence length. It does not grow with the number of truncated masked tokens. 3.2. Sampling with Sparse Parameterization Given prompt p, we sample sequence of response tokens X0 (image or text) starting from fully masked sequence X1. We first prefill the KV cache with prompt tokens from p. At any sampling step k, the model input consists of the prompt p, all previously decoded tokens (C1 . . . Ck1), and the tokens to be decoded (Ck). Of these inputs, and C1 . . . Ck2 are already in the KV cache. The new cache tokens Ck1 (decoded in the previous step) are processed and added to the cache in this step. Then, the model produces logits only for the decode tokens Ck, which are sampled to unmask them. This process repeats until all tokens are unmasked. To enable proper KV cache updates, we apply specialized attention mask. Queries from Ck1 attend only to {p, C1, . . . , Ck1} and cannot attend to Ck. This design ensures cached representations are unaffected by masked tokens, enabling efficient training (see Sec. 3.3). We also include register tokens at each step, which can attend to all tokens but are only attended to by Ck and R. This process is illustrated in Figure 3. The remaining question to be addressed is how to decide the order of unmasking C1...CN where is the number of sampling steps. This strategy differs according to the task. Text-to-Image and Image Editing. For visual generation tasks, several works [4, 32] have proposed to use pregenerated 2D sequences as the unmasking order. Compared with confidence-based approaches which dynamically decides which token to unmask at each step based on confidence score, these pre-generated unmasking order have been shown to have higher generation quality. We use the stratified random sampler proposed by LaViDa-O [32], which generates an unmasking order without the need for confidence scores. This allows us to easily set up C1...CN based on pre-generated unmasking order. Text generation and Image understanding. Unlike image generation and editing tasks, language generation of MDMs typically employ semi-auto-regressive sampling strategy. sequence of length is first divided into blocks of equal size S. These blocks are sampled autoregressively from left-to-right. When sampling tokens with in block, we pass in all tokens in the same block and dynamically decide which tokens to unmask based on confidence scores. In this setup, while we do not know the exact tokens that will be unmasked at k-th step Ck, we know it belongs to some block Ck. In this step, the input to the model at kth step includes the prompt p, all previously decoded tokens C1...Ck1 and all tokens in B. Ck is determined after the forward pass based on per-token confidences. While the default left-to-right behavior is similar to Block Diffusion, we highlight that Sparse-LaViDa still supports bi-directional context and arbitrary decoding order of blocks, without assuming left-to-right block-causal attention masks. This means it can uniquely perform tasks such as text-infilling, constraint generation while methods like block diffusion are not capable because they only have one-sided context. Some of these examples are shown in Sec. 4.5. 3.3. Training Pipeline Step-Causal Attention Mask. To enable efficient parallel training while maintaining consistency with the inference of Sparse-LaViDa, we design step-causal attention mask that simulates the incremental token caching behavior observed during inference. In vanilla MDM training step, we have prompt and partially masked response Xt, and the model learns to predict the clean sequence X0 given (p, Xt). Full attention is applied, and all tokens may interact freely. However, this is incompatible with SparseLaViDa, since KV caching and token truncation imply that during inference: (1) not all tokens can attend to each other, and (2) the model may not observe all tokens. To close this traininginference gap, we partition the sequence Xt into + blocks and apply structured attention mask. Prompt tokens are assigned block number 0. Clean tokens in Xt are randomly assigned block numbers in {1, . . . , }. Masked tokens are randomly assigned block numbers in {M + 1, . . . , + }. clean token in block {1, . . . , } may only attend to tokens in blocks i. For masked tokens in block {M + 1, . . . , + }, attention is allowed only to blocks = (same masked block) or (prompt and clean tokens). Thus, masked tokens may interact with prompt and clean tokens, but not with masked tokens in other blocks. For every masked block, we append corresponding register tokens to the sequence and assign them the same block number. They follow the same attention rules as masked tokens. We note that although we use the term block, tokens with the same block number need not form contiguous Figure 5. Step-Causal Mask. We employ step-causal attention mask during training to match the inference behavior of SparseLaViDa. Consider sequence containing prompt tokens P0P3 and answer tokens X0X15, where some tokens are clean and others are masked (color-coded in blue and gray). During inference, prompt tokens and clean tokens are sequentially added to the KV cache. To simulate this behavior during training, we assign block number 0 to the prompt, and block numbers 13 to clean tokens, such that each token may only attend to tokens in its own block or previous blocks. The bottom figure shows tokens sorted by block number.At inference, the model only observes subset of masked tokens. To mimic this behavior in training, we assign block numbers 45 to masked tokens (e.g., X10, X12 in block 4 and X7, X8 in block 5). Each block is accompanied by corresponding register token (R4, R5). We apply an attention mask such that tokens in one masked block cannot attend to tokens in another masked block, but may attend to all clean and prompt tokens. This simulates inference paths 0 1 2 3 4 and 0 1 2 3 5 within single training step. segment in Xt. This design is illustrated in Fig. 5. Training Objective. Since Sparse-LaViDa is an alter5 Table 1. Text to Image Generation Performance on GenEval Dataset. *These models do not support 1024px generation. Parms Single Two Position Counting Color Attribution Overall Latency Speedup SDXL[48] DALLE 3[46] SD3[15] Flux-Dev[27] Playground v3[30] BAGEL [12] Show-o [65] MMaDa[66] 2.6B - 8B 12B - 14B 1B 8B LaViDa-O [32] Sparse-LaViDa 10.4B 10.4B 0.98 0.96 0.99 0.99 0.99 0.99 0.98 0.99 0.99 0. 0.74 0.87 0.94 0.85 0.95 0.94 0.80 0.76 0.85 0.93 0.39 0.47 0.72 0.74 0.72 0.64 0.31 0.20 0.65 0.63 0.85 0.83 0.89 0.79 0.82 0.81 0.66 0.61 0.71 0. 0.15 0.43 0.33 0.21 0.50 0.88 0.84 0.84 0.86 0.88 0.23 0.45 0.60 0.48 0.54 0.63 0.50 0.37 0.58 0.64 0.55 0.67 0.74 0.68 0.76 0.82 0.68 0.63 0.77 0. 5.2 - 23.3 31.6 - 45.1 * * - - - - - - - - 21.27 10.86 1.00 1.95 native parameterization of the standard MDM, it uses the standard MDM training objective. At each step, we sample ground truth sequence X0 from the data distribution (containing both images and text), sample time [0, 1], and draw Xt q(Xt X0) from the forward masking process. We then optimize the MDM objective Eq. (1) on the model output pθ(X0 Xt). The only difference from vanilla MDM training lies in how we implement pθ(X 0 Xt), following our sparse parameterization and step-causal masking. Unlike methods such as D2F [57], which rely on distillation-based objectives for post-hoc acceleration, Sparse-LaViDa provides fundamentally efficient parameterization that supports scalable training and inference without additional distillation stages. 4. Experiments 4.1. Setup We initialize Sparse-LaViDa with the pretrained weights of LaViDa-O [32], state-of-the-art 10.4B unified diffusion model that supports wide range of multimodal tasks, including image understanding, text-to-image generation, and image editing. Training. We perform supervised fine-tuning (SFT) on mixture of image understanding, generation, and editing datasets to adapt LaViDa-Os dense parameterization to the sparse design of Sparse-LaViDa. We source image understanding data from MAmmoth-VL [18] and VisualWebInstruct [22]. For text-to-image generation, we subsample 20M textimage pairs from LAION-2B [53], COYO-700M [6], SA-1B [25], JourneyDB [56], BLIP3o60k [10], and ShareGPT4o-Image [9]. We include GPTEdit-1.5M [59] for image editing. Overall, our training dataset is filtered subset of the LaViDa-O SFT data, selected for higher quality and efficient fine-tuning. We train for 100k steps on 64 NVIDIA H100 GPUs. Details of our data pipeline and hyperparameters are provided in the ApTable 2. Text-to-Image Generation Results on DPG-Bench and MJHQ-30K. We report the benchmark score of DPG and PickScore, HPS v3, HPS v3, and FID on MJHQ-30k. *We perform SFT on the same data mix as Sparse-LaViDa. DPG MJHQ-30k PickScore HPS v2 HPS v3 FID LaViDa-O [32] 81.8 LaViDa-O* [32] 82.1 82.4 Sparse-LaViDa 21.02 21.04 21.04 0.271 0.297 0.291 8.81 8.87 8. 6.68 8.11 7.63 pendix. Evaluation. We conduct extensive evaluations across diverse multimodal benchmarks to demonstrate the effectiveness of Sparse-LaViDa, including GenEval [17], DPG [21], and MJHQ-30k [30] for text-to-image generation; ImgEdit [68] for image editing; and suite of image understanding benchmarks [16, 37, 39, 43, 44, 72, 74]. We also report inference latency (seconds per image) and relative speedup with respect to the base model LaViDa-O. Unless otherwise stated, all image generation experiments are performed at 1024 resolution on single A100 GPU. 4.2. Text-to-Image Generation We report text-to-image generation results on the GenEval benchmark [17] in Table Tab. 1. We compare against the base model LaViDa-O and other state-of-the-art textto-image models such as Flux.1-Dev [27] and unified multimodal models such as MMaDa [66]. SparseLaViDa achieves performance comparable to LaViDa-O (+0.01) while substantially reducing end-to-end latency (21.27s vs. 10.86s), achieving 1.95 speedup. It also surpasses models such as Flux.1-Dev in both performance and efficiency, highlighting the effectiveness of our sparse parameterization. To further assess performance, we evaluate SparseLaViDa on DPG-bench [21] and MJHQ-30k [30]. DPG6 Table 3. Image Editing Performance on ImgEdit benchmark. We report per-category scores and the overall scores. Model GPT-4o [47] Qwen2.5VL+Flux [59] FluxKontext dev [28] OmniGen2 [61] UniWorld-V1 [34] BAGEL [12] Step1X-Edit [36] OmniGen [64] UltraEdit [75] AnyEdit [70] InstructAny2Pix[31] MagicBrush [73] Instruct-Pix2Pix[5] Add Adjust Extract Replace Remove Background Style Hybrid Action Overall Latency Speedup 4.61 4.07 3.76 3.57 3.82 3.56 3.88 3.47 3.44 3.18 2.55 2.84 2.45 111.4 55.2 51.4 84.8 56.2 88.2 - 126.2 - - 48.2 - 9.5 4.93 4.84 4.38 4.81 4.21 4.49 4.63 4.19 3.76 2.85 3.51 2.38 3. 3.96 3.04 2.96 2.52 2.96 2.38 2.64 2.24 1.91 1.56 1.42 1.62 1.20 4.33 3.79 3.45 3.06 3.64 3.31 3.14 3.04 2.81 2.95 1.83 1.58 1.83 4.35 4.13 3.98 3.74 3.47 3.30 3.40 2.94 2.96 2.47 2.54 1.97 2.01 3.66 3.89 2.94 3.20 3.24 2.62 2.41 2.43 1.45 2.23 1.17 1.58 1.50 2.90 2.04 2.15 1.77 2.27 1.70 1.76 1.71 2.13 1.88 2.10 1.51 1.44 4.20 3.80 3.52 3.44 3.26 3.20 3.06 2.96 2.70 2.45 2.12 1.90 1. 4.89 4.52 4.26 4.68 2.74 4.17 2.52 3.38 2.98 2.65 1.98 1.22 1.46 4.57 3.90 3.78 3.57 2.99 3.24 3.16 3.21 2.83 2.24 2.01 1.75 1.44 - - - - - - - - - - - - - LaViDa-O [32] Sparse-LaViDa 4.04 4.08 3.62 3. 2.01 2.10 4.39 4.29 3.98 3.98 4.06 4.06 4.82 4.84 2.94 3. 3.54 3.76 3.71 3.79 63.98 22.55 1.00 2.83 bench is evaluated using VQA model for prompt alignment, while MJHQ-30k reports FID and reward-based metrics including PickScore [26], HPS v2 [62], and the latest VLM-based reward HPS v3 [42] (higher is better). As shown in Tab. 2, Sparse-LaViDa outperforms the LaViDa-O baseline on DPG-bench (+0.6). On MJHQ-30K, Sparse-LaViDa achieves superior results across all perceptual metrics except FID (lower is better), which increases marginally by less than one point. Importantly, when both models are trained on the same 20M subset, our SparseMDM (FID 7.63) outperforms the baseline LaViDa-O* (FID 8.11), demonstrating that the sparse parameterization and step-causal training not only maintain but can improve generation quality under identical data conditions. 4.3. Image Editing We evaluate image editing performance on the ImgEdit benchmark [68], which measures both visual quality and prompt compliance via GPT-4 judge model. SparseLaViDa achieves higher accuracy (+0.08) compared to LaViDa-O and other state-of-the-art unified models such as BAGEL [12], while reducing end-to-end latency from 63.98s to 22.55s, achieving 2.83 speedup. 4.4. Image Understanding To assess text generation efficiency, we evaluate SparseLaViDa on the MathVista reasoning benchmark with generation length set to = 1024 tokens and block size = 32. We compare against LaViDa-Os vanilla sampling and FastdLLM [60], training-free KV-caching baseline. Results in Table Tab. 4 show that Sparse-LaViDa matches the base models accuracy while achieving 2.80 speedup. It also outperforms Fast-dLLM in both accuracy and latency, confirming the advantage of our learned truncation strategy. For completeness, we also report Sparse-LaViDa on additional understanding benchmarks including MME-C [16], MMMU [72], ChartQA [43], DocVQA [44], and MathVerse [74]. These results  (Table 5)  show that SparseLaViDa achieves competitive performance across all benchmarks. However, speedups are minimal for short QA tasks where outputs contain fewer tokens than one block (32 tokens), effectively reducing Sparse-LaViDa to prompt caching without truncation. Table 4. Quantative Results on Visual Math Reasoning. We compare Sparse-LaViDa with other caching strategies on MathVista accuracy and latency. Model MathVista Latency Speedup LaViDa-O [32] LaViDa-O+Fast-dLLM [60] Sparse-LaViDa 56.9 56.1 56.7 10.41s 5.57s 3.72s 1.00 1.87 2.80 Table 5. Image Understanding Performance. We report performance on wide range of image understanding tasks and compare the performance of Sparse-LaViDawith LaViDa-O baseline. Model MME MMMU MMB ChartQA DocVQA MathVista MathVerse LaViDa-O [32] Sparse-LaViDa 488 45.1 43.6 76.4 75.0 80.0 82.0 73.7 75.7 56.9 56.7 36.9 37. 4.5. Qualitative Results In Fig. 6, we present qualitative examples across understanding and generation tasks, including text-to-image generation and image editing. Notably, unlike semiautoregressive methods such as Block Diffusion, SparseLaViDa natively supports tasks requiring bidirectional context, such as image inpainting/outpainting, parallel object grounding, and constrained captioning. 4.6. Ablation Studies Effect of token caching and truncation. The speed advantage of Sparse-LaViDa primarily arises from two sources: 7 Table 7. Ablation Studies on the Number of Registers. We report text-to-image generation performance with different number of registers. #Reg GenEval DPG HPS v3 FID 0 1 32 64 0.76 0.76 0.77 0.78 80.3 79.6 82.1 82.4 8.68 8.71 8.87 8.89 9.32 9.50 8.25 7. fine-grained prompt alignment using VQA model, the absence of register tokens leads to larger performance drop. We also observe measurable differences in image quality metrics (FID and HPS v3), suggesting that register tokens primarily enhance low-level visual detail rather than highlevel structural coherence. Table 8. Ablation Studies on the Training Strategies. We demonstrate the effectiveness of our training strategy through performance on GenEval and DPG-bench. Model LaViDa-O [32] Sparse-LaViDa -No Step Causal Attention Mask -No Training GenEval DPG 0.77 0.78 0.71 0.24 81.8 82.4 78.9 47.9 Training strategy. We examine several design choices in our training pipeline, as summarized in Table Tab. 8. We find that applying the inference pipeline of SparseLaViDa to pretrained LaViDa-O without fine-tuning (No Training) results in significant performance degradation on GenEval and DPG. Additionally, removing Step-Causal Attention Mask adversely affect the performance because of mismatched behaviors between training and inference. 5. Conclusion and Future works. In conclusion, we propose Sparse-LaViDa, novel parameterization for multi-modal MDMs. It offers significant speedup on wide range of visual and text generation tasks such as text-to-image generation, image editing, and visual math reasoning without compromising generation quality. Despite promising results, Sparse-LaViDa has several limitations. First, while Sparse-LaViDa offers significant speedup, it requires additional training. We emphasize that Sparse-LaViDa offers faster speedup than most aggressive KV caching strategy that caches all possible tokens (Tab. 6), which is an upper-bound for all heuristic-based trainingfree methods. Second, while in principle Sparse-LaViDa is just an efficient parameterization for the standard MDM and can be used to pre-train large models from scratch, we conFigure 6. Qualitative results. Unlike semi-AR approaches like Block Diffusion, Sparse-LaViDa supports tasks requiring bidirectional context, such as inpainting/outpainting, parallel grounding, and constrained captioning. In text generation examples, colored regions denote masked tokens initialized for infilling. Table 6. Ablation Studies on Speed. We report the speedup contribution of each key deigns of Sparse-LaViDa on T2I task. Cache Prompt Cache Res Truncate Res Latency Speedup 21.27 16.43 18.87 17.93 14.09 15.79 13.72 10.86 1.00 1.29 1.13 1.19 1.51 1.35 1.55 1.96 token caching and truncation. To isolate their contributions, we perform ablations on text-to-image (T2I) tasks. Specifically, we decompose speedup into three components: caching prompt tokens, caching decoded response tokens, and truncating redundant tokens. We test all combinations of these components and report results in Table Tab. 6. As shown, enabling any single component improves efficiency, and combining all yields the maximum speedup. Effect of register tokens. To study the impact of register tokens, we experiment with 0, 1, 32, and 64 registers and report results in Table Tab. 7. We evaluate GenEval and DPG scores as well as FID and HPS v3 metrics on MJHQ-30k. On GenEval, which evaluates high-level prompt alignment via object detection, removing register tokens causes little degradation. However, on DPG-bench, which evaluates 8 ducted our experiments in post-trianing setup due to compute costs. In future we will explore additional scaling and train from scratch. 6. Acknowledgement AG would like to acknowledge the support from Schmidt Sciences and NSF Career Award #2341040. SL is in part supported by Amazon Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Interpolating beVolodymyr Kuleshov. Block diffusion: tween autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. 2, 3 [2] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5 [4] Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord. Halton scheduler for masked generative image transformer. arXiv preprint arXiv:2503.17076, 2025. 4 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 7 [6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. 6, [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 2 [8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 2 [9] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Sharegpt-4o-image: Aligning multimodal modWang. arXiv preprint els with gpt-4o-level image generation. arXiv:2506.18095, 2025. 6, 4 [10] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 6, 4 Biqing Qi, et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. 3 [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 6, 7 [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 6 [16] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 6, 7 [17] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 6 [18] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. 6, 4 [19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2 [20] Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, and Ponnuthurai Suganthan. Unified discrete diffusion for simultaneous vision-language generation. arXiv, 2022. 4 [21] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu Ella. Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 5(7): 16, 2024. [22] Yiming Jia, Jiachen Li, Xiang Yue, Bo Li, Ping Nie, Kai Zou, and Wenhu Chen. Visualwebinstruct: Scaling up multimodal instruction data through web search. arXiv preprint arXiv:2503.10582, 2025. 6, 4 [11] Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenhai Wang, Qipeng Guo, Kai Chen, [23] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in pho9 tographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 4 [24] Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 6, 4 [26] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:36652 36663, 2023. 7 [27] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 6 [28] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 7 [29] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. In Advances in Neural Information Processing Systems, pages 3014630166. Curran Associates, Inc., 2023. 4 [30] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. [31] Shufan Li, Harkanwar Singh, and Aditya Grover. Instructany2pix: Flexible visual editing via multimodal instruction following. arXiv preprint arXiv:2312.06738, 2023. 7 [32] Shufan Li, Jiuxiang Gu, Kangning Liu, Zhe Lin, Zijun Wei, Aditya Grover, and Jason Kuen. Lavida-o: Elastic masked diffusion models for unified multimodal understanding and generation. arXiv preprint arXiv:2509.19244, 2025. 1, 2, 3, 4, 6, 7, 8 [33] Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025. 1, 3 [34] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 7 Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 5 [36] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 6 [38] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 3, 1 [39] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6 [40] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, BINGYUE PENG, and XIAOJUAN QI. Unitok: unified tokenizer for visual generation and understanding. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 1 [41] Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. 3 [42] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1508615095, 2025. 7 [43] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. 6, [44] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 6, 7 [45] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. 3, 1 [46] OpenAI. Dalle 3. https://openai.com/index/ dall-e-3/, 2023. 6 [47] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 [48] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6 [35] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [50] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S. Khan. The Glamm: Pixel grounding large multimodal model. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4 [51] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. 3, 1 [52] Christoph Schuhmann. https : / / laion.ai/blog/laion-aesthetics/, 2022. Accessed: 2024 - 03 - 06. 4 Laion-aesthetics. [53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 6, 4 [54] Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. 1, 3 [55] Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. arXiv preprint arXiv:2508.02558, 2025. 3 [56] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. 6, 4 [57] Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar arXiv preprint inference via discrete diffusion forcing. arXiv:2508.09192, 2025. 3, 6 [58] XuDong Wang, Shaolun Zhang, Shufan Li, Kehan Li, Konstantinos Kallidromitis, Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Segllm: Multi-round reasoning segmentation with large language models. In The Thirteenth International Conference on Learning Representations, 2025. 5 [59] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 6, 7, 4 [60] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. 3, Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 7 [62] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 7 [63] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. VILA-u: unified foundation model integrating visual understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. 1 [64] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 7 [65] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 6 [66] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. 1, 3, 6 [67] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. 3 [68] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 6, [69] Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. 1 [70] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 7 [71] Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. 1, 3 [72] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 6, 7 [61] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie [73] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction11 guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 7 [74] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 6, 7 [75] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 7 [76] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1 [77] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 12 Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Technical Details 7.1. Formulation of Discrete Diffusion Models In this section, we include an overview of the standard formulation of Masked Diffusion Models (MDMs) that are widely adopted by literature [32, 33, 38, 51, 69]. Given sequence X0 consisting of discrete tokens [X 1 0 ], where is the sequence length, the forward process q(XtXs) gradually mask the sequence and convert clean tokens to special mask token [M ] over the continuous time interval [0, 1], with 1 0. At = 1, the sequence X1 = [X 1 1 ] consists of only masked token. This forward process is formally defined as 0 , . . . , 1 , . . . , 0 , 2 1 , 2 (cid:40) q(X X s) = Cat(X Cat(X ; M), ; 1t 1s Xi + ts 1s M), if if = [M ] = [M ], (2) where Cat() denotes categorical distribution, and M, Xi RV are probability vectors, and is the vocabulary 0, Xi size. In particular, is one-hot vector corresponding to the special token [M ]. This forward process yields the following marginal distribution: Prior works [51] show that the posterior of the reverse process p(XsXt, X0) has the following form: q(X X 0) = Cat(X ; (1 t)Xi 0 + tM). p(X sX , 0) = (cid:40) Cat(X Cat(X s; Xi t), s; ts Xi 0 + M), if if = [M ] = [M ]. (3) (4) At inference, X0 is not known, so we replace Xi 0 with the neural network prediction pθ(X 0Xt), which gives the following emprical sampling process: (cid:40) pθ(X sXt) = Cat(X Cat(X s; Xi t), s; ts pθ(X Sampling process. To sample sequence of clean tokens X0, we start with fully masked sequence X1, where 1 1 = = 1 = [M ]. We discretize the continuous time interval [0, 1] into discrete timesteps 0 = t0 < t1 < < tK = 1, and iteratively sample Xtk1 pθ(Xtk1 Xtk ) using Equation 5. We start with = and end when we obtain maskfree sequence X0. At each step, we assume pθ(Xtk1 Xtk ) factorizes as (cid:81)L Xtk ), following previous works [38, 45, 51]. = [M ] = [M ]. i=1 pθ(X 0Xt) + if if M), tk1 (5) Training process. At each training step, given clean sequence X0, we sample random timestep [0, 1] and obtain Xt q(XtX0) through the forward process defined in Equation 3. This gives us partially masked sequence. The loss is then computed using Equation 1 from Section 2.1. We highlight that Sparse-LaViDa does not fundamentally change these formulations. Rather, it proposes an equivalent but efficient parameterization by introducing sparse representation for the partially masked sequence Xt. 1 Figure 7. Sparse-Representation of Partially Masked Sequence. Given partially masked sequence, we can partition its tokens into two subsets and B, where consists of clean tokens and consists of masked tokens. The standard MDM (Left) need to pass all + tokens. By contrast, Sparse-LaViDa (Middle and Right) only need to pass + + tokens, where is subset of and is the number of register tokens (set to 3) in this case. 7.2. Register Tokens In this section, we discuss the detail implementation of register tokens. Given partially masked sequence Xt of length L, which can be partitioned into two subsets A, B, where consists of clean token positions and consists of masked positions. Let = {1, 2..L} be the set of all token positions, we have that = and = . In the standard parameterization, we need to pass in = + tokens, among which are masked tokens, to the model, even if we are only interested in obtaining the prediction at subset of masked position at the current diffusion step. The sparse parameterization introduces an extra set of register tokens = [R1..Rm]. Concretely, these are achieved by adding special tokens [reg] that is similar to mask token [M] to the vocabulary. All register tokens are represented with the same special token [reg] at the tokenization level, but their positional ids are + 1, + 2...L + respectively, leading to different rope embeddings and different beahvior in the attention process. In this setup, the total token count is + + m. When B, we can achieve considerable speedup from reduced sequence length. We visualize this design in Fig. 7. Sparse-LaViDa allows us to flexibly truncate masked tokens depending on which token prediction we want to obtain. When, = B, Sparse-LaViDa reduces to the standard MDM parameterization. Hence, Sparse-LaViDa is generalization to the standard MDM. For simplicity, we use simple partition A, to elaborate the design of register tokens and sparse representation, as opposed to more complex partition C1...Ck1 that depends on sampling steps, which are used in Sec. 3.2 of the main text and the following Sec. 7.3 of the appendix. 7.3. Step-Causal Attention Masks In this section, we provide detailed account of step-causal attention mask, with particular focus on how the design of step-causal attention mask aligns with the inference process with KV caching. Inference Time. Suppose we have prompt tokens and generate clean response with tokens through diffusion steps. We can naturally obtain k-partition of the response tokens C1..Ck depending on when they are unmasked during the diffusion steps. We also define C0 as the set of prompt tokens, which are never masked. Fig. 8a illustrates an example, which consists of = 3 prompt tokens P0...P2 and = 6 response tokens X0...X5. We also assume we have = 4 diffusion steps and there is only one register token R. As illustrated in the figure, sampling process #1 induces the partition C0 = {P0, P1, P2}, C1 = {X1, X3}, C2 = {X0}, C3 = {X2, X5}, C4 = {X4}, where tokens in C1, C2, C3, C4 are unmasked at the first, second, third, and fourth diffusion steps respectively. After token is generated at ith step, it is passed back to the model in (i + 1)th step and added to the cache after the model forward call. This is also illustrated in the figure (black dashed box). For example, tokens in C1 = {X1, X3} is added to the KV cache after step 2. At the ith diffusion step, the KV cache consist of all tokens in Cj where < 1. The input of the model consist of the previously generated tokens in Ci1, the set of masked tokens to be decoded at the current step Ci, and the register token R. 2 (a) Inference with Sparse Parameterization (b) Design of Step-Causal Attention Mask Figure 8. Connection between the Diffusion Sampling Process and the Step-Causal-Attention-Mask. (Left) Given prompt tokens P0...P2 and response tokens X0...X5, we partition the response tokens with C1...C4 and assign prompt tokens to C0. Both 0-1-2-3-4 and 0-1-2-4-3 are viable sampling order of tokens in 4-step diffusion process. We use to represent mask tokens and represent registers. (Right), for the two sampling process, we can simulate step 3 of both processes simultaneously via special attention mask. Since Ci1 are to be added to the KV cache, we prevent the queries of Ci1 to interact with keys and values of Ci and as discussed in Sec. 3.3. Taking step 3 as an example, the KV cache consists of tokens in C0, C1 (boxed), which are P0, P1, P2, X1, X3. The input consists of tokens in C2, which is just X0, and masked tokens in C3, denoted by M2, M5, as well as register token R. In this step, X0 from C2 cannot attention to M2, M5, R, (color-coded as gray), while M2, M5, can attend to all other tokens. Note that while the partition C0, C1...C4 derives from the sampling process #1, its decoding order 0 1 2 3 4 does not uniquely leads to this partition. For instance, sampling process #2 with decoding order 0 1 2 4 3 also has the same partition (Fig. 8a, Right). Moreover, we note that at step 3 of sampling process #2, the KV cache is exactly the same as sampling process #1. Among the input tokens, X0 from C2 has exactly the same behavior as sampling process #1 because of the attention mask. The only differences lies in the input of masked tokens. In sampling process #1, we have M2, M5 since at this step we want to unmask C3. In sampling process #2, we have M4 since at this step we want to unmask C4. This observation gives us the opportunity to parallelize the training by simulate sampling process #1 and sampling process #2 simultaneously through special attention mask. Training time. We can merge sampling process #1 and sampling process #2 by constructing the sequence P0, P1, P2, X1, X3, X0, X2, X5, R3, X5, R4, where R3, R4 are duplicates of the same register token R. We assign stepcausal attention mask for tokens P0, P1, P2, X1, X3, X0, as they have exactly the same attention pattern in both sampling steps. To accommodate the difference of masked token inputs where sampling process #1 has M2, M5, and sampling process #2 has M4, we concatenate the sequences to form M2, M5, R3, M4, R4 and design block-diagonal attention mask that ensures input tokens from one sampling process cannot attend to tokens from another same sampling process (bottomright section of Fig. 8b). We denote two copies of as R3, R4 since M2, M5 comes from partition C3 and M4 comes from partition C4. This design of attention mask is illustrated in Fig. 8b. The training-time attention mask is formally defined in Algorithm 1. In the above example, to simulate sampling process 0 1 2 3 4 and 0 1 2 4 3, we set the number of clean blocks = 2 (i.e. C0, C1) and the number of masked blocks = 2 (i.e. C3, C4). We assign the block number to tokens based on which of the partition C0...C4 they belong to. The duplicated register tokens R3, R4 are assigned with block numbers 3 and 4 respectively. Algorithm 1 Step-Causal Attention Mask Construction Require: Block assignments ZL+S, number of clean blocks , number of masked blocks 1: Initialize mask {0, 1}(L+S)(L+S) to zeros 2: for qi = 0 to + 1 do 3: 4: 5: qb A[qi] for kj = 0 to + 1 do kb A[kj] if qb = 0 then Block index of query token Block index of key token Prompt token: full attention to prompt if kb 0 then mask[qi, kj] 1 end if else if 1 qb then if kb qb then mask[qi, kj] 1 end if Clean token: attend up to its block else if + 1 qb + then Masked tokens: attend to prompt/clean tokens, or those in the same block if (kb ) (kb = qb) then mask[qi, kj] 1 end if 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end if end for 18: 19: 20: end for 21: return mask 8. Additional Experiment Details and Results 8.1. Data pipeline Our training dataset consist of the following tasks. A: Text-to-Image Pairs. We source data from LAION-2B [53] and COYO-700M [6]. We additionally include BLIP3o-60k [10], and ShareGPT4o-Image [9]. Each dataset is heavily filtered to remove NSFW prompts, low CLIP scores [49], low aesthetic scores [52], and low-resolution images following LaViDa-Os pipeline. We include all data from BLIP3o-60k and ShareGPT4o-Image, and select highest-quality samples from LAION and COYO based on CLIP scores and aesthetic scores. The final data mix consist of 20M images. Unlike LaViDa-O, we did not include SA-1B [25], JourneyDB [56] because of quality issues. Specifically, human faces in SA-1B are blurred, while JourneyDB images have many artifacts even after heavy filtering. B: Image-level Understanding Data. We include MAmmoth-VL [18], and VisualWebInstruct [22]. C: Region-level Understanding Data. We include GranD [50] and RefCOCO [23]. D: Image Editing Data. We include ShareGPT4o-Image [9], GPT-Edit-1.5M [59], and the image editing subset of UniWorld-V1 [20]. 8.2. Training Setup We perform SFT training on the LaViDa-O weights using our proposed sparse-parameterization and step-causal attention mask. The hyper-parameters largely followed the SFT stage of LaViDa-O. We document the details of training setup and relevant hyperparameters in Tab. 9. The main experiments is conducted with 64 A100 GPUs across 8 nodes. The training takes 5 days in total, which is 15% of the LaViDa-Os training budget (34.2 days from scratch). 8.3. Additional Results on Object Grounding As shown in Fig. 6, one of the advantages of Sparse-LaViDa is that it does not employ block-causal attention mask like block-diffusion, making it capable of tasks that requires bi-directional context such as inpainting, object grounding through infilling, and constrained text generation just like vanilla MDM. We report quantitative results on object grounding tasks in Table 10. Overall, Sparse-LaViDa achieves comparable performance as the LaViDa-O baseline. Since all coordinates are generated in single step, the two methods have identical 4 Table 9. Training configurations of Sparse-LaViDa. We report the relevant hyperparameters for training, including the learning rate, number of training steps, optimizer setup, image resolution for understanding and generation tasks. Learning Rate Steps β1 β2 optimizer SFT 2 105 100k 0.99 0.999 AdamW Loaded Parameters Trainable Parameters Und. resolution Gen. resolution 10.4B 10.4B 384 {(1, 3), (2, 2)} 1024 inference speed because Sparse-LaViDa cannot benefit from token truncation in this setup. Table 10. Precision@0.5 on RefCOCO, RefCOCO+, and RefCOCOg REC tasks. Model RefCOCO RefCOCO+ RefCOCOg SegLLM-7B[58] Qwen2.5-VL-7B [3] GroundingDINO [35] InternVL3-8B [77] LaViDa-O Sparse-LaViDa val 90.0 90.0 90.6 92.5 91.9 92.3 testA 92.1 92.5 93.2 94.6 94.6 94.9 testB 86.2 85.4 88.2 88.0 88.4 89. val 82.2 84.2 88.2 88.2 87.4 87.4 testA 85.5 89.1 89.0 92.5 91.7 92.5 testB 76.1 76.9 75.9 81.8 82.2 81. val 83.9 87.2 86.1 89.6 89.5 89.6 test 85.9 87.2 87.0 90.0 89.8 89.9 8.4. Additional Qualitative Comparisons In Figure 9, we provide side-by-side qualitative comparisons of Sparse-LaViDa and the LaViDa-O baseline on text-to-image generation and image editing tasks. As shown in the figure, Sparse-LaViDa achieves comparable generation quality while offering speedup of 1.95 on text-to-image generation and speedup of 2.83 on image editing. 9. Limitations Despite the promising results of Sparse-LaViDa, it has several limitations. First, the speedup only benefits long sequence generation such as text-to-image generation, image-editing, or visual math problem-solving with long reasoning chains. It does not improves the speed of short QA tasks or object grounding tasks, where the number of output tokens are very small. We emphasize that Sparse-LaViDa will benefit any tasks involving image generation, since an image is represented by 4096 VQ tokens. Second, our model inherits many limitations from the base model LaViDa-O, such as hallucination in responses. Further, we observe the same pixel shift issue discovered by LaViDa-O, where image editing results may have subtle changes in un-edited regions. We hope this issue will be addressed by future foundational models. Lastly, while we have demonstrated Sparse-LaViDa is an efficient post-training technique, in principle it should be general to all stages including pretraining. It would be interesting for researchers with more compute resources to apply this to different stages of training. 5 Figure 9. Qualitative Comparisons of Sparse-LaViDa and LaViDa-O baseline. We show qualitative results of text-to-image generation and image editing results of Sparse-LaViDa and LaViDa-O baseline.Sparse-LaViDa achieves speedup of 1.95 on text-to-image generation and speedup of 2.83 on image editing, while maintaining comparable visual quality"
        }
    ],
    "affiliations": [
        "Adobe",
        "UCLA"
    ]
}