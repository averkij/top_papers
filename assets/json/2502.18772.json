{
    "paper_title": "Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance",
    "authors": [
        "Xueqing Peng",
        "Triantafillos Papadopoulos",
        "Efstathia Soufleri",
        "Polydoros Giannouris",
        "Ruoyu Xiang",
        "Yan Wang",
        "Lingfei Qian",
        "Jimin Huang",
        "Qianqian Xie",
        "Sophia Ananiadou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite Greece's pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domain-specific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greek-specific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutus-ben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of cross-lingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 7 7 8 1 . 2 0 5 2 : r Plutus: Benchmarking Large Language Models in Low-Resource Greek Finance Xueqing Peng The Fin AI USA xueqing.peng2023@gmail.com Triantafillos Papadopoulos Athens University of Economics and Business, Archimedes/Athena RC Athens, Greece t.papadopoulos@athenarc.gr Polydoros Giannouris The University of Manchester Manchester, UK polydoros.giannouris@postgrad.manchester.ac.uk Lingfei Qian The Fin AI USA lfqian94@gmail.com Ruoyu Xiang The Fin AI USA xry0408@gamail.com Jimin Huang The Fin AI USA jimin.huang@thefin.ai Efstathia Soufleri Archimedes/Athena RC Athens, Greece e.soufleri@athenarc.gr Yan Wang The Fin AI USA wy2266336@gmail.com Qianqian Xie The Fin AI USA xqq.sincere@gmail.com Sophia Ananiadou The University of Manchester Manchester, UK Archimedes/Athena RC Athens, Greece sophia.ananiadou@manchester.ac.uk Abstract Despite Greeces pivotal role in the global economy, large language models (LLMs) remain underexplored for Greek financial context due to the linguistic complexity of Greek and the scarcity of domainspecific datasets. Previous efforts in multilingual financial natural language processing (NLP) have exposed considerable performance disparities, yet no dedicated Greek financial benchmarks or Greekspecific financial LLMs have been developed until now. To bridge this gap, we introduce Plutus-ben, the first Greek Financial Evaluation Benchmark, and Plutus-8B, the pioneering Greek Financial LLM, fine-tuned with Greek domain-specific data. Plutus-ben addresses five core financial NLP tasks in Greek: numeric and textual named entity recognition, question answering, abstractive summarization, and topic classification, thereby facilitating systematic and reproducible LLM assessments. To underpin these tasks, we present three novel, high-quality Greek financial datasets, thoroughly annotated by expert native Greek speakers, augmented by two existing resources. Our comprehensive evaluation of 22 LLMs on Plutusben reveals that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. These findings underscore the limitations of crosslingual transfer, the necessity for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We release Plutus-ben, Plutus-8B, and all associated datasets1 publicly to promote reproducible research and advance Greek financial NLP, fostering broader multilingual inclusivity in finance. CCS Concepts Applied computing Annotation; Computing methodologies Language resources. 1 https://huggingface.co/collections/TheFinAI/plutus-benchmarking-greek-financialllms-67bc718fb8d897c65f1e87db Keywords Financial Large Language Models, Financial Application, Benchmark, Greek, Low-resource Languages Figure 1: Radar graph of model performance on Plutus-ben, the first Greek financial benchmark. Plutus-8B achieves the best performance, surpassing GPT-4 by 15.38%, GPT-4o by 46.34%, and Deepseek-V3 by 93.55%."
        },
        {
            "title": "1\nAs an official language of the European Union2 and the dominant\nlanguage of Greece’s merchant navy, which controls over 20% of",
            "content": "2 https://european-union.europa.eu/principles-countries-history/languages_en the worlds merchant fleet3, Greek is central to international trade, banking, and regulatory affairs. Greek financial documents such as regulatory filings, maritime trade records, and economic reports hold substantial international relevance, yet their processing remains difficult [13]. Greeks complex morphology, inflectional system, and unique orthographic structures [12, 19] make it fundamentally different from high-resource financial languages such as English and Chinese. These linguistic complexities introduce challenges in financial information extraction, entity recognition, and numerical reasoning [33]. Despite recent advancements in applying large language models (LLMs) to financial natural language processing (NLP) tasks, Greek remains largely unexplored. Extensive financial LLMs have been developed for English [42, 44, 47, 49, 51], Chinese [7, 24], and Spanish [55]. Moreover, financial benchmarks have been established for English [34, 43, 44], as well as for Chinese [31], Spanish [55], and Japanese [18]. However, no dedicated benchmark exists for Greek, and while some multilingual evaluations include Greek [2], they lack financial-specific datasets, making it difficult to assess LLMs performance on Greek financial area. At the same time, Greek LLM research has largely overlooked finance. While Meltemi [38] is the first Modern Greek LLM, it lacks financial domain adaptation. Existing Greek datasets focus on general NLP tasks [8, 26, 54], failing to capture the domain-specific terminology and numerical reasoning essential for financial applications. In this work, we introduce Plutus-ben, the first Greek financial evaluation benchmark and Plutus-8B, the pioneering Greek financial LLM. Plutus-ben addresses the aforementioned gap by defining five core financial NLP tasks in Greek, including numeric and textual named entity recognition (NER), question answering (QA), abstractive summarization, and topic classification, establishing foundation for systematic and reproducible assessments of LLMs in Greek financial area. Notably, several of these tasks, such as financial numeric NER and financial QA, are introduced for the first time in Greek, enabling more comprehensive evaluation of models ability to extract, comprehend, and reason over Greek financial texts. To support these tasks, we develop three high-quality Greek financial datasets, including GRFinNUM, GRFinNER, and GRFinQA, each carefully annotated by expert native Greek speakers with deep financial and linguistic expertise. Our annotation process follows strict, standardized guidelines, ensuring consistency, accuracy, and high inter-annotator agreement. Annotators meticulously label complex financial entities and structured summaries, capturing the nuanced language of Greek financial discourse. These newly developed datasets are curated from authoritative financial sources, including Greek financial reports and university exams, and are further supplemented by two existing financial resources, GRFNS-2023 and GRMultiFin. Beyond benchmarking, to access the influence of fine-tuning on Greek financial data on enhancing model performance,we also develop Plutus-8B, the first Greek financial LLM fine-tuned on Greek domain-specific data to bridge the gap between existing models and Greek financial tasks. 3 https://ugs.gr/en/greek-shipping-and-economy/greek-shipping-and-economy-2024/ the-international-perspective/ Xueqing et al. In our evaluation of 22 representative LLMsincluding both English-centric and Greek models across general and financial domains in various sizes, as well as our Plutus-8Bwe reveal fundamental limitations in LLM performance on Greek financial tasks. Despite their success in high-resource languages, even top-tier models like GPT-4o struggle with Greek financial text, while smaller opensource models like LLaMA-3.2-1B, Qwen2.5-1.5B, and Mistral-7B fail entirely on key tasks such as NER. The challenge goes beyond language, financial text introduces specialized terminology, numerical reasoning, and ambiguous context, making adaptation even harder. English-trained financial models fail to generalize to Greek financial tasks, and Greek-focused models like Meltemi-7B, despite excelling in general linguistic tasks, lack the financial expertise needed for robust performance. Scaling model size provides some improvement but quickly reaches diminishing returns, as seen in Qwen2.5-72B failing to outperform Qwen2.5-32B, proving that scaling alone is not the answer. Our fine-tuned model, Plutus-8B, achieves the highest mean score, showing that training on Greek financial data significantly boosts performance. However, challenges remain, particularly in summarization, where all models including our Plutus-8B struggle with long-form financial documents. Our main contributions are as follows: We introduce Plutus-ben, the first comprehensive Greek financial evaluation benchmark covering five essential financial NLP tasks, alongside Plutus-instruction, the inaugural Greek financial instruction fine-tuning dataset, and Plutus8B, the first Greek financial LLM that achieves state-of-theart (SOTA) performance on the Plutus-ben benchmark. We develop four new high-quality Greek financial datasets, meticulously annotated by expert native Greek speakers, and enhance these with two existing resources to improve coverage and utility. We conduct comprehensive evaluation of 22 LLMs on Plutus-ben, revealing that Greek financial NLP remains challenging due to linguistic complexity, domain-specific terminology, and financial reasoning gaps. Our results highlight the limitations of cross-lingual transfer, the need for financial expertise in Greek-trained models, and the challenges of adapting financial LLMs to Greek text. We publicly release Plutus-ben, Plutus-8B, and all associated datasets to drive reproducible research and advance Greek financial NLP, fostering greater multilingual inclusivity in finance."
        },
        {
            "title": "2 Related Work\n2.1 Financial and Greek LLMs\nIn recent years, an increasing number of LLMs have been tailored to\nfinancial applications. Most existing work is English-centric, such as\nFinLLaMA [47], BloombergGPT [42], PIXIU [44], InvestLM [51],\nand FinGPT [49], leveraging domain-specific financial corpora for\ntasks. In parallel, recent research in Chinese (DISC-FinLLM [7]\nand CFGPT [24] and bilingual financial LLMs (FinMA-ES [55] for\nSpanish and English) extend these efforts by covering related non-\nEnglish and bilingual finance tasks. Despite these notable advance-\nments, there is a conspicuous absence of specialized Greek financial\nLLMs. Existing Greek open-source LLMs, such as Meltemi [38]",
            "content": "Plutus and Llama-Krikri4, do not include finance-oriented training data, which highlights the critical need for developing financial model specifically tailored to the Greek context."
        },
        {
            "title": "Benchmark",
            "content": "In this section, we introduce Plutus-ben, the first Greek financial evaluation benchmark. As shown in Table 1, Plutus-ben encompasses wide range of tasks, including numeric NER, textual NER, question answering, abstractive summarization, as well as topic classification, enabling comprehensive evaluation of models. To support these tasks, we developed three new high-quality Greek financial datasets from scratch, including GRFinNUM, GRFinNER, and GRFinQA. Additionally, we use two established resources, GRFNS-2023 and GRMultiFin, with examples provided in Table 5 (see Appendix for more details). As shown in Table 1, GRFinNUM and GRFinNER each consist of 500 samples, while GRFinQA contains 540 questionanswer pairs. GRFNS-2023 and GRMultiFin include 262 and 268 data samples, respectively. All these datasets are sourced from realworld financial documents, such as annual reports, exam questions, and article headlines. Various evaluation metrics are employed in these benchmarks, including Entity F1, Accuracy (Acc), and Rouge1 score [25], to assess LLMs performance across multiple dimensions: topical content categorization, long-form financial document comprehension, language understanding and reasoning, and both textual and numerical information extraction. These datasets were rigorously annotated by expert native Greek speakers with deep financial and linguistic expertise, following standardized guidelines to ensure consistency and accuracy."
        },
        {
            "title": "3.1 Task Definition and Dataset Curation\n3.1.1 Numeric NER. Numerals are crucial in financial narra-\ntives, conveying essential quantitative information and actionable\ninsights [4]. Accurate numeral recognition is vital for interpreting nu-\nanced financial data, especially when various categories exist simul-\ntaneously, i.e, monetary values, timestamps, and quantities [6, 50].\nTask Definition: We introduced the first Greek financial numeric\nNER task, involving both number span identification and classi-\nfication into fine-grained numeral types. Inspired by the English\nnumeric NER framework FinNum [5], we approach this task as",
            "content": "a sequence labeling problem. Our task processes the input sentence 𝑋 = (𝑥1, 𝑥2, . . . , 𝑥𝑛) consisting of 𝑛 tokens 𝑥𝑖 to the output labels 𝑌 = (𝑦1, 𝑦2, . . . , 𝑦𝑛) consisting of 𝑛 labels 𝑦𝑖 . The goal is to assign each token 𝑥𝑖 label 𝑦𝑖 from the predefined set = {MONETARY, PERCENTAGE, TEMPORAL, QUANTITY, OTHERS, 𝑂 } , which includes specific numeric entity types and the outside label 𝑂. Among these categories, MONETARY includes financial amounts, such as prices, quotes, and changes, which are central to financial analysis. PERCENTAGE denotes ratios or relative changes, crucial for trend and growth tracking. TEMPORAL covers dates, times, and durations, integral to time-series analysis. QUANTITY captures measurable or countable values, such as inventory levels or investment positions. OTHERS encompasses numeric data not captured by the previous categories, leaving room for future exploration. Data Source: To create our novel high-quality GRFinNUM dataset, we collected real-world, publicly available financial annual reports from Greek firms listed on the Athens Stock Exchange 5. These reports include textual information and reviews provided by the firms management and board of directors, offering rich, detailed financial data and narratives. We curated dataset of 64 financial reports, each spanning 30 to 267 pages, with an average length of 105 pages or approximately 44,000 words per document. Due to their extensive length and inclusion of non-essential content, we meticulously filtered the text to extract sentences containing target entities. This rigorous selection process yielded refined dataset of 500 sentences, ensuring relevance and quality for fine-grained numeral classification. Expert Annotation: Rigorous annotation guideline (Appendix D) was developed for GRFinNUM, comprising both general rules for the overall task and specific rules tailored to each numeral category. These guidelines were iteratively refined through multiple rounds of pre-annotation and collaborative discussions, focusing on resolving ambiguous cases to ensure high consistency and accuracy across the dataset. To minimize annotator variability, only numbers, decimal points (.), and the percent sign (%) were included in annotated spans. To construct novel high-quality dataset, we enlisted three highly educated Greek native speakers with expertise in economics, business, and informatics from leading academic institutions (Appendix H). The annotation process was conducted using Label Studio platform [37] (Appendix I), ensuring streamlined and reproducible workflow. Quality Validation: To gauge the quality and reliability of our GRFinNUM annotation process, we utilized three key inter-annotator agreement metrics: F1 score [16], Cohens Kappa [41], and Krippendorffs Alpha [17] (Appendix L). F1 Score evaluated annotator consistency in span identification and classification. Cohens Kappa adjusted for random agreement, while Krippendorffs Alpha addressed category distribution imbalances. The results demonstrated excellent inter-annotator agreement for the GRFinNUM dataset, with an F1 score of 0.988, Cohens Kappa of 0.979, and Krippendorffs Alpha of 0.978  (Table 2)  . These high scores confirm the robustness and quality of our GRFinNUM dataset. 4https://huggingface.co/ilsp/Llama-Krikri-8B-Base 5https://www.athexgroup.gr/el/web/guest/financial-statements-in-pdf-format Table 1: Overview of the Plutus-ben benchmark. For each task, both raw data volume and processed size are listed, along with dataset source, split sizes for train/validation/test, evaluation metrics, licenses, and tested capabilities. Task Dataset Raw Processed Source Train Valid Test Metrics License Tested Capabilities Xueqing et al. Numeric NER Textual NER Question Answering Abstractive Summarization [53] Topic Classification [22] 1 https://www.athexgroup.gr/web/guest/company-fin.-statements/ 2 https://www.athexgroup.gr/web/guest/company-fin.-statements/ GRFinNUM GRFinNER GRFinQA GRFNS-2023 [53] GRMultiFin [22] 64 64 540 262 268 500 500 540 262 268 Annual Reports1 Annual Reports2 Exam Questions Annual Reports Article Headlines 320 320 267 169 171 80 80 48 43 43 100 100 225 50 54 Entity F1 Entity F1 Acc Rouge-1 Acc Public Public Public CC-BY-4.0 CC BY-NC 4.0 Numeric information extraction Textual information extraction Language comprehension and reasoning Long-form financial document comprehension Language comprehension and topical content categorizing Table 2: Inter-annotator agreement metrics for human expert annotations on GRFinNUM and GRFinNER datasets. Dataset F1-score Cohens Kappa Krippendorffs alpha GRFinNUM GRFinNER 0. 0.974 0.979 0.993 0.978 0.948 3.1.2 Textual NER. Identifying core financial entities, such as companies, is crucial for extracting meaningful insights from financial activities in the Greek financial domain. Unlike numeric NER, which focuses on recognizing numerical values, textual NER in Greek presents unique challenges due to the languages distinct expression patterns. For instance, long-form names with attribution, such as George Demetriou of Konstantinos, should be treated as single entity span. Task Definition: To test LLMs understanding of Greek financial entities, we introduce the first Greek financial textual NER task. Inspired by FinNER-ORD [35] and Farmakiotou et al. [14], our task involves span identification and classification of company-related information into three key entity types: Person, Location, and Organization. Our task processes the input sentence 𝑋 = (𝑥1, 𝑥2, . . . , 𝑥𝑛) consisting of 𝑛 tokens 𝑥𝑖 to the output labels 𝑌 = (𝑦1, 𝑦2, . . . , 𝑦𝑛) consisting of 𝑛 labels 𝑦𝑖 . The goal is to assign each token 𝑥𝑖 label 𝑦𝑖 from the predefined set = {PERSON, LOCATION, ORGANIZATION, 𝑂 }, which includes specific textual entity types and the outside label 𝑂. Data Source: We constructed the GRFinNER dataset using the same set of financial annual reports from Greek firms as in GRFinNUM. total of 64 reports were collected. Similar sentences filtering is utilized for different final dataset of 500 sentences with high relevance and quality for company-related entity classification. Expert Annotation: Rigorous annotation guideline (Appendix E) was also iteratively developed for GRFinNER through multiple rounds of pre-annotation and collaborative discussions, consisting of general rules for the entire task, specific rules for each entity category, and distinct rules for handling ambiguous situations. The same three highly educated Greek native speakers (Appendix H) completed the annotation process. The entire annotation workflow was carried out using Label Studio platform (Appendix I). Quality Validation: The inter-annotator agreement was meticulously assessed using the same rigorous framework: F1 score [16], Cohens Kappa [41], and Krippendorffs Alpha [17] (Appendix L). The GRFinNER task exhibited exceptional inter-annotator reliability, achieving an F1 score of 0.974, Cohens Kappa of 0.993, and Krippendorffs Alpha of 0.948  (Table 2)  , ensuring the datasets quality for application. 3.1.3 Question Answering. Effective financial decision-making and question answering require LLMs to comprehend and reason within financial contexts. The nuances of Greek financial terminology, combined with the complex morphology of the Greek language, pose unique challenges that demand rigorous assessment. Task Definition: To evaluate LLMs comprehension and reasoning capabilities in Greek financial contexts, we introduce the first Greek financial question-answering task. This task requires models to infer the correct answer using provided text under multiplechoice format, testing their ability to process financial terminology, apply reasoning, and understand contextual nuances in Greek. Each question, along with its answer choices, is given as input, with the correct answer designated as the output. Our task processes the input question 𝑄 = (𝑞1, 𝑞2, . . . , 𝑞𝑛) consisting of 𝑛 tokens 𝑞𝑖 and the possible choices = {𝑐1, 𝑐2, . . . , 𝑐𝑘 } which is the set of 𝑘 possible choices 𝑐𝑖 . The task aims to map the question 𝑄 and choices to the correct answer 𝐴, selected from C. Data Source: We propose the novel GRFinQA dataset which is the first in the Greek financial domain. It is comprised of 540 multiple-choice financial exam or revision questions sourced from Greek university courses and publicly available Greek finance, business and economics textbooks. We collected the PDF files, and extracted the text that each question was grouped with its appropriate choices and the correct choice. Quality Validation: To ensure the quality of the dataset, we first identified three distinct types of questions present in the QA dataset: (1) right and wrong questions, which require binary judgment on whether statement is correct or incorrect; (2) fill-in-the-gap questions, where missing word or phrase must be completed based on contextual understanding; and (3) generic multiple-choice questions, which present several answer options, with only one being correct. From this dataset, we selected representative sample that included several questions from each category. The domain experts manually reviewed these questions to confirm that the designated correct answer was factually accurate. Following that, we used GPT-4o to process the questions, prompting it to read the text and explain its reasoning for selecting an answer. This approach helped us verify both the factual accuracy of the datasets answers and the difficulty of the selected questions. 3.1.4 Abstractive Summarization. The task of abstractive summarization originates from the Financial Narrative Summarization Shared Task (FNS 2023), which focuses on summarizing annual Plutus reports from the UK, Greece, and Spain [53]. This task aims to test LLMs abilities in understanding and reorganizing the given context. The challenge lies in condensing essential information while preserving factual accuracy and coherence. The structural and linguistic complexities of Greek financial texts further heighten this difficulty, requiring models to generate fluent, paraphrased summaries that remain faithful to the original content. Task Definition: To evaluate LLMs abilities of understanding the Greek financial contexts, we adopt the abstractive summarization task from FNS 2023 [53]. This task involves generating concise summaries of Greek financial annual reports, emphasizing both informativeness and readability while preserving key details. The task processes the input document 𝐷 = (𝑑1, 𝑑2, . . . , 𝑑𝑛) consisting of 𝑛 tokens 𝑑𝑖 to the abstractive summary 𝑆 = (𝑠1, 𝑠2, . . . , 𝑠𝑚) consisting of 𝑚 tokens 𝑠𝑖 . The goal is to map the document 𝐷 to concise summary 𝑆 that conveys the essential information in natural language, which is paraphrased or restructured rather than directly copied from 𝐷. Data Source: The FNS 2023 shared task [53] comprises UK, Greek, and Spanish financial annual reports. The dataset includes narrative sections from finanical annual reports, each paired with both short and long gold summary. For GRFNS-2023, we focus solely on the Greek portion, using the short gold summary as our target. As the original authors did not release test set, we repurposed their validation set as our test set and split the training data to create our training and validation sets. 3.1.5 Topic Classification. The topic classification task is derived from MultiFin [22], and it focuses on categorizing financial news headlines into predefined financial topics. This task is particularly challenging due to the brevity and ambiguity characteristic of financial news headlines. Furthermore, financial categories often exhibit thematic and lexical overlaps, demanding that models discern the appropriate category from limited context and shared terminology. Task Definition: To improve LLMs comprehension of Greek financial topics, we incorporated the Greek financial topic classification task adapted from MultiFin [22]. This task requires assigning financial article headlines to one of six predefined thematic categories. The objective is to evaluate models proficiency in distinguishing between overlapping topics and extracting significant insights from brief and ambiguous texts. Our task processes the input document 𝐷 = (𝑑1, 𝑑2, . . . , 𝑑𝑛) consisting of 𝑛 tokens 𝑑𝑖 and the possible top- , . . . , Topic𝑘 } which is the set of 𝑘 possible ics = {Topic1 topics. The goal is to map the input document 𝐷 to the correct topic 𝑇 from C, based on the content of 𝐷. , Topic2 Data Source: The dataset utilized for this task is the MultiFin dataset [22]. It comprises 10,048 financial article headlines in 15 languages, each reflecting diverse language families and writing systems. These headlines are categorized into one of six classes: Business & Management, Tax & Accounting, Finance, Technology, Government & Controls, and Industry. For our specific analysis, we extracted the Greek subset to create the GRMultiFin dataset. LLM, we converted our raw datasets into structured instruction datasets. Task-specific prompts were thoughtfully crafted by Greek domain experts, as shown in Table 66. Each prompt adheres to the standardized template as outlined below: Task Instruction {Task Specific Instruction} Text: {Input} Answer: {Output} In this template, task specific instruction refers to the unique prompt designed for each task. The Input denotes the input financial data from each dataset, such as Greek annual report, while Output represents the corresponding output for the input text, such as summary of the Greek annual report."
        },
        {
            "title": "3.3 Evaluation\nWe partitioned our dataset into training, validation, and test subsets,\nas detailed in Table 1. To comprehensively assess model perfor-\nmance, we conducted both automated metrics and human evalua-\ntions.",
            "content": "3.3.1 Automatic Evaluation. We adopt the same metrics following previous studies in financial NLP tasks [43, 55]. The Entity F1 score [9] is applied to numeric and textual NER tasks due to its balance of precision and recall, crucial for accurate entity identification. Accuracy (Acc) [29] is used for QA and topic classification tasks as it straightforwardly measures the correctness of predictions. Rouge-1 [25] is employed for abstractive and extractive summarization tasks to assess the overlap in content between gold-standard and generated summaries focusing on unigram comparison. 3.3.2 Human Evaluation. Beyond automated metrics, we implement human evaluation to rigorously assess the quality of outputs from LLMs. This evaluation specifically concentrates on abstractive summarization task. We selected four representative models, including GPT-4, FinLLaMA-8B, Meltemi-7B, and Plutus-8B. Expert native Greek speakers with deep financial and linguistic expertise7 compare the model-generated summaries against gold standard summaries following rigorous, standardized annotation guideline8 using Label Studio platform9. The evaluation focuses on three critical dimensions: (1) Language Appropriate Fluency (Fluency): This dimension assesses the readability and naturalness of the summaries, emphasizing grammatical correctness, lexical accuracy, absence of repetition, and the use of domain-specific terminology, all within the context of Greeks linguistic intricacies. (2) Coherence: We examine the logical progression and structural consistency of the summaries, vital for maintaining integrity in financial narratives. (3) Factuality: This dimension verifies the factual accuracy of summaries against the original financial content, ensuring reliability and trustworthiness."
        },
        {
            "title": "3.2\nTo optimize task-specific performance, facilitate effective bench-\nmarking, and support instruction fine-tuning for the Greek financial",
            "content": "6More details in Appendix 7More details in Appendix 8More details in Appendix 9More details in Appendix I"
        },
        {
            "title": "3.4 Model Evaluation\nWe conduct a comprehensive evaluation of 22 prominent LLMs\nencompassing:",
            "content": "Proprietary Models: close source APIs, including GPT-3.5Turbo [3], GPT-4o-Mini [32], GPT-4o [20], and GPT-4 [1]. Open-source General Small Models: publicly available models with less than 10B parameters, including Mistral7B [30], LLaMA-3.2-1B [11], LLaMA-3-8B [11], LLaMA3.1-8B [11], Qwen2.5-1.5B [48], Qwen2.5-7B [48], Gemma2-2B [36], and Gemma-2-9B [36]. Open-source General Large Models: publicly available models with more than 20B parameters, including DeepseekV3 [27], LLaMA-3-70B [11], Qwen2.5-32B [48], and Qwen 2.5-72B [48], and Gemma-2-27B [36]. English Financial Models: publicly available models continual trained with English financial corpus, including Finma7B [45] and FinLLaMA-8B [46]. Greek General Models: publicly available models continual trained with Greek general corpus, including Meltemi7B [39] and Llama-Krikri-8B10. Notably, LLaMA-3-8B, Mistral-7B, and LLaMA-3.1-8b serve as the core foundational models for FinLLaMA-8B, Meltemi-7B, and Llama-Krikri-8B, respectively. 11. For evaluation integrity, we develop our own benchmark suites based on LM Evaluation Harness [15]. Models such as GPT and DeepSeek, are interfaced via their own APIs. In-house evaluation of open-source models is conducted using cluster of four A100 GPUs, each equipped with 80GB memory. We standardize the maximum generation token length to 8192 tokens for abstractive summarization and 1024 tokens for other tasks."
        },
        {
            "title": "4 Plutus-8B: the First Greek Financial LLM\nTo investigate the impact of fine-tuning on Greek financial data on\nenhancing model performance across various tasks, and to deter-\nmine its effectiveness in addressing the challenges posed by low-\nresource language conditions and domain-specific complexities, we\ndeveloped Plutus-instruction, the first instruction dataset tailored\nto the Greek financial domain. As shown in Table 1, we adopted\nGRFinNUM, GRFinNER, GRFNS-2023, and GRMultiFin. Specifi-\ncally, the GRFinQA dataset is withheld to evaluate the generalization\nperformance of the trained model.",
            "content": "Based on the instruction dataset, we selected Llama-Krikri-8BInstruct for further instruction-tuning, as this model performs best on the benchmark compared to other models of similar size. This is due to its training on extensive Greek texts, as well as its inclusion of code and mathematical data to enhance its mathematical reasoning abilities. To efficiently adapt the model parameters, we employ LowRank Adaptation (LoRA) [10] with rank of 𝑟 = 16, scaling factor of 𝛼 = 32, and no dropout. We applied int4 quantization to reduce memory overhead while preserving model expressiveness. Finetuning is conducted with block size of 4,096 tokens, while allowing sequences to extend to 42k tokens to accommodate the complex structure and extensive length of financial and legal documents. To 10https://huggingface.co/ilsp/Llama-Krikri-8B-Base 11More details in Appendix Xueqing et al. Table 3: LLM performance on the Plutus-ben benchmark, evaluated across multiple Greek financial NLP tasks. Bold values denote the highest scores, while underlined values indicate the second-highest scores in each column. Model GRFinNUM GRFinNER GRFinQA GRFNS-2023 GRMultiFin Mean Entity F1 Entity F1 Acc Rouge-1 Open-source Small Models LLaMA-3.2-1B LLaMA-3-8b LLaMA-3.1-8b Qwen2.5-1.5B Qwen2.5-7B Gemma-2-2B Gemma-2-9B Mistral-7B Deepseek-V3 LLaMA-3-70B Qwen2.5-32B Qwen2.5-72B Gemma-2-27B GPT-3.5-Turbo GPT-4o-Mini GPT-4o GPT-4 Finma-7B FinLLaMA-8B Meltemi-7B Llama-Krikri-8B Plutus-8B 0.00 0.00 0.10 0.00 0.00 0.00 0.02 0. 0.07 0.05 0.37 0.32 0.18 0.14 0.25 0.09 0.28 0.00 0.00 0.12 0.19 0.70 0.00 0.13 0.21 0.00 0.13 0.16 0.05 0. 0.29 0.33 0.40 0.36 0.43 0.22 0.31 0.30 Open-source Large Models 0.00 0.45 0.55 0.39 0.18 0.50 0.60 0.60 0.74 0.25 Proprietary Models 0.30 0.30 0.31 0. 0.51 0.12 0.78 0.71 English Financial Models 0.00 0.00 0.25 0.28 Greek General Models 0.50 0.45 0.48 0. Greek Financial Models 0.57 0.64 0.14 0.07 0.20 0.02 0.07 0.03 0.06 0.14 0.38 0.08 0.10 0.04 0.09 0.31 0.36 0.26 0.38 0.11 0. 0.19 0.22 0.34 Acc 0.39 0.70 0.54 0.31 0.54 0.41 0.61 0.39 0.61 0.61 0.70 0.72 0.61 0.50 0.59 0.59 0. 0.35 0.38 0.43 0.39 0.16 0.25 0.29 0.14 0.23 0.16 0.21 0.17 0.31 0.36 0.47 0.44 0.26 0.35 0.32 0.41 0.52 0.14 0. 0.34 0.36 0.72 0.60 ensure better optimization, we leveraged the AdamW optimizer [28] with learning rate of 5𝑒 4 and cosine learning rate schedule over 3 epochs. Additionally, we use gradient accumulation with step size of 4 to mitigate the constraints of batch size 1, leveraging mixedprecision training with bf16 for improved numerical stability. We further evaluate our model in Plutus-ben and compare it with all evaluated models12."
        },
        {
            "title": "5.1 Main Results\nTable 313 and Figure 1 summarize the performance of various LLMs\non our Greek-oriented financial benchmark, Plutus-ben. As shown in\nthe table, the scarcity of high-quality Greek linguistic data poses a\nfundamental challenge for current language models, particularly\nin capturing the rich morphological and syntactic complexities\nof Greek financial text. For example, open-source small models\nsuch as LLaMA-3.2-1B, Qwen2.5-1.5B, and Mistral-7B perform",
            "content": "12For demo, please see Appendix B. https://huggingface.co/spaces/TheFinAI/plutus8B-instruct 13Ranked results are visualized on our leaderboard. For more details, refer to Appendix A. https://huggingface.co/spaces/TheFinAI/open_greek_finance_llm_leaderboard Plutus poorly across all tasks, with near-zero scores on numeric and textual NER. Even open-source large models like LLaMA-3-70B and Gemma-2-27B show limited improvement, particularly struggling with numerical comprehension. Proprietary models, while generally performing better, still exhibit relative low performance on Greek financial tasks, with GPT-4 achieving the highest mean score of 0.52 but failing to maintain the same level of accuracy as in English [43, 44]. These results highlight the fundamental issue that models trained predominantly on high-resource languages fail to capture the linguistic complexity of Greek, including its rich morphology and inflectional structures, resulting in steep decline in performance. NER tasks, particularly GRFinNER, require an understanding of inflected Greek word forms and domain-specific abbreviations, which smaller models completely fail to capture. Larger models, though slightly better at general linguistic tasks such as GRFinQA, still underperform in recognizing financial entities and processing numerical values, underscoring the impact of Greeks low-resource status. Beyond the constraints of low-resource language training, financial texts introduce additional complexity, featuring highly specialized terminology, intricate numerical expressions, and ambiguous context-dependent constructions that general-purpose models fail to capture. English financial models like Finma-7B and FinLLaMA-8B perform poorly on Greek tasks, each registering mean score of only 0.14 and showing no success in NER tasks. This reflects the difficulty of transferring financial expertise developed from high-resource English data to the Greek context. Even proprietary models like GPT-4o, despite achieving the higher GRFinNER score (0.31), also face challenges with Greek financial numeric comprehension, as indicated by its low GRFinNUM score of 0.09. This reflects the difficulties in disambiguating financial terminology and numerical patterns specific to Greek texts. In contrast, Greek general-purpose models like Meltemi-7B and LlamaKrikri-8B show better adaptability to Greek linguistic structures. Meltemi-7B achieved mean score of 0.34, which is significantly higher than its backbone model, Mistral-7B (0.17). Similarly, LlamaKrikri-8B achieved mean score of 0.36, surpassing its backbone model, LLaMA-3.1-8b (0.29). Despite these strengths, both models underperform in financial numeric tasks, with scores of 0.12 (Meltemi-7B) and 0.19 (Llama-Krikri-8B) on GRFinNUM, despite their robust scores of 0.50 (Meltemi-7B) and 0.45 (Llama-Krikri8B) on GRFinNER. The stark contrast between the relatively strong GRFinNER performance of Greek general models and their weak GRFinNUM scores highlights that while linguistic adaptation helps with textual entity recognition, it is insufficient for financial contexts. Larger models generally perform better, but scaling alone does not consistently translate to superior results for Greek financial tasks, highlighting the need for specialized adaptation. Open-source large models such as Qwen2.5-32B and Qwen2.572B show substantial improvement over their smaller counterparts, particularly in GRFinNUM (0.37 and 0.32, respectively) and GRMultiFin (0.70 and 0.72). However, the diminishing returns seen in Qwen2.5-72B, which underperforms Qwen2.5-32B on multiple tasks, indicate that increased model capacity alone is insufficient. Similarly, LLaMA-3-70B achieves higher GRFinNER score (0.45) than smaller models but still struggles with numeric comprehension (GRFinNUM = 0.05). Proprietary models also follow this trend, Table 4: Human evaluation results assessing fluency, coherence, and factuality of representative LLMs, evaluated on the GRFNS2023 dataset within the Plutus-ben benchmark. Domain Model Fluency Coherency Factuality English general model English financial model Greek general model Greek financial model GPT-4 FinLLaMA-8B Meltemi-7B Plutus-8B 4.97 2.09 3.99 3. 4.33 1.48 1.49 3.51 3.06 1.54 1.60 2.93 with GPT-4o (mean 0.41) showing only marginal improvements over GPT-3.5-Turbo (0.35), despite their increased scale in training data. This reinforces that scaling provides only limited gains without explicit training on Greek financial data. While larger models display improvements in entity recognition and question answering, they fail to achieve comparable performance across all tasks, particularly those requiring complex numerical reasoning and financial domain knowledge. This suggests that larger models, despite having greater representational power, remain constrained by their pre-training data and struggle to bridge the gap between financial reasoning and Greek language structures without additional adaptation. Finally, fine-tuning on dedicated Greek financial corpus significantly enhances model performance but also reveals explicit bottlenecks that require further improvements. Our model, Plutus-8B, fine-tuned exclusively on Greek financial data, achieves the highest mean score of 0.60, outperforming all baseline models. Plutus-8B excels in numeric reasoning, achieving GRFinNUM score of 0.70, significantly surpassing all other models, including GPT-4 and Qwen2.5-32B. It also demonstrates strong results in GRFinNER and GRMultiFin, showing that fine-tuning allows better adaptation to Greek-specific entity extraction and financial classification. For the GRFinQA dataset, which was held out from our instruction fine-tuning, Plutus-8B achieved moderate score of 0.64, an improvement over Meltemi-7B (0.48) and Llama-Krikri-8B (0.57), demonstrating effective generalization ability from Greek financial instruction fine-tuning. However, its performance on GRFNS-2023 indicates that summarization remains challenge, due to the difficulty in modeling long-range contextual dependencies within financial documents. These results validate the importance of fine-tuning on Greek financial data for domain-specific improvements, particularly in tasks requiring numeric reasoning and entity recognition. Plutus-8B superior performance in GRFinNUM suggests that direct exposure to Greek financial numerical structures enables better performance in numeric entity extraction, task where general-purpose and proprietary models falter. However, the modest gains in summarization tasks highlight persistent challenges in long-form financial document comprehension, where models must understand nuanced contextual dependencies. Additionally, while Plutus-8B achieves SOTA performance across most tasks, it still operates within the constraints of limited Greek financial data, suggesting that further improvements may require additional strategies such as data augmentation, synthetic data generation, or cross-lingual transfer learning from related high-resource financial datasets. Xueqing et al. reports as input, averaging around 60 pages and 31,500 words per document, poses significant challenge for LLMs. Smaller models like Plutus-8B struggle to process such long-context inputs effectively, resulting in performance shortfalls across all dimensions. Nevertheless, Plutus-8B achieved 23.1% win rate in factuality rounds, with performance score of 2.93 compared to GPT-4s 3.06. This suggests that domain-specific training in Greek financial topics and language has equipped Plutus-8B with enhanced factual accuracy. The model benefits from instruction tuning that incorporates financial disambiguation terminology and numerical patterns unique to Greek texts, allowing Plutus-8B to grasp complex financial content more accurately and generate summaries with notable improvements in reliability and trustworthiness. This indicates the importance of targeted training in specific linguistic and domain contexts to enhance model performance. Overall, Plutus-8Bs domain-aware fine-tuning equips it to better navigate financial contextsnarrowing the gap with larger, generalpurpose models like GPT-4. This highlights the critical role of combining linguistic and domain-specific training to enhance LLM performance in non-English, domain-focused tasks."
        },
        {
            "title": "6 Conclusion\nIn this study, we introduced Plutus-ben, the first comprehensive\nGreek financial evaluation benchmark, together with Plutus-8B, the\ninaugural Greek financial LLM, achieving SOTA performance on\nthe Plutus-ben benchmark. These contributions address a notable\ngap, as there were previously no benchmarks or LLMs specifically\ntailored for Greek financial applications. Plutus-ben includes five\nessential NLP tasks—numeric and textual NER, QA, abstractive\nsummarization, and topic classification—facilitating systematic and\nreproducible evaluations of LLMs in the Greek financial domain.\nSignificantly, numeric and textual NER and QA are introduced in\nGreek for the first time. To support these tasks, we developed four\nhigh-quality datasets (GRFinNUM, GRFinNER, and GRFinQA),\nmeticulously annotated by expert native Greek speakers with substan-\ntial financial and linguistic expertise, supplemented by two existing\nresources. Our comprehensive evaluation of 22 LLMs, alongside a\ncarefully designed human evaluation, highlights the intrinsic chal-\nlenges of Greek financial NLP. These challenges arise from linguistic\ncomplexity, domain-specific terminology, gaps in financial reason-\ning, and the constraints of cross-lingual transfer. Notably, Plutus-8B\ndemonstrated SOTA performance and impressive win rates in hu-\nman evaluation, underscoring the importance of models trained with\nfinancial expertise and adapted to the nuances of Greek text. By\nreleasing Plutus-ben, and Plutus-8B, and all associated datasets, we\naim to promote reproducible research and advance Greek financial\nNLP, fostering greater multilingual inclusivity in the financial sector\nand paving the way for further innovations and applications in this\ndomain.",
            "content": "While this study provides valuable insights, our Plutus-ben benchmark is still limited in size and task types, and Plutus-8Bs performance on the abstractive summarization task remains an area for improvement. In the future, we plan to expand Plutus-ben to include datasets spanning diverse tasks. Additionally, we aim to continually pretrain Greek financial LLM with larger model size to enhance efficiency and performance in the Greek financial context. Figure 2: Comparison of model win rates in fluency, coherence, and factuality between Plutus-8B and GPT-4, evaluated on the GRFNS-2023 dataset within the Plutus-ben benchmark."
        },
        {
            "title": "5.2 Human Evaluation\nTo gain deeper insights into models’ performances on Greek lan-\nguage tasks, we conducted a standard human evaluation (Appen-\ndix G). The results presented in Table 4 show that while GPT-4 leads\nin fluency, domain-specific model like Plutus-8B excels other simi-\nlar size models in Greek financial tasks, particularly in coherency\nand factuality. This highlights the need to enhance both linguistic\nand domain-specific capabilities to effectively adapt general-domain\nLLMs for specialized tasks. Notably, Plutus-8B outperforms other\nmodels in terms of coherency (3.51) and factuality (2.93), showing\nsignificant improvements over other models of the same size. These\nresults demonstrate that our designed tasks and curated high-quality\ndataset within the benchmark are effectiveness for improving mod-\nels’ factuality and coherency, which are challenging and important\nfor financial tasks. The strong performance of Plutus-8B underscores\nthe importance of language and domain-specific training, as mod-\nels like FinLLaMA-8B — optimized for English financial data —\nstruggle to adapt to Greek tasks.",
            "content": "Though Meltemi-7B, trained specifically for Greek general-purpose tasks, performs second in fluency with score of 3.99 following GPT-4, its performance in coherency (1.49) and factuality (1.60) lags behind that of GPT-4 and Plutus-8B. This could be due to its Greek-specific training, which improves fluency but struggles with coherency and factuality in comparison to domain-optimized models. On the other hand, FinLLaMA-8B, trained on English financial data, performs poorly across all metrics, underscoring the challenges faced by English-centric models when applied to Greek. We conducted comparative analysis of Plutus-8B and GPT-4 using win rate pairwise competition, focusing on their abilities in processing long-context data 2. Larger model sizes and advanced training methodologies are pivotal for effective long-context processing capabilities. In this regard, GPT-4 demonstrated superiority by outperforming Plutus-8B across nearly all samples in every dimension, attributed to its larger model size, extensive training data, and advanced long-context processing abilities. The GRFNS-2023 dataset, which features the narrative sections of annual company Plutus References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The Belebele Benchmark: Parallel Reading Comprehension Dataset in 122 Language Variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. Association for Computational Linguistics, 749775. [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165 [4] Chung-Chi Chen, Hen-Hsen Huang, Yow-Ting Shiue, and Hsin-Hsi Chen. 2018. Numeral understanding in financial tweets for fine-grained crowd-based forecasting. In 2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI). IEEE, 136143. [5] Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Overview of the ntcir-14 finnum task: Fine-grained numeral understanding in financial social media data. In Proceedings of the 14th NTCIR Conference on Evaluation of Information Access Technologies. 1927. [6] Chung-Chi Chen, Hen-Hsen Huang, Chia-Wen Tsai, and Hsin-Hsi Chen. 2019. CrowdPT: Summarizing Crowd Opinions as Professional Analyst. In The World Wide Web Conference (San Francisco, CA, USA) (WWW 19). Association for Computing Machinery, New York, NY, USA, 34983502. doi:10.1145/3308558. 3314122 [7] Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, and Zhongyu Wei. 2023. DISC-FinLLM: Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. CoRR abs/2310.15205 (2023). doi:10.48550/ARXIV.2310. 15205 [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. CoRR abs/1803.05457 (2018). http://arxiv.org/abs/1803. [9] Leon Derczynski. 2016. Complementarity, F-score, and NLP Evaluation. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association (ELRA), Portorož, Slovenia, 261266. https://aclanthology.org/L16-1040/ [10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG] https://arxiv.org/abs/2305.14314 [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [12] Angeliki Efthymiou and Nikos Koutsoukos. [n. d.]. INFLECTIONAL AND SEMANTIC PROPERTIES OF VERBAL PAIRS IN MODERN GREEK. ([n. d.]). [13] Sharman Esarey. 2020. Lessons from financial assistance to GreeceTechnical appendix. (2020). [14] Dimitra Farmakiotou, Vangelis Karkaletsis, John Koutsias, George Sigletos, Constantine Spyropoulos, and Panagiotis Stamatopoulos. 2000. Rule-based named entity recognition for Greek financial texts. In Proceedings of the Workshop on Computational lexicography and Multimedia Dictionaries (COMLEX 2000). 7578. [15] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. doi:10.5281/zenodo. [16] Cyril Goutte and Eric Gaussier. 2005. probabilistic interpretation of precision, recall and F-score, with implication for evaluation. In European conference on information retrieval. Springer, 345359. [17] Andrew Hayes and Klaus Krippendorff. 2007. Answering the call for standard reliability measure for coding data. Communication methods and measures 1, 1 (2007), 7789. [18] Masanori Hirano. 2024. Construction of Japanese Financial Benchmark for Large Language Models. arXiv:2403.15062 [q-fin.CP] https://arxiv.org/abs/2403. 15062 [19] David Holton, Peter Mackridge, Irene Philippaki-Warburton, and Vassilios Spyropoulos. 2012. Greek: comprehensive grammar of the modern language. Routledge. [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [21] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023. FinanceBench: New Benchmark for Financial Question Answering. arXiv:2311.11944 [cs.CL] https://arxiv.org/abs/2311.11944 [22] Rasmus Jørgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. MultiFin: Dataset for Multilingual Financial NLP. In Findings of the Association for Computational Linguistics: EACL 2023, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 894909. doi:10.18653/v1/2023.findings-eacl.66 [23] Haohang Li, Yupeng Cao, Yangyang Yu, Shashidhar Reddy Javaji, Zhiyang Deng, Yueru He, Yuechen Jiang, Zining Zhu, Koduvayur Subbalakshmi, Guojun Xiong, Jimin Huang, Lingfei Qian, Xueqing Peng, Qianqian Xie, and Jordan W. Suchow. 2024. INVESTORBENCH: Benchmark for Financial Decision-Making Tasks with LLM-based Agent. arXiv:2412.18174 [cs.CE] https://arxiv.org/abs/2412. [24] Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and Changjun Jiang. 2023. CFGPT: Chinese Financial Assistant with Large Language Model. CoRR abs/2309.10654 (2023). doi:10.48550/ARXIV.2309. 10654 [25] Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 7481. https://aclanthology.org/W04-1013/ [26] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 3214 3252. [27] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. arXiv:1711.05101 [cs.LG] https://arxiv.org/abs/1711.05101 [28] [29] Spyros Makridakis. 1993. Accuracy measures: theoretical and practical concerns. International journal of forecasting 9, 4 (1993), 527529. [30] Mistral AI team. 2023. Mistral 7B in short. https://mistral.ai/news/announcingmistral-7b [31] Ying Nie, Binwei Yan, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang, and Dacheng Tao. 2024. CFinBench: Comprehensive Chinese Financial Benchmark for Large Language Models. arXiv:2407.02301 [cs.CL] https://arxiv.org/abs/2407.02301 [32] OpenAI, Josh Achiam, and Steven Adler etal. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774 [33] Katerina Papantoniou and Yannis Tzitzikas. 2024. NLP for The Greek Language: Longer Survey. arXiv:2408.10962 [cs.CL] https://arxiv.org/abs/2408. [34] Agam Shah and Sudheer Chava. 2023. Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks. arXiv:2305.16633 [cs.CL] https://arxiv.org/abs/2305.16633 [35] Agam Shah, Abhinav Gullapalli, Ruchit Vithani, Michael Galarnyk, and Sudheer Chava. 2023. FiNER-ORD: Financial Named Entity Recognition Open Research Dataset. arXiv preprint arXiv:2302.11157 (2023). [36] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024). [37] Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. 2020-2025. https:// github.com/HumanSignal/label-studio Open source software available from https://github.com/HumanSignal/label-studio. Label Studio: Data labeling software. [38] Leon Voukoutis, Dimitris Roussis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, and Vassilis Katsouros. 2024. Meltemi: The first open Large Language Model for Greek. CoRR abs/2407.20743 (2024). doi:10.48550/ARXIV. 2407. [39] Leon Voukoutis, Dimitris Roussis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, and Vassilis Katsouros. 2024. Meltemi: The first open Large Language Model for Greek. arXiv:2407.20743 [cs.CL] https://arxiv.org/abs/2407. 20743 doi:10.1145/3637528.3671554 Xueqing et al. [40] Neng Wang, Hongyang Yang, and Christina Dan Wang. 2023. FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets. arXiv:2310.04793 [cs.CL] https://arxiv.org/abs/2310.04793 [41] Nahathai Wongpakaran, Tinakon Wongpakaran, Danny Wedding, and Kilem Gwet. 2013. comparison of Cohens Kappa and Gwets AC1 when calculating inter-rater reliability coefficients: study conducted with personality disorder samples. BMC medical research methodology 13 (2013), 17. [42] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David S. Rosenberg, and Gideon Mann. 2023. BloombergGPT: Large Language Model for Finance. CoRR abs/2303.17564 (2023). doi:10.48550/ARXIV.2303. [43] Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Jiajia Huang, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, and Jimin Huang. 2024. FinBen: Holistic Financial Benchmark for Large Language Models. arXiv:2402.12659 [cs.CL] https://arxiv.org/abs/2402.12659 [44] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. [45] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: Large Language Model, Instruction Data and Evaluation Benchmark for Finance. arXiv:2306.05443 [cs.CL] [46] Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. 2024. Open-finllms: Open multimodal large language models for financial applications. arXiv preprint arXiv:2408.11878 (2024). [47] Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, Shunian Chen, Yifei Zhang, Lihang Shen, Daniel Kim, Zhiwei Liu, Zheheng Luo, Yangyang Yu, Yupeng Cao, Zhiyang Deng, Zhiyuan Yao, Haohang Li, Duanyu Feng, Yongfu Dai, VijayaSai Somasundaram, Peng Lu, Yilun Zhao, Yitao Long, Guojun Xiong, Kaleb Smith, Honghai Yu, Yanzhao Lai, Min Peng, Jianyun Nie, Jordan W. Suchow, Xiao-Yang Liu, Benyou Wang, Alejandro Lopez-Lira, Jimin Huang, and Sophia Ananiadou. 2024. Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications. arXiv:2408.11878 [cs.CL] https://arxiv.org/ abs/2408.11878 [48] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. 2025. Qwen2.5-1M Technical Report. arXiv preprint arXiv:2501.15383 (2025). [49] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. FinGPT: OpenSource Financial Large Language Models. CoRR abs/2306.06031 (2023). doi:10. 48550/ARXIV.2306. [50] Linyi Yang, Jiazheng Li, Ruihai Dong, Yue Zhang, and Barry Smyth. 2022. NumHTML: Numeric-Oriented Hierarchical Transformer Model for Multi-task Financial Forecasting. arXiv:2201.01770 [cs.LG] https://arxiv.org/abs/2201. 01770 [51] Yi Yang, Yixuan Tang, and Kar Yan Tam. 2023. InvestLM: Large Language Model for Investment using Financial Domain Instruction Tuning. CoRR abs/2309.13064 (2023). doi:10.48550/ARXIV.2309.13064 [52] Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, Jimin Huang, Honghai Yu, and Benyou Wang. 2025. UCFE: User-Centric Financial Expertise Benchmark for Large Language Models. arXiv:2410.14059 [q-fin.CP] https://arxiv.org/abs/2410.14059 [53] Elias Zavitsanos, Aris Kosmopoulos, George Giannakopoulos, Marina Litvak, Blanca Carbajo-Coronado, Antonio Moreno-Sandoval, and Mo El-Haj. 2023. The Financial Narrative Summarisation Shared Task (FNS 2023). In 2023 IEEE International Conference on Big Data (BigData). 28902896. doi:10.1109/ BigData59044.2023.10386228 [54] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can Machine Really Finish Your Sentence?. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers. Association for Computational Linguistics, 47914800. https://doi.org/10.18653/v1/p19-1472 [55] Xiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira, Xiao-Yang Liu, Meikang Qiu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. 2024. Dólares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29, 2024. ACM, 62366246. Plutus"
        },
        {
            "title": "A Open Greek Financial LLM Leaderboard",
            "content": "Figure 3: The Plutus-ben interface. Plutus-8B-instruct Figure 4: The Demo of Plutus-8B-instruct. Xueqing et al."
        },
        {
            "title": "C Dataset Curation and Conversion",
            "content": "Table 5: Datasets included in the Plutus-ben benchmark, presented in both the original Greek and their English translations. Dataset Version Input Output GRFinNUM Greek Original English Translation Σε επίπεδο ομίλου τα κέρδη ανα μετοχή είναι αυξημένα + 10,11% λόγω της επίδρασης λειτουργίας της Cosmokid AE που ξεκίνησε ουσιαστικά το Β εξάμηνο του 2008. At the group level, earnings per share are increased by +10.11% due to the impact of Cosmokid AEs operations, which started in the second half of 2008. 10,11%, ΠΟΣΟΣΤΑ 2008, ΧΡΟΝΙΚΑ 10.11%, PERCENTAGE 2008, TEMPORAL GRFinNER Greek Original English Translation GRFinQA Greek Original English Translation Στις 08.11.2019, η ΟΠΑΠ INVESTMENT LTD ήρθε σε συμφωνία με την Εταιρεία για την πώληση του συνόλου των μετοχών που κατέχει στην ΙΠΠΟΔΡΟΜΙΕΣ Α.Ε., έναντι συνολικού τιμήματος 10.411. On 08.11.2019, OPAP INVESTMENT LTD reached an agreement with the Company for the sale of all the shares it holds in HIPPODROMIES S.A., for C10,411. Βραχυχρονίως, μία αύξηση των δημοσίων δαπανών Πιθανές απαντήσεις: Α) αυξάνει το επίπεδο τιμών αλλά όχι το πραγματικό ΑΕΠ Β) αυξάνει το πραγματικό ΑΕΠ αλλά όχι το επίπεδο τιμών Γ) αυξάνει το πραγματικό ΑΕΠ και το επίπεδο τιμών Δ) δεν αυξάνει ούτε το πραγματικό ΑΕΠ ούτε το επίπεδο τιμών In the short term, an increase in public spending Possible answers: A) Increases the price level but not real GDP B) Increases real GDP but not the price level C) Increases both real GDP and the price level D) Increases neither real GDP nor the price level ΟΠΑΠ INVESTMENT LTD, ΟΡΓΑΝΙΣΜΟΣ ΙΠΠΟΔΡΟΜΙΕΣ Α.Ε., ΟΡΓΑΝΙΣΜΟΣ OPAP INVESTMENT LTD, ORGANIZATION HIPPODROMIES S.A., ORGANIZATION Γ GRFNSGreek Original English Translation Τα μέλη του Διοικητικού Συμβουλίου της ΚΑΠΝΟΒΙΟΜΗΧΑΝ (...TRUNCATED) The members of the Board of Directors of TOBACCO INDUSTRY (...TRUNCATED) Ετήσια Οικονομική Εκθεση της Χρήσης ΔΩΔΕΚΑΜΗΝΗ ΠΕΡΙΟ (...TRUNCATED) Annual Financial Report for the TWELVE-MONTH PERIOD (...TRUNCATED) GRMultiFin Greek Original English Translation Αναστολή συμβάσεων εργασίας Αυγούστου Suspension of employment contracts in August Επιχειρήσεις & Διοίκηση Business & Administration Table 6: Conversion prompts for instruction data, presented with original Greek prompts alongside their English translations. Dataset Original Greek Prompt English Translated Prompt GRFinNUM GRFinNER GRFinQA Στις παρακάτω προτάσεις που προέρχονται από οικονομικές εκθέσεις ελληνικών εταιρειών, αναγνώρισε αριθμητικές οντότητες που ανήκουν στις εξής κατηγορίες: χρηματικά ποσά (ΧΡΗΜΑΤΑ), ποσοστά (ΠΟΣΟΣΤΑ), χρονικές τιμές (ΧΡΟΝΙΚΑ), ποσότητες (ΠΟΣΟΤΗΤΕΣ) και άλλες αριθμητικές τιμές (ΑΛΛΑ). Η απαιτούμενη μορφή απάντησης είναι όνομα οντότητας, τύπος οντότητας. Κείμενο: {Input} Απάντηση: In the following sentences which originate from Greek Company filings, recognize the numeric entities which correspond to the following categories: monetary values (MONETARY), percentages (PERCENTAGES), temporal values (TEMPORAL), quantities (QUANTITIES) and other numeric values (OTHER). The required answer format is: entity name, entity type. Text: {Input} Answer: Στις παρακάτω προτάσεις που προέρχονται από οικονομικές εκθέσεις ελληνικών εταιρειών, αναγνώρισε τις οντότητες που αντιπροσωπεύουν ένα πρόσωπο (ΠΡΟΣΩΠΟ), έναν οργανισμό (ΟΡΓΑΝΙΣΜΟΣ) ή μία τοποθεσία (ΤΟΠΟΘΕΣΙΑ). Η απαιτούμενη μορφή είναι: όνομα οντότητας, τύπος οντότητας. Κείμενο: {Input} Απάντηση: Διάβασε προσεκτικά την παρακάτω ερώτηση και τις πιθανές απαντήσεις. Επίλεξε το γράμμα που αντιστοιχεί στη σωστή απάντηση. Ερώτηση: {Input} Απάντηση: In the following sentences which originate from Greek Company filings, recognize the entities which correspond to person (\"Person\"), an organization (\"Organisation\") or location (\"Location\"). The required answer format is: entity name, entity type. Text: {Input} Answer: Read the following question and the possible answers carefully. Choose the letter which corresponds to the correct answer. Question: {Input} Answer: GRFNS-2023 Σε παρακαλώ διάβασε το παρακάτω κείμενο και συνόψισε το σύντομα και με ακρίβεια. {Input} Please read the following text and summarize it briefly and accurately. {Input} GRMultiFin Διάβασε το κείμενο προσεκτικά και επέλεξε την σωστή κατηγοριά για το κείμενο από τις κατηγορίες Φορολογία & Λογιστική, Επιχειρήσεις & Διοίκηση, Οικονομικά, Βιομηχανία, Τεχνολογία, Κυβέρνηση & Ελεγχοι. Κείμενο: {Input} Απάντηση: Read the text carefully and choose the correct category for the text from the categories Tax & Accounting, Business & Management, Finance, Industry, Technology, Government & Controls. Text: {Input} Answer: Plutus GRFinNUM Annotation Guideline To ensure consistent annotation of numerical entities in financial texts, we define the following annotation guidelines. D.1 Entity Categories We annotate five types of numerical entities: Monetary Percentage Temporal Quantity Others D.2 General Annotation Rules (1) Only numbers are annotated: Include only numerical digits, decimal points (.), and the percent sign (%). (2) Decimal delimiter exclusion: When decimal point is used as delimiter (e.g., 2024.11.26), annotate each component separately as 2024, 11, and 26. (3) Exclusion of textual numbers: Text-based numbers (e.g., two weeks) are excluded, but numeric equivalents (e.g., 2 weeks) are included. (4) Exclusion of non-numeric symbols: Symbols such as $ are not included. D.3 Specific Entity Annotation Rules D.3.1 Monetary. Numbers related to money, including explicit currencies or monetary values. Include: The numeric value in $50 and 100 euros annotate as 50 and 100. D.3.2 Percentage. Numbers representing percentages, % symbol as part of the percentage. Include: 45%, 0.5%. D.3.3 Temporal. Numbers related to time, such as years, dates, and durations. Include: only numbers in 2024, 12.25, 12/25, 2 weeks, 1 year and 3 hours should be included. Exclude: Words such as two weeks, where the number is not explicitly written in numeric form. D.3.4 Quantity. Numbers representing measurable or countable quantities, excluding monetary values. Include: only numbers in 5 items and 100 shares. D.3.5 Others. Numbers that do not fit into the above categories, such as identifiers, version numbers, numerical codes, or numeric positions. Include: only 3 in 3rd place, 2 and 1 in v2.1, and 202 in model 202. Exclude: second investor (textual ordinal numbers). D.4 Annotation Examples Text $50 was paid. 45% of users agreed. The event happened in 2024. 5 items were sold. Version v2.1 is released. Annotated Entity 50 (Monetary) 45% (Percentage) 2024 (Temporal) 5 (Quantity) 2, 1 (Others) Table 7: Examples of annotated numerical entities. GRFinNER Annotation Guideline To ensure consistent annotation of named entities in financial texts, we define the following annotation guidelines. E.1 Entity Categories We annotate three types of named entities: Person Location Xueqing et al. Organization E.2 General Annotation Rules (1) Abbreviations: Annotate them together if they appear together; otherwise, annotate them as two entities. Include: World Health Organization (WHO) as one span. (2) Ambiguous Terms: Resolve ambiguity using context. Include: Amazon as company. Exclude: Amazon as river. (3) General Terms Exclusion: Exclude generic terms. Exclude: the professor, downtown, north, the team. (4) Definite Articles: Exclude the from entity spans. Exclude: the in the WHO. (5) Consecutive Entities: When two entities are consecutive, annotate them separately except postal addresses. Include separately: London and United Kingdom in London United Kingdom. Include separately: street Egnatias 127 and Thessaloniki in street Egnatias 127 in Thessaloniki (Postal Code 54 635). Include separately: Acharnes Attica and Parnithos Avenue in municipality of Acharnes Attica, 15 km Parnithos Avenue. Include as one span: 5900 Penn Avenue, Pittsburgh. E.3 Specific Entity Annotation Rules E.3.1 Person. Names of individual people. Include real people, fictional characters, and usernames. Exclude animal names. Exclude titles that are not part of the legal name. Include: Marie Curie, George Demetriou of Konstantinos. Include only John in Dr. John. Exclude: the professor. E.3.2 Location. Names of geographical places, such as cities, countries, natural landmarks, and fictional locations. Include: Paris, Mount Everest. Exclude: downtown, north. E.3.3 Organization. Names of companies, institutions, and formal groups. Including words like company, association, Inc., Co., and Ltd.. Include: World Health Organization, Tesla Inc., WHO, OPAP Association. Exclude: the team. E.4 Special Cases (1) Organizations with Location Names: If the location refers to specific organization, annotate both; otherwise, only annotate the location. Include: Only Cypriot in the Cypriot company. (2) Organizations Representing Administrative Units or Sports Teams: Annotate as Organization. Include: Baltimore and Indianapolis in Baltimore lost to Indianapolis last weekend as Organizations. GRFinSUM Annotation Guideline To ensure consistency and accuracy in extractive summarization, we established set of annotation guidelines for identifying and selecting relevant textual segments. Our approach focuses on extracting key financial metrics while excluding extra narrative content which elaborates upon other sentences. All annotations were conducted at the sentence level, ensuring that complete sentences are selected. We also include the final punctuation mark for each sentence. The following rules were applied during the annotation process: The text contains substantial amount of financial data; however, not all financial metrics are included in the annotations. Our selection criteria prioritize core earnings and expense-related metrics, while excluding explanatory narratives, interpretations, or alternative financial indicators. In principle metrics can be seperated into two categories, earnings related metrics and expense related metrics. For each category we focus on the following specific indicators: Earnings Metrics: We annotate sentences which contain information about Pre-tax earnings, After-tax earnings (Net profit), Revenue/Turnover, Profit margin, However, we do not annotate all earnings related metrics. We exclude metrics such as EBIT (Earnings Before Interest and Taxes) and EBITDA (Earnings Before Interest, Taxes, Depreciation, and Amortization) from our annotations. While these are commonly used financial indicators, they are considered alternative or non-GAAP metrics and are not universally standardized under frameworks such as IFRS. Plutus Expense Metrics: We annotate expenses, operating expenses and total expenses, prioritizing the most generic form of the expense metric in each instance. When financial reports provide breakdown of expenses by specific projects or operational segments, only the aggregate expense value is included unless the breakdown is contained within the same sentence. Complementary explanations regarding how specific projects contribute to overall costs are excluded from the annotation. Human Evaluation Annotation Guideline To ensure consistent annotation of summarization quality in financial texts, we define the following annotation guidelines. G.1 Evaluation Categories We evaluate summaries based on three criteria: Language Appropriate Fluency Coherence Factuality G.2 General Annotation Rules (1) Language Appropriate Fluency (Fluency): Measures how well the summary aligns with the expected language fluency and domain-specific terminology. 1 (Bad): Response is entirely in the wrong language (e.g., English instead of Greek). 2 (Poor): Response is mixture of English and Greek. 3 (Okay): Response is fully in Greek but contains grammatical or lexical errors or repetition. 4 (Good): Response is entirely in fluent Greek without grammatical or lexical errors or repetition. 5 (Excellent): Response is entirely in fluent Greek with appropriate domain-specific terminology. (2) Coherence: Evaluates the logical progression and structure of ideas in the text. 1 (Bad): The text is disorganized, with sentences or paragraphs lacking logical flow. 2 (Poor): The text attempts structure but has logical leaps, disjoint ideas, and is confusing. 3 (Okay): The text is mostly coherent, with general structure and minor logical errors or awkward transitions. 4 (Good): The text flows well, with clear progression and only minor errors. 5 (Excellent): The text flows naturally and consistently, with smooth transitions between ideas. (3) Factuality: Evaluates whether the summary is factually consistent with the original content. 1 (Bad): Multiple factual inaccuracies, such as misrepresented company names, locations, or numerical data. 2 (Poor): Some factual errors with key points missing or distorted. 3 (Okay): Fairly accurate, with only minor omissions or discrepancies. 4 (Good): Accurate, with only few minor omissions or discrepancies. 5 (Excellent): Entirely accurate, with all facts presented as found in the source document. Annotator Demography Our benchmark construction relies on the expertise of team of highly qualified annotators, who are native Greek speakers with diverse backgrounds in computer science, mathematics, statistics, and finance. Their combined knowledge ensures the high-quality annotation of financial texts, contributing to the robustness and reliability of our dataset. One annotator, currently pursuing Ph.D. in Computer Science at leading Greek university, has strong foundation in both mathematics and statistics, complemented by industry experience as credit risk analyst. This background provides valuable information on financial knowledge, risk assessment, and statistical modeling, which are essential to annotate our benchmark dataset. Another annotator, Ph.D. student in Computer Science at major UK institution, holds an Integrated Masters degree in Electrical and Computer Engineering. Their expertise in computer science enhances the annotation process by ensuring precision and alignment with modern NLP techniques. The team is further strengthened by postdoctoral researcher with an interdisciplinary background spanning electrical and computer engineering, computer science, and mathematics. Having obtained Ph.D. from prestigious U.S. university, this annotator brings extensive research experience and deep understanding of theoretical and applied aspects of financial computing, making them instrumental in refining annotation guidelines and resolving complex cases. The collective expertise of our annotators is critical to the development of our Greek financial benchmark. Their deep familiarity with the Greek financial ecosystem, combined with strong computational and analytical skills, ensures that our dataset accurately reflects domainspecific nuances while maintaining linguistic and terminological precision. By leveraging their diverse backgrounds, we are able to construct high-quality resource that will serve as foundation for advancing NLP research in financial applications."
        },
        {
            "title": "I Annotation Process",
            "content": "Xueqing et al. Figure 5: The Label Studio interface of the NER annotation process. Figure 6: The Label Studio interface of the human evaluation process. Plutus Model Evaluation The models are categorized as follows: GPT series: GPT-3.5-Turbo [3], GPT-4o-Mini [32], GPT-4o [20], and GPT-4 [1]. LLaMA series: LLaMA-3.2-1B [11], LLaMA-3-8B [11], LLaMA-3.1-8B [11], and LLaMA-3-70B [11]. Qwen series: Qwen2.5-1.5B [48], Qwen2.5-7B [48], Qwen2.5-32B [48], and Qwen2.5-72B [48]. Gemma series: Gemma-2-2B [36], Gemma-2-9B [36], and Gemma-2-27B [36]. Mistral-7B [30]: Implemented as the foundation of the Meltemi-7B, Mistral-7B serves as contrasting baseline model. Finma-7B [45]: Finma-7B is fine-tuned with large-scale multi-task instruction data, enhancing its utility in financial-specific task engagement. FinLLaMA-8B [46]: FinLLaMA-8B is instruction fine-tuned with 573K financial instructions, excelling in navigating contextually complex financial discourse. Meltemi-7B [39]: Built on Mistral-7B with continual pretraining in Greek and English, Meltemi-7B exhibits strong linguistic capabilities, though its financial methodology skills are untested. Llama-Krikri-8B14: Based on Llama-3.1-8B with continual pretraining with Greek, English, and math and coding data, Llama-Krikri-8B demonstrates robust linguistic abilities but lacks validation in financial domains. Plutus-8B: Derived from Llama-Krikri-8B, Plutus-8B is instruction fine-tuned using Greek financial data, enhancing its capacity for specialized financial reasoning. Evaluation Metrics The Entity F1 is the harmonic mean of Precision and Recall, calculated as follows. 𝑃𝑒𝑛𝑡𝑖𝑡 𝑦 = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑃 𝑅𝑒𝑛𝑡𝑖𝑡 𝑦 = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑁 𝑃𝑒𝑛𝑡𝑖𝑡 𝑦 𝑅𝑒𝑛𝑡𝑖𝑡 𝑦 𝑃𝑒𝑛𝑡𝑖𝑡 𝑦 + 𝑅𝑒𝑛𝑡𝑖𝑡 𝑦 Entity F1 = 2 (1) (2) (3) where 𝑃𝑒𝑛𝑡𝑖𝑡 𝑦 and 𝑅𝑒𝑛𝑡𝑖𝑡 𝑦 denote the Precision and Recall of entity prediction, respectively. 𝑇 𝑃 (True Positive) represents the number of actual entities correctly identified. In contrast, 𝐹 𝑃 (False Positive) refers to the number of non-entities incorrectly predicted as entities. 𝐹 𝑁 (False Negative) denotes the number of entities that were not correctly predicted. Accuracy Acc measures the proportion of correct predictions made by the model and is defined as follows. Rouge-1 is primarily used to compute the unigram-level (word-level) overlap between the generated summary and the reference summary, Acc = Number of correct predictions Total number of Predictions (4) and is defined as follows: 𝑃𝑟𝑜𝑢𝑔𝑒1 = Number of overlapping unigrams in generated and reference summary Total unigrams in generated summary 𝑅𝑟𝑜𝑢𝑔𝑒1 = Number of overlapping unigrams in generated and reference summary Total unigrams in reference summary Rouge-1 F1 = 2 𝑃𝑟𝑜𝑢𝑔𝑒1 𝑅𝑟𝑜𝑢𝑔𝑒1 𝑃𝑟𝑜𝑢𝑔𝑒1 + 𝑅𝑟𝑜𝑢𝑔𝑒1 (5) (6) (7) where 𝑃𝑟𝑜𝑢𝑔𝑒1 and 𝑅𝑟𝑜𝑢𝑔𝑒1 denote the Precision and Recall of Rouge-1, respectively. Rouge-1 F1 is the final Rouge-1 score that calculates the unigram (single-word) matches without considering word order. Dataset Quality Validation The F1-score, Cohens Kappa, and Krippendorffs alpha were calculated to measure the agreement of annotators for data quality control purposes. The F1-score is performance metric for classification models that combines Precision and Recall using their harmonic mean as shown in the equation (8). where Precision measures how many of the samples predicted as positive are actually positive; Recall measures the proportion of actual positive samples that the model correctly identifies. Cohens Kappa measures the agreement between two annotators on classification task, accounting for the possibility of random agreement, 𝐹 1 𝑠𝑐𝑜𝑟𝑒𝑠 = 2 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑅𝑒𝑐𝑎𝑙𝑙 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 + 𝑅𝑒𝑐𝑎𝑙𝑙 (8) as shown in equation (9). 𝜅 = 𝑃𝑜 𝑃𝑒 1 𝑃𝑒 (9) 14https://huggingface.co/ilsp/Llama-Krikri-8B-Base Xueqing et al. where 𝑃𝑜 means the observed agreement and 𝑃𝑒 is the expected agreement. Krippendorffs alpha is general measure of inter-rater reliability applicable to categorical, ordinal, interval, or ratio data, as shown in equation (10). where 𝐷𝑜 is the total disagreement observed among annotators, and 𝐷𝑒 is the total disagreement expected by chance. 𝛼 = 1 𝐷𝑜 𝐷𝑒 (10) Ethical Statement The authors take full responsibility for the development and dissemination of Plutus-ben and Plutus-8B, ensuring that all raw data used are publicly available, devoid of personal information, and conform to established ethical guidelines. The data are shared under the MIT license, requiring users to adhere to its terms. This manuscript, including large language models, source codes, and datasets, is intended for academic and educational purposes only and is not substitute for professional advice. While efforts have been made to ensure its accuracy, the authors and their institutions disclaim liability for any outcomes arising from its use. Users agree to take responsibility for ethical and lawful use and to indemnify the authors and their affiliates against any claims or damages resulting from reliance on this Material. Received 20 February 2007; revised 12 March 2009; accepted 5 June"
        }
    ],
    "affiliations": [
        "Archimedes/Athena RC Athens, Greece",
        "Athens University of Economics and Business, Archimedes/Athena RC Athens, Greece",
        "The Fin AI USA",
        "The University of Manchester Manchester, UK"
    ]
}