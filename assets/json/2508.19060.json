{
    "paper_title": "No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes",
    "authors": [
        "Blaž Rolih",
        "Matic Fučka",
        "Danijel Skočaj"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 6 0 9 1 . 8 0 5 2 : r No Label Left Behind: Unified Surface Defect Detection Model for all Supervision Regimes Blaˇz Rolih1*, Matic Fuˇcka1 and Danijel Skoˇcaj1 1*Faculty of Computer and Information Science, University of Ljubljana, Veˇcna Pot 113, Ljubljana, 1000, Slovenia. *Corresponding author(s). E-mail(s): blaz.rolih@fri.uni-lj.si; Contributing authors: matic.fucka@fri.uni-lj.si; danijel.skocaj@fri.uni-lj.si; Abstract Surface defect detection is critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet. Keywords: Surface Defect Detection, Surface Anomaly Detection, Industrial Inspection, Deep Learning, Mixed Supervision"
        },
        {
            "title": "1 Introduction",
            "content": "Enhancing production efficiency to ensure highquality products is key priority in modern manufacturing. Detecting and addressing imperfections or irregularities on the surfaces of manufactured components plays critical role in this effort. Historically, such detection has relied on manual inspection, labour-intensive process prone to human error and inefficiencies [1, 2]. Automated systems, however, enable real-time monitoring, precise localisation of defects and anomalies, and significant improvements in product quality. Over the past decade, the field of computer vision has experienced remarkable progress, largely driven by advancements in deep learning paradigms. Unsurprisingly, these methods have also made their way into the traditionally conservative domain of machine vision, significantly advancing tasks such 1 as visual inspection [35]. This progress exemplifies the power of collaboration between academic research and industrial applications, bridging the gap between cutting-edge technology and practical needs. However, the core premises underlying academic research on the detection of surface irregularities do not always align perfectly with the practical requirements and scenarios encountered in manufacturing processes. The existing literature on this topic predominantly (with only few exceptions) clusters into two distinct problem formulations. The first, surface defect detection (SDD) [2, 4, 6, 7], assumes the availability of both normal and defective surface images during training, framing the task as supervised learning problem. The second, unsupervised anomaly detection (UAD) [811], is used in scenarios where only images of normal surfaces are available during training. Each approach has its merits. Supervised methods typically deliver superior performance [4, 10], making them the preferred choice whenever sufficient number of labelled defect images is available. However, supervised models learn the decision boundary between normal and defective samples based on the training data, making them prone to missing novel or unseen defects that differ significantly from those in the training set [12, 13]. In addition, labelling images is often tedious task that disrupts the manufacturing process, and defective samples are frequently unavailable in sufficient quantities [4]. In contrast, UAD methods address these limitations by training exclusively on normal samples, allowing for the detection of previously unseen types of anomalies. This approach acknowledges the reality that normal samples are usually more abundant than defect samples in manufacturing environments [14]. However, unsupervised methods generally perform worse than supervised approaches when comparable amounts of labelled data are available. Moreover, UAD methods are unable to leverage the information from any anomalous samples that may exist [14], which represents significant limitation in many practical scenarios. In practice, at least some anomalous data typically exists at some stage, meaning this valuable information is completely disregarded in such cases. Fig. 1 Different supervision scenarios within manufacturing processes are illustrated. Images with green border contain no anomalies, while images with red border indicate the presence of an anomaly. For some images, the corresponding anomaly segmentation mask is also provided. The labelling effort required increases progressively from left to right. At present, only SuperSimpleNet supports training across all four scenarios. We argue that both extremespurely unsupervised or fully supervised methods-are not optimally suited for the complex task of detecting defects and anomalies on object surfaces in realworld applications. From an application-driven perspective, all available labels should be utilised. This necessitates the development of detection models capable of leveraging all possible supervision scenarios, combining the strengths of both approaches to better meet industrial demands. To address these requirements, we introduce SuperSimpleNet 1, novel discriminative model built upon the foundation established by SimpleNet [16]. SuperSimpleNet is designed to accommodate any available training data, as depicted in Fig. 1, including2: (i) normal images, (ii) anomalous images annotated at the image level (indicating whether an anomaly is present), and (iii) fully annotated images with pixel-level labels that provide segmentation masks of anomaly regions. Based on these types of training data, we define four distinct training regimes: (i) unsupervised, where only normal images are used; (ii) weakly 1Preliminary version presented in [19]. 2From now on, we will use the terms defect and anomaly interchangeably to emphasise the unification of the general problem of defect and anomaly detection. 2 normal only ano. image level ano. pixel level Speed (< 10ms) US WS MS FS Ours SDNet TNet MMNet DRA EAD BGAD DSR SN FF PC DRÆM PRN Table 1 Four learning regimes utilising different levels of supervision: (i) normal images only, normal and anomalous images annotated with (ii) image-level and (iii) pixel-level labels. Twelve SOTA methods are listed, indicating which types of labels they are capable of processing. Additionally, the inference speed is reported, highlighting whether the requirement of 10ms is met. Notably, only our proposed method fulfils all these requirements. Models in the table are as follows: SDNet [4], TNet [6], MMNet [15], DRA [14], EAD [11], BGAD [12], DSR [10], SN [16], FF [17], PC [18], DRÆM [9] and PRN [13]. supervised, where anomalous images with imagelevel labels are also included; (iii) fully supervised, where all images are pixel-level annotated; and (iv) mixed supervision, where all images have image-level labels, but only subset includes pixel-level segmentation masks. The primary strength of SuperSimpleNet lies in its ability to effectively utilise all the aforementioned input types and operate seamlessly across the four training regimes. This flexibility ensures its applicability to wide range of real-world industrial scenarios, bridging the gap between academic research and practical needs. To the best of our knowledge, it is the only method capable of handling all available input data, annotated at different levels of supervision, while achieving good results across all supervision scenarios. Additionally, it is highly efficient, addressing key requirement frequently encountered in industrial applications. This is illustrated in Table 1, which highlights the main properties of various methods for defect and anomaly detection, along with their respective capabilities. To achieve these goals, we significantly enhanced SimpleNet in terms of efficiency, effectiveness, and flexibility. In addition to several technical refinements that improved the original method, the key scientific contributions of our work are as follows: Improved Synthetic Anomaly Generation: We propose novel synthetic anomaly generation process that substantially enhances training in unsupervised scenarios and facilitates effective training in situations with limited or no pixel-level annotations. This innovation also strengthens segmentation capabilities, enabling robust and reliable predictions in cases where traditional methods often struggle. Effective Classification Head: We design simple yet highly effective classification head images that captures the global context of and enables efficiently utilisation of imagelevel annotations. This addition enhances the models ability to process diverse types of labelled data, improving its adaptability. Unified Model for All Supervision Paradigms: By integrating techniques and insights from different levels of supervision into single architecture, we present unified approach that facilitates knowledge transfer across regimes. This significantly improves the performance of defect detection and anomaly segmentation, offering versatile and scalable solution tailored to real-world industrial challenges. We conducted extensive experiments across four challenging datasets, demonstrating the versatility and performance of SuperSimpleNet. In the fully supervised setting, SuperSimpleNet achieved state-of-the-art results on two standard real-world defect detection datasets, SensumSODF [6] and KSDD2 [4], with AUROC scores of 98.0% and detection AP of 97.8%, respectively. In the weakly supervised setting, SuperSimpleNet achieved an AUROC of 97.4% on SensumSODF, outperforming previous fully supervised SOTA and detection AP of 97.2% on KSDD2. Similarly, it consistently outperforms existing methods on both SensumSODF and KSDD2 in the mixed supervised setting. For unsupervised anomaly detection, we tested SuperSimpleNet on two standard benchmarks, MVTec AD [3] and VisA [5], where it achieved AUROC scores of 98.3% and 93.6%, respectively, again matching state-of-theart performance. In addition to its superior accuracy, SuperSimpleNet demonstrated exceptional efficiency, with an inference time of 9.5 ms and throughput of 262 images per second, surpassing 3 most contemporary models in speed. Its efficiency, adaptability and outstanding results across all four settingsfully supervised, weakly supervised, mixed supervised, and unsupervisedhighlight its effectiveness and make it robust solution for wide range of real-world anomaly detection scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Unsupervised methods Unsupervised anomaly detection has become prominent research direction in recent years and has received lot of attention. Those approaches are particularly useful in scenarios such as setting up new production lines or other situations where normal samples are abundant, but anomalous samples are rare. Among unsupervised methods, reconstruction-based approaches initially gained popularity. These methods typically train an autoencoder-like (AE-like) network [20, 21], assuming the reconstructed image will differ from the input image on the anomalous regions. Models from this paradigm are not limited to AE networks but also use other generative networks, such as GANs [22], transformers [23] or diffusion models [24]. The assumption about the successful reconstruction of anomalous regions does not always hold, leading to decrease in overall performance. Another successful approach in the recent past involves methods that leverage features extracted from pretrained networks, such as ResNet [25]. These extracted features are then utilised in various ways, including through memory-bank [18], distillation techniques [26, 27], student-teacher architectures [28] or normalizing flows [17, 27]. The last distinctive group consists of discriminative methods that are trained using synthetic anomalies. The techniques of generating anomalies have evolved significantly over the past few years, progressing from simple cut-and-paste methods [29] to more advanced approaches like Perlin noise [9] and diffusion-based anomaly generation [30]. These anomalies can be generated either directly on input images [8, 9, 28] or, as more recently shown, at the feature-level [10, 16]. While unsupervised methods achieve great detection results, they often struggle to simultaneously achieve fast operation times, prompting recent research to focus on more efficient solutions [11, 31, 32]. Fully Supervised Methods Although anomalous samples are initially rare or unavailable, they gradually accumulate over time in industrial settings. Usually, these samples do not represent the entire distribution of potential anomalies, but research has shown that their use can improve detection performance [10]. Most unsupervised methods are not designed to leverage anomalous data, leading to the development of fully supervised methods, such as SegDecNet [4], TriNet [6], and MaMiNet [15] in industrial applications to maximise the detection performance. Recent approaches, such as BGAD [12] and PRN [13], attempt to enhance anomaly detection by generating additional synthetic anomalies, further diversifying the anomalies the model encounters. However, their complex architecture results in longer inference times, limiting their suitability for industrial environments. Moreover, most fully supervised methods still require numerous fully-labelled samples to outperform the most recent unsupervised methods. They also lack support for strictly unsupervised learning, making them impractical in certain real-world scenarios, such as setting up new production line. Weakly Supervised Methods In contrast to fully supervised methods, weakly supervised approaches require only image-level labels, demanding much less labelling effort than pixel-level annotations. The first significant attempt at weakly supervised anomaly detection was made by DeepSAD [33], one-class classification method that utilises image-level labels during training. similar strategy was recently employed by SegAD [34], which trains weakly supervised random forest classifier using the predictions of deep unsupervised methods. key paradigm in weakly supervised anomaly localisation involves using class activation maps (CAM) [35], which allows neural networks to localise anomalies without pixel-level supervision. Due to this property, several methods [3639] have adopted CAM for weakly supervised anomaly detection. Despite the absence of pixel-level labels, some weakly supervised methods indirectly train for localization by leveraging techniques such as 4 entropy minimization [40], training exclusively on anomaly-free images [4, 15], or generating pseudolabels from unsupervised models [41]. Mixed Supervision Methods Mixed supervision in anomaly detection has received limited attention in research. One of the pioneering approaches in this area is SegDecNet [2, 4], which employs dual-head architecture one head for detection and the other for segmentation. This foundational model has been extended in later work [15] by incorporating attention mechanisms to enhance performance. However, these models still dont support unsupervised training. Unlike previous approaches, SuperSimpleNet can perform aptly in all four supervision scenarios. It achieves great detection performance even in unsupervised and weakly supervised settings. Earlier methods either lack support for all four training regimes or require pixel-level annotations to enable segmentation."
        },
        {
            "title": "3 SuperSimpleNet",
            "content": "SuperSimpleNet is depicted in Figure 2. First, features are extracted from the input image using pretrained convolutional network. These are then upscaled and pooled to capture the neighbouring context (Section 3.1). Since anomaly segmentation and anomaly detection are distinct tasks, we use simple adaptor to adapt the features for segmentation. Synthetic anomalies are then injected into both the segmentation and detection features. The key advancement of SuperSimpleNet is its novel synthetic anomaly generation mechanism, which enables strong performance across all supervision regimes. In the unsupervised setting, SuperSimpleNet exclusively depends on generated synthetic anomalies and their labels. In other regimes, synthetic anomalies complement the real ones, enriching the training signal and improving overall performance. The new strategy creates more realistic anomaly regions at the feature level by leveraging binarised Perlin noise mask (Section 3.2). The processed features are then used in the segmentation and detection module (Section 3.3), whose dual-branch design enables learning with mixed supervision (Section 3.4). All training mechanisms are discarded during inference, allowing the network to swiftly predict an anomaly map and score (Section 3.5). The following sections describe all the mentioned modules including training specifics with mixed supervision, to provide clear understanding of SuperSimpleNet. in detail,"
        },
        {
            "title": "3.1 Feature extractor",
            "content": "Following SimpleNet [16], the input image is first passed through the pretrained feature extractor Φ (in our case, WideResnet50 [42] pretrained on ImageNet [43]) to extract features fl from subset of layers where and in our case = {2, 3}. Due to the architectural design of ResNet-like [25] networks, these features have relatively low spatial resolution. This limits the models ability to detect smaller anomalies and reduces localisation precision. To address this limitation, an upscaling layer is introduced before feature concatenation, that is: Fl = Upsample(fl, (2H0, 2W0)) , (1) where (H0, W0) is the size of the largest extracted feature map, and bilinear interpolation is used. This effectively doubles the feature resolution. In our case, this means that the 3rd layers features are enlarged by factor of 4 and the 2nd layers features are enlarged by factor of 2. This ensures that all layers match in spatial resolution and can be concatenated to produce ˆF: ˆF = Concatenate({Fl L}) . (2) In line with SimpleNet [16], the neighbouring context for each feature location is captured using local average pooling with 3 3 kernel, that is: = AvgKernel( ˆF, kernel size = 3, stride = 1) . (3) This all results in an upscaled feature map, where each location encodes contextual information. Although features from networks trained on natural images transfer well to the anomaly detection task [44], simple linear layer is used to adapt these features to task-specific latent space. Only the features used as the input for the segmentation head are adapted (A in Figure 2). We hypothesise that this is more effective because it allows segmentation and classification heads to operate in distinct latent spaces, leading to better specialisation for each task. 5 Fig. 2 SuperSimpleNets architecture. Features are first extracted, upscaled, and, in the case of the segmentation branch, also adapted. During training, synthetic anomalies are generated in the latent space and are limited to regions defined by the binarised Perlin mask. The segmentation head predicts an anomaly mask Mo, based on the perturbed feature map PA, which is then used in combination with the perturbed feature map PF by the classification head to produce the anomaly score s. The anomaly score s, and the predicted map Mo are supervised by the anomaly mask M, and the ground truth anomaly score y, where is set to 1 if the image contains an anomaly (synthetic or real) and to 0 otherwise. During inference, Mo and are produced directly, skipping the anomaly generation phase. The remaining parts of original SimpleNet are also shown in the image with green colour."
        },
        {
            "title": "3.2 Latent-space masked anomaly",
            "content": "generation In the supervised setting, while training solely with the real anomalous data is possible, it may not fully capture the distribution of potential defects. To address this, we propose novel synthetic anomaly generation technique inside the latent space of pretrained feature extractor capable of generating highly randomised anomalies. As result, all training regimes besides the unsupervised regime utilise mix of real and synthetic anomalies. Furthermore, the new synthetic anomaly generation enables the model to be trained in all four supervision regimes while simultaneously being able to learn anomaly segmentation despite not having any available labelled data. This enables SuperSimpleNet to bridge the performance gap between fully and weakly supervised settings, where many previous methods struggle. The new synthetic anomaly generation strategy is illustrated in Figure 3. The core idea is to generate synthetic anomalies using Gaussian noise, which is applied only to regions defined by 6 the synthetic anomaly mask Msynth. To obtain this mask, Perlin noise [45] is generated (as in [8, 9, 28]) and then thresholded, resulting in Perlin anomaly mask Mp. To get the final synthetic anomaly mask Msynth, the regions corresponding to actual anomalies, delineated by Mgt, are removed from the Perlin anomaly mask Mp, that is: Msynth = Mp Mgt . In the unsupervised or the weakly supervised setting, Mgt is always empty, so Mp directly serves as Msynth. (4) Gaussian noise, sampled from the distribution (µ, σ2), is restricted to regions defined by the synthetic anomaly mask Msynth to produce ϵ in Figure 3. This region-limited noise is later added to both and A, resulting in the perturbed feature map PF and perturbed adapted feature map PA, respectively. This approach ensures that SuperSimpleNet generates more realistic, spatially coherent, and highly randomised anomalous Fig. 3 Synthetic anomaly generation. Synthetic anomaly masks Msynth are generated by removing actual anomalous regions (captured by ground truth mask Mgt) from Perlin anomaly mask Mp (obtained by thresholding Perlin Noise [45]). Msynth is then used to limit the Gaussian noise only to specific regions, producing final noise ϵ, which is later added to the features to create synthetic anomalies. The final anomaly mask is constructed from Msynth and Mgt indicates regions with synthetic and actual anomalies. Since Mgt is empty in the case of weakly supervised and unsupervised learning, Mp directly becomes Msynth and the final mask M. regions. The randomness of this process also prevents overfitting to unrealistic patterns that may not represent unseen data. We retain the feature duplication mechanism from SimpleNet but apply noise to both the original and the duplicated features following the previously described process. This stabilises the training as the model is exposed to wider variety of anomalies in each batch."
        },
        {
            "title": "3.3 Segmentation-detection module",
            "content": "The architecture of SimpleNet [16] is extended with new classification head, Dcls, while retaining the original segmentation head, Dseg. Consistent with the overall simplicity of the architecture, the classification head consists of single 5 5 convolutional block, followed by pooling layers and final fully connected layer. Despite its simplicity, this design offers strong discriminative power, allowing the model to better capture the global context. This leads to reduction in false positives and improved detection of small and in-distribution defects. The simplicity of the design is important in achieving good performance across both unsupervised and supervised settings, providing sufficient discriminative power while minimising the risk of overfitting. Moreover, the dual-branch design is well-suited for mixed supervision, as the network can be trained efficiently just with image-level labels. 7 Fig. 4 Detailed architecture of segmentationdetection module. The design preserves the segmentation head from SimpleNet while introducing new classification head with wider kernel. This design allows for better improving anomaly detection contextual understanding, capabilities. the As shown in Figure 4, the segmentation head first generates an anomaly map Mo. This map is then concatenated with the feature map (or noise-augmented feature map PF during training) and serves as input to the classification heads convolutional block. The output from the convolutional block and the anomaly map are pooled, concatenated, and passed through the final fully connected layer to produce an image-level anomaly score s."
        },
        {
            "title": "3.4 Training with mixed supervision",
            "content": "The truncated L1 loss is used for the segmentation head: (cid:40) li,j = max(0, th Dseg(Pi,j)); if M[i, j] = 1 max(0, th + Dseg(Pi,j)); otherwise , (5) where th is the truncation term preventing overfitting (in our case, 0.5), Dseg is the segmentation head and Pi,j is the value of the predicted anomaly mask at the position (i, j). The total truncated L1 loss, denoted by L1t, is computed as the mean of terms li,j across all elements within the predicted anomaly mask. This loss encourages the model to learn soft decision boundary between anomalous and non-anomalous regions. Due to the soft decision boundary, the model does not overfit to the data, resulting in better generalisation. We additionally use focal loss [46] due to its good performance in cases of unbalanced data, resulting in the formulation of the final segmentation loss, denoted as Lseg: Lseg = L1t + Lf oc . (6) predicts the anomaly map using the segmentation head and anomaly score using the classification head. The anomaly map is upscaled to match the size of the input image and refined using Gaussian filter with σ = 4, yielding the final anomaly map. Focal loss [46] is also used as the classification loss Lcls: Lcls = Lf oc . (7) The final classification losses, with control term γ added: loss combines the segmentation and"
        },
        {
            "title": "4 Experiments",
            "content": "This section outlines the evaluation setup, including details on the datasets, metrics and implementation, followed by presentation and discussion of the main results and ablation study results. = γ Lseg + Lcls . (8)"
        },
        {
            "title": "4.1 Datasets",
            "content": "The γ term enables learning with mixed supervision and is determined based on the image label, following these rules: γ = 1; if image is normal; 1; if image is ano. & fully labelled; 0; if image is ano. & weakly labelled; (9) This allows the segmentation head to be trained images except anomalous images withon all out pixel-level labels, while the classification head is always trained. Thanks to the new synthetic anomaly generation strategy, the model can successfully train segmentation even in the total absence of pixel-level labels. Following the approach of SegDecNet [4], we apply segmentation loss weighting to emphasise pixels in the centre of anomalous regions while reducing the focus on the more uncertain border pixels. The weights are produced with the distance transform applied to the ground truth mask, and are then simply multiplied by the loss [4]. This approach efficiently addresses labelling uncertainty in the boundary areas. The target for the segmentation loss is the anomaly mask M, which marks areas containing both synthetic and real anomalies. The target anomaly label for classification loss is set to 1 if the image contains an anomaly (either synthetic or real) and 0 otherwise."
        },
        {
            "title": "3.5 Inference",
            "content": "In the inference stage, the anomaly generation mechanism is removed, and the model directly 8 is The performance of SuperSimpleNet across fully supervised, mixed supervised and weakly supervised settings evaluated using two realworld, well-annotated datasets: Sensum Solid Oral Dosage Forms (SensumSODF) [6] and Kolektor Surface-Defect Dataset 2 (KSDD2) [4]. SensumSODF consists of two categories representing different types of solid oral dosage forms: softgels and capsules. Each category includes normal and precisely annotated anomalous images featuring defects of various complexities and sizes. Since the dataset lacks predefined train-test split, we follow the original protocol and perform 3-fold cross-validation [6]. Figure 5 shows an example from both categories. KSDD2 contains images of production items captured using visual inspection system, with both train and test sets containing normal and annotated anomalous images. The defects present are often visually similar to good regions, making it well-suited for evaluating the models performance in industrial settings. Figure 5 shows an example from this dataset. The unsupervised setting is evaluated on MVTec AD [3] and VisA [5] datasets. MVTec AD comprises 15 categories, while VisA contains 12 different categories. Every category contains only normal images in the train set, intended to be used with unsupervised methods. The test set includes both normal and annotated anomalous images, covering diverse range of anomaly types, scales, and complexities. Figure 10 shows some examples from both datasets."
        },
        {
            "title": "4.2 Evaluation metrics",
            "content": "Dataset-specific evaluation metrics are adopted in line with original protocols and recent literature [68]. Image-level performance for SensumSODF, MVTec AD, and VisA is measured using the Area Under the Receiver Operator Curve (AUROC). The Area Under the Per-Region Overlap (AUPRO) is used for pixel-level performance. For KSDD2, we follow the majority of related works by using Average Precision for both image-level detection and pixel-level localisation performance (APdet and APloc)."
        },
        {
            "title": "4.3 Implementation details",
            "content": "Training The model is trained for 300 epochs using the AdamW [47] optimizer with batch size of 32. Larger batch sizes help improve the results due to the greater variety of synthetic anomalies generated within the batch. The learning rate for the segmentation and classification head is set to 2 104 with weight decay of 105, while the learning rate of the adaptor module is set to 104. learning rate scheduler that reduces the learning rate by factor of 0.4 after 240 and 270 epochs is used to stabilise the training. The gradient is clipped to norm 1 to further stabilise the training in the supervised setting. In contrast, in the unsupervised setting, we stop the gradient flow from the classification head to the segmentation head. Synthetic anomaly generation Gaussian noise for synthetic anomalies is sampled from (0, σ2) with σ = 0.015. The Perlin noise binarisation threshold varies according to the supervision regime. In the mixed and fully supervised setting, the pixel-level labels efficiently capture abnormality distribution, so synthetic anomalies primarily refine the normality boundary. To keep these anomalies small, threshold of 0.6 is used. Due to the lack of pixel-level labelled defects in the weakly supervised setting, synthetic anomalies are required to enhance the modelling of the global distribution. Larger synthetic anomalies are needed for this, achieved with threshold of 0.2. For unsupervised settings, thresholds used are: 0.6 for VisA and 0.2 for MVTec AD. Images All input images are normalised using ImageNet [43] normalisation. For MVTec AD and VisA, images are resized to 256 256, in line with recent literature [8, 9]. For SensumSODF and KSDD2, we follow the original protocols from [4, 6] and use 232640 for KSDD2, 192320 for SensumSODF capsule and 144 144 for SensumSODF softgel. All compared models use the same image sizes to ensure fair comparison. Flipping augmentations are applied in supervised settings to extend the set of anomalous images, as described in [6]. To improve training stability, we follow [4] by balancing each epoch with an equal number of normal and anomalous samples. Evaluation To enable comparison with the base SimpleNet [16], we modify its loss design to support fully supervised training by classifying defective samples in the ground truth mask (Mgt) as anomalous. All other parameters stay consistent with those in the original work [16]."
        },
        {
            "title": "Metrics are",
            "content": "calculated using the model obtained from the final training epoch in all cases. For SimpleNet and SuperSimpleNet, we perform 5 training runs with different random seeds and report the mean and standard deviation of the results. We use the predefined train-test splits for KSDD2, MVTec AD, and VisA, while we use 3-fold cross-validation for SensumSODF, as outlined in the original paper [6]. The test sets in all cases contain both real normal and anomalous images. All of the compared models follow the same evaluation protocol. Above-described hyperparameters were initially adapted from SimpleNet and empirically evaluated for best performance. We include the hyperparameter sensitivity analysis performed by cross-validation on SensumSODF in the Supplementary material. We adhere to standard protocol [4, 6, 11, 18] and fix the hyperparameters for all categories in datasets to enable fair comparison."
        },
        {
            "title": "4.4.1 Results in the fully supervised",
            "content": "setting. SuperSimpleNet is compared with the current state-of-the-art methods for the fully supervised setting: SegDecNet [4], TriNet [6], MaMiNet [15], BGAD [12], PRN [13], and modified SimpleNet [16]. The results of anomaly detection and localisation on SensumSODF and KSDD2 datasets are 9 SensumSODF KSDD SensumSODF KSDD2 Det. Loc. Det. Loc. Det. Loc. Det. Loc. SegDecNet [4] TriNet [6] MaMiNet [15] DSR [10] PRN [13] BGAD [12] SimpleNet [16] Ours 83.4 96.9 - - 80.6 94.3 88.4 ( 1.84) 98.0 ( 0.19) 75.2 - - - 66.0 97.0 89.6 ( 1.14) 95.8 ( 0.28) 93.9 - 96.2 95.2 78.6 92.7 93.5 ( 1.05) 97.8 ( 0.18) 75.0 - - 85.5 48.5 76.5 75.9 ( 2.40) 81.3 ( 0.64) Table 2 Results of fully supervised anomaly detection and localisation on the SensumSODF dataset (AUROC and AUPRO) and KSDD2 (APdet and APloc). presented in Table 2. SuperSimpleNet achieves the best results on SensumSODF, with mean anomaly detection AUROC of 98.0 %, surpassing the previous state-of-the-art by 1.1 percentage points (p.p.), reducing the error by 35.5 %. SuperSimpleNet also achieves state-of-the-art detection APdet of 97.8 % on KSDD2, surpassing the previous best result by 1.6 p.p., reducing the error by 42 %. It demonstrates greater stability than SimpleNet on both datasets, as indicated by the lower reported standard deviations. We hypothesise that the classification heads simple yet powerful design is the primary reason behind the high detection performance. It can efficiently learn to capture more global information in the presence of real and synthetic anomalies, enabling more robust distinction between normal and anomalous samples. The qualitative examples for the fully supervised setting are shown in Figure 5. Compared to related methods, SuperSimpleNet predicts anomaly maps with less noise, as well as more accurate anomaly scores."
        },
        {
            "title": "4.4.2 Results in the weakly supervised",
            "content": "setting. Table 3 displays results of weakly supervised setting for SensumSODF and KSDD2, compared against current state-of-the-art methods SegDecNet [4], TriNet [6], MaMiNet [15], and DRA [14] that support weak supervision. Despite no pixel-level annotations, SuperSimpleNet achieves high detection result of 97.4 % AUROC on SensumSODF. This is only 0.6 p.p SegDecNet [4] TriNet [6] MaMiNet [15] DRA [14] Ours - 91.5 - 90.1 97.4 ( 0.11) - - - - 92.8 ( 2.12) 73.3 - 80.0 89.3 97.2 ( 0.48) 1.0 - - - 47.6 ( 2.46) Table 3 Results of weakly supervised anomaly detection and localisation on the SensumSODF dataset (AUROC and AUPRO), and KSDD2 (AP-det and AP-loc). lower than in fully supervised scenario, while the performance of the best related method TriNet [6] drops by 5.4 p.p. Our method also obtains localisation result of 92.8 % AUPRO, while the other methods either dont produce localisation (DRA) or dont evaluate it (SegDecNet, TriNet and MaMiNet). On KSDD2, SuperSimpleNet achieves strong detection and localisation results with 97.2 % APdet and 47.6 % APloc. Our method again displays good performance in the absence of pixellevel labels, with only 0.6 p.p. reduction compared to the fully supervised setting. In contrast, the performance of related methods like SegDecNet [4] and MaMiNet [15] drops by 22.1 p.p and 16.2 p.p., respectively. Compared to related methods, SuperSimpleNet also excels in localisation performance due to synthetic anomaly generation during training."
        },
        {
            "title": "4.4.3 Results using mixed supervision",
            "content": "Figure 6 showcases the results obtained using mixed supervision. Unlike previous methods, SuperSimpleNet relies less on pixel-level labels, as reflected by superior performance when fewer annotations are available. This allows SuperSimpleNet to outperform TriNets [6] fully supervised performance in weakly supervised mode. Performance using mixed supervision on KSDD2 is displayed in Figure 7. In this case, results of DSR [10] are also reported, where the number of labelled samples indicates the number of fully-labelled anomalous samples used, while the method is unable to utilize the remaining weakly labelled samples. SuperSimpleNet outperforms related methods in detection, even with all samples only weakly 10 Fig. 5 Qualitative comparison of anomaly maps produced in fully supervised setting on SensumSODF and KSDD2. The first two rows display SensumSODF samples (capsule and softgel), while the last row shows KSDD2 examples. Each sample includes the input image, ground truth, and overlaid anomaly maps for each model. The anomaly scores are displayed in the top-right corner of each anomaly map. Fig. 6 Results of anomaly detection (AUROC) and localisation (AUPRO) on the SensumSODF dataset using mixed supervision. The ratio of available pixel-level labels is displayed on the x-axis. Fig. 7 Results for anomaly detection (APdet) and localisation (APloc) on the KSDD2 dataset using mixed supervision. The number of pixel-level labels is displayed on the x-axis. labelled. Other methods require significant number of fully labelled samples to reach performance comparable to that of fully supervised mode. This, in practice, translates into substantial reduction in the labelling efforts, making SuperSimpleNet highly suitable for real-world industrial environments. These results also prompt future research to focus on methods capable of mixed supervision. While detection is the most important task for industrial scenarios, SuperSimpleNet also outperforms SegDecNet [4] in localisation, thanks to the generated synthetic anomalies. However, DSR does outperform SuperSimpleNet in the localisation task. Figure 8 presents qualitative results on SensumSODF and KSDD2. The localisation performance noticeably improves with the availability of more labels, while anomaly scores are already well predicted without any pixel-level labels. This observation aligns with the previously discussed quantitative results. We also include results in numeric tabular format in the Supplementary material. 11 MVTec AD VisA Det. Loc. Det. Loc. AST [27] DSR [10] EfficientAD [11] FastFlow [17] PatchCore [18] DRÆM [9] SimpleNet [16] Ours 98.9 98.1 99.1 96.9 98.7 98.0 97.6 ( 0.40) 98.3 ( 0.14) 81.2 90.8 93.5 92.5 92.7 92.8 90.5 ( 0.75) 91.2 ( 0.14) 94.9 91.8 98.1 93.9 94.3 91.5 91.2 ( 1.08) 93.6 ( 0.77) 81.5 68.1 94.0 86.8 79.7 78.0 88.0 ( 0.87) 87.4 ( 0.98) Table 4 Anomaly detection and localisation (AUROC and AUPRO) on MVTec AD and VisA datasets."
        },
        {
            "title": "4.4.5 Computational efficiency.",
            "content": "computational efficiency Figure 9 showcases results, measured using an NVIDIA Tesla V100S following the benchmark protocol introduced in EfficientAD [11]. SuperSimpleNet achieves an inference time of 9.5 ms and throughput of 262 images per second, making it the fastest method that works in both unsupervised and supervised settings. Fig. 9 Inference time (milliseconds - lower is better) and anomaly detection performance (AUROC - higher is better) for different models, measured on an NVIDIA Tesla V100S. The size of the circles represents the models parameter size."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "In this section, we determine the contribution of each SuperSimpleNet module, investigate its performance in the medical domain, analyse failure cases, and validate performance at higher resolutions. First, we evaluate our contributed modules by systematically removing each component from the architecture and analysing the impact on performance. The results are shown in Table 5. For 12 Fig. 8 Qualitative comparison of anomaly maps produced in the mixed supervision setting on SensumSODF and KSDD2. The input image, the ground truth, and the overlaid anomaly maps for different ratio (or number) of fully labelled images are shown. The anomaly score is shown in the top right corner of each map."
        },
        {
            "title": "4.4.4 Results in the unsupervised",
            "content": "setting. SuperSimpleNet is compared with the current state-of-the-art methods for the unsupervised setting: AST [27], DSR [10], EfficientAD [11], FastFlow [17], Patchcore [18], DRÆM [9] and SimpleNet [16]. Table 4 presents the results on the MVTec AD and VisA datasets. SuperSimpleNet achieves state-of-the-art performance with mean anomaly detection of 98.3% on MVTec AD and anomaly detection AUROC of 93.6% on VisA. While SuperSimpleNet in this setting doesnt perform as efficiently as in supervised settings, it still achieves great results, outperforming the original SimpleNet in anomaly detection and demonstrating more stable results overall. Qualitative results of the unsupervised model on MVTec AD [3] and VisA [5] datasets are presented in Figure 10. SuperSimpleNet generates more accurate anomaly scores than SimpleNet. The anomaly maps are also easier to interpret due to the better separation of high-certainty defective areas from noise. Fig. 10 Qualitative comparison of anomaly maps produced by unsupervised SuperSimpleNet and SimpleNet. The top row presents the input image, followed by the ground truth anomaly mask in the second row. The third and fourth rows display the anomaly maps produced by SimpleNet and SuperSimpleNet, respectively. The anomaly score is indicated in the top right corner of each anomaly map. comparison, we also include the results of the original SimpleNet [16] and version of it with the new synthetic generation strategy. Upscaling module. Removing feature upscaling from the architecture (SSNno upscale) leads to reduction in overall detection performance by 0.7 p.p. The effect on localisation is even more pronounced, with 1.4 p.p. reduction. These findings indicate the importance of feature scaling for achieving accurate final predictions. Classification head. The importance of the classification head is evident from the SSNno cls experiment, where the anomaly score is derived as the maximum value of the predicted anomaly map. This modification results in decrease in supervised detection results by 1.0 p.p., as the removal of the classification head diminishes the networks discriminative power. On the other hand, this omission yields slight improvement in the unsupervised setting. We hypothesise that reducing discriminative power helps mitigate overfitting to synthetic anomalies in this case. Simplicity of classification head. An important part of SuperSimpleNet is powerful but simple classification head. When we replace it with more complex architecture consisting of three convolutional blocks, we obtain the results in SSNcomplex cls line. Results in the supervised detection remain unchanged, but unsupervised detection performance reduces by 2.6 p.p. These observations highlight the important role that the simplicity of the architecture plays in SuperSimpleNet, which still offers strong discriminative powers but simultaneously prevents overfitting. Classification head inputs. We remove the anomaly map (Mo predicted by the segmentation head) as classification head input in the SSNcls no experiment, forcing each head to learn independently. In the supervised setting, the performance slightly decreases (0.4 p.p.), suggesting that SuperSimpleNet does not heavily rely on pixel-level labels if real anomalies are available. However, the results in unsupervised settings significantly degrade, indicating that the anomaly map is crucial for preventing the classification head from overfitting to synthetic anomalies. We also evaluate the impact of using adapted features instead of regular features in the classification head, as seen in the SSNcls adapt experiment. This substitution results in slight but non-negligible drop in detection performance across both settings. Improved training. The SSNold train experiment showcases the impact of upgrading the loss function, adding learning rate scheduler, and adjusting gradient 13 Method Synth. anom. Architecture Fully Sup. Unsup. Avg. SSN SN Upscale Cls. head Train opti. Loss Weight Det. Loc. Det. Loc. Det. Loc. Ours SSNno upscale SSNno cls SSNcomplex cls SSNcls no SSNcls adapt SSNold train SSNoverlap SSNno loss weight SSNSN anom SSNno cls&SN anom SSNcomplex cls&SN anom SSNno anom SN SNSSN anom Overlap Simple Simple Complex No Mo in cls Simple Simple Simple Simple Complex Simple -0.7 -1.3 -0.7 -0.7 -1.3 98.8 97.3 96.0 89.3 97.4 93.3 -1.4 -0.7 -1.5 -1.0 +0.2 +0.1 +0.1 -0.5 +0.1 0.0 +0.2 -2.6 -0.2 -0.4 +0.2 -46.0 -1.8 -23.2 -0.8 0.0 -0.2 +0.1 -0.3 -0.3 -3.6 -1.8 -1.4 -1.8 -1.6 0.0 0.0 -0.2 0.0 -0.1 -0.9 -0.4 -1.9 0.0 -0.2 -0.4 +0.7 -4.4 -0.1 -2.4 -0.7 +0.2 0.0 -1.3 +0.9 -0.5 +0.7 -29.6 -2.9 -15.0 -1.1 -0.3 -2.5 -5.6 -4.1 -7.5 -6. 0.0 -5.4 0.0 0.0 -1.0 -0.6 - -2.1 -4.7 - -3.6 -4.4 - 0.0 -3.1 - -1.6 -1.4 Table 5 Ablation study results on anomaly detection and localisation (AUROC / AUPRO) in the fully supervised setting (average of results on SensumSODF and KSDD2) and the unsupervised setting (average of results on MVTec AD and VisA), as well as the average of both settings. SSN stands for SuperSimpleNet, while SN stands for SimpleNet. Every change in performance is also reflected in colour: blue indicates better result, red indicates worse result, and gray indicates no change in result. flow. These changes have noticeable impact on results, as their absence leads to an overall 1.6 p.p. reduction in detection performance and 3.6 p.p. decrease in localisation performance. We attribute this to more stable training in the final stages. Addition of synthetic anomalies to anomalous regions. We generate synthetic anomalies exclusively on non-anomalous regions when the image is fully labelled. The SSNoverlap experiment shows the results where we remove this constraint. This leads to slight reduction of 0.2 p.p. in detection performance, indicating that retaining as much genuine defect information as possible yields better performance. Unsupervised results remain unchanged, as this modification doesnt change anything in that case. Segmentation loss weighting. As described in Section 3.4, we use segmentation loss weighting, proposed in [4]. As evident from line SSNno loss weight, this approach contributes to detection performance (0.4 p.p.) and to localisation performance (1.9 p.p.). Given its simplicity, this technique could be widely adopted to enhance defect detection performance across supervised methods. Anomaly mask generation. 14 To verify the importance of the new anomaly mask generation strategy, we replace it with the original SimpleNet method, which involves perturbing the entire feature map with Gaussian noise. The results from the SSNSN anom experiment reveal reduction of 0.4 p.p. in the supervised setting and larger decline of 4.4 p.p. in the unsupervised setting. We hypothesise that this drop in performance stems from the incompatibility of this strategy with SuperSimpleNets classification head. further with SSNno cls&SN anom experiment, where we apply SimpleNets strategy without the classification head. Here, we do not observe this decline in the unsupervised setting. However, the performance in the supervised setting decreases by 1.3 p.p. confirm this We Next, we combine SimpleNets strategy with complex classification head in the SSNcomplex cls&SN anom experiment. The performance drop is larger than with simple classification head (29.6 p.p.). This indicates that the complex classification head tends to overfit in the case of SimpleNets strategy, further reinforcing the efficiency of SuperSimpleNets architectural and anomaly mask generation strategy. simplicity Fig. 11 Failure cases with misclassified or low certainty predictions for KSDD2 (top left), SensumSODF (top two rows right), MVTec AD (bottom two rows left), and VisA (bottom two rows right). Each sample includes the input image, ground truth, and overlaid anomaly map with the anomaly scores displayed in the top-right corner. Synthetic anomaly generation strategy. The synthetic significance anomalies of in supervised training is highlighted in the SSNno anom experiment, where only real anomalies were used. The results demonstrate that including synthetic anomalies, even in the presence of real defects, improves detection performance by 0.3 p.p. and localisation performance by 2.5 p.p. It is also important to recognise that synthetic anomalies are the primary mechanism for unsupervised learning and segmentation training in weakly supervised settings. We do not report unsupervised results in this case, as the network fails to learn without synthetic anomalies. Failure Cases. Figure 11 depicts failure cases of SuperSimpleNet. The Figure illustrates that many of the undetected defects are challenging to spot because they closely resemble normal appearances. Additionally, we notice that some false positive predictions (VisA in the bottom right corner) result from debris in the background, which could be seen as an anomaly in some scenarios. Label ablation. To assess the effect of the quantity of available data on the performance, an experiment is conducted on the SensumSODF Softgel category. First, the number of normal samples is increased, representing unsupervised learning with Fig. 12 Results of label ablation study. The anomaly detection performance (AUROC) is shown with respect to the amount of labelled data. Each data regime corresponds to specific supervision paradigm. Results indicate that our method is effective in resource-limited scenarios but enables improvement with additional annotated samples. varying amounts of data. Next, keeping all the normal samples fixed, anomalous samples with image-level labels are progressively added, simulating weakly supervised learning with different amounts of anomalous data. Once all normal and anomalous images are included in the training set, pixel-level labels are added for the anomalous samples. This corresponds to learning with mixed supervision. Finally, the setting becomes 15 Backbone Inference Time Memory [ms] [MB] Param. [M] WideResNet50 Ours WideResNet100 ResNet50 ResNet101 9.5 0.1 18.5 0.4 10.4 0.1 18.3 0. 413.1 48.5 565.2 71.3 332.0 8.0 413.1 0.0 33.7 91.7 17.4 36.4 Sup. Unsup. Det. Loc. Det. Loc. Det. Loc. Avg. 98.8 97.3 0.0 +0.3 -3.5 -0.5 0.0 -0.3 -0.7 +0.1 -1. 96.0 89.3 97.4 93.3 -1.7 -0.1 -1.1 -1.8 -0.4 -1.1 -3.7 -0.3 -2.2 Table 6 Average Anomaly Detection and Localisation results across the supervised and unsupervised training regimes for different feature extractors. In each row, the difference from the baseline model is shown. Performance deterioration is colored in red, performance boost is colored in blue, and no changes are colored in gray. fully supervised when all samples have pixel-level annotations. The results depicted in Figure 12 show that SuperSimpleNet already performs well with limited amount of normal samples in the unsupervised setting. Adding just few anomalous samples with image-level labels significantly boosts performance. However, the best results are only achieved when all the anomalous data has accompanying pixel-level annotations. This clearly demonstrates that SuperSimpleNet consistently improves with every additional available label (either image-level or pixel-level). Backbone ablation. To asses the importance of the chosen feature extractor, we have evaluated our model with several different and standard backbones. The results are presented in Table 6 and show that WideResNet50 offers the best performance and efficiency. Generalisation to medical domain. We evaluate the generalisation capabilities of our method to the medical domain. The Table 7 contains results on medical unsupervised anomaly including histopathology slides and detection, brain MRI images, using the BMAD benchmark [48]. This showcases that SuperSimpleNet also works outside the scope of its development, that is, inspection, and even further industrial strengthens its contribution. Image resolution ablation. In some industrial scenarios, it is required that the model operates with images of higher resolution. This, however, decreases the computational efficiency of the model. While we have implicitly shown that SuperSimpleNet is capable of handling high-resolution images via KSDD2 and SensumSODF, where full-image resolution is used, we have decided to evaluate this tradeoff on VisA, where images of size 256 256 are Brain MRI Histopathology Padim [49] CFlow [50] PatchCore [18] DRÆM [9] SimpleNet [16] Ours 79.0 ( 0.38) 74.8 ( 5.32) 91.6 ( 0.36) 62.4 ( 9.03) 82.5 ( 3.34) 83.0 ( 2.86) 67.2 ( 0.32) 55.7 ( 1.90) 69.3 ( 0.21) 52.3 ( 0.70) 62.4 ( 3.71) 68.7 ( 4.56) Table 7 Results of unsupervised anomaly detection (AUROC) on the Brain MRI, Liver CT and Histopathology datasets. used by default [8, 11] to enable fair comparison. To assess this tradeoff, the resolution has been increased to 512 512. The results in Table 8 show that higher resolution can also be beneficial, helping detect smaller anomalies. We additionally include comparison of inference speed and memory usage across various resolutions in Table 9, demonstrating that our model scales effectively with increased resolution. However, we note that beyond certain point, increasing the resolution offers diminishing returns given the associated increase in computational load. Misclassification analysis. The results of misclassification analysis are presented in Table 10. We find that false positives are relatively rare, and typically occur on images that exhibit minor imperfections, which may plausibly be considered defective. Representative examples are shown in Figure 11 (top-right quadrant - red square). The two capsule samples contain small, unclean regions that are difficult to spot without zooming. The one on the left is labelled as anomalous, while the one on the right is not. The right false positive could reasonably be 16 Det. Loc. 256 256 512 512 93.6 95.4 87.4 91.4 Table 8 Average Anomaly Detection and Localisation results ( AUROC / AUPRO ) for VisA dataset at different resolutions. Increased resolution in the presence of tiny anomalies in VisA leads to an increase in performance. 256 512 512 1024 1024 Inference T. [ms] FPS [img/s] Memory [MB] 9.5 0.1 105 0.63 413.1 48.5 16.2 0.3 61.6 1.3 542.0 27.3 46.0 0.1 21.7 0.0 1551.6 27. Table 9 Inference time, frames per second (FPS), and memory usage for different input image resolutions. SuperSimpleNet scales well with increased resolution, maintaining fast operation with 21 images per second and reasonable memory usage of 1.5 GB even when processing megapixel (1024 1024) image. interpreted as an anomaly based on visual similarity to an actual defect case in the right capsule. With thorough inspection of data, we found that almost all false positives exhibit this property. This highlights labelling ambiguity, which is often rooted in subjective interpretation by expert annotators, known challenge also discussed by the author of the dataset in Section 5.3 of [6]. For false negatives, our quantitative findings (summarized in Table 10, top two rows) show that the dominant factor is anomaly size. Most FNs correspond to extremely small defects, typically covering less than 1 % of the image area. This limitation arises from the limited resolution of the input features. However, we show in the ablation study (shown in Table 8) that increasing input resolution leads to performance improvements, suggesting that false negatives caused by tiny defects can be mitigated by higher-resolution processing. To analyse the effect of synthetic anomalies on misclassification, we compare the number of false positives and false negatives with and without synthetic data in Table 10. The top two rows show results with real and synthetic data, and the bottom two rows show results without synthetic data. This comparison focuses on datasets where real anomalies are present in the training set (e.g., KSDD2 and SensumSODF), since MVTec AD and VisA follow the unsupervised protocol and are trained using only synthetic anomalies. The results confirm that the addition of synthetic data helps reduce misclassification. On SensumSODF, the number of false positives decreases by 11 samples (a 21 % reduction) when synthetic anomalies are used. In case of KSDD2, the false positive count decreases from 6 to only 4 (a 33 % reduction). This suggests that synthetic anomalies act as regulariser, exposing the model to wider variety of anomalies and helping it better distinguish rare but benign patterns from true defects. We also observe modest reduction in the number of false negatives. While this effect is less pronounced, it is expected, as false negatives are primarily caused by very small defects, which are not explicitly addressed by synthetic anomalies."
        },
        {
            "title": "5 Conclusion",
            "content": "We proposed SuperSimpleNet, novel discriminative anomaly detection model tailored to meet industrial requirements for high performance, speed, and flexibility across diverse supervision scenarios involving labels at varying levels of annotation. As the first unified model to excel in unsupervised, weakly supervised, mixed, and fully supervised settings, SuperSimpleNet uniquely utilises all available training dataincluding images and different types of annotationsto deliver superior and consistent results. SuperSimpleNet demonstrates state-of-the-art results across all supervision scenarios. In the supervised setting, the method is evaluated on two well-established benchmarks, SensumSODF and KSDD2, achieving 98.0% AUROC on SensumSODF and 97.8% APdet on KSDD2. On SensumSODF and KSDD2, SuperSimpleNet surpasses all previous methods by 1.1 % and 1.6 %, respectively. Weakly supervised setting is evaluated on the same two datasets, achieving 97.4% AUROC on SensumSODF and 97.2 APdet on KSDD2, outperforming previous methods on SensumSODF and KSDD2 by 5.9 % and 7.9 %, respectively. Similarly, in the mixed supervision setting, it consistently outperformed previous methods under varying levels of labelled data. It also achieves state-of-the-art results in the unsupervised setting on two well-established benchmarks, MVTec AD 17 Dataset TP TN FP FN TP mean ano. ratio FN mean. ano. ratio Ours Without Synth. Ano. SensumSODF 432 95 KSDD SensumSODF 430 94 KSDD2 1641 890 1630 888 41 4 52 6 66 68 16 1.92 3.38 1.98 3.38 0.72 0.98 0.38 1.11 Table 10 Misclassification analysis. For each dataset and setup, we provide count of individual types of classification as well as the mean ratio of image covered by an anomalous region for true positive and false negative predictions. We can observe two things: false negative predictions predominantly come from very small anomalies covering less than 1 % of the image. The second observation is that synthetic anomalies help reduce misclassification. and VisA, with 98.3% and 93.6% AUROC, respectively. It achieves these results with highly efficient performance, achieving an inference time of just 9.5 ms and throughput of 262 images per second. The main limitation of our method is the dependence on the pretrained feature extractor. If the extracted features do not hold the necessary information to sufficiently model the normality of the object, it will deteriorate downstream anomaly detection performance. Additionally, although we show that the hyperparameters are robust across wide variety of models, they may need adjustments for some specific real-world cases. Another limitation arises in the detection of very tiny anomalies, where higher resolution is required for successful operation. Our results strongly indicate that practical anomaly detection applications should utilise all available information, including images and labels at different levels of annotation, to achieve optimal performance. The findings also underscore the promise of combining knowledge from unsupervised and supervised approaches to enhance detection capabilities. Furthermore, they highlight the advantages of mixed supervision as an effective trade-off between performance and annotation effort. By unifying diverse supervision paradigms while maintaining strong efficiency and reliability, SuperSimpleNet offers promising approach for developing models that address the complex needs of real-world industrial applications."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was in part supported by ARIS research projects L2-3169 (MV4.0), GC-0001 (AI4Sci), and J2-60055 (MUXAD), research programme P20214, and SLING (ARNES, EuroHPC Vega)."
        },
        {
            "title": "References",
            "content": "[1] Yang, C., Lv, D., Tian, X., Wang, C., Yang, P., Zhang, Y.: LEIS-net: Lightweight and Efficient Industrial Surface Defect Detection Network. Journal of Intelligent Manufacturing, 118 (2025) [2] Tabernik, D., ˇSela, S., Skvarˇc, J., Skoˇcaj, D.: Segmentation-Based Deep-Learning Surface-Defect Detection. Approach Journal of Intelligent Manufacturing 31(3), 759776 (2020) for [3] Bergmann, P., Batzner, K., Fauser, M., Sattlegger, D., Steger, C.: The MVTec Anomaly Detection Dataset: Comprehensive RealWorld Dataset for Unsupervised Anomaly Detection. International Journal of Computer Vision 129, 10381059 (2021) [4] Boˇziˇc, J., Tabernik, D., Skoˇcaj, D.: Mixed Supervision for Surface-Defect Detection: From Weakly to Fully Supervised Learning. Computers in Industry 129, 103459 (2021) [5] Zou, Y., Jeong, J., Pemula, L., Zhang, D., Dabeer, O.: SPot-the-Difference SelfSupervised Pre-Training for Anomaly Detection and Segmentation. In: Proceedings of the European Conference on Computer Vision, pp. 392408 (2022) [6] Raˇcki, D., Tomaˇzeviˇc, D., Skoˇcaj, D.: Detection of Surface Defects on Pharmaceutical Solid Oral Dosage Forms With Convolutional Neural Networks. Neural Computing and Applications 34, 631650 (2021) 18 [7] Boˇziˇc, J., Tabernik, D., Skoˇcaj, D.: End-toend Training of Two-Stage Neural Network for Defect Detection. In: Proceedings of the International Conference on Pattern Recognition, pp. 56195626 (2021) [8] Fuˇcka, M., Zavrtanik, V., Skoˇcaj, D.: Transfusiona Transparency-Based Diffusion Model for Anomaly Detection. In: Proceedings of the European Conference on Computer Vision, pp. 91108 (2025) [9] Zavrtanik, V., Kristan, M., Skoˇcaj, D.: Dræm-A Discriminatively Trained Reconstruction Embedding for Surface Anomaly Detection. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 83308339 (2021) [10] Zavrtanik, V., Kristan, M., Skoˇcaj, D.: DSR Dual Subspace Re-Projection Network for Surface Anomaly Detection. In: Proceedings of the European Conference on Computer Vision, pp. 539554 (2022) [11] Batzner, K., Heckler, L., Konig, R.: EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level Latencies. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 128 138 (2024) [12] Yao, X., Li, R., Zhang, J., Sun, J., Zhang, C.: Explicit Boundary Guided Semi-PushPull Contrastive Learning for Supervised Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2449024499 (2023) [13] Zhang, H., Wu, Z., Wang, Z., Chen, Z., Jiang, Y.-G.: Prototypical Residual Networks for Anomaly Detection and Localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1628116291 (2023) [14] Ding, C., Pang, G., Shen, C.: Catching Both Gray and Black Swans: Open-Set Supervised Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7388 7398 (2022) [15] Luo, X., Li, S., Wang, Y., Zhan, T., Shi, X., Liu, B.: MaMiNet: Memory-Attended MultiInference Network for Surface-Defect Detection. Computers in Industry 145, 103834 (2023) [16] Liu, Z., Zhou, Y., Xu, Y., Wang, Z.: SimpleNet: Simple Network for Image Anomaly Detection and Localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2040220411 (2023) [17] Yu, J., Zheng, Y., Wang, X., Li, W., Wu, Y., Zhao, R., Wu, L.: FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows. In: arXiv Preprint arXiv:2111.07677 (2021) [18] Roth, K., Pemula, L., Zepeda, J., Scholkopf, B., Brox, T., Gehler, P.: Towards Total Recall In Industrial Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1431814328 (2022) [19] Rolih, B., Fuˇcka, M., Skoˇcaj, D.: SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection. In: Proceedings of the International Conference on Pattern Recognition, pp. 4765 (2024) [20] Zavrtanik, V., Kristan, M., Skoˇcaj, D.: Reconstruction by Inpainting for Visual Anomaly Detection. Pattern Recognition 112, 107706 (2021) [21] Bergmann, P., Lowe, S., Fauser, M., Sattlegger, D., Steger, C.: Improving Unsupervised Defect Segmentation by Applying Structural Similarity to Autoencoders. In: Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, pp. 372 380 (2019) [22] Akcay, S., Atapour-Abarghouei, A., Breckon, T.P.: Ganomaly: Semi-Supervised Anomaly 19 Detection via Adversarial Training. In: Proceedings of the Asian Conference on Computer Vision, pp. 622637 (2019) [23] Pirnay, J., Chai, K.: Inpainting Transformer for Anomaly Detection. In: Image Analysis and Processing, pp. 394406 (2022) [24] Wyatt, J., Leach, A., Schmon, S.M., Willcocks, C.G.: AnoDDPM: Anomaly Detection With Denoising Diffusion Probabilistic Models Using Simplex Noise. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 650656 (2022) [25] He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning For Image Recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778 (2016) [26] Deng, H., Li, X.: Anomaly Detection via Reverse Distillation From One-Class Embedding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 97379746 (2022) [30] Zhang, X., Xu, M., Zhou, X.: Realnet: Feature Selection Network With Realistic Synthetic Anomaly for Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1669916708 (2024) [31] Rolih, B., Ameln, D., Vaidya, A., Akcay, S.: Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3866 3875 (2024) [32] Fisne, A., Kalay, A., Eken, S.: Fast and Efficient Computing for Deep Learning-Based in Lightweight Defect Detection Models Devices. Journal of Intelligent Manufacturing, 116 (2024) [33] Ruff, L., Vandermeulen, R.A., Gornitz, N., Binder, A., Muller, E., Muller, K.-R., Kloft, M.: Deep Semi-Supervised Anomaly Detection. In: Proceedings of the International Conference on Learning Representations (2020) [27] Rudolph, M., Wehrbein, T., Rosenhahn, B., Wandt, B.: Asymmetric Student-Teacher Networks for Industrial Anomaly Detection. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 25922602 (2023) [34] Baitieva, A., Hurych, D., Besnier, V., Bernard, O.: Supervised Anomaly Detection for Complex Industrial Images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1775417762 (2024) [28] Zhang, X., Li, S., Li, X., Huang, P., Shan, J., Chen, T.: DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 39143923 (2023) [29] Li, C.-L., Sohn, K., Yoon, J., Pfister, T.: Cutpaste: Self-Supervised Learning for Anomaly Detection and Localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9664 9674 (2021) [35] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning Deep Features for Discriminative Localization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921 2929 (2016) [36] Zhang, J., Su, H., Zou, W., Gong, X., Zhang, Z., Shen, F.: CADN: Weakly Supervised Learning-Based Category-Aware Object Detection Network for Surface Defect Detection. Pattern Recognition 109, 107571 (2021) [37] Pang, G., Ding, C., Shen, C., Hengel, A.v.d.: Explainable Deep Few-Shot Anomaly Detection With Deviation Networks. In: arXiv 20 Dollar, P.: Focal Loss for Dense Object Detection. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 29802988 (2017) [47] Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: Proceedings of the International Conference on Learning Representations, pp. 40614078 (2019) [48] Bao, J., Sun, H., Deng, H., He, Y., Zhang, Z., Li, X.: BMAD: Benchmarks for Medical Anomaly Detection. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4042 4053 (2024) [49] Defard, T., Setkov, A., Loesch, A., Audigier, R.: Padim: Patch Distribution Modeling Framework For Anomaly Detection And Localization. In: Proceedings of the International Conference on Pattern Recognition, pp. 475489 (2021). Springer [50] Gudovskiy, D., Ishizaka, S., Kozuka, K.: CFLOW-AD: Unsupervised Real-Time Anomaly Detection With Localization via Conditional Normalizing Flows. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 98107 (2022) Preprint arXiv:2108.00462 (2021) [38] Wu, X., Wang, T., Li, Y., Li, P., Liu, Y.: CAM-Based Weakly Supervised Method for Surface Defect Inspection. IEEE Transactions on Instrumentation and Measurement 71, 110 (2022) [39] Jiang, X., Feng, J., Yan, F., Lu, Y., Fa, Q., Zhang, W., Xu, M.: Foregroundbackground Separation Transformer for Weakly Supervised Surface Defect Detection. Journal of Intelligent Manufacturing 36, 42174232 (2025) [40] Xu, L., Lv, S., Deng, Y., Li, X.: Weakly Supervised Surface Defect Detection Based IEEE on Convolutional Neural Network. Access 8, 4228542296 (2020) [41] Raˇcki, D., Tomaˇzeviˇc, D., Skoˇcaj, D.: Coupling of Unsupervised and Supervised Deep Learning-Based Approaches for Surface Anomaly Detection. Journal of Electronic Imaging 33, 3120731207 (2024) [42] Zagoruyko, S., Komodakis, N.: Wide Residual Networks. In: Procedings of the British Machine Vision Conference, pp. 8718712 (2016). British Machine Vision Association [43] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: ImageNet: Large-Scale Hierarchical Image Database. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255 (2009) [44] Heckler., L., Konig., R.: Feature Selection for Unsupervised Anomaly Detection and Localization Using Synthetic Defects. In: Proceedings of the International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, pp. 154 165 (2024) [45] Perlin, K.: An Image Synthesizer. In: ACM Siggraph Computer Graphics, vol. 19, pp. 287296 (1985) [46] Lin, T.-Y., Goyal, P., Girshick, R., He, K.,"
        },
        {
            "title": "Appendix B Hyperparameter",
            "content": "of mixed supervision sensitivity analysis In this section, we also provide the numeric results for the experiments with mixed supervision. Results for SensumSODF are in Table A1 and A2, and for KSDD2 in Table A3 and A4. Model unsup SimpleNet 0 - 0.2 - 0.4 - 0.6 - 0.8 - Ours TriNet 91.5 97.4 ( 0.11) 92.2 97.8 ( 0.28) Table A1 Results of anomaly detection with mixed supervision on the SensumSODF dataset (AUROC) 93.9 97.8 ( 0.10) 95.2 97.8 ( 0.11) 92.5 97.8 ( 0.19) 71.1 ( 3.77) - 89.9 ( 0.82) 1 88.4 ( 1.84) 96.9 98.0 ( 0.19) To further support future reproducibility and transparency, we present the results of hyperparameter sensitivity analysis in Figure B1. This analysis was performed by cross-validation on SensumSODF. Our model involves five primary hyperparameters: batch size, learning rate, number of epochs, Perlin mask threshold, and Gaussian noise standard deviation (σ). The results demonstrate that the hyperparameter configuration performs consistently well across the tested ranges, with some values yielding slightly better results. Model unsup SimpleNet 0 - 0.2 - 0.4 - 0.6 - 0.8 - Ours TriNet - 92.8 ( 2.12) - 94.5 ( 0.65) Table A2 Results of anomaly localisation with mixed supervision on the SensumSODF dataset (AUPRO) - 94.9 ( 0.30) - 95.1 ( 0.52) - 95.7 ( 0.49) 34.7 ( 1.02) - 89.5 ( 0.31) 1 89.6 ( 1.14) - 95.8 ( 0.28) Model unsup SimpleNet DSR SegDecNet MaMiNet Ours 45.4 ( 22.29) 87.2 - - 55.8 ( 9.48) 0 - 16 - 53 - 126 - - 73.3 80.0 97.2 ( 0.48) 91.4 83.2 89.7 97.3 ( 0.35) 94.6 89.1 92.3 97.7 ( 0.63) - 92.4 94.1 97.4 ( 0.46) 246 93.5 ( 1.05) 95.2 95.4 96.2 97.8 ( 0.18) Table A3 Results of anomaly detection with mixed supervision on KSDD2 (APdet). Fig. B1 Hyperparameter sensitivity analysis, performed via cross-validation on SensumSODF. The results show that our method is relatively robust to hyperparameter selection, as performance remains stable across reasonable range around the selected values. Model SimpleNet DSR SegDecNet MaMiNet Ours unsup 0.4 ( 0.05) 61.4 - - 45.8 ( 1.81) 0 - 16 - 53 - 126 - - 1.0 - 47.6 ( 2.46) 71.2 45.1 - 59.1 ( 1.50) 81.6 52.2 - 68.0 ( 1.37) - - - 76.5 ( 0.62) 246 75.9 ( 2.40) 85.5 67.6 - 81.3 ( 0.64) Table A4 Results of anomaly localisation with mixed supervision on KSDD2 (APloc)."
        }
    ],
    "affiliations": [
        "Faculty of Computer and Information Science, University of Ljubljana, Veˇcna Pot 113, Ljubljana, 1000, Slovenia"
    ]
}