{
    "paper_title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination",
    "authors": [
        "Mingqi Wu",
        "Zhihao Zhang",
        "Qiaole Dong",
        "Zhiheng Xi",
        "Jun Zhao",
        "Senjie Jin",
        "Xiaoran Fan",
        "Yuhao Zhou",
        "Yanwei Fu",
        "Qin Liu",
        "Songyang Zhang",
        "Qi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reasoning capabilities of large language models (LLMs) have been a longstanding focus of research. Recent works have further enhanced these capabilities using reinforcement learning (RL), with many new methods claiming significant improvements with minimal or no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are mostly reported on the Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500, AMC, and AIME, while failing to achieve similar gains on other models like Llama, which warrants further investigation. Our analysis shows that although Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on large-scale web corpora makes it vulnerable to data contamination in popular benchmarks. As a result, results derived from these benchmarks may be unreliable. To address this, we introduce a generator that produces fully synthetic arithmetic problems of arbitrary length and difficulty, yielding a clean dataset we call RandomCalculation. Using these leakage-free datasets, we show that only accurate reward signals consistently improve performance, while noisy or incorrect signals do not. We advocate for evaluating RL methods on uncontaminated benchmarks and across diverse model families to ensure trustworthy conclusions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 3 5 0 1 . 7 0 5 2 : r Preprint. Work in progress REASONING OR MEMORIZATION? UNRELIABLE RESULTS OF REINFORCEMENT LEARNING DUE TO DATA CONTAMINATION Mingqi Wu1, Zhihao Zhang1 2 , Qiaole Dong1 , Zhiheng Xi1, Jun Zhao1, Senjie Jin1, Xiaoran Fan1, Yuhao Zhou1, Yanwei Fu1, Qin Liu3, Songyang Zhang2, Qi Zhang1,2 1 Fudan University 2 Shanghai Artificial Intelligence Laboratory 3 University of California, Davis {qz}@fudan.edu.cn {qinli}@ucdavis.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The reasoning capabilities of large language models (LLMs) have been longstanding focus of research within the community. Recent works have further enhanced these capabilities by reinforcement learning (RL), and numerous novel methods have been proposed, claiming to achieve significant improvements with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance reasoning performance. However, these breakthroughs are predominantly reported on the Qwen2.5 model family and evaluated on well-known mathematical benchmarks such as MATH500, AMC, and AIME, while often failing to yield comparable gains on other model families such as Llama, which warrants more in-depth investigation. In this work, our empirical analysis shows that, despite the Qwen 2.5 series attaining superior mathematical-reasoning performance relative to other models, its pretraining on massive web-scale corpora leaves it vulnerable to data contamination in widely used benchmarks (e.g., MATH-500). Consequently, experimental results derived from contaminated benchmarks on the Qwen2.5 series may be unreliable. To obtain trustworthy evaluation signals, we introduce generator that creates fully synthetic arithmetic problems of arbitrary length and difficulty, yielding clean datasets we call RandomCalculation. Using these leakage-free datasets, we further show that under the RL protocol, only accurate reward signals yield steady improvements that surpass the models performance ceiling in mathematical reasoning, whereas noisy or incorrect rewards do not. Thus, we recommend that future studies evaluate on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, advances in reinforcement learning (RL) techniques have markedly strengthened the reasoning abilities of large language models (LLMs). Flagship systems, including OpenAIs o1 (OpenAI, 2024b; 2025; 2024a), DeepSeek-R1 (DeepSeek-AI et al., 2025), and QwQ (Qwen Team, 2024; 2025), now match or surpass human-level accuracy on variety of challenging benchmarks. Among the contenders that excel on these mathematical benchmarks, the open-source Qwen family (Yang et al., 2024a;b; 2025), spanning parameters from 0.5B to 72B and maximally pre-trained on up to 36T high-quality tokens, yields state-of-the-art results in language understanding, mathematics, programming, and preference alignment. Equal contributions. Corresponding author. 1 Preprint. Work in progress Example from MATH-500 Problem: For how many positive integers > 1 is it true that 224 is perfect nth power? Answer:"
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: For how many positive integers > 1 is Qwen2.5-7B Output Llama3.1-8B Output it true that 224 is perfect nth power? To determine for how many positive integers > 1 the number 224 is perfect n-th power... First, we need to find... ... Lets count these values. There are 7 such values... ... The final answer is 7 . the set of positive divisors of (including 1 and n) less than 50 ? (A) 15 (B) 16 (C) 17 (D) 18 (E) The number of positive divisors of is given by... ... The number of factors of each of these numbers is... ... The answer is (B) 16 Figure 1: Memorization Example of Qwen2.5 on MATH-500. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration (see Table 1). The Qwen2.5-7B model accurately reproduces the original question verbatim and, moreover, generates complete and precise chain of reasoning that yields the correct answer. In contrast, Llama3.1-8B produces an incorrect completion and ultimately arrives at an incorrect answer. Within this landscape, mathematical reasoning has emerged as particularly discriminative test bed because it demands precise symbolic manipulation and multi-step logical deduction. Standard suites such as MATH-500 (Hendrycks et al., 2021), AIME (Li et al., 2024), AMC (Li et al., 2024) and Minerva Math (Lewkowycz et al., 2022) require models to parse natural-language problem statements, uncover the latent mathematical structure, and generate exact numeric answers. Recent work has further enhanced these capabilities by reinforcement learning with verifiable rewards (RLVR) (DeepSeek-AI et al., 2025): rule-based reward functions that return 1 when predicted answer equals ground truth and 0 otherwise. Because the reward is computed analytically, RLVR removes the need for separate learned reward model, lowering computational cost while providing an analytically precise training signal, especially attractive for domains like mathematics, where solutions are unambiguous. Although RL nominally depends on accurate reward signals to guide effective training, recent studies (Zuo et al., 2025; Shao et al., 2025) report that even random or partially incorrect rewards can lift Qwens performance on standard math benchmarks, whereas the same procedures offer little or no benefit to Llama3.1-8B (Dubey et al., 2024) models. To understand why these seemingly rewardagnostic variants help Qwen but not Llama, we undertake systematic comparison of the two model families under identical training protocols. We consider two working hypotheses to explain this phenomenon. (i) Data Contamination: Considering Qwen2.5 is pre-trained on massive web-scale corpora from the Internet, including GitHub repositories that store benchmark problems alongside their official solutions. If segments of the evaluation benchmarks leaked into Qwens pre-training corpus, spurious rewards could cue the model to retrieve memorized answers rather than acquire new reasoning skills. (ii) Strong Baseline Math Skills: Qwens pre-training endows it with higher mathematical capacity than Llama, so even noisy policy-gradient updates appear to help on MATH500; however, if strong skills are the real driver, the same random-reward trick should still work 2 Preprint. Work in progress on clean benchmark. Distinguishing between these possibilities requires both leakage audit and rigorously out-of-distribution (OOD) RLVR evaluation, which we present in Section 4.2 and Section 4.3. To assess the extent of potential data contamination in popular mathematical benchmarks, we evaluate two indicators: partial-prompt completion rate (can the model reconstruct the tail of problem?) and partial-prompt answer accuracy (can the model give the correct answer with an incomplete problem?). As Figure 1 illustrates, we observe that Qwen can accurately complete the problem statement and provide the correct answer, whereas Llama does not. Specifically, prompting Qwen2.5-Math-7B with the first 60% of each MATH-500 question, we find that it regenerates the remaining 40% with 54.60% exact-match rate and still answers 53.6% of these incomplete problems correctly. Llama3.1-8B, in contrast, scores 3.8% and 2.4% on both metrics. Crucially, on the newly released LiveMathBench (version 202505) (Liu et al., 2024), post-hoc benchmark compiled after the release of the Qwen2.5 model family, Qwens completion rate drops sharply to 0.0%, consistent with Llamas 0.0%. Its partial-prompt answer accuracy also falls to just 2.0%, comparable to Llamas 1.0%. These results confirm that the earlier gains on MATH-500 may stem from memorized content rather than genuine reasoning. Hence, results derived from MATH-500 and similar datasets for Qwen models should be interpreted with caution. Building on this evidence, we attribute that data contamination is the chief factor behind the seemingly magical success of random-reward or few-shot RLVR variants on Qwen models. To test this claim, we create fully fresh benchmark (representative examples shown in Figure 2): We use an automatic generator to construct arithmetic expressions of arbitrary length with uniformly random operands and operators, guaranteeing that every instance post-dates the public release of Qwen2.5. zero-shot evaluation on this benchmark shows no memorization: the accuracy of Qwen2.5 models declines monotonically with the number of computation steps, leaving ample room for improvement. To isolate the effect of reward quality, we next trained Qwen2.5-Math-7B under the standard RLVR protocol on two leakage-free subsets, respectively. The outcome is unambiguous: Correct rewards deliver consistent performance gains, surpassing the models performance ceiling; In contrast, random rewards make training highly unstable, yielding no reliable improvement, while inverse rewards rapidly erode the models mathematical-reasoning ability. These results rule out the Strong Baseline Math Skills explanation and directly implicate Data Contamination: once leakage is removed, the prior gains evaporate. The contributions of our work can be summarized as follows: We conduct systematic leakage audit of widely used math benchmarks and demonstrate that Qwens sudden performance surge on MATH-500 under spurious-reward RLVR is chiefly attributable to unfair evaluation protocol, data contamination, and memorization rather than its strong math skills. We design an automatic generator that creates arbitrarily long arithmetic expressions and corresponding out-of-distribution evaluation protocol. Zero-shot results on this clean suite expose the absence of memorization and place Qwen and Llama on equal footing, enabling fair assessment of mathematical reasoning. Using the leakage-free dataset, we conduct controlled RLVR experiments and demonstrate that only correct rewards yield stable performance gains, whereas random or inverse rewards provide no benefit, thereby highlighting the central role of reward fidelity. We recommend that future work rely on uncontaminated benchmarks or test various model series to draw trustworthy conclusions about RL-related methods."
        },
        {
            "title": "2.1 REINFORCEMENT LEARNING ON QWEN2.5 FOR MATHEMATICAL REASONING",
            "content": "learning (RL) can amplify the growing body of work investigates how reinforcement mathematical-reasoning skills of the open-source Qwen2.5 family. Early studies used explicit verifiable rewards that score an answer as 1 / 0 by exact numerical agreement. Test-time RL (TTRL) (Zuo et al., 2025) applies this signal on-the-fly during inference and yields sizeable gains on MATH500 and AIME2024. Subsequent efforts pursue extreme data efficiency: single labeled exam3 Preprint. Work in progress 5-Step Calculation Problem: Evaluate this LaTeX numerical expression step-by-step and give the final value within boxed{}: 452 94 6 /( 76 4 / 19 5 353) + Answer: 8586.00036544592 10-Step Calculation Problem: Evaluate this LaTeX numerical expression step-by-step and give the final value within boxed{}: 94 + 732 (62 10) (cid:32) (cid:33) 65 9 +47 49 81 7 622 (cid:19) (cid:18) 41 6 + 12 Answer: 6490.42220471333 Figure 2: Examples of RandomCalculation dataset. ple can suffice to boost performance (Wang et al., 2025a; see also the confidence-guided RLSC scheme of Li et al., 2025) or even to learn from an unlabeled instance via entropy minimization (Gao et al., 2025). parallel line replaces external supervision with intrinsic signals derived from the model itself. Entropy-based objectivesRENT (Prabhudesai et al., 2025) and EM-RL (Agarwal et al., 2025)reward low-entropy output distributions, while other methods such as INTUITOR (Zhao et al., 2025b) and online self-training (Shafayat et al., 2025) rely on self-consistency or selfcertainty as feedback. These approaches report large jumps on Qwen2.5-Math-7B, occasionally matching or surpassing stronger baselines (e.g. GRPO (Shao et al., 2024), RLOO (Ahmadian et al., 2024)). Other variants explore noisy or partial rewards (Lv et al., 2025) and label-free RLVR paradigm dubbed ABSOLUTE ZERO (Zhao et al., 2025a). The generality of these gains, however, has been questioned. Zhang et al. (2025) observe an underconfidence-to-overconfidence trajectory in RLIF(Reinforcement Learning from Internal Feedback): accuracy rises early but later collapses as policy entropy shrinks. Shao et al. (2025) further shows that random or incorrect rewards can still improve Qwen2.5 yet fail to transfer to Llama or OLMo (OLMo et al., 2025), suggesting model-specific idiosyncrasies. Together, these findings indicate that while RL can unlock strong mathematical competence in Qwen2.5, its effectiveness hinges on careful reward design, entropy control, and underlying model capacity."
        },
        {
            "title": "2.2 THE IMPACT OF PRETRAINING CORPORA ON THE REASONING ABILITY",
            "content": "The choice of pretraining corpora plays crucial role in shaping the reasoning abilities of LLMs, particularly in mathematical domains. Several math-specific datasets (Paster et al., 2024; Han et al., 2024; Wang et al., 2024; Allal et al., 2025) have been proposed and have been shown to significantly enhance performance on relevant benchmarks. As highlighted in prior studies, models such as Qwen and Llama, despite having comparable size, exhibit notable differences in mathematical reasoning capabilities. For example, Qwen models tend to benefit more readily from reinforcement learning, whereas Llama models often show limited improvement under similar conditions. Wang et al. (2025b) found that mid-training Llama models on high-quality mathematical corpora substantially improve their performance, both at the base level and after reinforcement learning. These observations suggest that differences in pretraining data may be key factor influencing models receptiveness to reinforcement learning, and warrant further investigation. 4 Preprint. Work in progress"
        },
        {
            "title": "3.1 MODEL SELECTION",
            "content": "Prior research on mathematical reasoning with large language models has focused predominantly on the Qwen-2.5 family (Yang et al., 2024a;b). Accordingly, we center our study on four representative checkpoints from this series: Qwen2.5-7B, Qwen2.5-7B-Instruct, Qwen2.5-Math-7B, and Qwen2.5-Math-7B-Instruct. For controlled comparison, we also evaluate Llama3.1-8B and Llama3.1-8B-Instruct (Dubey et al., 2024), which possess comparable parameter counts and thus help isolate model-specific differences in behavior."
        },
        {
            "title": "3.2 MEMORIZATION CAPABILITY",
            "content": "We provide the following details on evaluating the models memorization capability. Performance is assessed with two metrics: Partial-Prompt Completion Rate and Partial-Prompt Answer Accuracy."
        },
        {
            "title": "3.2.1 PARTIAL-PROMPT COMPLETION RATE",
            "content": "ROUGE Metric. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004) is family of metrics commonly used to evaluate automatic summarization and text generation systems. ROUGE score measures the overlap between the generated text and one or more reference texts, focusing primarily on recall. The most frequently used variants include: ROUGE-N: Measures n-gram overlap. ROUGE-1 and ROUGE-2 refer to unigram and bigram recall respectively. ROUGE-L: Measures the longest common subsequence (LCS) between the generated and reference text, capturing fluency and sentence-level structure. ROUGE-Lsum: variant of ROUGE-L adapted for multi-sentence summarization evaluation. In this work, we utilize average ROUGE-L score to measure the models ability to reconstruct the remaining parts of problem based on partial prefixes, serving as an indicator of the models memorization capacity. Exact Match (EM). EM is binary accuracy metric that checks whether the models continuation exactly reproduces the reference. For each instance, let denote the model-generated continuation if = y, EM = 1; otherwise EM = 0. We first and denote the ground-truth continuation. compute the ROUGE -L score between the and the y, then assign EM(y, y) = (cid:40)1, if ROUGE-L(y, y) = 1, 0, otherwise. The final EM score is obtained by averaging over all test instances. Because ROUGE-L = 1 implies an exact, character-level match, higher EM values directly indicate greater proportion of partialprompts that the model recalls verbatim."
        },
        {
            "title": "3.2.2 PARTIAL-PROMPT ANSWER ACCURACY",
            "content": "Answer-Match Accuracy. For each question, we supply the model with only truncated prompt (e.g., the first 60% of the original problem) and allow it to generate an unconstrained continuation. After generation, we check whether the completion contains the ground-truth answer; if so, the instance is scored as correct. Answer-Match Accuracy is defined as the fraction of prompts for which the models continuation embeds the correct answer. high accuracy indicates that the model frequently recovers the answer even from partial problem, which in turn may signal data contamination."
        },
        {
            "title": "3.3 RLVR-BASED EVALUATION",
            "content": "The GRPO Algorithm. We adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our RLVR algorithm. Formally, for each question q, GRPO samples group of outputs 5 Preprint. Work in progress Figure 3: Accuracy trends on the MATH-500 dataset for Qwen2.5-Math-7B, Qwen2.5-Math-7BInstruct and Llama3.1-8B-Instruct trained with RLVR under different reward signals. Greedy and pass@16 scores are reported without template. {o1, o2, , oG} from the old policy πθold and then optimizes the policy model by maximizing the following objective: JGRP O(θ) = E[q (Q), {oi}G (cid:88) oi (cid:88) (cid:26) 1 1 oi i=1 t= min i=1 πθold(Oq)] (cid:20) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) ˆAi,t, clip (cid:18) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) , 1 ϵ, 1 + ϵ (cid:19) (cid:21) ˆAi,t βDKL [πθπref ] (cid:27) , (1) where ϵ and β are hyperparameters, and ˆAi,t is the advantage calculated based on the relative rewards of the outputs inside each group only. Reward Function for RandomCalculation. The ground-truth answers to our randomly generated arithmetic problems often contain high-precision decimals. When the standard RLVR framework supplies only binary feedback (0 or 1), the model almost never receives positive reinforcement, making training highly unstable and prone to divergence. To address this limitation, we design continuous reward that ranges from 0 to 1 and jointly penalizes both absolute and relative errors between the model prediction and the reference answer. This denser signal greatly stabilizes reinforcement learning. Let be the model output, be the reference answer, and ϵ = 106 be small constant for numerical stability. The reward is computed as: = 1 0.5 min (a b, 1) (cid:125) (cid:123)(cid:122) absolute distance (cid:124) (cid:124) 0.5 min (cid:18) b + ϵ (cid:123)(cid:122) relative distance (cid:19) , 1 (cid:125) (2) Reward Design. Based on the configuration of Shao et al. (2025), we consider the following rewards for our RLVR experiments: Correct: assigns 1 to correct answer and 0 otherwise. Random: assigns 1 with probability γ and 0 otherwise (γ = 0.5 in our experiments). Inverted: flips the correct signal, i.e., 1correct, so that correct solutions receive 0 and incorrect ones 1. Mv-incorrect: retains only majority-voted incorrect labels and assigns 1 when the model output matches an incorrect label, and 0 otherwise."
        },
        {
            "title": "4.1 SPURIOUS REWARDS ON MATH-500",
            "content": "Following the work of Shao et al. (2025), we replicate the performance of Qwen2.5-Math-7B and Llama3.1-8B-Instruct on the MATH-500 benchmark under various reward signal configurations, 6 Preprint. Work in progress Figure 4: Accuracy (%) of Qwen and Llama models on the MATH-500 dataset under different generation configurations, using original questions as prompts. More detailed results can be found in Table 3. using the same experimental setup. The training accuracy curves of MATH-500 are illustrated in Figure 3. Interestingly, the results demonstrate that while random rewards and mv-incorrect rewards noticeably boost accuracy for Qwen2.5-Math-7B, they have little or even adverse impacts on the performance of Llama3.1-8B-Instruct. Additionally, we apply the same RLVR procedure to Qwen2.5-Math-7B-Instruct and discover that the resulting gains are marginal when compared with those of Qwen2.5-Math-7B, indicating that the two Qwen variants exhibit differential sensitivity to RLVR. Considering the base and instruct variants of Qwen are trained under different paradigms: the former is pre-trained as general language model without exposure to any dialogue-specific templates, whereas the latter undergoes an additional instruction-tuning stage on data wrapped in fixed dialogue template. This mismatch creates trainingtesting gap for the base model at the start point of RLVR, and its initial accuracy is therefore likely underestimated. Consequently, to obtain fair estimate of each models starting point, we next measure performance under four decoding configurations (w/ and w/o templates, with details shown in Table 1), and the corresponding results are summarized in Figure 4. Surprisingly, we discover that applying the official chat template substantially degrades performance for the Qwen base model: both Qwen-2.5-7B and Qwen-2.5-Math-7B suffer pronounced drops once the template is enabled, whether we use greedy or avg@16 sampling. Building on this observation, we report two additional reference baselines in Figure 3: (i) red dashed line, denoted Greedy (w/o Template), corresponds to the best performance of the initial model, and (ii) grey dashed line, denoted pass@16 and adopted from Yue et al. (2025), serves as plausible upper bound on the models initial performance. Viewed against these baselines, the seeming RL gains of Qwen-Math-7B largely reflect adaptation to the template format and merely converge to the Greedy (w/o Template) baseline, indicative of memory recall rather than genuine mathematical generalization. However, spurious rewards, e.g., random and mv-incorrect, still boost the accuracy of Qwen base and maintain the performance of Qwen instruct, while degrading LlamaInstruct eventually. This is the point we need to further explore in the following sections."
        },
        {
            "title": "4.2 ANALYSIS OF MEMORIZATION CAPABILITY RESULTS",
            "content": "Considering the Qwen series is trained on massive web-scale corpora, we hypothesize that its divergent RLVR behavior from Llama is because the evaluation set MATH-500 may be inadvertently contaminated in Qwens large-scale training data, which is hard to eliminate completely. To verify our hypothesis, we probe memorization on several widely used mathematical-reasoning benchmarks. Concretely, we truncate the original questions at 40%, 60%, and 80% of their lengths and feed these partial questions as prompts into the model, and then evaluate the models partial-prompt completion rate by computing ROUGE and EM scores (details in 3.2) between the generated com7 Preprint. Work in progress Table 1: The sampling parameters used under different generation configurations. Greedy (w/o Template) refers to greedy decoding performed with the default model.generate(...) call. Avg@16 (w/o Template) indicates that we sample 16 candidate answers with vLLM and report their average. Greedy (w/ Template) denotes the use of the official chat template combined with the default greedy decoding, whereas Avg@16 (w/ Template) applies the official chat template together with the average over 16 rollout samples. In all configurations, the max-tokens parameter is set to 4096."
        },
        {
            "title": "Configuration",
            "content": "Do-Sample Temperature Top-P Top-K Chat Template Greedy (w/o Template) Avg@16 (w/o template) Greedy (w/ Template) Avg@16 (w/ template) False False 1.0 0.7 1.0 0.7 1.0 0.8 1.0 0.8 20 Table 2: Accuracy (Exact Match, EM) and ROUGE-L scores on several datasets (lower scores in gray ) under different prompt prefix ratios in greedy decoding mode without applying chat template, namely Greedy (w/o Template) configuration."
        },
        {
            "title": "Size",
            "content": "80%-Problem 60%-Problem 40%-Problem"
        },
        {
            "title": "EM RougeL",
            "content": "EM Qwen2.5-Math-7B Qwen2.5-7B Llama3.1-8B MATH-500 AMC AIME2024 500"
        },
        {
            "title": "30\nAIME2025\nMinervaMath\n272\nLiveMathBench 100",
            "content": "MATH-500 AMC AIME2024 500 83 30 AIME2025 30 272 MinervaMath LiveMathBench 100 MATH-500 AMC AIME2024 500"
        },
        {
            "title": "30\nAIME2025\nMinervaMath\n272\nLiveMathBench 100",
            "content": "81.25 77.38 74.04 54.71 36.08 42.76 66.42 73.24 59.80 54.61 35.24 41.15 48.33 44.54 50.50 47.04 36.24 35. 65.80 55.42 56.67 16.67 2.94 5.00 40.20 49.40 30.00 10.00 2.94 4.00 17.80 4.82 13.33 10.00 2.21 5. 78.06 70.25 55.31 34.88 31.22 32.78 60.98 64.42 48.69 37.59 32.35 32.74 40.55 30.62 30.80 33.49 29.52 31. 54.60 42.17 20.00 0.00 0.37 0.00 21.20 33.73 13.33 0.00 0.37 0.00 3.80 0.00 0.00 0.00 0.00 0. 69.01 75.17 57.72 27.43 29.35 29.97 50.36 63.79 44.65 30.30 27.89 27.95 32.07 27.10 26.08 25.20 27.11 26. 39.20 36.14 16.67 0.00 0.00 0.00 8.20 28.92 10.00 0.00 0.00 0.00 0.60 0.00 0.00 0.00 0.00 0. pletion and ground-truth continuations. In addition, we evaluate the models partial-prompt answer accuracy by checking if the continuation contains the correct answer, across both partial and full question settings. The detailed results are presented in Table 2, revealing strong signs of data contamination in the Qwen2.5 series models when evaluated on commonly used benchmarks, such as MATH-500, AMC, and AIME2024. For instance, when only the first 60% of the questions are provided, Qwen2.5-Math7B is able to accurately reconstruct more than half of the remaining problems on MATH-500 with greedy decoding, achieving completion rate of 54.6%. Even when just 40% proportion of the questions are shown, the model still manages to recover 39.2% of the unseen problems on MATH-500. Similar patterns are observed on AMC and AIME2024. These results indicate that the evaluation 8 Preprint. Work in progress Figure 5: Accuracy (%) of different base models on various math datasets under Greedy (w/o Template) configuration with varying proportions of problem prefixes used as prompts. More detailed experimental results can be found in Table 4. benchmarks for Qwen2.5 may suffer from data contamination: Although pre-training on massive web-scale corpora brings strong capacity on mathematical reasoning, e.g., superior performance on recently introduced mathematical tasks like LiveMathBench and AIME2025, those large-scale corpora also include publicly available benchmark problems inevitably, leading to less convincing results of old benchmarks, while removing such instances during large-scale crawling is notoriously difficult. Meanwhile, we summarize the answer accuracy of the models output under different proportions of input questions. The detailed results are shown in Figure 5. We observe that under greedy decoding, even when partial questions are provided, the Qwen2.5 models achieve remarkably high accuracy on MATH-500. For instance, with 80% proportion of the questions, Qwen2.5-Math-7B reaches an accuracy of 63.8% on MATH-500. Even when only 40% proportion of the questions are given, the model still achieves 41.2% accuracy. Because our evaluation matches only the final numeric answer, the model can sometimes accidentally output the correct value despite faulty reasoning, which explains the anomalous jump observed for Llama on AIME2025 when only 40% of the questions are provided. Among the several questions that the model solves correctly, the responses contain coherent reasoning chains and even syntactically valid Python code, which, however, is not executed. And several examples are shown in Figure 8 to 12. The emergence of such structured solutions (also observed in Shao et al. (2025)) indicates that the training corpora may have included publicly available resources where benchmark problems are accompanied by detailed solutions. Therefore, the sudden performance surge under random rewards or even mv-incorrect rewards, observed in Figure 3, arises because such rewards inadvertently trigger Qwen to retrieve its memorized answers from the pre-training corpus owing to the clipping bias of GRPO (Shao et al., 2025), rather than stimulating Qwens existing distribution of reasoning patterns as explained in Shao et al. (2025)."
        },
        {
            "title": "4.3 SPURIOUS REWARDS ON RANDOMCALCULATION",
            "content": "To further support our hypothesis that the anomalous performance surge of the Qwen2.5-Math-7B on the MATH-500 benchmark is primarily caused by data contamination rather than the models intrinsic mathematical reasoning ability, we replicate this experiment on newly constructed dataset that the model has never encountered before. We hypothesize that for math problems free from 9 Preprint. Work in progress Figure 6: Math reasoning performance of the Qwen2.5 series models on the RandomCalculation datasets under different generation configurations. The configuration parameters are listed in Table 1. contamination, the models reasoning ability still requires properly aligned reward signals to yield meaningful performance improvements. RandomCalculation Dataset Construction. To obtain an uncontaminated evaluation benchmark, we employ Algorithm 1 to construct suite of mathematically challenging yet verifiable computation datasets. These datasets are composed of expressions built from basic numerical elements, including integers from 0 to 100, as well as fractions, squares, and cubes derived from them. Using these components, we randomly generate mathematical expressions involving between 1 and 20 steps, utilizing the four fundamental arithmetic operations: addition, subtraction, multiplication, and division. To construct the final datasets, we append standardized problem prefix to each generated expression, resulting in 20 sub-datasets, each containing 1,000 unique problems. We refer to this suite of datasets as RandomCalculation. Examples from the datasets can be found in Figure 2. Algorithm 1 Construction of RandomCalculation Dataset Require: Maximum computation steps: = 20 Initialize dataset S0 with basic mathematical expressions Initialize dataset list: DL {S0} Define operator set: OPSET {+, , , } for = 1 to do Di for = 0 to i/2 do Randomly select Left DL[j] Randomly select Right DL[i 1 j] Randomly select op OPSET Randomly swap Left and Right expr Left op Right Add expr to Di end for Append Di to DL end for Save DL as the RandomCalculation dataset Qwen2.5s initial performance on RandomCalculation. We first test the zero-shot performance of the Qwen2.5 series models on the RandomCalculation dataset, with the results shown in Figure 6. We find that when using chat templates, the models mathematical reasoning ability tends to degrade as the number of computation steps increases, leaving ample room for improvement in multi-step calculation problems. When chat templates are removed, the reasoning performance peaks on problems involving three computation steps, and then gradually declines. RLVR results on RandomCalculation. We conduct RLVR training on Qwen2.5-Math-7B using the RandomCalculation datasets, aiming to guide the model toward producing answers that are as close as possible to the correct ones. Experiments are set on two datasets comprising 5-step and 10 Preprint. Work in progress Figure 7: Training performance of Qwen2.5-Math-7B and Llama3.1-8B-Instruct using the RLVR algorithm on the RandomCalculation dataset. Results are presented for datasets with 5-step and 10-step calculations. 10-step calculation problems, respectively. Each dataset contains 1,000 problems, with 700 used for training and the remaining 300 reserved for validation. The training progress is shown in Figure 7, showing that under correct rewards, the models performance steadily improves throughout training. With random or incorrect rewards, training becomes unstable and inconsistent. Under inverted rewards, the model collapses rapidly. These findings suggest that for problems not leaked during pretraining, only correct reward signals can effectively guide the model toward improved reasoning performance. For comparison, we also evaluate Llama3.1-8B-Instruct and observe consistent findings: Only accurate reward signals consistently yield stable performance gains. Qwens stronger mathematical capabilities. Considering obtaining reward of 1 on RandomCalculation instance is virtually impossible, we report Max@16, the highest score among 16 samples, instead of Pass@16 in Figure 7. We observe that on our RandomCalculation datasets, Qwen2.5-Math-7B can surpass the Max@16 threshold when provided with correct reward signals. This finding indicates that reward-aligned RLVR effectively transfers the models high-accuracy single-step arithmetic skills (see template-enabled accuracy in Figure 6) to more complex multi-step calculations, as illustrated by one reasoning trace in Figure 13. In contrast, under the incorrect and random reward configurations, the Qwen model either maintains its base performance or exhibits only marginal and unstable improvements. This further highlights the critical role of correct and well-aligned reward signals in enhancing model performance on uncontaminated datasets. Notably, Llama3.1-8B-Instruct fails to surpass the Max@16 threshold even when trained with correct reward signals, and its accuracy falls below the greedy-decoding baseline when exposed to spurious signals. This discrepancy indirectly suggests that Qwen2.5 exhibits stronger mathematical capabilities compared to Llama3.1; however, such inherent strength is not the root cause of its performance boosting under spurious reward on contaminated datasets."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Due to limited computational resources, our experiments were restricted to subset of commonly used Qwen2.5 series models. The Qwen3 series will be included in our future experimental evaluation. As the successor to Qwen2.5, it is likely that Qwen3 still suffers from similar issues. Many reinforcement learning algorithms have recently been proposed to enhance mathematical reasoning capabilities. Given the rapid pace of development, it is infeasible to conduct comprehensive evaluation of all these methods in the short term. In future work, our efforts will focus on systematically evaluating more diverse set of benchmarks, reinforcement learning methods, and model families. In parallel, we will carry out an in-depth theoretical investigation of this unexpected behavior, with the goal of shedding light on its root causes and advancing our understanding of the underlying mechanisms. 11 Preprint. Work in progress"
        },
        {
            "title": "6 CONCLUSION",
            "content": "Recent studies have shown that the mathematical reasoning performance of Qwen2.5 models can be substantially improved with minimal supervision, ranging from few-shot prompts to single examples or even without additional data. Some studies further suggest that the mathematical reasoning abilities of the Qwen2.5 model can be improved with reward signals that are random or incorrect. While the effectiveness of these methods is striking, their gains appear largely limited to the Qwen2.5 series. This distinctive phenomenon motivated our study, which reveals that the largescale pretraining corpus of the Qwen2.5 series models may be contaminated by data that overlaps with several widely used mathematical benchmarks, including MATH-500, AMC, and AIME 2024. We suspect that such data contamination is key factor for Qwens improvement on MATH-500 under those spurious rewards. Moreover, to ensure reliable evaluation of RL-based algorithms, we construct leakage-free RandomCalculation dataset composed of expressions built from basic numerical elements, and zero-shot test on this dataset shows Qwen has ample room for improvement in multi-step calculation problems. Leveraging this contamination-free dataset, we run carefully controlled RLVR experiments and find that only accurate reward signals deliver consistent performance improvements, while noisy or reversed rewards yield no measurable gains, underscoring the pivotal importance of reward integrity. We therefore recommend that future research conduct evaluations on clean benchmarks and assess multiple model families to ensure their RL-based methods are robust."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "Qin Liu is supported by the Amazon Nova Trusted AI Prize."
        },
        {
            "title": "REFERENCES",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in LLM reasoning. CoRR, abs/2505.15134, 2025. doi: 10.48550/ ARXIV.2505.15134. URL https://doi.org/10.48550/arXiv.2505.15134. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1224812267. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.662. URL https://doi.org/10.18653/v1/2024.acl-long.662. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlıcek, Agustın Piqueres Lajarın, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big - data-centric training of small language model. CoRR, abs/2502.02737, 2025. doi: 10.48550/ARXIV.2502.02737. URL https://doi.org/ 10.48550/arXiv.2502.02737. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, 12 Preprint. Work in progress R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10. 48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. Zitian Gao, Lynx Chen, Joey Zhou, and Bryan Dai. One-shot entropy minimization, 2025. URL https://arxiv.org/abs/2505.20282. Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. CoRR, abs/2409.12568, 2024. doi: 10. 48550/ARXIV.2409.12568. URL https://doi.org/10.48550/arXiv.2409.12568. and Jacob Steinhardt. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Measuring mathematical problem solving with In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings Information Processing Systems Track on Datasets and Benchmarks URL Dawn Song, the MATH dataset. of the Neural 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/ hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models, 2025. URL https://arxiv. org/abs/2506.06395. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. 13 Preprint. Work in progress Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? CoRR, abs/2412.13147, 2024. doi: 10.48550/ARXIV.2412.13147. URL https://doi.org/10. 48550/arXiv.2412.13147. Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, and Rui Yan. The climb carves wisdom deeper than the summit: On the noisy rewards in learning to reason, 2025. URL https://arxiv. org/abs/2505.22653. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious. CoRR, abs/2501.00656, 2025. doi: 10.48550/ARXIV.2501.00656. URL https://doi.org/10.48550/arXiv.2501.00656. OpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Learning to reason with LLMs, 2024b. URL https://openai.com/index/ learning-to-reason-with-llms/. OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=jKHmjlpViu. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning, 2025. URL https://arxiv.org/abs/ 2505.22660. Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train?, 2025. URL https://arxiv.org/abs/2505.21444. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in rlvr, 2025. URL https://arxiv.org/abs/2506.10947. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example. CoRR, abs/2504.20571, 2025a. doi: 10.48550/ARXIV.2504.20571. URL https://doi.org/ 10.48550/arXiv.2504.20571. 14 Preprint. Work in progress Zengzhi Wang, Xuefeng Li, Rui Xia, and Pengfei Liu. Mathpile: billion-token-scale In Amir Globersons, Lester Mackey, Danielle Belgrave, pretraining corpus for math. Jakub M. Tomczak, and Cheng Zhang (eds.), Advances Angela Fan, Ulrich Paquet, in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - URL http://papers.nips.cc/paper_files/paper/2024/ 15, 2024, 2024. hash/2d0be3cd5173c10b6ec075d1c393a13d-Abstract-Datasets_and_ Benchmarks_Track.html. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling, 2025b. URL https://arxiv.org/abs/2506.20512. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024a. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv. 2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b. doi: 10.48550/ARXIV.2409.12122. URL https://doi.org/10.48550/arXiv.2409.12122. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? CoRR, abs/2504.13837, 2025. doi: 10.48550/ARXIV.2504.13837. URL https: //doi.org/10.48550/arXiv.2504.13837. Yanzhi Zhang, Zhaoxi Zhang, Haoxiang Guan, Yilin Cheng, Yitong Duan, Chen Wang, Yue Wang, Shuxin Zheng, and Jiyan He. No free lunch: Rethinking internal feedback for llm reasoning, 2025. URL https://arxiv.org/abs/2506.17219. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data. CoRR, abs/2505.03335, 2025a. doi: 10.48550/ARXIV.2505.03335. URL https: //doi.org/10.48550/arXiv.2505.03335. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards, 2025b. URL https://arxiv.org/abs/2505.19590. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, Biqing Qi, Youbang Sun, Ganqu Cui, Ning Ding, and Bowen Zhou. TTRL: test-time reinforcement learning. CoRR, abs/2504.16084, 2025. doi: 10.48550/ARXIV.2504.16084. URL https://doi.org/10.48550/arXiv. 2504.16084. 15 Preprint. Work in progress"
        },
        {
            "title": "A APPENDIX",
            "content": "Table 3: Accuracy (%) of Qwen and Llama models on the MATH-500 dataset under different generation configurations, using varying proportions of questions as prompts. The configuration parameters can be found in Table 1. Note that Llama3.1-8B lacks an official chat template, so template-dependent experiments are omitted. Model Configuration 100% 80% 60% 40% Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama3.1-8B Llama3.1-8B-Instruct Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) Greedy (w/o Template) Avg@16 (w/o Template) Greedy (w/ Template) Avg@16 (w/ Template) 72.20 68.53 50.60 48.84 83.60 81.76 82.20 82. 67.60 66.20 40.00 38.15 72.80 74.90 72.20 73.69 1.60 3.12 43.60 39.61 38.80 38.24 63.80 57.25 29.20 28.01 53.80 50.80 3.80 43. 49.60 46.04 22.20 21.75 50.00 50.18 36.00 37.16 2.00 2.81 24.80 24.76 17.20 18.46 53.60 45.51 20.20 18.99 31.80 31.54 24.40 25. 30.80 30.01 13.40 11.89 31.20 30.88 20.80 20.67 2.40 2.80 15.00 14.21 10.60 9.95 41.20 31.03 10.00 10.36 16.00 15.43 13.20 12. 18.40 16.10 6.80 5.25 15.20 15.79 10.00 8.75 2.00 2.30 7.40 6.84 5.00 4.25 16 Preprint. Work in progress Table 4: Accuracy (%) of different models on various math datasets under Greedy (w/o Template) configuration with varying proportions of problem prefixes used as prompts. Model Dataset Size 100% 80% 60% 40% Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama3.1-8B Llama3.1-8B-Instruct MATH-500 AMC AIME AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench 500 83 30 272 100 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 272 100 500 83 30 30 272 100 500 83 30 30 272 100 72.20 53.01 13. 16.67 14.71 7.00 83.60 57.83 16.67 6.67 20.96 9.00 67.60 43.37 6.67 3.33 12.87 9.00 72.80 44.58 16. 6.67 18.01 9.00 1.60 4.82 0.00 0.00 2.57 0.00 43.60 21.69 10.00 0.00 10.29 3.00 63.80 53.01 13. 3.33 6.25 9.00 53.80 27.71 3.33 0.00 7.72 4.00 49.60 32.53 6.67 0.00 4.41 8.00 50.00 34.94 6. 3.33 5.51 6.00 2.00 2.41 0.00 0.00 1.10 2.00 24.80 10.84 0.00 0.00 4.04 3.00 53.60 43.37 13. 0.00 2.94 2.00 31.80 7.23 3.33 0.00 3.68 4.00 30.80 24.10 6.67 0.00 2.57 3.00 31.20 22.89 0. 0.00 3.68 2.00 2.40 1.20 0.00 0.00 1.10 1.00 15.00 3.61 0.00 0.00 2.57 2.00 41.20 43.37 13. 0.00 2.57 3.00 16.00 4.82 0.00 0.00 1.84 1.00 18.40 22.89 3.33 0.00 2.21 4.00 15.20 14.46 0. 0.00 2.21 4.00 2.00 2.41 0.00 3.33 0.00 1.00 7.40 2.41 0.00 0.00 1.84 0.00 Preprint. Work in progress Table 5: Accuracy (%) of different models on various math datasets under Greedy (w/ Template) configuration with varying proportions of problem prefixes used as prompts. Note that Llama3.1-8B lacks an official chat template, so template-dependent experiments are omitted. Model Dataset Size 100% 80% 60% 40% Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama3.1-8B Llama3.1-8B-Instruct MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 30 272 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 30 272 50.60 37.35 10.00 6.67 9.19 5.00 82.20 55.42 20.00 16.67 26.47 8.00 40.00 27.71 6.67 6.67 8.09 8. 72.20 48.19 6.67 6.67 23.53 10.00 29.20 20.48 6.67 3.33 4.78 8. 43.80 14.46 0.00 0.00 6.62 12.00 22.20 9.64 0.00 0.00 4.41 6.00 36.00 10.84 3.33 0.00 6.25 10. 20.20 14.46 0.00 0.00 2.94 2.00 24.40 7.23 3.33 0.00 4.78 6. 13.40 3.61 0.00 0.00 2.21 2.00 20.80 4.82 0.00 0.00 3.68 3.00 38.80 25.30 6.67 0.00 15.81 2.00 17.20 6.02 0.00 0.00 2.94 2.00 10.60 2.41 0.00 3.33 3.68 4. 10.00 7.23 0.00 0.00 2.21 2.00 13.20 4.82 0.00 0.00 1.84 5.00 6.80 3.61 0.00 0.00 1.47 1. 10.00 2.41 0.00 0.00 2.94 2.00 5.00 1.20 0.00 0.00 2.21 1. 18 Preprint. Work in progress Table 6: Accuracy (%) of different models on various math datasets under Avg@16 (w/o Template) configuration with varying proportions of problem prefixes used as prompts. Model Dataset Size 100% 80% 60% 40% Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama3.1-8B Llama3.1-8B-Instruct MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME AIME2025 MinervaMath LiveMathBench 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 272 100 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 272 100 68.53 49.47 18.33 6.88 11.40 9.50 81.76 51.28 12.29 10.42 18.50 10.88 66.20 40.06 11. 7.92 10.66 7.75 74.90 43.07 11.67 6.67 18.18 10.06 3.12 0.98 0.00 0.00 1.93 0.25 39.61 21.84 5. 0.42 8.89 3.25 57.25 48.57 13.75 1.67 5.12 7.56 50.80 28.54 4.58 1.46 7.08 7.81 46.04 32.23 7. 0.83 5.51 6.50 50.18 33.51 5.42 1.04 6.82 7.75 2.81 1.58 0.00 0.00 1.24 1.38 24.76 9.26 1. 0.42 5.08 2.00 45.51 40.06 14.58 0.83 3.29 3.38 31.54 9.41 1.88 1.04 4.07 5.38 30.01 27.71 4. 0.42 3.26 4.06 30.88 17.62 1.04 0.42 4.11 5.12 2.80 1.66 0.00 0.21 0.92 0.44 14.21 3.31 0. 0.00 3.19 1.38 31.03 37.27 14.17 0.00 1.77 3.06 15.43 4.52 0.00 0.42 1.75 2.56 16.10 22.52 4. 0.21 1.38 3.19 15.79 11.75 0.21 0.00 1.59 3.31 2.30 1.88 0.21 0.21 1.06 1.06 6.84 2.48 0. 0.00 1.45 1.88 19 Preprint. Work in progress Table 7: Accuracy (%) of different models on various math datasets under Avg@16 (w/ Template) configuration with varying proportions of problem prefixes used as prompts. Note that Llama3.1-8B lacks an official chat template, so template-dependent experiments are omitted. Model Dataset Size 100% 80% 60% 40% Qwen2.5-Math-7B Qwen2.5-Math-7B-Instruct Qwen2.5-7B Qwen2.5-7B-Instruct Llama3.1-8B Llama3.1-8B-Instruct MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench MATH-500 AMC AIME2024 AIME2025 MinervaMath LiveMathBench 500 83 30 30 272 100 500 83 30 30 272 500 83 30 30 272 100 500 83 30 30 272 100 500 83 30 30 272 500 83 30 30 272 100 48.84 35.32 11.46 3.54 8.94 5.75 82.01 50.53 12.50 10.62 27.07 11. 38.15 20.78 5.42 0.62 8.18 5.00 73.69 44.80 12.92 5.21 24.95 10.25 28.01 14.83 5.83 1.46 4.83 4.56 43.66 14.68 0.83 0.00 7.01 10.94 21.75 8.89 0.00 0.21 3.52 3. 37.16 10.54 1.25 0.00 6.20 8.75 38.24 19.43 6.25 0.42 14.38 3. 18.46 6.10 0.42 0.00 3.15 2.56 18.99 10.99 0.83 0.42 2.96 3.06 25.14 6.70 0.21 0.42 4.76 6. 11.89 4.22 0.00 0.00 2.27 2.81 20.67 5.12 1.25 0.00 3.56 4.31 9.95 3.24 0.00 0.21 2.44 1.69 10.36 8.36 1.25 0.00 1.70 2.06 12.78 4.07 0.00 0.00 1.93 4. 5.25 2.86 0.21 0.00 1.77 1.12 8.75 1.88 0.00 0.00 1.79 3.56 4.25 2.48 0.00 0.21 1.47 2.06 20 Preprint. Work in progress"
        },
        {
            "title": "Example from AMC",
            "content": "Problem: The taxicab distance between points (x1, y1) and (x2, y2) in the coordinate plane is given by x1 x2 + y1 y2. For how many points with integer coordinates is the taxicab distance between and the origin less than or equal to 20? Answer:"
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: The taxicab distance between points (x1, y1) and (x2, y2) in the coordinate plane is given Qwen2.5-7B Output by x1 x2 + y1 y2. For how many points with integer coordinates is the taxicab distance between and the origin less than or equal to 20 ? To solve the problem, we need to find the number of points with integer coordinates such that the taxicab distance between and the origin (0, 0) is less than or equal to 20. The taxicab distance between = (x, y) and the origin is given by: + 20 We need to count all integer coordinate pairs (x, y) that satisfy this inequality. Lets break down the problem into manageable steps: 1. Understand the Constraint: The constraint + 20 means that the sum of the absolute values of and must be less than or equal to 20. 2. Iterate Over Possible Values: We need to iterate over all possible integer values of and and count the pairs that satisfy the constraint. 3. Count Valid Points: For each valid pair (x, y), increment counter to keep track of the number of such points. Lets implement this in Python to find the solution. 1 # Initialize the counter for valid points 2 count = 0 3 # Iterate over possible values of and 4 for in range(-20, 21): # ranges from -20 to 20 5 for in range(-20, 21): # ranges from -20 to 20 6 if abs(x) + abs(y) <= 20: 7 8 print(count) count += 1 output 1 841 The number of points with integer coordinates such that the taxicab distance between and the origin is less than or equal to 20 is 841 . Figure 8: Memorization Example of Qwen2.5-7B on AMC. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration. Preprint. Work in progress Example from AIME2024 Problem: Find the largest possible real part of (75 + 117i)z + 96 + 144i where is complex number with = 4. Answer:"
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: Find the largest possible real part of (75 + 117i Qwen2.5-7B Output where is complex number with = 4. )z + 96+144i To find the largest possible real part of the expression (75 + 117i)z + 96+144i complex number with = 4, we can proceed as follows: 1. Let = 4eiθ, where θ is real number. This is because = 4. 2. Substitute = 4eiθ into the expression. 3. Simplify the expression and find the real part. 4. Maximize the real part with respect to θ. Lets start by substituting = 4eiθ into the expression: ... Combining the real and imaginary parts, we get: where is (300 cos θ + 24 cos θ + 36 sin θ 468 sin θ) + i(300 sin θ + 468 cos θ 24 sin θ + 36 cos θ) This simplifies to: (324 cos θ 432 sin θ) + i(336 cos θ + 276 sin θ) ... Lets calculate this using Python to ensure accuracy. 1 import math 2 3 # Coefficients 4 = 324 5 = -432 6 7 # Maximum value of a*cos(theta) + b*sin(theta) 8 max_real_part = math.sqrt(a**2 + b**2) 9 print(max_real_part) output 1 540. The largest possible real part of the expression (75 + 117i)z + 96+144i number with = 4 is 540 . where is complex Figure 9: Memorization Example of Qwen2.5-7B on AIME2024. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration. 22 Preprint. Work in progress Example from MATH-500 Problem: Suppose and are different prime numbers greater than 2. How many wholenumber divisors are there for the integer a(2a + b) 2a2 + ab? Answer:"
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: Suppose and are different prime numbers greater than Qwen2.5-Math-7B Output 2. How many whole-number divisors are there for the integer a(2a + b) 2a2 + ab? To determine the number of whole-number divisors of the integer a(2a + b) 2a2 + ab, we first need to simplify the expression. Given the expression: a(2a + b) 2a2 + ab Lets simplify it step-by-step: a(2a + b) 2a2 + ab = 2a2 + ab 2a2 + ab = 2ab So, the expression simplifies to 2ab. Since and are different prime numbers greater than 2, 2ab is product of three distinct prime factors: 2, a, and b. The number of divisors of number with the prime factorization = pe1 given by (e1 + 1)(e2 + 1) (ek + 1). For 2ab, the prime factorization is 21 a1 b1. Therefore, the number of divisors is (1 + 1)(1 + 1)(1 + 1) = 2 2 2 = 8. Lets confirm this with Python code. 2 pek 1 pe2 is 1 # Function to calculate the number of divisors of number 2 def number_of_divisors(n): 3 count = 0 for in range(1, int(n**0.5) + 1): 4 5 6 7 8 if % == 0: if * == n: count += 1 else: count += 2 return count 10 11 # Lets choose two different prime numbers greater than 2, ... 12 = 3 13 = 5 14 expression_value = 2 * * 15 # Calculate the number of divisors of the expression value 16 divisors_count = number_of_divisors(expression_value) 17 print(divisors_count) output 1 8 The number of whole-number divisors of the integer a(2a + b) 2a2 + ab is 8 . Figure 10: Memorization Example of Qwen2.5-Math-7B on MATH-500. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration. 23 Preprint. Work in progress"
        },
        {
            "title": "Example from AMC",
            "content": "Problem: What is the value of 23 13 + 43 33 + 63 53 + + 183 173? Answer: 3159."
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: What is the value of 23 13 Qwen2.5-Math-7B Output To solve the given problem, we need to evaluate the alternating sum of cubes: +43 33 + 63 53 + + 183 173? 23 13 + 43 33 + 63 53 + + 183 173. We can break this down into pairs of terms and compute the difference for each pair. Specifically, we need to compute: (23 13) + (43 33) + (63 53) + + (183 173). Lets write Python code to compute this sum. 1 # Initialize the sum 2 result = 0 3 4 # Iterate through the pairs (2,1), (4,3), (6,5), ..., (18,17) 5 for in range(1, 18, 2): 6 result += ( (i+1)**3 - i**3 ) 7 8 print(result) output 1 3159 The value of the given expression 23 13 + 43 33 + 63 53 + + 183 173 is 3159 . Figure 11: Memorization Example ofQwen2.5-Math-7B on AMC. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration. 24 Preprint. Work in progress Example from AIME Problem: Find the number of triples of nonnegative integers (a, b, c) satisfying + + = 300 and a2b + a2c + b2a + b2c + c2a + c2b = 6, 000, 000. Answer:"
        },
        {
            "title": "Partial Prompt",
            "content": "Prompt: Find the number of triples of nonnegative integers (a, b, c) satisfying Qwen2.5-Math-7B Output + + = 300 and a2b + a2c + b2a + b2c + c2a + c2b = 6, 000, 000. To solve the problem, we need to find the number of triples (a, b, c) of nonnegative integers that satisfy the following conditions: 1. + + = 300 2. a2b + a2c + b2a + b2c + c2a + c2b = 6, 000, 000 Lets start by analyzing the given equations. We can use Python to iterate through possible values of a, b, and to find the valid triples that satisfy both conditions. First, we can express in terms of and using the first equation: = 300 Next, we substitute into the second equation and check if the equation holds for different values of and b. Lets write the Python code to iterate through possible values of and and check the conditions. 1 # Initialize the count of valid triples 2 count = 0 3 4 # Iterate through possible values of and 5 for in range(301): 6 for in range(301 - a): = 300 - - if a**2 * + a**2 * + b**2 * + b**2 * + c**2 * + **2 * == 6000000: count += 1 7 9 10 11 print(count) output 1 601 The number of triples (a, b, c) of nonnegative integers that satisfy the given conditions is 601 . Figure 12: Memorization Example of Qwen2.5-Math-7B on AIME2024. In this case, the first 40% of the original problem is used as the prompt, and the generation is performed under the Greedy (w/o Template) configuration. 25 Preprint. Work in progress Example from RandomCalculation, 5 Calculation Steps Problem: Evaluate this LaTeX numerical expression step-by-step and give the final value within boxed{}: 182/ 34 + 89 4 49 9 56 + 622 Answer: 3866.26307189542 Qwen2.5-Math-7B Output To evaluate the given LaTeX numerical expression step-by-step, we will follow the order of operations (PEMDAS/BODMAS): Parentheses/Brackets, Exponents/Orders, Multiplication and Division (from left to right), Addition and Subtraction (from left to right). The expression is: 182/ Step 1: Evaluate the exponents. 34 8 + 89 4 49 9 56 4 + 622 182 = 324 622 = 3844 ... Step 2: Simplify the fractions and perform the division. 34 8 = 4. ... Step 3: Simplify the remaining fractions and perform the multiplication. 89 4 = 22.25 5.444444444444444 49 9 ... Step 4: Perform the addition and subtraction from left to right. 76.23529411764706 + 22.25 98.48529411764706 98.48529411764706 76.22222222222223 22.26307189542483 22.26307189542483 + 3844 3866.263071895425 So the final value of the expression is: 3866.263071895425 Figure 13: Example of correct output produced by RLVR on the 5-Step RandomCalculation dataset. The result is obtained using the Qwen2.5-Math-7B model, trained for 300 steps under the correct reward signal."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "University of California, Davis"
    ]
}