{
    "paper_title": "Retrospective Learning from Interactions",
    "authors": [
        "Zizhao Chen",
        "Mustafa Omer Gul",
        "Yiwei Chen",
        "Gloria Geng",
        "Anne Wu",
        "Yoav Artzi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy a relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce ReSpect, a method to learn from such signals in past interactions via retrospection. We deploy ReSpect in a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show how ReSpect gradually improves task completion rate from 31% to 82%, all without any external annotation."
        },
        {
            "title": "Start",
            "content": "Zizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu & Yoav Artzi Department of Computer Science and Cornell Tech, Cornell University czz,momergul,annewu,yoav { @cs.cornell.edu } yc833,gcg46 } @cornell.edu {"
        },
        {
            "title": "ABSTRACT",
            "content": "Multi-turn interactions between large language models (LLMs) and users naturally include implicit feedback signals. If an LLM responds in an unexpected way to an instruction, the user is likely to signal it by rephrasing the request, expressing frustration, or pivoting to an alternative task. Such signals are task-independent and occupy relatively constrained subspace of language, allowing the LLM to identify them even if it fails on the actual task. This creates an avenue for continually learning from interactions without additional annotations. We introduce RESPECT, method to learn from such signals in past interactions via retrospection. We deploy RESPECT in new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with combinatorial solution space. Through thousands of interactions with humans, we show how RESPECT gradually improves task completion rate from 31% to 82%, all without any external annotation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Language models (LMs) often engage in multi-turn interactions with human users. Similar to humanhuman interactions, these interactions are naturally rich with implicit learning signals. If the LM fails to respond appropriately, the user is likely to follow with an expression of frustration, rephrase of their intent, or maybe even completely pivot what they ask for. Similarly, if the LM does well, the user may express approval or simply continue to their next objective. Such responses can inform the LM of its performance, thereby creating an opportunity to learn through retrospection. We study the efficacy of such signals, and how they can lead to system that improves over time. We introduce RESPECT, simple approach to learn from signals the model itself derives about its own past actions through retrospection of past interactions with human users. We deploy RESPECT in MULTIREF, new multi-turn grounded interaction scenario, which requires models to display 4 2 0 2 7 1 ] . [ 1 2 5 8 3 1 . 0 1 4 2 : r Figure 1: Learning via RESPECT. We deploy an LLM policy πθρ (a x) in rounds ρ, to interact with users in multi-turn interactions. Following each round, the LLM reasons retrospectively about each of its actions (highlighted in blue) to decode feedback given the interaction context, including follow up utterances. After each round, the model is retrained using all data aggregated so far Dρ. The LLM improves over time without any external annotations. The plot on the right shows the performance curve in our experiments the LLM improves from 31% to 82% task completion rate over six rounds."
        },
        {
            "title": "Preprint under submission",
            "content": "complex abstract reasoning, and humans to gradually instruct models to accomplish sequences of goals to complete their tasks. The key insight underlying RESPECT is that conversational implicit feedback signals occupy relatively constrained subspace of natural language. Such signals can include direct approvals (e.g., great!) or signs of frustration (e.g., not again), and also more subtle cues, such as when the user rephrases their request. Critically, it is relatively simple to disentangle them from task performance. human can easily figure out from such cues if they do well or not, even if they have little understanding about what they are asked for. It is this constrained nature that makes reasoning about such signals to be within the capacities of large language models (LLMs), even if they fail at the task at hand. RESPECT utilizes this signal in process where the model interacts with humans, and after interaction decodes feedback for each of its actions from the interaction context including the follow up utterances. Figure 1 illustrates this process. The model interacts with humans to accomplish tasks, retrospectively examines its own past interactions, and then re-trains. This process progresses in rounds, alternating between interaction and training, with the model improving over time. Critically, unlike common recipes for training from human feedback, RESPECT does not require any external annotation (Ouyang et al., 2022, RLHF) or even soliciting feedback from the users themselves (Suhr & Artzi, 2023). We deploy RESPECT in MULTIREF over multiple rounds of grounded interactions with human use and re-training. We use IDEFICS2-8B (Laurencon et al., 2024) as our LLM, and experiment with multiple learning methods, including supervised learning, REINFORCE-style policy gradient (Williams, 1992; Kojima et al., 2021), and KTO (Ethayarajh et al., 2024). Across our experiments, we observe that IDEFICS2-8B effectively decodes feedback, even as it initially performs poorly in the same interactions. In our longest running experiment, we observe model task completion rate improves from 31% to 82%. Our code, data, and models are at https://lil-lab.github.io/respect."
        },
        {
            "title": "2 TECHNICAL OVERVIEW AND NOTATION",
            "content": "We conduct continual learning studies by deploying our approach in MULTIREF, new multi-turn grounded interaction scenario (Section 3). Overall, the study progresses in rounds, where the LLM policy is first deployed to interact with users and complete tasks, and the interactions are then used to re-train the policy. Our study involves multiple rounds, and our goal is to observe and evaluate the long-term dynamics of the process. This includes the robustness of our award decoding and training methods to the changing distribution of the data likely to be seen in an adaptive system in the wild. Section 3 describes our interaction scenario in detail, and Section 4 our learning method. First, we outline our problem of interest and its notation in abstract terms. Task Notation The policys task is to respond effectively to human utterances given in conversational context. Formally, let π(at xt) be the policy that controls the listener behavior, with at an action string that represents the model response and xt being the context on which the policy is conditioned, both at time in the interaction. The context includes the instruction history up to and excluding time t, including current (i.e., at time 1) and past speaker utterances, as well as any other relevant context in which the interaction takes place. As our learning progresses in rounds, we denote θρ as the model parameters in round ρ, and πθρ as the parameterized policy. Learning and Deployment We study continual learning setup, where the learning signal is acquired from interactions of the deployed model with human speakers. Our study progresses in rounds (Figure 1). Each round ρ includes deployment, followed by training. During deployment xt), we record at round ρ, the model πθρ interacts with users. For each model action ˆat tuple (xt, ˆat, pt, ft), where xt is the context given to the model at time to predict action ˆat, pt is the probability of ˆat at the time of prediction, and ft is the remainder of the interaction following ˆat. Critically, these interaction tuples contain no explicit feedback. We compute the implicit feedback ˆγt using feedback decoder ϕ(xt, ˆat, ft), to obtain tuples (xt, ˆat, ˆγt, pt). We experiment with multiple learning objectives using this feedback: supervised learning (SFT), policy gradient, and KTO. πθρ (a Evaluation We measure the quality of the listener model πθρ (at xt) at each round ρ primarily by interaction success rates from live human-bot deployments. The same interactions are used to train the model for the next round. We track various characteristics of model behavior, such as number"
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 2: The interaction scenario we use in our experiments. MULTIREF is multi-turn reference game. speaker and listener both observe shared set of tangram shapes, but in different order. The goal of the speaker is to describe subset of targets for the listener to select. Because the target requires multiple abstract shapes, humans often communicate the targets gradually over multiple turns. As an interaction progresses naturally, the speaker produces implicit feedback signals that validate or reject the listeners actions. of turns per interaction as an efficiency measure. We also do post-hoc annotation of subset of the interactions to measure utterance-level policy success rate and feedback decoder accuracy."
        },
        {
            "title": "3 MULTIREF: A MULTI-TURN GROUNDED INTERACTION SCENARIO",
            "content": "Key to our study is that tasks are relayed gradually across multiple turns, as commonly happens in human interactions. We create MULTIREF, conversational interaction scenario where two partners, speaker and listener, coordinate on the selection of set of items. In our studies, the speaker is always human, and the listener is model. MULTIREF generalizes the commonly studied reference game scenario. Its design and our choice of stimuli are grounded in existing work from both cognitive science and computational language modeling (Rosenberg & Cohen, 1964; Clark & Wilkes-Gibbs, 1986; Schober & Clark, 1989; Goodman & Frank, 2016). Figure 2 illustrates the scenario. Both partners observe shared set of images, but in different order. The speaker is given subset of the images as targets, with the goal of communicating the targets to the listener, so the latter selects the exact subset. Only the speaker can write messages, and only the listener can select or deselect images. The interaction concludes successfully once all and only targets are selected, or fails if the partners run out of turns, 20 in our studies. The interaction progresses in turns t, alternating between speaker and listener turns. At each speaker turn, they provide single unrestricted natural language utterance. It may direct the listener to select one or more items, ask to deselect items if the listener previously made mistake, or include whatever other content they desire. This utterance as well as the history of the interaction, the set of images, and their selection status compose the context xt for the following model turn at time t. The follower responds with an action at, which includes one or more image selects or deselects according to their understanding of the speaker intention. The action space consists of all possible legal sequences of the form Deselect select or Select assuming images are code-named alphabetically. The motivation behind the design of MULTIREF is to create task-oriented scenario that is both accessible to non-expert humans and encourages constructing solution in multiple turns, thereby creating multi-turn interactions and eliciting the learning signals we aim to study. The rules of the interaction are simple: the speaker needs to describe targets to select, and the listener has to select what the speaker is referring to. This makes MULTIREF easily accessible to crowdsourcing workers. At the same time, the solution the speaker communicates to the listener is relatively complex, because of the enormous solution space: Consider conventional reference games, where the goal is to select single image. The number of possible solutions is the number of images in the context. In MULTIREF, the goal is to select subset of unknown size, so the combinatorial solution space the listener faces is exponential in the number of images. At the same time, the solution itself is structured, creating easy opportunities to decompose it to multiple steps."
        },
        {
            "title": "Preprint under submission",
            "content": "Key to making MULTIREF work well is the choice of images. We use tangram shapes from the diverse KILOGRAM dataset (Ji et al., 2022). Tangrams are abstract shapes that are designed to elicit common concepts in humans. This abstractness often leads to ambiguous descriptions open to interpretation. For example, Shape in Figure 2 can be described as bat, lowercase w, or even star wars star fighter. We select tangrams because they naturally provide an ambiguous and challenging stimuli for human interaction (Clark & Wilkes-Gibbs, 1986; Schober & Clark, 1989; Fox Tree, 1999; Hawkins et al., 2020b), thereby leading to highly diverse language. They also remain challenging for contemporary multimodal LLMs (MLLMs) to reason about (Ji et al., 2022), leaving significant room for learning. The free-form natural language human speakers produce in MULTIREF is very diverse, and balances between competing pressures. First, it often requires complex pragmatic reasoning (Clark & WilkesGibbs, 1986; Schober & Clark, 1989; Horton & Gerrig, 2002), because of the abstractness of tangrams. This is compounded by how the combinatorial solution space drives humans to balance between relaying as much information as possible, and relaying clear objectives to make gradual progress. This is balance between two Gricean maxims: quantity and manner.1 Speakers may or may not include explicit feedback such as good, or deselect the last one; the speaker may describe more than one target in single utterance, for example, select two men; speakers may refer to previous selections without directly describing targets, for example, the other one, or try again. In combination with the abstract stimuli tangrams provide, this creates challenging reasoning problem for the listener model. MULTIREF is not designed to increase complexity in arbitrary ways, but to provide an environment for humans to naturally expose core aspects of human communication. At the same time, the scenario is both controlled and scoped, allowing for easy measurement of task completion and progress, as well as making learning feasible with relatively limited data. This makes MULTIREF particularly suitable to research in academia or other low-resource settings."
        },
        {
            "title": "4 RESPECT: RETROSPECTIVE LEARNING FROM PAST INTERACTIONS",
            "content": "RESPECT has two components: decoding implicit feedback from past interactions (retrospection) and learning from the decoded feedback signals (learning). We deploy RESPECT in an iterative continual learning scenario, where each round includes both steps. This deployment allows us to observe the dynamics of RESPECT over time. However, the method itself is not limited to continual learning, and can be applied single time as well. The goal of RESPECT is to re-estimate the parameters of model given interactions that were collected by the model itself, or previous versions of it. We assume access to raw dataset Draw = (x(i), ˆa(i), p(i), (i)) i=1, where x(i) is the policy context, ˆa(i) is the predicted action, p(i) is the { } probability of this action, and (i) is the remainder of the interaction following ˆa(i).2 In our continual learning setup, Draw is union of all data collected from past rounds. The feedback decoder ϕ computes categorical feedback ˆγ(i) positive, neutral, negative } for each action ˆa(i) holistically based its context x(i), action taken ˆa(i), follow up utterances (i).3 (x(i), ˆa(i), p(i), ˆγ(i)) This process transforms Draw to = i=1. We use this dataset for training. } { { 4.1 DECODING IMPLICIT FEEDBACK THROUGH RETROSPECTION We implement the feedback decoder ϕ by prompting the model to analyze past interaction tuples (x, ˆa, p, ) to compute feedback ˆγ = ϕ(x, ˆa, ). The goal is process where the model bootstraps from its own interactions. Our hypothesis is that LLMs have the ability to reason about the relatively constrained space of implicit signals, even if they fail at the task. We show this empirically in our experiments. Critically, this process does not rely on stronger LLM for critique or on past interactions created by other LLMs. Figure 3 shows the decoder prompt. We experiment with binary 1Grices maxim of quantity: one tries to be as informative as one possibly can, and gives as much information as is needed, and no more; Grices maxim of manner: one tries to be as clear, as brief, and as orderly as one can in what one says, and where one avoids obscurity and ambiguity. (Grice, 1975) 2For simplicity of notation, we omit the time step in this section. 3We do not compute feedback for the last action in each interaction because there is not followup interaction. For simplicity, Draw does not include them."
        },
        {
            "title": "Preprint under submission",
            "content": "Feedback Decoder Prompt User: Please carefully read the following conversation and answer: Is the very last utterance from the speaker positive or negative positive, neutral, or negative feedback? Often negative feedback include corrections and keywords like no, not, undo, dont, with generally negative sentiment, while positive feedback often includes good, yes, correct, okay, or simply move on to the next stage. Lean towards negative if it sounds neutral. (start of the conversation) Listener: Deselect select Speaker: yes, pick the thin person with triangle head Listener: Select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Action to focus on) Speaker: yes, pick the house with chimney . . . . . . . . . . . . . . . . . . . . . . . (Feedback) (end of the conversation) Answer single word, Positive, or Negative Positive, Neutral or Negative. Assistant: Positive Figure 3: The prompt used to decode feedback from past interactions. The figure combines the prompts for both binary and ternary feedback decoding. The parts that belong to the binary case only are colored green, while parts that belong the ternary case are colored orange. The verbal feedback generated by the model is in bold. Additional comments for readability are in magenta italics. or ternary feedback. Ternary adds neutral on top of the positive and negative binary options. The feedback decoder is designed to identify general linguistic cues, and not for the specific task we study. We assume no access to any auxiliary annotation or privileged information (e.g., not inferring based on whether the policy selects ground truth target in turn, or whether an entire interaction ends early), although they are likely to be useful signals as explored in Pang et al. (2023). 4.2 LEARNING The feedback decoding process transformers the dataset from Draw to = i=1. We study several learning approaches using this data: supervised learning, offline reinforcement learning (RL), or the KTO-style utility maximization (Ethayarajh et al., 2024). (x(i), ˆa(i), p(i), ˆγ(i)) } { Supervised Learning We fine-tune on positive data points (ˆγ(i) = positive) and discard data points predicted as neutral or negative. We use cross entropy loss with additional label smoothing to prevent overfitting and encourage exploration. Our setup is distinct from conventional supervised learning in that the data is coming from the model interactions (i.e., on-policy), and not from given dataset. Also, we run the learning process iteratively, each time with more data. We do not design the supervised approach in any special way to fit these changes, but this is potential avenue for future work, which can further improve performance. Reinforcement Learning We follow prior work (Kojima et al., 2021) and use simple REINFORCEstyle policy gradient (Williams, 1992). The categorical feedback γ(i) (i.e., the text generated by the prompted LLM) is mapped to numerical value with simple reward function: R(γ) = 1, 0, γ = positive γ = neutral 0.1, γ = negative . Dropping the i-superscripts for simplicity, the gradient estimator for single example is: = cR(ˆγ) x; θρ+1) logP (ˆa = (cid:40) 1, (ˆax;θρ+1) , 0 if R(ˆγ) if R(ˆγ) < 0 , (1) (2) where the coefficient downweights examples with negative reward by their inverse propensity score (Kojima et al., 2021). This is critical because limP ()0 logP ( . In practice, we also discard data points with predicted neutral feedback (R(ˆγ) = 0). ) ="
        },
        {
            "title": "Preprint under submission",
            "content": "We choose REINFORCE for its simplicity. The positive case reduces to be mathematically equivalent to the gradient of supervised fine-tuning (SFT), whose optimization is relatively well understood. As opposed to other methods, such as PPO (Schulman et al., 2017), REINFORCE does not require reward model and has relatively few hyperparameters. This is critical with human-in-the-loop experiments, where broad parameter sweeps are not possible. Recent work (Ahmadian et al., 2024) also suggests REINFORCE can produce on-par results in LLMs with PPO despite its simplicity. Utility Maximization To experiment with utility maximization, we use Kahneman-Tversky Optimization (Ethayarajh et al., 2024). KTO was developed to learn from per-example binary human feedback, scenario that fits ours well. We consider examples with decoded positive feedback as desired utterances, those with decoded negative feedback as undesired, and discard those with neutral feedback. We refer readers to Ethayarajh et al. (2024) for the definition of the objective."
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "Interaction Instantiation We use the KILOGRAM (Ji et al., 2022) tangram images, following Gul & Artzi (2024). KILOGRAM contains 1,013 images. We randomly split them into main split (912 tangrams) and development split (101 tangrams). We create interaction contexts by randomly sampling 10 tangrams, and randomly select 35 as targets. The development split is exclusively used for seeding the initial listener policy πθ0 , and all human-bot interactions are conducted on images from the main split, i.e., tangrams that the seed policy πθ0 has never seen before. Model and Initialization We use IDEFICS2-8B (Laurencon et al., 2024) as our model for both the policy and feedback decoder. We fine-tune with LoRA (Hu et al., 2022). We seed the initial policy πθ0 by fine-tuning the pretrained IDEFICS2-8B weights on small supervised dataset of 90 successful turns from 25 human-human games constructed with the development split tangrams, augmented with 12 synthetically generated deselection turns, because while necessary for human-model interactions, deselections are rare in human-human interactions (Appendix B.2). D0 is reused in continual training via rehearsal. We validate our design online with 30 main-split human-bot pilot interactions, or offline with validation set of 344 successful main-split human-human turns (Appendix A). We use the original IDEFICS2-8B for feedback decoding, because the narrow focus of our data is likely to inhibit some general linguistic knowledge. This means we cannot see improvement in the model feedback decoding capability, likely low-balling the potential of the approach. It remains an important direction for future work to keep the decoder model in sync with the policy. This requires deployments that include high domain diversity. We observe the original IDEFICS2-8B to provide robust feedback decoding out of the box, confirming our hypothesis, and providing solid ground for our experiments. System Variants We study six system variants based on two dimensions: (a) feedback decoder configuration (binary vs. ternary); (b) optimization methods (supervised vs. REINFORCE vs. KTO): B-SUP and T-SUP binary (B) / ternary (T) that only trains on positive data points with supervised fine-tuning objective (SUP). B-RL and T-RL trains on both positive and negative data points using REINFORCE. B-KTO and T-KTO are like B-RL and T-RL, but using KTO. For variants involving negative data points (B-RL, T-RL, B-KTO, and T-KTO), we subsample negative ones to keep the positive:negative ratio close to 5:4 (Ethayarajh et al., 2024). Deployment We conduct three rounds of training-deployment for all six systems and three more rounds for B-SUP. We select B-SUP for another three rounds because it is the most promising variant after three rounds, and we want to observe its progress over longer period. The reason for this cascaded design is the high cost of experiments. We do not distinguish between training and evaluation in the traditional sense. Instead, all listener policies are evaluated live on MTurk on about 330 human-bot interactions each round containing roughly 2400 turns. Then the same data is used to train the next iteration of policies respectively. The policies in the same round are deployed concurrently in randomized experiment on the same set of games to mitigate human biases and variances due to game difficulty. More details on crowdsourcing are in Appendix A.3."
        },
        {
            "title": "Preprint under submission",
            "content": "Learning Implementation Details We use the validation set for model selection throughout continual learning. Following prior work (Misra et al., 2017; Muller et al., 2019; Liu et al., 2022), we add an entropy term and length normalization to all three objectives to reduce over-fitting given the relatively small amount of data. Appendix provides additional reproducibility details. Unlike with REINFORCE, where we train from scratch each round, when using KTO, we continually fine-tune from previous model checkpoint θρ to obtain θρ+1 with data accumulation. This was shown to outperform training from scratch in pilot studies. Evaluation We evaluate each system variant at each round by the success rate during the live deployment. We report both interactionand utterance-level success rates. The interaction level success rate is straightforward - whether the game ended with all targets selected by the listener and nothing else. The utterance level success rate is more nuanced because we do not have access to the ground truth, i.e., the intended action. We sample 1,000 utterances per round from B-SUP to annotate by MTurk workers post hoc. We report two measures: exact match between the annotation and model action and similarity score, which is based on the computed similarity between the tangrams selected or deselected during the turn by the human annotator and the system. We also evaluate the quality of the feedback decoder by comparing its predictions with human interpretations collected during the post-hoc annotation. Because of cost, we cannot do post-hoc annotation for all system variants, so we also report click accuracy, which approximates utterance-level performance well. It measures the ratio of the model actions that lead to selection statuses that do not violate the set of targets (i.e., selections of target tangrams are good, deselection of non-target tangrams are good). Lastly, we track the number of turns per interaction. Appendix B.4 provides full definitions of our metrics."
        },
        {
            "title": "6 RESULTS AND ANALYSIS",
            "content": "We deploy our models for three rounds, with additional three rounds for B-SUP, the best-performing variant, to better understand long-term dynamics. All our results are from concurrent randomized deployment, where the models interact with humans in real time. We collected total of 7,230 interactions consisting of 55,004 utterances over 4 weeks, at cost of $11,180 USD. Figure 4 shows the deployment statistics for all six system variants, as well as control deployments for the initial policy and human-human games.4 Figure 5 shows utterance-level statistics for B-SUP from the post-hoc annotations we collected. The interaction success rate of all systems improves monotonically in the first three rounds, except for B-KTO in round three. We conduct three more rounds with B-SUP, the leading system after the first three rounds. B-SUP then plateaus, and even shows temporary decrease in performance, before resuming its improvement.5 82%) and utterance-level Overall, B-SUP improves interaction-level success rate by 51% (31% 53%). At the last round, following the plateau, B-SUP interaction success exact match by 22% (31% rate improves by 5% (77% 82%). The number of turns follows these trends. As the policy gets better, more games are completed within the allotted number of turns, and even faster. B-SUP starts with 8.9 turns per game, and concludes with 6.7 per game. The center panel of Figure 5 shows that actions taken by the policy increasingly resemble human actions, even mistakes (actions that receive negative feedback) become more similar to human actions. All other statistics largely track these, except some of the utterance-level statistics around when B-SUP plateaus. While all show deviation from the monotonous earlier trend, some show temporary decrease and not just stagnation, but delayed by one round. This illustrates the complex dynamics of continual learning, which we explore in more detail below. There remains significant gap between B-SUP (our leading system) and HH (human-human interactions), which shows perfect task success rate and almost double efficiency (i.e., tasks are completed 4We present results in rounds for simplicity. Appendix connects rounds to cumulative number of interactions. 5The reasons behind the plateau are hard to infer. One hypothesis we considered is that changes in the amount of data over time made some settings sub-optimal. Specifically, we considered our LoRA adapter settings, as they impact the expressiveness of fine-tuning. We conducted separated deployment, branching out from round three for two rounds (four and five) with B-SUP and more expressive adapters. We observed this increase in expressivity allows the model to continue its monotonous improvement. Appendix provides the details. This mini experiment illustrates the complexities of continual learning with current learning systems."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 4: Task performance and efficiency improve as the policy learns from more past interactions. We present deployment results across three rounds for six concurrent systems, and three more rounds for the top system B-SUP, together with human-human references (HH) and redeployment of the initial policy πθ0 (CONTROL). Left: interaction-level success rate ( , higher is better). Center: ). Right: micro-level performance by click interaction-level efficiency by # turns per interactions ( accuracy ( ). Shades are 95% confidence intervals by bootstrapping with 10,000 resamples. Figure 5: Turn-level performance of B-SUP evaluated by post-hoc human annotations. Left: % turns where the policys action ˆa matches exactly the human listeners action ( ). Center: similarity ). Even actions that receive negative between the policys action and the human listeners action ( feedback in deployment (NEG FB) are increasingly similar to human actions. Right: % turns that ). annotated to have received positive implicit feedback from human listeners ( with much shorter interactions). Our intuition is that the gap is due to the lack of long-term credit assignment in our learning method. This is especially influential in learning to reasons about later turns. Later turns shows much stronger dependence on earlier turns, creating more complex reasoning problem and harder credit assignment problem with the longer context. This learning challenge is compounded by the data scarcity: we have significantly less data for later turns, because not all interactions include them. This can potentially be addressed by not including all past turns in the context (i.e., sliding window approach). User Adaptation potential confounder is user adaptation: the improvement in interaction success rate could have been attributed to users adapting to the interaction scenario and the system, instead of policy improvement (Hawkins et al., 2020a). We redeploy the initial policy πθ0 concurrently to final B-SUP round to test this (CONTROL in Figure 4). The interaction success rate of CONTROL remains relatively stable over time (31% 33%), suggesting that speaker familiarity and adaptation do not explain the overall 51% absolute improvement in B-SUP interaction success rate. Positive Only vs. All Data The difference between systems that use positive learning signals only (B-SUP, T-SUP) and these that use all (B-RL, T-RL, B-KTO, T-KTO) is in learning objectives (supervised vs. RL/KTO) Overall, the systems based on positive signals only perform better. It is expected that positive signals will be more informative for learning. Our policy acts in large action space. Negative rewards suppress specific action, but without more information about what is good action, they simply encourage uniform distribution. This has been shown to have helpful regularizing effect in past work (Kojima et al., 2021). However, not only negative feedback does not help in meaningful way, it seems to confuse the learner. The positive-only systems that, in effect,"
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 6: Confusion matrices of the binary (top row) and ternary (bottom row) feedback decoders over rounds. The feedback decoder yields precise positive signals, even in early rounds. have access to fewer learning signals perform better. Utilizing negative signals better is an important direction for future work. Feedback Decoder Evaluation We evaluate the quality of the feedback decoder through our annotation task. For each turn, workers annotate if the speaker was satisfied with the answer given their followup utterances. Figure 6 shows feedback decoding confusion matrices over time. The feedback decoder performance is relatively stable throughout the rounds, showing robustness to changes in the data distribution. If we collapse together actual positives and neutrals, we observe above 90% precision consistently. The ternary feedback decoder is more conservative compared to the binary one and labels more positive turns as neutrals. This is task-dependent trade-off. The zero feedback of neutrals essentially eliminates the examples, but allows for slightly cleaner data. Here we empirically observe it is beneficial to have slightly noisy data but more of it. Supervised vs. REINFORCE vs. KTO Overall, the supervised variants (B-SUP and T-SUP) perform best. The KTO variants (B-KTO and T-KTO) trail after the REINFORCE variants (B-RL, T-RL). B-KTO even diverges at some point and starts losing performance fast. We suspect this is because the KTO recipe does not work well in the challenging optimization scenario of continual learning, where the model is fine-tuned multiple times. We observe that B-KTO deteriorates in rounds two and three, and starts generating illegal outputs (e.g., Deselect select). Appendix B.3 describes quick intervention we applied to try to mitigate this issue. Although it eliminated the illegal outputs, the quality remained low. It is possible that further refinement of how KTO is used or further tuning of its hyperparamters will help. However, this is complex process in live deployment. Language Analysis We analyze the human instructions and how they change as the policy learns from more interactions (Figure 7). We observe reduction in vocabulary size and utterance length early on. This is expected, and follows known observations in how humans adapt to reduce cognitive costs (e.g., Clark & Wilkes-Gibbs, 1986; Effenberger et al., 2021). However, in later rounds, BSUP witnesses an increase in vocabulary size and utterance length. This surprising trend reversal is attributed to only three outlier workers, so does not express significant change in population behavior. The number of reset signals drops, another reflection of improved collaborated task performance. Such trends are fairly consistent across system variants, except for B-KTO, which also shows divergence in performance. We observe that initially workers tend to use Try again instead of directly describing target, or request reset with instructions like Deselect everything (Figure 15 and Figure 16). The occurrences of both decrease in later rounds. Even though the workers change their language, this does not really help the initial policy πθ0, which remains poor (Figure 4)."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Learning from Feedback Learning from feedback for LLMs is being studied extensively. RL from human feedback (RLHF) is maybe the most common technique (Ouyang et al., 2022). It relies on soliciting pair-wise preferences from annotators, which is significantly different than our reliance on signals from the interaction itself. Learning from feedback on single system output has also been studied, either in the form of binary feedback (Ethayarajh et al., 2024; Suhr & Artzi, 2023; Gao et al.,"
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 7: Language analysis of human instructions. All systems show decrease in instruction complexity in the first three rounds, except for B-KTO, suggesting adaptation and improved efficiency on the speakers side. Keyword-based analysis reveals that the number of reset/frustration signals drops, reflection of the model learning and collaboration improving. 2023) or through more expressive editing (Gao et al., 2024) or commenting and refinement (Li et al., 2017; Sumers et al., 2021; Scheurer et al., 2023). Hancock et al. (2019) trains separate supervised model to continually predict satisfaction levels, which is then used to pause interactions and solicit explicit feedback. Unlike this line of work, we do not solicit feedback, but rely on natural signals that arise from the followup interaction. Some of these utterances include explicit feedback, but many do not. Most important, our approach relies on the natural course of human interactions, and does not add solicitation of feedback. Learning from Naturally Occurring Signals Kojima et al. (2021) presents an approach to learn to generate instructions by observing how humans follow them, complementary mode of learning to our focus on general response. Pang et al. (2023) maximizes heuristics, such as the chance of long responses from humans, in chatbot scenario. Artzi & Zettlemoyer (2011) studied the use of naturally occurring recovery efforts (i.e., when the user switches to simpler language to relay information) to train symbolic semantic parser from corpus of dialogue interactions. In contrast, we opt for general approach to infer feedback from natural language interactions of the model itself. Concurrently to our work, Don-Yehiya et al. (2024), as well as Petrak et al. (2023), proposes an approach that uses naturally occurring feedback in conversations to filter large conversational corpus. The linguistic cues they rely on are similar to ours. Unlike our study, the model they improve is not the model that generated the interactions, creating distillation-like setup, where improvement is not coming from the models own interaction, but from other models. We focus on model self-improvement, where it is critical that no stronger model is involved, and the model drives its own improvement. second distinguishing factor is our interest in continual deployment with humans, whereas they follow standard train-test benchmarking recipe. This allows our work to expose dynamics that are otherwise hidden. Taken together, our work and Don-Yehiya et al. (2024) complement each other, and strengthen our conclusions. Their work shows the signal can be derived from large-scale diverse data, whereas ours shows how single-model loop can work over long period of time, and the dynamics it creates. LLMs that Self-improve common approach to improve models is via AI feedback, solicited from the model itself or another model (Bai et al., 2022; Burns et al., 2023; Madaan et al., 2023; Kumar et al., 2024; Qu et al., 2024; Yuan et al., 2024; Li et al., 2024a). As opposed to AI feedback, we elicit real human feedback automatically from the interaction the model has while it is deployed. This signal is more on-the-job, and less likely to be influenced by model biases. We also use the same model for interaction and inferring feedback, ruling out concerns about distillation. Our work is somewhat analogous to the generator-verifier line of work (Anil et al., 2021; Li et al., 2024b; Kirchner et al., 2024), which relies on automatic verifiers (e.g., unittests). We retrospect and learn from implicit human feedback that naturally occurs in interactions, as an alternative verification signal."
        },
        {
            "title": "8 DISCUSSION",
            "content": "We introduce RESPECT: retrospective learning from interactions, an annotation-free approach by leveraging signals from naturally occurring feedback in interactions. We demonstrate its effectiveness in long-term deployments and robustness to system variants. As opposed to evaluating on static"
        },
        {
            "title": "Preprint under submission",
            "content": "benchmark, we design MULTIREF to study real interactions over period of time. We make trade-offs between the generality of the task, and the ability to iterate on prototype fast, and without high costs. It is important to expand this type of study to other tasks, such as summarization or conversational question answering, where similar signals may be more complex, far apart, or demand long-term credit assignment. Another interesting direction is expanding the expressivity of the feedback decoder, such that it recovers more expressive signal (e.g., natural language explanation)."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "ZC developed and implemented the methods, trained models, conducted pilot studies, and the continual learning study. MOG contributed code for tangram reference games and designed the human evaluation study. MOG, AW, YC, and GG designed the MULTIREF scenario. YC and GG implemented the MULTIREF interface, recruited MTurk workers, and collected human-human interactions, with MOG and AWs mentoring. ZC, MOG, and AW contributed to the writing. YA advised and oversaw all parts of the project."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research was supported by NSF under grants No. 1750499 and OAC-2311521, NASA under award No. 20-OSTFL20-0053, gift from Open Philanthropy, gift from Apple, the National Artificial Intelligence Research Resource (NAIRR) Pilot, the Frontera supercomputer supported by the National Science Foundation (award NSF-OAC 1818253) at the Texas Advanced Computing Center (TACC) at The University of Texas at Austin, and the Delta advanced computing and data resource which is supported by the National Science Foundation (award NSF-OAC 2005572). GG was supported by an NSF REU supplement for NSF grant No. 175049. YC was supported by Bowers Undergraduate Research Experience program. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, NASA, or the other funders."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1224812267, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.662. URL https://aclanthology. org/2024.acl-long.662. 6 Abdullah Almaatouq, Joshua Becker, James P. Houghton, Nicolas Paton, Duncan J. Watts, and Mark E. Whiting. Empirica: virtual lab for high-throughput macro-level experiments. Behavior Research Methods, 53(5):21582171, October 2021. ISSN 1554-3528. doi: 10.3758/s13428-020-01535-9. URL https://doi.org/10.3758/s13428-020-01535-9. 16 Cem Anil, Guodong Zhang, Yuhuai Wu, and Roger Grosse. Learning to Give Checkable Answers with Prover-Verifier Games, August 2021. URL http://arxiv.org/abs/2108.12099. 10 Yoav Artzi and Luke Zettlemoyer. Bootstrapping semantic parsers from conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 421432, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL http://www.aclweb. org/anthology/D11-1039. 10 Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas"
        },
        {
            "title": "Preprint under submission",
            "content": "Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, December 2022. URL http://arxiv.org/abs/2212.08073. 10 Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-toStrong Generalization: Eliciting Strong Capabilities With Weak Supervision, December 2023. URL http://arxiv.org/abs/2312.09390. 10 Herbert Clark and Deanna Wilkes-Gibbs. Referring as collaborative process. Cognition, 22(1): 139, 1986. 3, 4, 9 Shachar Don-Yehiya, Leshem Choshen, and Omri Abend. Learning from Naturally Occurring Feedback, July 2024. URL http://arxiv.org/abs/2407.10944. Anna Effenberger, Rhia Singh, Eva Yan, Alane Suhr, and Yoav Artzi. Analysis of language change in collaborative instruction following. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 28032811, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.239. URL https://aclanthology.org/ 2021.findings-emnlp.239. 9 Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. KTO: Model Alignment as Prospect Theoretic Optimization, September 2024. URL http://arxiv.org/abs/ 2402.01306. 2, 5, 6, 9, 19 Jean Fox Tree. Listening in on monologues and dialogues. Discourse processes, 27(1):3553, 1999. 4 Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi. Continually improving extractive QA via human feedback. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 406423, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 27. URL https://aclanthology.org/2023.emnlp-main.27. 9 Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, and Dipendra Misra. Aligning LLM Agents by Learning Latent Preference from User Edits, June 2024. URL http://arxiv.org/ abs/2404.15269. Noah Goodman and Michael Frank. Pragmatic language interpretation as probabilistic inference. Trends in cognitive sciences, 20(11):818829, 2016. 3 Herbert Paul Grice. Logic and conversation. Syntax and semantics, 3:4358, 1975. 4, 25 Mustafa Omer Gul and Yoav Artzi. CoGen: Learning from Feedback with Coupled Comprehension and Generation, August 2024. URL https://arxiv.org/abs/2408.15992v1. 6, 15, 16 Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 36673684, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1358. URL https://aclanthology.org/P19-1358. Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. Continual Adaptation for Efficient Machine Communication. In Raquel Fernandez and Tal Linzen (eds.), Proceedings of the 24th Conference on Computational Natural Language Learning, pp. 408419, Online, November 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.33. URL https://aclanthology.org/2020.conll-1.33. 8 Robert D. Hawkins, Michael C. Frank, and Noah D. Goodman. Characterizing the Dynamics of Learning in Repeated Reference Games. Cognitive Science, 44(6):e12845, 2020b. ISSN 1551-6709. doi: 10.1111/cogs.12845. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs. 12845."
        },
        {
            "title": "Preprint under submission",
            "content": "William S. Horton and Richard J. Gerrig. Speakers experiences and audience design: Knowing when and knowing how to adjust utterances to addressees. Journal of Memory and Language, 47 (4):589606, November 2002. ISSN 0749-596X. doi: 10.1016/S0749-596X(02)00019-0. URL https://www.sciencedirect.com/science/article/pii/S0749596X02000190. 4 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. 6, 18 Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and Yoav Artzi. Abstract Visual Reasoning with Tangram Shapes. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 582601, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.38. URL https://aclanthology.org/2022.emnlp-main.38. 4, 6, 15, 20 Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda. Prover-Verifier Games improve legibility of LLM outputs, August 2024. URL http://arxiv. org/abs/2407.13692. 10 Noriyuki Kojima, Alane Suhr, and Yoav Artzi. Continual Learning for Grounded Instruction Generation by Observing Human Following Behavior. Transactions of the Association for Computational Linguistics, 9:13031319, December 2021. ISSN 2307-387X. doi: 10.1162/tacl 00428. URL https://doi.org/10.1162/tacl 00428. 2, 5, 8, 10 Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, May 2024. URL http://arxiv.org/abs/2405.02246. 2, 6 Jiwei Li, Alexander H. Miller, Sumit Chopra, MarcAurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=HJgXCV9xx. 10 Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 1618916211, Bangkok, Thailand and virtual meeting, August 2024a. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.958. 10 Xiang Lisa Li, Vaishnavi Shrivastava, Siyan Li, Tatsunori Hashimoto, and Percy Liang. Benchmarking and improving generator-validator consistency of language models. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id= phBS6YpTzC. 10 Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:19501965, 2022. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB. 10 Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In Martha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 10041015, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1106. URL https://aclanthology.org/D17-1106."
        },
        {
            "title": "Preprint under submission",
            "content": "Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: distributed framework for emerging AI applications. In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation, OSDI18, pp. 561577, USA, October 2018. USENIX Association. ISBN 978-1-931971-47-8. 16 Rafael Muller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? Advances in neural information processing systems, 32, 2019. 7 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 2, 9 Richard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, and Jason Weston. Leveraging Implicit Feedback from Deployment Data in Dialogue, July 2023. URL http://arxiv.org/abs/ 2307.14117. 5, 10 Dominic Petrak, Nafise Moosavi, Ye Tian, Nikolai Rozanov, and Iryna Gurevych. Learning from free-text human feedback collect new datasets or extend existing ones? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1625916279, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.1011. URL https://aclanthology.org/2023.emnlp-main.1011. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching LLM agents how to self-improve. In ICML 2024 Workshop on Foundation Models in the Wild, 2024. URL https://openreview.net/forum?id=g5wp1F3Dsr. 10 Seymour Rosenberg and Bertram D. Cohen. Speakers and Listeners Processes in WordCommunication Task. Science, 145(3637):12011203, September 1964. doi: 10.1126/science.145. 3637.1201. URL https://www.science.org/doi/abs/10.1126/science.145.3637.1201. 3 Jeremy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training Language Models with Language Feedback at Scale, April 2023. URL http://arxiv.org/abs/2303.16755. 10 Michael Schober and Herbert Clark. Understanding by addressees and overhearers. Cognitive Psychology, 21(2):211232, April 1989. ISSN 0010-0285. doi: 10.1016/0010-0285(89)90008-X. URL https://www.sciencedirect.com/science/article/pii/001002858990008X. 3, 4 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, August 2017. URL http://arxiv.org/abs/1707.06347. Alane Suhr and Yoav Artzi. Continual learning for instruction following from realtime feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=ez6Cb0ZGzG. 2, 9 Theodore R. Sumers, Mark K. Ho, Robert D. Hawkins, Karthik Narasimhan, and Thomas L. Griffiths. Learning Rewards From Linguistic Feedback. Proceedings of the AAAI Conference on Artificial Intelligence, 35(7):60026010, May 2021. ISSN 2374-3468. doi: 10.1609/aaai.v35i7.16749. URL https://ojs.aaai.org/index.php/AAAI/article/view/16749. 10 Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. 2, 5 Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=0NphYCmgua."
        },
        {
            "title": "A THE MULTIREF GAME DESIGN AND DATA COLLECTION",
            "content": "A."
        },
        {
            "title": "INTERACTION DESIGN",
            "content": "MULTIREF is multi-target, multi-turn reference game between two players, speaker and listener. Each game starts with 10 tangrams as the context, with 35 tangrams designated as targets. The target designations are revealed to the speaker but hidden to the listener. The goal is to select all targets without selecting any non-targets. The speaker can only communicate with the listener through sequence of utterances, and only the listener can take selection and deselection actions. The interaction starts with speaker turn. Turns alternate between speaker and listener, with maximum of 20 turns. In each speaker turn, they type an utterance to send to the listener. Speaker turns are limited to 25 seconds. In each listener turn, they have 45 seconds to select or deselect images as instructed to by the speaker. The game concludes when the listener selects only and all targets, or the when the partners run out of turns. Appendix A.3 shows screenshots of the interface. Context Construction We follow Gul & Artzi (2024) and construct game contexts using 1,013 tangram images from KILOGRAM Ji et al. (2022). We group tangrams randomly into two splits: development split (101 tangrams) and main split (912 tangrams). The development split is exclusively used for seeding the initial listener policy π0. All human-bot interactions are constructed from the main split, i.e., tangrams that the seed policy π0 has never seen before. We construct all games with 35 target tangrams. More targets are generally harder, given the same maximum number of turns per interaction. A.2 HUMAN EVALUATION DESIGN Automatically evaluating turn-level policy performance is hard, because we have no ground truth (i.e., the selection and deselection actions intended by the speaker in each turn) to compare against. Similarly, we have no ground truth to systematically assess the feedback decoder quality. We conduct human evaluation surveys to address these problems. We annotate subset of B-SUP interactions, roughly 120 interactions or 1,000 turns per system-turn. We show human annotators complete interaction turn by turn, without revealing the underlying targets. For each turn, the annotation consists for two phases: 1. Ground-truth: we show context, currently selected tangrams, and instruction given by the speaker. We ask the annotator to annotate the listener action. The annotator action is considered as ground truth action for this turn. We use these labels for tune-level evaluation. After the action annotation, we reveal the action ˆa actually taken by the listener (i.e., the model) during the interaction. 2. Satisfaction: we present the follow-up utterance. We ask the annotator to rate if the speaker is satisfied with the listeners action, based on the follow-up utterance. They choose one of the following options: a. Yes. b. Yes, even though the listener did not perform all required selections/deselections. c. Yes, even though the listener made incorrect selections/deselections. d. No. The third option accounts for the listener accidentally selecting target tangram not intended by the speaker, but the speaker choosing to move on without correction or even validating the selection. We treat these labels as ground truth for evaluating feedback decoders. We annotate 5% of long-term human-bot interactions annotations by three different annotators, to estimate how reliable the annotations are. We observe 85% agreement on the correctness (whether ˆa = a) on ground truth stage,6 and 65% agreement on the ground-truth action across workers.7 For satisfaction annotation, we observe 93% agreement rate, illustrating the relative simplicity of extracting the signal that drive our learning process. 6The percentage of cases where all annotators agree that the bot did right or wrong. 7The percentage of cases where all three annotators provided exactly the same set of actions."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 8: The MULTIREF interface for the speaker in turn 1. Predefined targets are revealed to the speaker in black boxes. A.3 MTURK DETAILS Worker Recruitment We follow Gul & Artzis (2024) worker recruitment recipe. We require workers to have minimum 98% approval rate, at least 1,000 approved HITs (Human Intelligence Task), and be located in English-majority locales. All workers must watch video tutorial and pass quiz before gaining qualification to work on MULTIREF interactions. They must read thorough guideline and pass another quiz before granted access to human evaluation surveys. We recruit 33 expert workers to interact with LLMs in the main study and annotate by completing surveys after the main study. This study is exempted by Cornells Institutional Review Board. Payment We pay workers $0.81 USD per MULTIREF game, and bonus if the game is successful. Overall the estimated hourly wage is $13.00 USD, and closer to $23.00 USD by the end of the continual study when the LLM is fairly good at the game. On average human-bot game takes under 2 minutes. We pay workers $0.06 USD per turn for human evaluation surveys, or $0.08 USD if the turn annotation involves error modes. The estimated hourly wage is $16.00 USD for human evaluation surveys. On average it takes under 2.5 minutes to annotate one game. We set the payment scheme through pilot studies and aim at $15.00 USD hourly wage. Interface and Serving We implement MULTIREF using Empirica (Almaatouq et al., 2021) and on top of the code base of Gul & Artzi (2024). The speaker has 25 seconds to type into chat box each turn and hit Enter or submit, and the listener has 45 seconds to click on the tangrams to select or to deselect. The game ends if one party idles for one turn, and the party idling is not compensated. We serve on an EC2 instance. We serve LLM policies with the Ray framework (Moritz et al., 2018). We walk through the first turns of sample interaction in Figure 8, Figure 9, and Figure 10."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 9: The MULTIREF interface for the listener in turn 2, following the speaker turn in Figure 8. Targets are hidden for the listener, and the context tangrams are in different order. Here the listener has selected tangram given the instruction select the butterfly. Figure 10: The MULTIREF interface for the speaker in turn 3, following the listener turn in Figure 9 The listener selected non-target tangram, shown in red to the speaker."
        },
        {
            "title": "Preprint under submission",
            "content": "Policy prompt System: <img>A <img>B <img>C <img>D <img>E <img>F <img>G <img>H <img>I <img>J . . . . . . . . . . . . . . . . . . . . . (Context tangrams tagged by code names) System: none is selected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Turn 1 state) User: Select the letter with the large triangle back side . . . . . (Turn 1 utterance) Assistant: Select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Turn 1 action) System: currently selected User: unselect that one and try again Assistant: Deselect select System: currently selected User: almost, unselect that one and try one more time Assistant: Deselect select System: currently selected User: correct. Now select the rabbit facing right Assistant: Select System: currently selected User: no that is bird, try again Assistant: Deselect select System: currently selected User: no that is another bird, try again Assistant: Deselect select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Action) Figure 11: Policy prompt example with model predicted action and additional comments for readability."
        },
        {
            "title": "B LEARNING DETAILS",
            "content": "B.1 INTERACTION REPRESENTATION We encode the context as in Figure 11. We standardize action representation by ordering actions, for example, always produce Select rather than Select A. We shuffle the context images during training as the order of context tangrams should not have any impact on the interaction logic. B.2 POLICY INITIALIZATION We seed the initial policy π0 by fine-tuning the model on small dataset of 90 turns D0, where both the speaker and the listener are humans. We also experimented with prompting to initialize the policy. We find early that few-shot prompting yields random policy at best, likely because reasoning with abstract shapes such as tangrams is visually out-of-distribution for the model. There is significant distribution shift between human-human interactions, and human-policy interaction, especially early on when the model performs poorly. In practice, two major differences are the length of interactions and the prevalence of deselection instructions, which are rare in human-human interactions. We address the deselection issue with data augmentation. We synthetically generate turns where the speaker asks for deselections, and the listener complies. We augment the data with these at ratio of 1:12 to the existing data. This helps the LLM policy learn to deselect and recover from mistakes. This augmentation is only used for D0 and such distribution shift is not present in alter rounds, when learning from actual human-bot interactions. B.3 HYPERPARAMETERS AND OTHER IMPLEMENTATION DETAILS We use the instruction-tuned IDEFICS2-8B model for all policies. We fine-tune with LoRA adapters (Hu et al., 2022) (α=r=8, dropout=0.1) due to compute constraints. Appendix provides more LoRA details. We train each model with single GPU, RTX A6000, NVIDIA A100 40GB or 80GB. The time to train ranges between 224 hours, longer in later rounds as more data accumulates. For stopping criteria, we pick checkpoints by highest accuracy (exact match) among"
        },
        {
            "title": "Preprint under submission",
            "content": "Policy prompt with deselection augmentation [Previous turns omitted] System: none is selected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Previous turns) Speaker: Man in hat Listener: Select System: currently selected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Augmented state) Speaker: Wrong, undo what you selected . . . . . . . . . . . . . . . (Augmented utterance) Listener: Deselect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (Augmented action) Figure 12: An example of deselection augmentation with augmented action and comments. three seeds on hold-out validation set of 344 turns DHH human-human games the main split of tangrams. We summarize hyperparameters in Table 1. val . The validation set is curated from 92 Hyperparameter Search Space Supervised REINFORCE KTO Optimizer Learning rate Learning rate decay Epochs Warm-up steps Weight decay Effective batch size Entropy weight βKTO Temperature } { 1e-6, 1e-5, 1e-4, 2e-4 } { no, cosine, linear } { 5, 10, 20, 40 0, 10, 50 } { 0, 0.01, 0.1 } { 16, 32, 48, 64, 128 { 0, 0.01, 0.5, 0.1 } { 0.01, 0.1, 0.5 } { } AdamW 1e-4 cosine 20 10 0.01 64 0.01 AdamW 1e-4 cosine 20 10 0.01 64 0.01 1 RMSProp 1e-5 no 20 10 0.01 64 0.1 0.5 1 Table 1: Hyperparameter settings. Data Imbalance The decoded feedback is imbalanced, with more negative examples than positive examples (3:1 to 2:1), especially at early rounds of continual learning. We address this by weighing 0.1 for RL or λd and λu for KTO, and by the loss by the absolute value of the reward, i.e., downsampling negative examples per batch, such that the number of positive examples and negative examples is roughly 5:4. KTO Stability Deviation from the original KTO implementation by higher learning rate, higher β, more epochs, produce better results empirically on the validation set in pilot and round ρ = 1. However, in round ρ = 2, B-KTO policy start to degenerate by producing nonsensical actions such as Deselect select or Deselect select select. We attempt to mitigate this issue during training round ρ = 3 by switching from weighing λd = 4 and λu = 1 as recommended in Ethayarajh et al. (2024) to λd = λu = 1, plus downsampling negative examples. We also introduce regex-based constrained decoding to prevent nonsensical actions for B-KTO and T-KTO policies in round ρ = 3. Despite that, the KTO group performs worse in live interactions (Figure 4). We suspect KTO is more challenging to optimize for iterative continual learning, but we suspect further tuning (with higher computational costs) can reduce or even eliminate these issues. B.4 EVALUATION METRICS Interaction-level Metrics live deployment interactions. They do not require further annotation. Interaction performance and statistics are computed automatically from 1. Success rate = # successful interactions / # all interactions. An interaction is successful if the listener selects all and only targets before running out of 10 turns. This is the primary metric we use to evaluate the performance of the LLM policy. 2. # Turns per interaction. This is measure of collaborative efficiency."
        },
        {
            "title": "Preprint under submission",
            "content": "Turn-level Metrics with Reference to Human Annotation We compute turn-level metrics either with respect to HH games where we consider human listener action as ground truth (e.g., DHH val ), or with respect to B-SUP games where we consider actions annotated in post-hoc surveys as ground truth. When computed with live interactions, these metrics are biased towards longer or failed interactions because they have more turns than successful interaction. 1. Exact match = # exact match / # all turns. An exact match is when the action taken by the policy matches exactly the action labeled/taken by human listeners (ˆa = a). 2. Similarity = Sim(ˆa, a) is composite metric. Let (p, q) : evaluates the similarity of between two images p, ˆa = tangrams. Denote the ground truth actions as = similarity between two actions is defined as: ˆp1, ˆp2, ..., ˆpˆn, ˆq1, ˆq2, ..., ˆq ˆm { be function that . Let the action taken by policy be where are the selected tangrams and are the deselected 2, ..., . The } 1, p 2, ..., m , 1, { } Sim(ˆa, a) = 1 ˆnn + ˆmm (cid:16) i=1Σn Σˆn j=1f (ˆpi, ) + Σ ˆm i=1Σm j=1f (ˆqi, ) (cid:17) . max(ˆn, n), and If only one of ˆn and is zero, we rewrite Σˆn ˆnn in the denominator with max(ˆn, n), intuitively assigning -1 for each missed selection. This edge case is similarly treated for ˆm, and deselection. We compute similarities using embeddings from the tangram fine-tuned CLIP model of Ji et al. (2022) . j=1f (ˆpi, ) with i=1Σn 3. Positive feedback = # turns receiving positive feedback / # all turns. An action receives positive feedback if speaker is satisfied with the listeners action in the followup interaction. This is labelled in human evaluation survey. Micro-level Metric with Reference to Ground Truth Targets We compute click accuracy with respect to the ground truth targets (instead of the targets intended by the speaker). This is cheaper because it does not require human annotation, so we can compute it for all system variants and all interactions. However, this measure produces false positives when an action selects target not intended by the speaker. In practice, though, we find it correlates well with our human-annotated evaluation. We compute click accuracy for turn given its context and action ˆa. We denote the set of ground truth targets in this interaction as , then for each click in ˆa (select or deselect), we compute the click accuracy as: , the set of currently selected context tangrams as Click accuracy(c, ) = , (cid:26)1 0 if (c otherwise / ) (c / ) Intuitively, click is approximately accurate if it selects target or deselects non-target. We compute this for all clicks from all interactions in round for all systems in Figure 4. Corpus-level Metrics We analyze speaker instructions per system-round. The keyword used to generate the analysis in Figure 7 are: 1. # Reset = occurrences of phrases in reset, restart, from scratch, all over, start over, deselect { everything, deselect all, remove everything, remove all, clear everything, clear all, unselect everything, unselect all, drop everything, drop all 2. # Try again = occurrences of phrases in { try again, try one more time, the other one } }"
        },
        {
            "title": "C CUMULATIVE NUMBER OF INTERACTIONS OBSERVED",
            "content": "The main text includes results by round. We collect roughly 330 interactions per policy per round. Due to the uncertainty of live data collection, we do not always hit this exact number for each variant and round. Figure 13 shows the cumulative number of human-bot interaction seen by policy variant by each round."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 13: Cumulative number of human-bot interactions used to train the policy each round."
        },
        {
            "title": "D ADDITIONAL ENHANCED LORA LAUNCH",
            "content": "We suspect the plateau of B-SUP in Figure 4 is partially due to the limited expressivity of LoRA adapters we used. We test this hypothesis by deploying round ρ = 4 and ρ = 5 again with enhanced LoRA adapters. We use the same hyperparameters as in Section B.3 except additional adapters. The original adapter placement is on the text model, the modality projector, and the perceiver resampler. Adapters include the down projection layers, the gate projection layers, the up projection layers, and the key/query/value projection layers. In comparison, the enhanced launch adds adapters on the vision model, including the out projection, the first and the second fully connected layers, besides the projection layers on text models. Figure 14 shows the results from this complementary deployment. The enhanced LoRA adapters yield small improvement in interaction success rate compared to the original launch, yet the overall slowdown is evident. This suggests LoRA expressivity has some effect, but other effects are also limiting the LLM policy from continuing its earlier improvement trends. Figure 14: Success rate of B-SUP with additional LoRA adapters in round 4 and 5."
        },
        {
            "title": "E FEEDBACK DECODER DESIGN",
            "content": "The prompt design is minimal, general, and task-agnostic. We validate the prompt with manual inspection prior to continual learning launch and human surveys. Considering only the most recent is sufficient to produce satisfactory decoding results, two action-utterance turns and more history seems to distract the decoder. ˆai1, ui, ˆai, ui+1 We also experimented with numerical reward (i.e., decoding real number), experimenting with discretized reward space of . Our experiments show the model is not well calibrated for such decoding. .0, .1, .5, .9 { }"
        },
        {
            "title": "F INTERACTION CASE STUDIES",
            "content": "Figures 1518 show case studies that illustrate the diversity of MULTIREF interaction scenarios. Black borders indicate targets. Yellow dots indicate actions taken by the listener. Green borders indicate correct selections, while red borders indicate wrong selection. Figure 15: The speaker is left with the last target at Turn 4. Failing, they provide an additional description in Turn 5, and eventually resort to try again without describing the target in Turn 6. The initial turns illustrate how feedback is implied, rather than specified explicitly. The interaction concludes successfully."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 16: The speaker asks to deselect everything in Turn 3 to reset, an expression of frustration. The interaction concludes successfully."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 17: The abstractness and ambiguity of tangrams lend to complex interactions. There are two dogs in the context, and the listener struggles to disambiguate or identify the target. The interaction concludes successfully."
        },
        {
            "title": "Preprint under submission",
            "content": "Figure 18: The speaker asks for two targets in Turn 1, exemplifying Grices Maxims of Quantity - one tries to be as informative as one possibly can, and gives as much information as is needed, and no more (Grice, 1975). The interaction concludes successfully."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Cornell Tech, Cornell University"
    ]
}