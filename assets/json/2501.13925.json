{
    "paper_title": "GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing",
    "authors": [
        "Akashah Shabbir",
        "Mohammed Zumri",
        "Mohammed Bennamoun",
        "Fahad S. Khan",
        "Salman Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large multimodal models (LMMs) have recognized fine-grained grounding as an imperative factor of visual understanding and dialogue. However, the benefits of such representation in LMMs are limited to the natural image domain, and these models perform poorly for remote sensing (RS). The distinct overhead viewpoint, scale variation, and presence of small objects in high-resolution RS imagery present a unique challenge in region-level comprehension. Moreover, the development of the grounding conversation capability of LMMs within RS is hindered by the lack of granular, RS domain-specific grounded data. Addressing these limitations, we propose GeoPixel - the first end-to-end high resolution RS-LMM that supports pixel-level grounding. This capability allows fine-grained visual perception by generating interleaved masks in conversation. GeoPixel supports up to 4K HD resolution in any aspect ratio, ideal for high-precision RS image analysis. To support the grounded conversation generation (GCG) in RS imagery, we curate a visually grounded dataset GeoPixelD through a semi-automated pipeline that utilizes set-of-marks prompting and spatial priors tailored for RS data to methodically control the data generation process. GeoPixel demonstrates superior performance in pixel-level comprehension, surpassing existing LMMs in both single-target and multi-target segmentation tasks. Our methodological ablation studies validate the effectiveness of each component in the overall architecture. Our code and data will be publicly released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 2 9 3 1 . 1 0 5 2 : r a"
        },
        {
            "title": "GeoPixel",
            "content": ": Pixel Grounding Large Multimodal Model in Remote Sensing Akashah Shabbir 1 Mohammed Zumri 1 Mohammed Bennamoun 2 Fahad S. Khan 1 3 Salman Khan 1 4 1Mohamed bin Zayed University of AI,2The University of Western Australia, 3Linkoping University, 4Australian National University {akashah.shabbir,mohammed.zumri}@mbzuai.ac.ae (cid:128) https://github.com/mbzuai-oryx/GeoPixel"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large multimodal models (LMMs) have recognized fine-grained grounding as an imperative factor of visual understanding and dialogue. However, the benefits of such representation in LMMs are limited to the natural image domain, and these models perform poorly for remote sensing (RS). The distinct overhead viewpoint, scale variation, and presence of small objects in high-resolution RS imagery present unique challenge in region-level comprehension. Moreover, the development of the grounding conversation capability of LMMs within RS is hindered by the lack of granular, RS domain-specific grounded data. Addressing these limitations, we propose GeoPixel - the first end-to-end high-resolution RS-LMM that supports pixellevel grounding. This capability allows fine-grained visual perception by generating interleaved masks in conversation. GeoPixel supports up to 4K HD resolution in any aspect ratio, ideal for high-precision RS image analysis. To support the grounded conversation generation (GCG) in RS imagery, we curate visually grounded dataset GeoPixelD through semi-automated pipeline that utilizes set-of-marks prompting and spatial priors tailored for RS data to methodically control the data generation process. GeoPixel demonstrates superior performance in pixel-level comprehension, surpassing existing LMMs in both single-target and multi-target segmentation tasks. Our methodological ablation studies validate the effectiveness of each component in the overall architecture. Our code and data will be publicly released. 1. Introduction Recent large multimodal models (LMMs) (Liu et al., 2024a; Dai et al., 2023; Bai et al., 2023b; Chen et al., 2024b) have utilized the foundational capabilities of Large Language Models (LLMs) (Touvron et al., 2023; Chiang et al., 2023; Javaheripi et al.; Bai et al., 2023a) and successFigure 1. An example of visually grounded detailed descriptions generated by the proposed GeoPixel, highlighting its ability to interpret and segment high-resolution remote sensing imagery with fine-grained precision. The model applies distinct masks to key objects (ground track field, swimming pool, soccer field) and semantic mask to smaller objects (vehicles). It effectively identifies spatial positions (e.g., center, top) and relationships (within the sports complex) while distinguishing between the global context (buildings, roads, green spaces) and localized structures. fully expanded their horizon to the visual modality with promising capabilities. Recent LMMs can not only perform visual recognition, but also excel in advanced perception and reasoning required for vision-language tasks such as visual question answers, image captioning, visual grounding, and referring expression segmentation. Grounding LMMs (Rasheed et al., 2024; Ma et al., 2025; Zhao et al., 2023) have further advanced the fine-grained contextaware interpretation of complex visual information by allowing textual outputs to be associated with object instances. Facilitated by large-scale data in the natural images domain, grounding multimodal models pre-trained on extensive datasets have shown impressive capabilities, achieving performance levels comparable to specialist models. However, with increasing granularity of vision and language understanding, these general domain models exhibit signifi1 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing cant limitations in adequately supporting complex earth observation tasks. The performance degradation is influenced not only by the unique vantage point inherent to remote sensing (RS) images but also by large variations in the objects size and orientation. Moreover, in high-resolution remote sensing imagery, objects of interest may exhibit challengingto-segment spatial footprints, such as narrow bridges that connect urban landscapes and play critical role in city traffic planning, adding further complexity to the task. Existing vision language models in RS (Luo et al., 2024; Zhang et al., 2024b; Kuckreja et al., 2024) use quantized coordinates in the form of bounding boxes to localize and ground objects in their response. Such representation structure is not adequate to associate correct object semantics and also adds computational burden to the LLM that scales with the number of distinguishable objects. Furthermore, monitoring the geospatial environment and its entities demands broader spatial perspective, now increasingly achievable through advancements in RS technologies that provide high-resolution imagery. However, despite the availability of such rich data, current LMMs in RS struggle to fully exploit this spatial detail. These models often struggle with suboptimal resolution capabilities, hindering their ability to capture the intricate patterns present in high-resolution RS images. In addition, existing RS datasets often lack fine-grained spatial association between objects and their corresponding linguistic descriptions. To address these issues, we present GeoPixel, model that can generate detailed natural language response for highresolution RS image with corresponding geospatial object segmentation masks. Our contributions are as follows: Our proposed LMM, GeoPixel, is explicitly designed for high-resolution RS image analysis with advanced multi-target pixel grounding capability. Our model adaptively divides the input images into local and global regions, enabling efficient encoding and analysis by accommodating up to 4k resolution. We create GeoPixelD, multi-modal grounded conversation generation (GCG) dataset comprising 53,816 grounded phrases linked to 600,817 object masks, specifically tailored for RS image understanding. GeoPixelD offers hierarchically structured annotations, providing rich semantic descriptions that integrate both comprehensive, scene-level contextual information and precise, localized object-level details. Extensively granular annotations are created with segmentation masks through semi-automated, scalable pipeline that integrates prior-informed visual prompting with state-of-the-art LMMs and ensures quality via rigorous verification and filtering steps. We introduce comprehensive benchmark designed for the systematic evaluation of RS LMMs in finegrained visual understanding tasks. This benchmark includes 5,427 manually validated pairs of referring expressions and segmentation masks, encompassing 61,384 annotated objects in RS imagery within detailed descriptions having an average length of 647 characters. Our benchmark offers robust basis for assessing the models capabilities in interpreting and responding to complex, spatially grounded information. 2. Related Work Large Multimodal Models (LMMs): LMMs build on the success of LLMs to acquire vision capabilities. Pioneer works such as LLaVA (Liu et al., 2024b), MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023) and mPLUGOwl (Ye et al., 2023b) aligned visual features with language representations through vision language connector, enhanced by instruction tuning to improve multimodal integration. Improving beyond image-level understanding, models such as GPT4RoI (Zhang et al., 2023), InternGPT (Liu et al., 2023b) and RegionGPT (Guo et al., 2024) introduce regional understanding by allowing inputs such as points, masks, and bounding boxes. Some models feed image coordinates directly into the language model, while others employ additional feature extraction modules to represent specific image regions features effectively. Grounding LMMs: Region-level comprehension is further expanded by models such as Kosmos-2 (Peng et al., 2024), Ferret (You et al., 2023), Shikra (Chen et al., 2023), Pink (Xuan et al., 2024) and LION (Chen et al., 2024a) that allow for the precise location of objects in their outputs based on textual descriptions, capability known as grounding. These models localize objects on coarse scale using bounding boxes. Recent models (Lai et al., 2024; Rasheed et al., 2024; Xia et al., 2024; Ren et al., 2024; Zhang et al., 2024d; Liu et al., 2023a) focus on achieving more fine-grained visual and linguistic semantic alignment, by exploring pixel grounding. LISA (Lai et al., 2024), PixelLM (Ren et al., 2024) and GLaMM (Rasheed et al., 2024) incorporate [SEG] token into the LLMs vocabulary, leveraging its corresponding token embedding as conditioning input for SAM (Kirillov et al., 2023) to enable segmentation. Additionally, GSVA (Xia et al., 2024) introduces [REJ] token to explicitly learn to reject specified targets. Whereas Llava-plus (Liu et al., 2023a) employs LLMs as agents to assign tasks to the segmentation expert. Our work aligns with pixel-grounding approaches, such as those in (Lai et al., 2024; Ren et al., 2024; Rasheed et al., 2024). However, these models do not interpret the distinct top-down perspective and cannot differentiate complex spatial arrangements of remote sensing (RS) imagery. In addition, the models restricted input size, typically limited 2 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Table 1. Comparison of remote sensing large multimodal models (RS-LMMs), focusing on their grounding capabilities. The Region Output column highlights the models ability to associate objects with specific spatial regions. Existing models primarily utilize LLMs to generate bounding box coordinates for object grounding. However, none of the current RS-LMMs possess the capability for pixel grounding, i.e., generating detailed segmentation masks, which are crucial for fine-grained spatial interpretation. MODELS RESOLUTION IMAGE REGION REGION OUTPUT DECODER GROUNDING MODEL END TO END PIXEL RSGPT (HU ET AL., 2023) H2RSVLM (PANG ET AL., 2024) RS-LLAVA (BAZI ET AL., 2024) GEOCHAT (KUCKREJA ET AL., 2024) SKYEYEGPT (ZHAN ET AL., 2024) EARTHGPT (ZHANG ET AL., 2024C) LHRS-BOT (MUHTAR ET AL., 2024) SKYSENSEGPT (LUO ET AL., 2024) 224 224 336 336 336 336 504 504 448 448 - 224224 504 GEOPIXEL DYNAMIC UPTO 4K to dimensions such as 224224, exacerbates this issue by constraining the field of view and spatial perception. High-Resolution Understanding: Vision encoders, such as CLIP ViT (Radford et al., 2021), are widely utilized for various vision tasks but are typically constrained by low resolution (e.g. 224224) restricting their applicability in high-resolution (HR) scenarios. To address this limitation, some approaches (Dosovitskiy et al., 2021; Bai et al., 2023b; Li et al., 2023) scale positional encodings within the CLIP model through interpolation to accommodate larger input sizes, while others such as CogAgent (Hong et al., 2024) and Vary (Wei et al., 2025), employ an additional HR branch. Models such as Monkey (Li et al., 2024), SPHNIX (Lin et al., 2023), Llava-Next (Liu et al., 2024a), IXC2.5 (Zhang et al., 2024a), Textmonkey (Liu et al., 2024d) and Ureader (Ye et al., 2023a) divide the image into grids, encoding each section independently to enhance performance on HR text-centric tasks. Remote Sensing (RS) LMMs: RSGPT (Hu et al., 2023) is pioneering RS model that enables natural language conversation and generates detailed captions. This was followed by GeoChat (Kuckreja et al., 2024) that supports region-specific inputs and visual grounding through oriented bounding box coordinates in its responses. Furthermore, SkyEyeGPT (Zhan et al., 2024) extends its functionality to RS video captioning, while EarthGPT (Zhang et al., 2024c) and EarthDial (Soni et al., 2024) integrate various multisensor RS interpretation tasks within the LMM framework. Models such as RS-LLaVA (Bazi et al., 2024) and H2RSVLM (Pang et al., 2024) improve the interpretation of RS data, with H2RSVLM uniquely recognizing and rejecting unanswerable questions. SkySenseGPT (Luo et al., 2024) contributes by implementing image-level scene graph generation and relation reasoning, while LHRS-Bot (Muhtar et al., 2024) enhances multilevel vision-language alignment. However, these models operate on low resolution and lack pixel-level understanding and grounding capabilities. 3. Method In the current remote sensing landscape, large multimodal models (LMMs) face significant limitations in terms of grounding and resolution capabilities (as seen in Table 1). Specifically, the outputs generated by these models lack precise spatial and semantic association with the imagery, leading to either ungrounded or only coarsely grounded text. Furthermore, most LMMs operate on relatively lowresolution data, which restricts their ability to perform finescale analysis essential for RS tasks such as detailed land use and transportation network extraction, infrastructure mapping, damage assessment, and environmental monitoring. To address these limitations, we present GeoPixel, model designed to interpret high-resolution remote sensing images and generate finely detailed, pixel-grounded outputs that encompass multiple target objects. 3.1. GeoPixel Architecture Overview GeoPixel primarily consists of 5 components (see Figure 2). (1) Adaptive Image Divider (2) Vision Encoder (3) Large Language Model (4) Grounding Vision Encoder (5) Pixel Decoder. The first three components are discussed in Section 3.2, while the latter two in Section 3.3. Jointly, these modules enable high-resolution perception, fine-grained interpretation, and grounding, as detailed below. 3.2. High Resolution Understanding For high resolution, we adopt the dynamic image partitioning strategy of IXC-2.5 (Zhang et al., 2024a). Initially, the 3 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Figure 2. Overview of GeoPixel Architecture: Left: High-resolution RS images are dynamically partitioned into local patches and resized global view, encoded by frozen vision encoder. The encodings are projected into the language domain with separator tokens. Middle: Vision tokens, combined with text, are input into the LLM, where pLoRA is applied to vision tokens for efficient and effective multimodal alignment. Right: The corresponding embeddings for the [SEG] tokens are passed to decoder through text projector, along with vision embeddings from the grounding vision encoders, to generate precise segmentation masks. adaptive image divider processes the input image ximg, with dimensions [hi wi], by up-scaling and padding it to align with the closest grid size denoted as [gh gw]. gh = k1 B, s.t., k1, k2 N, gw = k2 B, k1 k2 (1) where is the base resolution of the vision encoder and is the number of maximum allowable image patches. Subsequently, the image is divided into k1 k2 non-overlapping patches xpi,j , where = 0, 1, 2, . . . , (k1 k2 1), and i, denote the row and column indices of each patch in the grid. We employ the scaled CLIP ViT-L/14 (Zhang et al., 2024a) as our vision encoder (I), with base resolution of = 560, facilitating large patches for enhanced visual representation. Furthermore, global view xglob is generated by resizing ximg to fixed dimension of 560 560, aligned with the base resolution B. Feature embeddings of patches fpi,j are appended with learnable token at the end of each row before flattening and merging (Dong et al., 2024b). Finally, global features fglob and patch features fp are concatenated () with special separator (sg) inserted between them (Ding et al., 2019), effectively integrating global semantics with fine-grained local details. xv = Pv(fglobsgfp) (2) s.t. fglob = I(xglob), fpi,j = I(xpi,j ) 4 We project the final unified image features onto the LLM, InternLM2 7B model (Cai et al., 2024), denoted as L, through two-layer MLP as vision projector Pv. InternLM2 is LLM designed to process sequences of text tokens, where its input consists of sequence of discrete embeddings derived from textual data. These embeddings correspond to either natural language tokens or special placeholders inserted to represent external modalities. The placeholder <IMAGE> in the input text query xt is special token that represents the position of the image within the input sequence. When processing multimodal input, this placeholder is replaced with visual features xv, extracted from the image, and projected into the same embedding space using Pv. Partial Low-Rank Adaptation (LoRA) (Dong et al., 2024a) is then applied to ensure efficient alignment of the vision tokens. Partial LoRA is modality-specific plug-in module designed to align features from new modality with LLM, preserving the models inherent capabilities while enriching it with modality-specific insights. By applying low-rank adaptations selectively to visual tokens, Partial LoRA enhances alignment efficiency while reducing the computational cost. Formally, it introduces low-rank matrices WA RCrCin and WB RCoutCr within each LLM linear layer, modifying the visual token outputs xv without altering the language token outputs xt, thus achieving tailored cross-modal integration. GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Figure 3. The GeoPixelD Annotation Pipeline provides detailed multi-tier descriptions of remote sensing imagery with object phrases aligned precisely with manually annotated masks. It begins with Holistic Image Annotation (bottom left), where an LMM generates concise scene descriptions. Individual Instance Annotation (bottom right) uses spatial({pos}) and categorical ({catagorory name}) priors with SOM ({mark number}) prompting to describe key objects. Cluster Annotation (top right) organizes smaller or dense objects using refined grids for precise spatial analysis. 3.3. Pixel Grounding To establish grounding in LMM, we initialize the grounding vision encoder (Ig) with pre-trained SAM-2 (Ravi et al., 2024) encoder together with dedicated pixel decoder module (D). The SAM2 visual encoder is Masked Autoencoder (MAE) (He et al., 2022) pre-trained Hiera (Ryali et al., 2023) image encoder having hierarchical structure that allows the use of multiscale features during decoding. The tokenizers vocabulary is expanded by incorporating an additional <SEG> token, with its corresponding last-layer embedding (E) mapped to the decoder through text projection layer Pt. The text projection is two-layer MLP that receives embeddings of dimension 4096 and transforms them into the input space of the pixel decoder, which has dimensionality of 256. The pixel decoder processes the image features from the frozen grounding vision encoder, along with projected LLM embeddings, to generate segmentation masks (M ). The grounding vision encoder (SAM-2) is already pre-trained on large-scale datasets, making it highly effective at extracting robust, generalized image features for segmentation. Freezing the encoder ensures that these pretrained features are preserved. However, the light-weight pixel decoder and projection layer are trained to adapt pretrained vision features for segmentation tasks in GeoPixel. = D[Ig(ximg), Pt(E)] (3) Given the variable length of the input image tokens, resulting from adaptive image partitioning, the output embedding mask for <SEG> tokens is dynamically adjusted to align with these variations. This configuration ensures accurate detection of the <SEG> token and its associated embedding. 4. GeoPixelD-RS Pixel Grounding Dataset Remote sensing imagery captures intricate semantic information and complex inter-object relationships across diverse spatial scales. To enable LMMs to acquire detailed comprehension ability, it is essential to integrate broad contextual views with object-level distinction. Addressing the current deficit in datasets capable of facilitating fine-grained understanding of top-down perspectives, we introduce GeoPixelD, dataset established to provide hierarchical descriptions derived through automated multilevel image analysis. GeoPixelD structures its descriptions at three primary levels: (1) holistic scene representation, (2) individual instance observations, and (3) densely populated object groups annotations (as depicted in Figure. 3). 4.1. Holistic Image Annotation Initially, we generated descriptive captions for RS images using robust open source model, IXC (Zhang et al., 2024a) to capture comprehensive and diverse image details. We chose the IXC model (Zhang et al., 2024a) based on comparative study conducted with other state-of-the-art vision language models, where IXC consistently outperformed its counterparts in terms of qualitative performance. These open-ended descriptions are constrained to limited length, integrated in prompts like, \"<image> Describe 5 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing the image in four short sentences.\" (Figure 3 (bottom left)). Thus, redundancy is effectively minimized in subsequent annotations, and the model is driven to provide holistic, context-rich depiction of each image. 4.2. Individual Instance Annotation Next, we identify prominent objects for the depiction and employ technique known as set-of-mark (SOM) prompting (Yang et al., 2023). This approach involves adding distinct set of visual markers over specific regions in an image, providing auxiliary information to obtain visually grounded outputs. However, directly employing this method for aerial imagery, which is characterized by expansive views and diverse objects and landscapes within single frame, leads to challenges, such as the generation of hallucinated markers and incorrectly associated details (see Figure 6). To address the challenge of accurate object description in complex RS images, we implemented an enhanced approach to spatially guide the model. We introduce prior knowledge in the query in the form of category name and location along with marked number to accurately direct the model and create comprehensive description of the target object. Specifically, we partition each image into 33 grid (nine quadrants). For each object, we calculate its positional reference by determining the degree of overlap with these quadrants, thereby localizing it within the grid structure. This quadrant-based localization, combined with categorical labels and marked numbers, is then fed as positional and categorical priors into the LMM, enabling it to focus more accurately on the intended object and retrieve relevant details, process that proves effective given the densely packed and spatially complex nature of RS imagery, where objects often vary in scale, orientation, and proximity. In addition, we conducted comprehensive evaluation of various open-source and proprietary models for priorinformed modified SOM prompting applied to RS imagery (see Figure 7). The analysis also included comparative assessment of combined versus individual querying approaches. ChatGPT (OpenAI, 2023) demonstrated the ability to generate detailed descriptions while incorporating inferred information, whereas Gemini (Team et al., 2023) and InternVL (Chen et al., 2024b) exhibited repetitive output as the number of target objects within the image increased. InternLM-XComposer (Zhang et al., 2024a) achieved performance comparable to ChatGPT in terms of the proportion of accurate responses generated and diversity in details. 4.3. Cluster/Crowd Annotation Once prominent large objects are identified, marked and annotated, the remaining objects are grouped or identified along with determining their spatial properties, which is obtained by structured three-stage positional analysis. In the first stage, the image is divided into 33 grid, with each grid cell assigned unique identifier corresponding to its spatial location. To enhance alignment with human perceptual tendencies, the central region of the grid is given larger spatial weight. In the second stage, 22 gird is considered for more dispersed objects localization. Similarly, in the third stage, half image as (12 and 21) grid is considered to assign positional information. This gridding provides systematic framework for analyzing the location of clusters as well as large groups of objects within the image. An LMM is then used to describe the group attributes given the quantitative information along with the determined positional information. 4.4. Unifying Annotations and Language Marking of the subset training preprocessed For the iSAID (Waqas Zamir et al., 2019) dataset (Appendix A), we derive total of 16,795 holistic image-level annotations, 36,793 instance-specific annotations, and 17,023 group annotations, collectively encompassing 600,817 objects within RS imagery. The annotations were rigorously filtered to eliminate aerial perspective inconsistencies, removing artifacts such as marker identifiers, fore/background references, distance perception, and contextually inconsistent descriptors. The key noun chunk corresponding to the object category in individualand group-level annotations is tagged with unique identifiers (phrase-number), each linked to an instance or semantic mask, process termed text marking. To unify these hierarchical annotations into coherent description, the marked annotations are then combined with holistic scene representations to form single descriptive narrative. We employ Llama-3.1-instruct 8B (Dubey et al., 2024) LLM to paraphrase concatenated annotations while preserving their semantic integrity (see Figure 8). The LLM processes the concatenated text under strict constraints to retain all marked phrases unchanged, ensuring consistent link to their associated visual masks. The outputs are rigorously evaluated for consistency, and iterative paraphrasing is applied if any marked phrases are not preserved. By adopting this language marking strategy, the GeoPixelD dataset achieves robust framework to generate high-quality GCG descriptions that are contextually rich and precisely aligned with visual elements. similar procedure is followed for the test set GCG descriptions derived from the iSAID validation subset. Each GCG description within this set undergoes meticulous manual curation, an effort that requires approximately 350 man-hours to ensure annotation completeness. The process includes correcting for any omissions, inaccuracies, or partial annotations, including adjustments to object attributes that do not align with the corresponding image, thereby establishing high-quality evaluation benchmark. 6 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Table 2. Performance Comparison on RS-GCG task. LISA and PixelLM denote the pretrained LISA and PixelLM models adopted for RS-GCG and finetuned on GeoPixelD training data. GLaMM represents the zero-shot performance, whereas GLaMM-FT refers to the pretrained model finetuned on GeoPixelD. GeoPixel outperforms other models across all metrics. UNI-TARGET MULTI-TARGET OVERALL MODEL CIDER METEOR AP50 MIOU RECALL AP50 MIOU RECALL AP50 MIOU RECALL GLAMM (CVPR24) LISA (CVPR24) PIXELLM (CVPR24) GLAMM-FT (CVPR24) GEOPIXEL 0.1 14.6 18.3 15.7 21.6 5.8 22.3 22.5 23.0 24.0 1.2 9.5 13.5 18. 25.5 18.1 41.7 41.2 44.4 50.8 14.8 43.1 44.0 48.5 55.6 0.5 8.3 10.4 12. 18.0 16.5 43.1 42.9 47.1 52.9 6.3 27.5 28.1 31.1 37.0 0.5 8.5 10.5 12. 19.0 16.9 42.7 42.4 46.4 52.3 7.1 29.0 29.6 32.8 38.8 5. Experiments Here, we explain the implementation details, present comparative performance analysis on Remote Sensing Grounded Conversation Generation (RS-GCG) and Referring Remote Sensing Image Segmentation (RRSIS), and include an ablation study to assess the impact of key components. 5.1. Implementation Details The model weights are initialized using the pre-trained InternLM-XComposer-2.5 model (IXC-2.5) with 7B parameters, utilizing LoRA for efficient fine-tuning of the LLM. fixed CLIP ViT-L vision encoder with resolution of 560560 is employed, along with grounded vision encoder initialized from SAM2 weights. The trainable components of the architecture include pixel decoder (D), LoRA parameters (Î± = 8), vision projector Pv, and language projector Pt. For the adaptive image divider, we set the maximum patch number to 9 for training. In our training process, we use an effective batch size of 20 over 10 epochs. The learning rate is scheduled to increase linearly to maximum value of 3 104 over the initial 100 training steps, followed by gradual decrease governed by cosine decay strategy. We train GeoPixel on the GeoPixelD dataset for grounded conversation generation task on two NVIDIA A6000-48GB GPUs, which take around 3 days. 5.2. Baselines To rigorously evaluate the efficacy of the GeoPixel, we introduce three robust baselines for comparative analysis on the GeoPixelD benchmark. The first baseline, LISA, is an improved version of the LISA model, modified to incorporate multitarget segmentation masks within its output pipeline. Furthermore, the tokenizer is updated to include phrase tokens (<p> and </p>) essential for the GCG task, allowing precise identification of contextual phrases within descriptive outputs that correspond to the associated segmentation masks. The second baseline is derived from the PixelLM model, configured without the SAM encoder. In this setup, the codebook is configured using image feature scaling fixed at factor of 2, the number of segmentation tokens adjusted to 3, and the vision tower resize parameter defined at 448. Phrase tokens are added, and <SEG> token in data is replaced with multiple codebook tokens according to the selected configuration. The third baseline, GLaMM, specifically focuses on the GLaMM-GCG variant, model tailored for the Grounded Conversation Generation task. For LISA, PixelLM and GLaMM-ft model weights are initialized using pretrained LISA-7B-v1, PixelLM-7B and GLaMM-GCG (7B), respectively, and additionally trained on GeoPixelD data for RS-GCG task. 5.3. Results Remote Sensing Grounded Conversation Generation: Table 2 provides comparative analysis of the performance of various models on the RS-GCG task. The models are evaluated across different metrics, including CIDEr, METEOR, AP50, mIoU, and recall, segmented into Uni-Target, MultiTarget, and Overall categories. GeoPixel demonstrates superior performance in all metrics compared to the baselines showing better fluency and text relevance in textual outputs. In more complex multi-target scenarios, GeoPixel maintains strong performance. In contrast, LISA struggles with segmentation-based tasks, as evidenced by its low AP50 scores in all categories. PixelLM shows moderate improvement over LISA, benefiting from better image feature scaling and segmentation token adjustments. GLaMM-ft exhibits improved outcomes due to dedicated grounding encoder and GCG pre-training, however, its performance remains inferior to that of GeoPixel. Figure 4 presents the qualitative results. Referring Remote Sensing Image Segmentation: This task focuses on segmenting specific regions in aerial imagery guided by textual descriptions. The input prompt used is: \"Could you provide segmentation mask for {referring expression} in this image?\" The model generates the response, \"Sure, it is <SEG>.\" where the corresponding embeddings of <SEG> token is subsequently decoded to produce the segmentation mask. To address this task, we fine-tune the 7 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Figure 4. Qualitative results of GeoPixel on RS-GCG. Contextually rich descriptions of RS imagery with grounded object annotations. Depending on object scale and density, it employs instance masks for precise delineation of individual objects (right and middle-right images) while semantic masks capture broader categories, such as large clusters of vehicles or small objects (middle-left and left images). Table 3. Performance Comparison of GeoPixel in Referring Expression Segmentation on RRSIS-D dataset. The segmentation accuracy based on referring expressions is expressed through the Precision at IoU threshold of 0.5 (P@0.5), Overall Intersectionover-Union (oIoU) and Mean Intersection-over-Union (mIoU). METHOD VALIDATION SET TEST SET P@0.5 OIOU MIOU P@0.5 OIOU MIOU 51.09 66.53 46.06 51.07 66.43 45.64 RRN (LI ET AL., 2018) 55.68 69.68 48.85 55.32 69.39 48.54 CSMA (YE ET AL., 2019) LSCM (HUI ET AL., 2020) 57.12 69.28 50.36 56.02 69.05 49.92 CMPC (HUANG ET AL., 2020) 57.93 70.15 50.41 55.83 69.22 49.24 58.79 70.73 51.14 56.90 69.88 49.65 BRINET (HU ET AL., 2020) 59.19 70.14 51.41 57.65 68.64 50.24 CMPC+ (LIU ET AL., 2022) 68.10 76.68 60.16 67.65 76.34 59.37 LGCE (YUAN ET AL., 2024) 69.54 77.59 61.46 69.52 77.19 61.04 LAVT (YANG ET AL., 2024) 74.66 78.27 65.10 74.26 77.79 64.20 RMSIN (LIU ET AL., 2024C) GEOPIXEL-FT 80.00 81.77 67.99 83.33 84.90 67.30 GeoPixel model on the RRSIS-D (Liu et al., 2024c) dataset. The resulting GeoPixel-ft model demonstrates superior performance compared to recent approaches, as shown by results on the RRSIS-D test and validation sets in Table 3. The qualitative results are provided in Figure 9. 5.4. Ablation Study Inference Resolution Effect: Increasing the number of inference patches demonstrates consistent improvement across all evaluation metrics, reflecting improved model Table 4. Effect of Inference Resolution. Reported metrics show the relationship between resolution and overall performance. TRAINING INFERENCE CIDER METEOR AP50 MIOU RECALL PATCHES PATCHES = 9 = 1 = 4 = 9 14.6 17.7 20.5 23.1 23.9 24. 12.9 16.6 17.6 47.8 51.8 52.1 32.2 37.1 37.4 comprehension of visual content  (Table 4)  . For example, at = 9, CIDEr increases from 14.6 to 20.5, and METEOR improves from 23.1 to 24.3, indicating improved semantic understanding as the number of image tokens scales up. The moderate gains observed in mAP and mIoU suggest that while high-resolution inference contributes to superior localization accuracy, competitive performance can still be maintained at lower resolutions when the model is pretrained at higher resolutions. The superior results associated with training with high patch count (P = 9) underscore the critical role of incorporating fine-grained spatial details during the training phase for generalized feature learning. Annotation Complexity Effect: GeoPixel adjusts its masking output based on object size and distribution (as seen in Figure 4), utilizing instance masks for precise identification of individual objects, while semantic masks are generated to represent broader categories, such as clusters or small objects. In scenarios requiring both granularity and generalGeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Table 5. Effect of Annotation Complexity. Avg. Len is the average character length of captions. Table 6. Effect of Data Complexity and Training Vision Projection (VP) Layer. stands for Trainable and for Frozen. DATA OBJECTS PHRASES AVG. LEN MIOU RECALL INSTANCES ONLY SEMANTIC ONLY MIX DATA 1,740 21,483 38,161 1,740 698 2,989 634 518 737 58.4 44.1 50.9 48.8 37.7 33.3 ization, the model integrates hybrid annotations, blending instance-level and semantic mask representations(as seen in Figure 1). The effect of this complexity of the annotation is expressed in Table 5 with lowest mask recall seen in the case of mixed annotations. Remote sensing images often contain visually similar objects with subtle variations in appearance, spatial arrangement, and positional proximity, yet exhibit significant scale variations across different images. This inherent complexity challenges the models ability to accurately differentiate between object presence, quantity, and the corresponding type of annotation required (e.g., instance level or semantic level). The challenge is particularly evident in the semantic-only category, where the model exhibits the lowest mIoU scores. This indicates two key challenges: the models ability to cover all instances within category, leading to complete semantic masks, and its ability to group objects under unified semantic mask rather than individual instance identification. The comparatively low mask recall score in mixed data also suggests that the most difficult scenario is to generalize masking decisions effectively in the presence of visually dense objects due to the scale and spatial variability of objects in the image. Role of Data Complexity: In Table 6, we compare the performance of GeoPixel on different data partitions, segregated according to the level of complexity in masking. Set-1A is less complex, with no intra-class segmentation differences. Each instance of single class is either individually masked or represented using semantic mask uniformly across the dataset. Set-1B introduces higher level of complexity where larger instances within the same class are assigned individual instance masks, while smaller objects are grouped under common semantic mask. For example, two larger boats may be individually described, while all smaller boats in the image could be grouped together under single semantic description. This structured ablation helps evaluate how GeoPixel handles varying levels of annotation granularity, providing insights into its ability to generalize across different scales and segmentation strategies. The results indicate that inclusion of more complex annotation (Set-1B) leads to improved performance, especially in terms of segmentation accuracy and descriptive detail, as the model is trained with more diverse mask configurations. TRAINING DATA VP CIDER METEOR AP50 MIOU RECALL SET-1A SET-1B F 19.3 20.5 18.7 23.6 24.0 24.4 18.2 17.8 15.3 48.0 51.7 51.6 33.6 36.7 35.1 Vision Projection: Next we study the effect of training the vision projection layer by comparing the performance when the vision projection layer is fixed or trainable during the fine-tuning stage. Table 6 summarizes the results. Training the vision projection layer results in an improvement in some metrics, highlighting the role of feature alignment. Figure 5. Failure case due to incorrect mask association (left) and wrong instance segmentation in the same spatial region (right). 5.5. Limitations and Challenges While GeoPixel has demonstrated significant advances in pixel-level grounding for high-resolution RS images, several challenges remain. These challenges are particularly evident in the following failure cases (illustrated in Figure 5). The model occasionally produces erroneous masks due to ambiguities in the masking strategy, particularly in determining object presence and quantity, as well as deciding whether semantic segmentation or instance-level annotation is appropriate. An incorrect decision in this regard can result in repetitive descriptions of visually similar objects, leading to inconsistencies in the generated output. Furthermore, such errors may manifest as fragmented or overlapping masks, in9 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing troducing confusion in object delineation and undermining the overall segmentation quality. Moreover, the model often confuses instance masks within the same spatial location, particularly in densely populated or crowded images. Future work may focus on addressing these challenges by incorporating more robust masking strategies and dynamic resolution adjustment techniques to improve segmentation accuracy in complex scenes. Additionally, extending GeoPixels capabilities to integrate multimodal data, such as Synthetic Aperture Radar (SAR) or infrared imagery, could significantly enhance its ability to analyze diverse remote sensing datasets. GeoPixel is significant step forward in leveraging the potential of LMMs for remote sensing, opening new avenues for research in this critical domain. 6. Conclusion We present GeoPixel, large multimodal model (LMM) designed specifically for the unique challenges of highresolution remote sensing (RS) image analysis. GeoPixel introduces robust end-to-end architecture capable of adaptive image partitioning and pixel-level grounding, enabling the precise interpretation and generation of geospatially aware descriptions in RS imagery. By addressing key limitations of current LMMs, such as low-resolution constraints and coarse object-grounding, GeoPixel provides fine-grained visual understanding that bridges the gap between language and high-resolution RS data."
        },
        {
            "title": "References",
            "content": "Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2): 3, 2023b. Bazi, Y., Bashmal, L., Al Rahhal, M. M., Ricci, R., and Melgani, F. Rs-llava: large vision-language model for joint captioning and question answering in remote sensing imagery. Remote Sensing, 16(9):1477, 2024. Chen, G., Shen, L., Shao, R., Deng, X., and Nie, L. Lion: Empowering multimodal large language model with duallevel visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2654026550, June 2024a. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., Li, B., Luo, P., Lu, T., Qiao, Y., and Dai, J. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2418524198, June 2024b. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. Dai, W., Li, J., LI, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P. N., and Hoi, S. Instructblip: Towards generalpurpose vision-language models with instruction tuning. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 49250 49267. Curran Associates, Inc., 2023. Ding, G., Khan, S., Tang, Z., Zhang, J., and Porikli, F. Towards better validity: Dispersion based clustering for unsupervised person re-identification. arXiv preprint arXiv:1906.01308, 2019. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Wei, X., Zhang, S., Duan, H., Cao, M., Zhang, W., Li, Y., Yan, H., Gao, Y., Zhang, X., Li, W., Li, J., Chen, K., He, C., Zhang, X., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024a. Dong, X., Zhang, P., Zang, Y., Cao, Y., Wang, B., Ouyang, L., Zhang, S., Duan, H., Zhang, W., Li, Y., Yan, H., Gao, Y., Chen, Z., Zhang, X., Li, W., Li, J., Wang, W., Chen, K., He, C., Zhang, X., Dai, J., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024b. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., Dong, X., Duan, H., et al. Internlm2 technical report, 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, 10 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, Q., De Mello, S., Yin, H., Byeon, W., Cheung, K. C., Yu, Y., Luo, P., and Liu, S. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1379613806, June 2024. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1597915988, 2022. doi: 10.1109/CVPR52688.2022.01553. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Hu, Y., Yuan, J., Wen, C., Lu, X., and Li, X. Rsgpt: remote sensing vision language model and benchmark. arXiv preprint arXiv:2307.15266, 2023. Hu, Z., Feng, G., Sun, J., Zhang, L., and Lu, H. Bidirectional relationship inferring network for referring In 2020 IEEE/CVF Conference image segmentation. on Computer Vision and Pattern Recognition (CVPR), pp. 44234432, 2020. doi: 10.1109/CVPR42600.2020. 00448. Huang, S., Hui, T., Liu, S., Li, G., Wei, Y., Han, J., Liu, L., and Li, B. Referring image segmentation via crossmodal progressive comprehension. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1048810497, 2020. Hui, T., Liu, S., Huang, S., Li, G., Yu, S., Zhang, F., and Han, J. Linguistic structure guided context modeling for referring image segmentation. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pp. 5975. Springer, 2020. Javaheripi, M., Bubeck, S., et al. Phi-2: the surprising power of small language models (2023). URL https://www. microsoft. com/en-us/research/blog/phi-2-the-surprisingpower-of-small-language-models. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Kuckreja, K., Danish, M. S., Naseer, M., Das, A., Khan, S., and Khan, F. S. Geochat:grounded large vision-language model for remote sensing. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2783127840, 2024. doi: 10.1109/CVPR52733.2024. 02629. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and Jia, J. Lisa: Reasoning segmentation via large language In Proceedings of the IEEE/CVF Conference model. on Computer Vision and Pattern Recognition, pp. 9579 9589, 2024. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Li, R., Li, K., Kuo, Y.-C., Shu, M., Qi, X., Shen, X., and Jia, J. Referring image segmentation via recurrent refinement networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 57455753, 2018. doi: 10.1109/CVPR.2018.00602. Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., and Bai, X. Monkey: Image resolution and text label are important things for large multi-modal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2676326773, 2024. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. Liu, S., Hui, T., Huang, S., Wei, Y., Li, B., and Li, G. Cross-modal progressive comprehension for referring segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):47614775, 2022. doi: 10.1109/TPAMI.2021.3079993. 11 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023a. Liu, S., Ma, Y., Zhang, X., Wang, H., Ji, J., Sun, X., and Ji, R. Rotated multi-scale interaction network for reIn 2024 ferring remote sensing image segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2664826658, 2024c. doi: 10.1109/CVPR52733.2024.02517. Liu, Y., Yang, B., Liu, Q., Li, Z., Ma, Z., Zhang, S., and Bai, X. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024d. Liu, Z., He, Y., Wang, W., Wang, W., Wang, Y., Chen, S., Zhang, Q., Lai, Z., Yang, Y., Li, Q., et al. Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language. arXiv preprint arXiv:2305.05662, 2023b. Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R. M., Xing, E., Yang, M.-H., and Khan, F. S. Glamm: Pixel grounding large multimodal model. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1300913018, 2024. doi: 10.1109/CVPR52733.2024.01236. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Radle, R., Rolland, C., Gustafson, L., Mintun, E., Pan, J., Alwala, K. V., Carion, N., Wu, C.-Y., Girshick, R., Dollar, P., and Feichtenhofer, C. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https:// arxiv.org/abs/2408.00714. Ren, Z., Huang, Z., Wei, Y., Zhao, Y., Fu, D., Feng, J., and Jin, X. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2637426383, June 2024. Luo, J., Pang, Z., Zhang, Y., Wang, T., Wang, L., Dang, B., Lao, J., Wang, J., Chen, J., Tan, Y., et al. Skysensegpt: fine-grained instruction tuning dataset and model for remote sensing vision-language understanding. arXiv preprint arXiv:2406.10100, 2024. Ryali, C., Hu, Y.-T., Bolya, D., Wei, C., Fan, H., Huang, P.-Y., Aggarwal, V., Chowdhury, A., Poursaeed, O., Hoffman, J., Malik, J., Li, Y., and Feichtenhofer, C. Hiera: hierarchical vision transformer without the bells-andwhistles. ICML, 2023. Ma, C., Jiang, Y., Wu, J., Yuan, Z., and Qi, X. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference on Computer Vision, pp. 417435. Springer, 2025. Muhtar, D., Li, Z., Gu, F., Zhang, X., and Xiao, P. Lhrsbot: Empowering remote sensing with vgi-enhanced arXiv preprint large multimodal language model. arXiv:2402.02544, 2024. OpenAI. Chatgpt: Language model for dialogue applications. https://openai.com/chatgpt, 2023. Accessed: 2024-12-31. Pang, C., Wu, J., Li, J., Liu, Y., Sun, J., Li, W., Weng, X., Wang, S., Feng, L., Xia, G.-S., et al. H2rsvlm: Towards helpful and honest remote sensing large vision language model. arXiv preprint arXiv:2403.20213, 2024. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Ye, Q., and Wei, F. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations, 2024. Soni, S., Dudhane, A., Debary, H., Fiaz, M., Munir, M. A., Danish, M. S., Fraccaro, P., Watson, C. D., Klein, L. J., Khan, F. S., et al. Earthdial: Turning multi-sensory earth observations to interactive dialogues. arXiv preprint arXiv:2412.15190, 2024. Team, G., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Waqas Zamir, S., Arora, A., Gupta, A., Khan, S., Sun, G., Shahbaz Khan, F., Zhu, F., Shao, L., Xia, G.-S., and Bai, X. isaid: large-scale dataset for instance segmentation in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 2837, 2019. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Wei, H., Kong, L., Chen, J., Zhao, L., Ge, Z., Yang, J., Sun, J., Han, C., and Zhang, X. Vary: Scaling up the vision vocabulary for large vision-language model. In European Conference on Computer Vision, pp. 408424. Springer, 2025. 12 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Xia, Z., Han, D., Han, Y., Pan, X., Song, S., and Huang, G. Gsva: Generalized segmentation via multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 38583869, June 2024. Xuan, S., Guo, Q., Yang, M., and Zhang, S. Pink: Unveiling the power of referential comprehension for multi-modal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1383813848, June 2024. Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. J., Qiao, Y., Lin, D., and Wang, J. Internlm-xcomposer2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024a. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Liu, Y., Chen, K., and Luo, P. Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023. Zhang, W., Cai, M., Zhang, T., Zhuang, Y., and Mao, X. Earthgpt: universal multimodal large language model for multisensor image comprehension in remote sensing domain. IEEE Transactions on Geoscience and Remote Sensing, 62:120, 2024b. doi: 10.1109/TGRS.2024. 3409624. Yang, Z., Wang, J., Ye, X., Tang, Y., Chen, K., Zhao, H., and Torr, P. H. Language-aware vision transformer for referring segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 118, 2024. doi: 10.1109/TPAMI.2024.3468640. Zhang, W., Cai, M., Zhang, T., Zhuang, Y., and Mao, X. Earthgpt: universal multi-modal large language model for multi-sensor image comprehension in remote sensing domain. IEEE Transactions on Geoscience and Remote Sensing, 2024c. Zhang, Y., Ma, Z., Gao, X., Shakiah, S., Gao, Q., and Chai, J. Groundhog: Grounding large language models to holistic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1422714238, June 2024d. Zhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang, B. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Ye, J., Hu, A., Xu, H., Ye, Q., Yan, M., Xu, G., Li, C., Tian, J., Qian, Q., Zhang, J., et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023a. Ye, L., Rochan, M., Liu, Z., and Wang, Y. Cross-modal self-attention network for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1050210511, 2019. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023b. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.-F., and Yang, Y. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. Yuan, Z., Mou, L., Hua, Y., and Zhu, X. X. Rrsis: Referring remote sensing image segmentation. IEEE Transactions on Geoscience and Remote Sensing, 2024. Zhan, Y., Xiong, Z., and Yuan, Y. Skyeyegpt: Unifying remote sensing vision-language tasks via instruction tuning with large language model. arXiv preprint arXiv:2401.09712, 2024. Zhang, P., Dong, X., Zang, Y., Cao, Y., Qian, R., Chen, L., Guo, Q., Duan, H., Wang, B., Ouyang, L., Zhang, S., Zhang, W., Li, Y., Gao, Y., Sun, P., Zhang, X., Li, W., Li, J., Wang, W., Yan, H., He, C., Zhang, X., Chen, K., Dai, 13 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing A. GeoPixelD dataset Preprocessing and Marking: We utilize the instance-level annotated dataset, iSAID (Waqas Zamir et al., 2019), to generate grounded conversations through our annotation pipelines. The images undergo preprocessing step in which they are cropped into 800 800 pixel patches. Objects for instance annotations are selected based on an area threshold to ensure their reasonable size, therefore preventing the marker from obscuring significant portion of the object and maintaining its distinguishability. 14 14 pixels fixed size marker is used, regardless of the actual dimensions of the object. However, the markers placement is determined based on the segmentation masks area and shape. For large objects, the marker is positioned at the center of the mask if the calculated center falls within the mask boundaries; otherwise, it is adjusted to the nearest point on the objects border. For small objects, the center of the bounding box is aligned with point on the polygon mask boundary, which typically results in an average marker overlap of 50% with the object. In addition, multiple marking techniques were also explored, including bounding boxes, masks, contours, and numerical markers, to determine their impact on model accuracy and object fidelity. Our findings reveal that bounding boxes and contours tend to introduce superfluous visual information that can obscure the fine details of the object. In contrast, simple numerical markers placed directly on the object effectively signal its presence without compromising visual clarity or introducing noise, thereby preserving the integrity of object details for enhanced model performance. Figure 6. Comparative effectiveness of SOM prompting methods, highlighting the critical role of priors. Without priors, SOM relies solely on the VLM to detect and describe marked objects independently, resulting in inaccurate descriptions and hallucinated markers in complex remote sensing scenes. In contrast, SOM with priors utilizes explicit marker positions ({pos}) and predefined object categories ({category name}) as priors, providing structured prompts that reduce ambiguity and guide the VLM to produce precise and reliable descriptions. Incorrect parts are noted in red whereas correct parts are noted in green. Figure 7. Comparison of open-source and proprietary models for prior-informed set of marks (SOM) prompting for RS imagery. Incorrect parts are noted in red whereas correct parts are noted in green. 14 GeoPixel: Pixel Grounding Large Multimodal Model in Remote Sensing Figure 8. Unifying Annotations through LLM Paraphrasing and Text Marking to track associated masks. Objects are indexed numerically (e.g., object-N), and holistic (blue), individual (teal), and cluster (green) annotations are concatenated into single image description. Paraphrasing instructions with combined description produce concise, consistent GCG description that eliminates redundancy while preserving object-mask associations, even with reordering. Figure 9. Qualitative results of GLaMMs capability in referring remote sensing expression segmentation. The figure highlights Geopixels ability to interpret referring expressions of varying lengths and generate precise segmentation masks, adapting to scale variations, as shown in the ground track fields. Spatial descriptors (e.g right, lower right), and object characteristics (e.g red) are interpreted with precision to achieve accurate segmentation."
        }
    ],
    "affiliations": [
        "Australian National University",
        "Linkoping University",
        "Mohamed bin Zayed University of AI",
        "The University of Western Australia"
    ]
}