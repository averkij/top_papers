{
    "paper_title": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs",
    "authors": [
        "Jiayu Wang",
        "Aws Albarghouthi",
        "Frederic Sala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards."
        },
        {
            "title": "Start",
            "content": "COSMOS: Predictable and Cost-Effective Adaptation of LLMs Jiayu Wang Aws Albarghouthi Frederic Sala University of Wisconsin-Madison {milawang,aws,fredsala}@cs.wisc.edu May 6,"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) achieve remarkable performance across numerous tasks by using diverse array of adaptation strategies. However, optimally selecting model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) have scaled dramatically in both capability and availability, with over 1.6 million models now shared on Hugging Face (as of Apr. 2025). Each model offers distinct performance characteristics and computational demands. The emergence of diverse adaptation techniques has further expanded the space of possible deployment configurations. This raises key question: how can we systematically identify an optimal choice of model and adaptation strategy in cost-effective way? To answer this question, we introduce and formalize the strategy selection problem for LLMs, aiming to identify the optimal combination of model and adaptation strategies that balance the performance and cost. Figure 1 illustrates this problem, where practitioners can select from pool of foundation models and adaptation strategies, each with their own configuration space, to solve tasks across various domains. The challenge lies in navigating this vast search space to find the best modelstrategy combination without exhaustively evaluating all possibilities. Intuitively, one way to tackle this problem is to predict how much each adaptation buys us; this must be done in 5 2 0 2 0 3 ] . [ 1 9 4 4 1 0 . 5 0 5 2 : r Figure 1: Overview of the strategy selection problem for LLMs and performancecost tradeoff. (a) Given downstream task, practitioners select from pool of foundation models and adaptation strategies. (b) Each model strategy combination results in different performance and cost. The challenge lies in choosing optimal combinations that balance performance and cost. 1 cost-efficient manner. Based on this idea, we propose COSt-effective MOdelStrategy prediction (COSMOS), unified framework for efficiently predicting the effectiveness of adaptation strategies. It obviates the need to run resource-intensive experiments. We demonstrate COSMOSs versatility by instantiating it for two prominent adaptation strategies: 1) QLoRA fine-tuning, where we predict adaptation gains with an embedding-augmented lightweight proxy model, and 2) retrieval-augmented in-context learning, predicting performance using observed scaling laws. Through extensive experiments across eight representative benchmarks spanning both general and specialized tasks, we demonstrate that COSMOS achieves excellent prediction accuracy (mean absolute error of 1.09%) while reducing computational costs by an average of 92.72% (up to 98.71%) compared to exhaustive experimentation. This means that efficient prediction of adaptation outcomes is not only feasible, but can empower practitioners to navigate the space of modelstrategy combinations and make informed decisions that optimize both performance and cost. Our main contributions are summarized as follows: We introduce and formalize the optimal strategy selection problem for LLMs that jointly considers the model selection and adaptation strategies with performancecost tradeoffs. To address this problem, we propose COSMOS, novel framework that enables accurate and data-efficient estimation of both performance and cost across training-time and test-time adaptation strategies. Through extensive evaluation on eight diverse benchmarks, we demonstrate that COSMOS achieves superior prediction accuracy (MAE of 1.09%) while reducing computational costs by an average of 92.72% (up to 98.71%) compared to baselines."
        },
        {
            "title": "2 Related Works",
            "content": "Adaptation strategies for LLMs. Strategies for adapting pre-trained LLMs for downstream tasks broadly fall into two categories: training-time and test-time adaptation. Popular training-time strategies include supervised fine-tuning [22, 35] and parameter-efficient variants such as LoRA [14] and QLoRA [7]. Test-time methods such as in-context learning [2], advanced search and prompting [36, 39], and decoding algorithms [20, 34] obtained significant attention in recent years [37]. While these individual strategies are effective, efficiently selecting and configuring them jointly remains underexplored. Model routing. Routing (i.e., sending easier queries to cheaper models and reserving powerful models for harder queries) has been explored via variety of approaches [3, 4, 29, 31, 41]. While routers can balance performance and cost in model selection, they usually operate with fixed model pool and do not consider broad space of adaptation strategies. In contrast, our work goes beyond pure model selection and routing. Training and test-time scaling laws. Training-time scaling laws [13, 17] explore the relationship between model size, training compute, and performance. Recent work on test-time scaling [32] has systematically analyzed how performance gains from inference strategies such as best-of-N sampling [6, 19] and beam-search [9, 39] scale with task difficulty. Meanwhile, Ruan et al. [26] has revealed predictable relationships between LLM performance and low-level skills. However, existing approaches are limited to coarse-grained, task-agnostic predictions that frequently fail to consider the interplay between training-time and test-time strategy adaptations. Moreover, they rarely account for the full spectrum of costsincluding prediction, adaptation, and evaluation costs. In contrast, our work provides accurate predictions for both performance and overall cost across multiple dimensions. AutoML, hyperparameter optimization, and NAS. Model selection, training hyperparameter optimization (HPO), and the development of new architectures optimized for each task are the traditional domains of AutoML and Neural Architecture Search (NAS). Recent work focuses on lifting these approaches to modern LLM settings [24, 27]. Most of the techniques within these areas are either narrow (e.g., HPO [40] focuses on one specific type of adaptation) or extremely expensive (e.g., Roberts et al. [23] and Shen et al. [30]). The unified approach we propose is simultaneously general and cost-efficient. Our framework and techniques are compatible with tools from AutoML and NAS."
        },
        {
            "title": "3 The Strategy Selection Problem",
            "content": "In this section, we formalize the strategy selection problem (Section 3.1) and introduce COSMOS, unified framework for predicting performance and total cost across prediction, adaptation, and evaluation processes (Section 3.2). 2 We further analyze suitable adaptation strategies (Section 3.3), and develop cost analysis methodology to evaluate the computational efficiency of various adaptation strategies and predictors within COSMOS (Section 3.4)."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "Our goal is to systematically identify good choices of model and adaptation strategy in cost-effective way. To formalize this problem, we define strategy navigator (MD) whose purpose is to determine the optimal combination of model, adaptation strategy, and configuration for downstream task that balances performance and cost-efficiency. Formally, we are given the following: model pool = {f1, f2, . . . , fK}, where each model fk : maps an input query to model answer ˆy Y, An adaptation strategy pool = {T1, T2, . . . , TJ }, configuration space Ω where each configuration ω Ω specifies parameters for applying strategy, performance metric π and cost function c. Let ω (π(T ω (fk) represent model after applying strategy Tj with configuration ω, resulting in performance-cost pair (fk)), c(T ω (fk))). The navigator selects the optimal combination by solving: MD(F, , Ω) = arg max fkF ,Tj ,ωΩ s(π(T ω (fk)), c(T ω (fk))), (1) where : R+ is score function that captures the trade-off between performance and cost based on the practitioners preferences. Example 1. Consider sentiment analysis task where we have one 7B parameter model f1 and two adaptation strategies: Tprobe (linear probing) and Ttune (full fine-tuning). Each strategys configuration space includes learning rate, number of epochs, and training data size, each with 10 possible values. This results in 2000 total combinations (1 model 2 strategy 1000 configurations per strategy). Computational cost of strategy selection problem. One approach is to exhaustively try all combinations. However, this quickly becomes prohibitively expensive. This naturally raises the question: can we solve the strategy selection problem cost-efficiently? We answer this affirmatively by introducing COSMOS, unified framework to solve the strategy selection problem via predicting adaptation gains, resulting in cheaper way to explore the search space (Section 4)."
        },
        {
            "title": "3.2 Predictive Adaptation Framework",
            "content": "Given the computational challenges of the strategy selection problem, one intuitive approach is to employ cheap predictor with small amount of data rather than conducting expensive full-scale experiments. We formalize this predictive approach through COSMOS (COSt-effective MOdelStrategy prediction), unified framework that systematically predicts adaptation gains across diverse strategies while significantly reducing computational overhead. (fk)) = Pj,k(ω) π(T ω For an adaptation strategy Tj applied to model fk with configuration ω Ω, our framework aims to: 1) predict performance: ˆπ(T ω (fk)) = Cj,k(ω) c(T ω (fk)). Here, Pj,k and Cj,k are strategy-specific predictors that map configurations to expected performance and cost respectively. These predictors can take various forms depending on the nature of the adaptation strategy Tj. Examples of predictors include lightweight proxy model and calibration on small validation set to estimate adaptation outcomes, e.g., linear probing on frozen embeddings, which is suitable for parameter-updating strategies (e.g., fine-tuning); leveraging observed scaling law to extrapolate performance with small amount of early sparse measurements. (fk)), and 2) predict associated costs: ˆc(T ω Ideal performance and cost predictors should satisfy two properties: 1) Cost-efficiency: The prediction cost must be significantly lower than the actual adaptation cpredict(Pj,k, Cj,k) cadapt(T ω , fk) and 2) Strategy-specificity: While not strictly required, predictors tailored to strategy-specific characteristics typically yield superior prediction accuracy and cost efficiency in practice Pj,k Pj, Cj,k Cj, where Pj and Cj are the sets of valid performance and cost predictors respectively for the strategy Tj. Cost analysis. The total cost includes: 1) Prediction cost: cpredict(Pj,k, Cj,k) , which includes strategy-specific prediction overhead and calibration cost using validation data if necessary; and 2) Selected strategy cost: c(T ˆω , fˆk) ˆj 3 , which includes the cost of applying the chosen strategy and final evaluation cost, detailed in Section 3.4. The framework is efficient when: (cid:88) j,k cpredict(Pj,k, Cj,k) + c(T ˆω ˆj , fˆk) (cid:88) j,k,ω c(T ω , fk). (2) Framework instantiation. To apply this framework to specific adaptation strategy Tj, one needs to: 1) Choose an appropriate predictor type based on strategy characteristics; 2) Design the predictor architecture or model; 3) Define the prediction cost calculation. Example 2. Continuing with the sentiment analysis task from Ex. 1, For Ttune (full fine-tuning), the predictor Ptune,f1 could be lightweight linear model trained on frozen embeddings and calibrate the performance from small validation set. Assume the total cost of prediction including the training cost of the proxy model and validation cost arising from performance calibration, for total 1000 configs prediction, the prediction costs $5, however, the actual total cost of adaption can be $500. This demonstrates the efficiency property as cpredict ($5) cadapt ($500). In Section 4, we instantiate our framework for two diverse adaptation strategies: 1) QLoRA fine-tuning: predict performance via an embedding-augmented linear proxy model; and 2) retrieval-augmented ICL: predict performance via observed-scaling law. We consider the full spectrum of cost based on both computing-based and token-based methods."
        },
        {
            "title": "3.3 Adaptation Strategies",
            "content": "Adaptation strategies include, for example: Training-time adaptation strategies. Training-time adaptation strategy modifies the parameters of language modelfθ, where θ Θ. Examples include full fine-tuning to parameter-efficient methods like LoRA and QLoRA. Two techniques in Ex. 1 are both training-time methods. Formally, training time adaptation function tr : fθ fθ, transforms base model into task-specialized model. Test-time adaptation strategies. Test-time (or inference time) adaptation strategies complement training-time approaches by modifying the input and/or output processing rather than the model parameters such as prompt tuning [18], CoT, and ICL. test-time adaptation function inf : transforms the input-output space to enhance model performance without parameter updates. Hybrid adaptation strategies. Recent research demonstrates growing interest in hybrid adaptation strategies that fall into the intersection of both training-time and test-time adaptations [33]. Formally, hybrid approaches can be represented as composite adaptation functions where parameter transformations and input-output space modifications work in concert: hybrid = tr inf. Model routing as special case. Model routing (i.e., directing different queries to different models) can be viewed as constrained instance of the strategy selection problem where: 1) The adaptation strategy pool contains single strategy with fixed configuration; 2) The router operates at the query level rather than the task level."
        },
        {
            "title": "3.4 Cost Analysis Framework",
            "content": "The effectiveness of adaptation strategies must be balanced against their computational costs. To enable practical decision-making, we model the cost of adaptation, evaluation, and prediction phases. Total cost. Given specific task D, the total cost for any adaptation strategy ω fk comprises two components: configured by ω, applied to model c(T ω , fk) = cadapt(T ω , fk) + ceval(T ω (fk), D) (3) where cadapt represents the adaptation cost (i.e., cost of applying the adaptation strategy) and ceval (i.e., cost of evaluating adapted model performance). Strategy-specific adaptation cost. Different strategies incur adaptation costs cadapt through different mechanisms: In test-time strategies, the cost includes only inference expenses, determined by both input and output token costs for given model and scales with the number of inference passes. In training-time strategies, the cost can be calculated through either: 1) computing-based method: (GPU/TPU hourly rate) usage duration, or 2) token-based method: per-token price training data size epochs. 4 Prediction cost. Prediction cost typically arises from training and calibrating performance predictor Pj,k and cost predictor Cj,k. This includes: 1) lightweight proxy model training for performance prediction, 2) calibrating performance on validation data if necessary, and 3) strategy-specific costs (e.g., early points generation to extrapolate according to scaling law). Cost prediction typically leverages the same validation runs or dataset information used for performance prediction, thus incurring minimal additional overhead. cpredict(Pj,k, Cj,k) = cproxy + coverhead(T ω , fk) + cval(Dval). (4) This unified cost framework enables direct comparison between diverse adaptation approaches while accounting for their distinct resource requirements. It provides foundation for the cost-aware strategy selection developed in our prediction framework (Section 3.2)."
        },
        {
            "title": "4 COSMOS: Solving The Strategy Selection Problem",
            "content": "While there may exist some predictive approaches that can be applied universally across adaptation strategies, tailored prediction methods often yield superior results in both accuracy and efficiency. In this section, we demonstrate how COSMOS can be instantiated with strategy-specific predictors by presenting two complementary examples. 4."
        },
        {
            "title": "Instantiation Setup",
            "content": "We study two popular complementary adaptation strategies, each paired with tailored prediction approach. The first is QLoRA fine-tuning tr QLoRA; we predict its performance using an embedding-augmented linear model. The second is retrieval-based in-context learning (ICL) inf ICL; we predict its performance using scaling laws. Each strategy operates in distinct configuration space that affects its resource requirements and potential gains: For QLoRA, ΩQLoRA = [0, 1] N+, representing the continuous spectrum of data proportion and discrete training QLoRA : fη,ϕ([0, 1]N+) fη,ϕ. iterations. The adaptation function then maps model to its fine-tuned version: tr For ICL, we control the number of shots and sequence length ΩICL = {n N+ : C(n) Lmax} where C(n) = Lquery + (cid:80)n i=1 Ldemoi Lmax represents the total sequence length. The ICL adaptation function modifies the input space: inf ICL : ΩICL ."
        },
        {
            "title": "4.2 Predicting Fine-tuning Gain",
            "content": "For QLoRA fine-tuning, we develop an embedding-based prediction method that works as follows. We use language model fθ in the model pool that has two key components: (1) function gη : RLd RLe, parameterized by η that maps sequence = (x1, . . . , xL) to representation, where is the embedding dimension, and is the hidden dimension. It also has (2) projection head hϕ : Re RΣ, parameterized by ϕ. Inspired by LLM2Vec [1], we first transform the traditional causal language model into bidirectional embedding η (x) RT produces = gbi model. For input = (x1, . . . , xL), we compute: zbi contextualized representations. To obtain fixed-dimensional representation for the entire sequence, we use mean pooling. This sequence embedding eη(x) will serve as the input to the projector for fine-tuning performance train = {(xi, yi)}N estimator. Given the fine-tuning training data DFT i=1, we learn lightweight task-specific projector lϕ : Re that maps sequence embeddings to the target space: ˆy = lϕ (eη(x)) where lϕ is linear layer. This minimal architecture ensures computational efficiency while leveraging the rich representations from the frozen model embeddings. η (x1, x2, . . . , xL) where gbi Finally, to bridge the gap between projector predictions and actual fine-tuning performance, we use calibration QLoRA(fθ,ϕ)) = aπϕ + where πϕ is the projector performance and a, are parameters mechanism: ˆπ(T tr learned from small validation set (e.g., 10% of full training data). This step ensures our predictions align with actual performance while maintaining computational efficiency. Next, we describe costs for fine-tuning (and our predictor). Our primary method calculates costs based on computing time, which enables direct comparison with prediction costs. The fine-tuning cost cFT depends on the number of tokens in the training set FT train, the number of epochs E, batch size B, gradient accumulation step G, and the type of computational resources used. This approach factors in the total number of training steps, processing time per gradient update step tstep, and the hourly cost of compute resources γcompute. We use token packing to optimize token usage and consider memory utilization ψpeak (the ratio of peak training memory occupation to total available memory), enabling fair comparison between prediction costs and full 5 adaptation experiments. The total cost of fine-tuning is modeled as: cFT = pack(N FT train, Lmax) tstep γcompute Ncompute ψpeak + ceval where pack(, ) computes the number of effective sequences after optimal packing of training tokens FT train sequences subject to max sequence length Lmax constraint. In terms of prediction, we derive the peak memory usage from the small validation set during performance calibration."
        },
        {
            "title": "4.3 Predicting Retrieval-Augmented ICL Gain",
            "content": "For retrieval-based ICL, our key insight is that retrieval-based ICL performance typically follows an exponential saturation curve that early measurements can characterize. Performance prediction. Given sparse performance measurements {(di, πi)}m model: i=1, we fit an exponential saturation ˆπ(T inf ICL(fη,ϕ)) = α(1 eβd) + π0 (5) where is the shot count, and (α, β, π0) capture the saturation behavior. This allows us to predict performance at any demonstration count while requiring only few initial measurementsas few as two points. Cost prediction. For ICL, given each query x, we can estimate the cost: cICL(d, x) = ctoken(E[Lin] + E[Lout]) + ctoken(x + E[Lout]) + ceval where E[Lin] and E[Lout] are expected input/output lengths. This can be estimated from the demonstration knowledge base, providing cost estimation for arbitrary shot counts. The above methods equipped COSMOS practical approach to predicting adaptation gains, enabling efficient exploration of the adaptation space without exhaustive computation. Our empirical results (Section 5) demonstrate that these predictions closely align with actual performance while reducing computational overhead by orders of magnitude."
        },
        {
            "title": "5 Experiments",
            "content": "We conduct extensive experiments to validate COSMOS."
        },
        {
            "title": "Key Takeaway at a Glance",
            "content": "Optimizing training-time and inference-time strategies jointly can be more cost-effective than scaling them separately. Our COSMOS framework can help guide strategy selection by making accurate predictions on both performance and cost efficiently, enabling practitioners to make choices flexibly based on their specific performance-cost preferences. Our evaluation aims to answer the following key questions: Prediction Accuracy with Cost Efficiency (Section 5.1): Can COSMOS effectively predict the optimal adaptation strategy? We show that our method achieves 92.72% cost reduction while maintaining high prediction fidelity (1.09% MAE) across 8 tasks, 55 strategy combinations, and spanning multiple cost regimes. Robust Strategy-Specific Prediction Capabilities (Section 5.2): How well does COSMOS predict the performance and cost of each combination? We demonstrate strong prediction capabilities for both performance gains and computational costs across multiple adaptation strategies and for general and specific tasks. Cost-effective Training-time and Test-time Scaling Synergies and Tradeoffs (Section 5.3): What are the optimal performance-cost tradeoffs under different computational budgets when comparing training-time vs. test-time adaptation? We provide critical insights into the efficiency of these two scaling approaches. Strategy Space Expansion Benefits (Section 5.4): How does broadening the adaptation strategy pool beyond simple model selection enhance the performance-cost tradeoffs in routing? We show augmenting model routing with our approach advances the Pareto frontier. Potential Implications (Section 5.5): How will COSMOS benefit real industrial deployment? Language models. We use instruction-tuned versions of Gemma 2B [10] as weaker model and Llama 3 8B [8] as the stronger model. 6 Tasks"
        },
        {
            "title": "Winogrande",
            "content": "ARC-Challenge"
        },
        {
            "title": "FPB",
            "content": "FiQA-SA"
        },
        {
            "title": "Multifin EN",
            "content": "Cost Level L H L H L H L H Pred. Acc (%) Act. Acc (%) MAE Act. Cost ($) Ours Cost ($) CRR (%) 61.42 61.58 0.16 10.08 0.33 96.68 62.10 62.33 0. 17.50 0.32 98.17 61.97 61.97 0.00 10.96 0.14 98.71 58.30 63.27 4.97 0.35 0.07 80.91 63.92 66.54 2. 0.62 0.04 93.99 65.75 67.19 1.44 0.44 0.03 94.31 78.12 79.48 1.36 0.69 0.09 87.35 76.76 79.37 2. 1.12 0.05 95.35 76.64 77.89 1.25 0.92 0.04 96.12 94.38 94.38 0.00 13.30 0.67 94.99 93.68 94.11 0. 17.28 0.52 96.99 93.15 93.31 0.16 10.39 0.22 97.90 83.26 84.98 1.72 1.44 0.10 92.88 85.29 85.98 0. 2.51 0.07 97.33 84.78 86.01 1.23 1.78 0.04 97.81 82.41 84.54 2.13 0.26 0.06 76.48 83.97 85.96 1. 0.43 0.03 92.77 83.40 85.67 2.27 0.36 0.02 93.38 95.44 96.06 0.62 8.58 0.41 95.19 96.62 96.73 0. 15.10 0.33 97.81 96.80 96.90 0.10 10.71 0.17 98.44 80.91 80.91 0.00 0.29 0.08 70.92 83.94 83.94 0. 0.50 0.05 89.84 85.76 85.76 0.00 0.36 0.03 90.95 Avg. - - 1.09 - - 92. Table 1: Results summary of predicted vs. actual optimal strategies across tasks and low (L), medium (M), high (H) cost regimes (over 55 strategy combinations of QLoRA and ICL). Our method achieves substantial cost reduction across all levels (92.72% average savings) while maintaining prediction fidelity (1.09% mean absolute error). Cost efficiency scales favorably with task size, with larger tasks demonstrating greater absolute cost savings. Results demonstrate consistent performance across low, medium, and high-cost bands. Tasks. We evaluate COSMOS on comprehensive suite of tasks spanning multiple domains. 1) General Domain: We evaluate on established benchmarks including Winogrande [28], ARC-Challenge [5], HellaSwag [42] for commonsense reasoning, and MMLU [12] for knowledge-based language understanding. 2) Financial Domain: We include FPB and FiQA-SA for sentiment analysis, and Headline and Multifin EN [38] for classification, representing domain-specific challenges. Detailed dataset information can be found in the Appendix A. Evaluation Metrics. We assess COSMOS via: 1. Prediction Accuracy: We measure the fidelity of performance and cost predictions using Mean Absolute Error i=1 yi ˆyi. Lower MAE indicates better prediction accuracy. (MAE): MAE = 1 (cid:80)n 2. Cost Efficiency: We quantify computational savings using Cost Reduction Ratio (CRR): CRR = Cfull Cours Cfull 100% (6) where Cfull represents total cost of evaluating all adaptation configurations, and Cours is the total cost of COSMOS to predict all those possibilities. Setup. We evaluate COSMOS with two representative adaptation paradigms: QLoRA fine-tuning in training-time strategies and retrieval-augmented ICL in test-time strategies. For QLoRA, the configuration space we explore includes grid of hyperparameters including training iterations {4, 5, 6, 7, 8} and data portions {0.1, ..., 1.0} at 0.1 increments. For ICL, we vary the number of retrieved demonstrations {1, 2, 4, 8, 16}, constrained by the models maximum sequence length (8,196 tokens for both Llama 3 8B and Gemma 2B). We employ retrieval-augmented ICL using BM25 [25] retriever to identify demonstrations from the training set. This results in 55 distinct transformation combinations. All experiments are conducted with three random seeds and results are averaged. We present additional details in Appendices and C. We partition the strategy space into three cost bands (low, medium, high) by uniformly dividing the range between minimum and maximum observed costs for each task, then categorize strategies into these bands based on their computational costs. Within each band, we evaluate COSMOSs ability to identify strategies that optimize the accuracy-cost tradeoff by maximizing predicted accuracy while minimizing the total monetary cost. Our score function over performance and cost is thus: s(π, c) = π ϵ where cmax is the maximum cost in that cost band, cmax and ϵ is small positive constant (e.g., ϵ = 106)."
        },
        {
            "title": "5.1 How well does COSMOS address the strategy selection problem?",
            "content": "Table 1 presents comprehensive evaluation of COSMOS, comparing predicted vs. actual optimal strategies across 8 diverse tasks and multiple cost regimes on Llama 3 8B. For clarity, we report the accuracy of the predicted strategy, the actual best achievable accuracy, and MAE (Eq. 1). For each cost level, we report the total cost of running all combinations (Act. Cost), the total cost of running COSMOS (Ours Cost), and CRR (Eq. 6). Our approach demonstrates remarkable efficiency-accuracy trade-offs, achieving an average cost reduction of 92.72% while maintaining strong prediction fidelity with only 1.09% MAE. Notably, COSMOS exhibits two key scaling properties: (1) cost savings systematically increase as we move from low to high-cost ranges (improvement ranging from 2.03% for MMLU to 20.03% for Multifin EN), suggesting better prediction capability in computationally intensive scenarios, and (2) cost efficiency improves with task scale, with larger tasks demonstrating greater absolute cost savings (e.g., MMLU: $9.74-$17.18, HellaSwag: $12.64-$16.76). This dual scaling behavior suggests that COSMOS becomes increasingly advantageous as both computational demands and task complexity grow. 7 We provide detailed performance metrics including cost ranges and direct comparisons with established searchbased methods in Appendix D. We also present evaluation results across an expanded model pool that further validate COSMOSs generalizability and efficiency in Appendix H."
        },
        {
            "title": "5.2 Strategy-Specific Analysis",
            "content": "Having established COSMOSs overall effectiveness in Section 5.1, we now present detailed strategy-specific analysis of its prediction capabilities on all combinations. Figure 2: Predicted vs. actual performance-cost analysis for QLoRA fine-tuning. Each plot compares actual () vs. predicted () performance-cost trajectories for Llama 3 8B QLoRA fine-tuning. Base models Gemma 2B ( ), and Llama 3 8B ( ) serve as reference points. The closer predicted performance-cost trajectories (red) to the actual (purple) trajectories indicates better performance-cost prediction. The results show consistent alignment between predicted and actual curves across both general and domain-specific tasks. This demonstrates COSMOSs robust prediction capabilities. Fine-tuning. Figure 2 demonstrates the prediction accuracy for QLoRA fine-tuning across our task suite. Each point represents distinct fine-tuning configurations performance-cost outcome (e.g., training on 50% data for 5 iterations). The results reveal strong prediction capabilities for all tasks. For instance, on FPB, COSMOS achieves notably high accuracy with MAE of 0.007 for both performance and cost predictions. We observe improved prediction accuracy at higher computational budgets, where fine-tuning performance stabilizes. Even in situations with limited training data and high-performance variance (low-cost scenario), COSMOS is able to capture the vibrate pattern and indicates if fine-tuning is worthwhile. These patterns persist across task domains, indicating COSMOSs robust generalization. In-context learning. Figure 3 demonstrates the prediction accuracy for retrieval augmented ICL across our task suite. Each point represents distinct ICL configurations performance-cost outcome (e.g., providing 8 demonstrations of input-output pairs in the query). The results reveal strong prediction capabilities across general-domain benchmarks and specialized financial tasks. For instance, on FiQA-SA, COSMOS achieves notably high accuracy with MAE of 0.003 and 0.001 for performance and cost predictions, respectively. Full results and detailed analyses examining the accuracy and cost predictions separately for all tasks and strategies are provided in Appendix E."
        },
        {
            "title": "5.3 Combining Training- and Test-time Strategies",
            "content": "We also conduct an analysis of the fundamental trade-offs between training-time and inference-time adaptation strategies by comparing QLoRA fine-tuning and retrieval-augmented ICL as illustrated in Figure 4. Full results can be found in Appendix F. We find: (1) Non-linear Scaling Behaviors. Both adaptation strategies exhibit diminishing returns with increased computational investment, despite consistently outperforming the base Llama 3 8B model. Maximizing computational resources (shots for ICL or iterations/data for QLoRA) does not guarantee optimal performance. (2) Stability-Performance Trade-offs. QLoRA and ICL demonstrate distinct stability characteristics across different cost regimes. While QLoRA shows higher performance variance, particularly evident in Multifin EN where performance fluctuates significantly in low-cost settings ($0.019) before stabilizing at higher thresholds ($0.034), retrieval-augmented ICL maintains more consistent performance profiles, especially in resourceconstrained scenarios. (3) Resource-dependent Strategy Selection. The optimal choice between fine-tuning and prompting depends on available resources. Fine-tuning typically achieves superior performance in medium to high-cost scenarios. ICL is more reliable option in low-resource settings. 8 Figure 3: Predicted vs. actual performance-cost analysis for retrieval-based ICL. Each plot compares actual () vs. predicted () performance-cost trajectories for Llama 3 8B ICL. Base models Gemma 2B ( ), and Llama 8B ( ) serve as reference points. The consistent alignment between predicted and actual curves across all tasks demonstrates COSMOSs robust prediction capabilities. Figure 4: Actual QLoRA vs. ICL performance-cost trajectories across diverse tasks. Each plot presents the performance-cost curves for QLoRA () and ICL () on Llama 3 8B, with Gemma 2B ( ), and Llama 3 8B ( ) serving as baselines. Vertical dashed lines demarcate low, medium, and high-cost thresholds, determined by the minimum and maximum costs of both adaptation strategies. The shaded regions represent the standard deviation across 3 seeds for each configuration. (4) Hybrid Strategy Benefits. Strategically combining the approaches can achieve superior performance at lower costs. COSMOS can help produce this selection efficiently."
        },
        {
            "title": "5.4 Augmenting Routing",
            "content": "While traditional model routing focuses on selecting from pool of base models for each query, we demonstrate that expanding the routing space to include adaptation strategies can significantly enhance the performance-cost frontier. Instead of simply choosing between base models like Gemma 2B and Llama 3 8B, our approach considers richer set of options that includes both model selection and adaptation strategies such as QLoRA fine-tuning and retrieval-augmented ICL. The benefits of this expanded strategy space are illustrated in Figure 5. In conventional routing between Gemma 2B and Llama 3 8B (old Pareto frontier), the performance-cost tradeoff is limited to interpolating between these base models. However, by incorporating adaptation strategies, we establish new Pareto frontier that substantially dominates the original one. This expansion leads to significant adaptation gains, visualized by the shaded red region between the old and new frontiers, quantifying the superior performance that can be achieved at the same cost or the cost reduction for equivalent performance. Importantly, while this expanded strategy space offers Figure 5: Benefit of adaptation-augmented routing. The Old Pareto Frontier () connects performance-cost points achievable through traditional routing between base models; New Pareto Frontier () incorporates adaptation strategies (QLoRA, ICL). The red-shaded region represents the adaptation gainsquantifying how much better performance can be achieved at the same cost, or how much cost can be reduced for the same performance, by expanding the strategy space beyond pure model selection. 9 practitioners more flexibility, exhaustively evaluating all possible combinations becomes intractable as the space grows exponentially. We mitigate this challenge through COSMOS, which efficiently predicts the performance and cost of adaptation strategies without requiring extensive computation, as validated in Sections 5.1 and 5.2. This enables informed strategy selection while avoiding the computational overhead of evaluating every configuration, making adaptation-augmented routing practical and scalable."
        },
        {
            "title": "5.5 Potential Implications",
            "content": "Fine-tuning large language models (LLMs) at scale presents significant financial challenges. In Appendix G, we analyze these costs through practical case study of fine-tuning GPT-4o, using OpenAIs current pricing structure. We obtain rough approximation of cost savings of $939,830, bringing down the cost by factor of 24.7x."
        },
        {
            "title": "6 Conclusion",
            "content": "We formalized and studied the strategy selection problemdetermining optimal combinations of models, adaptation approaches, and configurations while balancing performance and cost constraints. We introduced COSMOS, unified framework to predict both task performance and costs. We explored our frameworks effectiveness through two representative strategies: (1) lightweight embedding-augmented linear proxy model for predicting finetuning performance, and (2) low-sample scaling laws predictor for retrieval-augmented in-context learning. We validated capabilities of both components via exhaustive experiments. COSMOS enables practitioners to make informed decisions by providing flexible and cost-effective approach to strategy selection based on their specific performance-cost trade-off preferences. References [1] BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., and Reddy, S. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961, 2024. [2] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [3] Chen, D., Zhuang, Y., Zhang, S., Liu, J., Dong, S., and Tang, S. Data shunt: Collaboration of small and large models for lower costs and better performance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1124911257, 2024. [4] Chen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. [5] Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [6] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 2021. [7] Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [8] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [9] Feng, X., Wan, Z., Wen, M., McAleer, S. M., Wen, Y., Zhang, W., and Wang, J. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. [10] Gemma, T., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivière, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [11] Gemma, T., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [12] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [13] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., et al. An empirical analysis of compute-optimal large language model training. Advances in Neural Information Processing Systems, 35:3001630030, 2022. [14] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [15] Hu, Q. J., Bieker, J., Li, X., Jiang, N., Keigwin, B., Ranganath, G., Keutzer, K., and Upadhyay, S. K. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. [16] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. [17] Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [18] Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243/. [19] Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [20] Park, K., Wang, J., Berg-Kirkpatrick, T., Polikarpova, N., and DAntoni, L. Grammar-aligned decoding, 2024. URL https://arxiv.org/abs/2405.21047. [21] Qwen, T. Qwen2 technical report. 2024. [22] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. [23] Roberts, N., Khodak, M., Dao, T., Li, L., Ré, C., and Talwalkar, A. Rethinking neural operations for diverse tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [24] Roberts, N., Guo, S., Gao, Z., GNVV, S. S. S. N., Cromp, S., Wu, C., Duan, C., and Sala, F. Pretrained hybrids with mad skills, 2024. [25] Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. [26] Ruan, Y., Maddison, C. J., and Hashimoto, T. Observational scaling laws and the predictability of language model performance. arXiv preprint arXiv:2405.10938, 2024. [27] Saad-Falcon, J., Lafuente, A. G., Natarajan, S., Maru, N., Todorov, H., Guha, E., Buchanan, E. K., Chen, M., Guha, N., Ré, C., et al. Archon: An architecture search framework for inference-time techniques. arXiv preprint arXiv:2409.15254, 2024. [28] Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. [29] Šakota, M., Peyrard, M., and West, R. Fly-swat or cannon? cost-effective language model choice via metamodeling. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pp. 606615, 2024. [30] Shen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., and Talwalkar, A. Cross-modal fine-tuning: Align then refine. In International Conference on Machine Learning (ICML), 2023. [31] Shnitzer, T., Ou, A., Silva, M., Soule, K., Sun, Y., Solomon, J., Thompson, N., and Yurochkin, M. Large language model routing with benchmark datasets. arXiv preprint arXiv:2309.15789, 2023. 11 [32] Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [33] Soylu, D., Potts, C., and Khattab, O. Fine-tuning and prompt optimization: Two great steps that work better together. arXiv preprint arXiv:2407.10930, 2024. [34] Wang, X. and Zhou, D. Chain-of-thought reasoning without prompting. arXiv preprint arXiv:2402.10200, 2024. [35] Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [36] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837, 2022. [37] Welleck, S., Bertsch, A., Finlayson, M., Schoelkopf, H., Xie, A., Neubig, G., Kulikov, I., and Harchaoui, Z. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. [38] Xie, Q., Han, W., Zhang, X., Lai, Y., Peng, M., Lopez-Lira, A., and Huang, J. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443, 2023. [39] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv. org/pdf/2305.10601. pdf, 2023. [40] Yu, T. and Zhu, H. Hyper-parameter optimization: review of algorithms and applications. arXiv preprint arXiv:2003.05689, 2020. [41] Yue, M., Zhao, J., Zhang, M., Du, L., and Yao, Z. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. arXiv preprint arXiv:2310.03094, 2023. [42] Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Appendix The appendix is organized as follows: We first detail the datasets (Appendix A) and experimental setup (Appendix B). Next, we provide comprehensive information about our performance and cost prediction frameworks in Appendix C). We then present detailed evaluations of COSMOS (Appendices D-E), including extended results for Sections 5.1 and 5.2, comparisons against baselines, and in-depth strategy-specific analyses. Additionally, we offer extended results for strategy combinations in Appendix F, discusses broader implications in Appendix G. We demonstrate COSMOSs generalizability across diverse model families in Appendix and with limited data access in Appendix I. Finally, we present concrete example illustrating how practitioners can conduct strategy selection using COSMOSs predicted metrics in Appendix J."
        },
        {
            "title": "A Datasets Details",
            "content": "Our evaluation spans both general-domain and domain-specific tasks, with dataset sizes varying from 380 to 21,570 examples  (Table 2)  . This range allows us to systematically investigate model performance across both low-resource and data-rich scenarios. For general-domain benchmarks (MMLU, Winogrande, HellaSwag, and ARC-Challenge), we maintain consistency with RouterBench [15] by adopting their prompting templates. This choice ensures comparability with model routing settings and eliminates potential performance variations due to prompt differences. For financial domain tasks (FPB, FiQA-SA, Headline, Multifin EN), we preserve the original dataset format as these typically include well-structured instructions and question-answer pairs."
        },
        {
            "title": "Tasks",
            "content": "# of Train MMLU Winogrande HellaSwag ARC-Challenge FPB FiQA-SA Headline Multifin EN 9,809 886 7,029 1,029 3,100 750 21,570 380 Table 2: Number of training examples per task."
        },
        {
            "title": "B Experimental Details",
            "content": "Training and retrieval setup. For datasets without predefined splits (general tasks), we implement standard partition ratio of 70/10/20 for training, validation, and testing, respectively. For QLoRA fine-tuning (both full dataset training and prediction-time validation), we select data from the training set and evaluate performance on the test set. To account for data sampling variability when training with different data portions, we report average performance across three random seeds. For ICL, we employ BM25-based retrieval to dynamically select demonstrations from the training set for each test query. Our reported results represent averages across multiple demonstration orderings: the original retrieval order and two random permutations. Fine-tuning hyperparameters. For fine-tuning, we use learning rate of 2e-4 (following Dettmers et al. [7]), batch size of 1, maximum sequence length of 512, gradient accumulation steps of 2, warmup ratio of 0.03, maximum gradient norm of 0.3, LoRA alpha of 32, LoRA dropout of 0.05, LoRA rank of 64. To standardize measurements across varying cluster loads, we assume an average step time of 1.09 seconds, which corresponds to the mean processing time on uncontested GPU resources. Hardware and software. We conduct experiments using 15 NVIDIA A100-PCIE-40GB and 1 NVIDIA A100SXM4-40GB GPUs, with Python 3.10, PyTorch 2.5.1, and Transformers 4.45.2. 13 Cost assumptions. We use an hourly rate of $1/h for A100-40GB based on Vast.ai pricing listed on GPU comparison website1 for computing-based cost estimation. For token-based fine-tuning costs, we extrapolate from Together.ai2s online cost calculator and fit Llama 3 8Bs pricing using power law to calculate the total cost per epoch cepoch = (Ntoken)b, excluding their $5 minimum cost requirement, where 8.69e 7, 0.956. For inference cost, we utilize Together.ais pricing of $0.2 for Llama 3 8B per million tokens and $0.1 for Gemma 2B per million tokens."
        },
        {
            "title": "C Performance and Cost Prediction",
            "content": "C.1 Fine-tuning Our prediction framework employs task-specific approaches based on complexity. For complex tasks (MMLU, HellaSwag, Winogrande, ARC-Challenge), we implement contrastive learning approach using lightweight linear projector. This model is trained using correct answers as positive examples and incorrect options as negative samples, with the following configuration: batch size of 8, maximum sequence length of 512, learning rate of 1e-6, and temperature of 0.07. The architecture consists of layer normalization followed by linear projection layer, with model selection based on peak test accuracy over 300 iterations. For financial domain tasks, we adopt more streamlined approach, utilizing single linear layer trained with cross-entropy loss on model-generated embeddings. This simplified architecture achieves rapid training convergence on CPU, resulting in negligible proxy model training costs. To ensure robust performance predictions across different data regimes, we employ calibration process using minimal subset of training dataeither 200 examples or 10% of the training set, whichever is larger. This one-time calibration yields scaling factors applicable across all data portion configurations (0.1 through 1.0) per epoch. Cost prediction. The training cost is calculated by multiplying GPU usage duration and peak memory utilization by the compute price. To standardize measurements across varying cluster loads, we assume fixed processing time of 0.0009 seconds per data point per epoch on an idle NVIDIA A100-PCIE-40GB, which corresponds to the mean processing time on uncontested GPU resources. The primary prediction cost stems from validation set training. This explains the varying cost reduction rates across tasksachieving up to 98.71% reduction for MMLU in high-budget scenarios, while showing lower reductions for smaller datasets like Multifin EN (380 total points). Since validation costs are amortized across all strategies using the same scaling factor (currently 10 in our experiments), increasing the number of strategies or configurations would further improve cost efficiency. C.2 Retrieval-augmented ICL We discover that retrieval-augmented ICL performance can be effectively predicted using minimal data (as few as 2 samples). Building on this finding, we fit an exponential saturation function (Eq. 5) using performance measurements from 1-shot and 8-shot settings. For the baseline performance π0, we select the lower value between zero-shot performance and 1-shot results. Cost estimation for ICL leverages the average input and output lengths observed in the training set for efficiency."
        },
        {
            "title": "D A Detailed Evaluation and Comparative Analysis of COSMOS",
            "content": "D.1 Detailed Performance Analysis of COSMOS In Section 5.1, we presented the comparison of predicted versus actual optimal strategies. Here, we provide comprehensive analysis through Table 3, which details multiple performance dimensions across different cost regimes. For each task and cost level (Low/Medium/High), we report both accuracy and cost metrics. The accuracy metrics include our strategys predicted performance (Pred Acc), the actual optimal strategys performance (Act Acc), and statistical measures across all strategies (minimum, maximum, and average accuracy). This allows us to evaluate our methods effectiveness from multiple angles. To quantify the cost-efficiency of our approach, we compare three key metrics: (1) the total cost of exhaustively evaluating all 55 strategy combinations (Act Total 1https://cloud-gpus.com/ 2https://www.together.ai/pricing 14 Tasks"
        },
        {
            "title": "Winogrande",
            "content": "ARC-Challenge"
        },
        {
            "title": "FPB",
            "content": "FiQA-SA"
        },
        {
            "title": "Multifin EN",
            "content": "Cost Level L H L H L H L H Min Acc (%) Max Acc (%) Avg Acc (%) Act. Acc (%) Range Avg Acc (%) Pred Acc (%) 55.91 61.58 59.96 61.58 58.75 61.42 57.32 62.33 61.14 62.33 59.83 62.10 61.14 61.97 61.60 61.97 61.55 61.97 54.64 57.91 58.82 73.70 75.96 75.06 63.27 66.54 67.19 79.48 79.37 77.89 58.22 62.16 63.73 77.44 77.62 76.70 63.27 66.54 67.19 79.48 79.37 77.89 58.95 62.22 63.01 76.59 77.62 76.47 58.30 63.92 65.75 78.12 76.76 76. 52.56 94.38 88.35 94.38 73.47 94.38 62.02 94.11 91.36 94.11 78.07 93.68 61.09 93.31 88.34 93.31 77.20 93.15 72.65 76.05 77.49 74.18 78.16 80.85 72.91 84.98 85.98 86.01 84.54 85.96 85.67 96.06 80.70 84.29 83.89 80.14 82.17 83.65 91.51 84.98 85.98 86.01 84.54 85.96 85.67 96.06 78.81 81.01 81.75 79.36 82.06 83.65 84.49 83.26 85.29 84.78 82.41 83.97 83.40 95.44 74.90 96.73 95.07 96.73 85.81 96.62 70.52 96.90 93.72 96.90 83.71 96. 51.52 70.68 75.27 80.91 83.94 85.76 70.05 77.86 80.27 80.91 83.94 85.76 66.21 77.31 80.52 80.91 83.94 85.76 Min Cost ($) Max Cost ($) Avg Cost ($) Act. Total Cost ($) Range Avg Cost ($) Ours Total Cost ($) 0.677 1.152 0.875 1.189 1.663 1.370 0.004 0.019 0.034 0.163 0.018 0.033 0.048 0.639 0.388 0.011 0.025 0.040 10.076 17.501 10.963 0.351 0.621 0.443 0.688 1.116 0.920 13.305 17.282 10.385 1.440 2.506 1.775 0.261 0.430 0.357 8.580 15.097 10.713 0.287 0.504 0.360 0.011 0.026 0.041 0.401 0.083 0.051 0.033 0.335 0.023 0.097 0.171 0.004 0.018 0.031 0.135 0.092 0.166 0.239 0.017 0.028 0.044 0.550 0.055 0.125 0.197 0.010 0.023 0.036 0. 0.005 0.024 0.042 0.011 0.047 0.079 0.023 0.041 0.059 0.044 0.072 0.110 0.013 0.031 0.049 0.026 0.059 0.092 0.057 0.131 0.205 0.010 0.023 0.036 0.342 0.103 0.067 0.039 0.061 0.031 0.024 0.413 0.014 0.033 0.051 0.027 0.059 0.095 0.067 0.037 0.025 0.087 0.052 0.036 0.758 1.192 0.960 0.584 0.999 0.755 1.034 1.448 1. 1.270 1.815 1.484 0.181 0.725 0.443 0.792 0.331 1.241 0.167 0.975 0.520 1.426 0. 1.543 0.218 0.914 0.320 0.453 0.666 Table 3: Comprehensive evaluation of our strategy prediction framework across 8 diverse tasks under different cost regimes. Results compare predicted vs. actual optimal strategies across low (L), medium (M), and high (H) cost settings, evaluating 55 combinations of QLoRA and ICL techniques. The analysis encompasses multiple accuracy metrics (predicted, actual, mean, extremal values, and range averages) and their corresponding cost measurements, demonstrating our methods effectiveness in identifying optimal strategies while maintaining performance across varying computational budgets. Cost), (2) our methods prediction cost (Ours Total Cost), and (3) baseline costs from random strategy selection within each cost band (Range Avg Cost). To further illustrate our methods effectiveness, consider the HellaSwag task under the high-cost regime: our predictor achieves 93.15% accuracy at $0.218, significantly outperforming random strategy selection which yields 77.2% accuracy at $1.543. This demonstrates that our approach not only maintains near-optimal performance but also reduces computational costs by over 7x compared to random selection within the target cost range. We also present concrete example on how to select the optimal strategy based on predicted metrics given by COSMOS in Appendix J. D.2 Comparative Evaluation Against Search-based Methods"
        },
        {
            "title": "Methods",
            "content": "Acc. Prediction Cost ($)"
        },
        {
            "title": "High",
            "content": "Oracle RS-CV SH-CV COSMOS (Ours) Oracle RS-CV SH-CV COSMOS (Ours) Oracle RS-CV SH-CV COSMOS (Ours) 0.944 0.933 0.923 0.944 0.941 0.936 0.934 0.937 0.933 0.927 0.931 0. - 1.474 1.391 0.666 - 3.621 3.999 0.520 - 5.990 5.914 0.218 Table 4: COSMOS achieves near-oracle accuracy (99.3%-100%) while reducing computational costs by up to 27.1x compare to RS-CV and SH-CV baselines across all budget constraints. We further conduct study comparing COSMOS against two established hyperparameter optimization approaches: Random Search with Cross-Validation (RS-CV) and Successive Halving with Cross-Validation (SH-CV) on the HellaSwag benchmark. Our evaluation focuses on two critical metrics: (1) prediction accuracy compared to the oracle (optimal performance), and (2) computational cost efficiency. As demonstrated in Table 4, COSMOS consistently outperforms both baselines across all cost constraints. In terms of prediction accuracy, our approach matches or closely approximates the oracle performance across all budget levels (achieving 100%, 99.3%, and 99.9% of oracle accuracy respectively), while simultaneously delivering substantial cost reductions. The computational efficiency advantages of COSMOS are also noteworthy. When compared to the best-performing baseline in each cost regime, COSMOS reduces computational expenditure by factors of 2.2, 7.0, and 27.1 for 15 low, medium, and high budget scenarios respectively. For instance, in the high-cost regime, COSMOS achieves 99.9% of oracle accuracy (0.932 vs. 0.933) while requiring only $0.218 in prediction costsa 27.1 reduction compared to SH-CVs $5.914. Beyond these quantitative improvements, COSMOS offers fundamental paradigm shift from traditional approaches. Unlike search-based methods that require running complete experiments to determine actual performance and cost, COSMOS accurately forecasts these metrics with minimal computational overhead. This predictive capability eliminates the need to execute full experimental cycles or extensive validation procedures, enabling practitioners to make informed decisions without incurring the substantial computational costs associated with conventional methods. This prediction-based approach is particularly valuable in resource-constrained environments where practitioners need reliable performance estimates without committing to extensive computational expenditures."
        },
        {
            "title": "E Detailed Analysis of Prediction Capabilities of COSMOS",
            "content": "E.1 Full Results for Strategy-specific Analysis (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 6: Predicted vs. actual performance-cost analysis for QLoRA fine-tuning across eight diverse tasks. Each plot compares actual () vs. predicted () performance-cost trajectories for Llama 3 8B QLoRA fine-tuning. Base models Gemma 2B ( ), and Llama 3 8B ( ) serve as reference points. The closer predicted performance-cost trajectories (red) to the actual (purple) trajectories indicates better performance-cost prediction. The results show consistent alignment between predicted and actual curves across both general and domain-specific tasks. This demonstrates COSMOSs robust prediction capabilities. Following our analysis in Section 5.2, we present comprehensive performancecost trajectories for all eight tasks in Figure 6 and 7, examining QLoRA fine-tuning and retrieval-augmented ICL, respectively. The strong alignment between predicted and actual performance trajectories across all tasks and strategies validates our methods robustness. Our framework demonstrates particular strength in capturing complex performance dynamicsnot only predicting standard improvement curves, but also accurately forecasting non-monotonic patterns, such as the performance degradation observed in the ARC-Challenge task as the computational budget increases. This ability to capture both positive and negative performance trends further substantiates the generalizability of our prediction framework. E.2 Closer Look at COSMOSs Prediction Ability for Fine-tuning Performance prediction. Figure 8 provides detailed analysis of COSMOSs prediction accuracy on fine-tuning across eight tasks. For each task, we plot predicted versus actual performance with axes deliberately zoomed to highlight prediction granularity. Each scatter point represents specific QLoRA fine-tuning configuration, with proximity to the diagonal indicating prediction accuracy. The Mean Absolute Error (MAE) ranges from 0.007 (FPB) to 0.040 (Multifin EN), with most tasks showing MAE below 0.02, demonstrating remarkable precision. The 16 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 7: Predicted vs. actual performance-cost analysis for retrieval-based ICL across eight diverse tasks. Each plot compares actual () vs. predicted () performance-cost trajectories for Llama 3 8B ICL. Base models Gemma 2B ( ), and Llama 8B ( ) serve as reference points. The consistent alignment between predicted and actual curves across both general and domain-specific tasks demonstrates COSMOSs robust prediction capabilities. framework exhibits consistent performance across both general-domain benchmarks (MMLU: 0.016, Winogrande: 0.024, ARC-Challenge: 0.013, HellaSwag: 0.019) and financial tasks (FPB: 0.007, FiQA-SA: 0.028, Headline: 0.018, Multifin EN: 0.040). The tight clustering around the diagonal, particularly evident in the zoomed visualization, underscores our methods robust predictive capabilities regardless of task domain or performance level. Cost prediction. Figure 9 demonstrates our frameworks cost prediction capabilities for QLoRA fine-tuning across eight tasks. The scatter plots reveal near-perfect diagonal alignment with remarkably low MAE (0.0000.007) across all tasks, from resource-intensive benchmarks like MMLU to lightweight tasks like FiQA-SA. This consistency across varying cost scales validates our frameworks robust cost estimation capabilities. E.3 Closer Look at COSMOSs Prediction Ability for Retrieval-Augmented In-Context"
        },
        {
            "title": "Learning",
            "content": "Performance prediction. Figure 10 demonstrates our frameworks prediction accuracy for retrieval-augmented ICL across eight tasks. With deliberately zoomed axes to highlight prediction granularity, the scatter plots reveal strong performance with MAE ranging from 0.003 (FiQA-SA) to 0.013 (Multifin EN and Headline). The framework maintains consistent accuracy across both general-domain tasks (MMLU: 0.007, Winogrande: 0.005, ARCChallenge: 0.008, HellaSwag: 0.012) and financial tasks (FPB: 0.007, FiQA-SA: 0.003, Headline: 0.013, Multifin EN: 0.013), with an average MAE of 0.0085. This low average deviation of 0.85% from actual performance validates our frameworks robust prediction capabilities across diverse domains. Cost prediction. Figure 11 demonstrates our frameworks cost prediction capabilities for ICL across eight tasks. Most tasks show excellent prediction accuracy with MAE ranging from 0.001 (Winogrande, ARC-Challenge, FiQA-SA, Multifin EN) to 0.009 (MMLU), with HellaSwag being the only outlier (MAE=0.154). This higher deviation in HellaSwag stems from our design choice to use training set averages for sequence length estimation instead of observed samples during performance model fitting, prioritizing efficiency over perfect accuracy. While this approximation is typically sufficient for cost estimation, prediction accuracy could be trivially improved by using observed sample lengths when higher precision is needed. Full Results for Combining Trainingand Test-time Strategies Following the four representative tasks shown in Section 5.3, we present in Figure 12 the full scaling behavior results across all eight tasks. 17 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 8: Scatter plots comparing predicted vs. actual performance (accuracy) for QLoRA fine-tuning across eight diverse tasks Axes are strategically zoomed to reveal fine-grained prediction details, with points closer to the diagonal indicating higher prediction accuracy. For example, in Headline, at (0.92, 0.915) shows that for specific configuration (training data size and iteration count), COSMOS predicts 0.915 accuracy while the actual performance achieves 0.92. The tight clustering around the diagonal across both general domain (a-d) and financial domain (e-h) tasks demonstrates COSMOSs robust prediction capabilities."
        },
        {
            "title": "G Extended Potential Implication",
            "content": "Fine-tuning large language models (LLMs) at scale presents significant computational and financial challenges. We analyze these costs through practical case study of fine-tuning GPT-4o, using OpenAIs current pricing structure of $25 per million training tokens3. Following the training protocol established in the Llama 3 report [8], typical fine-tuning run requires an average of 8.75K steps with sequences of 8,192 tokens each, amounting to approximately 71M tokens per complete pass through the training data. To identify optimal fine-tuning parameters, practitioners typically need to explore various hyperparameter combinations. Consider systematic exploration of epochs (ranging from 1 to 10) and data mixing strategies (10 options), resulting in 100 total trials. Each trial with single epoch over the full dataset costs $1,775 in training compute alone, with costs scaling linearly with the number of epochs. Assume the validation phase requires processing queries totaling 1M tokens, with model outputs averaging 3x the input length due to multi-step reasoning. Given OpenAIs pricing of $2.5 per million input tokens and $10 per million output tokens, each validation run incurs an additional cost of $32.5. Across all trials, the validation cost sums to $3,250, while the total training cost reaches $976,250 (55 total epochs 10 data mixing strategies $1,775). This brings the total adaptation cost to approximately $979,500. Our method, COSMOS, substantially reduces these costs through accurate performance prediction using small amount of data. In resource-intensive scenarios, our approach achieves cost reduction of 95.95% compared to the total cost, enabling practitioners to estimate the performance-cost trade-offs of different configurations while only running small subset of experiments. This reduces the total cost to approximately $39,670a 24.7x reduction. This dramatic cost reduction enables practitioners to make informed decisions about the performance-cost trade-offs that best suit their specific requirements and constraints. Expanded Model Evaluation: Diverse Families and Scales To establish the generalizability of COSMOS, we significantly expand our evaluation framework beyond the initial Gemma 2B and Llama 3 8B models to encompass diverse model families and scales. This comprehensive evaluation now incorporates models from multiple architectures (Mistral [16] and Qwen [21]), newer generations (Gemma 2 [11]), and various parameter scales (7B to 9B). Following the experimental protocol established in Section 5, our 3https://openai.com/api/pricing/ 18 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 9: Scatter plots of predicted vs. actual cost for QLoRA fine-tuning across eight diverse tasks. The near-perfect diagonal alignment and low MAE values (0.000-0.007) demonstrate precise cost prediction capabilities across both resource-intensive (a-d) and lightweight tasks (e-h)."
        },
        {
            "title": "Tasks",
            "content": "Cost Level Pred. Acc (%) Act. Acc (%) MAE (%) Act. Total Cost ($) Ours Cost ($) CRR (%)"
        },
        {
            "title": "Low\nMedium\nHigh",
            "content": "94.87 94.71 94.94 84.09 85.91 86.82 94.87 95.64 95.84 85.76 86.36 88.18 0.00 0.93 0.90 1.67 0.45 1. 27.042 62.838 124.414 0.748 1.751 3.399 2.354 2.470 2.759 0.271 0.265 0.243 91.30 96.07 97.78 63.72 84.88 92. Table 5: COSMOS demonstrates robust performance across 275 modelstrategyconfiguration combinations, spanning diverse architectures (Gemma, Llama, Qwen, Mistral) and scales (2B-9B), maintaining high prediction accuracy (average MAE of 0.61% for HellaSwag and 1.16% for Multifin EN) while drastically reducing computational costs. expanded evaluation framework includes: Model pool: 5 models including Gemma 2B, Llama 3 8B, Gemma 2 9B, Qwen 2 7B, Mistral 7B v0.3 Strategies: QLoRA and retrieval-augmented ICL QloRA configurations: Training iterations {4, 5, 6, 7, 8} and data portions {0.1, 0.2, ..., 1.0} at 0.1 increments ICL configurations: retrieved number of demonstrations {1, 2, 4, 8, 16} This systematic design yields 275 modelstrategyconfiguration combinations (5 models (50 QLoRA + 5 ICL)) per task, with all results averaged across three random seeds to ensure statistical robustness. The results in Table 5 demonstrate COSMOSs consistent effectiveness across this expanded evaluation space even though we adopt the same training protocols for all models. For the general domain HellaSwag benchmark, COSMOS achieves an average Mean Absolute Error (MAE) of merely 0.61% across all three budget constraints, with perfect prediction accuracy (0% MAE) in the low-cost setting while reducing total evaluation costs by an average of 95.05%. Even for the domain-specific Multifin EN benchmark, which presents high variance scenarios (38-380 training samples), our method maintains strong performance with an MAE of 1.16%, demonstrating its robustness to data scarcity and domain shift. 19 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 10: Scatter plots comparing predicted vs. actual accuracy for ICL across eight diverse tasks. Axes are zoomed to highlight fine-grained prediction details, with an average MAE of 0.85% demonstrating high prediction fidelity. The consistent performance across both general domain (a-d) and financial domain (e-h) tasks validates our frameworks robust prediction capabilities."
        },
        {
            "title": "Task",
            "content": "Cost Level Act. Acc (%) Pred. Acc (%) MAE Pred. Acc (%) MAE Act. Cost ($) Ours cost ($) CRR (%) Ours cost ($) CRR (%) (100% data) (100% data) (10% data) (10% data) (100% data) (100% data) (10% data) (10% data)"
        },
        {
            "title": "Low\nMedium\nHigh",
            "content": "94.38 94.11 93.31 94.38 93.68 93."
        },
        {
            "title": "Average",
            "content": "0.00 0.43 0.16 0.20 94.38 94.11 92.58 0.00 0.00 0.73 0.24 13.30 17.28 10. 0.67 0.52 0.22 94.99 96.99 97.90 96.63 0.60 0.30 0.16 95.51 98.25 98.44 97. Table 6: COSMOS maintains high prediction accuracy with limited data: Using only 10% of training data achieves comparable MAE (0.24%) to full data access (0.20%) while further improving cost reduction (97.40% vs. 96.63%) across all budget constraints on HellaSwag."
        },
        {
            "title": "Task",
            "content": "Cost Level Act. Acc (%) Pred. Acc (%) MAE Pred. Acc (%) MAE Act. Cost ($) Ours cost ($) CRR (%) Ours cost ($) CRR (%) (100% data) (100% data) (10% data) (10% data) (100% data) (100% data) (10% data) (10% data)"
        },
        {
            "title": "Low\nMedium\nHigh",
            "content": "80.91 83.94 85.76 80.91 83.94 85."
        },
        {
            "title": "Average",
            "content": "0.00 0.00 0.00 0.00 80.91 83.94 85.76 0.00 0.00 0.00 0.00 0.287 0.504 0. 0.083 0.051 0.033 70.92 89.84 90.95 83.91 0.082 0.045 0.030 71.41 91.05 91.67 84. Table 7: COSMOS demonstrates perfect prediction accuracy (0% MAE) on Multifin EN in both full and limited data scenarios, while the 10% data setting yields enhanced cost savings (84.71% vs. 83.91%), highlighting the methods robustness to data constraints in domain-specific tasks. Is COSMOS robust with limited data access? practical question for deployment scenarios concerns whether COSMOS requires full access to the training dataset during performance prediction. While our main experiments utilized complete datasets for comprehensive evaluation, this design choice was motivated by convenience rather than necessityour lightweight linear model for QLoRA trains efficiently regardless of data volume. Importantly, COSMOSs data portion is configurable parameter that can be adjusted based on resource constraints in real-world applications. To assess the data efficiency of our approach, we conduct comparative analysis examining prediction performance when accessing 100% versus only 10% of the dataset during the prediction phase. Following our experimental protocol from Section 5, we evaluate three distinct cost constraints with performance-prioritizing score function across configuration space covering training iterations {4, 5, 6, 7, 8} and data portions {0.1, ..., 1.0}, with all results averaged over three random seeds for statistical robustness. Tables 6 and 7 show that COSMOSs predictors are robust to limited data. For the general domain HellaSwag task, 20 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 11: Scatter plots of predicted vs. actual cost for ICL across eight diverse tasks. our method achieves an average MAE of just 0.24% across three budget levels when using only 10% of the data, with two perfect predictions (0% MAE). This approach further improved cost savings from 96.63% (with 100% data) to 97.40% (with 10% data). For the domain-specific Multifin EN challenge, our method perfectly predicts (0% MAE) the optimal strategies across all three cost levels while enhancing cost savings from 83.91% to 84.71%."
        },
        {
            "title": "Iter",
            "content": "# shots Pred. Acc Act. Acc Predicted Cost ($) Act. Cost ($) 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.3 0.3 0.3 0.3 0.3 0.4 0.4 0.4 0.4 0.4 0.5 0.5 0.5 0.6 0.6 0.7 0.8 - - - 4 5 6 7 8 4 5 6 7 8 4 5 6 7 8 4 5 6 7 8 4 5 6 4 5 4 4 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1 2 4 0.894 0.890 0.885 0.882 0.875 0.898 0.893 0.888 0.885 0.878 0.905 0.900 0.895 0.892 0.885 0.908 0.903 0.898 0.895 0.888 0.910 0.906 0.901 0.915 0.911 0.921 0.921 0.526 0.591 0.617 0.894 0.890 0.885 0.882 0.875 0.922 0.907 0.909 0.910 0.907 0.923 0.921 0.916 0.912 0.914 0.934 0.931 0.927 0.927 0.919 0.930 0.933 0.928 0.940 0.930 0.937 0.944 0.526 0.627 0.604 0.181 0.201 0.221 0.240 0.260 0.259 0.298 0.337 0.376 0.415 0.337 0.395 0.454 0.512 0.571 0.415 0.494 0.572 0.650 0.728 0.494 0.592 0.690 0.573 0.690 0.651 0. 0.177 0.262 0.432 0.181 0.200 0.220 0.239 0.259 0.258 0.297 0.336 0.375 0.413 0.335 0.393 0.452 0.510 0.567 0.414 0.491 0.569 0.647 0.724 0.477 0.570 0.664 0.570 0.687 0.648 0.725 0.219 0.326 0."
        },
        {
            "title": "ICL",
            "content": "Table 8: COSMOS accurately predicts that QLoRA finetuning with 0.8 portion of data for 4 iterations yields optimal performance within the low cost budget. This strategy achieves the highest predicted accuracy (0.921) among all 30 available strategies, and validation confirms it indeed delivers the best actual performance (0.944). COSMOS predicts this strategy will cost $0.728, closely matching the actual cost of $0.725. In Tables 8, 9 and 10, we provide concrete example illustrating how practitioners can select the optimal strategy 21 (a) MMLU (b) Winogrande (c) ARC-Challenge (d) HellaSwag (e) FPB (f) FiQA-SA (g) Headline (h) Multifin EN Figure 12: Actual QLoRA vs. ICL performance-cost trajectories across eight diverse tasks. Each plot presents the performance-cost curves for QLoRA () and ICL () on Llama 3 8B, with Gemma 2B ( ), and Llama 3 8B ( ) serving as baselines. Vertical dashed lines demarcate low, medium, and high-cost thresholds, determined by the minimum and maximum costs of both adaptation strategies. The shaded regions represent the standard deviation across 3 seeds for each configuration."
        },
        {
            "title": "Iter",
            "content": "# shots Pred. Acc Act. Acc Predicted Cost ($) Act. Cost ($) 0.5 0.5 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.8 0.8 0.8 0.9 0.9 0.9 1.0 1.0 - 7 8 6 7 8 5 6 7 8 5 6 7 4 5 6 4 5 - - - - - - - - - - - - - - - - - - 8 0.898 0.891 0.906 0.903 0.896 0.917 0.911 0.908 0.901 0.917 0.912 0.909 0.925 0.921 0.915 0.931 0.928 0.620 0.926 0.919 0.933 0.926 0.922 0.933 0.928 0.929 0.926 0.936 0.934 0.930 0.941 0.936 0.929 0.937 0.939 0.620 0.787 0.885 0.807 0.925 1.042 0.788 0.924 1.061 1.198 0.884 1.041 1.197 0.807 0.983 1.158 0.886 1. 0.772 0.758 0.851 0.803 0.920 1.037 0.784 0.920 1.056 1.192 0.880 1.036 1.191 0.803 0.978 1.153 0.881 1.076 0."
        },
        {
            "title": "ICL",
            "content": "Table 9: Predicted performance and cost given by COSMOS and actual performance and cost corresponding to each strategy within the medium cost level."
        },
        {
            "title": "Iter",
            "content": "# shots Pred. Acc Act. Acc Predicted Cost ($) Act. Cost ($)"
        },
        {
            "title": "ICL",
            "content": "0.8 0.9 0.9 1.0 1.0 1.0 - 8 7 8 6 7 8 - - - - - - - 0.902 0.913 0.906 0.920 0.917 0.910 0.620 0.927 0.926 0.926 0.931 0.933 0.929 0.611 1.353 1.334 1.510 1.277 1.472 1.668 1. 1.346 1.327 1.502 1.270 1.465 1.659 1.815 Table 10: Predicted performance and cost given by COSMOS and actual performance and cost corresponding to each strategy within the high cost level. based on COSMOSs predicted metrics. Using the HellaSwag dataset across three cost regimes with performanceprioritizing function (Same setting in Section 5.1), we demonstrate the decision-making process. In the low-cost regime (30 candidate strategies), our predicted values identify QLoRA with 0.8 data portion for 4 iterations as the optimal choice, and this is the actual optimal strategy. For medium-cost scenarios (18 candidates), COSMOS guides selection toward 1.0 data portion with 4 iterations (0.937 accuracy), which closely approximates the ground-truth optimal strategy of 0.9 data portion with 4 iterations (0.941 accuracy, MAE: 0.004). Similarly, in high-cost settings 22 (7 candidates), our predicted best strategy (1.0 data portion, 6 iterations, 0.931 accuracy) nearly matches the optimal strategy (1.0 data portion, 7 iterations, 0.933 accuracy, MAE: 0.002). Note that while we present these examples with our performance-prioritizing objective, practitioners can define custom trade-off functions between performance and cost as described in Section 3.1."
        }
    ],
    "affiliations": [
        "University of Wisconsin-Madison"
    ]
}