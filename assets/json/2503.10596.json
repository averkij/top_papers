{
    "paper_title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
    "authors": [
        "Rui Hu",
        "Lianghui Zhu",
        "Yuxuan Zhang",
        "Tianheng Cheng",
        "Lei Liu",
        "Heng Liu",
        "Longjin Ran",
        "Xiaoxin Chen",
        "Wenyu Liu",
        "Xinggang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \\times$ faster than the GLaMM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 6 9 5 0 1 . 3 0 5 2 : r GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding Rui Hu1 Tianheng Cheng1 Lei Liu2 Heng Liu2 Lianghui Zhu1 Yuxuan Zhang1 Longjin Ran2 Xiaoxin Chen2 Wenyu Liu1 Xinggang Wang1 Equal contribution Project lead Corresponding author 1 School of EIC, Huazhong University of Science & Technology 2 vivo AI Lab hustvl/GroundingSuite Figure 1. Examples of our GroundingSuite dataset. Including four aspects: stuff-class segmentation for context-aware localization, part-level segmentation requiring fine-grained understanding, multi-object segmentation with complex referential relationships, and singleobject segmentation across diverse appearance variations. Note that red color means the referring is wrong."
        },
        {
            "title": "Abstract",
            "content": "4.5 faster than the GLaMM. Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) largescale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, cIoU of 68.9 on gRefCOCO and gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., 1. Introduction Recent advancements in pixel grounding have fostered significant interest due to their remarkable segmentation performance in following natural language descriptions. However, this task remains constrained by existing benchmark limitations. As shown in Tab. 1, widely-used datasets like RefCOCO series [8, 17, 18, 32], while valuable for early research, restrict their scope to closed-set categories and limited manually annotated data samples. Specifically, only 80 object categories from the COCO dataset [13] hinder the pixel grounding task from generalizing to openvocabulary understanding, diverse granularity levels (e.g., part-level segmentation), and complex scene compositions (e.g., multi-object interactions and background separation). Besides, current datasets can not satisfy both the amount and annotation quality requirements, i.e., manually annotated datasets can not scale up due to the heavy human effort, and automatically annotated datasets often show inferior label quality."
        },
        {
            "title": "Some",
            "content": "automatic annotation approaches, such as 1 GLaMM [21] and MRES [26], are proposed to alleviate the heavy human annotation burden but also face low-quality annotation and high-cost utilization problems. From the perspective of annotation quality, GLaMM suffers from unresolved textual ambiguities, and MRES only works with restricted fixed vocabularies. As for the cost, GLaMM has 23 pipeline steps, requiring large amount of GPU resources, and MRES relies on human-annotated boxes as initialization. To address the above challenges, we propose GSSculpt, an annotation framework that involves vision-language models (VLM) as efficient annotation agents and effective quality checkers. Specifically, GSSculpt incorporates three critical components: entity spatial localization, grounding text generation, and noise filtering. Entity spatial localization is the first phase, in which we generate comprehensive image captions, employ precise phrase grounding techniques, and utilize SAM2 [22] to extract highquality masks. Then, we use state-of-the-art multimodal models with carefully designed prompts to generate unambiguous descriptions with clear positional relationships in the grounding text generation phase. Last, we employ instruction-based segmentation models to filter the noisy references, ensuring high data quality throughout the collection. Based on the GSSculpt, we further curated large-scale training set, i.e., GSTrain-10M, and comprehensive evaluation benchmark, i.e., GSEval. The above three components make up the proposed GroundingSuite. Specifically, GSTrain-10M comprises 9.56-million-image training corpus automatically annotated on SA-1B dataset images [9] using our hybrid framework, featuring diverse textual descriptions averaging 16 words in length and unambiguous instruction-mask pairs. GSEval is novel evaluation benchmark containing 3,800 images carefully selected from COCOs unlabeled datasets [13], ensuring zero overlap with existing annotated sets while maintaining natural scene diversity. As shown in Fig. 1, the proposed benchmark specifically covers four mainstream aspects of segmentation: stuff-class segmentation [3, 35, 36] for contextaware localization, part-level segmentation [4, 6, 12] of fine-grained understanding, multi-object segmentation [14] with complex referential relationships, and single-object segmentation [8, 17, 18, 32] across diverse appearance variations. Our main contributions of GroundingSuite can be summarized as follows: To address the low-quality annotation and high-cost utilization problems of existing auto-labeling methods, we propose GSSculpt, vision-language models (VLM) based automatic annotation framework, which produces accurate annotations and reduces 78% of pipeline steps when compared to GLaMM."
        },
        {
            "title": "Benchmarks",
            "content": "Cat. Len. Stuff Part Multi Single RefCOCO [8, 32] RefCOCO+ [8, 32] RefCOCOg [17, 18] gRefCOCO [14] RefCOCOm [26] GSEval 3.6 80 3.5 80 8.4 80 3.7 80 471 5.1 16.1 Table 1. Comparisons with previous Referring Expression Segmentation benchmark. Cat. and Len. denote the number of categories and average text length; Stuff: includes stuff classes; Part: includes part-level annotations; Multi: supports multi-object references; Single: supports single-object references; We introduce large-scale training dataset and carefully curated evaluation benchmark, laying solid foundation for future research in pixel grounding. This dataset addresses critical limitations in existing grounding datasets, i.e., limited object categories, insufficient textual diversity, and scarcity of high-quality annotations, while supporting diverse segmentation scenarios. The proposed dataset presents substantial performance enhancements across baseline methods. Models trained on our data consistently demonstrate superior results, establishing new state-of-the-art benchmarks across mulSpecifically, our approach tiple evaluation metrics. achieves cIoU of 68.9 on gRefCOCO and gIoU of 55.3 on RefCOCOm. 2. Related Work 2.1. Automatic Annotation for Pixel Grounding CLIP [19] and GLIP [11] pioneered using vision-language models for label generation, while SAM [9] enabled promptable segmentation at scale. Building on these, recent work explores automatic dataset creation through model collaboration. GranD [21] is Developed using an automatic annotation pipeline and verification criteria, it encompasses 7.5M unique concepts grounded in 810M regions. However, the GLaMM [21] process has many steps, leading to the accumulation of errors from multiple models, and as result, it hasnt solved the problem of text ambiguity. MRES [26] build large visual grounding dataset namely MRES32M, which comprises over 32.2M highquality masks and captions on the provided 1M images. However, it relies on box annotations from existing datasets and only uses fixed vocabulary, which limits its generality. 2.2. Pixel Grounding Benchmarks Mainstream benchmarks such as RefCOCO [8, 32], RefCOCO+ [8, 32], and RefCOCOg [17, 18] have driven progress in language-guided segmentation. However, their reliance on COCOs categorical constraints (80 classes) and 2 human-annotated referring expressions creates artificial domain limitations. Among recent works, RefCOCOm [26] contains 80 object categories and an associated 391 part categories. Despite its part-level advancements, RefCOCOms reliance on COCOs fixed 80 object categories limits its ability to benchmark open-vocabulary or cross-category referring segmentation, hindering evaluation of models adaptability to unseen object types in real-world scenarios. GRES [14] is new benchmark called Generalized Referring Expression Segmentation, which extends the classic RES to allow expressions to refer to an arbitrary number of target objects. The GRES benchmark has several limitations, including its restriction to the 80 object categories from the COCO dataset, the lack of part-level segmentation, and the absence of background class for segmentation. 3. GSSculpt: Large-scale Grounding Labeling 3.1. Overview We introduce our vision-language models (VLM) based automatic annotation framework GSSculpt, designed to automatically generate high-quality pixel grounding data at scale. Through comprehensive analysis of existing approaches, we propose three critical components essential for the effective auto-labeling framework, as shown in Fig. 2, i.e., (1) entity spatial localization: discovering regions/objects of interest and generating high-quality masks; (2) grounding text generation: generating precise and uniquely identifiable language descriptions for regions or objects; and (3) noise filtering: eliminating ambiguous or low-quality samples. The proposed framework GSSculpt provides elaborate designs for each component to ensure annotation quality and accuracy, collectively constructing an efficient streamlined annotation framework, which aims to advance the state of pixel-level grounding datasets. Data source. For large-scale training data, we primarily utilize SA-1B [9] as our data source, which comprises 1.1 billion high-quality segmentation masks across 11 million diverse, high-resolution images. In this paper, we sample 2M images for annotation. While the segmentation annotations in SA-1B focus on diverse geometric prompts, we concentrate on semantically meaningful objects or regions. Therefore, we employed SAM-2 [22] for segmentation annotation instead of directly using the mask annotations provided by SA-1B. 3.2. Entity Spatial Localization The proposed framework builds upon precise object/region recognition and localization within complex visual scenes. This critical first stage employs sophisticated three-tiered approach that systematically addresses the challenges of visual parsing. Global caption generation. We leverage cutting-edge large visual-language model, i.e., InternVL2.5 [5], to produce comprehensive scene descriptions, discovering all semantically significant objects or regions within each image with remarkable completeness. Phrase grounding. The generated captions serve as input for Florence-2 [29], as the advanced phrase grounding model, which precisely grounds texts to corresponding spatial locations. This process provides preliminary bounding box regions for candidate objects. Mask generation. Then we adopt SAM2 [22] to obtain pixel-level segmentation masks for the grounded texts using bounding boxes as spatial prompts. 3.3. Grounding Text Generation The second stage primarily further optimizes the textual descriptions of grounded regions with Large Language Models, enriching the grounding texts with the context of the image. We design specialized prompt templates to guide multimodal language models in generating distinct and unambiguous references. These templates strategically emphasize spatial relationships, distinctive visual features, and contextual cues. Using powerful models like InternVL2.5, we generate linguistically diverse and rich descriptions. Our method produces natural descriptions averaging 16 words, significantly more expressive than the shorter phrases in manually annotated datasets, while maintaining referential clarity and eliminating ambiguity. 3.4. Noise Filtering The noise filtering stage ensures dataset quality by eliminating ambiguous or incorrect annotations: We employ instruction-based segmentation models, i.e., EVF-SAM [33], to identify potentially ambiguous referring expressions by measuring consistency between the generated expression and the corresponding mask. Specifically, we use the generated texts in previous steps to prompt Referring Expression Segmentation (RES) model to produce masks, and then calculate the IoU between the generated masks and the annotated mask. By applying an IoU threshold, i.e., 0.5, we can effectively filter out inaccurate textmask pairs. This comprehensive filtering approach achieves an optimal balance between dataset scale and annotation quality, resulting in 9.56 million high-quality training samples while ensuring the integrity and quality of the final dataset. 3.5. GSTrain-10M Applying the proposed annotation framework to diverse subset of images from the SA-1B dataset, we have created GSTrain-10M, large-scale comprehensive training Figure 2. GSScuplt Automatic Annotation Framework. Our pipeline consists of three sequential phases: (1) Entity Spatial Localization, where we first identify potential objects of interest and generate high-quality segmentation masks; (2) Grounding Text Generation, where we then create unambiguous natural language descriptions that uniquely reference the segmented objects; and (3) Noise Filtering, where we finally eliminate ambiguous or low-quality samples to ensure dataset reliability. dataset, aiming for pixel grounding and comprising 9.56 million high-quality text-mask pairs across 2 million images. GSTrain-10M contains linguistically rich descriptions with an average length of 16 words, significantly more detailed than existing manually annotated datasets. The GSTrain-10M dataset covers an open vocabulary of concepts including common objects, fine-grained parts, amorphous stuff categories, and diverse scenes. Each annotation undergoes rigorous noise-filtering process, ensuring unambiguous references with clear spatial relationships. The presented GSTrain-10M represents substantial advancement in both scale and quality for pixel grounding, enabling more robust and generalizable model training across wider range of visual concepts and linguistic expressions. 4. GSEval: Comprehensive Benchmark Building upon the aforementioned annotation framework, we further develop GSEval, comprehensive evaluation benchmark for pixel grounding tasks. As shown in Fig. 3, we propose human-guided curation pipeline to ensure data quality and task diversity. 4 4.1. Automatic Data Curation We first apply the proposed annotation framework (Sec. 3) to the unlabeled images of the COCO dataset [13]. The generated annotations and labeled images have no overlap with existing labeled datasets. Then, we use vision-language models (VLM) to assign categories for referring prompts. This pre-categorization helps to reduce the cost of subsequent manual selection and filters out noisy referring-mask pairs. Last, we adopt matting methods to refine the boundaries of object masks. Specifically, we translate the mask areas to trimaps and use the off-the-shelf matting model [31] to generate precise boundaries. 4.2. Human Data Curation To ensure the quality of GSEval, we involve human reviewers for manual checks. Specifically, human reviewers carefully select and verify images across four distinct categories: stuff segmentation: 1000 images featuring background elements requiring context-aware localization, such as sky or sea; part-level segmentation: 500 images focusing on object components demanding fine-grained understanding, such as camera on the phone or mans beard; Figure 3. Curation pipeline for GSEval Benchmark. First, we apply our annotation pipeline to unlabeled COCO images. Next, we use VLM classifier to ensure the categories of referring prompts. Then, we translate the coarse masks to trimaps and apply matting methods for precise object boundaries. Finally, we organize human reviewers for manual checks. multi-object segmentation: 800 images containing complex referential relationships between multiple entities, such as flock of sheep or two dogs; single-object segmentation: 1500 images showcasing diverse appearance variations, such as brown cat or colorful parrot. This enhanced human-in-the-loop approach significantly improves efficiency while maintaining strict quality standards. The VLM pre-classification reduces the human reviewers annotation burden, allowing them to focus on nuanced verification. By strategically distributing images across the four categories, we ensure comprehensive evaluation across varying levels of granularity and complexity. The curated benchmark covers multiple domains and object types, providing robust assessment of models generalized segmentation capabilities in real-world scenarios. 4.3. GSEval-BBox In addition to the pixel-based segmentation benchmark, we construct bounding box version, namely GSEval-BBox. Specifically, GSEval-BBox is designed to evaluate the visual grounding capabilities of multimodal large language models. GSEval-BBox converts the high-quality segmentation masks into corresponding bounding boxes, enabling direct assessment of referential object localization. 4.4. Evaluation Metrics Pixel-level grounding. We adopt the standard metric for pixel level evaluation, i.e., gIoU. The gIoU metric calculates the average of per-image Intersection-over-Union (IoU) scores: gIoU ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Pi Gi Pi Gi , where Pi and Gi denote the predicted segmentation mask and ground truth mask for the i-th sample, respectively. is the total number of samples. In contrast to previous methods using cIoU, the proposed GSEval includes partlevel segmentation tasks and we prioritize gIoU as the primary evaluation metric. Specifically, gIoU provides more balanced assessment across objects of varying sizes, while cIoU exhibits bias toward larger objects and demonstrates greater volatility in measurements. Box-level grounding. For box-level grounding, we adopt the same metrics as in referring expression comprehension (REC), which typically includes accuracy at different IoU thresholds (e.g., Acc@0.5). 5. Evaluation on GSEval In this section, we evaluate several representative methods on our proposed GSEval under zero-shot settings. 5.1. Benchmark Details As shown in Tab. 1, GSEval significantly surpasses existing referring expression segmentation benchmarks in multiple dimensions. Compared to the RefCOCO dataset, which primarily relies on COCO category annotations, our proposed GSEval adopts an open-vocabulary setting, encompassing not only foreground objects but also stuff categories and various part-level categories. In addition, the average text 5 length in GSEval reaches 16.1 words, making its descriptions substantially more detailed and nuanced compared to others. Furthermore, GSEval is uniquely comprehensive in supporting all key features: stuff classes, part-level annotations, and both multi-object and single-object references. This comprehensive design enables more challenging and realistic evaluation scenarios that better reflect real-world language grounding applications. 5.2. Benchmark Results on GSEval Tab. 2 presents the zero-shot performance of previous stateof-the-art RES methods on our GSEval across four challenging subsets, i.e., stuff, part, multi-object and singleobject, which provides comprehensive assessment of model capabilities. For comparison, we also report the performance on RefCOCO/+/g benchmarks (averaged across val/testA/testB splits) using compound IoU (cIoU). Limited generalization ability of specialists. As shown in Tab. 2, although the specialists (e.g., LAVT [30] and ReLA [14]) achieved good results on RefCOCO-series benchmarks, the performance on GSEval is poor, especially for stuff and part cases, which were not major focus in the previous training and evaluation data. Compared to MLLMbased methods (multimodal large language models), it is evident that those specialists have weaker generalization capabilities. In addition, the dramatic performance drop highlights the limitations of existing benchmarks and validates the need for more comprehensive evaluation benchmarks like GSEval. Limited fine-grained localization ability of MLLMs. As shown in Tab. 2, although some MLLM-based methods have achieved good performance on GSEval, especially for the stuff category, their performance on fine-grained partlevel pixel grounding is clearly inferior to other subsets. While LLMs possess strong text understanding and reasoning capabilities, they still have significant limitations in fine-grained image localization. Notably, we observe significant discrepancy between the GSEval and RefCOCOs scores, such as InstructSeg [27] and LISA [10]. InstructSeg achieves state-of-the-art performance on RefCOCOs (81.9 cIoU) but performs poorly on GSEval (52.5 gIoU). In contrast, LISA has relatively low scores on RefCOCOs (67.9 cIoU) but achieves decent accuracy on GSEval (57.6 gIoU). We further analyze the training data of both methods and find that LISA incorporates wide variety of datasets from different tasks including ADE20K [35, 36] and COCO-Stuff [3], whereas InstructSeg was more focused on referring expression segmentation. This makes LISA more generalizable compared to InstructSeg and further validates that our proposed GSEval is better suited than RefCOCOs for evaluating the performance of general-purpose multi-task pixel grounding methods. Figure 4. The visualization comparisons of different methods on GSEval. All methods are evaluated under the zero-shot setting with the public code and weights. Moreover, we provide the visualization results on GSEval. As shown in the Fig. 4, InstructSeg performs poorly on stuff and part categories, tending to segment entire objects instead. In contrast, LISA and EVF-SAM are better at following instructions to locate the target regions, but their segmentation accuracy still has some limitations. 5.3. Benchmark Results on GSEval-BBox Additionally, we further evaluated the bbox-level grounding performance of multimodal large language models, as shown in Tab. 3. Gemini-1.5-Pro [25] demonstrates exceptional performance on GSEval, particularly in part-level grounding. However, for open-source models, part-level grounding remains challenging subset. As mentioned earlier, fine-grained object localization is relatively difficult for multimodal large language models. In Fig. 5, we further visualize the results of differIt is clear that ent multimodal large language models. Qwen2.5VL [2] and InternVL2.5 [5] struggle to locate partlevel targets. Even when the targets are located, the bounding box precision is relatively low, making it difficult to achieve accurate object localization. In contrast, Gemini1.5-Pro currently performs better, but there is still significant gap in overall performance. Since RefCOCO is more focused on foreground objectlevel grounding, with the development of multimodal large language models, it has become increasingly difficult to use RefCOCO as the primary benchmark for evaluating the performance of multimodal models. comprehensive, multi-granularity grounding benchmark is therefore greatly needed. As result, we believe that GSEval plays crucial role in the grounding evaluation of multimodal models."
        },
        {
            "title": "Methods",
            "content": "LAVT [30] ReLA [14] LISA-7B [10] GLaMM [21] PSALM [34] EVF-SAM [33] InstructSeg [27]"
        },
        {
            "title": "MLLM",
            "content": "RefCOCO/+/g (AVG) 65.8 68.3 67.9 75.6 77.1 79.3 81.9 Stuff 6.0 3.8 85.2 86.9 39.0 85.1 56. Part 10.7 8.8 21.2 16.5 10.0 23.1 24."
        },
        {
            "title": "GSEval\nMulti",
            "content": "Single 45.0 46.7 71.5 70.4 53.7 72.1 66.8 25.8 19.7 42.8 42.1 36.9 54.5 51.3 All 22.5 19.7 57.6 57.2 37.7 62.6 52.5 Table 2. Comparison among previous SOTA RES methods on our GSEval in terms of gIoU, while we report average cIoU for RefCOCO/+/g."
        },
        {
            "title": "Type",
            "content": "RefCOCO/+/g (AVG) Gemini-1.5-Pro [25] Doubao-1.5-vision-pro [24] Claude-3.7-sonnet [1] GPT-4o [7] InternVL2.5-78B [5] InternVL2.5-8B [5] Qwen2.5-VL-72B [2] Qwen2.5-VL-7B [2] DeepSeek-VL2 [28]"
        },
        {
            "title": "Proprietary Model",
            "content": "Open-source Model - - - - 92.3 87.6 90.3 86.6 93.0 Stuff 86.7 75.3 56.7 42.2 85.3 91.3 88.4 93.0 86. Part 58.5 19.2 2.6 2.0 16.8 7.3 31.2 17.6 12.7 GSEval-BBox Multi Single 61.7 62.6 20.7 15. 63.2 65.7 42.8 75.9 64.8 77.3 51.9 9.4 6.1 55.7 47.3 64.6 59.1 51.2 All 74.3 56.5 23.8 17.3 62.2 58.2 62.5 66.7 60. Table 3. Comparison among previous grounding methods on our GSEval-BBox. All metrics measure referring expression comprehension accuracy (%). 6.1. Effectiveness on Pixel Grounding Methods To evaluate the impact of our GSTrain-10M, we select two representative methods, i.e., EVF-SAM [33] and LISA7B [10], and test them with and without our GSTrain-10M. Experimental details. For the baseline, we followed the original settings of EVF-SAM and LISA-7B. Specifically, EVF-SAM utilizes multiple datasets, including the RefCOCO series [8, 17, 18, 32], Objects365 [23], ADE20K [35, 36], Pascal-Part [4], HumanParsing [12], and PartImageNet [6]. Meanwhile, LISA-7B employs the RefCOCO series [8, 17, 18, 32], ADE20K [35, 36], COCO-Stuff [3], PACOLVIS [20], PartImageNet [6], Pascal-Part [4], as well as LLaVAInstruct-150k for LLaVA-v1 [15], LLaVA-v1.5mix665k for LLaVA v1.5 [16], and ReasonSeg [10]. Results. In Tab. 4, we evaluate the performance using the proposed GSTrain-10M of EVF-SAM and LISA on several benchmarks. The results demonstrate that our dataset brings significant performance improvements to both methods across all benchmarks. For EVF-SAM, we observe 14.7, 2.9, and 4.0 points improvements on GSEval, gRefCOCO, and RefCOCOm benchmarks, respectively. The consistent gains across difFigure 5. The visualization comparisons of differnet methods on our GSEval-BBox. All open-source methods are evaluated under the zero-shot setting with the public code and weights. 6. Experiments In this section, we conduct series of ablation studies to further validate the effectiveness of the automatically annotated training dataset, GSTrain-10M. We describe the details of each experiment in the corresponding subsection. 7 Methods GSTrain-10M GSEval gRefCOCO RefCOCOm"
        },
        {
            "title": "GSEval RefCOCO gRefCOCO RefCOCOm",
            "content": "EVF-SAM EVF-SAM LISA-7B LISA-7B 63.5 62.6 77.3 +14.7 66.4 +2.9 57.6 73.6 +16.0 36.3 +4.1 32. 51.3 55.3 +4.0 34.2 39.3 +5.1 Table 4. Ablation study on the effectiveness of our dataset on different methods. ferent evaluation benchmarks highlight the transferability of knowledge acquired from our diverse training corpus. the improvements For LISA-7B, we observe 16.0, 4.1, and 5.1 points improvements on GSEval, gRefCOCO, and RefCOCOm benchmarks, respectively. The greater relative improvement observed in LISA-7B suggests that language-centric models particularly benefit from our datasets text diversity and the richness of our referring expressions (averaging 16 words in length). Furthermore, across consistent both external benchmarks (gRefCOCO and RefCOCOm) demonstrate that the knowledge gained from our dataset generalizes well beyond the specific distribution of GSEval. Considering the gRefCOCO focuses on multi-object relationships, while RefCOCOm evaluates fine-grained semantic understanding, the performance gains across these diverse evaluation scenarios demonstrate the robustness and generality of the proposed GSTrain-10M. These results collectively validate that the proposed GSSculpt produces high-quality training data that enhances model performance across architectural paradigms, supporting more accurate and nuanced referring expression segmentation capabilities. 6.2. Comparisons with Other Automated Datasets To further validate our approach, we conduct comparative analysis between the proposed GSTrain-10M and GranD, another state-of-the-art automatically generated dataset introduced with GLaMM [21]. Experimental details. For fair comparisons, we randomly selected 100k samples from both GranD and the proposed GSTrain-10M to train models under the EVF-SAM framework while incorporating the RefCOCO series datasets. This experimental setting allows us to directly assess the quality and effectiveness of the two datasets while keeping other variables constant. Results. As shown in Tab. 5, we evaluate models trained on each dataset across three standard benchmarks: RefCOCO, gRefCOCO, and RefCOCOm. The results reveal interesting performance patterns that highlight the relative strengths of each approach. On the standard RefCOCO benchmark, the proposed GSTrain-10M achieved performance gain of 0.7 percent8 GranD [21] 60.0 Ours 74.9 +14.9 76.7 +0.7 76.0 65.6 64.8 -0. 37.9 39.5 +1.6 Table 5. Ablation study on the effectiveness of different automated datasets."
        },
        {
            "title": "Ratio",
            "content": "0% 20% 50% 100% Stuff 85.1 93.2 94.6 94.7 Part 23.1 43.2 54.0 59."
        },
        {
            "title": "GSEval\nMulti",
            "content": "72.1 85.0 88.3 89.0 Single 54.5 68.2 72.7 72.4 All 62.6 75.4 79.6 80.3 Table 6. Ablations on the scalability. age points over GranD (76.7 vs. 76.0). More significantly, on the challenging RefCOCOm benchmark, which features more complex linguistic descriptions, our approach demonstrated substantial improvement of 1.6 points (39.5 vs. 37.9). 6.3. Scalability To investigate the effect of data scalability on model performance, we train EVF-SAM with different proportions of our GSTrain-10M without other datasets, such as RefCOCO. Experimental details. We follow the training settings from [33] and train EVF-SAM with different proportions of our GSTrain-10M (0%, 20%, 50%, 100%) for 10 epochs and then evaluate it on the proposed GSEval. Results. As shown in Tab. 6, the results show that model performance increases significantly across all aspects as the data proportion increases. Notably, we can observe clear rising curve with the increment of training data ratio, demonstrating the scalability of the proposed GSTrain10M. 7. Conclusion In this paper, we present GroundingSuite, which contains vision-language model (VLM) based automatic annotation framework, large-scale, textually diverse training corpus (9.56M masks), and comprehensive evaluation framework. Extensive experiments demonstrate that models trained on GSTrain-10M consistently establish new stateof-the-art results across multiple benchmarks. The proposed GroundingSuite lays solid foundation for the visual language understanding domain, providing valuable support for future research endeavors."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet system card. https:// assets.anthropic.com/m/785e231869ea8b3b/ original / claude-3-7-sonnet-system-card . pdf, 2024. 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 7 [3] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. CoRR, abs/1612.03716, 2016. 2, 6, 7 [4] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect what you can: Detecting and representing objects using holistic models and body parts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19711978, 2014. 2, 7 [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 3, 6, 7 [6] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xiaoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, Qihang Yu, and Alan Yuille. Partimagenet: large, highquality dataset of parts. In European Conference on Computer Vision, pages 128145. Springer, 2022. 2, [7] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 [8] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 1, 2, 7 [9] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2, 3 [10] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 6, 7 [11] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022."
        },
        {
            "title": "In Proceedings of",
            "content": "[12] Xiaodan Liang, Chunyan Xu, Xiaohui Shen, Jianchao Yang, Si Liu, Jinhui Tang, Liang Lin, and Shuicheng Yan. Human parsing with contextualized convolutional neural network. In Proceedings of the IEEE international conference on computer vision, pages 13861394, 2015. 2, 7 [13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 1, 2, 4 [14] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. 2, 3, 6, 7 [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 7 [16] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 7 [17] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. 1, 2, [18] Varun Nagaraja, Vlad Morariu, and Larry Davis. Modeling context between objects for referring expression understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 792807. Springer, 2016. 1, 2, 7 [19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [20] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts In Proceedings of the and attributes of common objects. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. 7 [21] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 2, 7, 8 [22] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 2, 3 [23] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 7 Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 51225130. 2, 6, 7 [36] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. 2, 6, 7 [24] Doubao Team. Doubao-1.5-pro. https : / / team . doubao . com / en / special / doubao _ 1 _ 5 _ pro, 2024. 7 [25] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, 7 [26] Wenxuan Wang, Tongtian Yue, Yisi Zhang, Longteng Guo, Xingjian He, Xinlong Wang, and Jing Liu. Unveiling parts beyond objects: Towards finer-granularity referring expresIn Proceedings of the IEEE/CVF Consion segmentation. ference on Computer Vision and Pattern Recognition, pages 1299813008, 2024. 2, [27] Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, and Yujiu Yang. Instructseg: Unifying instructed visual segmentation with multi-modal large language models. arXiv preprint arXiv:2412.14006, 2024. 6, 7 [28] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 7 [29] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 48184829. IEEE, 2024. 3 [30] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. 6, 7 [31] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. Vitmatte: Boosting image matting with pretrained plain vision transformers. Information Fusion, 103: 102091, 2024. 4 [32] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 1, 2, 7 [33] Yuxuan Zhang, Tianheng Cheng, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for arXiv preprint text-prompted segment anything model. arXiv:2406.20076, 2024. 3, 7, [34] Zheng Zhang, Yeyao Ma, Enming Zhang, and Xiang Bai. Psalm: Pixelwise segmentation with large multi-modal model. In European Conference on Computer Vision, pages 7491. Springer, 2024. 7 [35] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through In 2017 IEEE Conference on Computer Barriuso, and Antonio Torralba. ADE20K dataset. 10 GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding"
        },
        {
            "title": "Supplementary Material",
            "content": "A. GSEval Details Fig. 6 shows the word cloud visualization of our benchmarks textual descriptions, highlighting the linguistic diversity and domain coverage of GSEval. Fig. 7 illustrates additional samples from our GSEval. The six images on the left represent the stuff class category, while the six images on the right demonstrate partlevel segmentation examples. Fig. 8 further showcases the diversity of our dataset, with the left panel displaying multi-object instances and the right panel presenting single-object examples. B.4. Prompt for different grounding models For different grounding models, we apply customized prompt templates to generate bounding box coordinates: Gemini-1.5-Pro: Return bounding box for [Referring] in this image in [xmin, ymin, xmax, ymax] format. GPT-4o and Claude-3.7-sonnet: In this image, please locate the object described as: [Referring]. Provide the bounding box coordinates in the format [x min, min, max, max]. You can use either absolute pixel coordinates or normalized coordinates (0-1 range). Doubao-1.5-vision-pro: Please provide the bounding box coordinate of the region this sentence describes: [Referring] InternVL2.5: Please provide the bounding box coordinate of the region this sentence describes: \"<ref>[Referring]</ref>\" Qwen2.5-VL: Please nate <object ref start>[Referring]<object ref end> coordidescribes: bounding this provide the the region sentence box of Figure 6. The word cloud of GSEval Deepseek-VL-2: <image><ref>[Referring]</ref>. B. Prompt B.1. Global caption generation We utilize the InternVL2.5-78B model to generate comprehensive global captions for the images. The specifically designed prompt template employed for this purpose is presented in Figure 9. B.2. Grounding text generation We employ specialized prompt templates with InternVL2.5 to generate unambiguous references that emphasize spatial relationships and distinctive visual features. The detailed prompt template is illustrated in Figure 9. B.3. Noise filtering During the noise filtering stage, we employ two-step approach: first prompting the Vision-Language Model (VLM) to assess referring expression accuracy, then using specialized prompts to classify the referring expressions by category. Both prompt templates are illustrated in Figure 10. 1 Figure 7. More selected samples from our GSEval. Stuff class and part level Figure 8. More selected samples from our GSEval. Multi object and single object 2 Figure 9. Prompt for global caption and grounding text generation Figure 10. Prompt for noise filtering"
        }
    ],
    "affiliations": [
        "School of EIC, Huazhong University of Science & Technology",
        "vivo AI Lab"
    ]
}