{
    "paper_title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation",
    "authors": [
        "Siyou Li",
        "Pengyao Qin",
        "Huanan Wu",
        "Dong Nie",
        "Arun J. Thirunavukarasu",
        "Juntao Yu",
        "Le Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expert-written reports. To address these challenges, we propose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale $\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The novel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited data for RRG tasks. At the same time, for prompt engineering, we introduce a five-stage, LLM-driven pipeline that converts routine CT reports into paired visual-question-answer triples and citation-linked reasoning narratives, creating a scalable, high-quality supervisory corpus for explainable multimodal radiology LLM. All code, datasets, and models will be publicly available in our official repository. https://github.com/Siyou-Li/u2Tokenizer"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 2 6 1 3 0 0 . 7 0 5 2 : r µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation Siyou Li1, Pengyao Qin2, Huanan Wu3, Dong Nie4, Arun J. Thirunavukarasu5, Juntao Yu1, and Le Zhang2,6 1 School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK 2 School of Engineering, College of Engineering and Physical Sciences, University of Birmingham, Birmingham, UK 3 Guangdong University of Technology, Guangdong, China 4 Meta Inc. US 5 Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK 6 William Harvey Research Institute, NIHR Barts Biomedical Research Centre, Queen Mary University London, London, UK {siyou.li, juntao.yu}@qmul.ac.uk; l.zhang.16@bham.ac.uk Abstract. Automated radiology report generation (RRG) aims to produce detailed textual reports from clinical imaging, such as computed tomography (CT) scans, to improve the accuracy and efficiency of diagnosis and provision of management advice. RRG is complicated by two key challenges: (1) inherent complexity in extracting relevant information from imaging data under resource constraints, and (2) difficulty in objectively evaluating discrepancies between model-generated and expertwritten reports. To address these challenges, we propose µ2LLM, multiscale multimodal large language models for RRG tasks. The novel µ2Tokenizer, as an intermediate layer, integrates multi-modal features from the multiscale visual tokenizer and the text tokenizer, then enhances report generation quality through direct preference optimization (DPO), guided by GREEN-RedLlama. Experimental results on four large CT image-report medical datasets demonstrate that our method outperforms existing approaches, highlighting the potential of our fine-tuned µ2LLMs on limited data for RRG tasks. At the same time, for prompt engineering, we introduce five-stage, LLM-driven pipeline that converts routine CT reports into paired visual-question-answer triples and citation-linked reasoning narratives, creating scalable, high-quality supervisory corpus for explainable multimodal radiology LLM. All code, datasets, and models will be publicly available in our official repository.7 Keywords: Radiology Report Generation Computed Tomography Tokenizer Multimodal Large Language Models. 7 https://github.com/Siyou-Li/u2Tokenizer 2 1 Siyou Li et al."
        },
        {
            "title": "Introduction",
            "content": "Radiology reports are the primary means by which radiologists communicate their findings, likely diagnoses, and management recommendations to referring physicians and surgeons [21]. These reports must be accurate and interpretable, as ambiguous language or mistakes can lead to clinical error as well as increased patient anxiety [15]. Expert reports are especially important for imaging that referrers are frequently unable to interpret independently, such as computed tomography (CT). An increasing volume of CT examinations year-on-year generates pressure for radiologists to produce more high-quality reports, compounded by workforce shortages [4]. Emerging artificial intelligence (AI) and natural language processing (NLP) technologies built upon foundation model architectures show promise in automating radiology report generation (RRG) [17]. If accurate, automated RRG may streamline radiologist workflows, reduce reporting time, and enhance report quality. Automated RRG may also facilitate large-scale data extraction for clinical research, improving the usability of radiological data [12]. Integrating AI into clinical practice may thereby enhance diagnostic accuracy, improve patient outcomes, and help meet the demand for healthcare services. Existing RRG models are typically built around LLaVA [11], where input data consists of CT images resized to fixed dimensions. However, CT images exhibit variability in length, width, and height, and the resizing processes can distort anatomical details and lesions, potentially compromising diagnostic accuracy. Moreover, the direct incorporation of high-resolution CT images into analytical pipelines is inefficient and frequently prohibited by limited computational resources, rendering the comprehensive and efficient extraction of pertinent imaging data critical challenge in the report generation. Additionally, there is no unified standard for structuring radiology reports; clinicians prioritize the content of the findings over strict character-level alignment. Traditional NLP evaluation metrics, such as BLEU [13] and ROUGE [10], are therefore not wellsuited for evaluating RRG because they focus on lexical similarity rather than clinical salience or meaning. By comparing mainstream RRG models [19], [1], we identify two significant challenges: (1) limitations in image encoders, as conventional approaches that crop or downsample CT scans lead to significant information loss, particularly along organ boundaries; and (2) the inadequacy of conventional NLP evaluation methods, which fail to accurately capture the semantic and clinical relevance of generated reports compared to ground truth. Our contributions: we propose novel automated RRG approach based on multi-modal large language model (MLLM) named µ2LLM, as shown in Fig. 1. Our framework is designed to efficiently and cost-effectively preserve critical imaging details through guided question integration within the MLLM. Central to our approach is the proposed µ2Tokenizer, an intermediate processing layer that applies multi-level attention mechanisms and multi-scale aggregation on the outputs of the visual tokenizer (ViT3D). This layer further incorporates multi-modal attention to seamlessly fuse input question embeddings with refined CT image embeddings, thereby maximizing their semantic correspondence while maintaining computational efficiency. For evaluation, we identify and elucidate µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 3 Fig. 1: Overview of our proposed µ2LLM model that is centered with the µ2Tokenizer layer for high quality RRG task. clinically significant errors using the GREEN model [12]a specialized RRG metric that leverages large language model-based natural language understanding. To enhance the quality of generated reports, we employ direct preference optimization (DPO) [14] to align model outputs with expert-validated clinical accuracy. Comprehensive evaluations conducted on three extensive CT imagereport datasets demonstrate that our method produces radiology reports that contain clinically salient points and are computationally efficient, addressing critical limitations exhibited by existing automated RRG models."
        },
        {
            "title": "2.1 Problem Set-up",
            "content": "Given CT image and the corresponding question text Q, the RRG task aims to generate report ˆY that describes diagnostic findings by answering the given question Q. The perceiver approaches [8] do not consider the when compressing the image embeddings, which leads to suboptimal solutions. In our work, we introduce an intermediate tokenization layer, µ2Tokenizer, to effectively bridge the vision and language models. Figure 1 shows the pipeline of our model. Image Encoder and Text Tokenizer: We scale the CT image with height H, width , and depth D, and then divide it into frames each consisting of slices. i.e. RT KHW , = . we employ Vision Transformer (ViT3D) [1] as our image encoder, which first transforms and splits each frame Ii, [0, ) into frames and patches. The purpose of splitting the CT into several frames is to reduce the huge amount of computation caused by one-time input, and to avoid the information loss caused by direct downsampling and splitting. The image encoder then extracts local features for each patch and global representation of the CT image. Formally, this produces sequence of visual tokens: RT NvE, where Nv is the number of visual tokens, and is the embedding dimension. Simultaneously, we obtain text tokens RNqE after tokenizing the question with the text tokenizer, where Nq is the max length of the question. The µ2Tokenizer fuses the text tokens with visual tokens to create compact visual tokens RNvE using multi-scale multi-modal 4 Siyou Li et al. Fig. 2: The illustration of our proposed µ2Tokenizer. The improvement is applied to steps of Token Selection, Multi-scale Pooling, and the Positional Encoding. attention mechanism. This ensures that relevant image information is efficiently passed to the LLM while reducing computational overhead. Report Generation: The processed image embeddings are then integrated with text question to generate radiology report. We utilize M3D-LaMed-Phi3-4B [1] as the base LLM for report decoding. The LLM takes as input both the textual question and the µ2Tokenizer-processed visual tokens V, generating the radiology report ˆY accordingly. 2.2 µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer To improve the extraction efficiency for the CT image information, we propose the µ2Tokenizer module (as shown in Figure 2), which can process CT images with an arbitrary number of slices and leverage the pre-trained model for efficient alignment training. This module is built upon the Linear Video Tokenizer (LinVT) [5] that was originally introduced for the video understanding task. LinVT comprises two sub-modules: Spatio-temporal Visual Token Refiner (SVR) and Text-conditioned Token Aggregation (TTA). These modules adhere to the linearity principle, meaning that the output of each module is linear combination of part of its input, thereby preserving the visual-language alignment learned in the image-LLM. Relative Positional Encoding(RPE). The LinVT [5] uses absolute learnable positional embeddings added along the frame and token dimensions. For the jth visual token Vi,j in ith frame, two absolute positional embeddings (Pf (i) and Pt(j)) are added to the Vi,j. Such an approach is not effective in capturing local relationships that are particularly useful in 3D volumes where local patterns matter. Instead, we use relative positional encoding so that the model can better capture local relationships regardless of the absolute position. The relative positional encoding is integrated within the attention mechanism[16]. When computing the attention score between token and token j, we add learned posiµ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 5 Fig. 3: Overview of training process with Direct Preference Optimization (DPO). QiK tional embeddings based on their relative position: Aij = +Pr(ij). When used with multi-head attention, each head has its unique positional embeddings. Differentiable Token Selection(DTS). The LinVT [5] uses hard top-k selection which results in information loss when visual tokens are not selected. From training perspective, small also leads to slow optimization as the error can not be effectively backpropagated back to non-selected visual tokens. To solve this limitation, we replace the hard selection with fully differentiable soft selection mechanism. For each of the selections, it computes weight for all tokens and uses the weighted sum to produce soft top token. This not only mitigates the information loss but also makes the selection process fully differentiable and improves gradient flow. The top-k soft tokens were calculated globally, taking into account visual tokens in all frames. Formally, for each of the soft tokens Vf lat) where (r) we first compute an attention score α(r) = Sof tmax(W (r) RE1, Vf lat RT NvE, = 1, . . . , k. The soft token (r) top is calculated as the weighted sum of all visual tokens: (r) top = (cid:80)T Nv Dynamic Multi-scale Pooling (DMTP). The LinVT uses fixed pooling kernel sizes that treat individual kernels equally. We improved it to dynamic pooling, which allows the network to learn how to weight and choose the appropriate pooling. This is more effective alternative to fixed pooling. We adapt the dynamic multi-scale pooling to weight the multiple pooled outputs dynamically. To implement this, we first apply an average pooling on different kernel sizes [1, 2, 4]: ys = AvgPool(Vtop, kernel = s) and then compute dynamic weights ws via small MLP g() over the mean of the pooled outputs: exp(g(ys)) ws = sS exp(g(ys)) . The weight ws is multiplied by ys to create weighted pooling outputs. Then, the final pooled representation is created by concatenating these weighted outputs. This allows the network to adapt the pooling operation based on the input distribution. Vf lat(i). i=1 α(r) (cid:80)"
        },
        {
            "title": "2.3 Direct Preference Optimization with GREEN-Score",
            "content": "Our training consists of two stages. First, we perform supervised fine-tuning (SFT) using the CT-Reports dataset. Second, we adopted the Direct Preference Optimization(DPO) [14] method (as shown in Figure 3). In particular, we 6 Siyou Li et al. optimize our model on GREEN Score [12], which is arguably the most effective method to evaluate medical reports. GREEN [12] effectively identifies significant discrepancies between the reference and generated reports, providing detailed score from 0 to 1 for quantitative analysis and summary for qualitative analysis. This interpretable evaluation helps improve the quality of automated radiology reporting. To obtain the preference dataset, we use the trained SFT model to generate large number of medical reports on the existing dataset, and then these medical reports are scored by GREEN against the ground truth. Finally, the scored reports are used in DPO [14] training, to guide the model generating preferred reports that have the highest GREEN score. As result, the reports generated by our model are more accurate and semantically closer to human experts. Formally, the model is trained on the following DPO training objective: LDP O(πθ; πref ) = E(V,x,yw,yl)DDP [log σ(β log πθ(ywx) πref (ywx) β πθ(ylx) πref (ylx) )] with πθ is the policy model, πref is reference model, σ is sigmoid function, β (0.1, 0.5), yw represent good responses and yl represent bad responses."
        },
        {
            "title": "2.4 Prompt Engineering",
            "content": "Fig. 4: The pipeline of our proposed CT Report Reasoning Synthesis In our workflow, we applied three prompt-engineering techniquesCT Report Rewriting, CT Report Reasoning Synthesis, and CT Report Translationwith µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 7 particular emphasis on CT Report Reasoning Synthesis. The concrete process and involved prompts are listed in Appendix A. Our CT Report Reasoning Synthesis pipeline converts each CT Report and its free-text radiology report into rich supervisory package for multimodal learning by sequentially prompting single large-language model in five roles: First, the question-generation stage reads the full report (findings and impression) and asks the LLM to propose diverse collection of natural-language questions that radiologist, trainee or downstream AI system might reasonably ask. Prompt constraints force coverage across lesion attributes, anatomical localisation, diagnostic certainty, and suggested follow-up, giving each study rich inquiry space. Second, each question is paired with the original report and resubmitted to the LLM under think-step-by-step instruction. The model must clearly reason out, citing exact report fragments or well-established imaging priors, before providing concise answer. The resulting tuplesquestion, answer, and raw reasoningcapture both knowledge and justification in single pass. Third, an automatic quality gate re-examines every tuple. second LLM pass checks factual consistency between answer and report, heuristics reject nonEnglish or vacuous chains-of-thought, and domain-specific rules eliminate pathophysiologic contradictions (for example, claiming pneumothorax is improved when it is first detected). Only tuples that survive all three filters remain. Fourth, accepted reasoning traces are refined: the LLM compresses them into short, evidence-linked paragraphs whose citations reference specific report lines. Redundancy is pruned, hedging language is toned down and, where appropriate, probabilistic qualifiers are inserted to reflect clinical uncertainty in calibrated fashion. Finally, the pipeline fuses all refined traces into single, structured reportthinking narrative. The LLM merges overlapping rationale, orders arguments anatomically and separates them into Findings Rationale, Impression Rationale and Follow-up Rationale sections. The finished datapoint therefore contains CT volume, its VQA pairs (with answers) and coherent explanation grounding every key statement, enabling scalable training of multimodal models that can answer questions and justify their answers with radiologic evidence."
        },
        {
            "title": "3 Experiments",
            "content": "Datasets. AMOS-MM 2024 [9] consists of 2,088 chest, abdomen, and pelvis medical images and corresponding manually annotated text reports. The medical images are CT scans with spatial resolution from 256256 to 10241024 and slice thickness from 1mm to 5mm. CT-Rate [7] consists of 50,188 CT images of 21,340 patients and corresponding text reports. The scanning resolution and slice numbers range from 256256 to 10241024 and 46 to 1,277, respectively. AbdomenAltas 3.0 [3] is created by using RadGPT to generate reports for 17 public datasets, and through annotation, review, and refinement by 12 radiolo8 Siyou Li et al. gists to ensure the reports accuracy. It comprises over 1.8 million text tokens and 2.7 million images from 9,262 CT scans. Furthermore, we expanded the dataset based on manually annotated medical reports using GPT-4o mini. This expansion included report rewriting and the generation of clinically relevant question-answer pairs, enriching the datasets diversity and comprehensiveness for improved model training and evaluation. Implementation Details. We preprocess the 3D CT images using MinMax Normalization, then resizing and cropping to standard dimension of 8 32 256 256, and random noise added. Our 3D vision encoder employs 3D ViT from M3D-CLIP [1], and base-LLM is Llama-3.2-1B-Instruct [18]. All models are trained by AdamW optimizer with warm-up and cosine decay and use the bf16 mixed-precision training strategy enabled by DeepSpeed. Training is conducted in parallel across 4 NVIDIA A40 GPUs (48 GB VRAM each). For the µ2Tokenizer layer, we use four Spatio Temporal Attention Layers and four Text Condition Token Attention layers each consisting of eight attention heads and set = 1024 for top soft token selection. For scale-specific learnable queries we use 1024 queries with hidden size of 768. Baselines and Evaluation Metrics. We compare our model with several efficient and high-performing MLLMs, including LaMed-Phi-3-4B [1], LaMedLlama-2-7B [1], CT-CHAT(Llama-3.1-8B)[6], RadGPT-N [3], and RadFM-14B [19], which excel at capturing linguistic patterns and generating coherent text across various domains. We also include the comparison of our model µ2LLM-1B (SFT) with only SFT, and our model µ2LLM-1B (SFT&DPO) with both SFT and DPO. Given the complexity of evaluating content accuracy between generated reports and human references, we employ both traditional and LLM-based metrics. Traditional metrics include BLEU [13], ROUGE [10], METEOR [2], and BERT-Score[20], which quantify text similarity through n-gram overlap and variations, although they have limited semantic understanding. LLM-based metrics, i.e. the GREEN score [12], utilize models with strong semantic comprehension to evaluate the alignment between generated reports and human references. This metric assesses matching content and errors, offering more comprehensive report quality measure. Results Analysis. Our model demonstrates superior performance compared to baseline models across multiple datasets. Notably, despite its significantly smaller scale (1B parameters, only 14% of comparable models ranging from 7B to 14B), our model consistently outperforms these larger counterparts. Table 1 presents comparative evaluation across different datasets. Our model achieves state-of-the-art results, outperforming existing approaches. For instance, on the CT-Rate dataset, our model attains ROUGE-1 = 0.517, METEOR = 0.330, BERTScore = 0.879, and GREEN = 0.384, significantly surpassing CT-CHAT8B. These results underscore the effectiveness of our approach, particularly after SFT and DPO. The integration of the GREEN Score-based dataset selection for DPO fine-tuning further enhances model performance, leading to more accurate and clinically relevant report generation. Specifically, the GREEN Score evaluation indicates model capability improvement of 20% with DPO in GREEN µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 9 Table 1: Performance Comparison Across Different Datasets Datasets Models ROUGE-1 METEOR BERTScore GREEN AbdomenAltas CT-Rate AMOS-MM LaMed-Phi-3-4B LaMed-Llama-2-7B RadFM-14B RadGPT-N µ2LLM-1B(SFT) µ2LLM-1B(SFT&DPO) LaMed-Phi-3-4B LaMed-Llama-2-7B RadFM-14B CT-CHAT-8B µ2LLM-1B(SFT) µ2LLM-1B(SFT&DPO) LaMed-Phi-3-4B LaMed-Llama-2-7B RadFM-14B µ2LLM-1B(SFT) µ2LLM-1B(SFT&DPO) 0.136 0.139 0.037 0.247 0.529 0.567 0.130 0.103 0.054 0.294 0.517 0.539 0.126 0.163 0.046 0.421 0.459 0.058 0.060 0.013 0.112 0.295 0.319 0.050 0.048 0.017 0.221 0.330 0.359 0.047 0.065 0.015 0.249 0. 0.807 0.810 0.794 - 0.891 0.895 0.814 0.815 0.812 0.815 0.879 0.890 0.821 0.823 0.812 0.881 0.881 0.011 0.009 0.000 - 0.281 0.346 0.002 0.001 0.014 0.113 0.384 0.429 0.009 0.009 0.001 0.339 0. Table 2: Performance Comparison component effectiveness of µ2Tokenizer BLEU ROUGE-1 METEOR BERTScore GREEN Model 0.190 Baseline 0.281 +RPE 0.271 +DTS 0.254 +DMTP µ2LLM-1B(SFT) 0.279 µ2LLM-1B(SFT&DPO) 0.336 0.405 0.421 0.411 0.401 0.421 0.459 0.210 0.236 0.240 0.220 0.249 0.876 0.864 0.880 0.888 0.874 0.881 0.881 0.204 0.277 0.299 0.233 0.339 0. Score, with the most substantial gain observed on the AMOS-MM dataset, where the GREEN Score increased from 0.33 to 0.40. To further assess the impact of individual components, we conducted ablation experiments  (Table 2)  . These experiments confirm that each enhancement contributes meaningfully to model performance. When training parameters remain consistent, incorporating DTS yields the most substantial performance boost, improving the GREEN Score by up to 0.2 points. This finding highlights the effectiveness of DTS in optimizing token representation and selection, leading to more accurate and clinically meaningful text generation. Figure 5 illustrates an example report produced by µ2LLM-1B. On the left, 3D heat map visualizes the Question-CT cross-attention scores, indicating the regions of the CT scan most relevant to the models diagnostic reasoning. The center and right images depict the original CT scan and corresponding problem 10 Siyou Li et al. Fig. 5: An example of the generated report from our µ2LLM-1B (SFT&DPO). statement, where the model is tasked with identifying and diagnosing abnormalities in the abdominal region. The rightmost section displays the generated radiology report, which provides structured interpretation of the CT findings. The report includes descriptions of liver parenchyma density, gallbladder morphology, renal pelvis dilation, and other critical observations. The generated text demonstrates clinical coherence and diagnostic accuracy, aligning with standard radiology interpretations. This visualization highlights the models capability to focus on diagnostically relevant regions and produce detailed, structured radiology reports, supporting its potential use in automated medical imaging analysis."
        },
        {
            "title": "4 Conclusion",
            "content": "In this study, we introduced µ2Tokenizer, multi-scale, multi-modal middleware, and DPO optimization framework for radiology report generation. By integrating ViT3D with an LLM, our approach effectively combines visual and textual information, enabling accurate and coherent medical reports. To refine the model for RRG tasks, we utilized the GREEN-Model and SFT to curate high-quality datasets for DPO fine-tuning, improving alignment with clinical standards. Despite limited training data, our model outperformed larger baselines, particularly in GREEN Score, demonstrating the effectiveness of multimodal fusion and optimization techniques in automated radiology reporting. By jointly supplying questions with large-scale LLM, answers and anatomically ordered chains-of-thought, the framework lowers annotation costs and paves the way for trustworthy, clinically grounded VQA models in medical imaging. These results highlight the importance of structured fine-tuning in enhancing diagnostic accuracy. Moving forward, our approach could be further extended to other medical imaging modalities and clinical applications."
        },
        {
            "title": "References",
            "content": "1. Bai, F., Du, Y., Huang, T., Meng, M.Q.H., Zhao, B.: M3d: Advancing 3d medical image analysis with multi-modal large language models (2024) µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 11 2. Banerjee, S., Lavie, A.: METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In: Goldstein, J., Lavie, A., Lin, C.Y., Voss, C. (eds.) Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 6572. Association for Computational Linguistics, Ann Arbor, Michigan (Jun 2005) 3. Bassi, P.R., Yavuz, M.C., Wang, K., Chen, X., Li, W., Decherchi, S., Cavalli, A., Yang, Y., Yuille, A., Zhou, Z.: Radgpt: Constructing 3d image-text tumor datasets. arXiv preprint arXiv:2501.04678 (2025) 4. Everlight: Radiology unlocked: The global radiologist report 2025 5. Gao, L., Zhong, Y., Zeng, Y., Tan, H., Li, D., Zhao, Z.: Linvt: Empower your imagelevel large language model to understand videos. arXiv preprint arXiv:2412.05185 (2024) 6. Hamamci, I.E., Er, S., Almas, F., Simsek, A.G., Esirgun, S.N., Dogan, I., Dasdelen, M.F., Durugol, O.F., Wittmann, B., Amiranashvili, T., Simsar, E., Simsar, M., Erdemir, E.B., Alanbay, A., Sekuboyina, A., Lafci, B., Bluethgen, C., Ozdemir, M.K., Menze, B.: Developing generalist foundation models from multimodal dataset for 3d computed tomography (2024) 7. Hamamci, I.E., Er, S., Almas, F., Simsek, A.G., Esirgun, S.N., Dogan, I., Dasdelen, M.F., Wittmann, B., Simsar, E., Simsar, M., et al.: foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities. arXiv preprint arXiv:2403.17834 (2024) 8. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., Carreira, J.: Perceiver: General perception with iterative attention. In: International conference on machine learning. pp. 46514664. PMLR (2021) 9. Ji, Y., Bai, H., Ge, C., Yang, J., Zhu, Y., Zhang, R., Li, Z., Zhanng, L., Ma, W., Wan, X., et al.: Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in Neural Information Processing Systems 35, 3672236732 (2022) 10. Lin, C.Y.: ROUGE: package for automatic evaluation of summaries. In: Text Summarization Branches Out. pp. 7481. Association for Computational Linguistics, Barcelona, Spain (Jul 2004) 11. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023) 12. Ostmeier, S., Xu, J., Chen, Z., Varma, M., Blankemeier, L., Bluethgen, C., Md, A.E.M., Moseley, M., Langlotz, C., Chaudhari, A.S., Delbrouck, J.B.: GREEN: Generative radiology report evaluation and error notation. In: Al-Onaizan, Y., Bansal, M., Chen, Y.N. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2024. pp. 374390. Association for Computational Linguistics, Miami, Florida, USA (Nov 2024). https://doi.org/10.18653/v1/2024. findings-emnlp. 13. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: method for automatic evaluation of machine translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.) Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311318. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA (Jul 2002). https://doi.org/10.3115/1073083.1073135 14. Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., Finn, C.: Direct preference optimization: Your language model is secretly reward model. In: Thirty-seventh Conference on Neural Information Processing Systems (2023) 15. Rosenkrantz, A.B.: Differences in perceptions among radiologists, referring physicians, and patients regarding language for incidental findings reporting 208(1), 140143. https://doi.org/10.2214/AJR.16.16633, publisher: American Roentgen Ray Society 12 Siyou Li et al. 16. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position representations (2018) 17. Thirunavukarasu, A.J., Ting, D.S.J., Elangovan, K., Gutierrez, L., Tan, T.F., Ting, D.S.W.: Large language models in medicine 29, 19301940. https://doi.org/10. 1038/s41591-023-0244818. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models (2023) 19. Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Towards generalist foundation model for radiology (2023) 20. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: Bertscore: Evaluating text generation with BERT. CoRR abs/1904.09675 (2019) 21. Zhao, G., Zhao, Z., Gong, W., Li, F.: Radiology report generation with medical knowledge and multilevel image-report alignment: new method and its verification. Artificial Intelligence in Medicine 146, 102714 (2023) µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer"
        },
        {
            "title": "A Appendix",
            "content": "A.1 CT Report Rewriting: Prompt You are an expert radiologists. And your task is to paraphrase given radiology report. You need to: 1. Take the following 3 examples for style of writing. 2. You MUST NOT change any meaning of the original report, nor add or remove any information, not event correction. 3. Give out the paraphrased report directly, without any other content. 4. In English only. Here are some examples of CT reports: {SOME EXAMPLES OF DATASETS} The original report: {} A.2 CT Report Reasoning Synthesis: Prompt 1. Questions generation Use the following prompt to call LLM to generate question list, and then use the regular expression r\".*?d. ?([^n]*)\" to extract the questions separately: Here is medical radiology report for CT image. {report} Imagine you are assessing student who is looking at CT image, you are going to ask list of questions. Dont mention the report, just list out as the form of questions, and questions only, in sequenced list. 2. Answer&Thinking generation Use the following prompt to call LLM to generate the thinking process and answers, and extract the thought content with the regular expression r\"Thinking: ?([^n]*)\" and extract the answer content with the regular expression r\"Answer: ?([^n]*)\": You are radiology medicine expert. Your task is to answer the following radiology medicine question, using the patients medical record report provided below. When writing your thought process, imagine you are directly reviewing the patients radiology images (do not mention the report), and describe your logical reasoning step by step as an expert would. Then, provide your final, correct answer to the question. Your response will be used to guide and improve the training of multimodal large language model for radiology medicine images. And here is the radiology report that you can see: {report} Now we have question: {question} 14 Siyou Li et al. Please consider and answer the question in the following format: Thinking: <thought process> Answer: <answer to the question> 3. Filter Use the following prompt to call LLM to filter out incorrect questions or answers that were generated in the previous step: You are an expert in radiology. Now you are reviewing some questions and answers made by another expert. You need to determine if the question is proper for radiology exam, and the answer is correct. If the question is proper for radiology exam, and the answer is correct, return \"Yes\". If the question is not proper for radiology exam, or the answer is incorrect, return \"No\". Do not return anything else. The Report: {report} Question: {question} Answer: {answer} 4. Refine Thinking Use the following prompt to call LLM to rewrite the thinking data generated in the previous step to make it more in line with VQA habits: Help me edit the narrative below: - If the narrative refers to report, you change it as if you see it from the radiology image - Edit only the places mentioned above, leave all other text the same - Do not add/remove/change any other information - Directly output the result text **The narrative:** {report} 5. Report Thinking Synthesis Use the following prompt to call LLM to generate the data that generates thoughts in the report process. Thinking_before is spliced using all the questions, thinking, and answers in the QA dataset: ou are radiology medicine expert. Now you are looking at radiology image. Here is your self talk when viewing the image: {thinking_before} Please paraphrase the self talk text and output it as **thinking progress**. Remember: - Do not add/remove/alter any information - Mind the coherence and fluence of output - Deductions are prefered - Directly output the result text Your output: µ2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer 15 A.3 CT Report Translation: Prompt This is an {source_lang} to {target_lang} translation, please provide the {target_lang} translation for this text. Do not provide any explanations or text apart from the translation. {source_lang}: {source_input}"
        }
    ],
    "affiliations": [
        "Guangdong University of Technology, Guangdong, China",
        "Meta Inc. US",
        "Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, UK",
        "School of Electronic Engineering and Computer Science, Queen Mary University of London, London, UK",
        "School of Engineering, College of Engineering and Physical Sciences, University of Birmingham, Birmingham, UK",
        "William Harvey Research Institute, NIHR Barts Biomedical Research Centre, Queen Mary University London, London, UK"
    ]
}