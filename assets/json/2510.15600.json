{
    "paper_title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
    "authors": [
        "Haoran Sun",
        "Yankai Jiang",
        "Zhenyu Tang",
        "Yaning Pan",
        "Shuang Gu",
        "Zekai Lin",
        "Lilong Wang",
        "Wenjie Lou",
        "Lei Liu",
        "Lei Bai",
        "Xiaosong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 0 0 6 5 1 . 0 1 5 2 : r UNLEASHING SCIENTIFIC REASONING FOR BIOEXPERIMENTAL PROTOCOL GENERATION VIA STRUCTURED COMPONENT-BASED REWARD MECHANISM Haoran Sun1,2, Yankai Jiang1, Zhenyu Tang1,3, Yaning Pan1,2, Shuang Gu1,3, Zekai Lin2, Lilong Wang1, Wenjie Lou1, Lei Liu2, Lei Bai1, Xiaosong Wang1 1Shanghai Artificial Intelligence Laboratory 2Fudan University 3Shanghai Jiao Tong University {jiangyankai,wangxiaosong}@pjlab.org.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the Sketch-and-Fill paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly."
        },
        {
            "title": "INTRODUCTION",
            "content": "The planning and execution of scientific experiments hinge on protocols that serve not merely as textual guidelines but as operational blueprints detailing procedures, materials, and logical dependencies (Freedman et al., 2015; Goodman et al., 2016). well-structured protocol ensures that experiments are reproducible, safe, and scientifically valid (of Sciences et al., 2019), which is essential for cumulative progress in the life sciences. While recent advances in large language models (LLMs) have greatly expanded their role in biomedical research, ranging from literature-based discovery to domain-specific question answering (Devlin et al., 2019; Brown et al., 2020; Lee et al., 2020; Beltagy et al., 2019), their ability to produce reliable experimental protocols remains underdeveloped. Existing datasets and benchmarks are typically confined to comprehension tasks (Liu et al., 2025; Jiang et al., 2024), thereby neglecting the dimensions of planning and problem solving required to support reproducibility and practical execution. As result, researchers often find that models offer fragmented recommendations on experimental procedures but fall short of producing concise, logically ordered protocols that can be directly implemented in laboratory workflows. Currently, proprietary models (Achiam et al., 2023; Comanici et al., 2025; Jaech et al., 2024; OpenAI, 2025) such as GPT-5 demonstrate strong capabilities in procedural reasoning, while Corresponding authors. 1 domain-specific scientific systems complement LLMs with curated knowledge bases or tool-assisted pipelines (Huang et al., 2025; Jin et al., 2025). Despite these advances, the generated protocols often contain unordered steps, redundant operations, factual inconsistencies, or hallucinated actions, undermining both reproducibility and scientific credibility. Moreover, evaluation remains central bottleneck: metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2019) capture only superficial lexical overlap, failing to reflect whether the generated action sequence is logically consistent, semantically faithful, and practically executable. While LLM-as-a-judge frameworks align better with human preferences, they introduce prohibitive costs to reinforcement learning (RL) pipelines, limiting scalability (Liu et al., 2023; Zheng et al., 2023). Existing reward designs further neglect the structured and verifiable nature of protocols, producing outputs that are linguistically fluent but experimentally unreliable (Zeng et al., 2023). These limitations highlight the need for an approach that integrates structured and efficient evaluation. In this work, we introduce comprehensive framework that advances both data and modeling for protocol generation. At its core lies the Sketch-and-Fill paradigm, which formulates protocol generation as structured reasoning process: each step is decomposed into essential components and expressed in natural language with explicit correspondence, ensuring logical coherence and experimental verifiability. To support this paradigm, we curate SciRecipe, large-scale dataset of over 12K protocols spanning diverse domains and covering both Protocol-Comprehension and ProblemSolving tasks. Building on this foundation, we propose the Structured COmponent-based REward (SCORE) mechanism, the central innovation of our framework. SCORE provides structured reward and evaluation scheme that captures three complementary dimensions: step granularity (controlling scale and avoiding redundancy), action ordering (ensuring logically consistent sequences for reproducibility), and semantic fidelity (verifying alignment between predicted and reference actions, objects, and parameters). By jointly modeling these dimensions, SCORE moves beyond conventional text-based metrics to directly assess whether protocols are executable, interpretable, and scientifically sound. It serves both as an effective RL training signal and as reliable metric for evaluation. Based on these components, we develop Thoth, protocol-generation model trained to combine structured reasoning with SCORE-guided evaluation. Extensive experiments demonstrate that Thoth outperforms SOTA models, particularly in step alignment, logical sequencing, and semantic accuracy. Just as importantly, the protocols it generates are concise and reproducible, which are qualities often absent from existing systems. Our main contributions are as follows: We curate SciRecipe, large-scale, multi-task dataset covering over 27 biological subfields, designed to serve as foundation for both training and evaluating on protocol generation. We introduce the Sketch-and-Fill paradigm, reasoning framework that aligns with the logic of experimental design by converting open-ended queries into verifiable protocols. We propose the SCORE mechanism, structured reward and evaluation framework that jointly measures step granularity, order consistency, and semantic fidelity, ensuring that protocols are not only linguistically fluent but also experimentally executable. We develop Thoth, protocol-generation model with strong reasoning abilities, which achieves SOTA performance across protocol-specific and broader scientific benchmarks."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "LLMs in the Life Science Recent progress in life science LLMs has shifted from general-purpose systems to domain-specific models. BioBERT and SciBERT (Lee et al., 2020; Beltagy et al., 2019), pretrained on large-scale biomedical corpora, substantially improved tasks such as information extraction and question answering (QA). Autoregressive models like BioGPT (Luo et al., 2022) further advanced generative abilities, achieving strong and consistent results on PubMedQA (Jin et al., 2019). These works collectively demonstrate the effectiveness of domain-specific pretraining and task adaptation, yet remain limited to knowledge-based tasks (e.g., QA, summarization) and cannot produce executable protocols. In contrast, proprietary models (OpenAI, 2025; Comanici et al., 2025; Jaech et al., 2024) such as GPT-5 leverage massive corpora, parameter scales, and reasoning mechanisms to generate preliminary multi-step protocols. More recent efforts (Huang et al., 2025; Jin et al., 2025; Zhang et al., 2025c; Gao et al., 2025), including Biomni and STELLA, integrate external knowledge and tools to support task-oriented solutions. Nonetheless, current models still struggle with redundancy, misordered steps, and hallucinations, thereby limiting their practical usability. To 2 address this gap, we propose biologically oriented model trained on real protocols, designed to generate rigorous experimental procedures through structured scientific reasoning paradigm. Figure 1: (a) Thoths performance advantages over other models, shown through qualitative evaluation (left) and quantitative evaluation (right). (b) The construction pipeline of the SciRecipe dataset. Reward Design and Evaluation for Open-Ended Generation Designing effective evaluation remains key challenge in open-ended generation. Unlike short-text tasks with clear ground truth, long-text generation such as experimental protocols lacks definitive answers and must therefore be assessed along multiple dimensions, including coherence, factuality, and executability (Becker et al., 2024; Que et al., 2024). Metrics such as ROUGE, BLEU, and BERTScore (Lin, 2004; Papineni et al., 2002; Zhang et al., 2019) capture surface similarity but fail to fully reflect reasoning depth and often diverge from human judgment. The LLM-as-a-judge paradigm provides closer alignment with human preferences (Gu et al., 2024; Zheng et al., 2023), yet it usually introduces prohibitive computational costs in RL training (Shao et al., 2024; Schulman et al., 2017; Ahmadian et al., 2024; Hu, 2025). Alternative approaches, including smaller evaluator models or reward mechanisms such as PrefBERT, Direct Reasoning Optimization, and LoVeC (Li et al., 2025; Xu et al., 2025; Zhang et al., 2025a), can improve efficiency or reliability but still require repeated model calls. To address these issues, we propose rule-based evaluation that extracts key protocol elements through structured reasoning, thereby offering direct and verifiable reward signals with efficiency and interpretability."
        },
        {
            "title": "3 METHODS",
            "content": "This section outlines our approach to leveraging high-quality experimental protocol data and component-wise reward mechanism to enhance the models ability to generate experimental solutions. We first propose the Sketch-and-Fill reasoning paradigm, which aligns with experimental execution logic to guide the model in organizing experimental steps effectively. Building on this, we construct SciRecipe, dataset encompassing diverse experimental tasks and scenarios. Subsequently, we develop the SCORE mechanism to evaluate protocol quality across multiple dimensions, serving as reward signal to improve the models generalization in protocol generation tasks. Ultimately, by leveraging SciRecipe and multi-stage Knowledge-to-Action learning strategy, we train the Thoth model, which exhibits strong reasoning capabilities for protocol generation."
        },
        {
            "title": "3.1 SCIRECIPE DATASET",
            "content": "In life science research, protocols serve as essential documents to ensure experimental reproducibility and reliability, providing detailed records of materials, equipment, procedures, and critical notes. Recently, platforms such as Nature Protocols, Bio-protocol, and Protocols.io (Wikipedia contributors, 2025; Bio-protocol, LLC, 2025; Teytelman et al., 2016) have compiled vast number of standardized workflows, offering rich resources for the scientific community. From these sources, we 3 collected over 23K protocols spanning 27 subfields including neuroscience, molecular biology, and cancer biology. After cleaning and structural processing, approximately 12K high-quality data were retained as the foundation of our dataset (the preprocessing pipeline is detailed in the Appendix A.1). Building on this foundation, we introduce the SciRecipe dataset, designed to improve and evaluate LLMs in experimental protocol understanding and generation. SciRecipe comprises eight task types, grouped into two categories: Protocol-Comprehension Tasks (overview and specific), targeting global summarization and fine-grained analysis, and Problem-Solving Tasks (retrieval, planning, troubleshooting, constraint, scaling, and safety), simulating typical challenges encountered throughout experimental workflows. Together, these tasks form complementary understandingapplication loop. The construction pipeline of SciRecipe is illustrated in Figure 1 and further elaborated in the Appendix A.4. Notably, benchmarks focused on scientific experimental protocol generation are currently scarce (Liu et al., 2025). To address this gap, we introduce SciRecipe-Eval, built with similar pipeline, with dataset splits and difficulty levels detailed in the Appendix B. 3.2 SKETCH-AND-FILL REASONING PARADIGM To transform protocol generation tasks into an executable and evaluable form, we propose the Sketch-and-Fill reasoning paradigm. This paradigm is centered on structured three-stage output: <think> <key> <orc>, with an additional <note> section dedicated to laboratory safety precautions. The core idea is to organize outputs in the sequence of reasoning, structuring, and expression, thereby enabling the design of structured reward mechanism for RL. An overview of this paradigm is provided in Figure 2. Specifically, <think>, <key>, and <orc> denote the reasoning process, the extraction of key information, and the final natural language output, respectively. In <think>, the model decomposes sub-goals, identifies sequential dependencies, and justifies the necessity of the proposed experimental steps, ensuring that the protocol is grounded in scientific reasoning. The parsed steps are then organized through two-phase process of Sketch and Fill. In the Sketch phase, represented by <key>, the strategies from <think> are transformed into sequence of atomic, machine-readable steps. Each step is constrained to single JSON dictionary representing one action unit: {\"action\": verb, \"objects\": [...], \"parameters\": [...]}. This abstraction reformulates natural language steps into predicateobjectadverbial triplets, bridging free-form instructions with structured sequences. The subsequent Fill phase, represented by <orc>, expands these steps into fluent natural language instructions, ensuring readability and executability. Formally, <key> is defined as Y: yi = (ai, Oi, Pi) = (y1, . . . , ym) , (1) where ai is the experimental operation, Oi denotes the objects being acted upon, and Pi specifies the parameters (e.g., temperature, concentration). Consistency constraints, such as enforcing One-Action-Per-Step and maintaining uniform parameter application across objects, are applied to standardize the data format. In the Fill phase, represented by <orc>, the elements of <key> are rendered into human-readable natural language. strict one-to-one correspondence in step count and semantics is enforced, ensuring no information is added or omitted, with the focus solely on readability. Overall, the Sketch-and-Fill paradigm standardizes scientific protocol generation by grounding open-ended language output in an executable structural space. This not only supports stable RL training but also provides consistent foundation for automatic evaluation."
        },
        {
            "title": "3.3.1 OPTIMIZATION OBJECTIVE",
            "content": "In scientific research, experimental protocols are not merely narrative texts but highly structured action guidelines refined through long-term practice. qualified protocol must describe steps completely while conveying experimental logic: what to do (action), on what objects (objects), and under what conditions (parameters). Traditional text generation metrics (e.g., ROUGE, BLEU, BERTScore) often overlook such structure, rewarding lexical overlap even when action sequences Its optimization objective are disordered. To address this, we propose the SCORE mechanism. emphasizes both local alignment of protocol content and global logical consistency, defined as: max θ E(x,A,y)D Eyπθ(x,A) (cid:88) (cid:88)"
        },
        {
            "title": "Rlocal",
            "content": "(cid:0)yi, (cid:1) Rglobal (cid:0)y, y(cid:1) (2) i=1 j=1 4 Figure 2: Illustration of the SCORE mechanism and the Sketch-and-Fill reasoning paradigm. Three representative rollouts are subjected to concise analysis across step scale, order consistency, and semantic consistency, while detailed computations are provided in Section 3.3.2. which defines πθ as the policy model with parameters θ, as the context, as the action library, and as the ground-truth protocol. Rlocal and Rglobal denote reward functions at different granularities, combined by (details in Section 3.3.2). The core idea is to shift evaluation from superficial similarity to multidimensional alignment, including logical order and execution precision. Unlike metrics focusing only on text overlap or embeddings, SCORE directly measures the operability of generated protocols, providing scientific, interpretable, experiment-aligned optimization signals."
        },
        {
            "title": "3.3.2 REWARD DESIGN",
            "content": "The SCORE mechanism adopts progressive design (see Figure 2). First, gating stage ensures basic structural and consistency requirements. Then, fine-grained rewards for step scale and step semantics are combined into the final reward signal. This approach encourages the model to follow rational reasoning paradigm and enhances both coherence and executability of the generated protocols. Format Gate: The output must contain the four sections <think>, <key>, <orc>, and <note> in the correct order. Each step in <key> must follow the format Step x:{json}, explicitly specifying action, objects, and parameters, ensuring parseability for subsequent reward calculation. Consistency Gate: This gate verifies step-by-step correspondence between <key> and <orc>. Every action, object, and parameter in <key> must appear in <orc> with at least 95% coverage (see Appendix C.1 for details), ensuring the protocol is practical guide rather than hollow framework. Only protocols passing both gates are used for reward computation. Step Scale: The quality of protocol depends on proper granularity, as too few steps cause omissions while too many lead to redundancy. We design step scale reward that measures the gap between generated and gold step counts, with one atomic action per step guaranteed by the Sketch-and-Fill paradigm. Matching counts yield score of 1, and deviations are penalized by cosine decay that drops to zero beyond threshold. length penalty is also applied, and when the average word length per step exceeds limit, the score is proportionally reduced to discourage verbosity. Formally: (d) = (cid:26) cos(cid:0) πd 2M 0, (cid:1) , < M (cid:27) , g( L) = (cid:26) 1, L, (cid:27) L > , rscale = (Npred Ngold) g( L) (3) where Npred and Ngold are predicted and gold step counts, and = max(1, 0.6Ngold) is the deviation threshold. denotes the average text length per step. (d) penalizes step mismatch via cosine decay, while g( L) penalizes verbosity. Together, they yield the step scale reward rscale. Step Semantics The step semantics reward is central to SCORE, as it reflects the core execution logic of protocols. It consists of two components: Order Consistency and Semantic Consistency. a) Order Consistency, denoted as Order(), assesses whether the generated sequence of experimental actions matches the ground truth. We adopt Strict mode (see details in Appendix C.2), rewarding only if the predicted and ground-truth sequences are identical or mutually subsequences, and zero otherwise. This design mirrors laboratory reality since some steps may be repeated or omitted, but disordered sequence renders the protocol invalid regardless of textual similarity. b) Semantic Consistency is then calculated on the basis of action alignment, since actions function as the anchor of experimental steps (see Appendix C.3). In practice, researchers first decide what operation to perform (e.g., incubate, add), and only then specify the relevant objects and conditions. Without the action anchor, textual similarity cannot guarantee executability. Thus, we align predicted and gold actions step by step. For each aligned pair (i, j), the overlap of object sets is measured using intersection-over-union, with subword-based similarity used as compensation when objects differ but are semantically related. Parameters are compared only if object overlap 0.5, reflecting the principle that condition descriptions are meaningless for incorrect objects. Formally: Obj(i, j) = ˆOi ˆOi , Par(i, j) = 1, 0, K( ˆPi)K(P K( ˆPi)K(P ) ) , = = ˆPi = , ˆPi = otherwise (4) where ˆOi and ˆPi are the predicted object and parameter sets at step i, are the corresponding ground truth sets at step j, and K() extracts subword sets for reliable comparison. To better incorporate positional fidelity, we apply decay factor mij = max{0, 1 (i j/D)λ}, where is the number of ground truth steps. This reduces the score when predicted actions, though correct in type, appear far from their reference positions. The final score is: and rsemantics = Order(ˆa, a) + 1 (cid:88) (i,j)W mij (cid:0)Obj(i, j) +"
        },
        {
            "title": "2 Par(i, j)(cid:1)",
            "content": "(5) where is the set of aligned action pairs. Unlike the step scale (equation 6), we adopt an additive combination to avoid over-penalization. This allows partial credit when objects or parameters are imperfectly matched, as long as the action sequence remains reasonable. Such tolerance improves training stability and aligns with how real researchers judge protocols, where minor detail errors may be acceptable if the experimental logic is preserved. Finally, the overall SCORE is defined as: SCORE(y, y) = Iformat(y) Icons(y) rscale(y, y) rsemantics(y, y) (6) where Iformat and Icons are gating functions. This design ensures that only protocols satisfying structural, step scale, and semantic requirements obtain high rewards (Wang et al., 2025), thereby mitigating reward hacking and more faithfully simulating how protocols are evaluated in practice. SCORE also serves as multidimensional evaluation framework, with details in Appendix F."
        },
        {
            "title": "3.4 KNOWLEDGE-TO-ACTION LEARNING STRATEGY",
            "content": "Inspired by curriculum learning (Bengio et al., 2009; Wang et al., 2021), we propose three-stage Knowledge-to-Action Learning framework that progressively enables the transition from textual knowledge to protocol generation. The framework parallels human learning, progressing from knowledge accumulation to standardized operations and finally to exploratory optimization. In the first stage, pre-training, the model learns the semantic structure and operational logic of experimental language from large-scale protocol texts (see Appendix K.2 for further experiments). The second stage, supervised instruction tuning (SFT), is conducted on data following the Sketch-and-Fill paradigm, incorporating subtasks such as parameter filling, step ordering, and error correction (Liu et al., 2025). SFT both injects domain knowledge and provides cold start for RL, aligning outputs with the designated paradigm. The third stage applies RL with the GRPO algorithm (details in Appendix & K.1.1) (Guo et al., 2025). By removing entropy loss and reducing the KL penalty, we enhance exploration and avoid premature convergence. Combined with SCORE rewards, this stage improves generalizability and robustness, ensuring more reliable and executable protocol generation."
        },
        {
            "title": "4.1 EXPERIMENT SETTINGS",
            "content": "Benchmarks & Baselines To evaluate our model, we adopt two types of benchmarks. The first focuses on real-world protocol generation, represented by SciRecipe-Eval, which emphasizes linguistic quality and practical executability. The second covers broader scientific reasoning and question answering across domains, including Humanitys Last Exam (HLE), LAB-Bench, and PubMedQA (Phan et al., 2025; Laurent et al., 2024; Jin et al., 2019). (Additionally, BioProBench sub-tasks (Liu et al., 2025) are used as supplementary experiments for protocol comprehension.) We further compare against proprietary, open-source, reasoning, and scientific LLMs (see Appendix for details). Evaluation Metrics We designed metrics tailored to different task types. For natural language generation, BLEU, ROUGE, METEOR, and keyword matching (Liu et al., 2025) were used to assess surface-level semantic similarity (Papineni et al., 2002; Lin, 2004; Banerjee & Lavie, 2005; Grootendorst, 2020). Based on the SCORE mechanism, we introduced five executability metrics, namely Step-MATCH (Step-M), Order-LSC/S/Tau, and Semantic-Alignment (Semantic-A), to evaluate step scale, order, and semantic fidelity (see Appendix for details). For multiple-choice, classification, and ranking tasks, we employed Accuracy, F1, Exact Match, and Kendalls Tau (Abdi, 2007). Implementation Details We used GPT-5 Chat (OpenAI, 2025) (temperature 0.6) to construct SciRecipe and Gemini 2.5 Flash (Comanici et al., 2025) (temperature 0.2) for validation. In the SCORE mechanism, hyperparameters = 30 and λ = 1.5 were determined through iterative experiments. For training Thoth, we adopted Qwen3-8B as the base model (Yang et al., 2025). Pretraining and SFT used LoRA fine-tuning via LLaMA-Factory (Zheng et al., 2024), while RL fullparameter tuning was conducted with the VeRL framework (Sheng et al., 2025). All experiments ran on eight Nvidia H100 GPUs. Further hyperparameters and data ratios are given in Appendix G."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Table 1: Main results on SciRecipe-Eval. Metrics left of the dashed line evaluate executability, those on the right measure lexical similarity. Bold denotes the best score (see Appendix J.2 for details)."
        },
        {
            "title": "Methods",
            "content": "Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI-o1 OpenAI-o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth 40.04 27.79 36.30 39.35 41.32 36.35 35.80 33.33 40.34 36.40 35.68 41.72 32.86 35.92 36.99 36.07 37.40 36.73 34.74 35.40 24.37 44.28 28.89 46. 73.27 58.12 73.21 71.97 71.70 70.61 72.68 73.29 72.88 70.82 72.07 73.97 69.97 69.46 71.83 71.38 73.27 72.08 73.40 70.38 53.55 74.68 63.51 75.34 24.00 11.35 21.17 20.83 21.80 20.00 21.83 23.67 21.83 21.00 20.03 21.44 17.67 18.03 20. 20.37 21.92 20.25 18.53 15.38 13.67 25.33 11.17 25.50 70.33 53.55 65.67 70.00 71.93 70.33 70.17 70.00 71.33 69.17 69.12 70.54 64.17 67.78 69.83 69.12 69.73 65.34 67.45 65.05 50.50 70.83 58.67 73. 44.00 18.79 25.00 35.83 34.59 32.33 32.00 47.50 47.50 42.17 37.73 41.71 27.83 39.23 40.00 32.89 39.25 34.66 35.39 24.08 28.83 52.67 24.33 53.00 38.95 21.31 29.57 34.24 34.69 33.19 31.37 38.16 30.81 29.55 32.48 38.18 30.72 36.42 33. 39.80 34.72 37.21 35.68 28.62 14.52 43.32 16.66 43.62 48.42 32.96 42.04 44.27 44.42 42.91 44.16 46.88 48.02 46.06 44.37 48.49 43.44 44.86 44.49 45.86 46.04 46.17 46.29 43.08 24.74 49.23 32.31 50. 44.66 32.55 41.95 40.97 40.36 39.26 45.59 38.71 43.82 43.44 44.30 45.08 42.90 42.17 42.99 38.19 46.21 40.18 43.82 44.33 23.95 46.41 34.72 47.39 52.05 39.17 47.87 49.40 50.00 48.07 48.58 48.74 51.98 49.73 47.89 52.33 49.92 48.09 49. 49.83 48.59 51.81 50.13 50.26 27.69 53.13 38.63 54.13 48.41 32.84 42.53 45.21 45.65 43.67 44.69 46.70 47.61 45.37 44.85 48.16 42.16 44.66 45.53 44.83 46.35 44.94 45.05 41.84 29.09 51.10 34.32 52. Protocol Generation We evaluated protocol generation across two dimensions: executability, which emphasizes consistency in objects and step order, and lexical similarity, which reflects surface-level matching. As shown in Table 1, Thoth achieves SOTA results across all metrics. Compared to baselines, Thoth and Thoth-mini improve average performance by 17.78% and 22.01%, underscoring the effectiveness of our approach. Thoth also surpasses much larger proprietary models, outperforming ChatGPT-4o by 3.69% on average. Against the strongest open-source model, 7 DeepSeek-V3, Thoth achieves gains of 4.88%, 4.06%, and 11.29% on Semantic-Alignment, OrderS, and Step-MATCH, respectively, showing clear advantages in step alignment, logical order, and action fidelity. General-purpose SOTA models show notable capability, likely due to large-scale pre-training, but mainly achieve superficial similarity and lack real-world executability. Reasoningoriented models also perform poorly, often producing overly complex outputs unsuited for laboratory workflows (see Appendix J.1 for qualitative cases). Overall, Thoth demonstrates consistent advantages across metrics, bridging knowledge-based text generation with executable protocols. Fundamental Scientific Tasks Beyond protocol generation, we evaluated Thoth on three outof-domain biomedical benchmarks against recent scientific LLMs (Intern-S1 and SciDFM), with results shown in Figure 3. Thoth consistently outperforms models fine-tuned on biomedical corpora, exceeding Intern-S1 and Intern-S1-mini by 7.09% and 2.22% on average, and notably both Thoth and Intern-S1-mini share Qwen3-8B as their base model. As illustrated in Figure 3, Thoth also shows clear improvements over its baseline models (slash shading), achieving an average improvement of 10.87%, while Thoth-mini improves by 7.43%. The performance gap between Thoth and Thoth-mini further confirms the benefits of our approach. These results demonstrate that knowledge and reasoning skills acquired from scientific protocols generalize effectively to broader biomedical tasks. Additional experiments on protocol comprehension are provided in Appendix J.3."
        },
        {
            "title": "4.3 ANALYSES",
            "content": "SciRecipe To assess the impact of different data components, we conducted ablation studies on SciRecipe  (Table 2)  . Models trained only on QA or generation QA showed weak results, with BLEU-AVG below 40%. Adding Protocol-Comprehension tasks markedly improved executability, with Order-LCS reaching 74.50%. Joint training on both task types further enhanced alignment, with Step-M rising to 52.33%. The best overall performance was obtained when combining QA with scientific review, achieving 46.60% on Semantic-A and 49.45% on AVG. These findings highlight that diverse task coverage and systematic quality checks enhance the reliability of protocol generation. Figure 3: Results on other scientific benchmarks. Slash shading denotes baseline of Thoth models. Table 2: Ablation studies on training set. : trained only on SciRecipe Protocol-Comprehension tasks; : trained on both task types. QA and SciCheck denote inclusion of basic protocol QA and scientific review of SciRecipe, respectively. : trained only on protocol generation QA."
        },
        {
            "title": "SciRecipe QA SciCheck",
            "content": "Semantic-A Order-LCS Order-S Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 42.34 44.19 44.51 44.54 46.60 74.50 74.85 74.67 75.14 75.34 24.33 23.83 25.00 23.50 25.50 50.50 43.67 52.33 47.00 53. 23.88 23.71 41.33 42.42 42.74 41.79 43.62 31.50 32.08 47.34 47.59 48.53 48.09 50.02 29.53 27.92 44.81 45.44 46.84 47.01 47.36 34.50 37.78 51.72 52.29 52.30 51.89 54.13 29.85 30.37 47.11 46.79 48.37 47.37 49.45 Reasoning Paradigm To align model outputs with experimental logic and generate evaluable protocols, we propose the Sketch-and-Fill reasoning paradigm. Its effect was systematically tested through ablation experiments on four representative models: DeepSeek-V3, GPT-5 Chat, Thoth, and its base model Qwen3-8B. Since outputs are in natural language, executability metrics were not applicable, and Figure 4 reports results based on lexical similarity. The results show that Sketchand-Fill improves performance for all models except the base model, with notable gains on BLEUAVG. DeepSeek-V3, GPT-5 Chat, and Thoth achieve average improvements of 3.89%, 2.92%, and 3.79% over counterparts without the paradigm. For the base model, the lack of improvement is attributed to reasoning failure due to missing knowledge injection from scientific protocols. 8 Table 3: Ablation studies on the SCORE mechanism. Evaluation covers step scale, semantic alignment, and overall reward design. KL() denotes the KL-divergence penalty in the loss function, and Vanilla refers to using standard semantic similarity metrics as the reward signal."
        },
        {
            "title": "SCORE",
            "content": "Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG"
        },
        {
            "title": "Reward",
            "content": "w/o (d) w/o g( L) w/o (d)+g( L) w/o mij w/o Order() w/o Order() +f (d)+g( L) w/o KL() Vanilla"
        },
        {
            "title": "Thoth",
            "content": "43.67 38.89 45.35 38.68 40.93 44.15 39.96 38.74 46.60 55.97 74.37 49. 73.70 61.27 58.22 74.43 63.41 75.34 6.83 23.67 3.00 23.17 12. 7.67 22.67 21.50 25.50 51.83 71.00 34.17 67.67 48.00 42. 69.33 52.36 73.33 10.00 51.00 4.17 50.17 33.33 43.33 46.67 44. 53.00 43.09 43.18 42.49 42.84 37.26 38.45 42.73 45.52 43. 37.28 46.14 33.44 48.91 45.35 45.78 46.03 50.12 50.02 26.08 44.92 21. 45.58 43.27 43.52 44.82 43.90 47.39 43.31 52.80 40.41 52.85 49. 35.34 49.55 30.41 49.29 41.30 48.74 41.41 53.36 50.54 54. 48.89 45.62 52.10 SCORE Mechanism In Section 3.3.2, we introduced the SCORE reward mechanism, consisting of step scale, order consistency, and To evaluate its role semantic consistency. in RL training, we conducted ablation studies, including variants that removed individual components, dropped the KL divergence penalty, or replaced SCORE with vanilla reward based only on BLEU-AVG, ROUGEL, and BERTScore. Results are summarized in Table 3. Omitting the step scale reward leads to sharp declines in Order-S and StepM (6.83%/10% and 3%/4.17%, rows 1 and 3), indicating failures to generate executable protocols and producing either verbose or incomplete outputs. Excluding the positional decay factor (row 4) weakens penalties for misaligned steps and reduces semantic consistency, under which Thoth still achieves 7.92% gain in Semantic-A. The order consistency reward is also critical because without enforcing execution order, steps become misarranged and semantic coherence breaks down. Thoth surpasses the variants in rows 5 and 6 by 6.36% and 5.17% on BLEU-AVG, respectively. Removing the KL penalty further degrades performance, with Table 3 showing 3.21% average drop compared to Thoth. Although the vanilla reward improves BLEUAVG and ROUGE-L, the generated protocols exhibit poor executability with 10.65% average reduction. These results demonstrate the effectiveness of SCORE in guiding the model to generate coherent and executable protocols. For further analyses of reward, see Appendix K.1.2 & K.1.3. Figure 4: Ablation studies on reasoning paradigm. Performance differences with/without Sketchand-Fill (SaF) are shown in the top panel, and visual comparison is provided in the bottom panel. Figure 5: Ablation studies on the training strategy. Left and middle: rightmost marks extreme values across strategies. Right: Thoths training trajectory across stages. Training Strategy To progressively equip the model for solving complex scientific problems, we propose the three-stage Knowledge-to-Action Learning strategy. We compared models trained at different stages with Thoth to validate its effectiveness. As shown in Figure 5 (left and middle), Thoth achieves SOTA performance after staged training and consistently outperforms models trained with only Stage 1+2 (pre-training and SFT) or without pre-training (Stage 2+3 or Stage 2 alone). Specifi9 cally, Thoth improves by an average of 11.08% on executability and 4.2% on lexical similarity over Stage 1+2. It further achieves mean gains of 1.44% and 8.79% over Stage 2+3 and Stage 2 across both dimensions. Figure 5 (right) further demonstrates that the three-stage process progressively enhances performance, with SFT improving lexical similarity and RL strengthening rationality and executability."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced Thoth, protocol-generation model grounded in the SciRecipe dataset, the Sketch-and-Fill reasoning paradigm, and the SCORE mechanism. By integrating structured reasoning with verifiable rewards, Thoth achieves SOTA performance on both protocol-specific and broader scientific benchmarks. Equally important, the protocols it generates are concise, logically coherent, and experimentally executable. This work provides blueprint for building reliable scientific assistants and demonstrates that structured reasoning frameworks combined with targeted rewards are critical for advancing protocol generation and improving reproducibility."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have made extensive efforts to ensure the reproducibility of our results. Details on training data distributions, hyperparameter settings for pre-training, SFT, and RL, as well as implementation frameworks, are provided in Appendix G. We will also release all code, models, and processed datasets to facilitate faithful reproduction of our findings."
        },
        {
            "title": "REFERENCES",
            "content": "Herve Abdi. The kendall rank correlation coefficient. Encyclopedia of measurement and statistics, 2:508510, 2007. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Anthropic. Claude (opus / sonnet series) language models. https://www.anthropic.com/ models/claude, 2025. Accessed: 2025-09-22. Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Jonas Becker, Jan Philip Wahle, Bela Gipp, and Terry Ruas. Text generation: systematic literature review of tasks, evaluation, and challenges. arXiv preprint arXiv:2405.15604, 2024. Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019. Yoshua Bengio, Jerˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. 10 Bio-protocol, LLC. Bio-protocol. https://bio-protocol.org, 2025. Accessed: 2025-0917. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. ByteDance. Doubao 1.5 pro. https://www.doubao.com/chat/, 2025. Accessed: 2025-0922. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. Leonard Freedman, Iain Cockburn, and Timothy Simcoe. The economics of reproducibility in preclinical research. PLoS biology, 13(6):e1002165, 2015. Shanghua Gao, Richard Zhu, Zhenglun Kong, Ayush Noori, Xiaorui Su, Curtis Ginder, Theodoros Tsiligkaridis, and Marinka Zitnik. Txagent: An ai agent for therapeutic reasoning across universe of tools. arXiv preprint arXiv:2503.10970, 2025. Steven Goodman, Daniele Fanelli, and John PA Ioannidis. What does research reproducibility mean? Science translational medicine, 8(341):341ps12341ps12, 2016. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Maarten Grootendorst. Keybert: Minimal keyword extraction with bert., 2020. URL https: //doi.org/10.5281/zenodo.4461265. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin Qiu, Gavin Li, Junze Zhang, et al. Biomni: general-purpose biomedical ai agent. biorxiv, 2025. 11 Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Shuo Jiang, Daniel Evans-Yamamoto, Dennis Bersenev, Sucheendra Palaniappan, and Ayako Yachie-Kinoshita. Protocode: Leveraging large language models (llms) for automated generation of machine-readable pcr protocols from scientific publications. SLAS technology, 29(3):100134, 2024. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Ruofan Jin, Zaixi Zhang, Mengdi Wang, and Le Cong. Stella: Self-evolving llm agent for biomedical research. arXiv preprint arXiv:2507.02004, 2025. Jon Laurent, Joseph Janizek, Michael Ruzo, Michaela Hinks, Michael Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew White, and Samuel Rodriques. Lab-bench: Measuring capabilities of language models for biology research. arXiv preprint arXiv:2407.10362, 2024. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, 2020. Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, and Jordan Lee Boyd-Graber. Semantically-aware rewards for open-ended r1 training in free-form generation. arXiv preprint arXiv:2506.15068, 2025. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. Yuyang Liu, Liuzhenghao Lv, Xiancheng Zhang, Li Yuan, and Yonghong Tian. Bioprobench: Comprehensive dataset and benchmark in biological protocol understanding and reasoning. arXiv preprint arXiv:2505.07889, 2025. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics, 23(6):bbac409, 2022. National Academies of Sciences, Medicine, Policy, Global Affairs, Board on Research Data, Division on Engineering, Physical Sciences, Committee on Applied, Theoretical Statistics, Board on Mathematical Sciences, et al. Reproducibility and replicability in science. National Academies Press, 2019. OpenAI. Gpt-5 chat. https://chat.openai.com, 2025. Accessed: 2025-09-17. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, and Kai Yu. Scidfm: large language model with mixture-of-experts for science. arXiv preprint arXiv:2409.18412, 2024. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Leonid Teytelman, Alexei Stoliartchouk, Lori Kindler, and Bonnie Hurwitz. Protocols. io: virtual communities for protocol development and discussion. PLoS Biology, 14(8):e1002538, 2016. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Acting less is reasoning more! teaching model to act efficiently. arXiv preprint arXiv:2504.14870, 2025. Xin Wang, Yudong Chen, and Wenwu Zhu. survey on curriculum learning. IEEE transactions on pattern analysis and machine intelligence, 44(9):45554576, 2021. Wikipedia contributors. the free encyclopedia, 2025. URL https://en.wikipedia.org/w/index.php?title=Nature_Protocols& oldid=1296318228. [Online; accessed 17-September-2025]. Nature protocols Wikipedia, xAI. Grok 4. https://docs.x.ai/docs/models/grok-4-0709, 2025. Accessed: 202509-22. Yifei Xu, Tusher Chakraborty, Srinagesh Sharma, Leonardo Nunes, Emre Kıcıman, Songwu Lu, and Ranveer Chandra. Direct reasoning optimization: Llms can reward and refine their own reasoning for open-ended tasks. arXiv preprint arXiv:2506.13351, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Sihang Zeng, Kai Tian, Kaiyan Zhang, Junqi Gao, Runze Liu, Sa Yang, Jingxuan Li, Xinwei Long, Jiaheng Ma, Biqing Qi, et al. Reviewrl: Towards automated scientific review with rl. arXiv preprint arXiv:2508.10308, 2023. Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, and Andreas Vlachos. Reinforcement learning for better verbalized confidence in long-form generation. arXiv preprint arXiv:2505.23912, 2025a. 13 Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Zhongyue Zhang, Zijie Qiu, Yingcheng Wu, Shuya Li, Dingyan Wang, Zhuomin Zhou, Duo An, Yuhan Chen, Yu Li, Yongbo Wang, et al. Origene: self-evolving virtual disease biologist automating therapeutic target discovery. bioRxiv, pp. 202506, 2025c. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A SciRecipe",
            "content": "A.1 Preprocessing of Original Scientific Protocols . . . . . . . . . . . . . . . . . . . . A.2 Structured Integration of Scientific Protocols . . . . . . . . . . . . . . . . . . . . . A.3 Supplementary Details on SciRecipe . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Supplementary Details on SciRecipe Construction Pipeline . . . . . . . . . . . . . SciRecipe-Eval"
        },
        {
            "title": "C SCORE Mechanism",
            "content": "C.1 Consistency Gate . . C.2 Order Consistency . C.3 Action Anchor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "J Additional Experimental Results",
            "content": "J.1 Qualitative Study of Reasoning Models . . . . . . . . . . . . . . . . . . . . . . . J.2 Fine-grained Results on SciRecipe-Eval . . . . . . . . . . . . . . . . . . . . . . . J.3 Other Results on Protocol Benchmark . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "K Futher Analyses",
            "content": "K.1 Reward Analyses & Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K.2 Pre-training Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "M Prompts",
            "content": "16 16 16 16 17 19 19 20 21 21 22 23 24 24 24 29 34 34 34 38"
        },
        {
            "title": "A SCIRECIPE",
            "content": "A.1 PREPROCESSING OF ORIGINAL SCIENTIFIC PROTOCOLS In the preprocessing phase, we first employed MinerU (Wang et al., 2024) to identify and extract protocol texts, as the majority of online experimental protocols are distributed in non-editable PDF format. This step was essential because protocols published in PDF form often contain heterogeneous content such as figures, tables, references, and lengthy narratives, which substantially increase the difficulty of accurate parsing and subsequent questionanswer pair construction. To mitigate this issue, we analyzed both the page count and the number of figures/tables in each document, and applied empirically determined thresholds to filter out overly long or structurally complex protocols. Another important consideration was the removal of redundancy, since many protocols share highly similar structures or represent slightly modified versions of the same experiment. Without careful filtering, these near-duplicate entries could cause information leakage across training and evaluation splits. To address this, we applied similarity detection and clustering. Specifically, we used Qwen3Embedding-8B (Zhang et al., 2025b) to vectorize protocol titles, built an efficient similarity search index using FAISS (Douze et al., 2024), and conducted Top-K nearest-neighbor searches. Similar pairs were identified by thresholding cosine similarity scores, after which unionfind algorithm was applied to partition protocols into connected components. This process produced both pairwise and grouped similarity sets, effectively eliminating redundant samples while preserving data diversity and representativeness. In addition, some protocols were accompanied by electronic supplementary materials, which frequently contained fragmented or ambiguous instructions. Because these supplementary files risked introducing inconsistencies and annotation difficulties, they were uniformly removed from the dataset. To further guarantee the reliability of the corpus, we recruited three doctoral students with extensive experimental experience to manually review subset of the data. Their verification ensured not only correctness but also balanced coverage across experimental domains, ultimately enhancing both the diversity and representativeness of the retained corpus. A.2 STRUCTURED INTEGRATION OF SCIENTIFIC PROTOCOLS Because experimental protocols vary widely in writing style and formatting, we extracted common elements to form unified structured representation. The core fields included experiment name, research background, reagents and materials, experimental equipment, and experimental procedures. This integration was carried out in two complementary steps. First, we applied rule-based method that relied on regular expressions to match predefined keywords and extract the corresponding fields. During this stage, Markdown tables were removed to avoid parsing errors, while figure and table captions were preserved as valuable contextual information. The rule-based approach achieved reliable performance for protocols written in highly standardized format, but its applicability was limited for documents with more complex or irregular structures. To address these limitations, we further adopted model-based strategy. Specifically, we leveraged Grok-4 (xAI, 2025) to summarize and restructure the original protocols, ensuring accurate extraction of key fields without introducing fabricated content. The model also adjusted the ordering of experimental steps to maintain logical and procedural consistency. This hybrid two-step pipeline allowed us to combine the precision of rule-based parsing with the flexibility of model-based restructuring. Through this process, we obtained over 12K structured protocols. Quality control measures were subsequently applied to filter out entries with missing fields, excessively lengthy procedures, or dependencies on external online resources. The resulting corpus provides clean, logically consistent, and representative collection of protocols suitable for downstream training and evaluation. A.3 SUPPLEMENTARY DETAILS ON SCIRECIPE When designing the eight task types in SciRecipe, we defined explicit objectives and representative scenarios for each. Together, these tasks cover the entire lifecycle of experimental research, ensuring that the dataset trains and evaluates both general reasoning and practical execution. Overview: Requires the model to generate hierarchically organized summary of the protocol, enabling rapid comprehension of its global structure. Specific: Focuses on step-by-step decomposition of procedures, ensuring the model captures the micro-level operational logic. 16 Retrieval: Targets precise extraction of experimental parameters (e.g., temperature, pH, concentration), emphasizing accuracy in reporting. Planning: Involves transforming high-level objectives into coherent, logically ordered experimental steps. Troubleshooting: Simulates diagnosing experimental failures, identifying error sources, and proposing corrective strategies. Constraint: Tests adaptability under resource limitations, such as restricted equipment or reagent availability. Scaling: Requires numerical adjustments and unit conversions to adapt experiments across different scales. Safety: Focuses on compliance with safety standards and identification of potential laboratory risks. By encompassing comprehension, planning, error handling, adaptability, scalability, and safety, SciRecipe ensures comprehensive and targeted task design. See Figure 21 for detailed prompts. A.4 SUPPLEMENTARY DETAILS ON SCIRECIPE CONSTRUCTION PIPELINE During the construction of SciRecipe, we first generated QA pairs for multiple task types using the structured protocol corpus as the foundation. Particularly, for Protocol-Comprehension tasks, we relied on hierarchical tree extraction (e.g., Step 1: xxx; Step 1.1: xxx; Step 1.1.1: xxx), which preserved both the hierarchical structure and the stepwise dependencies of experimental procedures. This design ensured that the model could capture not only surface-level descriptions but also the nested logic inherent in complex experiments. Then, we introduced the Sketch-and-Fill reasoning paradigm. This approach first summarizes core elements in an outline-like format and then fills in the specific operations, thereby enhancing both interpretability and training effectiveness. The paradigm was particularly effective in decomposing high-level objectives into concrete experimental actions while maintaining logical consistency. Because LLMs may produce outputs with randomness and formatting inconsistencies, directly discarding such samples would risk losing high-quality data. To address this, we designed format validation and repair module. This module automatically detected non-compliant outputs and prompted the model to regenerate them under low-temperature settings, ensuring structural consistency while retaining valid content. After passing the format checks, each QA pair underwent content review across six scientific dimensions: 1. Scientific Accuracy: correctness of operations, parameters, and outcomes. 2. Safety & Compliance: adherence to laboratory safety standards and ethical guidelines. 3. Logical Coherence & Actionability: stepwise consistency and rational flow of procedures. 4. Clarity & Ambiguity: precision and unambiguity of textual descriptions. 5. Generality & Specificity: balanced coverage of broad applicability and detailed context. 6. Efficiency & Resource Optimization: optimization of time, materials, and experimental resources. For this step, we employed Gemini 2.5 Flash (Comanici et al., 2025) as the validation model, which performed dimension-wise evaluations and produced review reports. QA pairs that failed to meet requirements were systematically discarded. Finally, to guarantee the reliability of the constructed dataset, we performed manual verification. Doctoral students from interdisciplinary backgrounds conducted thorough inspections, ensuring that the resulting data were not only structurally correct but also scientifically feasible and practically reasonable. Through this multi-stage pipeline, which combines automated validation, targeted repair, model-assisted review, and expert-level manual checks, SciRecipe achieved both rigor and diversity, providing robust foundation for protocoloriented training and evaluation. 17 Table 4: Disciplinary distribution of the SciRecipe dataset. Subdomain Cell Biology Biochemistry Molecular Biology Microbiology Plant Science Immunology Neuroscience General Laboratory Procedure Bioinformatics Bioimaging Technologies Cancer Biology Model Organism-Specific Techniques Others Genomics Technologies Structural Biology Techniques Biophysics Pharmacology & Drug Development Developmental Biology Genetics Histology Techniques Synthetic Biology & Bioengineering Stem Cells Bioengineering Toxicology & Safety Testing Systems Biology Medicine Drug Discovery Count Percentage 2366 1935 1788 1163 936 717 688 521 319 284 275 246 127 111 79 77 67 64 55 31 31 18 11 11 8 4 3 19.82% 16.21% 14.98% 9.74% 7.84% 6.01% 5.77% 4.37% 2.67% 2.38% 2.30% 2.06% 1.06% 0.93% 0.66% 0.65% 0.56% 0.54% 0.46% 0.26% 0.26% 0.15% 0.09% 0.09% 0.07% 0.03% 0.03% Figure 6: Data distribution of experimental actions, parameters (each step), and step counts in SciRecipe after quality control. 18 SCIRECIPE-EVAL Cuurrently, benchmarks specifically dedicated to experimental protocol generation remain scarce, and most existing efforts are limited to tasks that simply restate the original protocol content. Such designs make it difficult to evaluate models ability to generalize in realistic scientific contexts. Furthermore, the reliance on natural language alone as the ground truth introduces additional challenges for systematic evaluation. To address this gap, we constructed SciRecipe-Eval by following the design principles of SciRecipe while independently processing 400 protocols. The benchmark is organized into two major task categories, each containing approximately 300 QA pairs with balanced distribution across subtasks. Protocol-Comprehension: focuses on understanding and restructuring protocol content. Overview: generating hierarchical summaries of experimental procedures. Specific: decomposing steps to capture micro-level logic. Problem-Solving: emphasizes reasoning and execution in realistic laboratory contexts. Retrieval: extracting precise parameters such as temperature, pH, or concentration. Planning: transforming objectives into coherent and logically ordered plans. Troubleshooting: diagnosing potential causes of experimental failure. Constraint: adapting protocols under resource limitations. Scaling: performing numerical adjustments and conversions for different scales. Safety: ensuring compliance and risk control. Because the QAs were designed under specific reasoning paradigms, experimental actions (predicates) were treated as the core elements of the ground truth. The number of actions defines the size of the action space and thereby provides direct measure of experimental complexity. Based on this criterion, tasks containing fewer than four actions were categorized as Level 1, whereas those with four or more actions were categorized as Level 2. For Level 2 tasks, we further inserted one to four randomly sampled distractor actions, enabling more comprehensive evaluation of models ability to discriminate between correct and incorrect operations. In line with the data quality control standards established for SciRecipe, we recruited three doctoral students in biology-related fields to perform manual sampling and review of the dataset. Their verification ensured that the final benchmark was not only structurally consistent but also scientifically accurate and representative of real laboratory practices."
        },
        {
            "title": "C SCORE MECHANISM",
            "content": "C.1 CONSISTENCY GATE In protocol evaluation, consistency refers to the step-by-step correspondence between the structured outline (<key>) and its natural language expansion (<orc>). If an action, object, or parameter declared in <key> is missing from the corresponding <orc> description, the generated protocol is semantically incomplete and cannot serve as reliable experimental guide. Therefore, the consistency gate is adopted as the second checkpoint in SCORE, and only protocols passing this constraint are eligible for reward computation. Concretely, we first require that the number of steps in <key> matches exactly with <orc>, with consecutive numbering covering 1 through . For each step i, we construct token set Ti by concatenating the declared action ai, object set Oi, and parameter set Pi. After normalization (e.g., case folding, unit standardization, removal of subscript/superscript variants), we compute the coverage rate of these tokens in the natural language step <orc>i as: (cid:12){t Ti : norm(<orc>i)}(cid:12) (cid:12) (cid:12) Ti covi = (7) If all steps satisfy covi τ (we set τ = 0.95 in this work) and the step indices are strictly aligned, the consistency gate is considered passed: Icons(y) = (cid:40)1, ˆN = {ˆn1, . . . , ˆn ˆN } = {1, . . . , } mini covi τ 0, otherwise (8) where ˆN and denote the step counts of the generated and reference protocols, respectively, and ˆni denotes the predicted step indices. If the gate fails, all subsequent rewards are set to zero. This hard gating design reflects fundamental experimental principle: when the structured outline and its natural language realization are inconsistent, even partially correct fragments cannot make the protocol executable. C.2 ORDER CONSISTENCY 1, . . . , Let ˆa = (ˆa1, . . . , ˆan) denote the predicted action sequence extracted from <key>, and = (a m) the reference sequence. The goal of order consistency is to evaluate whether the predicted execution order preserves the logical progression of the reference protocol, since deviations in ordering can undermine reproducibility even when individual steps appear correct. Two complementary scoring modes are provided. The first mode, Strict Subsequence, enforces conservative executability constraint. The score equals 1 if the predicted sequence is identical to the reference, or if one is subsequence of the other; otherwise it is 0: Orderstrict(ˆa, a) = (cid:40)1, ˆa = subseq(ˆa, a) subseq(a, ˆa) 0, otherwise (9) where subseq(x, y) holds when can be embedded in with order preserved. This criterion accepts insertions or omissions as long as the preserved actions remain in order, but strictly rejects any reordering. It is also computationally efficient, requiring only O(n + m) time. The second mode, Longest Common Subsequence (LCS), provides graded similarity that reflects how much of the reference order is retained. The score is defined as: OrderLCS(ˆa, a) = LCS(ˆa, a) (10) where = is the length of the reference sequence and LCS(, ) is the length of the longest common subsequence between the predicted and reference sequences. This normalization yields value in [0, 1] that represents the fraction of the reference order preserved. Unlike the strict mode, LCS assigns partial credit when the prediction maintains some correct subsequences even if other steps are misplaced. For example, given the ground truth [harvest, lyse, centrifuge, quantify], prediction [harvest, lyse, quantify] achieves strict score of 1 and an LCS score of 3/4 = 0.75 (omission but order preserved), whereas [harvest, centrifuge, lyse, quantify] scores 0 under the strict rule but 3/4 = 0.75 under LCS, since most of the order is still preserved. By contrast, [lyse, harvest, quantify, centrifuge] is more severely misordered, yielding an LCS score of 2/4 = 0.5. For completeness, the LCS length is computed via dynamic programming. Let L(i, j) denote the LCS length between the prefix subsequences (ˆa1, . . . , ˆai) of the predicted sequence ˆa and (a 1, . . . , ) of the reference sequence a. Then 0, L(i, j) = L(i 1, 1) + 1, max{L(i 1, j), L(i, 1)}, = 0 or = 0 ˆai = ˆai = (11) with LCS(ˆa, a) = L(n, m). The algorithm runs in O(nm) time and requires O(nm) space. Together, these two modes serve distinct but complementary purposes, with the strict subsequence mode being the primary criterion. Strict alignment best reflects the hard requirements of laboratory executability, where even minor order violations typically render protocol unusable. The LCS mode is introduced only as supplementary measure, offering smoother and more informative feedback during training by granting partial credit when some procedural intent is preserved. In practice, strict mode is preferred for evaluation of executable protocols, while LCS can be selectively applied to provide graded optimization signals when softer supervision is required. All formal experimental results in this work are reported under the strict mode. See Appendix K.1.2 for further experiments. C.3 ACTION ANCHOR In semantic consistency evaluation, actions (i.e., step verbs) are treated as anchors to align predicted steps with reference steps before any further comparison. The alignment is performed in left-toright, order-preserving manner. Formally, let the predicted action sequence be ˆa = (ˆa1, . . . , ˆan) and the reference action sequence be = (a m). For each predicted action ˆai, we select the earliest reference index after the previous match such that = ˆai. If such exists, we record the pair (i, j), otherwise ˆai remains unmatched and contributes no anchor. This procedure yields an ordered set of index pairs = {(i1, j1), . . . , (iK, jK)} with 1 i1 < < iK n, 1 j1 < < jK m, and ˆaik = jk for all (ik, jk) W. The construction is greedy and monotone (first-comefirst-match), which guarantees that (i) sequence order is preserved, (ii) repeated actions are matched consistently, with each predicted occurrence aligning only to the next unused reference occurrence, and (iii) the overall alignment runs in linear time O(n + m). 1, . . . , Example For the ground truth sequence [harvest, lyse, centrifuge, quantify] and the prediction [harvest, centrifuge, lyse, stain, quantify], the alignment proceeds as ˆa1 = harvest ˆa2 = centrifuge ˆa3 = lyse ˆa5 = quantify 1 = harvest, 3 = centrifuge, 2 = lyse (rejected: order violation), 4 = quantify. Hence the retained alignment is = {(1, 1), (2, 3), (5, 4)}, while the unmatched action stain is discarded. This anchor-based procedure ensures that semantic comparisons focus only on correctly aligned procedural units, avoiding misleading matches across unrelated operations. (cid:55) (cid:55) (cid:55) (cid:55) a"
        },
        {
            "title": "D RL ALGORITHM",
            "content": "During the RL stage, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), variant of PPO designed to reduce variance and stabilize updates by exploiting multiple responses generated from the same query. Unlike standard PPO, which normalizes the reward across batch, GRPO performs group-wise normalization within each query group. This strategy ensures that the relative quality of responses to the same query is emphasized, thereby mitigating reward scale differences across queries and improving training stability. Formally, let denote query, and {yi}G i=1 be responses sampled from the current policy πθ(x). For each response yi, the normalized advantage is defined as r(x, yi) mean(cid:0){r(x, yj)}G std(cid:0){r(x, yj)}G where r(x, yi) denotes the scalar reward assigned to yi. By centering and scaling rewards within the group, the advantage highlights relative performance among candidate responses. ˆAi = (12) j=1 j= (cid:1) (cid:1) The GRPO objective combines this normalized advantage with the clipped surrogate function: LGRPO(θ) = xD, {yi}G i=1πθ(x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min si(θ) ˆAi, clip(cid:0)si(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAi (cid:35) (cid:17) (13) where ϵ is clipping threshold controlling the size of policy updates. This formulation prevents excessively large updates while still encouraging exploration. The importance sampling ratio si(θ) is computed at the token level to account for the sequential nature of text generation. Specifically, it is defined as the geometric mean of the per-token probability ratios: si(θ) = exp 1 yi yi (cid:88) t=1 log πθ(yi,t x, yi,<t) πθold (yi,t x, yi,<t) (14) 21 where πθ and πθold denote the current and reference policy models, respectively, and yi,t is the t-th token of response yi. This per-token formulation maintains sensitivity to fine-grained action probabilities and avoids domination by sequence length. Overall, GRPO stabilizes training by normalizing advantages within query groups, constrains policy updates through clipping, and ensures robustness at the token level via per-token importance ratios."
        },
        {
            "title": "E BASELINE MODELS",
            "content": "Given the lack of models tailored for scientific protocol generation, we compare our approach against broad set of large-scale language models. The closed-source baselines include the GPT-5 series (OpenAI, 2025) (GPT-5, GPT-5 Chat), the Claude series (Anthropic, 2025) (Claude Sonnet 4, Claude Opus 4.1), the Gemini 2.5 series (Comanici et al., 2025) (Flash and Pro), as well as ChatGPT4o (Achiam et al., 2023), Doubao-1.5-Pro (ByteDance, 2025), and Qwen2.5-Max (Qwen et al., 2025). For open-source baselines, we evaluate strong instruction-tuned models such as Qwen2.572B (Qwen et al., 2025), Qwen3-235B (Yang et al., 2025), DeepSeek-V3 (Liu et al., 2024), GPTOSS-120B (Agarwal et al., 2025), Llama-3.1-405B (Grattafiori et al., 2024), and Kimi-K2 (Team et al., 2025). In addition, we consider reasoning-oriented models including DeepSeek-R1 (Guo et al., 2025), Grok 3/4 (xAI, 2025), and OpenAIs o1 and o3 (Jaech et al., 2024). Finally, to assess scientific capabilities, we further evaluate domain-specific models including Intern-S1-mini, Intern-S1 (Bai et al., 2025), and SciDFM (Sun et al., 2024). Together, these baselines provide comprehensive landscape of frontier, open-source, reasoning, and domain-oriented models for comparison."
        },
        {
            "title": "F EVALUATION METRICS",
            "content": "It is noteworthy that SCORE is not merely reward mechanism for RL but also naturally serves In particular, the step scale reward, action order score, and object and as an evaluation metric. parameter matching scores can all be directly used as assessment criteria for protocol generation tasks. Beyond this setting, the structured component-based comparison approach has the potential to generalize to broader scenarios, such as task planning in dialogue systems, generation of robotic operation scripts, or construction of reasoning chains in complex question-answering. By aligning the predicateobjectadverbial structure, we can evaluate not only the surface similarity of model outputs but also their reasoning depth and logical coherence. This provides novel perspective for evaluating open-ended problems and highlights the broad applicability of the SCORE mechanism. 1, . . . , Formally, let the predicted key-step sequence be ˆs = (ˆs1, . . . , ˆsn) and the reference sequence be = (s m). We denote by = {(ik, jk)}K k=1 the set of monotone action anchors obtained from the alignment procedure in Appendix C.3. On this basis, we introduce five structured metrics for evaluation. m), with corresponding action sequences ˆa = (ˆa1, . . . , ˆan) and = (a 1, . . . , The first, Step-M, measures step-level completeness through strict length matching, Step-M(ˆs, s) = I[ ˆs = ] The Order-S then enforces exact sequential agreement, Order-S(ˆa, a) = I[ ˆa = ] (15) (16) while the more tolerant Order-LCS employs normalized longest common subsequence, Order-LCS(ˆa, a) = 2 L(ˆa,a) where L(, ) denotes the LCS length. To further capture order fidelity, Order-Tau evaluates the concordance of action anchors using Kendall-style correlation coefficient, (17) n+m Order-Tau(W) = + (18) with and the counts of concordant and discordant pairs in the aligned index sequence, defaulting to 0 when + = 0. Finally, Semantic-A is defined identically to the semantic consistency score introduced in Section 3.3.2. It evaluates object overlap, parameter similarity, and positional fidelity on the basis of aligned action anchors, thereby reflecting both the executability and semantic correctness of generated protocols."
        },
        {
            "title": "G REPRODUCIBILITY STATEMENT",
            "content": "We provide essential information to ensure the reproducibility of our experiments. Table 5 reports the training data distribution across stages, covering both SciRecipe tasks and BioProBench subtasks. Key hyperparameters for pre-training, SFT, and RL are listed in Tables 6, 7, and 8. All experiments were conducted with the LLaMA-Factory framework for pre-training and SFT, and VeRL for RL training. We release detailed settings, including LoRA configurations, learning rates, epochs, scheduling, and hardware, so that independent researchers can replicate our pipeline and validate the reported results. All data, code, and models will be released publicly. Table 5: Training data distribution across different stages (one epoch). PC denotes ProtocolComprehension tasks and PS denotes Problem-Solving tasks. Four task types from BioProBench (Liu et al., 2025) are also incorporated to enhance the models ability to interpret protocols."
        },
        {
            "title": "SciRecipe PC\nSciRecipe PS\nBioProBench PQA\nBioProBench ERR\nBioProBench ORD\nBioProBench GEN",
            "content": "34461 16164 52640 103983 31039 59477 6956 3263 Table 6: Key hyperparameters used in LLaMA-Factory for pre-training."
        },
        {
            "title": "Parameter",
            "content": "finetuning type lora rank lora target preprocessing num workers dataloader num workers per device train batch size gradient accumulation steps learning rate num train epochs lr scheduler type warmup ratio bf"
        },
        {
            "title": "Value",
            "content": "lora 8 all 16 4 1 8 1e-4 6 cosine 0.1 true Table 7: Key hyperparameters used in LLaMA-Factory for SFT training."
        },
        {
            "title": "Parameter",
            "content": "finetuning type lora rank lora target preprocessing num workers dataloader num workers per device train batch size gradient accumulation steps learning rate num train epochs lr scheduler type warmup ratio bf"
        },
        {
            "title": "Value",
            "content": "lora 32 all 8 4 1 8 3e-4 5 cosine 0.05 true 23 Table 8: Key hyperparameters used in VeRL for RL training."
        },
        {
            "title": "Parameter",
            "content": "algorithm.adv estimator data.train batch size data.max prompt length data.max response length actor.optim.lr actor.ppo mini batch size actor.ppo micro batch size per gpu actor.use kl loss actor.kl loss coef actor.kl loss type actor.entropy coeff rollout.tensor model parallel size rollout.gpu memory utilization rollout.n trainer.total epochs trainer.n gpus per node trainer.nnodes"
        },
        {
            "title": "Value",
            "content": "grpo 1024 1024 1024 7e-6 256 32 true 0.001 low var kl 0 2 0.6 5 15"
        },
        {
            "title": "H LIMITATIONS",
            "content": "While our work presents structured framework for scientific protocol generation and shows consistent improvements across benchmarks, some limitations remain. First, although SciRecipe spans 27 biological subfields, it is still skewed toward widely used laboratory techniques, with rare or highly specialized protocols underrepresented. Second, the SCORE metrics mainly emphasize step completeness and order fidelity, but do not yet capture finer aspects such as stylistic variation, crossstep dependencies, or long-range experimental context. Third, our experiments were conducted in relatively controlled settings, which may not fully reflect the complexity and variability of real laboratory environments. We believe these issues can be progressively addressed in future work by expanding the dataset to cover rarer domains, enriching evaluation dimensions beyond structural executability, and validating the framework under more diverse practical scenarios."
        },
        {
            "title": "I USAGE OF LLMS ON PAPER WRITING",
            "content": "We used LLMs (e.g., GPT-5) to aid in polishing the writing of this paper. Specifically, LLMs were employed to refine grammar, improve fluency, and adjust the tone of academic writing (e.g., smoothing transitions between sections, shortening overly long sentences, and rephrasing repetitive expressions). The models were not used for generating technical content, conducting experiments, or formulating research ideas. All substantive contributions remain the work of the authors."
        },
        {
            "title": "J ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "J.1 QUALITATIVE STUDY OF REASONING MODELS In the permeability assay, As discussed in the main text, general-purpose reasoning models often overthink protocol generation, producing overly long procedures with redundant actions and no clear stopping criteria. the gold-standard protocol requires only four steps (washremoveincubatewash) to ensure structural preservation and experimental efficiency. Thoth closely followed this structure, preserving both the correct sequence and action fidelity. In contrast, Grok 4 expanded the task into 13-step immunostaining workflow, adding permeabilization, blocking, antibody staining, and repeated washes far beyond the intended fixation step. Although logically coherent within its reasoning chain, this output diverges from laboratory practice, where clarity and 24 parsimony are essential. This case underscores broader limitation of general-purpose reasoning models: their tendency to prioritize exhaustive reasoning over practical executability, reinforcing our quantitative findings that domain-adapted models like Thoth achieve more concise, faithful, and actionable protocols. Figure 7: Case study of the reasoning model Grok 4 on Protocol-Comprehension task. 25 Figure 8: Case study of the reasoning model o1 on Protocol-Comprehension tasks. 26 Figure 9: Case study of the reasoning model Grok 4 on Problem-Solving task. 27 Figure 10: Case study of the reasoning model o1 on Problem-Solving task. 28 J.2 FINE-GRAINED RESULTS ON SCIRECIPE-EVAL Table 9: Main results on SciRecipe-Eval at level 1. Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Reasoning Models DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 44.44 32.82 41.50 44.35 45.43 39.66 38.78 37.14 45.77 40.13 39.68 45.10 36.56 39.80 42.58 37.86 41.53 42.52 39.78 42.15 27.21 49.49 29.98 52.21 78.95 63.40 78.91 76.44 75.42 75.32 77.51 78.42 78.43 77.15 78.19 78.59 76.12 74.42 76. 77.08 78.84 77.27 77.70 74.94 59.83 80.41 68.08 80.12 39.49 19.27 35.75 33.70 34.82 33.70 36.08 38.47 37.78 37.10 35.08 33.84 31.66 28.34 34.72 35.43 37.68 34.92 28.00 26.02 24.19 42.88 21.12 41. 80.04 63.63 73.50 76.90 76.88 75.66 76.56 76.00 79.28 77.93 78.28 75.64 74.18 73.06 78.60 76.35 77.96 75.66 74.43 72.95 59.89 80.30 65.00 83.20 56.49 27.62 35.06 47.65 46.89 43.91 40.17 60.23 61.93 59.21 54.82 52.27 44.25 49.50 52. 45.97 54.19 49.52 43.02 36.29 40.17 64.66 38.80 62.27 40.98 22.34 31.74 35.91 36.29 34.60 32.71 39.78 31.96 31.90 35.68 39.66 34.12 37.28 35.40 40.81 37.12 38.91 37.25 31.10 16.29 46.50 17.98 46. 52.11 35.92 45.77 47.54 46.90 46.21 46.84 49.57 51.43 49.87 48.10 51.46 47.31 47.38 47.65 48.96 49.82 49.75 49.23 46.42 28.17 51.91 34.59 52.69 48.02 35.41 45.80 43.61 42.14 42.55 47.84 41.77 47.16 46.58 47.11 48.04 46.23 45.11 45. 42.45 49.98 43.72 46.19 46.78 27.22 48.24 36.67 49.21 55.37 42.61 50.60 52.96 52.36 50.68 50.64 51.50 55.29 52.54 51.20 54.55 53.08 50.29 51.61 52.56 52.28 54.34 53.12 53.59 31.41 55.77 40.15 56. 55.10 38.11 48.74 51.01 50.79 49.14 49.68 52.54 54.34 52.49 52.02 53.24 49.28 49.46 51.71 50.83 53.27 51.85 49.86 47.80 34.93 57.80 39.15 58.24 Table 10: Main results on SciRecipe-Eval at level 2. Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 32.00 10.31 15.34 24.48 22.78 21.21 24.16 35.27 33.64 25.80 21.32 31.57 12.06 29.37 28. 20.33 24.90 20.39 28.06 12.35 17.94 41.16 10.43 44.10 37.00 20.33 27.49 32.64 33.16 31.84 30.09 36.61 29.71 27.30 29.41 36.76 27.46 35.60 32.29 38.83 32.42 35.58 34.18 26.24 12.82 40.27 15.40 40. 44.88 30.12 38.46 41.13 42.04 39.74 41.59 44.30 44.75 42.40 40.79 45.64 39.73 42.44 41.46 42.89 42.41 42.74 43.47 39.88 21.45 46.66 30.12 47.46 41.44 29.81 38.26 38.44 38.65 36.10 43.43 35.77 40.62 40.43 41.61 42.24 39.71 39.35 40. 34.10 42.59 36.78 41.55 41.98 20.81 44.66 32.85 45.65 48.87 35.87 45.25 45.98 47.74 45.57 46.61 46.09 48.80 47.04 44.71 50.20 46.89 45.98 46.50 47.21 45.05 49.38 47.26 47.07 24.12 50.60 37.17 51. 42.00 27.79 36.57 39.64 40.71 38.42 39.89 41.09 41.16 38.54 37.97 43.29 35.34 40.05 39.60 39.08 39.70 38.30 40.43 36.12 23.48 44.67 29.68 46.21 35.82 22.96 31.31 34.55 37.38 33.17 32.94 29.67 35.13 32.82 31.84 38.48 29.31 32.20 31. 34.36 33.44 31.17 29.90 28.92 21.65 39.28 27.85 41.21 67.82 53.05 67.74 67.68 68.13 66.09 68.04 68.37 67.55 64.74 66.19 69.54 64.07 64.70 66.91 65.91 67.92 67.10 69.27 66.00 47.52 69.18 59.12 70. 9.12 3.75 7.17 8.47 9.30 6.84 8.14 9.46 6.51 5.54 5.58 9.53 4.23 8.13 7.49 5.91 6.78 6.16 9.44 5.16 3.57 8.47 1.62 10.43 61.01 43.87 58.15 63.38 67.18 65.21 64.04 64.24 63.70 60.76 60.32 65.64 54.56 62.71 61. 62.18 61.83 55.43 60.75 57.46 41.48 61.74 52.59 63.85 29 Table 11: Main results on SciRecipe-Eval (Overview). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Reasoning Models DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 32.36 27.85 30.99 30.08 30.72 28.87 28.96 26.85 34.73 30.22 28.70 35.24 27.34 31.99 30.92 31.65 30.56 32.19 27.78 28.48 28.05 37.49 30.46 40.15 72.47 65.51 70.02 72.64 71.52 70.62 71.83 71.67 71. 69.32 68.69 73.23 67.29 71.19 72.21 70.90 70.29 72.43 72.80 71.01 62.52 71.74 65.41 72.10 13.33 9.59 10.01 16.00 14.52 12.67 12.00 12.01 10.00 10.00 8.67 15.54 10.01 14.67 12.00 12.00 12.42 12.52 12.67 8. 8.02 11.34 2.68 9.35 72.02 64.64 68.01 71.33 68.38 73.67 75.35 76.34 76.67 75.35 70.70 71.01 64.68 79.36 74.67 69.67 64.01 67.45 70.70 72.00 62.00 71.67 62.01 71.01 30.67 16.29 11.33 30.67 24.69 26.00 26.67 34.67 36. 27.35 21.33 37.16 20.00 32.00 36.67 27.34 26.86 29.30 31.33 20.64 24.00 38.34 10.67 36.35 34.24 23.26 21.35 31.42 31.65 31.85 28.04 37.87 28.11 24.89 26.67 35.33 24.75 36.59 30.90 40.45 30.53 36.06 31.80 25. 13.46 38.02 11.60 37.75 42.36 33.20 32.70 38.94 39.70 39.04 38.99 42.05 42.12 39.97 37.29 43.43 36.10 40.78 39.20 42.47 39.60 42.33 40.20 37.21 24.50 42.61 26.80 43.27 37.24 30.29 32.98 35.62 36.02 33.00 38.83 30.53 36. 37.00 37.33 39.58 36.71 35.59 37.88 32.09 40.02 36.12 37.84 38.26 25.40 38.55 31.97 39.09 44.03 35.30 38.10 41.20 42.32 41.57 41.38 41.36 44.08 42.17 39.26 43.86 39.61 40.62 41.18 43.34 40.21 45.54 40.75 41. 27.38 43.79 32.14 44.79 42.08 33.99 35.05 40.88 39.95 39.70 40.23 41.48 42.17 39.59 37.63 43.82 36.28 42.53 41.74 41.10 39.39 41.55 40.65 38.07 30.59 43.73 30.42 43.76 Table 12: Main results on SciRecipe-Eval (Specific). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 48.67 24.26 18.67 37.33 35.95 30.67 30.00 52.00 46.67 43.33 46.31 39.33 35.33 39.33 38. 40.00 37.72 34.22 34.23 26.64 41.33 54.34 26.00 54.33 36.26 19.21 23.93 33.11 35.76 31.72 27.89 37.91 27.73 28.58 30.86 33.72 30.02 35.22 32.87 39.22 34.00 35.45 33.31 24.97 16.21 42.41 15.92 42. 45.07 27.31 36.23 40.55 40.69 40.34 40.09 44.14 45.37 43.82 42.41 43.54 42.74 43.75 41.68 42.88 43.73 43.69 43.55 39.31 28.38 48.91 32.10 48.19 41.29 24.67 36.46 35.55 33.60 36.03 41.84 35.84 40.97 41.51 41.47 40.02 40.32 40.18 38. 33.83 42.28 37.98 40.49 41.27 26.77 45.34 35.22 44.89 46.81 28.01 39.56 42.88 44.92 43.34 42.08 43.69 46.94 44.89 43.07 46.49 46.92 45.16 45.07 44.65 43.09 48.54 45.53 44.71 29.94 51.32 37.55 50. 46.00 27.40 37.69 42.18 42.64 41.93 42.41 45.43 44.86 44.19 43.88 44.29 42.69 43.26 43.74 44.28 44.59 43.27 42.33 38.69 33.68 51.22 35.41 50.80 33.63 18.99 27.99 36.39 37.78 36.67 35.48 30.96 36.91 36.45 32.02 37.84 32.81 35.87 36. 32.25 34.28 37.94 31.77 29.05 23.34 45.21 28.54 45.50 70.95 48.00 71.05 66.46 67.09 68.25 71.68 71.96 71.18 68.45 70.81 70.37 68.77 67.13 69.21 70.05 71.43 68.65 70.89 67.67 59.81 75.14 67.34 73. 24.00 12.59 20.67 20.67 20.18 20.00 23.33 26.00 23.33 23.33 23.49 17.99 22.67 19.99 20.67 26.67 21.95 16.07 18.12 15.27 19.33 28.68 14.00 27.34 67.32 43.55 64.67 66.67 67.81 70.33 69.33 66.33 64.67 67.33 64.45 69.34 64.67 62.70 70. 68.99 72.81 66.90 63.10 59.33 58.00 69.67 62.00 70.34 30 Table 13: Main results on SciRecipe-Eval (Retrieval). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Reasoning Models DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 51.78 39.77 54.81 53.10 58.97 48.78 45.71 43.40 51.18 50.77 55.90 57.83 47.62 42.10 45.13 44.54 52.84 44.04 50.51 51.73 21.50 59.35 32.20 54.51 80.87 69.06 81.86 80.66 82.42 75.45 78.66 78.01 81. 82.11 80.23 80.95 80.48 72.88 79.94 75.14 81.15 76.50 80.38 77.50 43.11 85.47 62.75 84.64 42.00 18.72 40.00 40.00 39.75 36.00 40.00 34.00 46.00 44.00 38.00 42.83 38.00 28.54 38.00 27.97 42.12 30.33 32.00 33. 18.00 45.98 24.00 47.99 73.99 48.16 72.00 78.00 72.07 71.00 70.00 67.00 78.00 70.00 70.00 69.48 78.00 59.18 74.00 72.97 70.74 68.00 78.00 66.67 38.00 76.34 54.00 75.65 60.00 29.02 52.00 48.00 45.36 48.00 56.00 54.00 58. 70.00 59.97 53.06 44.00 44.89 48.00 36.00 54.50 40.67 42.00 35.39 22.00 66.34 36.00 67.66 46.27 24.88 42.55 44.13 42.26 38.37 38.62 39.83 34.33 35.23 43.30 46.00 41.26 38.83 41.45 42.01 39.45 39.83 41.74 35. 12.84 50.29 19.32 48.55 58.29 41.52 56.07 55.60 56.09 50.88 52.35 54.14 58.01 56.68 56.28 59.70 55.77 50.29 52.77 51.71 54.61 51.44 55.37 51.02 21.80 59.62 35.11 59.76 53.51 41.95 54.05 50.88 50.66 47.87 55.49 47.25 51. 53.80 54.85 55.70 52.12 49.29 50.93 44.71 55.06 43.08 51.96 51.68 20.96 57.24 35.19 59.03 61.29 51.04 61.58 60.91 61.08 56.35 59.20 57.64 59.62 60.13 59.04 62.66 59.97 54.95 58.59 56.55 57.88 57.84 60.02 60. 24.10 64.06 39.29 64.92 58.67 40.46 57.21 56.81 56.52 52.52 55.11 52.81 57.65 58.08 57.51 58.69 55.02 48.99 54.31 50.18 56.48 50.19 54.44 51.46 24.70 62.74 37.54 62.52 Table 14: Main results on SciRecipe-Eval (Planning). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 60.00 10.72 26.00 36.00 53.24 28.00 36.00 62.00 54.00 50.00 39.99 46.00 32.00 44.00 52. 42.01 52.72 45.69 42.00 23.97 28.00 60.34 38.00 63.66 42.65 21.93 32.32 35.42 39.58 33.95 35.37 39.11 32.97 32.45 33.99 40.97 34.02 34.96 35.83 42.10 36.60 39.96 37.93 31.74 14.34 44.60 21.65 46. 53.16 35.86 45.75 47.67 50.28 44.78 50.58 49.19 51.46 51.16 47.24 52.75 46.87 45.09 48.26 50.88 50.29 51.50 49.47 47.89 23.85 52.66 38.76 54.26 49.96 36.76 47.28 44.29 44.81 42.62 51.43 43.81 49.50 48.38 48.51 49.20 48.50 44.00 47. 45.40 52.12 44.76 48.84 50.66 22.56 49.67 41.38 52.95 56.70 44.45 52.21 52.51 55.88 50.99 54.52 53.34 57.75 54.94 51.82 57.95 56.85 49.95 51.41 54.77 52.12 56.70 54.52 57.41 26.90 57.45 46.75 61. 55.28 34.12 44.13 45.92 53.18 45.33 48.79 49.81 49.51 48.64 46.65 51.87 45.23 42.41 47.57 50.24 51.40 50.27 47.57 43.39 28.26 55.59 38.49 57.23 44.20 27.41 33.96 38.06 51.68 40.29 41.38 34.52 43.08 39.57 31.86 45.53 37.09 33.79 33. 43.05 40.66 39.19 35.64 34.12 28.50 48.05 30.02 50.64 76.84 62.34 75.62 73.34 76.84 76.34 75.86 73.33 72.84 73.26 74.41 76.42 71.76 63.95 71.64 75.01 77.72 76.44 75.73 70.76 48.20 79.19 63.84 79. 34.01 8.68 22.00 22.00 23.69 26.00 26.00 24.01 22.00 24.01 22.00 21.98 18.00 11.96 26.00 23.97 33.78 34.63 20.00 15.94 13.99 32.00 12.00 27.98 79.99 58.95 62.00 64.00 82.66 65.00 68.00 68.99 62.00 64.00 69.99 76.01 61.99 54.00 62. 74.98 66.59 63.54 64.00 58.00 48.00 76.34 54.01 77.66 31 Table 15: Main results on SciRecipe-Eval (Troubleshooting). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Reasoning Models DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 24.46 17.86 26.45 24.92 20.24 26.76 22.05 22.53 22.66 18.12 26.43 30.92 26.48 18.65 23.10 23.93 26.57 19.08 18.78 24.50 17.53 35.15 18.46 29.89 62.76 35.51 65.98 63.21 54.78 59.99 61.06 67.37 62. 61.45 66.89 65.57 57.03 62.63 60.74 62.64 68.38 61.60 66.89 54.81 52.83 67.81 57.11 70.37 10.00 0.30 8.00 3.98 0.35 6.00 10.00 20.00 8.00 8.00 11.97 5.97 0.00 5.97 1.98 7.97 9.97 5.30 4.00 -0. 8.00 11.98 6.00 13.99 53.99 16.24 46.00 52.00 56.48 58.99 52.00 55.00 55.97 56.00 62.00 47.98 46.00 52.00 53.98 54.97 64.20 35.62 48.00 47.97 40.00 58.31 46.00 75.65 26.00 1.82 16.00 9.99 -0.12 31.98 16.00 50.00 56. 34.00 19.97 32.00 0.00 29.97 16.00 20.00 31.28 19.26 16.00 1.94 22.00 50.33 25.98 53.66 32.80 10.31 28.80 23.83 16.77 27.13 25.36 33.61 29.08 25.78 26.47 33.12 22.57 28.69 24.85 31.06 29.55 27.40 29.11 20. 16.22 40.65 16.00 40.53 41.80 20.29 39.98 37.34 30.78 36.19 37.48 40.67 40.45 39.30 38.58 43.59 35.45 36.64 36.66 39.30 40.59 36.75 40.40 34.55 24.78 44.95 29.18 44.68 40.10 26.51 38.21 38.26 34.10 36.58 40.39 35.77 39. 37.42 40.38 42.50 36.83 37.14 39.49 37.12 42.84 35.81 40.04 38.84 24.04 41.82 29.89 42.02 47.80 29.46 46.97 45.72 40.49 45.68 45.48 43.73 47.33 44.15 43.66 49.35 46.09 43.03 44.33 47.28 42.84 44.64 46.53 45. 29.37 52.04 35.00 51.19 37.75 17.55 35.15 33.25 28.21 36.59 34.42 40.96 40.27 36.02 37.37 39.00 30.05 34.97 33.46 36.03 39.90 31.72 34.42 29.91 26.09 44.78 29.29 46.89 Table 16: Main results on SciRecipe-Eval (Constraint). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 53.98 12.59 44.00 33.97 42.74 31.97 33.99 41.99 53.99 46.00 39.97 35.99 23.97 55.97 31. 28.00 49.11 38.31 40.00 17.94 21.97 64.31 31.97 59.66 39.94 18.55 37.21 32.52 29.59 33.88 33.30 37.64 32.62 31.00 32.22 38.49 29.09 38.27 30.80 37.76 33.61 39.79 37.58 30.23 9.76 42.72 19.66 44. 50.87 33.55 50.27 46.00 43.40 44.40 47.75 49.48 50.31 46.22 45.24 49.54 43.66 48.68 45.14 47.20 48.02 50.63 49.99 47.49 16.13 50.53 34.50 52.68 48.34 35.68 47.02 41.46 43.25 39.63 46.43 39.95 47.20 43.65 46.91 46.21 44.23 47.82 44. 41.49 50.49 43.57 48.57 47.97 15.51 48.19 34.38 51.19 55.32 42.92 54.26 53.92 51.95 50.71 51.12 51.36 55.84 52.61 51.63 55.19 51.98 51.67 51.35 55.16 52.35 57.57 55.81 54.09 18.47 54.34 42.67 58. 51.93 33.03 49.62 47.70 46.88 43.01 46.61 47.39 51.82 44.93 45.88 47.48 39.49 49.67 45.22 44.65 48.07 47.75 47.83 43.82 18.83 52.28 34.45 53.96 40.34 19.47 33.40 48.20 40.64 36.61 32.36 32.72 40.38 33.26 35.41 35.01 18.19 36.13 35. 38.65 30.66 37.78 29.63 34.13 14.71 35.26 25.59 48.86 74.56 61.94 78.43 73.30 70.68 68.91 74.58 74.37 76.12 67.67 71.54 72.93 70.31 72.50 74.28 68.66 72.92 73.55 74.94 74.69 34.91 74.85 57.32 76. 30.00 6.35 30.00 17.97 19.38 11.99 25.97 26.00 31.97 20.00 19.97 17.97 12.00 21.97 21.97 15.97 24.84 26.73 21.99 13.91 8.00 29.98 12.00 27.99 73.99 66.20 72.00 82.00 80.29 68.97 74.00 73.00 77.97 64.00 70.00 75.98 62.00 74.00 71. 68.97 70.64 61.81 72.00 73.97 30.00 70.31 52.00 65.65 32 Table 17: Main results on SciRecipe-Eval (Scaling). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Reasoning Models DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 74.95 51.76 62.62 63.92 65.47 53.67 56.34 59.81 65.70 62.21 58.53 65.90 47.77 58.56 62.16 50.18 67.04 48.13 60.64 65.13 29.58 68.87 36.87 74.83 77.87 70.97 80.70 78.50 82.42 77.64 78.80 81.55 81. 79.42 79.31 83.85 79.96 74.66 76.45 78.14 78.41 79.31 77.34 80.59 40.68 85.46 60.84 87.69 38.00 31.97 44.00 32.00 46.70 36.00 32.00 44.00 40.00 34.00 33.97 43.97 34.00 25.97 38.00 30.58 27.72 37.75 28.00 35. 18.00 47.98 18.00 53.99 71.99 72.24 70.00 72.00 77.39 71.00 78.00 77.00 82.00 76.00 74.00 75.98 70.00 72.00 70.00 78.60 72.64 78.98 70.00 72.00 40.00 88.31 60.00 93.65 50.00 43.83 50.00 46.00 51.12 40.00 34.00 52.00 50. 46.00 41.97 60.00 50.00 37.97 52.00 32.65 36.69 39.74 44.00 41.94 26.00 68.34 26.00 69.66 51.36 34.89 45.85 44.63 51.38 39.74 38.05 41.74 37.65 35.97 45.93 50.00 44.23 43.69 44.62 44.83 46.19 46.50 46.62 42. 16.51 54.25 20.87 55.02 62.12 49.73 59.93 56.48 63.26 54.01 54.83 61.12 61.83 58.81 57.84 63.04 56.22 56.50 59.76 55.15 59.32 57.48 58.80 60.70 25.92 63.90 41.02 66.14 58.19 44.19 57.32 52.89 58.90 49.62 59.07 54.51 55. 56.04 53.45 58.98 54.42 52.97 53.93 49.54 56.66 50.66 52.93 54.73 22.42 62.71 40.61 63.61 69.93 66.47 70.79 69.78 73.30 64.62 64.73 68.72 70.13 68.89 67.16 71.61 69.45 66.62 68.27 64.70 69.14 67.51 69.43 71. 30.26 72.65 51.26 74.15 61.60 51.78 60.13 57.36 63.33 54.03 55.09 60.05 60.52 57.48 56.91 63.70 56.23 54.33 58.35 53.82 57.09 56.23 56.42 58.40 27.71 68.05 39.50 70.97 Table 18: Main results on SciRecipe-Eval (Safety). Metrics left of the dashed line evaluate executability, those on the right measure semantic similarity. Bold denotes the best score. Methods Close-Source SOTA ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Claude Opus 4.1 Gemini 2.5 Flash Gemini 2.5 Pro Doubao-1.5-pro Qwen2.5-Max Open-Source SOTA Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct-2507 DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction"
        },
        {
            "title": "Reasoning Models",
            "content": "DeepSeek-R1 Grok 3 Grok 4 OpenAI o1 OpenAI o3 Qwen3-4B Thoth-mini Qwen3-8B Thoth Semantic-A Order-LCS Order-S Order-Tau Step-M BLUE-AVG ROUGE-L METEOR KW-F1 AVG 40.00 5.85 22.00 52.00 40.82 38.00 38.00 50.00 50.00 48.00 47.97 44.00 18.00 43.97 54. 34.00 52.96 41.69 44.00 25.94 30.00 44.34 24.00 49.66 42.88 17.75 32.27 36.76 34.47 34.50 37.95 38.65 35.55 33.76 35.26 42.43 33.16 37.17 36.86 40.83 37.65 38.51 39.85 31.53 15.56 46.04 19.86 48. 52.51 33.04 45.69 49.68 48.06 46.52 49.69 49.39 51.71 49.18 48.16 52.35 46.79 47.53 48.65 50.03 49.66 48.18 50.20 45.75 25.76 44.54 32.45 48.34 50.23 40.63 51.20 50.35 43.74 47.71 52.26 44.12 49.62 46.46 51.10 49.57 47.61 47.51 49. 42.26 50.45 41.98 48.51 49.49 25.40 45.62 33.62 47.94 61.04 45.77 55.65 57.72 55.58 53.76 57.53 54.94 60.03 54.86 54.38 60.15 55.11 53.52 55.30 55.53 55.99 55.22 56.41 56.15 31.22 51.69 39.52 52. 51.50 33.00 45.88 52.27 51.86 47.70 48.28 48.62 50.47 47.97 49.41 52.87 43.03 48.20 51.00 46.94 51.30 48.62 50.94 44.84 30.70 44.87 35.11 49.99 46.78 36.69 47.42 44.59 53.34 33.47 38.44 33.55 46.16 32.86 37.87 46.21 36.72 38.23 42. 40.79 36.51 42.15 43.03 42.60 26.45 36.58 26.54 43.52 76.08 57.09 72.72 77.33 77.43 72.38 72.67 73.96 71.19 72.62 73.96 77.12 71.92 71.94 74.65 74.12 75.50 74.32 74.45 70.17 55.88 62.74 62.01 67. 22.00 3.97 18.00 24.00 27.63 26.00 22.00 22.00 14.00 22.00 17.97 23.97 12.00 17.97 26.00 21.97 21.50 22.49 24.00 13.94 16.00 15.98 12.00 23.99 71.99 56.24 68.00 78.00 85.70 77.00 66.00 71.00 76.00 72.00 78.00 80.00 66.00 76.00 72. 62.97 81.49 73.08 78.00 68.00 50.00 56.33 66.00 67.65 33 J.3 OTHER RESULTS ON PROTOCOL BENCHMARK To further assess protocol comprehension, we evaluated Thoth on BioProBench sub-tasks, with results summarized in Table 19. Consistent with the findings in protocol generation, the Thoth series achieved substantial gains over baseline models, with average improvements of 28.02% and 20.41% for Thoth and Thoth-mini, respectively. Scaling effects were also more evident in this setting, as Thoth outperformed Thoth-mini by 6.98%, gap larger than in protocol generation. On individual tasks, Thoth showed clear advantages in error correction and protocol QA, exceeding GPT-5 by 17.05% and 12.09%, respectively, and consistently outperforming other strong baselines such as Claude Sonnet 4 and Gemini 2.5 Flash. These results confirm that Thoth generalizes well across protocol-related tasks. Nevertheless, because BioProBench partially overlaps with SciRecipe and does not disclose the exact origins of its QA pairs, there remains possibility of data leakage, and thus the reported results should be regarded as reference only. Table 19: Results on the protocol QA tasks."
        },
        {
            "title": "Methods",
            "content": "ChatGPT-4o GPT-5 GPT-5 Chat Claude Sonnet 4 Gemini 2.5 Flash Qwen2.5-Max Doubao-1.5-pro Qwen2.5-72B-Instruct Qwen3-235B-A22B-Instruct DeepSeek-V3 GPT-OSS-120B Llama-3.1-405B-Instruct Kimi-K2-Instruction Qwen3-4B Thoth-mini Qwen3-8B Thoth"
        },
        {
            "title": "ACC",
            "content": "F"
        },
        {
            "title": "EM K tau ACC",
            "content": "59.33 67.72 60.33 62.92 62.17 54.83 56.43 56.08 59.33 57.33 64.33 57.30 59.50 58.00 75.83 56.83 80.75 36.95 59.65 41.38 54.64 51.91 17.38 24.49 25.25 39.15 30.25 53.17 39.33 37.05 48.78 72.49 32.90 80.73 41.89 48.86 45.66 47.71 43.79 41.51 43.38 36.99 41.63 40.83 39.88 39.06 42.62 22.53 41.72 28.63 49. 68.05 73.44 71.71 72.31 73.12 67.96 70.86 62.46 69.18 70.56 68.08 65.42 71.70 51.82 65.90 56.12 71.92 60.75 70.58 60.75 67.08 64.47 62.25 63.00 60.42 62.58 62.83 64.83 57.67 63.00 47.08 74.33 50.58 82."
        },
        {
            "title": "K FUTHER ANALYSES",
            "content": "K.1 REWARD ANALYSES & RESULTS K.1.1 RL ALGORITHMS We conducted ablation experiments to assess the robustness of the SCORE mechanism under different RL algorithms, replacing the third-stage optimization algorithm while keeping earlier training stages consistent. As shown in Table 20, SCORE remained stable across variety of methods, confirming its reliability as reward mechanism. Executability-oriented metrics revealed that GRPO offered the best overall balance, benefiting from group-wise normalization that stabilized training and strengthened action fidelity. This supports our choice of GRPO as the primary optimization method in Thoth. K.1.2 REWARD COMPUTATIONS We further examined the impact of different reward computation strategies within the SCORE mechanism, considering the choice of order consistency logic (LCS or Strict), the method of combining order and semantic rewards (Product or Sum), and the integration of step scale with step semantics (Product or Sum). As summarized in Table 21, strict order scoring consistently outperformed LCS, product-based integration proved more effective than summation, and multiplicative incorporation of step scale with semantics yielded stronger executability. These trends collectively support Table 20: Further analysis results on RL algorithms. (a)"
        },
        {
            "title": "Algorithm",
            "content": "BERTScoreF1 BERTScoreP BERTScoreR BLEU-1 BLEU-2 GPG GRPO Dr GSPO REINFORCE ++-baseline ReMax RLOO Thoth (GRPO) 97.81 97.81 97.81 97.82 97.81 97.81 97.82 98.01 97.99 97.98 98.08 98.05 98.04 97."
        },
        {
            "title": "Algorithm",
            "content": "BLEU-3 BLEU-4 GPG GRPO Dr GSPO REINFORCE ++-baseline ReMax RLOO Thoth (GRPO) 34.44 34.44 33.50 34.71 33.59 33.60 34. 21.80 21.85 22.08 22.69 22.79 22.61 23.09 (b) (c) 97.62 97.63 97. 97.56 97.57 97.59 97.65 BLEUAVG 43.03 42.93 42.90 43.60 43.05 42.80 43. 68.56 68.10 68.65 68.83 67.93 67.35 68.56 47.30 47.31 47.36 48.18 47.87 47.64 48. ROUGE-1 ROUGE-2 59.98 60.31 60.40 59.92 59.96 60.41 60.30 36.58 36.89 37. 37.08 36.59 37.15 37."
        },
        {
            "title": "Algorithm",
            "content": "ROUGE-L"
        },
        {
            "title": "Meteor",
            "content": "KW-F1 KW-P KW-R GPG GRPO Dr GSPO REINFORCE ++-baseline ReMax RLOO Thoth (GRPO) 49.01 49.12 49.32 49. 49.52 50.00 50.02 53.47 53.98 54.20 53.59 53.47 53.65 54.13 45.81 47.14 46.91 46. 45.95 46.66 47.39 (d) 58.52 59.29 58.59 60.40 60.27 60.29 58.82 50.84 51.06 51. 49.71 49.56 49.94 51."
        },
        {
            "title": "Algorithm",
            "content": "Order-LCS Order-S Order-Tau Semantic-A Step-M GPG GRPO Dr GSPO REINFORCE ++-baseline ReMax RLOO Thoth (GRPO) 74.02 75.34 74.93 75.69 73.58 74.42 75.34 22.50 26.67 23.67 25.50 23.00 25.00 25. 70.33 71.50 68.67 70.67 69.33 70.17 73.33 45.07 44.11 43.93 44.14 44.89 43.87 46. 46.67 50.50 51.17 50.67 51.11 51.50 53.00 the adoption of the S-S-P configuration in Thoth, which achieved the best balance between surface similarity and procedural fidelity. K.1.3 REWARD MAGNITUDES To better understand the sensitivity of SCORE to the absolute scale of feedback, we examined different strategies for adjusting reward magnitude. Four configurations were compared: constant scaling, scaling up, symmetric shifting with equal positive and negative ranges, and the default Thoth setting. As shown in Table 22, model performance remained highly stable across all strategies, 35 Table 21: Further analysis results on Reward Computations. (a)"
        },
        {
            "title": "Combine",
            "content": "BERTScoreF1 BERTScoreP BERTScoreR BLEU-1 BLEU-2 L-P-P L-P-S L-S-P L-S-S S-P-P S-P-S S-S-S Thoth (S-S-P) 97.76 97.73 97.78 97.59 97.80 97.80 97.80 97.82 97.67 97.66 97.69 97.51 97.61 97.70 97.68 97.65 97.85 97.80 97.87 97.66 98.00 97.91 97.92 97.99 (b) 63.17 62.56 63.23 61.91 70.04 64.18 67.28 68.56 44.25 43.74 44.37 43.03 49.18 45.14 47.15 48."
        },
        {
            "title": "Combine",
            "content": "BLEU-3 BLEU-4 BLEU-AVG ROUGE-1 ROUGE-2 L-P-P L-P-S L-S-P L-S-S S-P-P S-P-S S-S-S Thoth (S-S-P) 31.31 30.95 31.32 30.36 35.18 32.20 33.73 34.58 39.87 39.39 39.86 38.77 44.46 40.71 42.66 43.62 20.74 20.30 20.54 19.79 23.44 21.34 22.49 23.09 (c) 58.81 58.86 59.24 58.49 59.52 59.33 60.08 60.30 35.40 35.31 35.82 34.62 36.48 36.05 36.56 37."
        },
        {
            "title": "Combine",
            "content": "ROUGE-L"
        },
        {
            "title": "Meteor",
            "content": "KW-F1 KW-P KW-R L-P-P L-P-S L-S-P L-S-S S-P-P S-P-S S-S-S Thoth (S-S-P) 48.17 47.23 48.09 46.85 49.40 48.47 48.47 50.02 53.21 52.67 53.50 52.83 53.35 53.62 53.56 54. 45.93 46.11 46.08 45.63 45.57 46.37 46.54 47.39 (d) 55.78 54.75 56.30 54.64 59.23 56.23 57.36 58.82 52.72 52.47 52.78 53.05 50.02 53.10 51.65 51."
        },
        {
            "title": "Combine",
            "content": "Order-LCS Order-S Order-Tau Semantic-A Step-M L-P-P L-P-S L-S-P L-S-S S-P-P S-P-S S-S-S Thoth (S-S-P) 71.30 69.70 71.82 71.52 75.21 72.57 74.61 75.34 19.50 16.17 20.83 18.00 25.17 21.00 23.67 25.50 65.50 64.00 66.67 65.00 71.50 66.50 69.17 73.33 45.02 45.65 45.18 45.47 44.78 45.23 45.41 46.60 32.33 34.50 35.33 33.83 50.00 34.50 50.00 53.00 with only minimal variations observed on both text-level and executability-oriented metrics. This confirms that the effectiveness of SCORE arises mainly from its structured reward design rather than the precise numerical range, and also suggests that it can be applied robustly to more complex open-ended tasks where reward scales may differ. 36 Table 22: Further analysis results on Reward Magnitudes. (a)"
        },
        {
            "title": "Reward Scale",
            "content": "BERTScoreF1 BERTScoreP BERTScoreR BLEU-1 BLEU-2 Constant [0, 2.5] Scaling up [0, 5] Shift [-1.25, 1.25] Thoth [0, 1] 97.80 97.78 97.79 97.82 97.98 97.92 97.98 97.99 (b)"
        },
        {
            "title": "Reward Scale",
            "content": "BLEU-3 BLEU-4 Constant [0, 2.5] Scaling up [0, 5] Shift [-1.25, 1.25] Thoth [0, 1] 33.60 33.63 33.57 34.58 22.00 22.31 22.82 23.09 (c) 97.62 97.64 97.60 97.65 BLEUAVG 42.72 42.74 42.90 43.62 67.79 67.74 67.76 68.56 47.50 47.27 47.45 48.28 ROUGEROUGE-2 60.06 59.97 60.11 60.30 36.38 36.24 36.69 37."
        },
        {
            "title": "Reward Scale",
            "content": "ROUGE-L"
        },
        {
            "title": "Meteor",
            "content": "KW-F1 KW-P KW-R Constant [0, 2.5] Scaling up [0, 5] Shift [-1.25, 1.25] Thoth [0, 1] 49.06 49.07 49.15 50.02 53.92 53.02 53.70 54. 58.80 56.72 58.67 58.82 51.30 51.28 51.11 51.70 46.12 46.01 46.75 47.39 (d)"
        },
        {
            "title": "Reward Scale",
            "content": "Constant [0, 2.5] Scaling up [0, 5] Shift [-1.25, 1.25] Thoth [0, 1] OrderLCS 74.71 73.92 73.70 75.34 Order-S Order-Tau SemanticA Step-M 25.17 26.17 26.50 25.50 71.50 72.33 72.49 73.33 45.22 45.14 45.36 46.60 51.50 51.67 52.12 53.00 K.2 PRE-TRAINING ANALYSIS Table 23: Performance differences on the MMLU dataset (Hendrycks et al., 2021) before and after pretraining. To examine potential catastrophic forgetting of general scientific knowledge, we evaluate MMLU on the OpenCompass platform (Contributors, 2023). pt denotes the Thoth model after the pretraining stage only, and indicates the performance gap between pretrained models and their base counterparts. The results clearly show that pretraining on large collection of high-quality protocols not only preserves general capability but even yields improvements in certain disciplines."
        },
        {
            "title": "Dataset",
            "content": "Version Qwen3-4B Thoth-mini pt Qwen3-8B Thoth pt lukaemon mmlu abstract algebra lukaemon mmlu anatomy lukaemon mmlu astronomy lukaemon mmlu business ethics lukaemon mmlu clinical knowledge lukaemon mmlu college biology lukaemon mmlu college chemistry lukaemon mmlu college computer science lukaemon mmlu college mathematics lukaemon mmlu college medicine lukaemon mmlu college physics lukaemon mmlu computer security lukaemon mmlu conceptual physics lukaemon mmlu econometrics lukaemon mmlu electrical engineering lukaemon mmlu elementary mathematics lukaemon mmlu formal logic lukaemon mmlu global facts lukaemon mmlu high school biology lukaemon mmlu high school chemistry lukaemon mmlu high school computer science lukaemon mmlu high school european history lukaemon mmlu high school geography lukaemon mmlu high school government and politics lukaemon mmlu high school macroeconomics lukaemon mmlu high school mathematics lukaemon mmlu high school microeconomics lukaemon mmlu high school physics lukaemon mmlu high school psychology lukaemon mmlu high school statistics lukaemon mmlu high school us history lukaemon mmlu high school world history lukaemon mmlu human aging lukaemon mmlu human sexuality lukaemon mmlu international law lukaemon mmlu jurisprudence lukaemon mmlu logical fallacies lukaemon mmlu machine learning lukaemon mmlu management lukaemon mmlu marketing lukaemon mmlu medical genetics lukaemon mmlu miscellaneous lukaemon mmlu moral disputes lukaemon mmlu moral scenarios lukaemon mmlu nutrition lukaemon mmlu philosophy lukaemon mmlu prehistory lukaemon mmlu professional accounting lukaemon mmlu professional law lukaemon mmlu professional medicine lukaemon mmlu professional psychology lukaemon mmlu public relations lukaemon mmlu security studies lukaemon mmlu sociology lukaemon mmlu us foreign policy lukaemon mmlu virology lukaemon mmlu world religions"
        },
        {
            "title": "AVG",
            "content": "2db373 72183b d3ee01 1dec08 cb3218 caec7d 520aa6 99c216 678751 38709e 4f382c ce7550 63588e d1134d 770ce3 269926 cfcb0c ab07b6 37b125 ae8820 9965a5 eefc90 0780e6 3c52f9 a01685 ed4dc0 04d21a 93278f 7db114 8f3f3a 8932df 048e7e 82a410 42407c cf3179 001f24 9cebb0 0283bb 80876d 7394e3 881ef5 935647 a2173e f6dbe2 4543bd 08042b bbb197 444b7f 5f7e6c 857144 221a16 e7d39b 9b1743 c266a2 528cfe 12e270 232c09 91.00 70.37 84.87 77.00 80.38 90.28 66.00 80.00 90.00 83.24 94.12 85.00 88.94 71.93 83.45 97.35 92.86 51.00 85.81 89.16 91.00 82.42 86.87 91.19 87.95 95.19 93.70 86.75 91.93 89.81 85.78 85.65 73.99 83.97 76.03 75.00 85.28 76.79 84.47 91.88 88.00 88.12 74.28 62.46 76.14 72.03 81.48 73.40 49.80 82.72 77.61 68.18 71.84 80.10 86.00 55.42 84.21 81.41 38 87.00 71.85 89.47 72.00 81.89 88.89 63.00 83.00 86.00 84.97 94.12 82.00 88.09 71.93 80.00 97.35 88.89 52.00 87.74 89.66 92.00 81.82 91.92 90.67 86.92 95.19 93.70 86.75 92.84 89.35 87.75 82.70 71.75 81.68 78.51 79.63 84.66 76.79 85.44 91.45 88.00 87.48 72.83 60.67 76.80 68.81 80.25 73.40 50.07 84.19 75.65 70.00 75.51 80.60 86.00 51.81 85.38 81. -4.00 1.48 4.60 -5.00 1.51 -1.39 -3.00 3.00 -4.00 1.73 0.00 -3.00 -0.85 0.00 -3.45 0.00 -3.97 1.00 1.93 0.50 1.00 -0.60 5.05 -0.52 -1.03 0.00 0.00 0.00 0.91 -0.46 1.97 -2.95 -2.24 -2.29 2.48 4.63 -0.62 0.00 0.97 -0.43 0.00 -0.64 -1.45 -1.79 0.66 -3.22 -1.23 0.00 0.27 1.47 -1.96 1.82 3.67 0.50 0.00 -3.61 1.17 -0.20 88.00 75.56 90.79 81.00 83.77 95.14 71.00 87.00 77.00 84.97 95.10 84.00 94.04 80.70 80.69 97.62 89.68 44.00 90.00 91.13 98.00 87.27 93.94 95.34 92.82 67.41 96.22 90.07 95.05 91.67 90.20 88.61 76.68 89.31 77.69 87.96 86.50 78.57 88.35 94.87 91.00 91.70 72.25 60.22 85.29 76.53 85.80 81.21 55.61 89.34 79.90 73.64 76.33 87.06 87.00 54.82 85.96 83.88 91.00 75.56 88.82 77.00 86.04 94.44 71.00 83.00 79.00 84.97 95.10 86.00 93.62 84.21 84.14 97.88 90.48 57.00 93.87 89.16 96.00 83.03 91.92 95.34 91.54 64.44 95.80 86.75 94.31 90.74 88.24 89.03 76.68 85.50 80.99 80.56 85.89 82.14 86.41 91.45 89.00 91.44 73.99 62.46 84.31 79.10 84.88 80.85 56.39 87.50 79.41 76.36 77.96 83.08 85.00 57.23 88.30 83. 3.00 0.00 -1.97 -4.00 2.27 -0.70 0.00 -4.00 2.00 0.00 0.00 2.00 -0.42 3.51 3.45 0.26 0.80 13.00 3.87 -1.97 -2.00 -4.24 -2.02 0.00 -1.28 -2.97 -0.42 -3.32 -0.74 -0.93 -1.96 0.42 0.00 -3.81 3.30 -7.40 -0.61 3.57 -1.94 -3.42 -2.00 -0.26 1.74 2.24 -0.98 2.57 -0.92 -0.36 0.78 -1.84 -0.49 2.72 1.63 -3.98 -2.00 2.41 2.34 -0."
        },
        {
            "title": "L CASE STUDIES",
            "content": "Figure 11: Case result of Thoth on the Retrieval task. 39 Figure 12: Case result of Thoth on the Planning task. 40 Figure 13: Case result of Thoth on the Troubleshooting task. Figure 14: Case result of Thoth on the Constraint task. 42 Figure 15: Case result of Thoth on the Scaling task. 43 Figure 16: Case result of Thoth on the Safety task."
        },
        {
            "title": "M PROMPTS",
            "content": "Figure 17: System prompt 1 for structured integration with model based. 45 Figure 18: System prompt 2 for structured integration with model based. 46 Figure 19: System prompt 3 for structured integration with model based. Figure 20: System prompt used for constructing SciRecipe. 48 Figure 21: Prompt used for defining each task. 49 Figure 22: Input prompt used for constructing SciRecipe. Figure 23: System prompt for the repair module. 51 Figure 24: System prompt for the scientific review module. 52 Figure 25: Input prompt for the scientific review module."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}