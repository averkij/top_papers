{
    "paper_title": "OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System",
    "authors": [
        "Yujie Luo",
        "Xiangyuan Ru",
        "Kangwei Liu",
        "Lin Yuan",
        "Mengshu Sun",
        "Ningyu Zhang",
        "Lei Liang",
        "Zhiqiang Zhang",
        "Jun Zhou",
        "Lanning Wei",
        "Da Zheng",
        "Haofen Wang",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE's efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code at https://github.com/zjunlp/OneKE and released a Video at http://oneke.openkg.cn/demo.mp4."
        },
        {
            "title": "Start",
            "content": "OneKE: Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System Xiangyuan Ru Kangwei Liu Zhejiang University ZJU-Ant Group Joint Research Center for Knowledge Graphs Hangzhou, China Lin Yuan Mengshu Sun ZJU-Ant Group Joint Research Center for Knowledge Graphs Ant Group Hangzhou, China Yujie Luo luo.yj@zju.edu.cn Zhejiang University ZJU-Ant Group Joint Research Center for Knowledge Graphs Hangzhou, China 4 2 0 2 8 2 ] . [ 1 5 0 0 0 2 . 2 1 4 2 : r Ningyu Zhang* Zhejiang University ZJU-Ant Group Joint Research Center for Knowledge Graphs Hangzhou, China Lei Liang Zhiqiang Zhang ZJU-Ant Group Joint Research Center for Knowledge Graphs Ant Group Hangzhou, China Jun Zhou* ZJU-Ant Group Joint Research Center for Knowledge Graphs Ant Group Hangzhou, China Lanning Wei Da Zheng ZJU-Ant Group Joint Research Center for Knowledge Graphs Ant Group Hangzhou, China Haofen Wang Tongji University Shanghai, China Huajun Chen* Zhejiang University ZJU-Ant Group Joint Research Center for Knowledge Graphs Hangzhou, China Abstract We introduce OneKE, dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKEs efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code1 and released Video2. Keywords Information Extraction; Natural Language Processing; Large Language Models 1https://github.com/zjunlp/OneKE 2http://oneke.openkg.cn/demo.mp4 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, June 0305, 2018, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX ACM Reference Format: Yujie Luo, Xiangyuan Ru, Kangwei Liu, Lin Yuan, Mengshu Sun, Ningyu Zhang*, Lei Liang, Zhiqiang Zhang, Jun Zhou*, Lanning Wei, Da Zheng, Haofen Wang, and Huajun Chen*. 2018. OneKE: Dockerized SchemaGuided LLM Agent-based Knowledge Extraction System. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym XX). ACM, New York, NY, USA, 4 pages. https: //doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nKnowledge extractionâ€“obtaining knowledge from data, is a critical\ncomponent for a wide range of practical systems such as Knowl-\nedge Graph (KG) construction [1], Retrieval Augmentation (RAG)\n[3], and domain-specific applications like scientific discovery [2]\nand intelligence analysis [7]. The last decades have witnessed the\ndevelopment of various knowledge extraction systems [5, 9, 10]. In\nparticular, with the emergence of Large Language Models (LLMs),\nnew works such as InstructUIE [6], iText2KG [4] and AgentRE [8]\nhave been continuously emerged. However, previous approaches\nstill struggle to effectively extract information from raw data fol-\nlowing complex schemas and face challenges in debugging and\ncorrecting errors when they occur.",
            "content": "Note that previous efforts have primarily focused on the capabilities of individual models while neglecting the design of comprehensive system to address the knowledge extraction task. To this end, we introduce OneKE, dockerized schema-guided knowledge extraction system. We adopt multi-agent design with configure knowledge base to provide knowledge extraction support for various scenarios and error debugging, aiming to meet the practical needs of users as much as possible. Following [6], Conference acronym XX, June 0305, 2018, Woodstock, NY Luo et al. Figure 1: The overview of the OneKE system, supporting various domains (science, news, etc.) and data (Web HTML, PDF, etc.). we design three agents: Schema Agent for schema analysis with various data types, Extraction Agent for extracting knowledge with various LLMs, and Reflection Agent to debug and handle erroneous cases. Based on this design, OneKE can efficiently process source texts of varying lengths and formats, such as HTML and PDF, and demonstrates robust capability to adapt to diverse task configurations, yielding comprehensive range of output schemas tailored to specific requirements. We evaluate OneKE using two benchmark datasets for Named Entity Recognition (NER) and Relation Extraction (RE), demonstrating the effectiveness of our framework. Furthermore, to explore the versatility of OneKE in practical applications, we conduct case analyses on specific extraction tasks. These scenarios encompass extracting structured information from the Web news articles and raw PDF book chapters, highlighting OneKEs capability to manage diverse data formats and varying task contexts effectively. This flexible framework, which operates without the necessity of finetuning, is adept at swift adaptation to forthcoming LLMs, thereby amplifying their capabilities and elevating their overall efficacy."
        },
        {
            "title": "2 Design and Implementation\nOneKE is thoughtfully designed to address the complexities and\nchallenges inherent in knowledge extraction. As shown in Figure 1,\nthe framework is guided by several key considerations that enhance\nits functionality and adaptability in real-world scenario:",
            "content": "(1) Adaptability to real-world data. Real-world information extraction tasks often handle raw data, like HTML, PDF, etc. Based on this, the OneKE framework supports variety of data types rather than pure text. We also reserve user-defined interface to support new data types in the future. (2) Generalization for complex schemas. Practical knowledge extraction scenarios should handle diverse and complex schemas, or even no schema. Thus, we design the OneKE-specific Schema Agent to support both pre-defined schemas and self-schema deduction using LLMs. OneKE also supports various LLMs, including LLaMA, Qwen and ChatGLM, as well as proprietary models like GPT-4, enabling effective knowledge extraction. (3) Debugging and fixing errors. Most previous works require retraining the model when encountering error cases. In contrast, we integrate Case Repository into OneKE to equip the model with reflective and error-correcting capabilities, enabling its continuous improvement in knowledge extraction tasks."
        },
        {
            "title": "2.2 Extraction Agent\nUpon receiving the unified output schema from the Schema Agent,\nwe design the Extraction Agent to utilize LLMs for extracting knowl-\nedge, thereby generating the preliminary extraction results. Specif-\nically, this module supports a variety of models, including locally\ndeployed open-source models such as LLaMA, Qwen, and Chat-\nGLM, as well as API services like OpenAI and DeepSeek. To enhance\nperformance, the Extraction Agent learns from similar cases and\napplies this knowledge to the extraction process. Relevant cases\nare retrieved from the Case Repository using semantic similarity",
            "content": "OneKE: Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System Conference acronym XX, June 0305, 2018, Woodstock, NY combined with string matching (via the all-MiniLM-L6-v2 model and the FuzzyWuzzy package, we set Top two as default). These cases are then incorporated as few-shot examples into the original context to form the prompt, after which the LLM is called to obtain the extraction results. After the above steps, we obtain the preliminary extraction results; however, errors often occur. To address this, we design reflection agent to debug and fix these errors. We use self-consistency to filter out the cases where the model is uncertain and pass these cases to the Reflection Agent."
        },
        {
            "title": "2.4 Configure Knowledge Base\nThe Configure Knowledge Base provides essential information for\nthe three agents, including manually defined schemas for various\ntasks, and historical extraction cases, enhancing the performance\nand error correction capabilities:\nSchema Repository. In the OneKE system, the Schema Repository\nprovides the pre-defined schema for the Schema Agent, supporting\nthe subsequent extraction process. Specifically, the Schema Repos-\nitory includes pre-defined output schemas for NER, RE, and EE\ntasks, along with templates for various data scenarios such as scien-\ntific academic papers and news reports. The schemas in the Schema\nRepository are structured as Pydantic objects, enabling seamless\nserialization into JSON format for the Extraction Agent. Moreover,\nthis structure allows users to customize new schemas within the\nrepository, thus enhancing adaptability and extensibility.\nCase Repository. To enable debugging and error correction in\nOneKE, we design the Case Repository, which primarily stores\ntraces of past knowledge extraction cases. This repository sup-\nports the Extraction Agent in performing extractions and assists\nthe Reflection Agent in reflecting on and correcting errors. Specif-\nically, knowledge extraction cases stored in Case Repository can\nbe divided into two categories: Correct Cases and Bad Cases. Cor-\nrect Cases provide the Extraction Agent with reasoning steps of\nsuccessful extraction, while Bad Cases offer the Reflection Agent\nwarnings about avoidable mistakes. The Case Repository will be au-\ntomatically updated once a knowledge extraction task is completed.\nConcretely, this module first generates reasoning steps derived\nfrom the correct answer, storing both the correct answer and its\nreasoning steps in the Correct Case Repository to enhance task\nunderstanding. Additionally, the agent compares its answer with",
            "content": "the correct one and reflects on its original response to identify potential issues. It then stores the original answer along with the corresponding reflections in the Bad Case Repository for future reference. Figure 2: Performance of different components in OneKE."
        },
        {
            "title": "4 Application\nIn practical applications, the OneKE framework supports diverse\ndata formats (HTML, PDF, Word), accommodating both short and\nlong contexts for seamless integration into various downstream\napplications. We provide case analyses in the following two repre-\nsentative extraction scenarios.\nWeb News Extraction. In the news domain, OneKE enhances the\nknowledge extraction of Web news content, thereby facilitating\ndownstream tasks such as effective sentiment monitoring, proactive\nrisk management, as well as a variety of additional applications. As\nillustrated in Figure 3, the extraction task starts with Extracting key\ninformation from news articles on a randomly selected raw HTML\npage from the web, aiming to identify the overall nature of the\nnews and obtain structured key insights. After parsing the HTML-\nformatted text, the Schema Agent first identifies its domain and\ngenre as a Politics News Report, offering crucial guidance. Utilizing\nthis metadata, the Schema Agent generates a structured Output\nSchema in code format that effectively captures the key information",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Luo et al. Figure 3: Using OneKE on Web News Extraction and Book Knowledge Extraction. of the news. Once the Output Schema has been serialized into JSON format description, the Extraction Agent and the Reflection Agent collaborate to undertake subsequent extraction and reflective optimization tasks. This cooperative effort of the agents culminates in JSON output that captures the key information and structure of the news report. Book Knowledge Extraction. Another application of OneKE lies in extracting structured knowledge from extensive corpora, including books, documents, or manuals. Specifically, we use the first chapter of the Harry Potter series (with total length of 17 pages in PDF format) as the target text. The extraction focuses on the main characters and the background setting within this chapter. As shown in Figure 3, the generated Output Schema accurately identified the two target extraction objects: main_characters and background_setting. Subsequently, with the collaboration of the Extraction Agent and the Reflection Agent, OneKE successfully extracted relevant information."
        },
        {
            "title": "5 Conclusion and Future Work\nIn this paper, we introduce OneKE, a Dockerized Schema-Guided\nLLM Agent-based Knowledge Extraction System. OneKE is de-\nsigned for flexible application across a spectrum of extraction tasks\nin real-world scenarios. It can handle source texts of varying lengths\nand formats (such as HTML and PDF) while demonstrating the ca-\npability to adapt to diverse task configurations, generating a broad\nrange of output schemas tailored to specific requirements. More-\nover, the integration of a self-reflection mechanism enables iterative\nimprovement informed by external feedback, thereby enhancing\nboth accuracy and adaptability.\nLong-term Maintenance. We will maintain OneKE over the long\nterm, adding new features and fixing bugs. OneKE is built on a\nmodular architecture that promotes extensibility, and we plan to\nexpand the Configure Knowledge Base by integrating additional\ndomain-specific knowledge from fields such as science, thereby",
            "content": "broadening its applicability across domains. To advance document data extraction and comprehension, we plan to develop methodologies for the integration and analysis of diverse chart types and content. We hope OneKE can serve as helpful tool for researchers and engineers engaging in knowledge extraction with LLMs. References [1] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In WWW. [2] John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew Rosen, Gerbrand Ceder, Kristin Persson, and Anubhav Jain. 2024. Structured information extraction from scientific text with large language models. Nature Communications 15, 1 (2024), 1418. [3] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997 (2023). [4] Yassir Lairgi, Ludovic Moncla, RÃ©my Cazabet, Khalid Benabdeslem, and Pierre ClÃ©au. 2024. iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models. CoRR abs/2409.03284 (2024). arXiv:2409.03284 [5] Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. 2023. Evaluating ChatGPTs Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. arXiv preprint arXiv:2304.11633 (2023). [6] Yuchen Shi, Guochao Jiang, Tian Qiu, and Deqing Yang. 2024. AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024. ACM. [7] Nan Sun, Ming Ding, Jiaojiao Jiang, Weikang Xu, Xiaoxing Mo, Yonghang Tai, and Jun Zhang. 2023. Cyber threat intelligence mining for proactive cybersecurity defense: survey and new perspectives. IEEE Communications Surveys & Tutorials 25, 3 (2023), 17481774. [8] Xiao Wang, Weikang Zhou, Can Zu, et al. 2023. InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction. CoRR abs/2304.08085 (2023). arXiv:2304.08085 [9] Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2024. Chatie: Zero-shot information extraction via chatting with chatgpt. arXiv preprint arXiv:2302.10205 (2024). [10] Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, and Enhong Chen. 2024. Large language models for generative information extraction: survey. Frontiers of Computer Science 18, 6 (2024), 186357."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Tongji University",
        "ZJU-Ant Group Joint Research Center for Knowledge Graphs",
        "Zhejiang University"
    ]
}