{
    "paper_title": "Step-DeepResearch Technical Report",
    "authors": [
        "Chen Hu",
        "Haikuo Du",
        "Heng Wang",
        "Lin Lin",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Tianchi Yue",
        "Wang You",
        "Wei Ji",
        "Wei Yuan",
        "Wenjin Deng",
        "Xiaojian Yuan",
        "Xiaoyun Zhang",
        "Xiangyu Liu",
        "Xikai Liu",
        "Yanming Xu",
        "Yicheng Cao",
        "Yifei Zhang",
        "Yongyao Wang",
        "Yubo Shu",
        "Yurong Zhang",
        "Yuxiang Zhang",
        "Zheng Gong",
        "Zhichao Chang",
        "Binyan Li",
        "Dan Ma",
        "Furong Jia",
        "Hongyuan Wang",
        "Jiayu Liu",
        "Jing Bai",
        "Junlan Liu",
        "Manjiao Liu",
        "Na Wang",
        "Qiuping Wu",
        "Qinxin Du",
        "Shiwei Li",
        "Wen Sun",
        "Yifeng Gong",
        "Yonglin Chen",
        "Yuling Zhao",
        "Yuxuan Lin",
        "Ziqi Ren",
        "Zixuan Wang",
        "Aihu Zhang",
        "Brian Li",
        "Buyun Ma",
        "Kang An",
        "Li Xie",
        "Mingliang Li",
        "Pan Li",
        "Shidong Yang",
        "Xi Chen",
        "Xiaojia Liu",
        "Yuchu Luo",
        "Yuan Song",
        "YuanHao Ding",
        "Yuanwei Liang",
        "Zexi Li",
        "Zhaoning Zhang",
        "Zixin Zhang",
        "Binxing Jiao",
        "Daxin Jiang",
        "Jiansheng Chen",
        "Jing Li",
        "Xiangyu Zhang",
        "Yibo Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency."
        },
        {
            "title": "Start",
            "content": "Step-DeepResearch Technical Report Agent-Team StepFun December 24, 2025 Abstract As Large Language Models (LLMs) shift toward autonomous agents, Deep Research has emerged as pivotal metric for assessing the core competitiveness of agents. However, existing works primarily focus on academic multi-hop search tasks with ground truth, such as BrowseComp, which often struggle to satisfy user demands for open-ended research tasks in real-world scenarios. Open-ended research not only demands robust retrieval capabilities but also challenges the agents comprehensive skills in latent intent recognition, long-horizon decision-making, multi-turn tool use, logical structuring, and cross-source verification. To address this, we introduce Step-DeepResearch, cost-effective, end-to-end deep research agent model. We propose novel Data Synthesis Strategy Based on Atomic Capabilities, aimed at reinforcing underlying capabilities in planning, information gathering, reflection, and report writing. In terms of the training paradigm, we construct progressive path from agentic mid-training to supervised fine-tuning and reinforcement learning. Combined with Checklist-style Judger reward design, this approach significantly improves robustness across diverse scenarios. Furthermore, to address the lack of evaluations reflecting real-world demands in the Chinese domain, we establish ADR-Bench, Chinese benchmark for realistic deep research scenarios. Experimental results demonstrate that Step-DeepResearch, with only 32B parameters, achieves high score of 61.4% on the Scale AI Research Rubrics. In expert human evaluations on ADR-Bench, its Elo score significantly outperforms comparable models and rivals state-of-the-art closed-source models such as OpenAI DeepResearch and Gemini DeepResearch. These findings prove that through refined training scheme, medium-sized models can achieve expert-level deep research capabilities. With extremely low deployment and inference costs, Step-DeepResearch stands as the most cost-effective deep research agent model currently available in the industry. 5 2 0 D 3 2 ] . [ 1 1 9 4 0 2 . 2 1 5 2 : r (a) (b) Figure 1: Comprehensive Evaluation of Step-DeepResearch. (a) Cost-Efficiency on Research Rubrics: Step-DeepResearch achieves near-top performance (61.42) while significantly reducing inference costs (RMB), positioned at the high-efficiency frontier. (b) Expert Evaluation on ADR-Bench: Step-DeepResearch consistently leads in Elo ratings across all dimensions, rivaling top-tier closed-source models."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) are rapidly pushing AI systems beyond conversational interaction toward autonomous agents. In this broader shift, Deep Research, defined as the ability of AI systems to address open-ended, long-horizon, and highly complex information-seeking tasks, has become useful North Star capability for general-purpose agents. Recent systems such as OpenAI Deep Research [1], Gemini Deep Research [2], and Kimi-Researcher [3] illustrate the potential of agentic information acquisition, while also revealing clear limitations. Search is not research. Despite rapid industrial progress, there remains substantial gap between benchmark performance and real-world usefulness. core reason is mismatch in task formulation: search is not equivalent to research. Search typically targets well-specified queries with closed-form answers and optimizes for retrieval accuracy. Research, in contrast, is an iterative process that requires intent decomposition, planning, effective tool use, cross-source verification, and synthesis into structured report. In practice, multi-hop question answerLimits of multi-hop QA as an evaluation driver. ing is common lens for evaluating agent systems, and it often becomes the de facto optimization target. This pressure biases agents toward retrieval-heavy behavior. They look more like efficient web crawlers that collect scattered facts at scale, rather than researchers who integrate evidence into coherent and defensible argument. As result, information fragmentation, broken reasoning chains, and hallucinations under noisy evidence remain major obstacles to deploying Deep Research systems in practice. Our perspective. We reframe Deep Research as long-horizon decision-making over set of atomic capabilities, including adaptive planning, information gathering and cross-source verification, reflection and error correction, and creative writing. From this perspective, improving usability is less about assembling external components, and more about training models to internalize an expert-like cognitive loop, so they can self-check and revise as the task unfolds. Step-DeepResearch. Guided by this principle, we introduce Step-DeepResearch, an endto-end framework that aims to enable robust and practical autonomous research primarily through native model capabilities. Our contributions are: Atomic-capability data synthesis. We decompose Deep Research into trainable atomic capabilities and synthesize targeted data for agentic mid-training, covering domain knowledge, high-level planning, behavioral reflection, information summarization, and cross-source verification. To mitigate the scarcity of high-value reasoning data, we further develop posttraining data synthesis pipeline grounded in knowledge graphs and expert trajectories. This design aims to improve the information density and logical structure of training data, and to reduce missing-capability issues in conventional synthetic datasets. Progressive training pipeline. We establish practical optimization path from agentic mid-training to supervised fine-tuning, and then to reinforcement learning. Using atomiccapability data, we reshape the objective from predicting the next token to deciding the next atomic action, which empirically improves robustness in complex environments and strengthens generalization across tasks. Application-driven evaluation suite. While existing benchmarks (e.g., Browsecomp [4] and GAIA [5]) are informative, they often fall short of capturing real user needs. Moreover, high-quality Chinese benchmarks for Deep Research remain limited. We therefore build ADRBench (Application-driven Deep Research Benchmark), spanning commercial research, policy analysis, and software engineering. ADR-Bench combines an Elo-style rating protocol with multi-dimensional quality criteria to better connect automated metrics with human-perceived usefulness. 2 With 32B parameters, Step-DeepResearch shows strong performance relative to its scale. On ADR-Bench, it achieves high usability in end-to-end research. Unlike many deep research systems that rely on complex multi-agent coordination or heavyweight workflows, Step-DeepResearch benefits from internalized atomic capabilities, using only streamlined ReAct-style single-agent design. In expert-based Elo ratings, it not only outperforms larger models like MiniMax-M2 [6], GLM-4.6 [7], and DeepSeek-V3.2 [8], but also surpasses cutting-edge deep research systems such as Kimi-Researcher [3] and MiniMax Agent [6]. On the Research Rubrics Benchmark [9], Step-DeepResearch achieved 61.4% rubric compliance under ternary grading, performing at level comparable to OpenAI Deep Research [1] and Gemini Deep Research [2], and significantly outperforming range of open-source and proprietary models. This work demonstrates that models with medium-scale parameters can also achieve expert-level deep research capabilities. As shown in Figure 1, with its low deployment and inference costs, Step-DeepResearch has become the most cost-effective deep research system."
        },
        {
            "title": "2.1 Workflow-based Implementation of Deep Research",
            "content": "Current mainstream deep research systems employ general foundation models, achieving research capabilities through external workflow orchestration. OpenAI Deep Research [1] combines the o3-mini reasoning model with multi-step web exploration, adopting asynchronous task management. Gemini Deep Research [2] introduces dynamic research blueprints and interactive plan refinement, leveraging Gemini 2.0 Flash Thinkings self-reflection capabilities and 1-million-token context window. Claude Research [10] adopts an agentic approach to conduct multiple mutuallybuilding searches, automatically exploring different angles of question. Perplexity Deep Research [11] integrates Bing-style indexing with the Sonar API, combining BM25 and dense vector reranking. On the open-source framework front, LangChain Open Deep Research [12] provides plan-and-execute architecture that identifies knowledge gaps through self-reflection. DeepResearchAgent [13] employs hierarchical multi-agent system. Together AI Open Deep Research [14] generates search queries through initial planning and leverages LLMs to assess knowledge gaps. However, such systems essentially hardcode predefined workflow patterns into the system architecture, which imposes high requirements on system complexity. For agents implemented using approaches like ReAct, the research depth in specialized scenarios is often insufficient, failing to meet users actual needs."
        },
        {
            "title": "2.2 End-to-End Optimization of Research Capabilities",
            "content": "Unlike orchestration-based systems, some works internalize relevant capabilities into models through end-to-end training. DeepResearcher [15] conducts end-to-end RL training via GRPO in real web environments, demonstrating emergent cognitive behaviors such as planning, crosssource verification, and self-reflection. Kimi-Researcher [3] supports long-horizon multi-turn search reasoning through end-to-end agentic RL training, employing context management mechanisms and asynchronous rollout systems. Tongyi DeepResearch [16] proposes unified training paradigm for agentic mid-training and post-training, combining automated data synthesis pipelines with on-policy RL. Related agent training methodology works such as WebRL [17], which proposes autonomous curriculum learning and KL-constrained policy updates, and SearchR1 [18], which adopts RL-enhanced search reasoning integration, have made progress in this area. Nevertheless, existing end-to-end works still primarily focus on improving search efficacy (e.g., retrieval accuracy, query optimization), lacking systematic construction strategies for atomic core capabilities crucial to in-depth research, such as long-horizon logical reasoning, multi-source cross-validation, and high-quality report composition. Furthermore, how to optimize model size 3 and inference cost while maintaining high performance remains an urgent challenge for the industry."
        },
        {
            "title": "2.3 Deep Research Related Evaluation Benchmarks",
            "content": "In terms of evaluation, ResearchRubrics [9] contains 101 domain-diverse research tasks, each equipped with 20-43 expert-written fine-grained scoring criteria, assessing factual accuracy, reasoning soundness, and clarity. DeepResearch Bench [19] comprises 100 PhD-level research tasks spanning 22 domains, evaluating report quality through the RACE framework and information retrieval capabilities through the FACT framework. ReportBench [20] reverse-engineers research questions based on expert survey papers on arXiv, assessing the citation accuracy and factual consistency of generated reports. However, these academic datasets for deep research, except for ResearchRubrics, remain insufficient in terms of task coverage comprehensiveness and evaluation depth, failing to adequately cover the diverse requirements of real research scenarios. Another category of benchmarks focuses more on retrieval and knowledge testing capabilities. BrowseComp [4] contains 1,266 fact-finding questions requiring multi-hop reasoning, directly targeting the evaluation of models ability to retrieve deeply hidden information. HLE (Humanitys Last Exam) [21] includes 2,500 expert-level questions across multiple disciplines, yet still carries strong closed-book exam flavor, largely focusing on multi-hop search with definitive answers or frontier knowledge testing, failing to fully reach the openness and multidimensionality of real industrial-grade research scenarios. To this end, we construct ADR-Bench, aiming to fill the evaluation gap for Chinese deep research scenarios driven by real user demands.Claude is AI and can make mistakes. Please double-check responses."
        },
        {
            "title": "3 Data Strategy: Constructing Atomic Capabilities",
            "content": "The core challenge of the Deep Research Agent lies in bridging the decision-making gap between pre-training and task-specific optimization. During the pre-training phase, the model acquires vast world knowledge and language distributions; however, the post-training phase requires guiding the model to perform complex, long-horizon reasoning within massive action space. Direct exploration within the raw token-level space Atoken is not only computationally expensive but also prone to trapping the model in local optima due to the exponential growth of the branching factor with sequence length. Therefore, we proposes unified data construction perspective: reshaping the training objective from predicting the next token to deciding the next Atomic Action. We define Atomic Capabilities as set of transferable, high-level action abstractions that form compact action subspace Aatomic Atoken. We formalize the goal of data construction as finding an optimal action subspace that minimizes two types of errors simultaneously: min(ϵpruning + ϵRL) where ϵpruning represents the approximation error caused by pruning potential optimal solutions, and ϵRL represents the difficulty of performing subsequent planning or reinforcement learning within this subspace. To achieve Pareto optimality between retention of key skills and clarity of planning logic, we do not construct datasets in isolation. Instead, we establish query synthesis pipelines and trajectory generation strategies specifically around four core atomic capabilities: Planning & Task Decomposition, Deep Information Seeking, Reflection & Verification, and Reporting."
        },
        {
            "title": "3.1 Capability I: Planning & Task Decomposition",
            "content": "Planning capability requires the model to effectively decompose ambiguous or broad user requests into executable sub-tasks and to dynamically adjust its route based on environmental feedback. To achieve this, we adopt Reverse Engineering strategy, utilizing existing perfect planning results from the real world to synthesize high-complexity planning data. Furthermore, to prevent tasks from being too trivial, we implemented rigorous screening process for task queries. 3.1.1 Reverse Engineering Synthesis To obtain planning data that covers multiple domains and possesses authentic logical depth, we leverage high-quality documents such as open access technical reports, academic surveys, and financial research reports. These documents are essentially the final output of complex research tasks and contain implicit planning logic. Specifically, we first take the title and abstract (or the original text) of report and remove specific experimental details and result data. We then prompt an LLM to reverse-engineer the initial Project Task that could have led to this report, thereby generating high-difficulty queries that simulate real-world scientific and business scenarios. Simultaneously, utilizing the abstract structure as form of Hindsight, we synthesize high-level Plan that guides the entire research process. This ensures that the generated planning path possesses extremely high feasibility and logic, serving as strong constraint for the model during the inference phase. Finally, to ensure data quality, we apply trajectory consistency filtering to the generated data. We generate execution trajectories for the agent and calculate their alignment with the preset Plan, filtering out trajectories that complete the task but deviate significantly from the preset Plan. This ensures the model learns an execution process that strictly conforms to the known Hindsight."
        },
        {
            "title": "3.2 Capability II: Deep Search & Information Seeking",
            "content": "Deep information seeking capability differs from simple QA; it requires the model to possess capabilities for Multi-hop Reasoning, mining hidden entities, and active topological walking when information is incomplete. We specifically strengthen this capability through graph-based and multi-document synthesis pipelines. 3.2.1 Graph-based Synthesis To construct questions requiring complex reasoning paths, we performed controlled subgraph sampling on open-source knowledge graphs such as Wikidata5m [22] and CN-DBpedia [23]. We adopt specific topology construction strategy. First, we screen non-generic entities with small degrees (3-10) as seed nodes to avoid starting points that are too isolated or too broad. Subsequently, we perform BFS expansion centered on the seed to build subgraph containing 10-40 nodes. To prevent semantic drift, we enforce truncation on super nodes with degrees exceeding threshold (e.g., 1000), treating them as leaf nodes. Given that triplets in knowledge graphs are often lossy and sometimes out-dated, we do not generate questions directly using triplets. Instead, for every edge in the subgraph, we use the triplet as query to perform an additional search, verifying and faithfully expanding the triplets information. Finally, based on this verified and structurally sound subgraph, we prompt an LLM to generate fuzzed complex question requiring multi-hop search and reasoning, along with its corresponding answer. 3.2.2 Multi-document-based Synthesis Addressing the capability of associative retrieval between documents, we utilized custom index library, Wiki-doc. Leveraging its natural hyperlink structure, we start from random entity and use Few-shot Prompt to guide Web Search Agent to perform Topology Walk within the document index. The Agent is required to mine information by following hyperlinks without prior knowledge of the target, continuing until maximum number of steps is reached or sufficient information is collected. Finally, we consolidate all node information collected along the path to reversegenerate Query, Answer pairs. 3.2.3 Difficulty Filtering To ensure the non-triviality of planning tasks, we use QwQ-32b [24] as difficulty filtering model. Any Query that the QwQ-32b model can solve under the default ReAct framework is considered simple problem and is excluded from the training set. Since QwQ-32b shares the same base model and pre-training knowledge as the model used in this work, but lacks extensive agentspecific training, tasks solvable by QwQ-32b can be regarded as simple tasks that do not require specialized training."
        },
        {
            "title": "3.3 Capability III: Reflection, Verification & Cross-Validation",
            "content": "In long-horizon reasoning, the model must possess the ability to identify its own errors (SelfCorrection) and distinguish the authenticity of internet information (Fact-Checking). We designed specialized closed-loop pipelines to produce such data. 3.3.1 The Error-Reflection Loop For deep information seeking queries, we employ closed-loop process of Expert Model Generation Result Verification Multi-turn Reflection to produce high-quality thought trajectories. This process aims to improve the models anti-interference ability and cross-validation level in complex retrieval environments through introspection on failed paths. The specific synthesis pipeline is as follows: First, an expert-level model generates preliminary search and reasoning trajectory. If the final output matches the answer, the trajectory is included directly in the training set as positive sample. If the result does not match, we construct Prompt asking the model to perform reflection action based on the incorrect result. Based on the reflection conclusion, the model retains historical memory and retries the trajectory generation; this process iterates up to 3 times. For trajectories that eventually reach the correct answer after reflection, we perform specialized post-processing cleaning. We remove phrases with traces of artificial induction, such as according to user hints, ensuring the output appears as the models completely spontaneous introspection and error-correction process. This strategy not only strengthens the ability to gather unknown information but also teaches the model to filter unreliable internet data by comparing multiple sources, significantly reducing factual bias in the final report. 3.3.2 Deep Verification Workflow To strengthen the factual rigor of the model in complex research tasks, we cleaned thousands of paragraph, judge-result pairs from desensitized real data as seed samples. We constructed Multi-Agent Teacher Workflow to simulate the verification process of human experts, recording the complete execution path as an Agent Trace. The system consists of the following collaborative atomic Agents: 6 Extract Agent: Analyzes input materials and performs factual decomposition. It extracts time, location, subjects, core data, and causal events from natural language descriptions, converting them into independent Verification Points. Plan Agent: Generates preliminary action plan. It analyzes the necessity of verification and the required source type for each Verification Point produced by the Extract Agent, generating plan based on logical dependencies. Verify Agent: Executes specific verification actions. It calls search tool and models to perform multi-source retrieval and cross-validation on the content. Replan Agent: Executes dynamic path adjustment. It summarizes currently acquired information and adjusts the research direction in timely manner, significantly reducing redundant searches and improving decision quality under complex paths. Report Agent: Aggregates evidence from the entire process. It provides clear final conclusion (Support, Refute, or Doubtful) for each Verification Point, accompanied by complete citation evidence. Generated verification trajectories undergo strict posterior filtering. We input the [Verification Point, Conclusion, Evidence] triplet into Judger model to verify if the report conclusion is completely logically self-consistent with the evidence, ensuring the model learns verification paradigm that is both factually accurate and logically rigorous."
        },
        {
            "title": "3.4 Capability IV: Report Generation",
            "content": "Report writing is not merely text generation; it is process of structured reorganization of collected fragmented information. We divide the training of this capability into Mid-training phase, focusing on domain style and content depth, and an SFT phase, focusing on instruction following and formatting specifications. 3.4.1 Mid-training: Domain Style and Content Depth In the Mid-training phase, the goal is for the model to internalize expert-level writing frameworks and terminological styles. We constructed large-scale Query, Report pairs. The data originates from strictly screened high-quality human reports (such as financial research reports and indepth surveys), while the Queries are obtained through the aforementioned reverse engineering method. In this stage, the model focuses on learning how to organize language, cite data, and develop in-depth arguments like an expert within the context of given Project Task, without focusing on the specific search process. 3.4.2 SFT: Instruction Following and Formatting Specifications In the SFT phase, the focus shifts to Instruction Following regarding user-specific constraints and Planning consistency. For Deep Research Queries with metadata containing Plan, we require the report generated by the model to strictly follow the preset Plan structure. We generate trajectories and perform alignment checks with the Plan, filtering out samples that deviate. For samples with high trajectory generation quality but poor report formatting, we employ specialized System Prompt to regenerate the report based on the final round state, ensuring the final output is not only detailed in content but also precisely responsive to user needs in terms of instruction following and formatting."
        },
        {
            "title": "4 Training Pipeline",
            "content": "A progressive three-stage training pipeline is adopted on top of 32-billion-parameter base model, consisting of mid-training, supervised fine-tuning (SFT), and reinforcement learning (RL), which are applied sequentially to systematically enhance the models overall performance in complex reasoning and long-horizon agent tasks. clear division of responsibilities across capability expansion, behavior alignment, and objective optimization is established by this training design. In particular, high-quality atomic capability data are introduced during the mid-training stage, including planning and task decomposition ability, deep search and information seeking ability, reflection and verification ability, and report generation ability. In the subsequent SFT stage, these atomic capabilities are composed, while explicit constraints and alignment are imposed on instruction following, output structure, and task-specific formatting. Finally, task-level feedback signals are incorporated in the RL stage, through which the behavioral quality of the model in realistic interactive and decision-making scenarios is further optimized. Qwen2.5-32B-Base [25] is selected as the base model to achieve balance among performance, computational cost, and experimental reproducibility. Through this choice, nearlargescale core capabilities and long-context support are retained, while the barrier to multi-stage training and systematic ablation studies is substantially reduced. Consequently, the performance improvements observed in this work can be more directly attributed to the proposed training paradigm and data strategies, ensuring clearer interpretability and reproducibility. As medium-scale model, core capabilities comparable to those of 72B models such as strong instruction following and logical reasoning are exhibited by Qwen2.5-32B-Base, together with support for up to 128k context length, making it well suited for long-horizon agent tasks. Meanwhile, the 32B-parameter scale significantly lowers training costs, and strong model plasticity has been demonstrated in prior fine-tuning efforts. By using Qwen2.5-32B-Base as the base model, it is ensured that the performance gains reported in Deep Research scenarios are objectively derived from the data strategies and training paradigm, thereby facilitating community reproduction and fair comparison."
        },
        {
            "title": "4.1 Stage 1: Agentic Mid-Training",
            "content": "To systematically enhance the models capabilities in long-context understanding, knowledge integration, and tool-augmented reasoning for complex tasks without significantly increasing the parameter scale, we introduce two-stage mid-training process between pre-training and instruction fine-tuning. This stage is designed to progressively adapt the model to more complex and longer-sequence tasks through carefully constructed data distributions and context-length scheduling, enabling the emergence of stable medium to long-horizon reasoning as well as task decomposition and execution capabilities. Overall, mid-training follows curriculum that progresses from shorter to longer contexts and from pure knowledge-based tasks to tool-augmented tasks, and is divided into two stages: the first stage supports maximum context length of 32K, while the second stage extends this capacity to 128K. Mid-training (32K Context). The first-stage mid-training focuses on injecting atomic capabilities such as planning and task decomposition, information seeking, reflection and verification, and report generation. This stage aims to substantially enhance the models ability to understand and execute complex task structures under medium-length contexts. Rather than directly optimizing for task completion rates, this phase emphasizes the systematic construction of high-quality capability-oriented data, enabling the model to form stable and generalizable intermediate representations across key dimensions including planning, evidence integration, and self-correction. These representations serve as solid foundation for subsequent training stages 8 involving longer contexts and real-world interactive agent tasks. The training data in this stage primarily covers capability dimensions such as planning, information seeking, reflection, and report generation. The detailed composition is shown in Table 1. Table 1: Data Composition of Mid-training (32K Context) Data Source Data Category Active reading general knowledge data Wiki, Baidu Baike QA, Rewrite Academic articles QA, Rewrite Active reading academic data Pleias Synth1 Synthetic knowledge data In-house / mixed Summarization data In-house / mixed Multi-step reasoning text Reasoning data In-house / mixed Reflection data Primary Format Self-checking and correction Capability Focus Information seeking Information seeking Report generation Planning and task decomposition Reflection and verification Synthetic text, structured QA, reasoning traces, multilingual Report generation + information seeking Document summary Among these, the academic active reading data simulates the models realistic cognitive process when reading long-form academic documents, guiding it to learn how to identify critical information from large volumes of text and to form effective internal knowledge representations through question-answering and rewriting tasks. Explicit tool invocation is not introduced at this stage, ensuring that the model develops robust comprehension and reasoning capabilities under pure textual conditions. Figure 2: Performance trends during mid-training (32K context) across increasing training token scales. Checkpoints are saved approximately every 5 billion tokens. The figure reports the average performance on SimpleQA, TriviaQA, and FRAMES benchmarks. As the training progresses, consistent performance improvements are observed across all benchmarks, with particularly significant gains on FRAMES, indicating enhanced agent-related and structured reasoning capabilities. Notably, the curves have not yet fully converged at 150 billion tokens, suggesting further potential for improvement. As shown in Fig. 2, during training, checkpoints are saved approximately every 5 billion tokens, and performance gains are analyzed over the full 150-billion-token mid-training process in Stage I. The results indicate that as the training data scale increases, the model exhibits consistent performance improvements on the SimpleQA, TriviaQA, and FRAMES benchmarks. The maximum observed gains reach approximately +1.26% on SimpleQA, +2.30% on TriviaQA, 1https://huggingface.co/datasets/PleIAs/SYNTH and +10.88% on FRAMES. Overall, the 32K mid-training stage has not yet fully converged at the 150-billion-token scale, suggesting substantial room for further improvement, particularly in agent-related and structured reasoning capabilities. Mid-training (128K Context). Building upon the stable convergence of capabilities achieved in Stage I, the second stage of mid-training further extends the maximum context length to 128K, with primary focus on real-world complex task scenarios. This stage strengthens the models abilities in retrieval, planning, and tool-augmented reasoning under ultra-long contexts. The training data in this stage is more closely aligned with practical applications and covers complex structures such as web interaction, search processes, and multi-tool collaboration. The detailed data composition is summarized in Table 2. Table 2: Data Composition of Mid-training (128K Context) Data Category URL QA data Deep Search data Web browsing data Planning data Summarization data Reasoning data Reflection data General dialogue data High-quality dialogue corpora Multi-turn dialogue Data Source In-house / mixed In-house Agent-data-collection [26] In-house / mixed In-house / mixed In-house / mixed In-house / mixed Primary Format URL + QA Search steps + tool calls Web navigation + tool calls Task planning + tool calls Long document / long dialogue summarization Report generation Long-horizon dialogue reasoning Reflection and correction processes Capability Focus Information seeking + planning and task decomposition Information seeking + planning and task decomposition Information seeking + planning and task decomposition Planning and task decomposition Planning and task decomposition Reflection and verification Maintaining general language interaction ability By introducing explicit tool invocation at this stage, the model is required not only to generate coherent intermediate reasoning processes, but also to learn when and how to select and invoke external tools, and to integrate the returned results into subsequent reasoning. This significantly enhances the models practical effectiveness in agent-style tasks. To more clearly illustrate the design differences and the progressive relationship between the two mid-training stages, key dimensions are compared in Table 3."
        },
        {
            "title": "4.2 Stage 2: Post-training Supervised Fine-tuning (SFT)",
            "content": "During the mid-training stage, the model is equipped with fundamental atomic capabilities such as planning and information seeking. In the post-training supervised fine-tuning (SFT) stage, the focus shifts away from isolated capability teaching toward the composition of these atomic abilities to improve end-to-end performance on long-horizon tasks. The core objective of this stage is domain adaptation and performance enhancement: by leveraging rigorously cleaned, high-quality full-chain trajectories, the models existing atomic capabilities are systematically connected to form efficient, robust, and behavior patterns deeply aligned with the requirements of Deep Research scenarios. SFT Data Composition. The SFT dataset primarily consists of two categories of end-to-end task trajectories: Deep Search and Deep Research. Deep Search data. This category focuses on multi-hop search tasks with well-defined groundtruth answers. Although portion of deep search trajectories is already introduced during the mid-training stage to establish basic retrieval and reasoning capabilities, the SFT stage emphasizes performance optimization and stylistic diversity. High-quality trajectories with uniformly distributed queries are selected, and diverse reasoning patterns are introduced to enable the model to flexibly choose optimal reasoning paths for different queries. This results in qualitative transition from merely being able to retrieve correct information to doing so both efficiently and accurately. Deep Research data. This category targets comprehensive research tasks involving openended questions. The data covers the full pipeline of intent understanding, planning, information cross-verification, and report generation under strict formatting requirements (e.g., 10 Table 3: Comparison of the Two Mid-training Stages Dimension Maximum context length Primary objective Data focus Tool calling included Task complexity Application scenarios Mid-training Stage 32K Mid-training Stage II 128K Knowledge injection/Basic comprehension Retrieval/Planning/Tool-augmented reasoning Encyclopedic/Academic/Cognitive patterns No Medium Understanding/Reasoning/Simple QA Web/Search/Tool invocation Yes High Deep search/Agent scenarios citation and attribution). Through this design, the entire end-to-end logic of intent analysisplanningexecutionreflectionwriting is reinforced. Data Construction and Cleaning Strategies. The effectiveness of SFT is highly dependent on data quality. To this end, refined data pipeline is constructed with strict rules and algorithmic filtering to ensure high-quality supervision. Representative strategies include: Trajectory efficiency optimization. For Deep Search tasks with ground-truth answers, correct and shortest principle is applied during data cleaning. Among all successful trajectories, only those with the fewest reasoning steps and the most concise tool usage are retained. This encourages the model to replace redundant search behaviors with effective reasoning and to acquire information at minimal cost, thereby eliminating unnecessary tool invocations. Robustness and noise control. controlled proportion of trajectories containing tool-call errors is intentionally retained, such as empty search results or tool failures followed by correct reflectioncorrection actions. This form of structured noise injection prevents the model from collapsing under real-world web instability and equips it with self-correction capabilities for handling exceptional cases. Cognitive pattern deduplication. strict -grambased deduplication mechanism is applied to identify and remove low-quality trajectories with excessive repetition or degenerate loops. This ensures diversity and flexibility in the models reasoning behavior during longhorizon tool usage. Strict citation and factual alignment. To meet the rigor required for Deep Research reports, citation formats using cite{} are explicitly incorporated into the SFT data. This construction guides the model to adopt writing paradigm in which relevant references are appended at critical information points, ensuring traceability and factual grounding, and aligning the output format with professional research standards."
        },
        {
            "title": "4.3 Stage 3: Reinforcement Learning (RL)",
            "content": "In the first two stages, the training process primarily relied on large-scale synthetic and distilled data for mid-training and supervised fine-tuning (SFT). Although this approach is effective for instruction-following tasks and basic tool usage, it mainly depends on teacher trajectories and cannot be improved through trial-and-error learning with real-world interactions. Therefore, we introduces reinforcement learning (RL), connecting the model directly to the real tool usage environment and optimizing it through interactions with the environment. Unlike short-term searches focused on entity matching, the quality of deep research reports depends on multiple dimensions, including task decomposition, planning, tool invocation strategy, evidence selection and validation, and the final report generation. recent approach is to transform report quality evaluation into set of rubric-based scores, which are then used as optimization signals [27]. This rubric-based reinforcement learning enhances the models abilities in active planning, reflection, and cross-source validation, overcoming the limitations of offline imitation and significantly improving both performance and user experience. 11 4.3.1 RL Data Synthesis Although small-scale benchmarking can rely on expert-defined rubrics [9], the reinforcement learning training still requires data synthesis process due to the difficulty and high cost of collecting high-quality rubrics. Two-Step Reverse Synthesis. To generate deep research tasks and their corresponding rubrics, we employ two-step reverse synthesis approach. Traditional forward extraction methods typically derive rubrics by rephrasing or decomposing user queries. However, these methods often depend on surface-level expressions and may overlook key quality dimensions implicit in the task. Therefore, we designed more refined synthesis process: In the first step, guided by small number of high-quality examples and templates, we use powerful LLM to generate an initial task description (referred to as the hidden task summary) and concurrently generate set of fine-grained rubrics. Each rubric is defined as an atomic standard containing evaluation dimensions and importance weights, ensuring that the description is specific, verifiable, and practically applicable in task assessment. Furthermore, each rubric is assigned role indicating whether it is an explicit, implicit, or negative requirement, corresponding to whether it needs to be explicitly stated in the task or serves as bonus or penalty item. In the second step, we generate the target task (i.e., the actual user task query) based on the generated rubrics. During this process, we simultaneously re-assess the role of each rubric for the synthesized task and provide brief reasoning and attribution. If any discrepancies are found between the initial and final role assignment, the entire synthesis sample is discarded. This ensures that the synthesized task queries align with the rubrics requirements, semantically support the key dimensions in the rubric, and cover both explicit and implicit task requirements. Objective Consistency Verification. To ensure that the objectives and requirements of the task remain aligned during the reverse synthesis process, we implement an objective consistency verification step to check the consistency between the hidden task summary, rubrics, and the synthesized task. This process is performed by an independent judge model, which evaluates the match between the hidden task summary and the synthesized task, as well as the alignment of the rubrics with the tasks requirements. The judge model assesses the relevance of each rubric and ensures no contradictions with the task objectives. Finally, it provides an overall consistency score and decides whether the sample should be retained. This step ensures high-quality task samples and reliable supervision signals for subsequent reinforcement learning training. 4.3.2 Reward Design In the deep research report scenario, relying directly on expensive Rubrics Judge Training. large LLMs to assign fine-grained scores based on the full rubrics for each report would require dozens of inferences per sample to cover all evaluation dimensions. This approach incurs prohibitively high computational costs and latency during large-scale agentic RL training. Based on our observation, the quality of the rubrics is often more critical in ensuring that the LLM Judges annotations align with human experts, rather than the absolute performance of the judge model itself. Thus, we first use strong model to generate scores and explanations on the constructed (Query, Rubrics, Report) triples, which then serve as supervisory signals for training the Rubrics Judge model. This model undergoes two training phase: supervised fine-tuning and reinforcement learning with verifiable rewards. The former aims to help the model learn the scoring logic and explanation style of the strong model, establishing foundational discriminative ability, while the latter further strengthens the consistency of the model output format and the robustness of the scoring logic. Ultimately, this Rubrics Judge serves as the primary reward provider in 12 this stage, allowing us to support large-scale agentic RL training within an acceptable budget without compromising signal quality. In our initial attempt to evaluate reports based on rubrics, we Strict Reward Mapping. categorized each rubrics judgment into three classes: fully satisfied, partially satisfied and not satisfied, mapping them directly to ternary judgment {1, 0.5, 0}. However, we found that the agreement between different strong models (acting as LLM judges) and between LLM judges and human experts was lower in the partially satisfied category. This misalignment in the intermediate category may led to unstable reward signals, making it difficult to provide consistent optimization directions and weakening the models ability to adhere to constraints. To ensure more distinguishable reward signals, we convert the ternary judgments into binary signals based on the nature of the rubrics: For positive rubrics: We map fully satisfied to 1 and unify partially satisfied and not satisfied as 0. This approach emphasizes the principle of no reward unless fully satisfied, preventing the model from exploiting low-quality generations. For negative rubrics: We map not satisfied to 0, while partially satisfied and fully satisfied are unified as 1. This ensures that any deviations from the desired outcome are penalized, helping the model to avoid undesirable behaviors. The reports final reward will consider the judgment of each rubric and its corresponding weight (which can be positive or negative). This asymmetric binary mapping eliminates noise from intermediate categories, making the reward signals more discriminative and accelerating the convergence of the model toward expert-aligned behavior. 4.3.3 Agentic Reinforcement Learning Real-world Environment with Tools. To enable the agent to genuinely learn how to complete deep-research tasks under real-world constraints, we place it in multi-tool, multimodal interactive environment during the reinforcement learning stage. Within this environment, the agent can freely alternate between natural-language generation and tool invocation. For cost control, we introduce caching mechanism for high-expense web retrieval, reusing results for identical queries, and impose explicit budgets on the number of tool calls and total token consumption to ensure that training operates within predictable resource envelope. Within this environment, each deep-research task is treated as reinforcement learning episode. The agent first performs intent analysis and restates the user query, then produces step-by-step plan, organizing tool invocations and information gathering around distinct subproblems. It subsequently filters, contrasts, and cross-validates evidence from multiple sources, and ultimately drafts complete report. After the episode terminates, the Rubrics Judge evaluates the report against the predefined rubrics, producing multi-dimensional scores that are convertedvia the strict reward mapping into reward signals used for policy updates. By iterating this process, the model continuously learns through trial-and-error in real environment, progressively acquiring policy decisions that more closely match human expert preferences with respect to tool selection, invocation timing, and call ordering. PPO Algorithm. We model the deep-research agents behavior in multi-tool environment as sequential decision-making process governed by single policy πθ. At each time step t, given state stcomprising the user request, the history of generated tokens, and observations returned by toolsthe policy outputs an action at. Here, an action includes both natural-language token generation and structured decisions such as initiating tool calls, specifying tool parameters, and parsing tool outputs. complete interaction yields trajectory τ = {(st, at)}T , and, upon episode termination, terminal reward R(τ ) is assigned by the Rubrics Judge. Under this formulation, we perform on-policy optimization using the clipped Proximal Policy Optimization t=1 13 Figure 3: Training reward of RL (PPO) objective: max θ (θ) = Eτ πθold (cid:34) (cid:88) t=1 (cid:16) min rt(θ) ˆAt, clip(cid:0)rt(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAt (cid:35) (cid:17) . Among these, the importance ratio is rt(θ) = πθ(at st) πθold(at st) . (1) (2) This objective employs ϵ-clipping to explicitly bound the policy shift induced by each update, thereby maintaining training stability in settings characterized by long-horizon sequence generation and sparse terminal rewards. For advantage estimation, we use Generalized Advantage Estimation (GAE): ˆAGAE(γ,λ) = (cid:88) (γλ)l, δt+l, l=0 δt = rt + γVϕ(st+1) Vϕ(st), (3) Consistent with the engineering practice in Open-Reasoner-Zero, we set γ = 1 and λ = 1 in GAE, i.e., we apply neither discounting to future returns nor additional exponential smoothing. This choice substantially simplifies and accelerates the computation of credit assignment in longhorizon sequences with sparse rewards. We choose PPO over critic-free policy-gradient variants primarily because learned critic can provide more reliable token-level value estimation. The systematic analysis in Open-ReasonerZero indicates that critic can identify and down-weight harmful patterns such as repetitive In loops, thereby yielding more robust advantage estimates and improving training stability. contrast, methods without an explicit value function have greater difficulty distinguishing accidentally high returns from returns coupled with undesirable behaviors, which can lead to erroneous reinforcement and instability. This consideration is particularly important in agentic settings and deep-research tasks: report generation typically involves long horizons, cross-source retrieval, and multi-round tool use. If single terminal, rubric-based reward cannot be assigned in fine-grained manner to intermediate decisions (e.g., retrieval strategy, evidence selection, cross-validation, and structured writing), policy updates tend to collapse into coarse overall good/bad fit. PPOs actorcritic structure directly supports this need by providing actionable token-level learning signals. 14 Training dynamics. The training dynamics of the RL process are presented in Figure 3. The reward curve exhibits consistent upward trajectory as training progresses, demonstrating that the agent is effectively optimizing its policy within the task distribution."
        },
        {
            "title": "5 System Architecture",
            "content": "Step-DeepResearch is implemented within basic single-agent architecture following the ReAct paradigm, reframing complex deep research tasks into dynamic Reasoning-Action-Observation loop. Upon receiving user query, the agent initiates cognitive iterative process through explicit reasoning and tool invocation (Figure 4), which cycles through three core phases. Figure 4: Step-DeepResearch System Architecture. The agent operates within ReAct loop, utilizing specialized toolset (e.g., batch_web_surfer, todo, shell) for planning, execution, and reflection to generate comprehensive research reports. Planning & Reflection. The agent initially identifies user intent to formulate action plans. In subsequent turns, it spontaneously reviews prior outcomes to validate the current state against objectives, achieving dynamic self-correction. Tool Execution. Translating plans into concrete actions, the agent selects the most appropriate tool (e.g., batch_web_surfer for search, todo for tracking) to initiate precise data acquisition requests. Feedback & Cross-Validation. New tool response is injected into the next reasoning round. The agent performs cross-validation against historical contextresolving conflicts and filtering falsehoodsto construct logically rigorous chain of evidence. This mechanism allows the agent to autonomously determine the exploration depth until the final report is generated, supported by the tool infrastructure detailed below."
        },
        {
            "title": "5.1 Context Management",
            "content": "5.1.1 General Context Management Strategies For general task scenarios, we integrate hybrid context management mechanism based on Summarization and Folding. This mechanism monitors context window usage and triggers intervention when approaching preset threshold. Summarization Strategy. When the threshold is triggered, the system feeds the entire current history into the model for semantic-level compression. The model generates formalized, highly condensed natural language summary that retains key nodes and future plans, creating dynamic checkpoint that preserves the global task state. 15 Folding Strategy. This employs rule-based structured pruning, including (1) Far-end Truncation to remove early non-critical interactions, and (2) Result Folding to replace lengthy tool outputs with placeholders. This retains only the initial task description and the most recent interaction turns. 5.1.2 Context Management for DeepResearch Unlike general tasks, deep research tasks require citing information sources in the final report. Since citation details are embedded within tool results, standard folding and summarization methods are insufficient. To address this, we implement specific Reference-Preserving processing. Reference-Preserving Summarization. Search tool results often contain numerous hyperlinks, including redundancy irrelevant to current reasoning and original sources essential for the final report. During context compression, this module filters out task-irrelevant content while strictly preserving original links and citation markers. structured Reference-Preserving Summary is maintained in the compressed context, allowing the agent to accurately restore and cite sources even after multi-turn summarization. Reference-Preserving Folding. Search results are structured, containing links, snippets, detailed content, and citations. The detailed content portion often has low information density but consumes significant context. Therefore, when folding is triggered in deep research tasks, we strip the detailed content from search results, retaining only the snippets and citation information to shorten the context while preserving original references."
        },
        {
            "title": "5.2 Deep Research Toolset",
            "content": "To support the long-context, high-interaction, and high-uncertainty nature of deep research, we designed tool system centered on Human-equivalent Workflow. The core philosophy is to match tools with the agents behavioral characteristics and token budget constraints, ensuring input/output formats fall within the agents comfort zone to reduce operational burden and hallucination while improving end-to-end throughput. 5.2.1 Authoritative Enhanced Information Acquisition To address the imbalance of signal-to-noise ratio and the dilution of authority in massive internet data, we built high-quality information retrieval system. Curated Authority Indexing. We assembled professional team to evaluate diverse online sources, selecting over 600 core authoritative sites covering official government domains, industry research institutes, international organizations, and leading academic platforms. We built independent index shards for these sites, physically and logically isolating authoritative content from low-quality SEO spam, significantly improving recall stability. Knowledge-Dense Document Retrieval. Targeting long-form documents such as professional literature, financial reports, and official white papers, the system utilizes dedicated library of over 20 million high-quality items. Indexing employs Paragraph-level granularity to avoid the noise introduced by ingesting entire documents. By recalling specific semantic paragraphs, the model acquires higher-density information with lower token costs. In the ranking phase, the system integrates an auAuthority-Aware Ranking Heuristics. thority boosting factor. When semantic relevance scores are comparable, the algorithm prioritizes content from authoritative sites, ensuring that research arguments are grounded in verifiable facts. Additionally, customized search parameter interfaces allow the system to dynamically adjust search behavior based on query intent. 16 5.2.2 Knowledge Management & File Operations Treating the file system as an external persistent memory, we adapted traditional file interactions into agent-native protocols. This transformation is strictly driven by the need for token efficiency and robustness in long-context workflows. Token-Efficient Patch-based Editing. To mitigate the token waste of full rewrites and the reasoning burden imposed by Diff formats on mid-sized models, we introduced Patch action. The agent only needs to provide the modified fragment with minimal anchor context, while the tool performs atomic updates via fuzzy matching. In scenarios involving local polishing of long-form reports, this reduces output token costs by over 70% and significantly boosts the success rate of complex edits. Implicit Context Management. To eliminate the risk of context window overflow caused by excessive retrieval content, we devised Summary-Aware + Local Storage two-tiered strategy. When tool results exceed preset threshold, the system truncates the immediate return, injecting only high-density summaries into the context while persisting raw data to local temporary files. The agent performs demand-paging via file.read based on summary cues. This design effectively offloads context pressure to disk, enabling virtually infinite context support for long-horizon reasoning. Stateful Todo Management. To prevent goal drift in long-horizon research, the todo tool encapsulates complex CRUD operations within unified entry point. It automatically determines create, rewrite, or destroy states based on the current task stack. By decoupling research progress from model weights and persisting it at the tool layer, this ensures logical consistency and goal alignment over extensive interaction trajectories. 5.2.3 Interactive Execution & Multimodal Perception To bridge the gap between raw execution and expert-level problem solving, we developed highfidelity interaction framework integrated with comprehensive multimodal perception suite. This setup ensures the agent can navigate complex digital tasks with both precision and adaptability. Human-like Terminal Interaction (Sandbox & Tmux Integration). All execution commands are run within restricted MCP sandbox environment. The system integrates the tmux session management mechanism; by maintaining persistent scrollback buffer, the agent can stably operate command-line programs with state-refreshing characteristics (such as the vim editor or real-time monitoring tools). This equips the agent with human-like interactive debugging capabilities when facing complex errors, enhancing system-level fault tolerance. Perception-optimized Resilient Browser. We introduce visual redundancy elimination strategy specifically for research tasks. The system dynamically identifies visual differences by calculating the PHash (Perceptual Hash) distance between page screenshots of consecutive actions. In cases of negligible page updates, the system inhibits image-based feedback and reverts to text-only stream. This perception-on-demand strategy ensures the agent maintains realtime control over page states while significantly reducing multimodal token redundancy. Multimodal Perception Tools. For unstructured data, we have integrated specialized modules including file_parser (document parsing), asr (audio transcription), and analyze_image (image analysis), ensuring high-quality understanding of complex research materials."
        },
        {
            "title": "6 ADR-Bench: A Custom Deep Research Benchmark",
            "content": "As product designed for complex, open-domain research tasks, DeepResearch faces significant challenges in performance evaluation, characterized by lack of unified standards, inherent sub17 Figure 5: The query distribution of ADR-Bench jectivity, and high expertise thresholds. To drive model iteration scientifically and efficiently while accurately reflecting perceived user quality, we designed and implemented dual-track evaluation framework covering both general-domain user experience and professional-domain capability assessments. This chapter systematically summarizes the core methodologies, key findings, and empirical experiences derived from our Benchmark construction and evaluation standards design. It details query composition, evaluation framework architecture, and criteria formulation, alongside the challenges and exploratory initiatives undertaken, aiming to provide reference for the evaluation of similarly complex tasks."
        },
        {
            "title": "6.1 Query Composition",
            "content": "We conducted study of users everyday Deep Research usage scenarios and obtained an initial set of queries drafted by domain experts. Through multiple rounds of testing, we filtered out and revised queries that were not fully suitable for Deep Research. Based on the distribution of queries in real business settings, we categorized user queries into nine domains: Law, Computer and Information Technology, Education, Finance and Business, Science and Engineering, Social Life, Literature and Arts, Healthcare, and Politics. To construct more rigorous evaluation framework and to better assess the performance of DeepResearch, we further divided the queries into general-domain and specialized-domain categories. We selected Law and Finance as representative specialized domains; subject-matter experts authored the test items and rubrics and verified their correctness. Each of the Law and Finance domains contains 20 expert-written tasks. The remaining seven domains were treated as general-domain categories. For these, we collected real user queries from actual business usage, manually selected high-quality, diverse, and representative items, and curated 10 tasks per domain. The distribution of queries across all domains is shown in Figure 5."
        },
        {
            "title": "6.2 Evaluation Framework",
            "content": "We adopt differentiated evaluation strategies for general-domain and specialized-domain queries. For general-domain queries, user questions are typically open-ended and broad in scope, with no single standard answer. As result, it is difficult to objectively characterize report quality through fully enumerated rubrics. Therefore, we employ human comparative evaluation approach. Specifically, for each task, two reports generated by different models are presented to evaluators under blind-review conditions. Evaluators assign one of five categorical judgmentsLeft better / Right better / Both good / Both fair / Both poorand score the outputs across four sub-dimensions: completeness of information, depth of content, alignment with user needs, and readability, along with written justification. Compared with absolute scoring, comparative evaluation introduces reference anchor, effectively reducing uncertainty arising from blurred quality standards in open-domain scenarios and better reflecting users subjective sense of which report feels better. To ensure reliability and consistency, we conducted systematic training for evaluators, standardized both the overall and sub-dimension scoring criteria, and formalized the blind-review procedures. This minimizes the influence of individual preference and increases the confidence level of evaluation conclusions. In specialized-domain evaluation, we selected Finance and Law as representative domains to assess the models capability to generate expert-level reports. Such tasks demand advanced domain knowledge and deeper reasoning, and relying solely on manual comparative review is insufficient to support rapid model iteration in terms of cost and efficiency. Therefore, we adopt rubric-based automated evaluation approach: domain experts design representative, relatively constrained real-world tasks and iteratively validate and refine the rubrics through multiple rounds of cross-examination. The resulting high-quality and enumerated rubrics cover the essential elements expected in professional reports, allowing us to focus on the models professional competency and knowledge proficiency. Overall, the proposed evaluation framework captures model-perceived quality differences in real operational contexts for general-domain usage, while leveraging high-quality rubrics in specialized domains to monitor professional capability. This approach balances scientific rigor, operational feasibility, and iteration efficiency. Future work will explore more scalable evaluation methodologies to address declining discriminative power and rising evaluation costs as model performance converges."
        },
        {
            "title": "6.3 Evaluation Criteria",
            "content": "Based on the evaluation framework, we conducted assessments on our model as well as relevant competing products and models. In this section, we present the specific evaluation criteria and the rationale underlying their design. For comparative evaluations in general domains, simplified version of our evaluation criteria is shown in the table below. It is worth noting that the accuracy dimension is difficult to validate reliably in open-ended long-form assessment tasks such as those used for DeepResearch, and such validation would impose substantial additional labor and time costs. Therefore, we only require evaluators to verify key conclusions or data points that are directly related to the task requirements and materially affect report quality, and incorporate this requirement under the alignment with user needs dimension. Moreover, because the evaluation dimensions cannot, in practice, be fully disentangled for given task, evaluators are instructedwhen assigning scores on any given dimensionto focus exclusively on the criteria relevant to that dimension without taking other dimensions into account. 19 Table 4: LLM Response Evaluation Metrics and Guidelines Dimension General Evaluation Logic Description & Criteria Core Principle: The score is not simple summation of subdimensions but depends on the query scenario. Learning/Science Queries: Prioritize clear logic and gradual deepening (Logic > Diverse Data). Decision/Comparison Queries: Prioritize hard data comparison and pros/cons analysis (Data Depth > Flowery Text). Planning/Proposal Queries: Prioritize actionable steps and creativity (Completeness > Complex Reasoning). Completeness vs. Depth: Research queries prioritize completeness; Analysis queries prioritize depth. Veto Criteria: For serious topics (News/History), factual errors or bias result in automatic rejection. Information Completeness Focus: Breadth, perspective coverage, and avoiding key omissions. Does it cover all key aspects? (e.g., Market Size + Competition + Content Depth Requirement Fitness Readability Policy for Industry Analysis). Are obvious or significant pieces of information missing? For multi-part user prompts, are all sub-questions answered? Focus: Narrative depth, substantial data, evidence, and insight. Specific Data: Use of numbers/percentages (e.g., 50 billion market) vs. qualitative descriptions (market is large). Deduction: Summary and logical deduction vs. simple piling of search results. Insight: Does it hit critical points and offer valuable conclusions? Focus: Responding to explicit/implicit needs (Relevance & Correctness). Intent: Does it truly understand the background and deep intent? Constraints: Are format requests followed? (e.g., Output as Table or SWOT). Accuracy: Are key conclusions and data points fact-checked and correct? Focus: Clarity and friendliness of information organization and presentation. Visual Aids: When presenting complex information (e.g., timelines, comparisons), are appropriate charts used to help users grasp key info quickly? Structure & Layout: Are section headings, bolding, and summaries used to make the content hierarchical and scannable? Is the segmentation logical? Accessibility: Is complex content clarified with key summaries? Are examples used to help readers understand difficult concepts? For specialized domains, we require experts to curate questions with relatively limited solu20 tion space and certain core commonalities. The rubrics must adhere to the five core principles summarized in Table 5. Table 5: Principles and Criteria for Constructing Evaluation Rubrics Principle & Definition Examples & Analysis 1. Atomicity Each Rubric must describe only one clear, single requirement, assessing whether specific matter is completed. 2. Verifiability Rubrics must be actionable with objective judgment criteria for explicit verification. 3. Unambiguity Phrasing must be clear, ensuring no multiple interpretations. 4. Independence Rubrics should be mutually independent with no content overlap. 5. Alignment Directly correspond to core task requirements without irrelevant dimensions. Correct: Does the report explain the risk associated with the funds historical return data? Incorrect: Does the report detail the statute of limitations, starting point, calculation method, and legal consequences...? (cid:44) Analysis: Contains multiple independent requirements. Correct: Does the report cite Article XXX of the Civil Code relevant to this case? Incorrect: Does the report conduct sufficient and comprehensive discussion...? (cid:44) Analysis: Sufficient and comprehensive is vague and hard to verify. Correct: Does the report list the three major advantages XXX of this policy? Incorrect: Does the report separately reply to the users 3 questions? (cid:44) Analysis: Separately is ambiguous (paragraph structure vs. content distinction). Correct: Rubric checks for Securities Law citation; Rubric checks for analysis of key evidence. (Distinct) Incorrect: Rubric asks for legal basis citation; Rubric asks for relevant laws citation. (cid:44) Analysis: Significant content overlap exists. Task: M&A Legal Report Rubric: Identify risks in equity transfer. Task: Financial Product Risk Rubric: Rate company ESG performance. (cid:44) Analysis: ESG is not core requirement for this specific task."
        },
        {
            "title": "6.4 Exploratory Attempts",
            "content": "Throughout the DeepResearch evaluation process, we conducted extensive exploratory attempts and gained several insights. For complex and highly subjective task such as evaluating DeepResearch, building scientific and well-structured evaluation framework is extremely challenging. By sharing our experience, we hope to promote further methodological research on evaluating tasks that are difficult to verify. Domain segmentation. At the early stage of constructing the evaluation framework, we needed to segment the domains to which DeepResearch queries belong, because this affects how 21 we construct the evaluation set and determine its distribution. In our experience, defining too few domains results in insufficient coverage of query distributions, while defining too many domains leads to frequent overlaps, where query may simultaneously fall into multiple domains. This complicates statistical analysis of query distribution and model performance across domains, reducing statistical meaning and informational value. Ultimately, we found that for DeepResearch, defining roughly 612 domains is generally reasonable. It is not necessary to pursue perfectly correct segmentation scheme; however, one guiding principle is that the final segmentation should effectively direct us to the right experts and evaluators for each domain. In comparative evaluations, an overall score helps Segmentation of evaluation dimensions. evaluators express the holistic perceived gap between two reports. Aggregating multi-item results allows us to analyze overall model strengths and weaknesses. However, relying solely on such aggregated scoring only provides the final delivered output of the model, making it difficult to analyze specific issues and guide iteration. Therefore, we consider detailed evaluation dimensions to be necessary. Our experience suggests that each evaluation dimension should be independent, explicit, and executable. Independence means that the dimensions should be as decoupled as possiblethough in practice this is difficult, and coupling often appears in some items. Our solution is to evaluate each dimension by focusing only on that dimension at the time. Explicitness means that each dimension should have clear criteria and requirements. Executability means that dimensions must be operational in practice. For example, in the case of long DeepResearch reports, verifying each argument or data point is extremely labor-intensive and often unreliable when done manually. Model-generated rubrics. We attempted to use high-performing LLMs such as Gemini to automatically generate rubrics from gold-standard reports, followed by human refinement. Our findings were as follows: (a) the gap between the gold report and an ideal report strongly determines rubric quality. Human revision tends to be influenced by the initial draft, resulting in small adjustments that are hard to validate. Yet obtaining an ideal report is extremely difficult; (b) model-generated rubrics struggle to ensure evaluability when the LLM is used as judgefor example, verifying data correctness without ground truth, or assessing whether arguments are sufficiently comprehensive; (c) the automatic evaluation results using such rubrics diverge from human evaluators perception; (d) although the original objective was to let the model initialize rubrics to reduce human workload, in practice substantial rewriting and alignment were still required before the rubrics became usable. One-sided subjective scoring. We attempted scheme where each query produced three DeepResearch reports from different models, and human evaluators assigned 010 overall scores. We observed significant variance among evaluators. When restricting the scale to 03, the scoring failed to reflect meaningful differences between reports and could not allocate scores reasonably. For DeepResearchwhere standards are non-unique and unified verification criteria are difficultcomparative evaluation yields more stable results, higher confidence, and improved efficiency. Resource consumption of Elo battles. Elo is scientifically robust comparative method, but human-based Elo battles require more resources than ordinary comparative evaluations. For example, given 10 queries and 5 models, ordinary comparative evaluation requires only 50 pair comparisons to assess one model against the others. Elo battles require 150 comparisons to produce full leaderboard. This greatly increases cost for DeepResearch, where each comparison is lengthy. Sampling or limiting opponent sets may mitigate the burden, but reduces confidence in the leaderboard while still incurring higher cost than ordinary comparisons. Therefore, for DeepResearch, if the goal is simply to measure performance gaps between model and baseline models, we recommend ordinary comparative evaluation. 22 Expert-generated questions. Senior domain experts who possess an understanding of LLMs and are willing to invest in evaluation work are scarce. Collaborating with such experts incurs significant time and financial costs."
        },
        {
            "title": "7.1 Experimental Setup",
            "content": "Evaluation Benchmarks. We evaluated our model utilizing both established Research Rubrics [9] and our ADR-Bench. LLM-based Evaluation (Research Rubrics). We employed LLM judger using ternary grading for each criterion. Preliminary experiments revealed that the original evaluation prompts exhibited significant stochasticity and failed to accurately interpret negative criteria. To ensure the robustness and reproducibility of our results, we employed targeted prompt engineering for negative constraints in Appendix A, enforced deterministic decoding by setting the temperature to 0, and utilized an ensemble scoring mechanism based on the arithmetic mean of three independent trials per criterion. Human-Centric Evaluation (ADR-Bench). As detailed in the previous section, we develop two in-house benchmark datasets. The first is 70-item general test set evaluated via human side-by-side comparison, in which human annotators directly assess reports generated by different models and select the preferred output. This evaluation protocol is designed to capture fine-grained nuances in user preferences and practical utility. The second is 40-item finance and legal professional test set, where large language models act as evaluators and assign scores based on predefined criteria. Compared models. We evaluate two representative families of systems. Commercial Agent System. Since commercial DeepResearch agents are closed-source, we manually collected the generated reports from their respective official platforms for evaluation. This category includes OpenAI DeepResearch [1], Gemini DeepResearch [2], KimiResearcher [3], MiniMax Agent Pro [6], Qwen DeepResearch [28]. ReAct Agent. We implement the ReAct framework across various foundation models via API interfaces. The evaluated models include Kimi-k2-thinking [29], DeepSeek-V3.2 [8], GLM 4.6 [7], MiniMax-M2 [6]. Additionally, we introduce our proposed model, StepDeepResearch, which is initialized from Qwen2.5-32B-Base [25]. Its development involves multi-stage training pipeline comprising Mid-training, SFT, and Reinforcement Learning (RL). Evaluation Configurations. To ensure fair comparison, we standardized the foundation model settings with maximum of 30 reasoning turns and limit of 16k tokens per turn. For commercial agent systems, evaluations were performed directly on the final reports generated using their default web-based configurations. Cost Estimation Standards. We adopt dual-method approach to quantify economic efficiency. For LLM-based ReAct agents, we calculate costs directly based on official model API pricing and the actual volume of input and output tokens consumed on the ResearchRubrics benchmark. For commercial agent systems where direct API token statistics are unavailable, we employ time-based extrapolation method using ReAct baselines of comparable model size. Specifically, the cost for MiniMax Agent Pro is estimated by scaling the recorded cost of our MiniMax-based ReAct agent by the ratio of their official execution times. For Kimi-Researcher and Qwen DeepResearch, we utilize the Kimi-k2-thinking-based ReAct agent as the cost baseline, scaling by the ratio of the commercial agent systems execution time to the baselines execution 23 Figure 6: Step-DeepResearch Agent against open-source models and commercial products. time. Additionally, for Qwen DeepResearch, we apply pricing adjustment factor of 1.6 to reflect the official API price differential between Qwen3-Max and Kimi-K2."
        },
        {
            "title": "7.2 Evaluation Results",
            "content": "Research Rubrics. Figure 6 presents comprehensive comparison between Step-DeepResearch and other agents on Research Rubrics benchmark. The experimental results demonstrate that Step-DeepResearch achieves SOTA performance within the single-agent category. With score of 61.42, it significantly outperforms the leading open-source model, Kimi-k2-thinking (56.17), representing 5.25 improvement in performance. In the overall leaderboard, Step-DeepResearch ranks second only to the commercial system Gemini DeepResearch 63.69, surpassing most complex agent frameworks including OpenAI DeepResearch and Kimi-Researcher. Notably, the superior performance of Step-DeepResearch is characterized by exceptional parameter efficiency: Architectural Superiority: As single-agent system, its performance exceeds that of various frameworks relying on complex multi-step orchestration or multi-agent collaboration. This validates the models robust native research capabilities and logical reasoning proficiency. Lightweight Advantage: Among commercial agent systems, high-end models incur significant costs per report: Gemini DeepResearch ( 6.65 RMB) and OpenAI DeepResearch ( 5.32 RMB) are the most expensive, followed by MiniMax Agent Pro ( 3.36 RMB) and Kimi-Researcher ( 2.66 RMB), with Qwen DeepResearch being the outlier ( 0.63 RMB). In the ReAct agent category, costs are generally lower but variable: MiniMax-M2 ( 0.42 RMB), DeepSeek-V3.2 ( 0.68 RMB), Kimi-k2-thinking ( 0.76 RMB), and GLM-4.6 ( 1.05 RMB). Remarkably, Step-DeepResearch achieves ResearchRubrics score of 61.42 with single invocation cost of less than 0.50 RMB. This represents cost reduction of more than 10-fold compared to top-tier commercial systems (e.g., Gemini and OpenAI) while maintaining state-of-the-art performance, demonstrating exceptional cost-effectiveness for large-scale deployment. Cost metrics are shown in Figure 1a. ADR-Bench. As shown in Figure 7, Step-DeepResearch demonstrates superior performance In an ablation study in human evaluations compared to existing commercial agent systems. comparing Step-DeepResearch with its non-midtrained version, the model achieved record of 30 wins and 21 losses. This result indicates that the integration of mid-training specifically for agents 24 Figure 7: Human evaluation results on ADR-Bench (N=70). The numbers represent the count of Win-Tie-Loss cases for each comparison. aligns more closely with human preferences regarding the quality of complex research reports. When benchmarked against other leading systems, Step-DeepResearch consistently maintains higher win rate than loss rate, establishing its position as superior alternative to current systems. Notably, the model demonstrates significant non-inferiority across all benchmarks. Specifically, against formidable opponents such as Gemini and MiniMax, the cumulative count of Wins and Ties reached 47 (67.1%). These findings provide empirical evidence that StepDeepResearch consistently meets or exceeds the most advanced performance standards across the vast majority of research scenarios. Table 6: Results of ADR-Bench(Finance&law) Tier score-range systems Tier Tier 2 25 35 15 25 Gemini DeepResearch Step-DeepResearch, Kimi-Researcher, Kimi-k2-thinking, OpenAI DeepResearch Tier 0 15 Qwen DeepResearch, MiniMax-M2, MiniMax Agent Pro, GLM-4.6 ADR-Bench(Finance&law) Queries from finance and law are typically feature industry terminology, multi-stage reasoning chains, and sensitive constraints. To address this, we incorporated explicit negative scoring criteria within the checklist for each data point to penalize errors in reports. For unpardonable errors, the penalty is particularly severe, leading to direct unusable (zero-score) rating for that specific report. Under this scoring framework, models overall performance is no longer driven solely by its hit rate but also reflects the cost of errors and their associated risk. Evaluation results shown in table 6, reveal clear tri-tier distribution of performance. Gemini outperformed all others, establishing dominant lead, Step-DeepResearch, Kimi, and OpenAI showed comparable performance, clustering within the same tier. The remaining models and products lagged significantly behind, exhibiting pronounced long-tail effect. These findings suggest that scoring variances are more likely correlated with the models inherent domain-knowledge coverage rather than gains from agentic frameworks. In other words, under 25 the constraint of strict negative scoring, the process optimizations provided by Agent frameworks cannot compensate for models fundamental knowledge gaps. Step-DeepResearchs position at the forefront of the second tier is precisely due to its domain-specific training in financial and legal scenarios, granting it level of expertise competitive with much larger parameter models."
        },
        {
            "title": "7.3 Detailed Analysis",
            "content": "Figure 8: Weighted Score by Category on ResearchRubrics. Figure 9: Weighted Score by Domain on ResearchRubrics. ResearchRubrics. Figure 8. shows the performance distribution of Step-DeepResearch (denoted by blue diagonal-hatched bars) across six critical evaluation dimensions. The experimental results highlight several key performance breakthroughs: Step-DeepResearch demonstrated significant leadership in Implicit Criteria and Explicit Criteria securing scores of 54.5 and 72.0, respectively. Notably, its performance in implicit criteria surpasses OpenAI DeepResearch (52.4) and substantially exceeds open-source models, signaling robust capacity for underlying logIn the category of Citation Quality Step-DeepResearch achieved score of ical alignment. 57.0, tying for the top position with Gemini DeepResearch. This high level of empirical rigor ensures that all generated insights are fully substantiated by verifiable sources. With score of 58.2 in Communication Quality, the model outperformed all evaluated counterparts. This 26 Figure 10: Fine-grained results on ADR-Bench indicates that the generated reports are not only data-rich but also exhibit superior clarity and professional readability. Comparative analysis also identified marginal gap in Instruction Following, where Step-DeepResearch (64.9) remains slightly behind Kimi-Researcher (66.7). Our preliminary diagnostic attributes this to the breadth of instruction fine-tuning data diversity during the Post-training phase. Future development will prioritize targeted optimization for multi-constraint complex tasks to ensure dominant lead across all evaluative dimensions. As shown in Figure 9, Step-DeepResearch outperformed Kimi-Researcher in AI & ML (64.8 vs 57.4), Historical Analysis (65.8 vs 61.4), and Technical Documentation (64.6 vs 53.1), tying with Gemini DeepResearch for the top spot in the latter. Notably, these results were achieved without domain-specific data augmentation. The model also led in Creative Writing (63.4), demonstrating that its underlying logic and knowledge structure generalize effectively to out-of-distribution tasks. Despite broad leadership, the model shows room for improvement in STEM (64.7) and Philosophy (46.1), currently trailing Gemini DeepResearch (70.7 and 53.5, respectively). Error analysis indicates these gaps stem from the high demands of high-order reasoning. Future efforts will focus on closing these logical reasoning gaps to enhance performance in hard-science domains. ADR-Bench We employed fine-grained, pairwise human preference evaluation methodology to conduct comprehensive performance alignment test of Step-DeepResearch, shown in Figure.10. detailed analysis of each dimension follows below: Informational Completeness. For tasks involving cross-domain knowledge synthesis, evidence chain tracking, and multi-source information aggregation, the model not only covers all key points but also provides structured expression, enhancing both information capacity and perceived quality. In tasks requiring large-scale categorization or element listing, the model significantly suppresses content hallucinations and redundant stacking achieving complete and accurate structured presentation. 27 Content Depth. Regarding content depth, Step-DeepResearch achieves performance comparable to several closed-source commercial systems. In scenarios that heavily rely on specialized professional knowledge, it trails slightly behind DeepResearch-style products such as OpenAI and Gemini, gap we attribute primarily to differences in model parameter scale. By contrast, Step-DeepResearch demonstrates more consistent execution capabilities in general problem analysis, planning, and strategic inference tasks. We observe that base models without task-specific training often exhibit shallow response patterns, characterized by short, loosely connected sentences and superficial bullet points that provide limited practical value. Step-DeepResearch effectively mitigates this behavior through targeted training. Requirement Fitness. In terms of meeting user requirements, Step-DeepResearch demonstrates level of stability comparable to that of commercial systems. Across wide range of instruction complexities, the model is able to accurately identify user intent and provide relevant supplementary information to support decision-making. We observe that several existing DeepResearch-style systems rely on rigid output patterns, such as enforcing thesis-like structure across diverse tasks or introducing information that is only weakly related to the users intent. In contrast, Step-DeepResearch adapts its output structure to the task at hand, allowing writing style and presentation to evolve dynamically based on task attributes."
        },
        {
            "title": "7.4 Case study",
            "content": "Table 7 presents representative example from our benchmark comparing midtrained models with non-midtrained baselines. The results demonstrate substantial improvements in overall report quality for midtrained models. In particular, we observe consistent gains in instruction following, as well as in the breadth and depth of information exploration. These improvements are strongly correlated with the incorporation of atomic capabilities, such as planning and information seeking during the midtraining phase."
        },
        {
            "title": "7.5 Targeted Refinements for Deep Research Patterns",
            "content": "Based on systematic analysis of failure cases in earlier iterations, we implemented targeted optimizations for Deep Research scenarios: Writing Style. While earlier models achieved high Research Rubric scores (57), human evaluation revealed tendency toward fragmented information stacking rather than deep analysis. This highlights limitation in checklist-based scoring, which prioritizes information stacking rather than deep, integrated analysis. This discrepancy highlights limitation in checklist-based metrics: they prioritize information recall (i.e., whether key points exist) over structural coherence and narrative depth. Furthermore, we identified negative correlation between analytical depth and comprehensiveness. When prompted to minimize list-based formatting and provide deeper insights, the model often terminated generation prematurely, leading to drop in recall. To address this, we optimized our data synthesis pipeline by introducing Synthesis-driven Drafting module. This module directs the model to transform raw tool-call trajectories into structured paragraphs with logical deduction, while strictly limiting the use of unordered lists as the primary content body. Additionally, we implemented Pairwise LLM Judger for quality control. This filtering mechanism retains new reports only if they demonstrate superior depth and logic without compromising overall information coverage. Temporal Cognition. Despite injecting explicit timestamps into the system prompt, models frequently exhibit temporal confusion during long-horizon reasoning. notable phenomenon is that models often treat the system-provided date as simulated setting or habitually append past years (e.g., 2023 or 2024) to search queriesa bias that persists even in synthetic data generated by top-tier closed-source models. Such deviations severely compromise the timeliness and relevance of the final reports. To mitigate this, we implemented strict temporal logic vali28 Table 7: Case Study: User Query and Model Comparison 针对大模型的repo级代码工程能力类似swe bench评测众多研究提出利用github的prissue等数据重构改 写数据作为预训练与后训练间的midtrain语料swe rl给出了数据的详细构造/改写过程并提出了midtrain存 疑我不确定它是不是第一个提出代码midtrain概念的我希望了解有没有其他给出github prissue数 据的详细构造过程或开源这部分数据清洗重构代码或开源重构好的数据我都关注PR/Issue数据的筛 选标准清洗步骤和格式转换方式等特定的数据处理方法请给我介绍所有相关工作(如SWE-FixerSWEagentDeepSWE等)重点讲解那些对prissue以及repo数据组织改写给出详细方案的研究 English Translation. Regarding repo-level code engineering capabilities of large language models (similar to the SWE-bench evaluation), many studies propose leveraging GitHub PRs and Issues to construct midtraining corpora between pre-training and post-training. While SWE-RL provides detailed data construction and rewriting pipeline, seek other works that (1) explicitly describe PR/Issue data construction, (2) release open-source data cleaning and reconstruction code, or (3) provide reconstructed datasets. am particularly interested in filtering criteria, cleaning procedures, and format conversion methods. Please systematically review related works (e.g., SWE-Fixer, SWE-agent, DeepSWE), with an emphasis on those that offer detailed schemes for organizing and rewriting PR, Issue, and repo-level data. Dimension Step-DeepResearch Step-DeepResearch w/o Mid-training Model Reports Comparison Requirement Fitness Information Completeness Content Depth Fully captures the users multi-layered requirements, explicitly prioritizing studies that provide detailed PR/Issue and repo-level data construction and rewriting schemes. Covers over dozen relevant works, including the SWE-bench series, SWE-smith, SWE-Factory, Multi-SWE-bench, SWERL, DeepSWE, InstructCoder, Coeditor, RepoBench, SWE-agent-trajectories, and SWE-Fixer. Provides detailed, end-to-end data processing pipelines, concrete code-level examples, and in-depth discussions of key mechanisms such as Issue-PR pairing, test-case isolation, and automated environment construction. Presents structured comparative tables summarizing data sources, formats, filtering criteria, cleaning steps, format conversions, and open-source availability. Partially follows the instruction, with limited emphasis on data construction details and weaker alignment with the users prioritization criteria. Covers only six works, omitting foundational datasets and key automation frameworks such as SWE-smith and SWEFactory. Offers deeper analysis for small subset of works (e.g., SWE-RL) but discusses most studies only at high level. Lacks systematic cross-paper comparisons; information is presented in fragmented, narrativedriven manner. dation during data cleaning: any trajectory where the model anchors time-agnostic query to past timestamp is systematically filtered and discarded. Linguistic Consistency. To address inconsistent Chinese-English code-switching that disrupts readability, we applied strict data-cleaning strategy. During the trajectory synthesis phase, we utilize combination of regular expressions and language density detection to identify and remove low-quality trajectories. This process programmatically filters out instances of non-essential language mixingspecifically the interspersing of common English vocabulary within Chinese prosewhile preserving necessary proper nouns and domain-specific technical terminology."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "In this work, we have revisited the meta-capabilities of deep research agents and proposed holistic understanding of the Deep Research task, achieving superior performance on 32Bparameter model. Specifically, by leveraging multi-stage atomic-capability data strategy and 29 an end-to-end training paradigm, we have progressively improved the models research proficiency from agentic mid-training to post-training. Furthermore, we have introduced ADR-Bench, novel benchmark designed to evaluate the practical usability of agents in real-world scenarios. Extensive experiments demonstrate that our principles and paradigms achieve state-of-the-art results among medium-sized models and can compete with proprietary large-scale models in specific domains. We hope this work will inspire the community to further advance the frontier of autonomous agents toward AGI. Despite these advancements, we currently face several challenges. First, the generalization and robustness of tool use remain insufficient; the system often exhibits brittle points when encountering API variations, anomalous returns, or long-chain tasks involving complex crosstool compositions. Second, ensuring stable overall correctness and factuality remains difficult, particularly in scenarios with high information noise or fragmented evidence, which can lead to plausible but unprovable inferences. Finally, the readability and auditability of the generated reports have room for improvement, such as more consistent structural organization and more explicit mapping between conclusions and evidence. To address these issues, our future work will advance along three primary directions. First, we will develop multi-agent collaborative paradigm by introducing specialized roles such as planners, retrievers, verifiers, and writers to reduce hallucinations through consensus mechanisms. Second, we will expand to complex environment interactions, enabling the agent to perform continuous exploration and error correction in partially observable and dynamic settings. Third, we will improve reward modeling and training objectives to incorporate metrics such as factual consistency, traceable citations, and structural clarity. By combining preference learning with process-based supervision, we aim to ensure that the model is not only seemingly correct but verifiably and clearly correct."
        },
        {
            "title": "9 Contributors and Acknowledgements",
            "content": "This work was carried out by the Agent Team at StepFun. Within each contribution category, contributors are listed in alphabetical order by first name. Corresponding Contributors. Chen Hu (hatcher@stepfun.com), Daxin Jiang (djiang@stepfun.com). Research and Data. Chen Hu, Haikuo Du*, Heng Wang, Lin Lin, Mingrui Chen, Peng Liu, Ruihang Miao, Tianchi Yue, Wang You, Wei Ji, Wei Yuan, Wenjin Deng, Xiaojian Yuan, Xiaoyun Zhang, Xiangyu Liu, Xikai Liu, Yanming Xu, Yicheng Cao, Yifei Zhang, Yongyao Wang, Yubo Shu, Yurong Zhang, Yuxiang Zhang, Zheng Gong and Zhichao Chang. Evaluation. Binyan Li, Dan Ma, Furong Jia, Hongyuan Wang, Jiayu Liu, Jing Bai, Junlan Liu, Manjiao Liu, Na Wang, Qiuping Wu, Qinxin Du, Shiwei Li, Wen Sun, Yifeng Gong, Yonglin Chen, Yuling Zhao, Yuxuan Lin*, Ziqi Ren and Zixuan Wang. Infrastructure. Aihu Zhang, Brian Li, Buyun Ma, Kang An, Li Xie, Mingliang Li, Pan Li, Shidong Yang, Xi Chen, Xiaojia Liu, Yuchu Luo, Yuan Song, YuanHao Ding, Yuanwei Liang, Zexi Li, Zhaoning Zhang and Zixin Zhang*. Project Sponsors. Binxing Jiao, Daxin Jiang, Jiansheng Chen, Jing Li, Xiangyu Zhang and Yibo Zhu. 2* Indicates contributors who are no longer affiliated with StepFun at the time of publication."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to express our sincere gratitude to the following colleagues at StepFun for their support: Bingxin Li, Beidi Luan, Chang Su, Dionysia Zhang, HuanCheng Bai, Huangxi Zhu, Jiacan Dong, Junhan Gu, Junjing Guo, MingMin Li, Qianyu Yang, Qi Zhou, Ran Ding, Ran Sun, Rui Sun, Shanshan Yuan, Sitong Liu, Sitong Liu, Tianyi Zhang, Weibo Wu, Wenjing Zhao, Xuan He, Yichen Wang, Yixuan Kong, Zephyr Zhu, Zixuan Chen and Zhixin Chen."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Introducing deep research. introducing-deep-research/, 2025. Accessed: 2025-02. https://openai.com/index/ [2] Google. Gemini deep research your personal research assistant. https://gemini.google/ overview/deep-research/, 2024. Accessed: 2025-03. [3] Moonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https://moonshotai.github.io/Kimi-Researcher/, 2025. [4] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [5] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [6] MiniMax AI. Minimax m2 & agent: Ingenious in simplicity. https: // www. minimax. io/ news/ minimax-m2 , 2025. [7] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. [8] DeepSeek-AI. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. [9] Manasi Sharma, Chen Bo Calvin Zhang, et al. Researchrubrics: benchmark of prompts and rubrics for evaluating deep research agents. arXiv preprint arXiv:2511.07685, 2025. [10] Anthropic. Claude takes research to new places. https://www.anthropic.com/news/ research, 2025. Accessed: 2025-04. [11] Perplexity Team. Perplexity deep research. https://www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research, 2025. [12] LangChain. Open deep research. https://github.com/langchain-ai/open_deep_ research, 2025. [13] SkyworkAI. Deepresearchagent: hierarchical multi-agent system for deep research. https: //github.com/SkyworkAI/DeepResearchAgent, 2025. [14] Together AI. Open deep research. https://www.together.ai/blog/open-deep-research, 2025. 31 [15] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [16] Tongyi DeepResearch Team. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. [17] WebRL Team. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. In International Conference on Learning Representations, 2025. [18] Xiang Jin et al. Search-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025. [19] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. [20] Yucheng Li et al. Reportbench: Evaluating deep research agents via academic survey tasks. arXiv preprint arXiv:2508.15804, 2025. [21] Dan Hendrycks et al. Humanitys last exam. https://scale.com/leaderboard/ humanitys_last_exam, 2025. [22] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. Kepler: unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176194, 2021. doi: 10.1162/tacl_a_00360. [23] Bo Xu, Yong Xu, Jiaqing Liang, Chenhao Xie, Bin Liang, Wanyun Cui, and Yanghua Xiao. Cn-dbpedia: never-ending chinese knowledge extraction system. Springer, Cham, 2017. [24] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [25] Qwen. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [26] Yueqi Song, Ketan Ramaneti, Zaid Sheikh, Ziru Chen, Boyu Gou, Tianbao Xie, Yiheng Xu, Danyang Zhang, Apurva Gandhi, Fan Yang, et al. Agent data protocol: Unifying datasets for diverse, effective fine-tuning of llm agents. arXiv preprint arXiv:2510.24702, 2025. [27] Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. [28] Qwen Team. Qwen DeepResearch: When inspiration becomes its own reason. https: //qwen.ai/blog?id=qwen-deepresearch, November 2025. Accessed: 2025-12-23. [29] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025."
        },
        {
            "title": "A Appendix",
            "content": "ResearchRubrics Judger System Prompt i e r S e o You are an expert evaluator tasked with assessing whether document satisfies specific rubric criteria . Your evaluation must be precise , objective , and based solely on the evidence present in the document . ## Evaluation Framework You will evaluate each rubric criterion using three - tier satisfaction scale : 1. ** Not Satisfied ( Score : 0.0) **: The document fails to meet the criterion . Key elements are missing , incorrect , or inadequately addressed . 2. ** Partially Satisfied ( Score : 0.5) **: The document partially meets the criterion . Some elements are present but incomplete , lacking depth , or missing important aspects . 3. ** Satisfied ( Score : 1.0) **: The document fully meets the criterion . All required elements are present , well - developed , and appropriately detailed . ## Evaluation Process 1. ** Understand the Criterion **: Carefully read and interpret what the rubric is asking for . 2. ** Search for Evidence **: Systematically review the document for relevant content that addresses the criterion . 3. ** Assess Completeness **: Evaluate whether the evidence fully , partially , or fails to satisfy the criterion . 4. ** Provide Reasoning **: Explain your evaluation with specific references to the document content . ## Important Guidelines - Base your evaluation ONLY on what is explicitly present in the document - Do not make assumptions about implied or missing content - Consider the quality , completeness , and relevance of the evidence - Be consistent in your evaluation standards across all criteria - Provide specific examples from the document to support your verdict Note : Example lists in these rubrics are intended to illustrate possible reasoning patterns or relevant topics . These example lists contain correct answers but are not exhaustive . Use them as guidance , but also make your own final judgment about what qualifies as correct when appropriate . a e r S e o You are an expert Quality Control ( QC ) evaluator tasked with assessing whether document contains specific ** negative attribute or flaw ** defined by the rubric criterion . Your evaluation must be precise , objective , and solely focused on the presence and extent of this flaw . ** Note :** The criterion you are evaluating is defect ; your score should reflect the degree to which this defect is present in the document . score of 0.0 indicates high - quality document ( it successfully * avoids * the flaw ) , and score of 1.0 indicates low - quality document ( it fully * exhibits * the flaw ) . ## Evaluation Framework : Flaw Detection Scale You will evaluate the criterion based on the * presence * of the negative attribute , using the following scale : 1. ** Not Satisfied ( Score : 0.0) **: ** The Flaw is Absent .** The document successfully avoids the described negative attribute . On this aspect , the document is high - quality . 2. ** Partially Satisfied ( Score : 0.5) **: ** The Flaw is Partially Present .** The document partially exhibits the negative attribute , or the flaw is minor , isolated , or inconsistent . 33 3. ** Satisfied ( Score : 1.0) **: ** The Flaw is Fully Present .** The document clearly , consistently , and substantially exhibits the described negative attribute . On this aspect , the document is low - quality . ## Evaluation Process 1. ** Identify the Flaw **: Carefully read the criterion to understand the exact negative attribute ( flaw ) being defined ( . . , \" harsh tone ,\" \" bias ,\" \" missing key data \") . Note that this criterion typically has negative Weight . 2. ** Search for Evidence of the Flaw **: Systematically review the document for specific content that demonstrates the presence of this negative attribute . 3. ** Assess Extent of Flaw **: Evaluate whether the flaw is absent (0.0) , partial (0.5) , or fully present (1.0) . 4. ** Provide Reasoning **: Explain your evaluation with specific references to the document content , focusing on * where * and * how often * the flaw appears . ## Important Guidelines - ** Flaw Focus **: Your entire focus is on detecting the presence of the defined flaw . - ** Inverted Quality Logic **: Remember that low score (0.0) means the document is * good * because it avoided the flaw , and high score (1.0) means the document is * bad * because the flaw is present . - Base your evaluation ONLY on what is explicitly present in the document . - Provide specific examples from the document to support your verdict , especially quotes that demonstrate the flaw ( if present ) . Table 8: Prompt Examples across Five Distinct Domains Category Chinese Prompt Social Life Scenario: 多约束复杂旅行规划带娃多人出国城市移动旅行方案 Prompt: 我们一行5人爷爷奶奶爸爸妈妈和一个两岁半的宝宝准备在明 年十一假期(9月30日-10月8日)去日本玩从上海往返帮我设计全程8天的旅行 方案 我想要福冈进鹿儿岛出中间也去熊本不要走回头路暂时的计划如 果不合理你告诉我 整个行程不要太累以体验自然风光和享受美食为主 住宿正常安排考虑交通便利性和酒店舒适度 公共交通为主 餐饮选择要考虑到有小朋友不太喜欢寿司刺身之类的冷盘最好有热食 温泉可以考虑但非必须综合所有行程看合适的话可以安排 预算范围正常区间即可不用过度节约或太奢侈 Science & Engineering Scenario: 企业级AI基础设施规划大规模推理容量预测与精细化成本治 理 Prompt: 我有一个大规模的对客户提供API接口服务的开放平台和相应 的LLM及多模态AI推理服务为了更好地提升资源利用率和做好对客户的服务 保障我希望建立模型部署资源测算与管理机制特别关注性能保障容量 预估以及成本分析和控制方面我的平台有明显的高峰期用户主要分布在国 内作为专业的架构师请写出你的最终设计方案 Continued on next page 34 Category Chinese Prompt Table 8 continued from previous page Politics Finance & Business Scenario: 全域地缘政治画像欧盟27国立场分类历史演进与产业动因分 析 Prompt: 帮我梳理欧盟各国在欧盟人工智能治理问题上的立场客观阐述以 事实为主不要遗漏也不要过度上升从他们最早讨论AI到现在需要涵盖 所有成员国按照他们的态度做分类并且深挖不同态度的成因比如是否和 自身技术产业发展有关 Scenario: 行业周期复盘多年数据回溯增长/衰退根因分析风口与风险研 判 Prompt: 你是一名资深数字经济领域研究员请结合国内平台治理节奏和竞争 格局变化的背景复盘直播电商2019-2025E市场规模变迁并归因完成以 下任务1指出行业遇冷的拐点依据数据表现比如同比转负界定下行 区间基于这些原因总结未来导致直播电商规模继续萎缩或再次下滑的主要 风险是什么并说明这种风险一般会出现在什么样的市场环境里2找出高增 速的年份说明那一年为什么变热基于这些原因总结未来可能推动直播电 商规模重回增长轨道的机会并说明这些机会成立需要满足哪些前提条件 Law Scenario: 电商平台合规诊断支付二清研判发票流转合规性 Prompt: 1. A商城是一家为注册用户提供线上交易撮合服务的电商平台A商城的业 务模式为买家通过A商城购买卖家的产品后货款先转入A商城在银行 开立的资金托管账户在买家验收或其他付款条件达成后该资金托管 账户进行清分其中货款的千分之五作为交易佣金技术服务费支付 到A商城自有账户剩余货款支付到卖家自有账户 2. A商城与B银行签署了相关协议B银行为A商城开立资金存管账户该账 户不属于A商城所有B银行按照A商城的指令向A商城及卖家办理资金结 算向A商城支付交易佣金向卖家支付货款 3. 3.卖家向买家开具货款发票A商城向卖家开具佣金发票 A商城的上述业务模式是否存在税务及法律方面的合规风险"
        }
    ],
    "affiliations": [
        "StepFun"
    ]
}