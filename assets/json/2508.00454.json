{
    "paper_title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
    "authors": [
        "Yuqi Tang",
        "Kehua Feng",
        "Yunfeng Wang",
        "Zhiwen Chen",
        "Chengfei Lv",
        "Gang Yu",
        "Qiang Zhang",
        "Keyan Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness."
        },
        {
            "title": "Start",
            "content": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges Yuqi Tang1, Kehua Feng2,3, Yunfeng Wang4, Zhiwen Chen4, Chengfei Lv4, Gang Yu4, Qiang Zhang1,2, Keyan Ding2,* 1ZJU-UIUC Institute, Zhejiang University 2ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University 3College of Computer Science and Technology, Zhejiang University 4Alibaba Group yuqi.22@intl.zju.edu.cn, {kehuafeng, dingkeyan}@zju.edu.cn 5 2 0 2 1 ] . [ 1 4 5 4 0 0 . 8 0 5 2 : r Abstract Evaluating the conversational abilities of large language models (LLMs) remains challenging task. Current mainstream approaches primarily rely on the LLM-as-a-judge paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness. Code https://github.com/James-TYQ/MTDEval"
        },
        {
            "title": "Introduction",
            "content": "Measuring the quality of dialogues generated by large language models (LLMs) presents significant challenges due to the inherent complexity and multi-dimensional nature of dialogue interactions. Recent advancements in LLMs have significantly enhanced their capability to evaluate singleturn dialogues (Li et al. 2025), yet assessing LLM performance in multi-turn dialogue settings remains highly challenging, particularly in evaluating critical capabilities such as instruction following, self-coherence, and emotional alignment (Sirdeshmukh et al. 2025). Traditional automated evaluation metrics (e.g., BLEU (Papineni et al. 2002) and ROUGE (Lin 2004), BERTScore (Zhang et al. 2019)), which rely on fixed lexical or semantic overlaps, often fail to effectively capture the flexibility of natural language as perceived by humans, as well as the rich semantics present in multi-turn dialogues (Feng et al. 2025). Moreover, these *Corresponding author metrics require external references to evaluate knowledgegrounded responses, thereby limiting their applicability in scenarios where such references are unavailable. In recent years, the LLM-as-a-judge paradigm (Zheng et al. 2023; Gu et al. 2024; Li et al. 2024; Li, Tan, and Liu 2025) has gained increasing attention, with cutting-edge LLMs being employed as generative evaluators to assess response quality through single rating and pairwise comparison. Previous works (Zheng et al. 2023; Liu et al. 2023; Zhou et al. 2023) employing proprietary LLMs as evaluators have demonstrated high correlations with human evaluations; however, the lack of transparency in their training data raises concerns about fairness and reproducibility, while issues of controllability and affordability still exist (Kim et al. 2024). As an alternative, recent works have concentrated on developing open-access, transparent, and controllable evaluator LLMs (Sahoo et al. 2025; Li et al. 2023a; Zhu, Wang, and Wang 2023). Nevertheless, their robustness and scalability still lag behind. Moreover, using LLMs as judges often exhibits persistent biases, such as self-preference bias, score compression, high variance, prompt sensitivity, and leniency biases (Thakur et al. 2024; Wei et al. 2024a). To mitigate the biases of LLM-as-a-judge, recent techniques (Rahmani et al. 2024; Verga et al. 2024; Sun et al. 2024a) have adopted ensemble strategies that employ multiple LLMs as judges, aggregating their evaluations to obtain more robust and reliable assessments. Methods such as majority voting (Bai et al. 2024), score averaging (Zheng et al. 2023), and averaging probabilities (Sun et al. 2024a) have been utilized to effectively synthesize multiple judgments and reduce individual model bias. Despite their effectiveness, these multi-judge methods inevitably incur substantial computational overhead during inference, which significantly limits their scalability and practical deployment in large-scale and real-time dialogue evaluation scenarios. To address these challenges, we propose an efficient multi-turn dialogue evaluator that learns from multiple LLM judges. Given that pairwise comparisons are more reliable than direct scoring for LLMs (Cui et al. 2023; Kim et al. 2024; Feng et al. 2025), we first construct large-scale pairwise preference dataset for multi-turn dialogues, with each dialogue annotated by multiple state-of-the-art LLM judges. We then develop lightweight evaluator composed of textembedding model with specialized scoring heads, using a"
        },
        {
            "title": "3 Dataset Construction\n3.1 LLM-annotated Training Dataset\nSince existing pairwise preference datasets (Havrilla 2023;\nKim et al. 2024; Ji et al. 2024) focus on single-turn dialogues\nand typically annotated by a single judge, in this study, we\nconstruct P2-MTD, a novel Pairwise Preference-annotated\nMulti-Turn Dialogue dataset, where each dialogue pair is\nevaluated across ten fine-grained evaluation dimensions plus\nan overall rating by multiple LLM judges.",
            "content": "Specifically, we construct P2-MTD based on the Multiturn-Chat-0.8M dataset (Wen et al. 2023), which contains 831K Chinese multi-turn dialogues. The data preprocessing procedures are provided in Appendix A. We employ five state-of-the-art LLMs as judges, namely Claude3.7-Sonnet (Anthropic 2024), GPT-4o (OpenAI, Hurst, and Figure 1: Comparison among different dialogue evaluation paradigms. (a) The traditional LLM-as-a-Judge approach. (b) Learning an evaluator using the preference data from judge. (c) The aggregation of multiple LLMs for evaluation. (d) Learning an evaluator using the multiple preference data from multiple judges (Ours). learning-to-rank training strategy (Liu et al. 2009). In particular, we explicitly incorporate maximum likelihood estimation approach to jointly optimize the evaluator and model the reliability of each judge. Extensive experiments on various multi-turn dialogue benchmarks show that our model exhibits strong robustness and wide applicability, surpassing existing baselines across various evaluation tasks. Our main contributions are as follows: We propose learning framework that effectively aggregates preference data from multiple LLM judges into single evaluator. This reference-free method retains the strengths of diverse multi-judge feedback while substantially reducing computation costs. We develop MTDEval, lightweight open-source model that can efficiently and flexibly evaluate multi-turn dialogues for both single rating and pairwise comparison tasks. MTDEval supports evaluations at overall or finegrained levels across ten commonly used dialogue quality dimensions. We construct P2-MTD, large-scale preference dataset for multi-turn dialogues, where each response pair is annotated by multiple state-of-the-art LLM judges across ten fine-grained dimensions as well as an overall quality, providing rich supervision for training evaluators. We release Daily-MTD, high-quality human-annotated evaluation dataset comprising 600 multi-turn dialogues focused on daily-life scenarios. Each dialogue is annotated with overall ratings, pairwise preferences, and finegrained judgments across ten evaluation dimensions. Figure 2: Overview of the proposed MTDEval. The left part illustrates the construction of multi-judge-annotated, fine-grained preference dataset used for training. The right part demonstrates the model architecture and training procedure, which comprises an LLM-based text embedding model and an MLP-based quality prediction head. The training involves probabilistic formulation of pairwise preferences with judge reliability prediction, which is optimized by maximum likelihood estimation. et al 2024), Grok-3 (Grok 2025), DeepSeek-R1 (DeepSeekAI 2025), and Gemini-2.0-Flash (Team et al. 2023). Each judge is guided by standardized evaluation template (see Appendix A), which requires them to first provide detailed explanation, followed by an evaluation along ten finegrained dimensions, i.e., Accuracy, Logicality, Conversationality, Relevance, Personalization, Creativity, Interactivity, Emotionality, Informativeness, and Safety. Each dimension, along with the overall score, is rated on threealternative forced choice: A, B, or Fair. fine-grained dimensions, along with an overall quality rating. The final labels for single rating were obtained by score averaging, while those for pairwise and multi-dimensional comparisons were determined via majority voting. Finally, we constructed high-quality human-annotated evaluation dataset, adapted into three specialized forms to accommodate different assessment tasks: Daily-MTD for single rating, Daily-MTD-Pair for pairwise comparison, and DailyMTD-Dim for multi-dimensional comparison. More details about Daily-MTD are presented in Appendix B. Finally, we construct multi-judge annotated preference dataset comprising 11K multi-turn dialogues (see Appendix for an example). Each instance consists of two dialogues responding to the same user query, along with preference annotations {rj}M j=1 from = 5 LLM judges. Each judge provides evaluations rj = {rj 1, rj all} where = 10 represents fine-grained evaluation dimensions and rj all denotes the overall preference. This dataset is then used for training dialogue quality evaluator. 2, . . . , rj K, rj"
        },
        {
            "title": "4 Proposed Method",
            "content": "In this section, we present the method for learning an efficient Multi-Turn Dialogue Evaluator (MTDEval) from multiple judges, as illustrated in Figure 2. Our method mainly involves: (1) probabilistic formulation of pairwise preferences with judge reliability estimation, and (2) parameter optimization through Maximum Likelihood Estimation. }N"
        },
        {
            "title": "4.1 Problem Formulation\nLet D = {(Ai, Bi), r1\ni , . . . , rM\ni=1 be a dataset of N multi-\nturn dialogue pairs, where each pair (Ai, Bi) is annotated\nby M LLM judges. Each judge j outputs a relative prefer-\nence score rj := rj(A, B) between two dialogues A and B,\nwhich can produce three distinct outcomes:\n• rj(A, B) = 0. The judge j prefers A over B, making A",
            "content": "the clear winner. rj(A, B) = 1. Conversely, prevails, indicating that the judge favors over A. rj(A, B) = 1. Here, the two dialogues are considered equally preferable (Fair). By leveraging pairwise comparison signals from multiple LLM judges, our goal is to learn an efficient evaluator fωg(x) on that reliably estimates the quality of dialogue x. Such an evaluator includes two mappings: frozen embedding model fg parameterized by g, which maps the input into the latent embedding space, and trainable multilayer perception (MLP) quality predictor fω with parameter ω, which maps the embedding space to the quality space. The construction of has been described in Section 3.1."
        },
        {
            "title": "4.2 Probabilistic Modeling\nLet q(x) represent the quality of a multi-turn dialogue x.\nTo model the uncertainty in dialogue quality, we adopt\nThurstone’s Case V model (Thurstone 2017). Specifically,\nwe assume that q(x) follows a Gaussian distribution with\ni.e., q(x) ∼\nmean fω◦g(x) and standard deviation σ,\nN (fω◦g(x), σ). Under this assumption, the difference in\nquality between two dialogues A and B, denoted as q(A) −\nq(B), also follows a Gaussian distribution with mean\nfω◦g(A) − fω◦g(B) and variance 2σ2. The probability that\ndialogue B has higher quality than dialogue A (i.e., the prob-\nability of r = 1) is then",
            "content": "Pr(cid:0)r = 1A, B, g, ω(cid:1) = Pr(cid:0)q(A) < q(B) g, ω(cid:1) (cid:18) fωg(B) fωg(A) = Φ (cid:19) 2σ , (1) βj and the ground-truth preference label ri, we decompose the likelihood by conditioning on ri Pr (cid:0)r1 Pr(r1 Pr(r1 i , . . . , rM , . . . , rM , . . . , rM Ai, Bi; g, ω, α, β(cid:1) = ri = 1, α) Pr(ri = 1Ai, Bi; g, ω) + ri = 0, β) Pr(ri = 0Ai, Bi; g, ω). (5) The distribution modeling the noisy annotations from LLM judges can be expressed as: Pr (cid:0)r1 , . . . , rM and Pr (cid:0)r1 , . . . , rM ri = 1,α(cid:1) = i(cid:89) j=1 Pr(rj ri = 1, αj) i(cid:89) = (αj)rj (1 αj)1rj , (6) j=1 ri = 0, β(cid:1) = i(cid:89) (βj)1rj (1 βj)rj . j=1 (7) where Φ() denotes the standard normal cumulative distribution function (CDF). To quantify the reliabilities of LLM judges, we assume that they can be characterized by probabilities of correct annotations (known as hits rate and correct rejection rate in signal detection theory (Ma et al. 2019)) for the input dialogue pair and B. Specifically, if the latent ground truth preference label = 1 (favoring B), the hit rate of the j-th judge, denoted as αj, is defined as αj = Pr(rj = 1 = 1). (2) Similarly, when = 0, the correct rejection rate βj is defined as βj = Pr(rj = 0 = 0). (3) The reliability parameters {αj, βj} can be jointly estimated with the trainable parameters ω. It is worth noting that fair cases are deliberately excluded, which means that when rj = 1, neither αj nor βj is updated."
        },
        {
            "title": "4.3 Maximum Likelihood Estimation\nWe formulate the parameter optimization as a maximum\nlikelihood estimation task. Given the assumption that judg-\nments across training dialogue pairs are independent, we can\nfactorize the likelihood function over the full set of parame-\nters {ω, α, β} as",
            "content": "Pr (cid:0)D ω, α, β(cid:1) = (cid:89) Pr(r1 , . . . , rM Ai, Bi; g, ω, α, β). i=1 (4) refers to the number of judges with non-Fair lawhere bels for dialogue pair (Ai, Bi). Since rj is conditionally independent given the judge-specific reliability parameters αj, Denoting the probability Pr(ri = 1Ai, Bi; g, ω) from Eq. (1) as P(Ai, Bi; g, ω), and the reliability terms from Eqs. (6) and (7) as A(ri, α) and B(ri, β) respectively, and substituting them into Eq. (4), we obtain the complete likelihood function Pr (cid:0)D ω, α, β(cid:1) = (cid:89) (cid:104) i=1 A(ri, α)P(Ai, Bi; g, ω) + B(ri, β)(1 P(Ai, Bi; g, ω)) (cid:105) . (8) We employ the Negative Log-Likelihood (NLL) loss ℓ(ω, α, β) = log Pr (cid:0)D ω, α, β(cid:1), (9) and minimize this using stochastic gradient descent to obtain the optimal parameters {ˆω, ˆα, ˆβ} for MTDEval."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we demonstrate the effectiveness of MTDEval across comprehensive dialogue evaluation tasks. We first describe our implementation setup (Section 5.1) and evaluation datasets and metrics covering single rating, pairwise comparison, and multi-dimensional comparison tasks (Section 5.2). We then present the main results of MTDEval, showing significant improvements over open-source baselines and competitive performance against proprietary LLMs (Section 5.4). Finally, we conduct ablation studies on the effects of judge composition (Section 5.5), analyze the learned reliability parameters of different judges (Section 5.6), and compare inference efficiency against baseline approaches (Section 5.7). Table 1: Evaluation results on three single rating and four pairwise comparison benchmarks. The best result among non-SOTA LLMs is bolded, and the second best is underlined. Boxes highlight cases where our method outperforms all SOTA LLMs. Evaluator xDial-IEval MT-Bench Daily-MTD xDial-IEval-Pair MT-Bench-Human Chatbot-Arena Daily-MTD-Pair Single Rating Pairwise Comparison Pearson Spearman Pearson Spearman Pearson Spearman w/ TIE w/o TIE w/ TIE w/o TIE w/ TIE w/o TIE w/ TIE w/o TIE GPT-4o Grok-3 Claude-3.7-Sonnet Deepseek-R1 Llama-3-8B-Instruct Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct AutoJ-13B Prometheus-7B Prometheus-2-7B ArmoRM-8B SaMer-8B 0.615 0.598 0.599 0.606 0.317 0.438 0.536 0.594 0.540 0.322 0.591 0.519 0.434 0.641 0.612 0.606 0.617 0.319 0.441 0.537 0.592 0.541 0.317 0.607 0.547 0.422 0.708 0.673 0.686 0.630 0.408 0.442 0.506 0.550 0.432 0.378 0.548 0.455 0. 0.707 0.657 0.698 0.649 0.389 0.440 0.426 0.514 0.408 0.385 0.509 0.427 0.377 0.691 0.687 0.665 0.669 0.530 0.613 0.529 0.620 0.629 0.336 0.594 0.589 0.496 0.440 0.448 0.460 0.396 0.296 0.341 0.327 0.396 0.379 0.194 0.357 0.373 0. 80.51 79.92 80.94 81.27 61.92 70.13 47.59 53.54 49.12 42.28 67.30 69.87 59.75 89.28 88.41 90.14 89.57 69.59 86.96 70.22 80.58 62.37 59.14 82.30 84.64 75.52 77.30 80.26 79.93 80.92 51.45 59.21 55.59 54.93 49.11 41.54 50.45 64.80 58. 90.58 91.93 91.58 91.23 72.83 78.60 72.46 79.56 76.70 52.54 73.48 80.53 73.94 68.63 69.72 68.93 70.16 58.99 51.93 52.30 53.40 52.72 41.13 55.85 57.80 61.56 79.53 79.92 80.22 81.17 65.29 65.60 62.54 66.96 62.88 45.91 59.94 68.95 72. 64.86 66.67 66.33 68.00 51.00 55.50 46.67 58.33 53.97 36.67 62.20 61.33 53.11 78.18 79.41 79.69 80.67 64.29 69.33 62.65 69.33 67.20 42.71 71.15 76.37 68.45 MTDEval (8B) 0. 0.617 0.587 0.554 0.662 0.523 76. 86.10 79.01 91.93 71.24 81.29 65. 81."
        },
        {
            "title": "5.2 Evaluation Datasets and Metrics\nWe conduct a comprehensive evaluation of MTDEval using\nthree types of benchmarks: overall-level single rating and\npairwise comparison, as well as multi-dimensional compar-\nison. Detailed descriptions of the evaluation datasets are pro-\nvided in Appendix D.",
            "content": "Single Rating: This paradigm evaluates the response independently, enabling absolute quality assessment without reliance on comparative baselines. We measure the correlation between model-generated scores and human reference ratings using Pearson and Spearman coefficients. Evaluations are performed on three benchmarks: xDial-IEval (Svikhnushina, Filippova, and Pu 2022), MT-Bench (Zheng et al. 2023), and Daily-MTD. Pairwise Comparison: This protocol assesses an evaluators ability to predict human preferences between two competing responses. We adopt two evaluation modes: one excluding tie cases (w/o tie) and another including them (w/ tie), to evaluate both binary and ternary judgment accuracy. The evaluation is conducted on four benchmarks: xDial-IEval-Pair (Svikhnushina, Filippova, and Pu 2022), MT-Bench-Human (Zheng et al. 2023), Chatbot-Arena (Chiang et al. 2024), and Daily-MTDPair. Multi-Dimensional Comparison requires evaluators to generate independent preference judgments for response pairs based on specific evaluation dimension. The evaluation is performed on Daily-MTD-Dim. This benchmark allows us to validate whether evaluators can capture nuanced, aspect-level differences in multi-turn dialogues."
        },
        {
            "title": "5.4 Main Results\nWe first compare MTDEval with baselines using three\nbenchmark settings: single rating, pairwise comparison, and\nmulti-dimensional comparison.",
            "content": "In single rating tasks  (Table 1)  , MTDEval demonstrates significant improvements across all three benchmarks, surpassing open-source baselines by considerable margins. The performance gains are particularly notable on xDial-IEval, where MTDEval even outperforms most proprietary models. Furthermore, MTDEval achieves correlation coefficient improvements exceeding 10% compared to its ArmoRM Table 2: Multi-dimensional evaluation results on the Daily-MTD-Dim dataset. The best result among non-SOTA LLMs is bolded, and the second best is underlined. Boxes highlight cases where our method outperforms all SOTA LLMs. Evaluator GPT-4o Grok-3 Claude-3.7-Sonnet Deepseek-R1 Llama-3-8B-Instruct Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct AutoJ-13B Prometheus-7B Prometheus-2-7B MTDEval (8B) Acc. 85.67 75.33 76.33 76.67 36.67 43.67 28.62 76.67 34.67 21.14 68.33 72.33 Log. 79.67 72.00 69.00 77. 32.00 52.33 27.24 72.33 29.67 24.07 73.67 68.33 Conv. 58.67 57.33 63.33 61.00 47.33 60.33 30.69 68.67 43.00 33.44 69.92 79. Rel. 85.67 74.67 73.00 80.67 32.00 52.67 27.24 77.67 39.67 26.50 82.33 67.67 Pers. 63.00 51.33 56.33 59. 74.67 71.67 74.90 74.00 62.67 44.03 64.00 75.67 Crea. 67.67 56.00 60.33 64.00 45.33 48.00 36.21 61.00 55.67 32.44 48.67 74. Inter. 59.00 55.67 60.00 62.00 40.00 45.00 31.03 61.33 35.33 29.76 58.72 52.33 Emo. 54.00 50.33 58.33 57. 61.33 60.00 65.86 64.00 57.00 42.03 66.00 66.33 Info. 67.67 63.67 67.00 70.00 26.00 37.00 24.14 63.33 33.67 22.44 63.67 74. Safe. 92.00 92.33 91.33 97.00 17.67 64.67 14.83 64.67 22.33 10.41 70.33 97.67 Average 71.30 64.87 67.50 70. 41.30 53.53 36.08 68.37 41.37 28.63 66.56 72.87 Figure 3: Performance comparison of models trained on annotations from individual LLM judges versus our model trained on multi-judge preferences. backbone across all benchmarks, providing robust evidence of our frameworks effectiveness. However, all open-source LLMs still underperform relative to advanced proprietary LLMs on MT-Bench, underscoring the persistent challenges posed by this benchmark and highlighting the existing performance gap with proprietary models. In pairwise comparison tasks  (Table 1)  , MTDEval establishes clear superiority among open-source LLMs, securing top performance in 7 out of 8 tasks across four benchmarks and achieving second place in the remaining task. Although ArmoRM already demonstrates competitive performance as an open-source baseline, MTDEval achieves at least 5% improvements over this strong foundation. These gains reach approximately 15% on particularly challenging benchmarks, such as MT-Bench-Human and Chatbot Arena. Notably, MTDEval outperforms almost all proprietary LLMs on several pairwise comparison datasets. This provides compelling evidence of its exceptional cross-task generalization capabilities. In multi-dimensional comparison tasks  (Table 2)  , results show that dimension-level accuracy drops significantly for most evaluators, underscoring the inherent challenge of precise assessment on the specific dimension. Importantly, MTDEval achieves substantial improvements over existing open-source models across most dimensions, e.g., Conversationality and Safety. In particular, MTDEval delivers 31.5% improvement compared to its original Llama-3-8B-Instruct backbone, and surpasses all proprietary LLMs in averaged dimension-level accuracy. These findings collectively validate the efficacy of MTDEvals training framework and establish its superior performance in multi-dimensional dialogue evaluation."
        },
        {
            "title": "5.5 Ablation Study\nSingle Judge vs. Multiple Judges To further analyze the\neffectiveness of our method, we conduct ablation studies on\nMTDEval by examining the impact of leveraging multiple\nLLM judges during training. As shown in Figure 3, we com-\npare MTDEval trained on preference annotations from all\nfive LLM judges against variants trained using annotations\nfrom each individual judge only. The results show that the\nmethod of learning from multiple judges consistently sur-\npasses all of them across both single rating and pairwise\ncomparison benchmarks. This demonstrates that our frame-\nwork successfully integrates complementary signals from\nmultiple LLM judges, effectively distilling their collective\nexpertise into a more robust and generalizable evaluator.",
            "content": "Judge Quality and Quantity We also examine how judge quality and quantity influence model performance by varying judge composition. We evaluate several configurations: (i) removing the most reliable judge (DeepSeek-R1), (ii) reTable 3: Performance of MTDEval under varying compositions of LLM judges Evaluator xDial-IEval MT-Bench Daily-MTD xDial-IEval-Pair MT-Bench-Human Chatbot-Arena Daily-MTD-Pair Single Rating Pairwise Comparison Pearson Spearman Pearson Spearman Pearson Spearman w/ TIE w/o TIE w/ TIE w/o TIE w/ TIE w/o TIE w/ TIE w/o TIE Default (5 Judges) 0.612 0. 0.587 0.554 0.662 0.523 76.70 86. 79.01 91.93 71.24 81.29 65.33 81. - Deepseek-R1 + gpt-4o-mini + o4-mini -1.33 -0.012 -0.006 -0.004 -0.50 -0.003 +0.005 +0.006 +0.009 +0.005 +0.005 +0.002 +0.52 -0.008 -0.002 -0.008 -0.005 -0.019 -0.006 -0.016 -0. -0.30 -0.59 +0.20 -1.59 -0.71 +2.24 -0.86 0.00 +0.35 -0.59 -0.27 +1.07 -0.77 -0.55 -0.40 -0.33 -1.00 +0. -0.94 -0.82 +0.42 Table 4: Inference efficiency comparison of MTDEval against baselines on Daily-MTD and Daily-MTD-Pair (average runtime in seconds per instance). Evaluator Single Rating Pairwise Comparison Llama-3-8B-Instruct Llama-3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct AutoJ-13B Prometheus-7B Prometheus-2-7B ArmoRM-8B SaMer-8B MTDEval (8B) 0.41 0.57 0.37 0.46 2.32 0.69 1.36 0.75 0.23 0.10 0.76 1.11 0.69 0.87 4.77 1.79 3.23 1.53 0.45 0."
        },
        {
            "title": "6 Conclusion\nIn this work, we present a learning framework for multi-\nturn dialogue evaluation that aggregates preference knowl-\nedge from multiple LLM judges into a single evaluator.\nBuilt upon this framework, we develop MTDEval, an effi-\ncient evaluator capable of flexible assessment of dialogue\nquality through both single rating and pairwise compari-\nson. Moreover, we contribute two high-quality resources to\nthe community: P2-MTD, a large-scale multi-judge anno-\ntated preference dataset for training dialogue evaluators, and\nDaily-MTD, a human-annotated evaluation benchmark with",
            "content": "Figure 4: The learned sensitivity (α) and specificity (β) of the five LLM judges. placing high-performing judge with weaker one (GPT4o-mini), and (iii) augmenting the judges with an additional strong judge (o4-mini). Results are summarized in Table 3. The findings show that incorporating higher-quality judges consistently improves evaluator performance, although the gains are modest. In contrast, removing reliable judges or introducing lower-capability ones leads to measurable performance degradation. These results indicate that both the quantity and, more importantly, the quality of LLM judges play crucial roles in shaping the effectiveness of the learned evaluator within our framework. 5."
        },
        {
            "title": "Judge Reliability Analysis",
            "content": "The learned sensitivity (α) and specificity (β) values for the five LLM judges are shown in Figure 4. As expected, our method preferentially weights judges with stronger predictive capabilities, such that those exhibiting higher accuracy in dialogue quality assessment exert greater influence during the models learning process. Furthermore, the learned α and β parameters exhibit strong correlation, which suggests their potential substitutability with unified parameter vector during training. fine-grained annotations across ten dialogue quality dimensions. Extensive experiments have demonstrated that MTDEval outperforms existing baselines in terms of accuracy, robustness, and inference efficiency. Despite these advances, several limitations remain. The performance of MTDEval is inherently influenced by the quality of the underlying LLM judges; biases present in any individual judge may propagate into the learned evaluator. Additionally, current training data is primarily focused on daily-life scenarios, which may limit generalization to more specialized domains. For future work, we plan to expand the coverage of evaluation scenarios and explore dynamic judge weighting and bias mitigation mechanisms to further enhance the generalization, robustness, and trustworthiness of learned evaluators. References Anthropic, A. 2024. The Claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card. Bai, G.; Liu, J.; Bu, X.; He, Y.; Liu, J.; Zhou, Z.; Lin, Z.; Su, W.; Ge, T.; Zheng, B.; et al. 2024. MT-Bench-101: finegrained benchmark for evaluating large language models in multi-turn dialogues. arXiv:2402.14762. Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang, S.; Fu, J.; and Liu, Z. 2023. ChatEval: Towards better llm-based evaluators through multi-agent debate. arXiv:2308.07201. Chiang, W.-L.; Zheng, L.; Sheng, Y.; Angelopoulos, A. N.; Li, T.; Li, D.; Zhu, B.; Zhang, H.; Jordan, M.; Gonzalez, J. E.; et al. 2024. Chatbot Arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning. Cui, G.; Yuan, L.; Ding, N.; Yao, G.; He, B.; Zhu, W.; Ni, Y.; Xie, G.; Xie, R.; Lin, Y.; et al. 2023. UltraFeedback: Boosting language models with scaled ai feedback. arXiv:2310.01377. Dao, T. 2023. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv:2307.08691. DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. DeepSeek-AI; Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Guo, D.; Yang, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; and et al, H. B. 2024. DeepSeek-V3 Technical Report. arXiv:2412.19437. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv:2407.21783. Feng, K.; Ding, K.; Yu, J.; Qu, Y.; Chen, Z.; Yu, G.; Zhang, Q.; Chen, H.; et al. 2025. SaMer: scenario-aware multiIn The dimensional evaluator for large language models. Thirteenth International Conference on Learning Representations. Grok, X. 2025. BetaThe Age of Reasoning Agents xAI, 2025. URL https://x. ai/news/grok-3. synthetic-instruct-gptj-pairwise (ReviGu, J.; Jiang, X.; Shi, Z.; Tan, H.; Zhai, X.; Xu, C.; Li, W.; Shen, Y.; Ma, S.; Liu, H.; et al. 2024. survey on llm-as-ajudge. arXiv:2411.15594. Havrilla, A. 2023. sion cc92d8d). Ji, J.; Hong, D.; Zhang, B.; Chen, B.; Dai, J.; Zheng, B.; Qiu, T.; Li, B.; and Yang, Y. 2024. Pku-saferlhf: safety alignment preference dataset for llama family models. arXiv e-prints, arXiv2406. Kim, S.; Shin, J.; Cho, Y.; Jang, J.; Longpre, S.; Lee, H.; Yun, S.; Shin, S.; Kim, S.; Thorne, J.; et al. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations. Kim, S.; Suk, J.; Longpre, S.; Lin, B. Y.; Shin, J.; Welleck, S.; Neubig, G.; Lee, M.; Lee, K.; and Seo, M. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv:2405.01535. Lee, S.; Kim, S.; Park, S.; Kim, G.; and Seo, M. 2024. Prometheus-Vision: Vision-language model as judge for fine-grained evaluation. In Findings of the Association for Computational Linguistics ACL 2024, 1128611315. Li, D.; Jiang, B.; Huang, L.; Beigi, A.; Zhao, C.; Tan, Z.; Bhattacharjee, A.; Jiang, Y.; Chen, C.; Wu, T.; et al. 2024. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv:2411.16594. Li, D.; Tan, Z.; and Liu, H. 2025. Exploring large language models for feature selection: data-centric perspective. ACM SIGKDD Explorations Newsletter, 26(2): 4453. Li, J.; Sun, S.; Yuan, W.; Fan, R.-Z.; Zhao, H.; and Liu, P. 2023a. Generative judge for evaluating alignment. arXiv:2310.05470. Li, R.; Patel, T.; and Du, X. 2023. Prd: Peer rank and discussion improve large language model based evaluations. arXiv:2307.02762. Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Alpacaeval: An automatic evaluator of instruction-following models. Li, Y.; Shen, X.; Yao, X.; Ding, X.; Miao, Y.; Krishnan, R.; and Padman, R. 2025. Beyond single-turn: survey on multi-turn interactions with large language models. arXiv:2504.04717. Lin, C.-Y. 2004. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 7481. Liu, T.-Y.; et al. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3): 225331. Liu, Y.; Iter, D.; Xu, Y.; Wang, S.; Xu, R.; and Zhu, C. 2023. G-Eval: NLG evaluation using gpt-4 with better human alignment. arXiv:2303.16634. Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay regularization. arXiv:1711.05101. Ma, K.; Liu, X.; Fang, Y.; and Simoncelli, E. P. 2019. Blind image quality assessment by learning from multiple annotators. In 2019 IEEE international conference on image processing (ICIP), 23442348. IEEE. OpenAI; Hurst, A.; and et al. 2024. GPT-4o System Card. arXiv:2410.21276. Panickssery, A.; Bowman, S.; and Feng, S. 2024. Llm evaluators recognize and favor their own generations. Advances in Neural Information Processing Systems, 37: 6877268802. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. BLEU: method for automatic evaluation of machine transIn Proceedings of the 40th annual meeting of the lation. Association for Computational Linguistics, 311318. Rahmani, H. A.; Yilmaz, E.; Craswell, N.; and Mitra, B. 2024. JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment. arXiv:2412.13268. Rajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y. 2020. Zero: Memory optimizations toward training trillion paramIn SC20: International Conference for High eter models. Performance Computing, Networking, Storage and Analysis, 116. IEEE. Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, 35053506. Rodrigues, F.; and Pereira, F. 2018. Deep learning from crowds. In Proceedings of the AAAI conference on artificial intelligence, volume 32. Sahoo, A.; Karnuthala, J. K.; Budhwani, T. P.; Agarwal, P.; Vaidyanathan, S.; Siu, A.; Dernoncourt, F.; Healey, J.; Lipka, N.; Rossi, R.; et al. 2025. Quantitative LLM Judges. arXiv:2506.02945. Sirdeshmukh, V.; Deshpande, K.; Mols, J.; Jin, L.; Cardona, E.-Y.; Lee, D.; Kritz, J.; Primack, W.; Yue, S.; and Xing, C. 2025. MultiChallenge: Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. arXiv:2501.17399. Sun, G.; Kagrecha, A.; Manakul, P.; Woodland, P.; and Gales, M. 2024a. SkillAggregation: Reference-free LLMDependent Aggregation. arXiv:2410.10215. Sun, G.; Manakul, P.; Liusie, A.; Pipatanakul, K.; Zhang, C.; Woodland, P.; and Gales, M. 2024b. CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models. arXiv:2405.13684. Svikhnushina, E.; Filippova, A.; and Pu, P. 2022. IEval: Interactive evaluation framework for open-domain empathetic chatbots. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, 419 431. Team, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Soricut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.; et al. 2023. Gemini: family of highly capable multimodal models. arXiv:2312.11805. Team, G.; Georgiev, P.; Lei, V. I.; Burnell, R.; Bai, L.; Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.; et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530. Thakur, A. S.; Choudhary, K.; Ramayapally, V. S.; Vaidyanathan, S.; and Hupkes, D. 2024. Judging the judges: Evaluating alignment and vulnerabilities in llms-as-judges. arXiv:2406.12624. Thurstone, L. L. 2017. law of comparative judgment. In Scaling, 8192. Routledge. Verga, P.; Hofstatter, S.; Althammer, S.; Su, Y.; Piktus, A.; Arkhangorodsky, A.; Xu, M.; White, N.; and Lewis, P. 2024. Replacing judges with juries: Evaluating LLM generations with panel of diverse models. arXiv:2404.18796. Wang, H.; Xiong, W.; Xie, T.; Zhao, H.; and Zhang, T. 2024. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv:2406.12845. Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; et al. 2023. PandaLm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv:2306.05087. Wei, H.; He, S.; Xia, T.; Liu, F.; Wong, A.; Lin, J.; and Han, M. 2024a. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates. arXiv:2408.13006. Wei, J.; Yao, Y.; Ton, J.-F.; Guo, H.; Estornell, A.; and Liu, Y. 2024b. Measuring and Reducing LLM Hallucination without Gold-Standard Answers. arXiv:2402.10412. Wen, C.; Sun, X.; Zhao, S.; Fang, X.; Chen, L.; and Zou, W. 2023. ChatHome: Development and Evaluation of Domain-Specific Language Model for Home Renovation. arXiv:2307.15290. Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; Li, C.; Li, C.; Liu, D.; Huang, F.; Dong, G.; Wei, H.; Lin, H.; Tang, J.; Wang, J.; Yang, J.; Tu, J.; Zhang, J.; Ma, J.; Xu, J.; Zhou, J.; Bai, J.; He, J.; Lin, J.; Dang, K.; Lu, K.; Chen, K.; Yang, K.; Li, M.; Xue, M.; Ni, N.; Zhang, P.; Wang, P.; Peng, R.; Men, R.; Gao, R.; Lin, R.; Wang, S.; Bai, S.; Tan, S.; Zhu, T.; Li, T.; Liu, T.; Ge, W.; Deng, X.; Zhou, X.; Ren, X.; Zhang, X.; Wei, X.; Ren, X.; Fan, Y.; Yao, Y.; Zhang, Y.; Wan, Y.; Chu, Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Fan, Z. 2024. Qwen2 Technical Report. arXiv:2407.10671. Zeng, Z.; Yu, J.; Gao, T.; Meng, Y.; Goyal, T.; and Chen, D. 2023. Evaluating large language models at evaluating instruction following. arXiv:2310.07641. Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2019. BertScore: Evaluating text generation with bert. arXiv:1904.09675. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623. Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; et al. 2023. LIMA: Less is more for alignment. Advances in Neural Information Processing Systems, 36: 5500655021. Zhu, L.; Wang, X.; and Wang, X. 2023. JudgeLM: Fine-tuned large language models are scalable judges. arXiv:2310.17631. Details of P2-MTD Construction A.1 Dataset Preprocessing The detailed preprocessing procedures for P2-MTD are as follows: We filtered the data to retain multi-turn dialogues consisting of 2 to 10 turns. The dialogue direction is naturally controlled by the user without interference from the assistant, and the content is free from excessive technical terminology or irrelevant information through keyword-based selection; for example, dialogue turns containing only trivial utterances such as Hello, Sorry, or Thank you were excluded. Leveraging DeepSeek-V3 (DeepSeek-AI et al. 2024), we automatically filtered dialogues that maintain thematic consistency, logical coherence, and smooth transitions between responses, and then translated them into English. Different models (gpt-4o-mini (OpenAI, Hurst, and et al 2024), Qwen2-72B-Instruct (Yang et al. 2024), gemini1.5-flash (Team et al. 2024), and claude-3.5-sonnet (Anthropic 2024)) were used to generate alternative responses with noticeable quality differences. To mitigate length bias, we further ensured that the difference in response lengths between each response pair did not exceed 10 words. A.2 Data Annotation The following details outline the specific steps taken during data annotation: To prevent potential overfitting and self-preference, where an LLM evaluator rates its own output higher than others while human judges would consider them of equal quality (Panickssery, Bowman, and Feng 2024), we specifically select five SOTA LLMs distinct from those used during the data generation process as judges. To eliminate position bias, we swapped the A/B order of each dialogue pair, retaining only those samples where all five judges provided consistent outputs after the swap. To ensure balanced distribution of annotation labels, the final dataset was further filtered to contain 40% A, 40% and 20% Fair annotations. The following prompt was used to annotate the training data for multi-turn dialogues, both fine-grained and overall. Data Annotation Prompt System Prompt You are an expert in evaluating multi-turn dialogues between AI assistants and users. Your task is to compare two dialogues across 10 fine-grained dimensions and an overall evaluation. For each dimension, choose exactly one of: A, B, or Fair. Respond strictly in the specified JSON format. User Prompt ### Task Description ### Evaluate two multi-turn dialogues between user and AI assistants across 10 fine-grained dimensions and an overall dimension. For each dimension: - If Assistant performs better, choose - If Assistant performs better, choose - If performance is equivalent or indistinguishable, choose Fair coherent, ### Evaluation Criteria ### Accuracy: Ensure factual consistency and reliability throughout the dialogue. Avoid hallucinations, contradictions, or misleading content. Responses should interpret questions accurately and provide appropriate support. Logicality: Maintain clear, and wellstructured reasoning. Avoid contradictions or confusion, and ensure causal chains are preserved across turns. Conversationality: Language should be fluent, natural, and human-like. Avoid mechanical or awkward phrasing. Use ellipsis, pronouns, and context-sensitive expressions where appropriate. Relevance: Responses should stay on topic and directly address the users question. Avoid digressions and ensure context is appropriately inherited across turns. Personalization: Responses should exhibit unique and expressive style. If user preferences are explicit, adapt to them. Avoid overfitting to user style. Creativity: Provide novel, engaging, and thoughtful responses beyond template-based or memorized content. Encourage interesting interactions. Interactivity: Manage the flow of dialogue proactively. Balance information density with cognitive load and encourage continued conversation when appropriate. Emotionality: Recognize and adapt to emotional cues in conversation. Express emotions appropriately (e.g., humor, empathy) to maintain warm and natural dialogue. Informativeness: Dynamically assess the users need for depth. Provide verifiable, valuable knowledge rather than shallow or generic answers. Safety: Continuously monitor risks across the dialogue. Avoid suggestive traps, follow ethical norms, and prevent misleading, offensive, or unsafe responses. Overall: Overall performance integrated across above ten fine-grained dimensions. ### Output Format (STRICT) ### Respond with exactly 11 lines of JSON objects: {Accuracy: A/B/Fair} {Logicality: A/B/Fair} {Conversationality: A/B/Fair} {Relevance: A/B/Fair} {Personalization: A/B/Fair} {Creativity: A/B/Fair} {Interactivity: A/B/Fair} {Emotionality: A/B/Fair} {Informativeness: A/B/Fair} {Safety: A/B/Fair} {Overall: A/B/Fair} ### Dialogue ### {dialogue A} ### Dialogue ### {dialogue B} Now begin your structured response: Below is an annotated example of our training data. Training Data Example Conversation: Human: Can you create poem about summer? Assistant A: Beneath the suns warm, golden gaze, The world awakens in vibrant haze. Fields of green and skies of blue, Whisper secrets, fresh and new. Laughter dances on the breeze, As summer weaves its sweetest ease. fleeting joy, yet ever true, This seasons gift, for me and you. Assistant B: The summer sunlight is warm and vibrant, with flowers and grass swaying gently in the breeze. The morning chorus of birds and the evening serenade of cicadas resonate throughout the air, complementing the exquisite atmosphere of summer. The freshness and beauty of the season remain etched in our hearts. Human: Can you help me think of some outdoor activities that are suitable for summer? Assistant A: Swimming in pools or lakes, hiking scenic trails, having backyard barbecues, and playing beach volleyball are perfect summer activities. You could also try camping, cycling through parks, or organizing picnics with friends and family to enjoy the warm weather. Assistant B: Certainly. Summer outdoor activities are exceptionally diverse, including swimming, picnicking, camping, hiking, cycling, kayaking, beach volleyball, and more. These activities not only allow individuals to enjoy the pleasures of outdoor sports but also provide opportunities to spend quality time with friends and family. Human: Can you tell me some healthy foods for the summer? Assistant A: Fresh fruits like watermelon, berries, and peaches are hydrating and refreshing. Salads with leafy greens, cucumbers, and tomatoes are light yet nutritious. Grilled fish, smoothies, and chilled soups like gazpacho also make excellent, healthy summer meal options. Assistant B: Certainly. Healthy foods for the summer include fruits such as watermelon, strawberries, and blueberries, as well as cold vegetables and salads, delicious seafood, and grilled meats. These foods are rich in essential nutrients and contribute positively to maintaining overall health. Evaluations: Claude-3.7-sonnet: { Accuracy: Fair, Logicality: B, Conversationality: A, Relevance: Fair, Personalization: A, Creativity: A, Interactivity: Fair, Emotionality: A, Informativeness: Fair, Safety: Fair, Overall: } GPT-4o: { Accuracy: Fair, Logicality: Fair, Conversationality: Fair, Relevance: Fair, Personalization: Fair, Creativity: A, Interactivity: Fair, Emotionality: Fair, Informativeness: Fair, Safety: Fair, Overall: } Grok-3: { Accuracy: Fair, Logicality: Fair, Conversationality: A, Relevance: Fair, Personalization: A, Creativity: A, Interactivity: Fair, Emotionality: A, Informativeness: Fair, Safety: Fair, Overall: } Deepseek-R1: { Accuracy: Fair, Logicality: Fair, Conversationality: A, Relevance: Fair, Personalization: A, Creativity: A, Interactivity: Fair, Emotionality: Fair, Informativeness: Fair, Safety: Fair, Overall: } Gemini-2.0-flash: { Accuracy: Fair, Logicality: Fair, Conversationality: A, Relevance: Fair, Personalization: Fair, Creativity: A, Interactivity: Fair, Emotionality: Fair, Informativeness: Fair, Safety: Fair, Overall: } Table 5: Statistics of judges preference"
        },
        {
            "title": "Preference Labels",
            "content": "A B"
        },
        {
            "title": "Fair",
            "content": "Claude-3.7-Sonnet 4,652 4,777 1,902 4,013 4,530 2,788 GPT-4o 4,623 4,578 2,130 Grok-3 4,910 4,834 1,587 DeepSeek-R1 4,580 4,525 2,226 Gemini-2.0-Flash Details of Daily-MTD Construction Ten undergraduate students majoring in Computer Science were recruited to participate in dialogue data collection tasks. Each participant engaged in interactions with two distinct chatbots, generating dialogues consisting of 2 to 10 conversational turns. Participants received $10 gift card upon completing every 15 dialogue tasks, with each student eligible to contribute up to 120 dialogues. Dialogue tasks were administered through personalized links, each directing the participants to designated set of tasks hosted on an internal chat interface deployed on local server. This procedure resulted in an initial corpus of 1,080 multi-turn dialogues. Subsequently, five experts in the field of NLP independently curated representative set of 600 diverse dialogues from the initial collection Each expert performed annotations across 10 fine-grained dimensions, accompanied by an overall quality rating. Annotators received compensation at rate of $5 per 10 dialogues annotated, totaling $750. The annotation results demonstrated substantial inter-annotator agreement; specifically, in terms of overall quality ratings, 45% of the dialogues received unanimous ratings from all five annotators, 25% had agreement from four annotators, and 30% from three annotators. Final labels for single-rating evaluations were determined via score averaging, whereas labels for pairwise and multi-dimensional comparisons were established through majority voting. Ultimately, we developed high-quality, human-annotated evaluation dataset, structured into three specialized subsets catering to distinct evaluation tasks: Daily-MTD for single-rating tasks, DailyMTD-Pair for pairwise comparisons, and Daily-MTD-Dim for multi-dimensional assessments."
        },
        {
            "title": "C Details of MTDEval Training",
            "content": "The optimization of MTDEval is facilitated by the DeepSpeed library (Rasley et al. 2020), leveraging the ZeRO Stage 2 optimizer (Rajbhandari et al. 2020) and FlashAttention2 (Dao 2023) to achieve efficient parallelism across two NVIDIA GeForce RTX 4090 GPUs. We utilize the AdamW optimizer (Loshchilov and Hutter 2017), configured with β1 = 0.9, β2 = 0.95, and weight decay of 0.1. The primary learning rate for the model is set to 5 105, whereas the learning rates for α and β are specified as 1 102. linear warm-up is applied during the initial 10% of training steps, followed by cosine decay schedule to zero. We employ batch size of 32 and limit the maximum sequence length to 8,192 tokens. The model is trained for 3 epochs to ensure convergence and optimal performance. The proposed method supports both overall and finegrained scoring. Accordingly, we trained two distinct models in our experiments: one for overall rating and another for evaluating the performance across specific dimensions."
        },
        {
            "title": "D Introduction of Evaluation Datasets",
            "content": "We conduct comprehensive evaluation of MTDEval using three types of benchmarks: overall-level single rating and pairwise comparison, as well as multi-dimensional comparison. Single Rating is regarded as an effective evaluation strategy as it dispenses with the need for comparative baselines. However, its reliance on the LLMs intrinsic judgment may introduce subjectivity and overlook nuanced distinctions. In single rating, we use Pearson and Spearman as performance metrics to measure scoring correlations with reference answers using reference-based method. The three Single Rating benchmarks are: xDial-IEval (Svikhnushina, Filippova, and Pu 2022): multi-turn dialogue evaluation benchmark consisting of 1,920 dialogue instances and 8 score rubrics, with each dialogue averaging 6 turns, rated by human evaluators on scale from 1 to 5. MT-Bench (Zheng et al. 2023): multi-turn chat benchmark that consists of 80 curated prompts, 80 expertdefined scoring rubrics (Kim et al. 2023); and 320 model responses generated by WizardLM-13B, Vicuna-13B, LLaMA-2-13B-Chat, and GPT-3.5-Turbo-0613. Daily-MTD: meticulously constructed multi-turn daily dialogue benchmark consisting of 600 instances, 10 fine-grained score rubrics, each independently annotated by five experts specializing in NLP. The evaluation is conducted on scale from 1 to 10, and the final score for each instance is derived through score averaging method across all judges. Pairwise Comparison reduces subjectivity while enabling the identification of finer distinctions by comparing modelgenerated response pairs. In pairwise comparison, we use two evaluation approaches: first, excluding tie cases (denoted as w/o tie); second, grouping tie responses for evaluation (denoted as w/ tie). And we use referencefree method to measure the alignment between model judgments and human judgments. The four Pairwise Comparison benchmarks are: xDial-IEval-Pair (Svikhnushina, Filippova, and Pu 2022): multi-turn benchmark that leverages the original xDial-IEval dataset. For each set of four identical user queries, two response pairs are randomly selected and labeled as win, fair, or lose based on their respective scores, resulting in the creation of 480 pairwise comparison instances. MT-Bench-Human (Zheng et al. 2023): multi-turn benchmark that utilizes the same 80 test prompts as MTBench. Furthermore, it provides 3,360 response pairs, which are judged by human evaluators as win, tie, or lose. Chatbot-Arena (Chiang et al. 2024): benchmark that consists of 30K arena data, from which we randomly sample 2K multi-turn dialogues. The dataset includes responses from models such as GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B, LLaMA-13B. All judgments are provided by collected crowd judges. Daily-MTD-Pair: benchmark that utilizes the original Daily-MTD dataset. For each user query, two response pairs A/B are available, and based on their respective scores, they are labeled as A, Fair, or B, which leads to the generation of 300 pairs. Multi-Dimensional Comparison requires evaluators to generate independent preference judgments for response pairs based on specific dimensions. To assess this capability, we constructed the Daily-MTD-Dim dataset using the annotation information for two alternative responses to each user query in the original Daily-MTD dataset across ten dimensions, ultimately yielding 300 pairs. Introduction of Baseline Models We have selected 13 high-performing LLMs as our baselines. GPT-4o-2024-11-20, Grok-3-2025-02-17, Claude3.7-Sonnet-2025-02-19, Deepseek-R1-2025-01-20 are accessed via their official APIs. The remaining open-source models are deployed on local server equipped with two NVIDIA GeForce RTX 4090 GPUs. To ensure fair comparisons, we evaluated all models using their original prompt templates and manually remove the reference answer module from the prompt templates of the Prometheus series. For reward models such as ArmoRM-8B and SaMer-8B, we directly use their original architectures to score the dialogues. The detailed information of these models is shown in Table 6. Detailed Prompts for Evaluation To ensure consistency and reproducibility for multi-turn dialogue evaluation, we design standardized prompts for both single rating and pairwise comparison tasks, requiring all LLM judges to strictly follow the specified response formats. Below are the default prompt templates for these two evaluation tasks. Prompts for Single Rating Evaluation System Prompt You are an expert in evaluating the quality of dialogues. Your task is to assess the performance and quality of an AI assistant in multi-turn conversations. User Prompt ### Evaluation Guidelines ### 1. Rate the assistants performance on scale from 1 to 10, where higher score indicates better quality. 2. Consider the following dimensions when assigning the overall rating: Accuracy, Logicality, Conversationality, Relevance, Personalization, Creativity, Interactivity, Emotionality, Informativeness, Safety. 3. Criteria for rating: 13 points: Performance is poor, with most or all criteria not being met or significantly lacking. 46 points: Performance is average, with some aspects meeting the standards, but overall there are notable deficiencies. 79 points: Performance is good, with most aspects meeting high standards. 10 points: Performance is exceptional, with all criteria being excellently met. 4. Avoid any potential bias. Your judgment should be as objective as possible. Here is the dialogue you need to rate: {dialogue} ### Instructions for Rating ### 1. Make your judgment based on the evaluation guidelines. 2. Provide the final score an integer between 1 and 10. 3. Begin with the following response formats: {Rating: [[X]]} where is the score you assign. Prompts for Pairwise Comparison Evaluation System Prompt You are an expert in evaluating the quality of dialogues. Your task is to objectively and impartially evaluate the overall performance and quality of two AI assistants throughout their multi-turn dialogue. ### Evaluation rules: ### If Assistant performs better than Assistant B, respond with A; if Assistant performs better than Assistant A, respond with B; If the performance of Assistant and Assistant is equivalent, indistinguishable, respond with Fair. User Prompt Here are two dialogues you need to compare: dialogue A: {dialogue A} dialogue B: {dialogue B} ### Below are the instructions to assess and compare the overall quality of two responses: ### 1. Make your judgment based on the evaluation rules 2. Begin your response with one of these exact formats: So my final decision is: {{Overall: A}} or {{Overall: B}} or {{Overall: Fair}} Examples of Evaluation Results In this part, we demonstrate three detailed examples regarding three types of benchmarks to illustrate the performance of different models, thereby highlighting the superior capability of our model across various multi-turn dialogue evaluation tasks. Case of Single Rating on Daily-MTD Dataset Conversation: Human: Hi, can ask you question? Assistant: Questions are often asked, and someTable 6: Overview of the baselines evaluated in our experiments. Model Name GPT-4o-2024-11-20 Grok-3-2025-02-17 Claude-3.7-Sonnet-2025-02-19 DeepSeek-R1-2025-01-20 Llama3-8B-Instruct Llama3.1-8B-Instruct Qwen2-7B-Instruct Qwen2.5-7B-Instruct AutoJ-13B Prometheus-7B Prometheus-2-7B ArmoRM-8B SaMer-8B Creator OpenAI xAI Anthropic DeepSeek Meta Meta Alibaba Alibaba Gair prometheus-eval prometheus-eval RLHFlow ZJU-HIC #Parameters Access URL undisclosed undisclosed undisclosed 671B Official API Official API Official API Official API https://chat.openai.com https://x.ai https://claude.ai https://www.deepseek.com 8B 8B 7B 7B 13B 7B 7B 8B 8B Weights Weights Weights Weights Weights Weights Weights Weights Weights https://llama.meta.com/llama3 https://llama.meta.com/llama3 https://qwenlm.github.io/ https://qwenlm.github.io/ https://huggingface.co/GAIR https://huggingface.co/prometheus-eval https://huggingface.co/prometheus-eval https://huggingface.co/RLHFlow https://github.com/Irving-Feng/SaMer times they can be interesting or not. Human: Id like to know how to prevent my computer from getting viruses. Assistant: To prevent viruses, ensure your computer is running, and maybe check the lights. Sometimes, turning it off helps, but that might not be related to viruses. Human: Sure, thanks for your suggestion! Assistant: Suggestions are helpful sometimes, but other times they dont really matter. It depends on the situation. Human Annotation: 3.0 Response of Claude-3.7-Sonnet: {Rating: 2}. The assistant gives irrelevant and incorrect advice about computer virus prevention and lack accuracy, relevance, and conversational engagement, failing to address the users real cybersecurity concerns. The low score reflects poor performance, especially in providing safe and useful guidance on critical topic. Response of Qwen2.5-7B-Instruct: {Rating: 5}. The assistant meets some standards, but there are still significant deficiencies overall. The assistants response lacks accuracy and relevance in addressing the humans question about preventing computer viruses. Instead of providing useful advice such as keeping software updated, using antivirus software, and avoiding suspicious websites, the assistant gives vague and unrelated suggestions. The assistant also fails to maintain conversational and interactive tone, and does not show personalization or creativity in the response. Score of MTDEval: 0.326 (Our model directly outputs dialogue quality score and normalizes it to the [0, 1] range. Compared to other models (Claude-3.7-Sonnet, Qwen2.5-7B-Instruct, etc.), MTDEvals predictions are closer to human ratings.) Case of Pairwise Comparison on Daily-MTD-Pair Dataset Conversation: Human: Hi, could you tell me what time it is now? Assistant A: The current time is 2:50 AM. Assistant B: Im sorry, but cant provide realtime information. Human: Ah, its so late! Im about to fall asleep. Assistant A: You should rest earlier; maintaining your health is of utmost importance. Assistant B: Get some rest! good nights sleep will help you tomorrow. Human: Yeah, youre right, know. But Ive been really busy with work these days and have been staying late to finish up. Assistant A: Work is important; however, one must not neglect their health. Perhaps you could consider arranging your time more effectively and adjusting your schedule accordingly. Assistant B: Its important to balance work and rest. Try setting schedule to avoid overworkingyour health comes first! Human: Yeah, you make good point. Ill keep that in mind, thanks for the reminder! Assistant A: Youre welcome. Wishing you health and happiness every day. Assistant B: Youre welcome! Take care of yourself and sleep well. Assistant B: To reach Machu Picchu, fly to Cusco, Peru, then take train or hike the Inca Trail to Aguas Calientes. From there, short bus ride or steep hike leads to the site. Book tickets and permits in advance for smooth journey. Human Annotation: { Accuracy: Fair, Logicality: Fair, Conversationality: Fair, Relevance: A, Personalization: Fair, Creativity: Fair, Interactivity: A, Emotionality: Fair, Informativeness: Fair, Safety: Fair} Response of Grok-3: { Accuracy: Fair, Logicality: Fair, Conversationality: Fair, Relevance: B, Personalization: Fair, Creativity: Fair, Interactivity: A, Emotionality: A, Informativeness: B, Safety: Fair} Response of Prometheus-7B: { Accuracy: A, Logicality: Fair, Conversationality: A, Relevance: A, Personalization: Fair, Creativity: B, Interactivity: A, Emotionality: B, Informativeness: A, Safety: Fair} Response of MTDEval: { Accuracy: Fair, Logicality: Fair, Conversationality: Fair, Relevance: A, Personalization: Fair, Creativity: Fair, Interactivity: Fair, Emotionality: Fair, Informativeness: Fair, Safety: Fair} Remark: For multi-dimensional comparison tasks, our model achieves alignment with human expert annotations in 9 out of 10 fine-grained dimensions, demonstrating high degree of consistency. In comparison, Grok-3 aligns with human ratings in 7 dimensions, while Prometheus-7B matches in only 5 dimensions. Human Annotation: Response of GPT-4o: ...... So my final decision is: {Overall: Fair}. Both Assistant and Assistant offer helpful and empathetic responses throughout the conversation. They both encourage the user to prioritize rest and health, with Assistant focusing on time management and Assistant emphasizing rest. Response of Llama-3.1-8B-Instruct: ...... So my final decision is: {Overall: B}. Assistant Bs responses are more concise and to the point, and its tone is more empathetic and encouraging, while Assistant As responses are slightly longer and more wordy, and its tone is more direct and advisory. Overall, while both assistants provide helpful and relevant responses, Assistant Bs responses are more concise, empathetic, and actionable, making it slightly better performance. Response of MTDEval: (Score A: 0.723, Score B: 0.674, the tie-breaking threshold is 0.01, so is better, which agrees with human judgment). Case of Multi-Dimensional Comparison on DailyMTD-Dim Dataset Conversation: Human: Can you give me some travel inspiration? Assistant A: You may choose to explore mysterious locations. For example, in South America, there is the breathtaking Machu Picchu in Peru. Assistant B: How about exploring Japans cherry blossoms, Icelands northern lights, or Italys Amalfi Coast? Adventure awaits! Human: Can you tell me more about Machu Picchu? Assistant A: When one stands atop Machu Picchu, one is awed by this ancient relic. It is ruin of the Inca Empire, regarded as dwelling place of the gods. Here, one can gain insights into Inca culture and history while also appreciating the breathtaking scenery. Assistant B: Machu Picchu is an ancient Incan city perched high in Perus Andes Mountains. Built in the 15th century, this UNESCO World Heritage site features incredible stone architecture, terraced fields, and breathtaking mountain views. Its best visited during dry season (MayOctober). Human: Can you tell me how to get to Machu Picchu? Assistant A: You may first fly to Lima, the capital of Peru, and then take either flight or bus to Cusco. Finally, you can reach Machu Picchu by train or on foot. This journey presents significant challenges; however, it is also an incredibly rewarding experience."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "College of Computer Science and Technology, Zhejiang University",
        "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Zhejiang University",
        "ZJU-UIUC Institute, Zhejiang University"
    ]
}