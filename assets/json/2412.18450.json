{
    "paper_title": "3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding",
    "authors": [
        "Tatiana Zemskova",
        "Dmitry Yudin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A 3D scene graph represents a compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with a user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose a method 3DGraphLLM for constructing a learnable representation of a 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 0 5 4 8 1 . 2 1 4 2 : r 3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D Scene Understanding Tatiana Zemskova1, 2, Dmitry Yudin1, 2, 1Artificial Intelligence Research Institute, 2Moscow Institute of Physics and Technology, zemskova@airi.net, yudin@airi.net,"
        },
        {
            "title": "Abstract",
            "content": "A 3D scene graph represents compact scene model, storing information about the objects and the semantic relationships between them, making its use promising for robotic tasks. When interacting with user, an embodied intelligent agent should be capable of responding to various queries about the scene formulated in natural language. Large Language Models (LLMs) are beneficial solutions for user-robot interaction due to their natural language understanding and reasoning abilities. Recent methods for creating learnable representations of 3D scenes have demonstrated the potential to improve the quality of LLMs responses by adapting to the 3D world. However, the existing methods do not explicitly utilize information about the semantic relationships between objects, limiting themselves to information about their coordinates. In this work, we propose method 3DGraphLLM for constructing learnable representation of 3D scene graph. The learnable representation is used as input for LLMs to perform 3D vision-language tasks. In our experiments on popular ScanRefer, RIORefer, Multi3DRefer, ScanQA, Sqa3D, and Scan2cap datasets, we demonstrate the advantage of this approach over baseline methods that do not use information about the semantic relationships between objects. The code is publicly available at https://github.com/CognitiveAISystems/3DGraphLLM."
        },
        {
            "title": "Introduction",
            "content": "In this paper, we consider scene understanding in the context of solving 3D vision-language problems: 3D referred object grounding task, 3D dense scene captioning and 3D visual question answering. The 3D referred object grounding task involves identifying region in 3D scene that corresponds to complex natural language query. This query may describe object properties (e.g., color, size) and spatial relationships (e.g., mug on table). common approach to solving this problem is to assume that one is given 3D reconstruction of the scene (e.g., point cloud, mesh, or NeRF). The goal is to predict the bounding boxes of the region that matches the query. The goal of the dense scene captioning task is to describe the selected object. Finally, the goal of the 3D visual question answering task is to generate text answer to various questions about the properties of the scene. It seems promising to explicitly use three-dimensional scene graph to solve these tasks. The 3D scene graph not only allows storing multimodal information about individual objects within scene but also captures the semantic relationships [Wang et al., 2023a, Koch et al., 2024] and hierarchical organization between them [Werby et al., 2024, Honerkamp et al., 2024]. Additionally, the graph scene representation enables real-time updates for dynamic environments [Rosinol et al., 2021, Özsoy et al., 2023], and supports the application of graph algorithms for tasks such as navigation [Zhou et al., 2023a, He and Zhou, 2024, Honerkamp et al., 2024] or object search based on textual queries [Feng et al., 2021, Chang et al., 2023, Werby et al., 2024, Gu et al., 2024]. The solving of 3D vision-language tasks [Chen et al., 2020, 2021, Azuma et al., 2022] is crucial for embodied intelligent agents. To interact with the user, an intelligent agent must be able to describe Preprint. Under review. Figure 1: Proposed 3DGraphLLM approach leverages 3D semantic scene graph learnable representation supplied as input to an LLM to perform various 3D vision-language tasks. the environment and answer questions about its properties using natural language. Large language models (LLMs) are particularly well-suited for this task, as their advanced capabilities in natural language understanding and common-sense reasoning make them highly effective in interpreting and matching user queries to objects in scene [Hong et al., 2023a, Wang et al., 2024, Gu et al., 2024]. Using LLMs makes it easier to adapt the method to new categories of objects and relationships found in referring expressions. LLMs can also handle complex queries that dont explicitly mention the class name, but instead describe its function (e.g. \"somewhere to sit\"). The 3D scene description input for LLMs can be represented either as text [Gu et al., 2024, Linok et al., 2024, Werby et al., 2024, Honerkamp et al., 2024, Yang et al., 2024, Yuan et al., 2024], or through learnable representations [Hong et al., 2023a, Chen et al., 2023, Huang et al., 2023, Chen et al., 2024, Cheng et al., 2024], which encode objects and their relationships using significantly fewer tokens and their corresponding embeddings than textual description of the scene. These learnable representations enhance the performance of the LLM in generating responses to user queries, while also improving response accuracy through adaptation to 3D scenes. However, current methods [Hong et al., 2023a, Chen et al., 2023, Huang et al., 2023, Chen et al., 2024] for 3D vision-language tasks using LLM and learnable 3D scene representations fail to leverage semantic relationships between objects, relying solely on their spatial coordinates. In this paper, we propose novel learnable representation of 3D scene graph called 3DGraphLLM, designed for use as input to LLM (see Figure 1). This representation consists of list of learnable embeddings for objects within the scene, where each object is represented by subgraph containing the object itself along with several of its nearest neighbors. These object subgraphs are provided to the LLM as sequence of triplets (object1, relation, object2). Semantic relations between objects are embedded using features derived from the semantic edges of the graph, which is generated using state-of-the-art methods for 3D semantic graph generation such as VL-SAT [Wang et al., 2023a]. Our experiments demonstrate that incorporating semantic relationships between objects significantly improves the accuracy of LLM responses for 3D vision-language tasks, outperforming baseline approaches for creating learnable scene representations. To summarize, our contributions are as follows: We introduce 3DGraphLLM, the first method to create learnable 3D scene graph representation for LLMs, enabling the mapping of semantic relationships between objects in the scene to LLMs token embedding space. We propose an algorithm that produces flat sequence of graph embedding tokens using k-nearest neighbor selection with minimum distance filter between objects, optimizing inference speed by reducing the number of tokens required to describe the scene. 3DGraphLLM shows state-of-the-art results for the 3D referred object grounding task on the Multi3DRefer [Zhang et al., 2023] (+5.8% F1@0.5) and ScanRefer [Chen et al., 2020] (+4.4% Acc@0.5) benchmarks and also for the 3D scene captioning on the Scan2Cap dataset Chen et al. [2021] (CIDEr@0.5 +5.8%)."
        },
        {
            "title": "2 Related works",
            "content": "Scene Graphs. The concept of scene graph was initially developed for 2D images, providing structured representation of scenes semantics by incorporating relationships between the semantic elements [Johnson et al., 2015]. In the context of images, scene graphs have proven effective for tasks such as content-based image retrieval [Johnson et al., 2015, Pei et al., 2023], 2D referring expression comprehension [Yang et al., 2019a, Shi et al., 2023, Han et al., 2024], image caption [Yang et al., 2019b, Phueaksri et al., 2023], image generation [Johnson et al., 2018, Farshad et al., 2023]. In 3D scenes, scene graph is commonly used to address robotics challenges such as planning [Werby et al., 2024, Honerkamp et al., 2024], object grounding for navigation [Werby et al., 2024, Gu et al., 2024, Linok et al., 2024, Honerkamp et al., 2024] and manipulation [Honerkamp et al., 2024], as well as scene generation [Zhai et al., 2024, Gao et al., 2024]. Our approach is part of class of methods that utilize an implicit representation of the scene graph, such as OVSG [Chang et al., 2023], which frames the problem of 3D object grounding as subgraph retrieval. 3DGraphQA [Wu et al., 2024] proposes to use the bilinear graph neural network for feature fusion between scene and question graphs for question answering task. Feng et al. [2021] build graph based on text query, which is used to refine the visual graph in order to select from its vertices the one that best fits the description. However, the application scope of this method is limited to specific tasks as 3D referred object grounding with one referred object or question answering. In contrast, we propose more versatile method capable of solving various 3D vision-language tasks. 3D Language Scene Understanding. 3D scene understanding is complex computer vision task that involves identifying the semantic, physical, and functional properties of objects, as well as their mutual relations. One of the goals of 3D scene understanding is to develop methods capable of responding to natural language queries about the scene. The queries may correspond to different visual-language tasks such as 3D referred object grounding [Chen et al., 2020, Zhang et al., 2023, Miyanishi et al., 2024], question answering [Azuma et al., 2022], and dense scene captioning [Chen et al., 2021]. Recent approaches address these queries by reconstructing the scene as 3D mesh [Peng et al., 2023] or point cloud [Zhao et al., 2021, Chen et al., 2022, Zhu et al., 2023], often enhanced with instance segmentation [Zhu et al., 2023]. The emergence of transformer models [Vaswani, 2017] has enabled the development of neural network models that create learnable representation of scene for answering various language queries. MultiCLIP [Delitzas et al., 2023] proposes to align 3D scene representation with text queries and multi-view 2D CLIP [Radford et al., 2021] embeddings to improve the quality of question answering. 3DVG-Transformer [Zhao et al., 2021] and Vil3DRef [Chen et al., 2022] methods introduce modules for modeling spatial relationships between objects to improve the quality of object grounding. 3D-VisTA [Zhu et al., 2023] presents transformer model for aligning 3D object and text representations, coupled with an unsupervised pre-training scheme to solve various 3D vision-text problems using specialized task-specific heads. However, these fully supervised approaches face challenges in generalizing to new tasks and domains. In contrast, leveraging large language models (LLMs) for scene understanding enhances generalization capabilities and taps into the extensive knowledge LLMs contain about the physical world [Hong et al., 2023a]. Large Language Models for Scene Understanding. Large language models (LLMs) offer several advantages for scene understanding, notably enhancing the ability to address complex queries that require common knowledge. LLMs can serve as agents that decompose user queries into elementary tasks, which can then be addressed by other methods [Yang et al., 2024, Yuan et al., 2024]. Additionally, LLMs can act as an interface for reasoning by processing textual descriptions of the scene as input [Linok et al., 2024, Gu et al., 2024]. BBQ [Linok et al., 2024] and ConceptGraphs [Gu et al., 2024] demonstrate that using text-based graph representation with an LLM interface significantly improves the quality of object retrieval compared to using CLIP features of objects. HOV-SG [Werby et al., 2024] construct hierarchical graph consisting of objects, rooms, and floors, and demonstrate the effectiveness of such representation for the task of object grounding given query containing object location hints. The authors of the MOMA [Honerkamp et al., 2024] method propose using hierarchical scene graph together with navigational Voronoi graph as input to LLM to predict high-level policy for object search for navigation and manipulation. However, using text to describe an object in scene graph inevitably leads to the loss of some of the information contained in its RGB point cloud. Additionaly, in the case of using text graph, several hundred tokens may be required to describe one object (its semantic class, pose), which will significantly slow down LLM inference in the case of large number of objects in the scene. Recent advancements have successfully integrated point cloud data into LLMs by employing pretrained point cloud encoders and training adapters to align the resulting representations with the LLM embedding space. 3D-LLM [Hong et al., 2023b] aggregates 3D point cloud features from sequence of 2D images and then solves the grounding problem as prediction of sequence of location tokens added to the LLM dictionary. Chat3D-v2 [Huang et al., 2023] generates 3D feature for each object in the scene and then treats the grounding problem as an object selection problem. LLA3D [Chen et al., 2023] proposes to use set of trainable fixed-length query tokens obtained by interacting potential visual cues, text cues, and object point cloud features in transformer model. Grounded 3D-LLM [Chen et al., 2024] uses referent tokens to decode object masks in point clouds. Additionally, research has demonstrated that incorporating spatial information, such as object coordinates [Huang et al., 2023] or depth maps [Cheng et al., 2024], enhances the accuracy of responses to user queries. Despite recent advances, existing methods do not fully leverage the rich semantic information in object relationships. In this paper, we introduce 3DGraphLLM, method that demonstrates the effectiveness of utilizing semantic relationships between objects to enhance performance across various scene understanding tasks."
        },
        {
            "title": "3 Method",
            "content": "Our approach uses set of point clouds of scene objects as input. The objects point clouds can be obtained either from ground-truth annotations or through state-of-the-art point cloud instance segmentation methods. These point clouds are used to extract scene graph features (see Section 3.1). scene graph consists of nodes representing the objects and edges corresponding to semantic relationships between them. To convert the scene graph into token sequence, we represent each object by an identifier, followed by subgraph comprising the objects nearest neighbors. The relationships between an object and its neighbors are encoded as triplets (objecti, relationij, objectj). The scheme of the 3DGraphLLM approach is shown in Figure 2. For more details on the scene graph representation, refer to Section 3.2. Our training process is two-stage. First, we pre-train the model on dataset for various 3D scene understanding tasks using ground-truth instance segmentation. Next, we fine-tune 3DGraphLLM with predicted instance segmentation of scene point clouds, considering scenario where ground-truth segmentation is unavailable (see Section 3.3). 3.1 Model Architecture The model architecture includes pre-trained encoders for 3D point clouds and their semantic relationships, alongside pre-trained LLM. We train projection layers to map the extracted object features and their relationships into the LLMs token embedding space. Following the approach of Chat-Scene [Huang et al., 2024], we introduce additional object identifier tokens {< OBJi >}n i=1 into the LLMs vocabulary. Here and throughout, we use to denote the number of objects in the scene. These learned identifiers, along with the features from object subgraphs composed of nearest neighbors for each object, are used to create flat representation of the scene graph, which is then fed into the LLM. Object Proposals. We use point clouds of objects in the scene as vertices in the scene graph G. In our experiments, we evaluate 3DGraphLLM in various modes, including ground-truth scene segmentation and instance segmentation using state-of-the-art neural network methods like Mask3D [Schult et al., 2023] and OneFormer3D [Kolodiazhnyi et al., 2024]. Thus, the set of vertices of the graph consists i=1, where Pi Rmi6. Here, mi is the number of points in the i-th object of point clouds {Pi}n proposal of instance segmentation of scene point cloud, and 6 dimensions of each point correspond to its 3D coordinates and RGB color. Object Identifiers. Following the approach in Chat3D-Scene, we add set of learnable identifier tokens {< OBJi >}n i=1 to the LLMs vocabulary for object identification. These tokens allow the model to identify objects in the scene by simply predicting the corresponding object identifier token. In our experiments, we assume maximum of 200 objects per scene. 2D Object Encoder. The results of Chat-Scene demonstrate that adding aggregated 2D DINOv2[Oquab et al., 2023] features increase the LLM performance on 3D vision-language tasks. 4 Figure 2: The overall architecture of our approach. 3DGraphLLM leverages pre-trained encoders for 3D object point clouds and semantic relationships between objects. We introduce trainable layers to map the extracted graph node and edge features into the token embedding space of pre-trained LLM. The scene graph is flattened for input into the LLM, with each object represented by subgraph of its nearest neighbors. To further adapt the LLM to 3D vision-language tasks, we add new object tokens to the LLMs vocabulary and fine-tune it using LoRa. R11024 features as an additional token describing the object Therefore, we add DINOv2 2d subgraph. DINOv2 object features are obtained by aggregating features from the masked multi-view images where masks come from the projection of the objects 3D point cloud. 3D Object Encoder. We extract vertex features using pre-trained Uni3D [Zhou et al., 2023b] encoder, which generates point cloud features aligned with their textual descriptions. Since this model is pre-trained on large dataset, it enables us to produce high-quality graph vertex embeddings across various data domains. For each object point cloud Pi, we extract Uni3D feature vp Edge Feature Encoder. One challenge in generating features for semantic relationships between objects is that most methods for 3D semantic scene graph generation are trained on 3RScan scenes [Wald et al., 2019], while visual grounding tasks are typically tested on ScanNet scenes [Dai et al., 2017]. Although both datasets belong to the indoor scene domain, existing methods struggle with performance in cross-domain testing, resulting in drop in accuracy for the grounding task [Miyanishi et al., 2024]. R11024. To extract semantic relationships between objects, we use VL-SAT [Wang et al., 2023a], method for generating 3D semantic scene graphs from point clouds. One of its key advantages is that it only 5 Table 1: Example of prompt for the language model containing scene graph. System: User: Assistant: chat between curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the users questions. The conversation centers around an indoor scene:[<OBJ001> 2d 14, ] According to the given description, there are brown wooden cabinets, placed on the side of the kitchen, please provide the ID of the object that closely matches this description. <OBJ001>. 4 ...<OBJN> 2d 1 , , 12, , , 1 , 1 , , k1 , k2 2 k k2 requires 3D point cloud coordinates as input during prediction, while leveraging knowledge transfer from the pre-trained CLIP model [Radford et al., 2021]. This allows the method to perform well when applied to new scene domains [Wang et al., 2023a], as confirmed by our experiments (see Section 4.3 and Tables 3 and 4). For each pair of point clouds Pi and Pj, we generate latent feature representing ij R1512, which corresponds to VL-SAT graph neural network feature before their relationship the classification head assigning semantic categories to the graph edges. While VL-SAT predicts fixed set of relationships between objects, these relationships are not mutually exclusive (e.g., \"larger\" and \"close\"). Therefore, we use latent features to capture possible combinations of these semantic relationships. 2D/3D object, and semantic relation projection. To adapt the extracted features for the language model, we use three trainable projection modules: the 2D Object Projection f2d(), which maps the 2D image features of objects, the 3D Object Projection fv(), which maps the point cloud features of objects, and the Semantic Relation Projection fe(), which maps the features of semantic relationships between objects. Therefore, for the i-th object, the 2D and 3D object features are projected to token embeddings and 2d respectively. For the pair of i-th and j-th objects, the semantic relation feature is projected to token embedding ij: = fv(Z 2d 2d ), = fv(Z ), ij = fe(Z ij). (1) 3.2 Flat Graph Representation The scene graph is complete graph because we can generate connections between all pairs of objects. However, such graph contains (n 1) edges between objects, and using the complete graph as sequence for the LLM would significantly increase the sequence length. However, intuitively, the most relevant relationships for answering user questions are those between an object and its nearest neighbors. Therefore, for each object, we consider subgraph of its nearest neighbors. The relationships between objects are encoded using features extracted from point clouds {F ij, {1, ..., n}, {1, ..., n}}, represented as triplet (F i=1 and semantic relations features {F ij, }n , ). When using the complete scene graph, the number of tokens required to describe the scene is 2 + 3n (n 1). For 100 objects, which matches the number of objects in the Mask3D [Schult et al., 2023] instance segmentation, this totals 29900 tokens. By using k-nearest neighbor subgraph, we reduce the token count to + 3n k. As shown in Section 4.3 (see Figure 5), setting = 2 improves accuracy in 3D visual-language tasks while reducing the number of tokens needed to describe scene with 100 objects to 800. Prompt template. Thus, we integrate the scene description as sequence of object subgraphs into the prompt for LLM in the following way, similar to the integration of the list of objects and their embeddings in the Chat-Scene method [Huang et al., 2024]. An example of prompt for LLM containing system prompt, scene description in the form of an object identifier and an object subgraph, user request, and an LLM assistant response is given in Table 1. The sequence describing an object starts with its identification token <OBJi>. Then there are triplets {(F describing the relationship between the object and its nearest neighbors. , ijk , jk )}k jk=1 3.3 Training Strategy Following the strategy used in Chat-Scene[Huang et al., 2024], we implement training approach that involves simultaneously training the projection layers and the language model. We also conduct joint training for various tasks, including visual grounding (ScanRefer [Chen et al., 2020] and Multi3DRefer [Zhang et al., 2023]), 3D scene description (Scan2Cap [Chen et al., 2021]), and 3D 6 visual question answering (ScanQA [Azuma et al., 2022] and SQA3D [Ma et al., 2022]). This adaptation of the tasks is designed for user-assistant interactions, as proposed by the authors of Chat-Scene. During training, we aim to optimize the trainable parameters θ of both the language model and the projection layers to minimize the negative log-likelihood of the target response sres compared to the response predicted by the model. We use the loss function from the Chat-Scene method, adapting it to fit our proposed graph representation of the scene given the input prefix sequence sprefix containing system and user prompts: L(θ) = ℓ (cid:88) i= log (sres sres [1,...,i1], sprefix), (2) where ℓ is the length of the token sequence in the LLM response, sres [1,...,i1] is the sequence generated up to the i-th token. The trainable parameters θ include the parameters of 3D Object Projection and Semantic Relation Projection Layers, added object identifier token embeddings and the language model. We use the encoder for semantic relationships between objects pre-trained using ground-truth (GT) point cloud scene segmentation data [Wang et al., 2023a]. Since the predicted point cloud segmentation typically contains more noise than the GT segmentation, we anticipate that the edge features derived from the GT segmentation will be of higher quality than those from the neural network instance segmentation. To address this problem, we employ two-stage training strategy for 3DGraphLLM. First, we pre-train the projection layers and the language model on the GT instance segmentation data to achieve effective projections of the semantic embeddings of relations and objects into the language models embedding space. Then, we fine-tune 3DGraphLLM using the noisy data from the neural network segmentation."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. We conduct experiments using the ScanNet [Dai et al., 2017] and 3RScan [Wald et al., 2019] scene datasets. For training 3DGraphLLM on ScanNet scenes [Dai et al., 2017], we utilize data from five 3D vision-language benchmarks: visual grounding tasks (ScanRefer [Chen et al., 2020], Multi3DRefer [Zhang et al., 2023]), scene description (Scan2Cap [Chen et al., 2021]), and 3D visual question answering (ScanQA [Azuma et al., 2022], SQA3D [Ma et al., 2022]). Each of these datasets follows standard split into training and validation sets, corresponding to 1201 training scans and 312 validation scans from ScanNet. Additionally, we include the RioRefer dataset [Miyanishi et al., 2024], which provides referring expressions for objects in 3RScan scenes [Wald et al., 2019] splitting into standard training and validation sets (1175 training scans and 157 validation scans). Since our method primarily targets visual grounding tasks, the majority of validation experiments are performed on the ScanRefer, Multi3DRefer, and RioRefer datasets. Implementation details. The projection layers for 3D object features and their semantic relations are three-layer MLPs. In our experiments, we use LLAMA3-8B-Instruct [AI@Meta, 2024], stateof-the-art large language model, as well as Vicuna-1.5-7B [Zheng et al., 2023] for ablation. For fine-tuning the language model, we apply LoRA [Hu et al., 2021] with rank of 16. We use batch size of 8 and train 3DGraphLLM for 3 epochs with an initial learning rate of 0.00002, following cosine annealing schedule. Training is performed on server equipped with an NVIDIA A100 GPU, and the entire training process takes approximately 72 hours. In our experiments, we select = 2 nearest neighbors to construct object subgraphs and, in the case of using Mask3D [Schult et al., 2023] instance scene point cloud segmentation, we use NMS filter and filter that ensures minimum distance between nearest neighbors of 1 cm (see Section 4.3). Evaluation metrics. For the visual grounding task on the ScanRefer [Chen et al., 2020] and RioRefer [Miyanishi et al., 2024] datasets, we use the standard metrics Acc@0.25 and Acc@0.5. prediction is considered true positive if the intersection-over-union (IoU) between the predicted objects 3D bounding box and the ground truth exceeds the thresholds of 0.25 and 0.5, respectively. The Multi3DRefer [Zhang et al., 2023] dataset contains queries that may refer to multiple objects. Therefore, we use the benchmark-standard F1 score at IoU thresholds of 0.25 and 0.5. During ablation experiments, we also assess the quality of object descriptions using the Scan2Cap [Chen et al., 2021] benchmark metrics CIDEr@0.5 and BLEU-4@0.5. For the visual question answering 7 Table 2: Performance comparison of 3DGraphLLM with state-of-the-art approaches for 3D visionlanguage tasks. \"Expert models\" use specialized heads to deal with different 3D vision-language tasks. Our approach falls into the category of \"LLM-based models\" that consider different tasks as different user queries to generative model. denotes the CIDEr metric. ScanRefer Multi3DRefer ScanQA Methods ScanRefer [Chen et al., 2020] MVT [Huang et al., 2022] 3DVG-Trans [Zhao et al., 2021] ViL3DRel [Chen et al., 2022] M3DRef-CLIP [Zhang et al., 2023] Scan2Cap [Chen et al., 2021] ScanQA [Azuma et al., 2022] Sqa3D [Ma et al., 2022] 3D-VisTA [Zhu et al., 2023] BUTD-DETR [Jain et al., 2022] PQ3D [Zhu et al., 2025] ZSVG3D [Yuan et al., 2024] 3D-LLM(Flamingo) [Hong et al., 2023b] 3D-LLM(BLIP2-flant5) [Hong et al., 2023b] Chat-3D v2 [Huang et al., 2023] Scene-LLM [Fu et al., 2024] LL3DA [Chen et al., 2023] Grounded 3D-LLM [Chen et al., 2024] Chat-Scene [Huang et al., 2024] 3DGraphLLM Vicuna-1.5 (ours) 3DGraphLLM LLAMA3-8B (ours) d r E d e - L - - - 42.8 - - - - - - A@0.25 A@0.5 F1@0.25 F1@0.5 37.3 40.8 45.9 47.9 51.9 - - - 50.6 52.2 - 36.4 21.2 30.3 35.9 - - 47.9 55.5 57.0 60.2 - - - - 38.4 - - - - - 50.1 - - - - - - 40.6 52.4 55.4 58.2 24.3 33.3 34.5 37.7 44.7 - - - 45.8 39.8 51.2 32.7 - - 30.4 - - 44.1 50.2 51.3 54.6 - - - - - - 64.9 - 72.9 - 87.8 - 59.2 69.4 77.1 80.0 76.8 72.7 87.7 87.6 83.1 - 45.2 57.1 60.1 63. - - - B-4 - - - - - - 10.1 - 13.1 - - - 7.2 12.0 7.3 12.0 13.5 13.4 14.3 12.1 12.5 Sqa3D EM - - - - - - - 47.2 48.5 - 47.1 - - - - 54.2 - - 54.6 53.1 55.2 Scan2Cap C@0.5 B-4@0.5 - - - - - - - - - - 22.4 35.2 - - - - 34.0 66.9 - - 36.0 80.3 - - - - - - - - - - 36.8 65.2 35.5 70.6 36.3 77.1 36.3 81.2 37.8 82.9 Figure 3: Qualitative examples of 3DGraphLLM performance on the ScanRefer dataset. For each query, we provide an RGB image from the ScanNet dataset showing the selected object, along with visualization of the RGB point cloud. In the point cloud, green points indicate the points that 3DGraphLLM identified as corresponding to the object from the text query, while the green box highlights the ground truth (GT) box for the query. task, we follow the validation strategy from Chat3Dv2, applying CIDEr [Vedantam et al., 2015] and BLEU-4 [Papineni et al., 2002] metrics for ScanQA [Azuma et al., 2022], and exact match accuracy (EM) for SQA3D [Ma et al., 2022]. 4.1 Experimental Results Comparison with state-of-the-art approaches. As shown in Table 2, our method significantly outperforms baseline approaches that use LLMs on the two ScanNet 3D referred object grounding benchmarks, ScanRefer [Chen et al., 2020] and Multi3DRefer [Zhang et al., 2023], as wall on the Scene Captioning benchmark Scan2Cap [Chen et al., 2021]. These results highlight the effectiveness of learnable graph-based scene representation 3D vision-language tasks. Its worth noting that the performance of our method is comparable to state-of-the-art specialized models with separate heads for different language tasks, such as 3D-VisTA [Zhu et al., 2023], PQ3D [Zhu et al., 2025] and M3DRef-CLIP [Zhang et al., 2023]. Notably, 3DGraphLLM demonstrates clear advantage over PQ3D [Zhu et al., 2025] and M3DRef-CLIP [Zhang et al., 2023] on the Multi3DRefer dataset. Qualitative results. Figure 3 shows the qualitative results of 3DGraphLLM on the ScanRefer dataset using Mask3D [Schult et al., 2023] instance scene segmentation. In the left part of the figure, 3DGraphLLM correctly identifies the bed on the right and leverages an additional spatial cue - pants that are lying on the bed. In the right part of the figure, 3DGraphLLM distinguishes the black suitcase next to the refrigerator, despite there being another suitcase farther away from the refrigerator in the scene. 8 Table 3: Ablation study on semantic edges role and training pipeline. denotes the CIDEr metric. Methods 3DGraphLLM-0 Vicuna1.5 3DGraphLLM-2 Vicuna1.5 3DGraphLLM-2 Vicuna1.5 3DGraphLLM-0 LLAMA3-8B 3DGraphLLM-2 LLLAMA3-8B 3DGraphLLM-2 LLLAMA3-8B Pre-train Number of edges 0 2 2 0 2 2 ScanRefer Acc@0.5 50.2 50.1 51.3 52.0 54.3 54.6 Multi3DRefer F1@0.5 52.4 52.7 55.4 55.1 57.3 58.2 87.7 92.2 87.6 84.0 87.4 83.1 B-4 14.3 15.5 12.1 15.8 14.9 12. Sqa3D EM 54.6 54.7 53.1 53.8 54.5 55.2 C@0.5 77.1 80.4 81.2 80.0 85.6 82.9 B-4@0.5 36.3 36.9 36.3 37.5 39.6 37.8 ScanQA Scan2Cap Table 4: Ablation study on semantic edges role depending on quality of instance segmentation. Methods 3DGraphLLM-0 3DGraphLLM-2 3DGraphLLM-0 3DGraphLLM-2 3DGraphLLM-2 3DGraphLLM-2 3DGraphLLM-0 3DGraphLLM-2 3DGraphLLM-2 Instance segmentation GT GT Mask3D Mask3D Mask3D Mask3D (+ NMS) OneFormer3D OneFormer3D OneFormer3D (+NMS) Number of edges 0 2 0 2 2 2 0 2 2 Minimal distance, cm - 0 - 0 1 1 - 0 1 ScanRefer Acc@0.25 48.9 54.4(+5.6%) 46.0 47.3(+1.3%) 48.0(+2.0%) 48.1(+2.1%) 45.4 47.1(+1.7%) 47.5(+2.1%) Acc@0.5 48.9 54.4(+5.6%) 34.2 35.6(+1.4%) 36.2(+2.0%) 36.5(+2.3%) 34.5 35.7(+1.2%) 36.1(+1.6%) 4.2 Ablation Studies. Role of Semantic Relations and Training Pipeline To isolate the impact of using scene graph representation, we conduct an experiment with different LLMs and training pipelines using Mask3D [Schult et al., 2023]instance segmentation. We train version of 3DGraphLLM (3DGraphLLM-0) where the scene is represented as sequence of object identifiers and features extracted by the 2D Object Encoder and the 3D Object Encoder, following the same training pipeline as 3DGraphLLM (3DGraphLLM-2) with two nearest neighbors. The 3DGraphLLM version with zero nearest neighbors serves as baseline, equivalent to the Chat-Scene approach, which uses the same LLM as 3DGraphLLM. As shown in Table 3, incorporating scene graph representation significantly improves the performance of the LLMs across all three 3D VisionLanguage tasks: visual grounding, scene description, and question answering. However, the effect is more noticeable for the more modern LLAMA3-8B-Instruct. The pre-training on GT instance segmentation data improves the quality of the 3D Referred Object Grounding for LLAMA3-8BInstruct and Vicuna-1.5-7B. For LLM Vicuna-1.5-7B, pre-training increases the Scene Captioning quality. For LLAMA3-8B-Instruct, pre-training improves the question answering on the Sqa3D dataset. The most interpretable metrics for the role of semantic edges are the accuracy metrics in the 3D Referred Object Grounding problem, so we keep this pre-training as part of the 3DGraphLLM training pipeline. Figure 4: Comparison of Uni3D object features and VL-SAT semantic edge features for the two nearest neighbors (NNs) based on ground-truth (GT) scene segmentation and Mask3D scene segmentation within the ScanNet training set. Left: Uni3D object features are relatively close for GT point clouds and Mask3D point clouds. Center: using the standard approach for selecting NNs to generate VL-SAT features, the features for pairs of Mask3D point clouds differ significantly from those of GT point clouds. Right: after applying minimum neighbor distance filter for selecting NNs, the VL-SAT features for object pairs from Mask3D instance segmentation align more closely with those from GT instance segmentation. 9 4.3 Ablation Studies. 3D Scene Graph Representation We conduct series of experiments to explore methods for constructing scene graph representation from point cloud. In these experiments, we use frozen version of LLAMA3-8B-Instruct [AI@Meta, 2024], training only the projection layers. We do not introduce new object tokens into the LLMs dictionary and follow three-stage training process, including 3D Object Alignment, 3D Scene Alignment, and Instruction Tuning, as outlined in Chat3D [Wang et al., 2023b]. Quality of instance segmentation. We evaluate how the quality of scene segmentation into objects impacts the performance of 3DGraphLLM. As shown in Table 4, even with noisy neural network segmentation, representing the scene as graph with semantic relationships is still more effective than using simple list of objects. We conduct experiments with different object proposal methods, including OneFormer3D [Kolodiazhnyi et al., 2024] and Mask3D [Schult et al., 2023], but found no significant difference between them for our task. Therefore, in subsequent experiments, we use the Mask3D method to maintain consistency with the baseline Chat3Dv2 approach. Neural network segmentation imperfections impact both the quality of object embeddings generated by the 3D Object Encoder and the embeddings of semantic relations between objects. We perform PCA analysis of Uni3D object embeddings and VL-SAT relation embeddings, comparing results for ScanNet training scenes using GT instance segmentation and Mask3D instance segmentation (see Figure 4). Our analysis shows that, with the standard selection of nearest neighbors, the relation embeddings differ significantly between GT and Mask3D three-dimensional masks. When applying the minimal distance filter, the similarity of VL-SAT relation embeddings significantly increases between GT and Mask3D instance segmentation. Figure 5: Dependence of inference speed and visual grounding accuracy on the number of nearest neighbors in the object subgraph. This experiment utilizes the RioRefer dataset along with GT instance segmentation. By examining the minimum distance between neighboring objects, we observed that duplicate objects were often selected as neighbors. To address this issue, we introduced minimum distance filter of 1 cm between neighboring objects, which made the relationship embeddings from GT masks and Mask3D results more consistent. Additionally, applying this filter improved performance on the visual grounding task, as shown in Table 4. We also experimented with adding an NMS filter to remove duplicates among the vertices that an object may be associated with, with threshold of IoU = 0.99. The results in Table 4 show that adding the filter allows for further improvement of the grounding quality. Number of nearest neighbors. We conducted an experiment to examine how the number of nearest neighbors affects the quality of visual grounding and the speed of model inference, as adding more connections increases the number of tokens used to describe each object. Table 5: Ablation study on spatial relation module on RioRefer dataset (GT Instance segmentation). Methods 3DGraphLLM 3DGraphLLM 3DGraphLLM Edge Number 0 2 Spatial relation Acc@0.5 42.6 48.9(+6.3%) 50.1(+7.5%) This experiment was performed using groundtruth scene segmentation and the RioRefer dataset [Miyanishi et al., 2024], as this setup provides the highest quality embeddings for semantic relations between objects. We vary the number of nearest neighbors in powers of two, capping it at 5 due to GPU memory constraints during training. As shown in Figure 5, increasing the number of nearest neighbors enhances visual grounding quality with slight increase in inference time. 10 Spatial relations. Previous research [Wang et al., 2023b, Huang et al., 2023] has shown that incorporating spatial relationships between objects, represented by 3D coordinates of their bounding boxes, can improve performance in visual grounding tasks. We attempted to integrate spatial relations into our method by using the output of the spatial transformer as the final token in the relation triplets between an object and its nearest neighbors (i.e., triplet (F represents the output of the Chat3Dv2 spatial relation module [Huang et al., 2023]). However, as shown in Table 5, our experiments did not find this approach effective for learning graph representation of scene. ), where rel jk , ijk , rel jk"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose new learnable approach to using 3D semantic scene graph for large language model solving the 3D vision-language tasks. Detailed experiments demonstrate the effectiveness of this approach, which explicitly takes into account semantic relations between objects represented as 3D point clouds. Our approach, called 3DGraphLLM, demonstrated state-of-the-art quality on popular ScanRefer, Multi3DRefer, and Scan2Cap datasets. limitation of the method is significant increase in resource consumption with an increase in the edge number for each graph node. At the same time, we showed that taking into account only two edges for each object demonstrates an acceptable trade-off between performance and model quality. For further development of the work, it seems appropriate to search for the methods to reduce token usage for encoding object relationships in our graph representation. Another important aspect for further work is the creation of methods for generating semantic relations between objects that are robust to imperfections in the instance segmentation of the scene point cloud."
        },
        {
            "title": "References",
            "content": "Ziqin Wang, Bowen Cheng, Lichen Zhao, Dong Xu, Yang Tang, and Lu Sheng. Vl-sat: Visuallinguistic semantics assisted training for 3d semantic scene graph prediction in point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2156021569, 2023a. Sebastian Koch, Narunas Vaskevicius, Mirco Colosi, Pedro Hermosilla, and Timo Ropinski. Open3dsg: Open-vocabulary 3d scene graphs from point clouds with queryable objects and open-set relationships. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1418314193, 2024. Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, and Wolfram Burgard. Hierarchical open-vocabulary 3d scene graphs for language-grounded robot navigation. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. Daniel Honerkamp, Martin Büchner, Fabien Despinoy, Tim Welschehold, and Abhinav Valada. Language-grounded dynamic scene graphs for interactive object search with mobile manipulation. IEEE Robotics and Automation Letters, 2024. Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca Carlone. Kimera: From slam to spatial perception with 3d dynamic scene graphs. The International Journal of Robotics Research, 40(12-14):15101546, 2021. Ege Özsoy, Tobias Czempiel, Felix Holm, Chantal Pellegrini, and Nassir Navab. Labrad-or: lightweight memory scene graphs for accurate bimodal reasoning in dynamic operating rooms. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 302311. Springer, 2023. Kang Zhou, Chi Guo, Huyin Zhang, and Bohan Yang. Optimal graph transformer viterbi knowledge inference network for more successful visual navigation. Advanced Engineering Informatics, 55: 101889, 2023a. Yu He and Kang Zhou. Relation-wise transformer network and reinforcement learning for visual navigation. Neural Computing and Applications, pages 117, 2024. 11 Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. In Proceedings of the IEEE/CVF international conference on computer vision, pages 37223731, 2021. Haonan Chang, Kowndinya Boyalakuntla, Shiyang Lu, Siwei Cai, Eric Jing, Shreesh Keskar, Shijie Geng, Adeeb Abbas, Lifeng Zhou, Kostas Bekris, et al. Context-aware entity grounding with open-vocabulary 3d scene graphs. arXiv preprint arXiv:2309.15940, 2023. Qiao Gu, Ali Kuwajerwala, Sacha Morin, Krishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Openvocabulary 3d scene graphs for perception and planning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 50215028. IEEE, 2024. Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 31933203, 2021. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question In proceedings of the IEEE/CVF conference on answering for spatial scene understanding. computer vision and pattern recognition, pages 1912919139, 2022. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023a. Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, et al. Large language models for robotics: Opportunities, challenges, and perspectives. arXiv preprint arXiv:2401.04334, 2024. Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, and Dmitry Yudin. Beyond bare queries: Open-vocabulary object retrieval with 3d scene graph. arXiv preprint arXiv:2406.07113, 2024. Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David Fouhey, and Joyce Chai. Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 76947701. IEEE, 2024. Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, and Zhen Li. Visual programming for zero-shot open-vocabulary 3d visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2062320633, 2024. Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding, reasoning, and planning, 2023. Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. arXiv preprint arXiv:2312.08168, 2023. Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. arXiv preprint arXiv:2405.10370, 2024. An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. arXiv preprint arXiv:2406.01584, 2024. 12 Yiming Zhang, ZeMing Gong, and Angel Chang. Multi3drefer: Grounding text description to multiple 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1522515236, 2023. Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36683678, 2015. Jiaming Pei, Kaiyang Zhong, Zhi Yu, Lukun Wang, and Kuruva Lakshmanna. Scene graph semantic inference for image and text matching. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(5):123, 2023. Sibei Yang, Guanbin Li, and Yizhou Yu. Cross-modal relationship inference for grounding referring expressions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41454154, 2019a. Hengcan Shi, Munawar Hayat, and Jianfei Cai. Open-vocabulary object detection via scene graph In Proceedings of the 31st ACM International Conference on Multimedia, pages discovery. 40124021, 2023. Zeyu Han, Fangrui Zhu, Qianru Lao, and Huaizu Jiang. Zero-shot referring expression comprehension via structural similarity between images and captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1436414374, 2024. Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. Auto-encoding scene graphs for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068510694, 2019b. Itthisak Phueaksri, Marc Kastner, Yasutomo Kawanishi, Takahiro Komamizu, and Ichiro Ide. An approach to generate caption for an image collection using scene graph generation. IEEE Access, 2023. Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12191228, 2018. Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, Böjrn Ommer, and Nassir Navab. Scenegenie: Scene graph guided diffusion models for image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8898, 2023. Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graphs. Advances in Neural Information Processing Systems, 36, 2024. Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Schölkopf. Graphdreamer: Compositional 3d scene synthesis from scene graphs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2129521304, 2024. Zizhao Wu, Haohan Li, Gongyi Chen, Zhou Yu, Xiaoling Gu, and Yigang Wang. 3d question answering with scene graph reasoning. In ACM Multimedia 2024, 2024. Taiki Miyanishi, Daichi Azuma, Shuhei Kurita, and Motoaki Kawanabe. Cross3dvg: Cross-dataset 3d visual grounding on different rgb-d scans. In 2024 International Conference on 3D Vision (3DV), pages 717727. IEEE, 2024. Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 815824, 2023. Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visual grounding on point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29282937, 2021. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. Advances in neural information processing systems, 35:2052220535, 2022. 13 Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pretrained transformer for 3d vision and text alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29112921, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Alexandros Delitzas, Maria Parelli, Nikolas Hars, Georgios Vlassis, Sotirios Anagnostidis, Gregor Bachmann, and Thomas Hofmann. Multi-clip: Contrastive vision-language pre-training for question answering tasks in 3d scenes. arXiv preprint arXiv:2306.02329, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. NeurIPS, 2023b. Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, et al. Chat-scene: Bridging 3d scene and large language models with object identifiers. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 82168223. IEEE, 2023. Maxim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, and Danila Rukhovich. Oneformer3d: One transformer for unified point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2094320953, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023b. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76587667, 2019. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 45664575, 2015. 14 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-view transformer for 3d visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1552415533, 2022. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In European Conference on Computer Vision, pages 417433. Springer, 2022. Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In European Conference on Computer Vision, pages 188206. Springer, 2025. Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024. Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. arXiv preprint arXiv:2308.08769, 2023b."
        },
        {
            "title": "A Common Failure Cases",
            "content": "We illustrate the most common failure cases of 3DGraphLLM related to spatial relationships in Figure 6. Figure 6: Common failure cases of 3DGraphLLM related to spatial relationships. Left: In the ScanQA dataset, 3DGraphLLM incorrectly identifies the front/back and left.right directions relative to the observer. Right: In the ScanRefer dataset, 3DGraphLLM confuses left and right. The GT object is highlighted in green, and the 3DGraphLLM prediction is highlighted in red."
        },
        {
            "title": "B Functional Queries",
            "content": "We illustrate the ability of 3DGraphLLM to leverage common sense knowledge in its responses to question types not present in the training dataset in Figure 7. Figure 7: Functional queries about the room and objects to the 3DGraphLLM. Left: 3DGraphLLM is capable of answering questions about functional properties of the room and its room type. Right: 3DGraphLLM is capable of answering questions about the functional properties of objects in room."
        }
    ],
    "affiliations": [
        "Artificial Intelligence Research Institute",
        "Moscow Institute of Physics and Technology"
    ]
}