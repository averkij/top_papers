{
    "paper_title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers",
    "authors": [
        "Daiqing Qi",
        "Handong Zhao",
        "Jing Shi",
        "Simon Jenni",
        "Yifei Fan",
        "Franck Dernoncourt",
        "Scott Cohen",
        "Sheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While editing directly from life, photographers have found it too difficult to see simultaneously both the blue and the sky. Photographer and curator, Szarkowski insightfully revealed one of the notable gaps between general and aesthetic visual understanding: while the former focuses on identifying the factual element in an image (sky), the latter transcends such object identification, viewing it instead as an aesthetic component--a pure color block (blue). Such fundamental distinctions between general (detection, localization, etc.) and aesthetic (color, lighting, composition, etc.) visual understanding present a significant challenge for Multimodal Large Language Models (MLLMs). Although some recent works have made initial explorations, they are often limited to general and basic aesthetic commonsense. As a result, they frequently fall short in real-world scenarios (Fig. 1), which require extensive expertise--including photographic techniques, photo pre/post-processing knowledge, and more, to provide a detailed analysis and description. To fundamentally enhance the aesthetics understanding of MLLMs, we first introduce a novel dataset, PhotoCritique, derived from extensive discussions among professional photographers and enthusiasts, and characterized by the large scale, expertise, and diversity. Then, to better learn visual aesthetics from PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a languageguided multi-view vision fusion mechanism to understand image aesthetics from multiple perspectives. Finally, we present a novel benchmark, PhotoBench, a comprehensive and professional benchmark for aesthetic visual understanding. On existing benchmarks and PhotoBench, our model demonstrates clear advantages over existing models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 8 5 8 1 . 9 0 5 2 : r The Photographers Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers Daiqing Qi1,2, Handong Zhao2, Jing Shi2, Simon Jenni2, Yifei Fan2 Franck Dernoncourt2, Scott Cohen2, Sheng Li1 1University of Virginia, 2Adobe Figure 1. Examples on our model (PhotoEye), existing MLLMs tailored for low-level vision or aesthetics, and GPT-4o (202408-06). The left example and the middle top example highlight notable limitation of existing open-source MLLMs for low-level vision and aesthetics: insufficient coverage of visual aesthetics. When they fail to identify issues, they either provide positive aspects or claim there are no issues, significantly limiting their usefulness in real-world scenarios. The middle bottom example shows existing MLLMs lack of expertise in photography: lowering exposure properly can instead enhance colors of objects, so is correct. While other models, including GPT-4o, made mistakes, our model is correct. The right example reveals another clear limitation of existing open-source MLLMs: their vision encoders are insensitive to low-level vision and aesthetics. In series of increasingly overexposed photos (2-nd, 3-rd, and 4-th), PhotoEyes vision modules, more attuned to low-level and aesthetic features, identified overexposure by the 2-nd photo, while other models recognized it only when the photos were severely overexposed (i.e., 3-rd and 4-th images). High-quality aesthetic content is highlighted."
        },
        {
            "title": "Abstract",
            "content": "While editing directly from life, photographers have found it too difficult to see simultaneously both the blue and the sky. John Szarkowski, William Egglestons Guide 1 Photographer and curator, Szarkowski insightfully revealed one of the notable gaps between general and aesthetic visual understanding: while the former focuses on identifying the factual element in an image (sky), the latter transcends such object identification, viewing it instead as an aesthetic componenta pure color block (blue). Such fundamental distinctions between general (detection, localization, etc.) and aesthetic (color, lighting, composition, etc.) Part of this work was completed during Daiqings internship at Adobe. 1William Eggleston is an American photographer, widely recognized as pioneering figure in color photography. visual understanding present significant challenge for Multimodal Large Language Models (MLLMs). Although some recent works have made initial explorations, they are often limited to general and basic aesthetic commonsense. As result, they frequently fall short in real-world scenarios  (Fig. 1)  , which require extensive expertiseincluding photographic techniques, photo pre/post-processing knowledge, and more, to provide detailed analysis and description. To fundamentally enhance the aesthetics understanding of MLLMs, we first introduce novel dataset, PhotoCritique, derived from extensive discussions among professional photographers and enthusiasts, and characterized by the large scale, expertise, and diversity. Then, to better learn visual aesthetics from PhotoCritique, we furthur propose novel model, PhotoEye, featuring languageguided multi-view vision fusion mechanism to understand image aesthetics from multiple perspectives. Finally, we present novel benchmark, PhotoBench, comprehensive and professional benchmark for aesthetic visual understanding. On existing benchmarks and PhotoBench, our model demonstrates clear advantages over existing models. Resources available at https://github.com/daiqing98/ThePhotographers-Eye. 1. Introduction Multimodal Large Language Models (MLLMs) [16, 19] have succeed in general visual tasks but still struggle with critical ability in human visual perception: aesthetic visual understanding, which is desired in various applications like image recommendation, editing, or generation. The challenge lies in the fundamental distinction between general and aesthetic visual understanding: while the former focuses on factual elements like identification of objects, the latter requires nuanced perception of aesthetic elements such as chromatic properties, or compositional principles. We attribute the shortcomings of MLLMs in aesthetic understanding to two primary factors: dataset and model. Dataset: Current datasets such as Q-Instruct [29] and AesExpert [9], made initial explorations in low-level vision or visual aesthetics with MLLM but suffer from limitations in scale, expertise, and diversity. With around 20K images and 58K88K image annotations, these datasets are insufficient for comprehensive aesthetic understanding, showing clear deficiencies in real-world scenarios, where images vary widely in subjects, scenes, styles, environments, and techniques, requiring larger datasets to ensure coverage. Additionally, their annotations come from small (typically tens of annotators), fixed groups of annotators, limiting diversity and expertise. Even if the annotators are reportedly trained, their expertise remains limited compared to professional photographers and enthusiasts. Model: When adapting to aesthetic visual understanding, existing approaches [8, 10, 30, 38] typically finetune existing MLLMs like LLaVA [16] on customized datasets, ignoring the unique requirements of visual features for aesthetic visual understanding. Comprehensive understanding of image aesthetics requires multi-view information: for example, high-level features are essential for interpreting narrative and storytelling, while detailed spatial information is critical for understanding composition. Similarly, low-level details are required for perception of color and lighting, etc. To address the challenges, we first introduce the PhotoCritique dataset, featuring (1) large scale: containing over 450K images with detailed aesthetic descriptions, and around 20 times the size of existing datasets [8, 30]; and (2) expertise and diversity: the aesthetic descriptions are derived from extensive discussions among large groups of professional photographers and enthusiasts online, rather than fixed small number of annotators as in existing works [8, 30]. Subsequently, given the notable different focuses in general and aesthetic visual understanding, we further introduce PhotoEye, an MLLM tailored for aesthetic visual understanding. PhotoEye features languageguided multi-view vision fusion mechanism to effectively learn aesthetic knowledge from PhotoCritique, enabling it to understand image aesthetics from multiple perspectives for various aesthetics-related questions. Finally, to provide comprehensive evaluation of our model and existing MLLMs on aesthetic visual understanding, we introduce our new benchmark PhotoBench. PhotoBench draws its questions from in-depth discussions among professional photographers and enthusiasts, covering 284 sub-topics in photography, making it notably more professional and diverse than existing works. We summarize our contributions as follows: We introduce PhotoCritique, the first large-scale, professional, and diverse dataset for aesthetic visual understanding, consisting of 450K images with 2.63M instructiontuning pairs, sourced from extensive discussions among professional photographers and enthusiasts. We propose PhotoEye, an MLLM specifically designed for aesthetic understanding, featuring language-guided multi-view vision fusion mechanism. We develop PhotoBench, professional benchmark for aesthetic visual understanding, offering 284 photography sub-topics and insights from photography professionals. We conduct comprehensive study on aesthetic visual understanding with our model and existing MLLMs, revealing the limitations of existing MLLMs and demonstrating how we overcome them through extensive experiments. 2. Related Works Multimodal Large Language Models. MLLMs [3, 16] are evolving rapidly recently, demonstrating advanced capabilities in image-text dialogue through enhanced alignment and fine-tuning. Further studies [2, 5, 6, 28, 33] have strengthened MLLMs by increasing the data scale or improving the backbone model. Furthermore, with customized data, branch of works [4, 21, 33, 35] extend MLLMs capabilities to referring and grounding. However, despite these advancements in general visual tasks, the aesthetic visual understanding of MLLMs remains largely underexplored. Understanding Image Aesthetics with MLLM. Traditional Image Aesthetics Assessment (IAA) [17, 26, 32] is often formalized as regression task, where the model learns to predict the score of the aesthetic quality of images. One notable limitation is the lack of interpretability, which is critical in practice because we also want to understand why the image is good or bad. Recent advances in MLLM make it possible: few recent works [810, 2931, 38] explore interpretable aesthetic understanding with MLLMs. However, they often struggle with the limited data scale or lack diverse, professionally curated aesthetic descriptions. Figure 2. An example from the dpchallenge platform. 3 of the total 64 comments for the photo are presented for illustration. Improving Visual Perception in MLLM. Recently, several studies [13, 2224, 28, 41] have investigated backbone vision modules for MLLM. While SPHINX [14] simply concatenates features from different encoders, Cambrian [28] and MoVA [41] merge different visual features into single one. However, the fusion process in MoVA is based on the CLIP feature, which may introduce bias, and the feature extraction (prior to fusion) lacks languagebased control. Cambrian [28] employs query-based attention, however, visual features from different encoders are simply averaged during fusion, and the queries are independent of language instructions. To address these limitations, we propose new MLLM, PhotoEye, featuring languageguided multi-view vision fusion mechanism to effectively learn aesthetic knowledge. 3. Learning with Photography Enthusiasts and"
        },
        {
            "title": "Professionals",
            "content": "One key reason existing MLLMs struggle with aesthetic visual understanding in real-world scenarios is the training data limitations: (1) Data Scale: Current datasets [8, 30] (20K images) lack sufficient coverage of photography genres, styles, themes, etc. High-quality aesthetic descriptions are also limited, with typically 57K-88K detailed descriptions in existing works [8, 30, 38], while our dataset provides over 450K images and detailed descriptions. (2) Description Quality: Most datasets [8, 30] rely on small, fixed group of annotators, limiting expertise and diversity. In contrast, our PhotoCritique dataset draws insights from hundreds of thousands of photographers and enthusiasts, yielding more professional, nuanced feedback. 3.1. Mining from Digital Photography Platforms To address the limitations, we avoided using annotators and instead drew insights from broad range of online communities. The Digital Photography Challenge (DPC) is an online platform, where users submit their works across various themes and genres and receive constructive feedback from other photographers or enthusiasts  (Fig. 2)  . The wealth of high-quality discussions paired with images on DPC provides valuable resources for MLLMs to learn visual aesthetics. However, directly using this data for supervised fine-tuning is impractical due to the significant noise present in the raw comments. To address this issue, we developed Figure 3. Data Generation Pipeline. The generation of one critique from group of comments of one image is shown as an example. Figure 4. Top: Examples of PhotoCritique. Bottom: Comparison with examples in Q-Instruct [30]. strategies to analyze, summarize, and refine raw comments into single, well-crafted critique for each image. First, we need to collect raw data from dpchallenge.com online. Since existing works [37] already collected raw data prior to 2022 from the website, we can skip the data scraping process and focus on data processing. Given post from the DPC platform (comprising one uploaded image and series of user comments, shown in Fig. 2), we design multistep pipeline to generate single unified photo critique using prompt-based methods from comments only without the image. Fig. 3 briefly illustrates the pipeline with the example of generating single critique from set of comments of one image. we first instruct the LLM to analyze and summarize the raw comments, Then the LLM is asked to integrate the comments into cohesive and unified photo critique. While LLMs possess strong reasoning and comprehension abilities, the immediate output critique is sometimes sparse in information or misleading. Therefore, we designed filtering strategy to eliminate low-quality data with smaller LLM for improved efficiency. During the filtering stage, generated photo critiques are input into the smaller LLM, and prompt-based strategies are used to detect whether the generated photo critique conveys enough aesthetics-related information. Only critiques that pass this filtering stage are accepted. We term this portion of data as aesthetic description. Details available in Appendix A. From General Photo Critiques to Diverse Instructiontuning Pairs. In the previous step, we generated photo critiques from DPCs raw comments, which provide overall aesthetic reviews of the given images. In this step, we further utilize LLMs to generate multiple high-quality, diverse Figure 5. Comparison of description length Distributions of aesthetic comments in PhotoCritique and Q-Instruct. Scale of PhotoCritique is Blue and that of Q-Instruct is Red."
        },
        {
            "title": "Dataset",
            "content": "#Image #Annotations #Training Samples #Avg. Length UNIAA AesExpert* Q-Instruct 57K 21K 18K"
        },
        {
            "title": "PhotoCritique",
            "content": "450K 57K 88K 58K 450K 57K - 66K 2.4M 32.8 - 46. 65.2 Table 1. Statistics of samples attributed to detailed aesthetic descriptions and conversations across different datasets. Datasets with * are currently not available. conversations addressing various aspects of image aesthetics, such as lighting, composition, color, emotion, narrative, and so on, as well as extensive photographic techniques and pre/post-processing skills. Another critical reason for this step is that raw comments are sometimes too lengthy, which can lead to certain aesthetic aspects being overlooked in the LLM-generated critique. By specifically prompting about these aesthetic aspects, the LLM can effectively summarize essential information related to each aspect from the lengthy raw comments. Similarly, after generating all the questionanswer pairs, we apply prompt-based filtering strategies to reject less informative pairs, accepting only the high-quality samples. We term this portion of data as aesthetic conversation. Additionally, we also generate MCQs from photo critiques with LLMs. Specifically, each critique is input into the LLM with prompts to generate five MCQs based on the ground truth provided in it. We term this portion of data as aesthetic VQA. Details available in Appendix A. 3.2. PhotoCritique Overview. Combining all above, we build PhotoCritique, the first large-scale dataset for expert-level image aesthetics understanding. PhotoCritique consists of three parts: aesthetic description, aesthetic conversation, and aesthetic VQA. Examples of each are shown in Fig. 4 (Top). The three parts contain 450K, 1.9M, and 250K samples, respectively. Born from large volume of shared images and discussions by photography enthusiasts and professionals on online photography forums, PhotoCritique covers wide range of over 70 photographic categories, encompassing diverse scenes and subjects. Fig. 6 shows the distribution of the top 40 categories along with several image examples. The extensive scale and diversity enhance the models effectiveness in real-world scenarios. Comparison with Existing Datasets. PhotoCritique represents fundamental improvement over existing works in both quantity and quality. Data Scale: we compares PhoFigure 6. Top: Distribution of Top 40 categories. Bottom: Examples of photos from subset of categories. toCritique with existing datasets by showing statistics for data related to detailed aesthetic descriptions and conversations in Tab. 1. This includes the number of source images, the annotations for these images, and the final count of instruction-tuning pairs generated from these annotations using LLMs. Additionally, we calculate the average length of aesthetic comments (including both descriptions and conversations) in the training data. Tab. 1 focuses on training data statistics for aesthetic comments, as these are essential in practical applications where users need detailed feedback on why their photos are strong or weak and how to improve them. In contrast, Multiple Choice Questions (MCQ) are relatively limited in this regard. Besides, aesthetic comments better reflect the information density of the dataset than MCQs, as generating MCQs from detailed comment is easy and can result in numerous MCQs of varying quality depending on the input prompt. Therefore, the number of MCQs serves only as relatively rough indicator of the information density, although the number of MCQs in PhotoCritique is also the largest. As shown in Tab. 1, PhotoCritique demonstrates notable improvement across all metrics compared to existing datasets. Data Quality: PhotoCritique is more professional, and diverse. Unlike most existing works [8, 30], which rely on small group (usually few dozen) of annotators to label data, our dataset is sourced from images and discussion from over 107,000 photography enthusiasts and professionals on online photography forums. Fig. 2 provides an intuitive comparison between our dataset and Q-Instruct, with examples of QInstruct in its original paper. While Q-Instruct primarily focuses on basic low-level visual facts, PhotoCritique not only addresses low-level details but also considers how these elements impact image aesthetics and ways to improve them. Additionally, we include discussions on photography techniques (e.g., the first example in the aesthetic description covers flash usage and lighting) and the overall impression conveyed by the photo. The comprehensiveness of aesthetic comments in PhotoCritique is also reflected in the average length: Fig. 5 shows the distribution of aesthetic comment lengths in PhotoCritique and Q-Instruct, with PhotoCritique containing large number of detailed descriptions. Tab. 1 also shows that the average length of aesthetic comments Figure 7. Framework of PhotoEye. Given an input photo and language instruction, visual features from different visual encoders are fused in our vision fusor with language guidance, then fed into the LLM with language embeddings to generate LLM response. in PhotoCritique is notably greater than that of existing datasets. More examples in Appendix A. 4. Improving Visual Aesthetics Perception 4.1. See like Photographers In addition to limitations in existing datasets, Open-source MLLMs face another notable challenge: the limited sensitivity of the vision encoder to aesthetic elements. Most existing works [8, 30] use CLIP as the vision encoder, which, however, is pre-trained with high-level image-text alignment in general domains and is relatively less effective at capturing aesthetic elements. Fig. 1 (right) illustrates this issue, where Q-Instruct and AesExpert only report overexposure when the image is severely overexposed. In Fig. 8, we quantitatively validate it by calculating the discriminability (defined as the average distance between these embeddings) of aesthetic features from different vision encoders. Details are available in Appendix B. It shows that CLIP is less effective in discriminating towards aesthetics-related features as they are more densely clustered. To handle the diversity of aesthetics understanding, from high-level elements like emotion and storytelling to low-level features such as lighting, color, and composition, we propose PhotoEye, MLLM with language-guided multi-view vision fusor, which enhances visual perception by fusing multiple vision encoders pre-trained on complementary tasks. 4.2. PhotoEye Figure 8. Comparison of aesthetics-related visual feature discriminability of ours and CLIP [15] in existing works [8, 30, 38]. group of visual features {Xi RCHW }N i=1 from different vision encoders are fused with learnable query Ql RCHW . Denote the output of the l-th block as Ol, Ql+1 = Ol, if = 0. The first learnable query is generated by the language-guided query generator. Language-guided Query Generation. At the first layer, Q1 is generated by the language-guided query generator (bottom left in Fig. 7). Specifically, we maintain group of learnable queries {Qi RCHW }M i=1. Given specific language instruction, it is encoded into text embedding T, which serves as the query to dynamically generate single query from all learnable queries with attention mechanism: Q1 = ATTN(W QM), where QM is the concatenation of all queries, and , are the projection matrixes of the query, key and value, respectively. Note that pre-trained BERT is introduced as the text encoder, where the [CLS] token from the output is used as the text token T. We avoid the CLIP text encoder to prevent bias towards the CLIP visual features in the later multimodal gating process. QM, q T, , Architecture. PhotoEye is shown in Fig. 7. It consists of multi-view vision fusor and backbone LLM. Given an image and an aesthetics-related language instruction, visual features of the image from different vision encoders are fused into single visual feature by the multi-view vision fusor, conditioned on both the image and the language instruction. Specifically, the multi-view vision fusor conIn each l-th block (or layer), sists of fusion blocks. In this way, for each language instruction that may focus on specific aesthetic concept, tailored query is generated from all learnable queries, which can best extract visual features related to the aesthetic concept from each vision encoder in the fusion block during inference. Feature Extraction with Learnable Query. At l-th layer, given query Ql and group of visual features {Xi}N i=1 from different vision encoders, each visual feature is i}N X Ql, n), where first interpolated to the same shape: = Interpolate(Xn), RCHW . Then, each of the extracted visual where features from the corresponding vision encoder is given by = ATTN(W Fl , n, , are the projection matrixes of the query, key and value. Multimodal Gating Network. The extracted visual features {Fl i=1 are then fused into single visual feature in the multimodal gating network. The multimodal gating network assigns different weights to each extracted visual feature conditioned on both image and language and fuses them into one single feature based on their importance. The weights of visual features are directly predicted by an MLP. Specifically, given the language embedding i}N and set of visual features {Fl i=1, weights are given by: i=1)), where RN 1, and wl = MLP(T; Pooling({Fl i}N ; means concatenation. The fused single feature is obtained by ˆFl = (cid:80)N i. Then, ˆFl is further fed forwarded to transformer block with residual links. The output of the vision fusor is input to the (l + 1)-th layer if < L; otherwise, it is fed to an MLP and input to the LLM. Remark. Compared to existing works, PhotoEye differs from MoVA [41] in removing its potential bias towards CLIP features by fusing visual features with group of learnable queries. Moreover, language-guided query generation mechanism is introduced to extract different features towards different language instructions. When compared to Cambrian-1 [28], PhotoEye conditions both feature extraction and fusion on language, allowing it to capture more refined, instruction-aware aesthetic visual features. In contrast, Cambrian-1 is independent of instructions, potentially resulting in relatively coarser visual features. i=1 wl Fl 5. Benchmarking Aesthetic Visual Understanding with Professionals In Section 3, we highlighted major limitation of existing datasets: annotations are typically provided by few dozen annotators, which limits the diversity and expertise of the annotations. Even with reported training, these annotators lack the expertise of the many experienced photographers and enthusiasts. Existing benchmarks, such as QBench [29] and AesBench [9], also suffer from this issue, relying on similar methods to obtain annotations. To address the limitation and cover wider range of scenarios, we directly draw insights from extensive photos and discussions on Reddits PhotoCritique, vibrant photography community. This approach allows us to generate comprehensive, diverse, and expert-level multiple-choice questions to evaluate the aesthetic visual understanding of MLLMs. Specifically, we do not generate MCQs from annotator-provided annotations; instead, we derive MCQs from photo reviews in Reddits PhotoCritique. We refer to our new benchmark as PhotoBench. Since RPCD [18] has already collected raw data from PhotoCritique posts between 2009 and 2022, we directly design pipelines that use an LLM (GPT-4-turbo) to generate MCQs. 5.1. From Insights to Questions The entire question-generation process consists of three steps: (1) summarization, (2) question generation, and (3) (1) Summarization. Since the raw multi-stage filtering. comments from the forum are quite noisy, empirically we find directly inputting the raw comments corresponding to an image to the LLM and prompting it to generate multiplechoice questions based on these comments does not yield ideal results. To address this issue, we first use the LLM to summarize the noisy raw comments for each image, producing single, clean, comprehensive, and detailed photo critique. This process aligns with the pipeline shown in Fig. 3. (2) Question Generation. For each image, we input its corresponding photo critique into the LLM and instruct it to generate five multiple-choice questions grounded on the provided critique. (3) Multi-stage Filtering. This step directly impacts the quality of PhotoBench. We first selected the top 5,000 most detailed photo critiques from the initial generation step and instructed the LLM (using GPT-4 Turbo, as we empirically found it slightly outperforms GPT4o) to generate 25,000 questions. The first filtering stage is LLM-based, primarily aimed at removing MCQs with low visual dependencythat is, questions that could likely be answered correctly by reading only the question and options without viewing the image. We show an example in Fig. 9 (1). Our method is intuitive yet effective: we provide only the question and options to the LLM (GPT-4o) without the image and filter out questions it answers correctly. This step eliminated about 70% of the items. We then instructed GPT4o to score the remaining questions based on aesthetics relevance, visual dependency, and expertise. We selected the top 1,500 questions based on their average score. Details available in Appendix C. 5.2. PhotoBench We show detailed information of PhotoBench in Fig. 9. Compared to existing works [9, 29, 38], Photobench gains more advantages in diversity and expertise. Due to potentially limited expertise and diversity among annotators [9, 29], MCQs from existing works are often too simple. We can have straightforward comparison in Fig. 10. While questions from existing works are relatively straightforward and focus on factual elements, our questions are more challenging and practical, requiring deeper aesthetics understanding and covering topics directly related to photography in practice, such as camera settings, post-processing, and photographic techniques (topics marked in blue in Fig 9. Examples are shown in Fig 9 (3,6). In Fig. 9 (3), to make the rainbow stand out more, the exFigure 9. We show one rejected example, several accepted examples, topic distribution, and some sub-topics of subset of topics of questions in PhotoBench. posure should be reduced (slight underexposure can instead enhance color saturation). In Fig. 9 (6), we can observe that the shadows are not purely black but rather grayish (zoom in for better view)a common post-processing technique achieved by raising the shadow curve to enhance the photos texture or create film-like aesthetic. They suggest that PhotoBench includes more professional and detailed questions, closely aligned with practical applications such as photo post-processing and editing. 6. Experiments Implementation Details. We build PhotoEye with Vicunav1.5-7B as the LLM backbone, following existing works [8, 30] for fair comparison. We also use minimal data from the general domain, including LLaVA-Pretrain [15] for pretraining and LLaVA-665K [15] for instruction finetuning. We mix our data with LLaVA-665K during the finetuning stage. We use batch size of 128 and learning rate of 1e-3 for pre-training, and 2e-5 for finetuning. The training takes around 96 hours with 8 A100 GPUs with ZeRO2. For vision encoders, we use: CLIP-ViT-L/14 [25], DINOv2giant [20] CoDETR-ViT-L [40] and SAM-ViT-H [11]. Details are available in the Appendix D. 6.1. Results We report results of PhotoEye on existing benchmarks. Although PhotoCritique is not specifically designed for lowlevel vision, but PhotoEye still achieves competing results on Q-Bench when finetuned with it [29]. PhotoEye clearly outperforms existing open-source models, including both general-purpose models and those specifically fine-tuned on low-level vision and image aesthetics datasets, such as Q-Instruct [30] and AesExpert [8]. Compared to closedFigure 10. Examples in related works from their original papers. source models, our lightweight approach also demonstrates competitive performance, slightly surpassing GPT-4o. In Tab. 3, we compare our model with most competing baselines on PhotoBench, including UNIAA [38], AesExpert [8] and Q-Instruct [30], which specially focus on lowlevel vision and aesthetics. Note that when reporting results in Tab. 3, we merge similar topics into single category. Categories may overlap - for instance, question about editing exposure would be counted in both Post-Processing and Exposure. PhotoEye notably outperforms baselines. As discussed in Section 5, PhotoBench shows substantially higher expertise and diversity than existing benchmarks, demanding stronger aesthetic understanding capabilities from models. Therefore, PhotoEye, which learns from extensive discussions by photography professionals and enthusiasts, holds clear advantage. Ablations. We show the effectiveness of our proposed vision fusor and dataset PhotoCritique in Tab. 4. Both contribute to the model. More ablations including qualitative results are available in Appendix. Qualitative Evaluation. Providing aesthetic recommendations in conversations is actually the primary need in realworld scenarios. In addition to examples in Fig 1, We show examples in Appendix E. with diverse images and contexts. Model LLM Backbone Random guess GPT-4o (2024-08-06) Qwen-VL-Max [2] Qwen-VL-Plus [2] Gemini-Pro Emu2-Chat [27] mPLUG-Owl2 [34] LLAMA-Adapter-V2 [7] IDEFICS-Instruct [1] SPHINX [14] InternLM-XComposer-VL [36] Qwen-VL [2] Otter-v1 [12] LLAVA-v1.5 [15] InstructBLIP [6] Shikra [4] MiniGPT-4 [39] AesExpert [8] Q-Instruct [30] PhotoEye (Ours) - Proprietary Model Proprietary Model Proprietary Model Proprietary Model LLaMA-2-33B LLaMA-2-7B LLaMA-2-7B LLaMA-2-7B LLaMA-2 InternLM QwenLM MPT-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-13B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Question Types Quadrants of Low-level Concerns Yes-or-No What How Distortion Other In-context Distortion In-context Other Overall 50.00 76.72 75.60 73.77 68.80 71.81 72.18 66.18 56.18 74.18 69.45 63.09 57.09 66.36 71.64 65.64 55.82 73.27 76.18 80.01 27.86 74.33 79.43 69.47 73. 67.25 57.96 59.29 44.69 68.81 65.27 58.19 40.71 58.19 52.65 47.35 50.22 64.38 66.37 76.10 33.31 67.74 66.09 53.88 62.34 56.18 56.19 52.13 44.02 62.07 60.85 56.39 39.55 50.51 43.81 49.09 40.37 53.75 57.61 67. 37.89 70.03 73.39 66.21 66.30 64.78 56.68 57.39 42.80 63.62 61.67 50.58 42.22 49.42 48.64 48.83 42.02 70.03 65.18 74.32 38.48 73.37 74.08 65.72 71. 63.19 69.21 56.25 54.17 71.76 70.14 62.73 49.31 65.74 62.50 59.49 48.38 73.38 67.59 74.59 38.28 73.68 71.00 63.81 63.91 63.48 53.29 63.16 44.74 66.12 56.91 57.89 44.08 54.61 55.59 50.00 51.97 73.68 73.06 77. 35.82 77.95 76.92 68.75 73.09 72.24 72.65 64.90 56.33 76.33 75.10 73.88 52.65 70.61 64.90 64.08 61.22 77.96 71.53 81.22 37.80 73.04 73.63 66.04 68. 65.28 61.61 59.46 48.70 68.56 65.35 59.40 46.35 58.66 56.72 54.65 49.03 64.15 67.09 74.50 Table 2. Model Performance on Q-Bench (LLVisionQA-dev). PhotoEye has clear advantage. Best open-source model results in bold. Proprietary model results are quoted from Q-Bench [29]. Model LLM Backbone Composition Equipments Contrast Techniques Color and Tone Lighting GPT-4o (2024-08-06) Proprietary Model LLaVA-v1.6-34B [15] UNIAA [38] AesExpert [8] Q-Instruct [30] PhotoEye (Ours) Hermes-2-Yi-34B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B 68.11 47.69 29.89 51.49 31.02 68.32 60. 52.56 38.61 60.24 34.77 61.39 59.09 56.82 45.45 56.82 40.91 72.97 62.75 49.33 39.56 53.99 31.37 65.69 63. 65.44 40.19 61.03 54.38 76.00 63.74 53.80 29.24 64.33 44.44 77.78 Model LLM Backbone Exposure Post-Processing Aperture and Focus Storytelling Sharpness and Clarity Overall GPT-4o (2024-08-06) Proprietary Model LLaVA-v1.6-34B [15] UNIAA [38] AesExpert [8] Q-Instructt [30] PhotoEye (Ours) Hermes-2-Yi-34B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B Vicuna-v1.5-7B 54.48 56.55 25.52 46.21 46.21 69.66 58. 53.33 38.57 61.90 46.19 80.95 59.87 52.87 40.13 52.87 38.85 70.70 69.44 66.67 30.56 72.97 55.56 75.00 61. 28.57 23.81 33.32 19.05 61.90 64.12 55.68 36.87 60.01 43.86 73.92 Table 3. Model Performance on PhotoBench Metrics. Best results in bold. Method Q-Bench PhotoEye PhotoEye (full method) w/o Multi-view Vision Fusor w/o Multi-view Vision Fusor and dataset 74.50 70.08 58.04 73.92 68.83 33.74 Table 4. Ablation Results. Figure 11. Weights of vision encoders across different layers. 6.2. See through PhotoEye As multiple visual features from different vision encoders are fused in PhotoEye, we are particularly interested in their roles under different cases and how they contribute to our model. Fig. 11 presents the normalized weights of each vision encoder in PhotoEye. We first randomly collect 100 samples from PhotoCritique on compositional aesthetics (e.g., photographic composition, distraction, framing, etc.) and general aesthetics (e.g., storytelling, lighting, etc), respectively, as well as 100 samples for general visual tasks from LLaVA [15] training data. Then we study the averaged weight distribution (normalized) of encoders on different data. We have an interesting observation: (1) aesthetic features are extracted in the first layers, and general features are extracted in the last layers. Fig. 11 shows CLIP and DINO features are dominant regardless of domains in the last layer, which is intuitive as the recognition of objects lays the foundation in both domains. In contrast, aestheticsrelated features are primarily extracted from the first layer, especially in compositional problems, where the localizaTopic CLIP-ViT-L/14 DINOv2-giant CoDETR-ViT-L SAM-ViT-H Composition 62.61 61.61 64.69 60.24 Table 5. PhotoEye on composition with different encoders. In this case, tion or arrangement of objects are critical. CoDETR (pre-trained with detection objectives) is dominant. Tab. 5 validated it, where we only activated one encoder during inference to detect their effectiveness in photographic composition. Case Study. We show how different choices of vision encoders affect model output in real-world conversation scenarios. We provide interesting examples in Appendix F. Learning of Aesthetic Concepts. Fig. 8 demonstrates how PhotoEye effectively learns discriminative aestheticsrelated visual representations. In Appendix B, we conduct deeper investigation into the learning of aesthetic concepts at feature level, which lays the foundation of high-quality aesthetic response from LLM. 7. Conclusion We present PhotoCritique dataset, and PhotoEye, an MLLM with language-guided multi-view vision fusion for aesthetic visual understanding, along with the new benchmark PhotoBench. Extensive experiments show PhotoEyes clear advantage over existing models, advancing aesthetic visual perception in MLLMs."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was partially supported by gifts from Adobe Research and the National Science FoundaIIS-2316306 and CNS-2330215. tion under Grants"
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. 8 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2, 8 [3] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 2 [4] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 2, 8 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 2, 8 [7] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2311.07575, 2023. 8 [8] Yipo Huang, Xiangfei Sheng, Zhichao Yang, Quan Yuan, Zhichao Duan, Pengfei Chen, Leida Li, Weisi Lin, and Guangming Shi. AesExpert: Towards multi-modality foundation model for image aesthetics perception. arXiv preprint arXiv:2404.09624, 2024. 2, 3, 4, 5, 7, 8 [9] Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, and Weisi Lin. AesBench: An expert benchmark for multimodal large language models on image aesthetics perception. arXiv preprint arXiv:2401.08276, 2024. 2, 6 [10] Zhipeng Huang, Zhizheng Zhang, Yiting Lu, Zheng-Jun Zha, Zhibo Chen, and Baining Guo. Visualcritic: Making lmms perceive visual quality like humans. arXiv preprint arXiv:2403.12806, 2024. [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 7 [12] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal arXiv preprint model with in-context instruction tuning. arXiv:2305.03726, 2023. 8 [13] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models. arXiv preprint arXiv:2311.07575, 2023. 3 [14] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2024. 3, 8 [15] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 5, 7, 8 [16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [17] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: large-scale database for aesthetic visual analysis. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 24082415. IEEE, 2012. [18] Daniel Vera Nieto, Luigi Celona, and Clara FernandezLabrador. Understanding aesthetics with language: photo critique dataset for aesthetic assessment. In NeurIPS Track on Datasets and Benchmarks, 2022. 6 [19] OpenAI. Gpt-4 technical report. 2023. 2 [20] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 7 [21] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2 [22] Daiqing Qi, Handong Zhao, and Sheng Li. Easy regional contrastive learning of expressive fashion representations. Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2 [34] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023. [35] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 2 [36] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 8 [37] Zhipeng Zhong, Fei Zhou, and Guoping Qiu. Aesthetically relevant image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2023. 3 [38] Zhaokun Zhou, Yiwei Su, Amin Zheng, Qiulin Wang, Rui Chen, Li Yuan, and Di Zhang. UNIAA: unified multimodal image aesthetic assessment baseline and benchmark. arXiv preprint arXiv:2404.09619, 2024. 2, 3, 5, 6, 7, 8 [39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 8 [40] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with colIn Proceedings of laborative hybrid assignments training. the IEEE/CVF International Conference on Computer Vision, pages 67486758, 2023. 7 [41] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. In Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS), 2024. 3, Advances in Neural Information Processing Systems, 37: 2048020509, 2024. 3 [23] Daiqing Qi, Handong Zhao, Zijun Wei, and Sheng Li. Taggrounded visual instruction tuning with retrieval augmentation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 20082026, 2024. [24] Daiqing Qi, Handong Zhao, Aidong Zhang, and Sheng Li. Generalizing to unseen domains via text-guided augmentation: training-free approach. In European Conference on Computer Vision, pages 285300. Springer, 2024. 3 [25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [26] Jian Ren, Xiaohui Shen, Zhe Lin, Radomir Mech, and David J. Foran. Personalized image aesthetics. In The IEEE International Conference on Computer Vision (ICCV), 2017. 2 [27] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. arXiv preprint arXiv:2312.13286, 2024. [28] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 2, 3, 6 [29] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, and Weisi Lin. Q-BENCH: benchmark for general-purpose foundation models on low-level In Proceedings of the International Conference on vision. Learning Representations (ICLR), 2024. 2, 6, 7, 8 [30] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Kaixin Xu, Chunyi Li, Jingwen Hou, Guangtao Zhai, Geng Xue, Wenxiu Sun, Qiong Yan, and Weisi Lin. Q-Instruct: Improving low-level visual abilities for multi-modality foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 4, 5, 7, 8 [31] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, and Weisi Lin. Towards open-ended visual quality comparison. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 2 [32] Yuzhe Yang, Liwu Xu, Leida Li, Nan Qie, Yaqian Li, Peng Zhang, and Yandong Guo. Personalized image aesIn Proceedings of thetics assessment with rich attributes. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1986119869, 2022. 2 [33] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,"
        }
    ],
    "affiliations": [
        "Adobe",
        "University of Virginia"
    ]
}