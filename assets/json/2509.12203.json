{
    "paper_title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence",
    "authors": [
        "Zixin Yin",
        "Xili Dai",
        "Duomin Wang",
        "Xianfang Zeng",
        "Lionel M. Ni",
        "Gang Yu",
        "Heung-Yeung Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms."
        },
        {
            "title": "Start",
            "content": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence ZIXIN YIN, The Hong Kong University of Science and Technology, StepFun XILI DAI, The Hong Kong University of Science and Technology (Guangzhou) DUOMIN WANG, StepFun XIANFANG ZENG, StepFun LIONEL M. NI, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology GANG YU, StepFun HEUNG-YEUNG SHUM, The Hong Kong University of Science and Technology Fig. 1. Pipeline of LazyDrag. (a) An input image is inverted to latent code ğ’›ğ‘‡ . Our correspondence map generation then yields an updated latentË†ğ’›ğ‘‡ , point matching map, and weights ğ›¼. Tokens cached during inversion are used to guide the sampling process for identity and background preservation. (b) In attention input control, dual strategy is employed. For background regions (gray color), Q, K, and tokens are replaced with their cached originals. For destination (red and blue colors) and transition regions (yellow color), the and tokens are concatenated with re-encoded (K only) source tokens retrieved via the map (c) Attention output refinement performs value blending of attention output. and denotes element-wise product and addition. The reliance on implicit point matching via attention has become core bottleneck in drag-based editing, resulting in fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for MultiModal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as reliable reference to boost the attention control. This reliable reference opens the potential for stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of dog and inpainting its interior, generating new objects like tennis ball, or for ambiguous drags, making context-aware changes like moving hand into pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves new way to editing paradigms. Here is the project website. 5 2 0 S 5 1 ] . [ 1 3 0 2 2 1 . 9 0 5 2 : r 2 Zixin et al."
        },
        {
            "title": "1\nDrag-based editing in diffusion models remains fundamentally chal-\nlenging. To preserve object identity during editing, prior methods\noften perform implicit point matching via attention. A common\nstrategy, introduced by MasaCtrl [Cao et al. 2023], shares key and\nvalue tokens during attention. However, this strategy allocates more\nattention weights to spatially nearby regions instead of semantically\nrelated ones [Feng et al. 2025; Wang et al. 2025a], which leads to\nunstable and degrading edits. Rather than tackling this fundamental\ncause, as a compromise, many methods rely on test-time optimiza-\ntion (TTO) or weakened inversion strength. These compromises\nmask the mismatch and incur costs, including unreliable inpainting,\nsuppressed text guidance, and distorted edits.",
            "content": "Instead of the compromise, we take principled alternative: replace implicit attention-based matching with an explicit correspondence map and inject it directly into the generation process. With this reliable map, editing under full-strength inversion becomes stable without TTO, enabling faithful inpainting and text-guided generation. Beyond addressing the fundamental issue, the choice of network architecture remains crucial for editing. The recent transition from U-Nets [Rombach et al. 2022] to Multi-Modal Diffusion Transformers (MM-DiT) [Esser et al. 2024] provides an ideal foundation for this shift, because MM-DiTs offer tighter visiontext fusion, which improves inversion robustness and raises the ceiling for attention control. As shown by ColorCtrl [Yin et al. 2025], this architecture supports stronger semantic consistency and controllability, allowing attention control to be applied across all single-stream attention (SS-Attn) layers without manual selection of specific layer indexes like that in U-Nets. We exploit these advantages by building our method on MM-DiTs. Unlike in U-Nets, identity preservation in MM-DiTs is non-trivial. Simply sharing key and value tokens, as in DiTCtrl [Cai et al. 2025], does not reproduce the identity-preserving behavior achieved by MasaCtrl with U-Nets [Cao et al. 2023]. Recently, CharaConsist [Wang et al. 2025a] showed that re-encoding and injecting semantically aligned tokens can preserve identity in MM-DiTs. However, its point matching relies on the average of attention similarity, which is fragile under full-strength inversion and often yields unsuitable edits. In contrast, drag instructions naturally define field that maps handle points to target points, forming deterministic correspondence map. We turn this explicit map into attention controls. This explicit correspondencedriven preservation resolves the root issue, stabilizes edits under full-strength inversion without TTO. As result, it enhances inpainting and text guidance ability, delivering higher fidelity and controllability than prior methods. In this work, we present LazyDrag, training-free method that uses an explicit correspondence map to drive attention controls in MM-DiTs. By resolving the core instability of implicit attention mappings, LazyDrag stabilizes edits under full-strength inversion without TTO, unlocking the full generation ability. Concretely, (i) the drag instructions are converted into an explicit correspondence map, and (ii) identity and background are preserved using attention controls with the map. Together, these components deliver edits under full-strength inversion without TTO, retaining inpainting capability and enabling text-guided edits under ambiguous instructions. As shown in Fig. 1, this allows our method to execute complex edits Fig. 2. Effect of inversion strength. Examples of LazyDrag under different inversion strengths. The additional prompt is red apple in the mouth. where prior works fail: it can open the mouth of the dog and inpaint its interior, or even generate tennis ball via text guidance, which is impossible for methods constrained by low inversion strength (see Fig. 2). Furthermore, it exhibits deep understanding of scene context. For example, when dragging hand using drag instructions alone, the ambiguity of the task, whether the hand should be placed behind back or into pocket, can be resolved through text guidance, allowing users to make precise and meaningful edits. Extensive experiments demonstrate that LazyDrag achieves state-of-the-art (SOTA) performance while requiring no test-time optimization. To the best of our knowledge, LazyDrag is the first drag-based editing method built with MM-DiTs and the first to adopt full-strength inversion across all sampling steps, which enables natural inpainting and precise text-guided control. Our contributions are threefold: We propose LazyDrag, the first to achieve full-strength inversion in drag-based editing with MM-DiTs. It is accomplished by an explicit correspondence-driven attention controls that eliminates the need for TTO and resolves the core instability of previous works. We resolve the ambiguity of drag instructions by coupling the explicit correspondence map with text guidance, enabling natural inpainting and semantically consistent edits. Extensive experiments demonstrate that LazyDrag significantly outperforms all existing methods on Drag-Bench in both quantitative metrics and human preference."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-image and video generation. GAN-based models [Reed et al. 2016; Wang et al. 2023; Yu et al. 2023] have been largely replaced by diffusion models with U-Net backbones [Ho et al. 2020; Rombach et al. 2022] due to better fidelity and stability. However, UNets scale poorly, prompting shift toward Diffusion Transformers (DiT) [Peebles and Xie 2023]. Among them, MM-DiT [Esser et al. 2024] has become the backbone of choice in recent state-of-the-art systems [AI 2024; Esser et al. 2024; Kong et al. 2024; Labs 2024; Liu et al. 2025b; Yang et al. 2024], including FLUX [Labs 2025]. We are the first to introduce drag-based editing method within MM-DiTs. Text-based editing. Training-free text-guided editing methods use pre-trained diffusion models without fine-tuning, offering strong flexibility. Prompt-to-Prompt [Hertz et al. 2023] edits attention maps for localized control, with extensions to images and videos [Cao et al. 2023; Ju et al. 2024; Liu et al. 2024b; Rout et al. 2025; Wang et al. 2025b; Xu et al. 2025]. Recent work explores attention control in MM-DiTs: DiTCtrl [Cai et al. 2025] for long video generation, ColorCtrl [Yin et al. 2025] for light-consistent color edits, and CharaConsist [Wang et al. 2025a] for preserving character identity. Modern approaches such as Step1X-Edit [Liu et al. 2025a] and GPT-4o [OpenAI 2025] have gained popularity due to their efficiency. However, all rely LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 3 solely on text, which limits spatial precision. We instead introduce more intuitive and controllable drag-based method. Drag-based editing. Drag-based editing enables users to specify explicit spatial transformations by defining source and target points. Existing methods can be divided into two categories: those requiring test-time optimization (TTO), and those that do not. Most prior works fall into the former, beginning with DragGAN [Pan et al. 2023], and expanding to diffusion-based approaches [Hou et al. 2024; Ling et al. 2024; Liu et al. 2024a; Mou et al. 2024a,b; Shi et al. 2024; Shin et al. 2024; Zhang et al. 2025; Zhou et al. 2025]. RegionDrag [Lu et al. 2024] extends the interface to support region-level editing. Some methods [Choi et al. 2025; Jiang et al. 2025] incorporate textual prompts to improve semantic understanding, but still suffers from complex instructions. FastDrag [Zhao et al. 2024] is one of only two notable TTO-free methods, achieving faster inference but still falling short of the quality delivered by TTO-based methods. Inpaint4Drag [Lu and Han 2025] is the other TTO-free method that build on an inpainting model rather than generative model with inversion. However, directly pasting warped image to fill the edited region introduces strong unnatural warping artifacts. Also, its strong sensitivity to the input mask leads to frequent boundary artifacts and blurring, even with assistance from modern mask generators (e.g., SAM [Kirillov et al. 2023]). Therefore, we adopt widely used generative model approach with inversion, rather than an inpainting formulation. Additionally, all prior approaches with inversion rely on low inversion strength, which degrades inpainting quality and limits semantic generation. In contrast, we introduce the first drag-based method for MM-DiTs that leverages full-strength inversion and text-guided attention mechanisms, achieving SOTA performance without any per-image tuning."
        },
        {
            "title": "3 Method",
            "content": "Our goal is to achieve identity-preserving edits with precise drag control, text guidance, and natural inpainting. To this end, we introduce LazyDrag, training-free method built with MM-DiTs under full-strength inversion property. Our approach replaces the fragile, implicit point matching of prior work with robust, explicit correspondence map derived from user input during attention control, stabilizing the inversion process without test-time optimization. We first review foundational concepts in Sec. 3.1. Then detail our two-stage approach: first, how to generate the explicit correspondence map from drag instructions (Sec. 3.2), and second, how this map drives novel two-part attention control for identity and background preservation (Sec. 3.3). Fig. 3 shows the pipeline."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "LazyDrag builds upon insights from training-free drag-based editing methods in U-Nets (Sec. 3.1.1) and identity preservation in MM-DiTs (Sec. 3.1.2), addressing core limitations of both (Sec. 3.1.3)."
        },
        {
            "title": "3.1.1 Training-Free Drag Editing in U-Nets: FastDrag. FastDrag [Zhao\net al. 2024] is the first training-free method for drag-based editing,\nwith U-Net models. It has two parts: (1) it computes a displacement\nfield from drag instructions to create an initial latent Ë†ğ’›ğ‘‡ , filling\nexposed regions via interpolation, and (2) it applies a MasaCtrl-\nlike [Cao et al. 2023] key and value token replacement during self-\nattention to preserve object identity. However, beyond the implicit",
            "content": "locality bias of self-attention, central trade-off arises: we want handle points to reach their targets while surrounding regions inpaint naturally. Yet after latent initialization, the cue specific to handles is lost, and all moved points are treated uniformly. Forcing exact positional accuracy yields warp artifacts, whereas enforcing naturalness reduces positioning accuracy. Thus, editing accuracy and visual fidelity are in inherent tension. Moreover, its fusion of multiple instructions is brittle: when drags are antagonistic (for example, opening mouth by moving the upper lip upward and the lower lip downward), averaging the displacements cancels motion near the seam and the mouth fails to open. Moreover, the interpolation used to fill newly exposed regions further replicates nearby textures, producing repeated artifacts in large uncovered areas, as in Fig. 11. Identity Preservation in MM-DiTs: CharaConsist. In parallel, 3.1.2 CharaConsist [Wang et al. 2025a] introduces identity preservation in MM-DiTs, though it is not an editing method. To enforce identity preservation, it controls attention by concatenating corresponding source tokens into the key (re-encoded) and value tokens and by blending attention outputs. However, its point matching mechanism is critically flawed: it relies on attention similarity to identify matching points between images, process that is computationally expensive (requiring additional denoising steps) and inherently unstable. Under full-strength inversion, even minor mismatches in the correspondence map can lead to significant visual artifacts, as proved in Tab. 3."
        },
        {
            "title": "3.2 Generating the Explicit Correspondence Map",
            "content": "We first compute an explicit correspondence map from the user drag instructions and the inverted source latent noise ğ’›ğ‘‡ . The map comprises matching point function and weight function A, which provides explicit guidance. Guided by this map, we generate the initial latent noise Ë†ğ’›ğ‘‡ . Displacement field calculation via winner-takes-all (WTA).. Let Î© denote the latent grid, and let = {ğ’‘ ğ‘— }ğ‘š ğ‘—=1 Î© be the editable regions (the bright area in Fig. 3), sampled as feature points. Let the drag instructions be = {(ğ’”ğ‘–, ğ’†ğ‘– )}ğ‘˜ ğ‘–=1, where ğ’”ğ‘– and ğ’†ğ‘– are the handle and target points of the ğ‘–-th instruction. We illustrate two modes for computing the displacement field. In drag mode, we adopt the elasticity-based per-instruction displacement ğ’—ğ‘– ğ‘— for each ğ’‘ ğ‘— under the ğ‘–-th instruction as in Zhao et al. [2024]; in move mode, we use standard translation and scaling. To avoid failures of averaging under opposing drags, we use robust winner-takesall [Aurenhammer 1991] fusion: each ğ’‘ ğ‘— is uniquely assigned to its nearest handle, inducing Voronoi partition [Aurenhammer 1991]. 4 Zixin et al. Fig. 3. Pipeline of LazyDrag. (a) An input image is inverted to latent code ğ’›ğ‘‡ . Our correspondence map generation then yields an updated latentË†ğ’›ğ‘‡ , point matching map, and weights ğ›¼. Tokens cached during inversion are used to guide the sampling process for identity and background preservation. (b) In attention input control, dual strategy is employed. For background regions (gray color), Q, K, and tokens are replaced with their cached originals. For destination (red and blue colors) and transition regions (yellow color), the and tokens are concatenated with re-encoded (K only) source tokens retrieved via the map (c) Attention output refinement performs value blending of attention output. and denotes element-wise product and addition. The final displacement ğ’— ğ‘— and weight ğ›¼ ğ‘— are determined solely by the winning instruction. ğ›¼ğ‘– ğ‘— = (cid:40) ğ’‘ ğ‘— ğ’”ğ‘– 1 2 , ğ›¼ ğ‘— = ğ›¼ğ‘– ğ‘— , where ğ’— ğ‘— = ğ’— ğ‘– ğ‘— , , ğ’‘ ğ‘— ğ’”ğ‘–, otherwise, (1) ğ‘– = arg max ğ‘– ğ›¼ğ‘– ğ‘— . Here, 2 denotes the Euclidean ğ¿2-norm distance. Thus, = {ğ’— ğ‘— }ğ‘š ğ‘—=1 is defined as the displacement field. This approach preserves the full magnitude of opposing drags, enabling complex edits like opening mouth, which is impossible with simple averaging. Details are in Appendix A.2. Initial latent construction and map formalization (Latent Init). With the displacement field established, we construct the initial latent Ë†ğ’›ğ‘‡ . This process defines our explicit deterministic correspondence map (M, A) and partitions the latent grid into distinct regions for targeted control. First, we define the set of discrete destination coordinates = {Î (ğ’‘ ğ‘— +ğ’— ğ‘— ) ğ’‘ ğ‘— P}, where Î () projects to the grid. By resolving collisions where multiple source points map to single destination ğ’™ (using winner-takes-all), we get the winner index ğ‘—(ğ’™) = arg max ğ‘—: Î  (ğ’‘ ğ‘— +ğ’—ğ‘— )=ğ’™ ğ›¼ ğ‘— and formalize our correspondence map: Matching point map, (ğ’™) = ğ’‘ ğ‘— (ğ’™ ) . Matching weight map, (ğ’™) = min(1, ğ›¼ ğ‘— (ğ’™ ) ). Next, we partition the latent space Î© into four disjoint sets based on the geometry of the warp. These sets correspond directly to the colored regions in Fig. 3 (a): Background Rbg (gray) that must remain unchanged, Destinations Rdst (red and blue, a.k.a., ) where moved content is rendered with identity preserved, Inpainting Rinp (yellow) initialized from noise, and Transition Rtrans (green) that blends boundaries smoothly. With these regions clearly defined, the updated latent Ë†ğ’›ğ‘‡ is constructed by applying specific rule to each region: Ë†ğ’›ğ‘‡ (ğ’™) = ğ’›ğ‘‡ (M (ğ’™)), ğ (ğ’™), ğ’›ğ‘‡ (ğ’™), if ğ’™ Rdst, if ğ’™ Rinp, if ğ’™ Rbg Rtrans, (2) where ğ (0, I). Crucially, replacing the BNNI interpolation used in FastDrag with Gaussian noise in Rinp is essential. Unlike the uniform noise compared in Zhao et al. [2024], this approach aligns with the diffusion prior, prevents repetitive artifacts as shown in Fig. 11, and enables the ability of high-fidelity, text-guided inpainting discussed in the introduction."
        },
        {
            "title": "3.3 Correspondence-Driven Preservation",
            "content": "Having established the explicit correspondence map, we now detail two-part mechanism operating at the input (Sec. 3.3.1) and output (Sec. 3.3.2) of the attention calculation in single-stream attention layers only [Deng et al. 2025; Yin et al. 2025]. Using this map, the mechanism provides fine-grained control that preserves identity and background, ensuring robust full-strength inversion."
        },
        {
            "title": "3.3.1 Attention Input Control via Token Replacement and Concatena-\ntion. To preserve the background and identity, the first part modifies\nthe attention inputs of different regions. Let (Qğ’™, Kğ’™, Vğ’™ ) denote the\ncurrent attention tokens at position ğ’™ in a given layer and step, and\n, Kğ’™, Vğ’™ ) the tokens cached without positional encoding during\n(Qğ’™\nthe previous inversion process. Let RoPEğ’™ (Â·) re-encode tokens with\nthe rotary embedding at position ğ’™ [Su et al. 2024].",
            "content": "Background preservation via replacement (BG Pres.). For the background region Rbg, the purpose of absolute untouched is achieved by hard-replacing the attention tokens with their cached originals at every step and every single-stream layer, similar to ColorCtrl [Yin LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 5 et al. 2025]: (Qğ’™, Kğ’™, Vğ’™ ) (RoPEğ’™ (Qğ’™ ), RoPEğ’™ (Kğ’™ ), Vğ’™ ), ğ’™ Rbg. (3) Identity preservation via concatenation (ID Pres.). For the destination and transition regions (Rdst Rtrans), where identity must be preserved while allowing for coherent adaptation, we use token (ğ’™), which concatenation. Define unified source point map, selects correspondence sources: (ğ’™) = (cid:40) (ğ’™), ğ’™, if ğ’™ Rdst, if ğ’™ Rtrans. (4) For any position ğ’™ Rdst Rtrans, we form an augmented key ğ’™ and value ğ’™ by concatenating the cached tokens from its designated source (ğ’™): ğ’™ = concat (cid:0)Kğ’™, RoPEğ’™ (K (ğ’™ ) )(cid:1), ğ’™ = concat (cid:0)Vğ’™, (ğ’™ ) (cid:1). (5) (6) This provides strong, correspondence-driven signal to the attention calculation, robustly preserving identity while allowing for smooth blending at the boundaries."
        },
        {
            "title": "3.3.2 Attention Output Refinement via Gated Merging (Attn Refine).\nThe second part refines the attention output so that it cooperates\nwith the above token concatenation (following Wang et al. [2025a]),\nimproving visual quality and emphasizing the importance of handle\npoints over others. Let yğ’™ be the attention output at ğ’™ and yğ’™ be\nthe cached output. For ğ’™ âˆˆ Rdst,",
            "content": "yğ’™ (cid:0)1 ğ›¾ğ’™,ğ‘¡ (cid:1) yğ’™ + ğ›¾ğ’™,ğ‘¡ yM (ğ’™ ) where the blending factor ğ›¾ğ’™,ğ‘¡ is gated by our pre-computed matching weight from the map A: (7) , (8) ğ›¾ğ’™,ğ‘¡ = â„ğ‘¡ (ğ’™), where ğ‘¡ indexes the timestep and â„ğ‘¡ [0, 1] is factor that decays over time. This correspondence-driven gated merge eliminating the extra denoising steps required by CharaConsist, and addressing the instability of attention-similarity matching and scaling under fullstrength inversion. By making the weight strongest at the handle points (where (ğ’™) is maximal), it ensures precise control where it matters most, removing the need for multi-step latent optimization in previous methods [Shi et al. 2024; Zhang et al. 2025], while allowing for natural relaxation in surrounding regions."
        },
        {
            "title": "4 Experiments\n4.1 Setup",
            "content": "Baselines. We compare against eight prior arts: DragDiffusion [Shi et al. 2024], DragNoise [Liu et al. 2024a], FreeDrag [Ling et al. 2024], DiffEditor [Mou et al. 2024a], GoodDrag [Zhang et al. 2025], DragText [Choi et al. 2025]1, FastDrag [Zhao et al. 2024], and Inpaint4Drag [Lu and Han 2025]. Others are excluded due to incompatible settings or unusable official implementations. Method TTO-Req MD SC PQ DragNoise [Liu et al. 2024a] DragDiffusion [Shi et al. 2024] FreeDrag [Ling et al. 2024] DiffEditor [Mou et al. 2024a] GoodDrag [Zhang et al. 2025] DragText [Choi et al. 2025] FastDrag [Zhao et al. 2024] Inpaint4Drag [Lu and Han 2025] Ours 37.87 0.23 34.84 0.30 34.09 0.60 26.95 0.24 22.17 0.16 21.51 0.21 31.84 0.96 23.68 0. 7.793 0.04 7.905 0.01 7.928 0.02 7.603 0.01 7.834 0.03 7.992 0.02 7.935 0.09 7.802 0.06 8.058 0.01 8.325 0.02 8.281 0.03 8.266 0.01 8.318 0.01 8.227 0.03 8.278 0.01 7.961 0.04 7.704 0.01 7.798 0.03 7.816 0.02 7.715 0.01 7.795 0.01 7.886 0.01 7.904 0.06 7.615 0.06 21.49 0.04 8.205 0.03 8.395 0. 8.210 0.03 Table 1. Quantitative results compared with baselines on Drag-Bench. Implementation details. Unless otherwise noted, all baselines are run with their official implementations and default hyperparameters. For Inpaint4Drag [Lu and Han 2025], we adopt the refined masks and point pairs provided by the authors at inference, and replace distilled models with original models. Our method is built on FLUX.1 Krea-dev [Labs 2025], adopting the inversion method of UniEditFlow [Jiao et al. 2025] while replacing the editing strategy with our approach. For fair comparison, the number of denoising steps is fixed to 50 for all methods. Following CharaConsist, we activate ID Pres. and Attn Refine (Sec. 3.3) for the first 40 denoising steps, referring to the last activate timestep as the activation timestep. Additional details are in Appendix A.1. Benchmark and evaluation protocol. We evaluate on DragBench [Shi et al. 2024], which contains 205 images with 349 handle and target point pairs. Our primary accuracy metric is MD (mean distance) [Pan et al. 2023]. Although IF (image fidelity) [Kawar et al. 2023], typically computed with LPIPS [Zhang et al. 2018], is widely used, we do not report IF. Previous work [Choi et al. 2025; Lu et al. 2024] shows that successful drag edits necessarily change the image, often increasing LPIPS, whereas an unchanged image trivially attains the best score. Hence, IF can be misleading for drag editing. To obtain complementary, perceptually grounded view, we adopt the VIEScore [Ku et al. 2024] metrics from GEdit-Bench [Liu et al. 2025a]: SC (Semantic Consistency): whether the intended edit has been achieved. PQ (Perceptual Quality): the naturalness of the result and absence of artifacts. (Overall): the overall performance defined in Liu et al. [2025a]. In our setting, the intended edit is specified by the dragging instruction rather than natural-language instruction, but the scoring criteria remain unchanged. Each score ranges from 0 to 10 (higher is better) and is produced by the stateof-the-art MLLM evaluator, GPT-4o2 [Hurst et al. 2024]. To mitigate stochasticity in evaluation, we run every evaluation metrics three times and report both the mean and standard deviation. We additionally report binary TTO-Req (Test-Time Optimization Required) flag indicating whether method requires per-edit test-time optimization (e.g., LoRA fine-tuning or multi-step latent optimization) during inference. More evaluation details are in Appendix A.3."
        },
        {
            "title": "4.2 Quantitative Evaluation",
            "content": "Tab. 1 presents the benchmark results on DragBench. Despite not requiring LoRA fine-tuning or multi-step latent optimization for each image and drag operation, our method consistently outperforms existing approaches in all metrics, especially in terms of drag accuracy and the perceptual quality of the generated images. Notably, our approach achieves SOTA performance out-of-the-box, 1Since DragText is plug-and-play method, we evaluate it in conjunction with bestperforming GoodDrag. 2API access as of August 2025 6 Zixin et al. Fig. 4. Qualitative results compared with baselines on Drag-Bench. Best viewed with zoom-in. without the need for test-time optimization, making it both efficient and effective. Specifically, Inpaint4Drag [Lu and Han 2025] often produces boundary artifacts and color shifts between edited and unedited regions. Consequently, the LLM evaluator assigns lower scores under its over-editing rule. This indicates that, even with additional optimization of masks and point pairs, mask sensitivity of inpainting models degrades results. By contrast, our full-strength inversion method with attention controls attains strong performance while being more robust to the choice of masks and point pairs. 4.3 Qualitative Evaluation Fig. 4 qualitatively demonstrates the superiority of our method over existing baselines. In the first example, only our method correctly lift the arm with background maintained, while others introduce artifacts, such as distorted hands (e.g., DragText [Choi et al. 2025]) or unintended background changes (e.g., DragNoise [Liu et al. 2024a]). In the second example, most baselines fail to preserve the front structure of the vehicle, whereas our approach maintains it faithfully while applying the desired transformation. Inpaint4Drag [Lu and Han 2025] generates artifacts in the background. The third case shows that only our method successfully modifies the sofa geometry while preserving the integrity of pillows. In the fourth example, our approach correctly interprets hand proximity as intent to insert it into the pocket, while other baselines introducing artifacts. Finally, in the fifth example, only our approach and DragText successfully rotates the head of the tiger to the right without compromising overall image quality. These results are consistent with our quantitative evaluations and highlight the robustness and generality of our method, even without per image tuning or per instruction multi-step latent optimization. More results are shown in Appendix B.1. 4.4 User Study total of 20 expert participants evaluated comparisons between methods on 32 cases randomly sampled from DragBench. For each"
        },
        {
            "title": "Method",
            "content": "Preference (%) DragNoise [Liu et al. 2024a] DragDiffusion [Shi et al. 2024] FreeDrag [Ling et al. 2024] DiffEditor [Mou et al. 2024a] GoodDrag [Zhang et al. 2025] DragText [Choi et al. 2025] FastDrag [Zhao et al. 2024] Inpaint4Drag [Lu and Han 2025]"
        },
        {
            "title": "Ours",
            "content": "5.00 7 8.75 8 8.75 8 2.50 4 3.75 4 3.13 4 3.13 6 3.13 4 61.88 19 Table 2. User study on Drag-Bench. Fig. 5. Comparison between drag and move mode on Drag-Bench. comparison, method order positions were randomized and method identities were anonymized. Participants selected the preferred result according to predefined criteria (edit success, naturalness, and background preservation). Overall, LazyDrag was preferred in 61.88% of comparisons, outperforming all baselines (Tab. 2). More details are in Appendix A.4. LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 7 Fig. 7. Qualitative ablation of activation timesteps on Drag-Bench. From left to right, the activation timestep is increased."
        },
        {
            "title": "Method",
            "content": "MD SC PQ"
        },
        {
            "title": "Ours",
            "content": "21.49 0.04 8.205 0.03 8.395 0.03 8.210 0.03 - WTA - Latent Init - BG Pres. - ID Pres. - Attn Refine 23.69 0.16 24.73 0.08 56.49 0. 8.129 0.03 7.998 0.04 5.307 0.08 8.060 0.05 8.043 0.01 7.944 0.02 7.938 0.01 7.863 0.03 5.953 0.06 Table 3. Quantitative cumulative ablation on Drag-Bench under the same setting as Fig. 6 Method MD SC PQ Ours (40 as activation timestep) 21.49 0.04 8.205 0. 8.395 0.03 8.210 0.03 + 20 as activation timestep + 50 as activation timestep 34.23 0.29 21.81 0.26 7.036 0.03 8.298 0.03 8.788 0.01 8.072 0. 7.605 0.02 8.087 0.03 Table 4. Quantitative ablation of activation timesteps on Drag-Bench. et al. 2024] as the latent initialization. (ii) Without ID Pres. and Attn Refine (Sec. 3.3) we switch to the attention-similarity matching and scaling introduced in CharaConsist [Wang et al. 2025a]. Fig. 6 and Tab. 3 report benchmark results on Drag-Bench. Removing WTA and Latent Init increases MD and slightly reduces PQ and O, indicating that our initialization with the winner-takes-all fusion strategy and random initialization for inpainting regions suppresses repetitive artifacts and improves inpainting quality as proven in the figure. Further disabling background preservation causes additional drops in SC and due to color shifting and artifacts in the background. Finally, replacing our correspondence-driven preservation with attention-similarity control leads to sharp degradation, highlighting the sensitivity of full-strength inversion to mismatched attention alignment. The full method achieves the best performance. Effect of activation timesteps. We conduct an ablation study on the effect of activation timesteps by varying the activation timestep to 20, 40, and 50, as shown in Fig. 7 and Tab. 4. From the results, we observe that increasing the number of the activation timestep leads to more accurate destination points for dragging, though it may introduce more warping artifacts. Conversely, reducing the activation timestep results in more natural outputs, but may cause slight variations in identity or motion. More results are in Appendix B.4. For benchmark evaluations, we use 40 as balanced value. Fig. 6. Qualitative cumulative ablation on Drag-Bench. Rows remove one component relative to the row above. When WTA and Latent Init are removed we use latent init in FastDrag. When ID Pres. and Attn Refine are removed we switch to CharaConsist attention-similarity control."
        },
        {
            "title": "4.5 Comparison Between Drag and Move Modes",
            "content": "We evaluate LazyDrag with both drag and move modes on DragBench, with qualitative results shown in Fig. 5. The move mode tends to better preserve identity, as seen in the last two cases, rather than performing edits involving rotation or extension, as in the second and third examples. In contrast, the drag mode enables natural geometric transformations, including 3D rotations and extensions, albeit with slight degradation in detail texture preservation. Both of two modes can generate reasonable results. These findings highlight the flexibility of our explicit correspondence map when paired with our correspondence-driven preservation strategy. Future work may explore more matching strategies, such as 2D rotation, to further enhance diversity and controllability."
        },
        {
            "title": "4.6 Ablation Study",
            "content": "Effect of each component. We conduct an ablation study in which components are progressively removed from the full method. To keep functionality comparable when component is absent, we adopt controlled replacements: (i) Without WTA and Latent Init (Sec. 3.2) we revert to latent warpage optimization of FastDrag [Zhao 8 Zixin et al."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented LazyDrag, the first training-free method for dragbased editing with MM-DiTs under full-strength inversion. We begin by identifying the fundamental cause of instability in dragbased editing: the unreliability of implicit attention-based point matching. This diagnosis explains why prior methods adopted compromises such as test-time optimization or weakened inversion strength, which suppress text guidance, harm inpainting, and limit generative ability. Our approach directly solves this core issue by replacing fragile implicit point matching with an explicit correspondence map that drives attention controls during generation. This correspondence-driven preservation enables robust edits under full-strength inversion without TTO. As result, LazyDrag preserves identity and background, supports faithful inpainting, and leverages text guidance to resolve ambiguity in drag instructions. Extensive experiments show that LazyDrag achieves state-of-theart performance, unifying precise control with text guidance to execute complex semantic edits. By revealing that the perceived stabilityquality compromise is an artifact of flawed point matching, LazyDrag establishes more powerful and principled foundation for future research and marks concrete step toward intuitive, AI-native creative workflows and more sophisticated generative control."
        },
        {
            "title": "References",
            "content": "Stability AI. 2024. Stable Diffusion 3.5. https://github.com/Stability-AI/sd3.5. Accessed: May 2025. Franz Aurenhammer. 1991. Voronoi diagramsa survey of fundamental geometric data structure. ACM computing surveys (CSUR) 23, 3 (1991), 345405. Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. 2025. Ditctrl: Exploring attention control in multimodal diffusion transformer for tuning-free multi-prompt longer video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 77637772. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. 2023. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision. 2256022570. Gayoon Choi, Taejin Jeong, Sujung Hong, and Seong Jae Hwang. 2025. Dragtext: Rethinking text embedding in point-based image editing. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 441450. Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. 2025. FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing. In Forty-second International Conference on Machine Learning. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Haoran Feng, Zehuan Huang, Lin Li, Hairong Lv, and Lu Sheng. 2025. Personalize anything for free with diffusion transformer. arXiv preprint arXiv:2503.12590 (2025). Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohenor. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In The Eleventh International Conference on Learning Representations. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In NeurIPS"
        },
        {
            "title": "2021 Workshop on Deep Generative Models and Downstream Applications.",
            "content": "Xingzhong Hou, Boxiao Liu, Yi Zhang, Jihao Liu, Yu Liu, and Haihang You. 2024. Easydrag: Efficient point-based manipulation on diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84048413. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). Ziqi Jiang, Zhen Wang, and Long Chen. 2025. CLIPDrag: Combining Text-based and Drag-based Instructions for Image Editing. In The Thirteenth International Conference on Learning Representations. Guanlong Jiao, Biqing Huang, Kuan-Chieh Wang, and Renjie Liao. 2025. UniEditFlow: Unleashing Inversion and Editing in the Era of Flow Models. arXiv preprint arXiv:2504.13109 (2025). Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. 2024. PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code. In The Twelfth International Conference on Learning Representations. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 60076017. Diederik Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013). Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision. 40154026. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. HunyuanVideo: Systematic Framework For Large Video Generative Models. CoRR (2024). Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. 2024. VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1226812290. Black Forest Labs. 2024. Flux. https://github.com/black-forest-labs/flux. Accessed: May 2025. Black Forest Labs. 2025. FLUX.1 Krea-dev. https://bfl.ai/announcements/flux-1-kreadev. Accessed: July 2025. Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, Yi Jin, and Jinjin Zheng. 2024. Freedrag: Feature dragging for reliable point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 68606870. Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, and Shengfeng He. 2024a. Drag your noise: Interactive point-based editing via diffusion semantic propagation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 67436752. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. 2025a. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761 (2025). Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, et al. 2025b. Generative video propagation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1771217722. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. 2024b. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 85998608. Jingyi Lu and Kai Han. 2025. Inpaint4Drag: Repurposing Inpainting Models for DragBased Image Editing via Bidirectional Warping. In International Conference on Computer Vision (ICCV). Jingyi Lu, Xinghui Li, and Kai Han. 2024. Regiondrag: Fast region-based image editing with diffusion models. In European Conference on Computer Vision. Springer, 231 246. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. 2023. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378 (2023). Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2024a. Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84888497. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. 2024b. DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models. In The Twelfth International Conference on Learning Representations. Naylor. 1969. Theoretical elasticity, by AE Green and W. Zerna . Clarendon Press, Oxford, 1968. xv+ 457 pages. Canad. Math. Bull. 12, 4 (1969), 537538. OpenAI. 2025. GPT 4o Image Generation. https://openai.com/index/introducing-4oimage-generation/. Accessed: 2025-06-13. Xingang Pan, Ayush Tewari, Thomas LeimkÃ¼hler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 conference proceedings. 111. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 41954205. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International conference on machine learning. PMLR, 10601069. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. 2025. Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations. In The Thirteenth International Conference on Learning Representations. LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 9 Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. 2024. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 88398849. Joonghyuk Shin, Daehyeon Choi, and Jaesik Park. 2024. Instantdrag: Improving interactivity in drag-based image editing. In SIGGRAPH Asia 2024 Conference Papers. 110. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. 2023. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1797917989. Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. 2025b. Taming Rectified Flow for Inversion and Editing. In Forty-second International Conference on Machine Learning. Mengyu Wang, Henghui Ding, Jianing Peng, Yao Zhao, Yunpeng Chen, and Yunchao Wei. 2025a. CharaConsist: Fine-Grained Consistent Character Generation. arXiv preprint arXiv:2507.11533 (2025). Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, Charles Ling, and Boyu Wang. 2025. Unveil inversion and invariance in flow transformer for versatile image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference. 2847928489. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. Cogvideox: Textto-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024). Zixin Yin, Xili Dai, Ling-Hao Chen, Deyu Zhou, Jianan Wang, Duomin Wang, Gang Yu, Lionel Ni, and Heung-Yeung Shum. 2025. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer. arXiv preprint arXiv:2508.09131 (2025). Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, and Baoyuan Wang. 2023. Talking head generation with probabilistic audio-to-visual diffusion priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76457655. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586595. Zewei Zhang, Huan Liu, Jun Chen, and Xiangyu Xu. 2025. GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models. In The Thirteenth International Conference on Learning Representations. Xuanjia Zhao, Jian Guan, Congyi Fan, Dongli Xu, Youtian Lin, Haiwei Pan, and Pengming Feng. 2024. Fastdrag: Manipulate anything in one step. Advances in Neural Information Processing Systems 37 (2024), 7443974460. Yuan Zhou, Junbao Zhou, Qingshan Xu, Kesen Zhao, Yuxuan Wang, Hao Fei, Richang Hong, and Hanwang Zhang. 2025. DragNeXt: Rethinking Drag-Based Image Editing. arXiv preprint arXiv:2506.07611 (2025). Implementation Details Inference Settings A.1 For all baselines, we use their official code with default hyperparameters for inference. The number of denoising steps is set to 50, and classifier-free guidance (CFG) [Ho and Salimans 2021] is set to 1. All images on Drag-Bench are generated at their original resolution, while other images are generated at 1024 1024. All generations are performed on single NVIDIA H800 GPU. EasyDrag [Hou et al. 2024] and CLIPDrag [Jiang et al. 2025] are excluded from comparison because their released implementations either fail to execute reliably or do not reproduce the results reported in the papers. For Inpaint4Drag, we remove the LCM [Luo et al. 2023] LoRA and fix the number of denoising steps to 50. We also replace the distilled VAE [Kingma and Welling 2013] with the original VAE to improve reconstruction and generation quality. These settings are chosen to obtain the strongest editing performance rather than to optimize for speed. For our inversion process, we adopt the official inversion method of UniEdit-Flow [Jiao et al. 2025] but replace the editing component with our proposed strategy. We apply our correspondence-driven preservation (Sec. 3.3) only to the single-stream attention layers in FLUX.1 Krea-dev [Labs 2025]. Since additional manipulation in dual-stream attention layers does not lead to noticeable improvements [Deng et al. 2025; Wang et al. 2025b; Yin et al. 2025], we adopt more efficient and concise design by limiting modifications to single-stream layers only. A."
        },
        {
            "title": "Implementation Details of Displacement Field\nCalculation",
            "content": "Per-instruction displacement. Following the principles of elasticity [Naylor 1969; Zhao et al. 2024], the influence of an external force decays inversely with distance from the force origin, and the direction of the induced displacement aligns with the direction of the applied force. We represent each drag instruction ğ’…ğ‘– as vector from source ğ’”ğ‘– to target ğ’†ğ‘– . For ğ’‘ ğ‘— P, we write ğ‘— = ğœ†ğ‘– ğ‘– ğ‘— ğ’…ğ‘–, ğ’— (9) where ğœ†ğ‘– ğ‘— is stretch factor. Using reference circle ğ‘‚ that circumscribes the bounding rectangle of P, extend the ray ğ’”ğ‘– ğ’‘ ğ‘— to ğ‘— . Enforcing parallelism between ğ’—ğ‘– intersect ğ‘‚ at ğ’’ğ‘– ğ‘— and ğ’…ğ‘– yields ğœ†ğ‘– ğ‘— = ğ’—ğ‘– ğ‘— 2 ğ’…ğ‘– 2 = ğ’‘ ğ‘— ğ’‘ğ‘– ğ‘— 2 ğ’”ğ‘– ğ’†ğ‘– 2 = ğ’‘ ğ‘— ğ’’ğ‘– ğ’”ğ‘– ğ’’ğ‘– ğ‘— 2 ğ‘— 2 . (10) Winner-takes-all blending. Weighted averaging multiple instruction can fail when different drags point in opposite directions. We therefore assign each ğ’‘ ğ‘— to its nearest handle ğ’”ğ‘– (a Voronoi partition [Aurenhammer 1991]) as illustrated in Fig. 3 (a), where the red and blue regions correspond to two drag instructions, with weights ğ›¼ğ‘– ğ‘— = (cid:40) ğ’‘ ğ‘— ğ’”ğ‘– 1 2 , , ğ’‘ ğ‘— ğ’”ğ‘–, otherwise. (11) 10 Zixin et al. Fig. 8. Examples of Drag-Bench cases with various additional text prompts."
        },
        {
            "title": "Method",
            "content": "MD SC PQ FastDrag [Zhao et al. 2024] 31.84 0. 7.935 0.09 8.278 0.01 7.904 0.06 + WTA + Latent Init 28.55 0.07 28.97 0.17 8.049 0.06 8.081 0. 8.339 0.01 8.341 0.01 8.012 0.03 8.050 0.02 Table 5. Quantitative ablation of WTA and Latent Init with U-Nets on DragBench. The final displacement is determined by the winning instruction ğ‘– = arg maxğ‘– ğ›¼ğ‘– ğ‘— : ğ’— ğ‘— = ğ’— ğ‘— = ğœ†ğ‘– ğ‘– ğ‘— ğ’…ğ‘– . (12) This yields sharper spatial separation and avoids interference between opposing drags. Fig. 9. Effect of activation timestep sensitivity on Drag-Bench. From left to right, the activation timestep is progressively increased. Unified move/scale model. For axis-aligned resizing, we introduce scaling vector ğ’“ R2 to form unified model: ğ’— ğ‘— = ğœ†ğ‘– ğ‘— ğ’…ğ‘– + (ğ’“ 1) (ğ’‘ ğ‘— ğ‘ ğ‘–), (13) where denotes element-wise product. For move-and-scale operation, we set ğœ†ğ‘– ğ‘— = ğ›¼ğ‘– ğ‘— = 1. A.3 Evaluation Details For the VIEScore evaluation, we follow GEdit-Bench [Liu et al. 2025a], using the same prompts for PQ and O. For SC, we adopt the instruction shown in Fig. 14, together with the source image, drag-instruction image, and the edited image. Score collection and calculation are carried out using the official GEdit-Bench codebase. A.4 User Study Details To evaluate the effectiveness of our method, we randomly selected 32 results for eight comparison methods on Drag-Bench [Shi et al. 2024] and shuffled their indices to ensure fair comparison. We invited 20 participants, each with relevant skills, to perform the tasks following the instructions provided through the user interface, as shown in Fig. 13. Fig. 10. Failure cases on Drag-Bench. More Results and Analysis B.1 More Results on DragBench Fig. 12 presents additional qualitative results on Drag-Bench. As shown, our method produces more natural and accurate outputs while better preserving background consistency compared to other baselines. These results further demonstrate the robustness and effectiveness of LazyDrag. LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 11 U-Net-based FastDrag [Zhao et al. 2024]. First, we replace the original average blending of multiple drag instructions with our WTA blending. Second, we substitute the original BNNI interpolation with standard normal noise added to the image latent, scaled to the inversion strength. As shown in the top row of Fig. 11, our blending method improves target localization under complex, multiinstruction scenarios. This is reflected in improved MD and SC scores in Tab. 5, computed on Drag-Bench (which includes 97 multidrag cases). In the bottom row of Fig. 11, our random initialization reduces repetitive pattern artifacts, aligning with the quantitative gains in PQ and O. B.4 Limitations Fig. 9 illustrates failure cases on Drag-Bench when the final activation timestep is set too high for handling multiple dragging instructions. While the results show accurate target positions for the dragged points, they exhibit unnatural artifacts, especially when target points overlap. By slightly reducing the final activation timesteps, the results appear more natural while still preserving reasonable target positions. Additionally, due to the VAE compression in diffusion models and the latent patching strategy[Esser et al. 2024], the model struggles with very small drag distances. As shown in Fig. 10, the model can execute fine-grained edits such as closing the eyes, but slight positional shifts may still occur. Moreover, the quality of both the edit and generation heavily depends on the underlying base model. As foundation models continue to improve, we anticipate that the performance and applicability of our method will evolve accordingly. Fig. 11. Qualitative ablation of WTA and Latent Init with U-Nets on DragBench. B.2 Effect of Text Guidance Fig.8 shows examples from Drag-Bench with different text guidance prompts. The results demonstrate that LazyDrag effectively resolves ambiguities caused by drag instructions alone when additional guided prompts are provided. Unlike prior methods such as DragText[Choi et al. 2025] and CLIPDrag [Jiang et al. 2025], our approach enables more complex and precise text guidance. B.3 Effect with U-Nets While our full method is designed for MM-DiTs, key components such as WTA and Latent Init (Sec. 3.2) are also compatible with U-Nets. To demonstrate this, we conduct an ablation study on the 12 Zixin et al. Fig. 12. Additional qualitative results compared with baselines on Drag-Bench. LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence 13 Fig. 13. User interface for user study. 14 Zixin et al. Fig. 14. Instruction of SC evaluation."
        }
    ],
    "affiliations": [
        "StepFun",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}