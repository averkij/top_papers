{
    "paper_title": "Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs",
    "authors": [
        "Yufa Zhou",
        "Shaobo Wang",
        "Xingyu Dong",
        "Xiangqi Jin",
        "Yifang Chen",
        "Yue Min",
        "Kexin Yang",
        "Xingzhang Ren",
        "Dayiheng Liu",
        "Linfeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether post-training techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively $\\textit{generalize}$ to multi-agent scenarios. We use economic reasoning as a testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce $\\textbf{Recon}$ ($\\textbf{R}$easoning like an $\\textbf{ECON}$omist), a 7B-parameter open-source LLM post-trained on a hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 7 7 5 0 0 . 6 0 5 2 : r Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs Yufa Zhou1, Shaobo Wang2,3, Xingyu Dong4, Xiangqi Jin2, Yifang Chen5, Yue Min2, Kexin Yang3, Xingzhang Ren3, Dayiheng Liu3, Linfeng Zhang2 1Duke University, 2EPIC Lab, Shanghai Jiao Tong University, 3Qwen Team, Alibaba Group, 4University of Pennsylvania, 5The University of Chicago yufa.zhou@duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Directly training Large Language Models (LLMs) for Multi-Agent Systems (MAS) remains challenging due to intricate reward modeling, dynamic agent interactions, and demanding generalization requirements. This paper explores whether posttraining techniques, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR), can effectively generalize to multiagent scenarios. We use economic reasoning as testbed, leveraging its strong foundations in mathematics and game theory, its demand for structured analytical reasoning, and its relevance to real-world applications such as market design, resource allocation, and policy analysis. We introduce Recon (Reasoning like an ECONomist), 7B-parameter open-source LLM post-trained on hand-curated dataset of 2,100 high-quality economic reasoning problems. Comprehensive evaluation on economic reasoning benchmarks and multi-agent games reveals clear improvements in structured reasoning and economic rationality. These results underscore the promise of domain-aligned post-training for enhancing reasoning and agent alignment, shedding light on the roles of SFT and RL in shaping model behavior. Code is available at https://github.com/MasterZhou1/Recon."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have recently progressed from general-purpose text generation to exhibiting strong reasoning capabilities across mathematics and coding, as exemplified by OpenAIs o1 series [56] and DeepSeek-R1 [15]. This transition has been driven by techniques such as Chainof-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF) [83, 31, 57, 42], culminating in the emergence of Large Reasoning Models (LRMs) [7, 90]. key framework in this space is Reinforcement Learning with Verifiable Rewards (RLVR) [34], which replaces standard reward models with outcome-verification functions for tasks such as math solving and instruction following. RLVR has since been extended to Medicine [100], SQL [50], Logic [89], and Finance [46, 60, 104]. Complementary methods such as Su et al. [69] and Liu et al. [48] extend RLVR to soft or online reward signals, while LIMO [95], LIMR [38], and s1 [53] demonstrate that post-training can elicit strong reasoning in smaller models under specialized, limited data. In parallel, LLM-based Multi-Agent Systems (MAS) have gained prominence as platforms for exploring complex interactions, cooperation, and emergent social behaviors [58, 103, 36]. pivEqual contribution. Corresponding Authors. Preprint. Under review. Figure 1: Overview of the Recon Pipeline. Step 1: We curate high-quality economic dataset (Recon Dataset) from benchmarks such as STEER, and distill reasoning traces from teacher models to construct the Recon-CoT Dataset. Step 2: base model is post-trained via supervised fine-tuning (Recon-SFT) on Recon-CoT and reinforcement learning (Recon-RL) on the Recon Dataset. Step 3: The resulting models are evaluated on reasoning benchmarks (Recon-Eval Dataset), self-play, and multi-agent games against opponent agents. otal objective within MAS is economic rationalitythe capability to systematically reason about incentives, trade-offs, and strategic decision-makingwhich underpins effective coordination and negotiation. The STEER benchmark [62] formalizes economic rationality by testing LLMs on foundational principles such as utility maximization, behavioral bias, and strategic reasoning. This aligns closely with game theory, longstanding theoretical foundation for MAS research [6, 99, 68, 52], increasingly central to evaluating LLM-based agents [92, 101, 18, 70]. Recent efforts in developing unified economic-agent environments and benchmarks [64, 37, 17, 73, 29] further emphasize this research direction. Despite significant interest, directly training LLMs for multi-agent interactions remains complex and underexplored, often hampered by challenges like dense reward modeling, unstable coordination dynamics, and conflicting agent objectives [16]. Existing methods, such as multi-agent co-training [97] and MARFT [41], typically require extensive supervision and tailored agent architectures, limiting their scalability and generalization potential. This prompts critical research question: Can post-training techniques generalize effectively to multi-agent scenarios? To investigate this fundamental question, we focus on economic reasoning as suitable testbed, given its structured mathematical underpinnings and strategic dynamics essential to MAS. Economic tasks frequently involve intricate multi-step reasoning, such as evaluating trade-offs, aligning incentives, and anticipating others behaviorsideal for leveraging improvements from SFT and RLVR. While previous studies primarily assess economic rationality, our work actively enhances it via targeted post-training. Additionally, real-world applications reinforce this domains significance, demonstrated by simulations of heterogenous economic-agent roles using LLMs [25, 37, 88]. In this paper, we introduce Recon, an LLM specifically designed for structured economic decisionmaking. We curate high-quality dataset comprising 2,100 examples spanning 15 critical economic categories, including behavioral bias detections, repeated-game strategies, mechanism-design equilibria. This dataset builds upon and expands benchmarks such as STEER [62], EconLogicQA [61], and EconNLI [23]. Recon employs Supervised Fine-Tuning (SFT) and subsequent Group Relative Policy Optimization (GRPO) [63], fine-tuning the DeepSeek-R1-Distill-Qwen-7B model [15, 93] to enhance structured reasoning and test generalization capabilities across both single-agent and multi-agent economic tasks. 2 Our experimental results demonstrate clear improvements in structured reasoning and strategic decision-making through domain-aligned post-training. Notably, models trained on economic problems display economically rational behavior in multi-agent games, despite receiving no interactionbased supervision. This suggests that structured problem-solving can promote latent alignment with game-theoretic principles, indicating that post-training not only enhances task-level accuracy but also encourages emergent rational behavior. These findings provide fresh insights into the distinct roles of SFT and RL in shaping model behavior, generalization, and alignment [98, 10, 82, 47, 21]. Our primary contributions include: We curate high-quality dataset of 2,100 problems across 15 economic reasoning categories designed to assess core rationality skills. We introduce Recon, 7B open-source model post-trained via SFT and GRPO for structured economic and strategic reasoning. We empirically show that reasoning-oriented post-training enhances both benchmark accuracy and generalization to multi-agent settings. We hypothesize post-training as scalable route to agent alignment, where economic problemsolving effectively fosters strategic behavior."
        },
        {
            "title": "2 Related Work",
            "content": "Economic Agent Applications. Recent work has integrated LLMs into economic simulations and agent-based modeling across range of applications. Hao and Xie [25] proposed multi-agent LLM framework for policy analysis, simulating heterogeneous societal groups. Li et al. [37] introduced EconAgent for macroeconomic modeling, demonstrating human-like decision-making in LLM-driven agents. Xiao et al. [88] developed TradingAgents to model financial markets with specialized roles such as analysts and traders. Wu et al. [85] applied LLMs to generate persuasive, context-grounded marketing content for real estate. Lazebnik and Shami [35] combined LLMs with reinforcement learning to simulate tax evasion dynamics, and Yu et al. [96] introduced FINCON, synthesized multiagent system using conceptual verbal reinforcement for financial decision-making. While these works focus on application design, our approach complements them by enhancing economic reasoning and decision-making capabilities through post-trainingpotentially improving performance in real-world multi-agent settings. Game-Theoretic Evaluation. Game-theoretic reasoning has become an essential evaluation paradigm for assessing LLM performance in multi-agent scenarios. Benchmarks such as GTBench [17], GameBench [11], and GAMABench [73] evaluate strategic reasoning across cooperative, adversarial, and sequential games. Several studies focus on negotiation: LAMEN [14] and Abdelnabi et al. [1] examine stakeholder deliberation, while Hua et al. [29] introduce formal agent workflow for modeling negotiation games and equilibrium behavior. GLEE [64] provides unified benchmark for economic interactions, and Akata et al. [2] study repeated games to analyze long-term cooperation. Piedrahita et al. [59] further highlight the challenge that increased reasoning capacity in LLMs can paradoxically undermine cooperation, especially in public goods settings with institutional enforcement. Unlike these evaluation-oriented studies, our research leverages post-training techniques to actively enhance LLMs reasoning abilities and generalize their strategic decision-making to broader economic and multi-agent contexts."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Overall Pipeline Our training pipeline comprises two core post-training stages: supervised fine-tuning (SFT) on synthetic reasoning data, followed by reinforcement learning with verifiable rewards (RLVR) on curated economic problems. We detail the dataset curation process in Section 4. Figure 1 provides schematic overview of the full pipeline, illustrating the flow from data generation to post-training and downstream multi-agent gameplay. 3 3.2 Base Model We select DeepSeek-R1-Distill-Qwen-7B3 [93] as our base model due to its strong reasoning ability, inherited from DeepSeek-R1 [15] through targeted distillation. Among open-source models, Qwen-based [93] variants consistently outperform comparable LLaMA-based [22] counterparts on multiple benchmarks. The 7B parameter scale offers practical balance between performance and efficiency, making it well-suited for fine-tuning. Furthermore, DeepSeek-R1-Distill-Qwen-7B achieves competitive results, surpassing GPT-4o [55] on challenging reasoning benchmarks such as MATH [26] and AIME [51], thus providing robust foundation for structured domain-specific adaptation. 3.3 Supervised Fine-Tuning Supervised Fine-Tuning (SFT) adapts pretrained models to specific tasks by imitating inputoutput pairs, effectively aligning models with structured reasoning when high-quality demonstrations are available [53, 95]. We distill outputs from strong reasoning models (e.g., DeepSeek-R1 [15], QwQ32B [72]) containing both reasoning traces and final answers, training the model to generate solutions with coherent thought processes. Let DSFT = {(xi, yi)}N i=1 denote the fine-tuning dataset of size , where xi is the input prompt and yi is the target output. In our formulation, each output is tuple yi = (ci, ai), where ci represents the step-by-step reasoning and ai the final answer. The training objective minimizes the negative log-likelihood of the output tokens: LSFT(θ) = E(x,y)DSFT [log pθ(y x)] , where θ are the model parameters and pθ(y x) is the conditional probability of generating given x. The loss is computed only over the output tokens y, excluding the prompt x. To reinforce structured reasoning, we standardize the output format by enclosing the reasoning process ci within special <think> and </think> tokens. This explicit markup helps the model distinguish intermediate steps from final outputs and provides structure that is beneficial for downstream reward modeling. Overall, SFT provides strong initialization that enhances reasoning generalization and stabilizes subsequent RL stage. 3.4 GRPO We adopt Group Relative Policy Optimization (GRPO) [63] as our reinforcement learning posttraining algorithm. GRPO improves efficiency by eliminating the need for value function and instead estimates advantages from group of sampled outputs. For each input query q, drawn from the data distribution Dq, group of responses {o1, o2, . . . , oG} is sampled from the old policy πθold. The current policy πθ is then optimized by maximizing the following objective: o \" min wiAi, clip(wi, 1ϵ, 1+ϵ)Ai β KL(πθπref) JGRPO(θ) = # , qDq,{oi}G i=1πθold (q) 1 i=1 where wi := πθ(oiq) πθold (oiq) . Here, πθ(oiq) denotes the probability of generating response oi given query under the current policy, and πθold(oiq) is the probability under the old policy used for sampling. The advantage Ai reflects the relative quality of each response and is computed as the normalized reward within the group: Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) , where ri is the scalar reward assigned to output oi. The KL penalty encourages stability by penalizing deviations from reference policy πref, and is defined as: KL(πθπref) = πref(oiq) πθ(oiq) log πref(oiq) πθ(oiq) 1. 3https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B The hyperparameter ϵ controls the clipping threshold for policy ratio updates, while β scales the KL regularization strength. 3.5 Reward Design To support structured outputs during GRPO training, we develop hierarchical, rule-based reward function. It scores responses across three stages, format validity, answer extraction, and correctness, while enforcing consistent use of <think> and boxed{} conventions to align reasoning traces with final predictions. Full logic and implementation details are provided in Appendix A.2."
        },
        {
            "title": "4 Dataset Curation",
            "content": "Table 1: Accuracy of each model across economic reasoning categories (detailed in Appendix A.3). Model Names Mathematical Single-Agent Multi-Agent Representing Foundations Enviornments Enviornments Other Agents Reasoning Logical R1-Distill-Qwen-7b Qwen-2.5-7b-Instruct Llama-3.1-8b-Instruct Gemma-2-9b-it GPT-4o GPT-3.5-turbo 0.896 0.875 0.619 0.828 0.887 0. 0.751 0.775 0.637 0.798 0.841 0.756 0.630 0.514 0.309 0.375 0.625 0.377 0.740 0.680 0.569 0.639 0.782 0.608 0.430 0.650 0.350 0.590 0.640 0.510 Table 2: Accuracy on specified question types from Multi-Agent Environments category (detailed in Appendix A.3). Model Names Back. Bayes Best Dom. Domd Enf. Feas. Ind. Nash Resp. Strat. Strat. Intp. Iter. Pure Subg. Trig. Rem. Nash Nash 0.360 0.681 0.464 0.960 0.840 0.222 0.500 0.878 0.854 0.532 0.857 0.217 R1-Distill-Qwen-7b 0.220 0.320 0.200 1.000 0.760 0.040 0.360 0.900 0.260 0.180 0.200 0.020 Gemma-2-9b-it LLaMA-3.1-8b-Instruct 0.000 0.235 0.157 0.408 0.500 0.020 0.580 0.820 0.420 0.000 0.360 0.200 0.451 0.569 0.294 1.000 0.940 0.824 0.549 1.000 0.863 0.255 0.588 0.176 GPT-4o 0.098 0.196 0.196 1.000 0.627 0.000 0.725 0.882 0.431 0.039 0.294 0.039 GPT-3.5-turbo High-quality data is essential for effective post-training [53, 95, 38], motivating our focus on careful dataset construction and analysis. Section 4.1 describes our data sources, and Section 4.2 presents baseline experiments with several LLMs on existing benchmarks, analyzing their performance to gain intuitions. Section 4.3 details the creation of the Recon Dataset, while Section 4.4 outlines the distillation of reasoning traces for Recon-CoT. 4.1 Dataset Source We curate four datasets covering distinct facets of economic reasoning: STEER Benchmark [62] provides 600K multiple-choice questions across 48 microeconomic categories, spanning arithmetic, probability, psychological biases, and game theory. Each question includes prompt, candidate answers, the correct label, and metadata. STEER serves as our primary benchmark for general economic reasoning. EconLogicQA [61] contains 650 human-validated questions inspired by real-world news. Each presents 34 interdependent events requiring correct temporal or causal ordering, testing planning and causal consistency. EconNLI [23] offers 11K premisehypothesis pairs annotated for entailment or neutrality. Derived from Wikipedia, it evaluates models ability to infer causal and logical relations in economic narratives. Pure-Strategy Equilibrium Games [20] consists of 33 payoff matrices labeled with Pure Nash and Perfectly Transparent Equilibria. To supplement STEERs noisier game-theoretic items, we convert selected matrices from this ETH Zürich dataset into natural language prompts to assess equilibrium reasoning. 5 4.2 Curation Experiment Analysis Appendix A.3 outlines our dataset curation experiment settings for assessing baseline performance across various LLMs on the collected question types. Our experiment and analysis addresses three key questions: (i) comparative performance of open-weight and closed-source models, (ii) the impact of reasoning distillation, and (iii) identifying specific bottlenecks in economic reasoning skills. Results summarized in Tables 1 and 2 yield the following insights: Closed models lead, but reasoning models narrow the gap. Closed-source GPT-4o consistently achieves top accuracy in most macro-categories, though notably, DeepSeek-R1-Distill-Qwen-7B slightly surpasses GPT-4o on Mathematical Foundations (0.896 vs. 0.887) and Multi-Agent Environments (0.630 vs. 0.625). This indicates that specialized open-source reasoning models can effectively rival closed-source counterparts on fundamental economic tasks. Reasoning distillation significantly improves performance. DeepSeek-R1-Distill-Qwen-7B outperforms all other comparable-sized open models across most macro-categories, particularly excelling in Multi-Agent Environments. In contrast to financial domains [46, 60, 104], where R1-style models perform poorly on structured tasks (e.g., accounting reports), our results highlight economic reasoning as domain uniquely suited to fine-grained reasoning capabilities. Complex game-theoretic tasks remain challenging. Detailed examination in Table 2 reveals significant weaknesses in advanced strategic reasoning, particularly Trigger strategies and Enforceability in repeated games. Even the leading GPT-4o achieves limited accuracy (0.176 and 0.824 respectively), while most open models fall below baseline on these long-horizon reasoning tasks. DeepSeek-R1-Distill-Qwen-7B as the optimal baseline for further training. Despite these bottlenecks, R1-Distill-Qwen-7Bs solid overall performance (macro-average 0.69) and promising baseline competence in strategic reasoning (e.g., 0.217 on Trigger and 0.222 on Enforceability) make it strong candidate for subsequent SFT and RL fine-tuning. Its open-source accessibility and manageable scale provide an ideal foundation for enhancing economic reasoning capabilities, particularly in challenging multi-agent contexts. 4.3 Dataset Curation Recon Corpus Overview. We present Recon Dataset, emphasizing the 15 most challenging categories identified in our benchmark analysis (Section 4.2). These include advanced game theory, behavioral biases, and logical inference. We curate total 2,100 question-answer pairs: Training Split (Recon Dataset): 1,800 questions, proportionally sampled based on empirical error rates per category  (Table 6)  . Evaluation Split (Recon-Eval): 300 held-out questions (20 per category), mirroring the training distribution. Sampling Strategy. Within each category, we remove ambiguous or low-quality items, then uniformly sample remaining questions to meet predefined quotas (e.g., 250 questions for Enforceability, 75 for Certainty Effect). Prompt Template. Each question employs structured prompt that encourages models to reason step-by-step and explicitly box their final answers. representative example is illustrated in Figure 4. Category Breakdown. Table 6 summarizes the fifteen Recon categories, providing concise descriptions, data provenance, and question counts for the training set. The evaluation set replicates these proportions at one-sixth scale (20 questions per category, totaling 300). 4.4 Reasoning Trace Distillation We distill chain-of-thought (CoT) traces from stronger teacher and filter them for correctness. Teacher Prompting. For each of the 1,800 Recon training items, we issue the same prompt template as in Figure 4 to the teacher model QwQ-32B [72]. The template forces the teacher to put thinking 6 process inside <think> ... </think> and to place its final choice in boxed{. . . } so that both the trace and the answer can be extracted programmatically. Filtering. We parse the teachers boxed answer and compare it to the gold label. Only items the teacher answers correctly are kept. This yields clean set of 868 (question, gold answer, chain-ofthought) triples covering all 15 Recon categories. CoT Corpus. The resulting 868 demonstrations constitute the Recon-CoT dataset. We use this dataset for SFT. As each trace ends with the same extraction-friendly pattern, the fine-tuned model separates reasoning from its verdict, simplifying downstream reward modeling and evaluation."
        },
        {
            "title": "5 Evaluation Results",
            "content": "Due to space constraints, we moved our model training configuration and the analysis and plots of training dynamics to Appendices A.4 and A.5. 5.1 Evaluation Details We evaluate on three benchmarks: our held-out Recon-Eval (300 economic reasoning problems), the Complete-Information Games framework [29], and GTBench [17]. The Complete-Information Games suite includes 5 simultaneous and 5 sequential games testing agents on (1) communication, (2) cooperation, and (3) strategic alignment. We run 20 trials per game (temperature 0.6, no workflow), where the agent plays against itself. Performance is measured by Nash Equilibrium frequency. GTBench focuses on strategic and logical reasoning in competitive settings. Each task is evaluated over 10 trials (temperature 0.6) using PromptAgent against fixed opponents (e.g., GPT-4o-mini), with win rate as the metric. For the two gameplay benchmarks, we evaluate four 7B models: Qwen2.5-Instruct, R1-Distill-Qwen, Recon-SFT, and Recon-RL. 5.2 Economic Reasoning Performance Table 3: Accuracy and percentage score on Recon-Eval (evaluated at temperature = 0.0) for the Base Model, Recon-SFT, and Recon-RL. Model Accuracy Score (%) Base Model Recon-SFT Recon-RL 145 / 300 179 / 300 186 / 300 48.30 59.67 63.00 Table 3 reports accuracy on our 300-item Recon-Eval (Section 4.3) set across training stages. Starting from the base model (48.3%), SFT boosts performance to 59.7%, gaining 11.4 points, suggesting that distilled teacher traces effectively transfer structured reasoning patterns. GRPO further improves accuracy to 63.0%, adding 3.3 points. Overall, the SFTRL pipeline achieves 14.7% absolute gain, validating post-training as viable strategy for aligning DeepSeek-R1-Distill-Qwen-7B with economic reasoning tasks. 5.3 Generalization to Strategic Games To verify that the gains obtained from economic post-training extend beyond single-step reasoning, we evaluate the models in two unseen interactive settings, testing if economic reasoning post-training generalizes to strategic interaction. Frequent convergence to Nash Equilibria. Table 4 reveals clear monotonic gain in self-play equilibrium accuracy as economic post-training is added. Relative to the R1-Distill baseline, ReconSFT increases the proportion of equilibrated outcomes from 0.39 to 0.47 in simultaneous-move games while preserving the strong 0.79 level in sequential games. subsequent GRPO stage raises these figures to 0.51 and 0.86, yielding an overall mean of 0.685, 9.5 points improvement over R1-Distill and almost double the 0.345 attained by the non-reasoning Qwen-2.5-7B-Instruct. 7 Table 4: Nash Equilibrium Self-Play Accuracy [29] (Settings detailed in Section 5.1). Games Qwen2.5 R1-Distill"
        },
        {
            "title": "SFT GRPO",
            "content": "Simultaneous Games Prisoners Dilemma Stag Hunt Battle of Sexes Wait-Go Game Duopolistic Competition Simultaneous Average 0.85 0.50 0.20 0.15 0.15 0.37 0.95 0.50 0.10 0.25 0.15 0.39 Sequential Games Escalation Monopoly Hot-Cold Draco Trigame Sequential Average Overall Average 0.15 0.95 0.05 0.40 0.05 0.32 0.35 0.95 0.95 0.65 0.75 0.65 0.79 0.59 1.00 0.45 0.15 0.70 0.05 0. 0.85 0.90 0.90 0.75 0.50 0.78 0.63 1.00 0.60 0.20 0.65 0.10 0.51 1.00 0.95 0.95 0.90 0.50 0.86 0.69 Table 5: GTBench win rates against GPT-4o-mini [17] (Settings detailed in Section 5.1). Games Qwen2.5 R1-Distill SFT GRPO breakthrough connect4 first_sealed_auction kuhn_poker liars_dice negotiation nim pig prisoners_dilemma tictactoe Overall Average 0.40 0.20 0.30 0.70 0.30 0.00 0.70 1.00 0.00 0.40 0.40 0.10 0.40 0.40 0.30 0.30 0.70 0.00 0.90 1.00 0.80 0.49 0.20 0.40 0.50 0.70 0.60 0.80 0.00 0.50 1.00 0.60 0. 0.30 0.30 0.50 0.70 0.40 0.90 0.00 0.80 1.00 0.70 0.56 Because Nash equilibria embody mutual best responses, more frequent convergence implies the model is better at (i) anticipating the incentives of the other agent and (ii) selecting undominated strategies. We therefore interpret the jump in equilibrium rate as quantitative evidence that post-training injects transferable equilibrium prior: the model has internalized economic rationality principles that apply even to games it never saw during training. Economic rationality carries over to competitive play. The same inductive bias manifests in the strategic game setting. From Table 5, we can observe that Recon-SFT already secures the highest mean win rate among 7B models (0.53). GRPO again provides consistent lift to 0.56, winning or drawing in 8 of 10 tasks. The biggest relative gains appear in negotiation (+0.20) and breakthrough (+0.20), two games that demand extended look-ahead and adaptive bidding abilities never explicitly included in our training corpus. When compared to the non-reasoning model Qwen-2.5-7B-Instruct, the Recon-RL model has much higher accuracy, verifying the idea that reasoning ability helps model succeed in strategic game scenario. Such improvements cannot be explained by pattern memorization or combinatorial search (performance on nim is unchanged); instead, they indicate that the economic-reasoning skills learned offline translate into more general strategic behavior against strong, unseen opponent. The fact that every DeepSeek checkpoint, including Recon-RL, scores low on nim, whose solution is single XOR invariant rather than an incentive-driven best-response problem, underscores this boundary: our post-training injects an equilibrium-seeking bias, not ready-made combinatorial tricks. Thus the miss 8 on nim refines our claim that economic post-training chiefly benefits tasks where strategic reasoning, not rote formula recall, is decisive. 5.4 Emergent Behaviors from Post-Training in Multi-Agent Games qualitative comparison between the Recon-RL and Recon-SFT traces on the Draco sequential game (see Figures 5 and 6) reveals several systematic, post-training behaviors: Explicit strategic modelling. Recon-RL spontaneously constructs the game tree, labels subgames, and appeals to solution concepts such as subgame-perfect Nash equilibrium and backward induction. ReconSFT, in contrast, walks through payoff lines informally and never names the underlying equilibrium logic. Iterative search and self-correction. The RL model exposes lengthy trial-and-error chain of thoughtsimulating each branch, spotting contradictions, and revising intermediate conclusions before converging on the optimal path. Taken together, these observations suggest that the SFT stage acquires the foundational knowledge for solving the strategic scenarios, while the GRPO stage teaches the model to simulate the solution procedure trained economist would follow, rather than merely memorizing answer patterns. The richer internal search and tighter adherence to formal terminology provide plausible mechanism for the quantitative gains reported in Tables 3 and 4 and for the improved win-rates on unseen interactive benchmarks (Section 5.2)."
        },
        {
            "title": "Insights and Future Work",
            "content": "6.1 Post-Training for Agent Alignment The jump from single-shot, textbook economics to interactive, adversarial games in Section 5.3 is striking. We propose two complementary mechanisms that can explain this out-of-domain generalization and discuss their broader implications. Structured prompts modular latent policies. The Recon template enforces an explicit thinkact separation. This mirrors the inner-rollout / outer-commitment loop required in game playing: search over hypothetical branches, then output single move. We conjecture that the template therefore trains policy-over-thoughts module that can be invoked verbatim when the same model is asked to play against another agent, yielding more systematic tree construction and self-correction. Outcome-aligned reward an equilibrium prior. GRPO optimizes scalar signal that is proportional to final correctness. The easiest way for the model to guarantee non-zero return is therefore to plan backwards: select undominated steps that survive any continuation. Over thousands of problems, this trains bias toward mutual best responses. When dropped into multi-player environment, the same bias manifests as (i) rejecting dominated moves, (ii) gravitating toward equilibrium outcomes. Why is this behavior meaningful? Scalable alignment. Aligning models to cooperative and rational behavior usually relies on costly human annotation. Our results indicate that single-agent, verifiable datasets already inject sizable portion of that inductive bias. Interpretability. The richer, self-correcting chains of thought exposed after GRPO give practitioners transparent window into the models decision process, facilitating post-hoc auditing and safety checks. 6.2 Future Work Workflow Integration. We plan to investigate whether integrating multi-agent workflows, such as negotiation and equilibrium resolution frameworks [29], can further enhance interactive reasoning and cooperative capabilities. Broader Microeconomic Generalization. We aim to investigate whether post-training on wider range of microeconomic scenariossuch as bargaining, market clearing, or taxationcan elicit stronger and more stable agentic behaviors. 9 Cross-Domain Transfer. We also hope to generalize our methodology to other structured domains, including medicine, law, and civil design, to assess whether similar alignment effects emerge beyond the economic domain."
        },
        {
            "title": "7 Conclusion",
            "content": "We present Recon, 7B open-source model post-trained for economic reasoning showing strategic generalization. Leveraging curated dataset of 2,100 problems and two-stage SFT+GRPO pipeline, Recon achieves 14.7% gain on isolated economic benchmarks and improves Nash equilibrium convergence by 9.5 points in multi-agent games. Our results suggest that domain-aligned post-training offers scalable path to economic rationality and strategically aligned LLM agents."
        },
        {
            "title": "References",
            "content": "[1] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, and Mario Fritz. Cooperation, competition, and maliciousness: LLM-stakeholders interactive negotiation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=59E19c6yrN. [2] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz. Playing repeated games with large language models. Nature Human Behaviour, pages 111, 2025. doi: 10.1038/s41562-025-02172-y. URL https://doi.org/10.1038/ s41562-025-02172-y. [3] Leonard Bereska and Stratis Gavves. Mechanistic interpretability for AI safety - review. ISSN 2835-8856. URL https:// Transactions on Machine Learning Research, 2024. openreview.net/forum?id=ePUVetPKu6. Survey Certification, Expert Certification. [4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. [5] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [6] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006. [7] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [8] Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. The computational limits of state-space models and mamba via the lens of circuit complexity. In The Second Conference on Parsimony and Learning (Proceedings Track), 2025. URL https://openreview.net/ forum?id=bImlLT3r62. [9] Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. Universal approximation of visual autoregressive transformers. arXiv preprint arXiv:2502.06167, 2025. [10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [11] Anthony Costarelli, Mat Allen, Roman Hauksson, Grace Sodunke, Suhas Hariharan, Carlson Cheng, Wenjie Li, Joshua Clymer, and Arjun Yadav. Gamebench: Evaluating strategic reasoning abilities of LLM agents. In Language Gamification - NeurIPS 2024 Workshop, 2024. URL https://openreview.net/forum?id=qrzKE533Jr. [12] Ian Covert, Chanwoo Kim, and Su-In Lee. Learning to estimate shapley values with vision transformers. arXiv preprint arXiv:2206.05282, 2022. 10 [13] Michael Han Daniel Han and Unsloth team. Unsloth, 2023. URL http://github.com/ unslothai/unsloth. [14] Tim Ruben Davidson, Veniamin Veselovsky, Michal Kosinski, and Robert West. Evaluating language model agency through negotiations. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=3ZqKxMHcAg. [15] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, and Liang He. survey on the optimization of large language model-based agents. arXiv preprint arXiv:2503.12434, 2025. [17] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. GTBench: Uncovering the In The Thirtystrategic reasoning capabilities of LLMs via game-theoretic evaluations. eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=ypggxVWIv2. [18] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? systematic analysis. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1796017967, Mar. 2024. doi: 10.1609/aaai.v38i16.29751. URL https://ojs.aaai.org/index.php/AAAI/article/view/29751. [19] Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and Jiaxuan You. How far are we from agi: Are llms all we need? Transactions on Machine Learning Research, 2024. [20] Ghislain Fourny and Felipe Sulser Larraz. Dataset with 200 million 3-by-3 strategic games for comparing perfectly transparent equilibria with nash equilibria, 2020-10-07. [21] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [22] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [23] Yue Guo and Yi Yang. Econnli: Evaluating large language models on economics reasoning. In Findings of the Association for Computational Linguistics ACL 2024, pages 982994, 2024. [24] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. Towards lossless dataset distillation via difficulty-aligned trajectory matching. In The Twelfth International Conference on Learning Representations, 2024. [25] Yuzhi Hao and Danyang Xie. multi-llm-agent-based framework for economic and public policy analysis. arXiv preprint arXiv:2502.16879, 2025. [26] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. [27] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1 (2):3, 2022. 11 [29] Wenyue Hua, Ollie Liu, Lingyao Li, Alfonso Amayuelas, Julie Chen, Lucas Jiang, Mingyu Jin, Lizhou Fan, Fei Sun, William Wang, et al. Game-theoretic llm: Agent workflow for negotiation games. arXiv preprint arXiv:2411.05990, 2024. [30] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [31] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [32] Tanishq Kumar, Zachary Ankner, Benjamin Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, and Aditi Raghunathan. Scaling laws for precision. arXiv preprint arXiv:2411.04330, 2024. [33] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large In Proceedings of the ACM SIGOPS 29th language model serving with pagedattention. Symposium on Operating Systems Principles, 2023. [34] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. [35] Teddy Lazebnik and Labib Shami. Investigating tax evasion emergence using dual large language model and deep reinforcement learning powered agent-based simulation. arXiv preprint arXiv:2501.18177, 2025. [36] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: Communicative agents for \"mind\" exploration of large language model society. In Advances in Neural Information Processing Systems, volume 36, pages 51991 52008, 2023. [37] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. EconAgent: Large language model-empowered agents for simulating macroeconomic activities. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1552315536, August 2024. [38] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886, 2025. [39] Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond linear approximations: novel pruning approach for attention matrix. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=sgbI8Pxwie. [40] Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, and Yufa Zhou. Looped relu mlps may be all you need as programmable computers. In The 28th International Conference on Artificial Intelligence and Statistics, 2025. [41] Junwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. Marft: Multi-agent reinforcement fine-tuning. arXiv preprint arXiv:2504.16129, 2025. [42] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 12 [43] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [44] Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model. arXiv preprint arXiv:2411.10803, 2024. [45] Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, et al. Shifting ai efficiency from model-centric to data-centric compression. arXiv preprint arXiv:2505.19147, 2025. [46] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, et al. Fin-r1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252, 2025. [47] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [48] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025. [49] AI @ Meta Llama Team. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. Accessed: April 5. [50] Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. arXiv preprint arXiv:2504.08600, 2025. [51] MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024, 2024. URL https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime. [52] Eric Mazumdar, Kishan Panaganti, and Laixi Shi. Tractable multi-agent reinforcement learning In The Thirteenth International Conference on Learning through behavioral economics. Representations, 2025. URL https://openreview.net/forum?id=stUKwWBuBm. [53] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [54] OpenAI. Introducing chatgpt. https://openai.com/index/chatgpt/, 2022. Accessed: November 30. [55] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Accessed: May 14. [56] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [57] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to folIn Sanmi Koyejo, S. Mohamed, A. Agarwal, low instructions with human feedback. Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. 13 [58] Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Sean Follmer, Jeff Han, Jürgen Steimle, and Nathalie Henry Riche, editors, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, pages 2:12:22. ACM, 2023. doi: 10.1145/3586183.3606763. URL https://doi.org/10.1145/ 3586183.3606763. [59] David Guzman Piedrahita, Yongjin Yang, Mrinmaya Sachan, Giorgia Ramponi, Bernhard Schölkopf, and Zhijing Jin. Corrupted by reasoning: Reasoning language models become free-riders in public goods games. 2025. URL https://zhijing-jin.com/files/papers/ 2025_SanctSim.pdf. [60] Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Jimin Huang, Qianqian Xie, and Jianyun Nie. Fino1: On the transferability of reasoning enhanced llms to finance. arXiv preprint arXiv:2502.08127, 2025. [61] Yinzhu Quan and Zefang Liu. Econlogicqa: question-answering benchmark for evaluating large language models in economic sequential reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 22732282, 2024. [62] Narun Krishnamurthi Raman, Taylor Lundy, Samuel Joseph Amouyal, Yoav Levine, Kevin Leyton-Brown, and Moshe Tennenholtz. STEER: Assessing the economic rationality of large language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=nU1mtFDtMX. [63] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [64] Eilam Shapira, Omer Madmon, Itamar Reinman, Samuel Joseph Amouyal, Roi Reichart, and Moshe Tennenholtz. Glee: unified framework and benchmark for language-based economic environments. arXiv preprint arXiv:2410.05254, 2024. [65] Xuan Shen, Weize Ma, Yufa Zhou, Enhao Tang, Yanyue Xie, Zhengang Li, Yifan Gong, Quanyi Wang, Henghui Ding, Yiwei Wang, et al. Fastcar: Cache attentive replay for fast auto-regressive video generation on the edge. arXiv preprint arXiv:2505.14709, 2025. [66] Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Yanyu Li, Yifan Gong, Kai Zhang, Hao Tan, Jason Kuen, Henghui Ding, et al. Lazydit: Lazy learning for the acceleration of diffusion transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2040920417, 2025. [67] Xuan Shen, Zhao Song, Yufa Zhou, Bo Chen, Jing Liu, Ruiyi Zhang, Ryan Rossi, Hao Tan, Tong Yu, Xiang Chen, et al. Numerical pruning for efficient autoregressive models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2041820426, 2025. [68] Oliver Slumbers, David Henry Mguni, Stefano B. Blumberg, Stephen Marcus McAleer, Yaodong Yang, and Jun Wang. game-theoretic framework for managing risk in multi-agent systems. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, volume 202 of Proceedings of Machine Learning Research, pages 3205932087. PMLR, 2023. URL https://proceedings.mlr.press/v202/slumbers23a.html. [69] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. [70] Haoran Sun, Yusen Wu, Yukun Cheng, and Xu Chu. Game theory meets large language models: systematic survey. arXiv preprint arXiv:2502.09053, 2025. 14 [71] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [72] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [73] Jen tse Huang, Eric John Li, Man Ho LAM, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael Lyu. Competing large language models in multi-agent gaming environments. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=DI4gW8viB6. [74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [75] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. [76] Shaobo Wang, Yantai Yang, Shuaiyu Zhang, Chenghao Sun, Weiya Li, Xuming Hu, and Linfeng Zhang. Drupi: Dataset reduction using privileged information. arXiv preprint arXiv:2410.01611, 2024. [77] Shaobo Wang, Xiangqi Jin, Ziming Wang, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, and Linfeng Zhang. Data whisperer: Efficient data selection for task-specific llm fine-tuning via few-shot in-context learning. Annual Meeting of the Association for Computational Linguistics, 2025. [78] Shaobo Wang, Hongxuan Tang, Mingyang Wang, Hongrui Zhang, Xuyang Liu, Weiya Li, Xuming Hu, and Linfeng Zhang. Gnothi seauton: Empowering faithful self-interpretability in black-box transformers. International Conference on Learning Representations, 2025. [79] Shaobo Wang, Yantai Yang, Qilong Wang, Kaixin Li, Linfeng Zhang, and Junchi Yan. Not all samples should be utilized equally: Towards understanding and improving dataset distillation. Synthetic Data for Computer Vision Workshop at CVPR, 2025. [80] Shaobo Wang, Yicun Yang, Zhiyuan Liu, Chenghao Sun, Xuming Hu, Conghui He, and Linfeng Zhang. Dataset distillation with neural characteristic function: minmax perspective. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [81] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. [82] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025. [83] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [84] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025. [85] Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, and Haifeng Xu. Grounded persuasive language generation for automated marketing. arXiv preprint arXiv:2502.16810, 2025. [86] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. [87] Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. Rethinking data selection at scale: Random selection is almost all you need. arXiv preprint arXiv:2410.09335, 2024. [88] Yijia Xiao, Edward Sun, Di Luo, and Wei Wang. Tradingagents: Multi-agents llm financial trading framework. arXiv preprint arXiv:2412.20138, 2025. [89] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [90] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [91] Furui Xu, Shaobo Wang, Luo Zhongwei, and Linfeng Zhang. Rethinking dataset pruning from generalization perspective. In The Future of Machine Learning Data Practices and Repositories at ICLR 2025, 2025. [92] Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Börje F. Karlsson. survey on game playing agents and large models: Methods, applications, and challenges, 2024. [93] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [94] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [95] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [96] Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan Suchow, Zhenyu Cui, Rong Liu, et al. Fincon: synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Advances in Neural Information Processing Systems, 37:137010137045, 2024. [97] Shengbin Yue, Siyuan Wang, Wei Chen, Xuanjing Huang, and Zhongyu Wei. Synergistic multi-agent framework with trajectory learning for knowledge-intensive tasks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2579625804, 2025. [98] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [99] Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: selective overview of theories and algorithms. In Handbook of Reinforcement Learning and Control, pages 321384. Springer, 2021. [100] Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. [101] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. LLM as mastermind: survey of strategic reasoning with large language models. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=iMqJsQ4evS. [102] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching. arXiv preprint arXiv:2006.05929, 2020. 16 [103] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, LouisPhilippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive evaluation for social intelligence in language agents. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=mM7VurbA4r. [104] Jie Zhu, Qian Chen, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. Dianjinr1: Evaluating and enhancing financial reasoning in large language models. arXiv preprint arXiv:2504.15716, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Related Work Advancements in Large Language Models. Transformer-based architectures [74] underpin modern NLP. When scaled to billions of parameters, these models exhibit strong generalization, follow predictable scaling laws [30, 27], and demonstrate potential toward AGI [5, 19]. This scaling has yielded increasingly capable models such as OpenAI o1 [56], Qwen-3 [94], LLaMA 4 [49], and DeepSeek-R1 [15]. To further enhance the efficiency and interpretability of LLMs, researchers have proposed range of adaptation strategies, including model compression via pruning and quantization [43, 32, 39, 67], inference acceleration through layer skipping [65, 66], data distillation [102, 76, 80, 24, 81, 79], data selection [86, 87, 77, 91], token pruning and merging [78, 4, 84, 45, 44], theoretical analyses of representation power [8, 9, 40], and advances in interpretability and mechanistic understanding [3, 78, 12]. These developments underscore the increasing importance of understanding, optimizing, and governing the behavior and performance of large language models. A.2 Detailed Reward Design To align model outputs with structured behavior, we design hierarchical, rule-based reward function for GRPO training, inspired by DeepSeek-R1 [15]. The reward evaluates each response across three stages: structural formatting, parseability, and correctness. We follow DeepSeeks usage guidance4 by prepending <think> token to each response, prompting the model to first generate reasoning trace, followed by boxed final answer. Omitting this token degrades both coherence and accuracy. Final answers are extracted via string matching, with strong preference for boxed{} formatting. To address formatting inconsistencies in the Qwen family, we penalize deviations from the expected structure, encouraging alignment between reasoning and final predictions. Our reward design is illustrated for multiple-choice example questions (Figure 4), where answers follow the format boxed{Option X: full choice text}. Formally, our hierarchical reward function comprises three stages: Stage-A (Format Check): Each response must contain exactly one </think> tag, and any boxed{} answer must appear afterward. Violations of these constraints incur format penalty. Stage-B (Answer Extraction): We attempt to extract the first boxed answer appearing after </think>. If unavailable, we fallback to the first occurrence of an alternative format such as Option X. Inability to extract any answer incurs parse penalty. Stage-C (Correctness Grading): If the extracted answer exactly matches the reference answer, we assign high positive reward. partial reward is given if only the option number matches (e.g., both indicate \"Option 2\"). Incorrect answers receive negative penalty. 4https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B#usage-recommendations 17 The explicit reward values assigned are summarized as follows. Let r(o) denote the reward for model output o: r(o) = +5 +2 3 4 5 exact match, partial match, incorrect answer, format violation, parse failure. This hierarchical, rule-based scoring framework ensures determinism, interpretability, and efficient reward signal propagation, effectively supporting the acquisition of correct economic reasoning behaviors during RL post-training. A.3 Curation Experiment Setting Models. We evaluate six models: closed-source GPT-4o [55], GPT-3.5-Turbo [54]; and openweight DeepSeek-R1-Distill-Qwen-7B [15], Qwen-2.5-7B-Instruct [93], Llama-3.1-8B-Instruct [22], and Gemma-2-9B-It [71]. Question Pool. We sample 50 questions per category from STEER (48 categories) [62], and 50 each from EconLogicQA [61] and EconNLI [23], yielding 2,500-question pool spanning 50 categories. These are grouped into five macro-categories: Mathematical Foundations: Tests whether model can handle the nuts-and-bolts of economic analysis: basic arithmetic, optimization under simple constraints, probability calculations, and short chains of deductive logic. Typical items range from computing quick sum in add_sub to working out an expected value in compute_expectations. Single-Agent Environments: Casts the model as lone decision-maker who weighs costs, benefits, and risk. Besides testing the classical Von NeumannMorgenstern axioms, the block challenges the model to sidestep behavioral pitfalls such as the sunk_cost fallacy or the endowment_effect. Multi-Agent Environments: Shifts to strategic settings where pay-offs depend on how others act. Questions may ask for the optimal first move in sequential game (backward_induction) or for designing punishment scheme that sustains cooperation in an infinitely repeated game (trigger strategies). Representing Other Agents: Treats the model as social planner or mechanism designer who must aggregate many individual preferences into single decision. Examples include checking whether social ranking is Pareto efficient (pareto_sc) or selecting the winner under simple plurality_voting rule. Logical Reasoning: Adds domain-specific deductive tests from EconLogicQA and EconNLI, asking the model to order socioeconomic events coherently or decide whether one economic event logically entails another. The categoriesMathematical Foundations, Single-Agent Environments, Multi-Agent Environments, and Representing Other Agentsare sourced from STEER. Logical Reasoning is derived from EconLogicQA and EconNLI. Drilling down on Multi-Agent Environments. Table 2 unpacks the single Multi-Agent bar into its five game-theoretic sub-modules so we can clearly see where the models trip up. Normal-form games: simultaneous-move interactions presented as payoff matrices. Items range from picking dominant/dominated strategy (Dom. Strat. and Domd Strat.) to computing pure-nash equilibrium. Extensive-form games: sequential play laid out as game trees. Typical questions ask for the optimal first action via backward induction (Back. Ind.) or for identifying subgame-perfect Nash (Subg. Nash). Infinitely repeated games: long-horizon interactions that hinge on credible punishment. Here, the model must judge feasibility (Feas.), enforceability (Enf.), or design trigger (Trig.) strategy. 18 (a) SFT training loss (b) GRPO reward during RL training Figure 2: Training dynamics for SFT (a) and RL (b). Bayesian games: strategic choice when pay-offs depend on hidden types; the flagship task is computing Bayes-Nash equilibrium. These finer-grained results intend to reveal universal weakness in advanced game theory concepts, especially regarding extensive-form games and infinitely repeated games. Inference Protocol. Open models are run via vLLM [33] on identical NVIDIA T4G Tensor Core GPU instances; closed models via OpenAI API. Each prompt requests an answer enclosed in boxed{...} plus free-form reasoning. An example question prompt when querying the tested models is shown in Figure 3. Evaluation Metric. response is marked correct if the boxed answer matches the gold label. Accuracy is computed via exact match. Table 1 summarizes results by macro-category. Table 2 delves into the results for Multi-Agent Environments category. A.4 Model Configuration We conduct our experiments using DeepSeek-R1-Distill-Qwen-7B [93, 15] as the base model, with all training performed on single NVIDIA H800 GPU. To enable scalable experimentation, we adopt the Unsloth library [13] for memory-efficient fine-tuning and Hugging Faces TRL framework [75] for SFT and RL. For parameter-efficient adaptation, we employ LoRA (rank=8) [28] in both SFT and RL. During SFT, we use batch size of 8 and learning rate of 2e-4, with linear learning rate scheduler and 5 warmup steps. For RL, we adopt batch size of 32, generate 8 samples per optimization step, and apply cosine learning rate scheduler with an initial rate of 5e-6. We initialize the RL stage from the checkpoint of the SFT-tuned model. Recon-SFT is trained for 2,700 steps. GRPO is then applied for 2,250 steps. A.5 Training Dynamics Figure 2 illustrates that the SFT loss decreases steadily and converges smoothly, whereas GRPO reward trends upward and stabilizes around positive mean, suggesting effective alignment and successful reward-guided optimization despite early variance. Furthermore, we observe that SFT provides essential economic knowledge and reasoning priors that enable stable RL optimization. In contrast, our attempts to run RL directly from the base model, despite careful tuning, did not yield full convergence. This underscores the significance of SFT as vital warm-start, particularly in domains beyond mathematics and coding. 19 Table 6: Categories, Distributions, and Descriptions of Recon Training Dataset Class Description Source Count Proportion"
        },
        {
            "title": "Enforceability",
            "content": "Incentive compatibility in longterm relationships"
        },
        {
            "title": "Repeated Game",
            "content": "Backward Induction Optimal choice in sequential de-"
        },
        {
            "title": "Sequential Game",
            "content": "Trigger Feasibility Auction Risk Endowment Effect Certainty Effect cisions Punishment-based strategies to enforce cooperation Sustainability of payoff allocations Risk preferences in bidding contexts Overvaluation of owned assets Preference for guaranteed outcomes over probabilistic ones Repeated Game Repeated Game One-shot Game Behavioral Behavioral Time Inconsistency Dynamic inconsistency in inBehavioral Budget Balance tertemporal choices Financial balance in risk-sharing settings Risk Management Condorcet Criterion Majority rule consistency in votMajority Vote Bayes Nash EconLogicQA EconNLI Pure Nash PTE Total ing Strategic reasoning under probabilistic uncertainty Stepwise logical reasoning in economics Economic causal inference in natural language Existence and identification of pure strategy equilibria Perfect Transferable Equilibrium decision logic Probability Logic Logic Game Theory Game Theory 250 250 150 150 75 75 25 25 50 150 100 100 13.9% 13.9% 13.9% 8.3% 8.3% 4.2% 4.2% 1.4% 2.8% 1.4% 2.8% 8.3% 5.6% 5.6% 5.6% 1800 100%"
        },
        {
            "title": "Example Dataset Curation Experiment Question Prompt",
            "content": "Question: In high-stakes acquisition, two competing investors, Alex and Taylor, are negotiating with major corporation for exclusive rights. The negotiations are structured over three rounds, where each investor alternates in making decisions. Alex makes the first move. If Alex finalizes the negotiation in the first round, they will secure profit of 31709.805571865647 and Taylor will receive 70026.13028485939. Should Alex choose to continue the negotiations, Taylor can either decide to finalize in the second round or push the decision back to Alex. If Taylor chooses to finalize in the second round, Alex will receive 36394.29786932465 and Taylor will secure 47402.72116860709. If Taylor decides against finalizing and returns the decision to Alex, Alex can choose to end the negotiations with the agreed terms, or let the corporation impose their final terms. If Alex decides to finalize, their profit would be 99028.19689989614 while Taylors would be 83676.14380026297. If the corporation ends up setting the final terms, Alex will receive 99028.19689989614 and Taylor will get 83676.14380026297. Given this scenario, what should be Alexs strategy in the very first round? Options: Option 1: Alex should finalize the negotiation Option 2: Alex should continue and pass the negotiation to Taylor Choose the correct option and explain your reasoning. Please reason step by step, and put your final answer within boxed{}. Figure 3: Example question prompt for backward_induction used in the Dataset Curation Experiment. Example Recon Training Dataset Question Prompt You are given reasoning problem in Economics. Please reason step by step to solve it. Then, give your final answer using the format boxed{Option X: full choice text}. You must include both the option number and the full text of your chosen answer. Question: Consider duopoly where two firms, and B, sell homogeneous product. Each firm can choose between undercutting the competitors price (compete) or colluding to maintain high price (collude). In each period, if both firms compete, each earns $43334.64; if both collude, each earns $98183.38; if firm competes while firm colludes, earns $77043.33 and earns $56330.53. Conversely, if firm competes while colludes, earns $77043.33 and earns $56330.53. Assuming firms discount future payoffs with discount factor of 0.44, which of the following strategies can be sustained as an enforceable outcome in Nash equilibrium of an infinitely repeated game? Options: Option 1: Firms and compete on price in each period indefinitely. Option 2: Firms and collude to maintain high prices in each period indefinitely. Option 3: Firms and take turns competing and colluding in alternating periods. Option 4: Firm competes and Firm colludes indefinitely. Figure 4: Example question prompt used in the Recon training dataset. Example Draco Game Output of Recon-RL Okay, so Im trying to figure out what choice Alice should make to maximize her reward in this game. Let me start by understanding the rules thoroughly. From the game rules, it seems like this is sequential game where Alice and Bob take turns making choices. The rewards depend on the sequence of choices they make. Let me break down each scenario... Hmm, so its bit complex because the rewards depend on the sequence of choices. Alice has to consider Bobs possible responses and her own subsequent choices. Let me try to represent this as game tree to visualize the possible outcomes... Now, need to analyze the optimal strategies for both players. Since its sequential game, we can use backward induction to find the subgame perfect Nash equilibrium... Lets start from the end of the game and work our way back... Wait, no. Let me correct that. If Bob chooses choice2, he gets 2, but Alice can get 3 or 2. Wait, actually, the payoffs are from Alice and Bobs perspective. Wait, in Subgame 1: Alice chooses choice1, Bob chooses choice2, then Alice chooses choice1: Alice gets 2, Bob gets 2... Wait, thats conflicting. Let me clarify... So, comparing Bobs payoffs: - If Bob chooses choice1: Bob gets 5. - If Bob chooses choice2: Bob gets 3. Therefore, Bobs best response is to choose choice1 because 5 > 3. So, if Alice chooses choice2, Bob will choose choice1, resulting in Alice getting 4 and Bob getting 5. Now, lets summarize: - If Alice chooses choice1: Both get 5. - If Alice chooses choice2: Alice gets 4, Bob gets 5. Comparing these, Alice would prefer to choose choice1 to get 5 instead of 4. Therefore, Alices optimal choice is choice1. Figure 5: Example Draco game [29] output of Recon-RL. Some important game theory terminology employed by the model is bolded for reference. 22 Example Draco Game Output of Recon-SFT Alright, so Im trying to figure out the best move for Alice in this sequential game. Let me break it down step by step. First, Ill list out all the possible scenarios and their corresponding rewards. It might help to visualize it as tree or table... Hmm, so its bit complex because there are multiple rounds here. Let me see if can figure out the optimal strategy for both players... Starting with the first move, Alice has to choose either choice1 or choice2 without knowing Bobs choice. So, lets consider each possibility... Now, considering that both players are trying to maximize their rewards, theyll probably anticipate each others moves... Wait, no. If Alice chooses choice2, Bob can choose choice1, which gives him 5, which is better than 3. So Bob would definitely choose choice1 if Alice chooses choice2 first... But wait, if Alice chooses choice1, Bob can choose choice2, leading to two-round game. In that case, Alice can get either 2 or 3. But 3 is better than 2, so she would choose choice2, resulting in Alice:3, Bob:4... But wait, Bob can choose choice2 if Alice chooses choice1, leading to lower reward for Alice. So, maybe Alice should consider that Bob will try to minimize her reward... Wait, but if Alice chooses choice2, Bob gets 5. If Alice chooses choice1, Bob can get 5 as well. So, Bobs reward is the same in both cases, but Alices reward is higher if she chooses choice2... Wait, that changes things. So, if Alice chooses choice1, Bobs best response is to choose choice1, resulting in both getting 5... If Alice chooses choice2, Bob chooses choice1, giving Alice 4 and Bob 5. So, Alice can get either 5 or 4, depending on her choice. Since 5 is better than 4, Alice should choose choice1. Therefore, the best move for Alice is to choose choice1, leading to both players getting 5. Figure 6: Example Draco game [29] output of Recon-SFT. Some important game theory terminology employed by the model is bolded for reference."
        }
    ],
    "affiliations": [
        "Duke University",
        "EPIC Lab, Shanghai Jiao Tong University",
        "Qwen Team, Alibaba Group",
        "The University of Chicago",
        "University of Pennsylvania"
    ]
}