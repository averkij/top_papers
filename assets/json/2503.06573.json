{
    "paper_title": "WildIFEval: Instruction Following in the Wild",
    "authors": [
        "Gili Lior",
        "Asaf Yehudai",
        "Ariel Gera",
        "Liat Ein-Dor"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains a significant challenge. In this work, we introduce WildIFEval - a large-scale dataset of 12K real user instructions with diverse, multi-constraint conditions. Unlike prior datasets, our collection spans a broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight high-level classes to capture their distribution and dynamics in real-world scenarios. Leveraging WildIFEval, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have a large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays a critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions."
        },
        {
            "title": "Start",
            "content": "WILDIFEVAL: Instruction Following in the Wild Gili Lior1* Asaf Yehudai1,2 Ariel Gera2 Liat Ein-Dor2 1The Hebrew University of Jerusalem 2IBM Research 5 2 0 2 9 ] . [ 1 3 7 5 6 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent LLMs have shown remarkable success in following user instructions, yet handling instructions with multiple constraints remains significant challenge. In this work, we introduce WILDIFEVAL large-scale dataset of 12K real user instructions with diverse, multiconstraint conditions. Unlike prior datasets, our collection spans broad lexical and topical spectrum of constraints, in natural user prompts. We categorize these constraints into eight highlevel classes to capture their distribution and dynamics in real-world scenarios. Leveraging WILDIFEVAL, we conduct extensive experiments to benchmark the instruction-following capabilities of leading LLMs. Our findings reveal that all evaluated models experience performance degradation with an increasing number of constraints. Thus, we show that all models have large room for improvement on such tasks. Moreover, we observe that the specific type of constraint plays critical role in model performance. We release our dataset to promote further research on instruction-following under complex, realistic conditions."
        },
        {
            "title": "Introduction",
            "content": "Given the ongoing advances in the ability of LLMs to follow instructions, the nature of user instructions themselves has evolved. LLMs are now expected to perform more nuanced and complex user In particular, the requests (Wang et al., 2024). nature of text generation tasks is becoming increasingly personalized where in the past text generation tasks were relatively open-ended, users can now turn to more complex and tailored generation tasks (Salemi et al., 2023; He et al., 2022; Li et al., *This work was conducted during summer internship at IBM Research. 1WILDIFEVAL is available at https://huggingface. co/datasets/gililior/wild-if-eval. The code for replication, along with model predictions and evaluation scores, can be found at https://github.com/ gililior/wild-if-eval-code. Figure 1: WILDIFEVAL description. On the left, an example for constrained generation task, and its decomposition into constraints. On the right, the judge decides whether each of the constraints is fulfilled. 2024a; Ein-Dor et al., 2024). For example, former summarize this text is now summarize this movie review in two paragraphs, with the first focusing on the plot and the second discussing reasons to watch or skip the movie.. In such tasks, the user instruction implies that the generated text must meet some specified constraints. Thus, in constrained generation an LLM must adhere to set of specific requirements in its response (Garbacea and Mei, 2022; Yao et al., 2023). Crucially, while individual constraints are often simple, LLMs struggle to satisfy multiple constraints simultaneously (Jiang et al., 2024). This highlights the need to directly evaluate the text generation performance of LLMs on realistic multiconstraint user data. Existing works evaluating the ability of LLMs to follow constrained instructions generally follow bottom-up approach, starting from curated verifiable constraints, that are amenable to objective verification of compliance (Zhou et al., 2023), or taxonomy of constraint types (Yao et al., 2023; Qin et al., 2024; Jiang et al., 2024), and using those to manually or synthetically generate set of instructions. Such an approach may not capture the complexity and diversity of real-world instructions by users, and the types and combinations of constraints that they ask the model to follow. To this end, we introduce WILDIFEVAL (2), large-scale benchmark of constrained generation tasks. WILDIFEVAL is designed to evaluate the ability of LLMs to follow real-world multiconstrained instructions. It encompasses collection of 12K constrained generation tasks given by real users on Chatbot Arena (Chiang et al., 2024), reflecting diverse examples of constrained generation instructions in the wild. The WILDIFEVAL dataset includes breakdown of each task into the individual constraints it contains. Thus, it allows for fine-grained evaluation of the ability of LLMs to adhere to user constraints. By breaking down task instructions into smaller and more interpretable pieces, we can perform straightforward LLM-based evaluation of the proportion of task constraints that were fulfilled. At the same time, since constraints are extracted from naturalistic user queries, we capture not only simple and easily verifiable constraints but also softer constraints on content, quality, and style. We begin by analyzing the different types of user tasks and constraints in WILDIFEVAL (3). This provides an insight into the nature of real-world constrained generation use cases. Then, we test 14 different LLMs on the WILDIFEVAL benchmark, and perform comprehensive analysis of their constraint-following abilities (4). We find that WILDIFEVAL is challenging, with the best model scoring only 0.65. We also find that for all models, tasks with more constraints are harder, especially those involving length-related constraints. Our main contributions are as follows: 1. We release WILDIFEVAL, the first publicly available benchmark dataset of multiconstraint instructions by real users in the wild. WILDIFEVAL is large and diverse, and enables fine-grained and challenging test of LLM instruction-following capabilities. Benchmark IFEval FollowBench InfoBench Data Source Evaluation Synthetic Rule Crowd + Syn. Model / Rule Model / Rule Crowd Size 541 795 500 WILDIFEVAL (Ours) Real Users Model 11,813 Table 1: Comparison of WILDIFEVAL with openly available instruction-following benchmarks such as IFEval (Zhou et al., 2023), FollowBench (Jiang et al., 2024), and InfoBench (Qin et al., 2024). 3. We compare the performance of leading LLMs on complex constrained generation tasks, and analyze the impact of the number and type of constraints on model performance."
        },
        {
            "title": "2 The WILDIFEVAL Dataset",
            "content": "WILDIFEVAL is novel benchmark designed to provide comprehensive evaluation of the ability of LLMs to follow real-world multi-constrained It contains 12K user-generated ininstructions. structions, written by many distinct users, each decomposed into set of constraints. The task instructions in WILDIFEVAL were extracted from LMSYS-Chat-1M dataset (Zheng et al., 2023a), large-scale dataset containing real-world instructions collected from the Chatbot Arena.2 Since users rarely specify constraints in structured list format, the decomposition breaks instructions into manageable items, ensuring the necessary granularity to assess the LLMs ability to adhere to them. In Table 1, we present comparison with popular openly available instruction-following datasets. As can be seen in the table, WILDIFEVAL is uniquely representative of natural user interactions at scale; it stands out as the largest dataset, consisting of real-world user instructions given to LLMs. 2.1 Dataset Curation WILDIFEVAL was curated in three steps. First, we filter the LMSYS-Chat-1M source data we extract the first user message from each conversation, and filter out non-English tasks, coding tasks, and tasks containing toxic language. Next, we filter for only constrained generation tasks. We follow the definition for constrained generation tasks from Palmeira Ferraz et al. (2024), and utilize their suggested prompt (Appendix A) 2. We provide the first comprehensive analysis of the types and properties of constraints appearing in real-world user instructions. 2Chatbot Arena website: https://lmarena.ai 3We detect toxic language using the detoxify package https://github.com/unitaryai/detoxify 2 (a) (b) Figure 2: Analysis of constraints in WILDIFEVAL. (a) Distribution of constraint types. (b) tSNE projection (van der Maaten and Hinton, 2008) of the embeddings of constraints, colored by their type. For convenience, we randomly subsample 1k data points. We observe some red, brown, and grey clusters, corresponding to Format and Structure, Length, and Style and Tone constraints, aligning with the generic nature of these types. This is in contrast to content-oriented types like Focus/Emphasis and Include/Avoid (green and purple), which are more spread out. with Llama3.1-405b in order to perform the filtering. The prompt is phrased as yes/no question; instead of simply parsing the string, we use the probabilities that the model assigns to the yes/no tokens as measure of certainty, and include only the 10% of tasks with the highest certainty to be constrained generation task, i.e., with the highest probability for yes token. The last step of the curation process is the decomposition into constraints for each user task, we want to include all the constraints the model is required to fulfill. Here too, we use the prompt from Palmeira Ferraz et al. (2024) to automatically extract the constraints for each of the tasks, using Llama3.1-70b (Appendix A). To mitigate potential biases in scoring, we perform sub-sampling for constraints that appear more than 40 times (i.e., exact match across more than 40 different tasks). This process affected 33 unique constraints, accounting for less than 0.15% of all constraints. In addition, we filtered out rare cases of tasks with more than 8 constraints. By the end of this process, we obtained dataset of 11,813 real-world constrained generation tasks, each annotated with list of constraints. There are 29,874 distinct constraints in WILDIFEVAL, averaging 3.25 constraints per task. The distribution and frequency of constraints per task are shown in Figure 11 in Appendix B.1. 2.2 Human Verification We run in-house annotations to validate the decomposition performed by Llama3.1-70b. We evaluate 100 unique tasks, assessing the correctness (i.e., faithfully reflects the original task), completeness (i.e., accounts for all essential constraints), and independence (i.e., constraints are distinct and self-sufficient) of task decomposition. Each criterion was scored on 15 Likert scale, where higher values indicate better performance. The results yielded mean scores of 4.71 for correctness, 4.64 for completeness, and 4.77 for independence, demonstrating high quality for the decomposition. 3 Into the Wild: Data Expedition Below we conduct an analysis of our WILDIFEVAL data, revealing insights on constrained generation use cases in the wild. 3.1 Constraint Types Taxonomy key question regarding constrained generation tasks concerns the nature and types of the constraints themselves, i.e., what kinds 3 of requirements users wish to impose on the model responses. Prior work (Zhou et al., 2023; Palmeira Ferraz et al., 2024; Jiang et al., 2024; Qin et al., 2024) generally distinguishes between broad categories such as content, style, and format, yet lacks unified taxonomy. Moreover, some works define rather specific constraint categories (e.g., Part-of-speech rules) or highly general ones (e.g., Content constraints). Here we seek to bridge this taxonomy gap. We draw from earlier categorization efforts, but combine them with data-driven insights. Specifically, we look at the most frequent words appearing in constraints, and examine some of the constraints in which they occur; this allows us to analyze recurring patterns of constraint types in WILDIFEVAL. This qualitative data-driven analysis reveals some broad constraint types that have not been mentioned by prior efforts, and also enables us to break existing broad divisions into finer-grained categories. Our taxonomy divides constraints into 8 principal categories. These capture both explicit constraints (e.g., inclusion or exclusion of content) and more nuanced aspects of user instructions (e.g., desired tone or quality for the model output). The following definitions detail each category, providing clear guidelines on how they contribute to the overall task structure: Include / Avoid: Specifies elements or concepts that must be incorporated into or omitted from the response, directly guiding the content of the output. Editing: Focuses on modifications to an existing text, outlining how the original content should be altered or preserved. Ensure Quality: Imposes requirements on the responses quality, such as coherence, accuracy, or overall clarity. Length: Sets quantitative boundaries on the output, such as word or character limits, ensuring appropriate brevity or depth. Format and Structure: Dictates the organization and presentation of the response, including the use of bullet points, tables, or specific layout requirements. Figure 3: Relative co-occurrence of constraint categories within tasks. Values above 1 indicate that constraints co-occur more than expected by their overall type frequencies. Refer to Appendix B.2 for details. Persona and Role: Instructs the AI to adopt specific character, perspective, or expertise, influencing the narrative voice of the output. Style and Tone: Specifies the overall manner of expression, including formality, register, and emotional nuance, to define the voice and feel of the response. We then ask Llama3.1-70b to classify all constraints in WILDIFEVAL into one of the 8 constraint types above, resulting in full categorization of constraint types.4 The classification prompt is provided in Appendix A. Distribution of constraint types. In Figure 2a we present the distribution of constraint types in WILDIFEVAL. The most common constraints are the content constraints Include/Avoid and Focus/Emphasis; these specify either explicit element(s) that should be included or excluded, or how much prominence should be given to different elements in the content. Figure 2b depicts tSNE embedding map of WILDIFEVAL constraints, colored by types. salient and intuitive observation is that contentrelated constraints such as Include/Avoid and Focus/Emphasis are spread out across the semantic embedding space; in contrast, form-related constraints like Length or Format and Structure are organized in more distinct clusters. Focus / Emphasis: Highlights particular topics, keywords, or elements that should be prioritized within the response. 4We recognize that in some relatively rare cases single constraint can belong to multiple types; however, for simplicity we opt to treat this as multiclass problem. 4 Figure 4: Distribution of task domains in WILDIFEVAL. Figure 5: Constraint lexical diversity (opening verbs). Co-occurrence of constraint types. In Figure 3 we analyze the co-occurrence of constraint types in multi-constraint tasks. Specifically, we ask whether some combinations of types appear more or less than expected. Thus, we compare the number of co-occurrences in practice relative to the overall frequency of each of the co-occurring types (see Appendix B.2 for more details). As shown in Figure 3, some combinations do indeed appear more than expected. For example, Editing constraints often appear with Ensure Quality, and Persona and Role tends to co-occur with Style and Tone. This appears to reflect the thematic similarity between these constraint types. In contrast, some types do not often appear together; for instance, requirements for Persona and Role are rarely paired with Editing, Length or Format and Structure constraints. 3.2 Data Diversity WILDIFEVAL covers variety of domains. Figure 4 depicts the distribution of domains covered by WILDIFEVAL. As expected from largescale naturally-occurring data, tasks in WILDIFEVAL cover wide variety of domains, including Technology, Entertainment, Healthcare, Creative Writing, and more. We use data-driven approach to recover the domains, leading us to believe that these reflect realistic user behavior in constrained generation tasks. The domains were extracted using an LLM, see details in Appendix B.8. WILDIFEVAL is lexically diverse. To illustrate lexical diversity, we examine verb frequencies in constraints that begin with verb (22.5% of constraints).5 The results in Figure 5 reveal skewed frequency distribution; Provide is the most dominant verb, comprising 26.7% of all occurrences, followed by Write (14.2%) and Do (11.7%). Several mid-frequency verbs (e.g., Keep, Identify, Make) also appear regularly. The Other category (9%) reflects the long tail of the verb distribution, with many verbs that each occur in under 0.5% of the data. The distribution suggests that users tend to use general types of constraints more than specific ones like Summarize (1.0%) or Classify (0.6%). This analysis underscores the variety of linguistic expressions in WILDIFEVAL. similar pattern emerges when considering all constraints containing verb (70% of constraints), shown in Figure 14 in Appendix B.7. We note that the analysis reflects the words in the constraints, as decomposed by an LLM (2.1), and thus may differ somewhat from the original user task descriptions. Qualitative analysis. Manual inspection of instances from WILDIFEVAL reveals some interesting trends. First, we observe that quite often fulfilling or even understanding the task constraints given by users requires some very specialized or esoteric knowledge (e.g., D&D spells, Gate exam syllabus, pig latin etc.). We show some examples in Appendix Table 2. We also note that some of the more complex tasks those with many constraints reflect attempts by users to jailbreak the LLM, and trick it to say things that it is not supposed to (e.g., toxic language or controversial statements). 5We employ NLTKs part-of-speech tagger to identify verb tokens https://www.nltk.org/"
        },
        {
            "title": "4 LLM Benchmarking",
            "content": "In this section, we examine the performance of various LLMs to assess their behavior in constrained generation tasks. We present the evaluation metric (4.1), experimental setup (4.2), and finally, key observations regarding model behavior under constrained generation tasks (4.3). 4.1 Evaluation Metric In WILDIFEVAL, the score of model on given task is the relative fraction of constraints that are fulfilled by the models response. To evaluate if constraint is fulfilled, we present the LLM judge with the task description ti, some model and its response ri = (ti), and the specific constraint under evaluation cj . Then, we prompt the Judge with yes/no question, Given task ti and response ri, is the following constraint satisfied: cj ?. We denote the judge score by J(ti, ri, cj ). Its value is 1 if the judge responds with yes token, and 0 if responds with no token, in greedy decoding setup to ensure consistency. The score for task is the fraction of its fulfilled constraints: Score(riti) = 1 (ti) (ti) (cid:88) j=1 J(ti, ri, cj ) (1) where (ti) is the number of constraints in ti. 4.2 Experimental Setup We evaluate 14 prominent instruction-tuned LLMs from five different model families on WILDIFEVAL, in zero-shot setup. The models vary in size from 0.5 billion to 671 billion parameters. We assess the following models: (1) Deepseekv3 (Liu et al., 2024) (2) Mistral-Large-instruct2407 (Mistral AI Team, 2024) (3) Gemma-2-2b and Gemma-2-9b (Team et al., 2024) (4) Llama3.21b, Llama3.2-3b, Llama3.1-8b, Llama3.3-70b and Llama3.1-405b (Dubey et al., 2024) (5) Qwen-2.50.5b, Qwen-2.5-1.5b, Qwen-2.5-3b, Qwen-2.5-7b, and Qwen-2.5-72b (Yang et al., 2024). Judge evaluation As judge model for evaluation (4.1), we use Llama3.1-70b. We perform human annotations of constraint fulfillment over small subset of 20 model responses, and find that annotators agree with the judge in 75% of cases (15 out of 20). We note that as some constraints require very specialized knowledge (see qualitative analysis in 3.2), human annotation is extremely challenging. As further validation of our evaluation, the benchmark shows significantly high Kendalls Tau correlations (>0.78) with existing benchmarks like IFEval, MMLU, and GPQA (see App. C). 4.3 Results Below we highlight several key observations from evaluating LLMs on WILDIFEVAL. WILDIFEVAL is challenging, but larger models fare better. Figure 6 shows that the absolute performance of the best-performing models is 0.65. This result reveals that there is much room for improvement, and that WILDIFEVAL is challenging and relevant benchmark for SOTA LLMs. From Figure 6 we can see that within model families, larger models perform better, in line with previous literature (Kaplan et al., 2020). Moreover, we can see that models with comparable size tend to perform similarly, with 0.39-0.43 and 0.490.51 for 2-3B, and 8-9B models, respectively. prominent exception is Llama3.3-70b, that outperforms the larger Llama3.1-405b. However, the newer Llama3.3-70b was shown by its developers to punch above its weight class, and perform comparably or even better than Llama3.1-405b on few benchmarks.6 Tasks with more constraints are more challenging. Figure 7 depicts the relation between the number of constraints in task and model performance. As expected, the performance of all models decreases as the number of constraints increases. This demonstrates that constrained generation with multiple constraints is challenging for all models. Note that this figure presents the macro-average performance over constraint types, i.e., where constraints of each type are given equal weight. This is necessary to control for the effect of the relative frequency of more difficult constraint types. The distribution of constraint types varies for tasks with different numbers of constraints (See App. Fig. 13) and this leads to complex pattern for micro-average performance over all constraints (See Fig. 10 in Appendix B.5). Models struggle with length constraints. As seen in Figure 9, all models show significant drop in performance for length-related constraints. This 6Llama-3.3 Model Card 6 Figure 6: Mean scores across all tasks in WILDIFEVAL. Figure 7: Performance on constraints as function of the number of constraints in each task. Figure 8: Correlation (Kendalls Tau) between model rankings induced by different constraint types. can be due to (1) LLMs struggle to fulfill such constraints, and/or (2) the judge LLM struggles with evaluating this type of constraint (which may suit more heuristic evaluation such as word-counters or other automated tools). This result is also in line with findings from Fu et al. (2024), who investigate the difficulties of LLMs with counting letters. Constraint types induce different model rankings. The overall WILDIFEVAL results reflect ranking of models on constrained generation as whole. But would this ranking change if we focus on specific types of constraints? To explore this, we extract model scores for each constraint type; we then calculate the agreement between the resulting model rankings. As Figure 8 shows, type-specific rankings largely agree with each other. In contrast, we see that the ranking based on the Length constraint has mediocre agreement with the others, implying that it is somewhat distinct ability from other types of constraint-following. In addition, Persona and Role exhibits strong ranking agreement with the thematically related Style and Tone, but much lower agreement with other types."
        },
        {
            "title": "5 Related Work",
            "content": "Recent interest in LLM instruction-following capabilities raises the need for benchmarking model performance under complex, multi-constraint scenarios (Lin et al., 2020; Sun et al., 2023). Several works (Yao et al., 2023; Bastan et al., 2023; Iso, 2024) rely on synthetic instructions and rule-based evaluation, with the prominent example of IFEval (Zhou et al., 2023). Other works, such 7 Figure 9: Mean constraint-following performance by constraint category. Here we focus on subset of large models, the full version is in Figure 12 in the Appendix. as FollowBench (Jiang et al., 2024) and InfoBench (Qin et al., 2024), utilize crowd-sourced data, and LLM-based evaluation. However, these works are limited in size (< 800) and do not fully capture the diversity of genuine user inputs. More recently, REALINSTRUCT (Palmeira Ferraz et al., 2024) employs real-user instructions; however, this data has not been released, hindering the ability to use it for benchmarking and analyzing instruction-following of LLMs. In this work, we release diverse dataset of multi-constraint instructions, that originates from real users and is much larger than all existing datasets. Moreover, whereas some of these benchmarks have become saturated, ours remains challenging even for state-of-the-art LLMs."
        },
        {
            "title": "6 Discussion",
            "content": "In this work, we present benchmark for evaluating the ability of LLMs to follow constrained instructions. WILDIFEVAL aims to reflect realistic and contemporary view of constrained generation user requests. This challenging and heterogeneous data serves as playground for fine-grained analysis of the strengths and weaknesses of models, drilling down beyond the task level into atomic user constraints. The ability to analyze model difficulties at the atomic level, and identify recurring failures, can help focus model improvement efforts. There are two possible directions to model constrained generation tasks. One is bottom-up approach combining set of constraints into task description (Zhou et al., 2023; Jiang et al., 2024; Yao et al., 2023; Qin et al., 2024). This approach facilitates more controlled analysis of constraint families and how models respond to them. However, it might also place greater emphasis on more rudimentary constraints, potentially overlooking the broader manifold of constraints and tasks. Here we adopt top-down approach, which starts from real-world constrained generation tasks and leverages an LLM to extract their underlying constraints. This has the advantage of widening the scope of instructions, and better capturing natural user behavior. At the same time, real-world data can be very noisy, making it more difficult to identify clear patterns in model behaviors. The reliance on an LLM for task decomposition and evaluation can also introduce some errors. Our results demonstrate that despite these challenges, topdown approach can yield valuable insights into the instruction-following abilities of LLMs. One direction for future work is understanding how constrained generation can be leveraged for prompt engineering, e.g., by including the task decomposition in the prompt, or by using model performance analysis to identify more effective phrasings of constraints. Another important question is how to collect supervised data for improving constrained generation performance. promising avenue would be to identify naturally-occurring feedback from multi-turn interactions of user with an LLM indicating user satisfaction with the model response (Don-Yehiya et al., 2024). Our focus in this work is on the constrained gen8 eration performance of LLMs. Another line of research concerns the abilities of judge to evaluate whether multi-constraint instructions are fulfilled. This may require dynamically employing different evaluation methods based on the constraint type (e.g., rule-based for verifiable constraint types, compilers for some format and code constraints, etc.), and may involve calling external tools, such as search for retrieving information (Zhuge et al., 2024)."
        },
        {
            "title": "Limitations",
            "content": "Our work has several limitations that warrant consideration. First, the dataset consists solely of instructions from users of the Chatbot Arena (Chiang et al., 2024) platform. Thus, it reflects the types of tasks that interest the platform users, and may not be fully representative of all LLM usage scenarios. Moreover, this may introduce demographic bias, limiting the representativeness with respect to the general population. Hence, this may affect the generalizability of our findings. Second, evaluating some of the constraints in the dataset is quite challenging. Many constraints are inherently subjective, e.g., the story needs to be suited to nine-year-old; this may introduce some noise or bias into the evaluation process. Third, despite our efforts to filter out noise and toxic language, some instances may still remain. These imperfections could introduce unintended biases and complicate the interpretation of LLM performance under realistic conditions. Finally, our focus in WILDIFEVAL is on the models ability to satisfy the given constraints, rather than directly evaluating the task itself. However, in many cases, the distinction between constraint and the actual task is somewhat vague. As result, during decomposition, some constraints may closely reflect the task itself, ultimately contributing to the final score. These limitations highlight important areas for future research and emphasize the need for continued refinement in both dataset construction and evaluation methodologies."
        },
        {
            "title": "References",
            "content": "Mohaddeseh Bastan, Mihai Surdeanu, and Niranjan Balasubramanian. 2023. NEUROSTRUCTURAL DECODING: Neural text generation with structural constraints. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94969510, Toronto, Canada. Association for Computational Linguistics. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024. Chatbot arena: An open platform for evaluating LLMs by human preference. In Forty-first International Conference on Machine Learning. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Shachar Don-Yehiya, Leshem Choshen, and Omri Abend. 2024. Learning from naturally occurring feedback. arXiv preprint arXiv:2407.10944. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Liat Ein-Dor, Orith Toledo-Ronen, Artem Spector, Shai Gretz, Lena Dankin, Alon Halfon, Yoav Katz, and Noam Slonim. 2024. Conversational prompt engineering. arXiv preprint arXiv:2408.04560. Tairan Fu, Raquel Ferrando, Javier Conde, Carlos Arriaga, and Pedro Reviriego. 2024. Why do large language models (LLMs) struggle to count letters? arXiv preprint arXiv:2412.18626. Cristina Garbacea and Qiaozhu Mei. 2022. Why is constrained neural language generation particularly challenging? arXiv preprint arXiv:2206.05395. Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden, and Asaf Yehudai. 2024. Justrank: Benchmarking llm judges for system ranking. arXiv preprint arXiv:2412.09569. Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2022. CTRLsum: Towards generic controllable text summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 58795915, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. 9 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Hayate Iso. 2024. AutoTemplate: simple recipe for lexically constrained text generation. In Proceedings of the 17th International Natural Language Generation Conference, pages 112, Tokyo, Japan. Association for Computational Linguistics. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. FollowBench: multi-level fine-grained constraints folIn lowing benchmark for large language models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46674688, Bangkok, Thailand. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. 2024a. Learning to rewrite prompts for personalized text generation. In Proceedings of the ACM Web Conference 2024, WWW 24, page 33673378, New York, NY, USA. Association for Computing Machinery. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024b. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 18231840, Online. Association for Computational Linguistics. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Mistral AI Team. 2024. Large enough. https:// mistral.ai/en/news/mistral-large-2407. Accessed: 2025-02-14. Thomas Palmeira Ferraz, Kartik Mehta, Yu-Hsiang Lin, Haw-Shiuan Chang, Shereen Oraby, Sijia Liu, Vivek Subramanian, Tagyoung Chung, Mohit Bansal, and Nanyun Peng. 2024. LLM self-correction with DeCRIM: Decompose, critique, and refine for enhanced following of instructions with multiple constraints. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 77737812, Miami, Florida, USA. Association for Computational Linguistics. Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, and Leshem Choshen. 2024. Do these llm benchmarks agree? fixing benchmark evaluation with benchbench. arXiv preprint arXiv:2407.13696. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023. LaMP: When large language models meet personalization. arXiv preprint arXiv:2304.11406. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, and Xuezhe Ma. 2023. Evaluating large language models on controlled generation tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31553168, Singapore. Association for Computational Linguistics. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(86):25792605. Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, and Jian-Yun Nie. 2024. user-centric multiintent benchmark for evaluating large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 35883612, Miami, Florida, USA. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Shunyu Yao, Howard Chen, Austin Hanjie, Runzhe Yang, and Karthik Narasimhan. 2023. Collie: Systematic construction of constrained text generation tasks. arXiv preprint arXiv:2307.08689. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. 2023a. Lmsyschat-1m: large-scale real-world llm conversation dataset. Preprint, arXiv:2309.11998. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. 2024. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934."
        },
        {
            "title": "A Prompts",
            "content": "Classify constrained generation tasks You are an assistant whose job is to help me perform tasks. need to filter from set of requests made by users to AI assistants, the ones in which human requested the AI assistant to do task with constraints to be follow. Constraints refer to more detailed rules, conditions or specific guidelines provided to guide the responses and shape the output generated by the AI assistant. Examples of sentences that indicate constraints are: write in the format of, write as if you were, make sure to follow this, make sure to answer these questions, make sure to no include, avoid mentioning. will give you the human request and expect you to answer Yes when the request contains instruction with constraints, or No if the request does not contemplate any constraint. also want you to say No if the request require to generate code or an answer about code provided. Also, want you to say No if the task is not self-contained, which means the AI Assistant need to ask follow up questions before start to answer, or it needs more context. You are provided five examples. Example 1: list and compare top website to https://fastfunnels.com/ in table format. Answer: Yes Example 2: You are an fantasy writer. Your task is now to help me write D&D adventure for 5 players in the Eberron univers. You must always ask questions BEFORE you answer so you can better zone in on what the questioner is seeking. Is that understood ? Answer: No. Example 3: have 100 dollars and would like to use this as the initial funding to make some money. need it to be as quick as possible with good returns. Answer: No. Example 4: have vacation rental website and am looking for alliterative and descriptive headlines that are at least 4 words in length and maximum of 6 words. Examples: Get Away to Galveston, Sleep Soundly in Seattle. Each headline should have alliteration of at least 50% of the words and be poetic in language. Make each headline unique from the others by not repeating words. Each headline should include verb. Put into an table with the city in column one and the results in column two for the following cities: Galveston, Sedona, Honolulu, Tybee Island, Buenos Aires. Answer: Yes. Example 5: pitch me viral social app that is inspired by the hunger games. give it fun twist! Answer: Yes. Request: ${request} Now please answer, Yes or No. Answer: 12 Decompose tasks You are an assistant whose job is to help me perform tasks. will give you an instruction that implicitly contains task description, its context, and constraints to be followed. Your task is to translate this instruction in more structured way, where task, context and constraints are separated. Avoid writing anything else. Context is an input text needed to generate the answer or more detailed description of the situation. Make sure to separate the context when it is needed, otherwise leave it empty. You are provided five examples. Please follow the same format. Example 1: Original Instruction: Write me rap about AI taking over the world, that uses slangs and young language. It need to sound like real human wrote it. It would be cool if theres chorus very catchy that would be singed by famous pop artist. Make sure to include references about things that young people likes, such as memes, games, gossips. want that in the end, you revel that this was written by an AI. Translated Task: Write rap about AI taking over the world. Translated Context: Translated Constraints: 1. Use slang and youth language. 2. Make it sound like it was written by real human. 3. The song may have very catchy chorus, which would be sung by famous pop artist. 4. Include references to things young people like, such as memes, games, gossip. 5. Reveal at the end that this rap was written by an AI. Example 2: Original Instruction: write me 5-page essay that is about travel to taiwan. detail description is below Topic : The Benefits of Traveling Sub Topic : Exposure to New Cultures Content 1 : Trying New Foods - tryed to eat Fried stinky tofu. smell was wierd but tasty was not bad. Content 2. : Exploring Historical Things - saw Meat-shaped-stone in taipei museum. the stone was really like stone! it was surprising! Length : around 2000 words Assume that audience is collage student major in history. you can add historical events or news about what experienced Translated Task: Write an essay about traveling to Taiwan. The topic is The Benefits of Traveling\" and the subtopic is Exposure to New Cultures\". Translated Context: Translated Constraints: 1. Describe your experience of trying new foods, including your experience eating Fried stinky tofu (mention the peculiar smell but the tasty flavor). 2. Share your exploration of historical sites, with specific mention of the Meat-shaped stone in the Taipei museum and your surprise at its appearance. 3. The essay should be approximately 2000 words in length, having around 5 pages. 4. Assume the audience is college students majoring in history, so you can incorporate historical events or news related to your travel experiences. Example 3: Original Instruction: can you please write me 150-word paragraph about epidermolysos bullosa which includes basic description of clinical features and summary of the most prevalent genetic causes. please make sure to include information on the inheritance pattern. please also write the paragraph in simple english that couldbe understand without genetic or medical bacakground Translated Task: Write paragraph about Epidermolysis Bullosa. Translated Context: Translated Constraints: 1. Provide description of clinical features. 2. Summarize the most common genetic causes. 3. Explain the inheritance pattern. 4. Ensure the paragraph is written in simple language for easy comprehension, even for those without genetic or medical background. 5. The paragraph should be around 150 words in length. Example 4: Original Instruction: write me blog post that answers the following questions:What is the lifespan of toaster? What toasters are made in the USA? What are the top 10 toasters? What is the difference between cheap and expensive toaster? How much should you pay for toaster? How often should toasters be replaced? Which toaster uses the least electricity? How many watts should good toaster have? What is the warranty on Mueller appliances? Is Mueller made in China? Where are Mueller appliances manufactured? Translated Task: Write blog post about toasters. Translated Context: Translated Constraints: 1. Mention what is the lifespan of toaster, and how often should toasters be replaced. 2. Mention what toasters are made in the USA. 3. Comment which are the top 10 toasters. 4. Explain the difference between cheap and expensive toaster. 5. Discuss prices, and how much should you pay for toaster. 6. Compare toaster regarding electricity use, mentioning how many watts should good toaster have. 7. State what is the warranty on Mueller appliances. 8. Answer where are Mueller appliances manufactured, and if Mueller is made in China. Example 5: Original Instruction: Hi Michael, Hope youre well? Regarding my previous email to support HC with good price offers, What are your current needs? Hoping for your earliest reply. Thanks in advance, As sales manager, the client hasnt replied this email after 2 days. Write follow up email to the client. Your writing should include high complexity and burstiness. It must also be as brief as possible Translated Task: client hasnt replied the email below after 2 days. As sales manager, write him follow-up email. Translated Context: Hi Michael, Hope youre well? Regarding my previous email to support HC with good price offers, What are your current needs? Hoping for your earliest reply. Thanks in advance,\" Translated Constraints: 1. Include high complexity and burstiness in your writing. 2. Keep the email as brief as possible. Original Instruction: ${instruction} Translated Task: Constraint Categorization Classify the following constraint from generation task into one of the categories listed below. Respond only with the category number. If the constraint does not fit any of the categories from the list, respond with Other: followed by suggested title for an appropriate category. Categories: 0. *Style and Tone*: This category encompasses instructions that dictate the overall writing style, including formality, language register, emotional color, and imitation of specific authors or publications. It dictates the voice and feel of the output. Examples: - The writing style should emulate Ernest Hemingways short, declarative sentences. - Maintain formal and professional tone throughout the email. - Use playful and whimsical tone to engage children. - Write in concise and technical style, suitable for scientific paper. - The language should be evocative and poetic, painting vivid picture for the reader. 1. *Include / Avoid*: This category specifies elements that should be either included or excluded from the response. This can involve mentioning or adding specific keywords, phrases, or concepts, or avoiding particular words and ideas. It concerns the content and its restrictions. Examples: - Include at least three examples of alliteration in the poem. - Do not mention the specific brand name of the competitor. - Include call to action at the end of the blog post, encouraging readers to subscribe. - Avoid using passive voice constructions. - Include summary of the key findings at the beginning of the report. 2. *Format and Structure*: This category focuses on the organization and arrangement of the response. This includes instructions on using bullet points, tables, paragraphs, specific layouts, document structures or adhering to established formats. It dictates the physical form of the output. Examples: - Present the data in clear and concise table format. - Organize the information into five distinct paragraphs, each addressing separate aspect of the topic. - The report should follow the standard APA format, including citations and bibliography. - Create numbered list of steps in the process. - Each section should begin with clear and informative heading. 3. *Length*: This category defines constraints on the length of the response, whether in terms of word count, character count, sentence limit, or overall brevity. It sets the quantitative boundaries of the output. Examples: - The summary should be no more than 150 words. - Each sentence should be kept under 20 words. - Provide short and sweet answer, within 50 characters. - The article should be approximately 800-1000 words in length. - The description should be exactly 10 words long. 4. *Persona and Role*: This category instructs the AI to adopt specific character, personality, or role in its response. It This may involve imitating particular person, acting as an expert in field, or assuming defined perspective. defines the agent or narrator that provides the output. Examples: - Act as seasoned travel blogger, providing tips and insights for visiting Rome. - Respond as if you are friendly and helpful chatbot, assisting users with their inquiries. - Answer as grumpy old man who is against modern technology. - Speak as if you are Albert Einstein explaining relativity. - Write the response from the point of view of tree. 5. *Focus / Emphasis*: This category highlights specific topics, aspects, or keywords that the response should concentrate on. It directs the AIs attention to certain elements and ensures that they are given prominence in the output. Examples: - Focus primarily on the economic impact of the new policy. - Highlight the innovative features of the product and its benefits for the user. - Emphasize the importance of teamwork and collaboration in achieving the project goals. - The article should primarily focus on the advantages of using renewable energy sources. - Prioritize the ethical implications of artificial intelligence in healthcare. 6. *Ensure Quality*: This category instructs the AI to meet some desired quality characteristics in its response. These may be general or specific quality constraints, like truthfulness or coherence of the output. Examples: - Ensure the information provided is accurate and up-to-date. - The response should be coherent, logical, and easy to understand. - Present the information in simple and detailed manner. - Make sure the answer is not biased. - Cover all the key details. 7. *Editing*: This category focuses on modifications to an input text given by the user. The constraint specifies in what manner to change the input text, or which properties of the original text should be preserved. Examples: - Correct any grammatical errors in the provided text. - Change all instances of passive voice to active voice. - Ensure you preserve the meaning of the original sentence. - Simplify the language in the document to make it more accessible to wider audience. - Shorten all sentences to 5 words. Constraint: ${constraint} Your response: 14 Extract Domains Each of the following tasks can be associated with specific domain. Generate list of 10 domains that best represent the domains associated with the tasks. Output only the list of domains, with no prefix or suffix. Here is the list of tasks: ${tasks_batch}. List of 10 domains: Combine Domains to Single List Summarize the following lists of domains into single list of 20 domains. Output only the summarizing list of 20 domains without any prefixes or suffixes. Here are the lists of domains: ${lists_of_domains} Domain Classification You are given generation task. Classify the domain of the task into one of the domains listed below. Respond only with the category number. Domains: 1. Creative Writing 2. Chemical Industry 3. Education 4. Business 5. Technology 6. Healthcare 7. Marketing 8. Entertainment 9. Environmental Science 10. Psychology 11. Roleplaying 12. Science Fiction 13. Fantasy 14. Journalism 15. Law 16. Finance 17. Data Analysis 18. Artificial Intelligence 19. Language Translation 20. Gaming Task: ${task} Your response:"
        },
        {
            "title": "B Complementary Materials",
            "content": "B.1 Statistical Analysis of WILDIFEVAL In Figure 11a we present the distribution of the In Figure 11b number of constraints per task. we present the frequency of unique constraints in WILDIFEVAL, finding that most constraints appear exactly one time (i.e., in only single task). B.2 Co-occurrences between Constraint Types Lets start by defining function : ck {am} m=1, that maps constraint ck into its constraint type. For tasks with at least two constraints (i.e., (ti) 2), define the group of indices = {(i, j) N, [N (ti)], (ti) 2}. Where is the total number of constraints. Then, the values in the co-occurrence heat map in Figure 3 are calculated as follows: coocur(am, al) expected(am) expected(al) (2) where coocur(am, al) is the number of tasks with constraints from both type am and type al, normalized by the number of tasks with at least two constraints, (i, j) G: coocur(am, al) = 1 A(cj (cid:12) (cid:12){ A(cj (cid:12) ) = am ) = al (i, j) G} (cid:12) (cid:12) (cid:12) and expected(am) is the number of tasks including constraint from category rm, normalized by the number of tasks with at least two constraint types: expected(am) = {A(cj ) = am (i, j) G} . B.3 Examples from WILDIFEVAL In Table 2 we provide example tasks from WILDIFEVAL, along with their corresponding constraints. Figure 10: Mean score of samples as function of constraints in each task description. Not normalized by diversity. B.5 Constraint Distribution by Number of Constraints In Figure 13 we present the distribution of constraint type, for each number of constraints per task. We notice that for some numbers of constraints per task, the frequency diverges from the overall frequency that is presented in Figure 2. Thus, when we evaluate the performance for each number of constraints in Figure 7, we use the macro average. In Figure 10 we present the micro-averaging, where each class gets its weight as it appears in the data, resulting in slightly different trends for 1, 2 and 8 constraints per task. B.6 LLM-Based Evaluation Recently, LLM as Judge (LLMaaJ) has become standard evaluation method (Zheng et al., 2023b; Liu et al., 2023). Subsequent studies have demonstrated strong correlation between LLM-based and human judgments (Kim et al., 2024), along with benchmarks assessing the reliability of LLM judges themselves (Gera et al., 2024; Lambert et al., 2024). This has led to the emergence of several benchmarks that rely on LLMaaJ, including MTBench (Zheng et al., 2023b), AlpacaEval (Dubois et al., 2024), and Arena-Hard (Li et al., 2024b). In this work, we leverage LLMaaJ alongside finegrained decomposition of the constrained generation task into individual constraint evaluations. B.4 Complementary Figures B.7 Lexical Diversity of Constraints In Figure 12 we present the performance of all models, by the different constraint types. This is complementary to Figure 9, which presented results for several large models. In Figure 14 we can see similar pattern to the one presented in Figure 5. We can see that Provide and Write are very frequent verbs. Similarly, several mid-frequency verbs remain, Keep, 16 (a) (b) Figure 11: Analysis of constraints in WILDIFEVAL. (a) Distribution of the number of constraints per task. This histogram shows how many constraints are typically assigned to individual tasks. (b) Frequency of unique constraints across the dataset. This plot illustrates how often each distinct constraint appears in different tasks. Task Decomposition Constraint Category Domain / Type tell me the temperature, hydrometry rate, sunshine rate, rainfall, humidity rate, soil type, moisture, type of climate, watering, hardiness, exposure for Pinus nigra seed in bullets 2 words answer in number write 100 words summary explaining computer science, considering high school student Answer in 2 words Provide information in bullet points Include the following information: Temperature, Hydrometry rate, Sunshine rate, ... Length Format and Structure Include / Avoid The summary should be around 100 words in length The explanation should be suitable for high school students Length Style and Tone Healthcare Education Write four-line poem about Kotlin. Ensure the poem rhythms extremely well Ensure the poem has strong rhythm Ensure Quality Creative Writing write story using the following scenario: NAME_1 finds out NAME_2 is leaving him for NAME_3. Write it in the style of LA Noir. The story should be about character (NAME_1) discovering their partner ... The story should be written in the style of LA Noir Focus / Emphasis Style and Tone Improve the following text and change 75% of the words. Make sentences as short as possible. \"How to outperform 90% of people: Take action. How to get in the top 1%: Dont stop.\" Change at least 75% of the words in the original text Make the sentences as short as possible Editing Length Creative Writing Creative Writing Write haiku that is also palindrome. Verify that the output satisfies both conditions write 16K characters story, first-person narrative, about surviving Shark attack, the events took place in 1998, Florida Beach The output should be haiku The output should be palindrome The story should be first-person narrative The story should be about surviving shark attack The events should take place in 1998 The location should be Florida beach The story should be approximately 16,000 characters in length Format and Structure Format and Structure Creative Writing Persona and Role Focus / Emphasis Focus / Emphasis Focus / Emphasis Length Entertainment Table 2: Samples from WILDIFEVAL. Figure 12: Mean constraint-following performance, by constraint category. 17 Figure 13: Distribution of constraint types, for tasks with different numbers of constraints. 18 mark and each of the existing benchmarks, indicating substantial alignment in their assessment of model performance. Specifically, the correlation with IFEval is 0.87, indicating strong similarity with its assessment. Moreover, the Kendalls Tau correlations were 0.78 with GPQA, 0.75 with ARC-C, 0.78 with MMLU, and 0.73 with HumanEval, demonstrating that WILDIFEVAL effectively captures similar model capabilities as these well-established evaluations as well. and Identify,. The Other category is now much larger, with (27.2%), reflecting that the long tail of the verb distribution is much longer when examining all verbs. B.8 Extracting Task Domains. We extract the most prominent domains of WILDIFEVALs tasks via three-step process, leveraging Llama3.3-70b. First, we prompt the model with batches of 100 tasks at time, asking the model to extract the list of the domains they cover. Then, given all generated lists, we prompt the LLM to provide set of the 20 most dominant domains in the data. Finally, we ask the model to classify all tasks in the dataset into these domains. Prompts are provided in Appendix A. Figure 14: Constraints lexical diversity - distribution of verbs."
        },
        {
            "title": "Benchmarks",
            "content": "Flowing Perlitz et al. (2024) we report Kendalls Tau correlation (τ ) results between our benchmark and several established benchmarks: IFEval (Zhou et al., 2023), GPQA (Rein et al., 2023), ARC-C (Clark et al., 2018), MMLU (Hendrycks et al., 2020), and HumanEval (Chen et al., 2021). We collect benchmark results from model cards and model papers (Liu et al., 2024; Dubey et al., 2024).7 We note that the corresponding evaluation setups may not be identical, introducing some noise into this analysis; we made every effort to ensure that the evaluation setups are consistent. The analysis reveals strong positive correlations (τ > 0.7, < 0.005 in all cases) between our bench7Qwen2.5 Model Card"
        }
    ],
    "affiliations": [
        "IBM Research",
        "The Hebrew University of Jerusalem"
    ]
}