{
    "paper_title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "authors": [
        "Jiangshan Duo",
        "Hanyu Li",
        "Hailin Zhang",
        "Yudong Wang",
        "Sujian Li",
        "Liang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 8 6 4 8 0 . 1 0 6 2 : r JudgeRLVR: Judge First, Generate Second for Efficient Reasoning Jiangshan Duo Hanyu Li Sujian Li Hailin Zhang Liang Zhao Yudong Wang State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University CFCS, School of Computer Science, Peking University LLM-Core Xiaomi"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is prerequisite for efficient generation: by learning to distinguish valid solutions, model can internalize guidance signal that prunes the search space. We propose JudgeRLVR, two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves better qualityefficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has become widely adopted method for improving large language models on complex reasoning tasks (Shao et al., 2025; Zhu et al., 2025). By providing sparse yet automatically checkable supervision signals (e.g., whether the final answer is correct), RLVR enables models to explore strategies beyond supervised demonstrations and has achieved strong results on reasoning benchmarks like math and code. However, RLVR does not merely optimize correctness; it also implicitly shapes models thinking pattern. When trained primarily with final-answer reward, many LLMs drift toward generative search style: enumerating tentative branches, revising intermediate steps, and performing explicit self-correction (Team et al., 2024). Figure 1 shows that the reasoning models output can be divided into 2 parts: chain-of-thought (CoT) trajectories and solution responses. CoT trajectories usually include all the details of the models lengthy thought process, whereas solution responses tend to present clear logical process along with the final answer. The resulting CoT trajectories Contribution during internship at Xiaomi LLM-Core Team. Co-corresponding authors. 1 Figure 1 Qualitative comparison of reasoning patterns between Vanilla RLVR and JudgeRLVR. While outcome-based RL guarantees final answer correctness, it often induces verbose and unstructured exploration (left). JudgeRLVR implicitly regularizes the reasoning process, leading to direct and coherent solution path (right). are often long and exhibit frequent backtracking (e.g., heavy use of but, however, lets try again) (Cai and et al., 2025). Long outputs are expensive, but deeper issue is quality: such trajectories are low in information density. Existing methods such as Kimi K1.5 (Moonshot AI Team, 2025) and DAPO (Yu et al., 2025) introduce length penalties to reduce tokens and stabilize training, but often come with accuracy drops, making it difficult to achieve an ideal qualityefficiency trade-off. This raises more general question: can we explore training paradigm that explicitly targets higher-quality thinking patterns, rather than hoping the model eventually stumbles upon them through repeated trial-and-error? Cognitive science suggests that expert reasoning is not defined by externalizing exhaustive search, but by early discrimination and pruning: unpromising paths are filtered before they are expanded (Chi et al., 1981). This aligns with common intuition: experts quickly spot low-value reasoning paths and focus on high-yield hypotheses, while novices resort to trial-and-error and frequent backtracking. This intuition motivates the idea that, to achieve efficient reasoning, one should first develop the ability to judge what constitutes direct and valid reasoning, and then learn how to reason for specific problems. Based on the idea above, we propose JudgeRLVR, two-stage judge-then-generate paradigm. 2 Figure 2 Pipeline for the two-stage training of JudgeRLVR As shown in Figure 2, in judging stage, we train the model as judge: given candidate solution responses generated by diverse models, it learns to classify them as correct or incorrect. In generating stage, we initialize the model from the judging stage and apply Vanilla RLVR finetuning on the same model. Under this training paradigm, the model learns to implicitly prune low-quality branches prior to generating extensive textual search, thereby fostering more direct and reliable reasoning patterns. It is worth noting that, we do not impose any explicit length or quality penalties on the CoT trajectories; thus, any reduction in verbosity or refinement in reasoning style can be attributed to the discriminative priors transferred from the judging stage. With our training paradigm, model can achieve an optimal balance between performance and efficiency. Compared to Vanilla RLVR using the same training data, JudgeRLVR achieves qualityefficiency trade-off on Qwen3-30B-A3B (Team, 2025): on in-domain math, it achieves about +3.7 points average accuracy improvement while reducing average generation length by about 42%; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement. From extensive experiments, we also analyze the specific impact of this training paradigm on thinking patterns. We observe shift in the linguistic style of the generated text, with notably fewer explicit backtracking cues such as but during reasoning. This suggests that JudgeRLVR moves from an externalized trial-and-error process to an internal decision-making process, thereby producing higher-quality reasoning patterns. In summary, our main contributions are: (i) we propose JudgeRLVR, new training paradigm for inducing higher-quality thinking patterns via judge-then-generate; (ii) JudgeRLVR demonstrates better qualityefficiency trade-off than Vanilla RLVR on both in-domain and out-of-domain tasks; (iii) we provide interpretable linguistic evidence which is consistent with reduced explicit backtracking."
        },
        {
            "title": "2.1 RLVR and Verifiable-Reward Training for Reasoning",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) is leading approach for improving LLM reasoning using automatic, task-specific rewards, often outperforming supervised fine-tuning on math and logic tasks (Guo et al., 2025a; Lambert et al., 2024; Xu et al., 2025). However, RLVR 3 has notable limitations: it can yield gains even under spurious rewards (Shao et al., 2025), and it mainly reweights existing high-reward reasoning paths rather than inducing new reasoning behaviors (Yue et al., 2025). This is consistent with concerns that RLVR may implicitly encourage suboptimal thinking by optimizing final correctness instead of process quality (DataLearner, 2025). Most variants emphasize reward design or optimization, leaving the qualityefficiency trade-off underexplored."
        },
        {
            "title": "2.2 Mathematical Reasoning in Large Language Models",
            "content": "Mathematical reasoning is core benchmark for LLMs, where recent progress relies heavily on RLVR and targeted training (Shen et al., 2025; Team et al., 2024). Data diversification via paraphrasing can improve generalization (Shen et al., 2025), while extended RLVR training improves accuracy but tends to produce verbose reasoning traces (Team et al., 2024). Synthetic data scaling further boosts capability yet does not eliminate redundancy (Zhou et al., 2025), and scaling studies still report low information density in reasoning outputs (Sun et al., 2024). Overall, improving both correctness and efficiency remains challenging."
        },
        {
            "title": "2.3 Learning to Judge: Evaluators, Verifiers, and Discriminative Supervision",
            "content": "Training evaluators and using discriminative supervision can steer LLM outputs by separating good from bad generations (Guo et al., 2025b; Tang et al., 2025). Discriminative fine-tuning can suppress negative outputs without preference data (Guo et al., 2025b), and stronger evaluators improve reasoning-quality assessment via coherence-focused criteria (Tang et al., 2025). Verifiers can detect reasoning errors with uncertainty signals (Han et al., 2024), and systematic studies highlight the need for task-specific validation metrics (Shimabucoro et al., 2024). Yet existing work rarely adopts two-stage pipeline that first trains judgment and then uses it to guide generation, which we address in JudgeRLVR."
        },
        {
            "title": "3 Method",
            "content": "JudgeRLVR divides the training of single reasoning policy into two sequential stages with different roles but intended capability transfer: judge stage that trains discriminative error awareness, followed by generating stage that optimizes solution generation. Our core hypothesis is that discriminative competence is prerequisite for efficient generation. Once the model internalizes what constitutes valid CoT trajectory, it can down-weight erroneous branches early in generation without any explicit length penalty. Figure 2 illustrates the complete pipeline of our two-stage training."
        },
        {
            "title": "3.1 Notation and Task Definition",
            "content": "The dataset consists of problems in specific domain: ğ‘¥ with gold final answers ğ‘(ğ‘¥). The solution response is denoted ğ‘§ = (ğ‘œ1, . . . , ğ‘œğ‘‡ ), token sequence containing clear logical process and ending with final answer string. The final answer ğ‘(ğ‘§) is extracted from the solution responses via deterministic parser. We do not train the model to judge directly on full outputs that include complete CoT trajectories because such outputs are often extremely long and contain large amount of distracting information that can interfere with the judgment. Instead, we expect the model to judge directly based on the clean solution response, so that it can identify errors with greater precision. 4 We define binary correctness label â„“ {0, 1}: â„“(ğ‘¥, ğ‘§) = I(cid:0)ğ‘(ğ‘§) = ğ‘(ğ‘¥)(cid:1) . (1) The policy (language model) is denoted ğœ‹ğœƒ."
        },
        {
            "title": "3.2 Stage 1: Judging Stage",
            "content": "In judging stage, the model is trained as judge. Given problem ğ‘¥ and candidate solution response ğ‘§, the policy produces critique/commentary ğ‘ which contains the CoT trajectory, and outputs discrete verdict token ğ‘£ {0, 1} (0 for incorrect and 1 for correct): (ğ‘, ğ‘£) ğœ‹ğœƒ( ğ‘¥, ğ‘§). (2) Data construction. We construct discriminative dataset ğ·judge consisting of triplets (ğ‘¥, ğ‘§, â„“): Response generation (rollout). For each problem ğ‘¥, we sample set of candidate solution responses {ğ‘§1, . . . , ğ‘§ğ‘›} from several models, and obtain â„“(ğ‘¥, ğ‘§ğ‘–) by comparing ğ‘(ğ‘§ğ‘–) with ğ‘(ğ‘¥). Hard negative mining. We prioritize moderate-difficulty problems whose empirical pass rates under rollouts is neither 0 nor 1, yielding more informative nearly-correct errors. Class balancing. For each ğ‘¥, we subsample to balance positives (â„“ = 1) and negatives (â„“ = 0), avoiding majority-class bias. Reward. The reward is whether the final verdict ğ‘£ğ‘– matches the true label â„“(ğ‘¥, ğ‘§ğ‘–): ğ‘Ÿğ‘– = I(ğ‘£ğ‘– = â„“(ğ‘¥, ğ‘§ğ‘–)) (3) The training objective encourages ğ‘£ğ‘– to match the true label â„“(ğ‘¥, ğ‘§ğ‘–). Meanwhile, the commentary ğ‘, as an explanatory prefix, is optimized by the same policy-gradient signal."
        },
        {
            "title": "3.3 Stage 2: Generating Stage",
            "content": "In generating stage, the model is trained under Vanilla RLVR setting and outputs solutions of the problems. The policy is initialized from the judging stage weights. Given problem ğ‘¥, the policy generates CoT trajectory and the solution response ğ‘§ (including the final parsed answer ğ‘(ğ‘§)): The reward remains the sparse binary correctness signal based only on the final answer: ğ‘§ ğœ‹ğœƒ( ğ‘¥). ğ‘Ÿ = â„“(ğ‘¥, ğ‘§) = I(cid:0)ğ‘(ğ‘§) = ğ‘(ğ‘¥)(cid:1) . (4) (5)"
        },
        {
            "title": "3.4 Hypothesized Mechanism and Verification Overview",
            "content": "We hypothesize that JudgeRLVR improves reasoning-pattern quality through the two stages in two corresponding ways: (i) style transfer: in judging stage, learning to judge induces global shift in the models reasoning language style; (ii) reduced backtracking: in generating stage, training activates the internalized efficient pattern, reducing explicit self-correction and backtracking in text. We design targeted experiments to test both points; definitions and results are provided in Section 4."
        },
        {
            "title": "4.1 Model Configuration and Training Data",
            "content": "We use Qwen3-30B-A3B (Team, 2025), mixture-of-experts (MoE) architecture. To ensure basic level of reasoning capability, we first conduct supervised fine-tuning (SFT) on curated open-source CoT datasets, obtaining Qwen3-30B-A3B-SFT with improved reasoning and instruction-following abilities. For RLVR, we collect and filter open-source resources to build dataset of 113k math problems with gold answers. To construct specialized training data for the judging stage, we adopt strict sampling strategy: Rollout generation. For each problem, we use MiMo-7B RL (Xiaomi, 2025) and the target model Qwen3-30B-A3B-SFT to each generate 8 reasoning traces and answers, yielding 16 distinct reasoning paths. Balanced sampling. We filter outputs so that each problem retains an equal number of correct and incorrect solutions, avoiding bias from class imbalance. Our training algorithm is DAPO (Yu et al., 2025), GRPO-family policy gradient method. We implement dynamic sampling to filter out samples with pass rates 0 or 1 during rollout. The batch size is set to 256 and the rollout size is ğ‘› = 8. We use AdamW optimizer (Loshchilov and Hutter, 2018) for training. The learning rate is set to 3 106 with 10-step warmup. We use training temperature of 1.0 and testing temperature of 0.6, with top-ğ‘ set to 1.0 and max tokens set to 65536 for both training and testing."
        },
        {
            "title": "4.2 Evaluation Benchmarks",
            "content": "We evaluate JudgeRLVR on in-domain math reasoning: AIME24 (Zhang and Math-AI, 2024), AIME25 (Zhang and Math-AI, 2025), MATH500 (Hendrycks et al., 2021), HMMT_feb_2025 (BalunoviÄ‡ et al., 2025), BeyondAIME (ByteDance-Seed, 2025) and on diverse out-of-domain benchmarks: GPQA Diamond (Science Reasoning) (Rein et al., 2023), IFEval (Instruction Following) (Zhou et al., 2023), LiveCodeBenchv6 (Coding) (Jain et al., 2024), MMLU-Redux (General Knowledge) (Gema et al., 2025), ZebraLogic (Logical Reasoning) (Lin et al., 2025), covering wide range of general capabilities. The detailed setups of these benchmarks are in appendix C."
        },
        {
            "title": "4.3 Main Experiments",
            "content": "Our main experiments are designed to answer single overarching question: does the proposed judge-then-generate paradigm improve the qualityefficiency trade-off of RLVR beyond Vanilla RLVR? To make this comparison meaningful, we evaluate three training settings that isolate the effect of the staged paradigm from other factors. Base SFT: the static baseline model Qwen3-30B-A3B-SFT. Base SFT serves as reference point for the models reasoning ability before RL and its default generation style, allowing us to measure how much of the downstream gains come specifically from RLVR. Vanilla RLVR: Vanilla generating RLVR optimizing only for final-answer correctness. Vanilla RLVR serves as strong baseline to demonstrate the advantages of our method. The total training is 250 steps. JudgeRLVR: our judge-then-generate method. The judging stage is 145 steps and the generating 6 stage is 105 steps. Vanilla RLVR and JudgeRLVR use the same training and evaluation hyperparameters and the same total training steps. We report both accuracy and generation length because our goal is not only to increase correctness, but also to reduce unnecessary computation and improve information density in the produced reasoning. The generating and rollout prompt, and the judging prompt are in appendix A."
        },
        {
            "title": "4.4 Ablations",
            "content": "We conduct targeted ablations to validate that JudgeRLVRs gains come from the divided stages itself rather than from simply adding more training signals. Judge Only: only judging stage training. Judge Only setting evaluates whether judge training alone can already improve generation quality, or whether generating stage is necessary to translate judging competence into improved generation performance. This ablation directly tests our core assumption that judging is prerequisite but not substitute for generating optimization. Mixed Strategy: judging stage is replaced by parallel training on mixed Judge/Generative data (ratio 1:1), with each data type using its corresponding reward; generating stage is then run as usual. Mixed Strategy setting probes whether the ordering and separation of roles matters: by interleaving judge-style and generation-style updates, we test whether simultaneously optimizing two behaviors interferes with learning clean internal decision policy. In other words, this ablation examines whether the model benefits from first consolidating an error-aware judge prior before being asked to explore and exploit it during generation. Together, these ablations aim to distinguish (i) benefit from judging from (ii) benefit from judging first, then generating, clarifying which component is responsible for improved efficiency and generalization."
        },
        {
            "title": "4.5 Mechanism Verification",
            "content": "To test style transfer and reduced backtracking, we design: Perplexity (PPL). PPL is the exponential of language models average negative log-likelihood over sequence; it measures how surprised or confused the model is, and lower values indicate the model is more familiar with the sequence content. Changes in perplexity can reflect the extent to which model is familiar with particular linguistic style. Using the Base SFT model as fixed evaluator, we compute changes in the PPL of outputs produced under different training methods; these changes then reflect stylistic shifts during training. During judging stage training of JudgeRLVR, we sample model outputs at each step (10 samples per step) for Vanilla RLVR and JudgeRLVR, and compute the average PPL of the Base SFT model on these outputs. Transition-word statistics. Behaviors such as explicit backtracking and reflection in model outputs are typically manifested through the use of transition-words. Therefore, by counting the number and frequency of such discourse markers in outputs over the course of training, these statistics can indicate, to some extent, that backtracking is decreasing. During generating stage training of JudgeRLVR, we sample 1000 outputs per step and count occurrences and frequencies of contrast/backtracking markers: but, however, though, although, yet, still, nevertheless, nonetheless, instead, conversely, whereas, while, actually, wait. 7 Table 1 Main results comparing Base SFT, Vanilla RLVR, and JudgeRLVR (Sequential). We report accuracy (Acc) and average generation length in tokens (Len). For JudgeRLVR, Î” is absolute change in accuracy (pp:percentage point) vs Vanilla RLVR (higher is better); for Len, Î” is relative change vs Vanilla RLVR (shorter is better)."
        },
        {
            "title": "Benchmark",
            "content": "Acc Len Acc Len Acc Î” (pp) Len Î” (%)"
        },
        {
            "title": "Vanilla RLVR",
            "content": "JudgeRLVR (Seq) In-Domain (Math) AIME24 AIME25 MATH500 HMMT_feb_2025 BeyondAIME Overall Avg Out-of-Domain GPQA Diamond IFEval LiveCodeBenchv6 MMLU Redux ZebraLogic Overall Avg 85.7 20.8k 74.1 26.8k 97.4 6.7k 57.1 24.9k 55.5 33.8k 74.0 22.6k 63.0 12.0k 80.6 3.2k 57.4 20.6k 2.1k 86.8 84.6 7.7k 9.1k 74.5 86.3 77.0 98.0 60.8 58.3 76. 61.2 79.9 58.2 85.4 83.6 73.7 21.8k 27.2k 6.8k 26.9k 34.3k 23.4k 12.1k 4.1k 22.8k 2.1k 7.9k 9.8k 89.0 78.7 97.2 70.0 63.9 79.8 66.4 86.4 63.9 87.0 86.8 78.1 +2.7 12.9k +1.7 17.2k -0.8 2.0k +9.2 20.4k +5.6 21.3k +3.7 14.8k +5.2 11.2k 4.6k +6.5 +5.7 18.7k +1.6 1.5k 9.6k +3.2 +4.5 9.1k -41 -37 -71 -24 -38 -42 -7.5 +12 -18 -28 +22 -3.9 The above experiments are used to demonstrate shifts in style and backtracking. Combined with improvements in model performance, they can provide comprehensive indication that these shifts are moving in favorable direction."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 1 compares Base SFT, Vanilla RLVR that optimizes only final-answer correctness, and our proposed JudgeRLVR (sequential two-stage). The overall conclusion is that, compared with Vanilla RLVR, JudgeRLVR achieves higher or comparable accuracy while producing substantially shorter generations on most benchmarks, demonstrating better qualityefficiency trade-off. In-domain math: improving accuracy while markedly reducing redundant reasoning. On AIME24/25, HMMT_feb_2025, and BeyondAIME, JudgeRLVR consistently improves accuracy over Vanilla RLVR, while significantly reducing generation length. This matches the motivation of our divided stages: judging stage, via strict balanced sampling (equal numbers of correct and incorrect traces per problem), strengthens the models ability to recognize error patterns and unproductive reasoning; then in generating stage, generating optimization can more reliably follow higher-confidence, lower-branching solution paths, reducing trial-and-error backtracking and verbose exploration. The larger gains on HMMT and BeyondAIME further suggest that the advantage of judge first, then generate is more pronounced when problems require longer chains of reasoning and stronger strategy selection. 8 Table 2 Main results comparing JudgeRLVR (Sequential), Judge Only, and Mixed Strategy. We report accuracy (Acc) and average generation length in tokens (Len). For Judge Only and Mixed Strategy, Î” is absolute change in accuracy (pp:percentage point) vs JudgeRLVR (higher is better); for Len, Î” is relative change vs JudgeRLVR (shorter is better). Benchmark In-Domain (Math) AIME24 AIME25 MATH500 HMMT_feb_2025 BeyondAIME Overall Avg Out-of-Domain GPQA Diamond IFEval LiveCodeBenchv6 MMLU Redux ZebraLogic Overall Avg JudgeRLVR (Seq) Acc Len Acc Î” (pp) Len Î” (%) Acc Î” (pp) Len Î” (%) Judge Only Mixed Strategy 89.0 78.7 97.2 70.0 63.9 79.8 66.4 86.4 63.9 87.0 86.8 78.1 12.9k 17.2k 2.0k 20.4k 21.3k 14.8k 11.2k 4.6k 18.7k 1.5k 9.6k 9.1k 87.0 76.3 97.2 61.5 57.3 75. 66.9 85.9 61.4 87.4 87.3 77.8 22.4k 28.5k -2.0 -2.4 +0.0 -8.5 17.9k 35.8k -6.6 22.4k -3.9 +74 +66 7.4k +266 -12 +68 +92 +0.5 -0.5 -2.5 +0.4 +0.5 -0.3 7.9k 4.2k 21.3k 1.0k 8.0k 8.5k -30 -9.0 +14 -31 -17 -15 88.3 79.7 98.0 69.0 63.4 79.7 63.7 80.4 56.4 87.1 87.4 75.0 -0.7 12.8k -0.8 +1.0 18.2k +6.0 2.2k +7.9 +0.8 +27 -1.0 25.9k -0.5 22.9k +7.3 -0.1 16.4k +9.5 -2.7 16.1k -6.0 7.9k -7.5 26.7k +44 +72 +43 3.3k +119 -19 7.9k +52 -3.1 12.4k +0.1 +0.6 On MATH500, JudgeRLVR is slightly worse in accuracy than Vanilla RLVR, but reduces generation length drastically. This indicates that for relatively saturated datasets, there is limited headroom for further accuracy gains, while JudgeRLVR can still substantially reduce reasoning cost when accuracy is largely maintained. Out-of-domain: both generalization and efficiency gains, with task-dependent trade-offs. On GPQA Diamond, LiveCodeBenchv6, and MMLU-Redux, JudgeRLVR improves accuracy while also reducing length. This suggests that the error-sensitive / avoid-detours preference learned in judging stage is not restricted to math, but can transfer to science reasoning, coding, and broad knowledge tasks, reducing ineffective exploration and improving overall decision quality. In contrast, on IFEval and ZebraLogic we observe higher accuracy but longer outputs. This suggests that for tasks emphasizing format compliance, constraint satisfaction, or discrete rule verification, the model may need to generate more explicit checks to ensure correctness, trading off some length advantage for higher pass rates. This does not contradict our goal: JudgeRLVR is not designed to mechanically minimize length, but to reduce unproductive reasoning; when task intrinsically requires more explicit structure, moderate increase in length may be necessary cost."
        },
        {
            "title": "5.2 Ablation Results",
            "content": "Table 2 reports two ablationsJudge Only and Mixed Strategyto test: (i) whether judging stage alone is sufficient; and (ii) whether sequential staging is essential. Judge Only Judge Only: better judging does not reliably translate into better generation. shows clear degradations on multiple benchmarks, especially on in-domain math: compared with JudgeRLVR, accuracy drops on math benchmarks. More importantly, generation length increases substantially. This indicates that the discriminative capability learned in judging stage 9 Figure 3 Base SFT perplexity (PPL) evaluated on sampled outputs along training steps. Vanilla RLVR stays stylistically close to Base SFT (flat PPL), while JudgeRLVR (judging stage) exhibits increasing PPL, indicating style transfer. does not automatically induce more concise generation policy; it can even make the model more prone to writing out the checking process or reasoning more cautiously and verbosely, which lengthens outputs and hurts final accuracy. In other words, error-aware discrimination is necessary prerequisite, but not substitute for generating optimization; generating stage is still required to convert error sensitivity into better path selection and more information-dense expression during generation. We also observe that Judge Only can achieve comparable or slightly better accuracy on some outof-domain tasks while producing shorter outputs. This suggests that some judging preferences learned in judging stage may directly benefit certain tasks; however, this benefit is unstable and cannot replace the full two-stage pipeline for our core objective. Mixed Strategy: interleaving judge and generate updates weakens the benefit of sequential stages. Mixed Strategy can be close to JudgeRLVR on some tasks, but is overall more unstable: it slightly exceeds JudgeRLVR on AIME25 and MATH500, yet drops substantially on tasks such as IFEval and LiveCodeBenchv6, often with significantly longer outputs. This aligns with our hypothesis: when judge-style and generation-style behaviors are optimized in 1:1 interleaved manner, the model must satisfy two different objectives and reward structures in the same phase, which can lead to interference in policy-gradient updates and prevent the emergence of clean, stable internal decision process of judge first, then generate. The result is occasional local accuracy gains, but weaker overall efficiency and generalization."
        },
        {
            "title": "5.3 Mechanism Verification",
            "content": "Style transfer measured by PPL under Base SFT. Figure 3 reports the Base SFT models average perplexity on sampled outputs across training steps. For Vanilla RLVR, the average PPL remains largely unchanged, suggesting the output distribution stays stylistically close to Base 10 Figure 4 Counts(left) and Frequencies(right) of transition/backtracking markers (e.g., but, however, wait) in sampled outputs across JudgeRLVR generating stage training steps. SFT. In contrast, during JudgeRLVR judging stage, Base SFT PPL increases noticeably, indicating systematic style shift induced by judge training. Reduced explicit backtracking measured by transition words. Figure 4 show that during JudgeRLVR generating stage, both the absolute count and frequency of transition/backtracking words decrease substantially over training, consistent with reduced explicit backtracking and shift of verification/correction into internal decision-making."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we proposed JudgeRLVR, two-stage training paradigm that acts as precursor to efficient reasoning. By explicitly training the model to discriminate solution validity before generating, we achieved superior qualityefficiency trade-off. Our experiments on Qwen3-30BA3B show that JudgeRLVR improves in-domain math accuracy while reducing generation length by approximately 42%, with notable generalization to out-of-domain tasks. Many assume that judging an answer is the easy part, because verification seems to come with checklist that generation does not. Our study points to sharper truth: on identical math problems with the same model, judging yields greater improvement than solving alone, so is this correct? is problem in its own right. Training the judge makes the model internalize what good line of thought looks like, and later generation can cross its own limits with fewer false starts. In this sense, to generate is simply to judge so well that the correct answer is the only thing left to say."
        },
        {
            "title": "References",
            "content": "M. BalunoviÄ‡, J. Dekoninck, I. Petrov, N. JovanoviÄ‡, and M. Vechev. Matharena: Evaluating llms on uncontaminated math competitions, Feb. 2025. URL https://matharena.ai/. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. [https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME ](https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME), 2025. H. J. Cai and et al. How much backtracking is enough? exploring the interplay of sft and rl in enhancing llm reasoning. 2025. M. T. H. Chi, P. J. Feltovich, and R. Glaser. Categorization and representation of physics problems by experts and novices. Cognitive Science, 5(2):121152, 1981. A. DataLearner. Reinforcement learning with verifiable rewards (rlvr): Why the focus of llm training is shifting in 2025. DataLearner Official Blog, 2025. URL https://wap.datalear ner.com/blog/1051766245990867. A. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. Van Krieken, and P. Minervini. Are we done with mmlu? In Proceedings of NAACL 2025, 2025. J. Guo, J. Tang, and X. Li. Deepseek-r1: Scaling rlvr for mathematical reasoning. 2025a. Y. Guo, X. Wang, Y. Zhang, and P. Liu. Discriminative finetuning of generative large language models without reward models and preference data. 2025b. X. Han, Y. Wang, Z. Li, and C. Zhang. Uncertainty-based verifiers for llm reasoning outputs. 2024. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the MATH dataset. Advances in Neural Information Processing Systems (NeurIPS), 2021. N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. 2024. N. Lambert, N. A. Smith, and H. Hajishirzi. Scalable verifiable reward learning for large language models. 2024. B. Y. Lin, R. Le Bras, K. Richardson, A. Sabharwal, R. Poovendran, P. Clark, and Y. Choi. Zebralogic: On the scaling limits of llms for logical reasoning. 2025. I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, Sept. 2018. URL https://openreview.net/forum?id=Bk g6RiCqY7. Moonshot AI Team. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. 2023. Y. Shao, T. Liu, M. Chen, and S. Li. Spurious rewards: Rethinking training signals in rlvr. 2025. W. Shen, J. Li, H. Zhang, and Z. Wang. Llm reasoning engine: Specialized training for In Proceedings of the 4th International Workshop on enhanced mathematical reasoning. Knowledge-Augmented Methods for Natural Language Processing (KnowledgeNLP25), pages 118128. Association for Computational Linguistics, 2025. L. Shimabucoro, R. Chavhan, T. Hospedales, and H. Gouk. Evaluating the evaluators: Are validation methods for few-shot learning fit for purpose? Transactions on Machine Learning Research, pages 113, 2024. Z. Sun, Y. Liu, D. Chen, and W. Li. Skywork-math: Scaling mathematical reasoning with corpus fine-tuning. 2024. 12 X. Tang, J. Chen, B. Zhang, and Z. Liu. Unifiedreward-think: comprehensive evaluator for reasoning quality. 2025. D. Team, F. Liu, H. Wang, and B. Zhang. Deepseek-math: Enhancing mathematical reasoning via rlvr. In Proceedings of the 38th International Conference on Machine Learning (ICML24), pages 23452356. PMLR, 2024. Q. Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. L.-C.-T. Xiaomi. Mimo: Unlocking the reasoning potential of language model from pretraining to posttraining, 2025. URL https://arxiv.org/abs/2505.07608. Y. Xu, T. Chakraborty, S. Sharma, L. Nunes, E. KÄ±cÄ±man, S. Lu, and R. Chandra. Direct reasoning optimization: Llms can reward and refine their own reasoning for open-ended tasks. 2025. Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, W. Dai, T. Fan, G. Liu, L. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. X. Yue, Z. Chen, and G. Huang. Does reinforcement learning with verifiable rewards elicit novel reasoning abilities? 2025. Y. Zhang and T. Math-AI. American invitational mathematics examination (aime) 2024, 2024. Y. Zhang and T. Math-AI. American invitational mathematics examination (aime) 2025, 2025. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. 2023. M. Zhou, J. Li, X. Wang, and L. Zhang. Jiuzhang3.0: Efficient mathematical reasoning via synthetic data pre-training. 2025. X. Zhu, M. Xia, Z. Wei, W.-L. Chen, D. Chen, and Y. Meng. The surprising effectiveness of negative reinforcement in llm reasoning. 2025."
        },
        {
            "title": "A Prompt Templates of Judge and Generate",
            "content": "Judging Prompt Given the question: {question} And the proposed answer: {solution response} Analyze concisely whether the answer is correct. analysis, put only your final judgment in boxed{} as 1 for correct and 0 for incorrect."
        },
        {
            "title": "Generating and Rollout Prompt",
            "content": "{question} Lets think step by step and output the final answer within boxed{}."
        },
        {
            "title": "B Case Study",
            "content": "Full output of Figure 1 Problem Statement Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (ğ‘Ÿ, ğœƒ), where ğ‘Ÿ > 0 and 0 ğœƒ < 2ğœ‹. Vanilla RLVR <think> need to convert the point (0,3) from rectangular coordinates to polar coordinates. The rectangular coordinates are given as (x,y) = (0,3), and need to find the polar coordinates (r, ğœƒ), where is the distance from the origin, and ğœƒ is the angle from the positive x-axis. First, recall the formulas for conversion. The radius is given by = (ğ‘¥2 + ğ‘¦2). Plugging in the values, is 0 and is 3, so = (02 + 32) = (0 + 9) = Now, for the angle ğœƒ, its given by ğœƒ = tan1 ( ğ‘¦/ğ‘¥). But here, is 0, and is 3. So if plug in, tan1 (3/0), which is undefined because division by zero. That makes sense because when is 0, the point is on the y-axis. 9 = 3. So is 3. know that when point is on the positive y-axis, the angle ğœƒ should be ğœ‹/2 radians, or 90 degrees. Since the problem asks for ğœƒ in radians between 0 and 2ğœ‹, should use radians. So, ğœƒ = ğœ‹/2. Let me confirm. If is 3, and ğœƒ is ğœ‹/2, then the rectangular coordinates should be ğ‘¥ = ğ‘Ÿ cos ğœƒ = 3 cos(ğœ‹/2) = 3 0 = 0, and ğ‘¦ = ğ‘Ÿ sin ğœƒ = 3 sin(ğœ‹/2) = 3 1 = 3. Perfect, that matches. The problem says > 0, which is satisfied since r=3>0, and ğœƒ between 0 and 2ğœ‹, ğœ‹/2 is within that range. recall that sometimes people might think of ğœƒ as ğœ‹/2 or 3ğœ‹/2, but that would give negative y-value, which is not the case here. Since is positive, it has to be in the first or second quadrant, but since is 0, its specifically on the positive y-axis, so ğœƒ = ğœ‹/2. If ğœƒ were 3ğœ‹/2, that would be the negative y-axis, where is negative, but here is positive, so its not that. So, think (3, ğœ‹/2) is the correct polar coordinate. The answer should be in the form (r, ğœƒ), so (3, ğœ‹/2). should write it as pair. And the problem says to box the final answer. So, think thats it. Just to be thorough, is there any other representation? For example, could be negative? But the problem specifies > 0, so no. And ğœƒ is usually taken in [0, 2ğœ‹), so ğœ‹/2 is fine. Sometimes people use degrees, but here its specified to use radians, and the range is given, so radians are implied. The problem doesnt specify units, but in mathematics, especially with ğœ‹, its standard to use radians. So, Im confident. </think> First, need to convert the point (0, 3) from rectangular coordinates to polar coordinates (ğ‘Ÿ, ğœƒ) with ğ‘Ÿ > 0 and 0 ğœƒ < 2ğœ‹, use the conversion formulas. 14 The radius ğ‘Ÿ is given by: ğ‘¥2 + ğ‘¦2 = 02 + 32 = ğ‘Ÿ = 9 = 3. The angle ğœƒ is found using ğœƒ = tan1 ( ğ‘¦/ğ‘¥). However, since ğ‘¥ = 0 and ğ‘¦ = 3 > 0, the point lies on the positive y-axis. Thus, ğœƒ = ğœ‹/2. Verify the solution: ğ‘¥ = ğ‘Ÿ cos ğœƒ = 3 cos(ğœ‹/2) = 3 0 = 0, ğ‘¦ = ğ‘Ÿ sin ğœƒ = 3 sin(ğœ‹/2) = 3 1 = 3, which matches the original point. The conditions are satisfied: ğ‘Ÿ = 3 > 0 and ğœƒ = ğœ‹/2 is in [0, 2ğœ‹). Thus, the polar coordinates are (3, ğœ‹/2).] The final answer is: (cid:16)3, (cid:17) ğœ‹ 2 JudgeRLVR <think> We need to convert (0,3) to polar coordinates. = sqrt(xË†2 + yË†2) = sqrt(0+9)=3. Theta is angle from positive x-axis to point (0,3). Thats 90 degrees = pi/2 rad. Since x=0, y>0, theta = ğœ‹/2. So answer (3, ğœ‹/2). Ensure within 0 to 2ğœ‹. So final answer (3, ğœ‹ 2 ) . </think> First, need to calculate r. The point (0, 3) has and lies on the positive ğ‘¦-axis, so the angle from the positive ğ‘¥-axis is 02 + 32 = 3 ğ‘Ÿ = Thus the polar coordinates are (3, ğœ‹ The final answer is: 2 ). ğœƒ = ğœ‹ . (3, ğœ‹ 2 )"
        },
        {
            "title": "C Evaluate Details",
            "content": "Table 3 shows the metric setup of evaluation benchmarks. 15 Table 3 Evaluation Benchmarks Metric Acc avg@10 Acc avg@10 Acc Benchmark AIME24 AIME25 MATH500 HMMT_feb_2025 Acc avg@10 BeyondAIME GPQA Diamond IFEval LiveCodeBenchv6 Pass@1 MMLU-Redux ZebraLogic"
        },
        {
            "title": "Exact Match\nAcc",
            "content": "Acc avg@8 Exact Match avg@5 Prompt Strict"
        }
    ],
    "affiliations": [
        "CFCS, School of Computer Science, Peking University",
        "LLM-Core Xiaomi",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}