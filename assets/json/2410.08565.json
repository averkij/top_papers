{
    "paper_title": "Ocean-omni: To Understand the World with Omni-modality",
    "authors": [
        "Yadong Li",
        "Haoze Sun",
        "Mingan Lin",
        "Tianpeng Li",
        "Guosheng Dong",
        "Tao Zhang",
        "Bowen Ding",
        "Wei Song",
        "Zhenglin Cheng",
        "Yuqi Huo",
        "Song Chen",
        "Xu Li",
        "Da Pan",
        "Shusen Zhang",
        "Xin Wu",
        "Zheng Liang",
        "Jun Liu",
        "Tao Zhang",
        "Keer Lu",
        "Yaqi Zhao",
        "Yanjun Shen",
        "Fan Yang",
        "Kaicheng Yu",
        "Tao Lin",
        "Jianhua Xu",
        "Zenan Zhou",
        "Weipeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks a high-performing open-source counterpart. In this paper, we introduce Ocean-omni, the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim for this contribution to serve as a competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction."
        },
        {
            "title": "Start",
            "content": "4 2 0 5 6 5 8 0 . 0 1 4 2 : r OCEAN-OMNI: TO UNDERSTAND THE WORLD WITH OMNI-MODALITY Yadong Li1 Haoze Sun1 Mingan Lin1 Tianpeng Li1 Guosheng Dong1 Tao Zhang1 Bowen Ding2,3 Wei Song2,3 Zhenglin Cheng2,3 Yuqi Huo1 Song Chen1 Xu Li1 Da Pan1 Shusen Zhang1 Xin Wu1 Zheng Liang1 Jun Liu1 Tao Zhang1 Keer Lu1 Yaqi Zhao1 Yanjun Shen1 Fan Yang1 Kaicheng Yu2 Tao Lin2 1 Baichuan Inc. Jianhua Xu1 Zenan Zhou1 Weipeng Chen1 2 Westlake University 3 Zhejiang University {xujianhua, zhouzenan}@baichuan-inc.com (cid:135) https://github.com/westlake-baichuan-mllm/bc-omni Figure 1: Evaluation across image, video, and audio modalities. (Left) Ocean-omni covers more modalities than Qwen2 VL [79] and outperforms the current leading omni-modal model, VITA [24]. (Right) Average scores across benchmarks for all modalities. All the scores are normalized by xnorm = (x xmin + 10)/(xmax xmin + 10)."
        },
        {
            "title": "ABSTRACT",
            "content": "The salient multimodal capabilities and interactive experience of GPT-4o highlight its critical role in practical applications, yet it lacks high-performing open-source counterpart. In this paper, we introduce Ocean-omni , the first open-source 7B Multimodal Large Language Model (MLLM) adept at concurrently processing and analyzing modalities of image, video, audio, and text, while delivering an advanced multimodal interactive experience and strong performance. We propose an effective multimodal training schema starting with 7B model and proceeding through two stages of multimodal alignment and multitask fine-tuning across audio, image, video, and text modal. This approach equips the language model with the ability to handle visual and audio data effectively. Demonstrating strong performance across various omni-modal and multimodal benchmarks, we aim Equal Core Contributors. Corresponding author. for this contribution to serve as competitive baseline for the open-source community in advancing multimodal understanding and real-time interaction."
        },
        {
            "title": "Introduction",
            "content": "The burgeoning field of artificial intelligence has witnessed remarkable evolution, especially with the development of Large Language Models (LLMs) [1, 8, 105] and the subsequent emergence of Multimodal Large Language Models (MLLMs) [46, 63, 93], signifying paradigm shift in how machines understand and interact with the world. The introduction of MLLMs like GPT-4o [63], characterized by their exceptional multimodal capabilities and enriched interactive experiences, has not only spotlighted the indispensable role of these technologies in real-world applications but also set new benchmark for what is achievable in terms of human-computer interaction. Despite the remarkable progress of MLLMs, current open-source solutions exhibit notable deficiencies, particularly in multimodal capabilities and the quality of user interaction experiences [24]. These shortcomings significantly impede the broader adoption and effectiveness of such models in diverse applications, from natural language processing [18, 68] to computer vision [84, 73] and beyond. In response to these challenges, we introduce an omni-modal LLM Ocean-omni alongside multimodal training scheme designed to facilitate advanced multimodal processing and naturalistic user interactions. The architecture of Ocean-omni is depicted in Figure 2. The scheme of Ocean-omni is built upon three core components: Figure 2: Architecture of Ocean-omni . Ocean-omni is designed to process both pure text/audio inputs and combinations of video/image with text/audio. In terms of interactivity, the model initially predicts the start and end of audio inputs. During this period, incoming images and videos are encoded into features and fed into the MLLM in streaming fashion to calculate attention. The audio features are then input into the MLLM for inference following the end of the audio input, facilitating streaming input of audio and video. Omni-Modal Data Construction. We utilize substantial collection of high-quality, omni-modal data to train Ocean-omni with blend of open-source, synthetic, and internally annotated datasets. In the multimodal alignment pretraining phase, we curate wide-ranging assortment of training corpora that encompasses image captions, interleaved data, OCR data, and image-text data. For audio alignment, we collect both open-source and in-house datasets for Automatic Speech Recognition (ASR) and Audio Question Answering (AQA). In the realm of video alignment, we acquire video data from both open-source and in-house sources. During the multimodal supervised fine-tuning phase, we compile and synthesize an extensive dataset that covers over 200 tasks and comprises 600,000 instances across pure text, audio, image-text, video-text, and image-audio interaction data. Multimodal Alignment. During the pre-training phase for multimodal alignment, we meticulously align encoders and connectors across various modalities. Initially, we train the vision-language model using substantial dataset of image-text pairs. This foundational training enables us to harness the visual capabilities developed during the image-text training to further train the video projector. Concurrently, we train the audio-language model utilizing Automatic Speech Recognition (ASR) data. Building upon this robust foundation, we integrate high-quality image, audio, and video data to achieve comprehensive multimodal alignment. Multitask Fine-tuning. For the omni-modal fine-tuning stage, we utilize multi-task cross-modal interaction training corpus derived from combination of open-source, synthetic, and internally annotated data. We select data for the final supervised fine-tuning (SFT) phase based on criteria that whether factual knowledge is already learned by the pre-trained model [27]. During this phase, we implement packing technique to concatenate multiple samples, using the cuseq_len from flash-attention2 for effective sample isolation. With this, multiple samples can be packaged into large batch while ensuring each sample is correctly isolated during the computational process, preventing data confusion between different samples. This accelerates the training process and optimizes memory usage. The contributions of this paper are summarized below: We introduce Ocean-omni , an open-source, high-performance foundational omni-modal model capable of concurrently processing text, images, videos, and audio inputs. It also provides multilingual support for languages including English and Chinese. Our training framework features comprehensive pipeline that includes the construction of omni-modal training data, multimodal alignment pre-training, and multimodal supervised fine-tuning, with particular emphasis on enhancing omni-modal instruction-following capabilities. We explore early-stage research in natural multimodal human-computer interactions. Our approach initiates with the prediction of audio input boundaries, while simultaneously streaming and encoding incoming visual data into features. These features are then processed by multimodal large language model (MLLM) for dynamic attention computation. Upon completion of the audio input, the corresponding features are fed into the MLLM for inference, thus facilitating the support for handling audio and video inputs. This integrated approach allows for real-time processing and enhances the interactive capabilities of the system. We have made our Ocean-omni model, training code, and evaluation scripts publicly available, aiming at fostering progress within the research community. As pioneers in this field, we remain committed to furthering the development of multimodal foundational models and their interactions."
        },
        {
            "title": "2 Related works",
            "content": "Recent advancements in Large Language Models (LLMs) have reshaped the AI landscape, paving the way for the emergence of Multimodal Large Language Models (MLLMs). These advanced models expand AI capabilities beyond text, allowing understanding and generation of content across multiple modalities, including images, audio, and video, signaling significant leap in AI development. Open-source MLLMs have demonstrated increasingly powerful capabilities, with efforts from both academia and industry fueling the rapid development of models. LLMs such as LLaMA [80, 81], MAP-Neo [101], Baichuan [89], Qwen [5, 90], and Mixtral [36] are trained on extensive text data, exhibiting strong capacities in natural language comprehension and task resolution through text generation. Vision-Language Models (VLMs) [43, 109, 102] have shown promising potential in addressing vision-focused issues, with representative models including LLaVA [51], DeepSeek-VL [54], the Qwen-VL series [6, 79], InternVL families [12, 11], and MiniCPM [32]. Additionally, AudioLanguage Models (ALMs) [86, 17, 38] leverage audio-text pairs to perceive audio signals based on singular audio encoder. Notable instances of these models encompass Qwen-Audio [15, 14], SALMONN [77], SpeechGPT [100], etc. However, compared to proprietary models like GPT-4o [63], open-source models still exhibit substantial gaps in their capabilities for multimodal interactions, and there is considerable scarcity of open-source models that effectively facilitate comprehensive multimodal interactions [24]. To address these, we propose Ocean-omni , an open-source and capable MLLM which concurrently supports interactions across modalities including audio, image, video, and text."
        },
        {
            "title": "3 Training",
            "content": "3.1 High-Quality Multimodal Data For training an omni-modal model with strong ability, we build an extensive cross-modal dataset with high quality, including text, image-text, video-text, audio-text, and their interactions. 3 Image Data. Image data can be categorized into several types: Caption, Interleaved image-text, OCR data and Chart data [35]. From the perspective of sources, it is divided into Open-source data and Synthetic data. Regarding open-source data, we have collected major open-source datasets, including PIN-14M [83], MINT-1T [4], LAION5B [70], OBELIC [39], etc. for Stage training of Image-language branch (Detailed introduction in Section 3.2.1), and Cauldron [40], Monkey [47], ArxivQA [45], TGDoc [85], MM-Self-Instruct (Train split) [103], MMTab [106], etc. for Stage II/III training of Image-language branch. These publicly available open-source datasets are subjected to series of processing steps and careful sampling techniques within our data pipeline. As for synthetic data, the purpose is to obtain higher quality data to enhance the performance of our models. One part is derived from books and papers, which are parsed to generate Interleaved image-text, OCR data and Chart data. It is highly complete and specialized, making it high-quality and knowledge intensive data. Another part involves training dedicated models to produce image captions. These captions describe the content of the images in detail from different perspective, belonging to high-quality caption data. Video Data. Video dataset comprises diverse array of publicly available resources, encompassing multiple tasks such as video classification, action recognition, and temporal localization. The video-text sources can be categorized into two main types: question-answering (QA) data and caption data. For QA data, we incorporate: NExTVideo, introduced in LLaVA-NExT [104] and ActivityNet-QA (Train split) [95]. Our caption data sources include ShareGPT4Video [10], large-scale dataset that leverages GPT-4 to generate rich, contextual captions for videos, and WebVid [7]. To further enrich our dataset, we have employed GPT-4o to generate diverse captions for videos collected from YouTube. The sampling ratio for each dataset within our compilation is carefully determined based on the relative sizes of these datasets. This strategic approach ensures balanced representation of various video types, tasks, and domains in our final dataset. Audio Data. Considering the diversity of audio data, we extract audio from various media modalities, which includes different recording environments, languages, accents, and speakers. Guided by the principles in previous work [67], we posit that the variation in audio quality contributes to robust speech understanding capability. To facilitate more sophisticated classification and filtering procedure, we implemented data processing pipeline comprising speaker voice recording, dialect recognition, accent recognition, sound effect detection, and quality assessment. To enhance the quality of audio-text pairs derived from the dataset, we utilized an in-house ASR system along with several open-source models [67, 25, 75] to generate multiple transcript versions. These generated data are then refined through model ensemble strategy for effective text filtering and error correction. Text Data. In handling text corpus, we collected data from various domains such as web pages, books, academic papers, code, etc.. Following the data processing protocols proposed in previous works [19, 55], we implemented selection process to enhance the diversity and quality of the dataset. The diversity criterion ensures broad coverage of topics and linguistic styles in the training corpus, accommodating various applications. High-quality processing removes redundancy and noise from the text data, increasing knowledge density. Cross-Modal Interaction Data. To enhance the cross-modal interaction capabilities of our model, we synthesized collection of visual-audio-text cross-modal interaction data, including both image-audio-text and video-audio-text datasets. For the image-text data, we segmented the textual data into 1:3 ratio, converting the initial quarter of text into audio descriptions using text-to-speech (TTS) technology. Our audio encompasses 44 different timbres, ensuring diversity of vocal tones. This setup is complemented by task prompts such as Please listen to the following audio describing the content of the image. Your task is to supplement more information by integrating the image after listening, aiming to predict the remaining three-quarters of the textual description. For the video-text data, we directly extracted the audio from the videos to serve as the cross-modal audio component. 3.2 Multimodal Alignment Pre-training In this section, we will further illustrate the pre-training and alignment processes for the Image-Language, VideoLanguage, and Audio-Language branches. 3.2.1 Image-Language Branch We utilize Siglip-384px [97] as the visual encoder, which processes 384384 image input and generates 182 tokens through visual projector composed of two-layer MLP and 22 convolution layer serving as the pooling layer. To scale the input to arbitrary resolutions while preserving the intricate details of high-resolution images, we adopt 4 Figure 3: Data illustration of Ocean-omni . We build an extensive cross-modal dataset, including text, image-text, video-text, audio-text, and their interactions. Our collection also features integrated image-audio-text and video-audiotext data. Figure 4: Training Pipeline of Ocean-omni . During the pretraining phase, we initially train vision-language model using extensive image-text data, followed by training an audio-language model with ASR data. Subsequently, we integrate high-quality images, audio, and video data for comprehensive multimodal alignment. In the fine-tuning phase, we synthesize subset of cross-modal interaction data to blend with existing high-quality datasets. From this enriched dataset, we select subset of data that the model is already capable of handling and proceed with multimodal multitask fine-tuning. This process aims to enhance the models adherence to omni-modal instructions. AnyRes [50], which splits the image into grids and concatenates the features of down-sampled image to provide global context. The training of our image-language branch is divided into three stages. 5 Stage I: In the first stage, we train the visual projector to establish the initial alignment between image representations and text through image captioning task. During this phase, we freeze the LLM and the visual encoder, only training the visual projector with learning rate of 1e 3. Stage II: In the second stage, we freeze the LLM and train both the projector and visual encoder with smaller learning rate of 1e 5. In addition to general VQA tasks, we specifically synthesized 130k high-quality QA data for OCR and charts to enhance the models abstract visual understanding. We also introduced interleaved data and image caption data in this stage, which help maintain and promote better alignment between image and text representations, mitigating shifts caused by changes in the image feature space after unfreezing the visual encoder. Stage III: Based on the second stage, we unfreeze the LLM and continue updating the parameters of all model components with learning rate of 1e 5 to further enhance visual-language performance. In addition to VQA and image-caption pairs, we also introduce interleaved data and pure text data in this stage to better maintain the original capabilities of the LLM. 3.2.2 Video-Language Branch Based on the visual capabilities acquired from the pre-training of the Image-Language Branch, we proceed to train the video projector using frozen vision encoder (Siglip-384px, the same as that used in the Image-Language Branch) alongside an LLM (Large Language Model) backbone. This training process employs low learning rate of 4e 6 to refine the alignment with the language modality. During the training phase, the input video frames are sampled at rate of 1 frame per second, with maximum of 48 frames per video. Each input frame is resized to maximum resolution of 384768 pixels to maintain optimal quality and detail. Furthermore, 22 convolution layer is applied prior to the video projector. This convolutional step serves to regulate the length of the video token sequence, ensuring minimum of 182 tokens and maximum of 546 tokens. This thoughtful configuration strikes balance between performance and efficiency, facilitating effective model training while managing the computational load. Rather than immediately proceeding with the pre-training of the Video-Language Branch using only pure video-text pairs, we have opted for more nuanced two-stage approach. Initially, we leverage image-text pre-training data to strengthen the models visual understanding capabilities. After establishing robust foundation, we incrementally integrate mixed image-text pairs and video-text pairs into the training regimen. This strategy has proven to yield superior results. By gradually enhancing the models visual competence, we provide valuable guidance for the video pre-training pipeline, allowing the model to better understand and integrate the complexities of video data in conjunction with language. This methodology underscores the importance of comprehensive training strategy that incorporates diverse data modalities for improved alignment and performance. 3.2.3 Audio-Language Branch The Audio-Language branch extends an LLM pre-trained on visual and video data by incorporating an audio encoder from the Whisper-large-v3 model [67] and newly introduced audio projector. The audio encoder processes the audio signal (30s, 128 mel-spectrum) into an audio representation in 1280-channel feature space, while the audio projector (typically linear projector [14] or MLP) maps that to the embedding space of LLM. Prior to projection, pooling operation with stride of is traditionally used to down-sample the audio representation into fewer tokens (i.e., frames) for the downstream LLM. However, when we aggressively reduce the number of audio tokens, this simple pooling approach leads to loss of audio information. In our approach, we replace the pooling with Convolutional-Gated MLP (Conv-GMLP), leveraging convolution layers for down-sampling to preserve more audio information. Figure 5 illustrates the Conv-GMLP architecture, which functions similarly to gated MLP [49] but replaces linear layers with convolutional ones. Each of the two convolutional layers reduces the sequence length of the audio representation by factor of n, while proportionally expanding the feature space. In our projector, residual shortcut is along with Conv-GMLP, enabling more efficient gradient back-propagation. Results in Section 4.5.3 demonstrate strong robustness in audio performance when setting the down-sampling rate3 aggressively. During training, the LLM remains frozen, and only the audio encoder and projector are trained using long audio-text sequences (up to 4K tokens). cosine learning rate scheduler is employed to enhance performance. 3The down-sampling rate is named as avg_pooler in our code configuration. 6 Figure 5: Illustration of Conv-GMLP. Conv-GMLP down-sampling is applied to the audio representation. With down-sampling rate of 4, the output sequence length is reduced to quarter of the input, while the number of feature channels increases fourfold. 3.2.4 Image-Video-Audio Omni-Alignment The right part of Figure 4 illustrates the Omni-Alignment stage, which follows the individual training of the ImageLanguage, Video-Language, and Audio-Language branches. During this stage, all modules are trained together on mixture of high-quality image-text, video-text, and audio-text pairs to develop comprehensive multimodal understanding. 3.3 Multimodal Supervised Fine-Tuning In this section, we describe the multimodal supervised fine-tuning process aimed at improving the models ability to follow complex, multimodal instructions across various tasks. We leveraged diverse set of open-source, synthetic, and internally annotated data, covering over 200 distinct tasks and comprising approximately 600K pairs across text, audio, image-text, video-text, and image-audio modalities. Text-only Data. The text-only data covers broad range of tasks, including knowledge-based question answering, mathematics, logical reasoning, code generation, text creation, information processing, persona-based tasks, and safetyrelated data. To further strengthen the models ability to handle complex, multi-step tasks, we included specialized datasets that feature intricate instructions, some of which contain system message designed to structure more elaborate scenarios. Image Understanding Data. For tasks involving image understanding, we primarily utilized the vFLAN dataset [9], focusing on its instruction-following data. Given the quality issues present in some of the samples, we employed loss-based filtering method to clean the dataset: 1. We computed the loss for all vFLAN English instruction samples using the pretrained model and fit the resulting values to Gaussian distribution. 2. Samples are removed if their loss values fell outside the range of µ σ. (a) Samples with loss < µ σ typically included trivial issues, such as cases where the prompt and response content are nearly identical. (b) Samples with loss > µ + σ often had significant problems, such as reversed prompt-response pairs or hallucinations in the responses. subset of the cleaned vFLAN instruction data is then translated into Chinese, followed by manual re-annotation to ensure high-quality alignment. Alongside vFLAN, we incorporated several other open-source datasets, including synthdog-en/zh [37], handwritten OCR, street view OCR, reference grounding and grounded captioning duality tasks, and ImageInWords [26]. Most of these datasets are translated into Chinese. For ImageInWords, we ensured that if an image contained recognizable entity, the corresponding caption explicitly referenced that entity by name (e.g., identifying Samoyed dog by breed rather than simply labeling it as dog). 7 Although vFLAN covers 191 tasks, we found that it lacked variety in instruction types. To address this, we sampled data from our textual SFT dataset and rendered some of the prompts as images to increase the diversity of image-based instructions. Additionally, to enhance the models mathematical reasoning with images, we used the method from [108] to generate large dataset of multimodal math problems involving images. In experiments, we found that adding too much external world knowledge that the model inherently did not know resulted in diminishing performance returns. To mitigate this, we adopted the filtering method from [27] to exclude unknown data from the constucted SFT dataset. Video Understanding Data. The video-text data is primarily sourced from the VideoInstruct100K dataset [57]. While each video in the dataset includes multiple instructions, the instructions tend to be relatively homogeneous, often focusing on simple video descriptions. To enhance the diversity of video-based tasks, we applied semantic deduplication to the instructions for each video and translated the dataset into Chinese, enriching the variety of video-based tasks for the model. Audio Understanding Data. Most of the audio data is generated using TTS 4, with prompts derived from text-only, image-text, and video-text datasets. To ensure the quality of the synthesized audio, we transcribed the generated audio using an ASR model and compared the transcriptions with the original prompts. Only those audio samples with accurate transcriptions are retained as final audio prompts. To further enrich the audio data, we included human-recorded audio samples that captured various dialects, accents, and background noises. In addition to the general QA tasks, we also constructed specific ASR dataset sourced from open-source data and internal logs. To improve training efficiency, we filtered out easily recognizable samples, focusing instead on more challenging audio data for supervised fine-tuning."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Language Performance 4.1.1 Evaluation Benchmarks We perform evaluations on 4 comprehensive benchmarks, including MMLU [31], CMMLU [42], AGIEval [107] and CEval [33]. MMLU includes 57 unique tasks consisting of multiple-choice questions across various fields of knowledge, including the humanities, social sciences, and hard sciences. CMMLU represents an extensive assessment framework tailored to assess the sophisticated knowledge and reasoning capabilities of LLMs within the context of Chinese language and culture. AGIEval is human-centric benchmark crafted to evaluate the foundational models general cognitive and problem-solving abilities, based on official, public, and qualification tests designed for human participants. C-EVAL provides the comprehensive Chinese evaluation suite designed to evaluate the advanced knowledge and reasoning skills of LLMs within Chinese context, encompassing 13,948 multiple-choice questions across 52 varied disciplines, from humanities to science and engineering. We conduct all the evaluations with zero-shot measurement. 4.1.2 Major Performance We compare Ocean-omni with state-of-the-art proprietary multimodal models such as Gemini 1.5 Pro [69], and GPT4o [63], as well as series of competitive open-source LLMs and MLLMs such as VITA [24], MAP-Neo [101], Qwen1.5-Chat [5], Llama3-Instruct [3] and OLMo [29]. We list major results on comprehensive benchmarks in Table 1. As shown in Table 1, our Ocean-omni significantly outperforms open-source, general pure-text LLMs in comprehensive benchmarks. Compared to the open-source multimodal model VITA, Ocean-omni demonstrates substantial advantage in Chinese benchmarks, such as CMMLU (72.2% v.s 46.6%) and C-Eval (68.9% v.s 56.7%), and slightly surpasses VITA in AGIEval (47.7% v.s 46.2%). 4.2 Image Understanding 4.2.1 Evaluation Benchmarks We evaluate Ocean-omni on 13 representative vision-language benchmarks, including MMBench-EN, MMBenchCN [52], M3GIA [74], SEEDBench [41], RealWorldQA [87], MMMU [96], MathVista [56], MME [22], MMVet [94], TextVQA [72], OCRBench [53], ChartQA [60], and HallusionBench [30]. To ensure reproducible evaluation results, 4TTS tool: https://github.com/2noise/ChatTTS Table 1: Major results on comprehensive benchmarks. : Officially reported results. : Retrieved results from official leaderboard or recent papers. The rest unlabeled results are reproduced by ourselves. Comprehensive Tasks Model MMLU (Acc.) CMMLU (Acc.) AGIEval (Acc.) C-Eval (Acc.)"
        },
        {
            "title": "Proprietary Models",
            "content": "GPT 4o 88.0 78.3 62.3 86.0 Open-source Models (Pure text) MAP-Neo (7B) Qwen1.5-Chat (7B) Llama3-Instruct (8B) OLMo (7B) 58.2 61.5 67.1 28.4 55.1 68.0 51.7 25.6 33.9 39.3 38.4 19.9 57.5 68.8 50.7 27.3 Open-source Models (Omni-modal) VITA (8x7B) Ocean-omni (7B) 71.0 65.3 46.6 72.2 46.2 47.7 56.7 68.9 we use VLMEvalKit [20] uniformly for all evaluations. All evaluations are conducted in zero-shot manner, adhering to the original setup of the models to ensure fair and consistent comparisons across all models and benchmarks. 4.2.2 Major Performance We compare Ocean-omni with state-of-the-art proprietary multimodal models such as Gemini 1.5 Pro [69], and GPT4o [63], as well as series of competitive open-source multimodal models such as VITA [24] and Qwen2-VL [79]. We list major results on VQA (Visual Question Answering) benchmarks and results on MCQ (Multi-choice & Yes-or-No Question) benchmarks in Table 2 and Table 3. Table 2: Major Results on Multi-choice benchmarks and Yes-or-No benchmarks. : Officially reported results. : Retrieved results from official leaderboard or recent papers. The rest unlabeled results are reproduced by ourselves. Model MMBench (Acc.) MMbench-CN (Acc.) M3GIA (Acc.) SEED-IMG (Acc.) MME (Score) MMMU (val) (Acc.) HallusionBench (Acc.) Multi-choice & Yes-or-No Question Proprietary Models GPT-4o GPT-4o-mini 83.4 - 82.1 - 59.8 - - - 2328.7 2003.4 69.1 60.0 Qwen2 VL (7B) MiniCPM-Llama3-V 2.5 (8B) VITA (8x7B) Ocean-omni (7B) Open-source Models (Vision-language) 81.9 73. 37.3 30.3 76.5 72.4 2326.8 2024.6 Open-source Models (Omni-modal) 71.4 74.9 27.7 34. 72.6 74.1 2189.1 2186.9 86.4 76.7 74.7 76.2 52.7 45.8 45.3 47. 55.0 46.1 50.6 42.5 39.7 47.8 As shown in Table 2 and Table 3, our Ocean-omni comprehensively outperformed VITA-8*7b [24], which has 12B activated parameters, in multiple visual tasks, both on VQA benchmarks and MCQ benchmarks. Besides, we also demonstrates competitive performance comparable to, or even better than, open-source image-specialized models like MiniCPM-Llama3-V 2.5 [92]. Specifically, Ocean-omni outperformed MiniCPM-Llama3-V 2.5 on most VQA tasks, including MMBench-CN, SEED-IMG, MME, HallusionBnech and MMMU which requres expert-level perception and reasoning. However, despite the advantage of incorporating an additional audio modality compared to Qwen2-VL [79], the performance gap between our model and Qwen2-VL in image tasks remains evident. Furthermore, it is worth noting that, beyond Qwen2-VL, the stark divide between open-source and closed-source models remains substantial. 9 Table 3: Major Results on VQA benchmarks. : Officially reported results. : Retrieved results from official leaderboard or recent papers. The rest unlabeled results are reproduced by ourselves. Visual Question Answering Model RealWorldQA (Acc.) MMVet (Acc.) MathVista-mini (Acc.) TextVQA (val) (Acc.) ChartQA (Acc.) OCRBench (Acc.) GPT-4o GPT-4o-mini Proprietary Models 75.4 67.1 69.1 66.9 63.8 52.4 Open-source Models (Vision-language) Qwen2 VL (7B) MiniCPM-Llama3-V 2.5 (8B) 69.7 63.5 62.0 52.0 58.2 54.3 VITA (8x7B) Ocean-omni (7B) 59.0 62.6 41.6 65.4 44.9 51.9 Open-source Models (Omni-modal) - - 84.3 76.6 71.8 74.3 85.7 - 83.0 72.0 76.6 79.6 73.6 78.5 84.5 72.5 68.5 70.0 4.3 Video Understanding 4.3.1 Evaluation Benchmarks We perform thorough evaluation on general video understanding tasks (General VQA) and open-ended video question answering (Open-ended VQA) to comprehensively assess the video understanding capabilities of Ocean-omni . For general video understanding tasks, we select Perception-Test [65], MVBench [44], VideoMME [23], and EgoSchema [58] for long-form video-language understanding. We report top-1 accuracy for all benchmarks. For VideoMME, we report the results under the setting of \"w/o subs\". For open-ended video question answering part, we choose ActivityNet-QA [95] and MSVD-QA [88] as evaluation benchmarks. Following previous work [57], we utilize GPT to assess the quality of the response snippets. Specifically, we use GPT-3.5-Turbo to provide \"Yes-or-No\" decision on the correctness of answers and rating scaled from 0 to 5. We report the percentage of \"Yes\" responses as Accuracy and the average rating as Score. We conduct all evaluations in zero-shot way while avoiding the use of elaborate prompts. Besides, we follow the original setup of the models to be reproduced regarding the (maximum) number of frames, frame sampling rate, etc. they applied, ensuring fair and consistent comparisons across all models and benchmarks. 4.3.2 Major Performance We compare Ocean-omni with state-of-the-art multimodal proprietary models such as Gemini 1.5 Pro [69], GPT 4V [62], and GPT 4o [63], and series of competitive open-source multimodal models such as VITA [24], Qwen2-VL [79], AnyGPT [98], VideoLLaMA 2 [13], VideoChat2 [44], LLaVA-NeXT-Video [104], and Video-LLaVA [48]. We list major results on general video understanding benchmarks in Table 4 and results on open-ended video question answering in Table 5. Results on general video understanding benchmarks. As shown in Table 4, Ocean-omni demonstrates competitive results over proprietary models on benchmarks like Egoschema and MVBench, and achieves strong performance across open-source multimodal models, which shows comprehensive video understanding capabilities of Ocean-omni . Compared to VITA, MoE omni-modal LLM with about 12B activated parameters, Ocean-omni (7B) outperforms it on all General Video QA benchmarks, and achieve an average improvement of about 4%. Additionally, Ocean-omni excels series of open-source models such as VideoLLaMA 2, VideoChat2, LLaVA-NeXT-Vide, and Video-LLaVA. Notably, Ocean-omni also outperforms the proprietary model GPT 4V on MVBench (43.7%) and Egoschema (55.6%). Results on open-ended video question answering benchmarks. The performance on Open-ended VQA is listed in Table 5. Ocean-omni demonstrates SoTA performance (both Accuracy and Score) on ActivityNet-QA and MSVD-QA across all open-source models, such as the most recent SoTA multimodal models VITA and Qwen2 VL, and outperforms the proprietary model Gemini 1.5 Pro (56.7%) on ActivityNet-QA. The superior results indicate that Ocean-omni is also effective in open-ended question answering, i.e., Ocean-omni is more capable of generating informative and descriptive responses. 10 Table 4: Major results on general VQA benchmarks: MVBench, Egoschema, VideoMME and Perception-Test. max: Maximum number of sampling frames. : Officially reported results. : Retrieved results from official leaderboard or recent papers. The rest unlabeled results are reproduced by ourselves. General VQA Model # Frames MVBench (Acc.) Egoschema (Acc.) VideoMME (Acc.) Perception-Test (Acc.)"
        },
        {
            "title": "Proprietary Models",
            "content": "Gemini 1.5 Pro GPT 4o GPT 4V - - - 81.3 - 43.7 63.2 77.2 55.6 75.0 71.9 59.9 - - - Qwen2 VL (7B) AnyGPT (8B) VideoLLaMA 2 (7B) VideoChat2 (7B) LLaVA-NeXT-Video (7B) Video-LLaVA (7B) Open-source Models (Vision-language) 66.7 66.6 67.0 64.4 32.1 33.2 51.7 54.6 42.1 51.1 43.9 46.5 38.4 41.0 2 fps (max 768) 48 16 16 32 8 Open-source Models (Omni-modal) 63.3 59.0 29.8 46.6 33.7 33.7 39.9 62.3 60.3 29.1 51.4 47.3 48.8 44.3 VITA (8x7B) Ocean-omni (7B) 1 fps (max 32) 1 fps (max 48) 53.4 60.9 53.9 58.8 56.1 58.2 56.2 56. Table 5: Major results on ActivityNet-QA and MSVD-QA. max: Maximum number of sampling frames. : Officially reported results. The rest unlabeled results are reproduced by ourselves. Model # Frames Open-ended VQA ActivityNet-QA (Score) (Acc.) MSVD-QA (Acc.) (Score) Gemini 1.5 Pro GPT 4o GPT 4V Proprietary Models - - - 56.7 61.9 59.5 - - - Open-source Models (Vision-language) Qwen2 VL (7B) VideoLLaMA 2 (7B) VideoChat2 (7B) LLaVA-NeXT-Video (7B) Video-LLaVA (7B) 2 fps (max 768) 16 16 32 8 17.4 50.2 49.1 53.5 45.3 1.9 3.3 3.3 3.2 3.3 Open-source Models (Omni-modal) - - - 61.1 70.9 70.0 67.4 70.7 VITA (8x7B) Ocean-omni (7B) 1 fps (max 32) 1 fps (max 48) 55.0 58. 3.5 3.7 63.9 72.2 - - - 3.5 3.8 3.9 3.4 3.9 3.7 4.0 4.4 Audio Understanding 4.4.1 Evaluation Benchmarks To validate the audio understanding capacity of Ocean-omni , we present the evaluating results on benchmarks with three tasks: Automatic Speech Recognition (ASR). This is fundamental task for audio-language model pre-training which directly transcribes the audio into the text. For ASR evaluation in the general scene, we report results on the Fleurs [16] 11 Chinese (zh) and English (en) test sets, as well as the WenetSpeech [99] test_net dataset. To assess performance in more challenging ASR scenarios, we include results from the WenetSpeech [99] test_meeting dataset and the KeSpeech [78] test set, which evaluate the models ASR capabilities in Meeting and Chinese dialect contexts. For WenetSpeech, we use both Word Error Rate (WER) and Character Error Rate (CER) as evaluation metrics, while for others, only WER is used. Speech-to-Text Translation (S2TT). The task aims to translate the audio signal in the source to the target language. We evaluate the models S2TT performance between Chinese and English using the zh2en and en2zh subsets of the Covost2 [82] dataset, with BLEU [64] scores as the evaluation metric. AIR-Bench. The goal of this benchmark is to evaluate the chat capabilities to follow instructions of the given audio. We evaluate chat performance on the chat benchmark [91] (test set), using Score as the metric. 4.4.2 Major Performance Ocean-omni is compared with the state-of-the-art baselines across ASR, S2TT and SER tasks, including the recent leading large audio-language model Qwen2-Audio-Instruct [14] and the large omni-modal language model VITA [24]. On top of that, the performance of the classical pre-trained audio language model, Whisper-large-v3 [67], is presented for ASR and the performance of SALMONN [77] is presented for S2TT. Results on ASR benchmarks. Ocean-omni exhibits strong audio transcription capacity in Table 6. Ocean-omni primarily targets the Chinese corpus. In the general Chinese ASR scene, Ocean-omni has 2.0% WER (2.6% CER) superiority on the Fleurs test-zh subset and 4.1% WER (4.2% CER) improvement on the WenetSpeech test_net when comparing with Qwen2-Audio-Instruct. The evaluation results on WenetSpeech consistently demonstrate the superiority of our model over VITA. Ocean-omni achieves nearly 50% improvement in the CER performance of VITA, both in test_net (7.1% v.s 12.2%) and test_meeting (8.9% v.s 16.5%) subsets. On the more challenging Chinese dialect benchmark, KeSpeech, our model maintains comprehensive lead, with an average CER of 6.7% over the performance of all dialects. Notably, while our model excels in Chinese audio transcription, Ocean-omni also maintains robust general ASR performance in English. We achieve 4.7% of WER, which exceeds Qwen2-Audio-Instruct by 11% WER. Table 6: Major results on Fleurs, WenetSpeech, and KeSpeech. Test sets of WenetSpeech are evaluated with WER and CER, while other test sets are evaluated only with WER. VITAs evaluation results are officially reported in their paper [24], marked with . The rest unlabeled results are reproduced by ourselves, and any performance divergence may be attributed to differences in decoding parameters. Scene Dataset Model General Fleurs test-zh test-en WenetSpeech test_net Meeting WenetSpeech test_meeting Chinese Dialect KeSpeech mandarin beijing southwest lan-yin zhongyuan northeast jiang-huai ji-lu jiao-liao Whisper-large-v3 (1.55B) Qwen2-Audio-Instruct (7B) Ocean-omni (7B) Whisper-large-v3 (1.55B) Qwen2-Audio-Instruct (7B) VITA (8x7B) Ocean-omni (7B) Whisper-large-v3 (1.55B) Qwen2-Audio-Instruct (7B) VITA (8x7B) Ocean-omni (7B) Whisper-large-v3 (1.55B) Qwen2-Audio-Instruct (7B) Ocean-omni (7B) 12 Results WER (CER) 12.4 7.2 9.0 15.7 7.0 4.7 17.5 (18.5) 11.0 (11.3) - (12.2) 6.9 (7.1) 30.8 (31.7) 10.7 (10.8) - (16.5) 8.4 (8.9) 18.7 44.8 52.9 54.8 50.1 22.9 54.7 47.0 50.4 5.8 9.7 10.5 11.0 8.2 8.4 13.8 10.3 11.2 2.8 6.4 7.0 7.7 6.1 5.8 9.0 8.3 7. Results on S2TT and AIR-Bench benchmarks. In addition to ASR, Ocean-omni excels in both S2TT and SER tasks. The evaluation results are summarized in Table 7. Notably, when translating from English to Chinese on the Covost-2 en2zh test set, Ocean-omni outperforms Qwen2-Audio-Instruct by approximately 7 BLEU points. For the reverse translation, from Chinese to English, our models performance on the Covost-2 zh2en test set is comparable to that of Qwen2-Audio-Instruct. On the AirBench, Ocean-omni scores 7.42 and 7.26 for speech and sound, respectively, outperforming Qwen2-Audio-Instruct and showcasing Ocean-omni superior ability to generate realistic human speeches and sounds. Table 7: Major results on Covost2 and AirBench. represents the results from the official leaderboard or recent papers. The rest unlabeled results are reproduced by ourselves, and any performance divergence may be attributed to differences in decoding parameters. Task Dataset Model Metrics Results S2TT Covost-2 zh2en en2zh AIR-Bench Chat Benchmark speech sound music mix-audio SALMONN (7B) Qwen2-Audio-Instruct (7B) Ocean-omni (7B) Qwen2-Audio-Instruct (7B) VITA (8x7B) Ocean-omni (7B) BLEU Score - 33.1 23.3 34.1 22.1 40.2 7.18 6.99 6.79 6.77 6.40 6.59 6.59 5.94 7.42 7.26 6.12 5.76 4.5 Ablation Study 4.5.1 Image-Language Branch Visual encoder. To compare the performance of different visual encoders in Ocean-omni , we conducted experiments across various vision encoders with differing parameter sizes, input resolutions, and output token counts. We selected five mainstream vision encoders: OpenAIs CLIP series [66], Googles Siglip series [97], Apples DFN series [21], OpenGVLabs InternViT series [11], and BAAIs EVA series [76], totaling 14 models. All models are trained with contrastive learning, with parameters ranging from 300M (ViT-L) to 18B. The training data used during the pre-training of the visual encoders varied from 400M to 10B, with input resolutions spanning from 224224 to 448448 and output token counts from 256 to 1024. All comparative experiments are conducted under the same experimental conditions, specifically using batch size of 8 and the same data for IFT training (with data ratio of Caption: Interleaved data: Pure text set at 0.45: 0.45: 0.1). Table 8: Comparative study across various vision encoders with differing parameter sizes and input resolutions. We evaluated the model on 10 benchmarks, including SEEDBench2 [41], TextCaps (val) [71], TextVQA (val) [72], OCRBench [53], OCRBench (CN), OKVQA [59], Nocaps [2], VQAv2 [28], DocVQA (val) [61], and GQA [34]. We compiled the Average Performance of the model across these 10 benchmarks. Additionally, based on the specific tasks targeted by these benchmarks or their subcategories, we calculated the average scores of the model in six areas: OCR, Nature Image Understanding (NIU), Spatial, Chart, Common Sense Knowledge, and Video. Model Params. Resolution OCR NIU Spatial Chart Common Sense Video Avg. siglip-so400m-patch14-384 clip-vit-large-patch14-336 dfn5b-clip-ViT-H-14-378 InternViT-6B-224px InternViT-6B-448px-v1-5 eva-clip-8b eva-clip-8b-448 428M 304M 631M 5.9B 5.5B 7.5B 7.5B 384 px 336 px 378 px 224 px 448 px 224 px 448 px 44.67 30.19 29. 14.17 17.49 28.86 32.66 56.91 55.60 54.75 40.60 46.97 56.61 58.09 41.70 41.21 37. 29.98 35.06 40.92 43.26 25.00 15.63 21.88 15.63 18.75 25.00 21.88 51.57 48.05 49. 40.63 41.02 49.22 49.61 40.63 37.50 34.38 34.38 31.25 40.63 37.50 43.80 39.51 39. 29.99 32.80 41.51 41.86 As shown in Table 8, while increasing the resolution does lead to performance improvements (eva-448 vs eva-224, InternViT-6B-224px vs InternViT-6B-448px), the number of encoder parameters does not exhibit direct relationship with the metrics. Overall, siglip-so400m-patch14-384 [97] achieved the highest average score and excelled in four out 13 of the six tasks, particularly demonstrating outstanding performance in OCR. Considering these factors along with efficiency issues, we ultimately selected siglip-so400m-patch14-384 as the visual encoder for our Ocean-omni . we further explored the impact of using AnyRes [50] on the models visual-language performance. We found that using AnyRes [50] results in significant performance improvement compared to fixed input of 384 pixels, particularly in tasks that rely on image details, such as visual document understanding, as shown in Table 9. Table 9: Comparative study on AnyRes. Using AnyRes results in significant improvement in visual document understanding and OCR. Method TextVQA (val) DocVQA (val) InfographicVQA OCRBench Baseline Baseline+Anyres 66.48 69. 72.61 87.48 47.54 62.80 76.92 78.44 Projector. Regarding the projector, we compared the following approaches: (1) MLP: Directly passes through two-layer MLP, aligning the dimensions to that of the LLM, without reducing the number of image tokens. (2) C-abs: Passes through two convolutional layers and one pooling layer, aligning the dimensions to that of the LLM while reducing the number of tokens as needed (e.g., from 576 to 144). (3) Concat: Concatenates adjacent tokens and then processes them through MLP, allowing for token reduction but increasing the number of parameters (as the MLPs input dimension increases). (4) Mean Pool: Applies convolutional layer with stride of 2 for pooling before passing the tokens through two-layer MLP, enabling token reduction while maintaining consistent parameter count with the MLP. In early experiments, we found that models trained with different projectors exhibited little difference in general image understanding, but shows disparities in Chinese OCR comprehension after adding 1 million pure Chinese OCR VQA data. The results showed that while the model with the C-abs projector struggled to learn Chinese OCR capabilities, the model with the MLP projector began fitting the data and demonstrated zero-shot capability after 0.75 epochs. Ultimately, we ranked the projectors as follows: MLP > Mean Pool > Concat > C-abs. On the other hand, to minimize the number of tokens per sub-image after the AnyRes operation (where MLP produces 729 tokens, while Mean Pool, Concat, and C-abs each produce 182 tokens), we chose Mean Pool as our visual projector. 4.5.2 Video-Language Branch For video modality, we conduct an ablation study from three perspectives to thoroughly investigate the impact of various factors on model performance. Number of frames. Within the constraints of the context length, we systematically adjust the frame sampling rate to control the maximum number of input video frames. Resolution of vision encoder. We explore the effect of different vision encoder resolutions on the models ability to extract meaningful visual features. Our investigation spans from fixed resolutions (such as 384 384 pixels) to dynamic resolution approaches like AnyRes. Video-language pre-training. We evaluate the models performance both with and without video-language pretraining. This comparison helps us quantify the benefits of leveraging large-scale multimodal datasets for pre-training, potentially enhancing the models ability to understand video-text relationships and generalize across various videounderstanding tasks. Table 10: Ablation study on Video-language branch. We analyze the influence of number of frames, resolution of vision encoder, and video-language pre-training. w/ Pre-training Resolution # Frames # Tokens MVBench VideoMME ActivityNet-QA Avg. 384 px 384 px 384 px AnyRes 64 48 64 48 45 45 45 182 - 50.5 47.6 46.8 60.9 56.9 54.6 56.0 58.2 56.8 48.1 44.6 58.6 54.7 50.1 49.1 59.2 As demonstrated in Table 10, the models performance in video comprehension is significantly impacted by the number of input frames processed. When the quantity of input frames is decreased from 64 to 48, there is notable decline (from 54.7% to 50.1% average) in the models ability to understand and interpret video content. When testing the model by inputting total of 48 frames, it has been observed that the version of the model that utilizes AnyRes technology demonstrates superior performance compared to the version that operates with fixed resolution set at 384384. This performance advantage is evident across various benchmarks, including MVBench, VideoMME, and ActivityNet-QA. In fact, the model with AnyRes enabled shows an average improvement of around 5% over its fixed-resolution counterpart. Besides, from the first and third rows of the table, it shows that incorporating video-text pre-training has pronounced impact on the models video understanding capability. For instance, in MVBench, the model without pre-training lags approximately 6% behind the one with pre-training. Overall, we found that increasing the number of video frames, enhancing the resolution of the visual encoder, and incorporating video-text data during the pre-training stage all contribute to improving the models capabilities of understanding videos. We will leave the exploration of these three factors in situations where the input exceeds the context length (increasing number of frames and resolution) for future work. 4.5.3 Audio-Language Branch The audio projector in the audio-language branch plays key role in bridging the representations of audio and natural language modalities. Notably, our newly introduced projector with Conv-GMLP demonstrates the great performance robustness of the feature down-sampling rate. For analysis, we measure the average WER on all our ASR benchmarks across Fleurs, WenetSpeech and KeSpeech by training 1.5B audio-language model with three different downsampling rates 2, 4, 8. To simulate the actual training of the audio branch in Ocean-omni , we only train our audio encoder and projector while keeping the LLM frozen. This setup is consistent with the configuration described in Section 3.2.3. From the Figure 6, we observe that when the down-sampling rate is set to 2, the audio-language model achieves the best ASR performance, with an average WER of 7.7%. When the downsampling rate is adjusted to 4 and 8, there is slight degradation in ASR performance, but the decrease is minimal (ranging from 0.3% to 0.6%). Surprisingly, despite the greater degree of downsampling, the model with rate of 8 outperforms the one with rate of 4 (8.0% vs. 8.3%). This highlights the exceptional sequence compression capability of the Conv-GMLP. 4.5.4 Multimodal Supervised Fine-Tuning Figure 6: Ablation study on down-sampling rate. Average WER across multiple test sets (Fleurs zh/en, WenetSpeech net/meeting, and KeSpeech) using various down-sampling rates of Conv-GMLP. Table 11 and Table 12 compare the performance of Ocean-omni on various image and video benchmarks with and without multimodal supervised fine-tuning (SFT). The results indicate that the model exhibits superior overall performance after undergoing multimodal SFT compared to the version that only undergoes instruction fine-tuning (IFT). This improvement can be attributed to the use of high-quality, diverse instructions and our SFT data construction method, which avoid compromising the base models capabilities. (See Section 3.3 for more details.) Table 11: Performances of Ocean-omni on image tasks before and after the supervised fine-tuning stage. Generally, the models performance has improved across most image benchmarks. Multi-choice Question Visual Question Answering Method MMBench MMBench-CN MMMU SEED-IMG ChartQA MathVista MMVet RealWorldQA (Acc.) 75.6 76.2 IFT SFT (Acc.) 69.3 74.9 (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) 48.3 47.3 73.0 74.1 76.0 79.6 51.6 51. 55.0 65.4 62.9 62."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we have open-sourced Ocean-omni as step toward developing truly omni-modal LLM that encompasses all human senses. With omni-modal pretraining and fine-tuning using high-quality omni-modal data, this version of 15 Table 12: Performances on video tasks before and after the supervised fine-tuning stage. Multimodal supervised fine-tuning brings significant improvements for the vast majority of video benchmarks. General VQA Open-ended VQA Method Egoschema MVBench VideoMME Perception ActivityNet-QA (Score) (Acc.) (Acc.) (Acc.) (Acc.) (Acc.) MSVD-QA (Acc.) (Score)"
        },
        {
            "title": "IFT\nSFT",
            "content": "54.0 58.8 61.3 60.9 56.3 58.2 56.9 56.8 55.4 58.6 3.6 3. 66.6 72.2 3.8 4.0 Ocean-omni has achieved leading levels in integrating comprehension across video, image, text, and audio. Despite its promising performance, there remains significant room for improvement in the foundational capabilities across each individual modality. This include (1) enhancing text extraction capabilities, (2) supporting longer video understanding, (3) developing an end-to-end TTS system integrated with LLMs, and (4) improving the ability to comprehend not only human voices but also natural environmental sounds, such as flowing water, bird calls, and collision noises, among others. We anticipate efforts from both academia and industry in the field, and we believe that the expansion of model modalities, along with simultaneous advancements in enhancing model capabilities, will ultimately bring the dream of Artificial General Intelligence closer to reality."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. [3] AI@Meta. Llama 3 model card, 2024. [4] Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, Caiming Xiong, Ran Xu, Yejin Choi, and Ludwig Schmidt. Mint-1t: Scaling open-source multimodal data by 10x: multimodal dataset with one trillion tokens. arXiv preprint arXiv:2406.11271, 2024. [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [7] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [8] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [9] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. [10] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions, 2024. [11] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. [13] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [14] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [15] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [16] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. arXiv preprint arXiv:2205.12446, 2022. [17] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for audio tasks. Advances in Neural Information Processing Systems, 36:1809018108, 2023. [18] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [19] Guosheng Dong, Da Pan, Yiding Sun, Shusen Zhang, Zheng Liang, Xin Wu, Yanjun Shen, Fan Yang, Haoze Sun, Tianpeng Li, et al. Baichuanseed: Sharing the potential of extensive data collection and deduplication by introducing competitive large language model baseline. arXiv preprint arXiv:2408.15079, 2024. [20] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. [21] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [22] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2024. [23] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [24] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. [25] Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition. In INTERSPEECH, 2022. [26] Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Imageinwords: Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793, 2024. [27] Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations?, 2024. [28] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [29] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. [30] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [31] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 17 [32] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [33] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024. [34] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [35] Anil Jain. Image data compression: review. Proceedings of the IEEE, 69(3):349389, 1981. [36] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [37] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. [38] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831, 2024. [39] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. [40] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. [41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [42] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. [43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [44] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [45] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal ArXiv: dataset for improving scientific comprehension of large vision-language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [46] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [47] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024. [48] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [49] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. Pay attention to mlps. arXiv preprint arXiv:2105.08050, 2021. [50] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. 18 [51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [52] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [53] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2024. [54] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu towards real-world vision-language understanding. arXiv preprint Li, Yaofeng Sun, et al. Deepseek-vl: arXiv:2403.05525, 2024. [55] Keer Lu, Zheng Liang, Xiaonan Nie, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Wentao Zhang, et al. Datasculpt: Crafting data landscapes for llm post-training through multi-objective partitioning. arXiv preprint arXiv:2409.00997, 2024. [56] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [57] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [58] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [59] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [60] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [61] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [62] OpenAI. GPT-4V(ision) system card. https://openai.com/index/gpt-4v-system-card/, 2023. [63] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. [64] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. [65] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, et al. Perception test: diagnostic benchmark for multimodal video models. arXiv preprint arXiv:2305.13786, 2023. [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [67] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. [68] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [69] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [70] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. 19 [71] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. [72] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. [73] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29983009, 2023. [74] Wei Song, Yadong Li, Jianhua Xu, Guowei Wu, Lingfeng Ming, Kexin Yi, Weihua Luo, Houyi Li, Yi Du, Fangda Guo, et al. M3gia: cognition inspired multilingual and multimodal general intelligence ability benchmark. arXiv preprint arXiv:2406.05343, 2024. [75] Tongyi SpeechTeam. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. arXiv preprint arXiv:2407.04051, 2024. [76] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2023. [77] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. [78] Zhiyuan Tang, Dong Wang, Yanguang Xu, Jianwei Sun, Xiaoning Lei, Shuaijiang Zhao, Cheng Wen, Xingjun Tan, Chuandong Xie, Shuran Zhou, Rui Yan, Chenjia Lv, Yang Han, Wei Zou, and Xiangang Li. Kespeech: An open source speech dataset of mandarin and its eight subdialects. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [79] Qwen Team. Qwen2-VL: To See the World More Clearly. Qwen, August 2024. [80] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [81] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [82] Changhan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text translation, 2020. [83] Junjie Wang, Yin Zhang, Yatai Ji, Yuxiang Zhang, Chunyang Jiang, Yubo Wang, Kang Zhu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Bei Chen, Qunshu Lin, Minghao Liu, Ge Zhang, and Wenhu Chen. Pin: knowledge-intensive dataset for paired and interleaved multimodal documents, 2024. [84] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36, 2024. [85] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and Houqiang Li. Towards improving document understanding: An exploration on text-grounding via mllms. arXiv preprint arXiv:2311.13194, 2023. [86] Haibin Wu, Xuanjun Chen, Yi-Cheng Lin, Kai-wei Chang, Ho-Lam Chung, Alexander Liu, and Hung-yi Lee. Towards audio language modeling-an overview. arXiv preprint arXiv:2402.13236, 2024. [87] x.ai. Grok-1.5 vision preview. https://x.ai/blog/grok-1.5v, 2024. [88] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 16451653, 2017. [89] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. [90] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [91] Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual 20 Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19791998, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [92] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [93] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. [94] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [95] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134, 2019. [96] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. [97] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [98] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. [99] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition, 2022. [100] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities, 2023. [101] Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024. [102] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [103] Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, et al. Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model. arXiv preprint arXiv:2407.07053, 2024. [104] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [105] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. [106] Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. Multimodal table understanding. arXiv preprint arXiv:2406.08100, 2024. [107] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. [108] Minxuan Zhou, Hao Liang, Tianpeng Li, Zhiyu Wu, Mingan Lin, Linzhuang Sun, Yaqi Zhou, Yan Zhang, Xiaoqin Huang, Yicong Chen, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, and Zenan Zhou. Mathscape: Evaluating mllms in multimodal math scenarios through hierarchical benchmark, 2024. [109] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        }
    ],
    "affiliations": [
        "Baichuan Inc.",
        "Westlake University",
        "Zhejiang University"
    ]
}