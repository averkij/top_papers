{
    "paper_title": "Rethinking the Trust Region in LLM Reinforcement Learning",
    "authors": [
        "Penghui Qi",
        "Xiangxin Zhou",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Min Lin",
        "Wee Sun Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning."
        },
        {
            "title": "Start",
            "content": "Penghui Qi * 1 2 Xiangxin Zhou * 1 Zichen Liu 2 Tianyu Pang 1 Chao Du 1 Min Lin 1 Wee Sun Lee 2 (cid:135) https://github.com/sail-sg/Stable-RL 6 2 0 2 4 ] . [ 1 9 7 8 4 0 . 2 0 6 2 : r Figure 1. Comparison of PPO and the proposed DPPO (the Binary-TV variant in Section 4.4). (Left) The surrogate objective and corresponding masks for PPO and DPPO. PPO (and variants like GRPO) employs heuristic mask based on the probability ratio, which over-penalizes low-probability tokens and under-penalizes high-probability ones (Section 4.2). In contrast, DPPO utilizes more principled mask based on direct approximation of policy divergence (e.g., Total Variation), ensuring updates stay within theoretically grounded trust region (Section 3). (Right) Experimental results on the AIME24 using Qwen3-30B-A3B-Base. DPPO significantly outperforms GRPO baselines, achieving superior training efficiency and stability even without rollout routing replay (R3) (Section 7)."
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has become cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally illsuited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as noisy single-sample Monte Carlo estimate of the true policy divergence. This creates sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping *Equal contribution 1Sea AI Lab, Singapore 2School of Computing, National University of Singapore. Correspondence to: Wee Sun Lee <leews@comp.nus.edu.sg>, Penghui Qi <penghuiq@comp.nus.edu.sg>. Preprint. February 5, 2026. 1 with more principled constraint based on direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering more robust foundation for RL-based LLM fine-tuning. 1. Introduction Reinforcement learning (RL) is foundational paradigm for fine-tuning Large Language Models (LLMs), enabling alignment with human preferences (Ouyang et al., 2022; Rafailov et al., 2023) and complex reasoning tasks (Guo et al., 2025; Qi et al., 2025a). Proximal Policy Optimization (PPO1) (Schulman et al., 2017) has established itself as the de facto standard in this domain, favored for its simplicity and empirical scalability. Central to PPO is heuristic clipping mechanism designed to prevent destructive policy updates. By constraining the probability ratio between new 1We denote PPO by its ratio-clipping loss, regardless of advantage estimation. Under this definition, GRPO is PPO variant. Rethinking the Trust Region in LLM Reinforcement Learning gence Proximal Policy Optimization (DPPO), framework that substitutes PPOs heuristic clipping with more principled constraint grounded in trust region theory. Rather than relying on noisy single-sample ratios, DPPO directly estimates policy divergence (e.g., TV or KL divergence). To ensure memory feasibility for LLMs, we introduce two efficient approximations, Binary and Top-K divergence, which capture essential distributional shifts with negligible overhead. This allows DPPO to rigorously distinguish between safe and unsafe updates, effectively resolving the problems of overand under-constraining inherent in standard PPO. In this work, we provide comprehensive rethinking of the trust region in the context of LLM fine-tuning. Our contributions are threefold. Theoretical Formulation: We derive policy improvement bounds specifically tailored to the finite-horizon, undiscounted setting of LLM generation, establishing rigorous theoretical foundation for trust-region methods in this domain. Stability and Efficiency Analysis: We isolate the primary sources of training instability to provide practical stabilization guidelines, while further highlighting the significant role that low-probability tokens play in driving exploration. Algorithmic Performance: We demonstrate that DPPO achieves superior stability and final performance compared to existing methods like GRPO, providing robust new framework for RL-based fine-tuning. 2. Background 2.1. Policy Performance Difference We begin with the standard formulation of Markov Decision Process (MDP), defined by the tuple = (S, A, P, r, ρ0, γ), which includes the state space S, action space A, transition dynamics (ss, a), reward function r(s, a), initial state distribution ρ0(s), and discount factor γ [0, 1]. stochastic policy π(as) generates trajectories τ = (s0, a0, r0, s1, a1, r1, . . .) by sampling actions at π(st) and transitioning to states st+1 (st, at). The central goal of RL is to find policy that maximizes the expected discounted return: η(π) = Eτ π (cid:34) (cid:88) (cid:35) γtrt . t=0 (cid:104) (cid:80) To facilitate policy optimization, we define the standard value functions under policy π: the state-value function (cid:105) (cid:12) π(s) = Eτ π t=0 γtrt (cid:12)s0 = , the action-value func- (cid:105) (cid:104) (cid:80) tion Qπ(s, a) = Eτ π t=0 γtrt , and the advantage function Aπ(s, a) = Qπ(s, a) π(s). key theoretical tool for relating the performance of two distinct policies is the policy performance difference theorem (Kakade & Langford, 2002). It states that for any two policies, target policy (to be optimized) π and behavior (cid:12) (cid:12)s0 = s, a0 = Figure 2. The plots show numerical differences between training and an inference engine for Qwen3-30B-A3B-Base with identical parameters. (Left) The probability ratio (used in PPO) is highly volatile for low-probability tokens. (Right) In contrast, the TV divergence (used in DPPO) is more stable. This highlights key flaw of PPOs clipping mechanism: it over-penalizes low-probability tokens, which can slow down learning; and under-penalizes highprobability tokens, which can permit large, destabilizing updates. and old policies, the algorithm aims to confine learning to trust region where monotonic improvement is theoretically guaranteed (Schulman et al., 2015). Despite its widespread adoption, we argue that PPOs core mechanism, ratio clipping, is structurally ill-suited for the expansive, long-tailed vocabularies inherent to LLMs. Unlike Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), which constrains the KL or Total Variation (TV) divergence of the policy distribution, PPO constrains updates based on the probability ratio of the sampled token. As we demonstrate later, this approach functions as noisy, single-sample Monte Carlo estimate of the true policy divergence. While this approximation suffices for classical RL environments with limited action spaces, it fails in the LLM regime due to the ratios hypersensitivity to the probability magnitude. For example, increasing rare tokens probability from 105 to 103 generates massive ratio of 100 that triggers clipping, even though the actual divergence is negligible. Conversely, small ratio changes on high-probability tokens can make catastrophic shifts in probability mass (e.g., drop from 0.99 to 0.8), yet it often remains unpenalized by the clipping mechanism. This implicit bias is exacerbated by the training-inference mismatch (Yao et al., 2025; Qi et al., 2025b; Zheng et al., 2025), where numerical discrepancies arise between training and inference engines even under identical parameters. As illustrated in Figure 2, the probability ratio becomes highly volatile for low-probability tokens, while TV divergence remains stable. Consequently, PPO creates sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, slowing learning, while updates to high-probability tokens are under-penalized, risking instability. These limitations necessitate fundamental rethinking of the trust region approach in LLM fine-tuning to ensure both efficiency and stability. To address these fundamental limitations, we propose Diver2 Rethinking the Trust Region in LLM Reinforcement Learning policy (for rollout) µ, their expected returns are related by: η(π) η(µ) = 1 1 γ Esρπ, aπ(s) (cid:2)Aµ(s, a)(cid:3). (1) Here, ρπ(s) = (1 γ) (cid:80) t=0 γt Pr(st = π) is the normalized discounted state-visitation distribution induced by the policy π. This identity is fundamental, as it implies that any policy update that results in non-negative expected advantage guarantees monotonic performance improvement, i.e., η(π) η(µ). (Kakade & Langford, 2002; Schulman et al., 2015; Zheng et al., 2025). Therefore, maximizing Lµ(π) within small trust region guarantees stable and meaningful policy improvement. This insight motivates the trust-region optimization approach (Schulman et al., 2015; Xie et al., 2024), which involves maximizing Lµ(π) subject to constraint that keeps the new policy π within trust region around the current policy µ, thereby ensuring the validity of the approximation. Formally, this is expressed as the following constrained optimization problem: 2.2. Policy Improvement Bound While Equation 1 provides direct expression for policy improvement, its dependence on the state-visitation distribution ρπ of the new policy makes it intractable for direct optimization. To overcome this, Schulman et al. (2015) derive lower bound on performance improvement that can be estimated using samples from the behavior policy µ, with penalty term that measures the divergence between the old and new policies. This lower bound forms the basis of trust-region methods. Theorem 2.1. (Schulman et al., 2015; Achiam et al., 2017) Given any two policies, µ and π, the following bound holds: η(π)η(µ) 1 1 γ Esρµ, aµ(s) (cid:34) π(as) µ(as) (cid:35) Aµ(s, a) (2) 2ξγ TV (µ π)2, (1 γ)2 Dmax (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) and Dmax (cid:12)Aµ(s, a) TV (µ π) = (cid:0) µ(s) π(s)(cid:1), which is the maximum Total where ξ = maxs,a maxs DTV Variation (TV) divergence among all states. This bound provides direct path to guaranteed policy improvement. The right-hand side of the inequality forms surrogate objective that is tight lower bound on the true performance improvement, touching the objective when π = µ. Therefore, iteratively maximizing this surrogate guarantees monotonic improvement in η(π), following the principles of the Minorize-Maximization (MM) algorithm (Hunter & Lange, 2004; Schulman et al., 2015). 2.3. Trust Region Policy Optimization The policy improvement bound in Equation (2) directly justifies surrogate objective, Lµ(π) = 1 1 γ Esρµ, aµ(as) (cid:20) π(as) µ(as) (cid:21) Aµ(s, a) . (3) Lµ(π) max π s.t. Dmax TV (µ π) δ, (4) where the constraint can also be applied on KL divergence DKL, justified via Pinskers inequality: DTV(µ π)2 1 2 DKL(µ π). 3. Trust Region Under LLM Regime In this section, we adapt the trust region framework to the specific context of LLM fine-tuning. This setting differs from the classical RL paradigm in two crucial ways. First, the learning problem is structured as an undiscounted (γ = 1) episodic task with finite horizon , which makes the 1 original bound in Equation (2) ill-defined, as the 1γ term diverges to infinity. Second, due to the sparse reward nature, advantages are often estimated at the sequence level (Shao et al., 2024), rather than on per-token basis. Formally, given prompt x, policy π (the LLM) generates response = (y1, . . . , yT ) by sequentially sampling tokens. At each step t, the policy defines conditional distribution π(ytst) over the vocabulary A, where the state st = (x, y1, . . . , yt1) consists of the prompt and previously generated tokens. The probability of the complete response is the product of these conditional probabilities: π(yx) = (cid:81)T t=1 π(ytst). After the full response is generated, scalar reward R(y, x) is provided. For brevity, we will omit the dependency on the initial prompt and write the objective function as: (π) = Eyπ[R(y)]. We now derive performance difference identity and policy improvement bound tailored to this regime. Theorem 3.1 (Performance Difference Identity for LLMs). In finite-horizon setting (T ) with no discount (γ = 1), for any two policies π and µ, the performance difference can be decomposed as: (π) (µ) = µ(π) (µ, π), µ(π) is surrogate objective defined as: where This objective serves as first-order approximation of the true performance improvement η(π) η(µ), as their values and gradients match at the point of expansion π = µ µ(π) = Eyµ R(y) (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:19) , (5) 3 Rethinking the Trust Region in LLM Reinforcement Learning and (µ, π) is an error term given by: (µ, π) =Eyµ (cid:104) R(y) (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:19) 1 1 (cid:89) j=t+1 π(yjsj) µ(yjsj) (6) (cid:105) . This theorem provides an exact expression for the policy improvement. The surrogate µ(π) represents first-order approximation, while the error term captures the higherorder effects of the policy change. To make this practical for optimization, we bound the error term. Theorem 3.2 (Policy Improvement Bound for LLMs). In finite-horizon setting (T ) with no discount (γ = 1), the policy improvement is lower-bounded by: (π)J (µ) µ(π)2ξT (T 1)Dmax TV (µ π)2, (7) where ξ = maxy R(y) is the maximum absolute reward, (cid:0) µ(st) π(st)(cid:1) is the and Dmax maximum Total Variation (TV) divergence over all states. TV (µ π) = maxst DTV This theorem establishes lower bound on policy improvement, and it is structurally analogous to the bound in Theorem 2.1 (see Appendix B.4), with the horizon playing role similar to the effective horizon 1 1γ in the discounted setting. It provides clear theoretical justification for adapting the trust region approach into LLM regime. Similar to Equation (4), we can solve the following constrained optimization problem to guarantee stable learning: max µ(π) π s.t. Dmax TV (µ π) δ, (8) where the constraint can also be applied on KL divergence. The proofs for Theorem 3.1 and Theorem 3.2 are deferred to Appendix B. In Appendix B.3, we further derive more practical bound that depends linearly, rather than quadratically, on the horizon length . 4. Methodology 4.1. Proximal Policy Optimization While theoretically appealing, the constrained optimization in TRPO requires second-order methods that are computationally expensive and difficult to scale. PPO (Schulman et al., 2017) was introduced to achieve the stability of trust region method using only first-order optimization. Its simplicity and strong empirical performance have established it as standard algorithm for fine-tuning LLMs. Instead of an explicit trust region constraint, PPO discourages overly large policy updates with heuristic clipping mechanism applied to the surrogate objective. Specifically, PPO optimizes the following clipped objective: (π) = Eyµ (cid:88) (cid:16) min LPPO µ rt ˆAt, clip(rt, 1ϵ, 1+ϵ) ˆAt t=1 rt = π(ytst) µ(ytst) , (cid:17) , (9) where ˆAt is an estimated advantage at timestep t. In LLM fine-tuning, the advantage is usually estimated by ˆA = (cid:80)G R(y) 1 i=1 R(yi) (Shao et al., 2024; Liu et al., 2025d), where {yi}G i=1 is group of responses for the same prompt. The connection between PPOs clipping and the formal trust region can be understood by examining the TV divergence:"
        },
        {
            "title": "DTV",
            "content": "(cid:0) µ(st) π(st)(cid:1) = Eytµ (cid:12)rt 1(cid:12) (cid:2)(cid:12) (cid:12) (cid:3) . (10) 1 From this perspective, PPOs clipping condition, rt1 ϵ, can be interpreted as constraining single-sample Monte Carlo estimate of the expected value in Equation (10). In essence, PPO enforces its trust region not on the true TV divergence, but on noisy, single-point estimation. As we will argue next, this crude approximation is the source of significant pathologies when applied to the large, long-tailed vocabulary distributions characteristic of LLMs. 4.2. Limitations of PPO Ratio Clipping The key limitation of PPO is that whether an update is clipped depends heavily on the sampled tokens probability, rather than the true TV divergence between µ(st) and π(st). Concretely, consider fixed state and two tokens alow and ahigh with µ(alows) = 104, µ(ahighs) = 0.99, π(alows) = 102, π(ahighs) = 0.80. The probability ratio for the low-probability token is rlow = 102 104 = 100, which is far outside typical clipping range [1ϵ, 1+ϵ] (e.g., ϵ = 0.2). PPO would thus heavily clip the contribution of this update. In contrast, the actual contribution of this change to the TV divergence can be very small, because the total mass moved at alow is tiny. For the highprobability token, rhigh = 0.80 0.99 0.808, which can still lie inside the clipping range for moderate ϵ. Yet this update removes 0.19 probability mass from the dominant token, and therefore induces much larger contribution to DTV. These examples highlight structural flaw in PPOs clipping heuristic. For low-probability tokens, an update that produces large probability ratio is aggressively constrained, even when its impact on the TV divergence is negligible, thereby slowing training efficiency. Conversely, for highprobability tokens, an update producing ratio close to Rethinking the Trust Region in LLM Reinforcement Learning one may go unpenalized, even when the absolute change in probability mass is large enough to cause substantial TV divergence, which in turn risks training instability. Connections to Existing Work The insight that PPOs ratio clipping disproportionately penalizes low-probability tokens aligns with several prior studies. For instance, methods like Clip-Higher (Yu et al., 2025) and CISPO (Chen et al., 2025) observe that important exploration or reasoning tokens often have low initial probabilities (see Appendix E). These tokens usually get high importance ratios during policy updates and are consequently clipped, hindering the learning process. However, the solutions proposed remain heuristic and problematic. Clip-Higher suggests manually increasing the upper clipping bound, while CISPO continues to apply the gradient even for large divergence, completely ignoring the trust region. While these methods correctly identify the symptom, they fail to address the root cause: the fundamental mismatch between the single-sample probability ratio and the true distributional divergence. 4.3. Divergence Proximal Policy Optimization To address the limitations of ratio clipping, we introduce Divergence Proximal Policy Optimization (DPPO), method that replaces PPOs flawed heuristic with more principled constraint grounded in trust region theory. Similar to Chen et al. (2025); Zheng et al. (2025), DPPO employs dynamic mask to prevent updates that would leave the trust region. The DPPO objective is: LDPPO µ (π) = Eyµ (cid:88) DPPO rt ˆAt . (11) t=1 Our key innovation lies in the design of this mask. Instead of relying on the noisy single-sample ratio, it is conditioned on direct measure of the policy distributions divergence: DPPO = 0, 1, otherwise, if ( ˆAt > 0 and rt > 1 and > δ) or ( ˆAt < 0 and rt < 1 and > δ) (12) where D(cid:0) µ(st) π(st)(cid:1) is the divergence (e.g., TV or KL) between the policy distributions, and δ is divergence threshold hyperparameter. This design directly approximates the formal trust region constraint from Theorem 3.2 while preserving the beneficial asymmetric structure of PPOs clipping. The mask only considers blocking an update if it is already moving away from the trusted region (i.e., rt > 1 for positive advantage or rt < 1 for negative advantage). It never blocks updates that move the policy ratio towards one (e.g., when ˆAt > 0 and rt < 1), desirable property for accelerating learning. Unlike PPO, the final decision to block an update is based on whether the entire policy distribution has shifted too far (D > δ), not on the noisy and often misleading ratio of single sample. This resolves the overand under-constraining issues inherent in standard PPO. The primary remaining challenge is the overhead of calculating the full divergence over large vocabulary in LLMs, which we address next. 4.4. Approximating Distribution Divergence Directly computing the policy divergence is memoryprohibitive for LLMs. To make it practical, we introduce two lightweight approximations, which serve as principled lower bounds of the true divergence (see Appendix C). Binary Approximation The binary approximation collapses the original categorical distribution into simple Bernoulli distribution, distinguishing only between the sampled token and all other tokens. We define the new distribu1 π(atst)(cid:1), where π can be µ tion as: pπ or π. The TV and KL divergences are then computed as: = (cid:0)π(atst), DBin TV (t) =(cid:12) (cid:12) µ(atst) π(atst)(cid:12) (cid:12), µ(atst) π(atst) DBin KL (t) = µ(atst) log + (1 µ(atst)) log 1 µ(atst) 1 π(atst) . (13) (14) This binary divergence can be computed at negligible overhead. Crucially, it correctly distinguishes between large versus small shifts in absolute probability mass, thereby resolving the primary failure mode of PPOs clipping. Top-K Approximation To provide richer and more faithful approximation of the distributional shift, the top-K variant explicitly tracks the most probable tokens. First, we define small, representative set of tokens as: = TopK (cid:0) µ(st), K(cid:1) {at}, which includes the highest-probability tokens under the behavior policy, augmented with the sampled token at if it is not already present. We then form reduced categorical distributions, pµ and = , over the new vocabulary pπ {other}. For any token t, its probability is its original probability, while all other tokens are aggregated into the other (a) = π(ast) A category: pπ (other) = 1 (cid:80) π(ast), where π can be µ or π. The divergence is then computed over this reduced distribution: (a)(cid:12) (cid:12), (a) pπ t, and pπ TV (t) = DTopK (cid:12) (cid:12)pµ aA (15) (cid:88) 1 aA DTopK KL (t) = pµ (a) log pµ (a) pπ (a) . (cid:88) aA (16) This approach better captures changes in the head of the policy distribution, which typically dominates the true divergence value. The overhead is minimal, making it practical and high-fidelity choice for DPPO. 5 Rethinking the Trust Region in LLM Reinforcement Learning Figure 3. DPPO variants achieve stable training while controlling the training-inference mismatch at low level. In contrast, methods without trust region (PG-IS, CISPO) or with misspecified one (MiniRL) suffer from growing mismatch and eventual collapse. 5. Analysis on Training Stability 5.1. The Necessity of Trust Region The RL fine-tuning of LLMs is prone to training instability due to training-inference mismatch (see Appendix A.2). In this section, we conduct an empirical study to dissect this issue and verify the stability of our DPPO algorithm. To formalize our analysis, we denote the parameters being optimized as θ and the parameters used for data generation as θ. We aim to answer three fundamental research questions: 1. Given the extremely low learning rates (e.g., 106) common in LLM fine-tuning, is trust region still necessary to ensure training stability? 2. Should the trust region be defined with respect to the original rollout distribution (µθ) or recomputed policy distribution (πθ)? 3. What specific types of policy updates are the primary drivers of training instability? Experimental Setting: Our experimental setup follows the sanity test proposed by Qi et al. (2025b). We finetune DeepSeek-R1-Distill-Qwen-1.5B (Guo et al., 2025) on curated set of 1,460 problems from the MATH dataset (Hendrycks et al., 2021). In this setting, stable algorithm should theoretically converge to 100% training accuracy, as all problems are known to be solvable by the initial model. We evaluate several algorithms, each representing different approach to managing the policy update. The baselines include: PG-IS and its truncated variant PG-TIS (also known as CISPO (Chen et al., 2025)), which use standard policy gradients with token-level importance sampling; GRPO with Clip-Higher, PPO-like algorithm where clipping is based on the rollout policy ratio rt = πθ (Shao et al., 2024; µθ Liu et al., 2025d); and MiniRL & MiniRL-TIS, PPO variant where clipping is based on recomputed policy ratio rt = πθ (Zheng et al., 2025). We compare these against πθ DPPO (Ours), our proposed method using either binary KL or TV divergence, with the trust region defined with respect to the rollout distribution µθ. Detailed configurations for each algorithm are provided in the Appendix D. Our first question addresses whether trust region is redundant at low learning rates. Figure 3 provides clear answer. The unconstrained methods, PG-IS and PG-TIS (CISPO), both suffer from an increasing training-inference mismatch, which culminates in collapse of performance. In contrast, our DPPO variants, which enforce principled trust region, maintain stable, low level of mismatch throughout training and achieve near-perfect final rewards. Takeaway 1: trust region is essential for stable training, even with very small learning rates. Without it, the traininginference mismatch accumulates and leads to collapse. Figure 4. Switching the stable DPPO-KL to decoupled objective causes the mismatch to grow and performance to collapse, confirming that the trust region must be anchored to the rollout policy. 5.2. The Correct Anchor for the Trust Region Next, we investigate to which distribution the trust region should be anchored. common practice in open-source implementations (Sheng et al., 2024; Zhu et al., 2025) is to use decoupled objective (Hilton et al., 2022), where the trust region is enforced relative to recomputed policy distribution (πθ) instead of the original behavior policy (µθ). The MiniRL algorithm, for example, follows this design (Zheng et al., 2025). Our results show this choice is detrimental. As in Figure 3, MiniRL fails to control the training-inference mismatch and its performance collapses, despite using trust region. To confirm this, we created decoupled version of our stable DPPO-KL algorithm. Figure 4 shows that this single change corrupts the stable training process, 6 Rethinking the Trust Region in LLM Reinforcement Learning causing the mismatch to grow and performance to collapse. Takeaway 2: The trust region must be defined with respect to the original behavior policy (µθ). Using recomputed on-policy distribution as the anchor leads to instability. This finding aligns with the theoretical bound in Equation (7) and offers significant practical benefit: by removing the need for recomputation, we can reduce training costs by approximately 25% (Qi et al., 2025b). Figure 5. Isolating the source of instability. The solid curves are training rewards, while the dashed lines are the percentage of bad updates. Starting with the unstable PG-IS, applying minimal mask that only blocks large-divergence bad updates on negative samples is sufficient to stabilize training, indicating these bad updates are the primary cause of training instability. subset of updates on negative samples that push the policy far outside the trust region. likely reason is that aggressively penalizing token the model deems probable can corrupt the LLMs internal knowledge and destabilize the learning process. This finding confirms the critical need for trust region, particularly when handling negative feedback. 5.4. The Pitfalls of Truncated Importance Sampling Our empirical results also reveal surprising finding regarding Truncated Importance Sampling (TIS), technique widely adopted to control the variance of policy gradient estimates (Yao et al., 2025; Chen et al., 2025). Contrary to its intended purpose, TIS consistently degrades training stability in our experiments. As illustrated in Figure 3, the TIS-enabled variants (PG-TIS and MiniRL-TIS) suffer from premature collapse and significantly underperform their untruncated counterparts. We hypothesize that this detrimental effect stems from the same issue as PPOs ratio clipping: low-probability tokens, which naturally produce high-variance ratios, are the most likely to be truncated by TIS. While this does reduce variance, it systematically down-weights the gradient signal from these tokens, introducing significant and harmful bias into the policy update. This suggests that naive truncation can be just as damaging as naive clipping. 5.3. Identifying the Source of Instability 6. Analysis on Training Efficiency Finally, we seek to pinpoint which specific policy updates are most responsible for the instability. Our methodology is to start with the unstable PG-IS algorithm, which applies no update masking, and introduce the most minimal mask necessary to restore stability. This allows us to isolate the most detrimental class of updates. Since updates on positively rewarded samples are typically safe, we focus on negative samples where the policy is penalized (Liu et al., 2025a; Ren & Sutherland, 2025). We design simple mask that only blocks updates on negative samples where the probability of the sampled token is decreased by more than threshold δ: Mt = 0 if ˆAt < 0 and µθ(ytst) πθ(ytst) δ. As shown in Figure 5, applying this minimal mask with δ = 0.5 is sufficient to stabilize the training. In contrast, slightly looser mask (δ = 0.8) or one anchored to the recomputed distribution (Mask-0.5-Recompute) both fail to prevent the eventual collapse. We define bad updates as those where this divergence exceeds 0.5 and plot their percentage over time. The plot reveals that only very small fraction of updates are bad ( 0.5%) yet they are the primary culprits behind training collapse. Furthermore, the percentage of these bad updates strongly correlates with reward fluctuation; as the fraction of bad updates rises, the reward curve becomes more erratic, reinforcing causal link. Takeaway 3: The primary source of instability is small Beyond training stability, the design of trust region is also critical for training efficiency. As motivated in Section 4.2, PPOs ratio-clipping over-constrains the updates to low-probability tokens, which might be permitted by divergence-based trust region. In this section, we aim to analyze how low-probability tokens affect the training dynamics, thus justifying the adoption of divergence-based trust region in our DPPO algorithm. Figure 6. Analysis of relaxing trust regions for low-probability tokens. (Left) Training reward curves. (Middle) Rollout probability of clipped tokens. (Right) Entropy of clipped tokens. Experimental Setting: We fine-tune Qwen3-1.7B-Base (Yang et al., 2025) on the DAPO dataset (Yu et al., 2025). We employ GRPO (Guo et al., 2025; Liu et al., 2025d) with the Clip-Higher trick (Yu et al., 2025) as the baseline algorithm. We then relax trust regions by setting the clipping threshold ϵ in Equation (9) as infinity 7 Rethinking the Trust Region in LLM Reinforcement Learning for tokens with µ(ytst) < α, thus isolating the effect of low-probability tokens. The learning curves for varying values of α are presented in Figure 6. Notably, relaxing the clipping constraint for tokens with µ(ytst) < 0.1 yields substantial improvement in training efficiency compared to the GRPO baseline (α = 0). This observation validates our hypothesis that the ratio-clipping mechanism in PPO over-constrains thereby hindering updates to low-probability tokens, overall learning progress. The middle plot reveals that clipped tokens are predominantly characterized by low probabilities (typically below 0.15 for the baseline in blue). As α increases, the probabilities of clipped tokens also rise, confirming that PPOs ratio-clipping is structurally biased against low-probability tokens. Furthermore, the right plot demonstrates that clipped tokens frequently exhibit high entropy. Consistent with Wang et al. (2025a), which posits that RL is driven primarily by high-entropy tokens in LLMs, our results suggest that relaxing constraints on these tokens enables more informative policy updates and thus achieves higher training efficiency (see Appendix for most frequent clipped tokens). Furthermore, we examine the effect of directional clip relaxation with fixed α = 0.1. We generalize the clip operation with asymmetric thresholds, denoted as clip(rt, 1ϵlow, 1+ ϵhigh), where ϵlow = 0.2 and ϵhigh = 0.28 by default. We relax either one end (Relax-high or Relax-low) or both ends (Relax-both). For example, Relax-high is implemented by (ϵlow = 0.2, ϵhigh = ) for tokens with µ(ytst) < α. As illustrated in Figure 7, the direction of clip relaxation plays critical role in the training efficiency and stability. Relax-high can be viewed as an extreme variant of the Clip-Higher trick (Yu et al., 2025) applied only to lowprobability tokens. While this approach maintains high entropy, it fails to yield significant gains in training efficiency. Conversely, Relax-low exhibits substantially faster initial learning2. However, this strategy eventually drops due to entropy collapse (Cui et al., 2025). Ultimately, we find that Relax-both is the most effective strategy for achieving both efficient and stable training, thereby validating the design of DPPO in relaxing both ends of the trust region. 7. Scaling Experiments Experimental Setting: We conduct large-scale experiments to further validate our methods. We train on filtered subset of DAPO-Math (Yu et al., 2025), containing approximately 2In contrast to the Clip-Higher intuition (Yu et al., 2025), we observe that Clip-Lower (relaxing ϵlow) for low-probability tokens is more vital for efficiency. This aligns with findings by Tajwar et al. (2024) regarding the role of negative gradients in accelerating preference learning. Figure 7. Analysis of trust region relaxation direction. (Left) Training reward curves. (Right) Policy entropy. 13k samples. Five model configurations (different base models and training techniques) are evaluated: (1) MoE Base: Qwen3-30B-A3B-Base (Yang et al., 2025); (2) MoE Base w/ R3: Qwen3-30B-A3B-Base with rollout router replay (R3) (Ma et al., 2025); (3) MoE Thinking: Qwen3-30BA3B; (4) Dense Base: Qwen3-8B-Base; (5) MoE Base w/ LoRA: Qwen3-30B-A3B-Base with LoRA (Hu et al., 2022). Baseline methods include GRPO-ClipHigher(Shao et al., 2024; Liu et al., 2025d; Yu et al., 2025) and CISPO(Chen et al., 2025; Khatri et al., 2025). All methods use the behavior policy (µθ) instead of recomputed policy distribution (πθ) to construct the trust region (i.e., for clipping or masking). We compare our proposed methods, DPPO-BinaryKL and DDPO-Binary-TV, against these baselines. More details are provided in Appendix F. Main Results. We present online evaluation results on AIME24 and AIME25 (MAA, 2025) during RL training in the following figures: Figure 8 (MoE Base with and without R3) and Figure 9 (MoE Thinking and Dense Base). Results for MoE Base with LoRA are provided in Appendix G.1. Our proposed method consistently demonstrates superior stability and efficiency across all five large-scale experiments. Specifically, DPPO optimizes rewards at significantly faster speed than the GRPO-ClipHigher baseline and achieves better converged performance, providing empirical validation for the motivations discussed in Section 4.2. While all baseline methods frequently exhibit training instability or catastrophic collapse (e.g., CISPO in MoE Base without R3 and GRPO-ClipHigher in MoE Thinking), our approach maintains remarkably stable training process. Rollout router replay (R3) is widely considered necessary technique for stabilizing RL training in MoE models (Ma et al., 2025; Zheng et al., 2025; Liu et al., 2025a). However, as illustrated in Figure 8, our DPPO variants (without R3) even consistently outperform the R3-enhanced baselines, which underscores the superior training efficiency and inherent stability of the DPPO framework. We provide additional detailed results and extended discussions in Appendix G.1. Ablation on TV/KL Approximation. In the above scaling experiments, DPPO is implemented using the binary TV/KL 8 Rethinking the Trust Region in LLM Reinforcement Learning Figure 8. Evolution of AIME24 and AIME25 Avg@32 scores during RL training using Qwen3-30B-A3B-Base. The first and third panels correspond to the same experiment without rollout router replay (w/o R3), while the second and fourth panels correspond to the same experiment with rollout router replay (w/ R3). Figure 9. Evolution of AIME24 and AIME25 scores during RL training using Qwen3-30B-A3B (left) and Qwen3-8B-Base (right). 8. Conclusion In this work, we have presented comprehensive rethinking of the trust region framework within the context of LLM fine-tuning. We derived policy improvement bounds specifically tailored to the finite-horizon, undiscounted setting of LLM generation, establishing rigorous theoretical foundation for future trust-region research. Furthermore, through extensive empirical analysis, we investigated the trade-offs between training stability and efficiency, providing practical guidelines to optimize both. Central to our contribution is the introduction of Divergence Proximal Policy Optimization (DPPO). We identified and addressed critical structural flaw in the standard PPO algorithm: it over-constrains updates to low-probability tokens while under-constraining potentially catastrophic shifts in high-probability tokens. This implicit bias results in suboptimal training dynamic, particularly for the expansive, long-tailed vocabularies inherent to LLMs. By substituting heuristic ratio clipping with more principled policy divergence, DPPO significantly enhances both efficiency and stability. To avoid huge memory footprint for computing an exact policy divergence, we introduced Binary and Top-K approximations, which capture essential divergence with negligible overhead. Our evaluations demonstrate that DPPO consistently outperforms existing methods like GRPO in both training efficiency and stability, offering more robust foundation for the RL-based LLM fine-tuning. Figure 10. Evolution of AIME24 and AIME25 scores for baselines and DPPO with binary/Top-K (K=20) TV/KL approximation under the same setting as MoE Base w/o R3. approximation (Equations 13 and 14). To assess the impact of this simplification, we compare it against DPPO with the top-K (K=20) TV/KL (Equations 15 and 16) under the same setting as MoE Base. The results, presented in Figure 10, show that both approximations perform similarly and significantly outperform the baselines. This finding indicates that the easy-to-implement binary approximation is sufficient and computationally efficient choice for scalable RL. We provide more detailed results in Appendix G.2. Generalization to Other Model Families and Tasks. We also conduct experiments on models from the Llama family (Touvron et al., 2023; Wang et al., 2025b) and on tasks beyond math reasoning (Liu et al., 2025e). The results, which are presented in Appendix G.3, show DPPO outperforms the baseline across most settings, highlighting its broad applicability. 9 Rethinking the Trust Region in LLM Reinforcement Learning"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Achiam, J., Held, D., Tamar, A., and Abbeel, P. Constrained policy optimization. In International conference on machine learning, pp. 2231. PMLR, 2017. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, H. Defeating nondeterminism in llm inference. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250910. https://thinkingmachines.ai/blog/defeatingnondeterminism-in-llm-inference/. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hilton, J., Cobbe, K., and Schulman, J. Batch sizeinvariance for policy optimization. Advances in Neural Information Processing Systems, 35:1708617098, 2022. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Hunter, D. R. and Lange, K. tutorial on mm algorithms. The American Statistician, 58(1):3037, 2004. Kakade, S. and Langford, J. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pp. 267274, 2002. Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., and Agarwal, R. The art of scaling reinforcement learning compute for llms. arXiv preprint arXiv:2510.13786, 2025. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Liu, collapse J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Y. When speed kills stability: Demysfrom the tifying inference-training rl https://yingru.notion.site/Whenmismatch, 2025b. Speed-Kills-Stability-Demystifying-RLCollapse-from-the-Inference-Training-Mismatch271211a558b7808d8b12d403fd15edda. Liu, Z., Chen, C., Du, C., Lee, W. S., and Lin, M. Oat: research-friendly framework for llm online alignment. https://github.com/sail-sg/oat, 2025c. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025d. Liu, Z., Sims, A., Duan, K., Chen, C., Yu, S., Zhou, X., Xu, H., Xiong, S., Liu, B., Tan, C., et al. Gem: gym for agentic llms. arXiv preprint arXiv:2510.01051, 2025e. Ma, W., Zhang, H., Zhao, L., Song, Y., Wang, Y., Sui, Z., and Luo, F. Stabilizing moe reinforcement learning by aligning training and inference routers. arXiv preprint arXiv:2510.11370, 2025. MAA. American invitational mathematics examination - aime. https://maa.org/, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Qi, P., Liu, Z., Pang, T., Du, C., Lee, W. S., and Lin, M. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025a. Qi, P., Liu, Z., Zhou, X., Pang, T., Du, C., Lee, W. S., and Lin, M. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025b. 10 Rethinking the Trust Region in LLM Reinforcement Learning Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Ren, Y. and Sutherland, D. J. Learning dynamics of LLM finetuning. In The Thirteenth International Conference on Learning Representations, 2025. Wan, X., Qi, P., Huang, G., Ruan, C., Lin, M., and Li, J. Revisiting parameter server in llm post-training. arXiv preprint arXiv:2601.19362, 2026. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Schulman, J. and Lab, T. M. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250929. https://thinkingmachines.ai/blog/lora/. Wang, Y., He, H., Tan, X., and Gan, Y. Trust region-guided proximal policy optimization. Advances in Neural Information Processing Systems, 32, 2019. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, In International P. Trust region policy optimization. conference on machine learning, pp. 18891897. PMLR, 2015. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Tajwar, F., Singh, A., Sharma, A., Rafailov, R., Schneider, J., Xie, T., Ermon, S., Finn, C., and Kumar, A. Preference fine-tuning of LLMs should leverage suboptimal, onpolicy data. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4744147474. PMLR, 2127 Jul 2024. Team, L., Han, B., Tang, C., Liang, C., Zhang, D., Yuan, F., Zhu, F., Gao, J., Hu, J., Li, L., Li, M., Zhang, M., Jiang, P., Jiao, P., Zhao, Q., Yang, Q., Shen, W., Yang, X., Zhang, Y., Ren, Y., Zhao, Y., Cao, Y., Sun, Y., Zhang, Y., Fang, Y., Lin, Z., Cheng, Z., and Zhou, J. Every attention matters: An efficient hybrid architecture for long-context reasoning. arXiv preprint arXiv:2510.19338, 2025a. Team, L., Shen, A., Li, B., Hu, B., Jing, B., Chen, C., Huang, C., Zhang, C., Yang, C., Lin, C., et al. Every step evolves: Scaling reinforcement learning for trillion-scale thinking model. arXiv preprint arXiv:2510.18855, 2025b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 11 Wang, Y., He, H., and Tan, X. Truly proximal policy optimization. In Uncertainty in artificial intelligence, pp. 113122. PMLR, 2020. Wang, Z., Zhou, F., Li, X., and Liu, P. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Preprint. Xie, Z., Zhang, Q., Yang, F., Hutter, M., and Xu, R. Simple policy optimization. arXiv preprint arXiv:2401.16025, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, F., Liu, L., Zhang, D., Dong, C., Shang, J., and Gao, J. Your efficient rl framework secretly brings you off-policy rl training, August 2025. https://fengyao.notion.site/offpolicy-rl. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zhang, Z., Ding, X., Yuan, J., Liu, R., Mao, H., Xing, J., and Liu, Z. Deterministic inference across tensor parallel sizes that eliminates training-inference mismatch. arXiv preprint arXiv:2511.17826, 2025. Zheng, C., Dang, K., Yu, B., Li, M., Jiang, H., Lin, J., Liu, Y., Lin, H., Wu, C., Hu, F., et al. Stabilizing reinforcement learning with llms: Formulation and practices. arXiv preprint arXiv:2512.01374, 2025. Zhu, Z., Xie, C., Lv, X., and slime Contributors. slime: An llm post-training framework for rl scaling. https:// github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv. Rethinking the Trust Region in LLM Reinforcement Learning A. Related Work A.1. Extended Connections to Existing Work In this work, we identify structural flaw in PPOs ratio-clipping mechanism within the LLM regime: it over-penalizes low-probability tokens and under-penalizes high-probability ones, thereby impairing training efficiency and stability. Our proposed DPPO addresses this issue by directly constraining the policy divergence. This methodology aligns with the insights of Wang et al. (2019; 2020), who observed similar exploration issues and proposed adaptive clipping based on KL divergence in traditional RL settings. However, in the context of LLMs, computing the exact divergence is prohibitive due to the huge memory footprint. To overcome this, we propose binary divergence approximation, which empirically captures most of the benefits (see Appendix G.2). Furthermore, as demonstrated in Section 5 and Section 6, the challenges of training stability and efficiency are exacerbated in LLMs by their expansive vocabularies, because low-probability tokens form non-trivial portion of the entire distribution due to the long-tailed nature (see Figure 2). Finally, the training-inference mismatch inherent to the LLM era introduces additional algorithmic complexities, as further detailed in Section 5. A.2. Training-inference Mismatch Recent work has identified key culprit for training instability: the training-inference mismatch (πθ = µθ), where the policy distribution used for gradient computation (πθ) diverges from the one used for data generation (µθ), even when using identical model parameters θ (Yao et al., 2025; Qi et al., 2025b; Liu et al., 2025b; Zheng et al., 2025). This discrepancy arises from numerical precision errors (Qi et al., 2025b) and subtle differences in implementation (Team et al., 2025a; He, 2025). As training progresses, this mismatch can be amplified if the RL algorithm cannot manage it appropriately, leading to catastrophic performance degradation (Qi et al., 2025b; Liu et al., 2025b). Existing efforts to mitigate this issue primarily focus on correcting biased gradients through importance sampling. Building on this principle, techniques such as Truncated Importance Sampling (TIS) (Yao et al., 2025; Zheng et al., 2025) and Masked Importance Sampling (Liu et al., 2025b; Team et al., 2025b) have been introduced at both the token and sequence levels. However, as suggested by Qi et al. (2025b), these methods often fail to achieve satisfactory balance between training efficiency and stability. In contrast, our DPPO algorithm significantly enhances both aspects compared to these existing approaches. Another line of research attempts to resolve the mismatch issue through higher precision (Qi et al., 2025b) or rigorous engineering alignment (Team et al., 2025a; He, 2025; Zhang et al., 2025). While promising, these methods face limited applicability. For instance, aligning implementation details often requires specific training engines or model architectures, hindering broad adoption. Furthermore, in low-precision settings optimized for high-speed training, we must tolerate significant training-inference mismatch. In such scenarios, robust and fast algorithm like DPPO remains essential. Finally, our algorithmic design is orthogonal to these engineering-level optimizations and can be combined with them to achieve even greater performance gains. B. Trust Region in LLMs B.1. Proof of Performance Difference Identity Proof of Theorem 3.1. We begin by expressing the difference in expected returns by its definition: (π) (µ) = Eyπ[R(y)] Eyµ[R(y)] (cid:88) = (cid:0) π(yx) µ(yx)(cid:1)R(y). The core of the proof is to establish an identity for the difference in the probabilities of generating sequence y, π(yx) µ(yx). We use the following telescoping sum identity, which can be verified by expanding the terms: π(yx) µ(yx) = (cid:88) (cid:32)t1 (cid:89) t=1 k=1 (cid:33) µ(yksk) (cid:16) π(ytst) µ(ytst) (cid:17) (cid:89) j=t+1 π(yjsj) . 12 Rethinking the Trust Region in LLM Reinforcement Learning Substituting this identity into the expression for the performance difference yields: (π) (µ) = R(y) (cid:88) (cid:88) (cid:32)t1 (cid:89) t=1 k=1 (cid:33) µ(yksk) (cid:16) π(ytst) µ(ytst) (cid:17) (cid:89) j=t+ π(yjsj) (cid:88) = µ(yx)R(y) (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:19) 1 (cid:89) j=t+1 π(yjsj) µ(yjsj) = Eyµ R(y) = Eyµ R(y) (cid:88) t=1 (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:18) π(ytst) µ(ytst) (cid:19) 1 (cid:89) j=t+1 π(yjsj) µ(yjsj) (cid:19) 1 Eyµ R(y) (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:19) 1 1 (cid:89) j=t+1 π(yjsj) µ(yjsj) . By identifying the terms with the definitions in the theorem statement, we arrive at: (π) (µ) = µ(π) (µ, π), where µ(π) = Eyµ R(y) (µ, π) = Eyµ R(y) (cid:88) t=1 (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:18) π(ytst) µ(ytst) (cid:19) 1 , (cid:19) 1 1 (cid:89) j=t+1 π(yjsj) µ(yjsj) . This completes the proof. B.2. Proof of Policy Improvement Bound Lemma B.1 (Bound on Sequence-Level TV Divergence). Let µ and π be two policies that generate sequences of length . Let µN (s1) and πN (s1) denote the distributions over sequences = (y1, . . . , yN ). The total variation (TV) divergence between these sequence distributions is bounded by the sum of the expected single-step TV divergences: DTV (cid:0)µN (s1)πN (s1)(cid:1) (cid:88) t=1 Estµ (cid:2)DTV (cid:0) µ(st) π(st)(cid:1)(cid:3) , where the expectation is over the state distribution induced by policy µ. Proof. Let (y) = µN (ys1) and Q(y) = πN (ys1). 2DTV(P Q) = (cid:88) P (y) Q(y) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:89) t=1 (cid:88) µ(ytst) (cid:89) t=1 (cid:12) (cid:12) (cid:12) π(ytst) (cid:12) (cid:12) . We use the algebraic identity a1 . . . aN b1 . . . bN = (cid:80)N t=1 (cid:16)(cid:81)t1 k=1 ak (cid:17) (at bt) (cid:16)(cid:81)N j=t+1 bj (cid:17) . Applying this to the policy 13 Rethinking the Trust Region in LLM Reinforcement Learning probabilities and then using the triangle inequality, we get: 2DTV(P Q) (cid:88) (cid:32)t1 (cid:89) (cid:88) (cid:33) µ(yksk) µ(ytst) π(ytst) t=1 k=1 (cid:88) (cid:88) (cid:32)t1 (cid:89) t=1 k=1 = (cid:33) µ(yksk) µ(ytst) π(ytst) (cid:89) j=t+1 (cid:89) j=t+1 π(yjsj) π(yjsj) . For each term in the outer sum over t, we can sum over the variables yj for > t. Since (cid:80) yj product of terms for > sums to 1 when we integrate out yt+1, . . . , yN . This leaves: π(yjsj) = 1 for all sj, the 2DTV(P Q) (cid:88) (cid:88) (cid:32)t1 (cid:89) (cid:33) µ(yksk) µ(ytst) π(ytst) t=1 y1,...,yt (cid:88) = (cid:88) k=1 (cid:32)t1 (cid:89) t=1 y1,...,yt1 k=1 (cid:33) µ(yksk) (cid:88) yt µ(ytst) π(ytst). The inner sum is 2DTV(µ(st) π(st)). The outer sum over y1, . . . , yt1 defines an expectation over states st under policy µ. Thus, we have: 2DTV(P Q) (cid:88) t=1 Estµ (cid:2)2DTV (cid:0) µ(st) π(st)(cid:1)(cid:3) . Dividing by 2 yields the desired result. Proof of Theorem 3.2. From Lemma 3.1, we start with the exact performance difference identity: (π) (µ) = µ(π) (µ, π). For brevity, we define yt = {x, y1, . . . , yt} and y>t = {yt+1, yt+2, . . . }, then we can rewrite (µ, π) as: (µ, π) = Eyµ R(y) (cid:88) t=1 (cid:18) π(ytst) µ(ytst) (cid:19) (cid:18) 1 π(y>tst+1) µ(y>tst+1) (cid:19) . Our goal is to find an upper bound for the error term (µ, π). We begin by bounding the reward by its maximum absolute value, ξ = maxy R(y). (µ, π) ξ Eyµ (cid:34) (cid:88) t=1 (cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 π(y>tst+1) µ(y>tst+1) (cid:35) (cid:12) (cid:12) (cid:12) (cid:12) = ξ (cid:88) t=1 Eytµ (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:12) (cid:12) (cid:12) (cid:12) Ey>tµ(st+1) (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) 1 π(y>tst+1) µ(y>tst+1) (cid:21)(cid:21) . (cid:12) (cid:12) (cid:12) (cid:12) (17) The inner expectation is exactly twice the TV divergence between the distributions over future trajectories: Ey>tµ(st+1) (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) 1 π(y>tst+1) µ(y>tst+1) (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) = 2DTV (cid:0)µ>t(st+1)π>t(st+1)(cid:1). Using Theorem B.1 on this sequence-level TV divergence (for sequence of length t), we get: DTV (cid:0)µ>t(st+1)π>t(st+1)(cid:1) (cid:88) k=t+1 Eskµ(st+1) (cid:2)DTV (cid:0) µ(sk) π(sk)(cid:1)(cid:3) . Rethinking the Trust Region in LLM Reinforcement Learning We bound each term in the sum by the maximum single-step TV divergence, Dmax which gives: TV (µ π) = maxs DTV(µ(s) π(s)),"
        },
        {
            "title": "DTV",
            "content": "(cid:0)µ>t(st+1)π>t(st+1)(cid:1) (cid:88) k=t+1 Substituting this back into the bound for (µ, π):"
        },
        {
            "title": "Dmax",
            "content": "TV (µ π) = (T t)Dmax TV (µ π). (µ, π) ξ (cid:88) t=1 Eytµ (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:12) (cid:12) (cid:12) (cid:12) 2(T t)Dmax TV (µ π) (cid:21) = 2ξ Dmax TV (µ π) = 2ξ Dmax TV (µ π) 2ξ Dmax TV (µ π) (cid:88) t=1 (cid:88) t=1 (cid:88) t=1 (T t)Estµ (cid:34) (cid:88) yt µ(ytst) (cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) (cid:35) (cid:12) (cid:12) (cid:12) (cid:12) 1 (T t)Estµ [2DTV(µ(st) π(st))] (T t) Estµ [2Dmax TV (µ π)] = 4ξ Dmax TV (µ π)2 (cid:88) (T t) t= = 2ξT (T 1) Dmax TV (µ π)2. (18) Substituting this into the performance difference identity gives the desired result: (π) (µ) µ(π) 2ξT (T 1) Dmax TV (µ π)2. This completes the proof. B.3. Tighter Policy Improvement Bound The policy improvement bound derived in Equation (18) suffers from quadratic dependence on the horizon length, 2. This makes the bound excessively loose for typical LLM fine-tuning tasks where sequences can be very long. By leveraging the property that the total variation divergence is always bounded by one, i.e., DTV(P Q) 1, we can derive an alternative bound that is only linear in , offering much tighter and more practical guarantee for long-horizon problems. We begin from the intermediate step in Equation (17): (µ, π) ξ (cid:88) t=1 Eytµ (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) (cid:12) (cid:12) (cid:12) (cid:12) Ey>tµ(st+1) (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) 1 π(y>tst+1) µ(y>tst+1) (cid:21)(cid:21) . (cid:12) (cid:12) (cid:12) (cid:12) The inner expectation is exactly twice the TV divergence between the future trajectory distributions, TV (µ π), we now apply the simple 2DTV (cid:0)µ>t(st+1)π>t(st+1)(cid:1). Instead of bounding this term with 2(T t)Dmax 15 Rethinking the Trust Region in LLM Reinforcement Learning upper bound of 2: (µ, π) ξ (cid:88) t=1 Eytµ (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:12) (cid:12) (cid:12) (cid:12) 2DTV (cid:0)µ>t(st+1)π>t(st+1)(cid:1) (cid:21) 2ξ = 2ξ = 2ξ (cid:88) t=1 (cid:88) t=1 (cid:88) t=1 Eytµ (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) Estρµ Eytµ(st) (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) π(ytst) µ(ytst) 1 (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) Estρµ [2DTV(µ(st) π(st))] = 4ξ Eyµ (cid:88) DTV(µ(st) π(st)) . (19) (20) This provides bound that is linear in the expected sum of single-step divergences. By combining this with our original quadratic bound from Equation (18), we can form tighter, composite bound by taking the minimum of the two: t=1 (π) (µ) µ(π) (µ, π) 2ξT (T 1) Dmax TV µ(π) min 2, 4ξ Eyµ (cid:88) DTV(µ(st) π(st)) . t=1 This composite bound provides more robust guarantee on policy improvement, leveraging the quadratic bound for infinitesimal updates and the linear bound for larger updates or longer horizons. B.4. Comparing Surrogate Objectives with Classical RL At first glance, the surrogate objective for the LLM regime in Equation (5) appears distinct from the classical RL surrogate in Equation (3). The former is an expectation over full trajectories weighted by the reward R(y), while the latter is an expectation over state-action pairs (s, a) weighted by the advantage Aµ(s, a). However, we will now show that their gradients with respect to the policy parameters θ are fundamentally analogous, confirming that our LLM-specific formulation is valid adaptation of the standard policy gradient theorem. Let the policy π be parameterized by θ. We will use the identity θπθ(as) = πθ(as)θ log πθ(as). Gradient of the Classical Surrogate Objective. We begin with the classical surrogate objective from Equation (3): Lµ(πθ) = 1 1 γ Esρµ, aµ(as) (cid:20) πθ(as) µ(as) (cid:21) Aµ(s, a) . Taking the gradient with respect to θ and moving it inside the expectation, we get: θLµ(πθ) = = 1 1 γ 1 1 γ Esρµ, aµ(as) Esρµ, aµ(as) (cid:20) θ πθ(as) µ(as) (cid:20) πθ(as) µ(as) (cid:21) Aµ(s, a) (cid:21) θ log πθ(as)Aµ(s, a) . (21) Gradient of the LLM Surrogate Objective. Next, we consider our LLM-specific surrogate from Equation (5): µ(πθ) = Eyµ R(y) (cid:88) t=1 (cid:18) πθ(ytst) µ(ytst) (cid:19) 1 . 16 Rethinking the Trust Region in LLM Reinforcement Learning Taking the gradient with respect to θ and noting that the 1 term has zero gradient: θL µ(πθ) = Eyµ R(y) (cid:88) t=1 θ πθ(ytst) µ(ytst) = Eyµ (cid:88) t=1 θ log πθ(ytst)R(y) . πθ(ytst) µ(ytst) If we define sequence-level advantage as Aµ(st, yt) = R(y) (x), where (x) is baseline value function for the prompt, the gradient becomes: θL µ(πθ) = Eyµ (cid:88) t=1 πθ(ytst) µ(ytst) θ log πθ(ytst)Aµ(st, yt) . (22) This form is directly analogous to the classical policy gradient in Equation (21), where the sum over timesteps in trajectory replaces the expectation over the state distribution ρµ. Thus, our LLM surrogate objective is theoretically sound adaptation of the classical trust region framework to the undiscounted, sequence-reward setting. C. Approximations as Lower Bounds of True Divergence In this section, we provide formal justification for our Binary and Top-K divergence approximations. We demonstrate that both are principled lower bounds on the true divergence and explicitly state the conditions under which these approximations become exact. Let = {C1, . . . , Cm} be any partition of the vocabulary A. Our Binary and Top-K approximations correspond to specific choices of this partition. We will show that the divergence computed on the partitioned space is lower bound on the true divergence. C.1. Total Variation Divergence (cid:80)m TV = 1 2 j=1 µ(Cjst) π(Cjst). The true TV divergence is DTV(µ π) = 1 2 DC Proof of Lower Bound. By definition, µ(Cjst) π(Cjst) = (cid:80) (µ(ast) π(ast)). The triangle inequality states that the absolute value of sum is less than or equal to the sum of the absolute values. Applying this, we get (cid:80) aA µ(ast) π(ast). The divergence on partitioned space is µ(ast) π(ast). Summing over all partitions j: (µ(ast) π(ast)) (cid:80) aCj aCj aCj (cid:80) DC TV = 1 2 1 (cid:88) j=1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) aCj (µ(ast) π(ast)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) µ(ast) π(ast) = DTV(µ π). j=1 aCj Thus, DTV(µ π) DC TV. This holds for both Binary and Top-K partitions. Analysis of the Approximation Gap. The gap between the true and approximated TV divergence is the sum of the gaps (cid:16)(cid:80) (cid:17) within each partition. For any partition Cj, the gap is 1 . 2 This gap is bounded by the total probability mass of the partition: (cid:12) (cid:12) (µ(ast) π(ast)) (cid:12) µ(ast) π(ast) aCj aCj (cid:80) (cid:12) (cid:12) (cid:12) Gap(Cj) 1 2 (cid:88) aCj (µ(ast) + π(ast)) = 1 2 (µ(Cjst) + π(Cjst)). For the Top-K approximation, the only partition with potential gap is the other category, which contains the tail of the distribution. The total probability mass of this tail, µ(Cotherst), is typically very small. Therefore, the approximation gap is also small, justifying Top-K TV as high-fidelity approximation. Rethinking the Trust Region in LLM Reinforcement Learning Equality Condition. Equality DTV = DC has the same sign for all tokens within each partition Cj. TV holds if the gap is zero for all partitions. This occurs when µ(ast) π(ast) C.2. KL Divergence The true KL divergence is DKL(µ π) = (cid:80) (cid:80)m j=1 µ(Cjst) log µ(Cj st) π(Cj st) . aA µ(ast) log µ(ast) π(ast) . The divergence on the partitioned space is DC KL = Proof of Lower Bound. The proof relies on the log-sum inequality, which states that for any two sets of non-negative numbers {x1, . . . , xn} and {y1, . . . , yn}: (cid:88) i=1 xi log xi yi (cid:32) (cid:88) i=1 (cid:33) xi log (cid:80)n (cid:80)n i=1 xi i=1 yi . We apply this inequality to each partition Cj in our vocabulary, setting xa = µ(ast) and ya = π(ast): (cid:88) aCj µ(ast) log µ(ast) π(ast) (cid:88) aCj µ(ast) log (cid:80) (cid:80) aCj aCj µ(ast) π(ast) Summing over all partitions gives the desired result: = µ(Cjst) log µ(Cjst) π(Cjst) . DKL(µ π) = (cid:88) (cid:88) j=1 aCj µ(ast) log µ(ast) π(ast) (cid:88) j=1 µ(Cjst) log µ(Cjst) π(Cjst) = DC KL. Equality Condition. The log-sum inequality holds with equality if and only if the ratio xi is constant for all i. In our yi context, this means that for each partition Cj, the ratio µ(ast) π(ast) must be constant for all tokens Cj. For both Binary and Top-K approximations, this implies the policy update must scale the probabilities of all tokens within the other category by uniform factor. D. More Details for Stability Analysis Our experimental setup strictly follows the sanity test established in Qi et al. (2025b). Each policy iteration begins by sampling batch of 64 questions. For each question, we generate 8 responses (rollouts) using maximum context length of 8,000. The collected data is then used to perform 4 gradient steps. All experiments are conducted using the VeRL framework (Sheng et al., 2024) together with the ODC optimization (Wan et al., 2026), and models are trained in BFloat16 precision to better expose potential numerical instabilities between algorithms. For the evaluation on AIME, we sample 32 responses for each test question to ensure robust assessment. D.1. Algorithmic Details for Stability Analysis In this section, we provide the specific policy gradient formulations for each algorithm evaluated in our stability analysis (Section 5). To facilitate direct comparison, we show how each algorithms gradient update can be interpreted through the lens of single, unified framework. Unified Policy Gradient Formulation. The policy gradient for the algorithms we tested can be generalized into the following form, where the gradient of the objective L(θ) is expressed as: θL(θ) = Eyµθ (cid:88) t=1 Mt min (cid:18) πθ(ytst) µθ(ytst) (cid:19) , ˆAt θ log πθ(ytst) . (23) 18 Rethinking the Trust Region in LLM Reinforcement Learning In this formulation, ˆAt is the advantage, estimated following the GRPO method but without standard deviation normalization (Shao et al., 2024; Liu et al., 2025d). The algorithms differ primarily in their definition of the binary mask Mt and the clipping bound C. For PG-IS, we have Mt = 1 and = . For PG-TIS (CISPO), we have Mt = 1 and = 3. For GRPO, the mask Mt is the PPO-style clipping mask, and = . For MiniRL, the mask Mt is also PPO-style clipping mask but is conditioned on recomputed policy ratio. For this algorithm, = . For MiniRL-TIS, the mask Mt is the same as in MiniRL, but with = 3. For DPPO (Ours), the mask Mt is conditioned on the policy divergence, and = . Mask Definitions. The specific forms of the masks are as follows: For GRPO, the mask uses the rollout ratio rt = πθ(ytst) µθ (ytst) and experimental hyperparameters ϵhigh = 0.28, ϵlow = 0.2: (cid:40) Mt = if ( ˆAt > 0 and rt > 1 + ϵhigh) or ( ˆAt < 0 and rt < 1 ϵlow) 0, 1, otherwise. For MiniRL and MiniRL-TIS, the mask is structurally identical to GRPOs and uses the same hyperparameters, but it is conditioned on the recomputed ratio = πθ(ytst) πθ (ytst) : (cid:40) Mt = if ( ˆAt > 0 and 0, 1, otherwise. > 1 + ϵhigh) or ( ˆAt < 0 and < 1 ϵlow) For DPPO, our mask is conditioned on the policy divergence Dt: (cid:40) Mt = 0, 1, otherwise. if ( ˆAt > 0 and rt > 1 and Dt > δ) or ( ˆAt < 0 and rt < 1 and Dt > δ) In our experiments, we set the divergence threshold δ = 0.15 for TV divergence and δ = 0.05 for KL divergence. E. Characterizing Clipped Tokens To understand the practical consequences of ratio clipping, we analyzed which tokens are most frequently penalized by PPO-style algorithm. We trained Qwen3-4B-Base model on the DAPO dataset with GRPO and, at training step 50, collected two sets of tokens: Clipped Positive Tokens: From samples with ˆAt > 0, tokens whose updates were blocked due to high ratio (rt > 1.28). Clipped Negative Tokens: From samples with ˆAt < 0, tokens whose updates were blocked due to low ratio (rt < 0.8). The 50 most frequent tokens in each category reveal striking pattern. Far from being random noise, the clipped tokens are often critical for task performance. The lists for both positive and negative samples are dominated by two key categories: 1. Numerical and Mathematical Tokens: significant portion of the clipped tokens are numbers (e.g., 1, 4) and mathematical symbols (e.g., +, =, div). 19 Rethinking the Trust Region in LLM Reinforcement Learning 2. Reasoning and Structural Words: The list also includes many words essential for logical exposition, such as Wait, Next, Thus, and Since. These findings highlight fundamental flaw in ratio-based clipping. For positive samples, it blocks beneficial updates to tokens that are integral to constructing correct solutions. For negative samples, it blocks the necessary suppression of these same tokens when they are part of an incorrect reasoning path. By systematically interfering with the learning signal for these high-utility tokens, the algorithm inadvertently slows learning, stifles exploration, and hinders the models ability to refine its problem-solving capabilities. The 50 most frequently clipped tokens from positively-rewarded samples. the, (, 1, Let, in, , ,, We, +, , numbers, :nn, Wait, 4, 6, Identify, (, Next, from, ), k, -, Since, solve, [, how, ->, to, are, Sub, I, ):n, nn, spiral, Instead, this, If, div, Conditions, vector, have, =, feasible, Or, inconsistency, express, _{, increase, exact, consider The 50 most frequently clipped tokens from negatively-rewarded samples. (, the, ,, a, , , 2, 1, :nn, 0, 3, and, (, that, -, to, 5, of, However, , is, =, 4, in, for, all, we, We, ), .nn, our, ., :n, but, with, So, both, From, Let, this, Thus, Wait, if, -, +, ˆ, only, at, Since, integer F. More Details for Scaling Experiments In this section, we provide detailed training and evaluation settings of the scaling experiments in Section 7. Training Settings. We conduct experiments using the VeRL framework (Sheng et al., 2024) on NVIDIA Series GPUs. All methods follow the hyperparameter configurations detailed in Table 1. Rollout router replay (R3) (Ma et al., 2025) records the routed experts used in the inference engine and replays them in the training engine, which mitigates the training-inference mismatch and stabilizes RL training for MoE models. We only use R3 in the MoE Base w/ R3 experiment and do not use it in all other experiments. For experiments that utilize LoRA, as suggested by Schulman & Lab (2025), we employ larger learning rate of 1 105. For the MoE Base w/ LoRA experiment, we set lora rank=32 and lora alpha=64. As suggested in Section 5.2, for all methods, we use the behavior policy (µθ) instead of recomputed policy distribution (πθ) to construct the trust region (i.e., for clipping or masking). Under the unified policy gradient formulation (Equation 23), the method-specific hyperparameters (C = 5 by default) are configured as follows: For GRPO-ClipHigher, we have (cid:40) Mt = 0, 1, otherwise. if ( ˆAt > 0 and rt > 1 + ϵhigh) or ( ˆAt < 0 and rt < 1 ϵlow) where ϵhigh = 0.27 and ϵlow = 0.2, which follows the hyperparameters used in Zheng et al. (2025). For CISPO, we have Mt = 1. For DPPO-Binary-KL and DPPO-Binary-TV, we have (cid:40) Mt = 0, 1, otherwise. if ( ˆAt > 0 and rt > 1 and Dt > δ) or ( ˆAt < 0 and rt < 1 and Dt > δ) where Dt is binary approximation of KL or TV as defined in Section 4.4. For DPPO-Binary-KL, δ = 0.05 for all scaling experiments. For DPPO-Binary-TV, we use δ = 0.15 for MoE Base w/ LoRA experiment and δ = 0.2 for all other scaling experiments. Rethinking the Trust Region in LLM Reinforcement Learning Table 1. Detailed RL training hyperparameters of scaling experiments. Hyperparameters MoE Base MoE Base w/ R3 MoE Thinking Dense Base MoE Base w/ LoRA max prompt length max response length train batch size ppo mini batch size optim.lr rollout.temperature rollout.n 1024 16384 256 32 1e-6 1.0 1024 16384 256 32 1e-6 1.0 16 1024 16384 256 32 1e-6 1.0 16 1024 8000 128 32 1e-6 1.0 8 1024 8000 128 16 1e-5 1.0 8 Detailed Results Figure Figure 12 Figure 13 Figure 14 Figure 15 Figure 11. Evolution of metrics for MoE Base w/o R3 experiment (based on Qwen3-30B-A3B-Base, without rollout router replay). Evaluation Settings. We perform online evaluation for each method and experimental configuration, monitoring AIME24 and AIME25 scores throughout RL training. Evaluations are conducted every 5 training steps for MoE Base, MoE Base w/ R3, and MoE Thinking, and every 10 steps for Dense Base and MoE Base w/ LoRA. Across all scaling experiments, we use consistent sampling parameters: temperature=0.7, top p=0.95, and n=32. The n=32 setting indicates that each question from AIME24 and AIME25 is sampled 32 times, and we report the average scores. The max response length remains identical to that used during training rollouts. G. More Empirical Results G.1. Extended Main Results In addition to the results provided in Section 7, here we provide more detailed results of the five scaling experiments: Figure 11 for MoE Base w/o R3, Figure 12 for MoE Base w/ R3, Figure 13 for MoE Thinking, Figure 14 for Dense Base, Figure 15 for MoE Base w/ LoRA. We record the following metrics throughout the RL training: training rewards (denoted as Rewards), AIME 2024 Avg@32 scores, AIME 2025 Avg@32 scores, mean of µθ πθ (denoted as Mean of π µ), mean of the response length (denoted as Response Length), and mean of token entropy (denoted as Entropy). For clearer visualization, all metrics except AIME24 and AIME25 are smoothed using Gaussian filter with standard deviation σ = 2. The original unsmoothed curves are shown in the background as shaded regions. 21 Rethinking the Trust Region in LLM Reinforcement Learning Figure 12. Evolution of metrics for MoE Base w/ R3 experiment (based on Qwen3-30B-A3B-Base, with rollout router replay). Overall, across the five experiments, our method DPPO demonstrates consistent and robust improvements in training rewards, highlighting its stability and efficiency. On both AIME 24 and AIME 25 benchmarks, DPPO exhibits clear, stable upward trend during training and maintains superior performance after convergence. The stability of our approach is evidenced by learning curves that generally show less fluctuation compared to baseline methods. Its efficiency is reflected in the rapid increase of training rewards and the strong final performance. DPPO variants consistently demonstrate healthy training dynamics. The training-inference mismatch (measured by the mean absolute deviation π µ) and policy entropy remain within stable, proper region throughout RL training. DPPO also effectively increases the generated response length across all scaling experiments, except for MoE Thinking. We note that the model Qwen3-30B-A3B already produces extremely long responses; as our training enforces maximum length of approximately 16k tokens, RL training naturally shortens responses to fit this constraint. In contrast, the GRPO-ClipHigher baseline, which relies on the ratio clipping mechanism of PPO, shows lower stability than DPPO and achieves inferior final performance in all five large-scale experiments. For example, in MoE Base w/o R3 (see Figure 11), GRPO-ClipHigher, though more stable than CISPO, improves more slowly and converges to lower training rewards and AIME scores than DPPO. In MoE Thinking (see Figure 13), GRPO-ClipHigher suffers significant training collapse. Notably, GRPO-ClipHigher consistently leads to excessively high entropy in all large-scale experiments, phenomenon not observed with other methods. The CISPO baseline, which retains gradients for all tokens, is generally less stable and prone to collapse in certain settings. For instance, in MoE Base w/o R3 (see Figure 11), CISPO experiences sudden and severe collapse leading to complete failure. In Dense Base (see Figure 14), CISPO shows degenerative trend, particularly on AIME25. In MoE Base w/ LoRA (see Figure 15), the AIME24 scores, mean of π µ, and response length exhibit noticeable fluctuations, further indicating instability. We also analyze the effect of rollout router replay (R3). Remarkably, DPPO variants without R3 already outperform baselines that use R3, underscoring the importance of proper masking mechanism in RL training (see Figures 11 and 12). Furthermore, incorporating R3 yields additional gains for DPPO, suggesting that the benefits of R3 and DPPO are largely orthogonal. This implies that DPPO provides robust foundation for LLM RL fine-tuning, capable of further improvement even when training-inference mismatch is mitigated by other techniques. 22 Rethinking the Trust Region in LLM Reinforcement Learning Figure 13. Evolution of metrics for MoE Thinking experiment (based on Qwen3-30B-A3B). Figure 14. Evolution of metrics for Dense Base experiment (based on Qwen3-8B-Base). 23 Rethinking the Trust Region in LLM Reinforcement Learning Figure 15. Evolution of metrics for MoE Base w/ LoRA experiment (based on Qwen3-30B-A3B-Base, with LoRA). G.2. Ablation on TV/KL Approximation In the scaling experiments, we compared DPPO variants using binary TV/KL approximations (Equations 13 and 14) against several baselines. To further investigate the approximation strategy, we experiment with DPPO variants with top-K TV/KL approximations (Equations 15 and 16), where we set = 20; these variants are denoted as DPPO-TopK-TV and DPPO-TopK-KL. The choice = 20 is limited by vLLM (Kwon et al., 2023), which supports returning log probabilities for at most 20 candidate tokens per step. We strictly replicate the experimental setting of MoE Base w/o R3. As in the main scaling experiments, for DPPO-Binary-TV and DPPO-TopK-TV we set the clip threshold δ = 0.2, while for DPPO-Binary-KL and DPPO-TopK-KL we set δ = 0.05. As presented in Figure 16, introducing the top-K approximation does not yield significant performance gains, indicating that the simpler binary approximation already provides sufficient and efficient proxy for constructing the trust region. This finding is encouraging, suggesting that DPPO with binary TV/KL remains highly scalable without sacrificing effectiveness. G.3. Extended Results for Different Model Task Combinations Besides experimental results presented in Section 7, we evaluate DPPO on more model task settings to validate its advantage over the GRPO baseline. The settings we considered include: 1. Different model family. Training on new model different from the Qwen family, OctoThinker-3B-Hybrid-Base (Wang et al., 2025b), on the standard math reasoning dataset (Hendrycks et al., 2021). 2. Abstract reasoning and induction. Training the Qwen3-1.7B-Base model on abstract reasoning task (Arc1D) and induction task (Acre) from the Gem library (Liu et al., 2025e). 3. Multi-turn reasoning. Training the Qwen3-1.7B-Base model on the multi-turn reasoning environment (Sudoku-v0easy) from Gem the library (Liu et al., 2025e). The training is conducted using Oat (Liu et al., 2025c) with their example scripts (thereby the standard hyper-parameters) for math RL and Gem RL. For the TV divergence clipping, we use threshold of δ = 0.2. Figure 17 shows the comparison between the TV variant of DPPO and the vanilla ratio-based PPO, both based on the GRPO algorithmic framework with the only difference being the trust region masking strategy. We can observe DPPO improves the efficiency (and sometimes asymptotic performance) over the baseline across different settings, validating its general effectiveness. 24 Rethinking the Trust Region in LLM Reinforcement Learning Figure 16. Evolution of metrics for baselines, DPPO with binary TV/KL approximation, and DPPO with Top-K (K=20) approximation under the same setting as MoE Base w/o R3. Figure 17. Learning curve comparison of using ratio (PPO-Ratio) and TV divergence (DPPO-Binary-TV) for the trust region clipping."
        }
    ],
    "affiliations": [
        "School of Computing, National University of Singapore",
        "Sea AI Lab, Singapore"
    ]
}