{
    "paper_title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language",
    "authors": [
        "Guilherme Penedo",
        "Hynek Kydlíček",
        "Vinko Sabolčec",
        "Bettina Messmer",
        "Negar Foroutan",
        "Amir Hossein Kargaran",
        "Colin Raffel",
        "Martin Jaggi",
        "Leandro Von Werra",
        "Thomas Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains a challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to a large number of languages. In this work, we introduce a new pre-training dataset curation pipeline based on FineWeb that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on a set of nine diverse languages, guided by a set of meaningful and informative evaluation tasks that were chosen through a novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create non-English corpora that produce more performant models than prior datasets. We additionally introduce a straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases."
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. FineWeb2: One Pipeline to Scale Them All Adapting Pre-Training Data Processing to Every Language Guilherme Penedo Hynek Kydlıˇcek Negar Foroutan Leandro Von Werra"
        },
        {
            "title": "Amir Hossein Kargaran",
            "content": "Vinko Sabolˇcec"
        },
        {
            "title": "Colin Raffel Martin Jaggi",
            "content": "5 2 0 2 6 2 ] . [ 1 0 2 9 0 2 . 6 0 5 2 : r a"
        },
        {
            "title": "EPFL",
            "content": "Pipeline code: https://github.com/huggingface/fineweb-2 FineWeb2 dataset: https://hf.co/datasets/HuggingFaceFW/fineweb-"
        },
        {
            "title": "Abstract",
            "content": "Pre-training state-of-the-art large language models (LLMs) requires vast amounts of clean and diverse text data. While the open development of large high-quality English pre-training datasets has seen substantial recent progress, training performant multilingual LLMs remains challenge, in large part due to the inherent difficulty of tailoring filtering and deduplication pipelines to large number of languages. In this work, we introduce new pre-training dataset curation pipeline based on FineWeb (Penedo et al., 2024) that can be automatically adapted to support any language. We extensively ablate our pipeline design choices on set of nine diverse languages, guided by set of meaningful and informative evaluation tasks that were chosen through novel selection process based on measurable criteria. Ultimately, we show that our pipeline can be used to create nonEnglish corpora that produce more performant models than prior datasets. We additionally introduce straightforward and principled approach to rebalance datasets that takes into consideration both duplication count and quality, providing an additional performance uplift. Finally, we scale our pipeline to over 1000 languages using almost 100 Common Crawl snapshots to produce FineWeb2, new 20 terabyte (5 billion document) multilingual dataset which we release along with our pipeline, training, and evaluation codebases."
        },
        {
            "title": "Introduction",
            "content": "One of the main drivers of the improving capabilities of large language models (LLMs) is increased scale, in terms of both model and pre-training dataset size. To satiate the ever-growing hunger for text data, most pretraining datasets include large amounts of text scraped from the public internet (Raffel et al., 2020; Penedo et al., 2023; 2024). Consequently, pre-training data tends to be most readily available in the high-resource languages (English, Chinese, etc.) that are most prevalent on the internet. Since LLM capabilities largely stem from the data they were trained on (Grosse et al., 2023; Roberts et al., 2020; Razeghi et al., 2022), this has resulted in language models having better performance on high-resource languages. Furthermore, commercial and open language model development frequently only targets these languages (Grattafiori et al., 2024; Jiang et al., 2024; 01.AI et al., 2025). This state of affairs leaves the majority of the worlds population Figure 1: The FineWeb2 pipeline: Evaluation results of models trained on 350 billion tokens show that each pipeline step Language Identification (LID), Deduplication (Dedup), Filtering, and Dedup-informed upsampling (Rehydration) improves performance. Correspondence to guilherme at huggingface dot co 1 Preprint. Under review. (speaking over 7,000 languages (Eberhard et al., 2024)) unable to interact with state-of-the-art LLMs in their native tongue. Why not just curate datasets in underrepresented languages and train LLMs on them? Putting aside the possible lack of data (recent LLM training runs typically require trillions of tokens (AI@Meta, 2024; DeepSeek-AI et al., 2024)), key challenge is that high-resource languages benefit from the existence of well-tuned and battle-tested data processing and curation pipelines, whereas low-resource languages face vastly different landscape: evaluating corpora quality, ensuring accurate language identification, customizing filtering recipes, and even separating words can be major challenges for many languages. While some past work has successfully curated single-language pre-training datasets and used them to produce strong language-specific models (Carmo et al., 2020; de Vries et al., 2019; Le et al., 2020; Martin et al., 2020; Delobelle et al., 2020; Luukkonen et al., 2023; PLLuM Consortium, 2025; Pipatanakul et al., 2023, etc.), hand-designing different pipeline for each language does not scale. Consequently, most past work on multilingual datasets (e.g. Xue et al., 2021; Wenzek et al., 2020; De Gibert et al., 2024) has used (mostly) fixed pipeline across all languages. This one-size-fits-all approach risks applying inappropriate filtering to different languages, obviating the goal of creating performant data in many languages. In this work, we introduce new data processing pipeline based on the approach used for the state-of-the-art English pre-training dataset FineWeb (Penedo et al., 2024). Importantly, our pipeline can be automatically adapted based on language-specific statistics to produce high-quality pre-training corpora in any language. We follow data-driven approach and validate our design choices by running extensive ablation experiments where we train monolingual models on set of nine diverse languages and evaluate on tasks chosen through novel selection process based on measurable criteria that ensure meaningful signal. In addition, we introduce straightforward and principled approach to rebalance datasets using the original duplication counts and quality signals that allows globally neardeduplicated datasets to obtain performance uplift. Ultimately, we show that models trained on language-specific corpora produced by our pipeline perform better than those trained on other public web-based multilingual datasets by training models on additional unseen languages that were not used to inform pipeline design decisions. Finally, we use our pipeline to process almost 100 Common Crawl1 snapshots spanning the summer of 2013 to April 2024 to create FineWeb2, new 20 terabyte (5 billion document) dataset covering over 1000 languages. FineWeb2 is released under the permissive ODC-By License, and we additionally release the pipeline, training, and evaluation codebases, as well as the preliminary version of the dataset obtained after the deduplication stage, to facilitate further research on multilingual pre-training dataset curation."
        },
        {
            "title": "2 Preliminaries",
            "content": "Before detailing our dataset creation process, we first establish critical considerations that arise when dealing with massively multilingual data. Notation When considering thousands of languages, its important to have an unambiguous way of referring to languages and scripts. In our work we identify languages by their official ISO-639-3 codes2 which cover significantly more languages than the commonly used ISO-639-1 codes (such as en, zh, etc). As many languages use multiple writing systems (scripts), we optionally designate individual languages by (ISO-639-3 language code, ISO 15924 script code) pair. For instance, arb Arab is Standard Arabic in Arabic script, while arb Latn is Standard Arabic in Latin script. Separating words Many parts of our processing and evaluation pipeline require the ability to separate (tokenize) text into individual words. For example, we rely on word tokenization when we filter documents based on the ratio of words that have given property, when selecting n-grams for deduplication, or even when evaluating generative tasks. While 1https://www.commoncrawl.org/ 2https://iso639-3.sil.org/code tables/639/data 2 Preprint. Under review. whitespace and punctuation often mark word boundaries, many writing systems use different boundary markers or have no visible markers at all (Daniels & Bright, 1996). This is particularly common in Southeast Asian languages, as well as Chinese, Japanese, and Korean. Therefore, word tokenizers/segmentators tailored to each language and script are needed. We collected large number of tokenizers from SpaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), as well as from libraries targeting specific languages (or language groups). We then assigned proxy tokenizers based on the closest language according to language family data from the Ethnologue3 to languages without native word tokenizer. For more details on this process, see Appendix A.1. These tokenizer assignments were crucial to adapt filtering, deduplication, and evaluation setups to thousands of languages."
        },
        {
            "title": "3 Experimental setup",
            "content": "To compare and validate pipeline design choices, we followed an experimental setup similar to Penedo et al. (2024). Specifically, to assess data quality, we relied on training small models and evaluating them on early-signal benchmark tasks, i.e., tasks where models perform reasonably well after only few tens of billions or hundreds of billions of training tokens, allowing us to confidently establish comparisons between them. For each processing step, we conducted comparative evaluations using two identical models that differed only in their training data: one model was trained on data with the processing step applied, while the other used the unprocessed (ablated) version. By keeping all other variables constant (number of parameters, architecture, tokenizer, and training token count), we could isolate the impact of each data processing step on downstream model quality. While ideally we would have tested each processing step across every language, computational constraints and the lack of evaluation tasks for many of the languages made this impractical. We therefore chose to conduct our experiments on select set of nine canary languages (i.e. test languages): Arabic, Chinese, French, Hindi, Russian, Swahili, Telugu, Thai, and Turkish. Testing across these languages allowed us to evaluate the impact of each design decision across different language families, scripts, and levels of resource availability, while keeping computational requirements manageable. These details are available on Table 1, where Resource Availability was determined following Joshi et al. (2020). We trained separate models per language, rather than single multilingual model, to avoid introducing confounders between languages. This means that for every ablation experiment or validation run reported in this paper, we trained nine different models (one per language)."
        },
        {
            "title": "3.1 Tokenizer and model architecture",
            "content": "Tokenizer The choice of tokenizer can induce differential downstream model performance across different languages based on how compactly it maps given languages words into tokens (Mielke et al., 2021). Given that our experiments target different languages and, in particular, different scripts, we evaluated the subword fertility and proportion of continued words (Rust et al., 2021) of different existing open-source tokenizers from leading multilingual LLMs on our nine canary languages. Concretely, we split text from each languages Wikipedia into individual real words using our word-level tokenizers (discussed in Appendix A.1) and then measured the average number of tokens per word for each tokenizer. From the tokenizers that showed reasonable fertility on our nine canary languages, we chose the tokenizer used in Gemma (Gemma Team et al., 2024), modern tokenizer with vocabulary size of around 250,000 tokens that showed better average fertility than similarly sized tokenizers. Detailed results are available in Appendix A.3. Model architecture We used similar model architecture setup to Penedo et al. (2024), with reduced number of layers given the additional embedding parameters due to the larger vocabulary size. All models used in our experiments were trained using the nanotron training framework, and followed the Llama (Touvron et al., 2023) architecture with 14 layers, 3https://www.ethnologue.com/browse/families/ 3 Preprint. Under review. 32 attention heads, length-2048 sequences, and tied embeddings, for total of 1.46 billion parameters. Further details and training hyperparameters are provided in Appendix A.4."
        },
        {
            "title": "3.2 Baseline datasets",
            "content": "We selected existing widely used multilingual datasets to use as comparison baselines. For each language, we trained one model on language-specific data from each reference dataset: CC-100 (Wenzek et al., 2020; Conneau et al., 2020), mC4 (Xue et al., 2021), CulturaX (Nguyen et al., 2024), and HPLT (de Gibert et al., 2024). We additionally trained multiple models on raw Common Crawl data (after text extraction and Language Identification, but without any additional filtering or deduplication). Unfortunately, all datasets except raw Common Crawl only contained limited amount of data for Telugu and Swahili, and only CulturaX and HPLT had enough data for pre-training run in Hindi at 30 billion tokens without requiring an excessive number of epochs over the training data."
        },
        {
            "title": "3.3 Selecting evaluation (Fine)tasks",
            "content": "The selection of English evaluation tasks is straightforward due to the existence of wellestablished benchmarks such as MMLU (Hendrycks et al., 2021) or HellaSwag (Zellers et al., 2019), which are widely used and supported by all major evaluation frameworks. The situation is significantly different for non-English languages, which often lack evaluation tasks. When available, these tasks often lack broader community validation and suffer from quality issues many are machine-translated and may even include English words in their formulations (Artetxe et al., 2020b). Additionally, we find that non-English tasks are often unsuitable for early pre-training evaluation due to suboptimal task formulations and/or excessive difficulty that results in random-level performance. To identify informative evaluation tasks, we established four key criteria for what we call early-signal tasks: Monotonicity the performance of models evaluated on this task should improve as training progresses, though possibly at different rates depending on the pretraining dataset; Low noise when comparing models trained on different datasets, we want to ensure that the relative performance differences between them are due to inherently better training data, and not due to evaluation noise; Non-random performance early in training tasks reflecting model capabilities that are only acquired later in training are not informative for small scale pre-training ablations, as near-random scores cannot meaningfully differentiate between datasets; Ordering consistency if model outperforms model B, then falls behind, then leads again within short span of training steps, we cannot confidently determine which model (and, correspondingly, dataset variant) is superior and we therefore need tasks that provide consistent relative performance. We defined quantitative metrics to measure these characteristics and applied them to hundreds of candidate zero-shot evaluation tasks targeting our 9 canary languages on the models trained on our baseline datasets. See Appendix A.5 for the precise definition of early-signal tasks and additional description of our evaluation setup. We strove to cover different task types in all languages: Reading Comprehension, RC; General Knowledge, GK; Natural Language Understanding, NLU; and Common-Sense Reasoning, CR. Our in-depth analysis of existing evaluation tasks resulted in final suite of 84 selected benchmarks out of 197 tested across our nine canary languages. We list all the tasks and employed metrics in Appendix A.5.3. To produce an aggregate score across tasks, we follow the approach used by Fourrier et al. (2024); Li et al. (2024b) and average scores across tasks after first rescaling scores based on the random baseline any score below the random baseline is considered 0, and for the remaining scores we subtract the random baseline value and shift the scores as new score = (score random baseline)/(1 random baseline). As some languages might have an unbalanced number of tasks for each task category (RC, GK, NLU and CR), during score averaging we first average within categories themselves and then take the average of each category. This per-category macro-average score is our final reported aggregate score. Preprint. Under review."
        },
        {
            "title": "4.1 Starting point: FineWeb",
            "content": "We started by applying the first few processing steps used in the creation of the English-only FineWeb dataset (Penedo et al., 2024): downloaded WARC (web archive) files from all available (almost 100) CommonCrawl snapshots, applied URL filtering using blocklist to remove adult content (an approach discussed in Penedo et al. (2023)), and used trafilatura (Barbaresi, 2021) to extract text content from the HTML in the WARC files. We then aimed to adapt the remaining components of the FineWeb pipeline filtering and deduplication starting with all the data that was excluded during FineWebs language filtering step (which uses the FastText language identifier (Joulin et al., 2016) to identify English text with threshold of 0.65). Since approximately 40% of all documents met the FineWeb English language threshold, our starting point for FineWeb2 comprises the remaining 60% of all the text extracted from CommonCrawl content."
        },
        {
            "title": "4.2 Language Identification (LID)",
            "content": "A critical first step for curating multilingual dataset from web scrapes is accurately identifying the main language of each document. The choice of Language Identification (LID) tool determines not only how reliably each language (label) is predicted, but also the set of identifiable languages if the LID does not have label for specific language, then its content will either be removed or misclassified as some other language. Additionally, as LID classifiers usually assign confidence score to each prediction, the choice of filtering thresholds further affects the amount of data retained, as well as its quality, as LID confidence can often be correlated with the noisiness of given document (NLLB Team et al., 2022). Choice of classifier While Transformer-based LID classifiers exist (Bapna et al., 2022), they are too slow and expensive to run at large scale. Most commonly used LID classifiers are simple character level n-gram models, including CLD3 (Salcianu et al., 2018) (107 supported languages, used in mC4 (Xue et al., 2021)) and classifiers following the fastText architecture (Joulin et al., 2016), such as FT1764 (176 languages, used in CC-100 (Wenzek et al., 2020; Conneau et al., 2020) and CulturaX (Nguyen et al., 2024)), OpenLID (Burchell et al., 2023) (193 languages, used in HPLT2 (Burchell et al., 2025)), and the recent GlotLID (Kargaran et al., 2023) (1880 languages). Although FineWeb Penedo et al. (2024) used FT176, using GlotLID would allow us to support much larger number of languages, as well as to run separate processing for different scripts of the same language, as GlotLID explicitly separates them. Additionally, it includes special labels for non supported scripts and for common formats of noise documents, preventing this content from being classified as one of the other languages. While GlotLID reports strong performance on language classification benchmarks and supports large number of languages, we are primarily interested in the downstream model quality resulting from using given LID tool. Therefore, for each canary language we trained one model on documents classified as this language (regardless of confidence) by FT176 and another based on GlotLID. We then evaluated the models on our set of evaluation tasks and found that GlotLID outperforms FT176  (Fig. 5)  on higher resource languages while being slightly behind on lower resource languages. We consider the increased language coverage to make up for this difference and employ GlotLID for our pipeline. See Appendix A.6.1 for additional discussion and results. Confidence thresholds In addition to providing the most likely language of document, LID classifiers typically also return confidence threshold for that prediction. Many works rely on single confidence threshold applied to all languages, e.g., in mC4 (Xue et al., 2021) only documents whose language prediction score is above 70% are kept, while in CC-100 (Wenzek et al., 2020) score of 50% is used for all languages. However, this does not account for inherent differences in prediction confidence between languages some 4https://fasttext.cc/docs/en/language-identification.html Preprint. Under review. languages have closely related cousin that might confound the LID classifier, therefore requiring lower threshold, whereas higher value can be employed for high resource languages for which the classifier is often quite confident (NLLB Team et al., 2022). To determine appropriate thresholds per language following our data-driven philosophy, we train models for each of our nine languages at different confidence thresholds, corresponding to removal rates of 5% of the data at time. Languages such as Arabic  (Table 16)  or Russian  (Table 20)  prefer high thresholds (>0.8), while for Swahili lower threshold around 0.3 (corresponding to removal rate of almost 65%) performs best, as this languages distribution is right-skewed. After analyzing the score distributions and the highest performing thresholds, we defined filtering thresholds to be one standard deviation below the median of the score distributions, clipped to the range [0.3, 0.9]: max{0.3, min{0.9, Med(X) σ(X)}}, where is the distribution of confidence scores for this languages data. We found that this formula selects values within the highest performing threshold regions for most languages  (Table 15)  ."
        },
        {
            "title": "4.3 Deduplication",
            "content": "Deduplication is the process of removing highly similar documents from pre-training dataset to increase training efficiency and improve model performance (Lee et al., 2022). While deduplication requires large amount of computation and is therefore typically applied as the very last processing step, we employ it as an initial step, before filtering. This allowed us to directly observe the final dataset performance each time we ran one of our many filtering experiments without the possibility of deduplication later influencing the results. We rely on MinHash (Broder, 1997), fuzzy deduplication method that finds clusters of similar documents that are then filtered to keep single document per cluster. We used the same MinHash hyperparameters used for FineWeb (14 buckets of size 8, with 5-grams) and deduplicated globally per language. We used our word-level tokenizers (Section 2) to obtain word n-grams. When keeping single document per duplicate cluster, we record the number of documents that were in the cluster to explore duplication-aware upsampling schemes later in Section 4.5. To measure the impact of deduplication on data quality, we trained per-canary-language models on 350 billion tokens, both on the data before deduplication (with the LID filtering) and after. Results in Fig. 1 show that while we generally observed improved performance across languages, the impact of deduplication seems to vary significantly from language to language, without any discernable relationship to the languages resource level. However, we note that even languages showing little to no improvement from deduplication still benefit from rehydration (our duplication-aware upsampling scheme, described in Section 4.5)."
        },
        {
            "title": "4.4 Filtering recipe",
            "content": "Filtering aims to remove documents that are deemed to be lower-quality (i.e. those that might worsen model performance) using heuristic rules, such as the number of times words are repeated within the document, the average number of characters per words in the document, or the ratio of lines ending with punctuation Albalak et al. (2024). Unfortunately, many of these rules are language-specific: in languages like Chinese, words have, on average, fewer characters, while in languages like German the opposite is true. We began with the list of filtering rules from FineWeb and sought to devise methods that would allow us to automatically adapt them to large number of languages, tailoring specific thresholds according to each languages characteristics. To this end, we collected statistics for each language on different corpora and used the distributions on different metrics to determine adequate filtering thresholds. We relied on our nine canary languages to inform our decisions and trained large number of models to test how well each rule adaptation method would generalize. We leveraged three main sources to collect statistics for each language: Wikipedia, the Glotlid-Corpus (Kargaran et al., 2023) (used to train the GlotLID classifier) and our language-filtered data obtained from Common Crawl. 6 Preprint. Under review."
        },
        {
            "title": "4.4.1 Stopwords",
            "content": "Stopwords are common words in language that, while not indicative of text quality, when absent can help identify non-linguistic low-quality content (e.g. boilerplate, non-natural text, or gibberish), or content whose language was misclassified. The number of stop words in document is therefore used as signal to remove such data, and stopword filtering is part of the widely used Gopher quality filters (Rae et al., 2022) for English. To determine stopwords for each language, we analyzed word frequencies in our reference datasets, using our word tokenizers to identify the most frequently occurring words. Instead of selecting fixed number of words, we defined stop words as those exceeding set frequency threshold. This method allowed us to account for variations across languages. For example, in English, the is highly frequent, whereas in German, its equivalentsder, die, and dasshare the same role. We additionally addressed specific issues: some words were actually non-alphabetic and had to be excluded, and for some languages the source data (particularly Wikipedia) contained large portions of English content that caused significant number of the stop words to be in English. This underscores the importance of having clean data when creating filters in an automated fashion. Further discussion in Appendix A.7.1. For our filtering pipeline, we require at least 2 words from the stopwords list to be present in each document, in line with Rae et al. (2022)."
        },
        {
            "title": "4.4.2 Filtering threshold selection",
            "content": "To automatically determine filter thresholds for different languages, we propose an empirical approach based on the distribution of the metric we are filtering. We consider variety of different methods: English, use English-based filtering values from FineWeb without change (one of the baselines); MeanStd, assuming the threshold is standard deviations from the mean in the metric distribution in English, we set the threshold to the corresponding distance from the mean in the target language distribution (a variation using the median instead of mean produces similar values); Quantile, where we define the threshold for each language so as to remove the same fraction of data as the English threshold removes in English; 10Tail, inspired by CulturaX (Nguyen et al., 2024), we select threshold to remove the tail exactly 10% of the reference data; MedianRatio, inspired by HPLT2 (de Gibert et al., 2024), thresholds are selected such that the ratio between English and the target language matches the ratio of the medians of English and the target language on this metric. For each method, thresholds are computed on different reference corpora for each filter and then models are trained on the data filtered using these filters. We then compare method for each filter across all languages with each other, as well as with no filtering baseline. Precisely, we computed thresholds for each filter used in three of the FineWeb filter groups: Gopher Quality (goq), Gopher Repetition (gor), and FineWeb Quality (fwq). We then trained nine models (one per canary language) on data filtered using each method on each of the filter groups, for all method-filter group combinations except those that removed an excessive amount of data (more than 75%), or that did not remove any data at all. In total, these experiments required total of 207 ablation models, each trained for 29B tokens. We report the average rank of the aggregate score of each method across languages, in Table 25. Ultimately, we employ the best performing methods for each filter group: the 10Tail method and Quantile methods computed on Wikipedia (or on GlotLID-Corpus for languages without Wikipedia) for the FineWeb and Gopher Quality filters, respectively, and the MeanStd method computed on Common Crawl data for the Gopher Repetition filters. This step noticeably improves performance for all languages  (Fig. 1)  ."
        },
        {
            "title": "4.4.3 Precision filtering lower-resource languages",
            "content": "Low-resource languages often suffer from low LID precision: due to the large class imbalance between highand low-resource languages on web corpora, real precision is often much lower than that measured on balanced test set (Caswell et al., 2020). In practice, this means that corpora for low-resource languages with closely related high-resource language are often heavily contaminated with false positives from the high-resource language, sometimes accounting for more than 90% of the data. 7 Preprint. Under review. After inspecting data for low-resource languages produced by our pipeline, we decided to employ final filtering step exclusively to low-resource languages to address this issue. Inspired by Caswell et al. (2020); Bapna et al. (2022), we compiled lists of words that are common in each language but uncommon in other languages (i.e., have high affinity). We then measured the contamination of each corpora as the ratio of documents not containing any of these words. While the majority of languages had extremely low contamination scores, roughly third of the 1900 languages had contamination scores above 10%. For these languages, we filtered documents using the high-affinity wordlists to remove false positive documents. Additionally, since we noticed the high-affinity wordlists could be too short and strict for some languages (such as English-based pidgins, for example), we also kept documents removed by the wordlist filtering whose URLs included specific terms related to the language (the language code, the language name, domain name extensions etc). manual audit of three lower-resouce languages shows precision improvements of almost 30% for some languages. We provide additional details in Appendix A.7.3."
        },
        {
            "title": "4.5 Rehydration",
            "content": "In contrast to standard deduplication practices (Lee et al., 2022), Penedo et al. (2024) makes the case for per-snapshot deduplication and claims that additional deduplication beyond the removal of the largest duplicate clusters may actually harm model performance by artificially upsampling documents that are completely unique but high-entropy and lowquality. While we perform global deduplication, as mentioned in Section 4.3, we also save the original size of each duplicate cluster in the metadata of the kept documents, which allows us to selectively upsample specific documents (and therefore rehydrate the dataset), to obtain more performant models. In Tang et al. (2024), the authors explore one such strategy with hand-picked upsampling weights based on MinHash cluster sizes: documents with 2 to 5 duplicates are repeated 3 times, 5-100 5 times, 101-1000 8 times, and documents with over 1000 duplicates are repeated 10 times. While this provides duplication-aware upsampling strategy, it is heavily datasetdependent smaller datasets will have their distribution of cluster sizes shifted left and therefore might not be scalable across different languages. Additionally, the chosen weights favor highly duplicated documents the most, which we find are generally of lower quality, and therefore should be repeated less rather than more. While we initially trained models for each of our nine canary languages on data of different ranges of minhash cluster sizes (e.g., we trained one model on data that had no duplicates, another on data that had 2 duplicates, data that had 3-4 duplicates, etc) to empirically define upsampling weights, simpler and more scalable approach is to use the results from our filtering stage as proxy for cluster size quality: we obtain the global filtering rate (the percentage of documents removed by our entire filtering process), as well as the filtering rate for each value of metadata minhash cluster size, as shown in Fig. 2 (for French). Figure 2: Filtering rates by MinHash cluster size for French documents. The global filtering rate represents the overall percentage of documents removed during the full filtering process. Individual filtering rates are shown for each cluster size, providing proxy for cluster qualityhigher removal rates may indicate lower-quality clusters. We assign upsampling weights to each cluster size based on the filtering rates. The figure suggests that both data that was never repeated (cluster size of 1), as well as data that is repeated many times (especially the most-repeated 0.1% long tail of data grouped Preprint. Under review. as the last bar), is generally of lower quality, as our filters removed more than the global removal rate. Surprisingly, this looking shape we observe for French is present in most languages we verified, but often shifted based on the size of the corpora for each language. The differences we observe for different cluster sizes align closely with experimental results from training runs on different ranges of cluster sizes for the languages we tested, and so we experimented with setting upsampling weights based on the removal rates: we assigned weight of 10 (meaning documents should be repeated 10 times) to the cluster size with the smallest removal rate, and weight of 1 to every cluster size above the global removal rate. For the remaining cluster sizes, we resorted to simple interpolation between these 2 endpoints. For French, the resulting weights are shown in Fig. 2. While upsampling weights are dataset-dependent, using the filtering rates as proxy for quality is scalable and affordable method to determine them and rehydration itself generally provides strong performance uplift  (Fig. 1)  with little downside."
        },
        {
            "title": "5 Validating and Applying the FineWeb2 Pipeline",
            "content": "Having established the pipeline for FineWeb2, and having shown the positive effect of each pipeline step  (Fig. 1)  , we now perform additional evaluations to confirm the effectiveness of our approach and use the pipeline to generate per-language datasets in over 1,000 languages. Creating the FineWeb2 dataset We apply our pipeline to 96 Common Crawl snapshots, spanning the summer of 2013 to April 2024, to produce the FineWeb2 dataset, comprising 20 terabytes of text content covering total of 1,868 language-script pairs, of which 1,226 have over 100 documents, 474 more than 1 thousand documents, and 203 at least 10 thousand documents. Additional details and per-language statistics can be found in Appendix A.11. In addition to the filtered dataset, we also release the preliminary version before filtering is applied, to facilitate further research into alternative filtering methods. As FineWeb2 itself does not include English, for full language coverage we recommend complementing it with FineWeb, whose pipeline inspired FineWeb2. Comparison to other datasets We now compare to other non-English datasets, both on the canary languages used to design the pipeline as well as set of unseen languages that were not used for ablations. As discussed previously, prior multilingual datasets often use fixed pipelines across languages, whereas FineWeb2s pipeline adapts to the statistics and characteristics of each language. By comparing to other multilingual datasets, we can confirm the benefit of FineWeb2s adaptive approach. To provide point of comparison against pipelines tuned to specific language, we additionally evaluate single-language datasets (whose pipelines are designed and tuned for specific language, often by native speakers) when available. For canary languages, we use the same set of benchmarks used for pipeline design ablations. Since the FineWeb2 pipeline was designed specifically around the canary languages, evaluating on unseen languages validates that the pipeline generalizes effectively. To choose unseen languages, we first followed the same procedure (detailed in Section 3.3) for selecting reliable evaluation tasks across wide range of languages and chose languages that had sufficient number of reliable tasks: German, Indonesian, Italian, Japanese and Vietnamese. The chosen tasks are detailed in Appendix A.10. Canarylanguage and unseen-language models were trained for 29 billion and 100 billion tokens respectively. All evaluated models follow the same architecture, hyperparameters, and (Gemma) tokenizer as considered previously and detailed in Section 3.1. summary of the results is shown in Fig. 3, with detailed per-task results in Appendix A.10.2. Overall, we found that FineWeb2 produces more performant models than prior multilingal datasets on 11 out of 14 of the languages we considered. In some cases, FineWeb2 produces worse performance than language-specific dataset, which highlights that pipelines hand-designed by language experts can still outperform our adaptive pipeline approach. These trends hold up both for our canary datasets as well as held-out datasets, which supports the utility of the 1,000+ language-specific datasets we generated with the FineWeb2 pipeline. On the whole, our results confirm the effectiveness and generalization of our consistent-but-adaptable cross-lingual curation pipeline. 9 Preprint. Under review. Figure 3: High-level performance comparison of FineWeb2 to other multilingual and language-specific datasets. We evaluate performance both on the canary languages used to design the FineWeb2 pipeline as well as unseen languages. For brevity, for each language we plot the performance of only the best-performing single-language dataset. The bestperforming dataset for each language is marked with . Expanded results are provided in Appendix A.9 and Appendix A.10.2. Inspecting low-resource corpora natural concern is whether low-resource corpora, often with fewer than 20 documents, contain content that is genuinely useful for training. Manual inspection of over 500 languages reveals that many corpora are composed almost exclusively of Bible and/or Wikipedia content. We categorized the most common document domain names and computed the proportion belonging to Bibleor Wikipedia-related sources: out of 1868 language-script pairs in the final dataset, 70% (1320 of them) have more than half their documents from Bibleor Wikipedia-related domains. This reflects both the limited availability of online data for many languages and the narrow diversity of sources in the language identifiers training datawhere often the only clean data comes from the Bible (Kargaran et al., 2023). While we hope these corpora remain useful to the research community, their limited diversity highlights the broader challenges of collecting data for the long tail of the worlds languages. For more details, see Appendix A.12."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we used data-driven approach to design multilingual pre-training data processing pipeline that can automatically adapt to all languages, in contrast to prior work that employs fixed pipelines for each language. We extensively ablate our design choices on new suite of quantitatively identified multilingual benchmarks that provide reliable evaluation signal, ultimately covering 14 languages. We additionally show how duplication counts and filtering results can be leveraged to selectively upsample higher quality content, providing performance uplift. Finally, we scaled our pipeline to create FineWeb2, pretraining dataset covering 1,868 language-script pairs, spanning 20 terabytes of text content curated from 96 Common Crawl snapshots. While our experiments show that our pipeline yields strong performance, we point out few limitations. First, although we strove to make the language coverage as wide as possible, computational constraints, language-specific task availability, and excessively small lowresource datasets only enabled us to test small proportion of the languages in FineWeb2. These factors also forced us to only consider relatively short ablation runs. Second, we studied early-signal properties of each task at the very early stages of model training, and so it is possible that the properties could change significantly as training progresses, making some tasks more viable. Additionally, we do not explore additional criteria for task selection, such as cultural alignment, with which translated tasks struggle. Similarly, our chosen tasks do not measure other important attributes such as bias or diversity. Lastly, while we strove to include large number of low-resource languages in our dataset, large number of them consist almost or even entirely of Bibleor Wikipedia-related content. Overall, we hope our findings, datasets, and code pave the way for further improvement of datasets that cover wider range of languages. 10 Preprint. Under review."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Abdeljalil El Majjodi, Ihssane Nedjaoui, and Zaid Chiech for labeling data for our precision filtering audit; Bram Vanroy, Loıck Bourdois, Omar Kamali, Per Kummervold, Qian Liu, Edwin Rijgersberg, Michael S. Mollel, Faton Rekathati, and Mikhail Tikhomirov for inspecting and providing valuable feedback on their respective native language subsets of FineWeb2; and the many contributors of the FineWeb-C community annotation project. We extend our gratitude to the Common Crawl project for freely providing and maintaining their regular crawls, which have enabled much of modern LLM research. We thank Pedro Ortiz Suarez from the Common Crawl team, as well as Gema Ramırez, Marta Ba on, and other members of the HPLT team for fruitful discussions about multilingual data. Additionally, we thank our colleagues Nouamane Tazi, Phuc Nguyen, Ferdinand Mom, and Haojun Zhao for designing and building our training framework, Nanotron; Clementine Fourrier and Nathan Habib for creating and maintaining our evaluation framework, LightEval; and Loubna Ben Allal and Anton Lozhkov for discussions throughout the project. Finally, we thank Hugo Larcher and Mathieu Morlon for tirelessly assisting us whenever we encountered issues with the Hugging Face Science cluster, which they manage with incredible dedication, as well as all the other cluster users for their gracious patience."
        },
        {
            "title": "References",
            "content": "01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai, 2025. URL https://arxiv.org/abs/2403.04652. Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sebastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio Cesar Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL CARD.md. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. Transactions on Machine Learning Research, 2024. Preprint. Under review. Mohamad Alhajar. Open llm turkish leaderboard v0.2. https://huggingface.co/spaces/ malhajar/OpenLLMTurkishLeaderboard, 2024. Ebtesam Almazrouei, Ruxandra Cojocaru, Michele Baldo, Quentin Malartic, Hamza Alobeidli, Daniele Mazzotta, Guilherme Penedo, Giulia Campesan, Mugariya Farooq, Maitha Alhammadi, Julien Launay, and Badreddine Noune. AlGhafa evaluation benchmark for Arabic language models. In Hassan Sawaf, Samhaa El-Beltagy, Wajdi Zaghouani, Walid Magdy, Ahmed Abdelali, Nadi Tomeh, Ibrahim Abu Farha, Nizar Habash, Salam Khalifa, Amr Keleg, Hatem Haddad, Imed Zitouni, Khalil Mrini, and Rawan Almatham (eds.), Proceedings of ArabicNLP 2023, pp. 244275, Singapore (Hybrid), December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.arabicnlp-1.21. URL https://aclanthology.org/2023.arabicnlp-1.21/. Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, and Chehir Dhaouadi. 101 billion arabic words dataset, 2024. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020a. doi: 10.18653/ v1/2020.acl-main.421. URL http://dx.doi.org/10.18653/v1/2020.acl-main.421. Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, and Eneko Agirre. In Proceedings of the 58th call for more rigor in unsupervised cross-lingual learning. Annual Meeting of the Association for Computational Linguistics, pp. 73757388. Association for Computational Linguistics, 2020b. doi: 10.18653/v1/2020.acl-main.658. URL http: //dx.doi.org/10.18653/v1/2020.acl-main.658. Kawahara Lab at Waseda University. Japanese massive multitask language understanding benchmark, 2023. URL https://huggingface.co/datasets/nlp-waseda/JMMLU. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: parallel reading comprehension dataset in 122 In Proceedings of the 62nd Annual Meeting of the Association for language variants. Computational Linguistics (Volume 1: Long Papers), pp. 749775, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.44. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. Building machine translation systems for the next thousand languages, 2022. URL https://arxiv.org/abs/2205.03983. Adrien Barbaresi. Trafilatura: Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pp. 122131. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pp. 2129. IEEE, 1997. Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield. An open dataset and model for language identification. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 865879, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.75. URL https://aclanthology.org/2023.acl-short.75/. 12 Preprint. Under review. Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Ba on, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajiˇc, Jindˇrich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kyt oniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, Farrokh Mehryary, Vladislav Mikhailov, Nikita Moghe, Amanda Myntti, Dayyan OBrien, Stephan Oepen, Proyag Pal, Jousia Piha, Sampo Pyysalo, Gema Ramırez-Sanchez, David Samuel, Pavel Stepachev, org Tiedemann, Duˇsan Variˇs, Tereza Vojtˇechova, and Jaume Zaragoza-Bernabeu. An expanded massive multilingual dataset for high-performance language technologies, 2025. URL https://arxiv.org/abs/2503.10267. Diedre Carmo, Marcos Piau, Israel Campiotti, Rodrigo Nogueira, and Roberto Lotufo. PTT5: Pretraining and validating the t5 model on brazilian portuguese data. arXiv preprint arXiv:2008.09144, 2020. Isaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. Language ID in the wild: Unexpected challenges on the path to thousand-language web text corpus. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 65886608, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020. coling-main.579. URL https://aclanthology.org/2020.coling-main.579/. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. doi: 10.1162/tacl 00317. URL https: //doi.org/10.1162/tacl 00317. Cohere. Command r+. Web, 2024. URL https://docs.cohere.com/docs/command-r-plus# model-details. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 84408451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https://aclanthology.org/2020.acl-main.747. Danilo Croce, Alexandra Zelenanska, and Roberto Basili. Neural learning for question answering in italian. In Chiara Ghidini, Bernardo Magnini, Andrea Passerini, and Paolo Traverso (eds.), AI*IA 2018 Advances in Artificial Intelligence, pp. 389402, Cham, 2018. Springer International Publishing. ISBN 978-3-030-03840-3. Yiming Cui, Ting Liu, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. Dataset for the first evaluation on chinese machine reading comprehension, 2018. URL https: //arxiv.org/abs/1709.08299. Peter T. Daniels and William Bright (eds.). The Worlds Writing Systems. Oxford University Press, New York, 1996. Ona De Gibert, Graeme Nail, Nikolay Arefyev, Marta Ba on, Jelmer Van Der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramırez-Sanchez, Andrey Kutuzov, et al. new massive multilingual dataset for high-performance language technologies. arXiv preprint arXiv:2403.14009, 2024. Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Ba on, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema Ramırez-Sanchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, and org Tiedemann. new massive multilingual dataset for high-performance language technologies, 2024. URL https://arxiv.org/abs/2403.14009. 13 Preprint. Under review. Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. BERTje: dutch BERT model. arXiv preprint arXiv:1912.09582, 2019. DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism, 2024. URL https://arxiv.org/abs/2401.02954. Pieter Delobelle, Thomas Winters, and Bettina Berendt. RobBERT: dutch RoBERTa-based language model. arXiv preprint arXiv:2001.06286, 2020. Martin dHoffschmidt, Wacim Belblidia, Tom Brendle, Quentin Heinrich, and Maxime Vidal. Fquad: French question answering dataset, 2020. URL https://arxiv.org/abs/2002. 06071. Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong Feng, Xin Mao, Man Tsung Yeung, Kunat Pipatanakul, Fajri Koto, Min Si Thu, Hynek Kydlıˇcek, Zeyi Liu, Qunshu Lin, Sittipong Sripaisarnmongkol, Kridtaphad Sae-Khow, Nirattisai Thongchim, Taechawat Konkaew, Narong Borijindargoon, Anh Dao, Matichon Maneegard, Phakphum Artkaew, Zheng-Xin Yong, Quan Nguyen, Wannaphong Phatthiyaphaibun, Hoang H. Tran, Mike Zhang, Shiqi Chen, Tianyu Pang, Chao Du, Xinyi Wan, Wei Lu, and Min Lin. Sailor2: Sailing in south-east asia with inclusive multilingual llms, 2025. URL https://arxiv.org/abs/2502.12982. Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. Chinese tiny llm: Pretraining chinese-centric large language model, 2024. URL https://arxiv.org/abs/2404.04167. David M. Eberhard, Gary F. Simons, and Charles D. Fenning. Ethnologue: Languages of the world, 2024. URL http://www.ethnologue.com. Pavel Efimov, Andrey Chertok, Leonid Boytsov, and Pavel Braslavski. SberQuAD Russian Reading Comprehension Dataset: Description and Analysis, pp. 315. Springer International Publishing, 2020. ISBN 9783030582197. doi: 10.1007/978-3-030-58219-7 1. URL http: //dx.doi.org/10.1007/978-3-030-58219-7 1. May Farhat, Said Taghadouini, Oskar Hallstr om, and Sonja Hajri-Gabouj. Arabicweb24: Creating high quality arabic web-only pre-training dataset, 2024. URL www.lighton.ai/ lighton-blogs/arabicweb24. Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Ant onio Loison, Duarte M. Alves, Caio Corro, Nicolas Boizard, Joao Alves, Ricardo Rei, Pedro H. Martins, Antoni Bigata Casademunt, Francois Yvon, Andre F. T. Martins, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Croissantllm: truly bilingual french-english language model, 2024. Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, 14 Preprint. Under review. and Sergei Markov. Mera: comprehensive llm evaluation in russian, 2024. URL https://arxiv.org/abs/2401.04531. Clementine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. https://huggingface.co/spaces/open-llm-leaderboard/ Open llm leaderboard v2. open llm leaderboard, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, GeorgeChristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. URL https://arxiv.org/abs/2403.08295. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Preprint. Under review. Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji 16 Preprint. Under review. Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations, 2025. URL https://arxiv. org/abs/2406.08446. Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov, Ivan Koychev, and Preslav Nakov. Exams: multi-subject high school examinations dataset for cross-lingual and multilingual question answering, 2020. URL https://arxiv.org/abs/2011.03080. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Phan Viet Hoang. Khmer natural language processing tookit. https://github.com/ VietHoang1512/khmer-nltk, 2020. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-strength Natural Language Processing in Python. 2020. doi: 10.5281/zenodo. 1212303. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models, 2023. URL https://arxiv.org/abs/2305.08322. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mixtral of experts, 2024. URL https://arxiv. org/abs/2401.04088. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 62826293, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https://aclanthology.org/2020.acl-main.560/. 17 Preprint. Under review. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Herve Jegou, and Tomas Mikolov. Fasttext.zip: Compressing text classification models, 2016. URL https: //arxiv.org/abs/1612.03651. Amir Hossein Kargaran, Ayyoob Imani, Francois Yvon, and Hinrich Schuetze. GlotLID: Language identification for low-resource languages. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 61556218, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.410. URL https://aclanthology.org/2023. findings-emnlp.410/. Amir Hossein Kargaran, Francois Yvon, and Hinrich Schuetze. GlotCC: An open broadcoverage commoncrawl corpus and pipeline for minority languages. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=aJ1yse8GEr. Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad G, Varun Balan G, Sparsh Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, and Mitesh M. Khapra. Indicllmsuite: blueprint for creating pre-training and fine-tuning datasets for indian languages. arXiv preprint arXiv: 2403.06350, 2024. Fajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass primary school exams in Indonesia: comprehensive test on IndoMMLU. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), Singapore, 2023. Association for Computational Linguistics. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin. Arabicmmlu: Assessing massive multitask language understanding in arabic, 2024. URL https://arxiv.org/abs/2402.12840. Anoop Kunchukuttan. The IndicNLP Library. https://github.com/anoopkunchukuttan/ indic nlp library/blob/master/docs/indicnlp.pdf, 2020. Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. JGLUE: Japanese general language understanding evaluation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 29572966, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.317. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback, 2023. URL https://arxiv.org/abs/2307.16039. Hang Le, Loıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabbe, Laurent Besacier, and Didier Schwab. FlauBERT: Unsupervised language model pre-training for French. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 24792490, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://www.aclweb. org/anthology/2020.lrec-1.302. Open Arabic LLM Leaderboard. Open-arabic-llm-leaderboard-v1. https://huggingface. co/spaces/OALL/Open-Arabic-LLM-Leaderboard-v1, 2024. Accessed: 2025-03-28. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better, 2022. URL https://arxiv.org/abs/2107.06499. Minchul Lee. Kiwipiepy: Kiwi package for python, 2024. URL https://github.com/ bab2min/kiwipiepy. 18 Preprint. Under review. Patrick Lewis, Barlas guz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. Mlqa: Evaluating cross-lingual extractive question answering, 2020. URL https://arxiv.org/ abs/1910.07475. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2024a. URL https://arxiv.org/abs/2306.09212. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024b. URL https://arxiv.org/abs/ 2406.11794. Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond English: Evaluating and improving multilingual language models for commonsense reasoning. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 12741287, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.102. URL https://aclanthology.org/2021.acl-long.102/. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language models. CoRR, abs/2112.10668, 2021b. URL https://arxiv.org/ abs/2112.10668. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language models, 2022. URL https://arxiv.org/abs/2112.10668. Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, HannaMari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al. Fingpt: Large generative models for small language. arXiv preprint arXiv:2311.05640, 2023. Lovish Madaan, Aaditya K. Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv.org/abs/2406.10229. Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suarez, Yoann Dupont, Laurent Romary, Eric de la Clergerie, Djame Seddah, and Benoˆıt Sagot. CamemBERT: tasty French language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 72037219, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.645. URL https://www.aclweb.org/anthology/2020. acl-main.645. Sabrina Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Galle, Arun Raja, Chenglei Si, Wilson Lee, Benoˆıt Sagot, et al. Between words and characters: 19 Preprint. Under review. brief history of open-vocabulary modeling and tokenization in nlp. arXiv preprint arXiv:2112.10508, 2021. MOP-LIWU Community and MNBVC Team. Mnbvc: Massive never-ending bt vast chinese corpus. https://github.com/esbatmop/MNBVC, 2023. Hussein Mozannar, Elie Maamary, Karl El Hajal, and Hazem Hajj. Neural Arabic question answering. In Proceedings of the Fourth Arabic Natural Language Processing Workshop, pp. 108118, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-4612. URL https://www.aclweb.org/anthology/W19-4612. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2022. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 42264237, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.377. NLLB Team, Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022. URL https://arxiv.org/abs/2207.04672. Omnia Russica Team. Omnia russica. https://omnia-russica.github.io/, 2024. OpenPecha. Botok: State-of-the-art tokenizers for tibetan language, 2025. URL https: //github.com/OpenPecha/Botok. Support for various dialects, fully customizable word lists and adjustment rules. Shantipriya Parida, Shakshi Panwar, Kusum Lata, Sanskruti Mishra, and Sambit Sekhar. Building pre-train llm dataset for the indic languages: case study on hindi. https: //huggingface.co/OdiaGenAI, 2024. Triamamornwooth Patteera. Hellaswag-th, 2023. URL https://huggingface.co/datasets/ Patt/HellaSwag TH. v1.0. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023. URL https://arxiv.org/abs/2306.01116. Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 3081130849. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper files/paper/2024/ file/370df50ccfdf8bde18f8f9c2d9151bda-Paper-Datasets and Benchmarks Track.pdf. Wannaphong Phatthiyaphaibun. Laonlp: Lao language natural language processing, July 2022. URL https://doi.org/10.5281/zenodo.6833407. 20 Preprint. Under review. Wannaphong Phatthiyaphaibun, Korakot Chaovavanich, Charin Polpanumas, Arthit Suriyawongkul, Lalita Lowphansirikul, and Pattarawat Chormai. PyThaiNLP: Thai natural language processing in Python, June 2024. URL https://github.com/PyThaiNLP/ pythainlp/. Kunat Pipatanakul, Phatrasek Jirabovonvisut, Potsawee Manakul, Sittipong Sripaisarnmongkol, Ruangsak Patomwong, Pathomporn Chokchainant, and Kasima Tharnpipitchai. Typhoon: Thai large language models. arXiv preprint arXiv:2312.13951, 2023. PLLuM Consortium. Pllum: family of polish large language models. 2025. Pluto-Junzeng. Chinesesquad. https://github.com/pluto-junzeng/ChineseSquad, 2019. Accessed: 2025-03-28. Edoardo Maria Ponti, Goran Glavaˇs, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. Xcopa: multilingual dataset for causal commonsense reasoning, 2020. URL https://arxiv.org/abs/2005.00333. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. Stanza: python natural language processing toolkit for many human languages, 2020. URL https://arxiv.org/abs/2003.07082. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022. URL https://arxiv.org/abs/2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140), 2020. Yasaman Razeghi, Robert Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? arXiv preprint arXiv:2002.08910, 2020. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models, 2021. URL https://arxiv.org/abs/2012.15613. Oishi Sakana. Pyidaungsu: Python library for myanmar language, 2024. URL https: //github.com/kaunghtetsan275/pyidaungsu. Alex Salcianu, Andy Golding, Anton Bakalov, Chris Alberti, Daniel Andor, David Weiss, Emily Pitler, Greg Coppola, Jason Riesa, Kuzman Ganchev, et al. Compact language detector v3. Technical report, 2018. URL https://chromium.googlesource.com/external/ github.com/google/cld 3/. Accessed 2024. Preprint. Under review. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering, 2022. URL https://arxiv.org/ abs/2210.01613. Abhishek Kumar Singh, Vishwajeet kumar, Rudra Murthy, Jaydeep Sen, Ashish Mittal, and Ganesh Ramakrishnan. Indic qa benchmark: multilingual benchmark to evaluate question answering capability of llms for indic languages, 2025. URL https://arxiv.org/ abs/2407.13522. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. URL https://arxiv.org/abs/2402.00159. Fatih Soygazi, Okan iftci, gurcan ok, and Soner Cengiz. Thquad: Turkish historic question answering dataset for reading comprehension. In 2021 6th International Conference on Computer Science and Engineering (UBMK), pp. 215220, 2021. doi: 10.1109/UBMK52708. 2021.9559013. Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging Chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. doi: 10.1162/tacl 00305. URL https://aclanthology.org/ 2020.tacl-1.10/. Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, Suqi Sun, Cun Mu, Victor Miller, Xuezhe Ma, Yue Peng, Zhengzhong Liu, and Eric P. Xing. Txt360: top-quality llm pre-training dataset requires the perfect blend, 2024. TigerResearch. Tigerbot: multi-language multi-task llm. TigerResearch/TigerBot, 2023. https://github.com/ Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. K. Trakultaweekoon, S. Thaiprayoon, P. Palingoon, and A. Rugchatjaroen. The first wikipedia questions and factoid answers corpus in the thai language. In 2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP), pp. 14. IEEE, 2019. Meliksah Turker, Erdi Ari, and Aydin Han. Vbart: The turkish llm. arXiv preprint arXiv:2403.01308, 2024. Barack W. Wanjawa, Lilian D. A. Wanzare, Florence Indede, Owen Mconyango, Lawrence Muchemi, and Edward Ombui. Kenswquada question answering dataset for swahili low-resource language. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(4):120, April 2023. ISSN 2375-4702. doi: 10.1145/3578553. URL http: //dx.doi.org/10.1145/3578553. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Nicoletta Calzolari, Frederic Bechet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hel`ene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference, 22 Preprint. Under review. pp. 40034012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoˆıt Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurencon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo Gonzalez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gerard Dupont, German Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, org Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Mu noz, Maraim Masoud, Marıa Grandury, Mario ˇSaˇsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tasar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Francois Lavallee, Remi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stephane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, AnneLaure Ligozat, Arjun Subramonian, Aurelie Neveol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu noz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed 23 Preprint. Under review. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clementine Fourrier, Daniel Le on Peri nan, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P`amies, Maria Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Theo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. Bloom: 176b-parameter open-access multilingual language model, 2023. URL https://arxiv.org/abs/2211.05100. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer, 2021. URL https://arxiv.org/abs/2010.11934. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models, 2023. URL https://arxiv.org/abs/2306.05179. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023a. URL https://arxiv.org/abs/2304.06364. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023b. URL https://arxiv.org/abs/2304.06364. 24 Preprint. Under review."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Word tokenizers for segmentation For full language coverage, we relied on wide range of word tokenizers from SpaCy (Honnibal et al., 2020) and Stanza (Qi et al., 2020), as well as from libraries targeting specific languages (or language groups) like IndicNLP (Kunchukuttan, 2020) for Indic Languages, PyThaiNLP (Phatthiyaphaibun et al., 2024) for Thai, Kiwipiepy (Lee, 2024) for Korean, KhmerNLTK (Hoang, 2020) for Khmer, LaoNLP (Phatthiyaphaibun, 2022) for Lao, Botok (OpenPecha, 2025) for Tibetan, and Pyidaungsu (Sakana, 2024) for Burmese. For languages without native word tokenizer, we assigned proxy tokenizer from another language based on language family data from the Ethnologue5 using the following approach: 1. Build tree for each language family based on the taxonomy from the Ethnologue. 2. Assign tokenizers to each language+script pair that had native tokenizer from one of the libraries mentioned above 3. Perform an upward pass through the tree, propagating the available tokenizers to the parent nodes, one per script. When multiple tokenizers for the same script are present in the children of given node, we pick the one from the language with more available data. We do not propagate to the root node as different subfamilies are usually quite different (for example, for Pidgin, English-based and Swahilibased are two branches; for Indo-European, Italic and Armenian) 4. Perform downward pass through the tree, assigning as proxy tokenizer the previously propagated parent node tokenizers when available. This method allowed us to quickly scale tokenizer assignments for many languages by assigning tokenizers from closely related language. An illustrative example is available in Fig. 4. We relied on the SpaCy multilingual tokenizer for the remaining languages with Latin or Cyrillic scripts, which was trained on multiple languages that use these scripts. For any remaining script, we assigned the remaining languages to the tokenizer of the highest resource language that uses the script and has native tokenizer. 5https://www.ethnologue.com/browse/families/ 25 Preprint. Under review. Indo-European Language Family Tree Indo-European [-] (455) ... Germanic [english] (49) Greek [greek] (6) Indo-Iranian [hindi] (312) Italic [spanish] (45) Romance [spanish] (44) Latino-Faliscan [latin] (1) Eastern [romanian] (4) Southern [spanish] (5) Italo-Western [spanish] (35) Latin [latin] Italo-Dalmatian [italian] (6) Western [spanish] (29) Italian [italian] Napoletano [italian] Sicilian [italian] Figure 4: Example tokenizer assignments based on language family data in Indo-European. Triangles correspond to languages for which native word tokenizer was available, while squares are languages for which proxy tokenizer was assigned. The tokenizer assigned to each language is written inside brackets [], and the number of languages in each subnode is in parantheses (). The Italian word tokenizer was propagated to other languages in the Italo-Dalmatian subfamily, while Spanish was propagated up the tree from the Western branch, given that it is higher resource language than Italian. Latin has its own native tokenizer. Word tokenizers are propagated all the way to the first level, but not to the root or across top level subfamilies. 26 Preprint. Under review. A.2 Canary Languages While our corpus and pipeline covers more than thousand languages, we perform in-depth evaluations of the following subset of languages:"
        },
        {
            "title": "Arabic\nHan\nLatin",
            "content": "Afro-Asiatic Sino-Tibetan Indo-European (Italic) Indo-European (Indo-Iranian) Devanagari Medium Indo-European (Balto-Slavic) Niger-Congo Dravidian Kra-Dai Turkic"
        },
        {
            "title": "Medium\nHigh\nHigh",
            "content": "Table 1: The 9 canary languages and their families, main script, and resource availability. 27 Preprint. Under review. A.3 Multilingual tokenizers comparison Following Rust et al. (2021), we considered two metrics: Subword fertility (sf): the average number of tokens per real text word. Measures how aggressively tokenizer splits words. The theoretical minimum of 1 would mean the tokenizer vocabulary contains every single word from the reference text; Proportion of continued words (pcw): the ratio of real text words encoded with 2 tokens or more. Measures how often tokenizer splits words. value of 0 means that the tokenizer never splits and 1 that it always splits. We split each languages Wikipedia into individual words (see Section 2) and computed the two metrics using tokenizers from variety of popular multilingual models: Mistral-7BV3 (Jiang et al., 2023), Phi3 (Abdin et al., 2024), Llama3 (AI@Meta, 2024), Qwen2.5 (Yang et al., 2024), mT5 (Xue et al., 2021), Bigscience-Bloom (Workshop et al., 2023), CommandR (Cohere, 2024), Gemma (Gemma Team et al., 2024), and XGLM (Lin et al., 2022). We did not include tokenizers in our comparison if they had vocabulary size over 256,000, as these would make the embedding layer consume considerable number of paramaters: at the relatively small model scale we targeted for our experiments (around 1.5 billion parameters), this would force us to significantly reduce the number of model layers due to computational constraints. Following inspection of the computed metrics in Appendix A.3, where we additionally show the average and worst-case (max), and lower is better for both metrics, we excluded tokenizers that showed very low subword fertility or proportion of continued words on at least one of our canary languages. The Mistral-7B-V3, Phi3, Command-R and Llama3 tokenizers all do not provide good coverage of Telugu. Additionally, while XGLM and mT5 both showed strong performance, they do not preserve whitespaces, and some characters (particularly for Chinese) would be encoded as unknown token ([UNK]). Ultimately, our tokenizer of choice was Gemma, modern BPE tokenizer that performed slightly better than Bigscience-Bloom on average for our experimental setup. 28 Preprint. Under review. Tokenizer Vocab size No [UNK] Mistralv3 32,768 English sf English pcw Chinese sf Chinese pcw French sf French pcw Russian sf Russian pcw Turkish sf Turkish pcw Arabic sf Arabic pcw Thai sf Thai pcw Hindi sf Hindi pcw Swahili sf Swahili pcw Telugu sf Telugu pcw Max sf Max pcw Avg sf Avg pcw 1.45 0.23 3.03 0.95 1.69 0.40 2.42 0.59 3.18 0.74 4.76 0. 4.87 0.93 4.99 0.91 2.30 0.63 9.83 0.79 9.83 0.95 4.12 0. Phi3 Llama3 Qwen2.5* mT5 151,643 Bloom Cmd-R Gemma XGLM 250,100 250,680 255,000 256,000 256,008 100,352 128,000 1.40 0.28 2.30 0. 1.74 0.47 2.99 0.66 2.63 0.70 3.72 0.86 3.80 0.85 4.60 0. 2.09 0.62 1.40 0.28 1.60 0.43 1.73 0.47 2.34 0.62 2.32 0. 2.32 0.74 2.18 0.66 2.71 0.81 2.07 0.62 10.11 0.76 10.11 0. 3.78 0.71 10.11 0.76 10.11 0.81 3.04 0.64 1.47 0.29 1.44 0. 1.76 0.47 2.50 0.64 2.55 0.70 2.23 0.67 2.44 0.64 3.98 0. 2.16 0.63 8.41 0.77 8.41 0.86 3.05 0.63 1.52 0.45 2.29 0. 1.71 0.55 1.96 0.73 1.99 0.73 2.10 0.79 1.99 0.68 2.02 0. 1.78 0.62 2.44 0.86 2.44 0.91 2.03 0.73 1.42 0.31 1.29 0. 1.49 0.35 2.86 0.63 2.59 0.67 1.86 0.60 3.96 0.86 1.59 0. 1.72 0.52 2.10 0.59 3.96 0.86 2.16 0.54 1.35 0.22 1.35 0. 1.50 0.35 1.99 0.56 2.13 0.64 2.16 0.68 4.01 0.87 3.39 0. 1.95 0.59 9.74 0.78 9.74 0.87 3.14 0.61 1.31 0.19 1.43 0. 1.50 0.34 2.05 0.57 2.22 0.66 2.19 0.69 1.92 0.46 2.22 0. 1.84 0.53 3.51 0.74 3.51 0.74 2.10 0.55 1.34 0.28 2.21 0. 1.45 0.35 1.68 0.50 1.72 0.53 1.72 0.52 1.78 0.53 1.52 0. 1.54 0.42 2.24 0.69 2.24 0.82 1.76 0.52 Table 2: Multilingual Tokenizers Comparison on Wikipedia. * denotes tokenizers that were not originally available when we first ran this comparison. [UNK] is the unknown token: mT5 and XGLM are unable to encode some characters, particularly for Chinese. Avg is the average across all languages, and Max the maximum (worst-case) across all languages. Lower is better for all rows. Preprint. Under review. A.4 Model architecture and training"
        },
        {
            "title": "Parameter",
            "content": "Architecture Number of attention heads Number of hidden layers Number of key-value heads RMS Norm epsilon model Tied word embeddings Embedding size Total number of parameters Random initialization std Tokenizer"
        },
        {
            "title": "Value",
            "content": "Llama 32 14 32 1e-05 2048 True 256008 1.46B 0.02 Gemma Table 3: Architecture configuration for all models"
        },
        {
            "title": "Parameter",
            "content": "29BT 100BT 350BT Data parallelism (dp) Tensor parallelism (tp) Pipeline parallelism (pp) 64 1 1 56 1 64 1 1 Sequence length Batch size (samples) Batch size (tokens) 2048 1024 2097152 2048 840 1720320 2048 1280 2621440 Table 4: Training settings for the 3 training scales we consider: 29, 100 and 350 billion tokens. For 100BT and 350BT, we compute critical batch size based on DeepSeek-AI et al. (2024)"
        },
        {
            "title": "Parameter",
            "content": "Adam beta1 Adam beta2 Adam epsilon Gradient clipping Weight decay 29BT 0.9 0.95 1.0e-8 1.0 0.1 100BT 0.9 0.95 1.0e-8 1.0 0.1 350BT 0.9 0.95 1.0e-8 1.0 0.1 3e-4 Learning rate 14000 Total train steps 500 Warmup steps linear Warmup style 13500 Decay steps 500 Decay starting step Decay style cosine Minimum decay LR 3.0e-5 8e-4 59000 2950 (5%) linear 11800 (20%) 47200 linear 0 7e-4 134000 6700 (5%) linear 26800 (20%) 107200 linear 0 Table 5: Optimizer settings for the 3 training scales we consider: 29, 100 and 350 billion tokens. For 100BT and 350BT, we train with constant learning rate until the last 20% of steps (computed following DeepSeek-AI et al. (2024)), so that the resulting models can easily undergo continued pretraining. Preprint. Under review. A.5 Evaluation details A.5.1 Task selection criteria As noted in Section 3.3, we define precise quantitative criteria for each of the properties of the early-signal task. To compute each criterion, we only use models trained on available reference datasets for given language (see Section 3.2), denoted as M. Every task in our final selection had to satisfy all of the following criteria requirements: Monotonicity. To assess Monotonicity of task, we compute the average Spearman rank correlation between the evaluation steps and the corresponding model scores. For given model m, let the score at step be denoted m(s). The average monotonicity across all models is then defined as: ρ = 1 mM ρ ([s0, s1, . . . , sn], [m(s0), m(s1), . . . , m(sn)]) Here, the Spearman correlation ρ(x, y) between sequences = [x1, . . . , xn] and = [y1, . . . , yn] is computed as: ρ(x, y) = 1 6 i=1 d2 n(n2 1) where di = rank(xi) rank(yi) is the rank difference for element i, and is the number of evaluation steps. We consider task to meet the monotonicity criterion if: ρ 0.5 Signal-to-Noise Ratio (SNR). Inspired by Madaan et al. (2024) we estimate how robust is task to training noise, by computing its Signal-to-Noise Ratio (SNR) using four models trained on unfiltered CommonCrawl data under different random seeds: seed-3: Trained on random subset with data and model seed set to 3 seed-4: Trained on same subset as seed 3, with data and model seed set to 4 seed-5: Trained on different random subset with data and model seed set to seed-6: Trained on the same subset as 5 with data seed = 6 and model seed = 42 We refer to this set of four models as MC. For each evaluation step s, we define the mean score (signal) as: µ = 1 MC mMC m(s) and the standard deviation (noise) as: (cid:115) σs = 1 MC mMC (m(s) µ )2 The overall task SNR is then the average ratio of signal to noise across all training steps: SNR = 1 s=0 µ σs 31 Preprint. Under review. We chose the minimum required SNR to 20, with the exception of generative tasks, which we found to be considerably noisier in general, but we wanted to have at least one generative task per language. Generative tasks are quite relevant in multilingual context as they provide insights into how the model behaves when prompted to generate unconstrained, i.e., without limited set of answer options. Models trained in multiple languages can sometimes exhibit high scores in multiple choice tasks but reply in the wrong language for generative tasks (accidental translation), or otherwise lack fluency (Xue et al., 2021). Non-Random Performance. To assess that non-zero task results are not just consequence of random noise, we look at the best score at the last evaluation step among models from M. We first compute the maximum improvement over random baseline b: maxd = max mM (m(n) b) We then estimate the variance at the end of training using the standard deviation (from previous calculation) averaged over the last 5 steps: σend = 1 5 s=n4 σs Finally, The non-randomness score is defined as the ratio of max improvement to this terminal variance: non randomness = maxd σend task satisfies the non-randomness criterion if: non randomness 3 Ordering Consistency To compute how consistently models are ordered as training progresses, we calculate the average Kendall Tau-a between model rankings at consecutive steps in the second half of training. We ignore the first 15 billion tokens, as we are interested in this property at later stage of training, and in the first half, we found the ordering to be very inconsistent, skewing the overall score. First, we define Kendall Tau-a of model ranking as: τa(x, y) = (n 2) where and are the number of concordant and discordant pairs between the rankings and of the model scores at steps si and si+1. The overall consistency is: ordering consistency = 1 (si,si+1)P τa (r(si), r(si+1)) where is the set of consecutive step pairs in the latter half of the training, and r(s) is the ranking of model scores in step s. While we first considered using the criteria for selection, we could not determine reliable threshold for the criterion and therefore only use it for observational reasons. A.5.2 Metrics and Formulation For non-generative tasks, we compute accuracies using Cloze Formulation (CF, completing with the full option text) in place of the more commonly used Multi-Choice Formulation 32 Preprint. Under review. (MCF, completing with A/B/C/D), as previous work has shown that MCF has random performance in the early stages of training (Gu et al., 2025; Li et al., 2024b). Additionally, since all models that we compare use the same tokenizer, we normalize answer log-probabilities based on token count instead of number of characters, and use pointwise mutual information (PMI) (Gu et al., 2025) for more difficult tasks such as AGIEval (Zhong et al., 2023b) or translated versions of MMLU (Hendrycks et al., 2021). For these tasks, we use the F1-score of overlapping words, as it is generally less noisy and more resilient to small changes in the generations than exact matching (which in turn might be more appropriate for math related tasks, which we do not evaluate on). 33 Preprint. Under review. A.5.3 List of selected evaluation tasks for canary languages Task Type Metric Mono SNR Rand Order RC Acc (Char) Belebele (Almazrouei et al., 2023) GK Acc (PMI) ArabicMMLU (Koto et al., 2024) X-CSQA (Lin et al., 2021a) RES Acc (PMI) Alghafa: MCQ Exams (Almazrouei et al., 2023) GK Acc (Token) RC Acc (Token) Alghafa: SOQAL (Almazrouei et al., 2023) GK Acc (Token) Alghafa: ARC Easy (Leaderboard, 2024) NLU Acc (Token) Okapi: Hellaswag (Lai et al., 2023) RES Acc (Token) OALL2024: PIQA (Leaderboard, 2024) RC Acc (Token) OALL2024: RACE (Leaderboard, 2024) GK Acc (Token) OALL2024: SCIQ (Leaderboard, 2024) RES Acc (Token) X-CODAH (Lin et al., 2021a) NLU Acc (Token) X-Story Cloze (Lin et al., 2021b) GK ARCD (Mozannar et al., 2019) RC MLQA (Lewis et al., 2020) RC Tydiqa (Clark et al., 2020) F1 F1 F1 0.61 0.81 0.65 0.51 0.74 0.74 0.80 0.81 0.82 0.80 0.75 0.87 0.83 0.86 0.86 58.23 14.67 80.00 18.28 33.44 11.13 35.49 8.89 46.22 33.78 76.58 35.41 43.05 12.01 69.34 7.69 66.01 18.22 74.06 32.87 24.80 8.50 93.20 9.76 28.28 35.58 17.27 24.83 27.17 55.07 0.13 0.91 0.91 0.61 0.11 0.91 0.97 0.71 0.43 0.70 0.31 0.83 0.83 0.87 0. Table 6: Selected tasks for Arabic satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order AGIEval (ZH subset) (Zhong et al., 2023a) GK Acc (PMI) RES Acc (PMI) X-CSQA (Lin et al., 2021a) RC Acc (Token) Belebele (Bandarkar et al., 2024) RC Acc (Token) C3 (Sun et al., 2020) GK Acc (Token) C-Eval (Huang et al., 2023) CMMLU (Li et al., 2024a) GK Acc (Token) NLU Acc (Token) Okapi: Hellaswag (Lai et al., 2023) M3Exam (Zhang et al., 2023) GK Acc (Token) RES Acc (Token) X-CODAH (Lin et al., 2021a) RES Acc (Token) X-COPA (Ponti et al., 2020) NLU Acc (Token) X-Story Cloze (Lin et al., 2021b) NLU Acc (Token) X-Winograd (Muennighoff et al., 2022) RC Chinese SQuAD (Pluto-Junzeng, 2019) RC CMRC (Cui et al., 2018) RC MLQA (Lewis et al., 2020) F1 F1 F1 0.46 0.83 0.51 0.87 0.75 0.91 0.87 0.74 0.66 0.80 0.87 0.88 0.85 0.91 0. 98.82 15.86 25.63 10.09 74.30 15.36 72.89 36.01 50.20 8.04 117.92 21.93 70.60 21.95 36.02 8.75 32.65 14.72 77.20 15.06 79.20 15.57 102.87 21.83 27.71 27.40 25.33 34.43 23.76 20.40 0.86 0.89 0.70 0.66 0.53 0.96 0.97 0.67 0.66 0.69 0.84 0.86 0.90 0.67 0.86 Table 7: Selected tasks for Chinese satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) 34 Preprint. Under review. Task Type Metric Mono SNR Rand Order Okapi: ARC (Lai et al., 2023) GK Acc (PMI) Meta MMLU (Grattafiori et al., 2024) GK Acc (PMI) RES Acc (PMI) X-CSQA (Lin et al., 2021a) RC Acc (Token) Belebele (Bandarkar et al., 2024) NLU Acc (Token) Okapi: Hellaswag (Lai et al., 2023) RES Acc (Token) X-CODAH (Lin et al., 2021a) RC FQuad (dHoffschmidt et al., 2020) GK Mintaka (Sen et al., 2022) F1 F1 0.69 0.87 0.83 0.85 0.96 0.74 0.91 0.82 30.10 3.33 107.58 10.95 30.50 11.01 5.65 33.68 71.11 30.84 33.68 9.19 14.64 19.08 12.92 6.91 0.47 0.56 0.76 0.39 0.70 0.74 0.69 0. Table 8: Selected tasks for French satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order Meta MMLU (Grattafiori et al., 2024) GK Acc (PMI) RES Acc (PMI) X-CSQA (Lin et al., 2021a) RC Acc (Token) Belebele (Bandarkar et al., 2024) NLU Acc (Token) Okapi: Hellaswag (Lai et al., 2023) GK Acc (Token) Okapi: ARC (Lai et al., 2023) RES Acc (Token) X-CODAH (Lin et al., 2021a) NLU Acc (Token) X-Story Cloze (Lin et al., 2021b) RC IndicQA (Singh et al., 2025) F1 0.68 0.60 0.61 0.87 0.95 0.53 0.74 0. 97.78 9.13 22.84 4.45 66.05 6.65 47.47 16.35 62.19 23.11 39.83 14.13 87.75 8.39 13.20 12.20 0.33 1.00 0.76 1.00 0.67 0.67 1.00 0.81 Table 9: Selected tasks for Hindi satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order GK Acc (PMI) Okapi: ARC (Lai et al., 2023) GK Acc (PMI) RUMMLU (Fenogenova et al., 2024) RES Acc (PMI) X-CSQA (Lin et al., 2021a) RC Acc (Token) Belebele (Bandarkar et al., 2024) NLU Acc (Token) Okapi: Hellaswag (Lai et al., 2023) Parus (Fenogenova et al., 2024) RES Acc (Token) OpenBookQA (Fenogenova et al., 2024) RES Acc (Token) RES Acc (Token) X-CODAH (Lin et al., 2021a) NLU Acc (Token) X-Story Cloze (Lin et al., 2021b) RC Sber SQuAD (Efimov et al., 2020) RC Tydiqa (Clark et al., 2020) RC X-QuAD (Artetxe et al., 2020a) F1 F1 F1 0.55 0.77 0.73 0.81 0.97 0.93 0.73 0.85 0.93 0.89 0.92 0.90 35.17 3.76 64.24 6.10 38.45 16.03 61.97 19.26 86.76 28.22 81.06 24.61 43.43 18.08 26.97 6.79 66.81 12.04 9.93 10.85 10.44 11.28 7.56 8.79 0.53 0.56 0.71 0.71 0.83 0.67 0.73 0.50 0.84 0.84 0.83 0.60 Table 10: Selected tasks for Russian satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order Okapi: ARC (Lai et al., 2023) Belebele (Bandarkar et al., 2024) M3Exam (Zhang et al., 2023) X-COPA (Ponti et al., 2020) X-Story Cloze (Lin et al., 2021b) KenSWQuAD (Wanjawa et al., 2023) RC RC Tydiqa (Clark et al., 2020) GK Acc (Token) RC Acc (Token) GK Acc (Token) RES Acc (Token) NLU Acc (Token) F1 F1 0.88 0.44 0.63 0.82 0.86 0.91 0.65 6.32 60.69 5.44 65.26 3.52 34.82 74.71 4.66 130.08 20.54 12.95 12.43 12.67 15. - - - - - - - Table 11: Selected tasks for Swahili satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) 35 Preprint. Under review. Task Type Metric Mono SNR Rand Order Okapi: Hellaswag (Lai et al., 2023) NLU Acc (Token) GK Acc (Token) Okapi: MMLU (Lai et al., 2023) RES Acc (Token) X-COPA (Ponti et al., 2020) NLU Acc (Token) X-Story Cloze (Lin et al., 2021b) RC IndicQA (Singh et al., 2025) F1 0.82 0.92 0.77 0.67 0.72 56.06 7.84 148.57 4.11 69.31 6.01 108.25 8.02 9.65 12.39 - - - - - Table 12: Selected tasks for Telugu satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order GK Acc (PMI) Meta MMLU (Grattafiori et al., 2024) Belebele (Bandarkar et al., 2024) RC Acc (Token) Translated Hellaswag (Patteera, 2023) NLU Acc (Token) GK Acc (Token) M3Exam (Zhang et al., 2023) ThaiQA (Trakultaweekoon et al., 2019) RC RC X-QuAD (Artetxe et al., 2020a) F1 F1 0.54 0.63 0.69 0.75 0.90 0.90 93.51 6.42 53.88 13.65 52.78 11.51 45.32 4.24 20.39 15.92 17.45 20. 0.60 0.66 0.53 0.50 0.66 0.80 Table 13: Selected tasks for Thai satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Task Type Metric Mono SNR Rand Order GK Acc (Char) TR Leaderboard: ARC (Alhajar, 2024) RC Acc (Char) Belebele (Bandarkar et al., 2024) GK Acc (Char) Exams (Hardalov et al., 2020) NLU Acc (Char) Okapi: Hellaswag (Lai et al., 2023) X-COPA (Ponti et al., 2020) RES Acc (Char) TR Leaderboard: MMLU (Alhajar, 2024) GK Acc (PMI) THQuAD (Soygazi et al., 2021) X-QuAD (Artetxe et al., 2020a) RC RC F1 F1 0.91 0.50 0.78 0.95 0.61 0.81 0.93 0.92 49.33 21.32 47.97 5.93 31.73 5.96 58.56 21.45 81.18 11.43 95.48 12.60 17.06 20.03 26.33 28.74 0.79 0.09 0.33 0.90 0.66 0.61 0.60 0.73 Table 14: Selected tasks for Turkish satisfying the early-signal conditions: Monotonicity (Mono), Signal-to-noise ratio (SNR), Non-Randomness (Rand) and Ordering Consistency (Order) Preprint. Under review. A.6 Language Identification A.6.1 Classifier choice While Transformer-based LID classifiers exist (Bapna et al., 2022), they are too slow and expensive to run at large scale. Most commonly used LID classifiers are simple models based on character level n-grams like CLD3 (Salcianu et al., 2018) (107 supported languages), used in mC4, or classifiers following the fastText architecture (Joulin et al., 2016), such as FT176 (176 languages) used in CC-100 (Wenzek et al., 2020; Conneau et al., 2020), and CulturaX (Nguyen et al., 2024), as well as in many English-only datasets (Soldaini et al., 2024; Penedo et al., 2023); OpenLID (Burchell et al., 2023) (193 languages), used in HPLT2 (Burchell et al., 2025); and the recent GlotLID (Kargaran et al., 2023) (1880 languages). We used the GlotLID (Kargaran et al., 2023), specifically version V3the latest available at the time of our experiments (Kargaran et al., 2024). This LID classifier covers large number of languages and addresses some common issues in LID classifiers: Its large language coverage can reduce out-of-model cousin errors (Caswell et al., 2020; Bapna et al., 2022), where unsupported languages can be misclassified as closely related supported language It explicitly distinguishes scripts (Latin, Arabic, Cyrillic, etc), improving detection for languages that support multiple scripts It provides different labels based on script e.g. arb Arab is Standard Arabic in Arabic script, while arb Latn is Standard Arabic in Latin script, allowing us to tailor the filtering to each script It includes an UND label for non supported scripts, so that languages that use them arent misclassified as supported languages Includes specific labels trained on noise documents, such as text decoded with the wrong encoding, binary content, or misrendered PDFs, preventing it from being classified as natural text language In Fig. 5 we present comparison between GlotLID and FT176, without any threshold filtering. Figure 5: FT176 vs GlotLID without any threshold filtering applied to either classifier. While GlotLID seems to outperform in higher resource languages, FT176 performs slightly better on lower resource languages. However, GlotLID supports considerably larger number of (lower-resource) languages. 37 Preprint. Under review. A.6.2 Confidence Threshold For each language, we set thresholds at regular removal intervals (threshold to remove 5% of data, 10% of data, etc) and at other values of interest (e.g., 0.7, 0.9). We then train models on 30 billion tokens of the filtered data using each threshold and evaluate the resulting models. In Tables 16 to 24, the highest scoring range of thresholds are marked in bold. Our formula to automatically set thresholds based on the mean and standard deviation of each languages confidence scores distribution selects values within the highest scoring range for all languages except Chinese and Hindi  (Table 15)  ."
        },
        {
            "title": "Arabic\nChinese\nFrench\nHindi\nRussian\nSwahili\nTelugu\nThai\nTurkish",
            "content": "0.883 0.895 0.750 0.483 0.8 0.186 0.701 0.9 0.866 0.9 0.937 0.932 0.557 - 0.544 0.701 0.961 - 0.8812 0.7415 0.8195 0.6827 0.9 0.3 0.7002 0.9 0.8753 Table 15: Minimum and Maximum refer to the highest performing threshold range endpoints. Formula is the value defined by our threshold setting formula, which we ultimately use for all languages. Model 1 Model 2 Model 3 Model 4 Model Threshold % Removed Aggregate Score 0.000 0.0% 14.9% 0.968 0.700 3.0% 10.0% 15.2% 15.4% 16.1% 15.2% 0.883 5.0% 0.900 5.4% Table 16: Arabic Threshold Analysis Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Threshold % Removed Aggregate Score 0.000 0.0% 14.4% 0.214 5.0% 14.4% 0.429 10.0% 14.8% 0.678 15.0% 14.8% 0.937 0.895 0.806 20.0% 30.0% 25.0% 14.4% 15.1% 15.1% Table 17: Chinese Threshold Analysis 38 Preprint. Under review. Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Threshold % Removed Aggregate Score 0.000 0.0% 10.1% 0.467 5.0% 10.5% 0.932 0.800 0.723 10.0% 20.0% 12.2% 10.5% 11.1% 10.7% 11.0% 11.2% 0.867 15.0% 0.750 10.7% Table 18: French Threshold Analysis Model 1 Model 2 Model 3 Model 4 Model 5 Threshold % Removed Aggregate Score 0.000 0.0% 9.7% 0.483 5.0% 9.8% 0.557 10.0% 9.8% 0.616 15.0% 9.2% 0.669 20.0% 9.3% Model 6 Model 7 Model 8 Model 9 Model 10 Model 11 Threshold % Removed Aggregate Score 0.714 25.0% 9.5% 0.752 30.0% 9.7% 0.786 35.0% 9.0% 0.815 40.0% 9.3% 0.840 45.0% 9.2% 0.862 50.0% 8.7% Table 19: Hindi Threshold Analysis Model 1 Model 2 Model 3 Model 4 Threshold % Removed Aggregate Score 0.000 0.0% 12.0% 0.918 0.800 0.750 2.5% 5.0% 2.9% 12.1% 12.4% 12.7% Table 20: Russian Threshold Analysis Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Model 8 Threshold % Removed Aggregate Score 0.000 0.0% 8.8% 0.075 5.0% 9.7% 0.098 10.0% 8.7% 0.132 20.0% 8.8% 0.167 30.0% 9.2% 0.544 0.300 0.186 70.0% 64.2% 50.0% 10.9% 10.9% 11.6% Table 21: Swahili Threshold Analysis Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Model 8 Threshold % Removed Aggregate Score 0.000 0.0% 5.4% 0.207 5.0% 5.0% 0.262 10.0% 5.4% 0.297 15.0% 5.1% 0.515 20.0% 5.2% 0.600 22.4% 5.3% 0.701 25.0% 5.9% 0.996 30.0% 5.4% Table 22: Telugu Threshold Analysis Model 1 Model 2 Model 3 Model Threshold % Removed Aggregate Score 0.000 0.0% 6.8% 0.800 2.7% 6.2% 0.900 3.5% 6.8% 0.961 5.0% 6.9% Table 23: Thai Threshold Analysis Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Threshold % Removed Aggregate Score 0.000 0.0% 10.9% 0.704 5.0% 10.3% 0.724 6.1% 9.3% 0.750 6.6% 10.2% 0.800 0.866 0.932 13.9% 10.0% 7.7% 10.5% 11.5% 11.4% Table 24: Turkish Threshold Analysis 39 Preprint. Under review. A.7 Filtering A.7.1 Stopwords As mentioned in Section 4.4.1, we analyzed word frequencies in our reference datasets (Wikipedia) using our word tokenizers to identify the most frequently occurring words. After counting word occurrences directly on the raw data of our reference datasets, we noticed that some stopwords were actually non-alphabetic symbols or numbers rather than meaningful words. To refine the list, we removed all numbers and symbols. If fewer than eight stopwords (eight being the number of stopwords in the original Gopher English stopword list (Rae et al., 2022)) remained after this filtering, we lowered the frequency threshold to increase the number of stopwords and ensure sufficient stopword coverage. When analyzing English stopwords from the Wikipedia reference dataset, we found that the original Gopher quality filter did not necessarily select the most frequent words. This suggested different selection criterion had been used. However, since our method is scalable across languages and performs well in experiments, we adopted it as our approach and collected stopwords for each language supported by GlotLID (on Wikipedia when available, and on GlotLID-Corpus for languages that do not have their own Wikipedia). When reviewing the languages with the largest amount of data after LID and stopwords filtering, we noticed that some low resource languages had an unexpectedly large amount of data. For example, Dagbani, language from the NigerCongo family with around 1 million native speakers and low internet presence, ended up with large amount (2TB) of text data after language filtering. Through manual inspection, we found that most of this data was misclassified English and German. We had expected that the stopwords filter would remove most of this non-Dagbani content; however, the filter removed very little. Inspecting the list of Dagbani stopwords revealed high amount of English words (shown in bold): the, ni, of, a, in, ka, and, o, be, daa, to, di, n, nyEla, or, is Through further investigation, we found that many other languages had English stopwords in their list. We traced this issue to the Wikipedias for lower resource languages, where many articles are directly copied untranslated from the English wikipedia (for later translation) and some boilerplate/meta pages exist in the original English. As language classifiers are often trained on Wikipedia, this may explain why English data is mislabeled as these lowe-resource languages in the first place. We cleaned Wikipedias by: a) removing the notes and references sections, which sometimes are in other languages and follow very specific format; b) dropping articles where the most common script doesnt match what we expected for the language; c) dropping articles where our language classifier predicted English with above 70% confidence. We then recomputed stopwords on this new clean version of Wikipedia, which resulted in >99% removal rate when filtering Dagbani data using the updated stopwords: ni, ka, o, daa, di, n, nyEla, din, ti, bE, be, nyE, maa 40 Preprint. Under review. A.7.2 Filtering thresholds Filtering details We employ the following filters from FineWeb with fixed thresholds for all languages, only changing the way words are defined depending on each languages word-level tokenizer (Section 2): FineWeb Quality filters: ratio of characters in duplicate lines 0.1; Gopher Quality filters: 50 #words 100000; ratio of symbols to #words 0.1; ratio of bullet points to #lines 0.9; ratio of ellipsis to #lines 0.3, stop words in document 2 (with stopwords determined following Section 4.4.1); We tune the following filters with the different adaptation methods we consider: FineWeb Quality filters: maximum ratio of lines not ending with punctuation; maximum ratio of #lines to #words Gopher Quality filters: maximum average word length; minimum average word length; maximum ratio of non alphabetic words; Gopher Repetition filters: fraction of duplicate lines, fraction of characters taken up by the most common 2-, 3and 4-grams; fraction of characters taken up by every single repeated 5-, 6-, 7-, 8-, 9-, and 10-gram Results from training models on data obtained by applying the different adaptations methods to each group filters can be seen in Table 25. We select the best performing method for each filter group (marked in bold) for our pipeline. We also show the average removal rates across languages of each method in Table 26. Filter Group Baseline English 10Tail MeanStd MedRatio Quant 10Tail MeanStd MedRatio Quant wiki cc fwq goq gor 7.00 6.33 6.22 - - 4.22 - 5.22 3.33 5.22 - 2.22 4.00 3.89 - 4.33 4.56 4. 3.00 4.44 - 5.00 4.11 3.89 3.89 4.22 - 3.56 3.22 4.00 Table 25: Average ranks by block and method across all languages. Baseline has no filtering, English is the default FineWeb English thresholds. We then compute each of the other 4 methods 10Tail, MeanStd, MedRatio (MedianRatio), and Quantile (Quant) on both Common Crawl (cc) data and on Wikipedia (wiki). Cells marked with - correspond to method-filter-group combinations that would remove over 75% of data with single filter on at least one of the languages, or that would not remove anything at all. Lower ranks are better. Filter Group English 10Tail MeanStd MedRatio Quant cc wiki 10Tail MeanStd MedRatio Quant fwq goq gor - - - 41.42% 26.39% 29.53% 25.63% 36.81% - 38.03% 33.82% 40.35% 38.04% 47.09% 45.14% 49.23% 47.58% 24.79% 26.08% - - 37.61% 44.31% 46.90% 46.81% 26.50% - Table 26: Average removal rates by method across datasets. Values represent percentage of data filtered. 41 Preprint. Under review. A.7.3 Precision filtering lower resource languages Language Identification precision computed on balanced test set does not correspond to the precision on web crawled data, due to class imbalance between highand low-resource languages. Precision on the crawled corpora can be calculated as in Caswell et al. (2020), where is the real proportion of the target language in the full web crawl: precisioncrawl = recall recall + (1 x) fpr For low-resource languages (low x), low false positive rate (fpr) is crucial, as higher false positives significantly reduce precision. For high-resource languages, web presence is high, so false positives are less critical. If low-resource language is sufficiently distinct from high-resource languages, the false positive rate will often be low. However, if closely related high-resource language exists, the high-resource language may be misclassified as low-resource. In such scenarios, ngram-based LID fails because common n-grams lead to misclassification of high-resource language sentences as low-resource. Wordlist filtering To maintain high precision for low-resource languages after LID, wordlist filtering is suggested to retain in-language documents (Caswell et al., 2020; Bapna et al., 2022). To build such wordlists, we propose simple approach: only consider tokens whose affinity exceeds high threshold (we use γ = 0.85) for each language. The affinity of token in language is defined as: Affinity(t, l) = ft,l lL ft,l where ft,l is the raw count of token in language l, and lL ft,l is the total count of token across all languages in the set L. text labeled as low-resource language is considered in-language if some of its words appear in the wordlist created for l; otherwise, it is considered contaminated. We used data from the GlotLID-Corpus (Kargaran et al., 2023) to create wordlists, applying the tokenizer specific to each language from Appendix A.1. For each language l, the same tokenizer (the tokenizer of language l) is used to compute ft,l to ensure consistent separation of words. We use wordlist filtering as an indicator of contamination, where the contamination score is defined as the percentage of documents removed by the filter. This helps identify languages with low quality for manual auditing. We select 10,000 random documents from each language and calculate the percentage of documents filtered. glk Arab is one of the languages with the highest contamination score. We present the distribution of contamination scores for 1,900 languages for which we have wordlists in Fig. 6. The majority of the languages have their data in-language (non-contaminated). However, around third of them have contamination scores above 10%. URL Whitelist Manual inspection of the filtering process revealed that some of the wordlists were too strict. This was the case of some English-based Pidgin languages, such as Nigerian Pidgin, for example, where the resulting wordlist was relatively short. To avoid excessive filtering caused by strict wordlists, we additionally kept documents removed by the wordlist filtering whose URLs contained specific terms related to the language (the language code, the name of the language, possibly top level domains for that region and/or names of regions where the language is spoken). For Nigerian Pidgin, this list contained the following words: pcm, pidgin, naija, .ng, nigerianpidgin, nigeria, and nigerian. We show example URLs containing in-language content that had been removed by wordlist filtering that are then caught by the URL Whitelist in Table 27. 42 Preprint. Under review. Figure 6: Contamination scores for 1,900 languages via wordlist filtering. The plot indicates that the majority of the languages have their data in-language (non-contaminated). URL Matched Words http://www.supersport.com/football/nigeria-naija/news/121221/ Uefa don ban Malaga https://manutdinpidgin.com/2018/06/28/ manchester-united-target-sergej-milinkovic-savic-don-react-ontop-the-transfer-rumour/ https://pcm.wikipedia.org/wiki/Japan https://www.bbc.com/pidgin/sport-43612518 nigeria, nigerian, naija pidgin pcm pidgin Table 27: Matched words for selected URLs in Nigerian Pidgin Filtering results We audit three low-resource languages: glk Arab, bar Latn, and ary Arab, by asking native speakers to manually label 2,000 randomly sampled documents as being in-language or not. The results of applying wordlist filtering with the URL Whitelist to these languages are shown in Table 28. Applying wordlist filtering maintains recall while improving precision for both glk Arab and bar Latn. However, for ary Arab, the improvement is not very significant. This is because the training data for LID does not adequately represent ary Arab. Precision could be increased further by requiring certain fraction of the document to be contained in the wordlist (instead of just single word), but this would require manual tuning and could result in drop in recall. Language glk Arab bar Latn ary Arab Pre-filtering Precision Filtering Recall Precision 2.10% 69.45% 1.75% 95.24% 27.21% 97.77% 94.90% 88.57% 4.14% Table 28: Evaluation Results for wordlist filtering Based on the Audit We publicly release our wordlists and code.6 6https://github.com/huggingface/fineweb-2/tree/main/misc/precision filtering 43 Preprint. Under review. A."
        },
        {
            "title": "Improvement from each pipeline step",
            "content": "LID Language Identification and threshold LID + LID & global MinHash deduplication LID + + LID + & heuristic filtering FW2 (R) LID + + & rehydration (deduplication informed upsampling) Task Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID PIQA SOQAL Alghafa: MCQ Exams (GK) Belebele (RC) Alghafa: (RC) Alghafa: ARC Easy (GK) Okapi: Hellaswag (NLU) OALL2024: (RES) OALL2024: (RC) OALL2024: (GK) X-CODAH (RES) X-CSQA (RES) X-Story Cloze (NLU) ARCD (GK) MLQA (RC) Tydiqa (RC) ArabicMMLU (GK) RACE SCIQ GK tasks RC tasks RES tasks NLU tasks Aggregate Score Task Okapi: ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) X-CODAH (RES) X-CSQA (RES) FQuad (RC) Mintaka (GK) Meta MMLU (GK) GK tasks RC tasks RES tasks NLU tasks Aggregate Score 25.0 25.0 20.0 37.1 16.1 35.5 14. 35.6 14.1 36.3 15.1 32.1 64.0 9.4 55. 31.4 62.3 8.5 52.9 32.6 67.9 10.1 59.9 33.9 67.9 11.9 59. 25.0 38.1 17.5 39.2 18.9 40. 21.2 41.1 21.5 25.0 36.4 15. 39.0 18.6 40.9 21.2 43.3 24. 50.0 58.4 16.8 60.9 21.8 61. 23.6 61.9 23.8 25.0 32.5 9. 32.9 10.5 33.9 11.8 34.4 12. 25.0 66.8 55.7 67.3 56.4 68. 58.4 67.9 57.2 25.0 20.0 50.0 0.0 0.0 0.0 28.0 - - - - - 35.8 32.8 59.0 29.9 22.2 39.5 39.9 14.4 16.0 18.1 29.9 22.2 39.5 16.6 34.0 32.5 58.9 32.0 21.5 37.9 40.0 12.0 15.6 17.8 32.0 21.5 37.9 16.7 40.0 32.7 59.8 33.0 21.3 36.8 40.1 19.9 15.8 19.7 33.0 21.3 36.8 16. 38.5 34.2 60.9 33.1 23.2 36.5 41.1 18.0 17.8 21.9 33.1 23.2 36.5 18.2 27.2 27.2 15.7 16.6 21.7 27.6 26.2 16.5 18.2 22. 28.7 28.0 19.8 20.4 24.2 29.0 28.8 19.8 23.1 25.2 Table 29: Arabic Results Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID 25.0 25.0 25.0 25.0 20.0 0.0 0.0 25. - - - - - 31.3 33.8 45.1 37.0 38.0 28.0 9.5 28.3 8.4 11.7 26.8 15.9 22.5 28.0 9.5 4. 30.0 33.1 47.3 37.0 34.6 27.5 7.5 28.4 6.7 10.8 29.7 16.0 18.2 27.5 7.5 4.5 31.9 35.2 51.8 40.5 39.8 29.3 8.9 29. 9.2 13.5 35.8 20.6 24.7 29.3 8.9 5.3 33.0 36.0 52.6 42.3 40.4 35.0 8.1 29.6 10.6 14.6 36.8 23.1 25.5 35.0 8.1 6. 7.4 19.9 19.2 26.8 18.3 6.2 19.2 17.1 29.7 18.0 7.8 21.4 22.7 35.8 21. 8.3 24.8 24.3 36.8 23.6 Table 30: French Results 44 Preprint. Under review. Task Okapi: ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) Parus (RES) OpenBookQA (RES) X-CODAH (RES) X-CSQA (RES) X-Story Cloze (NLU) Sber SQuAD (RC) Tydiqa (RC) X-QuAD (RC) RUMMLU (GK) GK tasks RC tasks RES tasks NLU tasks Aggregate Score Task Belebele (RC) Translated Hellaswag (NLU) M3Exam (GK) ThaiQA (RC) X-QuAD (RC) Meta MMLU (GK) GK tasks RC tasks NLU tasks Aggregate Score Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID 25.0 25.0 25.0 50.0 25.0 25.0 20.0 50.0 0.0 0.0 0.0 25.0 - - - - - 29.1 34.0 41.0 64.9 36.0 33.9 35.3 66.9 27.8 29.9 19.6 29. 5.4 12.0 21.3 29.9 14.7 11.8 19.2 33.7 27.8 29.9 19.6 5.7 30.4 33.7 43.6 65.7 36.0 34.9 37.4 66.7 32.4 32.4 22.8 29.0 7.1 11.6 24.8 31.4 14.7 13.2 21.8 33.5 32.4 32.4 22.8 5. 5.6 22.3 18.9 27.5 18.6 6.3 24.8 20.2 29.1 20.1 Table 31: Russian Results 33.8 34.8 45. 68.2 35.9 35.4 35.0 68.7 32.9 36.7 23.6 29.7 11.7 13.0 27.8 36.4 14.5 13.8 18.7 37.5 32.9 36.7 23.6 6.3 9.0 26.5 20.9 32.6 22.3 32.2 36.4 46. 68.1 38.3 37.1 38.6 69.4 37.1 35.5 25.2 30.1 9.6 15.2 29.0 36.2 17.7 16.2 23.3 38.9 37.1 35.5 25.2 6.8 8.2 28.2 23.4 34.0 23.4 Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID 25.0 25.0 22.9 0.0 0.0 25. - - - - 31.6 32.5 27.6 27.2 19.6 27.6 8.7 10.0 6.1 27.2 19.6 3. 31.5 33.1 28.1 23.8 18.6 27.4 8.7 10.8 6.7 23.8 18.6 3.2 32.0 35.9 27.5 22.1 17.3 28. 9.4 14.5 5.9 22.1 17.3 4.2 32.9 35.9 28.1 26.3 20.8 28.4 10.5 14.5 6.7 26.3 20.8 4. 4.7 18.5 10.0 11.1 5.0 17.0 10.8 11.0 5.1 16.2 14.5 11. 5.6 19.2 14.5 13.1 Table 32: Thai Results 45 Preprint. Under review. Task Leaderboard: TR ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) X-COPA (RES) THQuAD (RC) X-QuAD (RC) Exams (GK) TR MMLU (GK) Leaderboard: GK tasks RC tasks RES tasks NLU tasks Aggregate Score Task Belebele (RC) C3 (RC) Okapi: Hellaswag (NLU) M3Exam (GK) X-CODAH (RES) X-CSQA (RES) X-COPA (RES) X-Story Cloze (NLU) X-Winograd (NLU) Chinese SQuAD (RC) CMRC (RC) MLQA (RC) AGIEval (ZH subset) (GK) C-Eval (GK) CMMLU (GK) GK tasks RC tasks RES tasks NLU tasks Aggregate Score Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID 25.0 43.7 25.0 45. 26.8 47.9 30.6 46.3 28.4 25.0 25. 50.0 0.0 0.0 23.4 25.0 - - - - - 31.5 42.4 60.7 20.4 15.8 29.4 29.8 8.7 23. 21.3 20.4 15.8 7.8 6.4 32.3 43.3 60.7 25.6 18.2 29.3 30.0 9.7 24.3 21.5 25.6 18.2 7.7 6.7 33.0 45. 62.8 20.6 15.1 28.8 29.8 10.7 27.1 25.6 20.6 15.1 7.1 6.5 34.2 46.8 62.7 26.1 20.2 30.7 29.2 12.2 29. 25.3 26.1 20.2 9.6 5.7 13.1 15.0 21.3 23.3 18.2 13.7 17.8 21.5 24.3 19.3 14.7 15.5 25.6 27. 20.7 14.6 19.5 25.3 29.1 22.1 Table 33: Turkish Results Random Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled LID + + FW2 (R) LID + LID 25.0 27.1 25.0 25.9 25.0 20.0 50.0 50.0 50.0 0.0 0.0 0.0 26.8 25.0 25. - - - - - 32.3 47.6 38.3 32.8 34.0 38.9 60.9 63.0 70.2 23.5 38.2 26.8 32.9 31.6 32.0 9.8 28.2 17. 9.3 12.0 23.6 21.8 26.0 40.3 23.5 38.2 26.8 8.3 8.8 9.4 33.2 47.2 38.6 32.6 32.6 41.9 62.5 61.6 70.9 24.1 38.0 27.8 33.4 32.1 33.0 11.0 27.5 18. 9.0 10.2 27.4 25.0 23.1 41.7 24.1 38.0 27.8 9.1 9.5 10.7 33.0 50.6 41.4 34.1 35.3 41.2 62.0 63.1 72.1 24.1 38.8 28.5 34.1 32.6 34.1 10.7 32.2 21. 11.1 13.7 26.6 24.0 26.2 44.2 24.1 38.8 28.5 10.0 10.1 12.2 34.0 49.2 42.2 34.3 39.0 39.8 64.5 65.5 74.9 26.3 40.2 29.5 33.8 32.7 34.3 12.0 30.3 22. 11.3 18.6 24.7 28.9 30.9 49.8 26.3 40.2 29.5 9.6 10.3 12.4 8.9 25.3 19.1 28.0 20.3 9.6 25.7 20.9 27.7 20. 10.9 26.9 21.4 30.8 22.5 10.9 27.7 24.1 34.6 24.3 Table 34: Chinese Results Preprint. Under review. A.9 Dataset comparison on Canary Languages In addition to the Reference datasets (Section 3.2, we compare FineWeb2 with the concurrent work de Gibert et al. (2024), as well as with the following language-specific datasets: Arabic: ArabicWeb24 (Farhat et al., 2024), Arabic-101B (Aloui et al., 2024) French: Croissant (Faysse et al., 2024) Hindi & Telugu: Sangraha (Khan et al., 2024) Hindi: Odaigen (Parida et al., 2024) Russian: Omnia Russica (Omnia Russica Team, 2024) Thai: Sea CommonCrawl (Dou et al., 2025) Turkish: VNGRS-Web-Corpus (Turker et al., 2024) Chinese: MNBVC (MOP-LIWU Community & MNBVC Team, 2023), TigerBot (TigerResearch, 2023), MAP-CC (Du et al., 2024) Figure 7: Per language comparison of FineWeb2 to other multilingual and language-specific datasets. All models were trained for 30 billion tokens. The plots have sliding window smoothing of size 3. 47 Preprint. Under review. A.10 Dataset comparison on Unseen Languages A.10.1 List of selected evaluation tasks for unseen languages Task Metric Std Meta MMLU (Grattafiori et al., 2024) Acc (PMI) Belebele (Bandarkar et al., 2024) Okapi: Hellaswag (Lai et al., 2023) X-CODAH (Lin et al., 2021a) X-CSQA (Lin et al., 2021a) Mintaka (Sen et al., 2022) MLQA (Lewis et al., 2020) X-QuAD (Artetxe et al., 2020a) 0.0044 Acc (Token) 0.0097 Acc (Token) 0.0043 Acc (Token) 0.0104 Acc (Token) 0.0040 0.0028 0.0192 0.0134 F1 F1 F1 Table 35: Selected tasks for German alongside approximate standard deviation of the scores Task Type Metric Std 0.0093 GK Acc (PMI) Okapi: ARC (Lai et al., 2023) GK Acc (PMI) 0.0030 Indo-MMLU (Koto et al., 2023) Belebele (Bandarkar et al., 2024) RC Acc (Token) 0.0060 Okapi: Hellaswag (Lai et al., 2023) NLU Acc (Token) 0.0063 RES Acc (Token) 0.0061 X-COPA (Ponti et al., 2020) NLU Acc (Token) 0.0053 X-Story Cloze (Lin et al., 2021b) 0.0120 F1 RC Tydiqa (Clark et al., 2020) Table 36: Selected tasks for Indonesian alongside approximate standard deviation of the scores Task Type Metric Std 0.0119 GK Acc (PMI) Okapi: ARC (Lai et al., 2023) 0.0030 Meta MMLU (Grattafiori et al., 2024) GK Acc (PMI) RES Acc (PMI) X-CSQA (Lin et al., 2021a) 0.0096 RC Acc (Token) 0.0036 Belebele (Bandarkar et al., 2024) NLU Acc (Token) 0.0059 Okapi: Hellaswag (Lai et al., 2023) GK Acc (Token) 0.0038 M3Exam (Zhang et al., 2023) RES Acc (Token) 0.0203 X-CODAH (Lin et al., 2021a) RES Acc (Token) 0.0059 X-COPA (Ponti et al., 2020) 0.0029 F1 GK Mintaka (Sen et al., 2022) 0.0155 F1 RC SQuAD-It (Croce et al., 2018) Table 37: Selected tasks for Italian alongside approximate standard deviation of the scores 48 Preprint. Under review. Task Type Metric Std 0.0047 GK Acc (PMI) JMMLU (at Waseda University, 2023) RES Acc (PMI) 0.0168 X-CSQA (Lin et al., 2021a) Belebele (Bandarkar et al., 2024) RC Acc (Token) 0.0047 CommonSenseQA (Kurihara et al., 2022) RES Acc (Token) 0.0089 RES Acc (Token) 0.0088 X-CODAH (Lin et al., 2021a) X-Winograd (Muennighoff et al., 2022) NLU Acc (Token) 0.0092 0.0117 JSQuAD (Kurihara et al., 2022) RC F1 Table 38: Selected tasks for Japanese alongside approximate standard deviation of the scores Task Type Metric Std Okapi: ARC (Lai et al., 2023) 0.0045 GK Acc (PMI) Okapi: MMLU (Lai et al., 2023) 0.0012 GK Acc (PMI) X-COPA (Ponti et al., 2020) RES Acc (PMI) 0.0140 RC Acc (Token) 0.0148 Belebele (Bandarkar et al., 2024) Okapi: Hellaswag (Lai et al., 2023) NLU Acc (Token) 0.0099 GK Acc (Token) 0.0080 M3Exam (Zhang et al., 2023) RES Acc (Token) 0.0045 X-CODAH (Lin et al., 2021a) RES Acc (Token) 0.0120 X-CSQA (Lin et al., 2021a) 0.0118 F1 RC MLQA (Lewis et al., 2020) 0.0067 F1 RC X-QuAD (Artetxe et al., 2020a) Table 39: Selected tasks for Vietnamese alongside approximate standard deviation of the scores Preprint. Under review. A.10.2 Full evaluation results Task Belebele (RC) Okapi: Hellaswag (NLU) X-CODAH (RES) X-CSQA (RES) Mintaka (GK) MLQA (RC) X-QuAD (RC) Meta (GK) MMLU GK tasks RC tasks RES tasks NLU tasks Aggregate Score Task Okapi: ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) X-COPA (RES) X-Story (NLU) Tydiqa (RC) Indo-MMLU (GK) Cloze GK tasks RC tasks RES tasks NLU tasks Aggregate Score Random FineWeb2 (ours) Common Crawl Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled CulturaX HPLT2 25.0 25.0 25.0 20.0 0.0 0.0 0.0 25.0 - - - - - 36.6 42.5 39.7 29.1 5.9 28.1 26.2 29.5 15.4 23.4 19.6 11.4 5.9 28.1 26.2 6.0 34.2 37. 39.1 26.7 6.4 26.2 24.3 27.9 12.3 16.2 18.8 8.3 6.4 26.2 24.3 3.8 35.7 40.8 45.0 26.8 4.6 28.7 23.7 29.0 14.3 21. 26.7 8.5 4.6 28.7 23.7 5.3 36.0 41.3 41.8 29.0 7.7 28.9 24.3 30.0 14.7 21.7 22.4 11.3 7.7 28.9 24.3 6.7 6.0 23.2 15.5 23. 17.0 5.1 20.9 13.6 16.2 13.9 5.0 22.2 17.6 21.0 16.4 7.2 22.6 16.8 21. 17.1 Table 40: German Results Random FineWeb2 (ours) Common Crawl Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled CulturaX HPLT2 25.0 25.0 25. 50.0 50.0 0.0 25.0 - - - - - 30.8 31.8 41.4 63.3 66. 33.6 28.9 7.8 9.1 21.9 26.5 32.1 33.6 5.2 29.1 32.0 38.6 60.9 63. 34.6 28.7 5.4 9.3 18.1 21.7 27.1 34.6 4.9 30.5 32.3 41.8 65.9 63. 29.0 28.0 7.4 9.7 22.4 31.9 27.9 29.0 4.0 33.7 32.1 42.7 66.2 65. 32.3 29.6 11.6 9.5 23.6 32.4 31.5 32.3 6.1 6.5 21.4 26.5 27.0 20. 5.1 21.9 21.7 22.6 17.9 5.7 19.4 31.9 25.2 20.5 8.9 20.9 32.4 27.6 22. Table 41: Indonesian Results 50 Preprint. Under review. Task Okapi: ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) M3Exam (GK) X-CODAH (RES) X-CSQA (RES) X-COPA (RES) Mintaka (GK) SQuAD-It (RC) Meta (GK) MMLU GK tasks RC tasks RES tasks NLU tasks Aggregate Score Task Belebele (RC) CommonSenseQA (RES) X-CODAH (RES) X-CSQA (RES) X-Winograd (NLU) JSQuAD (RC) JMMLU (GK) GK tasks RC tasks RES tasks NLU tasks Aggregate Score Random FineWeb2 (ours) Common Crawl Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled CulturaX HPLT2 25.0 25.0 25.0 33.8 25.0 20.0 50.0 0.0 0.0 25.0 - - - - - 32.4 31.9 45.4 39.1 39.3 37.5 64.8 10.4 20.3 30.1 9.9 9.2 27.2 8.0 19.1 21.9 29.6 10.4 20.3 6.7 28.7 28.7 38. 38.3 38.7 32.8 61.7 7.9 18.2 29.0 4.9 5.0 18.0 6.8 18.2 16.0 23.3 7.9 18.2 5.3 30.0 30.5 43.6 40.0 38.0 37.6 63.0 9.8 22.2 29.1 6.6 7.4 24. 9.5 17.3 21.9 26.0 9.8 22.2 5.5 30.7 30.4 44.4 38.6 38.7 36.1 65.2 10.6 21.8 29.5 7.6 7.2 25.8 7.3 18.2 20.2 30.4 10.6 21.8 5.9 8.8 14.7 23.5 27. 18.6 6.2 11.6 19.2 18.0 13.7 7.8 14.8 21.8 24.8 17.3 7.9 14.5 22.9 25. 17.8 Table 42: Italian Results Random FineWeb2 (ours) Common Crawl Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled CulturaX HPLT2 25.0 20. 25.0 20.0 50.0 0.0 25.0 - - - - - 32.5 67.5 37.7 36.4 60. 40.5 31.7 10.0 59.4 16.9 20.5 20.6 40.5 9.0 31.7 60.9 37.7 36.4 54. 33.1 28.9 8.9 51.2 16.9 20.5 8.9 33.1 5.1 30.3 63.5 38.7 37.2 57. 28.5 30.7 7.1 54.4 18.2 21.5 15.4 28.5 7.5 29.3 50.3 37.4 31.0 59. 11.7 28.7 5.8 37.8 16.6 13.7 18.0 11.7 4.9 9.0 25.3 32.2 20.6 21. 5.1 21.0 29.5 8.9 16.1 7.5 17.8 31.4 15.4 18.0 4.9 8.7 22.7 18.0 13. Table 43: Japanese Results 51 Preprint. Under review. Task Okapi: ARC (GK) Belebele (RC) Okapi: Hellaswag (NLU) M3Exam (GK) X-CODAH (RES) X-CSQA (RES) X-COPA (RES) MLQA (RC) X-QuAD (RC) Okapi: MMLU (GK) GK tasks RC tasks RES tasks NLU tasks Aggregate Score Random FineWeb2 (ours) Common Crawl Baseline Raw Rescaled Raw Rescaled Raw Rescaled Raw Rescaled CulturaX HPLT2 25.0 25.0 25.0 25.2 25.0 20.0 50.0 0.0 0.0 25. - - - - - 31.3 33.0 48.7 35.2 40.3 29.6 75.7 19.4 17.3 29.4 8.4 10.6 31.6 13.3 20.4 12.0 51.3 19.4 17.3 5. 27.2 32.6 43.2 36.7 35.6 28.5 69.7 18.6 16.9 28.5 2.9 10.2 24.2 15.4 14.1 10.6 39.5 18.6 16.9 4.7 30.8 33.1 46.6 38.0 38.2 29.8 64.6 23.4 21.3 28. 7.7 10.8 28.8 17.0 17.6 12.3 29.2 23.4 21.3 4.1 31.1 34.1 44.5 39.1 38.4 29.7 70.5 22.3 21.2 28.8 8.1 12.1 26.0 18.6 17.9 12.2 40.9 22.3 21.2 5. 9.2 15.8 27.9 31.6 21.1 7.6 15.3 21.4 24.2 17.1 9.6 18.5 19.7 28.8 19. 10.6 18.5 23.7 26.0 19.7 Table 44: Vietnamese Results 52 Preprint. Under review. A.11 FineWeb2 language composition Figure 8: Language composition of FineWeb2 Distribution of languages in the final FineWeb2 dataset. Percentages refer to total utf-8 bytes of each language or language family. 53 Preprint. Under review. Table 45: FineWeb2 80 largest language stats ISO 639-"
        },
        {
            "title": "Script Name",
            "content": "rus"
        },
        {
            "title": "Language\nFamily",
            "content": "IndoEuropean"
        },
        {
            "title": "Documents Disk size",
            "content": "588,579,493,780 699,083,579 5.82TB cmn Hani Mandarin Chinese Sino-Tibetan 543,543,038,750 262,271,052,199 deu"
        },
        {
            "title": "Latn German",
            "content": "jpn spa fra ita"
        },
        {
            "title": "Italian",
            "content": "por"
        },
        {
            "title": "Portuguese",
            "content": "pol nld ind vie fas arb tur tha ukr ell kor ces"
        },
        {
            "title": "Grek Modern",
            "content": "(1453-) Hang Korean Latn Czech swe"
        },
        {
            "title": "Swedish",
            "content": "hun ron"
        },
        {
            "title": "Latn Hungarian\nRomanian\nLatn",
            "content": "109,536,087,117 139,116,026,491 220,662,584,640 331,144,301,801 261,523,749,595 74,634,633,118 73,119,437, IndoEuropean Japonic IndoEuropean IndoEuropean IndoEuropean IndoEuropean IndoEuropean IndoEuropean Austronesian 60,264,322,142 50,886,874,358 AustroAsiatic IndoEuropean Afro-Asiatic 32,812,858,120 41,933,799,420 Turkic 24,662,748,945 Kra-Dai 25,586,457,655 IndoEuropean IndoEuropean Koreanic IndoEuropean IndoEuropean Uralic IndoEuropean 30,919,839,164 35,017,893,659 48,613,120,582 35,479,428,809 35,745,969,364 39,705,799,658 22,827,957, 636,058,984 495,964,485 2.42TB 1.51TB 400,138,563 441,287,261 1.50TB 1.32TB 360,058,973 1.11TB 238,984,437 739.24GB 199,737,979 569.24GB 151,966,724 432.01GB 147,301,270 397.51GB 100,238,529 61,064,248 348.65GB 319.83GB 58,843,652 304.62GB 61,977,525 95,129,129 35,897,202 53,101,726 293.59GB 284.52GB 278.68GB 254.86GB 47,421,073 222.05GB 60,874,355 66,067,904 213.43GB 206.33GB 59,485,306 202.96GB 49,935,986 58,303,671 199.69GB 186.19GB nob Latn Norwegian Bokmal Indo32,008,904,934 38,144,343 172.05GB dan"
        },
        {
            "title": "Latn Danish",
            "content": "bul fin hin"
        },
        {
            "title": "Finnish",
            "content": "ben"
        },
        {
            "title": "Beng Bengali",
            "content": "slk"
        },
        {
            "title": "Slovak",
            "content": "heb"
        },
        {
            "title": "Hebr Hebrew",
            "content": "16,074,326,712 28,055,948,840 European IndoEuropean IndoEuropean Uralic IndoEuropean IndoEuropean IndoEuropean Afro-Asiatic 8,462,976,117 20,343,096,672 11,173,681,651 14,808,010,769 6,153,579, 45,391,655 150.72GB 25,994,731 145.75GB 36,710,816 22,095,985 143.03GB 120.98GB 15,185,742 87.04GB 29,991,521 85.43GB 14,491,748 68.71GB 54 Preprint. Under review. Table 45 Continued from previous page ISO 639-"
        },
        {
            "title": "Script Name",
            "content": "lit"
        },
        {
            "title": "Lithuanian",
            "content": "bos"
        },
        {
            "title": "Bosnian",
            "content": "slv"
        },
        {
            "title": "Language\nFamily",
            "content": "IndoEuropean IndoEuropean IndoEuropean"
        },
        {
            "title": "Documents Disk size",
            "content": "9,132,828,961 13,471,965 56.50GB 9,086,837,979 21,243,255 49.18GB 7,688,373,264 12,059,130 41.80GB tam Taml Tamil hrv"
        },
        {
            "title": "Latn Croatian",
            "content": "lvs"
        },
        {
            "title": "Latn",
            "content": "zsm Latn azj srp"
        },
        {
            "title": "Latn\nLatn Catalan",
            "content": "6,564,292,000 8,348,091,726 Standard Estonian Uralic IndoEuropean Dravidian IndoEuropean IndoEuropean Austronesian 5,648,387,840 3,894,255,826 2,858,500,314 1,937,150,898 6,609,299,"
        },
        {
            "title": "Standard Malay",
            "content": "5,371,151,279 1,439,572,993 1,642,856,349 1,054,187,581 1,876,843,453 2,733,266,"
        },
        {
            "title": "Cyrl Kazakh\nArab Urdu",
            "content": "Geor Georgian Deva Nepali (individual language) Latn North Azerbaijani Turkic IndoCyrl European Kartvelian IndoEuropean IndoEuropean Dravidian Turkic IndoEuropean IndoEuropean IndoEuropean 891,002,487 Dravidian 748,850,327 Dravidian Sino-Tibetan 854,400,671 934,124,052 IndoEuropean IndoEuropean IndoEuropean AustroAsiatic 667,495,"
        },
        {
            "title": "Cyrl",
            "content": "1,541,225,070 3,454,387,059 1,611,392,841 1,166,541,148 1,696,354,360 Cyrl Halh Mongolian Mongolic Latn Arab Moroccan Arabic Afro-Asiatic 843,523,994 Latn Afrikaans Austronesian 1,636,238,017 1,598,352,868 824,211,"
        },
        {
            "title": "Filipino",
            "content": "mal Mlym Malayalam kaz urd mkd Cyrl Macedonian"
        },
        {
            "title": "Telu\ntel\nKnda Kannada\nkan\nmya Mymr Burmese\nGujr Gujarati\nguj",
            "content": "ekk cat kat npi mar als bel isl khk fil ary afr khm Khmr Khmer 10,218,587 17,136,414 40.82GB 40.35GB 5,528,854 6,195,824 36.97GB 35.91GB 8,030,316 33.36GB 9,421,248 7,291,231 4,146,124 3,706,659 4,888,163 31.94GB 26.90GB 26.87GB 25.23GB 25.13GB 3,912,702 22.57GB 3,322,526 3,344,366 4,809,542 22.27GB 20.67GB 19.93GB 8,597,826 18.18GB 4,150,902 14.99GB 1,964,395 2,390,982 1,558,304 2,127,094 14.42GB 12.91GB 12.35GB 11.71GB 2,100,873 11.47GB 3,014,429 10.27GB 1,586,460 8.70GB 1,622,882 2,349,050 2,365,405 1,992,040 8.52GB 8.13GB 7.74GB 7.69GB hye"
        },
        {
            "title": "Armn Armenian",
            "content": "sin glg"
        },
        {
            "title": "Latn Galician",
            "content": "uzn"
        },
        {
            "title": "Cyrl Northern Uzbek",
            "content": "634,273,060 1,757,415 7.17GB 512,453,069 1,185,323 7.05GB 1,236,233,473 2,522,814 6.47GB 544,866,919 1,357,811 6.12GB IndoEuropean IndoEuropean IndoEuropean IndoEuropean Turkic 55 Preprint. Under review. ISO 639-"
        },
        {
            "title": "Script Name",
            "content": "pan"
        },
        {
            "title": "Guru Panjabi",
            "content": "ory"
        },
        {
            "title": "Orya Odia",
            "content": "Table 45 Continued from previous page"
        },
        {
            "title": "Documents Disk size",
            "content": "uzn kir eus"
        },
        {
            "title": "Latn Northern Uzbek\nCyrl Kirghiz\nBasque\nLatn",
            "content": "lat"
        },
        {
            "title": "Latin",
            "content": "tgk"
        },
        {
            "title": "Tajik",
            "content": "gmh swh arz nno"
        },
        {
            "title": "Latn",
            "content": "Latn Middle High German (ca. 1050-1500) Swahili (individual language) Arab Egyptian Arabic Latn Norwegian cym Latn Welsh"
        },
        {
            "title": "Nynorsk",
            "content": "amh pbt"
        },
        {
            "title": "Amharic\nSouthern Pashto",
            "content": "ckb"
        },
        {
            "title": "Arab Central Kurdish",
            "content": ". . . 7 Total 522,788,467 333,760,951 714,764,848 396,209,383 687,002,994 397,449,282 711,939, IndoEuropean IndoEuropean Turkic Turkic Language isolate IndoEuropean IndoEuropean IndoEuropean NigerCongo Afro-Asiatic 345,040,810 522,740,774 IndoEuropean IndoEuropean Afro-Asiatic 239,936,286 337,138,269 IndoEuropean IndoEuropean 569,542,024 236,342,609 523,226,616 506,396,917 944, 5.64GB 1,298,188 4.92GB 1,233,463 1,069,582 1,569,434 4.45GB 4.36GB 4.30GB 1,473, 3.86GB 688,384 3.75GB 84,495 3.41GB 1,206, 3.08GB 853,290 1,214,870 2.92GB 2.68GB 831,878 2.50GB 428,373 639, 2.49GB 2.41GB 554,993 2.39GB 3,339,271,691,958 5,018,505,566 20.78TB 7Full list available at fineweb2-language-distribution.csv https://github.com/huggingface/fineweb-2/blob/main/ 56 Preprint. Under review. A.12 Bible and Wikipedia content For each language low resource language, we first compiled the distribution of documents by domain name. We then averaged the frequency of each domain across all languages, to find specific domains that were common source of data for different languages (which from manual inspection was the case for specific Bible websites and Wikipedia). We manually labeled the top domains that belonged to Bible or Wikipedia websites  (Table 46)  , and then measured the fraction of each language corpora that belonged to these domains. Out of 1868 language-script pairs in the final dataset, 70% (1320 of them) have more than half their documents from Bibleor Wikipedia-related domains. This is mostly driven by Bible content, as can be seen in Fig. 9. Wiki Domains wikipedia.org wikimedia.org wikisource.org wiktionary.org Bible Domains ebible.org bible.is jw.org stepbible.org bibles.org bible.com breakeveryyoke.com png.bible americanbible.org pngscriptures.org globalrecordings.net gospelgo.com httlvn.org biblegateway.com jesusforafrica.net bible.com.au pacificbibles.org scriptureearth.org divinerevelations.info beblia.com aboriginalbibles.org.au eevangelize.com biblica.com e-alkitab.org alkitab.pw amazinggracebibleinstitute.com bibleforchildren.org aionianbible.org cyber.bible biblehub.com myanmarbs.org baebol.org christianchildmultilingualbibleverse.wordpress.com femissionaria.blogspot.com biblics.com churchofjesuschrist.org biblesa.co.za bible-tools.org torresstraitbibles.org.au Table 46: List of Bible-related and Wiki-related domains 57 Preprint. Under review. Figure 9: Ratio of Wikipedia and Bible content per language Most languages have small fraction of their content originating from Wikipedia (with some exceptions). Bible content, on the other hand, is big part of the corpora of many lower-resource languages. 58 Preprint. Under review. A.13 Train-Test Split Our dataset release is split into train and test set, per language. The test set should not be used for training but instead can help research questions such as on memorization or data attribution. The test set is obtained as random subset (by hash function applied on the document content), and contains min{1%, 100k} of the documents per language pre-filtering, with reduction in size when these documents are filtered with the same process as the train set. It is only provided for languages of sufficient size."
        }
    ],
    "affiliations": []
}