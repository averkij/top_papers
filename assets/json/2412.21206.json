{
    "paper_title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
    "authors": [
        "Hyunsoo Cha",
        "Inhee Lee",
        "Hanbyul Joo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present PERSE, a method for building an animatable personalized generative avatar from a reference portrait. Our avatar model enables facial attribute editing in a continuous and disentangled latent space to control each facial attribute, while preserving the individual's identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with a variation in a specific facial attribute from the original input. We propose a novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present a personalized avatar creation method based on the 3D Gaussian Splatting, learning a continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce a latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person."
        },
        {
            "title": "Start",
            "content": "PERSE: Personalized 3D Generative Avatars from Single Portrait"
        },
        {
            "title": "Hyunsoo Cha",
            "content": "Inhee Lee Seoul National University {243stephen, ininin0516, hbjoo}@snu.ac.kr https://hyunsoocha.github.io/perse/"
        },
        {
            "title": "Hanbyul Joo",
            "content": "4 2 0 2 0 3 ] . [ 1 6 0 2 1 2 . 2 1 4 2 : r Figure 1. PERSE. Given reference portrait image input, our method constructs an animatable 3D personalized avatar with disentangled and editable control over various facial attributes."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We present PERSE, method for building an animatable personalized generative avatar from reference portrait. Our avatar model enables facial attribute editing in continuous and disentangled latent space to control each facial attribute, while preserving the individuals identity. To achieve this, our method begins by synthesizing large-scale synthetic 2D video datasets, where each video contains consistent changes in the facial expression and viewpoint, combined with variation in specific facial attribute from the original input. We propose novel pipeline to produce high-quality, photorealistic 2D videos with facial attribute editing. Leveraging this synthetic attribute dataset, we present personalized avatar creation method based on the 3D Gaussian Splatting, learning continuous and disentangled latent space for intuitive facial attribute manipulation. To enforce smooth transitions in this latent space, we introduce latent space regularization technique by using interpolated 2D faces as supervision. Compared to previous approaches, we demonstrate that PERSE generates high-quality avatars with interpolated attributes while preserving identity of reference person. personalized 3D face avatar can represent each individual in VR/AR environments, replicating the users appearance and facial expressions. However, the exact replication of the appearance does not fully reflect real-world humans. In reality, people often change the attributes of their appearance, like hairstyles, or start growing beard or mustache. Users may also wish to adjust their facial features in the virtual world, like the shape of their nose, eyes, or mouth, enhancing their desired look while preserving their core identity. While most prior avatar creation methods focus on building an exact digital twin of the person from images or video data [7, 16, 52, 54, 63, 77, 78, 80], the personalized avatar model with the generative ability to control and edit facial attributes remains underexplored. In this work, we present PERSE, method to build 3D personalized generative avatar from single reference portrait image. Our method goes beyond merely creating an exact twin from video inputs, introducing novel approach that emphasizes flexibility and control over facial attributes, such as changing hairstyles or beards shown in Fig. 1. To build PERSE from single reference portrait image, we generate large-scale 2D synthetic video dataset of the reference 1 identity, where each video has variation in specific facial attribute from the original input (e.g., different hairstyle) driven by the face motion guidance, as shown in Fig. 5. Each video is also paired with text prompt description addressing the changed attribute. To build this high-quality and photorealistic synthetic video dataset, we introduce new pipeline that begins with synthesizing 2D images with attribute editing, followed by portrait animation process leveraging the combination of existing pre-trained 2D portrait animation method [18] and our newly trained image-to-video model extending prior work [79]. Notably, our synthetic video generation process is efficient, scalable, and provides significantly more attribute diversity by effectively synthesizing thousand attribute videos compared to tens in prior work [5]. Using this synthetic video dataset, we train an avatar model with the continuous and disentangled attribute latent space. To enhance the generative ability of our avatar model for unseen or interpolated attribute appearances, we also present novel technique to enforce continuous and smooth latent space. To achieve this, we present latent space regularization technique by using interpolated 2D face images from an image morphing technique [69] (e.g., synthesizing medium-length hair from short hair and long hair attributes), providing pseudo supervisions for the interpolated latent spaces. We show the efficacy of our regularization technique by producing novel and unseen attributes from interpolated latent spaces, as shown in Fig. 7. Furthermore, we present an efficient fine-tuning technique via Low-Rank Adaptation (LoRA) [26], to integrate any new facial attributes from in-the-wild images into our avatar model. Our contributions are summarized as follows: (1) the first method to generate personalized 3D avatar from reference portrait image with controllable facial attributes; (2) method to generate high-quality synthetic 2D video datasets with diverse attribute editing from reference portrait image; (3) latent space regularization by using face morphing supervision for continuous and smooth latent space to enhance the generative ability for unseen or interpolated attribute appearances; (4) an efficient fine-tuning technique via Low-Rank Adaptation (LoRA) [26] to integrate any new facial attribute into the avatar model. 2. Related Work 3D Facial Avatar Reconstruction. Since the introduction of foundational 3D Morphable Model [1] (3DMM), parametric 3D face models [1, 3, 38] have evolved to capture the diverse and dynamic nature of human faces, representing variations in shape, head pose, and facial expression through set of parameters. Building on these models, various methods reconstruct 3D face avatars from single portrait images by estimating 3DMM parameters [10, 12, 53]. Recently, monocular 3D avatar reconstruction methods [7, 16, 35, 36, 77, 78] generate morphable photorealistic avatars leveraging advancements in 3D representation [31, 41]. To move beyond single-subject avatars, PEGASUS [5] reconstructs personalized generative avatar enabling control over facial attributes while preserving the reference identity, using synthetic DB. Similarly, HeadGAP [76] trains generalizable prior model for 3D head avatar leveraging large-scale multiview dataset and an avatar model with part-specific and point-specific feature codes. Despite advancements, constructing unified 3D representation that can precisely capture and control all facial attributes remains challenging. To address this, disentangled or hybrid representations have been proposed, enabling selective modification of facial features or garments [13, 14, 34]. However, these approaches are limited by discrete 3D structures, restricting continuous interpolation capabilities. Recently, latent-conditioned generative models [5, 22, 33, 37] have been introduced to mitigate these constraints, yet they often lack the capacity for fine-grained editing and are confined to specific categories. Smooth Image Morphing and Interpolation. Generating plausible intermediate image between two pivot images has been widely studied within the context of image generative field [23, 29, 59]. The recent breakthroughs of diffusion model [23, 51, 60] improved the image interpolation methods to generate more plausible and better quality interpolated images with less limitation on categories [17, 55, 60, 61, 69, 72, 75]. Many diffusion-based interpolation methods follow the procedure of DDIM inversion [42, 59], interpolation in diffusion latent space, and DDIM forward sampling with slight modification. DiffMorpher [69] additionally utilizes personalized diffusion models finetuned on each pivot image with LoRA [26] to produce smooth interpolated sequences. SmoothDiffusion [17] finetunes diffusion models with LoRA [26] to preserve the distance of interpolated sample and pivots during denoising. Portrait Animation from Single Image. Generating animations from single image is challenging task that has seen significant advancements through generative models, particularly based on implicit keypoints and diffusion methods. Several approaches [25, 40, 57, 58, 62, 74] have introduced intermediate motion representations based on implicit keypoints estimation, enabling the mapping of source portrait image to driving image using optical flow. Extending the previous work [62], LivePortrait [18] enhances animation quality by integrating GAN-based decoder [46], resulting in effective and controllable portrait animations. Recent advancements in diffusion models have significantly enhanced portrait animation, offering improved control and realism. Several methods [6, 27, 64] have explored full-body animations guided by motion sequence drived from body keypoints. Building upon previous approaches [27], Champ [79] generates full-body animations guided by multiple reference videos such as SMPL [39] renderings. Figure 2. Overview of Synthetic Dataset Generation and Avatar Model Training. Starting with collection of edited portrait images, we generate RGB videos for each target attribute using Portrait Animator. The guidance for the Portrait Animator is derived from tracked FLAME parameters of predefined training motion sequence, which also serve as inputs to the avatar network in our avatar model. Using the generated RGB videos, we train our avatar model with reconstruction loss. Each edited portrait is paired with the text prompt used for its generation. The process of creating these edited portraits based on text prompts is detailed in Sec. 4.2. 3. Preliminary: PEGASUS [5] Our avatar model is based on previously proposed personalized generative 3D avatar, PEGASUS [5], by modifying its original 3D point cloud representation [78] into 3D Gaussian Splatting [31]. The PEGASUS avatar model is an animatable 3D avatar model of reference individual with disentangled controls to selectively alter facial attributes such as hair or nose, while preserving the reference identity. The PEGASUS model takes latent code R(Nc+1)d along with FLAME parameters β (shape), θ (pose), and ψ (expression) as inputs, and infers colorized point cloud to express the target individuals appearance, pose and expressions changes: {xd , nd, ai} = Mϕ(xgc , z, β, θ, ψ), (1) is the 3D point locations, ni R3 is the point where xd normals, and ai R3 is the point albedo colors. The input latent code R(Nc+1)d is concatenation of Nc subpart latent codes {zj}j=0...Nc, where each subpart latent code zj Rd controls specific aspects of the human identity or subpart. The identity latent code z0 controls overall identity variations, and the other latent codes zj=0 control each subpart, preserving the identity defined by z0. Notably, the PEGASUS model relies on constructing synthetic video collection of the reference identity with edited facial attributes. This is performed by replacing specific facial attributes in the reference persons video with those from multiple other individuals videos. Consequently, building the synthetic dataset requires not only the video of the reference individual but also numerous videos from other individuals for attribute variations. Moreover, this approach involves time-intensive process of creating 3D avatars for each individual to synthesize all attribute variations, which limits the scalability of the method. 4. Method We first describe our personalized avatar model creation (Sec. 4.1), extending the previous work [5]. Then, we introduce our pipeline for generating large-scale synthetic 2D facial attribute dataset (Sec. 4.2). Additionally, we present our novel training scheme including latent space regularization with interpolated 2D faces (Sec. 4.3), and also present our efficient fine-tuning technique to integrate arbitrary new attributes into our optimized latent space while preserving the existing distribution (Sec. 4.4). 4.1. Personalized Generative Avatar Model 3D Gaussian Splatting for Avatar. Our avatar model builds on the structure of PEGASUS [5] with several modifications. First, we change the 3D representation of the avatar from colorized point cloud to 3D Gaussian Splatting (3D-GS) [31] which enhances rendering quality. This is achieved by estimating 3D Gaussian parameters for each point, replacing the original point normal and albedo. Specifically, our model takes latent code and FLAME parameters {β, θ, ψ} as inputs, and infers 3D Gaussian parameters of posed avatar, R4, scale including the 3D position xd R3, opacity od sd R3, rotation rd R, and color ci R3 as follows: {xd , rd , sd , od , cd } = MΘ(xgc , z, β, θ, ψ). (2) To capture fine-grained deformations conditioned on head pose, we introduce an additional MLP deforming 3D Gaussians based on the input FLAME parameters {β, θ, ψ}, similar to MonoGaussianAvatar [7]. We densify the 3D Gaussians to capture fine detail using the upsampling strategy of prior work [5, 78] and prune distracting Gaussians through opacity resetting and thresholding as in the original 3D-GS framework [31]. By rasterizing the 3D Gaussians, we get 3 4.2. Synthetic Dataset Generation We create synthetic portrait video dataset with varying facial attributes from the input image of the reference individual to enable the generative ability for our 3D avatar model. Our synthetic dataset generation pipeline is performed via two-stage process: generating attribute-edited images and animating the edited portrait images. Attribute-Edited Portrait Image Generation. Given portrait image Iinput of the reference person, we pursue to photo-realistically edit each attribute to different style. We consider 9 attribute categories: beard, ears, eyebrows, eyes, hair, mouth, nose, hat, and clothing. To achieve this goal, we present text-conditioned image inpainting pipeline by leveraging multiple tools including pre-trained 2D diffusion models [9]. We first determine list of text prompts for each attribute category with specific adjectives (e.g., curly, straight, and wavy for hair). We leverage ChatGPT [45] to explore various possible distinctive adjectives. Then, for each text description , we synthesize corresponding portrait image with attribute changes using text-conditioned inpainting model [9]: Igen = I2Iinpaint(Iinput, T, Medit), (6) where Medit denotes the mask region where the inpainting module needs to modify. Importantly, we find providing suitable mask region Medit is essential to synthesizing photo-realistic output that adheres to the text guidance. segmentation mask directly derived from the original input typically results in minor color changes without substantial shape variations. To obtain Medit that aligns with the text guidances , we present pipeline to produce it from text-to-image synthesis outputs. Specifically, we synthesize new portrait image Itext from the text using text-to-image diffusion model [48], where we enforce the facial poses and expressions of the synthesized image align with the Iinput using ControlNet [70]: Itext = T2I(cid:0)T, C(Iinput)(cid:1), (7) where C(Iinput) is the OpenPose [4] keypoint image obtained by applying off-the-shelf keypoint estimator [67] on Iinput. Although the identity of Itext is not necessarily the same as Iinput, its facial pose is aligned to the Iinput, allowing us to obtain the attribute mask Medit accordingly. We extract the attribute mask Mtext from Itext using an off-the-shelf segmentation network [32] and use it as the target area to edit Medit = Mtext for Eq. (6). Examples are shown in the first column of Fig. 5. For attributes of hat and hair, an additional step is required to remove the original parts that may unexpectedly remain after the inpainting process (e.g., the case that the original hair shape is bigger than Mtext). We resolve this issue by editing the original input image Minput with version Figure 3. Image Synthesis. Starting from the initial input image, we generate the edited image by leveraging an off-the-shelf segmentation model, pretrained diffusion text-to-image model, and an image-to-image inpainting model with fixed prompt and target prompt [9, 32, 48]. rendering of the avatar as follows: (cid:16) ˆI = GSR {xd , rd , ssc , od , csc }i{1N } (cid:17) , (3) where GSR represents 3D-GS Rasterizer [31]. CLIP-guided Latent Space Configuration. Following PEGASUS [5] model, we represent our avatar model latent code RNcd as concatenation of Nc subpart latent codes {zj Rd}j=[1Nc]. This part-wise separated latent configuration allows to control each facial attribute while preserving other facial attributes. We can also selectively transfer the target attribute of the k-th subpart, such as hair, to the reference avatar by substituting the k-th subpart latent as follows: znew = (cid:40) zref ztarget if = if = (4) To achieve this disentangled latent space, we optimize single reference latent code zref RNcd, representing the identity of the input portrait image, along with set of subpart latent codes {ztarget Rd}, where each corresponds to specific subject in our synthetic dataset. However, directly optimizing these latent codes {ztarget Rd} are prone to be overfitted on each subject, resulting in poor generalization to unseen subjects. To address this and achieve more compact latent space, we constrain latent codes using well-established text-image feature model CLIP [49], which is key difference over previous work [5]. We define the target subpart latent as an output of shallow MLP network conditioned on CLIP image and text features fI , fT R512: zsubject = MLPz(fI , fT ). (5) The CLIP features are calculated from front-view reference synthetic image and text pairs from our synthetic datasets. Additionally, we define zzero as unique shared subpart latent code representing an empty subpart, such as the absence of hat or beard. 4 Figure 4. Comparison of LivePortrait and portrait-CHAMP. Examples from LivePortrait [18] and portrait-CHAMP demonstrate several limitations: (a) artifacts are visible in the hair region, (b) LivePortrait lacks adaptability to head poses involving hats, and (c) beard artifacts are prone to aliasing and disappearance. containing shortcut hair, denoted Mshortcut, before applying inpainting: Ishortcut = I2Iinpaint(Iinput, Tshortcut, Minput), (8) where Minput is the hair mask of the Iinput, and Tshortcut is corresponding text prompt: person with very shortcut hair. We also find that, in these categories, combining the mask of this shortcut hair image Ishortcut with the mask from the text-to-image output Mtext produces superior results with fewer artifacts: Medit = (Mshortcut Mtext). (9) See Fig. 3 for the overview of the editing pipeline. Animated Portrait Video Generation. We animate each edited portrait image Igen to synthesize video with varying head poses and expressions, which are used as pseudo monocular video dataset for training our 3D avatar model. To achieve this goal, we utilize two different portrait animation techniques, LivePortrait [18] and our customized face-specialized CHAMP [79]: portrait-CHAMP. These methods are chosen for their complementary strengths. The goal of both animators is the same producing video output following the motion guidance while preserving the identity given by the input image: Vgen = I2V(Igen, G), (10) where denotes set of motion guidance, including the FLAME depth map, FLAME normal map, and 2D body and facial keypoints, as shown in the guidance at Fig. 2. To obtain G, we capture short video with varied head poses and facial expressions, and apply monocular face capture method [10] to extract FLAME parameters, from which we extract the motion guidance cues G. The same is used for all generated videos, Vgen, resulting in collection of videos with the same motions and diverse attribute changes. Examples are shown in Fig. 5. For attribute categories excluding hair, hat, and beard, we use LivePortrait [18] to animate the edited portrait images. Although LivePortrait successfully generates high-quality face-animation videos, it performs suboptimally with certain Figure 5. Our Synthetic Dataset. The upper left black bounded image is the input portrait. (a) is an edited image from the input portrait and (b) is generated frame by the portrait animator. attributes and conditions. For example, with portrait images featuring voluminous hair, long beards, or large hats, particularly in cases with extensive head movements, the model often generates unnatural deformations, such as stretching and shrinking with noticeable artifacts as shown in Fig. 4. To address these limitations, we build and train our own alternative image-to-video diffusion model, portrait-CHAMP to leverage high temporal consistency of 2D video diffusion model [19, 79]. Our model shows superior performance for synthesizing hair, beard, and hat over LivePortrait, as demonstrated in our experiments. We build our model based on the CHAMP [79] that is originally designed for full-body animations, with few extensions. For concise control of head and facial expression, our portrait-CHAMP gets normal and depth rendering of EMOCA [10] as conditioning input instead of SMPL [39]. We add normal channel in VAE encoder and decoder of Portrait-CHAMP to enhance 3D-awareness of the video diffusion model [20], and trained it with 6k real-world videos capturing diverse identities and motions [68]. 4.3. Training In essence, training our avatar model on the synthetic dataset is identical to the process of reconstructing 3D avatar from real 2D video inputs. At each iteration, we render an image ˆI of posed subject from the synthetic dataset and calculate the reconstruction loss, Lrecon, by comparing it to the ground truth image, I. Lrecon(ˆI, I) = λL1ˆI I1 + λSSIMSSIM(ˆI, I) (11) We then compute latent regularization loss Lz enforcing the norm of the latent code close to be zero and estimated FLAME parameters regularizing loss LFLAME following PE5 Figure 6. Overview of Supervision for Interpolation. We propose an additional training strategy that leverages finetuned 2D diffusion model [26, 69] to enhance the quality of interpolated pivots in latent space. Starting from two pivots with text prompts and B, we generate interpolated latent codes through weighted summation based on α. We then compute the part-wise loss and backpropagate it through the avatar model. GASUS [5]. Our total loss is as follows: Ltot = λreconLrecon (cid:0)ˆI, I(cid:1) + λFLAMELFLAME + λzLz. (12) We train our model with this objective until convergence. Finetuning for Interpolated Samples. After convergence, our avatar model still suffers from sampling high quality avatar which is not included in the trained subject. The sampled avatars frequently contain artifacts, such as floating Gaussians or unnatural color blobs as illustrated in Fig. 8. To mitigate these artifacts, we propose an interpolation regularization loss leveraging prior knowledge from pretrained image diffusion model [51], as demonstrated in Fig. 6. By regularizing the interpolated renderings to be closer to image generated by the diffusion interpolation generator [69], we improve both the rendering quality and realism of interpolated samples. To calculate the interpolation loss, we sample two pivot subjects, (a, b) from the same category in our synthetic dataset and render an interpolated subject in every iteration: (cid:16) ˆIinterp,α = GSR (cid:17) MΘ(za(1 α) + zbα) , (13) where α denotes an interpolation weight. We use DiffMorpher [69] to generate semantically plausible and visually realistic interpolations between their images, controlled by the same interpolation weight α: Iinterp,α = DiffMorpherα (cid:16) (cid:17) . Ia, Ib (14) As DiffMorpher [69] generated image Iinterp,α often fails to preserve identity, we apply loss only on the subpart region Mpart which alters during interpolation: (cid:16) (cid:17) Mpart Iinterp,α, Mpart ˆIinterp,α . Linterp = Lrecon (15) We fine-tune the converged avatar model together with total loss in Eq. (12) until it converges. 6 4.4. Facial Attribute Transfer from Image To transfer facial attribute from an arbitrary image, such as transferring an unseen hairstyle to the reference individual, we need to find the corresponding latent code in our model. Although our model can retrieve the latent code by inputting the CLIP [24] features of an image into our MLP as described in Eq. (5), it struggles with perfectly handling unseen attributes. To incorporate these unseen attributes while preserving learned ones, we finetune our avatar model by optimizing the weights Θ of additional LoRA [26] layers while keeping the other network weights Θ frozen. Specifically, our model with additional LoRA layers takes the same inputs and outputs as described in Eq. (2): {xd , rd , od , sd , cd } = MΘ+Θ(xgc , z, β, θ, ψ). (16) We animate the image for transfer with our animation generation pipeline and use the resulting frames to optimize the LoRA layers. The loss is calculated only on the region targeted for transfer, using masked loss similar to Eq. (15). Refer to the Supp. Mat. for more details. 5. Experiments 5.1. Synthetic Dataset Configuration To assess the effectiveness of our method, we generate synthetic dataset using single portrait for model evaluation. We define nine attribute categories (clothing, beards, ears, eyebrows, eyes, hair, hats, mouth, and nose) and produce over 50 videos for each, totaling 957 attribute-edited videos for quantitative comparison. The text prompts are constructed from non-contradictory combinations of predefined, category-specific adjectives, such as curly, straight, wavy, and coily for hair. To animate the images, we employ single 513-frame video that captures variety of head poses and expressions, applying it consistently across all Figure 7. Interpolation Comparison on Baselines. Our method shows better interpolation smoothness and less artifact on interpolated samples, particularly on the texture and color of hair. instances. We split all video frames in our synthetic dataset into training and test sets with 400:113 frame ratio, using the first 400 frames for training and the remaining 113 for evaluation. Examples of our dataset can be found in Fig. 5. 5.2. Baselines and Metrics We compare our model with three different baselines, each using distinct 3D representation for avatar modeling: colorized point clouds [78], NeRF [41], and 3D Gaussians [31]. PEGASUS [5] is the first method for constructing personalized 3D generative avatar from 2D video inputs. It creates personalized avatar model using set of MLP networks and colorized point cloud, following the approach of PointAvatar [78]. For fair comparison, we train PEGASUS with its public code, replacing its synthetic database with our synthetic datasets. Conditional INSTA (Cond.TA) is modified version of vanilla INSTA [80], which reconstructs head avatars using an implicit representation, specifically iNGP [44]. To enable the model to capture diverse facial attributes, we add latent code conditioning the MLP of vanilla INSTA. We follow the PEGASUS latent code configuration and train Cond.TA with our synthetic dataset until it converges. Conditional SplattingAvatar (Cond.SA) is modified SplattingAvatar [54] which is method for reconstructing 3D avatar models from monocular video using 3D Gaussian Splatting [31]. Vanilla SplattingAvatar explicitly represents an avatar as set of 3D Gaussians embedded on 3D head mesh. To incorporate conditional latent code as input, we add an implicit network estimating changes of the 3D Gaussian parameters conditioned by the latent code. Similar to other baselines, we train the model until convergence using our synthetic dataset. See the Supp. Mat. for more details. Metrics. We evaluate our personalized generative model in two aspects: reconstruction performance and generative 7 Figure 8. Effect of Interpolation Loss. (a) and (b) represent supervised and unsupervised samples respectively supervised by personalized diffusion model [26, 69]. Even for unsupervised samples, our supervision method for interpolated samples mitigates unnatural artifacts and textures. Additionally, our method preserves the quality of the pivot samples. performance. Following standard practices in monocular 3D avatar reconstruction [54, 78, 80], we use peak signalto-noise ratio (PSNR), structural similarity (SSIM), and perceptual similarity (LPIPS) [71] to evaluate reconstruction performance of learned subjects in synthetic dataset. Additionally, we evaluate identity preservation by computing the cosine similarity of ArcFace [11] identity features. We compute the Frechet Inception Distance (FID) [21] and Kernel Inception Distance (KID) against FFHQ dataset [28] and our synthetic evaluation dataset to evaluate the quality of generated subjects. In addition, we compute the sum (Perceptual Path Length, PPL) and deviation (Perceptual Distance Variance, PDV) of perceptual loss between adjacent interpolated images to evaluate the smoothness of interpolation following DiffMorpher [56, 69]. 5.3. Quantitative and Qualitative Results We present the quantitative results of unseen pose rendering in Tab. 1. As shown in the table, our avatar model achieves the best results across all metrics, demonstrating superior reconstruction quality for the subjects in the synthetic dataset while preserving the identity. In Tab. 2, we provide additional quantitative comparisons on interpolation, along with qualitative comparisons in Fig. 7. Our avatar model outperforms baselines on both FIDFFHQ and KIDFFHQ scores, indicating that our interpolated samples align more closely with real human distribution in the FFHQ dataset. Additionally, our model achieves better FIDSYN and KIDSYN scores, confirming that our interpolated samples preserve the identity of the reference individual more effectively Method PSNR SSIM LPIPS Identity PEGASUS [5] Cond.TA [80] Cond.SA [54] 23.56 19.01 22.17 0.8661 0.7730 0.8690 0.1508 0.2875 0.2760 0.6471 0.3022 0.4759 Ours 23.84 0.8852 0.1458 0.7059 Table 1. Quantitative Results of Unseen Pose Renderings. We compare our method with the baselines for training accuracy of pivots in the synthetic dataset. Our method achieves the best results across all metrics, demonstrating superior accuracy in reconstructing pivots in the synthetic dataset while preserving identity. Method FIDFFHQ KIDFFHQ FIDsyn KIDsyn PDV PPL User Study PEGASUS [5] Cond.TA [80] Cond.SA [54] Ours 223.86 258.48 230.21 214.46 0.2502 0.3015 0. 0.2201 84.94 127.45 180.79 0.0959 0.1454 0.2316 0.2373 0.9724 0.9641 0.4047 0.6739 0.5789 57. 0.0420 0.2481 0.3308 36.3 - - 63.7 Table 2. Quantitative Results of Interpolated Renderings. PDV*=100 PDV. Ours shows the best score among the baselines including user study except for PDV. Method Interpolation w/ Linterp w/ CLIP FIDFFHQ KIDFFHQ FIDsyn KIDsyn PDV PPL Identity 224.00 224.67 214.46 0.2357 0.2335 0.2201 66.34 69.03 57.78 0.0546 0.0568 0.0420 0.3046 0.3037 0.2481 0.3387 0.3268 0. 0.7001 0.66724 0.7013 Table 3. Ablation Studies. PDV*=100PDV. w/ Linterp denotes fine-tuning model with interpolation loss and w/ CLIP means using latent conditioned on CLIP feature. Our full method achieves the best results on all metric except for PPL. than the baselines. While PEGASUS [5] achieves slightly better performance on the PDV metric with small gap, its lower FID, KID, and PPL scores suggest limited naturalness and smoothness in interpolation. It can be checked in Fig. 7, where PEGASUS shows unnatural transitions in hair color and texture, while ours produces smoother results. Moreover, in user studies, our interpolation results are preferred over PEGASUS. 5.4. Ablation Studies and More Results Ablation Studies. We conduct ablation studies to assess the effectiveness of our CLIP-guided latent configuration and interpolation loss Linterp. As shown in Tab. 3 and Fig. 8, our interpolation loss is essential for improving interpolated sample quality and reducing artifacts. The CLIP-guided latent also reduces PPL, resulting in smoother transitions while preserving rendering quality. Facial Attribute Transfer. We conduct facial attribute transfer experiments using few in-the-wild images. As shown in Fig. 9 our LoRA fine-tuning method successfully transfers the hair and hat attributes while preserving other aspects of identity. The transferred attributes are well integrated into the latent space, as reflected in the smooth interpolation results between subject in our synthetic dataset in Fig. Figure 9. Transferred Facial Attribute Results from In-TheWild Images. (a) is an in-the-wild image of attribute to transfer, (b) is an initial transferred result without optimization, and (c) is optimized results using LoRA layers. Figure 10. Transferred Facial Attribute Interpolation. (a) represents an in-the-wild input image, (b) denotes the interpolation result between (c), sample of our synthetic dataset. 6. Discussion We present PERSE, an animatable personalized generative avatar from single portrait image, enabling continuous and disentangled facial attribute editing while preserving the individuals identity. To achieve this goal, we present several key contributions, including: (1) method to generate highquality synthetic attribute video datasets from single image along with our newly trained portrait-CHAMP model; (2) latent space regularization for unseen or interpolated attribute appearances; and (3) an efficient fine-tuning technique via LoRA to integrate new facial attribute into the avatar model. As limitations, our avatar-building process is computationally intensive, requiring approximately 1.5 days on eight A6000 48GB GPUs for each new identity. Additionally, while our 3D avatars are of high quality, they do not yet achieve photorealism, particularly in fine hair strand details."
        },
        {
            "title": "References",
            "content": "[1] V. Blanz and T. Vetter. morphable model for the synthesis of 3d faces. In SIGGRAPH, 1999. 2 [2] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 5 [3] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. Facewarehouse: 3d facial expression database for visual computing. IEEE Transactions on Visualization and Computer Graphics, 20(3):413425, 2013. 2 [4] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh. Realtime multiperson 2d pose estimation using part affinity fields. In ICCV, 2017. 4 [5] H. Cha, B. Kim, and H. Joo. Pegasus: Personalized generative 3d avatars with composable attributes. In CVPR, 2024. 2, 3, 4, 6, 7, 8, 1, [6] D. Chang, Y. Shi, Q. Gao, H. Xu, J. Fu, G. Song, Q. Yan, Y. Zhu, X. Yang, and M. Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identityaware diffusion. In ICML, 2023. 2 [7] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and Y. Liu. Monogaussianavatar: Monocular gaussian point-based head avatar. In SIGGRAPH, 2024. 1, 2, 3 [8] CloudResearch. Connect cloud research. URL https:// connect.cloudresearch.com/researcher/. 5 [9] A. Creative. Flux.1-dev-controlnet-inpainting-alpha. https: / / huggingface . co / alimama - creative / FLUX . 1-dev-Controlnet-Inpainting-Alpha, 2024. 4, 3 [10] R. Danˇeˇcek, M. J. Black, and T. Bolkart. Emoca: Emotion driven monocular face capture and animation. In CVPR, 2022. 2, 5, 3, 4 [11] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 7, 5 [12] Y. Feng, H. Feng, M. J. Black, and T. Bolkart. Learning an animatable detailed 3d face model from in-the-wild images. ACM TOG, 2021. [13] Y. Feng, J. Yang, M. Pollefeys, M. J. Black, and T. Bolkart. Capturing and animation of body and clothing from monocular video. In SIGGRAPH ASIA, 2022. 2 [14] Y. Feng, W. Liu, T. Bolkart, J. Yang, M. Pollefeys, and M. J. Black. Learning disentangled avatars with hybrid 3d representations. arXiv preprint arXiv:2309.06441, 2023. 2 [15] Freepik. Portrait search results. https://www.freepik. com/search?ai=excluded&query=Portrait. Accessed: 2024-11-22. 6 [16] G. Gafni, J. Thies, M. Zollhofer, and M. Nießner. Dynamic neural radiance fields for monocular 4d facial avatar reconstruction. In CVPR, 2021. 1, 2 [17] J. Guo, X. Xu, Y. Pu, Z. Ni, C. Wang, M. Vasu, S. Song, G. Huang, and H. Shi. Smooth diffusion: Crafting smooth latent spaces in diffusion models. In CVPR, pages 75487558, June 2024. 2 [18] J. Guo, D. Zhang, X. Liu, Z. Zhong, Y. Zhang, P. Wan, Liveportrait: Efficient portrait animaand D. Zhang. tion with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 2, 5, 3, 4, [19] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. 5 [20] X. He, X. Li, D. Kang, J. Ye, C. Zhang, L. Chen, X. Gao, H. Zhang, Z. Wu, and H. Zhuang. Magicman: Generative novel view synthesis of humans with 3d-aware diffusion and iterative refinement. arXiv preprint arXiv:2408.14211, 2024. 5, 3 [21] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 7 [22] H.-I. Ho, L. Xue, J. Song, and O. Hilliges. Learning locally editable virtual humans. In CVPR, 2023. 2 [23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. [24] F. Hong, M. Zhang, L. Pan, Z. Cai, L. Yang, and Z. Liu. Avatarclip: Zero-shot text-driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535, 2022. 6 [25] F.-T. Hong, L. Zhang, L. Shen, and D. Xu. Depth-aware generative adversarial network for talking head video generation. In CVPR, 2022. 2 [26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 6, 7, 4 [27] L. Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, 2024. 2, 5 [28] T. Karras, S. Laine, and T. Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. 7, 5 [29] T. Karras, M. Aittala, S. Laine, E. Harkonen, J. Hellsten, J. Lehtinen, and T. Aila. Alias-free generative adversarial networks. In NeurIPS, 2021. 2 [30] Z. Ke, J. Sun, K. Li, Q. Yan, and R. W. Lau. Modnet: Realtime trimap-free portrait matting via objective decomposition. In AAAI, 2022. [31] B. Kerbl, G. Kopanas, T. Leimkuhler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 2, 3, 4, 7 [32] R. Khirodkar, T. Bagautdinov, J. Martinez, S. Zhaoen, A. James, P. Selednik, S. Anderson, and S. Saito. Sapiens: Foundation for human vision models. In ECCV, 2025. 4 [33] T. Kim, S. Saito, and H. Joo. Ncho: Unsupervised learning for neural 3d composition of humans and objects. In ICCV, 2023. 2 [34] T. Kim, B. Kim, S. Saito, and H. Joo. Gala: Generating animatable layered assets from single scan. In CVPR, 2024. 2 [35] T. Kirschstein, S. Giebenhain, and M. Nießner. Diffusionavatars: Deferred diffusion for high-fidelity 3d head avatars. In CVPR, pages 54815492, 2024. 2 [36] I. Lee, B. Kim, and H. Joo. Guess the unseen: Dynamic 3d scene reconstruction from partial 2d glimpses. In CVPR, 2024. 2 [37] J. Li, S. Saito, T. Simon, S. Lombardi, H. Li, and J. Saragih. Megane: Morphable eyeglass and avatar network. In CVPR, 2023. 2 [38] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero. Learning model of facial shape and expression from 4d scans. ACM Trans. Graph., 2017. 2, 1, 3 [39] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black. SMPL: skinned multi-person linear model. SIGGRAPH ASIA, 2015. 2, 5, 3 arXiv:2409.09605, 2024. 2 [56] K. Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual conference on Computer graphics and interactive techniques, 1985. 7, [57] A. Siarohin, S. Lathuili`ere, S. Tulyakov, E. Ricci, and N. Sebe. First order motion model for image animation. NeurIPS, 2019. 2 [58] A. Siarohin, O. J. Woodford, J. Ren, M. Chai, and S. Tulyakov. Motion representations for articulated animation. In CVPR, 2021. 2 [40] A. Mallya, T.-C. Wang, and M.-Y. Liu. Implicit warping for [59] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit animation with image sets. NeurIPS, 2022. 2 models. In Proc. ICLR, 2020. [41] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. 2, 7 [42] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, and D. CohenOr. Null-text inversion for editing real images using guided diffusion models. In CVPR, 2023. 2 [43] MooreThreads. https : / / github . com / MooreThreads / Moore - AnimateAnyone, 2023. 5 Moore-animateanyone. [44] T. Muller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM TOG, 2022. [45] OpenAI. Chatgpt, 2024. URL https://chat.openai. com/. 4 [46] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu. Semantic In image synthesis with spatially-adaptive normalization. CVPR, 2019. 2 [47] W. Peebles and S. Xie. Scalable diffusion models with transformers. In ICCV, 2023. 5 [48] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 4, [49] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In Proc. ICML, 2021. 4 [50] N. Ravi, J. Reizenstein, D. Novotny, T. Gordon, W.-Y. Lo, J. Johnson, and G. Gkioxari. Accelerating 3d deep learning with pytorch3d. arXiv:2007.08501, 2020. 4 [51] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 6, 3 [52] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam. Relightable gaussian codec avatars. In CVPR, 2024. 1 [53] S. Sanyal, T. Bolkart, H. Feng, and M. J. Black. Learning to regress 3d face shape and expression from an image without 3d supervision. In CVPR, 2019. [54] Z. Shao, Z. Wang, Z. Li, D. Wang, X. Lin, Y. Zhang, M. Fan, and Z. Wang. Splattingavatar: Realistic real-time human avatars with mesh-embedded gaussian splatting. In CVPR, 2024. 1, 7, 8, 4, 5 [55] L. Shen, T. Liu, H. Sun, X. Ye, B. Li, J. Zhang, and Z. Cao. Dreammover: Leveraging the prior of diffusion models for image interpolation with large motion. arXiv preprint 10 [60] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 2 [61] C. J. Wang and P. Golland. Interpolating between images with diffusion models, 2023. [62] T.-C. Wang, A. Mallya, and M.-Y. Liu. One-shot free-view In neural talking-head synthesis for video conferencing. CVPR, 2021. 2 [63] Y. Xu, H. Zhang, L. Wang, X. Zhao, H. Huang, G. Qi, and Y. Liu. Latentavatar: Learning latent expression code for expressive neural head avatar. In SIGGRAPH, 2023. 1 [64] Z. Xu, J. Zhang, J. H. Liew, H. Yan, J.-W. Liu, C. Zhang, J. Feng, and M. Z. Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. 2 [65] S. Yang, H. Li, J. Wu, M. Jing, L. Li, R. Ji, J. Liang, and H. Fan. Megactor: Harness the power of raw video for vivid portrait animation. arXiv preprint arXiv:2405.20851, 2024. 5 [66] S. Yang, H. Li, J. Wu, M. Jing, L. Li, R. Ji, J. Liang, H. Fan, and J. Wang. Megactor-σ: Unlocking flexible mixed-modal control in portrait animation with diffusion transformer. arXiv preprint arXiv:2408.14975, 2024. 5 [67] Z. Yang, A. Zeng, C. Yuan, and Y. Li. Effective whole-body pose estimation with two-stages distillation. In ICCV, 2023. 4, 3 [68] J. Yu, H. Zhu, L. Jiang, C. C. Loy, W. Cai, and W. Wu. Celebvtext: large-scale facial text-video dataset. In CVPR, 2023. 5, 3 [69] K. Zhang, Y. Zhou, X. Xu, B. Dai, and X. Pan. Diffmorpher: Unleashing the capability of diffusion models for image morphing. In CVPR, 2024. 2, 6, 7, [70] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 4, 3 [71] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7 [72] R. Zhang, Y. Chen, Y. Liu, W. Wang, X. Wen, and H. Wang. Tvg: training-free transition video generation method with diffusion models. arXiv preprint arXiv:2408.13413, 2024. 2 [73] Y. Zhang, J. Gu, L.-W. Wang, H. Wang, J. Cheng, Y. Zhu, and F. Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. 5 [74] J. Zhao and H. Zhang. Thin-plate spline motion model for image animation. In CVPR, 2022. 2 [75] P. Zheng, Y. Zhang, Z. Fang, T. Liu, D. Lian, and B. Han. Noisediffusion: Correcting noise for image interpolation with diffusion models beyond spherical linear interpolation. In ICLR, 2024. [76] X. Zheng, C. Wen, Z. Li, W. Zhang, Z. Su, X. Chang, Y. Zhao, Z. Lv, X. Zhang, Y. Zhang, et al. Headgap: Few-shot 3d head avatar via generalizable gaussian priors. arXiv preprint arXiv:2408.06019, 2024. 2 [77] Y. Zheng, V. F. Abrevaya, M. C. Buhler, X. Chen, M. J. Black, and O. Hilliges. Im avatar: Implicit morphable head avatars from videos. In CVPR, 2022. 1, 2 [78] Y. Zheng, W. Yifan, G. Wetzstein, M. J. Black, and O. Hilliges. Pointavatar: Deformable point-based head avatars from videos. In CVPR, 2023. 1, 2, 3, 7 [79] S. Zhu, J. L. Chen, Z. Dai, Y. Xu, X. Cao, Y. Yao, H. Zhu, and S. Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 2, 5, 3 [80] W. Zielonka, T. Bolkart, and J. Thies. Instant volumetric head avatars. In CVPR, 2023. 1, 7, 11 PERSE: Personalized 3D Generative Avatars from Single Portrait"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Avatar Model A.1.1. Avatar Model Architecture To model diverse attributes with single model, our avatar model follows three-stage deformations proposed in PEGASUS [5] with few modifications. First, we initialize the learnable generic canonical points gc with the vertices of FLAME [38] mesh with an open mouth: gc = {xgc }i={1N }, (17) where is the number of points. The generical canonical points gc are shared start points for all subjects in the synthetic dataset. By deforming the points with subject-specific latent as condition, we obtain subject-specific canonical points sc containing the shape of specific attribute, such as having long hair or grey cap. The mapping between two states is defined as an offset of each point Ogcsc , which is regressed using coordinate-based deformation MLP as follows: {Ogcsc , Oscf , Ei, Pi, Wi} = MLPd(z, xgc ). (18) It regresses FLAME LBS weight Wi and blendshapes {Ei, Pi} of each point jointly, which is crucial to reenact our avatars into any novel pose and expression. Subsequently, our avatar model defines mapping of subject-specific canonical points sc to the FLAME canonical points for better fidelity following the previous work [5, 78]. The mappings between two points are defined as another point offset Oscf which is also regressed by the deforming MLP jointly. The transformation between each state are summarized as follows: = xgc xsc xf = xsc + Ogcsc + Oscf i , . (19) (20) Finally, the points in the FLAME-canonical space are deformed into the final posed space using Linear Blend Skinning (LBS) and FLAME parameters {β, θ, ψ} as follows: xd = xf + BS(β; S) + BP (θ; P) + BE(ψ; E) xd = LBS(xd, J(ψ), θ, W), (21) (22) where xd denotes the point after applying the blendshapes and before applying transformation via linear blend skinning. Similar to PEGASUS [5], we infer the attributes of each Gaussian, oi (opacity), ri (rotation), si (scale), and ci (color) Figure 11. Network Configuration. We show detailed structure of the networks of our avatar model: pose-conditioned deformation MLPpose, canonical MLPc, latent mapping MLPz, and deformation MLPd. using coordinated-based MLP as follows: {osc , rsc , ssc , csc } = MLPc(z, xsc ). (23) This canonical MLPc is defined against subject-specific canonical points and conditioned by latent code z. We model additional 3D Gaussian change depending on the pose changes following MonoGaussianAvatar [7]. We calculate the deviation of each Gaussian center between before and after LBS deformation of (22) and query the change of each center to an MLP network together with latent to 1 estimate pose-conditioned deformation: xi = xd xf , {ri, si, oi, ci} = MLPpose(xi, z). where E, P, and are the pseudo ground truth from the k-nearest neighbor vertices of the FLAME [38]. This regularization is important to obtain better reenactment with unseen pose. (24) (25) We change all Gaussian parameters except the center xi. The final deformed Gaussians which are queried in the Gaussian Rasterizer [31] are as follows: A.2. Finetuning for Interpolated Samples A.2.1. Preliminaries: DiffMorpher = oi + osc od , = si + ssc sd , = ci + csc cd , = ri + Rot(rsc rd , xd xf ), (26) (27) (28) (29) on each quaternion rsc where Rot() denotes multiplying corresponding rotation xd occurred during LBS of (22). xf The overall optimizable parameters of our avatar model are summarized below: Θ = {MLPc, MLPd, MLPz, MLPpose, {xgc }i{1N }}. (30) The detailed network structure is shown in Fig. 11. A.1.2. Training Strategy We set the first epoch as warm-up stage for stable optimization. During this stage, the pose-conditioned deformation MLP is disabled, and only the remaining MLPs and points are optimized. It encourages the deformation module of the avatar network to generate valid offsets from the generic canonical space to the final deformed space. We optimize our avatar model for 26 epochs using DDP with 8 A6000 GPUs, which takes around 2 days. We follow prior work [78] to densify the Gaussians through upsampling, repeating this process iteratively every 2 epochs until the number of points reaches 180,000. In our Gaussian splatting network, the opacity-related network is constrained to return value of 1 until the number of points exceeds 100,000. Once this threshold is surpassed, the opacity-related layer is unfrozen, allowing it to infer change of opacity depending on latent z. This strategy ensures the model learns attribute changes primarily through Gaussian deformation rather than opacity adjustments, which can blur random samples of latent space. A.1.3. Loss Functions The FLAME loss [7, 78] included in total loss Ltot is regularization enforcing the inferred FLAME blendshapes and LBS weights ( ˆE, ˆP, ˆW) of each Gaussian to be close to the FLAME meshs one: LFLAME = 1 (cid:88) (λeEi ˆEi2 i=1 + λpPi ˆPi2 + λwWi ˆWi2), (31) By viewing diffusion sampling process as solution of ODE, we obtain deterministic mapping between latent variable in the Gaussian distribution ξT and an image through DDIM forward and inversion [59]: ξ = DDIMinv(I; W), = DDIM(ξ; W), where means pre-trained image diffusion model. By interpolating latents (ξa, ξb) inverted from two images (Ia, Ib), we obtain semantically meaningful smooth interpolation as follows: ξinterp,α = slerp(ξb, ξa, α), Iinterp,α = DDIM(ξinterp,α; W), where α is an interpolation weight and slerp() is spherical linear interpolation [56]. DiffMorpher [69] uses personalized diffusion models for DDIM sampling and inversion, resulting in smoother and better natural image interpolation. For two images (Ia, Ib), it trains LoRA [26] on UNet (Wa, Wb) for each image and uses the LoRA-integrated UNet for DDIM inversion: ξa = DDIMinv(Ia; + Wa), ξb = DDIMinv(Ib; + Wb). For the forward process on interpolated latent ξinterp,α, it uses interpolated LoRA with attention interpolation: Iinterp,α = DDIM(ξinterp,α; Θinterp,α), (32) where Winterp,α is an interpolated LoRA derived as Winterp,α = + Wa(1 α) + Wbα. For brevity, we denote the overall interpolation process with DiffMorpher from two images (Ia, Ib) and weight α as follows: Iαi = DiffMorpherαi (cid:16) (cid:17) . Ia, Ib (33) A.2.2. DiffMorpher LoRA Optimization We use DiffMorpher [69] to generate interpolated images, which serve as pseudo ground truth to fine-tune our avatar model. Specifically, we select two subjects from the synthetic dataset and fine-tune the model for interpolated renderings between them. To obtain the corresponding pseudo ground truth images with DiffMorpher, we require LoRA for each image. Training LoRA for each posed image is computationally prohibitive considering the number of images in our synthetic dataset. Therefore, unlike vanilla DiffMorpher [69], which uses single image, we train LoRA subject-wise using all animated frames in each subject. The LoRA training objective is equal to the standard diffusion training objectives [51] as follows: L(Θ) = Eϵ,τ,i[ϵ ϵΘ+Θ(ξτ i, τ, ci)2], ξτ = ατ ξ0i + 1 ατ ϵ, (34) (35) where ξ0i = E(Ii) represents the latent encoded by the VAE encoder of diffusion model, Ii is the ith animated image of the subject randomly selected at each iteration, ϵ (0, I) is Gaussian noise, and ξτ denotes the perturbed latent at diffusion step τ . To avoid confusion with our models latent variable z, we use ξ to refer to the VAE-encoded latents of the diffusion model here. We train the subject-specific LoRA with batch size 8 for 5 epochs per subject. A.2.3. Interpolation Loss Details To enhance the quality of the interpolated sample and ensure interpolation smoothness, we calculate reconstruction loss on the interpolated samples. In every iteration, we randomly sample two subjects (a, b) from the same category of our synthetic dataset, referred to here as pivots. Then, we generate 5 interpolated samples using linear interpolation as follows: zα,i = za(1 αi) + zbαi, (36) where {αi}[i=15] are 5 equally distributed interpolation weights from 1/6 to 5/6. For all 5 interpolated samples, we compare the rendering with DiffMorpher [69] generated images as follows: (cid:16) ˆIαi = GSR (cid:17) MΘ(zα,i) , (cid:16) (cid:17) , Ia, Ib Iαi = DiffMorpherαi 5 (cid:88) (cid:16) Linterp = Lpart Mpart Iαi, Mpart ˆIαi (37) (38) (39) (cid:17) . i=1 As the image Iα generated by DiffMorpher [69] fails to preserve the identity of the remaining regions, we apply the loss only to the subpart region Mpart is that changes during interpolation. All DiffMorpher inferences and target part segmentations are performed online during optimization, as the number of possible pairs is too large to process in advance. We finetune our avatar model using an interpolation loss applied to 40 arbitrary pairs per subject, resulting in total of 38,600 pairs. In each iteration, we also apply the total loss Ltot to the pivot subjects (a, b) to preserve their quality. 3 Category # of attributes w/ Portrait-CHAMP Hair Beard Cloth Ears Eyebrows Eyes Hat Mouth Nose Total 395 69 57 58 58 59 110 75 965 - - - - - - - Table 4. Number of Attributes of our synthetic dataset. PortraitCHAMP denotes the category used Portrait-CHAMP to animate the portrait images. Otherwise we use LivePortrait [18]. A.3. Synthetic Dataset A.3.1. Attribute-Edited Portrait Image Generation The number of attributes in each category is shown in Tab. 4. While we generate approximately 1k samples to demonstrate the effectiveness of PERSE here, the pipeline can be extended to produce any desired amount, as our synthetic dataset generation process is fully automated. We use FLUX with inpainting controlnet [9] for Image-to-Image (I2I) inpainting and SDXL with pose controlnet [48, 70] for attribute mask generation. A.3.2. Training portrait-CHAMP Our portrait-CHAMP builds upon the architecture introduced in Champ [79], incorporating modifications to enhance 3D-awareness and improve reenactment performance. Specifically, we integrate an additional Variational Autoencoder (VAE) encoder-decoder pair dedicated to normal maps, drawing inspiration from MagicMan [20]. Adopting the dual-branch strategy proposed in MagicMan [20], we introduce an additional U-Net for the normal maps. This U-Net shares all weights with the original RGB U-Net except for the first layer. The shared layers between the two U-Nets enable cross-domain feature integration, allowing the model to fuse features from both normal map and RGB image. By combining geometric and visual information, our approach enhances the geometric awareness of model, resulting in improved structural coherence. We replace the original SMPL [39] rendered motion guidance in vanilla CHAMP [79] with FLAME rendering. Specifically, we employ monocular face capture method [10] to extract FLAME parameters [38]. Using these parameters, we render the FLAME depth map and FLAME normal map. To provide motion guidance for the body, including shoulders, which are not covered by FLAME rendering, we supplement the guidance with full-body keypoints and facial landmarks inferred from RGB videos using DWPose [67]. We use 5,196 videos from CelebV-Text [68] datasets to train our portrait-CHAMP. Following previous work [79], we train portrait-CHAMP using 8 A6000 GPUs in two stages: 58,732 iterations with batch size of 32 in stage 1, and 26,450 iterations with batch size of 8 in stage 2. In stage 1, we optimize the model using randomly sampled frames from videos as an image diffusion model. In stage 2, we train only the temporal motion module with videos while freezing other modules. A.3.3. Animating Portrait Images Enhancing the reenactment capability of our avatar model requires training videos that cover wide range of facial expressions and head poses. We achieve this by animating portrait images with motion sequence containing diverse expressions and poses. To obtain motion sequence that satisfies both continuity and the minimal number of frames required by portrait-CHAMP, we record video for this motion sequence ourselves. Using reference portrait image and predefined motion sequence in an RGB video, we first generate an animated portrait video centered on the reference image using LivePortrait [18]. From this video, we extract normal maps, depth maps, and facial keypoint motion guidance using EMOCA [10] and DWPose [67]. With this guidance, we animate images whose hair, hat, and beard attributes edited using portrait-CHAMP. For other facial attributes, we directly generate RGB videos using LivePortrait. A.4. Attribute Transfer To transfer facial attributes from in-the-wild images, we incorporate LoRA layers [26] into the MLP network of the avatar model and optimize these layers. The LoRA layers are trained using animated videos generated from input inthe-wild images. We generate the animated videos following the procedure outlined in Sec. 4.2 of the main paper. To ensure only the desired attribute is transferred, we segment the relevant sub-part using an off-the-shelf segmentation network [32] and apply part-wise loss as described in Eq. (15) of the main paper: Lpartwise, lora = Lrecon (cid:16) Mpart Iitw, Mpart ˆIattr (cid:17) , (40) where Iitw represents the image from video animated in-thewild portrait image, ˆIattr denotes the rendered image with latent zitw regressed by latent mapping MLPz from CLIP features of input in-the-wild image. We observe that using only the partwise loss fails to preserve reference identity of our avatar model and collapse the pretrained latent space. To address this, we introduce 3D loss. The 3D loss encourages the LoRA layers in the avatar model to produce the same output as when the LoRA layers are absent. Specifically, Gaussian random latent codes zrandom from the pretrained latent space are sampled and used as inputs along with the FLAME parameters of an animatable portrait video. The model is trained to minimize the difference between the outputs of the avatar model with and without the LoRA layers, ensuring consistency in 3D Gaussian parameters and 3D positions. Specifically, for the Gaussian attributes inferred with and without LoRA layers: , rd , sd {xd i,lora, rd {xd , od ,cd i,lora,od } = MΘ(xgc i,lora, sd i,lora, cd = MΘ+Θ(xgc , zrandom, β, θ, ψ), i,lora} , zrandom, β, θ, ψ), (41) (42) we calculate the distance between them as follows: L3d = xd i,lora xd 1 + rd i,lora rd +od i,lora od 1 + sd i,lora sd 1 + cd 1. (43) 1 i,lora cd The total loss for LoRA layer optimization is defined as follows: Ltotal, lora = L3d + Lpartwise, lora (44) We perform LoRA layer optimization with learning rate of 1e4 for 5 epochs. B. Evaluation Details B.1. Baseline Implementation Details To demonstrate our pipelines effectiveness, we evaluated our methods compared to three different methods. B.1.1. PEGASUS We train PEGASUS [5] with our synthetic dataset using publicly available code, strictly following the settings described in the paper, including the latent space configuration and network configurations. The model is trained using DDP across 8 RTX 6000 GPUs until convergence. After point rendering with PyTorch3D [50], no additional denoising steps are applied. B.1.2. Conditional INSTA (Cond.TA) To train INSTA with multiple subjects, we introduce latent condition to the density MLP network, referred to as Conditional INSTA (Cond.TA). We adopt the PEGASUS [5] latent configuration to achieve similar sub-part disentangled control. Since the original density MLP network of INSTA is too small to encode thousand of attributes, we increase the MLP width from 64 to 512 and the depth from 2 to 4. As this adjustment sacrifices rendering speed and increases training time, we focus our comparisons solely on quality, excluding rendering speed. The final Conditional INSTA model is trained using DDP with 8 RTX 4090 GPUs until convergence. B.1.3. Conditional SplattingAvatar (Cond.SA) Since SplattingAvatar [54] does not include any network for receiving conditioning, we incorporate an MLP to deform single set of shared canonical 3D Gaussians into subjectspecific canonical 3D Gaussians, similar to the approach in 4 PEGASUS [5]. To ensure fair comparison, we configure the MLP with the same size as PEGASUSs canonical MLP, providing sufficient capacity to represent all subjects in the synthetic dataset. The densification interval is increased from vanilla SplattingAvatar [54] to address the low stability of optimization in early stages. Densification is halted after 5 epochs, as the gathered gradients do not converge, possibly due to exposure to different subjects in each iteration. We adopt the same latent configuration as the PEGASUS model, and the final Conditional SplattingAvatar model is trained using DDP on 8 RTX 4090 GPUs until convergence. B.2. Interpolation Evaluation Details To evaluate the rendering quality of avatars with unseen attributes and interpolation smoothness, we sample avatars from our model using interpolated latent codes. For each of the 9 categories in our synthetic dataset, we randomly select 200 subject pairs and generate 9 interpolated latent codes per pair, following (36). The intervals between the sampled latent codes are evenly spaced. Each interpolated latent code is used to render the corresponding avatar in 5 different poses. This process produces 9,000 images per category and total of 81,000 images across all categories for evaluation. Metrics. We compute FID and KID scores by comparing our renderings with two different image sets: FFHQ [28] and our synthetic evaluation dataset, which is built with the same input reference individual. Specifically, we use (FIDFFHQ, KIDFFHQ) to asses the realism and quality of the renderings by comparing with real face images, and (FIDSYN, KIDSYN) to evaluate identity preservation by comparison with the synthetic evaluation dataset. Since the rendered outputs do not include backgrounds, we remove the backgrounds of all portrait images in FFHQ using MODNet [30] before calculating metrics. The synthetic evaluation image sets are constructed with the same reference image, following our edited portraits generation pipeline. To prevent potential information leaks, we synthesize 2k novel images using text prompts not included in the training dataset. This approach provides more reliable measurement of identity preservation during attribute editing, particularly for changes that partially alter identity features, such as the eyes, eyebrows, and nose, which are challenging to evaluate with existing identity metrics like ArcFace [11]. B.3. User Studies We also conduct user study to evaluate the rendering quality of interpolated samples. Since only PEGASUS [5] and our method receive votes among the four methods in preliminary study, we exclude Cond.TA and Cond.SA from the options. Participants are asked to choose the better images based on interpolation smoothness and image quality for 20 pairs of Method Accuracy Naturalness PSNR SSIM LPIPS FID KID Moore-AnimateAnyone [27, 43] MimicMotion [73] MegActor-Σ [65, 66] Ours (portrait-CHAMP) 17.77 17.27 17.89 20.58 0.6841 0.6641 0.6986 0.2536 0.3012 0.2599 146.59 178.87 155. 0.0530 0.0980 0.0572 0.7417 0.1878 150.59 0.0555 Table 5. Quantitative Comparisons for Image-to-Video Models. We evaluate our portrait-CHAMP with recent diffusion based baselines in face reenactment scenarios. Ours portrait-CHAMP obtain the best scores in accuracy and comparable FID and KID. interpolations. The pairs are randomly selected from the hair category. We collect responses from 229 participants via CloudResearch [8]. C. More Experiments C.1. Additional Results We present additional sample results of attribute-edited portrait image generation, providing seven results for each attribute in Fig. 12. Furthermore, we demonstrate the rendering results of our personalized generative 3D avatar on unseen poses, trained with synthetic dataset created using additional portrait images in Fig. 13 and Fig. 14. Finally, we provide the interpolation results between two latent codes for each attribute in Fig. 15. C.2. Synthetic Monocular Dataset Generation from"
        },
        {
            "title": "Single Image",
            "content": "To demonstrate the effectivness of our Portrait-CHAMP, we evaluate the reconstruction quality and rendering realism compared to diffusion based baselines. Moore AnimateAnyone [43] is open-source repository fine-tuned AnymateAnyone [27] to be specialized on facial reenactment. MimicMotion [73] is full body animating model based on Stable Video Diffusion [2] also capable of reenactment using facial landmarks in DWPose. MegActor-Σ is Diffusion Transformer [47] based approach to solve reenactement problem. We disable the additional audio input option of MegActor-Σ during test here. We test the methods using 20 sequence randomly selected from CelebV-Text dataset [68] not seen during the trainining. We animate the first frame to make other frames and compare with ground truth frames in the video to compute accuracy. We additionally calculate FID and KID against FFHQ dataset [28] to evaluate the naturaless of the animated images. As shown in Tab. 5, our approach achieves the highest reconstruction score across all metrics compared to previous SOTA methods. Though our method scores wrose than Moore-AnimateAnyone in naturaless, our model shows better reconstruction quality, indicating more consistent in 3D important for avatar reconstruction. 5 D. Rights All portrait reference images used in this work are sourced from the FreePik [15] website under free license and are not AI-generated. Our code and synthetic dataset will be publicly released for research purposes only. E. Notations Refer to Tab. 6 for an overview of the notations used in this paper. 6 Figure 12. Example of Attribute-Edited Portrait Image Generation. We present samples of attribute-edited portrait image generation. For each attribute, we display results obtained through random sampling. 7 Figure 13. Unseen Pose Rendering Results. We present the rendering results using latent codes for novel head poses and facial expressions not included in the training dataset, categorized by each attribute. Figure 14. Unseen Pose Rendering from New Reference Individual Results. We present the rendering results from new reference individual using latent codes for novel head poses and facial expressions not included in the training dataset, categorized by each attribute. 9 Figure 15. Interpolation Between Two Latent Codes. We present the rendering results obtained by interpolating between two latent codes."
        },
        {
            "title": "Description",
            "content": "Table 6. Table of notations. Index Gaussian index {1, . . . , } in 3D Gaussian attributes Category index {1, . . . , Nc} of edited attributes in synthetic dataset. Learnable Parameters and Networks MLPc MLPd MLPpose MLPz gc = {xgc }i={1N } Canonical MLP estimating attributes of 3D Gaussians Deformation MLP estimating deformation attributes Pose-conditioned deformation MLP estimating change of Gaussian attributes Latent mapping MLP from CLIP feature fI , fT to subject-specific latent Learnable positions of 3D Gaussians gc sc P C() τ ξ0 ξτ ϵ xi R3 qi R4 si R3 ci R3 oi I2Iinpaint T2I I2V θ R15 β R100 ψ R50 R505023 R1005023 R155023 ˆI/I Spaces of our Avatar Model Generic canonical space, single space shared on all subject Subject-specific canonical space, conditioned by subject latent FLAME-canonical space, deformed from subject-specific canonical space with blendshape Deformed space, deforming with FLAME pose parameters Diffusion Related Text-prompt queried into the diffusion model 2D key points and face landmarks estimator and renderer (OpenPose) Diffusion denoising time-step Encoded latent of the queried RGB images of diffusion model Perturbed latent with noise time-step τ [0, 1] Noise added to the latent Attributes of 3D Gaussians Center of i-th Gaussian, or point position in PEGASUS Covariance Matrixs Quaternion of i-th Gaussian Covariance Matrixs Scale Component of i-th Gaussian Color of i-th Gaussian Opacity of i-th Gaussian Off-the-Shelf Network Text-conditioned Image-to-Image inpainting pipeline, based on image diffusion Text-to-Image diffusion model Portrait animating Image-to-Video model, portrait-CHAMP or LivePortrait [18]. FLAME Parameters of Avatar Deformation FLAME pose parameter FLAME shape parameters FLAME expression parameters FLAME expression blendshape parameters, estimated by MLPd for each Gaussian FLAME shape blendshape parameters, estimated by MLPd for each Gaussian FLAME Linear Blend Skinning (LBS) weight, estimated by MLPd for each Gaussian Rendered and Observed Images Rendered / Ground Truth Image Mask of subpart region"
        }
    ],
    "affiliations": [
        "Seoul National University"
    ]
}