{
    "paper_title": "Survey on Evaluation of LLM-based Agents",
    "authors": [
        "Asaf Yehudai",
        "Lilach Eden",
        "Alan Li",
        "Guy Uziel",
        "Yilun Zhao",
        "Roy Bar-Haim",
        "Arman Cohan",
        "Michal Shmueli-Scheuer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 1 4 6 1 . 3 0 5 2 : r Survey on Evaluation of LLM-based Agents Asaf Yehudai1,2, Lilach Eden2, Alan Li3, Guy Uziel2, Yilun Zhao3, Roy Bar-Haim2, Arman Cohan3, Michal Shmueli-Scheuer2 1The Hebrew University of Jerusalem 2IBM Research 3Yale University {Asaf.Yehudai, Guy.Uziel1}@ibm.com {lilache, roybar, shmueli}@il.ibm.com {haoxin.li, yilun.zhao, arman.cohan}@yale.edu"
        },
        {
            "title": "Abstract",
            "content": "The emergence of LLM-based agents represents paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) applicationspecific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must addressparticularly in assessing costefficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research."
        },
        {
            "title": "Introduction",
            "content": "Recent years saw huge leap in the ability of Large Language Models (LLMs) to address wide range of challenging tasks. Yet, LLMs are static models that are restricted to single-turn, text-to-text interactions. LLM-based agents, hereinafter also referred to as LLM agents, take the power of LLMs step further by integrating them into multi-step flow, while maintaining state that is shared by multiple LLM calls, providing context and consistency. They also utilize external tools to perform computations, access external knowledge and interact with their environment. Agents are able to autonomously conceive, execute and adapt complex 1 plans in real-world environments. This newfound agency empowers them to address problems previously beyond the reach of AI, paving the way for innovative applications across wide spectrum of domains. Reliable evaluation of agents is critical to ensure their efficacy in real-world applications, and to guide further progress in this rapidly evolving field. Since agents are sometimes applied to classical text-to-text AI tasks, there is some overlap between their evaluation and standard LLM benchmarking. However, as their applicability is much broader, they require new types of evaluation methodologies, benchmarks, environments and metrics. The very characteristics that define LLM-based agents their reliance on specific LLM abilities, their sequential operation within dynamic environments, and their capacity to undertake diverse, intricate tasks introduce novel challenges for their evaluation.1 This survey provides the first comprehensive mapping of LLM-based agent evaluation, serving four key audiences: (1) LLM agent developers assessing the capabilities of their systems, (2) practitioners deploying agents in domain-specific applications, (3) benchmark developers addressing evaluation challenges, (4) AI researchers broadly studying current capabilities, risks and limitations of agents. We begin by discussing the evaluation of fundamental agentic capabilities (2), which are commonly employed by agents across different domains and applications. These include planning and multi-step reasoning, tool use, self-reflection, 1The terminology for more complex systems that utilize LLMs as core components varies across the literature. Common terms include LLM-based Agents, LLM Agents, Language Agents, AI Agents, Agents, and Agentic Systems. We adopt the terms LLM-based agents and LLM agents to emphasize both the foundational technology (LLMs) and the agentic capabilities that extend beyond single model inference. Planning and MultiStep Reasoning (2.1) Agent Capabilities Evaluation (2) Function Calling & Tool Use (2.2) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); PFOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (2.4) Web Agents (3.1) Application-Specific Agent Evaluation (3) Software Engineering Agents (3.2) Scientific Agents (3.3) Conversational Agents (3.4) Agent Evaluation Generalist Agents Evaluation (4) Frameworks for Agent Evaluation (5) Development Frameworks NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE(Liu et al., 2024a); ReadAgent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); WebShop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoyager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); WebCanvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWEbench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWEbench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); ITBench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS2 (DeYoung et al., 2021); ScienceWorld(Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcadinho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileos Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CRMArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Databricks Mosaic AI (Databricks, 2023); Galileo Agentic (Galileo, 2025); Vertex AI Gen AI (Google Cloud, 2025); LangSmith (LangChain, 2023); Langfuse (Langfuse, 2023); Patronus AI (Patronus AI, Inc., 2023); LangChain AgentEvals (LangChain, 2025) Gym-like Environments MLGym(Nathani et al., 2025); BrowserGym(Chezelles et al., 2024); SWE-Gym(Pan et al., 2024a) Discussion (6) Current Trends (6.1) Realistic and Challenging Evaluation; Live Benchmarks Emergent Directions (6.2) Advancing Granular Evaluation; Cost and Efficiency Metrics; Scaling & Automating; Safety and Compliance Figure 1: Overview of the paper. and memory. We then review benchmarks and evaluation strategies for prominent types of agentic applications: web agents, software engineering agents, scientific agents and conversational agents (3). Next, we describe benchmarks and leaderboards for evaluating general-purpose agents (4), which assess the agents ability to perform different tasks that require diverse skills. The next section (5) reviews current evaluation frameworks for agent developers. These frameworks integrate with the agents development environment, and support its 2 evaluation throughout the entire development cycle. We conclude with discussion (6) of current trends and emerging research directions in agent evaluation. Figure 1 provides visual overview of the structure of our survey. Our survey is intended to offer researchers and practitioners comprehensive understanding of the current state of agent evaluation and to highlight key areas for future innovation. Scope In this survey, we specifically focus on evaluation methodologies for LLM-based agents. Consequently, widely-used single-call LLM benchmarks, such as MMLU, AlpacaEval, GSM8K, or similar standardized evaluation datasets, wont be extensively discussed. Additionally, detailed introduction to LLM-based agents, modeling choices and architectures, and design considerations are outside our focus, given their comprehensive treatment in existing surveys (e.g., Wang et al. (2024a)). Similarly, while we mention topics such as interactions between multi-agent systems, game agents, and embodied agents in some sections, they are not the main focus of this survey. Instead, our objective is to provide comprehensive overview of evaluation methods for LLM-based agents."
        },
        {
            "title": "2 Agent Capabilities Evaluation",
            "content": "LLM-based agents have evolved to rely on specific design patterns that encapsulate core suite of LLM abilities. Evaluating these capabilities is paramount to understanding the potential and limitations of LLM-based agents. Here we focus on four foundational capabilities of LLM-based agents."
        },
        {
            "title": "2.1 Planning and Multi-Step Reasoning",
            "content": "Planning and multi-step reasoning form the foundation of an LLM agents ability to tackle complex tasks effectively which enables agents to decompose problems into smaller, more manageable subtasks and create strategic execution paths toward solutions (Gao et al., 2023a). Multi-step reasoning in LLMs typically involves executing sequential logical operationstypically requiring 3-10 intermediate stepsto arrive at solutions that cannot be derived through single-step inference (Cobbe et al., 2021; Yang et al., 2018; Suzgun et al., 2022). This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)). Several of these benchmarks, particularly HotpotQA, ALFWorlds, and Game of 24, have been specifically adapted for evaluating agent-based approaches like ReAct, where planning and calling the tools proposed by the agent are interleaved in interactive problem-solving settings. Recent work has developed more specialized frameworks targeting LLM planning capabilities. ToolEmu (Ruan et al., 2023) introduces simulator-based approach for evaluating tool-using agents, revealing that successful planning requires explicit state tracking and the ability to recover from errors. The MINT benchmark (Wang et al., 2023) evaluates planning in interactive environments, showing that even advanced LLMs struggle with long-horizon tasks requiring multiple steps. PlanBench (Valmeekam et al., 2023) provides comprehensive evaluation framework specifically designed to assess planning capabilities in LLM agents across diverse domains, revealing that current models excel at short-term tactical planning but struggle with strategic long-horizon planning. Complementing this, AutoPlanBench (Stein et al., 2023) focuses on evaluating planning in everyday scenarios, demonstrating that even SoTA LLM agents lag behind classical symbolic planners. FlowBench (Xiao et al., 2024) evaluates workflow planning abilities, focusing on expertiseintensive tasks. ACPBench (Kokel et al., 2024) focuses on evaluating LLMs on core reasoning skills. The Natural Plan benchmark (Zheng et al., 2024) is designed to evaluate how LLMs handle real-world planning tasks presented in natural language. SoTA LLM agents perform poorly on this benchmark, particularly as complexity increases. These benchmarks highlight key abilities essential for effective agent planning: (1) task decomposition for breaking down complex problems, (2) state tracking and belief maintenance for accurate multi-step reasoning, (3) self-correction to detect 3 and recover from errors, (4) causal understanding to predict action outcomes, and (5) meta-planning to refine planning strategies."
        },
        {
            "title": "2.2 Function Calling & Tool Use",
            "content": "The ability of LLMs to interact with external tools through function calling is fundamental for building intelligent agents capable of delivering realtime, contextually accurate responses (Qin et al., 2023; Tang et al., 2023). Early works utilized targeted tools, such as retrieval in approaches by augmented language models with retrieval capabilities (Lewis et al., 2020; Gao et al., 2023b; Nakano et al., 2021). Later developments included more generalpurpose tools, exemplified by ToolFormer (Schick et al., 2023), Chameleon (Lu et al., 2023), and MRKL (Karpas et al., 2022). Function calling involves several sub-tasks that work together seamlessly. Intent recognition identifies when function is needed based on user requests. Function selection determines the most appropriate tool for the task. Parameter-value-pair mapping extracts relevant arguments from the conversation and assigns them to function parameters. Function execution invokes the selected function with those parameters to interact with external systems. Finally, response generation processes the function output and incorporates it into the LLMs reply to the user. This integrated process ensures accurate and efficient function calling within the LLMs workflow. Early evaluation efforts offered approaches to evaluate the above sub-tasks while focusing on relatively simple, one-step interactions with explicitly provided parameters. Benchmarks such as ToolAlpaca (Tang et al., 2023), APIBench (Patil et al., 2025), ToolBench (Qin et al., 2023), and the Berkeley Function Calling Leaderboard v1 (BFCL) (Yan et al., 2024) exemplify this phase, employing synthetic datasets and rule-based matching (e.g., via Abstract Syntax Trees) to establish baseline metrics like pass rates and structural accuracy. However, these methods were limited in capturing the complexities of real-world scenarios, which might include multistep conversations, parameters that are not explicitly mentioned in the conversation, and tools with complex input structures and long, intricate outputs. The live nature of BFCL aimed to bridge some of these gaps by introducing BFCL v2, which includes organizational tools, and BFCL v3, which incorporates integrated multi-turn and multi-step evaluation logic. These enhancements provide closer approximation of real-world complexity and emphasize the importance of continuous state management. Complementing this evolution, several benchmarks have broadened the evaluation landscape. For example, ToolSandbox (Lu et al., 2024) differs from previous benchmarks by incorporating stateful tool execution, implicit state dependencies, on-policy conversational evaluation with built-in user simulator, and dynamic evaluation strategies for intermediate and final milestones across arbitrary trajectories. Seal-Tools (Wu et al., 2024b) adopts self-instruct (Wang et al., 2022b) methodology to generate nested tool calls, effectively modeling layered and interdependent interactions. In parallel, API-Bank (Li et al., 2023) emphasizes realistic API engagements by utilizing dialoguebased evaluations and extensive training datasets. Frameworks like NexusRaven (team, 2023) further enrich this landscape by focusing on generalized tool-use scenarios that mirror the diverse challenges encountered in practice. API-Blend (Basu et al., 2024a) suggested comprehensive approach focusing on identifying, curating, and transforming existing datasets into large corpus for training and systematic testing of tool-augmented LLMs. API-Blend mimics real-world scenarios involving API tasks such as API/tool detection, slot filling, and sequencing of detected APIs, providing utility for both training and benchmarking purposes. RestBench (Song et al., 2023) facilitates exploration of utilizing multiple APIs to address complex realworld user instructions. APIGen (Liu et al., 2024c) provides comprehensive automated data generation pipeline that synthesizes high-quality functioncalling datasets verified through hierarchical stages. StableToolBench (Guo et al., 2024) addresses the challenges of function-calling evaluation by introducing virtual API server with caching and simulators to alleviate API status changes. Addressing the inherent complexity of multistep interactions, ComplexFuncBench (Zhong et al., 2025) was specifically designed to assess scenarios requiring implicit parameter inference, adherence to user-defined constraints, and efficient long-context processing. NESTFUL (Basu et al., 2024b) focuses on adding complexity by evaluating LLMs on nested sequences of API calls where outputs from one call serve as inputs to subsequent calls."
        },
        {
            "title": "2.3 Self-Reflection",
            "content": "An emerging line of research focuses on whether agents can self-reflect and improve their reasoning through interactive feedback, thereby reducing errors in multi-step interactions. This requires the model to understand the feedback and dynamically update its beliefs to carry out adjusted actions or reasoning steps over extensive trajectories. Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALFWorld (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025). Improvement was typically measured by determining if the final answer was corrected, providing only coarse evaluation and potentially ill-defined measurement, as observed improvements may depend on specific prompting techniques lacking proper standardization (Huang et al., 2024; Liu et al., 2025). As dedicated effort to establish standardized benchmark for interactive self-reflection, LLFBench (Cheng et al., 2023) was proposed. This benchmark extends diverse decision-making tasks and incorporates task instructions as part of the environment rather than as part of the agent. To mitigate overfitting to specific environments, LLFBench offers options to randomize textual descriptions of task instructions and feedback received by agents. Similarly, LLM-Evolve (You et al., 2024) was introduced to evaluate LLM agents self-reflection capabilities on standard benchmarks such as MMLU (Hendrycks et al., 2020). This approach evaluates agents based on past experiences by collecting previous queries with feedback and extracting them as in-context demonstrations. To provide more granular insights into different feedback types, (Pan et al., 2025) focused specifically on coding agents, extending existing coding benchmarks like APPS (Hendrycks et al., 2021a) and LiveCodeBench (Jain et al., 2024) to interactive settings. From cognitive science perspective, ReflectionBench (Li et al., 2024) was designed to assess LLMs cognitive reflection capabilities, breaking down reflection into components like perception of new information, memory usage, belief updating following surprise, decision-making adjustments, counterfactual reasoning, and meta-reflection."
        },
        {
            "title": "2.4 Memory",
            "content": "Memory mechanisms in LLM-based agents improve their handling of long contexts and information retrieval, overcoming static knowledge limits and supporting reasoning and planning in dynamic scenarios (Park et al., 2023). Unlike tool use, which connects agents to external resources, memory ensures context retention for extended interactions like processing documents or maintaining conversations. Agents rely on short-term memory for realtime responses and long-term memory for deeper understanding and applying knowledge over time. Together, these memory systems allow LLM-based agents to adapt, learn, and make well-informed decisions in tasks requiring persistent information access. One prominent line of research focuses on addressing the challenge of limited context lengths in LLMs by incorporating memory mechanisms to enhance reasoning and retrieval across extended contexts and conversations. Recent works, such as ReadAgent (Lee et al., 2024), MemGPT (Packer et al., 2024), and A-MEM (Xu et al., 2025), investigate these methods and evaluate their efficacy through reasoning and retrieval metrics. Specifically, ReadAgent structures reading by grouping content, condensing episodes into memories, and retrieving passages, with effectiveness shown on datasets like QUALITY (Pang et al., 2021), NarrativeQA (Koˇcisk`y et al., 2018), and QMSum (Zhong et al., 2021). Similarly, A-MEM introduces an advanced memory architecture evaluated using the LoCoMo benchmark (Maharana et al., 2024), while MemGPT manages tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021). For episodic memory evaluation, (Huet et al., 2025) proposes specialized benchmark to assess how LLMs generate and manage memories that capture specific events with contextual details. This benchmark utilizes synthetically created book chapters and events with LLMs-based judge evaluation metrics to measure accuracy and relevance. StreamBench (Wu et al., 2024a) represents more challenging setting, evaluating how agents leverage external memory componentsincluding the memory of previous interactions and external feedbackto continuously improve performance over time, with quality and efficiency assessed across diverse datasets including text-to-SQL tasks (e.g., Spider (Yu et al., 2018)), ToolBench (Xu et al., 2023), and HotpotQA (Yang et al., 2018)."
        },
        {
            "title": "Beyond context",
            "content": "length optimization, memory mechanisms also enhance real-time decisionmaking and learning in agent settings, focusing on action optimization (Liu et al., 2024a; Shinn et al., 2023; Wang et al., 2024b). For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with two-part memory system evaluated through human judgment on quality metrics and efficiency. Similarly, KARMA (Wang et al., 2024b) tests memory in household tasks using metrics such as success rate, retrieval accuracy, and memory hit rate, demonstrating how memory mechanisms significantly improve agent performance across diverse domains requiring complex reasoning and persistent information retention. LTMbenchmark (Castillo-Bolado et al., 2024a) evaluates conversational agents through extended, multitask interactions with frequent context switching to test long-term memory and information integration capabilities. The results demonstrate that while LLMs generally perform well in single-task scenarios, they struggle with interleaved tasks, and interestingly, short-context LLMs equipped with long-term memory systems can match or exceed the performance of models with larger context windows."
        },
        {
            "title": "3 Application-Specific Agents Evaluation",
            "content": "The landscape of application-specific agents is rapidly expanding, with an increasing number of specialized agents emerging across popular categories such as tools, web, software, game, embodied, and scientific agents (Wang et al., 2024a). In this section, we focus on four prominent categories that exemplify the diversity and potential of these agents, offering insights into their evaluation frameworks and performance metrics tailored to their unique applications. Agent benchmarks offer systematic framework for assessing the diverse capabilities of LLMbased agents by integrating three key elements. First, they utilize dataset of clearly defined tasksranging from website navigation to complex scientific problem-solvingthat outline what agents are expected to achieve. Second, they establish the operating environment, which may be simulated (whether static or dynamic) or real-world, and can incorporate user simulations, variety of tools, and specific policies to adhere to. Third, they apply evaluation metrics, such as success rate, efficiency, and accuracy, to measure performance. These metrics can be applied with varying degrees of granularity, from tracking individual actions and milestones to assessing overall, end-to-end task completion."
        },
        {
            "title": "3.1 Web Agents",
            "content": "Web agents are AI systems designed to interact with websites to perform tasks such as booking flights or shopping. Their evaluation involves testing how effectively they complete tasks, navigate web environments, and adhere to safety and compliance rules. As these agents have evolved, so too have the benchmarks used to assess them, with recent developments capturing an increasingly complex range of real-world interactions. Initial efforts in web-agent evaluation focused on basic simulation environments. Early benchmarks such as MiniWob (Shi et al., 2017) and MiniWoB++ (Liu et al., 2018) provided fundamental frameworks for assessing navigation and task automation capabilities. These pioneering studies established essential evaluation protocols and highlighted key challenges in executing web-based tasks, laying the groundwork for more sophisticated assessments. Building on these early efforts, subsequent research introduced static datasets that enable offline, reproducible evaluation. For example, WebShop (Yao et al., 2022) simulates online shopping scenarios, requiring agents to perform tasks ranging from product search to checkout processes. Similarly, Mind2Web (Deng et al., 2023) and WebVoyager (He et al., 2024) extend this paradigm by incorporating broader spectrum of web interactions, thereby allowing for comprehensive assessments of an agents ability to navigate complex website structures and achieve intermediate goals. These benchmarks have been instrumental in standardizing the evaluation process, and facilitating direct comparisons across different methodologies. More recent efforts have shifted toward dynamic, online benchmarks that more closely mimic realworld conditions. WebLinX (Lù et al., 2024) introduces dynamic interaction model in which agents must adapt to continuous changes in the web 6 interface, testing the robustness of their decisionmaking processes. WebArena (Zhou et al., 2023) and its visual variant, Visual-WebArena (Koh et al., 2024), incorporate realistic user interface elements and visual cues, requiring agents to not only follow predefined workflows but also interpret and respond to visual information. In addition, WorkArena (Drouin et al., 2024) and WorkArena++ (Boisvert et al., 2025) simulate complex, multistep tasks typical of office or enterprise environments, where coordinating several actions is necessary to achieve long-term objectives. MMInA (Zhang et al., 2024) provides multimodal, Multihop, holistic evaluation. AssistantBench (Yoran et al., 2024) focuses on realistic multi-site timeconsuming tasks. Notably, WebCanvas (Pan et al., 2024b) refines the dynamic evaluation framework by specifically measuring the completion rates of key navigational nodes, thereby offering more granular analysis of agent performance. Recent advances have further broadened the evaluation landscape. The introduction of STWebAgentBench (Levy et al., 2024) represents an effort to assess web agents in settings that integrate both static and dynamic elements, providing insights into agents performance under varied conditions. While these benchmarks have significantly advanced our understanding of web-agent capabilities, most of them continue to focus primarily on task completion and navigational efficiency. Critical aspects such as policy compliance, risk mitigation, and adherence to organizational safety protocols remain underexplored. As web agents move closer to real-world deployment, addressing these gaps will be essential for ensuring both their practical utility and safe operation."
        },
        {
            "title": "3.2 Software Engineering Agents",
            "content": "The evaluation of software engineering (SWE) agents began with benchmarks that measured fundamental coding capabilities, such as HumanEval (Chen et al., 2021b) and MBPP (Austin et al., 2021). These early benchmarks focused on short, selfcontained, algorithm-specific tasks, offering an initial glimpse into the potential of LLMs for code generation. More recently, open-domain coding benchmarks (Wang et al., 2022c; Lai et al., 2022) have emerged as notable step forward by incorporating application-specific scenarios and interactions with diverse libraries and tools. However, while these benchmarks mark clear progress, they generally evaluate simpler intents and limited tool usage, thus falling short of addressing the full complexity of real-world SWE tasks. SWE-bench (Jimenez et al., 2023) was introIt is duced to address the above shortcomings. constructed from real-world GitHub issues and offers an end-to-end evaluation framework, including detailed issue descriptions, complete code repositories, execution environments (e.g., Docker), and validation tests. To enhance evaluation reliability, several variants have been proposed. SWE-bench Lite (SWE-bench Lite, 2024) focuses on subset of 300 issues involving bug fixing, filtering out tasks requiring complicated multi-file edits or extraneous elements. SWE-bench LiteS (Xia et al., 2024) further refines the dataset by removing tasks with exact patches or insufficient descriptive information, while SWE-bench Verified (OpenAI, 2024) includes only those issues with clear, informative descriptions and robust test cases. More recently, SWE-bench+ (Aleithan et al., 2024) has been introduced to mitigate critical evaluation flaws such as solution leakage and weak test cases, thereby providing more robust benchmark for assessing SWE agents. Additionally, Java version of SWE-bench was introduced in (Zan et al., 2024). SWE-bench Multimodal (Yang et al., 2024) evaluates agents in visual software domains, targeting JavaScript-based applications with visual elements in problems and tests. It highlights challenges in visual problem-solving and cross-language generalization, with top systems showing lower resolution rates. TDD-Bench Verified (Ahmed et al., 2024) and SWT-Bench (Mündler et al., 2024) evaluate the agents ability to generate tests from user issues in real-world Github repositories. ITBench (Jha et al., 2025) offers benchmark for evaluating challenging real-world IT automation tasks. Complementing these developments, AgentBench (Liu et al., 2023b) has emerged as dedicated framework for evaluating the interactive capabilities of SWE agents. AgentBench provides insights into agent performance in dynamic settings through real-time interaction and environment manipulation. Finally, the introduction of SWELancer (Miserendino et al., 2025) represents the latest trend in benchmark development. By targeting freelance coding tasks, SWELancer links agent performance to monetary value, underscoring the challenges in achieving long-term reasoning and decision-making in complex, real-world scenarios."
        },
        {
            "title": "3.3 Scientific Agents",
            "content": "The evaluation of scientific agents has evolved from early benchmarks assessing basic reasoning to comprehensive frameworks evaluating diverse scientific research capabilities. Initial benchmarks emphasized scientific knowledge recall and reasoningexamples include ARC (Clark et al., 2018b), ScienceQA (Lu et al., 2022), and ScienceWorld (Wang et al., 2022a). Others focused on the synthesis and contextualization of scientific literature, such as QASPER (Dasigi et al., 2021), QASA (Lee et al., 2023), and MS2(DeYoung et al., 2021). More recent benchmarks, like SciRiff (Wadden et al., 2024), have expanded to evaluate broader range of tasks, emphasizing the ability to follow user instructions across scientific domains. Recent advancements have shifted the focus toward developing and assessing scientific agents in accelerating scientific research. Emerging benchmarks now cover various stages of the scientific research process: (1) Scientific Ideation: This stage explores whether scientific agents can autonomously generate novel, expert-level research ideas that are comparable to those proposed by human experts (Si et al., 2025). The emphasis is on creativity, relevance, and feasibility in scientific thinking. (2) Experiment Design: Benchmarks like the AAAR-1.0 dataset (Lou et al., 2025) assess an agents ability to systematically plan experiments. This includes formulating hypotheses, selecting appropriate methodologies, and outlining experimental procedures that adhere to scientific rigor. (3) Code Generation for Experiment Execution: Benchmarks such as SciCode (Tian et al., 2024a), ScienceAgentBench (Chen et al., 2025), SUPER (Bogin et al., 2024), and COREBench (Siegel et al., 2024) are pivotal in verifying whether agents can produce accurate, executable scientific code. These benchmarks ensure the code aligns with the specific demands of scientific protocols and maintains computational accuracy. (4) Peer-Review Generation: It examines whether agents can provide comprehensive, substantive feedback that matches or surpasses the quality of human peer reviewers (Chamoun et al., 2024). Beyond the individual task-specific benchmarks described above, there is growing interest in unified frameworks that integrate multiple, interrelated scientific tasks into single platforms. AAAR1.0 (Lou et al., 2025) evaluates scientific agents across four core research tasks, i.e., equation inference, experiment design, paper weakness identification, and review critique, focusing on tasks that require deep domain expertise. MLGym (Nathani et al., 2025) introduces gym-like environment for AI research tasks, covering 13 diverse challenges that simulate real-world research workflows, from hypothesis generation to experimentation and analysis. DiscoveryWorld (Jansen et al., 2024) offers virtual, text-based environment for simulating complete scientific discovery cycles across 120 diverse tasks, emphasizing hypothesis formation, experimentation, and result interpretation. LAB-Bench (Laurent et al., 2024) offers domainspecific evaluation tailored to biological research. It challenges agents with tasks ranging from experimental design to the interpretation of texts, images, and tables."
        },
        {
            "title": "3.4 Conversational Agents",
            "content": "Customer-facing agents are required to handle user requests, while adhering to the companys policies and procedures. Successful completion of such tasks requires the agent to engage with user in multi-turn, task-oriented dialogue, while performing sequence of actions that involve various function calls. common benchmarking approach for these agents is to collect ground truth trajectories with user and agent messages, and function calls. Given prefix of such trajectory, the agent is evaluated on predicting the next step. more flexible approach simulates both the environment and the user. The agent is assessed on its ability to bring the environment to the desired state and communicate the right answer to the user. The Action-Based Conversations Dataset (ABCD) (Chen et al., 2021a) includes over 10K customer-agent conversations, collected via crowdsourcing. These dialogues contain 55 distinct user intents, each requiring unique sequence of actions defined by the corresponding policy. Additional examples of crowdsourced task-oriented dialogue benchmarks are MultiWOZ (Budzianowski et al., 2018) and SMCalFlow (Andreas et al., 2020). fully automated pipeline for generating tests for conversational AI agents in the customer service domain is described in (Arcadinho et al., 2024). Utilizing an LLM as generator at each step, they create set of intents, procedure defining how each intent should be handled by the agent, tool 8 APIs to be called by the agent, flow graph and conversation graph, from which conversation paths are sampled. Finally, prefixes of the conversation path are extracted as tests. Their manually-filtered ALMITA benchmark includes 192 conversations for 14 intents, resulting in 1420 tests. The τ -Bench benchmark (Yao et al., 2024) emulates dynamic conversations between an agent and an LLM-simulated user in two customer service domains, airline and retail. The benchmark was constructed manually, with some LM assistance. Each domain includes several databases, associated APIs, and domain policy provided to the agent as system prompt. Task instances include an instruction for the user simulation and ground truth annotation for the expected database write operation and the required output for the users question. The dataset includes 115 retail tasks and 50 airline tasks. IntellAgent (Levi and Kadar, 2025a) provides an open-source framework for automatic benchmarking of conversational agents, taking schema of the system database and company policies document as input. It constructs policy graph, from which list of policies is sampled. It then creates an event addressing these policies, and simulates dialogue between the tested agent and user agent, based on the event information. Finally, critique agent analyzes the dialogue and provides detailed feedback on the tested policies."
        },
        {
            "title": "4 Generalist Agents Evaluation",
            "content": "Building on the evaluation of basic agentic capabilities and application-specific ones, we now turn to examine benchmarks for general agents. As LLMs evolved from task-specific to general-purpose, agents are now transitioning from applicationspecific to more general-purpose ones. These agents integrate core LLM abilities with skills like web navigation, information retrieval, and code execution to tackle complex challenges. This shift necessitates broader evaluation methods, leading to the development of benchmarks that assess their diverse capabilities. primary category of these benchmarks focuses on evaluating general capabilities that emphasize multi-step reasoning, interactive problem-solving, and proficient tool use. The GAIA benchmark (Mialon et al., 2023) includes 466 human-crafted, realworld questions that test an agents reasoning, multimodal understanding, web navigation, and general tool-use abilities. Similarly, Galileos Agent Leaderboard (Galileo, 2025) emphasizes the evaluation of agents abilities to perform function calls and API invocations in real-world applications such as database queries, online calculators, and web services. AgentBench (Liu et al., 2023a) introduces suite of interactive environments that include operating system commands, SQL databases, digital games, and household tasks. These benchmarks collectively highlight the core competencies required for general agentsflexibility, multi-step reasoning, and adaptive tool use. Beyond general reasoning and tool use, another crucial dimension in evaluating general agents lies in their performance within full-scale computer operating environments. Benchmarks like OSWorld (Xie et al., 2024), OmniACT (Kapoor et al., 2024a), and AppWorld (Trivedi et al., 2024) test whether agents can navigate real-world computer systems, execute complex tasks, and coordinate actions across multiple applications. In these settings, agents must write and modify interactive code, handle complex control flows, and ensure robust execution without causing unintended system changes. Motivated by the need to assess how general agents perform in realistic professional settings, recent benchmarks extend evaluation into digital work environments, where agents must manage tasks akin to those of human employees. TheAgentCompany (Xu et al., 2024) creates an extensible environment resembling small software company, in which agents browse internal websites, write code, run programs, and communicate with coworkers. CRMArena (Huang et al., 2025) focuses on customer relationship management (CRM), simulating large-scale CRM environment filled with interconnected data about accounts, orders, knowledge articles, and cases. It examines whether agents can perform multi-step operations using both UI and API access, adhere to domain-specific policies, and integrate various pieces of information to complete complex enterprise tasks. As benchmarks diversify, there is growing need for unified platforms that consolidate testing criteria. Holistic Agent Leaderboard (HAL) (Stroebl et al., 2025) serves as standardized evaluation platform that aggregates multiple benchmarks, covering coding, interactive applications, and safety assessments."
        },
        {
            "title": "5 Frameworks for Agent Evaluation",
            "content": "In response to the growing need for systematic assessment of LLM agents, several frameworks have recently emerged, providing developers with essential tools to evaluate, refine, and improve their performance, quality, and efficiency. Unlike the benchmarks discussed in the preceding section, which compare the performance of fully developed systems across predefined scenarios, these frameworks serve as integral components of the development ecosystem, enabling continuous monitoring and indepth error analysis across both development and deployment. Rather than supplying standardized test data, they allow developers to design and evaluate their own scenarios, offering greater flexibility. Moreover, these frameworks are designed to be highly general, supporting wide range of development use cases rather than focusing on specific tasks, making them versatile tools for AI research and application. While earlier evaluation frameworks for LLMbased applications primarily focused on assessing models task completion ability through single-call interactions (e.g. OpenAI Evals (OpenAI, 2023)), the rise of agentic workflows has created need for more advanced evaluation frameworks capable of assessing multi-step reasoning, trajectory analysis, and specific agent capabilities such as tool usage. There are many frameworks supporting the evaluation of wide range of agent types, including LangSmith (LangChain, 2023), Langfuse (Langfuse, 2023), Google Vertex AI evaluation service (Google Cloud, 2025), Arize AIs Evaluation Framework (Arize AI, Inc, 2025), Galileo Agentic Evaluation (Galileo, 2025), Patronus AI (Patronus AI, Inc., 2023), LangChains AgentEvals (LangChain, 2025); Databricks Mosaic AI Agent Evaluation (Databricks, 2023) which is mostly designed for RAG like tasks, Botpress Multi-Agent Evaluation System (Kargwal, 2025) and AutoGen (Dibia et al., 2024) for multi-agent systems, and more. All evaluation platforms provide continuous monitoring of agent trajectories, assessing key performance metrics such as task completion rates, latency, execution speed, and, in some cases, throughput and memory usage (LangChain, 2023). Some frameworks utilize the OpenTelemetry (Blanco, 2023) observability framework and their infrastructure, including Langfuse (Langfuse, 2023) and Google Vertex AI (Google Cloud, 2025). Beyond observability and monitoring, each framework incorporates unique methodologies for quality assessment, providing additional layers of evaluation. Evaluating agentic workflows occurs at multiple levels of granularity, each focusing on different aspects of the agents dynamics. Final Response Evaluation. Frameworks often incorporate LLM-based judges to evaluate agent responses against predefined criteria, with some offering proprietary judge models (e.g. Databricks Mosaic (Databricks, 2023), and PatronusAI (Patronus AI, Inc., 2023)). Additionally, most platforms allow for customizable assessment metrics, enabling domain-specific evaluation of output quality and relevance. Stepwise Evaluation. Most evaluation frameworks support granular assessments of individual agent actions or LLM calls, facilitating root cause analysis of errors. This includes assessing textual output with predefined judges, and assessing tool selection and execution by either comparing the selected tool against an expected tool for given step, or using an automated judge to verify the tool choice, its parameters, and the correctness of the execution output. Furthermore, Galileo Agentic Evaluation (Galileo, 2025) introduces an action advancement metric, which measures whether each step successfully contributes to or advances toward user-defined goal. This approach refines stepwise evaluation by assessing progress rather than solely relying on binary success/failure outcomes. key challenge in the current stepwise evaluation schemes lies in the scope and reliability of the automated judges. Many judges are task-specific, making them well-suited for particular evaluations but difficult to generalize across complex workflows. Conversely, more general-purpose judges may offer broad applicability but lack clear quality guarantees. Trajectory-Based Assessment. In addition to stepwise evaluation, some platformssuch as Google Vertex AI (Google Cloud, 2025) and LangSmith (LangChain, 2023) support trajectory-based assessments, which analyze the sequence of steps taken by an agent in relation to an expected optimal path. This method evaluates the agents decisionmaking process, particularly regarding tool selection and sequencing. AgentEvals (LangChain, 2025) also enables LLM-as-judge evaluation of agent trajectories, with or without reference trajectory. Additionally, it supports graph evaluation for frameworks like LangGraph, which model 10 Framework Stepwise Assessment Monitoring Trajectory Assessment Human in the Loop Synthetic Data Generation A/B Comparisons LangSmith (LangChain) Langfuse (Langfuse) Google Vertex AI evaluation (Google Cloud) Arize AIs Evaluation (Arize AI, Inc) Galileo Agentic Evaluation (Galileo) Patronus AI (Patronus AI, Inc.) AgentsEval (LangChain) Mosaic AI (Databricks) Table 1: Supported evaluation capabilities of major agent frameworks. Note that some of these capabilities are still in initial phases of development, as discussed further in the text. agents as graphs, by assessing whether an agent follows the expected workflow and correctly invokes the appropriate nodes and transitions. However, the reliance on reference sequences, combined with the non-deterministic nature of agentic workflows and the existence of multiple valid solutions, introduces significant challenges in defining and benchmarking optimal trajectories. Datasets. critical aspect of agent evaluation is input data curation and annotation. Most frameworks provide integrated annotation tools, support human-in-the-loop evaluation, where human feedback is collected from production runs to refine model configurations, and enable the extraction of evaluation datasets from production logs, leveraging real-world interactions to enhance assessment quality. Additionally, platforms such as Patronus AI (Patronus AI, Inc., 2023), and Databricks Mosaic (Databricks, 2023) facilitate synthetic data generation using proprietary seed data. A/B comparisons. Current evaluation frameworks support A/B comparisons, enabling the sideby-side analysis of inputs, outputs, and metrics from at least two test runs. In some casessuch as Patronus AI (Patronus AI, Inc., 2023)these frameworks also facilitate the comparison of aggregated results across multiple runs from distinct experimental setups. Additionally, these frameworks provide the capability to drill down into individual trajectories, identifying specific failure points. However, obtaining large-scale insights at the trajectory or stepwise level remains significant challenge. Table 1 presents key frameworks for agent evaluation along with their support for the evaluation features discussed in this section. Gym-like Environments. The frameworks discussed primarily monitor and assess agent behavior passively in real-world scenarios. However, evaluation often requires controlled, interactive setting with simulated environment, provided by Gym-like frameworks. Inspired by OpenAI Gym (Brockman et al., 2016), which was originally designed for training and evaluating Reinforcement Learning algorithms, these frameworks have been adapted to the training and evaluation of LLM agents using realistic task simulations, allowing LLM agents to interact with dynamic environments. Moreover, these frameworks enable standardized evaluation across various benchmarks, with proposals made for web agents (Chezelles et al., 2024), AI research agents (Nathani et al., 2025) and SWE agents (Pan et al., 2024a)."
        },
        {
            "title": "6.1 Current Trends",
            "content": "Our review of the evolution of agent benchmarking highlights several converging trends shaping the field. We identify two primary motives exhibited in the development of new evaluation methodologies, which we outline in the subsequent discussion. Realistic and Challenging Evaluation. Early agent evaluations often relied on simplified, static environments. However, there is clear shift toward benchmarks that more accurately reflect realworld complexities. In web agent evaluation, for example, we have moved from basic simulations like MiniWob to dynamic online environments like WebArena and VisualWebArena, and from LABBench which is static and narrow to DiscoveryWorld for scientific agents. In software engineering, SWE-bench utilizes real-world GitHub issues, moving beyond synthetic coding problems. This shift toward realism is key to evaluating agents in real-world scenarios, capturing interaction nuances missed by simpler benchmarks. Benchmarks like Natural Plan, which incorporates simulated API results from real-world tools like Google Calendar and Maps, further exemplify this drive for realistic task settings. Concurrently, to keep pace with increasingly capable agents and ensure benchmarks remain challenging, there is distinct trend toward 11 greater task complexity and difficulty. This is evident from benchmarks like SWE-bench and SWELancer targeting complex coding tasks, COREBench for scientific computational reproducibility, and intricate general agent benchmarks like GAIA and TheAgentCompany. key indicator of their difficulty is the low score of the best-performing agents in their papers, sometimes as low as 2%. This increased challenge is crucial for stress-testing agents, revealing limitations, and driving advances in long-horizon planning, robust reasoning, and tool use. Live Benchmarks. The rapid pace of LLM and agent development necessitates evaluation methodologies that are adaptive and continuously updated. Static benchmarks can quickly become outdated as models improve, potentially leading to benchmark saturation and reduced ability to differentiate between systems. We observe this dynamic approach in the evolution of BFCL, which has progressed through multiple versions, incorporating live datasets, organizational tools, and multiturn evaluation logic to remain relevant. Similarly, the continuous refinement and variant creation within the SWE-bench family (SWE-bench Lite, SWE-bench Verified, SWE-bench+) along with the development of IntellAgent based on τ -Bench , demonstrates an ongoing effort to enhance and adapt agent benchmarks to meet evolving evaluation needs. This dynamic approach is essential for maintaining the relevance of benchmarks in this rapidly advancing field."
        },
        {
            "title": "6.2 Emergent Directions",
            "content": "Observed but not yet fully established trends point to promising future research opportunities for advancing agent evaluation. Advancing Granular Evaluation. Many current benchmarks rely on coarse-grained, end-to-end success metrics that, while useful for gauging overall performance, fall short in diagnosing specific agent failures. This lack of granularity obscures insights into intermediate decision processes such as tool selection and reasoning quality. Addressing this limitation calls for the development of standardized, fine-grained evaluation metrics that capture the trajectory of an agents task execution. Future work should explore detailed, step-by-step assessmentssimilar to those emerging in benchmarks like WebCanvas (Pan et al., 2024b) and frameworks like LangSmith and Galileo Agentic Evaluations (Galileo, 2025) to provide richer feedback and guide targeted improvements. Cost and Efficiency Metrics. As observed by Kapoor et al. (2024b), current evaluations often prioritize accuracy while overlooking cost and efficiency measurements. This emphasis can inadvertently drive the development of highly capable but resource-intensive agents, limiting their practical deployment. Future evaluation frameworks should integrate cost efficiency as core metric, tracking factors such as token usage, API expenses, inference time, and overall resource consumption. Establishing standardized cost metrics will help guide the development of agents that balance performance with operational viability. Scaling & Automating. The reliance on static human annotated evaluation poses significant scalability challenges, as these methods can be resourceintensive and quickly outdated in rapidly evolving field. This shortcoming underscores the need for scalable, automated evaluation approaches. Future directions include leveraging synthetic data generation techniques to create diverse and realistic task scenarios imitated by efforts such as IntellAgent (Levi and Kadar, 2025b) and Mosaic AI Agent Evaluation (Databricks, 2023). Another avenue is automating evaluation by employing LLM-based agents as evaluators, termed Agent-as-a-Judge. As highlighted by Zhuge et al. (2024), this approach not only reduces the reliance on resource-intensive human annotation but also holds the potential to capture more nuanced aspects of agent performance through agentic evaluation processes. By harnessing these approaches, the community can achieve continuous, fine-grained, and cost-effective assessment of agent performance. Safety and Compliance. notable shortcoming in current benchmarks is the limited focus on safety, trustworthiness, and policy compliance. While early efforts (e.g., AgentHarm (Andriushchenko et al., 2024) and ST-WebAgentBench) have begun to address these dimensions, evaluations still lack comprehensive tests for robustness against adversarial inputs, bias mitigation, and organizational and societal policy compliance. Future research should prioritize developing multi-dimensional safety benchmarks that simulate real-world scenarios, particularly in multi-agent scenarios where emergent risks may arise (Hammond et al., 2025). This can ensure that agents are not only effective but also safe and secure."
        },
        {
            "title": "7 Conclusion",
            "content": "The field of LLM-based agent evaluation is rapidly advancing, driven by the need to assess increasingly complex and autonomous systems. While significant progress has been made in creating more realistic, dynamic, and challenging benchmarks, critical gaps remain, particularly in the areas of safety, fine-grained evaluation, and cost-efficiency. Addressing these shortcomings and pursuing the outlined future directions will be crucial for ensuring the responsible development and effective deployment of LLM-based agents in real-world applications."
        },
        {
            "title": "References",
            "content": "Toufique Ahmed, Martin Hirzel, Rangeet Pan, Avraham Shinnar, and Saurabh Sinha. 2024. Tdd-bench verified: Can llms generate tests for issues before they get resolved? arXiv preprint arXiv:2412.02883. Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. 2024. Swe-bench+: Enhanced coding benchmark for llms. ArXiv, abs/2410.06992. Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-oriented dialogue as dataflow synthesis. Transactions of the Association for Computational Linguistics, 8:556571. Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, and Xander Davies. 2024. Agentharm: benchmark for measuring harmfulness of llm agents. Preprint, arXiv:2410.09024. Samuel Arcadinho, David Oliveira Aparicio, and Mariana S. C. Almeida. 2024. Automated test generation to evaluate tool-augmented LLMs as conversational In Proceedings of the 2nd GenBench AI agents. Workshop on Generalisation (Benchmarking) in NLP, pages 5468, Miami, Florida, USA. Association for Computational Linguistics. Arize AI, Inc. 2025. Agent evaluation. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. ArXiv, abs/2108.07732. Kinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim Munawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis Lastras. 2024a. Api-blend: comprehensive corpora for training and benchmarking api llms. arXiv preprint arXiv:2402.15491. Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, et al. 2024b. Nestful: benchmark for evaluating llms on nested sequences of api calls. arXiv preprint arXiv:2409.03797. Pratik Bhavsar. leaderboard. https://huggingface.co/spaces/galileo-ai/ agent-leaderboard. Agent 2025. Daniel Gomez Blanco. 2023. Practical OpenTelemetry. Springer. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. Super: Evaluating agents on setting up and executing tasks from research repositories. Preprint, arXiv:2409.07440. Léo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault de Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. 2025. Workarena++: Towards compositional planning and reasoning-based common knowledge work tasks. Advances in Neural Information Processing Systems, 37:59966051. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. 2016. Openai gym. arXiv preprint arXiv:1606.01540. Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašic. 2018. MultiWOZ - largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 50165026, Brussels, Belgium. Association for Computational Linguistics. David Castillo-Bolado, Joseph Davidson, Finlay Gray, and Marek Rosa. 2024a. Beyond prompts: Dynamic conversational benchmarking of large language models. arXiv preprint arXiv:2409.20222. David Castillo-Bolado, Joseph Davidson, Finlay Gray, and Marek Rosa. 2024b. Beyond prompts: Dynamic conversational benchmarking of large language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 13 Eric Chamoun, Michael Schlichtkrull, and Andreas Vlachos. 2024. Automated focused feedback generation for scientific writing assistance. In Findings of the Association for Computational Linguistics: ACL 2024, pages 97429763, Bangkok, Thailand. Association for Computational Linguistics. Derek Chen, Howard Chen, Yi Yang, Alexander Lin, and Zhou Yu. 2021a. Action-based conversations dataset: corpus for building more in-depth taskIn Proceedings of the oriented dialogue systems. 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 30023017, Online. Association for Computational Linguistics. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021b. Evaluating large language models trained on code. ArXiv, abs/2107.03374. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. 2024. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Preprint, arXiv:2410.05080. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. 2025. Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. In The Thirteenth International Conference on Learning Representations. Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, and Adith Swaminathan. 2023. Llf-bench: Benchmark for interactive learning from language feedback. Preprint, arXiv:2312.06853. De Chezelles, Thibault Le Sellier, Maxime Gasse, Alexandre Lacoste, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, et al. 2024. The browsergym ecosystem for web agent research. arXiv preprint arXiv:2412.05467. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018a. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018b. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. dataset of information-seeking questions and answers anchored in research papers. Preprint, arXiv:2105.03011. Databricks. 2023. Mosaic ai agent evaluation: Assessing ai application performance. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114. Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. Ms2: Multi-document summarization of medical studies. Preprint, arXiv:2104.06486. Victor Dibia, Jingya Chen, Gagan Bansal, Suff Syed, Adam Fourney, Erkang Zhu, Chi Wang, and Saleema Amershi. 2024. Autogen studio: no-code developer tool for building and debugging multi-agent systems. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 7279. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam Laradji, Manuel Del Verme, Tom Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, et al. 2024. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718. Galileo. 2025. Introducing agentic evaluations. Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. 2023a. Large language models empowered agent-based modeling and simulation: survey and perspectives. Preprint, arXiv:2312.11970. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023b. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2. 14 Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346 361. Google Cloud. 2025. Evaluate your ai agents with vertex gen ai evaluation service. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714. Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenˇciak, et al. 2025. Multi-agent risks from advanced ai. arXiv preprint arXiv:2502.14143. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, E. Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir R. Radev. 2022. Folio: Natural language reasoning with first-order logic. EMNLP 2024. Simeng Han, Aaron Yu, Rui Shen, Zhenting Qi, Martin Riddell, Wenfei Zhou, Yujie Qiao, Yilun Zhao, Semih Yavuz, Ye Liu, Shafiq Joty, Yingbo Zhou, Caiming Xiong, Dragomir R. Radev, Rex Ying, and Arman Cohan. 2024. P-folio: Evaluating and improving logical reasoning with abundant human-written reasoning chains. EMNLP 2024 Findings. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: Building an end-toend web agent with large multimodal models. arXiv preprint arXiv:2401.13919. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021a. Measuring coding challenge competence with apps. Preprint, arXiv:2105.09938. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language models cannot self-correct reasoning yet. Preprint, arXiv:2310.01798. Kung-Hsiang Huang, Akshara Prabhakar, Sidharth Dhawan, Yixin Mao, Huan Wang, Silvio Savarese, Caiming Xiong, Philippe Laban, and Chien-Sheng Wu. 2025. Crmarena: Understanding the capacity of llm agents to perform professional crm tasks in realistic environments. Preprint, arXiv:2411.02305. Alexis Huet, Zied Ben Houidi, and Dario Rossi. 2025. Episodic memories generation and evaluation benchmark for large language models. Preprint, arXiv:2501.13121. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Peter Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, and Peter Clark. 2024. Discoveryworld: virtual environment for developing and evaluating automated scientific discovery agents. Preprint, arXiv:2406.06769. Saurabh Jha, Rohan Arora, Yuji Watanabe, Takumi Yanagawa, Yinfang Chen, Jackson Clark, Bhavya Bhavya, Mudit Verma, Harshit Kumar, Hirokuni Kitahara, et al. 2025. Itbench: Evaluating ai agents across diverse real-world it automation tasks. arXiv preprint arXiv:2502.05352. Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world github issues? ArXiv, abs/2310.06770. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. 2024a. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part LXVIII, page 161178, Berlin, Heidelberg. Springer-Verlag. Sayash Kapoor, Benedikt Stroebl, Zachary Siegel, Nitya Nadgir, and Arvind Narayanan. 2024b. Ai agents that matter. arXiv preprint arXiv:2407.01502. Aryan Kargwal. 2025. Mastering multi-agent evaluation systems in 2025. Botpress Blog. Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. 2022. 15 Mrkl systems: modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252262. Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. 2024. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649. Harsha Kokel, Michael Katz, Kavitha Srinivas, and Shirin Sohrabi. 2024. Acpbench: Reasoning about arXiv preprint action, change, and planning. arXiv:2410.05669. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Yih, Daniel Fried, Si yi Wang, and Tao Yu. 2022. Ds-1000: natural and reliable benchmark for data science code generation. In International Conference on Machine Learning. LangChain. 2025. Agentevals: Evaluating agent trajectories. Inc LangChain. 2023. Langsmith: Evaluation framework for ai applications. Langfuse. 2023. Langfuse: Observability for ai applications. Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. 2024. Lab-bench: Measuring capabilities of language models for biology research. Preprint, arXiv:2407.10362. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024. human-inspired reading agent with gist memory of very long contexts. Preprint, arXiv:2402.09727. Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee. 2023. Qasa: advanced question answering on scientific articles. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Elad Levi and Ilan Kadar. 2025a. Intellagent: multiagent framework for evaluating conversational ai systems. Preprint, arXiv:2501.11067. Elad Levi and Ilan Kadar. 2025b. Intellagent: multiagent framework for evaluating conversational ai systems. arXiv preprint arXiv:2501.11067. Ido Levy, Ben Wiesel, Sami Marreed, Alon Oved, Avi Yaeli, and Segev Shlomov. 2024. Stwebagentbench: benchmark for evaluating safety and trustworthiness in web agents. arXiv preprint arXiv:2410.06703. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474. Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, and Yingchun Wang. 2024. Reflection-bench: probing ai intelligence with reflection. Preprint, arXiv:2410.16270. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. Preprint, arXiv:2304.08244. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, Vancouver, Canada. Association for Computational Linguistics. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided exploration. Preprint, arXiv:1802.08802. Fengyuan Liu, Nouar AlDahoul, Gregory Eady, Yasir Zaki, and Talal Rahwan. 2025. Self-reflection makes large language models safer, less biased, and ideologically neutral. Preprint, arXiv:2406.10400. Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. 2024a. From llm to conversational agent: memory enhanced architecture with fine-tuning of large language models. ArXiv, abs/2401.02777. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023a. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, Hangliang Ding, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Shengqi Shen, Tianjun Zhang, Sheng Shen, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms as agents. ArXiv, abs/2308.03688. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. 2024c. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482. Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang, Jihyun Janice Ahn, Hongchao Fang, Zhuoyang Zou, Wenchao Ma, Xi Li, Kai Zhang, Congying Xia, Lifu Huang, and Wenpeng Yin. 2025. Aaar-1.0: Assessing ais potential to assist research. Preprint, arXiv:2410.22394. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. 2024. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Preprint, arXiv:2209.09513. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36:4344743478. Xing Han Lù, Zdenˇek Kasner, and Siva Reddy. 2024. Weblinx: Real-world website navigation with multiturn dialogue. arXiv preprint arXiv:2402.05930. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term converarXiv preprint sational memory of llm agents. arXiv:2402.17753. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: benchmark for general ai assistants. Preprint, arXiv:2311.12983. Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. 2025. Swe-lancer: Can frontier llms earn $1 million from real-world arXiv preprint freelance software engineering? arXiv:2502.12115. Niels Mündler, Mark Müller, Jingxuan He, and Martin Vechev. 2024. Swt-bench: Testing and validating real-world bug-fixes with code agents. Advances in Neural Information Processing Systems, 37:81857 81887. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, et al. 2025. Mlgym: new framework and benchmark for advancing ai research agents. arXiv preprint arXiv:2502.14499. OpenAI. 2023. Openai evals: framework for evaluating large language models. https://github.com/ openai/evals. OpenAI. 2024. Introducing swe-bench verified. urlhttps://openai.com/index/introducing-swe-benchverified/. Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Memgpt: Towards llms as operating systems. Preprint, arXiv:2310.08560. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa : large-scale multisubject multi-choice dataset for medical domain question answering. Preprint, arXiv:2203.14371. Jane Pan, Ryan Shar, Jacob Pfau, Ameet Talwalkar, He He, and Valerie Chen. 2025. When benchmarks talk: Re-evaluating code llms with interactive feedback. Preprint, arXiv:2502.18413. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024a. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. 2024b. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, et al. 2021. Quality: Question answering with long input texts, yes! arXiv preprint arXiv:2112.08608. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2025. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Patronus AI, Inc. 2023. Patronus ai: Automated testing and evaluation platform for generative ai applications. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. Preprint, arXiv:2405.06682. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto. 2023. Identifying the risks of lm agents with an lmemulated sandbox. ArXiv, abs/2309.15817. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 31353144. PMLR. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reIn Neural Information Proinforcement learning. cessing Systems. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021. Alfworld: Aligning text and embodied environments for interactive learning. Preprint, arXiv:2010.03768. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2025. Can LLMs generate novel research ideas? largescale human study with 100+ NLP researchers. In The Thirteenth International Conference on Learning Representations. Zachary S. Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. 2024. Core-bench: Fostering the credibility of published research through computational reproducibility agent benchmark. Preprint, arXiv:2409.11363. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. 2023. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2023. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv preprint arXiv:2310.16049. Katharina Stein, Daniel Fišer, Jörg Hoffmann, and Alexander Koller. 2023. Autoplanbench: Automatically generating benchmarks for llm planners from pddl. arXiv preprint arXiv:2311.09830. Benedikt Stroebl, Sayash Kapoor, and Arvind Narayanan. 2025. Hal: holistic agent leaderboard for centralized and reproducible agent evalhttps://github.com/princeton-pli/ uation. hal-harness. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models. Preprint, arXiv:2305.16653. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. SWE-bench Lite. 2024. Swe-bench lite. urlhttps://www.swebench.com/lite.html. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301. Nexusflow.ai team. 2023. Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling. Minyang Tian, Luyu Gao, Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, HAO TONG, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. 2024a. Scicode: research coding benchmark curated by scientists. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. 2024b. Scicode: research coding benchmark curated by scientists. Preprint, arXiv:2407.13168. 18 Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. 2024. AppWorld: controllable world of apps and people for benchmarking interactive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1602216076, Bangkok, Thailand. Association for Computational Linguistics. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2023. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36:3897538987. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, and Arman Cohan. 2024. Sciriff: resource to enhance language model instruction-following over scientific literature. Preprint, arXiv:2406.07835. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022a. ScienceWorld: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022b. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560. Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. 2022c. Execution-based evaluation for opendomain code generation. In Conference on Empirical Methods in Natural Language Processing. Zixuan Wang, Bo Yu, Junzhe Zhao, Wenhao Sun, Sai Hou, Shuai Liang, Xing Hu, Yinhe Han, and Yiming Gan. 2024b. Karma: Augmenting embodied ai agents with long-and-short term memory systems. Preprint, arXiv:2409.14908. Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, YunNung Chen, and Hung-yi Lee. 2024a. Streambench: Towards benchmarking continuous improvement of language agents. arXiv preprint arXiv:2406.08747. Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024b. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 372384. Springer. Chun Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. ArXiv, abs/2407.01489. Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, and Yongbin Li. 2024. Flowbench: Revisiting and benchmarking workflow-guided planning for llm-based agents. arXiv preprint arXiv:2406.14884. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, and Graham Neubig. 2024. Theagentcompany: Benchmarking llm agents on consequential real world tasks. Preprint, arXiv:2412.14161. Jing Xu, Arthur Szlam, and Jason Weston. 2021. Beyond goldfish memory: Long-term open-domain conversation. arXiv preprint arXiv:2107.07567. Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023. On the tool manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-mem: Agentic memory for llm agents. Preprint, arXiv:2502.12110. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling https://gorilla.cs.berkeley. leaderboard. edu/blogs/8_berkeley_function_calling_ leaderboard.html. John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriele Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. 2024. Swe-bench multimodal: Do ai systems generalize to visual software domains? ArXiv, abs/2410.03859. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: new benchmark for query-based multiarXiv preprint domain meeting summarization. arXiv:2104.05938. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: humancentric benchmark for evaluating foundation models. Preprint, arXiv:2304.06364. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854. Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. 2024. Agent-as-ajudge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik τ -bench: benchmark for Narasimhan. 2024. tool-agent-user interaction in real-world domains. Preprint, arXiv:2406.12045. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. 2024. Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711. Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. 2024. LLM-evolve: Evaluation for LLMs evolving capability on benchmarks. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 16937 16942, Miami, Florida, USA. Association for Computational Linguistics. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887. Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, et al. 2024. Swe-bench-java: github issue resolving benchmark for java. arXiv preprint arXiv:2408.14354. Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. 2024. Mmina: Benchmarking multihop multimodal internet agents. arXiv preprint arXiv:2404.09992. Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc Le, Ed Chi, et al. 2024. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520. Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. 2025. Complexfuncbench: Exploring multi-step and constrained function calling under long-context scenario. arXiv preprint arXiv:2501.10132."
        }
    ],
    "affiliations": [
        "IBM Research",
        "The Hebrew University of Jerusalem",
        "Yale University"
    ]
}