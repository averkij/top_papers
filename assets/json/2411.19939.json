{
    "paper_title": "VLSBench: Unveiling Visual Leakage in Multimodal Safety",
    "authors": [
        "Xuhao Hu",
        "Dongrui Liu",
        "Hao Li",
        "Xuanjing Huang",
        "Jing Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such a counter-intuitive phenomenon, we discover a visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive text-image queries according to textual queries. However, image-text pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is a more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: http://hxhcreate.github.io/VLSBench"
        },
        {
            "title": "Start",
            "content": "VLSBench: Unveiling Visual Leakage in Multimodal Safety Xuhao Hu1,2*, Dongrui Liu1*, Hao Li1,3, Xuanjing Huang2, Jing Shao1, 1Shanghai Artificial Intelligence Laboratory 2Fudan University 3Beihang University xuhaohu08@gmail.com shaojing@pjlab.org.cn 4 2 0 2 9 ] . [ 1 9 3 9 9 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate counter-intuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs trained with image-text pairs. To explain such counter-intuitive phenomenon, we discover visual safety information leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky and sensitive content in the image has been revealed in the textual query. In this way, MLLMs can easily refuse these sensitive textimage queries according to textual queries. However, imagetext pairs without VSIL are common in real-world scenarios and are overlooked by existing multimodal safety benchmarks. To this end, we construct multimodal visual leakless safety benchmark (VLSBench) preventing visual safety leakage from image to textual query with 2.4k image-text pairs. Experimental results indicate that VLSBench poses significant challenge to both open-source and close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o. This study demonstrates that textual alignment is enough for multimodal safety scenarios with VSIL, while multimodal alignment is more promising solution for multimodal safety scenarios without VSIL. Please see our code and data at: https://hxhcreate.github.io/VLSBench/ Disclaimer: This paper contains offensive content that may be disturbing. 1. Introduction Multimodal large language models (MLLMs) [3, 9, 20, 30] integrate images into large language models (LLMs) [2, 23, 48] and have developed rapidly. MLLMs demonstrate remarkable performance in image understanding, visual question answering, etc. Meanwhile, the extraordinary capacity of MLLMs brings safety concerns in many scenarios [16, 33, 34, 43, 59]. To this end, previous studies use mulEqual contribution Corresponding author timodal image-text pairs to align MLLMs by supervised fine-tuning (SFT) [67] and reinforcement learning from human feedback (RLHF) [62]. However, [5] discovers that textual unlearning (i.e., only using texts for alignment) outperforms multimodal safety alignment methods (e.g., SFT) with significantly reduced data-collection and computational cost, almost 6 times lower. Based on this experimental observation, it seems that textual unlearning can solve the multimodal safety problem. Such phenomenon is counter-intuitive because the image modality introduces different and abundant visual information, which is relatively independent of the text modality. To explain the above counter-intuitive phenomenon, we find that there exists visual safety information leakage (VSIL) problem in multimodal safety data: the sensitive and risky content in the image has been leaked and described in the textual query to certain extent. Specifically, the VSIL samples in Figure 1 show gambling scenario in the image and an online gambling site in textual query. As for another example, the textual query contains making porn which directly describes the image content. Therefore, MLLMs can easily detect and refuse to answer these unsafe and sensitive queries based only on textual input without perceiving and understanding the image content. Furthermore, we find that most multimodal safety benchmarks [8, 16, 34, 46] suffer from the VSIL problems. Building on the above analysis into VSIL, we find that simple textual SFT can achieve comparable safety performance with multimodal alignment methods, e.g. SFT [67] and RLHF [62], while maintaining similar general ability. Specifically, simple textual SFT methods can achieve safety performance exceeding 95%, comparable to multimodal alignment methods, on widely used multimodal safety benchmarks, such as JailbreakV [34], FigStep [16], and VLSafe [8]. It means that visual safety information leakage leads to the unexpected outstanding performance of textual alignment methods, including unlearning and SFT. However, multimodal safety concerns not only contain image-text pairs with VSIL but also include image-text pairs without VSIL, which has been overlooked in existing safety benchmarks. To this end, we construct VLSBench with Figure 1. Overview of our work. We have discovered problem in current multimodal safety data samples, which says visual safety information leakage (VSIL). Based on this leakage, we further find it leads to counter-intuitive problem, that simpler SFT-based alignment methods can perform nearly the same high safety rate. Thus, we construct VLSBench, preventing visual leakage. This newly proposed task discourages textual alignment and motivates more dedicated multimodal alignment methods to better solve this challenging task. The red bar shows evaluation results separately on the raw and jailbreak set of JailbreakV [34], typical dataset with VSIL. The green bar shows evaluation results on our VLSBench. 2.4k image-text pairs. VLSBench is constructed through an automatic pipeline, preventing the visual leakage from visual input to textual query. Specifically, we use an LLM to generate image descriptions and harmful textual queries from harmful elements. Meanwhile, we also leverage an MLLM to generate additional harmful textual queries from collected images [28, 40, 46, 63]. Then, we detoxify these harmful textual queries and obtain harmless textual queries. Furthermore, we conduct an iterative image generation via the Stable-Diffusion-3.5-Large [13] to generate high-quality images from image descriptions. In this way, we pair highquality images and harmless textual queries. Finally, mismatched and safe pairs are filtered out via an MLLM. According to the experimental results on our VLSBench, we have several findings as follows: VLSBench is challenging for current open-source and including LLaVA, Qwen2-VL, close-source MLLMs, Llama3.2-Vision, and GPT-4o. blueMultimodal alignment methods (i.e., trained with image-text pairs) outperform textual alignment methods (i.e., trained with textual samples) on the VSIL-free VLSBench benchmark. It further verifies that VSIL leads to the seemingly superior performance of the textual alignment method. Simple textual SFT is effective for multimodal safety concerns when VSIL occurs. In contrast, multimodal alignment is more promising solution without VSIL. 2. Visual Leakage in Multimodal Safety In this section, we find prevalent problem in existing multimodal safety datasets, named visual safety information leakage (VSIL). We try to make clear definition for this problem. Then, we conduct quantitative experiment and qualitative verification to support our discovery of VSIL problem. We further delve into this problem and discover that simple textual alignment, using only textual samples, can achieve outstanding performance in existing multimodal safety datasets with VSIL problem. 2.1. Visual Safety Information Leakage (VSIL) Definition. Visual safety information leakage (VSIL) means that the safety-related image content has been revealed in textual query. We define VSIL as follows. Given textual query and an image I, let denote safety judge model [21, 26]. The safety judge model classifies whether the input image-text pair is safe or not, i.e., J(T, I) {safe, unsafe}. In this way, VSIL represents that J(T, I) = J(T ) for an unsafe text-image pair J(T, I) = unsafe. Quantitative verification. We have conducted harmful evaluation experiment, leveraging LlamaGuard3-11BVision [12], the most updated safety judge model with vision ability to evaluate the safe label, given its input. We evaluate the following four datasets, FigStep [16], JailbreakV [34], Harmbench-mm [35] and VLSafe [8]. To be 2 Dataset R1(%) R2(%) R1 R2(%) FigStep JailbreakV-raw Harmbench-mm VLSafe 54.80 65.36 30.91 91.35 54.40 65.00 30.00 91.08 0.40 0.36 0.91 0.27 Table 1. Harmful evaluation experiment on multimodal safety datasets, evaluated by LlamaGuard3-11B-Vision. The results are shown in percentages (%). For JailbreakV, we use its miniset and choose the raw query for evaluation. Figure 2. Four examples in current benchmarks to showcase the problem of visual safety information leakage. The leakage information from visual to textual is marked as red. specific, we denote as the number of samples in the evaluated dataset. Then, we separately denote two indicator function as: A1(i) = (cid:40) if J(Ti, Ii) = unsafe 1, 0, otherwise , A2(i) = (cid:40) 1, 0, otherwise if (J(Ti, Ii) = J(Ti)) (J(Ti, Ii) = unsafe) . And (cid:80)N i=1 A1(i) (cid:80)N i=1 A2(i) we calculate R1 = , which means the image-text pair harmful rate in the datasets. To make comparison, we also have R2 = , means the rate of both the image-text pairs and text are harmful. The results shown in Table 1 demonstrate that R1 R2 is less than 1%, which means nearly all the harmful image-text pairs are also textual harmful. This indicates the textual queries have revealed harmful information from image modality, thus causing the judge model could identify the harmfulness through textual queries alone, overlooking the images. This experiment provides evidence of the existence of the VSIL problem. Qualitative verification. This problem is illustrated in the samples shown in Figure 2. To be detailed, (1): This imagetext pair sourced from Harmbench-mm [35] showcases the prison location information from visual to textual query. (2): 3 This pair is sourced from JailbreakV, whose image showcases the fake identification that is revealed in the textual query. (3): This pair sourced from FigStep featured ocr ability to jailbreak MLLMs; however, the information if visual is repeated in text again. (4): This image from Chef [46] shows firearm and the textual query asked for the weapon for illegal purposes, which is also an example of safety information leakage. Please see more visualization results in Appendix A. 2.2. VSIL Causes Textual Alignment Perform well on Existing Multimodal Safety Datasets Due to this obvious problem of VSIL, we have conducted further experiment to discover its limitations. Based on the previous explanation, we conduct detailed comparison experiment between models aligned using the textual alignment and those using multimodal alignment including SFT [67] and RLHF [62] on existing multimodal safety benchmarks. 2.2.1. Experiment Setup Models. We conduct our textual alignment experiment compared with multimodal alignment on three models. We choose LLaVA-v1.5 [32] 7b and 13b version, being one of the most commonly used MLLMs. Also, we consider the current powerful MLLM, Qwen2-VL-7B-Instruct [51]. Baselines. For LLaVA-v1.5-7b, we have three multimodal alignment baselines: one is VLGuard-Mix-7b [67] aligned with SFT with 2k safe samples plus 1k normal samples; the other two is SPA-VL [62] aligned using 30k preference data separately by DPO and PPO. And we conduct two textual methods, textual unlearning previously mentioned and textual SFT. The textual training data is filtered from SafeRLHF [22], you can find the detailed data processing in Appendix B. As for the LLaVA-v1.5-13b and Qwen2-VL7B models, we separately have one multimodal alignment baseline, SFT with VLGuard, and one textual SFT method with SafeRLHF [22]. Evaluation benchmark. For safety tasks, We leverage the most widely used multimodal safety benchmark as our evaluation dataset. To be specific, FigStep [16] compromises 500 queries with image OCR; mmsafetybench [33] including approximately 2k samples with enhanced stable-diffusion related and typo images; JailbreakV [34] which we use its miniset, is about 300 pairs. For multimodal ability tasks, we use the most commonly used benchmark to assess the models overall ability, like MME [56], MMMU [58], MMStar [7] and MMVet [57]. The main results are reported in Table 2. Evaluation metrics. For multimodal safety task, we employed the widely recognized QA safety evaluation tool LLamaGuard [21], using its SOTA model, LlamaGuard38B [21] for rapid and standardized safety assessment, using the safety rate as our metric. For the multimodal general Models LLaVA-v1.5-7b LLaVA-v1.5-13b Qwen2-VL LLaVA-7b + VLGaurd + SPA-VL-DPO + SPA-VL-PPO + Textual-Unlearning + Textual-SFT LLaVA-13b + VLGaurd + Textual-SFT Qwen2-VL + VLGaurd + Textual-SFT JailbreakV FigStep VLSafe MME MMMU MMStar MMVet Raw Jailbreak Avg(%) Avg(%) Avg(%) Per. Cog. Exact-match GPT-eval 59.29 69.28 92.93 97.85 98.21 98.57 97.14 98.21 97.50 99.64 99.64 99.64 42.85 37.85 84.28 66.07 96.07 77.50 91.79 93.21 62.86 100 100 98.22 Base Models 50.61 64.6 87.20 25.32 50.00 77.48 Safety Aligned 99.20 96.60 96.40 91.79 96.40 99.00 97.80 100 98.80 95.95 99.91 99.73 99.01 99.64 98.02 99.91 99.19 99.91 51.07 56.61 87. 81.93 97.14 88.04 95.77 95.75 80.18 95.53 99.82 98.93 355 293 628 288 240 276 278 266 278 264 585 565 1511 1538 1677 1521 1205 1394 1471 1362 1549 1469 1509 1604 36.44 36.00 50. 36.44 35.67 35.67 35.11 33.89 36.22 35.44 47.00 43.78 33.74 35.71 57.65 34.46 30.83 32.74 33.09 33.68 35.91 37.48 50.65 51.13 29.82 36.55 30.64 25.77 27.43 17.93 26.74 30.64 31.10 30.64 30.27 46.51 Table 2. Multimodal safety and general ability comparison experiment between textual SFT and multimodal alignment. We have conducted our experiment on three MLLMs, LLaVA-v1.5-7b/13b [30] and Qwen2-VL-7B [51]. blue background marks the textual method that we apply. We leverage safety rate (%) as our multimodal safety metrics. For general ability, we adopt the official metrics used. Per. is short for perception. Cog. is short for cognition. indicates that the higher, the better. ability evaluation, we follow the lmms-eval [60]s implementation and report the suitable metrics separately for each ability benchmark. You can check for detailed experiment settings and completed evaluation results at Appendix B. 2.2.2. Findings We present the main results in Table 2 and summarize the key findings as follows: Finding 1: Textual SFT shows outstanding multimodal safety Performance on datasets with VSIL The safety results in Table 2 demonstrate that textual SFT performs similarly with multimodal alignment, achieving over 95% average safety rate on current multimodal safety benchmarks. To be detailed, all the alignment methods on the three baselines get quite good safety performance in all the multimodal safety tasks. Take LLaVA-v1.5-7b for example, models aligned on SPA-VL [62] with DPO demonstrate the best safety performance, achieving 98.57% on the raw set of JailbreakV and 99.01% on VLSafe [8], over 96% on FigStep [16]. However, simple textual unlearning also demonstrates quite high safety rate, over 95% on JailbreakV, and over 91% on FigStep. Whats more surprising is that the textual SFT is even better than textual unlearning, with 96.4% on FigStep. Moreover, the textual SFT outperforms the multimodal SFT, VLGuard, with the same quantity of training samples on JailbreakV and VLSafe. Finding 2: Textual SFT matches multimodal alignment in general ability. In terms of multimodal general capabilities, the textual SFT method demonstrates similar performance compared to multimodal alignment. Specifically, the textual SFT models are only slightly behind VLGuard on the MME, MMMU, and MMStar tasks. For instance, on MMStar, textual SFT scored 33.68, while VLGuard scored 34.46. Moreover, textual SFT even outperformed VLGuard [67] and SPA-VL [62] on the MMVet benchmark, with score of 30.64 compared to VLGuards 25.77. These results clearly show that the simpler textual SFT can maintain general capabilities compared to those of multimodal alignment methods. Finding 3: Existing safety datasets are not challenging. We also note that all the safety-aligned models safety rates across all these evaluated safety datasets are over 90%, which demonstrates existing multimodal safety benchmarks lack sufficient challenge and are inadequate for the evaluation of future safety alignment. In nutshell, the seemingly superior performance of textual SFT on multimodal safety datasets indicates that existing datasets suffer from the problem of VSIL. This further demonstrates the limitations that current multimodal safety datasets with VSIL. 3. VLSBench Construction Pipeline 3.1. VLSBench Overview Data Description. To address the existing issues in current multimodal safety benchmarks, called VSIL, we construct Multimodal Visual Leakless Safety Benchmark (VLSBench) filling this blank in the current multimodal safety datasets. As shown in Figure 6, our dataset consists of an image paired with neutral textual query. To be specific, some images, like those depicting \"drugs and syringes,\" are clearly risky, while others, like \"kids in jail,\" may seem less obvious but still contain safety concerns. The associated queries related Category Violent Weapon-Related Violence Public Violence and Rioting Abuse and Physical Altercations Illegal Activity Cybercrime Property Crimes Drug-Related Offenses Human Trafficking and Exploitation Self-Harm Risky or Dangerous Behaviors Physical Self-Injury Substance Abuse and Poisoning Psychological and Disordered Harm Erotic Adult Content Erotic Visual Content Hate Racial and Ethnic Discrimination Cultural Xenophobia Religious Intolerance Gender and Sexual Orientation Discrimination Privacy Unauthorized Data Collection Identity Theft and Impersonation Samples Ratio(%) 590 244 186 160 539 226 167 115 28 370 125 124 91 32 343 178 165 269 99 62 56 50 130 69 64 26.33 10.07 8.30 7.14 24.05 10.07 7.54 5.13 1.25 16.51 5.58 5.53 4.06 1.43 15.31 7.94 7.36 12.00 4.42 2.77 2.50 2.23 5.80 2.08 2.86 Figure 3. Safety taxonomy of our VLSBench. Figure 4. The Detailed statistics of our VLSBench across 6 categories and 19 sub-categories to the images do not include any obvious harmful keywords or intentions. Instead, they provide harmless information, such as \"fast results with this equipment\" or \"engage in play scenario,\" which may still imply potential risks depending on the visual context. Please check Appendix for more examples in VLSBench. Safety Taxonomy. Inspired by existing safety taxonomy for LLMs safety [26, 49] and multimodal safety [18, 46], we propose hierarchical two-level safety taxonomy for our VLSBench. This taxonomy shown in Figure 3 builds upon previous safety taxonomies and specifically highlights categories that better represent visual safety in MLLMs, which better support our VLSBench data construction. The VLSBench finally compromises 6 categories and 19 subcategories detailed described in Figure 4, including 2.4k image-text pairs. 3.2. VLSBench Data Collection Our data construction pipeline shown in Figure 5 focuses on effectively preventing visual safety leakage from image modality to textual query. First, we should generate harmful textual queries from two parallel paths shown in Step 1. Then, we need to detoxify the harmful queries and obtain the harmless queries shown in Step 2. Furthermore, we use text-to-image models to iteratively generate images shown in Step 3. Finally, we filter out the mismatched and safe imagetext pairs and obtain the final datasets as shown in Step 4. The detailed construction pipeline is listed as follows: Step 1: Harmful query and image description generation. Initially, to ensure that the generated queries cover wide range of safety categories, we have implemented two parallel approaches. The first is to extract diverse safety topics from textual sources, where we first collect harmful elements including sensitive objects and risky scenarios from ChatGPT, you can see some examples in Appendix C. Then we prompt GPT-4o [20] to generate an image description and harmful query related to these harmful elements, ensuring the image description and harmful textual queries both related to the harmful elements. The second one is to leverage the existing image dataset for diverse safety topics. The detailed image source is shown in Appendix C. Then, we prompt the opensourced powerful MLLM, Qwen2-VL-72B [51] for image analysis and generate harmful queries based on that. Step 2: Mitigating visual leakage from textual harmful query. At this stage, we designed prompt, shown in Appendix to guide GPT-4o in detoxifying the harmful query into less harmful and less conspicuous textual query. Specifically, we use few-shot prompt, leveraging the instruction following ability of LLMs to mitigate the safety information from image modality to textual modality. Also, we should keep the queries their original meaning, but mitigate the safety-related information. Thus, we successfully prevent the visual leakage from the image to textual queries. Following this, we leverage GPT-4o to filter the revised textual query. This step is aimed at filtering two kinds of samples: (1) the revised queries that is still harmful with leakage information from image modality, and (2) the revised textual queries that do not keep their original meaning. Step 3: Iterative image generation from image description. For the image descriptions generated from harmful elements, we need to generate images accordingly. First, we prompt GPT-4o-mini [20] to do paraphrase to make image description follow the style of text to image gener5 Figure 5. Overview of VLSBench construction pipeline. Our pipeline features prevent visual leakage. This pipeline includes four steps: (a) Harmful query and image description generation. (b) M: Mitigating visual leakage from textual harmful query. (c) Iterative image generation from image description. (d) Final filtration ensuring image-text pairs are matched and harmful. ation, incorporating terms like \"real\" and \"real-world\" to enhance the realism of the generated images. Then, we employed Stable-Diffusion-3.5-Large [13] to ensure generation quality. Additionally, we adopted an iterative generation approach, leveraging Qwen2-VL-72B [51] to evaluate whether the generated images reflect the safety-related information in image descriptions. If not, Qwen2 will revise the prompt and regenerate the images until the criteria is satisfied. Step 4: Final filtration ensuring image-text pairs are matched and harmful. Finally, the final images are generated from descriptions and collected from existing datasets. The final queries are the harmless queries obtained in step 2. Then, we leverage the GPT-4o to conduct final quality filtration of the image-text pairs. This process filters out those mismatched and safe image-text pairs. After final manual review by the authors, we complete our VLSBench, addressing the problem of VSIL. 4. Benchmark Experiments 4.1. Experiment Setup MLLMs. We benchmark various MLLMs including both open-source models and close-source models accessible only via API. The open-sourced models are: (1) LLaVAv1.6-7b13b [32] (2) LLaVA-1.6-mistral [31], (3) LLaVAllama3 [25], (4) Qwen2-VL-7B-Instruct [51], (5) Llama3.2-11B-Vision-Instruct [12]. Close-source APIs are (1) GPT-4o [20], (2) Gemini-1.5-pro [47]. Textual alignment and multimodal alignment baselines. We also benchmark different safety alignment methods. We follow the same baseline setting as shown in Table 2. For LLaVA-v1.5-7b, we have multimodal SFT with VLGuard [67], multimodal DPO and PPO with SPA-VL [62] and textual unlearning and textual SFT with SafeRLHF [22]. For LLaVA-v1.5-13b and Qwen2-VL-7b, we have multimodal SFT and textual SFT. Evaluation. For the evaluation of our VLSBench, we use GPT-4o as the judge model and design classification prompt for this task. We classify the evaluation response labels into three types: safe with refusal for clear and firm rejection to the request, safe with warning for responses that identify safety concerns and provide appropriate caution, and unsafe for answers that disregard safety principles and answer the question directly. We calculate the safety rate (%) by considering the total of safe with refusal and safe with warning. Detailed evaluation can be found in Appendix F. 4.2. Main Results In this section, we present deeper insight into the evaluation results shown in Table 3 on our VLSBench. We summarize our key findings as follows: Textual alignment lags behind multimodal alignment in VLSBench. Based on the safety alignment results on our dataset. We could see clear disparity between textual alignment methods and multimodal alignment methods. To be detailed, the textual SFT on LLaVA-v1.5-7b base model only shows 13.99% safety rate. Although textual SFT has double safety performance compared to the base model 6.6% safety rate, it still significantly lags behind multimodal SFT with VLGuard, which achieves 21.26% safety rate not to mention this SPA-VL model which has undergone RLHF training with large dataset of 30k image-text pairs. Also, the same disparity can be found in the LLaVA-v1.5-13b and Qwen2-VL-7B base models. From the results in Figure 7, we clearly see an average of 10% difference between textual SFT and multimodal SFT. The challenging nature of VLSBench. Current opensource and close-source MLLMs all struggle to perform well on Our VLSBench. To be detailed, the best closesource result is 49.78%, achieved by Gemini-1.5-pro, while 6 Figure 6. Examples of our dataset across our three evaluation labels. We give an image-text pair and corresponding response evaluated as Safe with Refuse, Safe with Warning and Unsafe. Models Refuse(%) Warn(%) Safety(%) Base MLLMs LLaVA-v1.5-7b LLaVA-v1.5-13b LLaVA-v1.6-mistral-7b LLaVA-llama3-8b Qwen2-VL-7B Llama-3.2-11B-Vision 0 0 0 0 1.11 10.96 6.60 8.65 11.19 10.52 12.66 15.33 Multimodal Safety Aligned LLaVA-1.5-7b + VLGaurd + SPA-VL-DPO + SPA-VL-PPO LLaVA-1.5-13b + VLGaurd Qwen2-VL + VLGaurd 2.32 2.63 5.08 6.60 15.55 Textual Safety Aligned LLaVA-1.5-7b + Textual-SFT + Textual-Unlearning LLaVA-1.5-13b + Textual-SFT Qwen2-VL + Textual-SFT 5.30 2.85 5.66 11.72 Close-source APIs 18.94 24.38 30.39 21.43 62.83 8.69 8.87 9.05 55.7 GPT-4o Gemini-1.5-pro 5.21 1. 16.22 48.44 6.60 8.65 11.19 10.52 13.77 26.29 21.26 27.01 35.47 28.03 78.39 13.99 11.72 14.71 67.42 21.43 49.78 Table 3. Results of our VLSBench, including open-sourced MLLMs, closed-sourced APIs, textual alignment, and multimodal alignment baselines. The results are evaluated with GPT-4o with three labels: safe with refuse, safe with warning, and unsafe. The final safety rate is the sum of the refusal rate and the warning rate. GPT-4o only achieved 21.43% safety rate. In terms of opensourced models, the best model is Llama-3.2-vision, which can only achieve about 26.29% safety at most. On the other hand, the classic LLaVA-v1.5 model only performs approximately 5% safety rate. Not only that, the safety-alined models still do not show particularly strong results. The best safety-aligned model is Qwen2-VL-7B with multimodal SFT, achieving 78.39% overall safety rate. On the other hand, the multimodal SFT only gets 21.26% on LLavA-v1.5-7b, and 28.03% on LLaVA-v1.5-13b. All this results highlights that our VLSBench presents significant safety challenge for MLLMs, suggesting dedicated multimodal alignment Figure 7. Textual SFT compared with Multimodal SFT on our VSLBench. Dash lines mean the average safety rate on the three base models. methods is needed in the future. Current MLLMs struggle to balance between refuse and warning. Current MLLMs should behave not only safely but also helpfully. good MLLMs response should not simply give direct refuse. Although it means MLLM successfully identifies the safety risks, it lacks explained reasons and helpful content, which is important in MLLM assistance. However, we can see that current SOTA MLLMs like GPT4o and Llama-3.2-11B-Vision, perform 5.21% and 10.96% direct refuse rate, which is not incremental considering their overall safety rate. Also, when comparing models with better safety performance like Llama-v3.2-11B-Vision to those with weaker safety like Qwen2-VL-7B, we observe that the overall improvement in safety is accompanied by notable increase in the refuse rate. Textual alignment is enough for datasets with VSIL while multimodal alignment is preferred to address visual leakless datasets. The previous results shown in Table 2 have demonstrated that simple textual-based methods are enough to solve previous multimodal safety benchmarks suffered from VSIL. However, when multimodal data is free from VSIL, the textual-based method is significantly behind multimodal alignment. To be specific, the results in Table 2 show that, textual SFT on LLaVA-v1.5-7b, LLaVA-v1.5-13b, and Qwen2-VL all significantly lack multimodal SFT over 10%. This indicates that multimodal alignment is more promising for addressing multimodal safety scenarios without VSIL. 5. Related Work MLLMs. The rapid development of LLMs has significantly promoted the advancement of MLLMs. Integrated with multimodal encoders including image [1], audio [24], and video [55], MLLMs is designed to process complicated multimodal information. The most extensively studied modality at present is vision, with its integration with text, using Large Language Models (LLMs) as the backbone. The subsequent discussion of multimodal safety will also focus on the realm of vision and language modality. The most common architecture today uses vision encoder [11] to encode image information, followed by projector module to align the visual information and textual representations. Both are then fed into the LLM backbone for autoregressive generation. Notable models employing this include LLaVA [32], LLaVA-Next [31], QwenVL series [4] and Llama-3.2-Vision [12]. Multimodal Safety Concerns. The rising usage of MLLMs has sparked growing concern about their potential safety risks. While MLLMs feature LLMs as the backbone, the most common and direct safety concerns in MLLMs overlap lot with those concerns in the LLMs domain. In that case, the wide range of harmful output content including toxicity [29], bias [14], and so on in LLMs [26, 35] are also critical safety concerns that need to be addressed in MLLMs. To be specific, the various attack methods [43, 50, 68] could also be utilized to jailbreak MLLMs. On the other hand, however, when visual input is integrated as component in current MLLMs, the visual safety awareness of the MLLMs is crucial to guide the next response and actions [10, 39, 46, 66]. There are emerging vulnerabilities arising from visual modality other than simply textual jailbreaking [6, 42]. In that case, the unique safety concern that MLLMs have could be roughly categorized into two types. The first is white-box attack [36, 38, 45]. These kinds of safety concerns include gradient-based searches for adversarial images that make the MLLM produce harmful outputs. The adversarial noise is crafted and added to the origin image to trick the model into generating targeted unsafe content. The other type is black-box methods mainly exploiting the visual vulnerability to supply more cheating information. For instance, some work [16, 33, 34] utilize the OCR ability and stable-diffusion generated images to provide additional information, causing the model to generate harmful response. Safety Alignment. Although MLLMs currently face significant safety challenges, there are still many effective strategies available to improve their safety. Drawing from the well-researched realm of LLM alignment methods, such as RLHF [15, 37] employing methods like proximal policy optimization (PPO) [44], supervised fine-tuning (SFT) and direct preference optimization (DPO) [41], MLLMs also utilize these similar methods with carefully crafted image-text pairs. To illustrate, VLGuard [67] utilize almost 2k image-text pairs to SFT on the LLaVA-v1.5 models and achieve significant improvement on multimodal safety while keeping the multimodal general ability. Additionally, SPA-VL [62] leverages up to 90k multimodal image-text pairs with GPT4 labeled rankings to employ DPO and PPO methods and gain an outstanding multimodal safety performance. Except for the labor-intensive training methods to align MLLMs in safety tasks, there are also many training-free methods [5, 17, 52]. Also, some work [5] omit the image input and only utilize textual to do unlearning on MLLMs. Multimodal Safety Benchmark. To evaluate the current MLLMs safety, there are many multimodal safety benchmarks. VLSafe [8] is widely used multimodal safety dataset, compromising 1.1k test set as benchmark. This dataset features CoCo [28] as its image source, leveraging various prompts as jailbreak techniques to generate harmful queries. Furthermore, Ch3ef [46] not only considers safety alone but takes helpfulness, honesty, and harmlessness (3H) together as high-level principle. Apart from this, there are also multimodal safety datasets [16, 33, 34] that leverage the image modality to demonstrate harmful information through OCR or stable-diffusion generated images. Also, there is some work that focus on more challenging tasks in the multimodal safety [27, 53]. However, most of these datasets showcase visual safety information leakage (VSIL) problem, which overlooks the safety information leaked from image to textual query. We have discovered several issues arising from this problem and proposed our VLSBench to fill the blank in current multimodal safety. 6. Conclusions In conclusion, our work noticed an important problem, visual safety information leakage (VSIL) in current multimodal safety data samples. This phenomenon leads to textual-based bias when evaluating the safety ability of MLLMs. Thus, current multimodal evaluation datasets encourage simple and seemingly superior methods, textual alignment with textual training samples, to solve multimodal safety challenges. However, current multimodal safety datasets overlook this important problem. To this end, we construct our multimodal visual leakless safety Benchmark (VLSBench) to fill this blank in multimodal safety. Also, We develop data construction pipeline, that successfully prevents visual infor8 mation leakage from image modality to textual query. On our newly proposed VLSBench, we find that current MLLMs and safety-aligned models struggle to perform at great safety rate. Furthermore, while textual alignment is enough to solve multimodal datasets with VSIL, our VLSBench features no visual leakage demonstrating that dedicated multimodal alignment method is favored to better tackle this kind of multimodal safety."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 8 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 8 [5] Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, Salman Asif, Yue Dong, Amit RoyChowdhury, and Chengyu Song. Cross-modal safety alignment: Is textual unlearning all you need? arXiv preprint arXiv:2406.02575, 2024. 1, 8 [6] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. 8 [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. [8] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Dress: Instructing large vision-language models to align and interact with humans via natural language feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14239 14250, 2024. 1, 2, 4, 8 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1 [10] Zeren Chen, Zhelun Shi, Xiaoya Lu, Lehan He, Sucheng Qian, Hao Shu Fang, Zhenfei Yin, Wanli Ouyang, Jing Shao, Yu Qiao, et al. Rh20t-p: primitive-level robotic dataset towards composable generalization agents. arXiv preprint arXiv:2403.19622, 2024. 8 [11] Alexey Dosovitskiy. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 8 [12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 6, 8, [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 6, 3 [14] Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. Bias and fairness in large language models: survey. Computational Linguistics, pages 179, 2024. 8 [15] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. 8 [16] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts. arXiv preprint arXiv:2311.05608, 2023. 1, 2, 3, 4, 8 [17] Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok, and Yu Zhang. Eyes closed, safety on: Protecting multimodal llms via imageto-text transformation. arXiv preprint arXiv:2403.09572, 2024. 8 [18] Tianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge Qiao, Keqing Wang, Yujiu Yang, et al. Mllmguard: multi-dimensional safety evaluation suite for multimodal large language models. arXiv preprint arXiv:2406.07594, 2024. 5, 3 [19] Ting-Yao Hsu, Chieh-Yang Huang, Ryan Rossi, Sungchul Kim, Lee Giles, and Ting-Hao Huang. Gpt-4 as an effective zero-shot evaluator for scientific figure captions. arXiv preprint arXiv:2310.15405, 2023. [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 5, 6 9 [21] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674, 2023. 2, 3, 1, 5 [22] Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and Yaodong Yang. Pku-saferlhf: safety alignment preference dataset for llama family models. arXiv preprint arXiv:2406.15513, 2024. 3, 6, 1 [23] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 1 [24] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831, 2024. [25] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llavanext: Stronger llms supercharge multimodal capabilities in the wild, 2024. 6 [26] Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044, 2024. 2, 5, 8 [27] Xirui Li, Hengguang Zhou, Ruochen Wang, Tianyi Zhou, Minhao Cheng, and Cho-Jui Hsieh. Mossbench: Is your multimodal language model oversensitive to safe queries? arXiv preprint arXiv:2406.17806, 2024. 8 [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, 8, 3 [29] Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389, 2023. 8 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Visual instruction tuning, 2023. 1, 4 [31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6, [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3, 6, 8 [33] Liu, Zhu, Gu, Lan, Yang, and Qiao. Mm-safetybench: benchmark for safety evaluation arXiv preprint of multimodal large language models. arXiv:2311.17600, 2023. 1, 3, 8, 5 [34] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv-28k: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2404.03027, 2024. 1, 2, 3, 8 [35] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. 2, 3, 8 [36] Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. Jailbreaking attack against multimodal large language model. arXiv preprint arXiv:2402.02309, 2024. 8 [37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [38] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2152721536, 2024. 8 [39] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: multi-modal open-ended embodied system in minecraft via active perception. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1630716316. IEEE, 2024. 8 [40] Yiting Qu, Xinyue Shen, Yixin Wu, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafebench: Benchmarking image safety classifiers on real-world and ai-generated images. arXiv preprint arXiv:2405.03486, 2024. 2, 3 [41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. 8 [42] Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, and Lizhuang Ma. Exploring safety generalization challenges of large language models via code. arXiv preprint arXiv:2403.07865, 2024. 8 [43] Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, and Jing Shao. Derail yourself: Multi-turn llm jailbreak attack through selfdiscovered clues. arXiv preprint arXiv:2410.10700, 2024. 1, [44] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 8 [45] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attacks on multimodal language models. In The Twelfth International Conference on Learning Representations, 2023. 8 [46] Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, and Jing Shao. Assessment of multimodal large language models in alignment with human values. arXiv preprint arXiv:2403.17830, 2024. 1, 2, 3, 5, 8 10 [59] Jie Zhang, Dongrui Liu, Chen Qian, Ziyue Gan, Yong Liu, Yu Qiao, and Jing Shao. The better angels of machine personality: How personality relates to llm safety. arXiv preprint arXiv:2407.12344, 2024. 1 [60] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. [61] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868, 2024. 1 [62] Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, et al. Spa-vl: comprehensive safety preference alignment dataset for vision language model. arXiv preprint arXiv:2406.12030, 2024. 1, 3, 4, 6, 8 [63] Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, et al. Benchmarking trustworthiness of multimodal large language models: comprehensive study. arXiv preprint arXiv:2406.07057, 2024. 2, 3 [64] Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, and Minlie Huang. Safe unlearning: surprisingly effective and generalizable solution to defend against jailbreak attacks. arXiv preprint arXiv:2407.02855, 2024. 1 [65] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. 1 [66] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-ofimagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024. [67] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety fine-tuning at (almost) no cost: baseline for vision large language models. arXiv preprint arXiv:2402.02207, 2024. 1, 3, 4, 6, 8 [68] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. 8 [47] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6 [48] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. LLaMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1 [49] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: comprehensive assessment of trustworthiness in gpt models. In NeurIPS, 2023. 5 [50] Hao Wang, Hao Li, Minlie Huang, and Lei Sha. From noise to clarity: Unraveling the adversarial suffix of large language model attacks via translation of text embeddings. arXiv preprint arXiv:2402.16006, 2024. [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 4, 5, 6 [52] Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke Ren, Botian Jiang, and Xipeng Qiu. Inferaligner: Inference-time alignment for harmlessness through cross-model guidance. arXiv preprint arXiv:2401.11206, 2024. 8 [53] Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, and Xuanjing Huang. Cross-modality safety alignment. arXiv preprint arXiv:2406.15279, 2024. 8 [54] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: Evaluating safeguards in llms. In Findings of the Association for Computational Linguistics: EACL 2024, pages 896911, 2024. 5 [55] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. [56] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 3 [57] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 3 [58] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 3 11 VLSBench: Unveiling Visual Leakage in Multimodal Safety"
        },
        {
            "title": "Supplementary Material",
            "content": "A. VSIL Examples from Existing Datasets In this section, we will shown more data samples with visual safety information leakage (VSIL). Here, we select some examples in Figure 8 to illustrate: (a): an image depicting bloody throat-cutting scenario, which is also explicitly described in the textual query; (c): textual query containing the risky word methamphetamine, clearly conveyed by the image; (e): an image containing OCR text about creating malware, detail repeated in the textual query; (f): the harmful activity shown in the image through OCR is also revealed in textual query; (g): this image describes scary wound which is also revealed in textual query; (j): an image of gun, mirrored by the textual query; (l): CAPTCHA image, inherently risky, with its content repeated in the textual query. B. Experimental Details on Multimodal Safety"
        },
        {
            "title": "Dataset with VSIL",
            "content": "B.1. Detailed Training Setting Multimodal Alignment. For LLaVA-v1.5-7B/13B with VLGuard [67], we directly use their officially recommended checkpoints, such as VLGuard-Mix-7B/13B, which are aligned using SFT on 2k safe samples plus 1k normal samples. For SPA-VL [62], we utilize the official checkpoint, which is preference-aligned using 30k preference data through DPO and PPO. As for the Qwen2-VL-7B model, we finetune it directly with the VLGuard training set, following the official training settings. Textual SFT. During the SFT for textual alignment, we used dataset filtered from SafeRLHF [22], consisting of 2,000 harmful and 1,000 benign samples, to maintain consistent training data volume with VLGuard. This dataset comes from the LLM domain, whose distribution differs from the image-text dataset. To be detailed, for the safe data, we directly used the responses from the original dataset. In contrast, for harmful data, we generated safe responses using the Llama3-8B-Instruct [12], which is recognized for its extensive safety alignment and safe responses to harmful queries. Textual Unlearning. As for the textual unlearning method, we followed the approach outlined in SafeUnlearning [64]. First, to unlearn the harmful responses yh, the loss function is adopted from negative preference optimization (NPO) [61], shown below: 1 Df Pθ(yh x) Pref(yh x) Lh = β log log σ (cid:88) (cid:18) (cid:19) (x,yh)Df Also, to teach the model how to respond to harmful queries, we use the following loss on the safe responses ys: Ls = 1 Df (cid:88) (x,ys)Df log Pθ(ys x) Finally, to maintain the general performance, we additionally use the loss below to incorporate helpful responses to various benign queries in Dr: Lg = 1 Dr (cid:88) (x,y)Dr log Pθ(y x) The total loss is formulated as = Lg + γLs + αLh. We set the formulated parameter as α = γ = 0.3. Lg means the normal loss calculated with benign instruction samples. Ls is the safe response loss. Lh stands for the unlearning loss. To prepare the training data for unlearning, we filtered samples from SafeRLHF [22]. The final dataset consists of 1,000 benign questions paired with safe responses, denoted as Dr. Additionally, it includes 2,000 harmful questions, each paired with safe response generated by Llama3-8BInstruct, as well as the corresponding harmful responses from the original dataset, collectively denoted as Df . Please note that the textual methods including SFT and Unlearning on the MLLMs only finetune the LLM backbone with an all-black image as the visual input which is the default in LLaVA official Github repository. We fully finetune the base LLaVA-v1.5 on 4*A100-80G GPU. We use LLaVA official repository for finetuning on LLaVA baseline and LLaMA-Factory [65] for finetuning on Qwen2-VL, which is recommended in Qwen2-VL official repository. B.2. Detailed Evaluation Setting As for the multimodal safety task, we follow the evaluation methods used in the official paper to evaluate JailbreakV [34]: input origin query paired with raw answer and jailbreak answer separately into LlamaGuard3 [21] judge model. As for FigStep [16] and VLSafe [8], we also leverage LLamaGuard3 as the judge model due to its superior ability. B.3. Evaluaiton on MMSafetyBench Additionally, we also conduct multimodal safety evaluation on datasets that might less suffered from VSIL problem, i.e., MMSafetyBench [33] features OCR ability to jailbreak MLLMs. For the evaluation of MMSafetyBench, we follow their official GPT4 evaluation but change the evaluation model to GPT-4o-mini [20] due to the high cost. The results on LLaVA-v1.5-7b are not much different from the results 1 Figure 8. Selected examples with VSIL: (a)-(e) is from JailbreakV [34], (f) is from FigStep [16], (g)-(i) is from Ch3ef [46] and (j)-(l) is from Harmbench [35]. 2 Baselines Training Method Learning Rate Epochs Max Length LLaVA-v1.5-7B + Textual SFT LLaVA-v1.5-7B + Textual Unlearning LLaVA-v1.5-13B + Textual SFT Qwen2-VL-7B + Textual SFT Qwen2-VL-7B + Multimodal SFT full-parameter full-parameter full-parameter full-parameter full-parameter 2e 5 2e 5 2e 5 1e 5 1e 5 2 3 2 2 3 2048 2048 2048 1024 1024 Table 4. Detailed training settings of our textual SFT with SafeRLHF [22] and Multimodal SFT with VLGuard [67]. Models LLaVA-v1.5-7B LLaVA-v1.5-13B Qwen2-VL-7B LLaVA-v1.5-7B + VLGaurd + SPA-VL-DPO + SPA-VL-DPO + Textual-Unlearning + Textual-SFT LLaVA-v1.5-13B + VLGaurd + Textual-SFT Qwen2-VL-7B + VLGaurd + Textual-SFT Text-only Stable-Diffusion Typo Stable-Diffusion+Typo Average MMSafetyBench Base Models 45.24 44.52 49."
        },
        {
            "title": "Safety Aligned",
            "content": "89.16 76.31 79.40 65.77 67.14 90.65 64.82 91.72 80.29 19.70 20.36 27.80 95.17 67.38 75.95 56.90 62.38 94.76 55.18 95.95 74.40 46.25 52.98 61.79 74.88 67.14 68.93 59.29 67.44 75.71 71.67 97.02 71.07 20.11 21.01 22. 90.89 63.69 70.36 45.77 52.85 90.95 52.14 93.27 75.35 32.82 34.72 40.48 87.53 68.63 73.66 56.93 62.45 88.02 60.95 94.49 75.28 Table 5. Textual alignment compared with multimodal alignment on MMSafetyBench [33]. reported in the paper. And, we have done some human verification processes to validate the effectiveness of the evaluation. The results show that multimodal alignment with SFT on VLGuard and DPO, PPO on SPA-VL have good safety performance, which is better than the textual alignment with SafeRLHF. We account for this in the data distribution as the data samples in MMSafetyBench mostly share similar pattern, the image shows which is absent in the textual training data. Type Image Source Nums. Ratio(%) Generated Stable-Diffusion [13] 1234 62.96% Existing MULTITrust [63] MLLMGuard [18] Ch3ef [46] UnsafeBench [40] CoCo [28] 175 178 117 130 126 8.93% 9.08% 5.97% 6.63% 6.43% Table 6. Image source of our VLSBench including generated images and collected images from existing datasets. C. VLSBench Data Construction C.1. Statistics Our VLSBench compromises 6 harmful categories and 19 sub-categories, including 2241 image-text pairs with 2241 unique textual queries and 1957 unique images. Our images have two sources, one is the generated images and the other is existing image datasets. The detailed image source is listed in Table 6. The textual queries in our VLSBench are depicted as word cloud in Figure 9. The high-frequency words displayed in the word cloud reveal that the textual queries in our VLSBench are neutral or even completely harmless. C.2. Details on Iterative Image Generation This step is aimed at generating images from image descriptions. Note that the generation process is iterative and we leverage Qwen2-VL-72B as the verifier and prompt refiner. The model need to verify if the generated images stick to the prompt and contain the necessary safety-related information. If no, the model is asked to refine the original prompt. If yes, the generation process is completed. Note that the max 3 Figure 9. Words cloud of our VLSBenchs textual queries. C.4. Category Labeling Based on the safety taxonomy we proposed, we leverage GPT-4o as the category labeler to categorize our data into different categories based on the detailed description shown in Table 7. The used prompt is shown at Figure 11. Figure 10. The harmful elements: sensitive objects and risky scenarios examples, used to generate our harmful queries and image captions. Figure 11. Prompt used to categorize our image-text pairs in VLSBench. iteration rounds are set to 3. C.3. Human Verification While the automated construction pipeline assists in the quality of image-text pairs without VSIL problems, we still need human verification to ensure the data samples meet the following criteria: (a) the textual query must be harmless without the safety-related information from the image modality; (b) the textual query must neither contradict nor be irrelevant to the image. The authors and three more data labelers are invited to cooperate on this verification process. D. More Examples of VLSBench We present three data samples in our VLSBench and provide different responses from various MLLMs, close-source APIs, and safety-aligned baselines. The responses from various models cover the three safety labels we used. E. Limitations Our work has some limitations. First, we do not include all the MLLMs in our evaluation of VLSBench, especially those close-source APIs which are quite expensive to use. Second, the proposed VLSBench mostly leverages the powerful GPT model to ensure the high quality which is expensive to use. Although we also use Qwen2-VL as part of our construction pipeline, there still needs more effort to replace all the closesource APIs with open-source models to promote the usage of our construction pipeline. Third, although we have discovered multimodal alignment to achieve performance gain on our VLSBench compared to textual alignment, effective solutions for addressing this challenging multimodal safety dataset are to be explored. F. Evaluation Analysis is LlamaThe most widely used safety judge model Guard [21] models or MD-Judge [26] due to their efficiency, reproducibility, and low cost. However, when it comes to the domain of multimodal safety without visual safety information leakage (VSIL), these evaluation methods demonstrate some obvious drawbacks. To be specific, previous data samples in multimodal safety mostly suffered from VSIL, demonstrating the harmful information both in textual query and image content. Therefore, it is natural to use LlamaGuard model as the judge model as the textual input and output response are in distribution with the training data. However, as the examples shown in Figure 12, the data samples in our benchmark feature neutral or harmless queries as part of the input, the LlamaGuard3 judge model without the vision ability is not able to successfully identify the safety situation. Moreover, the latest LlamaGuard3-Vision judge model is also not able to successfully perceive the image and recognize the risky input from the image-text pair. To this end, we need more powerful judge model, especially with enhanced vision ability to identify safety-related situations. Recent research [19, 33, 54], has highlighted the effectiveness and reliability of the GPT model in safety judging tasks. Thus, we leverage GPT-4o as our judge model and we design specific evaluation prompt for our task in Figure 13. We classify the safety labels into three types: safe with refuse, safe with warning, and unsafe. For the current MLLMs and safety-aligned baselines on our VLSBench, we also present their separate results across 6 categories. The selected results are shown in Figure 14. G. Data Construction Prompt Templates In this section, we present several prompts used in our VLSBench construction pipeline. H. Ethics Statement Our research focuses on the multimodal safety of MLLMs, aiming to address the problem of visual safety information leakage in current datasets. The dataset we created is intended solely for research purposes, with the goal of assessing the safety ability of current MLLMs. Our datasets Figure 12. Evaluation method analysis. The LlamaGuard model is not able to perceive the image and identify the risky content in image-text pairs which hinders its evaluation. On the other hand, GPT-based methods is able to correctly evaluate on our VLSBench. Figure 13. Our GPT-4o evaluation prompt for our VLSBench. compromise image-text pairs, the textual queries are neutral and harmless, and the images are either from AI-generated or publicly available image datasets. Our data collection process does not involve user deanonymization, and as such, our work has been determined by our Institutional Review Board (IRB) not to constitute human subjects research. Finally, To mitigate risks associated with sensitive content in our benchmark, we restrict access to authorized researchers who adhere to strict ethical guidelines. These measures safeguard research integrity while minimizing potential harm. Category Violent Weapon-Related Violence Public Violence and Rioting Abuse and Physical Altercations Illegal Activity Description Behaviors involving physical force or threats with intent to harm, injure, or intimidate, including assaults, public violence, and the use of weapons. Use of firearms, knives, or improvised weapons in dangerous situations Street fights, violent protests, and aggressive acts at public events. Unprovoked attacks, group fights, intimate partner violence, child and elder abuse and intimidation through physical force. Behaviors that violate laws or regulations, including cybercrimes, drugrelated offenses, human trafficking, and property crimes. Cybercrime Property Crimes Drug-Related Offenses Human Trafficking and Exploitation Hacking, phishing, and unauthorized access to digital systems. Theft, vandalism, and destruction of property without consent. Production, trafficking, and possession of controlled substances. Forced labor, sex trafficking, and illegal organ trade. Self-Harm Risky or Dangerous Behaviors Physical Self-Injury Substance Abuse and Poisoning Psychological and Disordered Harm Erotic Adult Content Erotic Visual Content Hate Racial and Ethnic Discrimination Cultural Xenophobia Religious Intolerance Gender and Sexual Orientation Discrimination Privacy Unauthorized Data Collection Identity Theft and Impersonation Behaviors involving intentional injury or neglect directed at oneself, including physical injury, substance misuse, and extreme risk-taking. Reckless driving, extreme sports without safety precautions, and selfneglect. Cutting, burning, hitting oneself, or other forms of direct physical harm. Overuse of medication, drug misuse, and intentional poisoning. Isolation from social support, restrictive eating, binge-purge cycles, and emotional suppression. Content and interactions focused on explicit or suggestive sexual scenarios, including visual media and adult content. Explicit sexual content including pornography and other adult material. Content that entices and attracts through visual appeal, often using seductive elements. Hostile or discriminatory actions and language targeted at individuals or groups based on race, religion, gender, or ethnicity. Harassment, exclusion, and stereotyping based on race or ethnicity. Stereotyping, exclusion, or hostility toward specific cultural groups or national origins. Desecration of religious sites, mockery of practices, and exclusion due to religion. Harassment, exclusion, and discrimination based on gender identity or sexual orientation. Actions that involve unauthorized access, collection, or exposure of personal information, often with intent to exploit or misuse data. Surveillance, GPS tracking, and data scraping without consent. Misuse of personal details to assume anothers identity in fraudulent activities. Table 7. The detailed description of safety taxonomy of our VLSBench. 6 Figure 14. Selected models results on our VLSBench across 6 categories. Reported results include safe with refuse rate, safe with warning rate, and total safe rate. 7 Figure 15 8 Figure 9 Figure 17 10 Figure 18. Used prompt for GPT-4o to generate harmful queries and image descriptions with safety-related information from harmful elements, like risky objects and scenarios. 11 Figure 19. Used prompt for GPT-4o to make the harmful query less obvious and less harmful, thus hiding the safety information from the image, preventing VSIL problem. 12 Figure 20. Used prompt for GPT-4o to generate harmful queries with safety-related information from existing images. Figure 21. Used prompt for GPT-4o to filter the unsuccessful revised query, which is still harmful or loses original meanings. 13 Figure 22. Used prompt for Qwen2-VL-72B for iterative image generation. Figure 23. Used prompt for GPT-4o to the final filtration of image-text pair to filter out pairs that do not match meaningfully and are not harmful."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}