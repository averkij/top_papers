{
    "paper_title": "Efficient Model Development through Fine-tuning Transfer",
    "authors": [
        "Pin-Jie Lin",
        "Rishab Balasubramanian",
        "Fengyuan Liu",
        "Nikhil Kandpal",
        "Tu Vu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domain- or language-specific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector from one source model version, which represents the weight changes from fine-tuning, and apply it to the base model of a different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the target base model, often achieving performance comparable to its fine-tuned counterpart. For example, reusing the fine-tuning updates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on GPQA over the base Llama 3.1 8B without additional training, surpassing Llama 3.1 8B Instruct. In a multilingual model development setting, we show that this approach can significantly increase performance on target-language tasks without retraining, achieving an absolute improvement of 4.7% and 15.5% on Global MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B Instruct. Our controlled experiments reveal that fine-tuning transfer is most effective when the source and target models are linearly connected in the parameter space. Additionally, we demonstrate that fine-tuning transfer offers a stronger and more computationally efficient starting point for further fine-tuning. Finally, we propose an iterative recycling-then-finetuning approach for continuous model development, which improves both efficiency and effectiveness. Our findings suggest that fine-tuning transfer is a viable strategy to reduce training costs while maintaining model performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 1 1 0 2 . 3 0 5 2 : r Efficient Model Development through Fine-tuning Transfer Pin-Jie Lin1 Rishab Balasubramanian1 Fengyuan Liu2 Nikhil Kandpal2 Tu Vu1 1Virginia Tech 2University of Toronto & Vector Institute"
        },
        {
            "title": "Abstract",
            "content": "pinjie@vt.edu rishbb@vt.edu fy.liu@mail.utoronto.ca nkandpa2@cs.toronto.edu tuvu@vt.edu Modern LLMs struggle with efficient updates, as each new pretrained model version requires repeating expensive alignment processes. This challenge also applies to domainor language-specific models, where fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between model versions. Specifically, we derive the diff vector from one source model version, which represents the weight changes from fine-tuning, and apply it to the base model of different target version. Through empirical evaluations on various open-weight model versions, we show that transferring diff vectors can significantly improve the target base model, often achieving performance comparable to its fine-tuned counterpart. For example, reusing the fine-tuning updates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on GPQA over the base Llama 3.1 8B without additional training, surpassing Llama 3.1 8B Instruct. In multilingual model development setting, we show that this approach can significantly increase performance on target-language tasks without retraining, achieving an absolute improvement of 4.7% and 15.5% on Global MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B Instruct. Our controlled experiments reveal that fine-tuning transfer is most effective when the source and target models are linearly connected in the parameter space. Additionally, we demonstrate that fine-tuning transfer offers stronger and more computationally efficient starting point for further fine-tuning. Finally, we propose an iterative recycling-then-finetuning approach for continuous model development, which improves both efficiency and effectiveness. Our findings suggest that fine-tuning transfer is viable strategy to reduce training costs while maintaining model performance."
        },
        {
            "title": "1 Introduction",
            "content": "Todays LLMs are developed in two main stages: (1) pretraining on vast text corpora using self-supervised objectives such as next-word prediction, and (2) post-training, which involves additional alignment steps, such as supervised fine-tuning and reinforcement learning to ensure the model aligns with human preferences. While this approach creates powerful and versatile LLMs, it poses significant challenge for model updates. Specifically, each new version of the pretrained model requires repeating the alignment process, which is costly, particularly for frequent updates. This issue is further compounded when developing domainor language-specific models, as fine-tuning on specialized data must be redone for every new base model release. In this paper, we explore the transfer of fine-tuning updates between different model versions to improve the current LLM development process. Specifically, we propose incorporating the weight updates from source 1 Figure 1: To transfer fine-tuning (e.g., instruction tuning) from source model version (e.g., Llama 3.0) to target version (Llama 3.1), we first compute the diff vector = is the fine-tuned model (instruction-tuned Llama 3.0) and ms is the base model (pretrained Llama 3.0). Then, we add to the target base model (pretrained Llama 3.1) to approximate the fine-tuned model in version (instruction-tuned Llama 3.1). We explore two scenarios: (1) recyclingtransferring from an older model version to newer one to reduce retraining, and (2) backportingtransferring from newer version to an older one to take advantage of the newer fine-tuning while maintaining optimization for specific use cases. ms from version s, where model version to improve the training efficiency and effectiveness of target model version t. Transferring from an older model version to newer one (recycling) can save training time and computational resources by reducing the need for retraining. Conversely, transferring from newer version to an older one (backporting) is beneficial when the older base model is better optimized for specific use case (e.g., particular language), allowing the user to take advantage of the new fine-tuning improvements while maintaining optimization and compatibility. ms from version s, which represents Our approach (see Figure 1) first computes the diff vector = (e.g., instruction-tuned) and its base model ms (pretrained). the difference between the fine-tuned model Intuitively, encodes the specific updates to the model parameters during fine-tuning, and can be used to transfer knowledge from the source version (e.g., Llama 3.0) to the target version (Llama 3.1). We hypothesize that models fine-tuned using the same or similar training data and procedures exhibit linear mt. This suggests that we can approximate the fine-tuned relationships across versions: mt + s. This is similar to the concept of version task vectors (Ilharco et al., 2023), but instead of transferring across tasks using the same base model, we focus on transferring between different model versions trained on the same or similar data. of the target base model mt without training: ms We begin by evaluating the feasibility of our approach through the transfer of diff vectors across different versions of open-weight models, including Llama (Dubey et al., 2024), OLMo (OLMo et al., 2024), and Tülu (Lambert et al., 2024). Our results show that fine-tuning updates can be successfully transferred across model versions. Specifically, by merging the diff vector from source model version with the base model mt of target version t, we obtain new model, mt + s, which significantly improves the performance of mt on various tasks. In many cases, this method achieves performance comparable to the fine-tuned counterpart of mt (i.e., ) without requiring additional fine-tuning. For example, recycling the fine-tuning updates from Llama 3.0 yields 10.7% absolute accuracy improvement on GPQA over the base Llama 3.1 8B, surpassing the performance of Llama 3.1 8B Instruct, without additional training. Motivated by these results, we conduct case study on the development of multilingual models. Specifically, we first fine-tune Llama 3.0 8B Instruct on target languages and then transfer the resulting diff vectors to Llama 3.1 8B Instruct. This recycling approach produces models that outperform Llama 3.1 Instruct in the target language without requiring additional training. For example, we obtain an absolute accuracy 1In software development, backporting refers to the process of adapting features or updates from newer version of software system or component for use in an older version. 2 improvement of 4.7% for Malagasy and 15.5% for Turkish on the Global MMLU benchmark (Singh et al., 2024a), compared to Llama 3.1 8B Instruct. To shed light on when fine-tuning transfer is most effective, we perform controlled experiments using OLMo 2s intermediate pretrained checkpoints as different model versions. We fine-tune these models on the same data and investigate transferability across model versions. Our results suggest that fine-tuning transfer is most effective when the source and target models lie within linearly connected region of the parameter space, indicating linear mode connectivity (Qin et al., 2023; Ainsworth et al., 2023; Wortsman et al., 2022a;b; Frankle et al., 2020). Furthermore, we investigate whether the merged model mt + can serve as computationally efficient and effective starting point for fine-tuning. Our experiments demonstrate that initializing fine-tuning with this merged model can accelerate convergence and improve accuracy compared to training on top of mt. This suggests that fine-tuning transfer can be beneficial intermediate step in scenarios where further training is feasible. Finally, we explore continuous model development scenario in which new model versions are regularly released. We propose an iterative recycling-then-finetuning approach that incrementally accumulates finetuning updates from previous model versions. Our experiments show that this method can consistently improve both training efficiency and model performance. In summary, our key contributions are as follows. Introducing an approach for transferring fine-tuning updates between model versions via diff vector transfer. Demonstrating that this approach can reduce training costs while maintaining competitive performance. Validating the approach in multilingual model development setting, showing improved languagespecific performance without retraining. Establishing conditions for effective fine-tuning transfer, particularly when models exhibit linear mode connectivity. Proposing recycling-then-finetuning strategy to improve both efficiency and performance in continuous model development setting. In this paper, we use the terms transferring fine-tuning, transferring fine-tuning updates Terminology: interchangeably to refer to the process of reusing fine-tuning updates (i.e., weight changes) from source model version and incorporating them into target version."
        },
        {
            "title": "2 Transferring fine-tuning updates across model versions",
            "content": "In the development of modern LLMs, when new pretrained model is released, fine-tuned models (optimized for specific tasks or languages) often need to be retrained to take advantage of the improvements, which can be computationally expensive. Recycling updates from an older model can mitigate this issue by reducing the need for full retraining. Conversely, when an older base model remains better suited for specific use case, backporting updates from newer model allows users to benefit from fine-tuning improvements while maintaining optimization and compatibility. In this section, we explore transferring the weight changes from source model version to target model version t, denoted Tst, without additional training. Specifically, we directly merge (add) the diff vector = ms from version s, which captures the parameter adaptations from the base model ms to its fine-tuned counterpart , onto the new base model mt in version t, without any gradient-based training. Our results  (Table 1)  show that fine-tuning updates can be effectively transferred across model versions, as mt + often performs comparably to its fine-tuned counterpart ."
        },
        {
            "title": "Model",
            "content": "GSM8K MATH ARCC GPQA MMLU IFEval Llama 3.0 8B Instruct Llama 3.0 8B + 3.1 Llama 3.1 8B Instruct Llama 3.1 8B + 3.0 81.1 55.6 82.8 86.5 56.6 79.8 28.8 17.3 44.7 50.3 19.3 29. 82.4 79.7 83.0 83.8 79.2 82.9 31.5 22.3 25.9 31.3 21.9 32.6 64.9 66.7 70.0 72.9 66.8 65. 76.6 34.5 76.6 80.5 36.4 83.3 Table 1: Fine-tuning transfer significantly improves the performance of the target base model across various tasks, achieving results comparable to its fine-tuned counterpart in many cases. Here, 3.0 and 3.1 represent the diff vectors between Llama Instruct and Llama for versions 3.0 and 3.1, respectively. Notably, adding the diff vector from different model version can effectively transform non-instruction-tuned model (e.g., Llama 3.0 or Llama 3.1) into one that follows instructions well (Llama 3.0 + 3.1 or Llama 3.1 + 3.0) without further training. Additional results for OLMo and Tülu can be found in Appendix A, where we additionally find that advanced LLM capabilities, attained through alignment tuning stages such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), or Group Relative Policy Optimization (GRPO), can be successfully transferred across different model versions. 2.1 Experimental setup We conduct experiments with various open-weight models, including Llama (Dubey et al., 2024), OLMo (OLMo et al., 2024), and Tülu (Lambert et al., 2024). Our study explores both transfer directions: from an older model version to newer one (recycling) and from newer version to an older one (backporting). We emphasize that our goal is not to achieve state-of-the-art results, but instead to access the feasibility of transferring fine-tuning (by transferring weight changes) between model versions. We evaluate the merged model mt + on diverse set of benchmarks, including general knowledge with MMLU (Hendrycks et al., 2021a), math with GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b), reasoning with ARCC (Clark et al., 2018) and GPQA (Rein et al., 2024), and instruction-following with IFEval (Zhou et al., 2023). We compare its performance to that of fine-tuning mt directly (i.e., ).2 See Appendix for evaluation details. 2.2 Results and discussion Transferring fine-tuning substantially boosts the target base models performance: Table 1 shows our results when transferring fine-tuning (i.e., instruction tuning) updates between Llama 3.0 and Llama 3.1. Strikingly, adding the diff vector from different model version can effectively transform non-instruction-tuned model (e.g., Llama 3.0 or Llama 3.1) into one that follows instructions well (Llama 3.0 + 3.1 or Llama 3.1 + 3.0). For example, our approach yields 42.1% and 46.9% absolute accuracy improvements on the instruction-following benchmark IFEval over Llama 3.0 and Llama 3.1, respectively. Large gains are also observed across the board on math and reasoning benchmarks, including 27.2% over Llama 3.0 and 23.2% over Llama 3.1 on GSM8K. These results suggest that advanced knowledge and instruction-following abilities can be efficiently transferred between model versions without further training. In general, Llama 3.0 benefits more from the backported diff vector 3.1 from version 3.1 than Llama 3.1 does from recycling version 3.0s diff vector 3.0. Transferring fine-tuning can achieve performance comparable to the fine-tuned model: Our results demonstrate that the merged model mt + can perform on par with its fine-tuned counterpart across various tasks. This is particularly true for Llama 3.0 + 3.1, which matches or surpasses Llama 3.0 Instruct on five out of the six tasks we evaluated. Interestingly, Llama 3.1 + 3.0 outperforms LLama 3.1 Instruct on the GPQA and IFEval benchmarks. This is testament to the diff vectors ability to 2For evaluation, we use the lm-evaluation-harness library (Gao et al., 2024)."
        },
        {
            "title": "Malagasy Sinhala Turkish",
            "content": "Llama 3.0 8B Instruct + FT Llama 3.1 8B Instruct + 3.0 23.1 30.8 27.6 32.3 23.3 29. 33.0 32.3 30.8 43.2 27.7 43.2 Table 2: Recycling fine-tuning updates improves multilingual performance on Global MMLU without retraining, yielding 4.7% and 15.5% absolute improvement for Malagasy and Turkish, respectively, compared to Llama 3.1 8B Instruct. 3.0 represents the diff vector between Llama 3.0 Instruct and its monolingual fine-tuned (FT) version. encode advanced reasoning and instruction-following capabilities. Overall, our results suggest that finetuning transfer provides an effective and extremely low-cost method to improve model performance when training is prohibitively expensive."
        },
        {
            "title": "3 Efficient multilingual model development",
            "content": "Motivated by our results in Section 2, we now turn toward applying our fine-tuning transfer approach in multilingual model development setting. We focus exclusively on recycling scenario, where our aim is to transfer the language-specific instruction tuning updates from an older model version to newer one. This approach helps avoid retraining, as frequently fine-tuning new model versions from scratch can be prohibitively expensive for multilingual communities. For language-specific instruction tuning, we fine-tune an instruction-tuned model rather than pretrained one. This approach aligns with the common practice of using an instruction-tuned English or multilingual model as the foundation when developing language-specific models. key challenge in this setting is that state-of-the-art LLMs often include multilingual data in pretraining and instruction tuning, which makes it unclear whether language-specific fine-tuning is still necessary. How effective is our recycling approach when applied to multilingual instruction-tuned model? Our results show that recycling fine-tuning remains effective in this scenario, as long as the base model is still outperformed by its previous monolingual counterpart. In such cases, our approach produces models that outperform Llama 3.1 Instruct on target-language tasks without additional training. 3.1 Experimental setup We fine-tune Llama 3.0 Instruct (ms) separately on language-specific instruction tuning data for three languages: Malagasy, Sinhala, and Turkish. We use the Aya dataset (Singh et al., 2024b) for Malagasy (14.6K examples) and Sinhala (14.5K examples), and the InstrucTurca dataset (Altinok, 2024) for Turkish (16.7K examples).3 Each model is trained for 30K training steps with learning rate of 5e-6 and batch size of 8, using 4 NVIDIA A100-80G GPUs.4 After training on each language, we compute the diff vector = ms and add it to Llama 3.1 Instruct mt. We focus on low-resource setting, where no additional training on language-specific data is performed. Finally, we evaluate the merged model mt + against the base model mt using the Global MMLU benchmark (Singh et al., 2024a). 19.4 26.6 M1 M2 M3 M4 M5 65.5 24.4 13.2 19.6 32.0 17.3 39.8 70.3 77.1 64.5 27.5 25.9 68.6 19.0 14.3 11.8 11.9 25.0 18.0 16.0 22.6 24. 72.9 75.7 75.5 45.1 50.7 60. + 1 + 2 + 3 + 4 + 5 FT(Mi) Table 3: GSM8K accuracies indicating that more powerful models are better at leveraging transferred finetuning. Effective use of transferred fine-tuning only emerges once the target base model reaches certain level of capability. Furthermore, fine-tuning transfer works best when the source and target models are close within linearly connected region of the parameter space. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. FT(Mi) denotes applying fine-tuning directly to Mi. See Table 11 in Appendix for MATH500 results. 3.2 Results and discussion Transferring fine-tuning is effective for developing multilingual models: Our results in Table 2 demonstrate the benefits of reusing fine-tuning updates in multilingual model development. For Malagasy and Turkish, transferring the diff vector from Llama version 3.0 to 3.1 results in significant accuracy improvements (4.7% and 15.5%, respectively) compared to Llama 3.1 8B Instruct. Our recycling approach also improves the fine-tuned Llama 3.0 Instruct model for Malagasy (1.5% accuracy improvement) without requiring additional training, while maintaining similar performance for Turkish. This is especially beneficial for multilingual communities, as it allows for model improvements at very low cost by leveraging prior fine-tuning updates and an updated base model. On the other hand, for Sinhala, recycling fine-tuning offers no advantage, as Llama 3.1 Instruct already outperforms the previously fine-tuned Llama 3.0 Instruct. However, even in this case, recycling does not significantly impact performance."
        },
        {
            "title": "4 When is fine-tuning transfer effective?",
            "content": "Having demonstrated the effectiveness of fine-tuning transfer, we now conduct controlled experiments to better understand when this approach is most effective. At high level, we treat different checkpoints of pretrained model as distinct model versions. We then fine-tune these model versions on the same data and assess the impact of transferring fine-tuning updates between them. Our results reveal that fine-tuning transfer is most successful when the source and target models are close within linearly connected region of the parameter space, indicating linear mode connectivity (Qin et al., 2023; Ainsworth et al., 2023; Wortsman et al., 2022a;b; Frankle et al., 2020). 4.1 Experimental setup We conduct experiments with the publicly available intermediate pretrained checkpoints of OLMo 2 7B.5 The base OLMo 2 model was trained in two stages: (1) general web-based pretraining stage (stage 1), and (2) mid-training stage (stage 2) using high-quality web data and domain-specific data to enhance 3To simulate low-resource setting, we sampled 6.5% of the original InstrucTurca dataset, which contains 2.58 million examples, resulting in approximately 16.7K examples. 4We use the AdamW optimizer with linear scheduler and warmup ratio of 0.03. We disable dropout and exclude weight decay for embeddings. The sequence length is 2048. We use open-instruct (Lambert et al., 2024) for training and lm-evaluation-harness (Gao et al., 2024) for evaluation. 5https://huggingface.co/allenai/OLMo-2-1124-7B 6 STEM-related capabilities. We select five checkpoints: M1 (early-stage 1, at 300K steps), M2 (mid-stage 1, at 600K steps), M3 (end-stage 1, at 929K steps), M4 (mid-stage 2, at 6K steps), and M5 (end-stage 2, at 12K steps). Each Mi is treated as distinct model version. We investigate both transfer scenarios: (1) recycling from an older model version to newer one (TMiMj , < j), and (2) backporting from newer version to an older one (TMjMi, > i). Due to our limited computational resources, supervised fine-tuning with large instruction tuning dataset would be prohibitively expensive. We therefore fine-tune all model versions using subset of the math reasoning instruction tuning data from Tülu 3, which includes Tülu 3 Persona MATH, GSM, and Algebra (220K examples total). We follow the training procedure described in Section 3.1. We evaluate our models on GSM8K and the MATH500 subset (Hendrycks et al., 2021b) of the MATH dataset, which includes competition-level math problems of varying difficulty. These datasets are selected because fine-tuning on Tülu 3s math reasoning data significantly improves performance on them, allowing for clearer analysis of the impact of transferring fine-tuning updates between model versions."
        },
        {
            "title": "4.2 Results and discussion",
            "content": "More powerful models are better at leveraging transferred fine-tuning: Our results in Table 3 indicate that stronger models are more effective at leveraging transferred fine-tuning updates. While transferring fine-tuning can improve performance for M1, M2, and M3, the merged models Mi + (j denotes the diff vector from model version Mj, = i) still fall significantly short of their fine-tuned counterparts, denoted FT(Mi). On GSM8K, the accuracy gaps between the best Mi + and FT(Mi) are 26.1%, 24.1%, 20.6% for M1, M2, and M3, respectively. In contrast, for M4, this gap narrows to 2.8%. Notably, recycling fine-tuning from M4 to M5 (i.e., M5 + 4) surpasses fine-tuning directly on M5 (FT(M5)), achieving 1.6% accuracy improvement (77.1% vs. 75.5%). Similar trends are observed on MATH500. This pattern suggests an emergent abilityeffective use of transferred fine-tuning only emerges when the target base model is sufficiently strong. In other words, the benefits of transferring fine-tuning only become significant beyond certain level of capability. Fine-tuning transfer works best when models are close in the parameter space: Our results also suggest that fine-tuning transfer is most effective when the source and target models are closely connected in the parameter space. On both GSM8K and MATH500, models M1 and M2 benefit more from 3 than from 4 or 5. Similarly, M4 and M5 gain more from 3 than from 1 or 2. Overall, M1, M2, and M3 form mutually beneficial group, as do M4 and M5. However, transferring between these two groups can degrade performance. Specifically, M1, M2, and M3 do not benefit from 4 and 5, while M4 and M5 typically benefit only from 3."
        },
        {
            "title": "5 Fine-tuning transfer as a starting point for further fine-tuning",
            "content": "So far, we have explored scenario where fine-tuning is transferred between model versions without additional gradient-based training. We now switch gears to investigate whether the merged model mt + can serve as stronger and more computationally efficient starting checkpoint for further fine-tuning. We conduct controlled experiments comparing two approaches: fine-tuning the merged model mt + versus fine-tuning mt directly. Our results demonstrate that initializing fine-tuning with mt + often leads to faster convergence and higher performance on both seen and unseen tasks. This suggests that fine-tuning transfer can be useful intermediate step when additional training is feasible. We refer to this approach as transferring-then-finetuning. 5.1 Experiment setup We follow the training procedure outlined in Section 3.1. For evaluation, we use GSM8K and MATH500, along with several additional datasets to assess how well our transferring-then-finetuning approach general6For evaluation, we use the OLMES library (Gu et al., 2024). 7The only exception is M4 benefiting from M1 and M2 on MATH500. 7 M1 13. + 1 FT + 2 FT 50.1+31.1 + 3 FT 48.5+34.2 + 4 FT 48.2+36.4 + 5 FT 48.1+36.2 FT(Mi) 45.1 M2 19.4 56.9+30.3 57.6+32.6 56.7+38.7 55.6+39.6 50.7 M3 24.4 62.8+30.8 62.7+22.9 63.7+41.1 63.5+39.5 60. M4 64.5 77.8+50.3 78.6+52.7 77.6+9.0 76.2+3.3 75.7 M5 65.5 78.6+59.0 78.7+61.4 78.8+8.5 77.2+0.1 75.5 Table 4: GSM8K accuracies showing that fine-tuning transfer provides stronger starting point (i.e., Mi + j) for further fine-tuning (FT). Numbers in subscript indicate improvement over the baseline without fine-tuning. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. FT(Mi) denotes applying fine-tuning directly to Mi. See Table 12 in Appendix for MATH500 results. Figure 2: GSM8K performance showing that fine-tuning transfer provides more computationally efficient starting point (i.e., Mi + j) for further training. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. Additional results for M1, M2, M4 can be found in Appendix D. izes to unseen tasks, including PhD-level science questions with GPQA (Rein et al., 2024), tabular math word problems with TabMWP (Lu et al., 2023), and elementary school math word problems with ASDiv (Miao et al., 2020). Diamond 5.2 Results and discussion Transferring-then-finetuning can substantially boost performance: Our results are summarized in Table 4. Transferring-then-finetuning offers significant improvements over our vanilla transfer approach (without additional fine-tuning) on both GSM8K and MATH500. On GSM8K, the largest accuracy improvements are 36.4%, 39.6%, 41.1%, 52.7%, and 61.4% for M1, M2, M3, M4, and M5, respectively. The benefits are most pronounced for weaker base models (M1, M2, and M3) across all diff vectors, as well as for stronger base models when paired with weak diff vector (e.g., M5 + 1). Interestingly, fine-tuning also helps bridge the performance gap between the merged models Mi + (j = i) for each base model Mi. For example, it dramatically improves the performance of M5 + 1 and M5 + 2 from 10.6% and 17.3% to 78.6% and 78.7%, respectively, closing the gap with the fine-tuned versions of M5 + 3 (78.8%) and M5 + 4 (77.2%). This reduces the need to pre-select the best diff vector when multiple"
        },
        {
            "title": "GPQADiamond TabMWP ASDiv",
            "content": "M5 + 4 FT FT(M5) 25.2 28.2 26.2 22.4 46.4 48.5 28.1 82. 81.8 Table 5: Transferring-then-finetuning does not negatively impact model generalization to unseen tasks. Here, we recycle the diff vector 4 from previous pretrained version of OLMo 2 7B (M4) to newer pretrained results can be found in version (M5), before further fine-tuning the resulting model. Full GPQA Table 13 in Appendix D. Diamond choices are available. Importantly, transferring-then-finetuning generally outperforms standard fine-tuning regardless of the diff vector used. Transferring-then-finetuning can offer faster convergence: Figure 2 shows that using the merged model Mi + as the initial checkpoint improves training efficiency. Specifically, Mi + not only converges significantly faster than Mi during fine-tuning but also reaches higher peak accuracy on GSM8K. Overall, our results suggest that transferring-then-finetuning is cost-effective approach that reduces the number of fine-tuning steps, thereby improving training efficiency. Transferring-then-finetuning does not negatively impact model generalization: As shown in Table 5, this approach attains strong zero-shot generalization on the three unseen tasks, comparable to standard fine-tuning. These results suggest that transferring-then-finetuning does not lead to overfitting, demonstrating its broad applicability across diverse tasks."
        },
        {
            "title": "6 Iterative recycling-then-finetuning for improved performance and efficiency",
            "content": "Building on the insights from our previous experiments, we now explore continuous model development setting in which new versions of pretrained model are regularly released. At the core of our approach is an iterative recycling-then-finetuning strategy that incrementally incorporates fine-tuning updates, i.e., diff vectors, from past model versions. Instead of applying only the latest diff vector to the new base model, we recycle all previous diff vectors. Specifically, the diff vector at the current model version is carried forward to the next for subsequent fine-tuning. Our experiments show that this iterative recycling approach consistently improves both training efficiency and model performance. 6.1 Iterative recycling-then-finetuning We treat the five intermediate checkpoints of OLMo 2 7BM1, M2, M3, M4, M5 (described in Section 4.1) as different model versions of the pretrained OLMo 2 model. Our iterative recycling-then-finetuning algorithm, outlined in Algorithm 1, works as follows: At each iteration i, we first apply the most recent diff , to the new base model Mi, and then further fine-tune the resulting model. Next, we compute vector, iter i1 new diff vector between the fine-tuned model and the current base model Mi. This new diff vector is then carried forward to the next model version for fine-tuning in the subsequent iteration. We refer to our iterative recycling-then-finetuning approach as iter and compare it to dir, direct recycling-then-finetuning approach that applies the diff vector from the latest model version directly to the current model. We follow the training procedure outlined in Section 3.1. 6.2 Results and discussion Iterative recycling-then-finetuning significantly improves performance: Table 6 compares the performance of two recycling approaches: iterative recycling-then-finetuning (iter) and direct recyclingthen-finetuning (dir). Both approaches lead to significant performance improvements across model versions 9 Algorithm 1 Iterative recycling-then-finetuning 1: Notation: FT denotes fine-tuning 2: Input: Base models M1, M2, . . . , Mn 3: Output: Fine-tuned models 1, 1 FT(M1) 4: 5: for = 2 to do iter i1 Mi1 6: i1 FT(Mi + iter 7: i1 8: end for 9: return = 2, . . . , 1, ) 2, . . . , M3 24.4 62.7+38.3 67.0+42.6 60.4 M4 64.5 77.6+13.1 77.3+12.8 75.7 M5 65.5 77.2+11.7 77.5+12.0 75.6 + dir FT + iter FT FT(Mi) Table 6: Both iterative (iter) and direct (dir) recycling-then-finetuning approaches significantly boost GSM8K performance, surpassing standard fine-tuning without recycling (FT(Mi)). Numbers in subscripts indicate improvement over OLMo 2 7B checkpoints. At high level, iter gradually incorporates fine-tuning updates, i.e., diff vectors, from previous model versions, while dir directly applies the diff vector from the latest model version to the current model. Results for M1 and M2 are omitted as these models remain identical across the two approaches (see Algorithm 1). on GSM8K, with larger gains observed in earlier versions. For example, iter achieves absolute accuracy improvements of 42.6%, 12.8%, and 12% over M3, M4, and M5, respectively. Both approaches also outperform the standard fine-tuning baseline (without recycling) by substantial margin. Specifically, iter yields accuracy improvements of 6.6% on M3 and 1.9% on M5 compared to standard fine-tuning. In general, iter performs similarly to or significantly better than dir across all model versions. These results suggest that in scenarios where the base model is continuously updated, adopting an iterative recycling strategy is beneficial. Iterative recycling-then-finetuning leads to faster convergence: Figure 3 shows that both recycling approachesiterative (iter) and direct (dir) recycling-then-finetuningoffer more computationally efIn general, iter consistently outperforms dir in terms of ficient starting point for further fine-tuning. training efficiency and significantly improves standard fine-tuning without recycling. These results indicate that iterative recycling not only improves model performance but also enhances training efficiency by effectively leveraging the knowledge stored in the diff vectors across different model versions."
        },
        {
            "title": "7 Related work",
            "content": "Fine-tuning transfer: We build on prior work in fine-tuning transfer and model merging. The literature on fine-tuning transfer is rife with methods that adapt the entire model (Phang et al., 2018; Pruksachatkun et al., 2020; Vu et al., 2020; 2021; Aghajanyan et al., 2021) or use parameter-efficient modules such as adapters (Pfeiffer et al., 2021; Poth et al., 2021), soft prompts (Vu et al., 2022b;a; Su et al., 2022; Asai et al., 2022), and LoRA matrices (Huang et al., 2024; Zadouri et al., 2024; Ostapenko et al., 2024); see Yadav et al. (2024a) for comprehensive survey. However, most of these approaches focus on adapting the same base model to different tasks, domains, or languages. Similarly, model merging combines multiple task-specific models based on the same base model to create more powerful model (Ilharco et al., 2023; Yadav et al., 2023; Yu et al., 2024; Yadav et al., 2024b; Ahmadian et al., 2024; Bandarkar et al., 2025). In contrast, our 10 Figure 3: GSM8K performance showing that both iterative (iter) and direct (dir) recycling-then-finetuning approaches offer faster convergence. At high level, iter gradually incorporates fine-tuning updates, i.e., diff vectors, from previous model versions, while dir directly applies the diff vector from the latest model version to the current model. approach transfers fine-tuning updates across different model versions trained on the same or similar data, tackling the challenge of frequent model updates in modern LLM development. Cross-model fine-tuning transfer: Previous studies (Lester et al., 2022; Su et al., 2022) investigate crossmodel fine-tuning transfer, focusing mainly on transferring lightweight modules between non-instructiontuned models. Lester et al. (2022) demonstrate that recycling soft prompts between models of the same size, as well as those of different sizes, is possible but challenging. Su et al. (2022) also investigate the transferability of soft prompts and show that they can be transferred between pretrained models of different architectures. In contrast, our work leverages diff vectors to enable transfer across model versions and explores both pretrained and instruction-tuned models. Closely related to our work, Qin et al. (2023) study recyclable fine-tuning in continual domain adaptation setting from the BERT (Devlin et al., 2019) era, where pretrained RoBERTa (Liu et al., 2019) model is sequentially pretrained on new domains. They show that fine-tuning updates made to domainadapted checkpoint can be incorporated into new one via element-wise multiplication, leading to improved performance and training efficiency on classification tasks. Their approach includes two recycling methods: initialization-based (similar to ours) and distillation-based (which minimizes KL divergence and mean-square loss between checkpoints). In this work, we explore contemporary LLM development setting, focusing on todays LLMs and open-ended generation benchmarks. We investigate the use of diff vectors to facilitate transfer across model versions in both recycling and backporting scenarios. Their approach and ours can complement each other, and combining them is promising avenue for future work. Finally, there is also line of research focused on reusing small models for larger ones through methods such as duplication (Chen et al., 2022), progressive stacking (Gong et al., 2019), or merging parameters (Wang et al., 2023)."
        },
        {
            "title": "8 Conclusion",
            "content": "Our study demonstrates that fine-tuning transfer offers practical approach to mitigate the inefficiencies of frequent model updates. By applying diff vectors from fine-tuned source model version to different target model version, we achieve substantial performance improvements without the need for full fine-tuning. In multilingual context, this approach can significantly boost performance on target-language tasks, offering scalable solution for language-specific fine-tuning without retraining on every new base model release. Through controlled experiments, we show that fine-tuning transfer is most effective when the source and target models are linearly connected in the parameter space. Furthermore, this approach can offer more 11 robust and computationally efficient starting checkpoint for further fine-tuning and can be seamlessly integrated into continuous model development cycle. Taken together, we hope that our work will spur more fundamental research into the efficient development of modern LLMs."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Colin Raffel for valuable advice and useful suggestions on our experiments; Mohit Iyyer, Noah Constant, Tsendsuren Munkhdalai, Prateek Yadav, Naren Ramakrishnan, Alessandro Sordoni, Lucas Caccia, Minseon Kim, Ahmet Üstün, Tom Hosking, Matthias Gallé, Salaheddin Alzubi, Shayne Longpre, Quyet Do, Thinh Pham, Kavana Venkatesh, Nguyen Nguyen, Adam Nguyen, Rituraj Sharma, Aninditaa Chauhan, Yeana Lee, and the rest of the VT LLMs group for helpful discussions and suggestions at various stages in the project; and Brian Lester for sharing code for model merging. 12 References Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 57995811, 2021. URL https://aclanthology. org/2021.emnlp-main.468/. Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, Sara Hooker, et al. Mix data or merge models? optimizing for diverse multi-task learning. arXiv preprint arXiv:2410.10801, 2024. URL https://arxiv.org/abs/2410.10801. Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=CQsmMYmlP5T. Duygu Altinok. Instructurca: diverse instructional content dataset for turkish, 2024. URL https: //huggingface.co/datasets/turkish-nlp-suite/InstrucTurca. Akari Asai, Mohammadreza Salehi, Matthew Peters, and Hannaneh Hajishirzi. ATTEMPT: Parameterefficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 66556672, 2022. URL https: //aclanthology.org/2022.emnlp-main.446/. Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, and Bing Liu. Layer swapping for zero-shot cross-lingual transfer in large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= vQhn4wrQ6j. Cheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan Liu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21342148, 2022. URL https://aclanthology.org/2022.acl-long.151/. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, 2019. URL https://aclanthology.org/N19-1423/. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 32593269. PMLR, 2020. URL https: //proceedings.mlr.press/v119/frankle20a.html. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2024. URL https://zenodo.org/records/12608602. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of BERT by progressively stacking. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 23372346. PMLR, 2019. URL https: //proceedings.mlr.press/v97/gong19a.html. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. Olmes: standard for language model evaluations. arXiv preprint arXiv:2406.08446, 2024. URL https://arxiv. org/abs/2406.08446. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https://openreview.net/forum?id=7Bywt2mQsCe. Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-task generalization via dynamic loRA composition. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=TrloAXEJ2B. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali In The Eleventh International Conference on Learning Farhadi. Editing models with task arithmetic. Representations, 2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tülu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. URL https://arxiv.org/abs/2411.15124. Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. Reducing retraining by recycling parameter-efficient prompts. arXiv preprint arXiv:2208.05577, 2022. URL https://arxiv.org/abs/ 2208.05577. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. URL https://arxiv.org/abs/1907.11692. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=DHyHRBwJUTN. Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975984, 2020. URL https://aclanthology.org/2020.acl-main.92/. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. URL https://arxiv.org/abs/2501.19393. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. URL https://arxiv.org/abs/2501.00656. Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro Sordoni. Towards modular llms by building and reusing library of loras. arXiv preprint arXiv:2405.11157, 2024. URL https://arxiv.org/abs/2405.11157. 14 Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 487503, 2021. URL https://aclanthology.org/2021.eacl-main.39/. Jason Phang, Thibault Févry, and Samuel Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088, 2018. URL https://arxiv.org/ abs/1811.01088. Clifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna Gurevych. What to pre-train on? Efficient intermediate task selection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1058510605, 2021. URL https://aclanthology.org/2021.emnlp-main.827/. Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Intermediate-task transfer learning with pretrained Vania, Katharina Kann, and Samuel R. Bowman. language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 52315247, 2020. URL https://aclanthology.org/2020. acl-main.467/. Yujia Qin, Cheng Qian, Xu Han, Yankai Lin, Huadong Wang, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Recyclable tuning for continual pre-training. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1140311426, 2023. URL https://aclanthology.org/2023.findings-acl. 723/. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pp. 5372853741, 2023. URL https://proceedings.neurips. cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=Ti67584b98. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://arxiv.org/abs/2402.03300. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David Adelani, Jian Gang Ngui, Daniel VilaSuero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304, 2024a. URL https://arxiv.org/abs/2412.03304. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzemiński, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Chien, Sebastian Ruder, Surya Guthikonda, Emad Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1152111567, 2024b. URL https://aclanthology.org/2024.acl-long.620/. Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, and Jie Zhou. On transferability of prompt tuning for natural language processing. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 39493969, 2022. URL https://aclanthology.org/2022.naacl-main.290/. 15 Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew MattarellaMicke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across NLP tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 78827926, 2020. URL https://aclanthology.org/2020.emnlp-main.635/. Tu Vu, Minh-Thang Luong, Quoc Le, Grady Simon, and Mohit Iyyer. STraTA: Self-training with task augmentation for better few-shot learning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 57155731, 2021. URL https://aclanthology.org/2021.emnlp-main. 462/. Tu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mohit Iyyer, and Noah Constant. Overcoming catastrophic forgetting in zero-shot cross-lingual generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 92799300, 2022a. URL https://aclanthology.org/2022. emnlp-main.630/. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. SPoT: Better frozen model adaptation through soft prompt transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 50395059, 2022b. URL https: //aclanthology.org/2022.acl-long.346/. Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for In The Eleventh International Conference on Learning Representations, efficient transformer training. 2023. URL https://openreview.net/forum?id=cDYRS5iZ16f. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2396523998. PMLR, 2022a. URL https://proceedings.mlr. press/v162/wortsman22a.html. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 79597971, 2022b. URL https://openaccess.thecvf.com/content/ CVPR2022/html/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.html. Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: ReIn Advances in Neural Information Processing Systems, solving interference when merging models. volume 36, pp. 70937115, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf. Prateek Yadav, Colin Raffel, Mohammed Muqeeth, Lucas Caccia, Haokun Liu, Tianlong Chen, Mohit Bansal, Leshem Choshen, and Alessandro Sordoni. survey on model moerging: Recycling and routing among specialized experts for collaborative learning. arXiv preprint arXiv:2408.07057, 2024a. URL https: //arxiv.org/abs/2408.07057. Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, and Tsendsuren Munkhdalai. What matters for model merging at scale? arXiv preprint arXiv:2410.03617, 2024b. URL https://arxiv.org/abs/2410.03617. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 5775557775. PMLR, 2024. URL https://proceedings.mlr.press/v235/yu24p.html. Ted Zadouri, Ahmet Üstün, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= EvDeiLv7qc. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, Le Hou. 2023. URL https://arxiv.org/abs/2311.07911."
        },
        {
            "title": "Appendix",
            "content": "A Additional results for Section 2: Transferring fine-tuning updates across model versions"
        },
        {
            "title": "Model",
            "content": "GSM8K MATH ARCC GPQA MMLU IFEval Llama 3.1 8B Llama 3.1 8B Instruct Tülu 3 8B SFT Tülu 3 8B DPO Tülu 3 8B Llama 3.0 8B + SF + DP + RLV Tülu 3.1 8B Llama 3.0 8B Instruct Llama 3.0 8B + GRP 56.6 86.5 76.2 84.1 87.9 55.6 71.8 81.1 85.1 89.9 81.1 55.6 85.8 19.3 50.3 31.6 42.4 43.4 17.3 26.3 38.1 37.6 43.3 28.8 17.3 39. 79.2 83.8 79.1 79.6 79.4 79.7 77.9 78.6 79.1 79.0 82.4 79.7 78.2 21.9 31.3 31.0 33.3 34.4 22.3 32.1 31.9 32.4 31.4 31.5 22.3 29.4 66.8 72.9 65.1 68.4 67.9 66.7 63.5 67.5 66.2 67.6 64.9 66.7 65. 36.4 80.5 72.0 81.7 81.5 34.5 69.1 82.9 82.4 84.1 76.6 34.5 82.6 Table 7: We find that advanced LLM capabilities, attained through alignment tuning stages such as SFT, DPO, RLVR, and GRPO (encoded in SF , DP O, RLV R, and GRP O, respectively), can be successfully transferred across different model versions. Model GSM8K MATH ARCC GPQA MMLU IFEval OLMo 2 7B OLMo 2 7B SFT OLMo 2 7B DPO OLMo 2 7B Instruct M0 + SF + DP + RLV M3 + SF + DP + RLV M4 + SF + DP + RLV 67.2 71.7 82.5 85.3 2.5 2.2 2.1 2.0 24.4 31.7 40.4 40.2 63.7 71.1 79.9 82.8 19.2 25.2 31.3 29. 1.6 0.8 0.8 0.8 5.7 8.4 9.3 10.3 17.5 23.7 27.8 27.7 79.9 79.7 80.5 80.6 25.7 23.8 24.1 24.1 72.7 74.3 75.0 75. 78.6 79.0 79.3 79.3 20.5 27.9 30.6 29.7 18.1 1.3 1.1 0.6 15.4 24.8 29.9 29.9 22.5 28.3 29.0 27.2 63.6 61.2 62.1 63. 25.0 1.4 1.3 1.4 59.8 55.4 56.6 56.7 62.6 59.7 63.1 62.2 23.0 67.7 73.2 75.6 12.2 13.7 13.7 13.3 15.7 51.4 68.0 68. 16.1 64.3 72.6 72.1 Table 8: We find that advanced LLM capabilities, attained through alignment tuning stages such as SFT, DPO, and RLVR (encoded in SF , DP O, and RLV R, respectively), can be successfully transferred across different model versions. Here, M4 is an intermediate pretrained checkpoint of OLMo 2 7B (mid-stage 2, at 7K steps), which we selected before conducting our controlled experiments in Section 4.1. We also conduct experiments with Tülu (Lambert et al., 2024) and OLMo (OLMo et al., 2024), both of which were developed from Llama 3.1 through multiple alignment stages, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and final reinforcement learning 18 stageReinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) for OLMo 2 and Tülu 3, or Group Relative Policy Optimization (GRPO) (Shao et al., 2024) for Tülu 3.1. At high level, we subtract the weights of Llama 3.1 from these alignment-tuned checkpoints and then backport (add) the resulting diff vectors to Llama 3.0. Recycling is not applicable here, as we do not have the alignment-tuned checkpoints for Llama 3.0. Our results are summarized in Table 7 and Table 8. In general, we find that advanced LLM capabilities, attained through alignment tuning stages such as SFT, DPO, RLVR, and GRPO (encoded in SF , DP O, RLV R, and GRP O, respectively), can be successfully transferred across different model versions. For example, backporting GRP from Tülu 3.1 8B to Llama 3.0 8B significantly improves accuracy, boosting GSM8K performance from 55.6% to 85.8% (30.2% improvement) and IFEval from 34.5% to 82.6% (48.1% improvement). This surpasses Llama 3.0 8B Instruct (81.1% on GSM8K, 76.6% on IFEval) and performs competitively with Llama 3.1 8B Instruct (86.5% and 80.5%) and Tülu 3.1 8B (89.9% and 84.1%)."
        },
        {
            "title": "B Additional evaluation details",
            "content": "Task # Shots CoT Metric Reference eval. setup GSM8K MATH ARCC GPQA MMLU IFEval Global MMLU 8 4 0 0 0 0 exact match acc. exact match acc. acc. exact match acc. exact match acc. avg. acc. (strict & loose) acc. Llama 3 Evaluation Details8 Singh et al. (2024a) Table 9: Evaluation details for Llama 3. Task # Shots CoT Metric Reference eval. setup GSM8K MATH ARCC GPQA MMLU IFEval MATH500 GPQADiamond TabMWP ASDiv 8 4 5 0 0 0 0 0 0 0 exact match acc. flex exact match acc. acc. exact match acc. exact match acc. prompt-level loose acc. exact match acc. exact match acc. exact match acc. exact match acc. Lambert et al. (2024) Muennighoff et al. (2025) Yang et al. (2024) Table 10: Evaluation details for OLMo 2 and Tülu 3. We use the same evaluation setup and prompts as those in Llama 3 (Dubey et al., 2024) for Llama models and those in Tülu 3 (Lambert et al., 2024) for OLMo and Tülu models, whenever available. See Table 9 and Table 10 for more details. For evaluation, we use the lm-evaluation-harness library (Gao et al., 2024) for Llama models, and the OLMES library (Gu et al., 2024) for OLMo and Tülu models. Additional results for Section 4: When is fine-tuning transfer effective? See Table 11. 8See https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/eval_details.md. 19 14.6 11.6 8. M1 M2 M3 M4 M5 16.6 11.4 15.6 17.8 14.4 12.6 27.8 34.2 11.6 19.2 14.6 23.4 9.8 11.2 30.6 7.6 8.0 7.8 8.0 9.4 8.0 7. 13.4 17.6 21.6 31.4 33.0 + 1 + 2 + 3 + 4 + 5 FT(Mi) Table 11: MATH500 accuracies indicating that more powerful models are better at leveraging transferred fine-tuning. Effective use of transferred fine-tuning only emerges once the target base model reaches certain level of capability. Furthermore, fine-tuning transfer works best when the source and target models are close within linearly connected region of the parameter space. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. FT(Mi) denotes applying fine-tuning directly to Mi. Additional results for Section 5: Fine-tuning transfer as starting point for further fine-tuning Figure 4: GSM8K performance showing that fine-tuning transfer provides more computationally efficient starting point (i.e., Mi + j) for further training. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. See Table 12, Figure 4, and Table 13. M1 14.6 16.2+8.6 18.4+10.4 17.4+9.6 17.2+9.2 13.4 M2 11.6 21.0+12.2 21.2+11.8 19.0+11.0 21.4+14.0 17.6 M3 11.4 23.0+5.2 26.2+13.6 23.8+14.0 25.0+13.8 21. M4 11.6 32.0+12.8 31.6+17.0 31.0+7.6 30.4-0.2 31.4 M5 16.6 34.2+18.6 31.0+16.6 34.0+6.2 36.2+2.0 33.0 + 1 FT + 2 FT + 3 FT + 4 FT + 5 FT FT(Mi) Table 12: MATH500 accuracies showing that fine-tuning transfer provides stronger starting point (i.e., Mi + j) for further fine-tuning (FT). Numbers in subscript indicate improvement over the baseline without fine-tuning. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. FT(Mi) denotes applying fine-tuning directly to Mi. M1 23.7 + 1 FT + 2 FT 27.7+4.0 + 3 FT 27.7+4.0 + 4 FT 24.7+1.0 + 5 FT 26.7+3.0 25.7 FT(Mi) M2 24.2 25.2+1.0 27.7+3.5 24.7+0.5 26.7+2.5 26.7 M3 23.2 25.1+1.9 25.2+2.0 26.2+3.0 23.2+0. 26.7 M4 26.2 33.3+7.1 30.8+4.6 23.7-2.5 25.7-0.5 19.1 M5 25.2 25.7+0.5 27.2+2.0 23.2-2.0 28.2+3.0 26.2 Diamond Table 13: GPQA accuracies showing that fine-tuning transfer provides stronger starting point (i.e., Mi + j) for further fine-tuning (FT), and transferring-then-finetuning does not negatively impact model generalization to unseen tasks. Numbers in subscript indicate improvement over the baseline without finetuning. Here, Mi represents different intermediate pretrained checkpoints of OLMo 2 7B (with smaller values of indicating earlier checkpoints), and refers to the diff vector resulting from the fine-tuning of version i. FT(Mi) denotes applying fine-tuning directly to Mi."
        }
    ],
    "affiliations": [
        "University of Toronto",
        "Vector Institute",
        "Virginia Tech"
    ]
}