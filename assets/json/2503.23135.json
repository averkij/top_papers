{
    "paper_title": "LSNet: See Large, Focus Small",
    "authors": [
        "Ao Wang",
        "Hui Chen",
        "Zijia Lin",
        "Jungong Han",
        "Guiguang Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a ``See Large, Focus Small'' strategy for lightweight vision network design. We introduce LS (\\textbf{L}arge-\\textbf{S}mall) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 3 1 3 2 . 3 0 5 2 : r LSNet: See Large, Focus Small Ao Wang1 Hui Chen2* Zijia Lin1 1School of Software, Tsinghua University Jungong Han3 Guiguang Ding1 2BNRist, Tsinghua University 3Department of Automation, Tsinghua University wanga24@mails.tsinghua.edu.cn jichenhui2012@gmail.com linzijia07@tsinghua.org.cn jungonghan77@gmail.com dinggg@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose See Large, Focus Small strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and smallkernel aggregation. It can efficiently capture wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https: //github.com/jameslahm/lsnet. 1. Introduction Vision network designs have consistently been focal point of research in the field of computer vision [19, 24, 26, 52, 53, 102], where two prominent network architectures, i.e., Convolutional Neural Networks (CNNs) [26, 31, 40, 41, 53] *Corresponding author. and Vision Transformers (ViTs) [19, 52, 67, 78, 92, 97], have significantly pushed the boundaries in various computer vision tasks [3, 4, 25, 74, 86, 88, 96]. However, both of them have traditionally been computationally expensive, presenting remarkable challenges for their practical deployments, especially for real-time applications [46, 51]. Recently, researchers have been actively exploring the lightweight and efficient designs of vision networks [7, 36, 59, 61, 64, 80] for practical applications. Despite effective, these lightweight models typically rely on certain basic modules, such as self-attention mechanism [19, 81, 90] and convolution [40, 41], for token mixing [77]. This reliance poses challenges regarding the efficiency and effectiveness of the underlying perception and aggregation processes within lightweight networks, often compromising the architectural expressiveness or inference speed. Essentially, contextual perception and aggregation are core processes for token mixing [21, 77, 95], facilitating spatial information fusion. Perception models contextual relationships among tokens, while aggregation integrates token features based on corresponding relationships. In existing lightweight models, two dominant token mixing approaches, self-attention and convolution, employ distinct perception and aggregation processes. Specifically, selfattention employs global perception through holistic feature interaction and global aggregation via weighted sum of all features. Convolution uses the relative positional relationships among tokens for perception and aggregates features with static kernel weights. However, as shown in Fig. 1.(a) and (b), both approaches have limitations. (1) Self-attention often introduces excessive attention to regions lacking significant interconnections, leading to less critical aggregation, e.g., in less informative background [48, 69]. Besides, its perception and aggregation share the same mixing scope. The expansion of context in self-attention and its variants [21, 35, 51] comes at the expense of notable computational complexity. These hinder lightweight models from pursuing high representational ability under low computational budgets. (2) In convolution, the relationships Figure 1. The mechanism of self attention (a) and convolution (b). (c) shows that the human vision system can See Large through the peripheral vision, and Focus Small through the central vision. (d) shows the distribution of rods and cones depending on the eccentricity from the fovea of the human eye. They contribute to the formation of extensive peripheral vision and focal central vision. among tokens modeled by the perception, i.e., the aggregation weights, are determined by the fixed kernel weights. Consequently, while efficient, convolution lacks sensitivity to varying contextual neighborhoods. This imposes constraints on the expressiveness of lightweight models, especially considering that the model capabilities of lightweight networks are inherently limited. Given these, exploring token mixing way for lightweight models with more effective and efficient perception and aggregation processes under limited computational costs is imperative. To this end, we first thoroughly inspect the intuitions underlying the processes of perception and aggregation. We discover that they align closely with the phenomenon of dynamic heteroscale vision ability in the efficient human vision system. Specifically, as shown in Fig. 1.(c), the human vision system follows dual-step mechanism: (1) The broad overview of the scene is firstly captured through the peripheral visions large-field perception [66, 73], i.e., See Large. (2) Subsequently, attention can be directed towards specific elements of the scene, enabling detailed comprehension facilitated by the central visions small-field aggregation [63, 73], i.e., Focus Small. Such characteristic arises from distinct spatial distribution and vision abilities of two types of photoreceptor cells in the retina [38, 66], i.e., rod cells and cone cells, as shown in Fig. 1.(d). Rod cells are widely distributed in the peripheral regions of retina [63] and produce relatively unsharp images with limited spatial detail [82]. However, they exhibit broad responses across the visible spectrum and contribute to large-field peripheral vision in conjunction with cone cells in the retina periphery [76], allowing See Large. Furthermore, cone cells are primarily concentrated in the fovea, small area for central vision [91]. The fovea contains high density of cone cells, which constitute the sharpest region capable of capturing fine details and complex features [37, 79, 82], enabling Focus Small. Guided by efficient large-field perception of the peripheral photoreceptor cells, the fovea can effectively focus on precise imaging of subtle features via small-field aggregation [66]. This See Large, Focus Small approach empowers the human vision system to process visual information swiftly and proficiently [82], thereby facilitating accurate and efficient visual comprehension. These inspections motivate us to design effective and efficient vision networks with the ability to perceive large fields and aggregate small fields. To this end, we first propose novel operation, Large-Small (LS) convolution, which aims to emulate the See Large, Focus Small strategy observed in human vision system, thereby extracting discriminative visual patterns. Generally, LS convolution employs large-kernel static convolution for largefield perception and small-kernel dynamic convolution for small-field aggregation. Rather than simply combining large-kernel and small-kernel convolutions, it firstly leverages broad contextual information captured by large-kernel depth-wise convolution to model the spatial relationships. Then, parameterized by them, small-kernel dynamic convolution operation with group mechanism is constructed to fuse features within highly related visual field. In this way, large-kernel static convolution well perceive the enlarged neighborhood information, leading to improved relationship modeling, like the peripheral vision system. Furthermore, benefiting from this, small-kernel dynamic convolution can adaptively aggregate the intricate visual features in small surroundings, enabling detailed visual understanding like the central vision system. Meanwhile, we delicately design LS convolution efficiently with depth-wise convolution and group mechanism. The aggregation scope is limited in small region. These well ensure the low complexity of both perception and aggregation processes. Consequently, our LS convolution prioritizes both the performance and efficiency, enabling lightweight models to fully harness the representational capability under low computational costs. We consider LS convolution as the fundamental operation of token mixing and integrate it with other common architecture designs to form LS block. Building upon the LS block, we present new family of lightweight models, dubbed LSNet. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency compared with existing state-of-the-art lightweight models in various vision tasks [12, 49, 106]. We hope that LSNet can serve as strong baseline and inspire further advancements in the field of lightweight and efficient models. 2. Related Work Efficient CNNs. CNNs have emerged as the fundamental network architecture in various vision tasks [2, 17, 18, 54, 70, 83] over the past decade. To facilitate their practical applications, researchers have devoted significant efforts to designing lightweight and efficient networks [13, 14, 32, 33, 57, 75, 85]. For example, MobileNet [33] and Xception [8] proposes architectures utilizing depth-wise separable convolutions. MobileNetV2 [71] introduces inverted residual blocks with linear bottleneck for improving efficiency. ShuffleNet [104] and ShuffleNetV2 [57] incorporate channel shuffling and channel split operations to enhance group information exchange. Hardware-aware neural architecture search (NAS) has also been explored to obtain compact vision networks [32, 75]. Meanwhile, considering the limited receptive field, some works have explored enhancing lightweight CNNs capability for modeling long-range dependencies [36, 65, 101]. For example, ParC-Net [101] introduces position aware circular convolution to boast global receptive field. AFFNet [36] presents adaptive frequency filtering for global convolution via circular padding. Efficient ViTs. Later, since the inception of Vision Transformer [19], transformer-based architectures have gained significant popularity in the field of computer vision. ViTs have been adapted to diverse vision tasks and shown superior performance [20, 103]. Meanwhile, efforts have been made to enhance the efficiency, resulting in lightweight ViTs for practical deployments [46, 62, 80, 84]. For example, MobileViT [61] combines MobileNet blocks and MHSA blocks, achieving hybrid architecture. EdgeViT [64] proposes the integration of self-attention and convolutions to achieve cost-effective information exchange. Besides, to alleviate the inference bottleneck, EfficientFormer [46] presents dimension-consistent design paradigm that enhances the latency and performance tradeoff. FastViT [80] introduces structural re-parameterization and large-kernel convolutions to enhance hybrid ViTs. Efficient Token Mixing. CNNs and ViTs adopt different token mixing ways, i.e., convolution and self-attention, respectively, along with distinct perception and aggregation processes. Based on them, to develop lightweight vision networks, researchers have explored different efficient token mixing ways for spatial information exchange. For example, for convolution, Involution [43] leverages MLP for perception to derive the aggregation weights conditioned on single pixel. CondConv [94] proposes per-example routing with global context to linearly combining multiple convolution kernels. For self-attention, EdgeNeXt [59] presents split depth-wise transpose attention (SDTA) to mix multiscale features. PVTv2 [89] employs linear spatial reduction attention (LSRA) to achieve linear computational complexity for the attention layer. EfficientViT [51] designs the casFigure 2. Comparison of self-attention, convolution, and LS conv. caded group attention to enhance capability efficiently. 3. Methodology 3.1. Revisiting Self-Attention and Convolution Self-attention and convolution are two prominent token mixing ways [97] for modeling visual features in existing lightweight networks. For an input image, given its feature map RHW where is the spatial resolution and is the number of channels, token mixing generates the feature representation yi RC for each token xi RC based on its contextual region (xi) by: yi = A(P(xi, (xi)), (xi)), (1) where denotes perception, involving extracting contextual information and capturing the relationships among tokens, and denotes aggregation, integrating the features based on the outcome of perception and enabling the incorporation of information from other tokens. In self-attention, its perception Pattn obtains the attention scores between xi and through the pairwise correlations after softmax normalization. Its aggregation Aattn weights the features of by attention scores to obtain yi. As shown in Fig. 2.(a), the process can be summarized as: yi = Aattn(Pattn(xi, X), X) = Pattn(xi, X)(XWv); Pattn(xi, X) = softmax((xiWq)(XWk)T ), (2) (3) where Wq, Wk and Wv are the projection matrices. It can be observed that Pattn and Aattn involve redundant attention and excessive aggregation in less informative regions [48, 69], limiting the efficacy of lightweight models. Moreover, they operate at the same contextual scale for xi. Such homoscale property leads to the notable computational complexity when increasing the mixing scope (xi), imposing challenges in expanding the perception context under low computational budgets. Thus, self-attention and its variants in existing lightweight models [21, 51] struggle to achieve an optimal balance between representation capability and efficiency with limited computation cost [36]. For convolution with the kernel size of K, the contextual region is the neighborhood of size centered around xi, denoted as NK(xi). The perception Pconv utilizes the relative positions between xi and NK(xi) to derive the aggregation weights. For each xj NK(xi), its aggregation weight is the value at the corresponding relative position in the fixed convolutional kernel weights Wconv. The aggregation Aconv then leverages the weights to convolve the features in NK(xi). As shown in Fig. 2.(b), the whole process can be formulated as: yi = Aconv(Pconv(xi, NK (xi)), NK (xi)) = Pconv(xi, NK (xi)) NK (xi); Pconv(xi, NK (xi)) = Wconv, (4) (5) where denotes the convolution operation. It can be observed that the token mixing scope in convolution is determined by kernel size which is usually small for lightweight models, thus resulting in limited perception range. Besides, the relationships among tokens modeled by the perception Pconv, i.e., the aggregation weights, depend only on the relative positions and thus are shared and fixed for all tokens. It prevents tokens from adapting to their related context, restricting the expressive ability. Such limitation becomes particularly pronounced considering the inherently small modeling capability of lightweight networks. 3.2. LS (Large-Small) Convolution Inspired by dynamic heteroscale vision ability exhibited by human vision system [63, 66, 76], we introduce novel See Large, Focus Small strategy for the perception and aggregation processes, aiming for efficient and effective token mixing in lightweight models, as shown in Fig. 2.(c). Our approach enables the effective collection of comprehensive contextual information and modeling of the relationships by large-field perception. It further facilitates detailed visual representations through efficient fusion in highly related surroundings by small-field aggregation. Specifically, for token xi, with the contextual regions of perception and aggregation as NP (xi) and NA(xi), respectively, where NP (xi) encompasses larger spatial extent compared with NA(xi), the process can be formulated as: yi = A(P(xi, NP (xi)), NA(xi)). (6) It can be observed that (1) The perception and aggregation involves different contextual scopes, i.e., NP (xi) and NA(xi), respectively, allowing for utilizing heteroscale contextual information and capturing both the overall context and fine-grained details. (2) For the perception with large spatial extent, cost-effective operations, such as large-kernel depth-wise convolution, can be employed. The perception context can thus be enlarged with minimal overhead. (3) For the aggregation with small surrounding region, we can adopt adaptive weighted feature summation. Due to the limited range of aggregation, the efficiency can be guaranteed with low computation cost and the less important aggregation in self-attention can be mitigated. Based on these, we present novel LS (Large-Small) convolution. As shown in Fig. 3.(a), for each token, it introduces two steps: (1) Large-kernel perception Pls models the neighborhood relationships with the enlarged receptive field through large-kernel static convolutions. (2) Small-kernel aggregation Als adaptively integrates the surrounding features through small-kernel dynamic convolution. Large-Kernel Perception (LKP) adopts the design of large-kernel bottleneck block. Given visual feature map RHW C, we initially utilize the point-wise convolution (PW) to project the tokens into lower channel dimension, i.e., 2 by default, to reduce the computational cost and make the model lightweight as possible. For xi, we then employ large-kernel depth-wise convolution (DW) with the kernel size of KL KL to efficiently capture large-field spatial contextual information of NKL (xi), where NKL(xi) denotes the surroundings of size KL KL centered around xi. The large-kernel DW can well expand the receptive field and enhance the context perception capability under minimal cost. We then leverage point-wise convolutions (PW) to model the spatial relationships among tokens, i.e., generating the context-adaptive weights RHW for the aggregation step. The whole process can be formulated as: wi = Pls(xi, NKL (xi)) = PW(DWKLKL (PW(NKL (xi)))), where wi RD is the generated weights for xi. (7) Small-Kernel Aggregation (SKA) employs the design of grouped dynamic convolutions. For the visual feature map RHW C, we divide its channels into groups. Each group containing channels and the channels in the same group share the aggregation weights, to reduce the memory overhead and computational cost for lightweight models. For each xi, we reshape its corresponding weights wi RD generated by large-kernel perception to obtain RGKS KS , where KS KS is the small kernel size. We then leverage to aggregate its highly related context of NKS (xi), where NKS (xi) represents the neighborhood of size KS KS centered around xi. Specifically, we denote the c-th channel of xi as xic, which belongs to the g-th channel group. We obtain its aggregated feature representation yic through the convolution operation between ig RKS KS . In this way, the adaptive NKS (xic) and fine-grained features can be effectively represented, making model sensitive to dynamic and complex changes in diverse contexts. The whole process can be formulated as: ig NKS (xic). ig, NKS (xic)) = yic = Als(w (8) In contrast to simply combining large-kernel with smallkernel conv, and other dynamic convs, our LKP utilizes enriched large-field visual perception to guide adaptive feature fusion within highly related context by SKA. This enables more discriminative representations for intricate visual information. Thus, LS conv shows superiority over them, as Figure 3. (a) The illustration of our proposed LS convolution. (b) The illustration of our proposed LSNet. LSNet has four stages with 8 64 resolutions respectively, where and denote the width and height of the input image. represents the channel dimension. The norm layer and nonlinearity are omitted for simplicity. 32 , and 64 32 16 16 , 8 , shown in Tab. 6 and Tab. 7. We also present the comparisons from mathematical perspectives in supplementary. Complexity Analysis. The computation of LS convolution mainly consists of three parts: point-wise convolutions in Pls, depth-wise convolution with kernel size of KL in Pls, and convolution aggregation with kernel size of KS in Als. Their corresponding computations are ), O( HW CK2 O( 3HW C2 S), respectively. Therefore, the total amount is O( HW (3C + 2K 2 S)), enjoying the linear computational complexity with respect to the input resolution. + (2G + 4)K 2 ), and O(HW CK 2 + HW CD 2 4 4 3.3. LSNet: Large-Small Network Using LS convolution as the primary operation, we present the basic block, i.e., LS block, and the lightweight model design, i.e., LSNet, as shown in Fig. 3.(b). LS Block leverages LS convolution to perform effective token mixing. Skip connection is adopted to facilitate model optimization. Besides, we utilize the extra depthwise convolution and SE layer [34] to enhance model capability by introducing more local inductive bias [11, 51]. Feed forward network (FFN) is adopted for channel mixing. LSNet utilizes overlapping patch embedding [93] to project the input image into the visual feature map. For downsampling, we leverage the depth-wise and point-wise convolution to reduce the spatial resolution and modulate the channel dimension, respectively. Besides, we stack LS blocks in the top three stages. In the last stage, we adopt the MSA block to capture long-range dependencies due to the small resolution, following [61, 80]. MSA block incorporates multi-head self-attention (MHSA), and we utilize the same depth-wise convolution and SE layer to introduce more local structural information like LS block. We build three LSNet variants for different computational budgets. The LSNet with tiny size (LSNet-T), small size (LSNet-S), and base size (LSNet-B) has 0.3G, 0.5G, and 1.3G FLOPs, respectively. Following [23, 51], we employ more blocks in late stages, due to that processing on early stages with higher resolution is more time consuming. We empirically use KL = 7, KS = 3, and = 8 for all model variants by default, following [14, 53]. The architectural details can be found in the supplementary. 4. Experiments 4.1. Image Classification We conduct experiments on ImageNet-1K [12] under the same training recipe as [36, 51, 64] to assess the performance of LSNet on the image classification task. As shown in Tab. 1, we note that LSNet consistently achieves state-of-the-art performance across various computational costs. Besides, it shows the best trade-offs between accuracy and inference speed. For example, our LSNet-B outperforms the advanced AFFNet by 0.5% top-1 accuracy with nearly 3 faster inference speed. It also surpasses RepViT-M1.1 and FastViT-T12 with 0.9% and 1.2% top-1 accuracies with higher efficiency, respectively. For smaller models, our LSNet also obtains superior performance with lower computation costs. Specifically, LSNetS outperforms UniRepLKNet-A and FasterNet-T1 signifTable 1. Classification results on ImageNet-1K. The throughput is tested on Nvidia RTX3090 with maximum power-of-two batch size that fits in memory, following [36, 51]. * denotes the results with distillation using the RegNetY-16GF [68] with 82.9% top-1 accuracy as the teacher model. EFormer denotes EfficientFormer. Model EdgeNeXt-XXS [59] FasterNet-T0 [5] ShuffleNetV2 [57] AFFNet-ET [36] EfficientViT-M3 [51] StarNet-S1 [58] LSNet-T LSNet-T* EdgeNeXt-XS [59] PVT-Tiny [88] MobileNetV3-L [32] FastViT-T8 [80] EFormerV2-S0* [47] FasterNet-T1 [5] UniRepLKNet-A [16] EfficientNet-B0 [75] PoolFormer-S12 [97] SHViT-S3 [98] RepViT-M0.9 [85] LSNet-S LSNet-S* EdgeViT-XS [64] SwiftFormer-S* [72] UniRepLKNet-F [16] FastViT-T12 [80] EFormer-L1* [46] EdgeNeXt-S [59] RepViT-M1.1 [85] PVT-Small [88] AFFNet [36] LSNet-B LSNet-B* Params (M) FLOPs (G) Throughput (img/s) Top-1 (%) 1.3 3.9 3.5 1.4 6.9 2.9 11.4 11.4 2.3 13.2 5.4 3.6 3.5 7.6 4.4 5.3 12.0 14.2 5.1 16.1 16.1 6.7 6.1 6.2 6.8 12.3 5.6 8.2 24.5 5.5 23.2 23. 0.3 0.3 0.3 0.4 0.3 0.4 0.3 0.3 0.5 1.9 0.2 0.7 0.4 0.9 0.6 0.4 1.8 0.6 0.8 0.5 0.5 1.1 1.0 0.9 1.4 1.3 1.3 1.3 3.8 1.5 1.3 1.3 5089 14467 9593 2877 14613 5034 14708 14708 3118 2125 7921 3909 1329 8660 3931 4481 2769 8993 4817 9023 9023 2751 3376 3209 2586 3280 2128 3604 1160 1355 3996 71.2 71.9 72.6 73.0 73.4 73.5 74.9 76.1 75.0 75.1 75.2 75.6 75.7 76.2 77.0 77.1 77.2 77.4 77.4 77.8 79.0 77.5 78.5 78.6 79.1 79.2 79.4 79.4 79.8 79.8 80.3 81.6 icantly by 0.8% and 1.6% top-1 accuracies, respectively, along with higher throughput. Compared with StarNet-S1 and EfficientViT-M3, LSNet-T also improves the top-1 accuracy by 1.4% and 1.5%, respectively. These results well show the effectiveness and efficiency of our LSNet models. 4.2. Downstream Tasks Object Detection and Instance Segmentation. We evaluate LSNet on object detection and instance segmentation tasks to verify its transferability. Following [51, 64], we integrate LSNet into RetinaNet [50] and Mask R-CNN [27] and conduct experiments on COCO-2017 [49]. As shown in Tab. 2, our LSNet consistently shows superior performance compared with competitor models. Specifically, in the RetinaNet framework for object detection, LSNet-T outperforms StarNet-S1 by 0.6 AP and 1.3 AP50 under notably less computational cost. For large models, our LSNet-B also surpasses PoolFormer-S12 and PVT-Tiny with considerable margins of 3.0 AP and 2.5 AP, respectively. When integrated into the Mask R-CNN framework for object detection and instance segmentation, LSNet-S obtains the favorable improvements of 0.5 APb and 2.5 APb over SHViT-S3 and EfficientViT-M5, respectively. Compared with RepViTM1.1, LSNet-B also achieves 1.0 higher APb and 0.6 higher APm, demonstrating the superiority in transferring. Semantic Segmentation. We evaluate LSNet on the semantic segmentation task by conducting experiments on ADE20K [106]. Following [46, 64], we incorporate LSNet in the Semantic FPN [39] segmentation model. As shown in Tab. 3, LSNet performs clearly better in all comparisons across different model scales. It can achieve superior performance under low computational costs. Specifically, LSNet-T significantly outperforms VAN-B0 by 1.6 mIoU, and it also achieves 2.9 higher mIoU over PVTv2-B0. For larger models, LSNet-S obtains the improvements of 0.4 mIoU and 1.0 mIoU over the advanced RepViT-M1.1 and SHViT-S3, respectively, with lower computational complexity. Additionally, LSNet-B surpasses SwiftFormer-L1 and FastViT-SA24 by margins of 1.6 and 2.0 mIoUs respectively. These results further show the efficacy of LSNet. 4.3. Robustness Evaluation We conduct robustness evaluation for LSNet on various benchmarks, including ImageNet-C [28], ImageNet-A [30], ImageNet-R [29], and ImageNet-Sketch [87]. Following [53, 60, 80], we report mean corruption error (lower is better) for ImageNet-C and top-1 accuracies for other datasets. As shown in Table 4, LSNet shows strong domain generalization capabilities and promising robustness to corruptions, achieving state-of-the-art performance. For example, compared with UniRepLKNet-A, LSNet-B exhibits 1.3 mCE reduction on ImageNet-C, along with top-1 accuracy gains of 1.2%, 1.5%, and 1.5% on ImageNet-A, ImageNet-R, and ImageNet-Sketch, respectively. LSNetT also outperforms StarNet-S1 significantly by 2.2% and 3.7% on ImageNet-A and ImageNet-Sketch, respectively, highlighting the robust generalization ability. 4.4. Model Analyses We conduct experiments to analyze the design elements in LSNet on ImageNet-1K. Following [23, 51], all models are trained for 100 epochs for limitations in training time and computation resource. LSNet-T is employed for analyses, with KL = 7, KS = 3 and C/G = 8, by default. Effectiveness of LS convolution. We analyze the effectiveness of our proposed LS convolution by first comparing it with w/o LS conv., in which all LS convolutions are Table 2. Object detection and instance segmentation results on COCO. APb and APm indicate bounding box AP and mask AP, respectively. Following common convention [80], FLOPs (G) of backbone is measured on image crops of 512512."
        },
        {
            "title": "RetinaNet",
            "content": "Mask R-CNN MobileNetV2 [71] MobileNetV3 [32] FairNAS-C [9] EfficientViT-M4 [51] StarNet-S1 [58] LSNet-T ResNet18 [26] DFvT-T [22] EfficientViT-M5 [51] SHViT-S3 [98] LSNet-S ResNet50 [26] PVT-Tiny [88] PoolFormer-S12 [97] FasterNet-S [5] FastViT-SA12 [80] RepViT-M1.1 [85] LSNet-B 1.6 1.1 1.7 1.6 2.2 1.5 9.5 6.9 2.8 3.0 2. 21.4 11.8 9.5 23.8 7.7 7.0 6.2 AP 28.3 29.9 31.2 32.7 33.6 34.2 31.8 - 34.3 36.1 36.7 36.3 36.7 36.2 - - - 39.2 AP50 AP75 APS APM APL APb APb 48.3 30.7 46.7 48.6 33.3 49.3 51.2 34.4 50.8 54.4 35.3 52.2 56.1 36.0 53.3 57.0 37.1 54. 29.3 30.8 32.7 34.1 35.1 35.2 38.1 41.1 42.3 46.0 47.0 48.5 29.6 29.2 31.8 32.8 33.8 35.0 14.8 14.9 16.3 17.6 18.3 17.8 50 APb 31.5 30.3 33.8 34.5 35.5 37.3 75 APm APm 45.2 27.2 45.5 27.1 48.3 29.4 51.2 31.0 52.9 31.9 53.8 32. 50 APm 75 28.6 28.2 31.0 32.2 33.4 34.3 49.6 - 54.2 56.6 57.2 55.3 56.9 56.2 - - - 60.0 33.6 - 36.1 38.0 38.6 38.6 38.9 38.2 - - - 41.5 16.3 - 18.0 19.9 20. 19.3 22.6 20.8 - - - 22.1 34.3 - 36.9 39.1 39.7 40.0 38.8 39.1 - - - 43.0 43.2 - 48.2 50.8 51.8 48.8 50.0 48.0 - - - 52.9 34.0 34.8 34.9 36.9 37. 38.0 36.7 37.3 39.9 38.9 39.8 40.8 54.0 56.9 57.0 59.4 59.9 58.6 59.2 59.0 61.2 60.5 61.9 63.4 36.7 37.0 37.0 39.6 39.8 41.4 39.3 40.1 43.6 42.2 43.5 44.0 31.2 32.6 32.8 34.4 34. 34.4 35.1 34.6 36.9 35.9 37.2 37.8 51.0 53.7 53.7 56.3 56.8 55.1 56.7 55.8 58.1 57.6 58.8 60.5 32.7 34.5 34.6 36.1 36.6 36.7 37.3 36.9 39.7 38.1 40.1 40.1 Table 3. Semantic segmentation on ADE20K. Following [80], FLOPs (G) of backbone are measured on image crops of 512512. Table 4. Robustness evaluation results on benchmark datasets, where we report mCE for ImageNet-C and top-1 accuracies for ImageNet-A, ImageNet-R, and ImageNet-Sketch. Backbone FLOPs mIoU Backbone FLOPs mIoU StarNet-S1 MobileNetV3 PVTv2-B0 VAN-B0 LSNet-T EdgeViT-XXS SHViT-S3 FastViT-SA12 RepViT-M1.1 LSNet-S 2.2 1.1 3.8 4.5 1.5 3.2 3.0 7.7 7.0 2.6 36.0 37.0 37.2 38.5 40.1 39.7 40.0 38.0 40.6 41.0 EFormer-L1 PVT-Small PoolFormer-S24 FastViT-SA24 EdgeViT-XS SwiftFormer-L1 Swin-T EFormerV2-S2 PVTv2-B1 LSNet-B 6.8 23.1 17.8 15.0 6.3 8.3 25.6 7.3 12.8 6.2 38.9 39.8 40.3 41.0 41.4 41.4 41.5 42.4 42.5 43.0 replaced with identity functions. As shown in Tab. 5, our LS convolution improves 2.3% top-1 accuracy with only 0.02G FLOPs increase compared with w/o LS conv.. Furthermore, we compare our LS convolution with other effective token mixing methods by directly replacing all LS convolutions with others. As shown in Tab. 5, LS convolution achieves superior performance with low computational costs. By employing other methods, the top-1 accuracy consistently decreases. Compared with (S)W-SA [52], SDTA [59], and LSRA [89], LS convolution obtains improvements of 0.8%, 1.0%, and 1.1% top-1 accuracies, respectively, with fewer FLOPs. Besides, LS convolution outperforms RepMixer [80] and CGA [51] by 1.9% and 1.1% top-1 accuracies, respectively. Meanwhile, we compare our LS convolution with other dynamic convolutions by simply replacing the LS convolution. As shown in Tab. 6, thanks Model FLOPs () FasterNet-T0 [5] EdgeNeXt-XXS [59] EfficientViT-M3 [51] StarNet-S1 [58] LSNet-T FastViT-T8 [80] PVTv2-B0 [89] EdgeNeXt-XS [59] UniRepLKNet-A [16] LSNet-S PVT-Tiny [88] PoolFormer-S12 [97] FasterNet-T2 [5] EdgeNeXt-S [59] PVTv2-B1 [89] FastViT-T12 [80] LSNet-B 0.3 0.3 0.3 0.4 0.3 0.7 0.6 0.5 0.6 0.5 1.9 1.8 1.9 1.3 2.1 1.4 1. 89.8 94.6 71.1 77.5 68.2 72.1 75.4 88.4 67.0 65.7 79.6 67.7 70.8 72.1 62.2 64.3 59.3 2.3 3.6 5.2 4.5 6.7 6.9 4.2 6.3 8.4 9. 7.9 6.9 8.7 11.9 14.6 14.0 17.3 28.6 29.5 36.1 34.1 38.5 36.8 34.2 32.5 37.9 39.4 33.9 37.7 40.5 40.1 41.8 39.9 43.1 SK 16.3 18.5 23.4 21.8 25.5 25.5 21.5 22.0 26.0 27.5 21.5 25.2 27.2 28.8 28.9 27.6 30.7 to incorporating large-field perception and small-field aggregation, LS convolution exhibits superiority in terms of accuracy and efficiency compared with other methods. For example, LS convolution surpasses CondConv [94] and DYConv [6] by considerable margins of 1.8% and 1.6% top-1 accuracies, respectively, well showing the effectiveness. Importance of large-kernel perception. We verify the effect of large-kernel perception (LKP) by first comparing Table 5. Superiority of LS conv. Table 6. Comparing other conv. FLOPs Top-1 w/o LS conv. LS conv. (S)W-SA [52] SDTA [59] LSRA [89] RepMixer [80] CGA [51] AFF [36] 0.29 0.31 0.36 0.37 0.37 0.29 0.32 0.30 69.3 71.6 70.8 70.6 70.5 69.7 70.5 69.5 FLOPs TopLS conv. CondConv [94] DY-Conv [6] Involution [43] DCD [44] CoT [45] ODConv [42] 0.31 0.29 0.29 0.31 0.29 0.37 0.29 71.6 69.8 70.0 70.3 69.8 71.1 70. Table 7. LKP and SKA. Table 8. Other designs. FLOPs Top-1 FLOPs Top-1 LSNet-T w/o LKP KL = 3 KL = 5 KL = 9 w/o SKA KS = 1 KS = 5 0.31 0.31 0.31 0.31 0.32 0.31 0.30 0. 71.6 70.5 70.9 71.2 71.5 70.1 69.6 71.6 LSNet-T C/G = 1 C/G = 4 C/G = 16 C/G = 32 w/o DW w/o SE 0.31 0.38 0.33 0.31 0. 0.31 0.31 71.6 71.7 71.6 71.3 70.9 71.1 71.3 it with w/o LKP, in which we remove the large-kernel depth-wise convolution in the LKP. As shown in Tab. 7, we can observe that the top-1 accuracy is significantly reduced by 1.1% in the absence of the large-field perception. We further investigate the impact of the large-kernel size, i.e., KL, in the LKP. As shown in Tab. 7, the model performance continues to increase as the kernel size grows larger, showing the benefit of capturing contextual information with large receptive field. Besides, the top-1 accuracy reaches saturation point around kernel size of 7, which is similar to the observations in previous works [53]. Importance of small-kernel aggregation. We show the importance of small-kernel aggregation (SKA) by first comparing it with w/o SKA, in which we leverage static depth-wise convolution with the kernel size of KS KS to directly process the outcome of LKP as the output. Note that w/o SKA is the combination of large-kernel and small-kernel convolutions. Tab. 7 presents the comparison results. We can observe that our LS convolution significantly outperforms w/o SKA by 1.5% top-1 accuracy. It highlights the superiority of our LS convolution over the simple combination of large-kernel and small-kernel convolutions. Additionally, we inspect the impact of the contextual scope of aggregation, i.e., NKS (xi), by adopting different KS in the SKA. As shown in Tab. 7, we can achieve the optimal trade-off between accuracy and computational costs under the KS of 3. It demonstrates the efficacy of adaptive aggregation in highly related surroundings. Impact of the number of groups. We inspect the impact of different numbers of groups, i.e., G, in LS conv. As increases, the number of channels with shared aggregation weights, i.e., , decreases, with higher computational Table 9. Generalization ability of LS convolution on other architectures. We simply replace 33 convolution and self-attention with LS convolution for ResNet and DeiT, respectively. Model LS conv. FLOPs (G) Top-1 (%) ResNet50 ResNet50 DeiT-T DeiT-T 4.1 2.6 1.3 0.9 78.8 80.7 72.2 73.0 costs. As shown in Tab. 8, as increases from 1 to 32, the top-1 accuracy decreases from 71.7% to 70.9%, along with the reduced computation complexity. It shows the benefit of performing different aggregation ways for varying channels, due to that they usually encode different representation subspaces and diverse semantic attributes [1]. Besides, we can observe that = 8 achieves the best balance. Impact of extra DW and SE layers. We verify the effect of the extra depth-wise convolution and SE layer by removing them separately, which are denoted as w/o DW and w/o SE, respectively. In Tab. 8, they decrease the top-1 accuracy by 0.5% and 0.3%, respectively, showing the efficacy of introducing more local structural information. Generalization of LS convolution to other architectures. We show the generalization of LS convolution by transferring it to other vision networks. Specifically, we conduct experiments on two widely recognized architectures, i.e., ResNet [26] and DeiT [78], by simply replacing their all 33 convolution, and self-attention with LS convolution, respectively. All models are trained under the same settings for 300 epochs. As shown in Tab. 9, incorporating LS convolution into ResNet50, and DeiT-T significantly improves their top-1 accuracies by 1.9%, and 0.8%, respectively, which showcases its good generalization capability. 5. Conclusion In this work, we present LSNet, novel family of lightweight vision networks that integrates the See Large, Focus Small strategy inspired by the human vision system. LSNet incorporates LS convolution, new operation that combines large-kernel perception and small-kernel aggregation, enabling efficient and accurate processing of visual information. Extensive experiments demonstrate that LSNet achieves state-of-the-art performance and efficiency trade-offs. It shows the superiority over others across diverse tasks. We hope that LSNet can serve as strong baseline and inspire further advancements in the development of lightweight and efficient vision networks. 6. Acknowledgments This work was supported by Beijing Natural Science Foundation (Nos. L223023, L247026), National Natural Science Foundation of China (Nos. 62271281, 62441235, 62021002), and the Key & Program of Xinjiang, China (2022B01006)."
        },
        {
            "title": "References",
            "content": "[1] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. Understanding the role of individual units in deep neural network. Proceedings of the National Academy of Sciences, 117(48): 3007130078, 2020. 8 [2] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. In Proceedings Yolact: Real-time instance segmentation. of the IEEE/CVF international conference on computer vision, pages 91579166, 2019. 3 [3] Hui Chen, Guiguang Ding, Zijia Lin, Sicheng Zhao, and Jungong Han. Show, observe and tell: Attribute-driven attention model for image captioning. In IJCAI, pages 606 612, 2018. 1 [4] Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, and Jungong Han. Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1265512663, 2020. 1 [5] Jierun Chen, Shiu-hong Kao, Hao He, Weipeng Zhuo, Song Wen, Chul-Ho Lee, and S-H Gary Chan. Run, dont walk: In ProChasing higher flops for faster neural networks. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1202112031, 2023. 6, 7 [6] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and Zicheng Liu. Dynamic convolution: In Proceedings of Attention over convolution kernels. the IEEE/CVF conference on computer vision and pattern recognition, pages 1103011039, 2020. 7, [7] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobileformer: Bridging mobilenet and transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52705279, 2022. 1 [8] Francois Chollet. Xception: Deep learning with depthwise In Proceedings of the IEEE conseparable convolutions. ference on computer vision and pattern recognition, pages 12511258, 2017. 3 [9] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search. In Proceedings of the IEEE/CVF International Conference on computer vision, pages 1223912248, 2021. 7 [10] Ekin Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmenIn Proceedings of tation with reduced search space. the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 702703, 2020. 13 [11] Zihang Dai, Hanxiao Liu, Quoc Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for all data sizes. Advances in neural information processing systems, 34:39653977, 2021. 5 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 2, 5, 13, [13] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19111920, 2019. 3 [14] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vggstyle convnets great again. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1373313742, 2021. 3, 5 [15] Xiaohan Ding, Xiangyu Zhang, and Guiguang Ding. Scaling up your kernels to 31x31: ReIn Proceedings of visiting large kernel design in cnns. the IEEE/CVF conference on computer vision and pattern recognition, pages 1196311975, 2022. 14 Jungong Han, [16] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan. Unireplknet: universal perception large-kernel convnet for audio video point cloud time-series and image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55135524, 2024. 6, 7 [17] Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Jungong Han. Exploring structured semantic prior for multi label recognition with incomplete labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33983407, 2023. [18] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. 3 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 3 [20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 3 [21] Qihang Fan, Huaibo Huang, Xiaoqiang Zhou, and Ran He. Lightweight vision transformer with bidirectional interaction. Advances in Neural Information Processing Systems, 36, 2024. 1, 3 [22] Li Gao, Dong Nie, Bo Li, and Xiaofeng Ren. Doubly-fused vit: Fuse information from vision transformer doubly with local representation. In European Conference on Computer Vision, pages 744761. Springer, 2022. 7 [23] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herve Jegou, and Matthijs Douze. Levit: vision transformer in convnets clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1225912269, 2021. 5, [24] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. survey on vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45(1):87110, 2022. 1 [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 37(9):19041916, 2015. 1 [26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 1, 7, 8, 13 [27] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross GirIn Proceedings of the IEEE internashick. Mask r-cnn. tional conference on computer vision, pages 29612969, 2017. 6, 16 [28] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. 6, 13 [29] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generIn Proceedings of the IEEE/CVF International alization. Conference on Computer Vision, pages 83408349, 2021. 6, 13 [30] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1526215271, 2021. 6, [31] Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, and Jiashi Feng. Conv2former: simple transformer-style convnet for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [32] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13141324, 2019. 3, 6, 7 [33] Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 3 [34] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 71327141, 2018. 5 [35] Tao Huang, Lang Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu. Lightvit: Towards light-weight convolutionfree vision transformers. arXiv preprint arXiv:2207.05557, 2022. 1 [36] Zhipeng Huang, Zhizheng Zhang, Cuiling Lan, Zheng-Jun Zha, Yan Lu, and Baining Guo. Adaptive frequency filters as efficient global token mixers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60496059, 2023. 1, 3, 5, 6, 8, [37] Jost B. Jonas, Ulrike Schneider, and Gottfried O. H. Naumann. Count and density of human retinal photoreceptors. Graefes Archive for Clinical and Experimental Ophthalmology, 230(6):505510, 1992. 2 [38] Jeong-Sik Kim and Seung-Woo Lee. Peripheral dimming: new low-power technology for oled display based on gaze tracking. IEEE Access, 8:209064209073, 2020. 2 [39] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr In ProceedDollar. Panoptic feature pyramid networks. ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 63996408, 2019. 6, 16 [40] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. 1 [41] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. 1 [42] Chao Li, Aojun Zhou, and Anbang Yao. Omni-dimensional dynamic convolution. arXiv preprint arXiv:2209.07947, 2022. [43] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei Zhu, Tong Zhang, and Qifeng Chen. Involution: Inverting the inherence of convolution for visual recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1232112330, 2021. 3, 8, 14 [44] Yunsheng Li, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Ye Yu, Lu Yuan, Zicheng Liu, Mei Chen, and Nuno Vasconcelos. Revisiting dynamic convolution via matrix decomposition. arXiv preprint arXiv:2103.08756, 2021. 8 [45] Yehao Li, Ting Yao, Yingwei Pan, and Tao Mei. Contextual transformer networks for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45 (2):14891500, 2022. 8 [46] Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. Efficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems, 35: 1293412949, 2022. 1, 3, 6, 13 [47] Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Rethinking vision transformers for mobilenet size and speed. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1688916900, 2023. 6, 13 [48] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. 1, 3 [49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, 6, 13, 16 [50] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. 6 [51] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient vision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014430, 2023. 1, 3, 5, 6, 7, 8, 13, 14 [52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 1, 7, 8 [53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976 11986, 2022. 1, 5, 6, 8, 13 [54] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 34313440, 2015. 3 [55] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 13 [56] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolutional neural networks. Advances in neural information processing systems, 29, 2016. [57] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In Proceedings of the European conference on computer vision (ECCV), pages 116131, 2018. 3, 6 [58] Xu Ma, Xiyang Dai, Yue Bai, Yizhou Wang, and Yun Fu. Rewrite the stars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56945703, 2024. 6, 7 [59] Muhammad Maaz, Abdelrahman Shaker, Hisham Cholakkal, Salman Khan, Syed Waqas Zamir, Rao Muhammad Anwer, and Fahad Shahbaz Khan. Edgenext: efficiently amalgamated cnn-transformer architecture for In European Conference on mobile vision applications. Computer Vision, pages 320. Springer, 2022. 1, 3, 6, 7, 8 [60] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision transformer. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 1204212051, 2022. 6, 13 [61] Sachin Mehta and Mohammad Rastegari. Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021. 1, 3, 5 [62] Sachin Mehta and Mohammad Rastegari. Separable selfattention for mobile vision transformers. arXiv preprint arXiv:2206.02680, 2022. 3 [63] G. Østerberg. Topography of the Layer of Rods and Cones in the Human Retina. A. Busck, 1935. 2, 4 [64] Junting Pan, Adrian Bulat, Fuwen Tan, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and Brais Martinez. Edgevits: Competing light-weight cnns on mobile devices with vision transformers. In European Conference on Computer Vision, pages 294311. Springer, 2022. 1, 3, 5, 6, 13 [65] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel mattersimprove semantic segmentation by global convolutional network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 43534361, 2017. 3 [66] Dale Purves, George Augustine, David Fitzpatrick, Lawrence Katz, Anthony-Samuel LaMantia, James McNamara, and Mark Williams. Anatomical distribution of rods and cones. In Neuroscience. 2nd edition. Sinauer Associates, 2001. 2, 4 [67] Shengju Qian, Yi Zhu, Wenbo Li, Mu Li, and Jiaya Jia. What makes for good tokenizers in vision transformer? IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):1301113023, 2022. 1 [68] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10428 10436, 2020. [69] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34: 1393713949, 2021. 1, 3 [70] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 3 [71] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted In Proceedings of the residuals and linear bottlenecks. IEEE conference on computer vision and pattern recognition, pages 45104520, 2018. 3, 7 [72] Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Swiftformer: Efficient additive attention for transformer-based real-time mobile vision applications. arXiv preprint arXiv:2303.15446, 2023. 6 [73] Emma E. M. Stewart, Matteo Valsecchi, and Alexander C. Schutz. review of interactions between peripheral and foveal vision. Journal of Vision, page 2, 2020. 2 [74] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1263512649, 2023. 1 [75] Mingxing Tan and Quoc Le. Efficientnet: Rethinking In Inmodel scaling for convolutional neural networks. ternational conference on machine learning, pages 6105 6114. PMLR, 2019. 3, 6 [76] Alexandra Tikidji-Hamburyan, Katja Reinhard, Riccardo Storchi, Johannes Dietter, Hartwig Seitter, Katherine E. Davis, Saad Idrees, Marion Mutter, Lauren Walmsley, Robert A. Bedford, Marius Ueffing, Petri Ala-Laurila, Timothy M. Brown, Robert J. Lucas, and Thomas A. Munch. Rods progressively escape saturation to drive visual responses in daylight conditions. Nature Communications, 8(1), 2017. 2, [77] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34: 2426124272, 2021. 1 [78] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. 1, 8 [79] Vipin Tyagi. Understanding digital image processing, 2018. 2 [80] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: fast hybrid vision transformer using structural reparameterization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57855795, 2023. 1, 3, 5, 6, 7, 8, 13, 14 [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1 [82] Brian Wandell. Foundations of vision. Sinauer Associates, 1995. 2 [83] Ao Wang, Hui Chen, Zijia Lin, Zixuan Ding, Pengzhang Liu, Yongjun Bao, Weipeng Yan, and Guiguang Ding. Hierarchical prompt learning using clip for multi-label classification with single positive labels. In Proceedings of the 31st ACM International Conference on Multimedia, pages 55945604, 2023. 3 [84] Ao Wang, Hui Chen, Zijia Lin, Sicheng Zhao, Jungong Han, and Guiguang Ding. Cait: Triple-win compression towards high accuracy, fast inference, and favorable transferability for vits. arXiv preprint arXiv:2309.15755, 2023. 3 [85] Ao Wang, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1590915920, 2024. 3, 6, 7 [86] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time endto-end object detection. arXiv preprint arXiv:2405.14458, 2024. 1 [87] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. 6, [88] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: versatile backbone for dense In Proceedings of the prediction without convolutions. IEEE/CVF international conference on computer vision, pages 568578, 2021. 1, 6, 7 [89] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415424, 2022. 3, 7, 8 [90] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 77947803, 2018. 1 [91] Wells-Gray, Choi, Bries, and Doble. Variation in rod and cone density from the fovea to the mid-periphery in healthy human retinas using adaptive optics scanning laser ophthalmoscopy. Eye, 30(8):11351143, 2016. 2 [92] Yu-Huan Wu, Yun Liu, Xin Zhan, and Ming-Ming Cheng. P2t: Pyramid pooling transformer for scene understanding. IEEE transactions on pattern analysis and machine intelligence, 45(11):1276012771, 2022. 1 [93] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early convolutions help transformers see better. Advances in neural information processing systems, 34:3039230400, 2021. 5 [94] Brandon Yang, Gabriel Bender, Quoc Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Advances in neural information processing systems, 32, 2019. 3, 7, 8, [95] Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao. Focal modulation networks. Advances in Neural Information Processing Systems, 35:42034217, 2022. 1 [96] Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping Zhang, and Tao Mei. Dual vision transformer. IEEE transactions on pattern analysis and machine intelligence, 45 (9):1087010882, 2023. 1 [97] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1081910829, 2022. 1, 3, 6, 7, 13 [98] Seokju Yun and Youngmin Ro. Shvit: Single-head vision transformer with memory efficient macro design. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 57565767, 2024. 6, 7 [99] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable In Proceedings of the IEEE/CVF international features. conference on computer vision, pages 60236032, 2019. 13 [100] Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 13 [101] Haokui Zhang, Wenze Hu, and Xiaoyu Wang. Parc-net: Position aware circular convolution with merits from convnets and transformer. In European Conference on Computer Vision, pages 613630. Springer, 2022. [102] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision transformer with quadrangle attention. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [103] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chunhua Shen. Topformer: Token pyramid transformer for mobile semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1208312093, 2022. 3 [104] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 68486856, 2018. 3 [105] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, pages 1300113008, 2020. 13 [106] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 2, 6, 13, 16 A. Implementation and Architectural Details A.1. Implementation Details For image classification on ImageNet-1K [12], we adopt the same training recipe as [36, 51, 80]. Specifically, we employ the standard image size of 224224 for both training and testing. All models are trained from scratch for 300 epochs. We use the AdamW optimizer [55] with cosine learning rate scheduler. The initial learning rate is set to 4103, and the total batch size is set to 2048. For data augmentation, we leverage mixup [100], RandAugment [10], CutMix [99], and random erasing [105], etc. Tab. 10 provides the training details of LSNet. For object detection and instance segmentation on COCO-2017 [49], we employ the same training setting as [46, 51, 80]. Specifically, we utilize the AdamW optimizer and train the model for 12 epochs with batch size of 16. The training resolution is 1333800 and the initial learning rate is set to 2104. The learning rate decays with rate of 0.1 at the 8-th and 11-th epochs. We initialize the backbones with the pretrained ImageNet-1K weights. For semantic segmentation on ADE20K [106], following [47, 80], all models are trained for 40K iterations by the AdamW [55] optimizer with batch size of 32. We adopt the poly learning rate schedule with the power of 0.9 and the initial learning rate of 2104, like [47, 80]. We employ the training resolution of 512512 and report the single scale testing results on the ADE20K validation set, as in [64, 97]. The backbone models are initialized with the pretrained weights on ImageNet-1K. For robustness evaluation, following [53, 60, 80], we employ the ImageNet-C [28], ImageNet-A [30], ImageNetR [29], and ImageNet-Sketch [87] benchmarks. Specifically, ImageNet-C consists of algorithmically generated test set. corruptions that are applied to the ImageNet ImageNet-A contains naturally occurring examples misclassified by ResNets [26]. ImageNet-R comprises natural renditions of object classes in ImageNet, incorporating various textures and image statistics. ImageNet-Sketch includes white and black sketches of all ImageNet classes, gathered through google image queries. A.2. Architectural Details Tab. 11 presents the architectural details of LSNet variants, which are distinguished by the number of blocks and the number of channels within each stage. B. More Comparisons We present more comparisons between LS convolution and others from mathematical perspectives. Specifically, for simply combining large-kernel with small-kernel convolutions, it follows the similar perception Pconv and aggregation Aconv processes as the standard convolution, i.e., Table 10. Training details on ImageNet-1K. Model optimizer batch size training epochs LR schedule learning rate warmup epochs weight decay augmentation random erase color jitter mixup cutmix gradient clip label smooth LSNet-T/S/B AdamW 2048 300 cosine 0.004 5 0.025/0.025/0.05 RandAug(9, 0.5) 0.25 0.4 0.8 1.0 0.02 0. Table 11. Architectural details of LSNet variants. Stage Resolution Type Config 2 2 4 4 8 8 8 8 16 16 32 32 64 64 Convolution channels Convolution channels Convolution channels LS Block LS Block LS Block MSA Block channels blocks channels blocks channels blocks channels blocks stem 1 2 3 LSNet 24 48 96 96 1 192 2 320 8 448 10 16 32 64 64 0 128 2 256 8 384 10 64 128 128 4 256 6 384 8 512 10 leveraging relative positions for relationship modeling and static kernel weights for feature integration. However, compared with LS convolution, it suffers from the limited modeling capability due to the lack of adaptability for different contexts. In other dynamic ways, Involution [43] leverages MLP for perception Pinv to derive the aggregation weights conditioned on xi. Its aggregation Ainv then use the weights to convolve the features in NK(xi) with the process of yi = Ainv(Pinv(xi), NK(xi)) = MLP(xi) NK(xi). Although the aggregation process is dynamic, its perception process is confined to xi, which leads to inadequate neighborhood relationship modeling compared with LS convolution. Additionally, CondConv [94] proposes per-example routing with global average pooling and MLP to linearly combining multiple convolution kernels for the aggregation weights in its perception Pcond. Its aggregation Acond then convolves the features in NK(xi) with the weights. Its process yi = Acond(Pcond(X), NK(xi)) can be formulated as yi = ((cid:80) MLP(GAP(X)) Wcond) NK(xi). HowFigure 4. Visualization of the effective receptive field. Best viewed when zoomed in. (a) and (b) show that RepMixer and CGA exhibit unnatural patterns in the effective receptive field. (c) illustrates that LS convolution enables broad peripheral perception and central view focusing simultaneously. (d) shows that without LKP, LS convolution presents smaller receptive field compared with (c), indicating the effectiveness of LKP. ever, unlike LS convolution, CondConv leverages exampledependent perception, which prevents distinct tokens to adapt to diverse contexts. C. Qualitative Analyses C.1. Analyses for LS Convolution We present the visualization analyses to qualitatively show the effectiveness of LS convolution. Specifically, we employ the effective receptive field [15, 56] method to compare LS convolution with convolution and self-attention, based on LSNet-T. We introduce the state-of-the-art RepMixer [80] and CGA [51] as the representatives of convolution and self-attention, respectively. Besides, we simply replace all LS convolutions in the model with others. As shown in Fig. 4, RepMixer and CGA suffer from the unnatural patterns, caused by static convolution kernels and window-based self-attention, respectively. In contrast, LS convolution enjoys both central area focusing and extensive peripheral viewing, showing smooth visual processing. Meanwhile, compared with w/o LKP where the largekernel depth-wise convolution in the LKP is removed, LS convolution exhibits an enlarged effective receptive field. It is attributed to the ability of LKP to efficiently capture broad contextual information. Furthermore, we conduct visualization for the aggregation weights in LS convolution. Specifically, we obtain Figure 5. Visualization of the aggregation weights in LS convolution. The second row shows that the aggregation weights are well correlated with semantic relevant areas. The third row indicates that integrating LKP enables LS convolution to capture more precise visual patterns with improved contextual information. Figure 6. Visualization of the feature maps of LKP and SKA. The second column in each part shows that LKP can encompass broad view of the scene. The third column in each part indicates that based on LKP, SKA can further grasp more subtle features and detailed patterns. the cumulative value of the aggregation coefficients corresponding to each token in all aggregation processes it is involved in. We then visualize the average of the absolute values of all channels in the last layer at the third stage and perform upsampling for display. As shown in Fig. 5, the aggregation weights of SKA enjoy favorable interpretability. They effectively strengthen semantically relevant vision regions and accurately capture discriminative patterns in images. Besides, compared with w/o LKP, LS convolution exhibits more precise emphasis on important visual areas, showcasing the improved modeling of spatial relationships facilitated by LKP. Based on LKP and SKA, LS convolution Figure 7. Qualitative results for object detection and instance segmentation on COCO-2017 [49]. Figure 8. Qualitative results for semantic segmentation on ADE20K [106]. The upper row shows the ground truth masks, and the lower row presents the predicted masks. can thus help the model to grasp the critical visual information under limited computational costs, enhancing both efficiency and effectiveness. Besides, we also visualize the feature maps generated by the LKP and SKA for more inspection. Specifically, we use the features after the large-kernel depth-wise convolution and the small-kernel dynamic convolution in the first stage for demonstration. As shown in Fig. 6, the feature maps produced by LKP exhibit broad receptive field, capturing wide range of contextual information in the scene. This characteristic is reminiscent of the human peripheral vision system, adept at sensing the general surroundings. On the other hand, based on LKP, SKA further demonstrates the ability to grasp finer details within the image. It can result in more subtle features like gradients of hairs and clear outlines. This behavior is analogous to the human central vision system, which excels at discerning fine details and high-resolution information. Thanks to them, LS convolution can well help the model achieve the effective and efficient perception and aggregation processes. C.2. Analyses for Downstream Tasks We present the qualitative results when integrating LSNet into the Mask-RCNN framework [27] for object detection and instance segmentation tasks, and into the Semantic FPN framework [39] for the semantic segmentation task. As illustrated in Fig. 7, the model can achieve precise detection and segmentation of instances in diverse images. Besides, as shown in Fig. 8, the model demonstrates the ability to generate high-quality semantic segmentation masks. D. Contribution, Limitation, and Impact Contribution. In summary, our contributions are threefold, as follows: 1. We advocate new strategy See Large, Focus Small, inspired by the human vision system, for lightweight and efficient network design. By encompassing broad perceptual range with enriched contextual information, it facilitates focused feature aggregation, fostering detailed visual understanding. 2. We propose LS convolution as novel operation for modeling visual features in lightweight models. LS convolution integrates large-kernel perception and smallkernel aggregation, enabling proficient processing of visual information through both effective and efficient perception and aggregation processes. 3. We present new family of lightweight vision networks, namely LSNet, which is built on LS convolution. Extensive experiments demonstrate that LSNet achieves the state-of-the-art performance and efficiency tradeoffs compared with other lightweight networks across broad range of vision tasks. Limitation. Due to the limited computational resources, we do not extend the application of our LSNet to other scenarios, such as visual-language tasks or unsupervised learning. We do not investigate the pretraining of LSNet on large-scale datasets, e.g., ImageNet-21K [12], due to the same reason. However, we are enthusiastic about exploring more applications for LSNet in the future. Societal Impact. We observe that this study is purely academic, and we have not identified any direct negative social impact resulting from our work. Nevertheless, we acknowledge the potential for malicious use of our models, which is concern that affects the field. While we believe that it should be mitigated, discussions concerning this matter are beyond the scope of this paper."
        }
    ],
    "affiliations": [
        "BNRist, Tsinghua University",
        "Department of Automation, Tsinghua University",
        "School of Software, Tsinghua University"
    ]
}