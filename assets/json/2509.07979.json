{
    "paper_title": "Visual Representation Alignment for Multimodal Large Language Models",
    "authors": [
        "Heeji Yoon",
        "Jaewoo Jung",
        "Junwan Kim",
        "Hyungyu Choi",
        "Heeseong Shin",
        "Sangbeom Lim",
        "Honggyu An",
        "Chaehyun Kim",
        "Jisang Han",
        "Donghyun Kim",
        "Chanho Eom",
        "Sunghwan Hong",
        "Seungryong Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs."
        },
        {
            "title": "Start",
            "content": "Visual Representation Alignment for Multimodal Large Language Models"
        },
        {
            "title": "VISUAL REPRESENTATION ALIGNMENT\nFOR MULTIMODAL LARGE LANGUAGE MODELS",
            "content": "Junwan Kim2* Hyungyu Choi3 Sangbeom Lim4 Honggyu An1 Chaehyun Kim1 Sunghwan Hong5 Seungryong Kim1 Jisang Han1 Jaewoo Jung1* Heeji Yoon1* Heeseong Shin1 Donghyun Kim4 Chanho Eom3 2NYU 3Chung-Ang University 1KAIST AI https://cvlab-kaist.github.io/VIRAL 4Korea University 5ETH Zurich"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details during training. In this paper, we present VIsual Representation ALignment (VIRAL), simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from the input vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments demonstrate consistent improvements across all tasks on widely adopted multimodal benchmarks. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs. 5 2 0 2 9 ] . [ 1 9 7 9 7 0 . 9 0 5 2 : r Figure 1: VIsual Representation ALignment (VIRAL) preserves fine-grained visual attributes for multimodal reasoning. (a) VIRAL introduces an auxiliary regularization objective on the visual pathway to prevent MLLMs from discarding detailed attributes during training. (b) VIRAL, when trained with DINOv2 (Oquab et al., 2023) as the vision foundation model (VFM), consistently produces more accurate visually grounded responses and achieves substantial improvements over standard baselines (Liu et al., 2023) across diverse vision encoders, including CLIP (Radford et al., 2021) and SigLIPv2 (Tschannen et al., 2025). *These authors contributed equally. 1 Visual Representation Alignment for Multimodal Large Language Models"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in multimodal large language models (MLLMs) (OpenAI, 2023; Bai et al., 2023a; Team et al., 2023; Chen et al., 2024c), particularly those employing visual instruction tuning techniques such as LLaVA (Liu et al., 2023), have achieved notable success in diverse multimodal tasks. By connecting pretrained large language models (LLMs) (Touvron et al., 2023; Chiang et al., 2023; Chen et al., 2024c; Bai et al., 2025) and vision encoders (Radford et al., 2021; Chen et al., 2024c; Tong et al., 2024a) with lightweight vision-language projector, visual instruction tuning enables LLMs to understand visual context, demonstrating strong performance across diverse tasks (Chen et al., 2024a; 2025a; Li et al., 2025). Despite these successes, whether MLLMs truly achieve multimodal understanding remains an open question, as numerous studies report persistent limitations in vision-centric tasks such as object counting and spatial reasoning (Tong et al., 2024b; Qi et al., 2025; Yuksekgonul et al., 2022; Ma et al., 2023). Early approaches largely attribute these shortcomings to the visual encoder or the projector. In response, subsequent works have introduced stronger vision encoders (Lu et al., 2024; Li et al., 2024) and more expressive projectors (Liu et al., 2024; Cha et al., 2024; McKinzie et al., 2024), aiming to supply the language model with richer and more comprehensive visual representations. While they yield measurable improvements, approaches that rely solely on more powerful vision encoders or projectors are inherently constrained in scalability and efficiency. In this paper, we first revisit the conventional training paradigm of visual instruction tuning. Existing MLLMs are predominantly fine-tuned with language-modeling objective, updating both the LLM and the vision-language projector while concentrating supervision almost entirely on textual outputs (Li et al., 2024; Bai et al., 2023b; Chen et al., 2024c). As result, visual tokens receive only indirect, language-mediated supervision despite comprising substantial fraction of the multimodal input. In effect, the visual pathway remains under-supervised, motivating central question: Is the prevailing multimodal training setup adequate for capturing and preserving visual information? We hypothesize that text-only supervision encourages the model to retain only those visual details that immediately aid text prediction, discarding other potentially useful cues. For example, caption such as photo of group of people holding large flag. provides little incentive to preserve the flags color, the exact number of people, or their spatial layoutattributes needed for downstream scenarios as in examples shown in Figure 1. In short, text-only supervision aligns visual features with language efficiently (Venhoff et al., 2025; Neo et al., 2024), but does so at the cost of losing the richer and more structured representations provided by the vision encoder. To validate this hypothesis, we conduct an experiment (see Figure 2) and observe that visual representations trained under exclusive textual supervision rapidly diverge from those produced by the input vision encoder, which we refer to as visual representation misalignment. Importantly, we further demonstrate that explicitly preserving alignment with the input vision encoders representations yields substantial gains in fine-grained visual understanding. Motivated by these findings, we propose VIsual Representation ALignment (VIRAL), simple yet effective regularization strategy that directly supervises the visual pathway in MLLMs to prevent the model from discarding fine-grained visual attributes provided by the vision encoder during training. Specifically, we align the internal visual representations of the MLLMs with those of the initial vision encoder using an alignment loss based on cosine similarity. In addition, we further find that this alignment signal is much more effective when provided from stronger vision foundation models (VFMs) (Oquab et al., 2023; Kirillov et al., 2023; Yang et al., 2024; Ranzinger et al., 2024). Since VFMs are trained on vision-centric objectives, they provide rich visual representations that complement language supervision. Therefore, aligning the internal visual representations of MLLMs with those of VFMs allows the model to retain important visual details from the input vision encoder while also absorbing additional visual knowledge from VFMs, which in turn enhances its ability to reason over complex visual inputs. Through extensive experiments on widely adopted multimodal benchmarks, we show that VIRAL consistently delivers significant improvements across all tasks. We summarize our contributions as following: We show that, under the visual instruction tuning paradigm, internal visual representations in MLLMs often lose alignment with the rich features produced by vision encoders, leading to the degradation of fine-grained visual information. 2 Visual Representation Alignment for Multimodal Large Language Models We propose novel regularization strategy that explicitly aligns MLLM visual representations with informative features from pretrained VFMs, thereby preventing the loss of fine-grained attributes and enabling richer multimodal understanding. Through comprehensive experiments on standard multimodal benchmarks, we demonstrate consistent improvements, and further conduct extensive ablation studies to validate the effectiveness of our design choices."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Internal information flows in MLLMs. Recent studies (Kaduri et al., 2025; Zhang et al., 2025b) have revealed structured processing hierarchy in MLLMs for visionlanguage inputs: early layers primarily aggregate global visual context into token-level embeddings, intermediate layers capture fine-grained and spatially localized features, and later layers integrate multimodal information to facilitate response generation. Within this hierarchy, the middle layers have been shown to be particularly critical for visual understanding. Jiang et al. (2025) decompose these layers into enrichment and refinement phases, demonstrating that insufficient visual information from earlier stages propagates forward and induces object hallucination. Similarly, Kang et al. (2025) shows that only small subset of attention heads, concentrated in the middle layers, are pivotal for visual grounding. Consistent with these findings, our analysis of visual representation alignment indicates that the preservation of visual information in the middle layers is strongly associated with spatial reasoning ability, which in turn is crucial for vision-centric tasks. Improving visual information in MLLMs. While recent works have increasingly examined the internal information flow of MLLMs, most prior efforts remain concentrated on the input stageparticularly the use of frozen vision encoders. Improvements at this stage have largely focused on adopting stronger vision encoders (Kar et al., 2024; Lu et al., 2024; Shi et al., 2024; Azadani et al., 2025) or enhancing efficiency by reducing the overhead of visual tokens (Vasu et al., 2025; Yang et al., 2025; Wen et al., 2025). These advances have proven valuable, yet they primarily address the quality and efficiency of the initial visual representations, with comparatively less attention given to how visual information is processed and propagated once injected into the model. Recent efforts (Wang et al., 2024; 2025) take step further by advocating direct supervision of visual tokens, but their focus remains on endpoint supervision with less consideration of the internal information flow. Moreover, their reconstruction-based objectives, while effective for preserving low-level fidelity, are less suited for capturing the higher-level semantic abstractions required by complex reasoning tasks (Zhang et al., 2023; Tong et al., 2024a). In this context, our approach complements these directions by focusing on the internal visual representationsparticularly those in the middle layers where fine-grained semantics emerge. By aligning these intermediate features with embeddings from pretrained VFMs, we provide structured supervision that helps preserve semantically meaningful visual content throughout the model."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Multimodal large language models (MLLMs). MLLMs typically consist of pre-trained LLM LM θ() and vision encoder Vψ(), which is connected with vision-language projector Pϕ(), where θ, ψ, and ϕ denote corresponding learnable parameters. To generate answers grounded on both input image and text, the frozen vision encoder Vψ() first extracts patch-level features from an input image RHW 3 with height and width such that = Vψ(I) RN Dz, where and Dz denote the number of visual tokens and the dimension of the visual features, respectively. The subsequent projector Pϕ() maps these visual features into the language models embedding space, producing sequence of visual tokens eimg = Pϕ(z) RN D, where denotes the hidden dimension of the language model. The text sequence is tokenized and embedded into the same embedding space using the language models token embedding layer, resulting in textual embeddings etext RKD, where denotes the length of the text tokens. The language model then processes the concatenated multimodal sequence [eimg; etext] R(N +K)D and models the 3 Visual Representation Alignment for Multimodal Large Language Models causal distribution over the text tokens etext as: (cid:89) pθ,ϕ(etext 1:K eimg) = pθ,ϕ(etext etext <i , eimg). (1) During inference, the language model autoregressively generates text tokens conditioned on the visual representations, the given text prompt, and the previously generated text tokens. i=1 Training stages of MLLMs. To enable the language model to incorporate visual information, modern MLLMs typically follow two-stage training paradigm (Liu et al., 2023; 2024): visionlanguage pretraining stage followed by visual instruction tuning. Both stages share the same language-modeling objective but differ in parameter updates. During visionlanguage pretraining, only the projector parameters ϕ are optimized, while the language model parameters θ remain frozen. In contrast, visual instruction tuning jointly optimizes both ϕ and θ, enabling the language model to adapt more deeply to visual inputs. It is worth noting that both stages are trained using the same language-centric objective, which is designed to maximize the log-likelihood of the text outputs. Specifically, language modeling (LM) loss is given by: LLM = 1 (cid:88) i=1 log pθ,ϕ(etext etext <i , eimg). (2)"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "4.1 DO MLLMS UNDERGO VISUAL INFORMATION LOSS? While MLLMs ingest substantial number of visual tokens, they are typically trained with textonly language modeling loss applied to the output text tokens. Consequently, all learning signals are mediated through language supervision, and the visual representations eimg receive no pathwayspecific supervision, as illustrated in Fig. 2(a). In the absence of explicit visual supervision, we hypothesize that the model learns to prioritize only those visual features that immediately aid textual prediction, often discarding other potentially useful information. This, in turn, causes the internal visual representations to drift away from the rich features produced by the vision encoderan effect that can undermine performance on tasks requiring complex reasoning or grounding. To empirically validate this hypothesis, we measure the similarity between the internal visual representations of LLaVA (Liu et al., 2024) and the original visual features extracted by its vision encoder (e.g., CLIP (Radford et al., 2021)). We adopt CKNNA (Huh et al., 2024) as metric to quantify representational similarity. As shown in Fig. 2(e), similarity to CLIP features drops sharply after the early layers and remains low in deeper layers, indicating that the models internal representations increasingly diverge from the encoders input features. This trend suggests that, without explicit visual supervision, the model has little incentive to preserve the encoders rich visual information. Interestingly, despite the overall decline in alignment, we observe modest recovery in the middle layers, suggesting that the network implicitly benefits from retaining visual representations at these depths when generating visually grounded answers. This observation aligns with prior analyses of information flow in MLLMs (Zhang et al., 2025b; Kaduri et al., 2025) and is also confirmed by our later layer-wise ablations, which show that leveraging the middle layers for vision-centric tasks shows the largest gains (see Section 5.3). 4. IS PRESERVING VISUAL INFORMATION BENEFICIAL? Residual connection with post-projection features. Having observed the mid-layer local increase in representation alignment, we ask whether explicitly preserving such visual information is beneficial. Let eimg RN denote the visual representations at the ℓ-th layer of MLLMs. As direct approach (Fig. 2(b)), we re-inject the projected visual representation Pϕ(z) into an intermediate layer of the language model via residual path: ℓ eimg eimg ℓ + Pϕ(z). (3) ℓ Visual Representation Alignment for Multimodal Large Language Models Figure 2: Re-injecting or aligning input visual features improves visual representation alignment and task performance. (ad) Comparisons of (a) baseline visual instruction tuning (Liu et al., 2023), (b) re-injecting visual features from the post-projection layer, (c) re-injecting from the preprojection layer, and (d) the proposed visual representation alignment, all applied at the 16-th layer. (e) Layer-wise alignment between visual tokens in MLLMs and vision encoder features measured by CKNNA (Huh et al., 2024), with shaded regions highlighting middle layers that are especially important for visual understanding. (f) Benchmark performance corresponding to (ad). To isolate the effect of visual information retention without introducing new supervision, the model is trained solely with the original text loss LLM. Unless otherwise stated, we set ℓ = 16 in 32-layer model LLaVA (Liu et al., 2024), following our analysis that fine-grained visual understanding emerges most prominently in middle layers supported from later layer-wise ablations (see Section 5.3). As shown in Fig. 2(e), the residual connection preserves alignment with the encoders visual features, as indicated by higher CKNNA similarity. Evaluated across standard multimodal benchmarks (Fig. 2(f)), this approach shows general improvements over the baseline, supporting the hypothesis that retaining encoder-aligned visual information benefits downstream tasks. Although residual connection provides general gains, concerns remain that the vision-language projector, Pϕ(), may not fully preserve the original visual information (Verma et al., 2024). This raises the question of whether using the encoders visual representations directly could better preserve visual information. To validate this hypothesis, we explore directions for connecting the raw encoder features directly to the language model in the following part. Residual connection with pre-projection features. We first explore connecting the input visual features to the language model via residual branch using lightweight adapter Aϕ() for dimensional compatibility. As illustrated in Fig. 2(c), we first conduct one such experiment by leveraging the vision encoder features before the projector Pϕ() and re-injecting them into eimg ℓ eimg eimg ℓ + Aϕ(z). such that (4) ℓ However, as shown in Fig. 2(f), this approach generally performs worse than the baseline. This is because the raw encoder features, which have not passed through the projector, are not sufficiently aligned with language features (Liu et al., 2023), and their direct residual connection consequently disrupts visionlanguage alignment in the intermediate layers. These findings suggest that incorporating external features into the internal visual pathway of LLMs requires more careful design. Visual Representation Alignment for Multimodal Large Language Models 4.3 VISUAL REPRESENTATION ALIGNMENT FOR MLLMS Representation alignment with encoder features. Beyond residual connection, we further explore more principled approach, which is to explicitly align intermediate visual representations with the encoder features (Yu et al., 2024); see Fig. 2(d). Let denote the frozen encoder features from Vψ() and eimg ℓ RN the visual representations at the ℓ-th layer of the MLLM. We introduce learnable projection Pπ to map eimg into the encoder feature space and define the visual representation alignment loss: ℓ LVRA = 1 N (cid:88) i=1 (cid:16) sim Pπ(eimg ℓ,i ), zi (cid:17) , (5) where sim(, ) is cosine similarity and gradients do not flow into z. Finally, the total objective augments the language modeling loss with this alignment term: Ltotal = LLM + λ LVRA, (6) with λ controlling the strength of alignment. As shown in Fig. 2(e,f), this alignment outperforms residual connection both in CKNNA similarity and on multimodal benchmarks. This indicates that directly constraining intermediate features offers more effective mechanism for preserving fine-grained visual semantics by regularizing the representation space, rather than simply forwarding encoder features, which makes the aligned features easier for the language model to integrate. Despite the general performance boost from re-injecting projected vision tower features and adopting representation alignment, notable exception is MMVP (Tong et al., 2024b), which is designed to stress cases where CLIP-like features underperform. In this setting, we observe small drop in performance, suggesting that naively propagating the vision towers features can also transmit its inductive biases and limitations. These findings naturally prompt the question of the alignment target: should the model maintain alignment with the original encoder features z, or can it be guided to preserve more informative visual semantics? While aligning to helps retain salient attributes, its utility is bounded by the encoders representational capacity. From encoder features to other VFMs. Motivated by this, we adopt stronger vision foundation models (VFMs) as teachers to supervise internal visual representations, providing richer, vision-centric targets that complement language supervision. Building on this insight, we propose VIsual Representation ALignment (VIRAL), which aligns intermediate MLLM visual representations to features from pretrained VFM, thereby preserving richer visual semantics than those available from the input encoder alone. Let E() denote pretrained VFM encoder. Given an input image I, the encoder produces target features = E(I) RN d, where is the VFM channel dimension. Let eimg RN be the MLLMs visual representations at layer ℓ, and let Pπ be learnable projection that maps eimg into the VFM feature space. We instantiate the visual representation alignment loss by replacing the encoder target in Eq. 5 with y: ℓ ℓ LVRA = 1 (cid:88) i=1 (cid:16) sim Pπ(eimg ℓ,i ), yi (cid:17) . (7) Minimizing LVRA regularizes the MLLMs internal visual pathway to align with the VFM, thereby preserving informative spatial and semantic structure. The framework is illustrated in Fig. 3. 6 Illustration of VIRAL. Figure 3: in viBuilding upon our findings sual representation alignment, we align visual pathway representation from MLLMs to strong, informative representations from VFMs to improve the vision understanding performance of MLLMs. Visual Representation Alignment for Multimodal Large Language Models Language Model Vicuna-1.5-7B Vision Encoder CLIP SigLIPv2 Qwen2.5-7B CLIP Vicuna-1.5-13B CLIP LVRA CV-Bench2D MMVP Whats Up POPE MMStar MME 56.82% 59.67% 58.90% 62.66% 58.97% 60.50% 57.51% 58.97% 40.13% 85.70% 33.93% 1650.21 28.20% 33.33% 48.55% 87.43% 33.93% 1694.52 90.13% 36.53% 1738.96 28.22% 33.11% 44.40% 90.77% 37.20% 1835.62 40.90% 85.88% 39.20% 1743.56 33.47% 36.07% 63.57% 84.92% 39.67% 1765. 59.08% 39.33% 87.12% 34.47% 1599.04 45.33% 62.26% 87.79% 37.00% 1636.62 44.44% Table 1: Effect of visual representation alignment. We compare models trained with and without LVRA across various vision encoders and LLM backbones, evaluating them on both visioncentric and general multimodal benchmarks. Our simple regularization, LVRA, combined with DINOv2 (Oquab et al., 2023), consistently improves performance across all encoders."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTINGS Implementation details. We build on the widely used LLaVA-1.5 (Liu et al., 2024), which couples Vicuna-1.5 (Chiang et al., 2023) as the language model with CLIP vision encoder (Radford et al., 2021). Following its instruction-tuning recipe, we adopt LoRA (Hu et al., 2022) for efficient adaptation; prior work reports that LLaVA-1.5 with LoRA attains performance comparable to full fine-tuning. Unless otherwise noted, we use only the original LLaVA-665K dataset (Liu et al., 2024) without any additional data. The visual-representation projector Pπ is lightweight three-layer MLP with SiLU activations. All experiments are run on four NVIDIA A100 (40 GB) GPUs. To supervise internal visual representations, we employ diverse set of pretrained vision foundation models (kept frozen, with stop-gradient on their features): DINOv2 (Oquab et al., 2023), CLIP (Radford et al., 2021), Depth Anything V2 (Yang et al., 2024), RADIO-v2.5 (Heinrich et al., 2025), and SAM (Kirillov et al., 2023). Evaluation. To demonstrate the effectiveness of VIRAL, we evaluate it on widely used benchmarks across three categories: (1) vision-centric tasks requiring spatial reasoning or object counting, including CV-Bench2D (Tong et al., 2024a), Whats Up (Chen et al., 2025b; Kamath et al., 2023), and MMVP (Tong et al., 2024b); (2) multimodal hallucination detection, using POPE (Li et al., 2023); and (3) general multimodal understanding, assessed via MME (Yin et al., 2024), MMStar (Chen et al., 2024b). This selection of benchmarks is motivated by the goals of our method: improving visual grounding should enhance performance on vision-centric and hallucination-sensitive tasks, while ensuring strong performance on general multimodal benchmarks to preserve overall capability. For evaluation, we report overall accuracy for CV-Bench2D, MMVP, Whats Up, POPE, and MMStart and total score for MME, following the respective benchmark protocols. 5.2 MAIN RESULTS The results on vision-centric benchmarks, visual hallucination tasks, and general visionlanguage evaluations are summarized in Table 1. Across identical training settings, the model trained with VIRAL consistently outperforms the baseline, with the largest gains on vision-centric tasks that demand fine-grained visual understanding. These improvements arise from simple interventionaligning intermediate MLLM features to VFM targetswhich effectively enhances the visual pathway. To test whether these gains merely compensate for the limitations of contrastive-only vision encoder, we further evaluate with SigLIPv2 (Tschannen et al., 2025), which is trained using both contrastive and visually self-supervised objectives. Even with much stronger vision encoder, adding our alignment loss yields consistent improvements. Moreover, to examine whether our method follows scaling trend and is not confined to particular LLM, we also include results with scaledup backbone, comparing Vicuna-1.5-13B against 7B, and with an alternative language backbone, 7 Visual Representation Alignment for Multimodal Large Language Models VFM Layer Index Objective CV-Bench2D MMVP Whats Up POPE MME Baseline 56.82% 28.20% 40.13% 85.70% 1650.21 Ablation studies on different VFMs 16 DINOv2 16 CLIP 16 SAM 16 DAv2 16 RADIO Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. 59.67% 57.51% 57.58% 58.55% 57.59% 33.33% 48.55% 88.32% 1694.52 87.17% 1548.49 44.50% 29.33% 49.84% 88.34% 1648.77 30.27% 88.70% 1682.42 47.29% 28.67% 88.52% 1692.94 47.35% 31.80% Ablation studies on different single-layer targets DINOv2 DINOv2 DINOv2 DINOv2 DINOv2 DINOv2 DINOv2 DINOv2 Ablation studies on different multi-layer targets DINOv2 DINOv2 Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. Cos. Sim. 58.55% 58.28% 57.77% 59.67% 55.22% 55.77% 54.87% 56.12% 4 8 12 16 20 24 28 32 Cos. Sim. Cos. Sim. 15 17 14 18 59.32% 49.62% Ablation studies on different alignment objectives 58.83% DINOv2 59.67% DINOv2 Relation Cos. Sim. 16 16 45.05% 48.32% 48.19% 87.68% 1720.36 30.67% 88.43% 1662.67 27.70% 28.59% 88.27% 1648.88 33.33% 48.55% 88.32% 1694.52 88.39% 1705.97 27.41% 88.10% 1740.55 27.48% 88.56% 1755.86 27.19% 87.32% 1678.69 26.52% 48.04% 47.99% 47.82% 47.60% 28.00% 22.55% 47.17% 42.58% 87.61% 1639.72 87.90% 1444.32 26.60% 33.33% 48.55% 49.05% 87.58% 1674.30 88.32% 1694.52 Table 2: Ablation study on key design components. We analyze the effects of (i) different vision foundation models (VFMs), (ii) alignment target layers, and (iii) alignment objectives, through evaluation on both vision-centric and general multimodal benchmarks. All experiments are conducted on the LLaVA-1.5-7B baseline. Qwen2.5-7B (Bai et al., 2025). Taken together, these findings highlight broader principle: regularizing intermediate visual representations is generally applicable strategy that strengthens MLLMs across vision encoders, model scales, and language backbones. 5.3 COMPONENT-WISE ANALYSIS In this ablation study, we conduct comprehensive analysis of key design choices underlying our framework, focusing on three core components: the selection of target visual features, the choice of alignment layer, and the formulation of alignment objectives. As in Table 2, we evaluate the impact of each component across five benchmarksCV-Bench, MMVP, Whats Up, POPE, and MMEto elucidate their respective contributions to the models performance on vision-grounded tasks. Vision foundation models. We begin by identifying the most effective target visual features for enhancing the alignment of internal visual representations within MLLMs. While residual connections and alignment with CLIP (LLaVAs original encoder) help improve visual comprehension (Figure 2), their performance on spatial tasks like MMVP is limitedlikely due to CLIPs weakness in modeling spatial relations. To address this, we evaluate several stronger vision foundation models (VFMs), including DINOv2, CLIP, Segment Anything, Depth Anythingv2, and RADIO. As shown in Table 2, our analysis confirms that aligning with stronger visual features indeed enhances visual understanding, with DINOv2 and other VFMs demonstrating improved performance compared to CLIP. Results show that DINOv2 consistently emerges as the most effective and versatile, and we thus adopt DINOv2 as the default visual foundation model for all subsequent experiments. Target layers. Next, we examine performance of aligning features at individual target layers to identify the most effective alignment position and range. As shown in the single-layer targets ablation results in Table 2, we report performance at every 4th layer throughout the network. We observe that performance varies depending on the alignment layer, with the 16th layer of the 32-layer model consistently yielding stronger results across multiple benchmarks. This trend aligns with prior findings (Zhang et al., 2025b; Kaduri et al., 2025) and our earlier analysis, suggesting that certain intermediate layers in MLLMs are more attuned to visual information processing. To investigate 8 Visual Representation Alignment for Multimodal Large Language Models Figure 4: Analysis of attention. Qualitative comparison on text-to-image attention maps (left) and quantified spatial entropy of attention across layers and heads (right). Applying visual representation alignment encourages model to attend to more contextually important content, yielding more focused and structured attention pattern. the effective number of target layers, we evaluate multi-layer targets around the 16thspecifically 1 (1517) and 2 (1418) rangesand observe that applying alignment solely at the 16th layer achieves the best performance. These findings highlight that aligning visual representations at specific pathway responsible for visual representation processing, rather than uniformly across multiple layers, is more effective in enhancing the visual understanding capabilities of MLLMs. Based on this observation, we adopt the 16th layer as the default alignment target with DINOv2. Alignment objectives. We investigate the impact of different feature alignment objectives during instruction tuning. Specifically, we compare the performance of models trained with feature relation alignment objective, as substitute for the proposed direct visual representation alignment loss. Here, the alignment objective is defined as mean squared error (MSE) loss between the self-similarity matrices of the VFM features and the transformed intermediate representations, which effectively distills the structural relationships among visual features following recent approaches (Zhang et al., 2025a; Bolya et al., 2025). As shown in Table 2, we find that cosine similarity-based alignment loss yields higher performance, and adopt it as our default strategy for alignment. 5.4 ATTENTION ANALYSIS We analyze the effectiveness of our proposed framework with visual representation alignment in terms of text-to-image attention, as shown in Figure 4. The attention map produced by the LVRA trained model exhibits more semantically aligned focus on image regions corresponding to the given textual prompts. We also quantitatively assess attention localization using spatial entropy (Batty, 1974), motivated by the findings of (Kang et al., 2025), which demonstrate that certain attention heads at specific layers are responsible for effectively localizing referred objects, and that such behavior can be identified through spatial entropy. As shown in Figure 4(right), LLaVA-1.5-7B exhibit high entropy across layers and heads, reflecting less discriminative attention patterns. In contrast, our model consistently shows lower entropyparticularly at the aligned intermediate layer where fine-grained visual information is integratedsuggesting that alignment with VFM representations enables more selective and meaningful attention. 5.5 TRAINING EFFICIENCY LVRA 1K 2K 3K 4K 5K LVRA 1K 2K 3K 4K 5K LVRA 1K 2K 3K 4K 5K 84.2% 87.5% 86.8% 83.7% 84.8% 87.3% 87.9% 87.2% 88.1% 88.0% 57.4% 52.8% 54.5% 52.9% 52.8% 48.9% 55.6% 59.9% 58.9% 59.9% 24.7% 20.0% 20.7% 22.0% 24.0% 20.4% 25.7% 27.1% 30.0% 30.2% (a) POPE (b) CV-Bench2D (c) MMVP Table 3: Training efficiency. Performance with and without LVRA evaluated every 1K steps on (a) POPE, (b) CV-Bench2D, and (c) MMVP. To further assess additional benefits of VIRAL, we evaluate model performance on vision-centric benchmarks, including POPE, CV-Bench2D, and MMVP, at every 1K training steps from the total 9 Visual Representation Alignment for Multimodal Large Language Models Figure 5: Qualitative comparison of baseline and VIRAL. The left part presents PCA visualizations of intermediate representations, demonstrating that VIRAL yields more structured, semantically meaningful visual embeddings. The right part illustrates instance counting and spatial relation tasks, highlighting scenarios where VIRAL correctly answers questions while the baseline fails. 5,195 training steps of the visual instruction tuning stage of LLaVA-1.5 in Table 3. Our results show that models trained with VIRAL achieve significantly faster convergence, demonstrating improved performance even in early training stages. This indicates that aligning intermediate visual representations with strong vision foundation models not only enhances final task accuracy but also accelerates training. 5.6 ROBUSTNESS ANALYSIS We further investigate whether our representation alignment loss enables MLLMs to better capture visual information, such as spatial relationships shown in images. Recent observations (Qi et al., 2025) reveal that MLLMs often ignore important visual cues, such as spatial positions, demonstrated by minimal performance degradation even when the order of visual tokens is randomly permuted. To address this, we evaluate whether MLLMs become more sensitive to these random permutations after training with our proposed representation alignment loss. CLIP original 374 360 374 436 400 SigLIPv2 patch shuffle Vision Enc. LVRA 26 (6.5%) 54 (13.0%) Specifically, after extracting visual features = Vψ(I) from image I, we randomly permute the visual tokens before feeding them into the language model LMθ(). We measure performance differences between the original and randomly permuted inputs in the spatial reasoning category of the CV-Bench2D dataset, as shown in Table 4. The results indicate that the original baseline, trained with text-only supervision, exhibits minimal performance drops even when provided with randomly permuted inputs. In contrast, when trained with our proposed visual representation alignment loss, the model experiences significantly larger performance degradation under random permutation, indicating enhanced sensitivity to spatial token ordering. This observation confirms that our loss successfully encourages MLLMs to better capture and utilize fine-grained spatial relationships in images, rather than disregarding them as irrelevant cues. Table 4: Robustness to token permutation. Number of correct predictions out of 788 spatial reasoning tasks in CV-Bench2D. Models with LVRA show larger performance drops under random permutation, indicating stronger sensitivity to spatial relationships. 21 (5.6%) 83 (19.0%) 353 353 5.7 QUALITATIVE RESULTS We qualitatively demonstrate the effectiveness of our proposed approach through detailed analyses of model outputs and internal visual representations. By adopting VIRAL, we observe substantial 10 Visual Representation Alignment for Multimodal Large Language Models improvements in performance on vision-centric tasks such as instance counting and understanding spatial relationships. As illustrated in Figure 5, VIRAL correctly answers challenging visual questions related to the number of objects and spatial positioning, whereas the baseline model, LLaVA-1.5-7B, frequently fails. Furthermore, by aligning internal visual representations with robust vision foundation models (VFMs), the semantic quality of intermediate representations is significantly enhanced. This improvement is clearly evidenced in the PCA visualizations shown in Figure 5. We apply PCA to the visual representations obtained from the 16-th layer of Ours and LLaVA-1.5-7B, where our method yields more structured and semantically coherent embeddings compared to the baseline. These visualizations highlight that our alignment strategy effectively guides the model to preserve critical visual details, thereby facilitating better fine-grained visual comprehension. Additional visualizations are provided in Appendix B.1 and B.2."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose VIRAL, simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those from pre-trained vision foundation models. Our approach helps preserve fine-grained visual semantics that are often discarded under text-only supervision, thereby enabling more accurate spatial reasoning and object grounding. Extensive experiments across diverse benchmarks validate the effectiveness and generality of our method, showing that visual representation alignment improves both performance and training efficiency in multimodal learning."
        },
        {
            "title": "REFERENCES",
            "content": "Honggyu An, Jin Hyeon Kim, Seonghoon Park, Jaewoo Jung, Jisang Han, Sunghwan Hong, and Seungryong Kim. Cross-view completion models are zero-shot correspondence estimators. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 11031115, 2025. Mozhgan Nasr Azadani, James Riddell, Sean Sedwards, and Krzysztof Czarnecki. Leo: BoostarXiv preprint large language models. ing mixture of vision encoders for multimodal arXiv:2501.06986, 2025. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023b. URL https://arxiv.org/abs/2308.12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Michael Batty. Spatial entropy. Geographical analysis, 6(1):131, 1974. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. Honeybee: Locality-enhanced projector for multimodal llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1381713827, 2024. Jiuhai Chen, Jianwei Yang, Haiping Wu, Dianqi Li, Jianfeng Gao, Tianyi Zhou, and Bin Xiao. Florence-vl: Enhancing vision-language models with generative vision encoder and depthIn Proceedings of the Computer Vision and Pattern Recognition Conference, breadth fusion. pp. 2492824938, 2025a. 11 Visual Representation Alignment for Multimodal Large Language Models Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024b. Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, and Manling Li. Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas. arXiv preprint arXiv:2503.01773, 2025b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, 2024c. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality. https://lmsys.org/ blog/2023-03-30-vicuna/, March 2023. Accessed: 2025-08-19. Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34:90119023, 2021. Seokju Cho, Sunghwan Hong, and Seungryong Kim. Cats++: Boosting cost aggregation with convolutions and transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45 (6):71747194, 2022. Greg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Radiov2. 5: Improved baselines for agglomerative vision foundation modIn Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 22487 els. 22497, 2025. Sunghwan Hong and Seungryong Kim. Deep matching prior: Test-time optimization for dense correspondence. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 99079917, 2021. Sunghwan Hong, Seokju Cho, Jisu Nam, Stephen Lin, and Seungryong Kim. Cost aggregation with 4d convolutional swin transformer for few-shot segmentation. In European Conference on Computer Vision, pp. 108126. Springer, 2022a. Sunghwan Hong, Jisu Nam, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong Kim. Neural matching fields: Implicit representation of matching fields for visual correspondence. Advances in Neural Information Processing Systems, 35:1351213526, 2022b. Sunghwan Hong, Seokju Cho, Seungryong Kim, and Stephen Lin. Unifying feature and arXiv preprint cost aggregation with transformers for semantic and visual correspondence. arXiv:2403.11120, 2024a. Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jisang Han, Jiaolong Yang, Chong Luo, and arXiv preprint Pf3plat: Pose-free feed-forward 3d gaussian splatting. Seungryong Kim. arXiv:2410.22128, 2024b. Sunghwan Hong, Jaewoo Jung, Heeseong Shin, Jiaolong Yang, Seungryong Kim, and Chong Luo. Unifying correspondence pose and nerf for generalized pose-free novel view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20196 20206, 2024c. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 12 Visual Representation Alignment for Multimodal Large Language Models Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 67006709, 2019. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, and Xu Yang. Devils in middle layers of large vision-language models: Interpreting, detecting and mitigating object hallucinations via attention lens. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2500425014, 2025. Omri Kaduri, Shai Bagon, and Tali Dekel. Whats in the image? deep-dive into the vision of vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1454914558, 2025. Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats up with vision-language models? investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785, 2023. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model only needs few attention heads for visual grounding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 93399350, 2025. Oguzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin Kulshrestha, Amir Zamir, and Federico Tombari. Brave: Broadening the visual encoding of vision-language models. In European Conference on Computer Vision, pp. 113132. Springer, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1091010921, 2023. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. Mm1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pp. 304323. Springer, 2024. Visual Representation Alignment for Multimodal Large Language Models Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, and Fazl Barez. Towards interpreting visual information processing in vision-language models. arXiv preprint arXiv:2410.07149, 2024. OpenAI. technical work and authors. contributions/gpt-4v/, 2023. Accessed: 2025-08-02. Gpt-4v(ision) https://openai.com/ Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Jianing Qi, Jiawei Liu, Hao Tang, and Zhigang Zhu. Beyond semantics: Rediscovering spatial awareness in vision-language models. arXiv preprint arXiv:2503.17349, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 1249012500, 2024. Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024b. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, et al. Fastvlm: Efficient vision encoding for vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1976919780, 2025. Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, and Neel Nanda. How visual representations map to language feature space in multimodal llms. arXiv preprint arXiv:2506.11976, 2025. Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, and Srijan Kumar. Cross-modal projection in multimodal llms doesnt really project visual attributes to textual space. arXiv preprint arXiv:2402.16832, 2024. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024. 14 Visual Representation Alignment for Multimodal Large Language Models Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025. Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1979219802, 2025. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2024. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36:4553345547, 2023. Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025a. Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1978119791, 2025b. 15 Visual Representation Alignment for Multimodal Large Language Models"
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "Visual foundation models. We use diverse set of pretrained VFMs to supervise internal visual representations. DINOv2 (Oquab et al., 2023), CLIP (Radford et al., 2021), and Depth Anything v2 (Yang et al., 2024) (DAv2) are used as patch size 14 models, while RADIO-v2.5 (Heinrich et al., 2025) and SAM (Kirillov et al., 2023) are used as patch size 16 models. To match the 576 visual tokens produced by CLIP-ViT-L/14 at 336336 resolution in LLaVA-1.5 (Liu et al., 2024), we adopt the same resolution for patch size 14 models and resize inputs to 384384 for patch size 16 models. For SAM, which expects 10241024 inputs, we pad the interpolated features to 10241024 and crop them to the region corresponding to the original image, following AM-RADIO (Ranzinger et al., 2024) to avoid quality degradation. Loss function and weighting. The cosine similarity sim(x, y), as done in previous works (Hong & Kim, 2021; Hong et al., 2024c; 2022b;a; 2024b;a; Cho et al., 2021; 2022; An et al., 2025), is computed as following sim(x, y) = xy . To balance the alignment loss LVRA with the language modeling loss LLM, we set λ = 0.5 by default. x2y2 Spatial Entropy. For Figure 4, we compute average spatial entropy over generated text tokens. We use questionanswer pairs from (Zhang et al., 2025b), which augment GQA (Hudson & Manning, 2019) with diverse categories and constrain answers to single word or phrase. Among these, we focus on the Relation category and report the average spatial entropy within this subset. Figure A1: Visualization of patch random permutation experiments. Patch permutation. For our patch permutation experiment, we adopt the analysis pipeline originally proposed in (Qi et al., 2025). Specifically, we begin by extracting image features from the vision encoder using = Vψ(I), where is the input image. Here, RN , with denoting the number of visual tokens and the dimensionality of the vision encoder features. Before processing the vision features with the vision-language projector Pϕ() and language model LM θ(), we apply random permutation on the order of the visual tokens , which is shown in the visualization of Figure A1. This makes it extremely difficult to understand the visual attributes of the image, enabling us to evaluate how much the MLLM was understanding and utilizing the visual attributes originally available in the image. Experiments with different vision encoders. Our method builds on the strong visual representations provided by vision foundation models (VFMs), raising natural question: does the performance gain stem mainly from these enhanced representations rather than our proposed regularization strategy? To explore this, we conduct controlled experiment using the same LLaVA-7b-1.5 architecture, substituting its original CLIP vision encoder with SigLIPv2 (Tschannen et al., 2025). We use the google/siglip2-large-patch16-384 variant, which has comparable parameter 1 Visual Representation Alignment for Multimodal Large Language Models count to the original CLIP model. Results show that while stronger vision encoders do improve overall performance, alignment with rich visual features still degrades when supervision is provided only via text. This misalignment is effectively mitigated by our proposed LVRA,ℓ loss, leading to similar performance boost."
        },
        {
            "title": "B ADDITIONAL VISUALIZATIONS",
            "content": "B.1 LAYER-WISE INTERNAL REPRESENTATIONS We present PCA visualizations of the intermediate visual representations from all layers of LLaVA1.5-7B and VIRAL in Figure A2, enabling layer-wise comparison of their representational structures. qualitative comparison with the baseline reveals that visual representation alignment regularizes the MLLMs internal visual features, leading to more semantically coherent and structured representation, especially in the middle and later layers where meaningful vision understanding emerges. B.2 VISUAL REPRESENTATIONS WITH DIFFERENT VFMS In addition to Figure 5, we qualitatively present in Figure A3 PCA visualizations of how internal visual representations evolve when aligned with different VFMs. Compared to the baseline representation from LLaVA-1.5-7B, VFM features exhibit more semantically structured organization. Aligning the MLLMs internal representations with these VFM features distills such structure, enabling the model to refer to enhanced and more coherent visual representations. B.3 ATTENTION MAP VISUALIZATIONS. In Figure A4, we provide visualizations of text-to-image cross-attention maps in the MLLM to qualitatively support the attention analysis from the main paper. Compared to the baseline, the model trained with our method exhibits improved attention behavior by focusing more accurately and locally on regions relevant to the given multimodal context. This observation aligns well with the spatial entropy analysis in Figure 4, where models trained with visual representation alignment show more focused and discriminative attention patterns. 2 Visual Representation Alignment for Multimodal Large Language Models Figure A2: Layer-wise PCA visualizations of visual representations from (a) LLaVA-1.5-7B and (b) ours. 3 Visual Representation Alignment for Multimodal Large Language Models Figure A3: PCA visualizations of 16-th layer visual representations aligned with different VFMs: CLIP, DINOv2, SAM, DAv2, and RADIO. Figure A4: Cross-attention map comparison for vision centric tasks."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "ETH Zurich",
        "KAIST AI",
        "Korea University",
        "NYU"
    ]
}