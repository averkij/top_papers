{
    "paper_title": "TTT3R: 3D Reconstruction as Test-Time Training",
    "authors": [
        "Xingyu Chen",
        "Yue Chen",
        "Yuliang Xiu",
        "Andreas Geiger",
        "Anpei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a $2\\times$ improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 5 4 6 6 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TTT3R: 3D RECONSTRUCTION AS TEST-TIME TRAINING Xingyu Chen1 Yue Chen1 Yuliang Xiu1 Andreas Geiger2 Anpei Chen1 1Westlake University 2University of Tubingen, Tubingen AI Center Figure 1: Left: CUT3R [81] encodes observations into state (memory) St1, then interacts with new observation Xt and retrieves 3D information by reading out the output token Yt. However, it suffers from the forgetting problem and degrades significantly as the number of input views increases. Right: We treat the state St as fast weight updated via gradient descent, where the learning rate βt and the gradient are predicted by the frozen slow weights. These slow weights are learned from training datasets and act as meta-learner, enabling the fast weight to serve as an associative memory. In addition, TTT3R makes online state updates by balancing the retention of historical information St1 with confidence-aware learning rate βt."
        },
        {
            "title": "ABSTRACT",
            "content": "Modern Recurrent Neural Networks have become competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing In this work, we revisit the 3D reconstruction limited length generalization. foundation models from Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive closedform learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving 2 improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in rover-xingyu.github.io/TTT3R."
        },
        {
            "title": "INTRODUCTION",
            "content": "3D reconstruction foundation models aim to predict camera poses and scene representations from set of input RGB images. Building on the sequence modeling [68], recent advances [79, 83, 92] successfully map sequences of images into pixel-aligned pointmaps [10, 11]. Among these methods, the Transformer [74] has emerged as the dominant architecture, owing to its training efficiency and ability to capture long-range dependencies. However, fundamental limitation lies in the quadratic growth of computational and memory costs with respect to sequence length. Despite various engineering optimizations, such as KV-cache compression [40] and flash attention [21], the softmax attention remains unchanged and continues to face limited scalability for long contexts [41]."
        },
        {
            "title": "Preprint",
            "content": "Real-world applications often require handling an arbitrary number of images. As Figure 2 shows, recent feed-forward methods (e.g., VGGT [79], Point3R [87]) suffer from high memory consumption. Notably, only CUT3R [81] achieves constant memory usage with RNN-based design. However, as illustrated in Figure 1, CUT3R fails to generalize to long sequences due to training on most 64-frame sequences. Motivated by these observations, we ask ourselves if there are lessons from modern RNNs that can be used as design principles for 3D reconstruction. Figure 2: GPU memory cost for inference. Recent advances in Recurrent Neural Networks (RNNs) demonstrate performance on par with Transformers on language tasks [32, 34]. Recurrent architectures compress the history context into fix-length memory state, with each output depending solely on the current state and the incoming observation. This recurrent mechanism offers two benefits: efficient processing of long sequences with linear computational complexity, and the ability to scale to longer sequences by simply rolling out the state. Nevertheless, these benefits often come at the cost of substantial performance degradation, particularly when the sequence length exceeds the training context [55, 76]. This naturally raises two questions: (1) why do these models fail to provide robust length generalization? and (2) how can length generalization be achieved? To answer these questions, several studies [8, 55, 91] have investigated the length generalization of RNNs, identifying correlations with state overfitting [82], state forgetting [16, 33], and unexplored state distributions [55]. Solutions such as training on longer sequences and employing Truncated Backpropagation Through Time (TBTT) [55, 67, 86] have been proposed to improve length generalization. While these techniques have been incorporated into recent 3D reconstruction foundation models, such as CUT3R [81], they still struggle to generalize to sequences comprising hundreds of images. In this work, we revisit the state update rule of recurrent 3D reconstruction models through the lens of Test-Time Training (TTT) [7, 65, 80], and systematically investigate the factors that hinder their ability to generalize across varying sequence lengths. Specifically, inspired by recent findings that recurrent models struggle with length generalization due to state overfitting [55], we reformulate state updating as TTT-style online learning process [7, 41, 80]. In our framework, the historical information is compressed into state online. We interpret the state as fast weight [57, 58] learned at test time from the input in-context tokens, rather than from the training dataset. This perspective provides principled understanding of state overfitting, suggesting that associative recall [9, 53] over long contexts, combined with gradient-based updates using adaptive learning rates to balance forgetting and learning [5, 6, 32, 41, 57, 66, 89], can substantially enhance length generalization. Furthermore, we find that CUT3R [81] can be interpreted as test-time training mechanism, whereas simply extending the sequence length during training leads to extremely low FLOPs utilization. Therefore, we propose simple yet effective inference-time state update rule, termed TTT3R, derived as closed-form state transition for online associative recall in CUT3R. This transition function explicitly defines the learning rate required to update the state at test time, thereby enabling length generalization. Our approach exploits internal confidence signals to selectively suppress lowquality state updates. This yields stable, training-free gating mechanism that mitigates catastrophic forgetting [35] without requiring fine-tuning or additional parameters. We evaluate TTT3R on standard 3D reconstruction benchmarks, which are typically configured with short-sequence inputs. In this setting, TTT3R performs competitively with state-of-the-art online reconstruction models [81, 87, 96] and demonstrates significant improvements with long-sequence inputs. More importantly, these gains in length generalization come at NO additional computational cost over the baseline, thanks to the proposed state update rule. Overall, we introduce new TTT-based framework to analyze the dynamics of stateful 3D reconstruction models. Based on this, we propose simple, empirical state update rule to enhance sequence length generalization for CUT3R."
        },
        {
            "title": "Preprint",
            "content": "(a) Full Attention (b) Vanilla RNN (c) Test-Time Training Figure 3: Sequence Modeling Layers. Full attention appends states, which incurs quadratic cost. In contrast, vanilla RNNs use fixed-size state with linear complexity, but they suffer from the forgetting problem. Our approach adopts Test-Time Training (TTT), treating the state as fast weights learned during test time via gradient descent with adaptive learning rates, which improves length generalization."
        },
        {
            "title": "2 RELATED WORK",
            "content": "SfM and SLAM. Structure-from-Motion (SfM) [2, 36, 50, 51, 59, 62, 63] and Simultaneous Localization and Mapping (SLAM) [22, 27, 39, 44, 45, 95] have long been the foundation for 3D structure reconstruction and camera pose estimation. These methods rely on associating 2D correspondences [4, 23, 43, 44, 56] or minimizing reprojected photometric errors [27, 28], followed by bundle adjustment (BA) [1, 11, 69, 71, 72, 78] for structure and motion refinement. Although highly effective when assembled into comprehensive systems [44, 59], these approaches often struggle in conditions of small camera parallax or ill-posed conditions (e.g., dynamic or textureless), leading to performance degradation. In this work, we investigate data-driven models with generalizable priors to enable dense 3D reconstruction even from dynamic and textureless video sequences. Offline Reconstruction Foundation Model. The pioneering feedforward 3D reconstruction method DUSt3R [83] introduced an end-to-end formulation that directly predicts two pixel-aligned pointmaps [10, 11, 61] from an image pair. By leveraging Transformer-based architecture [26] and direct point supervision on large-scale 3D datasets, DUSt3R inherently accounts for image matching [3, 15] and pose estimation [25, 42], resulting in reconstruction foundation model. Although some follow-up methods [15, 93] extend DUSt3R for robust dynamic scene reconstruction, they inherit the limitation of DUSt3R, requiring costly global alignment when the number of input views exceeds two. To address this issue, Fast3R [88] and VGGT [79] propose to use large feedforward transformer with global attention that handles multiview inputs and predicts per-view pointmaps simultaneously, without the need for post-processing, leading to state-of-the-art 3D point and camera pose reconstruction. However, relying on the full attention [74] causes quadratic increase in computational and memory cost, and results in an offline process that requires re-running inference over all images whenever new frame arrives. Instead, we aim for compute-efficient, on-the-fly streaming inference that supports long-sequence, real-time interactive, and compute-efficient applications. Online Reconstruction Foundation Model. To improve the reconstruction efficiency, several works introduce memory mechanisms to maintain information from past frames, enabling incremental reasoning and add pointmaps to canonical 3D space. StreamVGGT [96] concurrently caches historical keys and values as memory in causal transformer framework, allowing incremental processing. However, similar to full attention in VGGT [79], the computation and GPU memory usage in StreamVGGT grow redundantly. promising alternative is the use of recurrent neural network architectures [13, 18, 77], such as CUT3R [81], which maintain constant-sized memory state, while incrementally integrating new observations by simultaneously updating the state with the newly added view and retrieving historical information from the state. Although the recurrent formulation effectively reduces computational complexity and keeps inference memory usage consistently low, the memory-based methods suffer from the forgetting problem from earlier frames, leading to significant performance degradation as the number of input views increases. To mitigate the forgetting issue, Point3R [87] proposes an explicit point-based memory, where the history tokens are anchored in the reconstructed 3D point positions. While the explicit memory cache mitigates the forgetting, it causes memory cost that grows linearly as the number of views increases because the reconstructed points accumulate. In this work, we take an opposite path, exploring closed-form state update rule that enhances the length generalization of implicit state memory to reasoning over thousands of views, while keeping memory and computation costs consistently low and unchanged as input view growth. Modern RNN. Recent developments in RNN layers, serve as more efficient alternatives to quadratic complex full-attention layers [74], have demonstrated competitive performance in language modeling tasks. One line of research originates from recurrent variant of attention [34, 57], known as linear attention [34], which uses the standard inner product between the query and key rather than the"
        },
        {
            "title": "Preprint",
            "content": "exponential softmax, allowing the output to be recurrently computed in linear time. However, linear attention equally compresses all key value pairs into its finite-sized state, resulting in performance degradation as the sequence length increases. To address this limitation, various works [5, 47, 66, 89], such as Mamba [20, 32], have proposed adding forgetting gates, in which previous values are attenuated by factor before the new memory is stored, to prevent the state from diverging over time. Recently, many of these models have been cast into framework of test-time training/regression [7, 65, 80], which views the recurrent update of state as online learning [41] from context [24, 75], balancing between retaining historical memory and adapting to new information, as shown in Figure 3. The iterate states are also known as fast weights [57, 58], as they change in-context with each timestep, rapidly adapting to the input tokens. In contrast to the slow weights in neural networkswhich act as meta-learners [29] and are only adjusted during trainingfast weights are learned to function as associative memory [9, 53]. Recent examples of such layers include DeltaNet [57, 90], TTT [65, 94], and Titans [6], each of these layers was derived from specific choice of retention and adaptation. Inspired by their success, we propose to introduce general test-time training framework that enables 3D reconstruction models to achieve both view scalability and memory retention."
        },
        {
            "title": "3 METHOD",
            "content": "Our method processes continuous stream of images received online. For each incoming image It RW H3, we aim to estimate, in real time and on the fly: the camera pose Tt R34, the camera intrinsic Ct R33, and the canonical point cloud Pt RW H3. We begin in Section 3.1 by introducing sequence modeling to understand and compare different prominent classes of methods that address the pointmap regression problem. Section 3.2 then reformulates recent incremental 3D reconstruction methods from Test-Time Training perspective. Finally, in Section 3.3, we propose TTT3R, showing how the cross-attention between memory and observation can be leveraged as confidence-guided state update rule for online associative recall. 3.1 SEQUENCE MODELING FOR POINTMAP REGRESSION To transform sequence of images to pixel-aligned pointmaps that lie in unified global coordinate space, generic formulation can be written as: Xt = Tokenize(It) St = Update(St1, Xt) Yt = Read(St, Xt) Pt = De-tokenize(Yt) (1) where the input image It is patchified into set of image tokens Xt R(hw)c through an image tokenizer [26], such as DINO [14, 46] and CroCo [84, 85]. The image tokens Xt update the previous state St1 into the current state St using new information. The model then retrieves information stored in the updated state St by reading out the output token Yt R(hw)c. Following the readout operation, the corresponding pixel-aligned 3D pointmaps are extracted via dense prediction de-tokenizers, such as linear layers with pixel shuffle [60] and DPT head [54]. The camera pose Tt and the camera intrinsic Ct, can either be solved from pixel-aligned 3D pointmaps using the PnP [37] and Weiszfeld [49] algorithms, or regressed from image tokens Xt through the MLP or trunk attention layers [78, 79]. This sequence formulation offers unified perspective for interpreting pointmap-oriented 3D reconstruction foundation models, where the update and read operations serves as the core distinction among different methods. They fall into two categories: full attention-based and RNN-based, each introducing specialized designs to the update rules of sequence modeling layers. For full attentionbased methods, such as Fast3R [88] and VGGT [79], all frames interact through global all-to-all self-attention, which can be interpreted as progressive state concatenation with growing state length: Update(St1, Xt) = St1.append(KXt, VXt) Read(St, Xt) = Xt + softmax(QXtK )VSt St (2)"
        },
        {
            "title": "Preprint",
            "content": "(a) CUT3R (b) TTT3R Figure 4: TTT3R Illustration. We present training-free solution for scalable online 3D reconstruction that mitigates forgetting issue in CUT3R. (a) Vanilla CUT3R [81] pipeline. (b) Our reformulation from test-time training perspective introduces confidence-guided state update, where alignment confidence between memory and observations serves as per-token learning rates. See Eq. 8 for more details. where state St1 = [(KX1, VX1 ), . . . , (KXt1 , VXt1 )] is list of key-value pairs. Each key-value pair (KXt, VXt) and query QXt are transformed from the input token Xt via linear layers, and KSt = concat[KX1 , . . . , KXt], VSt = concat[VX1, . . . , VXt]. This modeling requires O(t2) computing complexity, since all output tokens Y1, . . . , Yt must be updated upon receiving Xt. To efficiently process streaming input, StreamVGGT [96] uses causal attention architecture to model the causal nature of streaming data, which restricts each frame to attend only to itself and preceding tokens, allowing only Yt to be updated given Xt. Causal attention enables incremental processing and reduces the computational cost to O(t). However, it shares similar limitation with full attention: the state is represented as keyvalue list that grows redundantly at O(t), leading to increasing memory consumption as the number of views increases. For RNN-based methods [13, 18, 77, 81, 87], each incoming frame interacts with the state via one-to-one cross-attention, allowing for fixed-length state: Update(St1, Xt) = St1 + softmax(QSt1 Xt (3) where the state St1 Rnc, consisting of tokens with channel dimension c, encodes the scene with constant length. QSt1 denotes the query projection obtained by applying linear transformation to the state St1. Although this recurrent formulation effectively reduces the computational complexity to O(1) and inference memory usage constant at O(1), it suffers from forgetting and exhibits significant performance degradation as the number of input views increases. )VXt 3.2 REVISIT RNN-BASED RECONSTRUCTION THROUGH TTT Test-Time Training (TTT) [65] introduces fast weights [58] as rapidly adaptable states that are updated during both training and inference to dynamically capture context. In contrast, slow weights (i.e., model parameters) remain frozen during inference. Formally, TTT represents the state as fixed-length fast weight St1 Rnc and updates it via gradient descent: Update(St1, Xt) = St1 βt(St1, Xt) (4) where (St1, Xt) is learned gradient function of the previous state St1 and the current observation Xt, aiming to encourage the network to associate the current observation with the state, and βt is the learning rate. Intuitively, this online learning process encodes the KV-cache from the current observation into fixed length of memory (i.e., state) as accurately as possible [80]. For example, linear TTT (or DeltaNet [57, 90]) minimizes the reconstruction error (St1KXt VXt)2 by optimizing the state to accurately reconstruct the observation value VXt. This objective yields an analytical gradient: (St1KXt VXt )K Xt Update(St1, Xt) = St1 β(St1, Xt) Next, we analyze the learning rate term β, which serves as the most critical hyperparameter and has been extensively studied in recent advances. It is typically represented as: 1) learnable scalar parameter β R1 in RetNet [66]; 2) an input-dependent scalar function βt = σ (ℓβ (Xt)) R1 in DeltaNet [57, 90], TTT [65], and Mamba-2 [20]; and 3) per-token function βt = σ (ℓβ (Xt)) Rn1 in Gated Linear Attention [89], which enables token-wise adaptive learning rates across all state tokens. Up to this point, we reformulate the Eq. 3 using the above TTT formulation: (5) 1. softmax(QSt1 Xt )VXt St1 + softmax(QSt1K Xt )VXt = St1 βt(St1, Xt) (6)"
        },
        {
            "title": "Preprint",
            "content": "The gradient is defined as linear combination of the observation values VXt R(hw)c, weighted ) Rn(hw) between the state query by the softmax alignment scores softmax(QSt1 Xt QSt1 Rnc and the observation key KXt R(hw)c. This formulation has been demonstrated to be effective [83] for learning emergent functional 3D/4D alignment by implicitly matching crossview context [15]. However, the softmax operation limits CUT3Rs ability to balance retaining historical information with incorporating new inputs, as it forces the model to fully adapt to the latest observations. Specifically, because softmax weights are normalized to sum to 1.0 along the observation-token dimension, the model always prioritizes new information Xt over the historical state St1, leading to catastrophic forgetting."
        },
        {
            "title": "3.3 CONFIDENCE-GUIDED STATE UPDATE RULE",
            "content": "Our core idea is to utilize alignment confidence between memory and observation to guide state updates, with this confidence serving as the learning rate in the TTT formulation. Figure 4 provides an overview of TTT3R. Recall that the cross-attention (i.e., QSt1K ) aggregates information along the spatial dimension Xt = {1, . . . , h} {1, . . . , w} of the image into state tokens. This process yields normalized attention weights for each state token, which are then used to compute weighted sum over the value tokens VXt. To address the forgetting issue, we retain the original attention formulation but introduce per-token learning rate βt Rn1, derived from the alignment confidence between the state queries QSt1 and the observation keys KXt: (7) This learning rate can act as soft gate in gated attention, incorporating it into the attention output allows for better long-context extrapolation [52]. Consequently, the full closed-form state update rule is given by: mQSt1K Xt ) βt = σ((cid:80) σ((cid:80) QSt1 Xt ) softmax(QSt1 Xt )VXt Update(St1, Xt) = St1 βt(St1, Xt) (8) Note that, rather than ignoring quality variations and updating all state uniformly - which we find leads to suboptimal performance due to lowquality state updates (e.g., textureless regions, see Figure 5) - we leverage cross-attention statistics to estimate the alignment confidence of state updates and accordingly assign per-token learning rates βt. That is, higher alignment confidence in state updates generally indicates stronger match between the state and observation with lower uncertainty, leading to larger update step in our formulation. By aggregating token-level statistics, we suppress low-quality state updates to enhance performance. similar principle - leveraging internal confidence signals to selectively filter updates - has been explored in concurrent work [30] to improve test-time reasoning for large language models. Figure 5: By incorporating image attention (i.e., Xt Rn(hw)) as per-token learning rates QSt1 βt Rn1, TTT3R mitigates catastrophic forgetting and facilitates online loop closure. This formulation enables training-free, plug-and-play intervention for CUT3R, which can be directly applied to downstream tasks without additional fine-tuning."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate our method on variety of tasks, including camera pose estimation (Section 4.1), video depth estimation (Section 4.2), and 3D reconstruction (Section 4.3). Baselines. We first compare TTT3R with the state-of-the-art online 3D reconstruction method CUT3R [81], which performs on-the-fly reconstruction with an RNN-based architecture over stream-"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Comparison of Camera Pose Estimation. Results on ScanNet [19] (left) and TUM-D [64] (right). OOM denotes the method out-of-memory beyond this point. ing images. We also evaluate against Point3R [87] and StreamVGGT [96], which extend CUT3R and VGGT [79], respectively, to longer sequences by fine-tuning with explicit pointmap memory or KV-cachebased state representations. In contrast, our approach introduces general sequence modeling framework and an adaptive state learning rate, enabling training-free solution. In the following experiments, we compare these methods in terms of reconstruction accuracy, GPU memory usage, and inference speed. See Sec. A.1 and Sec. A.2 of Sup.Mat. for more details. 4.1 CAMERA POSE ESTIMATION Following prior works [81, 93], we evaluate camera pose accuracy on TUM dynamics [64] and ScanNet [19] datasets. We adopt the standard metric, Absolute Translation Error (ATE), computed after applying Sim(3) alignment [73] between the estimated and ground-truth camera trajectories. The results of the long-sequence evaluation are shown in Figure 6. For reference, we also include VGGT, an offline method that can be considered as an upper bound for online methods, since its full attention mechanism preserves the entire history context without forgetting. We further evaluate inference efficiency in Figure 7 and Figure 2, reporting two metrics: frames per second (FPS) and peak GPU memory usage (GB). All models are evaluated on single 48GB NVIDIA GPU, with the number of input views varied from 50 to 1000 and early termination if out-of-memory occurs. As expected, VGGT and StreamVGGT, both based on full attention, are relatively slow and prone to memory exhaustion. CUT3R, in contrast, maintains consistently low GPU usage and real-time inference but struggles to retain information over long sequences, leading to inaccurate pose estimation. Point3R achieves improved accuracy over CUT3R by trading off GPU usage and runtime, but inference is slow and memory runs out beyond 700 frames. By reformulating CUT3R, our method achieves accurate pose estimation (with 2 improvement) while preserving the same inference speed and memory efficiency as CUT3R. Figure 7: Runtime comparison on ScanNet [19]. OOM denotes the method out-of-memory beyond this point. Please refer to Sec. A.3 in Sup.Mat. for qualitative comparisons of camera trajectory estimation. 4.2 VIDEO DEPTH ESTIMATION Following common practice [81, 93], we evaluate video depth estimation on the KITTI [31] and Bonn [48] datasets, which cover dynamic/static as well as indoor/outdoor scenes. We adopt standard metrics: absolute relative error (Abs Rel) and δ < 1.25 (percentage of predicted depths within 1.25-factor of true depth) . Video depth estimation measures both per-frame quality and inter-frame consistency, by aligning predicted depth maps to ground truth with per-sequence scale, thereby evaluating relative depth accuracy. For methods that predict metric pointmaps (i.e., outputs in meters with absolute scale), we also report results without scale alignment, evaluating predictions directly in metric units to assess absolute-scale accuracy."
        },
        {
            "title": "Preprint",
            "content": "(a) Scale-invariant relative depth evaluation on Bonn [48] dataset. (b) Metric depth evaluation on KITTI [31], excluding VGGT-based methods that dont support metric depth. Figure 8: Comparison of Video Depth Estimation. OOM denotes out-of-memory beyond this point. Figure 9: Comparison of 3D Reconstruction on 7-scene [61]. Figure 8 shows quantitative comparison against online baselines. VGGT and StreamVGGT run out of memory after about 150 frames due to their reliance on full attention. Nonetheless, they serve as an upper bound in per-sequence relative depth evaluation. For metric depth estimation, we report only CUT3R, Point3R, and TTT3R, as these are the only online methods predicting metric pointmaps. Point3R achieves strong scale-invariant accuracy on short sequences ( 300 frames) due to its explicit pointmap memory, but suffers from forgetting and degraded metric-scale accuracy on longer sequences. In contrast, our approach consistently improves over baselines and achieves the best overall performance, without the need of fine-tuning. See Sec. A.4 of Sup.Mat. for more results. 4.3 3D RECONSTRUCTION We follow previous work [77, 81] to evaluate 3D reconstruction on the 7-scene [61] dateset by measuring the distances between estimated pointmaps and ground-truth point clouds. As in prior work [17, 77, 81], we use chamfer distance and normal consistency as evaluation metrics. Chamfer distance is computed as the average of accuracy (nearest Euclidean distance from reconstructed point to ground truth) and completeness (the reverse). Unlike prior approaches [77, 81], which sparsely sample 35 frames per scene, we evaluate performance on long image sequences to assess the memorization capability of different models."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Qualitative Results for 3D Reconstruction. Compared to CUT3R, TTT3R improves sequence length generalization, mitigates forgetting, and enables online loop closure. Other baselines (e.g., VGGT, Point3R) are omitted due to OOM on long sequences. Check our website to see more video comparisons. Figure 9 shows that our method significantly outperforms other online approaches such as CUT3R [81] and StreamVGGT [96], and achieves results comparable to the top offline, full-attention method VGGT, while operating online in real-time with only 6GB GPU memory. This highlights the effectiveness of our method for 3D reconstruction. Figure 10 presents qualitative comparison with CUT3R. TTT3R achieves more accurate reconstructions, whereas CUT3R suffers from catastrophic forgetting, leading to drifted camera poses, broken geometry, severe distortions, and ghosting artifacts. For more 3D reconstruction results, please refer to Sec. A.5 in Sup.Mat."
        },
        {
            "title": "5 DISCUSSION",
            "content": "This paper presents TTT3R, providing Test-Time Training perspective for recent 3D reconstruction foundation models, and proposes simple yet efficient modification to CUT3R that enhances its length generalization. Our experiments demonstrate that TTT3R achieves robust long-sequence 3D reconstruction and outperforms state-of-the-art methods in most cases. The update is performed during the forward pass without model fine-tuning, making it lightweight, plug-and-play solution. Limitations. TTT3R mitigates but does not resolve state forgetting, and it has not yet matched strong offline methods (e.g., VGGT) in reconstruction accuracy, where full attention - despite being slower and more memory-demanding - preserves the entire history context. While TTT3R shows clear boost of test-time regression for associative recall, its design space remains largely unexplored. Recent work [6, 7, 80, 94] highlights vast opportunity to develop more effective, stable, and parallelizable recurrent architectures. We hope our findings will motivate future research to revisit the foundations of 3D reconstruction models and further improve the reconstruction accuracy and length generalization."
        },
        {
            "title": "Preprint",
            "content": "Acknowledgments. We thank the members of Inception3D and Endless AI Labs for their help. Xingyu Chen and Yue Chen are funded by the Westlake Education Foundation. Xingyu Chen is also supported by the Natural Science Foundation of Zhejiang province, China (No. QKWL25F0301). Andreas Geiger are supported by the ERC Starting Grant LEGO-3D (850533) and DFG EXC number 2064/1 - project number 390727645."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Sameer Agarwal, Noah Snavely, Steven Seitz, and Richard Szeliski. Bundle adjustment in the large. In ECCV, 2010. 3 [2] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. ACM Communications, 2011. 3 [3] Honggyu An, Jinhyeon Kim, Seonghoon Park, Jaewoo Jung, Jisang Han, Sunghwan Hong, and Seungryong Kim. Cross-view completion models are zero-shot correspondence estimators. arXiv, 2412.09072, 2024. 3 [4] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust features (surf). Computer vision and image understanding, 2008. 3 [5] Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. In NeurIPS, 2024. 2, 4 [6] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. 2, 4, 9 [7] Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization. arXiv preprint arXiv:2504.13173, 2025. 2, 4, 9 [8] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. arXiv preprint arXiv:2406.14528, 2024. [9] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. In NeurIPS, 2023. 2, 4 [10] Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In CVPR, 2017. 1, 3 [11] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cavallari, Aron Monszpart, Daniyar Turmukhambetov, and Victor Adrian Prisacariu. Scene coordinate reconstruction: Posing of image collections via incremental learning of relocalizer. In ECCV, 2024. 1, 3 [12] Daniel Butler, Jonas Wulff, Garrett Stanley, and Michael Black. naturalistic open source movie for optical flow evaluation. In ECCV, 2012. 17, 18, 19, [13] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. Must3r: Multi-view network for stereo 3d reconstruction. In CVPR, 2025. 3, 5 [14] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 4 [15] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. 3, 6, 17, 18, 19 [16] Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Stuffed mamba: Oversized states lead to the inability to forget. arXiv preprint arXiv:2410.07145, 2024. 2 [17] Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, and Yuliang Xiu. Feat2gs: Probing visual foundation models with gaussian splatting. arXiv, 2412.09606, 2024. [18] Zhuoguang Chen, Minghui Qin, Tianyuan Yuan, Zhe Liu, and Hang Zhao. Long3r: Long sequence streaming 3d reconstruction. arXiv preprint arXiv:2507.18255, 2025. 3,"
        },
        {
            "title": "Preprint",
            "content": "[19] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 7, 17, 18 [20] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. 4, 5 [21] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. 1 [22] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. PAMI, 2007. 3 [23] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In CVPRW, 2018. 3 [24] Benoit Dherin, Michael Munn, Hanna Mazzawi, Michael Wunder, and Javier Gonzalvo. Learning without training: The implicit dynamics of in-context learning. arXiv preprint arXiv:2507.16003, 2025. 4 [25] Siyan Dong, Shuzhe Wang, Shaohui Liu, Lulu Cai, Qingnan Fan, Juho Kannala, and Yanchao Yang. Reloc3r: Large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization. arXiv, 2412.08376, 2024. 3 [26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, [27] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In ECCV, 2014. 3 [28] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. PAMI, 2017. 3 [29] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 4 [30] Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. 6 [31] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The international journal of robotics research, 2013. 7, 8, 19, 20 [32] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 2, 4 [33] Samy Jelassi, David Brandfonbrener, Sham Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying. arXiv preprint arXiv:2402.01032, 2024. [34] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. 2, 3 [35] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. 2 [36] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 3, 17, 18 [37] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An accurate o(n) solution to the pnp problem. In ICCV, 2009."
        },
        {
            "title": "Preprint",
            "content": "[38] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with MASt3R. In ECCV, 2024. 17, 18, 19 [39] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. MegaSaM: accurate, fast, and robust structure and motion from casual dynamic videos. In CVPR, 2025. 3 [40] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. 1 [41] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space models are amortized online learners. arXiv preprint arXiv:2407.14207, 2024. 1, 2, [42] Thibaut Loiseau, Guillaume Bourmaud, and Vincent Lepetit. Alligat0r: Pre-training through co-visibility segmentation for relative camera pose regression. arXiv preprint arXiv:2503.07561, 2025. 3 [43] David Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 2004. 3 [44] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 2015. 3 [45] Richard Newcombe, Steven Lovegrove, and Andrew Davison. Dtam: Dense tracking and mapping in real-time. In ICCV, 2011. 3 [46] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [47] Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In ICML, 2023. 4 [48] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019. 7, 8, 19, 20 [49] Frank Plastria. The weiszfeld algorithm: proof, amendments, and extensions. Foundations of location analysis, 2011. [50] Marc Pollefeys, Reinhard Koch, and Luc Van Gool. Self-calibration and metric reconstruction inspite of varying and unknown intrinsic camera parameters. International Journal of Computer Vision, 1999. 3 [51] Marc Pollefeys, Luc Van Gool, Maarten Vergauwen, Frank Verbiest, Kurt Cornelis, Jan Tops, International Journal of and Reinhard Koch. Visual modeling with hand-held camera. Computer Vision, 2004. 3 [52] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. arXiv preprint arXiv:2505.06708, 2025. 6 [53] Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020. 2, 4 [54] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. 4 [55] Ricardo Buitrago Ruiz and Albert Gu. Understanding and improving length generalization in recurrent models. arXiv preprint arXiv:2507.02782, 2025."
        },
        {
            "title": "Preprint",
            "content": "[56] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In CVPR, 2020. 3 [57] Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In ICML, 2021. 2, 3, 4, 5 [58] Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 1992. 2, 4, [59] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 3 [60] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 4 [61] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR, 2013. 3, 8 [62] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In SIGGRAPH, 2006. [63] Noah Snavely, Steven Seitz, and Richard Szeliski. Modeling the world from internet photo collections. International journal of computer vision, 2008. 3 [64] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, 2012. 7, 17, 18 [65] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. 2, 4, 5 [66] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. 2, 4, 5 [67] Ilya Sutskever. Training recurrent neural networks. PhD thesis, University of Toronto, 2013. [68] Ilya Sutskever, Oriol Vinyals, and Quoc Le. Sequence to sequence learning with neural networks. In NeurIPS, 2014. 1 [69] Chengzhou Tang and Ping Tan. Ba-net: Dense bundle adjustment network. arXiv preprint arXiv:1806.04807, 2018. 3 [70] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware unified world modeling. arXiv preprint arXiv:2503.18945, 2025. 17, 18, 19 [71] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. In NeurIPS, 2021. 3 [72] Bill Triggs, Philip McLauchlan, Richard Hartley, and Andrew Fitzgibbon. Bundle adjustmenta modern synthesis. In Vision Algorithms: Theory and Practice: International Workshop on Vision Algorithms Corfu, Greece, 2000. 3 [73] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. PAMI, 1991. 7, 17 [74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1,"
        },
        {
            "title": "Preprint",
            "content": "[75] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In ICML, 2023. 4 [76] Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. 2 [77] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv, 2408.16061, 2024. 3, 5, 8, 17, 18, 19 [78] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In CVPR, 2024. 3, [79] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 1, 2, 3, 4, 7, 17, 18, 19 [80] Ke Alexander Wang, Jiaxin Shi, and Emily Fox. Test-time regression: unifying framework for designing sequence models with associative memory. arXiv preprint arXiv:2501.12352, 2025. 2, 4, 5, 9 [81] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. 1, 2, 3, 5, 6, 7, 8, 9, 17, 18, 19, 20 [82] Shida Wang. Longssm: On the length extension of state-space models in language modelling. arXiv preprint arXiv:2406.02080, 2024. 2 [83] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: geometric 3d vision made easy. In CVPR, 2024. 1, 3, 6, 17, 18, 19 [84] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Bregier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jerˆome Revaud. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. In NeurIPS, 2022. 4 [85] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. Croco v2: Improved cross-view completion pre-training for stereo matching and optical flow. In ICCV, 2023. 4 [86] Ronald Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 1990. 2 [87] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 2, 3, 5, 7, 17, 18, [88] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In CVPR, 2025. 3, 4 [89] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. 2, 4, 5 [90] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In NeurIPS, 2024. 4, 5 [91] Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, and Yingyan Celine Lin. Longmamba: Enhancing mambas long context capabilities via training-free receptive field enlargement. arXiv preprint arXiv:2504.16053, 2025. 2 [92] Jiahui Zhang, Yuelei Li, Anpei Chen, Muyu Xu, Kunhao Liu, Jianyuan Wang, Xiao-Xiao Long, Hanxue Liang, Zexiang Xu, Hao Su, et al. Advances in feed-forward 3d reconstruction and view synthesis: survey. arXiv preprint arXiv:2507.14501, 2025."
        },
        {
            "title": "Preprint",
            "content": "[93] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3R: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 3, 7, 17, 18, 19, 20 [94] Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. 4, 9 [95] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Noah Snavely, Michael Rubinstein, and William T. Freeman. Structure and motion from casual videos. In ECCV, 2022. 3, 17, 18 [96] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 2, 3, 5, 7, 9, 17, 18,"
        },
        {
            "title": "Table of Contents",
            "content": "A More Results . . A.1 Experimental Settings A.2 Baselines . . . A.3 Camera Pose Estimation . A.4 Video Depth Estimation . . 3D Reconstruction . A.5 . . . . . . . . Use of Large Language Models"
        },
        {
            "title": "A MORE RESULTS",
            "content": "A.1 EXPERIMENTAL SETTINGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 17 17 17 20 21 We present two experimental settings: (1) long sequence evaluation, to compare TTT3R with online methods that could handle hundreds of images, which is challenging setting by measuring the state capability to memorize entire sequences, rather than short video clips; and (2) short sequence evaluation, to compare the performance of our method to wide range of baselines (since these baselines, hindered by out-of-memory, are infeasible to handle long sequences). Please refer to the main paper for the long-sequence results. We report the short-sequence evaluation here in the supplement. A.2 BASELINES We first compare TTT3R with pairwise 3D reconstruction foundation models, including DUSt3R [83], MASt3R [38], MonST3R [93], and Easi3R [15], which takes pair of views as input and requires an extra global alignment stage to consolidate the pairwise predictions. We also compare with AETHER [70] and VGGT [79], which could predict all pointmaps simultaneously, without the need for post-processing, leading to state-of-the-art 3D point and camera pose reconstruction. However, relying on global alignment or full attention limits all the aforementioned methods to handling only short image sequences, in an offline reconstruction manner, where it needs to rerun inference of all images whenever new frame arrives. For online methods, we compare TTT3R with Spann3R [77] and CUT3R [81], which operates online with RNN-based architectures and could handle streaming images on the fly. For concurrent works that are most similar to our method, we compare TTT3R with Point3R [87] and StreamVGGT [96], which aim to extend CUT3R and VGGT to handle long image sequences, but take different approach by fine-tuning CUT3R and VGGT with explicit pointmap memory and KV cache as state representation, respectively. Unlike these works, our method introduce sequence modeling as general framework and reformulate CUT3R from the Test-Time Training (TTT) perspective. As result, TTT3R achieves online associative recall by deriving closed-form update rule, without requiring fine-tuning CUT3R or training extra parameterized components. A.3 CAMERA POSE ESTIMATION Following prior works [81, 93], we evaluate camera pose estimation accuracy on Sintel [12], TUM dynamics [64], and ScanNet [19] datasets. We use standard error metrics: Absolute Translation Error (ATE), Relative Translation Error (RTE), and Relative Rotation Error (RRE), after applying the Sim(3) alignment [73] on the estimated camera trajectory to the ground truth. We compare with both 3D reconstruction foundation models and prior per-sequence optimize approaches, such as RobustCVD [36] and CasualSAM [95], which jointly optimize camera parameters and dense depth maps to fit each sequence."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: In-the-wild Video Reconstruction - Long Sequence. TTT3R offers simple state update rule to enhance length generalization for CUT3R, enabling robust long-sequence 3D reconstruction. The update is performed in the forward pass without any model fine-tuning, making it plug-and-play solution, while preserving CUT3Rs inference speed and memory footprint. Check our website to see video comparisons. Sintel (50 frames) TUM-dynamics (90 frames) ScanNet (90 frames) Method Online ATE RPE trans RPE rot ATE RPE trans RPE rot ATE RPE trans RPE rot Robust-CVD [36] CasualSAM [95] DUSt3R [83] MASt3R [38] MonST3R [93] Easi3R [15] AETHER[70] VGGT [79] Spann3R [77] CUT3R [81] Point3R [87] StreamVGGT [96] TTT3R 0.360 0.141 0.417 0.185 0.111 0.110 0.189 0.172 0.329 0.213 0.351 0.251 0. 0.154 0.035 0.250 0.060 0.044 0.042 0.054 0.062 0.110 0.066 0.128 0.149 0.063 3.443 0.615 5.796 1.496 0.869 0.758 0.694 0.471 4.471 0.621 1.822 1.894 0.617 0.153 0.071 0.083 0.038 0.098 0.105 0.092 0.012 0.056 0.046 0.075 0.061 0. 0.026 0.010 0.017 0.012 0.019 0.022 0.012 0.010 0.021 0.015 0.029 0.033 0.012 3.528 1.712 3.567 0.448 0.935 1.064 1.106 0.310 0.591 0.473 0.642 3.209 0.379 0.227 0.158 0.081 0.078 0.077 0.061 0.176 0.035 0.096 0.099 0.106 0.161 0. 0.064 0.034 0.028 0.020 0.018 0.017 0.028 0.015 0.023 0.022 0.035 0.057 0.021 7.374 1.618 0.784 0.475 0.529 0.525 1.204 0.377 0.661 0.600 1.946 3.647 0.592 Table 1: Evaluation on Camera Pose EstimationShort Sequence on Sintel [12], TUM-dynamic [64], and ScanNet [19] datasets. TTT3R achieves the best overall performance among online methods, while its accuracy has not yet matched strong offline methods (e.g., VGGT), where full attention - despite being slower and more memory-demanding - preserves the entire history context. The results of the long-sequence evaluation are presented in Figure 6. Since many baselines can only process short sequences due to out-of-memory, we also show the short sequence evaluation in Table 1, and separately highlight the leading approaches for methods that operate offline (i.e., require additional optimization or global attention) and those that do not (i.e., online). Although gap persists between offline and online methods, our approach achieves the best overall performance among online methods, particularly in TUM-dynamics and ScanNet datasets. We show qualitative comparisons of the estimation of the camera trajectory in Figure 12. TTT3R demonstrates more accurate and robust camera pose estimation over CUT3R, effectively leveraging"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Visualization of Estimated Camera Trajectories Long Sequence. The trajectories are plotted along the two axes with the highest variance to capture the most significant motion. Our estimated camera trajectory TTT3R deviates less from the ground truth GT compared to the baseline CUT3R. Alignment Method Online Abs Rel δ<1.25 Abs Rel δ<1.25 Abs Rel δ <1.25 Sintel (50 frames) BONN (110 frames) KITTI (110 frames) DUSt3R [83] MASt3R [38] MonST3R [93] Easi3R [15] AETHER [70] VGGT [79] Spann3R [77] CUT3R [81] Point3R [87] StreamVGGT [96] TTT3R MASt3R [38] CUT3R [81] Point3R [87] TTT3R 0.656 0.641 0.378 0.377 0.324 0.287 0.622 0.421 0.452 0.323 0. 1.022 1.029 0.777 0.977 45.2 43.9 55.8 55.9 50.2 66.1 42.6 47.9 48.9 65.7 50.0 14.3 23.8 17.1 24.5 0.155 0.252 0.067 0.059 0.273 0.055 0.144 0.078 0.060 0.059 0. 0.272 0.103 0.137 0.090 83.3 70.1 96.3 97.0 59.4 97.1 81.3 93.7 96.0 97.2 95.4 70.6 88.5 94.7 94.2 0.144 0.183 0.168 0.102 0.056 0.070 0.198 0.118 0.136 0.173 0. 0.467 0.122 0.191 0.110 81.3 74.5 74.4 91.2 97.8 96.5 73.7 88.1 84.2 72.1 90.4 15.2 85.5 73.8 89.1 Per-sequence Scale Metric Scale Table 2: Evaluation of Video Depth Estimation - Short Sequence. We report scale-invariant relative depth (aligned by per-sequence scale) and metric scale absolute depth accuracy on Sintel [12], Bonn [48], and KITTI [31] datasets. TTT3R achieves state-of-the-art or competitive performance among online methods, leading in KITTI for both metric and scale-invariant evaluations and ranking first or second in Sintel and Bonn. the inherent knowledge of the 3D reconstruction foundation model with just few lines of plug-andplay code by proposing general sequence modeling formulation and deriving novel state transition rule of CUT3R from TTT pespective."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Qualitative Results of 3D Reconstruction. TTT3R improves length generalization over CUT3R while preserving its speed and memory efficiency. Offline methods (e.g., VGGT) achieve accurate reconstruction on short sequences (150 frames) but fail on longer sequences (400 frames) due to memory constraints. Figure 14: In-the-Wild Video Reconstruction - Short Sequences. TTT3R performs online 3D reconstruction by estimating camera parameters and dense geometry for each incoming image. It supports varying-length image inputs, either video streams or sparse photo collections, across both static and dynamic scenes. A.4 VIDEO DEPTH ESTIMATION Following common practice [81, 93], we evaluate video depth estimation on KITTI [31], Sintel [12], and Bonn [48] datasets covering dynamic and static, indoor and outdoor scenes. We use absolute relative error (Abs Rel) and δ < 1.25 (percentage of predicted depths within 1.25-factor of true depth) as metrics. Video depth estimation evaluates per-frame depth quality and inter-frame depth consistency by aligning predicted depth maps to ground truth using per-sequence scale, which measures the relative depth accuracy. For methods that predict metric pointmaps (i.e., outputs in meters with absolute scale), we also report results without scale alignment, evaluating predictions directly in metric units to assess absolute-scale accuracy."
        },
        {
            "title": "Preprint",
            "content": "Figure 8 presents the quantitative comparison between our method and the online baselines. Our approach delivers the best overall performance without ANY fine-tuning. The results in Table 2 show that even for short sequences, our method still achieves state-of-the-art or competitive performance in online methods, leading in KITTI dataset for both metric and scale-invariant evaluations and ranking one or two in Sintel and Bonn datasets. A.5 3D RECONSTRUCTION The results are presented in Figure 14, Figure 11 and Figure 13. TTT3R supports both video sequences and sparse photo collections, across static and dynamic scenes, and performs online 3D reconstruction by estimating camera parameters and dense geometry for each incoming frame. TTT3R is simple modification to CUT3R that improves length generalization via closed-form state update, enabling robust long-sequence 3D reconstruction. The update is performed in the forward pass without any model fine-tuning, making it plug-and-play solution, while preserving CUT3Rs inference speed and memory footprint and operating online at realtime and only cost 6GB GPU memory."
        },
        {
            "title": "B USE OF LARGE LANGUAGE MODELS",
            "content": "We used large language model to assist with copy editinggrammar checking, wording suggestions, and minor style and clarity improvementsafter the scientific content, methodology, analyses, and conclusions had been written by the authors."
        }
    ],
    "affiliations": [
        "University of Tubingen, Tubingen AI Center",
        "Westlake University"
    ]
}