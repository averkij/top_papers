{
    "paper_title": "SWI: Speaking with Intent in Large Language Models",
    "authors": [
        "Yuwei Yin",
        "EunJeong Hwang",
        "Giuseppe Carenini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Intent, typically clearly formulated and planned, functions as a cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the model's underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates a novel avenue for enhancing LLMs' reasoning abilities with cognitive notions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 4 5 1 2 . 3 0 5 2 : r SWI: Speaking with Intent in Large Language Models SWI: Speaking with Intent in Large Language Models Yuwei Yin1, EunJeong Hwang1,2, Giuseppe Carenini1 1University of British Columbia; 2Vector Institute for AI {yuweiyin,ejhwang,carenini}@cs.ubc.ca"
        },
        {
            "title": "Abstract",
            "content": "Intent, typically clearly formulated and planned, functions as cognitive framework for reasoning and problem-solving. This paper introduces the concept of Speaking with Intent (SWI) in large language models (LLMs), where the explicitly generated intent encapsulates the models underlying intention and provides high-level planning to guide subsequent analysis and communication. By emulating deliberate and purposeful thoughts in the human mind, SWI is hypothesized to enhance the reasoning capabilities and generation quality of LLMs. Extensive experiments on mathematical reasoning benchmarks consistently demonstrate the superiority of Speaking with Intent over Baseline (i.e., generation without explicit intent). Moreover, SWI outperforms answer-trigger prompting methods Chain-of-Thought and Plan-and-Solve and maintains competitive performance with the strong method ARR (Analyzing, Retrieving, and Reasoning). Additionally, the effectiveness and generalizability of SWI are solidified on reasoning-intensive question answering (QA) and text summarization benchmarks, where SWI brings consistent improvement to the Baseline generation. In text summarization, SWI-generated summaries exhibit greater accuracy, conciseness, and factual correctness, with fewer hallucinations. Furthermore, human evaluations verify the coherence, effectiveness, and interpretability of the intent produced by SWI. This proof-of-concept study creates novel avenue for enhancing LLMs reasoning abilities with cognitive notions."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Zhao et al., 2023; Min et al., 2023; Minaee et al., 2024) have revolutionized Natural Language Processing (NLP) with their excellent generative capabilities (Hurst et al., 2024; Anthropic, 2024; Team et al., 2024). Many recent benchmarks focus on evaluating complex reasoning in mathematics problems (Cobbe et al., 2021; Hendrycks et al., 2021b; Lightman et al., 2024; Vendrow et al., 2025) and multidisciplinary questions (Hendrycks et al., 2021a; Srivastava et al., 2023; Suzgun et al., 2023; Wang et al., 2024e), presenting significant challenges for LLMs. Enhancing LLM reasoning is vital for their ongoing development (Huang & Chang, 2022; Qiao et al., 2023; Patil, 2025). Recent advancements have introduced various approaches to improve LLM reasoning (Yu et al., 2024; Cheng et al., 2025; Liu et al., 2025a; Xu et al., 2025). Chain-of-Thought (CoT) prompting enhances reasoning (Li et al., 2024b) by providing step-by-step solutions in few-shot exemplars (Wei et al., 2022) or explicitly instructing the model to think step by step (Kojima et al., 2022). Beyond CoT variants (Yao et al., 2023a; Wang et al., 2023; Yasunaga et al., 2024), Yin & Carenini (2025) propose Analyzing, Retrieving, and Reasoning (ARR), an answer-trigger prompting method that systematically guides LLMs in question answering and outperforms CoT prompting. Inspired by ARR (Yin & Carenini, 2025), where intent analysis is identified as its most effective component, we further investigate the powerful potential that intent can unleash in enhancing LLM reasoning. Intent, the goal-oriented intention in our mind (Adams, 1986; Mele, 1989; Mele & Moser, 1994), serves as guiding framework for problem-solving. As 1Appendix Reproducibility Statement. Source code: https://github.com/YuweiYin/SWI 1 SWI: Speaking with Intent in Large Language Models Figure 1: SWI Overview. (a) The intent, functioning as meta-thought and planning, guides the analysis with reasoning to answer the question. (b) concrete example generated by our method: Speaking with Intent (SWI) in large language models (LLMs). (c) The performance improvement on mathematical reasoning and multiple-choice QA tasks brought by SWI. illustrated in Figure 1(a), human reasoning (System 2 (Kahneman, 2011)) typically follows structured thinking loop where intent directs problem analysis and logical reasoning. Hence, we hypothesize that enabling LLMs to explicitly speak with their own intentrather than merely analyzing the intent of questionscan replicate this meta-cognitive planning process, thereby improving the reasoning ability and generation quality of LLM. This work introduces Speaking with Intent (SWI), requiring LLMs to articulate their own intent as planning mechanism during generation. Due to the autoregressive nature of LLMs (Radford et al., 2019) and the attention mechanism (Vaswani et al., 2017), explicitly stated intent provides high-level guidance for subsequent analysis and reasoning. Specifically, we implement SWI using instruction-following LLMs (Ouyang et al., 2022; Rafailov et al., 2023), incorporating system and user prompts for the model (Dubey et al., 2024) to generate intent. As shown in Figure 1(b), each analytical step (in blue) in solving math problem is guided by preceding intent statement (in orange), which is freely generated text instead of predefined class as in traditional intent modeling (Weld et al., 2022). To comprehensively evaluate the effectiveness and generalizability of the proposed SWI method, we conduct experiments across three diverse task categories: mathematical reasoning, multiple-choice QA, and text summarization. As shown in Figure 1(c), LLM speaking with intent (SWI) consistently outperforms Baseline, which generates responses without explicit intent. Furthermore, SWI surpasses answer-trigger prompting methods such as Chain-of-Thought (CoT) (Kojima et al., 2022) and Plan-and-Solve (PS) (Wang et al., 2023). Compared with the strong method ARR (Analyzing, Retrieving, and Reasoning) (Yin & Carenini, 2025), SWI maintains competitive performance on mathematical reasoning tasks. For text summarization, SWI produces summaries that are more accurate, concise, and factually reliable, with fewer hallucinations (Ji et al., 2023; Li et al., 2024a) in the output. Additionally, we perform human evaluations to assess the coherence, effectiveness, and interpretability of the intent generated by our SWI method. Evaluators largely agree on the quality of the generated intent, particularly for mathematical reasoning tasks. The evaluation results confirm that Speaking with Intent in LLMs enhances task performance and output explainability. The key contributions of this work are as follows: 1. This paper introduces Speaking with Intent (SWI) in LLMs, where the generated intent effectively guides problem analysis and logical reasoning, improving performance across various benchmarks. The source code will be publicly available. 2. Extensive experiments across diverse reasoning and generation tasks, including mathematical reasoning, multiple-choice QA, and text summarization, demonstrate that SWI consistently outperforms the Baseline and surpasses established answertrigger prompting methods such as Chain-of-Thought. 3. Human evaluations validate the coherence, effectiveness, and interpretability of the intent generated by SWI. Rather than intent classification, our evaluation practice provides standards for assessing freely generated intent statements. 2 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "2 Related Work",
            "content": "LLM Reasoning. Recent LLM research increasingly focuses on enhancing reasoning abilities (Sun et al., 2023; Patil, 2025; Cheng et al., 2025; Liu et al., 2025a). Chain-of-Thought (CoT) prompting facilitates problem-solving by prompting LLMs to generate intermediate reasoning steps (Kojima et al., 2022; Wei et al., 2022; Li et al., 2024b; Yeo et al., 2025). Building on CoT, various reasoning techniques have emerged (Xu et al., 2025). Among them, recent research ARR (Yin & Carenini, 2025) consistently outperforms CoT on multiple QA tasks, where intent analysis of questions is its most effective component. Inspired by ARR, we enable LLMs to articulate their intent, using it to guide subsequent analysis and reasoning for improved task performance. Results show that Speaking with Intent (SWI) surpasses CoT and ARR on multiple benchmarks. As an intent-driven approach, SWI introduces novel framework for advancing LLM reasoning and cognitive capabilities. LLM Planning. Planning plays critical role in advancing artificial intelligence (Wang et al., 2024c), particularly in developing LLM agents in real and complex environments (Qiao et al., 2024; Mialon et al., 2024; Xu et al., 2024; Drouin et al., 2024; Boisvert et al., 2024; Xie et al., 2024a;b). Aside from improving the tool-use capabilities (Wang et al., 2024f; Wu et al., 2024; Huang et al., 2024; Wang et al., 2024a; Liu et al., 2025b), recent LLM planning methods enhance reasoning abilities (Huang et al., 2022; Yao et al., 2023b; Hao et al., 2023; Valmeekam et al., 2023; Jiao et al., 2024) by generating problem-solving plans explicitly (Wang et al., 2023; Li & Zhang, 2024; Lyu et al., 2024) or implicitly (Goyal et al., 2024; Wang et al., 2024d; Pfau et al., 2024; Chen et al., 2024). In this work, we require LLMs to explicitly speak with their intent in an iterative and dynamic manner, as shown in Figure 1(a), instead of generating all plans first and then solving the task (Wang et al., 2023; Li & Zhang, 2024). The existing iterative planning framework (Lyu et al., 2024) focuses on addressing information retrieval issues in knowledge-intensive long-form generation tasks, while our SWI method generates intents that function as strategic planning to guide problem analysis and logical reasoning. Intent-related Research. Intent Detection (ID) and New Intent Discovery (NID) (Kumar et al., 2022; Liang et al., 2024; Zhang et al., 2024a; Tang et al., 2024; Zhang et al., 2024c; Qian et al., 2024; Yin et al., 2025), which classify utterances into known or novel intent categories, are longstanding challenges in natural language understanding (Larson et al., 2019; Casanueva et al., 2020; Zhang et al., 2021; Weld et al., 2022). Typically, these tasks are approached as classification problems (Wang et al., 2024b; Yoon et al., 2024; Zhang et al., 2024b; Sakurai & Miyao, 2024), where models assign sentences to predefined intent classes. In contrast, Speaking with Intent (SWI) generates intent as free-form text rather than fixed categories, enhancing flexibility and fluency. SWI naturally integrates intent statements as planning into the reasoning process, providing contextual guidance for subsequent analysis."
        },
        {
            "title": "3 Speaking with Intent",
            "content": "This section presents the problem-solving workflow of LLMs and introduces Speaking with Intent (SWI), enabling LLMs to explicitly articulate their intent for planning and reasoning. 3.1 Problem-solving Workflow using LLMs Let = {X , } be dataset, where = {X1, X2, . . . , Xn} is the input information (questions), = {y1, y2, . . . , yn} is the corresponding references (correct answers), and is the number of instances in D. For mathematical reasoning datasets, Xi is the math problem, and yi is the correct answer (usually an integer number). For multiple-choice QA datasets, Xi contains the question and options, and yi is the answer label such as (A)/(B)/(C) or Yes/No. For text summarization datasets, Xi is the source article, and yi is the reference summary. In this work, we employ instruction-following LLMs (aka Chat LLMs) for experiments and apply the chat template with the system prompt Ps and user prompt Pu. The system prompt specifies the general behavior of the model (assistant), and the user prompt poses 3 SWI: Speaking with Intent in Large Language Models questions to the model. Therefore, the generated output ˆyi is obtained by ˆyi = M(Ps, Pu, Xi; Θ, ζ), (1) where Ps is the system prompt, Pu is the user prompt, and Xi is the task input. These string objects are concatenated using line breaks (n) as the delimiter. With parameters Θ and hyper-parameters ζ, the LLM generates new tokens one by one until reaching the generation limit or generating the end-of-text special token provided by the tokenizer. 3.2 Baseline vs. Speaking with Intent In this work, we require LLMs to speak with intent (SWI) by presenting detailed instructions in the system prompts Ps and restating the SWI requirement in the user prompt Pu. Figure 2 presents the system prompt and user prompt of Baseline generation (Pbase ) and our SWI method (Pswi and Pbase and Pswi ). s Figure 2: LLM Prompts. The system and user prompts of Baseline and our SWI method. 3.3 Result Evaluation Lastly, we extract the final answer (after Final Answer: in ˆyi), denoted as yi, and compute the overall performance of on the dataset by = 1 i=1 S(yi, yi), (2) where the score function S(, ) returns value in the range of [0, 1]. Different tasks adopt different score functions to evaluate the model performance. For mathematical reasoning tasks, we first extract numbers in yi and apply text normalization to both yi and the reference yi, and then conduct exact match to check if the generated answer yi is correct. For multiple-choice QA tasks, we adopt the Option Selection metric introduced by Yin & Carenini (2025), which evaluates the LLM perplexity of different option concatenations and selects the one with lowest perplexity as the models choice. To evaluate the quality of summaries, we apply the standard ROUGE (Lin, 2004) as the automatic evaluation metric and complement it with more sophisticated fact-checking analysis as described in Section 5.2."
        },
        {
            "title": "4 Main Experiments on Mathematical Reasoning",
            "content": "This section presents the main experiments to verify the effectiveness of SWI on mathematical reasoning. To ensure reproducibility, we fixed all random seeds, set the generation temperature to zero, and conducted all experiments twice, yielding reproducible results.2 4.1 Models In all experiments, we employ LLaMA3-8B-Instruct (Dubey et al., 2024) as the model for generation and evaluation. It is an open-weights, instruction-following, and Transformerbased (Vaswani et al., 2017) LLM with 8 billion model parameters. We use the model checkpoint and tokenizer provided by Hugging Face Transformers (Wolf et al., 2020). 2Please refer to Appendix for the reproducibility statement and Appendix for dataset details. 4 SWI: Speaking with Intent in Large Language Models 4.2 Datasets The proposed SWI method aims to enhance LLM reasoning, so we conduct extensive experiments on various mathematical reasoning benchmarks, ranging from grade-school to competition-level problems. In the math reasoning task, the model is asked to solve the given math problem and present the final answer. We consider grade-school math problems including GSM8K (Cobbe et al., 2021), GSM8K-Platinum (Vendrow et al., 2025), and MATH500 (Lightman et al., 2024), as well as three competition-level math benchmarks: AMC23, AIME24, and AIME25, which have 40, 30, and 30 math problems, respectively. Despite the smaller scale, these math problems are much more challenging than those in GSM8K and MATH500. The dataset statistics are presented in Table 1. Task Dataset Split Size # Tok. Mathematical Reasoning GSM8K (Cobbe et al., 2021) GSM8K-P (Vendrow et al., 2025) MATH500 (Lightman et al., 2024) Test Test Test 1,319 1,209 500 124 124 Table 1: Mathematical Reasoning Datasets. # Tok. is the average number of tokens in the Baseline prompt of each instance, tokenized by the LLaMA (Dubey et al., 2024) tokenizer. The system prompt of SWI (Figure 2) bring about 80 extra tokens for each instance. 4.3 Experimental Comparison The main comparison is LLM generation with intent (SWI) or without intent (Baseline), and the effectiveness of SWI is verified if the former outperforms the latter. For mathematical reasoning and multiple-choice QA tasks, we also compare SWI with answer-trigger prompting methods CoT (Kojima et al., 2022) and ARR (Yin & Carenini, 2025), as well as previous LLM planning method Plan-and-Solve (PS) prompting (Wang et al., 2023). CoT aims to elicit LLM reasoning using the answer-trigger prompt ΦCoT as Lets think step by step, while ARR, with more systematic design, utilizes an enhanced ΦARR as Lets analyze the intent of the question, find relevant information, and answer the question with step-by-step reasoning. Finally, PS applies the following prompt ΦPS to construct plans before problem-solving: Lets first understand the problem and devise plan to solve the problem. Then, lets carry out the plan and solve the problem step by step. With answer-trigger prompts Φ i, the generation process (Eq. 1) is given by i (3) where Ps is the system prompt, Pu is the user prompt, and Xi is the task input. Θ and ζ are the parameters and hyper-parameters of the LLM M, respectively. ˆyi = M(Ps, Pu, Xi, Φ i; Θ, ζ), 4.4 Experimental Results SWI vs. Baseline. As shown in Table 2, SWI improves Baseline (generation without intent) performance by an average of +3.34 points, showing its effectiveness in enhancing LLMs problem analysis and mathematical reasoning abilities. Method GSM8K GSM8K-P MATH500 Competition Avg. Baseline SWI (ours) 78.47 81. 81.14 84.20 36.80 41.60 10.56 13. 51.74 55.08 Table 2: Mathematical Reasoning Results. The exact matching scores (%) of LLaMA-3-8BInstruct on multiple mathematical reasoning datasets with or without Speaking with Intent. SWI vs. Answer-Trigger Prompting Methods. Table 3 presents the results of our SWI method and three answer-trigger prompting methods: CoT, PS, and ARR. We observe that 5 SWI: Speaking with Intent in Large Language Models SWI significantly outperforms PS, suggesting that the dynamic generation of intents in SWI provides more effective analytical guidance than the static construction of multiple plans at the outset, as in PS. Furthermore, compared to the reasoning-eliciting methods CoT and ARR, SWI demonstrates superior performance over CoT and achieves competitive results with ARR. Notably, SWI surpasses ARR in solving competition-level mathematics problems, which are more challenging and thus indicate the potential of SWI. Method GSM8K GSM8K-P MATH500 Competition Avg. CoT (Kojima et al., 2022) PS (Wang et al., 2023) ARR (Yin & Carenini, 2025) SWI (ours) 79.30 73.54 81. 81.20 82.22 76.43 85.36 84.20 42.60 39.20 44.40 41.60 7.78 8.61 10. 13.33 52.98 49.45 55.50 55.08 Table 3: Mathematical Reasoning Results. The exact matching scores (%) of LLaMA-3-8BInstruct on multiple mathematical reasoning datasets using our SWI method or answertrigger prompting methods. The underlined values indicate the second-best scores."
        },
        {
            "title": "5 Generalizability of Speaking with Intent",
            "content": "The main experiments have verified the effectiveness of Speaking with Intent, this section further studies the generalizability of the proposed SWI method on different categories of tasks: multiple-choice QA and text summarization. 5.1 Multiple-choice Question Answering Many challenging benchmarks are designed as multiple-choice QA tasks, where the model is asked to select the most appropriate one from the given options to answer the question. 5.1.1 QA Datasets Our SWI method is evaluated on various QA datasets involving reading comprehension (Liu et al., 2020), commonsense reasoning (Talmor et al., 2019; Sap et al., 2019), world knowledge (Mihaylov et al., 2018), and reasoning-intensive multitask understanding tasks with multiple subtasks covering diverse topics (Clark et al., 2018; Suzgun et al., 2023; Hendrycks et al., 2021a; Wang et al., 2024e). The dataset statistics are presented in Table 4. Type Dataset Split Size # Tok. Reading LogiQA (Liu et al., 2020) Commonsense World Knowledge CSQA (Talmor et al., 2019) SIQA (Sap et al., 2019) OBQA (Mihaylov et al., 2018) ARC (Clark et al., 2018) Multitask Understanding BBH (Suzgun et al., 2023) MMLU (Hendrycks et al., 2021a) MMLU-Pro (Wang et al., 2024e) Test Valid Valid Test Test Test Test Test 651 1,221 1, 500 3,548 5,511 13,842 12,032 185 127 117 127 144 203 193 Table 4: Multiple-choice QA Datasets. # Tok. is the average number of tokens in the Baseline prompt of each instance, tokenized by the LLaMA (Dubey et al., 2024) tokenizer. 5.1.2 QA Results Table 5 shows that our SWI method consistently improves the Baseline (generation without intent) by large margin, i.e., +10.08 points on average. The results demonstrate the efficacy of Speaking with Intent in reasoning-intensive question answering. 6 SWI: Speaking with Intent in Large Language Models Method Reading Commonsense World Knowledge Multitask Understanding LogiQA CSQA SIQA OBQA Baseline SWI (ours) 20. 33.03 66.91 72.73 29.58 46.37 70. 81.60 ARC 76.44 87.82 BBH MMLU MMLU-Pro 56. 64.29 52.40 62.19 39.27 45.16 Avg. 51.57 61.65 Table 5: Multiple-choice QA Results. The accuracy scores (%) of LLaMA3-8B-Instruct on various multiple-choice QA datasets with or without Speaking with Intent. 5.2 Text Summarization The results on math and QA benchmarks have solidified the efficacy of SWI on reasoningintense tasks. Beyond reasoning tasks, we hypothesize that Speaking with Intent benefits natural language generation tasks like summarization by more explicitly analyzing the source document point by point and better planning the generation of the final summary. 5.2.1 Summarization Datasets To verify the hypothesis, we test the effect of SWI on various text summarization datasets. CNN/DailyMail (CDM) (Hermann et al., 2015; See et al., 2017), Extreme summarization (XSum) (Narayan et al., 2018), and XL-Sum (Hasan et al., 2021) contain diverse articlesummary pairs from news articles. SAMSum (Gliwa et al., 2019) and DialogSum (Chen et al., 2021) are dialogue summarization datasets. WikiLingua (Ladhak et al., 2020) extracts article and summary pairs from wikiHow. The dataset statistics are presented in Table 6. Type Dataset Split Size # Tok. News Article CDM (Hermann et al., 2015) XSum (Narayan et al., 2018) XL-Sum (Hasan et al., 2021) Dialogue SAMSum (Gliwa et al., 2019) DialogSum (Chen et al., 2021) Wiki Article WikiLingua (Ladhak et al., 2020) Test Test Test Test Test Test 11,490 11,334 11,535 819 1,500 3,000 920 542 209 263 525 Table 6: Text Summarization Datasets. # Tok. is the average number of tokens in the Baseline prompt of each instance, tokenized by the LLaMA (Dubey et al., 2024) tokenizer. 5.2.2 Summarization Results Automatic Evaluation of Summaries. We evaluate the quality of summaries using the ROUGE score (Lin, 2004), which counts the overlaps of the generated summaries and reference summaries. Specifically, we average the ROUGE-1 (unigrams), ROUGE-2 (bigrams), ROUGE-L (longest common subsequences), and ROUGE-LSum (sentence-level ROUGE-L) scores as the final ROUGE score. As shown in Table 7, our SWI method consistently surpasses the Baseline on ROUGE scores, confirming its effectiveness in enhancing the quality of text summaries. Method News Article Dialogue Wiki Article CDM XSum XL-Sum SAMSum DialogSum WikiLingua Baseline SWI (ours) 23.38 24.00 11.90 13.82 11.29 13. 24.14 24.37 16.92 20.64 15.01 17. Avg. 17.11 19.03 Table 7: Text Summarization Results. The ROUGE scores (%) of the LLaMA3-8B-Instruct model on various text summarization datasets with or without Speaking with Intent (SWI). 7 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "5.2.3 Fact Checking of Summaries",
            "content": "LLMs frequently generate hallucinated content (Ji et al., 2023; Li et al., 2024a), which can not be detected by lexical metrics like ROUGE. To assess this issue, we adopt more semantically sophisticated fact-checking metric proposed by Hwang et al. (2025), which quantifies factual consistency by calibrating the extent of fabricated statements (low precision) and omitted factual information (low recall). Specifically, we use GPT-4o-mini (Hurst et al., 2024) to decompose both generated and reference summaries into atomic fact sets and measure their coverage to quantify factual consistency. We evaluate 100 samples from each summarization dataset using this fact-checking metric, with results presented in Table 8. Speaking with Intent (SWI) generates more concise and accurate summaries, improving precision, whereas baseline summaries tend to be more lengthy and verbose, resulting in higher recall scores. Overall, SWI consistently outperforms the Baseline generation in terms of F1 score. Dataset Method Precision Recall F1 Dataset Method Precision Recall F1 CDM XSum XL-Sum Baseline SWI Baseline SWI Baseline SWI 26.06 34.22 11.06 14.77 8.96 12. 76.28 55.89 48.38 37.30 61.88 46.72 36.37 37.79 15.15 16.29 13.79 16. SAMSum DialogSum WikiLingua Baseline SWI Baseline SWI Baseline SWI 39.95 47.82 23.99 34.92 23.33 32.40 75.92 69.88 57.08 45.19 65.55 54. 46.90 50.88 29.55 31.20 30.63 35.78 Table 8: Summarization evaluation results. We compare the atomic facts drawn from the generated and reference summaries, and compute recall, precision, and F1 scores (%). 5.2.4 Case Study To provide more insights into the benefits of Speaking with Intent, we conduct case study on the CDM summarization dataset (Hermann et al., 2015; See et al., 2017). Figure 3 showcases the SWI outputs, where the generated intent is well formulated and articulated, guiding point-by-point summarization process that leads to final summary that is accurate, concise, and abstractive, while effectively capturing the key information of the source article. This probably leads to the high Precision and F1 scores of SWI in Table 8, suggesting the validity of Speaking with Intent in text generation tasks. Figure 3: Case Study. Task input and SWI output on text summarization task. 8 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "Intent Quality Evaluation",
            "content": "Although we have shown that Speaking with Intent boosts performance across broad range of tasks, verifying the quality of generated intents is also significant. Thus, we hire human evaluators to assess the quality of generated intent across three criteria: coherence, effectiveness, and interpretability. Coherence measures how well the intent guides analysis and reasoning, effectiveness evaluates its contribution to problem-solving, and interpretability assesses its role in enhancing user understanding of the generated content.3 For each instance, human evaluators are provided with evaluation instructions, task input (e.g., the math problem, question with options, or source article), SWI-generated output, and assessment check boxes. They are then asked to evaluate the following aspects: Coherence: In general, does the analysis align coherently with the intent statements? Effectiveness: Overall, do the intent statements help with the planning and reasoning for solving the problem? Interpretability: Do you think providing the intent can help you better understand the reasoning process than not providing it? Evaluation scores range from 1 (Bad), 2 (Fair), to 3 (Good). Agreement ratios are calculated as follows: 1 if all three evaluators agree, 0.5 if two agree, and 0 if all scores differ. As shown in Table 9, human evaluation scores for all aspects across datasets exceed 2, indicating that the generated intent is generally well-regarded. Notably, math reasoning tasks achieve scores near 3 with agreement ratios approaching 100%, demonstrating that SWI-generated intent is particularly coherent, effective, and interpretable in mathematical problem-solving. The relatively low scores observed in QA tasks may be attributed to the lack of multi-step guidance: the number of generated intents in QA tasks is often 1 or 2, while that in math and summarization is usually at least three. This finding indicates the advantages of multi-round iterative intents, although SWI can boost task performance even with few intents generated. Task Dataset Coherence Score Agree Effectiveness Score Agree Interpretability Agree Score Math Reasoning Multiple-choice QA Summarization GSM8K MATH500 BBH MMLU CDM XSum 2.90 2.87 2.37 2.67 2.83 2.70 85% 80% 55% 75% 80% 70% 2.97 2.87 2.37 2.53 2.77 2.60 95% 80% 50% 55% 70% 65% 2.97 2.83 2.33 2.37 2.83 2.57 95% 80% 45% 45% 75% 65% Table 9: Intent Quality Evaluation by Humans. The score ranges from 1 (Bad) to 3 (Good)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce Speaking with Intent (SWI) in LLMs, where the generated intent (as high-level planning) guides subsequent analysis, improving the reasoning and generation abilities. Extensive experiments across mathematical reasoning, multiple-choice QA, and text summarization benchmarks consistently show the benefits of SWI over the In addition, SWI outperforms Baseline method that generates without explicit intent. answer-trigger prompting methods like Chain-of-Thought on mathematical reasoning benchmarks. In text summarization, SWI produces summaries that are more accurate, concise, and factually reliable, with fewer hallucinations. Furthermore, human evaluations solidify the coherence, effectiveness, and interpretability of LLM-generated intent. This study opens new avenue for enhancing LLM reasoning abilities and beyond. Impact Statement. As intent is fundamental aspect of natural language processing, empowering, eliciting, and enhancing the intent understanding and generation abilities can potentially drive AI systems (including multimodal models) to the next level. Moreover, Speaking with Intent can also be applied to various domains beyond NLP, such as healthcare, law, and finance. These applications are cost-sensitive, so explicitly showing the intent of AI models will help with the transparency and interpretability of critical decision-making. 3Please refer to Appendix for more details on human evaluation. 9 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), Vector Institute for AI, and Accelerate Foundation Models Research Program Award from Microsoft. This research was supported in part by the computational resources and services provided by Advanced Research Computing at the University of British Columbia and the Digital Research Alliance of Canada (alliancecan.ca). We would also like to thank Vered Shwartz for constructive feedback on conducting evaluations."
        },
        {
            "title": "References",
            "content": "Frederick Adams. Intention and intentional action: The simple view. Mind & Language, 1(4): 281301, 1986. URL https://doi.org/10.1111/j.1468-0017.1986.tb00327.x. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 4 Mar 2024. URL https: //www.anthropic.com/news/claude-3-family. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. URL https://arxiv.org/abs/2212.08073. Leo Boisvert, Megh Thakkar, Maxime Gasse, Massimo Caccia, Thibault Le Sellier De Chezelles, Quentin Cappart, Nicolas Chapados, Alexandre Lacoste, and Alexandre Drouin. Workarena++: Towards compositional planning and reasoningbased common knowledge work tasks. In A. Globerson, L. Mackey, D. BelJ. Tomczak, and C. Zhang (eds.), Advances in Neugrave, A. Fan, U. Paquet, ral Information Processing Systems, volume 37, pp. 59966051. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2024/file/ Inc., 2024. 0b82662b6c32e887bb252a74d8cb2d5e-Paper-Datasets and Benchmarks Track.pdf. nigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. Efficient intent detection with dual sentence encoders. In Tsung-Hsien Wen, Asli Celikyilmaz, Zhou Yu, Alexandros Papangelis, Mihail Eric, Anuj Kumar, nigo Casanueva, and Rushin Shah (eds.), Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp. 3845, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/ v1/2020.nlp4convai-1.5. URL https://aclanthology.org/2020.nlp4convai-1.5/. Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. DialogSum: real-life scenario dialogue summarization dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 50625074, Online, August 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.findings-acl.449. URL https://aclanthology.org/2021.findings-acl. 449/. Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su, and Huan Sun. When is tree search useful for LLM planning? it depends on the discriminator. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1365913678, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 738. URL https://aclanthology.org/2024.acl-long.738/. Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, and Zhouchen Lin. Empowering llms with logical reasoning: comprehensive survey. arXiv preprint arXiv:2502.15652, 2025. URL https://arxiv.org/abs/2502.15652. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803. 05457. SWI: Speaking with Intent in Large Language Models Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168. Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1164211662. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/drouin24a.html. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407. 21783. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: human-annotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 7079, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409/. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ph04CRkPdC. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 81548173, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https: //aclanthology.org/2023.emnlp-main.507/. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 46934703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.413. URL https://aclanthology.org/ 2021.findings-acl.413/. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL https://openreview.net/forum?id= 7Bywt2mQsCe. Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper files/paper/2015/ file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf. SWI: Speaking with Intent in Large Language Models Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. URL https://arxiv.org/abs/2212.10403. Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. Planning, creation, usage: Benchmarking LLMs for comprehensive tool utilization in real-world complex scenarios. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 43634400, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.259. URL https://aclanthology.org/2024.findings-acl.259/. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 91189147. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/huang22a.html. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv.org/abs/2410.21276. EunJeong Hwang, Peter West, and Vered Shwartz. Bottlehumor: Self-informed humor explanation using the information bottleneck principle. arXiv preprint arXiv:2502.18331, 2025. URL https://arxiv.org/abs/2502.18331. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):248:1248:38, 2023. doi: 10.1145/3571730. URL https://doi.org/10.1145/3571730. Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, and Shafiq Joty. Learning planning-based reasoning by trajectories collection and process reward synthesizing. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 334350, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.20. URL https://aclanthology.org/2024.emnlp-main.20/. Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011. Large language models are zero-shot reasoners. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke IwaIn Advances in Neusawa. ral Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html. Rajat Kumar, Mayur Patidar, Vaibhav Varshney, Lovekesh Vig, and Gautam Shroff. Intent detection and discovery from user logs via deep semi-supervised contrastive clustering. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 18361853, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.134. URL https://aclanthology.org/2022.naacl-main.134. Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: new benchmark dataset for cross-lingual abstractive summarization. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 40344048, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://aclanthology. org/2020.findings-emnlp.360/. 12 SWI: Speaking with Intent in Large Language Models Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. An evaluation dataset for intent classification and out-of-scope prediction. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 13111316, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1131. URL https://aclanthology.org/D19-1131/. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1087910899, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.586. URL https://aclanthology.org/2024.acl-long.586/. Kunze Li and Yu Zhang. Planning first, question second: An LLM-guided method for controllable question generation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 47154729, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.findings-acl.280. URL https://aclanthology.org/2024.findings-acl. 280/. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=3EWTEy9MTM. Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. Synergizing large language models and pre-trained smaller models for conversational intent discovery. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1413314147, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.840. URL https: //aclanthology.org/2024.findings-acl.840/. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=v8L0pN6EOi. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/. Hanmeng Liu, Zhizhang Fu, Mengru Ding, Ruoxi Ning, Chaoli Zhang, Xiaozhang Liu, and Yue Zhang. Logical reasoning in large language models: survey. arXiv preprint arXiv:2502.09100, 2025a. URL https://arxiv.org/abs/2502.09100. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 36223628. International Joint Conferences on Artificial Intelligence Organization, 7 2020. doi: 10.24963/ijcai.2020/501. URL https://doi.org/ 10.24963/ijcai.2020/501. Main track. Yanming Liu, Xinyue Peng, Jiannan Cao, Shi Bo, Yuwei Zhang, Xuhong Zhang, Sheng Cheng, Xun Wang, Jianwei Yin, and Tianyu Du. Tool-planner: Task planning with clusters across multiple tools. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=dRz3cizftU. Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, and Enhong Chen. Retrieve-plan-generation: An iterative planning and answering framework for 13 SWI: Speaking with Intent in Large Language Models knowledge-intensive LLM generation. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 46834702, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.270. URL https: //aclanthology.org/2024.emnlp-main.270/. Alfred Mele. Intention, belief, and intentional action. American Philosophical Quarterly, (1):1930, 1989. URL https://www.jstor.org/stable/20014264. Alfred Mele and Paul Moser. Intentional action. Nous, 28(1):3968, 1994. URL https: //www.jstor.org/stable/2215919. Gregoire Mialon, Clementine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=fibxvahvs3. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260/. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent advances in natural language processing via large pre-trained language models: survey. ACM Computing Surveys, 56 (2):140, 2023. URL https://dl.acm.org/doi/abs/10.1145/3605943. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey. arXiv preprint arXiv:2402.06196, 2024. URL https://arxiv.org/abs/2402.06196. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 17971807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1206. URL https://aclanthology.org/D18-1206/. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/ paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html. Avinash Patil. Advancing reasoning in large language models: Promising methods and approaches. arXiv preprint arXiv:2502.03671, 2025. URL https://arxiv.org/abs/2502. 03671. Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=NikbrdtYvG. Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong, Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit user intention understanding of language model driven agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10881113, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 61. URL https://aclanthology.org/2024.acl-long.61/. 14 SWI: Speaking with Intent in Large Language Models Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. Reasoning with language model prompting: survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53685393, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.294. URL https://aclanthology.org/2023.acl-long.294/. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 114843 114871. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper files/paper/2024/file/d032263772946dd5026e7f3cd22bce5b-Paper-Conference.pdf. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. URL https://openai.com/research/better-language-models. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=HPuSIXJaa9. Hiromasa Sakurai and Yusuke Miyao. Evaluating intention detection capability of large language models in persuasive dialogues. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16351657, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.90. URL https://aclanthology.org/2024.acl-long.90/. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454. Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10731083, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https://aclanthology.org/P17-1099/. Robyn Speer, Joshua Chin, and Catherine Havasi. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 44444451. AAAI Press, 2017. doi: 10.1609/AAAI.V31I1.11164. URL https://doi.org/10.1609/ aaai.v31i1.11164. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023. URL https://arxiv.org/abs/ 2312.11562. 15 SWI: Speaking with Intent in Large Language Models Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.824. URL https://aclanthology.org/2023.findings-acl.824/. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology. org/N19-1421/. Kai Tang, Junbo Zhao, Xiao Ding, Runze Wu, Lei Feng, Gang Chen, and Haobo Wang. Learning geometry-aware representations for new intent discovery. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 56415654, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.306. URL https://aclanthology.org/2024.acl-long.306/. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=YXogl4uQUO. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on NeuInformation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, ral USA, pp. 59986008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do large language model benchmarks test reliability? arXiv preprint arXiv:2502.03461, 2025. URL https: //arxiv.org/abs/2502.03461. Hongru Wang, Rui Wang, Boyang Xue, Heming Xia, Jingtao Cao, Zeming Liu, Jeff Z. Pan, and Kam-Fai Wong. AppBench: Planning of multiple APIs from various APPs for complex user instruction. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1532215336, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.856. URL https://aclanthology.org/ 2024.emnlp-main.856/. Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, and Jian-Yun Nie. usercentric multi-intent benchmark for evaluating large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 35883612, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024. emnlp-main.210. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by 16 SWI: Speaking with Intent in Large Language Models large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.147. URL https://aclanthology.org/2023. acl-long.147/. Siwei Wang, Yifei Shen, Shi Feng, Haoran Sun, Shang-Hua Teng, and Wei Chen. Alpine: Unveiling the planning capability of autoregressive learning in language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 119662119688. Curran Associates, Inc., 2024c. URL https://proceedings.neurips.cc/paper files/ paper/2024/file/d848cb2c84f0bba7f1f73cf232734c40-Paper-Conference.pdf. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and Alessandro Sordoni. Guiding language model reasoning with planning tokens. In First Conference on Language Modeling, 2024d. URL https://openreview.net/forum?id= wi9IffRhVM. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 9526695290. Curran Associates, Inc., 2024e. URL https://proceedings.neurips.cc/paper files/paper/2024/ file/ad236edc564f3e3156e1b2feafb99a24-Paper-Datasets and Benchmarks Track.pdf. Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway? survey from the language model perspective. In First Conference on Language Modeling, 2024f. URL https://openreview.net/forum?id=Xh1B90iBSR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/ paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Henry Weld, Xiaoqi Huang, Siqu Long, Josiah Poon, and Soyeon Caren Han. survey of joint intent detection and slot filling models in natural language understanding. ACM Computing Surveys, 55(8):138, December 2022. doi: 10.1145/3547138. URL https://doi. org/10.1145/3547138. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: Stateof-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6. Qinzhuo Wu, Wei Liu, Jian Luan, and Bin Wang. ToolPlanner: tool augmented LLM for multi granularity instructions with path planning and feedback. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1831518339, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 1018. URL https://aclanthology.org/2024.emnlp-main.1018/. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan 17 SWI: Speaking with Intent in Large Language Models Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 5204052094. Curran Associates, Inc., 2024a. URL https://proceedings.neurips.cc/paper files/paper/2024/ file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets and Benchmarks Track.pdf. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Zeyu Liu, Yiheng Xu, Hongjin SU, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents in the wild. In First Conference on Language Modeling, 2024b. URL https://openreview.net/ forum?id=sKATR2O1Y0. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. URL https://arxiv.org/abs/2501.09686. Frank Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, et al. Theagentcompany: Benchmarking llm agents on consequential real world tasks. arXiv preprint arXiv:2412.14161, 2024. URL https://arxiv.org/abs/2412.14161. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. URL http://papers.nips.cc/paper files/paper/2023/hash/ 271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/ forum?id=WE vluYUL-X. Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language models as analogical reasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=AgDICX1h50. Edward Yeo, Yuxuan Tong, Xinyao Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in LLMs. In Scaling Self-Improving Foundation Models without Human Supervision, 2025. URL https://openreview.net/forum?id=6A861u4Crm. Shangjian Yin, Peijie Huang, and Yuhong Xu. Midlm: Multi-intent detection with bidirectional large language models. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of the 31st International Conference on Computational Linguistics, pp. 26162625, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https: //aclanthology.org/2025.coling-main.179/. Yuwei Yin and Giuseppe Carenini. Arr: Question answering with large language models via analyzing, retrieving, and reasoning. arXiv preprint arXiv:2502.04689, 2025. URL https://arxiv.org/abs/2502.04689. Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, and Taeuk Kim. Blendx: Complex multi-intent detection with blended patterns. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 24282439, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.218/. 18 SWI: Speaking with Intent in Large Language Models Fei Yu, Hongbo Zhang, Prayag Tiwari, and Benyou Wang. Natural language reasoning, survey. ACM Computing Surveys, 56(12):139, 2024. doi: 10.1145/3664194. URL https://doi.org/10.1145/3664194. Feng Zhang, Wei Chen, Fei Ding, Meng Gao, Tengjiao Wang, Jiahui Yao, and Jiabin Zheng. From discrimination to generation: Low-resource intent detection with language model instruction tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 1016710183, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.605. URL https://aclanthology.org/2024.findings-acl.605/. Hanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu. Discovering new intents with deep aligned clustering. Proceedings of the AAAI Conference on Artificial Intelligence, 35(16): 1436514373, May 2021. doi: 10.1609/aaai.v35i16.17689. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17689. Shun Zhang, Yan Chaoran, Jian Yang, Jiaheng Liu, Ying Mo, Jiaqi Bai, Tongliang Li, and Zhoujun Li. Towards real-world scenario: Imbalanced new intent discovery. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 39493963, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.217. URL https://aclanthology.org/2024.acl-long.217/. Yuwei Zhang, Siffi Singh, Sailik Sengupta, Igor Shalyminov, Hang Su, Hwanjun Song, and Saab Mansour. Can your model tell negation from an implicature? unravelling challenges with intent encoders. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 552567, Bangkok, Thailand, August 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.33. URL https://aclanthology.org/2024.acl-long.33/. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. URL https://arxiv.org/abs/2303.18223. 19 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "A Reproducibility Statement",
            "content": "A.1 LLM Generation For all experiments in 4 and 5, we employ LLaMA3-8B-Instruct (Dubey et al., 2024), an open-weights, instruction-following, and Transformer-based (Vaswani et al., 2017) LLM with 8 billion model parameters, and use the model checkpoint and tokenizer4 provided by Hugging Face Transformers (Wolf et al., 2020). Only in the fact-checking evaluation for summaries ( 5.2.3), we adopt the proprietary model GPT-4o-mini (Hurst et al., 2024) to decompose the generated summary and reference summary into two sets of atomic facts. For each running session, the experiments are conducted on single NVIDIA V100 GPU with 32GB memory. To avoid out-of-memory issues, all the models are loaded in halfprecision (float16) mode, and the generation batch size is 1. The input sequence is not truncated since we do not want to lose the context information, but we set the maximum number of newly generated tokens as 4096 during generation. To guarantee reproducibility, we fixed the seeds as 42 for all random modules, set the LLM generation temperature as 0 for deterministic generation without sampling, and ran all experiments twice, obtaining reproducible generation results and evaluation scores. The source code is available on GitHub: https://github.com/YuweiYin/SWI A.2 Experimental Cost In the generation stage, the total computational cost is approximately 1,500 GPU hours on NVIDIA V100 clusters (about 58 days). In the evaluation stage, only the option selection process for multiple-choice QA tasks requires GPU usage (Yin & Carenini, 2025), and the overall running time is roughly 100 hours on V100 clusters. In the fact-checking experiments ( 5.2.3), the expense for GPT-4o-mini API calls is below US$10. 4https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 20 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "B Tasks and Datasets",
            "content": "To comprehensively study the effectiveness and generalizability of the proposed SWI method, we consider three different categories of tasks in our experiments: mathematical reasoning, multiple-choice QA, and text summarization. Table 10 presents the dataset sources. Please note that the URLs may be subject to change by the dataset providers. Task Dataset Mathematical Reasoning GSM8K (Cobbe et al., 2021) GSM8K-P (Vendrow et al., 2025) MATH500 (Lightman et al., 2024) AMC23 (American Mathematics Competitions) AIME24 (American Invitational Math Examination) AIME25 (American Invitational Math Examination) Multiple-choice QA Text Summarization LogiQA (Liu et al., 2020) CSQA (Talmor et al., 2019) SIQA (Sap et al., 2019) OBQA (Mihaylov et al., 2018) ARC (Clark et al., 2018) BBH (Suzgun et al., 2023) MMLU (Hendrycks et al., 2021a) MMLU-Pro (Wang et al., 2024e) CDM (Hermann et al., 2015) XSum (Narayan et al., 2018) XL-Sum (Hasan et al., 2021) SAMSum (Gliwa et al., 2019) DialogSum (Chen et al., 2021) WikiLingua (Ladhak et al., 2020) Table 10: Dataset sources. URL Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link Link B.1 Mathematical Reasoning In the mathematical reasoning task, the model is asked to solve the given math problem and present the final answer. We consider the following math datasets. GSM8K. Grade School Math 8K (GSM8K) (Cobbe et al., 2021) is dataset of 8.5K highquality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning. GSM8K-P. GSM8K-Platinum (Vendrow et al., 2025) is revised version of the full test set of GSM8K (Cobbe et al., 2021). It revises the labels of mislabeled examples and removes any question that is determined to be poorly written (mostly due to ambiguity in the problem statement), providing more accurate assessment of mathematical reasoning capabilities. MATH500. MATH500 (Lightman et al., 2024) consists of 500 questions from the MATH test set (Hendrycks et al., 2021b). The distribution of difficulty levels and subjects in MATH500 is representative of the MATH test set as whole. Competition. We consider three competition-level math benchmarks: AMC23, AIME24, and AIME25, drawn from American Mathematics Competitions and American Invitational Mathematics Examination. AMC23, AIME24, and AIME25 have 40, 30, and 30 math problems, respectively. Despite the smaller scale, these math problems are much more challenging than those in GSM8K and MATH500. 21 SWI: Speaking with Intent in Large Language Models B.2 Multiple-choice QA Since many challenging benchmarks are designed as multiple-choice question-answering tasks, our SWI method is also evaluated on various reasoning-intense QA datasets, where the model is asked to select the most appropriate one from the given options to answer the question. LogiQA. LogiQA Liu et al. (2020) is reading comprehension dataset that requires the model to have logical reasoning for question-answering. CSQA. CommonsenseQA (CSQA) (Talmor et al., 2019) examines the model on commonsense question-answering problems constructed using information from ConceptNet Speer et al. (2017). SIQA. SocialIQA (SIQA) (Sap et al., 2019) is large-scale QA benchmark for commonsense reasoning about social situations, which probes emotional and social intelligence in everyday situations. OBQA. OpenBookQA (OBQA) (Mihaylov et al., 2018) asks the model to answer the question based on the given elementary-level science facts and broad commonsense knowledge. ARC. AI2 Reasoning Challenge (ARC) (Clark et al., 2018) contains grade-school science questions. It is divided into Challenge and an Easy set, where the former contains only questions answered incorrectly by both retrieval-based algorithm and word cooccurrence algorithm. BBH. BIG-Bench Hard (BBH) (Suzgun et al., 2023) is suite of challenging tasks filtered from BIG-Bench (Srivastava et al., 2023). Solving these problems often requires multi-step reasoning. In this work, 4 (out of 27) subtasks in BBH (word sorting, object counting, dyck languages, and multistep arithmetic two) are discarded as they are not multiplechoice QA tasks. MMLU. MMLU (Hendrycks et al., 2021a) comprehensively measures the multitask language understanding abilities on 57 subtasks including elementary mathematics, history, computer science, and more. MMLU-Pro. MMLU-Pro (Wang et al., 2024e) extends the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. B.3 Text Summarization Beyond math reasoning and multiple-choice QA, we hypothesize that Speaking with Intent benefits natural language generation tasks like summarization, where the generated intent can guide the model in summarizing the source article point by point. Hence, we test the effect of SWI on the following text summarization datasets (English only). CNN/DailyMail. CNN/DailyMail (CDM) (Hermann et al., 2015; See et al., 2017) contains over 300K unique news articles written by journalists at CNN and the Daily Mail. XSum. Extreme summarization (XSum) (Narayan et al., 2018) is real-world, large-scale dataset consisting of online articles from the British Broadcasting Corporation (BBC). XL-Sum. XL-Sum (Hasan et al., 2021) is comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC News. The dataset covers 45 languages and we only use the English subset. 22 SWI: Speaking with Intent in Large Language Models SAMSum. SAMSum (Gliwa et al., 2019) contains about 16K messenger-like conversations with summaries. Linguists were asked to create conversations similar to those they write on daily basis, reflecting the proportion of topics of their real-life messenger conversations. DialogSum. DialogSum (Chen et al., 2021) is large-scale dialogue summarization dataset, consisting of 13,460 dialogues with corresponding manually labeled summaries and topics. WikiLingua. WikiLingua (Ladhak et al., 2020) is large-scale, multilingual dataset for evaluating cross-lingual abstractive summarization systems. It extracts article and summary pairs in 18 languages from wikiHow, and we only use the English subset. 23 SWI: Speaking with Intent in Large Language Models"
        },
        {
            "title": "C Human Evaluation Details",
            "content": "Participant Requirements. We hire human evaluators from the cloud-sourcing platform CloudResearch5 to conduct human evaluation on the quality of the generated intent: coherence, effectiveness, and interpretability. To ensure the annotation quality, we apply several requirements to select qualified human evaluators, as shown in Table 11. Type Requirements Native Language English Country of Residence Australia, Canada, Ireland, New Zealand, UK, US Education Reputation Undergraduate student, Graduate student Approved Projects Count: 1,000 Approval Rating: 90% Table 11: The requirements for human evaluators. Evaluation Tasks. For each task category, we select two datasets: GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2024) for mathematical reasoning, BBH (Suzgun et al., 2023) and MMLU (Hendrycks et al., 2021a) for multiple-choice QA, and CDM (Hermann et al., 2015; See et al., 2017) and XSum (Narayan et al., 2018) for text summarization. We randomly sample 12 instances per dataset and divide them into two batches of six. Each batch includes dummy instance with deliberately reversed intents to ensure evaluators are actively engaged rather than randomly selecting responses. Evaluator submissions are accepted or rejected based on completion time and performance on the dummy instance. For each instance, human evaluators are provided with evaluation instructions, task input (e.g., the math problem, question with options, or source article), SWI-generated output, and assessment check boxes. They are then asked to evaluate the following aspects: Coherence: In general, does the analysis align coherently with the intent statements? Effectiveness: Overall, do the intent statements help with the planning and reasoning for solving the problem? Interpretability: Do you think providing the intent can help you better understand the reasoning process than not providing it? Each batch is assessed by three different human evaluators, with each person uniquely assigned to only one batch. Evaluation scores range from 1 (Bad), 2 (Fair), to 3 (Good). Agreement ratios are calculated as follows: 1 if all three evaluators agree, 0.5 if two agree, and 0 if all scores differ. Human Evaluation Quality. As mentioned above, we decided to accept or reject the evaluators submission based on the task completion time and the results on the dummy instance that is deliberately modified to have lower coherence. As result, about 60% of the evaluators still rated the dummy instance as good coherence, meaning they failed the dummy test and potentially did not fully focus on the evaluation process, which poses general caveat to the quality of cloud-sourcing annotations. Overall, we rejected about 10% of submissions that both failed the dummy test and took an unreasonably short time to complete the annotation. After rejecting them, we hired other evaluators until the intent quality evaluation was finished. Human Evaluation Cost. The pay rate for each human evaluator is US$10 per hour, completing batch of 6 instances takes an evaluator 10-15 minutes on average, and the total cost of the intent quality evaluation is about US$120. 5https://www.cloudresearch.com/"
        }
    ],
    "affiliations": [
        "University of British Columbia",
        "Vector Institute for AI"
    ]
}