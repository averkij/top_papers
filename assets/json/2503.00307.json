{
    "paper_title": "Remasking Discrete Diffusion Models with Inference-Time Scaling",
    "authors": [
        "Guanghan Wang",
        "Yair Schiff",
        "Subham Sekhar Sahoo",
        "Volodymyr Kuleshov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with a blog post on the project page: https://remdm.github.io."
        },
        {
            "title": "Start",
            "content": "Remasking Discrete Diffusion Models with Inference-Time Scaling Guanghan Wang * 1 Yair Schiff * 1 Subham Sekhar Sahoo 1 Volodymyr Kuleshov 1 5 2 0 2 1 ] . [ 1 7 0 3 0 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, method that can be applied to pretrained masked diffusion models in principled way and that is derived from discrete diffusion model with custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with form of inferencetime compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. We provide the code along with blog post on the project page: https://remdm.github.io 1. Introduction Diffusion models have gained significant traction as algorithms for generating high-quality images and videos (SohlDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020). Part of the success of diffusion stems from its ability to perform iterative refinementrepeatedly modifying outputs and fixing errors over multiple steps of generationwhich makes diffusion models especially effective at guided generation (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), *Equal contribution 1Department of Computer Science, Cornell Unversity. Correspondence to: Guanghan Wang <guanghanwang5@gmail.com>. 1 fast sampling (Song et al., 2020a; Salimans & Ho, 2022; Song et al., 2023), and supports inference-time scaling to improve sample quality (Ma et al., 2025). Discrete counterparts of diffusion models (Austin et al., 2021) have also been steadily improving in quality on tasks such as language modeling and biological sequence design (Lou et al., 2024; Sahoo et al., 2024; Schiff et al., 2024). However, modern discrete diffusionespecially the most performant kind that relies on masking (Ou et al., 2024; Shi et al., 2024; Sahoo et al., 2024)lacks the fundamental ability to iteratively refine outputs. Once discrete token is unmasked, it cannot be updated again, even if it introduces an error. The lack of error correction in turn sets limits on controllable generation, sampling speed, and sample quality. Here, we address the inability of masked diffusion models to perform iterative refinement by introducing new sampler that supports remasking during generation. Our method, the remasking diffusion model (ReMDM) sampler, is simple and allows users to directly specify the probability of remasking token at each time step. We augment the sampler with components that range from nucleus sampling to remasking schedules, significantly boosting performance. Our approach for deriving the sampler is rooted in probabilistic modeling-we show that our method corresponds to ancestral sampling in discrete diffusion model (also called ReMDM) characterized by remasking backward process. The ReMDM model admits an objective that is rescaled version of the MDLM objective (Sahoo et al., 2024)this suggests using the ReMDM sampler on top of pre-trained MDLMs, which we find to work well. We complement our analysis of the sampler by also showing that it can be interpreted as predictor-corrector technique (Song et al., 2020b; Campbell et al., 2022). Most interestingly, the ReMDM sampler endows discrete diffusion with form of inference-time compute scaling. By increasing the number of denoising steps on language modeling tasks, ReMDM generates samples with significantly higher sample quality metrics than any previous diffusion model and almost matches the performance of an autoregressive (AR) model with the same architecture. Conversely, when we reduce sampling steps to increase speed, performance degrades less than other samplers on text and image generation. Lastly, ReMDM models are more amenable Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 1. Our family of masked diffusion processes allow for more flexible generation with remasking of already decoded tokens. This improves sample quality and further closes the gap to AR models. (Left) An illustrative example of errors fixed by ReMDM. The first two tokens can be They sell or She sells, but due to the independence of the parallelized decoding processes, She sell is decoded. Such mistakes can be corrected by remasking samplers. (Right) MAUVE scores on OpenWebText. MDLM is from Sahoo et al. (2024). FB and DFM denote the forward-backward (Campbell et al., 2022) and discrete flow matching (Gat et al., 2024) correctors, respectively. to guidance (Schiff et al., 2024)in molecule design experiments, ReMDM pushes the Pareto frontier of novelty and the desired molecular property relative to alternatives relying on masking or uniform noise diffusion. Contributions We summarize our contributions as follows: 1. We introduce the remasking diffusion model (ReMDM) sampler and number of add-on components that bring performant iterative refinement via remasking to masked diffusion models. 2. We show that our method is form of ancestral sampling in probabilistic model whose ELBO is similar to that of classical masked diffusion. This analysis suggests using our sampler on top of pre-trained models, which we find to work well. 3. Across the domains of natural language, discretized images, and molecule string representations, we demonstrate empirically that ReMDM endows masked diffusion with inference-time scaling that improves sample quality with more computation, and that also enhances controlled generation. [0, 1] (i.e., for = 0, 1, . . . , ), each intermedit(i) = ate latent variable zt(i) {0, 1}V has the marginal: q(zt x) = Cat(zt; αtx + (1 αt)π) , (1) where π is user-specified prior and where we have dropped the explicit dependence of on i. The predefined schedule αt [0, 1] is monotonically decreasing in t. For sequences of tokens, we denote clean / noisy sequences as x(1:L) / and each token ℓ {1, . . . , L} as x(ℓ) / z(ℓ) z(1:L) . Masked Diffusion Language Models special case of this formulation, known as masking or absorbing state discrete diffusion, sets the limiting distribution π = m, one-hot vector centered on special [MASK] token. Letting s(i) = i1 be the time step directly preceding t, these posteriors take the form: q(zs zt, x) = Cat (cid:16) zs; αs αt 1 αt + 1 αs 1 αt (cid:17) . zt (2) 2. Background Discrete Diffusion Models Diffusion models (SohlDickstein et al., 2015; Ho et al., 2020) are characterized by parametric models pθ trained to reverse fixed noising process that maps clean data to increasingly noisy latent variables zt, for [0, 1]. Austin et al. (2021); Sahoo et al. (2024); Shi et al. (2024) extend this framework to discrete signals. Formally, we define {0, 1}V as onehot vectors, where is the vocabulary size and is the simplex over this vocabulary. For finite-time processes, we let be the number of time steps. For each time step Importantly, despite the recent success of MDLM (Sahoo et al., 2024) and other absorbing state discrete diffusion models, these models suffer from key drawback, which we call the failure to remask property. Namely, the posterior formula in (2) implies that any unmasked token zt = must remain unchanged throughout the entire denoising process, for all t, which Sahoo et al. (2024) formalize as q(zs zt, x) = Cat(zs; zt) if zt = m. Once token is decoded, this prediction is locked-in, limitation that these diffusion models share with AR models. As in Sahoo et al. (2024), denoising neural network xθ is used for parameterizing pθ(zszt) = q(zszt, xθ(zt)) and 2 Remasking Discrete Diffusion Models with Inference-Time Scaling trained using the following objective: = Ez0q(z0x) (cid:20) log pθ(x z0) (cid:21) where qσ(z1 x) = q(z1) = is the same limiting distribution as in masked diffusion. We construct the posteriors qσ(zs zt, x) as: (3) + t{ 1 ,...,1} Eztq(ztx)T (cid:20) αt αs 1 αt (cid:21) log(xθ(zt)x) Discrete Guidance An important property of diffusion models is their amenability to controlled generation through the mechanisms of classifier-based (CBG; Dhariwal & Nichol (2021)) and classifier-free guidance (CFG; Nichol et al. (2021); Ho & Salimans (2022)). Recently, these mechanisms have been extended to discrete diffusion (Nisonoff et al., 2024; Schiff et al., 2024). Letting, γ represent hyperparameter that controls guidance strength and the label of the desired class output, following the presentation in Schiff et al. (2024), we implement discrete CBG (D-CBG) by drawing samples for z(ℓ) from tempered distribution: pγ(z(ℓ) z(1:L) , z(1:L) , y) pϕ(y z(ℓ) z(1:L) ), (4) where pϕ is classifier trained on noisy sequences and evaluated on sequences where the ℓth token in z(1:L) is replaced by z(ℓ) . For discrete CFG (D-CFG), we sample from )γpθ(z(ℓ) z(1:L) pγ(z(ℓ) pθ(z(ℓ) , y) z(1:L) , y)γ pθ(z(ℓ) z(1:L) )(1γ), (5) where the same denoising network xθ is used to model both the conditional and unconditional distributions, with the class label simply masked-out for the latter. 3. Remasking Diffusion Models In this work, we alleviate the failure to remask limitation that affects state-of-the-art masked diffusion models. Our strategy is to design probabilistic model similar to MDLM (Sahoo et al., 2024), but whose denoising posterior generalizes (2) and supports remasking. Specifically, we will define discrete diffusion model whose posterior for zt = has the following form: (cid:40) Cat(zs; (1 σt)x + σtm), Cat(zs; αs(1σt)αt 1αt + 1αsσtαt 1αt zt = m), zt = m. (8) Observe how (8) maintains the remasking property laid out in (6). Additionally, our chosen form for q(zs zt = m, x) ensures that the marginals for qσ(zt x) are the same as in classical masked diffusion (e.g., Sahoo et al. (2024)). This property will yield an ELBO for ReMDM that is very similar to that of MDLM, and will help us argue for using the ReMDM sampling procedure on top of pre-trained MDLM model. Formally, we establish the following result (proof in Appendix A.1): Theorem 3.1. Given the posterior in (8), the marginal distribution qσ(zt x) is the same as in (1). In Appendix A.2, we demonstrate that this ability to remask necessitates that ReMDM is non-Markovian process (as in e.g., DDIM (Song et al., 2020a)). Intuitively, this is necessary because the latents zt can transition from the masked state back to x, and therefore the transition kernel needs to be conditioned on x. Understanding the Role of σt To ensure that the posterior in (8) is valid probability distribution, we place following constraints on σt (see derivation in Appendix A.3): 0 σt min (cid:110) 1, (cid:111) 1 αs αt =: σmax . (9) Further examining (8), we see that as σt increases, we move mass away from zt. Importantly, this means that when zt is unmasked, σt directly controls the probability of remasking. When σt = 0, we recover the posterior from MDLM in (2). Thus, ReMDM can be seen as more general formulation that admits MDLM as special case. q(zs zt = x, x) = (1 σt)x + σtm. (6) 3.2. Negative Evidence Lower Bound The parameter σt gives us the flexibility of remasking decoded token during generation. Because of this property, we dub our method ReMasking Diffusion Models (ReMDM). 3.1. ReMDM Forward Processes We define the complete forward process of ReMDM as: qσ(z0:1 x) = qσ(z1 x) (cid:89) i=1 qσ(zs zt, x) (7) We define the joint distribution of the parameterized forward process pθ as follows: pθ(x)pθ(z0:1 x) = pθ(z1)pθ(x z0) (cid:89) pθ(zs zt). (10) We define pθ(z1) = π and parameterize pθ(zs zt) = q(zs zt, xθ(zt)), where xθ(zt) is denoising model. i=1 With this parameterization, the NELBO for ReMDM is (see 3 Remasking Discrete Diffusion Models with Inference-Time Scaling Appendix A.4 for details): Algorithm 1 Sampling with ReMDM. Lσ = Ez0q(z0x) (cid:20) (cid:21) log pθ(x z0) (11) +E t{ 1 ,...,1} Eztq(ztx)T (cid:20) (1 σt)αt αs 1 αt log(x (cid:21) . θ x) Since αt αs, each term in the second expectation increases monotonically in σt. When σt = 0, for all t, we recover the MDLM objective from (3). This implies that the MDLM NELBO produces tighter bound compared to ReMDM. Furthermore, since the diffusion loss term in (11) is simply reweighted version of that in (3), one can presumably reuse weights from model trained with (3), i.e., using different σt at training and inference. Indeed, we find this to be performant strategy in practice. Moreover, we find the performance (test perplexity) between models trained with the MDLM and the ReMDM objectives to be comparable (see Appendix E.1), further justifying our reuse of pre-trained weights, with the added benefit of more flexible sampling unlocked by ReMDM. 4. ReMDM Samplers In practice, we recommend using ReMDM with different σt at training and inference, just as one might switch to different noise schedule during sampling. In particular, using σt = 0 for training is equivalent to combining the ReMDM sampler with pre-trained MDLM model xθ, which works well and does not require re-training xθ. Using ReMDM for sampling opens large design space for choosing σt. In the following, we explore several practical design strategies for σt that significantly improve performance, and in Section 5 we describe further add-ons to the sampler (e.g., nucleus sampling) that also improve quality. The high-level pseudocode for the ReMDM sampler is provided in Algorithm 1, with more detailed algorithms for implementing the schedules below deferred to Appendix C. 4.1. Design Strategies for the Remasking Schedule σt Here, we list several strategies for setting σt [0, σmax ], which can either be employed separately or in conjunction with the strategies introduced in Section 4.2. Note that for sequences, we can assign different σ(ℓ) for each token. When we omit this superscript, this implies that the same σt is used for all tokens. Max-Capped Schedule We can potentially reduce the maximum probability of remasking to constant ηcap [0, 1]. Concretely, we let σt = min{ηcap, 1αs }, for all αt [0, 1]. We denote this schedule as ReMDM-cap. Rescaled Schedule Alternatively, we can temper the chances of remasking by setting σt = ηrescale σmax , 4 // Differences to standard MDLM sampling noted in brown. Input: pre-trained denoising network xθ (e.g., MDLM), number of timesteps , noise schedule αt, remasking schedule σt. Initialize zt = m. for = to 1 do = i/T, = (i 1)/T . Set αt, αs according to noise schedule. Set σt [0, σmax Compute approximate posterior: ] according to remasking schedule. pθ(zs zt) = qσ(zs zt, = xθ(zt)) (cid:40) = Cat(zs; (1 σt)xθ + σtm), Cat(zs; αs(1σt)αt xθ + 1αsσtαt zt = m), zt = 1αt 1αt Sample zs pθ. Set zt = zs. end for Output: zt. with ηrescale [0, 1] as hyperparameter that controls this rescaling. We denote this schedule as ReMDM-rescale. Confidence-Based Schedule In conjunction with the two strategies above, we explore further reweighing of σt which is based on the intuition that tokens of which the denoising model is less confident should be assigned larger probability of remasking. Consider the ℓ-th token in sequence of latents at time t. For each ℓ {1, . . . , L}, we store its decoding probability at the time τ at which it was last unmasked. Concretely, if z(ℓ) = m, then we define := . Thus, ψ(ℓ) ψ(ℓ) serves as confidence score for unmasked tokens. τ . If zt = m, then ψ(ℓ) θ,τ z(ℓ) := x(ℓ) t We then compute = η(ℓ) σ(ℓ) conf σt , where η(ℓ) conf = exp(ψ(ℓ) ) l=1 exp(ψ(ℓ) (cid:80)L . ) With this schedule, masked tokens are decoded using the approximate posterior from MDLM, and the unmasked tokens are remasked negatively proportional to their confidence. We denote this schedule as ReMDM-conf. 4.2. Design Strategies for Turning On ReMDM There may be certain periods of the generation process where remasking is more valuable and some when it is detrimental / can slow down sampling, e.g., at the beginning of sampling, one may wish to generate some tokens in sequence using standard MDLM decoding, and only after generating reasonable starting candidate, then spend some Remasking Discrete Diffusion Models with Inference-Time Scaling computational budget fixing mistakes via the remasking posterior. We, therefore, propose two methods for optionally turning on/off ReMDM sampling, which amount to the following modification of the σt schedules above: σt = (cid:40) σt, 0, if [ton, tof ), with ton > tof otherwise. (12) Switch We choose some tswitch (0, 1] and in (12) we have [ton, tof ) = [tswitch, 0). We denote this strategy as ReMDM-switch. Loop In this strategy, we set both ton, tof (0, 1]. Furthermore, in the range when ReMDM is activated, we modify the noise schedule to be constant, such that αt = α(ton). As shown in Figure 2, this divides the sampling process into three phases. In the first phase, the model generates tokens without remasking (σt = 0, i.e., using MDLM). In the second phase, we hold α constant (i.e., αs = αt), and the model can correct potential mistakes by remasking and predicting fixed proportion of the generated tokens in loop. Finally, in the third phase, we let the model predict any remaining unmasked tokens using the MDLM posterior. We denote this strategy as ReMDM-loop. Note that we can also change the amount of computation spent in each of the three phases by taking non-fixed-width steps. Figure 2. Illustration of αt and σt schedules in the ReMDM-loop. 4.3. Comparison with Discrete Predictor-Corrector Samplers Previous works, e.g., the forward-backward (FB; Campbell et al. (2024)) and discrete flow matching (DFM; Gat et al. (2024)) correctors, propose to tackle the failure to remask property with discrete predictor-corrector samplers, special type of discrete diffusion samplers that decompose single sampling step into one predictor step followed by certain number of corrector steps that remediate possible mistakes without changing the marginals. Here, we demonstrate that these methods are either special case or reparameterization of ReMDM sampling by first noting the following result (see Appendix A.5 for the proof): Theorem 4.1. qσ(zs zt, x) from (8) is equivalent to an MDLM predictor step qpredictor(zsp zt, x) followed by 5 corrector step qcorrector(zsc zsp , x) in the form of (cid:40) Cat(zsc; (1 σt)x + σtm), + 1(1+σt)αs Cat(zsc; σtαs 1αs 1αs zsp = m), zsp = m. (13) Since the only constraint on ReMDMs sampler is the bound on σt, ReMDM offers general framework for the design of remasking samplers for discrete diffusion models. We now formalize the generality of ReMDM (proofs provided in Appendices A.6 & A.7). Proposition 4.2. The FB corrector (Campbell et al., 2022) on MDLM is special case of ReMDM where σt = αsαt . Proposition 4.3. The DFM corrector (Gat et al., 2024) on MDLM is reparameterization of ReMDM where σt = βt(αsαt) . βt denotes the corrector schecule. αt αt 5. Experiments 5.1. ReMDM Improves Sample Quality 5.1.1. TEXT GENERATION Experimental Setup We test the text generation capability of ReMDM with unconditional generation from models trained on OpenWebText (OWT; Gokaslan & Cohen (2019)). The OWT dataset was tokenized using the gpt-2 tokenizer (Radford et al., 2019) and sequences were wrapped to max length of = 1024. Our baselines consist of pretrained AR, SEDD (Lou et al., 2024), and MDLM models with checkpoints taken from the implementation in Sahoo et al. (2024). We also compare to the MDLM pre-trained models with the FB (Campbell et al., 2022) and DFM (Gat et al., 2024) correctors during inference. For our method, we also reuse the MDLM checkpoint and experiment with different ReMDM remasking schedules, specifically the max-capped, rescaled, confidence-based, and loop schedules (see Appendix E.2.3 for the full set of results). We generate 5,000 samples from each model / sampler and evaluate methods using the MAUVE score (Liu et al., 2021; Pillutla et al., 2021), as this metric balances sample quality and diversity. We also report the perplexity of the generated sequences under pre-trained GPT-2 Large model (Gen PPL.) to measure quality and average sequence entropy to reflect diversity of generated sequences as in (Zheng et al., 2024). Floating-Point Precision As indicated in Zheng et al. (2024), previous masked diffusion models (Lou et al., 2024; Sahoo et al., 2024) report sampling with 32-bit floating point precision, which was found to significantly curb the diversity of generated samples. We therefore use 64-bit floating point for all text-sampling experiments. Nucleus Sampling We observe that nucleus sampling, (Holtzman et al., 2019) is critical for generating high-quality Remasking Discrete Diffusion Models with Inference-Time Scaling text sequences. We therefore use this strategy (with top-p = 0.9) for all models (except SEDDit doesnt output logits). Results In Table 1, we report results for the best ReMDM setting. For inference time scaling results (T 1024), we use the max-capped schedule (ηcap = 0.02) in conjunction with the loop strategy (ton = 0.55, tof = 0.05, and α(ton) = 0.9 held constant in the ReMDM-loop). For faster sampling (T < 1024), we use the max-capped schedule (ηcap = 0.04) on its own. As shown in Table 1, ReMDM scales favorably with inference time compute, achieving 15.62 MAUVE score compared to the masked diffusion models and 2.23 MAUVE score compared to MDLM with corrector samplers. In contrast, masked diffusion models scale poorly with and corrector sampler methods saturate when is large. We also explore the scenario of faster sampling (i.e., < = 1024). Even with more limited computational budget, ReMDM is able to generate sequences with higher quality than the baselines. Ablation: Function of Each Component In Figure 3, we sequentially ablate each building block of ReMDM and report the MAUVE score of samples. Starting with MDLM, we see that each of our proposed sampling improvements increases the MAUVE score, with the largest improvement coming from remasking ability of ReMDM. Figure 3. Effect of ReMDM components on OWT generation quality. Inference-time scaling with {1024, 2048, 4096}. Ablation: Importance of Nucleus Sampling In Appendix E.2.2, we present the generation quality of models without nucleus sampling. We find that discrete diffusion models suffer from poor generation quality, specifically due to failure to effectively reduce generative perplexity when nucleus sampling is removed. These results demonstrate the importance of nucleus sampling for discrete diffusion models in text generation. Ablation: Sensitivity to η In Appendix E.2.4, we show the parameter search results of ReMDM-cap, ReMDM-loop, and ReMDM-rescale. For larger , as η increases, the generative quality tends to first increase and then decrease. 6 5.1.2. IMAGE GENERATION Experimental Setup We use pretrained MaskGiT model (Chang et al., 2022) that was trained on ImageNet (Deng et al., 2009) samples with 256 256 pixels. The images were patchified and flattened to sequence of = 256 and encoded according to codebook with 1024 tokens (Esser et al., 2021). See Chang et al. (2022) for more details about the model and training settings. We compare ReMDMs sampler to the original MaskGiT sampling and to MDLM. Note that MaskGiT does not follow an ancestral sampling method but rather relies on an effective heuristic where masked tokens are decoded proportional to model confidence. However, MaskGiT is still restricted by the failure to remask property. For each sampler, we conditionally generate 50,000 images, with class labels randomly chosen from the 1,000 ImageNet categories, and we measure sample quality using Frechet Inception Distance (FID; Heusel et al. (2017)) and Inception Score (IS; Salimans et al. (2016)). For all models, we explore the effect of scaling inference compute and tuning sampling temperature. Results We report the best results for each method in Table 2 (see Appendix E.3 for the more complete hyperparameter search). For MaskGiT, we found best results using no temperature, and for MDLM and ReMDM, we use temperature of 0.8. For our sampler, we report the ReMDM-rescale strategy with ηrescale = 0.05. Although for the smallest decoding setting MaskGiT outperforms the other methods, we see that ReMDM has the best scaling, producing the highest quality images of any model at = 64. Ablation: Max-capped vs. Rescaled schedules In Appendix E.3, we present the results from the full ReMDM parameter search for this experiment. We find that here ReMDM-rescale slightly outperforms ReMDM-cap and that for both increasing their corresponding η parameters within the tested values leads to improved results. 5.2. ReMDM Improves Guidance Experimental Setup We follow the setup from Schiff et al. (2024) to explore controlled small molecule generation. Specifically, we use the QM9 dataset (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014) of 133k molecules and their character-based SMILES string representations (Weininger, 1988). Sequences were tokenized using regular expression method (Schwaller et al., 2019) and padded to maximum length of = 32. We use the D-CFG and D-CBG methods defined in Schiff et al. (2024) to conditionally generate molecules with higher ring counts (greater than 90th percentile in the original dataset). We vary the guidance strength γ {1, 2, 3, 4, 5} and scale inference steps {32, 64, 128}. Our baselines consist of an AR model guided using either CFG or the popular classifierbased FUDGE method (Yang & Klein, 2021), MDLM, and Remasking Discrete Diffusion Models with Inference-Time Scaling Table 1. ReMDM improves sample quality in the case of inference-time scaling and faster sampling. ReMDM outperforms state-of-the-art masked diffusion models (SEDD; Lou et al. (2024), MDLM; Sahoo et al. (2024)) and masked diffusion models with corrector samplers such as Forward-Backward (FB; Campbell et al. (2022)) and Discrete Flow Matching (DFM; Gat et al. (2024)) corrector samplers. indicates nucleus sampling. For each , the best diffusion MAUVE score is bolded."
        },
        {
            "title": "Method",
            "content": "Data AR (T=1024) SEDD (absorb) MDLM MDLM+FB MDLM+DFM ReMDM SEDD (absorb) MDLM MDLM+FB MDLM+DFM ReMDM MAUVE () Gen PPL. () Entropy () 1.00 0.760 T=2048 0.008 0.037 0.197 0.294 0.610 T=256 0.007 0.023 0.084 0.144 0.216 T=1024 0.008 0.042 0.133 0.254 0.403 T=128 0.007 0.015 0.064 0.041 0. 14.8 12.1 T=2048 103.2 51.3 28.6 21.0 22.8 T=256 110.1 55.8 39.6 26.5 30.5 T=4096 0.009 0.035 0.243 0.269 0.656 T=512 0.008 0.031 0.100 0.211 0. T=1024 104.7 51.3 33.8 21.7 28.6 T=128 119.2 61.5 42.8 37.9 42.5 5.44 5.22 T=2048 5.61 5.46 5.28 5.19 5.30 T=256 5.63 5.49 5.41 5.26 5. T=4096 5.61 5.45 5.18 5.17 5.20 T=512 5.62 5.48 5.38 5.23 5.21 T=4096 102.5 50.9 22.8 20.7 17.6 T=512 107.2 53.0 37.1 23.3 21.1 T=1024 5.62 5.46 5.35 5.20 5.38 T=128 5.65 5.52 5.44 5.31 5. Table 2. ReMDM produces the highest quality images. Values reflect FID / IS for varying on discretized ImageNet conditional generation. For each metric and , the best value is bolded. Metric Sampler = 16 = 32 = FID () IS () MaskGiT MDLM ReMDM 6.74 7.88 7.40 4.92 5.37 4.92 4.85 4.69 4. 155.32 MaskGiT MDLM 140.97 ReMDM 145.27 181.57 169.79 182.05 196.38 187.93 209.45 the uniform diffusion language model (UDLM) proposed in Schiff et al. (2024), which was also introduced to alleviate the no remasking limitation of absorbing state diffusion. For ReMDM sampling, we use pretrained MDLM models. We explore the various strategies defined in Section 4 (see Appendix E.4 for full search results). After generating 1,024 sequences, we use the RDKit library (Landrum et al., 2013) to determine whether sequences are valid and compute novelty of the generated sequences (number of unique valid sequences that do not appear in the original QM9 dataset) and mean ring count of novel sequences. Results In Figure 4, we display the trade-off that comes from increasing guidance strength γ. We only visualize results for samples that had at least 50 novel sequences. For both forms of guidance, CFG and CBG, ReMDM outperforms AR and diffusion approaches, pushing the noveltyproperty maximization frontier beyond that of the baseline methods. Additionally, ReMDM scales favorably with more inference-time compute, seen by the curves for larger dominating those for smaller . For ReMDM, we found the best strategy to be combination of the ReMDM-rescale (with ηrescale = 0.9) and ReMDM-conf schedules. For calculating confidence scores, we use the log-probabilities of the conditional denoising network. For D-CFG, we use the loop strategy, with ton = 0.25, tof = 0.125. In addition to these findings, in Appendix E.4.1, we present experimental results for maximizing different property of interest, drug-likeness (QED; Bickerton et al. (2012)). Ablation: Sensitivity to ηrescale In Appendix E.4, we report extensive parameter sweeps for ReMDM. We find that larger ηrescale values tend to perform better than smaller ones and relatively little sensitivity in the range ηrescale 0.5 (see Appendix E.4.2). Ablation: Importance of using Confidence-based Schedule Our parameter tuning revealed that using ReMDMrescale in conjunction with the confidence-based scheduler led to improved results (see Appendix E.4.3). Ablation: Tuning Switch / Loop Parameters We observe trend where for D-CFG results improve when the switch / loop start parameter (tswitch / ton) are smaller, i.e., we turn on ReMDM later in the decoding process. In contrast, for D-CBG, when maximizing ring count, we observe the opposite trend, where, generally, starting ReMDM earlier in the sampling process improves results (see Appendix E.4.4). Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 4. ReMDM improves steer-ability by extending the novelty-property maximization frontier. Controlled generation for ring count maximization on QM9 dataset with varying inference compute and guidance strength γ. (Left) Discrete classifier-free guidance (D-CFG). (Right) Discrete classifier-based guidance (D-CBG) and FUDGE for AR. 6. Discussion and Related Works Denoising Diffusion Implicit Models An analogy can be drawn between our work and DDIM (Song et al., 2020a), where ReMDM is to MDLM for discrete signals as DDIM is to DDPM (Ho et al., 2020) for continuous data. That is, stemming from similar insights about the NELBO depending only on marginals, both our work and DDIM propose alternative forward and backward processes that maintain marginals while enabling more flexible generation relative to previous state-of-the-art diffusion models. Of note, Song et al. (2020a) also present way of adapting non-Markovian processes to discrete domains; however, they focus on uniform categorical as the limiting distribution. In Appendix B, we demonstrate how this can be adapted for absorbing state diffusion and derive an equivalence to our method. Discrete Predictor-Corrector Samplers Continuous time Markov chain (CTMC) theory provides framework for samplers that correct errors in the reverse process, extending the original predictor-corrector formulation for continuous data (Song et al., 2020b) to discrete domains. In addition to the FB and DFM correctors discussed above, other correctors include: the Stein operator (Zhao et al., 2024), and DPC (Lezama et al., 2023) which improves the sample quality of MaskGiT by corrector sampling. Unlike the plug-and-play methods of FB, DFM, and ReMDM, the Stein operator and DPC correctors require additional training. Although ReMDM sampler can be reformulated as equivalent to the DFM corrector sampler (Proposition 4.3), ReMDM outperforms this method  (Table 1)  . For DFM, we use the hyperparameters that were found to produce best results in Gat et al. (2024). However, unlike ReMDM the best DFM schedule was not designed with focus on reIn Figure 5, we depict masking and when to utilize it. the remasking schedule corresponding to results in Table 1 (T = 4096) for DFM and ReMDM, an illustrative example of this claim (see full results in Appendix E.2.1). The DFM schedule demonstrates spike in the beginning and sharp decay afterward. In contrast, the ReMDM-loop schedule is designed to provide non-trivial remasking probability after good candidate sequence has been generated by standard MDLM generation. Additionally, ReMDM possesses other theoretical advantages over DFM, namely the bound on σt and the NELBO analysis of the underlying probabilistic generative models that are not present in Gat et al. (2024). Figure 5. ReMDM and DFM σt schedules for OWT. 7. Conclusion In this work, we have presented novel family of absorbing state discrete diffusion samplers. Our method leverages the strong language modeling performance of this class of models by enabling the use of pretrained weights with the added benefit of more flexible sampling strategies that allow for remasking of predicted tokens. We demonstrate empirically that this leads to improved sample quality for both unconditional and conditional generation and leads to better controlled generation. Our approach also unlocks an 8 Remasking Discrete Diffusion Models with Inference-Time Scaling important inference-time compute scaling axis that is more limited for existing masked diffusion models."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, specifically those related to the generation of synthetic text and images. Our work can also be applied to the design of biological sequences, which carries both potential benefits and risks."
        },
        {
            "title": "References",
            "content": "Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):9098, 2012. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Campbell, A., Yim, J., Barzilay, R., Rainforth, T., and Jaakkola, T. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Falcon, W. and The PyTorch Lightning team. PyTorch Lightning, March 2019. URL https://github.com/ Lightning-AI/lightning. Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Synnaeve, G., Adi, Y., and Lipman, Y. Discrete flow matching. arXiv preprint arXiv:2407.15595, 2024. Gokaslan, A. and Cohen, V. Openwebtext corhttp://Skylion007.github.io/ pus. OpenWebTextCorpus, 2019. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Rıo, J. F., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019. Hunter, J. D. Matplotlib: 2d graphics environment. Computing in Science & Engineering, 9(3):9095, 2007. doi: 10.1109/MCSE.2007.55. Landrum, G. et al. Rdkit: software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8(31.10):5281, 2013. Lezama, J., Salimans, T., Jiang, L., Chang, H., Ho, J., and Essa, I. Discrete predictor-corrector diffusion models for image synthesis. In The Eleventh International Conference on Learning Representations, 2023. Liu, L., Pillutla, K., Welleck, S., Oh, S., Choi, Y., and Harchaoui, Z. Divergence frontiers for generative models: Sample complexity, quantization effects, and frontier integrals. Advances in Neural Information Processing Systems, 34:1293012942, 2021. 9 Remasking Discrete Diffusion Models with Inference-Time Scaling Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing arXiv preprint with text-guided diffusion models. arXiv:2112.10741, 2021. Nisonoff, H., Xiong, J., Allenspach, S., and Listgarten, J. Unlocking guidance for discrete state-space diffusion and flow models. arXiv preprint arXiv:2406.01572, 2024. Ou, J., Nie, S., Xue, K., Zhu, F., Sun, J., Li, Z., and Li, C. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. pandas development team, T. pandas-dev/pandas: Pandas, February 2020. URL https://doi.org/10. 5281/zenodo.3509134. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Wallach, H., Larochelle, H., Beygelzimer, A., dAlche Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc., 2019. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S., Choi, Y., and Harchaoui, Z. Mauve: Measuring the gap between neural text and human text using divergence frontiers. Advances in Neural Information Processing Systems, 34:48164828, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Ramakrishnan, R., Dral, P. O., Rupp, M., and Von Lilienfeld, O. A. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):17, 2014. 10 Ruddigkeit, L., Van Deursen, R., Blum, L. C., and Reymond, J.-L. Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of chemical information and modeling, 52(11):28642875, 2012. Sahoo, S. S., Arriola, M., Gokaslan, A., Marroquin, E. M., Rush, A. M., Schiff, Y., Chiu, J. T., and Kuleshov, V. Simple and effective masked diffusion language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=L4uaAR4ArM. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022."
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Improved techniques for Radford, A., and Chen, X. training gans. Advances in neural information processing systems, 29, 2016. Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A., Pierrot, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. Schwaller, P., Laino, T., Gaudin, T., Bolgar, P., Hunter, C. A., Bekas, C., and Lee, A. A. Molecular transformer: model for uncertainty-calibrated chemical reaction prediction. ACS central science, 5(9):15721583, 2019. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. K. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Remasking Discrete Diffusion Models with Inference-Time Scaling Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Waskom, M. L. seaborn: statistical data visualization. Journal of Open Source Software, 6(60):3021, 2021. doi: 10.21105/joss.03021. URL https://doi.org/10. 21105/joss.03021. Weininger, D. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):3136, 1988. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. Yadan, O. Hydra - framework for elegantly configuring complex applications. Github, 2019. URL https:// github.com/facebookresearch/hydra. Yang, K. and Klein, D. Fudge: Controlled text genarXiv preprint eration with future discriminators. arXiv:2104.05218, 2021. Zhao, Y., Shi, J., Mackey, L., and Linderman, S. Informed correctors for discrete diffusion models. arXiv preprint arXiv:2407.21243, 2024. Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. 11 Remasking Discrete Diffusion Models with Inference-Time Scaling A. Theoretical Results A.1. Proof of Theorem 3.1 Proof. We proceed by induction. The starting point q(z1 x) = Cat(z1; α1x + (1 α1)m) is true by design. For the induction hypothesis, assume that for any < 1, q(zt x) = Cat(zt; αtx + (1 αt)m). Recall that in absorbing state processes, given x, for all [0, 1], zt {x, m}. Then for the previous time step s, we have: q(zs = x) = q(zs = zt = x, x)q(zt = x) + q(zs = zt = m, x)q(zt = x) = (1 σt)αt + αs (1 σt)αt 1 αt (1 αt) = αs, and q(zs = x) = q(zs = zt = x, x)q(zt = x) + q(zs = zt = m, x)q(zt = x) = σtαt + 1 αs σtαt 1 αt (1 αt) = 1 αs. Combining (14) and (15) yields q(zs x) = Cat(zs; αsx + (1 αs)m). A.2. Deriving the non-Markovian forward step qσ(zt zs, x) By Bayes rule we have: qσ(zt zs, x) = qσ(zs zt, x)qσ(zt x) qσ(zs x) . From Theorem 3.1, we can replace the marginals qσ(zt x) and qσ(zs x) with those from (1). To derive the form for qσ(zt zs, x), we now look at the two possible values of zs = and zs = m. Case 1a: zs = m, zt = qσ(zt = zs = x, x) = qσ(zs = zt = x, x)qσ(zt = x) qσ(zs = x) = (1 σt)αt αs . Case 1b: zs = m, zt = qσ(zt = zs = x, x) = = = qσ(zs = zt = m, x)qσ(zt = x) qσ(zs = x) (cid:1)(1 αt) (cid:0) αs(1σt)αt 1αt αs αs (1 σt)αt αs . Case 2a: zs = m, zt = qσ(zt = zs = m, x) = qσ(zs = zt = x, x)qσ(zt = x) qσ(zs = x) = σtαt 1 αs . Case 2b: zs = m, zt = qσ(zt = zs = m, x) = = = qσ(zs = zt = m, x)qσ(zt = x) qσ(zs = x) (cid:1)(1 αt) (cid:0) 1αsσtαt 1αt 1 αs 1 αs σtαt 1 αs . 12 (14) (15) (16) (17) (18) (19) Remasking Discrete Diffusion Models with Inference-Time Scaling Combining (16), (17), (18), and (19) yields the non-Markovian forward process: (cid:40) qσ(zt zs, x) = Cat(zt; (1σt)αt αs Cat(zt; σtαt 1αs + αs(1σt)αt m), αs + 1αsσtαt 1αs m), zs = zs = . (20) A.3. Deriving Bounds for σt To ensure that we have defined valid probability, in both cases of the posterior in (8), since xm = 0, we require that the coefficients on both terms be within the range [0, 1]. Using this, we can derive the bounds for σt given in (9). For case where zt = m, both coefficients must satisfy: 0 σt 1. For the case where zt = m, ensuring that the coefficients are in the range [0, 1] leads to the restriction that: αt αs αt σt 1 αs αt . (21) (22) Since the noise schedule is monotonically decreasing in t, the lower bound in (22) is 0. Hence, combining (21) and (22) produces the bounds in (9). A.4. Deriving the ReMDM NELBO The variational objective or negative evidence lower bound (NELBO) for diffusion models has the following form (SohlDickstein et al., 2015): = Ez0:T q(z0:1x) (cid:20) = Ez0:1q(z0:1x) (cid:2) log(q(z0:1 x) / pθ(x)pθ(z0:1 x))(cid:3) log pθ(xz0) (cid:125) (cid:123)(cid:122) (cid:124) Lrecon + DKL[q(z1x)pθ(z1)] (cid:123)(cid:122) (cid:125) Lprior (cid:124) + (cid:88) i=1 (cid:124) DKL[q(zs zt, x)pθ(zs zt)] (cid:21) , (23) (cid:123)(cid:122) Ldif usion (cid:125) where DKL denotes the KullbackLeibler divergence. Starting from (23), we note that, in practice, we set pθ(z1) = q(z1 x) = q(z1) = π which ensures Lprior = 0. Additionally, the reconstruction loss Lrecon is equivalent in both (23) and (11). We therefore turn our attention to the ReMDM diffusion loss term Lσ dif usion = (cid:88) DKL[qσ(zs zt, x)pθ(zs zt)] (24) j= Additionally, we note that in Sahoo et al. (2024), MDLM is parameterized using the denoising xθ that is restricted in two ways. First the denoising model places zero probability mass on the [MASK] token, i.e., θ = 0. Second the model carries-over unmasked tokens, i.e., θ zt = 1, if zt = m. Below we assume that our denoising network follows similar parameterization. We break down each term in this summation by the two possible values zt = and zt = m. Case 1: zt = DKL[qσ(zs zt = x, x)pθ(zs zt = x)] = qσ(zs = zt = x, x) log = (1 σt) log 1 σt 1 σt Case 2: zt = qσ(zs = zt = x, x) pθ(zs = zt = x) σt σt = 0. + σt log + qσ(zs = zt = x, x) log qσ(zs = zt = x, x) pθ(zs = zt = x) (25) DKL[qσ(zs zt = m, x)pθ(zs zt = m)] 13 Remasking Discrete Diffusion Models with Inference-Time Scaling = qσ(zs = zt = m, x) log qσ(zs = zt = m, x) pθ(zs = zt = m) + qσ(zs = zt = m, x) log qσ(zs = zt = m, x) pθ(zs = zt = m) = = αs (1 σt)αt 1 αt log αs(1σt)αt 1αt αs(1σt)αt 1αt xxθ + 1 αs σtαt 1 αt log 1αsσtαt 1αt 1αsσtαt 1αt (1 σt)αt αs 1 αt log(xxθ) (26) The carry-over unmasked tokens property of xθ implies that log(xxθ) = log(1) = 0, which lets us write the two cases from (25) and (26) as single expression: DKL[qσ(zs zt, x)pθ(zs zt)] = (1 σt)αt αs 1 αt log(xxθ) (27) Then rewriting the summation in (24) as an expectation with sampled uniformly from { 1 recover the diffusion loss term in (11). , . . . , 1} and plugging in (27), we A.5. Proof of Theorem 4. Proof. Recall that the MDLM predictor step amounts to drawing sample zsp from the posterior given in (2), which we can reformulate as: (cid:40) q(zsp zt, x) = Cat(zsp ; zt), Cat(zsp ; αsαt 1αt zt = m, m), zt = m. + 1αs 1αt To prove the theorem statement, we must show that q(zsc zt, x) is equivalent to the ReMDM posterior from (8). We begin by noting that conditioned on the predictor sample zsp , the corrector sample zsc is independent of zt. That is We now look at the two cases of zt = and zt = m: q(zsc zsp , zt, x) = q(zsc zsp , x) Case 1a: zt = m, zsc = q(zsc = zt = x, x) = (cid:88) z{x,m} q(zsc = zsp = z, zt = x, x)q(zsp = zt = x, x) = q(zsc = zsp = x, x)q(zsp = zt = x, x) + q(zsc = zsp = m, x)q(zsp = zt = x, x) = q(zsc = zsp = x, x) = (1 σt). Case 1b: zt = m, zsc = Using the same argument as in Case 1a, we have that q(zsc = zt = x, x) = q(zsc = zsp = x, x) = σt. Case 2a: zt = m, zsc = q(zsc = zt = m, x) = (cid:88) z{x,m} q(zsc = zsp = z, zt = m, x)q(zsp = zt = m, x) = q(zsc = zsp = x, x)q(zsp = zt = m, x) + q(zsc = zsp = m, x)q(zsp = zt = m, x) = (1 σt) (cid:16) αs αt 1 αt αs (1 σt)αt 1 αt . = (cid:17) + (cid:16) σtαs 1 αs (cid:17)(cid:16) 1 αs 1 αt (cid:17) (28) (29) (30) (31) Case 2b: zt = m, zsc = q(zsc = zt = m, x) = (cid:88) z{x,m} q(zsc = zsp = z, zt = m, x)q(zsp = zt = m, x) Remasking Discrete Diffusion Models with Inference-Time Scaling = q(zsc = zsp = x, x)q(zsp = zt = m, x) + q(zsc = zsp = m, x)q(zsp = zt = m, x) + (cid:16) 1 (1 + σt)αs 1 αs (cid:17)(cid:16) 1 αs 1 αt (cid:17) = σt (cid:17) (cid:16) αs αt 1 αt 1 αs σtαt 1 αt = . (32) Combining (29), (30), (31), and (32) yields the desired result. A.6. Proof of Proposition 4.2 Proof. The forward-backward corrector sampler (Campbell et al., 2022) is derived using continuous time Markov chain theory. In particular, the rate matrix for the corrector sampler step Rcorrector is the sum of the forward rate matrix Rt and the backward rate matrix Rt, i.e. Rcorrector = Rt + Rt. We first extract the forward rate matrix Rt of MDLM using the following discretization formula. q(zt = zs = y) = δy,y + Rt(y, y)t + o(t) (33) From Sahoo et al. (2024), we know that the forward step of MDLM q(zt zs) = Cat(zt; αt αs denote the vocabulary set. zs + (1 αt αs )m). Let Case 1a: zs = m, zt = 1 + Rt(x, x)t + o(t) = q(zt = zs = x) = αt αs Rt(x, x) = lim t0 1 ( αt αs 1) = α αt Case 1b: zs = m, zt = Rt(x, m)t + o(t) = q(zt = zs = x) = 1 αt αs Rt(x, m) = lim t0 1 (1 αt αs ) = α αt Case 1c: zs = m, zt = {x, m} Rt(x, y)t + o(t) = q(zt = zs = x) = 0 Rt(x, y) = 0 Case 1d: zs = m, zt = 1 + Rt(m, m)t + o(t) = q(zt = zs = m) = 1 Rt(m, m) = 0 Case 1e: zs = m, zt = Rt(m, x)t + o(t) = q(zt = zs = m) = 0 Rt(m, x) = 0 Similarly, we can derive the backward rate matrix Rt using the following formula and MDLM posterior (2). q(zs = zt = y, x) = δy,y + Rt(y, y)t + o(t) Case 2a: zt = m, zs = 1 + Rt(x, x)t + o(t) = q(zs = zt = x) = 1 Rt(x, x) = 0 15 (34) (35) (36) (37) (38) (39) (40) Remasking Discrete Diffusion Models with Inference-Time Scaling Case 2b: zt = m, zs = Rt(x, y)t + o(t) = q(zs = zt = x) = 0 Rt(x, y) = Case 2c: zt = m, zs = Case 2d: zt = m, zs = Rt(m, x)t + o(t) = q(zs = zt = m) = Rt(m, x) = lim t0 1 αs αt 1 αt = α 1 αt αs αt 1 αt 1 + Rt(m, m)t + o(t) = q(zs = zt = m) = 1 αs 1 αt Rt(m, m) = lim 1 ( 1 αs 1 αt 1) = α 1 αt Case 2e: zt = m, zs = {x, m} Rt(m, y)t + o(t) = q(zs = zt = y) = 0 Rt(m, m) = 0 (41) (42) (43) (44) By adding Rt and Rt, we get the rate matrix of the forward-backward corrector step. The next step is to discretize it. Concretely, we apply (39) to the forward-backward rate matrix. qF B(zsc zsp , x) = (cid:40) Cat(zsc; 2αtαs αt Cat(zsc; αsαt 1αt + αsαt αt + 1αs 1αt m), m), zsp = zsp = m. (45) Note that (45) keeps the marginal at time unchanged while (13) keeps the marginal at time unchanged. In order to conduct fair comparison, we rewrite the time version of (13) as follows. qcorrector ReM DM (zsc zsp , x) = (cid:40) Cat(zsc ; (1 σt)x + σtm), + 1(1+σt)αt Cat(zsc; σtαt 1αt 1αt zsp = m), zsp = m. (46) Comparing (45) and (46), we can find that (45) is special case where σt = αsαt αt . A.7. Proof of Proposition 4.3 Proof. The DFM corrector sampler (Gat et al., 2024) is derived from generating velocity ucorr transformation equation. using the following zsc δzt() + ucorr (, zt)t (47) The corrector generating velocity is defined as weighted sum of the forward sampling generating velocity ˆut and the backward sampling generating velocity ˇut. ucorr (, zt) = (1 + βt) ˆut(, zt) βt ˇut(, zt) The weighting coefficient βt is referred to as the corrector schedule and can be user-specified. The forward and backward sampling generating velocities take the following forms: ˆut(, zt) = α 1 αt (cid:2)p0t( zt) δzt()(cid:3) (48) (49) Remasking Discrete Diffusion Models with Inference-Time Scaling ˇut(, zt) = α αt (cid:2)δzt() p1t( zt)(cid:3) (50) Note that our formulation is slightly different from the original presentation in Gat et al. (2024), since in our notation as goes from 1 to 0, we move from noise to the target distribution, whereas in the flow matching literature this direction is reversed. Plugging (49) and (50) into (48), we derive the following form for ucorr : ucorr (, zt) = (cid:104) (1 + βt)α 1 αt + βtα αt (cid:105) δzt() (1 + βt)α 1 αt p0t( zt) βtα αt p1t( zt) (51) By plugging (51) into (47), we have that zsc (cid:2)1 + (1 + βt)α 1 αt tt + βtα tt αt (cid:3)δzt() (cid:2)1 + (1 + βt)(αt αs) 1 αt + βt(αt αs) αt We can rewrite this as (1 + βt)α 1 αt (cid:3)δzt() tt p0t( zt) βtα tt αt p1t( zt) (1 + βt)(αt αs) 1 αt p0t( zt) βt(αt αs) αt p1t( zt) (52) qDF (zsc zt, x) = (cid:40) Cat(zsc; (1 + βt(αtαs) Cat(zsc; (1+βt)(αsαt) αt )x + βt(αsαt) αt + (1 + (1+βt)(αtαs) m), 1αt 1αt zt = )m), zt = m. (53) Comparing (53) and (8), we see that (53) is equivalent to (8) where σt = βt(αsαt) αt . B. Comparison to DDIM Non-Markovian Processes In DDIM (Song et al., 2020a), the authors present non-Markovian forward processes for both continuous and discrete signals. For discrete data, although in DDIM the proposed method assumes uniform categorical as the limiting distribution, here we demonstrate how one can adapt the proposed method in DDIM for absorbing state diffusion and derive an equivalence between ReMDM and this new DDIM process under reparameterization of σt. For clarity, we use σDDIM to denote the parameter from DDIM and σReM DM to denote the parameter used in our work. In DDIM, the processes assuming uniform categorical distribution as the limiting distribution is defined to have the following posteriors: q(zs zt, x) = (αs σtαt)x + σDDIM zt + (1 αs (1 αt)σDDIM )V 11, (54) where 1 is column vector of ones and is the vocabulary size. We can apply the methodology from DDIM to absorbing state diffusion by replacing the limiting distribution 1/V in (54) with m, which produces: q(zs zt, x) = (αs σtαt)x + σDDIM zt + (1 αs (1 αt)σt)m (cid:40) (αs + σDDIM (αs σDDIM = (1 αt))x + ((1 αs) (1 αt)σDDIM αt)x + (1 αs + αtσDDIM )m )m zt = zt = (55) Comparing the zt = case in (55) to that in the ReMDM posterior in (8), we can derive an equivalence between our proposed method and that from DDIM with the following reparameterization: σReM DM = 1 αs (1 αt)σDDIM . (56) Plugging this reparameterization into the zt = case in (8) yields an equivalence to the zt = case in (55) as well. In addition to extending the discrete formulation from DDIM to absorbing state processes, we believe that our formulation is easier to analyze relative to the one when reparameterizing to use σDDIM , e.g., deriving bounds on σt is more straightforward for our work and it is more natural to explore the various design decisions defined in Section 4 using our formulation. 17 Remasking Discrete Diffusion Models with Inference-Time Scaling C. ReMDM Sampler Algorithms Below we present algorithms for ReMDM-switch (Algorithm 2) and ReMDM-loop (Algorithm 3) both of which can optionally combine with the various strategies for setting σt described in Section 4.1. Algorithm 2 Sampling with ReMDM-switch. Input: pre-trained denoising network xθ (e.g., MDLM), number of timesteps , number of tokens in sequence L, noising schedule αt, maximum value for remasking ηcap [0, 1], rescale value for remasking ηrescale [0, 1], boolean value for whether to use confidence strategy use conf, time for switching on ReMDM tswitch (0, 1). Initialize z(1:L) Initialize ψ(1:L) for = to 1 do = {m}L. = {}L. = i/T, = (i 1)/T . Set αt, αs according to noise schedule. if tswitch then σ(ℓ) = ηrescale min{ηcap, (1 αs)/αt} for all ℓ = {1, . . . , L}. if use conf then Compute η(ℓ) σ(ℓ) = η(ℓ) end if conf = exp(ψ(ℓ)) for all ℓ {1, . . . , L}. for all ℓ {1, . . . , L}. conf σ(ℓ) exp(ψ(ℓ )) (cid:80) else σ(1:L) = {0}L. end if For all ℓ {1, . . . , L}, compute approximate posterior: z(1:L) pθ(z(ℓ) (cid:40) Cat(zs; (1 σ(ℓ) Cat(zs; αs(1σ(ℓ) z(1:L) ) = qσ(z(ℓ) θ + σ(ℓ) )x(ℓ) m), θ + 1αsσ(ℓ) x(ℓ) αt )αt , = x(ℓ) = 1αt 1αt θ (z(1:L) )) zt = m), zt = Sample z(ℓ) if use conf then pθ for all ℓ {1, . . . , L}. Store confidence scores ψ(ℓ) = (z(ℓ) )xθ(z(1:L) )(ℓ), for all newly decoded z(ℓ) = z(ℓ) = m. end if Set z(1:L) end for output z(1:L) . = z(1:L) . D. Additional Experimental Details D.1. OpenWebText In this experiment, we reuse the pretrained AR, SEDD, and MDLM checkpoints released by (Sahoo et al., 2024) where the diffusion models are trained using log-linear schedule, i.e., αt = 1 t. AR, SEDD, and MDLM share the same architecture: Transformer-based model (Vaswani, 2017) that augments the diffusion transformer (Peebles & Xie, 2023) with rotary embeddings (Su et al., 2024) and consists of 169M parameters. The neural network is comprised of 12 layers, 12 attention heads, and 768 hidden dimensions. Please see Sahoo et al. (2024) for the full model architecture and training details. As in Sahoo et al. (2024), we use gpt-2 tokenizer for our experiments. We use the same train-validation split as in Sahoo et al. (2024) (where the last 100k documents of OWT were designated as the validation set) and randomly select 5,000 samples from the validation set to serve as the reference for MAUVE score computation. For the discrete flow matching corrector sampler, we follow Gat et al. (2024) and set the corrector schedule β(t) = At0.25(1 t)0.25 where = 10 (see Remasking Discrete Diffusion Models with Inference-Time Scaling Appendix A.7 for the definition of corrector schedule). We empirically find that nucleus sampling performs better than the temperature technique proposed in Gat et al. (2024). We also use non-fixed-width time steps as in Gat et al. (2024). For evaluation metrics, we report MAUVE scores, generative perplexity, and entropy. For MAUVE, we generate 5,000 samples for each model/sampler. We use the gpt-2 tokenizer, GPT-2 Large (Radford et al., 2019) as the embedding model, and the MAUVE scaling hyperparameter is set to 5. For generative perplexity, we use GPT-2 Large as the external model. As in (Zheng et al., 2024), we also report the average sequence entropy as diversity metric. Specifically, we compute the entropy for the number of tokens for each sequence before it is decoded by the tokenizer and then report the mean entropy value of 5,000 generated sequences. D.2. ImageNet In this experiment, we reuse the pre-trained MaskGiT model for all samplers (Chang et al., 2022). The MaskGiT architecture is Transformer model (Vaswani, 2017) that consists of 24 layers, 8 attention heads, 768 embedding dimension, and 3,072 hidden dimension. Similar to MDLM, this model implements the carry-over unmasked tokens property in the parameterization of xθ. The full model architecture and training setup details are available in Chang et al. (2022). In Table 2, the MaskGiT sampler refers to the heuristic confidence-based decoding described in Chang et al. (2022). The MDLM sampler refers to using the outputs of the pre-trained MaskGiT denoising model, then applying the zero-mask parameterization and plugging xθ into the posterior from (2). The ReMDM sampler refers to using the same xθ as that used with the MDLM sampler, but with the posterior from (8). For ReMDM, we explore the max-capped (with ηcap {0.01, 0.02, 0.05}), rescaled (with η {0.01, 0.02, 0.05}), and confidence-based schedules described in Section 4.1. We do not combine these strategies, but rather test each separately. For all three samplers, we perform sweep over softmax temperature scaling using scale parameter τ {0.6, 0.8, 1.0}. For vector y, with yi denoting its ith component, softmax temperature scaling is implemented as follows: exp(yi/τ ) exp(yj/τ ) (cid:80) . For all samplers we use log-linear schedule for αt, i.e., α(t) = 1 t. We use the code from Dhariwal & Nichol (2021) to compute FID and IS metrics. D.3. QM For the guidance experiments on QM9, we follow the setup used in Schiff et al. (2024). The dataset was tokenized using regular expression-based tokenizer (Schwaller et al., 2019) with padded max sequence lengths of = 32. Including special tokens, the tokenizer vocabulary size is = 40. The AR, MDLM, and UDLM models used for discrete CBG and CFG were taken from the implementation provide in Schiff et al. (2024). These models are based on Transformer architecture (causally masked for AR) known as DiT (Peebles & Xie, 2023). See Schiff et al. (2024) for the full model and experimental setup details. Note that Schiff et al. (2024) provide first-order approximation for D-CBG that avoids the required O(V ) calls to the classifier pϕ, which comes from needing to evaluate every possible token replacement from vocabulary of size at every position in sequence of length for each step of the sampling process of duration . However, given the relatively shorter sequences and smaller vocabulary size used in the QM9 experiments and that Schiff et al. (2024) found generally higher quality results without the first order approximation, we omit the first-order approximation from our experiments and instead use the full, un-approximated D-CBG implementation. In the main text, we report results for experiments performed to maximize the ring count value of molecules. In Appendix E.4, we also present results for maximizing the property of drug-likeness (QED), which was also explored in Schiff et al. (2024). Guidance training and inference were performed on the molecular property of interest (ring count or QED) using binarized label, where molecules with property value the 90th percentile in the dataset were labeled = 1. The QED and ring count properties were extracted for each molecule in the QM9 dataset using the RDKit library (Landrum et al., 2013). For all the models and guidance mechanisms, we report results with varying guidance strength γ {1, 2, 3, 4, 5} and timesteps {32, 64, 128}. For the diffusion models, we generate using log-linear schedule for αt, i.e., α(t) = 1 t. For evaluation, we generated 1,024 sequences with each model / guidance mechanism. We used RDKit to parse the generated sequences. Sequences that could not be parsed were deemed invalid. Of the valid molecule strings, we remove duplicates and report the number of novel sequences, which we define as valid and unique generated sequences that do not appear in 19 Remasking Discrete Diffusion Models with Inference-Time Scaling the full QM9 dataset. We then use RDKit to compute QED / ring count of the novel generated sequences and report the mean value. For ReMDM samplers, we reuse the pre-trained MDLM weights. We perform an extensive grid search for both properties, QED and ring count, and guidance mechanisms, D-CFG and D-CBG. Namely we explore the ReMDM-rescale schedule with ηrescale {0.1, 0.5, 0.9, 1.0}. We test sampling with and without the confidence-based schedule being used in conjunction with ReMDM-rescale. For each combination, we also try both the switch and loop strategies. For switch, we use tswitch {0.1, 0.5, 0.9}. For loop, we sweep over the tuples (ton, tof ) {(0.5, 0.25), (0.25, 0.125), (0.1, 0.05)}. We use non-fixed width steps in ReMDM-loop, so that for each , the loop starts at discrete step = /2 and ends at step = (1 tof ) , and we rescale time accordingly before and after the loop phases, as described in Algorithm 3. E. Additional Experimental Results E.1. Training with MDLM v.s. ReMDM NELBO In Table 3, we report QM9 dataset validation set perplexities for models trained with either the MDLM NELBO from (3) or the ReMDM NELBO from (11), with σt = min{ηcap, (1 αs)/αt}, for some ηcap (0, 1]. We follow the same model architecture and training settings defined in Schiff et al. (2024) for this dataset. Overall, we find that validation perplexities for this dataset are fairly consistent across models trained with either objective. Table 3. Training with ReMDM and MDLM NELBO objectives leads to similar validation set perplexity values (for the QM9 dataset). In the top row, the model is trained using the continuous time formulation of the MDLM objective from Sahoo et al. (2024). In the bottom two rows, models are trained with discrete-time objectives. The first column, σt = 0, corresponds to the MDLM objective from (3) and the columns with varying ηcap correspond to training with the ReMDM objective from (11) with σt = min{ηcap, (1 αs)/αt}. We find that results are comparable across training settings. σt = 0 ηcap = 0.5 ηcap = 1.0 = = 4096 = 8192 2. 2.094 2.092 2.107 2.097 2.119 2.101 E.2. OpenWebText E.2.1. REMDM σt V.S. DFM σt In Figure 6, we plot the remasking probability schedules of ReMDM and DFM correctors in the experiments reported in Table 1. Similarly to Figure 5, the DFM schedules show spike at first and long tail afterward. Here, the best performing βt = At0.25(1 t)0.25 schedule reported in Gat et al. (2024) is used. We also adjust the width of the time steps accordingly, as reported in Gat et al. (2024). E.2.2. RESULTS OF MODELS WITHOUT NUCLEUS SAMPLING In Table 4, we present the sample quality of different models without nucleus sampling. Consistent with the results in Zheng et al. (2024), when using double-precision floating-point numbers during sampling, discrete diffusion models produce poor quality sequences, as reflected by the relatively higher generative perplexity values compared to Table 1. These results underscore the importance of nucleus sampling for discrete diffusion models in text generation. E.2.3. COMPARING DIFFERENT REMDM SCHEDULES In Table 5, we report the OpenWebText unconditional generation results for various ReMDM schedules. For ReMDM-conf, we set σt proportional to the upper bound σmax. In the case of inference-time scaling (T 1024), ReMDM-loop performs the best while in the case of faster sampling (T < 1024), ReMDM-cap is found to perform better than others. In both scenarios, ReMDM-rescale and ReMDM-conf do not perform as well as the other two schedules. 20 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 6. Remasking probability schedule of ReMDM and DFM corrector on OWT. Schedules used in experiments from Table 1 are plotted. 21 Remasking Discrete Diffusion Models with Inference-Time Scaling Table 4. Sample quality of absorbing state discrete diffusion models without nucleus sampling."
        },
        {
            "title": "Method",
            "content": "AR (T=1024) T=1024 0.008 SEDD 0.009 MDLM MDLM+FB 0.009 MDLM+DFM 0.008 0.007 ReMDM T=128 0.007 SEDD 0.007 MDLM MDLM+FB 0.007 MDLM+DFM 0.005 0.006 ReMDM MAUVE () Gen PPL. () Entropy () 0.407 T=2048 0.008 0.008 0.011 0.010 0.007 T=256 0.007 0.008 0.009 0.006 0.007 T=4096 0.009 0.009 0.010 0.010 0.007 T=512 0.008 0.008 0.010 0.007 0.006 T=1024 104.7 104.8 102.6 125.9 173. T=128 119.2 121.4 112.8 360.7 160.5 35.0 T=2048 103.2 104.4 101.9 119.1 256.2 T=256 110.1 111.8 106.4 189.5 176.8 T=4096 102.5 104.1 100.9 115.7 370.6 T=512 107.2 107.2 104.4 143.5 226. T=1024 5.62 5.63 5.65 5.70 5.76 T=128 5.65 5.67 5.67 5.85 5.73 5.58 T=2048 5.61 5.63 5.65 5.69 5.81 T=256 5.63 5.65 5.66 5.77 5.75 T=4096 5.61 5.63 5.64 5.69 5. T=512 5.62 5.64 5.65 5.73 5.79 Table 5. Comparing sample quality on OWT for various ReMDM schedules. indicates nucleus sampling. For each , the best MAUVE score is bolded. Method Gen PPL. () MAUVE () Entropy () ReMDM-cap ReMDM-loop ReMDM-rescale ReMDM-conf ReMDM-cap ReMDM-loop ReMDM-rescale ReMDM-conf T=1024 0.355 0.403 0.253 0.075 T=128 0.057 0.041 0.040 0.019 T=2048 0.474 0.610 0.289 0.071 T=256 0.216 0.159 0.107 0.035 T=4096 0.353 0.656 0.208 0. T=512 0.350 0.328 0.218 0.053 T=1024 27.7 28.6 27.4 39.2 T=128 42.5 51.5 46.2 57.7 T=2048 20.2 22.8 20.4 37.2 T=256 30.5 38.1 34.8 49.6 T=4096 14.4 17.6 14.6 36. T=512 21.1 29.3 25.6 43.3 T=1024 5.32 5.38 5.28 5.36 T=128 5.43 5.52 5.44 5.50 T=2048 5.21 5.30 5.15 5.34 T=256 5.34 5.45 5.36 5.46 T=4096 5.06 5.20 4.94 5. T=512 5.21 5.38 5.24 5.41 E.2.4. TUNING ηcap / ηrescale In Figure 7, we plot MAUVE scores when using the ReMDM-cap schedule with different ηcap values and varying number of decoding steps . As shown in Figure 7a, in the case of inference-time scaling (T 1024), the MAUVE scores at = 4096 tend to decrease as ηcap increases while the MAUVE scores at = 1024 tend to increase with larger ηcap. We attribute this to the perplexity-entropy trade-off. In particular, when ηcap increases, the probability of remasking increases, improving perplexity, but also harming diversity. In the case of = 4096, the models can generate higher quality sentences and the reduction in diversity outweighs marginal improvements in generative perplexity. For = 1024, the samples are of relatively poorer quality and the improvement in perplexity plays major role in driving up the MAUVE score. To achieve the best balance, we choose ηcap = 0.008 for the setting of inference-time scaling. For faster sampling (T < 1024), in Figure 7b, with the exception of η = 0.045 we see positive trend between increasing ηcap and improving MAUVE scores. In Tables 1 and 5, we report results using η = 0.040. We also performed similar studies for schedules that combine ReMDM-cap with the ReMDM-loop strategy (Figure 8) and for ReMDM-rescale schedules (Figure 9). For both of these settings, we observe trends for increasing ηcap / ηrescale that are similar to those described above. In Table 5, we report the following choice of hyperparameter for each of these settings. For ReMDM-loop combined with ReMDM-cap, in the inference-time scaling experiments (T 1024), we report results using ηcap = 0.020. For the faster sampling experiments (T < 1024), we report results for ηcap = 0.055. For the ReMDM-rescale inference-time scaling experiments, we report results with ηrescale = 0.015 and for the faster sampling Remasking Discrete Diffusion Models with Inference-Time Scaling experiments, we use ηrescale = 0.045 in Table 1. (a) MAUVE scores of ReMDM-cap inference-time scaling (b) MAUVE scores of ReMDM-cap faster sampling Figure 7. Impact of σt on ReMDM-caps unconditional text generation quality on OpenWebText. (a) MAUVE scores of ReMDM-loop inference-time scaling. (b) MAUVE scores of ReMDM-loop faster sampling. Figure 8. Impact of ηcap on unconditional text generation quality on OpenWebText using the ReMDM-loop strategy with ReMDM-cap. E.3. ImageNet In Table 6, we present the full results for varying the sampling temperature τ and the various ReMDM schedules and the their corresponding hyperparameters (ηcap for ReMDM-cap. and ηrescale for ReMDM-rescale). As noted in Section 5.1.2, while both MDLM and ReMDM benefit from using softmax temperature of τ = 0.8, the MaskGiT sampler produces the best results with no temperature scaling (i.e., τ = 1). Of note, we use FID to determine which setting constitutes the best results for each sampler. With reduced temperature, the entropy of the softmax is reduced leading to less diverse samples. This is reflected in the trade-off of the FID vs. IS metrics across sampling temperatures, with IS penalizing lack of diversity less than FID (Heusel et al., 2017). E.4. QM9 In Figures 11-26, we present the results from the extensive hyperparameter search we conducted across both properties, ring count (Figures 11-18) and QED (Figures 19-26), and guidance mechanisms D-CFG (Figures 11-14, 19-22) and D-CBG (Figures 15-18, 23-26). 23 Remasking Discrete Diffusion Models with Inference-Time Scaling (a) MAUVE scores of ReMDM-rescale inference-time scaling. (b) MAUVE scores of ReMDM-rescale faster sampling. Figure 9. Impact of ηrescale on unconditional text generation quality on OpenWebText using the ReMDM-rescale schedule. E.4.1. DRUG-LIKENESS PROPERTY MAXIMIZATION In Figure 10, we present D-CFG and D-CBG results for baseline and the best ReMDM setting when maximizing the drug-likeness (QED) property. Similar to the results for maximizing the ring count property in Section 5.2, we find that ReMDM improves the novelty-property maximization frontier relative to MDLM when maximizing QED, although the benefits of ReMDM relative to MDLM are less pronounced for QED than for ring count. For D-CFG, results reflect combining ReMDM-rescale (ηrescale = 0.1) with the confidence-based scheduler and switch strategy (tswitch = 0.1). For D-CBG, results reflect combining ReMDM-rescale (ηrescale = 1.0) with the confidence-based scheduler and switch strategy (tswitch = 0.5) Figure 10. ReMDM improves steer-ability by extending the novelty-property maximization frontier. Controlled generation for druglikeness (QED) maximization on QM9 dataset with varying inference compute and guidance strength γ. (Left) Discrete classifier-free guidance (D-CFG). (Right) Discrete classifier-based guidance (D-CBG) and FUDGE for AR. E.4.2. TUNING ηrescale Larger ηrescale is Better for Ring Count Maximization Across settings, we find that generally, with some exceptions in the settings where the confidence-based schedule is not used (where ReMDM performs comparatively worse than when confidence is used, as discussed below), we see trend where large ηrescale values lead to Pareto curves that are pushed further in the direction of more novelty and higher ring counts. Additionally, we find that for ηrescale 0.5, the results are Remasking Discrete Diffusion Models with Inference-Time Scaling Table 6. Discretized ImageNet conditional generation grid search over softmax temperature τ and ReMDM hyperparameters. Values reflect FID / IS for varying . For each sampler, the row corresponding to the hyperparameter setup that is reported in the main Table 2 is bolded."
        },
        {
            "title": "MDLM",
            "content": "ReMDM-cap, ηcap = 0.01 ReMDM-cap, ηcap = 0.02 ReMDM-cap, ηcap = 0.05 ReMDM-rescale, ηrescale = 0.01 ReMDM-rescale - ηrescale = 0.02 ReMDM-rescale - ηrescale = 0. ReMDM-conf = 16 FID () = 32 = 64 = 16 IS () = = 64 10.06 6.66 6.74 6.99 7.88 26.02 7.07 7.71 26.25 7.10 7.55 26.86 7.21 7.24 28. 7.09 7.75 26.09 7.09 7.64 26.58 7.19 7.40 27.39 7.46 6.91 22.14 11.03 7.09 4.92 7.67 5.37 18. 7.81 5.20 18.78 7.94 5.00 19.50 8.38 4.83 21.43 7.78 5.24 18.48 7.84 5.07 18.84 8.22 4.92 19. 8.54 5.16 13.14 11.72 7.75 4.85 8.43 4.69 13.96 8.75 4.56 14.72 9.01 4.53 15.85 9.91 4.52 19. 8.66 4.58 14.29 8.90 4.48 14.78 9.65 4.45 16.69 9.82 5.35 8.88 236.90 205.91 155.32 214.55 140.97 64. 216.00 141.31 63.97 217.12 142.33 63.06 223.96 147.20 60.47 215.52 141.11 64.45 217.67 142.50 63.40 223.05 145.27 62. 221.25 150.73 73.00 244.79 225.08 181.57 233.59 169.79 82.54 238.35 172.66 81.25 239.50 178.32 79.46 243.14 184.21 76. 237.82 171.56 82.26 239.87 176.57 81.20 242.99 182.05 78.82 243.56 189.51 101.79 243.64 233.87 196.38 242.28 187.93 96. 243.82 194.77 93.69 244.98 198.87 91.68 244.60 211.06 82.15 244.96 193.73 95.69 246.37 197.35 94.99 250.68 209.45 90. 251.83 212.66 126.29 τ 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1. 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 0.6 0.8 1.0 less sensitive to this parameter. Inconclusive ηrescale Results for QED Maximization For maximizing the QED property, the results are less conclusive. In the settings that use the confidence-based schedule, there is no clear ηrescale that dominates the others. For settings that do not use the confidence-based schedule, we observe trend where results improve with ηrescale = 0.1. E.4.3. ABLATING USE OF CONFIDENCE-BASED SCHEDULE For maximizing the QED property, we observe clear pattern where incorporating confidence-based schedules improves results. For ring count, this trend holds true as well, especially for larger values of ηrescale, where the confidence score potentially helps temper large remasking probabilities for relatively safe tokens, but the trend is less pronounced overall than for QED experiments. E.4.4. TUNING tswitch / LOOP PARAMETERS For both QED and ring count maximization, when using D-CFG as the guidance mechanism, we observe general trend that activating ReMDM, with either the switch or loop strategies, benefits from starting later in the decoding process, i.e., Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 11. ReMDM-rescaled with loop and confidence-based schedules hyperparameter tuning for maximizing ring count using D-CFG. Larger marker sizes indicate larger γ values. smaller tswitch / ton. This trend also holds true for D-CBG in the QED experiments. notable exception is when using D-CBG in the ring count experiments, where we see the opposite trend, i.e., activating ReMDM earlier (larger tswitch / ton) improves results. F. Generated Samples We visualize the generated sentences of MDLM (T = 4096) (Figure 27), MDLM+DFM (T = 4096) (Figure 28), and ReMDM (T = 4096) (Figure 29). We find that the MDLM samples contain many uncorrelated semantic fragments, with grammar errors appearing very often. MDLM+DFM can formulate the text around special topic, but the internal logic is not fluent. In contrast, ReMDM is able to generate high quality, fluent English with clear semantic topic. G. Assets In Table 7, we list the datasets (and corresponding licenses, when available) used in this work. In Table 8, we list the software packages (and corresponding licenses) used in this work. 26 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 12. ReMDM-rescaled with loop and without confidence-based schedules hyperparameter tuning for maximizing ring count using D-CFG. Larger marker sizes indicate larger γ values. 27 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 13. ReMDM-rescaled with switch and confidence-based schedules hyperparameter tuning for maximizing ring count using D-CFG. Larger marker sizes indicate larger γ values. 28 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 14. ReMDM-rescaled with switch and without confidence-based schedules hyperparameter tuning for maximizing ring count using D-CFG. Larger marker sizes indicate larger γ values. 29 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 15. ReMDM-rescaled with loop and confidence-based schedules hyperparameter tuning for maximizing ring count using D-CBG. Larger marker sizes indicate larger γ values. 30 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 16. ReMDM-rescaled with loop and without confidence-based schedules hyperparameter tuning for maximizing ring count using D-CBG. Larger marker sizes indicate larger γ values. 31 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 17. ReMDM-rescaled with switch and confidence-based schedules hyperparameter tuning for maximizing ring count using D-CBG. Larger marker sizes indicate larger γ values. 32 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 18. ReMDM-rescaled with switch and without confidence-based schedules hyperparameter tuning for maximizing ring count using D-CBG. Larger marker sizes indicate larger γ values. 33 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 19. ReMDM-rescaled with loop and confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CFG. Larger marker sizes indicate larger γ values. 34 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 20. ReMDM-rescaled with loop and without confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CFG. Larger marker sizes indicate larger γ values. 35 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 21. ReMDM-rescaled with switch and confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CFG. Larger marker sizes indicate larger γ values. 36 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 22. ReMDM-rescaled with switch and without confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CFG. Larger marker sizes indicate larger γ values. 37 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 23. ReMDM-rescaled with loop and confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CBG. Larger marker sizes indicate larger γ values. 38 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 24. ReMDM-rescaled with loop and without confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CBG. Larger marker sizes indicate larger γ values. 39 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 25. ReMDM-rescaled with switch and confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CBG. Larger marker sizes indicate larger γ values. 40 Remasking Discrete Diffusion Models with Inference-Time Scaling Figure 26. ReMDM-rescaled with switch and without confidence-based schedules hyperparameter tuning for maximizing drug-likeness (QED) using D-CBG. Larger marker sizes indicate larger γ values. 41 Remasking Discrete Diffusion Models with Inference-Time Scaling"
        },
        {
            "title": "License",
            "content": "Table 7. Datasets (and corresponding licenses) used in this work. ImageNet (Deng et al., 2009) OpenWebText (Gokaslan & Cohen, 2019) QM9 (Ruddigkeit et al., 2012; Ramakrishnan et al., 2014) N/A ImageNet License Creative Commons CC0 license (no rights reserved) Library License Table 8. Software (and corresponding license) used in this work. Guided Diffusion (Dhariwal & Nichol, 2021) HuggingFace (Wolf et al., 2019) Hydra (Yadan, 2019) Jax (Bradbury et al., 2018) NumPy (Harris et al., 2020) MaskGiT (Chang et al., 2022) Matplotlib (Hunter, 2007) OmegaConf Pandas (pandas development team, 2020) PyTorch (Paszke et al., 2019) PyTorch Lightning (Falcon & The PyTorch Lightning team, 2019) Apache 2.0 RDKit (Landrum et al., 2013) Seaborn (Waskom, 2021) MIT Apache 2.0 MIT Apache 2.0 NumPy license Apache 2.0 Matplotib license BSD 3-Clause BSD 3-Clause New or Revised BSD-3 Clause BSD 3-Clause New or Revised BSD 3-Clause New or Revised 42 Remasking Discrete Diffusion Models with Inference-Time Scaling Algorithm 3 Sampling with ReMDM-loop. Input: pre-trained denoising network xθ (e.g., MDLM), number of timesteps , number of tokens in sequence L, noising schedule αt, maximum value for remasking ηcap [0, 1], rescale value for remasking ηrescale [0, 1], boolean value for whether to use confidence strategy use conf, start time for ReMDM loop ton [0, 1), number of discrete time steps to spend prior to ReMDM loop nphase1 (0, ), number of discrete time steps to spend in ReMDM loop nphase2 (0, nphase1). Initialize z(1:L) Initialize ψ(1:t) for = to 1 do = {m}L. = {}L. = i/T, = (i 1)/T . if > (T nphase1)/T then // Phase 1 Rescale and shift time: = (t (1 ton) /nphase1) + (T (ton 1)/nphase1) + 1. Rescale and shift time: = (s (1 ton) /nphase1) + (T (ton 1)/nphase1) + 1. Set αt, αs according to noise schedule. σ(1:L) = {0}L. else if (T nphase1 nphase2)/T then // Phase 2: ReMDM loop Set αt = α(ton), αs = α(ton). σ(ℓ) = ηrescale min{ηcap, (1 αs)/αt} for all ℓ = {1, . . . , L}. if use conf then Compute η(ℓ) σ(ℓ) = η(ℓ) end if conf = exp(ψ(ℓ)) for all ℓ {1, . . . , L}. for all ℓ {1, . . . , L}. conf σ(ℓ) exp(ψ(ℓ )) (cid:80) else // Phase 3 Rescale time: = ton /(T nphase1 nphase2 ). Rescale time: = ton /(T nphase1 nphase2). Set αt, αs according to noise schedule. σ(1:L) = {0}L. end if For all ℓ {1, . . . , L}, compute approximate posterior: z(1:L) pθ(z(ℓ) (cid:40) Cat(zs; (1 σ(ℓ) Cat(zs; αs(1σ(ℓ) z(1:L) ) = qσ(z(ℓ) θ + σ(ℓ) )x(ℓ) m), θ + 1αsσ(ℓ) x(ℓ) )αt αt , = x(ℓ) = 1αt 1αt θ (z(1:L) )) zt = m), zt = Sample z(ℓ) if use conf then pθ for all ℓ {1, . . . , L}. Store confidence scores ψ(ℓ) = (z(ℓ) )xθ(z(1:L) )(ℓ), for all newly decoded z(ℓ) = z(ℓ) = m. end if Set z(1:L) end for output z(1:L) . = z(1:L) . 43 Remasking Discrete Diffusion Models with Inference-Time Scaling endoftext might be seen as tough choice, its smart to do so. According to past reports, the SIM cards are only made available in the powerful manufacturers range of carrier variants. And though the ZTE does its own case with these phones, this variant will not be disclosed. We also know the color of the phone and what it also will cost remains to be seen. With the Carrington showdown, the new edition of the exclusive TVD features more pictures with Samsung. endoftext Goodes The Derby winner was named first-team of the Match on the BBC evening. He feels his position as manager at Southall last season was great honour and understands the surarious delight in earning that he cant complain about delivering an endless ovation. The Newcastle coach knows how describes how people like their approach to games. As his biggest fans, he has criticised Paul Cruy and traded barbs with Chris Froome. And sad to admit, he admits he did not get everything right. He just betrayed them in his intention and left an exasperation in others. Ive finished telling players that wed get that, and they continued that attitude to how we didnt think we deserved the job Newcastle United Rovers manager Tony Wheeler All this sound of being needed for interviews in various places. Im quite pleased by the passion the fans have shown for me but believe me, Im not the fan - find the person out. It would be an extremely nice position to be in. Illuminate. Wed like to think, not to get player based on Newcastles team when he does take charge. My head coach says, Listen, its just too much to cry, Tony Wheeler told the BBC on Tuesday about the injury he played in the Northern League final at Paroles. If the player is part in the starting team, then Im confident he would be put in there, but alsove finished telling players we thought wed get the job. We had good options to go in the squad. We had, obviously, the people that were always willing for us to get the best player we could because as coaches we felt we had the best players, and we felt that no one had choice. And the only thing [I] was just going on to say was that we were aware beforehand that it was an unusual situation for us to hook him up with the potential of [Meagan] Forster. We like his game and he likes ours, and if hes good striker, he can go out there and play both for the team and for ourselves. Stephen [Biggish] Jackson, whos worked with here, obviously respects me with great respect and likes his opinions, but think [Tony Wheeler] can say that its satisfactory. Weve got lots of many, and them combine them with Van Ferdinand. All three people weve been approached to, they need to be taken. Could something be done to them? Not be done. United are certainly prepared to lose their star man for Tuesdays Rovers clash, with Borby to take charge of their trip to Dunferm on Saturday. Although they have won five of their six games, they sit sixth in the Northern League, which includes the Parkhead and the two assistant charges but on Tuesday their only goal of the season is worthy of their reward. United have more goals than Palace, scoring both goals this season despite the best form of Johnson as new coach Winger Darius Boyd joined Derby after replacing Marlon Wickham in June and recently, he stayed for year on loan until he joined Newcastle in defiance of Tony Wheeler. Tom Williams, future United captain, defender and striker, told BBC One Sports: Being in Rovers is great for them. They develop their chemistry very well in the dugout and they try to stay in good relationships together. Theyre unabashed about that you can play with both of them now, so theyve stayed loyal. Brendan Rodgers, the current Newcastle United manager after Wheeler took over from Mick Jagger, and succeeded David Moyes, as the most paid Premier League manager in 1997 while the Scot was the highest paid Everton manager. Leicester manager Leicester Leicester is yet to make terms with the joys of the job and has become so indebted he offered to forgive Newcastle Uniteds owners for the cost of lost profits after buying stake in the club earlier this year. endoftext Sometimes, enjoy having colleagues embarrassed to compete in sports, especially it comes to being one-richen employees career. cant own the jobs of the colleagues that pressure us from Chinese and Australian competitive athletes and even to compete in the United States. They seem arrogant endoftext Figure 27. Sample from MDLM and = 4096 diffusion steps. endoftext token is bolded to emphasize the boundary between paragraphs. 44 Remasking Discrete Diffusion Models with Inference-Time Scaling endoftext component to renovating the park, but no one wants to involve others in it before facing backlash. We are going to stand up and try to make real statement on this, Thomas told reporters at rally Tuesday night. would much rather have that there be consensus not just to make decision, but to have very real discussion, and to begin to build movement. The idea behind the proposal is call to the public, council and the youth advisory board, which was approved by residents on the first day of the meeting. Thomas said the message will be important to Hamilton youth. Its going to be very much about number of people, he said. There will be arguments on both sides. It is not going to be all about the type of person that you are or the committee member that you are with particular number of people. Thomas said that the plan is really about an initiative to help the community do both of those things. This isnt going to be positive thing, as it should be, but its going to put people in position to make sure that our focus is not just about how to deal with poverty . . . and with homelessness. Our focus is on keeping the people in our community affordable and to create opportunities for their families. We have believe it or not we have increased community involvement in our city, Thomas said. think its obvious from all of the issues now that well consider resource-generating development and how to devribute those resources. The renovated park is one of the 14 parks identified in the citys 2008 map of the parks 2-acre site, which was opened up for community arts festival. City staff started coming up with the project in late December, but the public has yet to come to terms. For their part, residents sign the petition only to receive two differing opinions. Although there are concerns that it would be an uphill battle that requires council approval, officials said Tuesday that the process could eventually take shape, with councillors likely to support it. Hamilton city hall spokesman John Little told reporters that the city would have to address petition from the public if there is one on site. We are not able to change the outcome because we are discussing it internally, so what we have has not made an official decision, Little said. Thomas said staff wants to hear how the public feels about the project, and it must be incorporated in such prompt fashion that the city is looking at more permanent changes. We implemented 30-year plan, about 10 years ago, and we will continue it this year, Thomas said. We want to once again really emphasize community involvement. mean, as we say we take this kind of step, we dont want to make separate efforts to change different things, he said. Any plans would be discussed with community boards, Conley said. Hamilton city councillor Steve Cowen said if residents sign the petition, actions have to be done, knowing when these changes are needed and that we will have the right options to move forward. Cowen said two renovations along with suggestions for revenue set by the Hulbert Company, which operates the Gerrard Elementary school theatre, will be included on city documents and website. They would set aside $2 million more than the result of Washtenaw Park Plaza renovation plans, sparked by the QEDA expansion that renovated the former Washtenaw Historic Restaurant and Bakery on Gerrard. The company has also purchased Gerrard and turned it into New American Food, Inc. restaurant, and brought the business before the city council in late 2015. Hamilton city council has been having this conversation for more than year, but it usually happens only now. Its really not, We need meeting soon, why we need to do this, Conley said. Its just forum where people can voice their views and inform council, and within the next year, hopefully council will do the right thing to do. With John Elliott: 905-526-2431 mmelliott@thespec.com endoftext Written by Steven B. Ollander In the last five years, we have seen huge financial crises around the world, and not just economic ones. Weve also seen rapid changes in economic systems, in central and peripheral countries. All these systems are in the hands of banks, but there are other forces that have also been used to dump more money into other countries endoftext Figure 28. Sample from MDLM+DFM and = 4096 diffusion steps. endoftext token is bolded to emphasize the boundary between paragraphs. 45 Remasking Discrete Diffusion Models with Inference-Time Scaling endoftext The Audi D3 is one of the fastest and most capable sports cars weve seen on the market. The retractable front wings were added to make the D3 more sporty car. The front wing design is painted yellow, which is big surprise. It looks like the A4 in completely different color scheme. While the front wings retract, the cooling system is drag for sure. The front and rear seats are also light-weighted, which is also convenient for racing driver. The car also features replaceable steering wheels for higher stability and better handling. D3 is easy to drive and easy to use thanks to adaptive suspension technology. Here are few of the other features Audi had in mind: Top speed is only four seconds higher, but more turbocharged engine power has been added to make it even faster. By means, you reach 60 mph in five seconds with strong acceleration. The front wing design makes the engine higher strength, making it just more powerful. Speaking of power, D3 is equipped with 3.0 litre V8 with Adaptive Brake Control, which means you wont complain if you may have to move around it in wet conditions. The engine has been also upgraded, making 204.26 horsepower. This is 2-liter V8, which means it can easily produce as much as 132 bhp. By comparison, 3.0 litre V8 produces 227.4 bhp which is bit too much for sporty cars. If you want sporty car, your only choice is to use this kind of engine in the guise of the D3 R. Other innovative changes that create better handling in the car are the retractable rear wing and the front suspension. The forks, front and rear, are fold-down. The 17-inch adjustable steering wheel makes it easier for the driver. The car also sports sport seats, which is quite different from the A4 R. Behind the steering wheel, the sport driving mode has full black screen touch option. You can move the car up and down by taping the button. The buttons can be mounted on the display, giving you more input of the screen. With the game on, you can walk to the steering wheel and then the interior button to start. The push button is an optional feature, similar to A4s push button. The car also has dedicated battery option that allows you to start the sport mode if the car is on the clock. In addition to these features, the car also has four different lights on the dashboard. The car also has an Assist Compensate Brake function that lets you drive the car in more controlled way by depending if the car is stopped. This is nice feature if you are more advanced driver. D3 comes with two different driving systems. The advanced driving system features side-by-side aggressive driving mode that lets you drive the car more aggressively at low speeds or by braking at higher speeds. It also includes complimentary steer-by-wire braking function. When driving the car, you can take control of the headlights by simply turning on the headlights and turning on the gearbox to speed up. When youre in the park, you can press the manual park controller, and youll see the power gauge on the steering wheel. It produces 211.4 bhp which is higher than the power made by the first-generation sporty in cars like the R8 V6. The car is able to push the limits and create dynamic performance that can take on power and drag extremely quickly. The D3 and A4 come with four different modes of driving. The four modes are being labeled as Focus, Safety, and Defensive Proactive. Additionally, there are four different lights on the dashboard. One is an LED display directly next to the other lights in the car. Another is 17-inch display located behind the steering wheel. When you press it on the wheel, the display then shows the performance of the car again. Compared to the Audi A4, youll get better fuel economy, fewer accidents, and less noise. The D3 is Audis most balanced car, and were happy to say that the D3 stands as the single most overall balanced sports car on the global market. PHOTOS by Sportscar365 Links: Please note that the user name is the source for this article. Local or media images may be removed. endoftext Written by Nick Ohr and James Chablofsky In its 13-episode second season, the comedy The Good Wife blends action with the real world, bringing the story of couple: Arianna (Garrett Rossi) and Michelle (Amy Braid) and Caitlin endoftext Figure 29. Sample from ReMDM and = 4096 diffusion steps. endoftext token is bolded to emphasize the boundary between paragraphs."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Cornell Unversity"
    ]
}