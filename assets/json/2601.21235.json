{
    "paper_title": "SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models",
    "authors": [
        "Alok Abhishek",
        "Tushar Bandopadhyay",
        "Lisa Erickson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), a framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as a multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with a union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as a primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on a fixed corpus of n=901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 5 3 2 1 2 . 1 0 6 2 : r SHARP: Social Harm Analysis via Risk Profiles for Measuring Inequities in Large Language Models Alok Abhishek San Francisco, USA alok@alokabhishek.ai Tushar Bandopadhyay San Francisco, USA tushar@kronml.com Lisa Erickson Boston, USA lisa.erickson@alum.mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly deployed in high-stakes domains, where rare but severe failures can result in irreversible harm. However, prevailing evaluation benchmarks often reduce complex social risk to mean-centered scalar scores, thereby obscuring distributional structure, cross-dimensional interactions, and worst-case behavior. This paper introduces Social Harm Analysis via Risk Profiles (SHARP), framework for multidimensional, distribution-aware evaluation of social harm. SHARP models harm as multivariate random variable and integrates explicit decomposition into bias, fairness, ethics, and epistemic reliability with union-of-failures aggregation reparameterized as additive cumulative log-risk. The framework further employs risk-sensitive distributional statistics, with Conditional Value at Risk (CVaR95) as primary metric, to characterize worst-case model behavior. Application of SHARP to eleven frontier LLMs, evaluated on fixed corpus of = 901 socially sensitive prompts, reveals that models with similar average risk can exhibit more than twofold differences in tail exposure and volatility. Across models, dimension-wise marginal tail behavior varies systematically across harm dimensions, with bias exhibiting the strongest tail severities, epistemic and fairness risks occupying intermediate regimes, and ethical misalignment consistently lower; together, these patterns reveal heterogeneous, model-dependent failure structures that scalar benchmarks conflate. These findings indicate that responsible evaluation and governance of LLMs require moving beyond scalar averages toward multidimensional, tail-sensitive risk profiling."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) and LLM-powered agents have moved from experimental artifacts to integral components of decision-making systems with material societal impact. They are increasingly deployed in high-stakes domains such as healthcare, finance, hiring, welfare allocation, and criminal justice, where failures can produce durable or irreversible harm [11, 3, 20, 16]. This trend is further accelerated by the emergence of agentic systems capable of autonomous planning, tool use, and multi-step reasoning, which are progressively entrusted with consequential decisions governing access to resources, opportunity, and liberty [6, 8]. As result, LLM failures no longer appear solely as isolated errors, but as systemic risks embedded within broader socio-technical infrastructures [9, 15]. This deployment trajectory exposes structural vulnerability. LLMs are trained on large-scale corpora assembled from web text, digitized media, and institutional sources that reflect historical inequities, uneven representation, and entrenched power asymmetries, and may therefore encode and amplify these patterns without normative grounding or contextual awareness. Prior work has characterized such systems as stochastic parrots, emphasizing their reliance on statistical association rather than grounded reasoning or social understanding [4]. When integrated into high-stakes pipelines, these properties can give rise to systematic harms spanning biased representation, inequitable treatment across demographic groups, ethical misalignment, and epistemic failures such as hallucination and Preprint. misinformation [5, 21, 13]. Empirical evidence from criminal justice risk assessment, lending, and hiring systems demonstrates that such harms arise predictably when models optimized for aggregate performance are deployed across heterogeneous populations [11, 14]. Despite substantial progress in fairness-aware learning and responsible AI, prevailing evaluation practices remain poorly aligned with deployment risk [3, 10]. Capability benchmarks, bias audits, and safety evaluations continue to rely predominantly on mean-centered scalar summaries, such as average accuracy, win rates, or aggregate bias scores. While informative for coarse comparison, these metrics collapse heterogeneous behavior into point estimates and obscure the distributional structure of harm. In particular, models with similar average performance can exhibit sharply divergent upper-tail behavior, producing rare but severe failures under specific prompts or contexts; in high-stakes settings, such tail events often dominate practical risk, yet tail-sensitive quantities and cross-dimensional interactions remain largely absent from standard evaluation pipelines [10, 17]. Governance and auditing mechanisms face analogous limitations. Regulatory frameworks such as the EU AI Act and algorithmic accountability initiatives articulate high-level principles of fairness, accountability, and transparency, but translating these principles into auditable, empirically grounded, and risk-sensitive metrics remains an open technical challenge [2, 12, 23]. Developer-led self-audits are susceptible to incomplete coverage and conflicts of interest, while participatory and communitycentered approaches, though normatively valuable, are difficult to scale and are rarely integrated into mainstream evaluation workflows [7, 18]. This paper argues that social harm in LLMs cannot be adequately characterized as scalar quantity. Harm varies across prompts, concentrates in the upper tail of model behavior, and manifests through multiple interacting mechanisms, including bias, fairness violations, ethical misalignment, and epistemic unreliability. In high-stakes contexts, these tail events correspond to worst-case failures with disproportionate impact. Therefore, effective evaluation requires methods that explicitly model the distributional geometry of harm and its cross-dimensional interactions, rather than reducing behavior to averaged scores. To address this gap, we introduce Social Harm Analysis via Risk Profiles (SHARP), framework for multidimensional, distribution-aware evaluation of social harm in LLMs under fixed evaluation protocol. SHARP departs from mean-based assessment along three axes. First, it decomposes harm into four constituent dimensions: bias, fairness, ethics, and epistemic reliability, enabling targeted diagnosis of distinct failure modes. Second, it treats harm as distributional quantity, preserving prompt-level variability rather than collapsing behavior into point estimates. Third, it adopts risk-sensitive aggregation, modeling compounded harm via union-of-failures formulation that is reparameterized as additive cumulative log-risk, and summarizes model behavior using tail-focused statistics such as Conditional Value at Risk (CVaR95). Methodologically, SHARP introduces layered evaluation architecture. At the prompt level, model responses are embedded into four-dimensional harm space using multi-judge LLM-as-a-judge protocol [1, 22, 24]. Judge outputs are aggregated via log-sum-exp to retain sensitivity to severe assessments while remaining robust to individual judge idiosyncrasies. Compounded harm across dimensions is captured through additive log-risk, yielding an interpretable measure of joint failure pressure. At the model level, prompt-level risk values induce empirical distributions that are summarized by expected risk, volatility, and tail risk, supporting comparative risk profiling under worst-case considerations rather than calibrated harm estimation. We apply SHARP to eleven frontier LLMs, spanning open-weight and proprietary systems, and evaluated them on curated corpus of 901 socially sensitive prompts derived from BEATS and related benchmarks [1, 19]. The resulting risk profiles reveal substantial heterogeneity across models. Systems with similar average risk often differ markedly in tail exposure and volatility, indicating qualitatively distinct failure modes with divergent deployment implications. Across models, marginal dimension-wise tail behavior is strongest for bias-related harms, while epistemic and fairness risks exhibit intermediate tail severities and ethical misalignment remains consistently lower; these patterns indicate heterogeneous, model-dependent harm profiles rather than dominance by any single dimension. These findings suggest that effective mitigation requires dimension-specific, risk-aware interventions informed by distributional analysis, rather than uniform remediation guided by scalar benchmarks. 2 SHARP is not intended to estimate real-world harm rates or provide population-level fairness guarantees. Instead, it offers reproducible protocol for comparative tail-sensitive risk profiling under explicit measurement assumptions, complementing human-centered audits and validation."
        },
        {
            "title": "2 Methodology",
            "content": "Social harm in large language models is multidimensional, prompt-dependent, and driven by worstcase failures rather than average behavior. SHARP formalizes harm as multivariate random variable over fixed prompt distribution and summarizes its empirical behavior using risk-sensitive statistics. This section describes the evaluation protocol and aggregation logic; full mathematical details and implementation specifics are provided in Appendix A. 2.1 Evaluation setup Let denote the set of evaluated models and = {q1, . . . , qn} fixed prompt corpus. For each model and prompt Q, the generated response yM,q is independently evaluated by three LLM-based judges (Claude Sonnet 4.5, Gemini 2.5 Pro, GPT-5.1), blinded to model identity. Each judge produces structured assessments comprising boolean indicators and ordinal severity scores, normalized to [0, 1] with larger values indicating greater estimated harm. Judge outputs are aggregated to form prompt-level harm estimates for each dimension. Because the judge set overlaps partially with the evaluated model set, all reported scores are conditional on the chosen judge ensemble; SHARP is intended as reproducible evaluation framework rather than source of invariant or normative harm labels. 2.2 Evaluation corpus We evaluate 11 frontier LLMs on curated corpus of 901 socially sensitive prompts derived from the Bias Evaluation and Assessment Test Suite (BEATS) [1]. BEATS establishes the prompt construction methodology, and empirically validates the corpus through extensive statistical analysis. The prompt set draws in part from established BBQ [19] benchmark, which provides carefully controlled bias probes for question answering. Prompts span broad range of demographic and contextual categories, including intersectional scenarios. Corpus composition and categorization were fixed prior to evaluation to ensure reproducibility and cross-model comparability. 2.3 Dimensional decomposition SHARP decomposes social harm into four dimensions: bias (B), fairness (F ), ethics (E), and epistemic reliability (K). For each dimension, sub-index in [0, 1] is constructed by aggregating relevant judge outputs using root-mean-square (RMS) aggregation, which emphasizes large deviations and admits geometric interpretation. Categorical attributes are mapped to ordered numeric scales. Full constructions are provided in Appendix A. 2.4 Risk-sensitive judge aggregation To account for disagreement among judges, SHARP aggregates judge scores using log-sum-exp (LSE) operator: LSEτ (X1, X2, X3) = τ log eXj /τ , (1) 1 3 (cid:88) j=1 which smoothly interpolates between mean and max pooling. Smaller τ emphasizes worst-case assessments while retaining differentiability. We fix τ = 0.20, with sensitivity analysis reported in Appendix E. 2.5 Harm representation and compounded risk Each prompt-level response is represented as four-dimensional harm vector hM,q = ( BM,q, FM,q, EM,q, KM,q) [0, 1]4, 3 where each coordinate denotes the judge-aggregated sub-index for distinct harm dimension. The quantities B, , E, are normalized harm intensities derived from structured judge assessments; they are not calibrated probabilities of real-world harm. SHARP aggregates multi-dimensional harm using monotone compounding operator inspired by union-of-failures models from reliability analysis. Specifically, we define prompt-level compounded harm score HM,q = 1 (1 hi,M,q), (2) (cid:89) i{B,F,E,K} which increases whenever any dimension exhibits elevated harm and amplifies coordinated activation across dimensions. Equation (2) should be interpreted as an operational risk aggregation, not as calibrated probability of harm occurrence. The conditional independence implicit in the product form is heuristic approximation that ensures monotonicity and interaction sensitivity; SHARP does not claim probabilistic or causal interpretation of HM,q. To obtain an additive and decomposable risk representation, SHARP reparameterizes residual safety via negative log transform. For each dimension, ℓi,M,q = log(1 hi,M,q + ε), ε = 106, and defines the cumulative log-risk LM,q = (cid:88) ℓi,M,q = log(1 HM,q + ε). (3) This transformation maps bounded harm intensities to unbounded risk units, preserves ordering, avoids saturation near one, and enables transparent attribution of compounded risk to individual dimensions through the additive decomposition of LM,q. Throughout the paper, HM,q and LM,q are treated as ordinally meaningful risk scores, rather than calibrated probabilities of real-world harm, whose primary role is comparative and distributional. SHARPs core claims concern relative tail behavior and structural differences across models under fixed evaluation protocol, rather than calibrated estimation of real-world harm probabilities. 2.6 Auxiliary magnitude diagnostic SHARP additionally reports harm magnitude diagnostic defined as the Euclidean norm of the harm vector, rM,q = hM,q2, (4) which measures overall harm intensity without requiring joint activation and serves as complementary indicator of prompt sensitivity and dispersion. 2.7 Model-level risk profiling Each model induces an empirical distribution {LM,q}qQ of prompt-level cumulative log-risk values. SHARP summarizes this distribution using the expected log-risk µL(M ), risk volatility σL(M ), and tail risk quantified via Conditional Value at Risk: CVaR0.95(M ) = E[LM,q LM,q VaR0.95(M )] . (5) CVaR0.95 captures the severity of worst-case compounded failures and serves as SHARPs primary safety-relevant statistic. Methodological contribution. SHARP combines explicit dimensional decomposition, union-based harm aggregation, additive log-risk reparameterization, and distributional profiling via CVaR to expose failure modes and tail behaviors that are systematically obscured by mean-centered scalar benchmarks."
        },
        {
            "title": "3 Empirical Evaluation",
            "content": "3.1 Model-Level Social Harm Risk Profiles We characterize model behavior using CVaR95 of cumulative log-risk as the primary evaluation statistic, capturing worst-case compounded social harm across bias, fairness, ethics, and epistemic 4 dimensions. CVaR95 measures the expected harm among the most adverse five percent of prompts and is therefore sensitive to tail behavior that mean-centered metrics systematically obscure. All reported statistics are computed over shared set of = 901 prompts. Table 1 summarizes model-level risk profiles using cumulative log-risk as the primary tail-sensitive metric, alongside complementary summaries of central tendency and probability-based tail behavior. Reported statistics include mean cumulative log-risk, CVaR95 of cumulative log-risk to characterize worst-case compounded harm, and CVaR95 of any-harm probability to illustrate saturation effects under probabilistic aggregation. Table 1: Model-level summary of key SHARP metrics. Lower values indicate lower estimated social harm. CVaR95 statistics characterize worst-case behavior across prompts. Model CVaR95 Cumulative Log-Risk Mean Cumulative Log-Risk Any-Harm CVaR95 claude-sonnet-4.5 gemini-1.5-pro claude-3.5-sonnet gemini-2.5-pro qwen3-235b gpt-4o gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b 1.689 3.499 3.782 3.868 3.895 4.519 5.286 5.735 5.988 7.934 8.397 0.158 0.463 0.449 0.560 0.589 0.748 1.209 1.296 1.322 1.968 2.154 0.772 0.962 0.965 0.971 0.967 0.984 0.994 0.994 0.995 0.998 0. Large divergence in worst-case risk. Models that appear similar under average risk exhibit pronounced separation in tail behavior. Claude Sonnet 4.5 attains CVaR95 of cumulative log-risk of 1.69, whereas DeepSeek-Chat and Mistral-Large reach 5.73 and 5.99, corresponding to more than threefold increase in worst-case compounded harm. At the upper end, LLaMA-3 405B exhibits CVaR95 of 8.40, exceeding the lowest-risk model by over factor of four. These separations are substantially larger than those observed under mean log-risk, which spans comparatively narrow range. Mean risk understates tail exposure. Mean cumulative log-risk systematically underestimates extreme behavior across models. Gemini-1.5-Pro and Claude-3.5-Sonnet exhibit nearly identical mean log-risk values (0.46 versus 0.45), yet differ meaningfully in CVaR95 (3.50 versus 3.78). This pattern recurs across the model set, demonstrating that mean-based summaries fail to capture worstcase failure modes that dominate deployment risk. Because low measured risk may reflect refusal or deflection rather than safe task completion, these results should be interpreted as intrinsic harm profiles conditional on observed outputs rather than utility-constrained performance. Probability metrics saturate under compounding. Probability-level metrics, such as any-harm probability, rapidly saturate in the tail. Most models exceed 0.96 CVaR95 in any-harm probability, with several approaching 0.99. This saturation limits discriminative power and motivates cumulative log-risk as more informative metric under compounding, where it remains well-spread and stable across models. Magnitude alone fails to capture interaction-driven risk. Geometric magnitude metrics, such as harm-radius CVaR95, capture overall severity but fail to reflect amplification due to joint activation across dimensions. Models with similar harm-radius profiles can diverge substantially in cumulative log-risk, confirming that worst-case harm is driven by coordinated elevation across dimensions rather than marginal magnitude alone. Collectively, these results demonstrate that worst-case social harm in LLMs is highly modeldependent, epistemically driven, and largely invisible to mean-based or probability-only evaluation. CVaR95 of cumulative log-risk provides stable and discriminative basis for comparing models under adverse prompt conditions. 3.2 Interpreting SHARP for Risk-Constrained Model Selection SHARP is intended for risk-constrained model selection rather than total ordering by average performance. We formalize this by framing evaluation as decision problem under tail-risk tolerance. Let denote candidate models and let LM be the prompt-level cumulative log-risk distribution induced by model . For specified tolerance τ > 0, define the admissible set Mτ = {M : CVaR0.95(LM ) τ }. This criterion excludes models with unacceptable worst-case compounded harm regardless of mean risk. As shown in Table 1, several models exhibit similar expected log-risk yet differ substantially in CVaR0.95. Such models are indistinguishable under mean-centered evaluation but are separated under SHARPs tail-sensitive criterion. Conversely, models with slightly higher mean risk but lower tail exposure may be preferable in high-stakes settings. Rather than inducing fragile total ranking, CVaR0.95(L) supports filtering, exclusion, and tiered comparisons under explicit risk tolerance, aligning evaluation with deployment regimes in which rare but severe failures dominate expected harm. 3.3 Risk Tiers and Statistical Indeterminacy Because SHARP operates on heavy-tailed prompt-level distributions, model comparison is best interpreted in terms of risk tiers rather than strict total orderings. Bootstrap confidence intervals for CVaR0.95(L) (Appendix G) show that while many model pairs are clearly separable, subset of near-neighbors remains statistically indistinguishable. Accordingly, SHARP induces partial ordering: models with non-overlapping CVaR0.95(L) intervals form distinct lowand high-risk tiers, while overlapping intervals define intermediate tiers where differences cannot be resolved under the current prompt distribution. This structure is stable across bootstrap resampling, repeated-measures tests, and aggregation robustness analyses. From governance perspective, these tiers support conservative decision-making: higher-risk tiers can be excluded outright, while models within the same tier may be treated as interchangeable absent additional domain constraints. 3.4 Decomposition of Harm into Four Spatial Dimensions Aggregate risk metrics obscure the structure of social harm. SHARP therefore decomposes promptlevel harm into four dimensions: bias, fairness, ethical misalignment, and epistemic unreliability, to characterize heterogeneous failure modes that scalar summaries conflate. Tables 2 and 3 report marginal prompt-level means and tail severities (CVaR95) for each sub-index. These quantities describe how severe each harm dimension can become in isolation, conditioning on dimension-specific tail events. They are therefore descriptive of marginal failure behavior rather than determinants of dominance in compounded risk. Central tendency across harm dimensions. Beyond tail behavior, the sub-index means in Tables 2 and 3 reveal systematic differences in central tendency across the four harm dimensions. Across all evaluated models, epistemic unreliability exhibits the highest mean values, followed by bias and fairness, with ethical misalignment consistently lowest. Relative to bias, mean epistemic scores are elevated by approximately 1060% across models (roughly 25% on average), indicating that epistemic issues arise more frequently across prompts. Bias and fairness exhibit comparable mean magnitudes, while ethics occupies uniformly lower central regime. These mean-level patterns characterize the prevalence of harm signals across typical prompts and should not be conflated with dominance in worst-case compounded risk, which we analyze separately via tail attribution. Interpretation of marginal tail severities. The tables above reveal substantial heterogeneity across dimensions. Bias-related harm often exhibits higher marginal tail severity than epistemic unreliability for several models, while ethical misalignment occupies an intermediate regime. Because each CVaR95 conditions on dimension-specific tail set, these quantities should be interpreted as characterizing isolated failure modes rather than identifying the dominant drivers of worst-case compounded harm. 6 Table 2: Marginal decomposition of model-level SHARP harm profiles into bias and fairness subindices, reporting prompt-level means and tail risk (CVaR95). Values correspond to expectations over prompts of log-sum-exp aggregated judge assessments (Section A.5); lower values indicate lower estimated harm. Model Bias Mean Bias CVaR95 Fairness Mean Fairness CVaR95 claude-sonnet-4.5 gemini-1.5-pro claude-3.5-sonnet qwen3-235b gemini-2.5-pro gpt-4o gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b 0.0210 0.0734 0.0652 0.1037 0.0997 0.1256 0.1928 0.2183 0.2154 0.3009 0.3147 0.3889 0.6052 0.6317 0.6910 0.7366 0.7523 0.8179 0.8327 0.8401 0.8828 0.8881 0.0352 0.0869 0.0815 0.1124 0.1050 0.1368 0.2023 0.2216 0.2137 0.3009 0.3160 0.2944 0.5720 0.5855 0.6196 0.6089 0.6551 0.7326 0.7411 0.7493 0.8043 0.8102 Table 3: Marginal decomposition of model-level SHARP harm profiles into ethics and epistemic sub-indices, reporting prompt-level means and tail risk (CVaR95). Values correspond to expectations over prompts of log-sum-exp aggregated judge assessments (Section A.5); lower values indicate lower estimated harm. Model Ethics Mean Ethics CVaR95 Epistemic Mean Epistemic CVaR95 claude-sonnet-4.5 gemini-1.5-pro claude-3.5-sonnet qwen3-235b gemini-2.5-pro gpt-4o gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b 0.0244 0.0687 0.0637 0.0846 0.0837 0.1129 0.1769 0.1848 0.1736 0.2566 0. 0.2713 0.4963 0.5116 0.5369 0.5582 0.6205 0.7323 0.7197 0.7177 0.7797 0.7962 0.0523 0.1177 0.1177 0.1280 0.1165 0.1606 0.2270 0.2424 0.2579 0.3427 0.4045 0.3247 0.6516 0.6959 0.6717 0.6629 0.7221 0.7774 0.7876 0.8069 0.8511 0.8857 Attribution within compounded tail events. To determine which dimensions drive extreme compounded failures, we perform tail attribution analysis conditioned on the tail of cumulative log-risk L. Specifically, for each model we define TM (0.95) = {q : LM,q VaR0.95(LM )} and decompose CVaR0.95(LM ) into additive log-risk contributions from each dimension. Table 4 reports normalized contribution shares Si(M ), which sum to one by construction. This analysis shows that dominance in compounded tail risk is model-dependent. Epistemic unreliability constitutes major contributor for several systems (e.g., Claude 3.5 Sonnet, GPT-4o), while bias-related mechanisms dominate the compounded tail for others, including higher-risk open-weight models. These findings clarify that dominance reflects persistent contribution across worst-case prompts rather than the largest marginal tail severity within any single dimension. Table 4 shows that the dominant contributors to compounded tail risk vary across models, underscoring that marginal sub-index tail severities alone are insufficient to characterize worst-case failure drivers. 3.5 Statistical Validation We validate SHARP using complementary analyses that assess estimator stability, cross-model discriminability, and the relative contribution of model and prompt effects. Full statistical details are reported in the appendix. Estimator stability and uncertainty. We quantify uncertainty due to finite prompt sampling using paired non-parametric bootstrap resampling over prompts (B = 10,000; Appendix H). For the primary outcomeprompt-level cumulative log-risk LM,q and its tail statistic CVaR0.95(L)bootstrap confidence intervals are tight and well separated for most models. Pairwise bootstrap tests on 7 Table 4: Tail attribution of compounded log-risk. For each model , we decompose the tail risk CVaR0.95(LM ) into dimension-wise additive log-risk contributions. Reported values are normalized shares Si(M ) = E[ℓi TM (0.95)]/CVaR0.95(LM ), where TM (0.95) denotes the top 5% of prompts by cumulative log-risk L. By construction, (cid:80) Si(M ) = 1. Model claude-sonnet-4-5 gemini-1.5-pro claude-3-5-sonnet gemini-2.5-pro qwen3-235b gpt-4o gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b CVaR0.95(L) SB SF SE SK 1.689 3.499 3.782 3.868 3.895 4.519 5.286 5.735 5.988 7.934 8.397 0.301 0.254 0.255 0.352 0.357 0.275 0.286 0.339 0.338 0.435 0.386 0.276 0.243 0.229 0.264 0.229 0.222 0.250 0.230 0.230 0.200 0.223 0.202 0.195 0.190 0.201 0.182 0.187 0.205 0.196 0.199 0.166 0.193 0.221 0.308 0.327 0.184 0.228 0.316 0.260 0.232 0.231 0.188 0.185 CVaR0.95(L) indicate that 80% of model pairs are statistically separable at the 95% level, with ambiguity concentrated among near-neighbor models. This pattern supports tiered interpretation of risk rather than fragile total ordering. Model-level heterogeneity. Because all models are evaluated on the same prompts, we apply the Friedman test, non-parametric repeated-measures alternative to ANOVA, on paired prompt-level log-risk values. The omnibus test strongly rejects the null hypothesis of identical model behavior (χ2(10) = 1629.9, 0), with small-to-moderate effect size (Kendalls = 0.181). Post-hoc Wilcoxon signed-rank tests with Holm correction find 87% of model pairs remain significantly different, confirming that observed differences reflect systematic model effects rather than sampling noise, while again highlighting limited separability among adjacent models. Sources of variability. To contextualize effect sizes, we decompose variance in LM,q using two-way fixed-effects model with model and prompt as factors (Appendix H.3). Prompt identity explains larger share of total variance (25.8%) than model identity (13.9%), with the remainder attributable to residual and stochastic effects. This structure is expected in social-harm evaluation, which intentionally probes heterogeneous, context-sensitive scenarios. Model effects are statistically material but operate against background of strong prompt dependence, motivating SHARPs distributional and tail-risk focus. Robustness checks. Comparative conclusions are invariant to reasonable choices of hyperparameters. Model rankings induced by CVaR0.95(L) are unchanged across LSE temperatures τ {0.15, 0.20, 0.25} and remain highly stable under variation of the CVaR tail threshold (α {0.90, 0.95, 0.975}), indicating that results are not artifacts of aggregation or tail-selection choices (Appendix E). Summary. Across bootstrap uncertainty analysis, non-parametric repeated-measures testing, variance decomposition, and robustness checks, SHARP yields stable, statistically distinguishable model risk profiles under the current compounded-risk definition. At the same time, persistent prompt-driven variability underscores why mean-centered metrics are insufficient and why distributional and tail-risk analysis is necessary for evaluating social harm in large language models."
        },
        {
            "title": "4 Limitations",
            "content": "SHARP is diagnostic evaluation framework for characterizing distributional social harm under fixed evaluation protocol, and several limitations constrain interpretation. First, SHARPs harm sub-indices, compounded harm scores, and cumulative log-risk are operational risk measures, not calibrated probabilities of real-world harm or normative ground-truth labels. Judge-derived harm intensities are ordinal and rubric-dependent; accordingly, all conclusions concern relative behavior, distributional structure, and tail exposure under specified measurement configuration rather than absolute harm rates or deployment-ready safety guarantees. Cross-dimensional results should therefore be interpreted as comparative descriptions of marginal and compounded risk patterns 8 induced by the evaluation protocol, rather than as cardinal claims about the intrinsic severity or real-world importance of specific harm dimensions. Second, SHARP relies on LLM-based judges and inherits known limitations of LLM-as-a-judge paradigms, including correlated alignment effects, prompt sensitivity, and ambiguity in epistemic reliability assessment. Inter-judge disagreement is highest for epistemic harm, reflecting the intrinsic difficulty of evaluating factual soundness and uncertainty. Because the judge pool partially overlaps with the evaluated model set, reported scores and rankings are conditional on the chosen judge ensemble and may reflect systematic calibration biases. While robustness analyses demonstrate stability of relative tail-risk ordering under judge ablations, SHARP does not claim judge neutrality or independence, nor does it provide external human calibration in this study. Third, measured social harm may be reduced by refusal, deflection, or minimal responses, which do not necessarily correspond to desirable behavior under real deployment constraints. SHARP evaluates intrinsic harm conditional on observed model outputs and does not explicitly model utility, coverage, or task completion; consequently, low estimated risk should not be interpreted as evidence of safe and useful performance absent additional capability or helpfulness constraints. Fourth, SHARP evaluates single-turn intrinsic behavior under fixed prompt corpus. The fairness dimension captures within-response inequitable treatment cues rather than population-level statistical fairness properties, which require aggregation across individuals or outcomes and are intentionally out of scope. Group-level fairness guarantees, downstream decision effects, and sociotechnical feedback loops are not modeled. Finally, all reported risk profiles are conditional on the evaluated prompt distribution, which is primarily English-language and Western-centric. Prompt identity explains more variance than model identity, underscoring that SHARP induces conditional risk profiles rather than global model safety rankings. Tail-risk estimates such as CVaR0.95 reflect the upper tail of the evaluated corpus under single-sample decoding; bootstrap uncertainty quantifies sampling variability, but distribution shift and generation stochasticity are not addressed in this study. Overall, SHARP should be interpreted as reproducible, distribution-aware framework for comparative tail-risk profiling under explicit measurement assumptions, not as calibrated estimate of real-world harm or substitute for context-specific validation in deployment settings."
        },
        {
            "title": "5 Conclusion",
            "content": "This work shows that social harm in large language models is fundamentally distributional and multidimensional, and therefore poorly characterized by mean-centered scalar summaries. As LLMs are increasingly deployed in high-stakes settings, evaluation must account for rare but severe failures, prompt-dependent variability, and cross-dimensional interactions that conventional benchmarks obscure. Contributions. SHARP advances LLM evaluation as an operational risk-profiling framework along three axes. First, it decomposes social harm into bias, fairness, ethics, and epistemic reliability, enabling structured diagnosis of failure modes that scalar aggregates conflate. Second, it introduces risk-sensitive aggregation via union-of-failures formulation reparameterized as additive cumulative log-risk, yielding decomposable measure aligned with worst-case behavior rather than average performance. Third, it characterizes model behavior using distributional statistics, with Conditional Value at Risk (CVaR0.95) as primary metric that explicitly surfaces tail exposure. Empirical findings. Across eleven frontier LLMs evaluated on fixed corpus of socially sensitive prompts, SHARP demonstrates that models with similar mean risk can exhibit substantially different tail-risk profiles under the same evaluation protocol. In multiple cases, low expected risk coexists with elevated CVaR, revealing susceptibility to rare but severe failures that are invisible to meancentered summaries and dominate worst-case exposure. Dimension-wise decomposition further shows heterogeneous marginal tail behavior across bias, fairness, ethics, and epistemic dimensions, with no single harm dimension uniformly driving tail outcomes across models. Statistical validation confirms that these differences reflect systematic model effects rather than sampling noise, while also highlighting strong prompt dependence in extreme-risk regimes. 9 Implications for evaluation and governance. By treating harm as multivariate random variable and prioritizing tail behavior, SHARP enables comparative assessments that are inaccessible to scalar benchmarks. Risk-sensitive thresholds support exclusion or tiering of models based on unacceptable worst-case exposure irrespective of mean performance, aligning evaluation with deployment regimes where tail events dominate practical risk. Dimensional decomposition further supports targeted mitigation by isolating dominant failure modes, without collapsing heterogeneous risks into single scalar score. Limitations and outlook. SHARP evaluates intrinsic, single-turn behavior under fixed prompt distribution and specified judge ensemble, and does not provide calibrated estimates of real-world harm or population-level guarantees. Reliance on LLM-based judges introduces rubricand ensembleconditionality, and low measured risk may reflect refusal or deflection absent explicit utility constraints. The evaluated corpus is primarily English-language and Western in context, and tail-risk estimates reflect single-sample decoding under the sampled prompt distribution. Future work should extend risk profiling to interactive and agentic settings, incorporate utility-aware constraints, broaden cultural and linguistic coverage, analyze generation stochasticity, and develop calibrated or hybrid judge-based assessment protocols. In summary, SHARP provides principled and reproducible foundation for comparative, tail-sensitive evaluation of social harm under explicit measurement assumptions. While not calibrated measure of real-world impact, it demonstrates that meaningful assessment of LLM risk requires moving beyond scalar averages toward multidimensional, distribution-aware profiling of worst-case behavior."
        },
        {
            "title": "Broader Impact",
            "content": "This work introduces SHARP, distribution-aware framework for evaluating social harm in large language models via multidimensional, tail-sensitive risk profiling. The primary goal of the framework is methodological: to improve how social harm is measured and compared under fixed evaluation protocols, particularly in regimes where rare but severe failures dominate practical risk. potential positive impact of this work is to enable more risk-aware model selection, auditing, and benchmarking in high-stakes deployment contexts. By exposing worst-case behavior and cross-dimensional failure structure that mean-centered metrics obscure, SHARP may support more conservative governance decisions, targeted mitigation strategies, and clearer communication of model risk profiles to practitioners and regulators. At the same time, SHARP is not intended to estimate real-world harm rates, provide population-level fairness guarantees, or substitute for domain-specific validation. Misuse could arise if SHARP scores are interpreted as calibrated measures of societal impact, or if models are ranked or deployed solely on the basis of intrinsic harm profiles without considering task utility, deployment context, or affected stakeholders. To mitigate such risks, the framework explicitly emphasizes comparative, conditional interpretation under stated assumptions and highlights its limitations. Overall, this work advances evaluation methodology for responsible machine learning by formalizing social harm as multidimensional, distributional object. Its societal implications are primarily indirect, mediated through improved measurement and governance practices rather than direct deployment or decision-making."
        },
        {
            "title": "References",
            "content": "[1] Alok Abhishek, Lisa Erickson, and Tushar Bandopadhyay. Beats: Bias evaluation and assessment test suite for large language models, 2025. URL https://arxiv.org/abs/2503. 24310. [2] Alok Abishek, Lisa Erickson, and Tushar Bandopadhyay. Data and ai governance: Promoting equity, ethics, and fairness in large language models. MIT Science Policy Review, 6:139146, August 2025. doi: 10.38105/spr.1sn574k4lp. URL http://dx.doi.org/10.38105/spr. 1sn574k4lp. [3] Saar Alon-Barkat and Madalina Busuioc. Humanai interactions in public sector decision making: automation bias and selective adherence to algorithmic advice. Journal of Public Administration Research and Theory, 33(1):153169, February 2022. ISSN 1477-9803. doi: 10.1093/jopart/muac007. URL http://dx.doi.org/10.1093/jopart/muac007. [4] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT 21, page 610623. ACM, March 2021. doi: 10.1145/3442188.3445922. URL http://dx.doi.org/10.1145/ 3442188.3445922. [5] Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings, 2016. URL https://arxiv.org/abs/1607.06520. [6] Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, and Tegan Maharaj. Harms from increasingly agentic algorithmic systems. In 2023 ACM Conference on Fairness Accountability and Transparency, FAccT 23, page 651666. ACM, June 2023. doi: 10.1145/3593013.3594033. URL http://dx.doi.org/10.1145/3593013.3594033. [7] Sanae El Mimouni and Mohamed Bouhdadi. Fairness and bias in ai: sociotechnical perspective. Journal of Information, Communication and Ethics in Society, August 2025. ISSN 1758-8871. doi: 10.1108/jices-12-2024-0182. URL http://dx.doi.org/10.1108/ jices-12-2024-0182. [8] Riya Fernando, Isabel Norton, Pranay Dogra, Rohit Sarnaik, Hasan Wazir, Zitang Ren, Niveta Sree Gunda, Anushka Mukhopadhyay, and Michael Lutz. Quantifying bias in agentic large language models: benchmarking approach. In 2024 5th Information Communication Technologies Conference (ICTC), page 349353. IEEE, May 2024. doi: 10.1109/ictc61510. 2024.10601938. URL http://dx.doi.org/10.1109/ictc61510.2024.10601938. [9] Emilio Ferrara. Fairness and bias in artificial intelligence: brief survey of sources, impacts, and mitigation strategies. Sci, 6(1):3, December 2023. ISSN 2413-4155. doi: 10.3390/sci6010003. URL http://dx.doi.org/10.3390/sci6010003. [10] Riccardo Fogliato, Maria De-Arteaga, and Alexandra Chouldechova. Human discernment of algorithmic errors: case study in child welfare. SSRN Electronic Journal, 2025. ISSN 15565068. doi: 10.2139/ssrn.4050125. URL http://dx.doi.org/10.2139/ssrn.4050125. [11] Ben Green and Yiling Chen. Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 19, page 9099. ACM, January 2019. doi: 10.1145/3287560.3287563. URL http://dx.doi.org/10.1145/3287560.3287563. [12] Emily Hadley, Alan Blatecky, and Megan Comfort. Investigating algorithm review boards for organizational responsible artificial intelligence governance, 2024. URL https://arxiv. org/abs/2402.01691. 11 [13] Yi-Jen (Ian) Ho, Wael Jabr, and Yifan Zhang. Ai enforcement: Examining the impact of ai on judicial fairness and public safety. SSRN Electronic Journal, 2023. ISSN 1556-5068. doi: 10.2139/ssrn.4533047. URL http://dx.doi.org/10.2139/ssrn.4533047. [14] Leilei Jiang, Guixiang Zhu, Jianshan Sun, Jie Cao, and Jia Wu. Exploring the occupational biases and stereotypes of chinese large language models. Scientific Reports, 15(1), May 2025. ISSN 2045-2322. doi: 10.1038/s41598-025-03893-w. URL http://dx.doi.org/10.1038/ s41598-025-03893-w. [15] Anna Kawakami, Venkatesh Sivaraman, Hao-Fei Cheng, Logan Stapleton, Yanghuidi Cheng, Diana Qing, Adam Perer, Zhiwei Steven Wu, Haiyi Zhu, and Kenneth Holstein. Improving human-ai partnerships in child welfare: Understanding worker practices, challenges, and desires for algorithmic decision support. In CHI Conference on Human Factors in Computing Systems, CHI 22, page 118. ACM, April 2022. doi: 10.1145/3491102.3517439. URL http://dx.doi.org/10.1145/3491102.3517439. [16] Mesut Kaya and Toine Bogers. Mapping stakeholder needs to multi-sided fairness in candidate recommendation for algorithmic hiring. In Proceedings of the Nineteenth ACM Conference on Recommender Systems, RecSys 25, page 257267. ACM, September 2025. doi: 10.1145/ 3705328.3748079. URL http://dx.doi.org/10.1145/3705328.3748079. [17] Ibomoiye Domor Mienye, George Obaido, Ikiomoye Douglas Emmanuel, and Ayodeji Akeem Ajani. survey of bias and fairness in healthcare ai. In 2024 IEEE 12th International Conference on Healthcare Informatics (ICHI), page 642650. IEEE, June 2024. doi: 10.1109/ichi61247. 2024.00103. URL http://dx.doi.org/10.1109/ichi61247.2024.00103. [18] Khalida Walid Nathim, Nada Abdulkareem Hameed, Saja Abdulfattah Salih, Nada Adnan Taher, Hayder Mahmood Salman, and Dmytro Chornomordenko. Ethical ai with balancing bias mitigation and fairness in machine learning models. In 2024 36th Conference of Open Innovations Association (FRUCT), page 797807. IEEE, October 2024. doi: 10.23919/fruct64283.2024. 10749873. URL http://dx.doi.org/10.23919/fruct64283.2024.10749873. [19] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. Bbq: hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, page 20862105. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022. findings-acl.165. URL http://dx.doi.org/10.18653/v1/2022.findings-acl.165. [20] Dragutin Petkovic. It is not accuracy vs. explainabilitywe need both for trustworthy ai systems. IEEE Transactions on Technology and Society, 4(1):4653, March 2023. ISSN 2637-6415. doi: 10.1109/tts.2023.3239921. URL http://dx.doi.org/10.1109/TTS.2023. 3239921. [21] Xinru Wang, Chen Liang, and Ming Yin. The effects of ai biases and explanations on human decision fairness: case study of bidding in rental housing markets. In Proceedings of the ThirtySecond International Joint Conference on Artificial Intelligence, IJCAI-2023, page 30763084. International Joint Conferences on Artificial Intelligence Organization, August 2023. doi: 10.24963/ijcai.2023/343. URL http://dx.doi.org/10.24963/ijcai.2023/343. [22] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in llm-as-a-judge, 2024. URL https://arxiv.org/abs/2410. 02736. [23] Chih-Cheng Rex Yuan and Bow-Yaw Wang. Ensuring fairness with transparent auditing of quantitative bias in ai systems. In 2024 Pacific Neighborhood Consortium Annual Conference and Joint Meetings (PNC), page 2532. IEEE, August 2024. doi: 10.23919/pnc63053.2024. 10697374. URL http://dx.doi.org/10.23919/pnc63053.2024.10697374. [24] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685. 12 Methodology: Technical Details This appendix provides complete mathematical specifications for SHARP, expanding the summary in Section 2. Throughout, all normalized quantities lie in [0, 1], with larger values indicating greater estimated harm. A.1 Normalization of judge outputs Judge outputs are heterogeneous: boolean indicators, categorical labels, and ordinal scales each require distinct treatment. SHARP normalizes all judge outputs to the unit interval [0, 1], where larger values uniformly indicate greater social harm. This normalization enables commensurable combination across output types while preserving ordinality. Ordinal scores. For any ordinal severity score {1, 2, . . . , 10}, the affine normalization ϕord(x) = 1 9 (6) maps the minimum score to 0 and the maximum to 1. This transformation preserves linear spacing under the assumption of equal intervals between adjacent ratings. Boolean indicators. For any boolean indicator {False, True}, ϕbool(b) = (cid:26)0 1 if = False, if = True. (7) Categorical variables. Certain categorical outputs admit natural orderings. For bias explicitness, which takes values in {none, implicit, explicit}, we employ the ordinal mapping ϕexp(t) = 0.0 0.5 1.0 if = none, if = implicit, if = explicit. (8) This trichotomous scale reflects that bias may manifest implicitly (subtle framing, omission, connotation) or explicitly (overt stereotyping or discriminatory statements). The mapping enforces monotonic escalation without introducing additional degrees of freedom. A.2 Sub-index construction SHARP decomposes social harm into four constituent dimensions, each capturing distinct mechanism by which model outputs can cause societal harm: bias (B), fairness (F ), ethics (E), and epistemic reliability (K). For model , prompt q, and judge j, we compute BM,q,j, FM,q,j, EM,q,j, KM,q,j [0, 1]. All sub-indices are constructed using fixed, interpretable coefficients chosen priori for transparency and controlled behavior rather than empirical optimization. A.2.1 Bias sub-index The bias sub-index models three facets of biased content: breadth of categories affected, intensity of harm, and amplifying factors. Bias coverage (breadth). Let = {c1, . . . , ck} denote the set of bias category indicators (e.g., gender, race, ethnicity, disability, age, religion, geographic origin). Coverage quantifies the fraction of categories for which bias is detected: covM,q,j = 1 (cid:88) cC ϕbool(c). 13 (9) Bias intensity. Let = ϕord(bias_severity) and = ϕord(bias_impact) denote normalized severity and impact scores. Intensity is computed as the RMS of these components: intM,q,j = (cid:114) s2 + i2 2 . Base bias. Coverage and intensity are combined with fixed weights:"
        },
        {
            "title": "Bbase",
            "content": "M,q,j = 0.35 covM,q,j + 0.65 intM,q,j. (10) (11) The heavier weight on intensity encodes the design choice that concentrated severe bias may be more consequential than diffuse mild bias. Amplification factors. Two bounded modifiers, explicitness and intersectionality, Let = ϕexp(explicitness) and = escalate harm conditional on presence. ϕbool(intersectionality). Define AM,q,j = 1 + 0.25 + 0.25 t, (12) so AM,q,j [1, 1.5]. Sub-index coefficients and design choices. All sub-index constructions in SHARP use fixed coefficients specified priori. These coefficients are not learned, optimized, or claimed to be statistically optimal. Instead, they encode transparent stakeholder risk preferences about how different facets of harm should be emphasized when constructing an operational risk signal. For example, in the bias sub-index, intensity is assigned greater weight than coverage because concentrated, severe bias affecting small number of groups may plausibly be more consequential than diffuse but mild bias spread across categories. This weighting reflects normative judgment about harm severity, not an empirical claim about real-world impact, and alternative stakeholders may reasonably adopt different weightings. SHARP does not assert that any particular coefficient configuration is uniquely correct. Rather, the framework separates measurement from normative aggregation: the underlying judge-derived indicators remain fixed, while coefficients act as explicit governance parameters that can be adjusted to reflect institutional priorities, regulatory thresholds, or domain-specific risk tolerances. Final bias sub-index. A.3 Fairness sub-index BM,q,j = min(cid:0)1, Bbase M,q,j AM,q,j (cid:1). (13) The fairness dimension in SHARP is intended to capture within-response inequitable treatment cues expressed in single model output, rather than population-level statistical fairness metrics that require aggregation across individuals or outcomes. Accordingly, SHARPs fairness sub-index is an intrinsic, prompt-level construct that operationalizes fairness as linguistic and normative property of the generated response. For each prompt response pair, judges assess three normalized indicators: (i) demographic disparity (f1), reflecting differential treatment or standards applied to explicitly referenced social groups within the response; (ii) equality-of-opportunity gap (f2), capturing denial, restriction, or conditional access to resources, opportunities, or rights based on group membership; and (iii) group disparity (f3), reflecting hierarchical framing, normative privileging, or systematic devaluation of particular groups. Each indicator is scored based on the content of the response itself, without requiring counterfactual generation or across-prompt aggregation. The fairness sub-index is computed using root-mean-square aggregation, FM,q,j = (cid:114) 1 + 2 2 + 2 2 3 3 , which emphasizes pronounced inequities while preserving boundedness and interpretability. 14 (15) (16) We emphasize that this formulation does not attempt to estimate demographic parity, equalized odds, or other statistical fairness criteria traditionally defined at the dataset or population level. Instead, SHARPs fairness axis captures whether single response exhibits inequitable treatment signals that would plausibly contribute to social harm if encountered in deployment. Group-level fairness analysis remains an important but distinct problem and is intentionally out of scope for SHARPs prompt-level evaluation setting. A.3.1 Ethics sub-index The ethics sub-index aggregates five normalized indicators of value misalignment: ethical misalignment (g1), value conflict (g2), harm risk (g3), cultural insensitivity (g4), and exclusion risk (g5): (cid:115) (cid:80)5 i=1 g2 5 . (14) EM,q,j = A.3.2 Epistemic sub-index The epistemic sub-index aggregates two normalized indicators of unreliability: epistemic unsoundness (k1) and epistemic risk (k2): KM,q,j = (cid:114) k2 1 + k2 2 . A.4 Geometric aggregation principle: Root-mean-square (RMS) For vector = (x1, . . . , xd) [0, 1]d, the root-mean-square is RMS(x) = (cid:118) (cid:117) (cid:117) (cid:116) 1 d (cid:88) i=1 x2 = x2 . RMS preserves boundedness, emphasizes large deviations relative to arithmetic averaging, and admits geometric interpretation as normalized Euclidean distance in the unit hypercube. SHARP uses RMS both within sub-index components and as an auxiliary magnitude diagnostic over the four harm dimensions. A.5 Judge ensemble aggregation via log-sum-exp Each prompt model pair is evaluated by multiple judges. Arithmetic averaging may attenuate rare but severe harm signals, while max pooling is brittle. SHARP employs log-sum-exp (LSE) aggregation as risk-sensitive compromise. For harm dimension {B, F, E, K} with judge-specific values X1, X2, X3 [0, 1] and temperature τ > 0: LSEτ (X1, X2, X3) = τ log 1 3 3 (cid:88) j=1 eXj /τ . (17) LSE satisfies LSEτ (x) maxj xj and interpolates between mean pooling (τ ) and max pooling (τ 0+). SHARP fixes τ = 0.20, with sensitivity analysis over τ {0.15, 0.20, 0.25} reported in Appendix E. The resulting prompt-level judge-aggregated sub-indices are denoted BM,q, FM,q, EM,q, and KM,q. A.6 Harm-space embedding and compounded risk via additive log-risk Embedding into harm space. For each model and prompt q, SHARP embeds the response into four-dimensional harm space using the judge-aggregated sub-indices: hM,q = (cid:0) BM,q, FM,q, EM,q, KM,q (cid:1) [0, 1]4. (18) Higher coordinates indicate greater estimated harm along the corresponding dimension. 15 Auxiliary harm radius. As magnitude diagnostic that does not require joint activation, SHARP reports the (normalized) Euclidean radius: (cid:115) rM,q = RMS(hM,q) = B2 M,q + 2 M,q + 2 M,q M,q + E2 4 [0, 1]. (19) Union-of-failures aggregated harm. To model the event that at least one harm dimension is activated, SHARP defines the prompt-level aggregated harm probability"
        },
        {
            "title": "H any",
            "content": "M,q = 1 (cid:89) (1 hi,M,q), i{B,F,E,K} (20) where hi,M,q denotes the corresponding coordinate of hM,q. Equation (20) is the standard unionof-failures construction. It assumes conditional independence across dimensions given (M, q) as first-order approximation for aggregation, not causal claim. Residual safety and additive log-risk. While any M,q yields an interpretable probability of any harm, its bounded scale can saturate near 1 and is less decomposable. SHARP therefore reparameterizes residual safety, SM,q = 1 any M,q = (cid:89) (1 hi,M,q), using negative log transform. Define dimension-wise log-risk contributions ℓi,M,q = log(1 hi,M,q + ε), ε = 106, and the cumulative log-risk (equivalently, negative log residual safety) LM,q = (cid:88) ℓi,M,q = log(cid:0)1 any M,q + ε(cid:1). i{B,F,E,K} (21) (22) This transformation preserves ordering (higher harm larger LM,q), maps hi = 0 to ℓi = 0, and diverges as hi 1, emphasizing near-maximal failures. Equation (22) also establishes that compounded risk is additive in log space, enabling transparent dimensional attribution via the summands ℓi,M,q. Interpretation. The quantity log(1 h) corresponds to cumulative hazard-style reparameterization of bounded harm into unbounded risk units. In this representation, independent failure pressures add, and tail events in any dimension contribute sharply to LM,q, aligning the compounded metric with worst-case risk sensitivity. A.7 Model-level risk profiling and risk statistics Each model induces an empirical distribution of prompt-level cumulative log-risk values {LM,q}qQ over the evaluation corpus. SHARP summarizes this distribution using complementary statistics capturing central tendency, dispersion, and tail behavior. Expected log-risk. Log-risk volatility. µL(M ) = 1 (cid:88) qQ LM,q. σL(M ) = (cid:115) 1 (cid:88) (cid:0)LM,q µL(M )(cid:1)2 . qQ (23) (24) Higher volatility indicates prompt sensitivity and behavioral instability, corresponding to models that are typically lower-risk but exhibit severe compounded failures on subset of prompts. 16 Tail risk via Conditional Value at Risk. For confidence level α = 0.95, define the Value at Risk (VaR) as the α-quantile of {LM,q}: VaRα(M ) = inf{x : Pr(LM,q x) α}, (25) and Conditional Value at Risk (CVaR) as CVaRα(M ) = E[LM,q LM,q VaRα(M )] . (26) For α = 0.95, CVaR0.95(M ) measures the mean compounded log-risk among the worst-performing 5% of prompts. CVaR is coherent risk measure that captures tail severity rather than merely the onset of extreme behavior and serves as SHARPs primary safety-relevant comparison metric. A.8 Matrix representation The per-judge harm vector is zM,q,j = (BM,q,j, FM,q,j, EM,q,j, KM,q,j) [0, 1]4. The judge-aggregated harm matrix for model is [0, 1]n4, ZM = M,q1 ... M,qn and the corresponding prompt-level cumulative log-risk vector is LM = (LM,q1, . . . , LM,qn ) Rn +. Dimension-specific expected harms are given by HM = 1 1ZM = (E[ B], E[ ], E[ E], E[ K]). (27) (28) (29) (30) For policy weight vector 3 (the 3-simplex), linear policy-weighted summary is sM = wHM . (31) This separation of measurement (ZM ) from normative weighting (w) supports transparent governance and stakeholder-specific aggregation. A.9 Methodological contributions (technical view) SHARP contributes the following methodological components relative to mean-centered scalar evaluation: 1. Dimensional decomposition. Harm is decomposed into bias, fairness, ethics, and epistemic dimensions, enabling targeted diagnosis and transparent reporting. 2. Risk-sensitive aggregation. Judge ensembling via LSE emphasizes elevated harm assessments while remaining robust to single-judge outliers. 3. Compounded risk via additive log-risk. Prompt-level multi-dimensional harm is aggregated through union-of-failures construction and reparameterized as cumulative log-risk, yielding an additive, decomposable compounded risk signal aligned with worst-case behavior. 4. Distributional profiling. Reporting mean, volatility, and CVaR0.95 over prompt-level risk distributions characterizes both typical behavior and tail failures."
        },
        {
            "title": "B Judge Agreement and Disagreement Structure",
            "content": "This appendix characterizes the reliability and disagreement structure of the three-judge protocol used throughout SHARP. We report (i) dispersion of judge assessments via mean absolute deviation (MAD) computed at the prompt level within each harm dimension, (ii) stability of model tail-risk rankings under leave-one-judge-out (LOJO) re-aggregation using CVaR0.95, and (iii) prompt-level concordance between judges via pairwise Kendalls τ computed separately for each model and harm dimension. These diagnostics are intended to quantify sensitivity to judge identity and to separate systematic disagreements from idiosyncratic noise. 17 Table 5: Inter-judge dispersion by dimension via mean absolute deviation (MAD). MAD is computed per (prompt, model) across the three judges and summarized over all evaluated prompt model pairs (n = 9,911 per dimension). Dimension items MAD mean MAD std"
        },
        {
            "title": "Ethics\nBias\nFairness\nEpistemic",
            "content": "9,911 9,911 9,911 9,911 0.0378 0.0447 0.0460 0.0568 0.0585 0.0796 0.0698 0.0679 Table 6: Leave-one-judge-out (LOJO) stability of model CVaR0.95 rankings. We compute Kendalls τ between the full three-judge CVaR ranking and the LOJO ranking after omitting the specified judge. Judge omitted models Kendall τ OpenAI::gpt-5.1 Anthropic::claude-sonnet-4-5 Google::gemini-2.5-pro 11 11 11 1.0000 0.8909 0.8909 p-value 5.01 108 1.37 105 1.37 105 Evaluation setting and configuration. All results in this appendix use = 901 prompts evaluated across = 11 models with = 3 blinded LLM judges. Judge scores are aggregated with log-sum-exp using temperature τ = 0.2. For tail-risk summaries, we report CVaR0.95. Pairwise Kendalls τ is computed only when the overlap between two judges prompt-level scores is at least 25 prompts per model and dimension. (Complete configuration and provenance are recorded in the accompanying generated report.) B.1 Inter-judge dispersion via mean absolute deviation Let s(k,d) q,M [0, 1] denote judge ks score for prompt and model under dimension {bias, fairness, ethics, epistemic}. For each (q, M, d) we compute the mean absolute deviation across judges, MAD(d) q,M = 1 (cid:88) k= (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) s(k,d) q,M 1 (cid:88) k=1 s(k,d) q,M (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . Table 5 reports the mean and standard deviation of MAD(d) q,M aggregated over all evaluated prompt model pairs (total = 9,911 per dimension). Lower MAD indicates tighter inter-judge agreement. Inter-judge dispersion varies systematically across harm dimensions, with epistemic assessments exhibiting the largest average MAD, followed by fairness and bias, and ethics exhibiting the lowest dispersion. These differences reflect dimension-dependent variability in judge scoring behavior under the current rubric, rather than calibrated differences in underlying harm severity. B.2 Rank stability under leave-one-judge-out We next test whether model ordering under tail risk is sensitive to the composition of the judge panel. For each LOJO condition, we remove one judge and re-aggregate the remaining two judges scores using the same log-sum-exp operator at τ = 0.2, then compute model-level CVaR0.95 over prompts using the same scalar harm construction as in the main pipeline for this analysis run. We compare the induced model ranking to the full three-judge ranking using Kendalls τ on ranks  (Table 6)  . The rankings are highly stable: removing the OpenAI judge yields identical ordering (τ = 1.0), while removing either of the other judges yields τ 0.891. This indicates that the CVaR-based model ordering is largely robust to single-judge ablations under the specified aggregation temperature and prompt distribution. 18 Table 7: Pairwise prompt-level judge concordance via Kendalls τ , aggregated over models. For each model and dimension, we compute Kendalls τ between judge pair over prompt-level sub-index scores, then summarize τ across the 11 evaluated models. Dimension Judge pair models Mean τ Median τ Std τ"
        },
        {
            "title": "Ethics\nEthics\nEthics",
            "content": "claude-sonnet-4-5 vs gemini-2.5-pro claude-sonnet-4-5 vs gpt-5.1 gemini-2.5-pro vs gpt-5.1 claude-sonnet-4-5 vs gemini-2.5-pro claude-sonnet-4-5 vs gpt-5.1 gemini-2.5-pro vs gpt-5.1 claude-sonnet-4-5 vs gemini-2.5-pro claude-sonnet-4-5 vs gpt-5.1 gemini-2.5-pro vs gpt-5."
        },
        {
            "title": "Epistemic\nEpistemic\nEpistemic",
            "content": "claude-sonnet-4-5 vs gpt-5.1 claude-sonnet-4-5 vs gemini-2.5-pro gemini-2.5-pro vs gpt-5.1 11 11 11 11 11 11 11 11 11 11 11 11 0.6980 0.5626 0. 0.6536 0.5978 0.5344 0.6333 0.5557 0.5357 0.6428 0.5595 0.4619 0.7127 0.0761 0.5774 0.0541 0.5518 0.0570 0.6830 0.0924 0.5928 0.0542 0.5316 0.0940 0.6522 0.0878 0.5637 0.0501 0.5394 0. 0.6440 0.0370 0.5566 0.1014 0.4452 0.1082 B.3 Prompt-level judge concordance We next measure prompt-level concordance between judges within each harm dimension. For each model and dimension d, we compute Kendalls τ between each judge pair using their prompt-level score vectors {s(k,d) q,M }qQ, restricted to prompts with non-missing overlap. We then summarize these per-model τ values across the 11 evaluated models by their mean, median, and standard deviation  (Table 7)  . Larger τ indicates stronger agreement in the relative ordering of prompts by estimated harm severity. Across dimensions, we observe moderate prompt-level concordance between judges, with systematic variation across judge pairs and harm categories. In several dimensions, the Claude Gemini pair exhibits higher average concordance than pairs involving GPT-based judges, while other pairings show lower but still positive agreement. These patterns indicate dimensionand pair-specific differences in prompt-level ranking behavior rather than uniformly noisy or unstable assessments. Implications for SHARP validity claims. Taken together, these diagnostics indicate that interjudge agreement is dimension-dependent and that judges exhibit consistent but imperfect concordance in ranking prompts by harm severity. Combined with the leave-one-judge-out analyses, these results show that SHARPs model-level tail-risk rankings remain stable with respect to judge identity under the chosen aggregation temperature, even in the presence of prompt-level ranking disagreements. Accordingly, SHARPs comparative conclusions should be interpreted as robust to reasonable variation in judge composition, while recognizing that prompt-level harm ordering is inherently subject to rubricand judge-specific variation."
        },
        {
            "title": "C Judge Ensemble Sensitivity and Overlap Considerations",
            "content": "Judge model overlap is known concern in LLM-as-a-judge evaluation, particularly when judges and evaluated models share training distributions or architectural lineage. SHARP is explicitly designed to accommodate alternative judge configurations without modification to its core metric definitions. Several robustness analyses are natural extensions of the present study. First, leave-one-judge-out protocol can be applied, recomputing all SHARP statistics using each two-judge subset to assess sensitivity to individual judges. Second, fully disjoint judge ensemble can be substituted to eliminate self-evaluation effects entirely, enabling direct quantification of overlap-induced shifts in absolute harm levels and relative rankings. Third, cross-ensemble comparisons can be used to characterize variance attributable to judge choice, complementing prompt-level variance decomposition. These analyses are orthogonal to SHARPs formulation and can be executed without altering the harm taxonomy, geometric embedding, or risk-sensitive aggregation. We leave comprehensive 19 Table 8: Empirical dependence among harm dimensions. For each model, we compute prompt-level correlations among (B, F, E, K) across all prompts and within the tail slice (top 5% by L). Reported values are the mean correlation across the six dimension pairs. Evaluated model claude-sonnet-4-5 claude-3-5-sonnet gemini-1.5-pro qwen3-235b gemini-2.5-pro gpt-4o llama3-1-405b gpt-oss-120b llama3-3-70b deepseek-chat mistral-large Mean ρ (all) Mean ρ (top 5% by L) 0.638 0.743 0.765 0.814 0.814 0.819 0.857 0.859 0.884 0.887 0.894 0.483 0.448 0.297 0.372 0.246 0.283 0.113 0.012 0.067 0.224 0.135 cross-judge sensitivity study to future benchmark instantiations as larger and more diverse judge pools become available."
        },
        {
            "title": "D Empirical Test of the Independence Assumption",
            "content": "A central modeling assumption underlying harm representation is that the four harm dimensions: bias (B), fairness (F ), ethics (E), and epistemic reliability (K), are not perfectly dependent at the prompt level. While SHARP does not require strict independence, excessive correlation among subindices would undermine the interpretability of multiplicative aggregation and weaken the motivation for modeling joint harm via union-of-failures formulation. To make this assumption empirically testable, we directly measure prompt-level dependence among (B, F, E, K) using the materialized harm embeddings. four sub-indices Methodology. For each evaluated model, we compute pairwise correlations among the stored in across prompts using the prompt-level sharp_harm_space_embedding_v6. Specifically, for each model , we compute correlations for all six unordered pairs in {(B, ), (B, E), (B, K), (F, E), (F, K), (E, K)}. Correlations are computed using Spearmans ρ, which captures monotonic dependence and is robust to non-Gaussian score distributions. We report, for each model, the mean correlation across the six pairs. embeddings To probe dependence in the regime most relevant for risk aggregation, we additionally compute tail-slice correlations restricted to the top 5% of prompts by cumulative log risk L. This tail slice isolates high-risk prompts where compounding effects would be most consequential and where violations of approximate independence would most strongly affect SHARPs tail-risk statistics. Results. Table 8 summarizes the results. Across all prompts, mean pairwise correlations are moderate to high for most models, indicating that harm dimensions are not statistically independent in general. This is expected, as many socially sensitive prompts activate multiple forms of harm simultaneously. However, when restricting attention to the high-risk tail (top 5% by L), correlations consistently decreaseoften substantiallyacross all models. Several models exhibit near-zero mean correlations in the tail slice, suggesting that extreme-risk prompts are characterized by more heterogeneous and asymmetric activation patterns across dimensions rather than uniformly elevated harm. Implications for Eq. (3). These findings support the use of multiplicative, union-of-failuresstyle aggregation despite moderate global dependence among sub-indices. While (B, F, E, K) are correlated on average, the reduced dependence observed in the high-risk tail implies that worst-case harm is frequently driven by dimension-specific failures rather than fully coupled effects. Consequently, Eq. (3) should be interpreted as an operational risk aggregation under approximate independence in 20 the regime that dominates tail risk, rather than as claim of strict statistical independence across all prompts. This empirical validation strengthens the methodological grounding of SHARP without requiring explicit copula modeling or stronger parametric assumptions."
        },
        {
            "title": "E LSE Temperature Sensitivity Analysis",
            "content": "SHARP aggregates three judge scores for each (q, ) evaluation using log-sum-exp (LSE). The temperature τ controls the aggregation regime: (cid:32) LSE(s; τ ) = τ log"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) exp (cid:16) sk τ (cid:33) (cid:17) , = 3, (32) k=1 where = (s1, . . . , sK) are judge scores for fixed harm dimension. Smaller τ increases sensitivity to the most severe judge assessment, while larger τ approaches arithmetic averaging and attenuates single-judge peaks. Sensitivity design and metric. We evaluate robustness over τ {0.15, 0.20, 0.25} using SHARPs current compounded-risk construction: cumulative log-risk LM,q computed after judge ensembling, and model-level ordering induced by CVaR0.95(L) (lower is safer). We use τ = 0.20 as the reference setting. Rank stability. Model rankings are invariant across the tested temperature grid. Rank correlations with the reference ordering at τ = 0.20 are perfect (Kendalls τb = 1.0 and Spearmans ρ = 1.0 for τ = 0.15 and τ = 0.25). This indicates that comparative conclusions are not an artifact of particular temperature choice. Table 9: Rank stability of model ordering induced by CVaR0.95(L) across LSE temperatures, measured against the reference ordering at τ = 0.20. τ 0.15 0.20 0.25 τref 0.20 0.20 0.20 Kendall τb Spearman ρ 1.000 1.000 1.000 1.000 1.000 1.000 Extremes and qualitative stability. The identity and ordering of the safest and riskiest models remain unchanged across τ . Table 10 reports the top-5 safest and bottom-5 riskiest models under CVaR0.95(L) at each temperature, alongside mean log-risk. Both the head and tail of the ordering are stable, which is the safety-relevant regime for SHARPs risk profiling. Cardinal sensitivity. Although ordinal conclusions are perfectly stable, absolute CVaR values change modestly with τ , reflecting the intended behavior of LSE as it interpolates between more pessimistic and more averaging aggregation. Across models, the maximum spread in CVaR0.95(L) over τ {0.15, 0.20, 0.25} is small relative to cross-model separations; the largest observed CVaR is 0.0915 (llama3-1-405b), and the smallest is 0.0208 (claude-sonnet-4-5-20250929). Table 11 reports the per-model maximum CVaR spread across τ . Interpretation. This analysis separates ordinal robustness from cardinal sensitivity. Under SHARPs current compounded-risk definition (cumulative log-risk after ensembling), model ordering by tail risk is invariant across reasonable temperature range, while absolute CVaR magnitudes exhibit mild, structured variation. Since SHARP is designed for comparative risk assessment and tail profiling, the invariance of CVaR0.95(L)-based rankings provides strong evidence that the choice τ = 0.20 does not induce cherry-picked outcomes."
        },
        {
            "title": "F Aggregation Robustness and Comparison to Simpler Risk Aggregates",
            "content": "This appendix evaluates the robustness of SHARPs tail-risk aggregation by comparing it to several simpler, commonly used risk aggregates, without re-running model inference. All results are computed post hoc from the materialized prompt-level metrics in sharp_harm_space_embedding_v6, 21 Table 10: Top-5 safest and bottom-5 riskiest models by CVaR0.95(L) across LSE temperatures. Lower is safer. τ Model Mean log-risk CVaR0.95(log-risk) Safest (lowest CVaR) 0.15 claude-sonnet-4-5 0.15 gemini-1.5-pro 0.15 claude-3-5-sonnet 0.15 gemini-2.5-pro 0.15 qwen3-235b 0.20 claude-sonnet-4-5 0.20 gemini-1.5-pro 0.20 claude-3-5-sonnet 0.20 gemini-2.5-pro 0.20 qwen3-235b 0.25 claude-sonnet-4-5 0.25 gemini-1.5-pro 0.25 claude-3-5-sonnet 0.25 gemini-2.5-pro 0.25 qwen3-235b Riskiest (highest CVaR) 0.15 gpt-oss-120b 0.15 deepseek-chat 0.15 mistral-large 0.15 llama3-3-70b 0.15 llama3-1-405b 0.20 gpt-oss-120b 0.20 deepseek-chat 0.20 mistral-large 0.20 llama3-3-70b 0.20 llama3-1-405b 0.25 gpt-oss-120b 0.25 deepseek-chat 0.25 mistral-large 0.25 llama3-3-70b 0.25 llama3-1-405b 0.156128 0.458956 0.445173 0.556568 0.584740 0.157465 0.462903 0.448861 0.560106 0.588877 0.159029 0.467818 0.453398 0.564731 0.594279 1.20068 1.28478 1.31141 1.95353 2.13724 1.20908 1.29552 1.32181 1.96802 2.15399 1.22001 1.30957 1.33531 1.98689 2. 1.68009 3.46838 3.75266 3.85193 3.87204 1.68902 3.49870 3.78177 3.86761 3.89523 1.70089 3.53697 3.81934 3.89111 3.92749 5.25536 5.70648 5.95198 7.90593 8.35638 5.28608 5.73468 5.98760 7.93393 8.39657 5.32640 5.77213 6.03271 7.96984 8. ensuring that differences arise solely from aggregation choice rather than evaluation noise or sampling variation. Baseline aggregates. For each evaluated model, we compute four alternative model-level risk summaries over the same prompt distribution: (i) CVaR0.95 of cumulative log-risk (SHARPs primary statistic), (ii) CVaR0.95 of the harm radius, (iii) CVaR0.95 of the maximum sub-index max(B, F, E, K), and (iv) the mean any-harm probability. Table 12 (derived from the accompanying artifact) reports these quantities alongside the mean of L. As expected, mean-based summaries are substantially less sensitive to extreme failures, while CVaR-based aggregates amplify tail behavior. Rank consistency across aggregates. To assess whether SHARPs conclusions depend on the specific choice of aggregate, we compute rank correlations between model orderings induced by each summary statistic. As shown in Table 13, CVaR0.95(L) is highly correlated with other tailsensitive aggregates, including CVaR of harm radius and CVaR of the maximum sub-index (Spearman ρ 0.95 in all cases). Correlation with the mean any-harm probability is also strong, indicating that SHARPs tail-risk ranking is broadly consistent with simpler baselines. At the same time, these correlations are not perfect, leaving room for meaningful reordering when tail behavior diverges. Concrete decision flip. Despite high overall rank agreement, simpler aggregates can obscure practically relevant differences. The final block of results identifies concrete decision flip between two models with nearly identical mean cumulative log-risk but materially different CVaR0.95(L). 22 Table 11: Per-model sensitivity magnitude: maximum CVaR0.95(log-risk) across τ {0.15, 0.20, 0.25}. Model Max CVaR across τ llama3-1-405b mistral-large gpt-oss-120b gemini-1.5-pro claude-3-5-sonnet gpt-4o deepseek-chat llama3-3-70b qwen3-235b gemini-2.5-pro claude-sonnet-4-5 0.0915134 0.0807286 0.0710353 0.0685905 0.0666781 0.0658673 0.0656494 0.0639092 0.0554545 0.0391798 0.0207980 Table 12: Model-level risk aggregates computed over the same prompt distribution (n = 901 per model). Lower values indicate lower estimated risk. CVaR0.95 denotes tail risk at the 95th percentile. Evaluated model Mean CVaR0.95(L) CVaR0.95(R) CVaR0.95(max) Mean any-harm claude-sonnet-4-5 claude-3-5-sonnet gemini-1.5-pro gemini-2.5-pro qwen3-235b gpt-4o gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b 0.157 0.449 0.463 0.560 0.589 0.747 1.209 1.296 1.322 1.968 2.154 1.689 3.782 3.499 3.868 3.895 4.519 5.286 5.735 5.988 7.934 8.397 0.344 0.590 0.569 0.600 0.586 0.653 0.715 0.729 0.737 0.783 0.796 0.438 0.749 0.701 0.752 0.718 0.812 0.864 0.856 0.873 0.918 0. 0.101 0.202 0.212 0.232 0.252 0.296 0.405 0.434 0.413 0.554 0.612 In this pair, the absolute difference in mean is negligible, yet the CVaR differs by approximately 0.45, leading to different risk ordering under tail-sensitive evaluation. This example demonstrates that mean-only aggregation can mask rare but severe failures that SHARP is explicitly designed to surface. Implications. Taken together, these results show that SHARP is not an idiosyncratic aggregation that contradicts simpler baselines. Instead, it refines them by focusing the evaluation on the regime that dominates societal risk.High rank correlation establishes robustness, while the observed decision flip illustrates why tail-aware aggregation provides additional discriminative power beyond meanbased or max-based summaries. This comparison strengthens the empirical case for using CVaR of cumulative log-risk as SHARPs primary model-level statistic. F.1 Comparison to Simpler Risk Aggregates All results in this subsection, shown in 12, 13, and 14 are computed post hoc, without re-running inference."
        },
        {
            "title": "G Supplementary Analysis for Harm Decomposition",
            "content": "Judge reliability and sensitivity. We evaluate the robustness of sub-index measurements to judge selection using inter-judge dispersion (mean absolute deviation), leave-one-judge-out re-aggregation, and prompt-level rank concordance. Results in Appendix show moderate, dimension-dependent disagreementhighest for epistemic assessmentsbut strong stability of model-level CVaR rankings, indicating that the comparative findings in the main text are not artifacts of particular judge configuration. 23 Table 13: Rank correlations between model orderings induced by different aggregation strategies. Spearmans ρ and Kendalls τ are computed over model ranks (n = 11). Aggregate CVaR0.95(L) CVaR0.95(L) CVaR0.95(L) CVaR0.95(R) CVaR0.95(R) CVaR0.95(max) Aggregate CVaR0.95(R) CVaR0.95(max) Mean any-harm CVaR0.95(max) Mean any-harm Mean any-harm ρ τ p-value (ρ) 0.973 0.955 0.982 0.982 0.945 0.918 0.927 0.855 0.927 0.927 0.855 0. 5.1107 5.0106 8.4108 8.4108 1.1105 6.7105 Table 14: Example decision flip illustrating divergence between mean risk and tail risk. Two models with nearly identical mean cumulative log-risk exhibit materially different CVaR0.95(L). Evaluated model Mean CVaR0.95(L) Rank (Mean L) Rank (CVaR) gpt-oss-120b deepseek-chat 1.209 1.296 5.286 5. 7 8 7 8 Dependence among harm dimensions. Although SHARPs aggregation does not require strict independence, excessive dependence would weaken the interpretability of union-of-failures formulation. Appendix reports prompt-level correlations among bias, fairness, ethics, and epistemic sub-indices. While moderate correlations are observed across all prompts, dependence weakens substantially within the high-risk tail (top 5% by cumulative log-risk), supporting multiplicative aggregation in the regime that dominates tail-risk statistics. Comparison to simpler aggregates. Appendix compares CVaR95 of cumulative log-risk to simpler post hoc aggregates, including CVaR of harm radius, CVaR of the maximum sub-index, and mean any-harm probability. Model rankings are highly correlated across aggregates, but concrete decision flips demonstrate that mean-based or max-based summaries can obscure rare but severe failures, motivating SHARPs emphasis on tail-sensitive aggregation."
        },
        {
            "title": "H Statistical Validation Details",
            "content": "This appendix reports the full statistical validation of SHARP under the current metric definition, using prompt-level cumulative log-risk LM,q as the primary outcome (lower is safer). All analyses use the repeated-measures design induced by evaluating all = 11 models on the same = 901 prompts. H.1 Paired bootstrap confidence intervals and tail-risk separability Motivation. SHARP risk statistics are computed over heavy-tailed, right-skewed prompt-level distributions. To quantify estimation uncertainty without normality assumptions, we use paired (blocked) nonparametric bootstrap over prompts. Paired bootstrap protocol. Let = {q1, . . . , qn} denote the prompt set, with = 901. For each bootstrap iteration {1, . . . , B} with = 10,000, we sample Q(b) by resampling prompts with replacement and preserve pairing across models by reusing the same bootstrap prompt multiset for every model. For each model , we recompute model-level statistics on Q(b) and store the resulting bootstrap estimate. We report 95% percentile intervals using the empirical 2.5% and 97.5% quantiles of the bootstrap distribution. All bootstrap results are reproducible under fixed seed. Primary tail metric. Our primary safety-relevant statistic is CVaR0.95(L), the conditional expectation of LM,q over the worst 5% of prompts for model . We additionally report mean log-risk E[L] as central tendency diagnostic. Tail-threshold sensitivity (α-sweep). To assess robustness to the tail threshold, we compute CVaRα(L) for α {0.90, 0.95, 0.975} and compare the induced model orderings against the 24 Table 15: Model-level risk profiles with paired 95% bootstrap confidence intervals. Lower values indicate lower risk. Model Mean log-risk E[L] CVaR0.95(L) claude-sonnet-4-5 gemini-1.5-pro claude-3-5-sonnet gemini-2.5-pro qwen3-235b gpt-4o openai.gpt-oss-120b deepseek-chat mistral-large llama3-3-70b llama3-1-405b 0.1575 [0.1317, 0.1855] 0.4629 [0.4015, 0.5238] 0.4489 [0.3873, 0.5157] 0.5601 [0.4896, 0.6325] 0.5889 [0.5147, 0.6668] 0.7475 [0.6658, 0.8316] 1.2091 [1.1051, 1.3118] 1.2955 [1.1852, 1.4077] 1.3218 [1.2030, 1.4461] 1.9680 [1.8169, 2.1242] 2.1540 [1.9979, 2.3115] 1.6890 [1.3062, 1.9987] 3.4987 [3.1519, 3.8314] 3.7818 [3.3013, 4.2474] 3.8676 [3.4762, 4.2652] 3.8952 [3.3412, 4.6666] 4.5190 [4.0658, 4.9440] 5.2861 [4.9544, 5.6064] 5.7347 [5.1103, 6.4948] 5.9876 [5.2804, 6.8811] 7.9339 [6.6536, 9.3290] 8.3966 [6.8947, 10.1705] reference α = 0.95 ordering using rank correlations. Rankings remain highly stable: Kendalls τb = 0.9636 and Spearmans ρ = 0.9909 for both α = 0.90 vs. 0.95 and α = 0.975 vs. 0.95. Table 16: Rank stability across CVaR tail thresholds, relative to α = 0.95. Tail threshold α Kendall τb Spearman ρ 0.90 0.95 0.975 0.9636 1.0000 0.9636 0.9909 1.0000 0.9909 Pairwise separability via bootstrap -CVaR. Interval overlap for per-model CIs is not valid test for pairwise equality. We therefore compute paired bootstrap confidence intervals for pairwise differences in CVaR0.95(L): A,B = CVaRA 0.95(L) CVaRB 0.95(L), under the same paired prompt resampling. pair is deemed separable when the 95% CI for A,B excludes 0. Under this criterion, 44 of 55 model pairs (80.0%) are separable, indicating substantial tail-risk differentiation while retaining non-trivial set of statistically ambiguous near-neighbors. Table 17: Most ambiguous model pairs under paired bootstrap -CVaR0.95(L) (small ; CI includes 0 indicates non-separability at 95%). Model Model -CVaR0.95(L) (95% CI) qwen3-235b gemini-2.5-pro claude-3-5-sonnet gemini-2.5-pro claude-3-5-sonnet qwen3-235b mistral-large deepseek-chat gpt-oss-120b deepseek-chat llama3-3-70b-instruct llama3-1-405b 0.028 [0.886, 0.645] 0.086 [0.728, 0.535] 0.113 [1.013, 0.618] 0.253 [1.181, 0.705] 0.449 [0.229, 1.250] 0.463 [1.360, 2.437] H.2 Non-parametric repeated-measures model comparison Design. Because all models are evaluated on the same prompts, prompt identity acts as blocking factor, yielding within-prompt paired design. Omnibus test (Friedman). We test the null hypothesis that all models have identical prompt-level risk distributions using the Friedman test on LM,q. The test statistic is χ2(10) = 1629.91 with 0 (numerically zero at machine precision), rejecting the null. As an effect size, Kendalls coefficient of concordance is = 0.1809, indicating non-trivial but not dominant between-model separation relative to prompt-level heterogeneity. 25 Table 18: Friedman repeated-measures test on prompt-level cumulative log-risk LM,q (lower is safer). Quantity Number of prompts (n) Number of models (k) Test statistic (χ2) Degrees of freedom (k 1) p-value Kendalls Value 901 11 1629.91 10 0 0. Average ranks. Table 19 reports Friedman average ranks (lower is safer). The rank ordering is broadly consistent with the bootstrap tail-risk ordering, while emphasizing that prompt-level comparisons need not induce strict total order when neighboring models exhibit small paired differences. Table 19: Friedman average ranks for prompt-level cumulative log-risk (lower rank is safer). Model Average rank claude-sonnet-4-5 gemini-1.5-pro gemini-2.5-pro claude-3-5-sonnet qwen3-235b gpt-4o gpt-oss-120b mistral-large deepseek-chat llama3-3-70b llama3-1-405b 4.1337 4.9884 5.1393 5.1465 5.2625 5.7991 6.3940 6.4345 6.6182 7.8135 8. Post-hoc paired tests (Wilcoxon, Holm correction). Following rejection of the omnibus null, we perform two-sided Wilcoxon signed-rank tests for all (cid:0)11 (cid:1) = 55 pairs on paired prompt-level differences, with Holm correction at familywise α = 0.05. We find 48 of 55 pairs (87.3%) remain significant after correction. The non-significant set is concentrated among near-neighbor models, consistent with tiered interpretation rather than an overfit total ordering. 2 H.3 Variance decomposition: model vs. prompt contributions We quantify how much variance in prompt-level cumulative log-risk is attributable to model identity versus prompt identity, and how much remains residual. Two-way fixed-effects decomposition. We fit descriptive two-way fixed-effects decomposition LM,q = µ + αM + βq + εM,q, and report variance proportions via η2 and partial η2. Table 20 shows that prompt identity explains larger share of total variance than model identity, reflecting strong context dependence of harm. Model effects remain material and non-negligible. Table 20: Two-way variance decomposition for cumulative log-risk (fixed effects). Component η2 Share (%) Partial η2 Interpretation Model Prompt Residual 0.1390 0.2583 0.6027 13.9 25.8 60.3 0.1875 0.3001 material model effect strong prompt dependence unmodeled / stochastic Mixed-effects check. As robustness check, we fit linear mixed model with prompt random intercept, LM,q = µ + αM + uq + εM,q, uq (0, σ2 prompt). The estimated random-intercept variance is σ2 marginal = R2 conditional = 0.1873. We interpret this as consistent with the prompt signal being better captured as high-dimensional fixed effect in the two-way decomposition than as single intercept-shift random effect. prompt 0 under this specification, yielding Tail-event add-on. To characterize prompt clustering for tail events, we additionally model ZM,q = 1[LM,q VaR0.95(M )] using logistic mixed model with prompt random intercept (threshold defined per model). The overall tail rate is 0.0511 and the approximate random-intercept variance is 0.2505, indicating non-trivial prompt-level clustering in extreme-risk regions. H.4 LSE temperature robustness We further assess robustness of the full pipeline to the judge-aggregation temperature τ used in LSE ensembling. Over τ {0.15, 0.20, 0.25}, rankings induced by CVaR0.95(L) are invariant (Kendall τb = 1.0, Spearman ρ = 1.0 relative to τ = 0.20), while absolute CVaR magnitudes vary mildly. Full details are provided in Appendix E. Synthesis. Across complementary validation lenses, paired bootstrap uncertainty, distribution-free repeated-measures testing, and variance decomposition, SHARP induces statistically distinguishable model risk profiles under the current compounded-risk definition. At the same time, localized ambiguity among near-neighbors persists, supporting interpretation as risk tiers rather than fragile total order. Tail-risk conclusions are robust to reasonable choices of both LSE temperature and CVaR tail threshold. Prompt-Level Distributions of SHARP Risk Metrics (a) Joint safety probability. (b) Any-harm probability. (c) Cumulative log risk. Figure 1: Prompt-level distributions of SHARP probabilistic and risk-sensitive metrics (box plots). Joint safety probability and any-harm probability characterize prompt-level failure likelihood, while cumulative log risk captures nonlinear aggregation that accentuates tail behavior. Distributional visualizations. We provide both violin and box plots for prompt-level metrics. Violin plots emphasize distributional shape and tail mass, while box plots emphasize robust summaries (median and IQR), facilitating cross-model comparisons under heavy-tailed behavior. 27 (a) Joint safety probability. (b) Any-harm probability. (c) Cumulative log risk. Figure 2: Prompt-level distributions of SHARP probabilistic and risk-sensitive metrics. Joint safety probability and any-harm probability characterize prompt-level failure likelihood, while cumulative log risk captures nonlinear aggregation and tail amplification effects. These distributions motivate SHARPs emphasis on tail-aware statistics over mean-centered evaluation. (a) Bias subindex (ensembled). (b) Fairness subindex (ensembled). (c) Ethics subindex (ensembled). (d) Epistemic subindex (ensembled). Figure 3: Prompt-level distributions of SHARP sub-index harms across evaluated models (violin plots). Each violin summarizes the empirical distribution over prompts for an ensembled judge-based sub-index, exposing dispersion, skew, and tail mass that are obscured by model-level means. 28 (a) Bias subindex (ensembled). (b) Fairness subindex (ensembled). (c) Ethics subindex (ensembled). (d) Epistemic subindex (ensembled). Figure 4: Prompt-level distributions of SHARP sub-index harms across evaluated models (box plots)."
        }
    ],
    "affiliations": [
        "Boston, USA",
        "San Francisco, USA"
    ]
}