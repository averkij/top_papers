{
    "paper_title": "MatTools: Benchmarking Large Language Models for Materials Science Tools",
    "authors": [
        "Siyu Liu",
        "Jiamin Xu",
        "Beilin Ye",
        "Bo Hu",
        "David J. Srolovitz",
        "Tongqi Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] - m . - c [ 1 2 5 8 0 1 . 5 0 5 2 : r MatTools: Benchmarking Large Language Models for Materials Science Tools Siyu Liu1, 2 Jiamin Xu1 Beilin Ye1 Bo Hu1 David J. Srolovitz1, 2 Tongqi Wen1, 2 1Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China 2Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China Source code: https://github.com/Grenzlinie/MatTools Datasets: https://www.kaggle.com/datasets/calvinlyu/mattools/data"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: materials simulation tool question-answer (QA) benchmark and real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1) Generalists outshine specialists: general-purpose LLMs significantly outperform materials science-focused LLMs in materials simulation tool knowledge assessments; (2) AI knows AI: documentation generated by LLMs outperforms both codebase and human-written documentation as retrieval sources; and (3) Simpler is better: LLMs utilizing only LLM-generated documentation as retrieval sources with self-reflection mechanisms outperform more complex approaches such as Agentic RAG system with multiple agents including task decomposition, named-entity recognition (NER) and reranking, or GraphRAG with its knowledge graph construction, structural reasoning, and hierarchical information retrieval in real-world tool-usage tasks. MatTools provides standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research."
        },
        {
            "title": "Introduction",
            "content": "LLMs are now widely applied in the AI for materials science field for applications such as literature and materials database knowledge extraction [13], materials property prediction [46], alloy design [7, 8], discovering new physical laws [9, 10], and proposing new scientific hypotheses [11, 12]. Corresponding author. Email: tongqwen@hku.hk, srol@hku.hk Preprint. Recently, researchers [1315] have built LLM agents that connect to existing software tools to collaboratively tackle complex scientific tasks posed by researchers. While these methods are efficient in completing some specific tasks, they still fundamentally rely on human-written and, hence, LLMs are not able to act autonomously to solve scientific problems. We performed the test of LLM performance in materials science knowledge QA and tool usage scenario, as shown in Figure 1. GPT-4o [16] can generate concise, refined, and accurate answers to scientific knowledge questions, whereas ChemLLM [17], domain-specific LLM for materials chemistry, produces long, verbose responses with numerous errors. For generating codes using materials simulation tools, the situation worsens: even GPT-4o fails to provide correct answers, and other domain-specific models are unable to generate functional Python code. This falls far short of the requirements in the scientific field; domain-specific models fail to outperform general-purpose LLMs. Figure 1: Three LLM responses to scientific knowledge questions and materials tool usage tasks (generating reasonable codes for using materials simulation tools). To address this issue, we first construct benchmarks for evaluating the ability of LLMs to use materials science tools. Previous scientific LLM benchmarks mainly focus on evaluating the reading comprehension [1821] and materials property prediction abilities of individual LLMs [2224], while neglecting the ability to generate code for performing physically meaningful calculations. In this paper, we present MatTools, comprehensive benchmark specifically designed to evaluate LLM capabilities in materials science tool utilization. MatTools consists of two complementary components: (1) materials simulation tool QA benchmark with 69,225 QA pairs derived from the pymatgen [25] (a widely adopted, robust open-source Python library for materials science that excels in data processing, analysis, and simulation) codebase and documentation and (2) real-world toolusage benchmark comprising 49 questions (138 tasks) that challenge LLMs to generate functional Python code for materials defect property calculations, constructed from unit test files from the pymatgen-analysis-defects [26] library. MatTools aims to overcome the challenges of current LLM benchmarks by introducing the following design choices. Automated data synthesis: Automatically generate real-world tool-usage benchmarks using unit test files, without manual data collection or materials science expert annotation. Comprehensive dual-benchmark design: MatTools includes both large-scale materials simulation tool QA benchmark and real-world tool-usage benchmark, enabling evaluation of both knowledge comprehension and practical tool usage abilities. We test both the performance of individual LLMs and the LLM-RAG agent systems. Secure and standardized evaluation: We employ Docker [27] sandbox to safely execute LLM-generated code, ensuring security and standardization. We design multi-level testing frameworks based on our benchmark to systematically evaluate LLM performance in materials science tool utilization. Our experimental results yield three key insights: Generalists outshine specialists: General-purpose LLMs (such as GPT-4o and Qwen2.5 series [28]) significantly outperform domain-specific materials science LLMs in knowledge QA tasks (80% vs. <32% accuracy for general-purpose vs. domain-specific LLMs). AI knows AI: Using LLM-generated documentation as the retrieval source in retrievalaugmented generation (RAG) systems substantially improves performance compared to using the original codebase and/or official documentation (e.g., the ability to generate runnable code and the task success rate increased by 47.8% and 115.7% over GPT-4o alone). Simpler is better: Our self-reflection LLM-doc RAG agent system (leveraging only LLMgenerated documentation and incorporating multi-round reflection) outperforms more complex approaches such as agentic RAG (with task decomposition, NER, and reranking) and the SOTA GraphRAG method LightRAG [29]; our method yields improvements of 58.8% and 149% in task success rate compared with the agentic RAG method and the LightRAG. Remarkably, even the single LLM+RAG system outperforms the agentic RAG and LightRAG by 13.7% and 78.3% in task success rate. These findings highlight the current limitations of domain-specific LLMs and the effectiveness of leveraging LLM-generated documentation and self-reflection for enhancing LLM tool-use abilities."
        },
        {
            "title": "2 MatTools",
            "content": "This section presents the development of MatTools, focusing on two key benchmarks: materials simulation tool QA benchmark (2.1) and real-world tool-usage benchmark (2.2). The materials simulation tool QA benchmark aims to evaluate the knowledge and comprehension of LLMs in materials science, while the real-world tool-usage benchmark assesses the capabilities of LLMs for using these tools for code generation. For each benchmark, we detail our methodology including data collection, data synthesis, and the design of testing frameworks to evaluate the capabilities of LLMs in materials science tool usage (see Figure 2). 2.1 Materials simulation tool QA benchmark Data collection We selected pymatgen as our primary benchmark data source. We leveraged Repoagent [31] to process pymatgen using the following steps: (1) Repository parsing: Repoagent automatically analyzes the codebase, constructing hierarchical project tree with the repository as the root node and directories/Python files as intermediate nodes; (2) Structure extraction: classes and functions were integrated as leaf nodes under their respective Python files, while caller-callee relationships were captured to form directed acyclic graph (DAG); (3) Documentation generation: documentation of each code segment was generated using Gemini-2.0-flash [32] with specialized RepoAgent prompts (Appendix A.1.1); (4) Dataset creation: two datasetspymatgen_code and pymatgen_docwere constructed, each comprising 7,192 datapoints extracted from code segments and their corresponding documentation, respectively (Appendix A.1.1). Benchmark data synthesis Two types of prompts were designed to generate QA pairs from the pymatgen_code and pymatgen_doc datasets (prompt templates in Appendix A.1.2). We instructed Gemini-2.0-flash to generate up to 5 distinct questions for each datapoint (code segment or documentation) with fewer questions when the datapoint content was insufficient to support 5 meaningful questions. Each generated question includes the question and four answer options (A, B, C, and D), requiring the LLMs to respond with only A, B, C, or D. This yielded two QA benchmarks: pymatgen_code_qa with 34,621 QA pairs and pymatgen_doc_qa with 34,604 QA pairs (see Appendix A.1.3). Testing framework design To systematically evaluate general LLMs for materials simulation tool comprehension and the scaling between performance and LLM size, we benchmarked 9 general LLMs (3 widely-used closed-source models and 6 Qwen2.5 open-source models with different parameter sizes). Recently, materials chemistry-focused LLMs demonstrated excellent performance Figure 2: Overview of MatTools. The upper half of the schematic represents the pipeline for constructing and applying the QA benchmark, where RepoAgent extracts code snippets from pymatgen and Gemini-2.0-flash generates documentation. Gemini (2.0-flash) creates pymatgen_code_qa and pymatgen_doc_qa benchmarks using the code snippets and documentation. For testing, LLMs answer questions by selecting one of the four options (A, B, C, or D). The lower half of the schematic illustrates the real-world tool-usage benchmark, where Tree-sitter [30] extracts test functions from unit test files and GPT-4o generates question-property-validation triples. Code generated by different LLM-based systems is verified in secure code sandbox. in understanding the materials science literature and in property prediction. To assess whether these domain-specific LLMs are proficient in materials simulation tool knowledge and instructionfollowing ability, we tested 3 materials chemistry LLMs (see Appendix A.1.4). We evaluated model performance accuracy (proportion of questions answered correctly) to compare understanding capabilities of different models on materials simulation tools. 2.2 Real-world tool-usage benchmark Benchmark data synthesis Examples of real-world material simulation tool usage are rare. Hence, we designed an automated process using LLMs to transform unit test code into triplets of: (1) problem statement (prompts LLMs to generate Python code for calculating material properties and returning dictionary of material properties), (2) dictionary of expected material properties to be calculated (the keys are material property names and the calculated results/values plus data types for verification), and (3) verification code to test the results from (2). We chose unit test code as the source because it contains three essential components: the problem to be solved, the implementation of the solution, and result verification. This automated pipeline enables the rapid generation of tool usage datasets (without constraint to specific LLMs) and facilitates benchmarking across models. We selected unit tests from the pymatgen-analysis-defects library to generate the triplets. This standalone pymatgen plugin is designed to analyze defects in materials (important material properties are controlled by the defects in materials). We first split the unit test files into unit test functions, then generated triplets for each function using GPT-4o [16]. Then, two materials science PhD students reviewed and revised errors in the generated triplets. (See Appendix A.2.1 for triplet generation prompts and generated triplet examples.) We generated 49 questions (138 tasks, where the number of tasks refers to the total number of properties to be calculated) for real-world tool-usage benchmark. Docker sandbox for result checking We designed Docker sandbox for testing LLM-generated code for safe code execution without affecting the local environment. The sandbox supports (1) running the LLM-generated code and returning execution results (material property dictionary) and 4 (2) running the verification code and returning verification results (the code returns ok if the results are correct and, if not, an error list). Figure 3: Illustration of the five benchmarked LLM-based systems for real-world tool-usage. Testing framework design We designed testing framework (utilizing the synthesized benchmark data and the Docker sandbox) to evaluate these 5 approaches. The process involves feeding problem statement from the generated triplets to each LLM-based system, which then attempts to generate the required Python code to calculate material properties. The generated code is executed within the Docker sandbox to obtain the calculated material properties dictionary. Subsequently, the verification code is executed in the Docker sandbox (with the obtained material properties dictionary as input) to verify the correctness of the results. For real-world materials simulation tool usage, we employ simple LLMs and agent systems to address complex code generation tasks. We designed and tested five distinct LLM-based systems (see Figure 3): (1) single LLM, (2) single RAG agent with pymatgen source code or documentation retrieval, (3) an agentic RAG system with multiple agents like task decomposition, NER and reranking, (4) GraphRAG agent system (here we use the state-of-the-art method LightRAG) leveraging structured knowledge representations, and (5) our self-reflection LLM-doc RAG agent system that incorporates LLM-generated documentation retrieval and iterative refinement (see Appendix A.2.2). For each, we analyzed the number of runnable functions (total 49) and successful tasks (total 138) by verifying the generated codes through our Docker sandbox."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Materials simulation tool QA benchmark results Table 1 shows the benchmark results of various LLMs. These results clearly demonstrate that general-purpose LLMsboth closed-source and open-sourcesignificantly outperform domainspecific materials chemistry LLMs in understanding and reasoning about materials simulation tool knowledge. Leading general models (Gemini-1.5-Pro, Qwen2.5-32B-Instruct and Qwen2.5-72BInstruct) achieve over 80% accuracy on both code and document QA tests, while specialized materials chemistry models (ChemDFM-v1.5-8B, ChemLLM-7B-Chat-1_5-DPO, and Darwin 1.5-7B) perform substantially worse, with accuracies of 30% (in one case, 0). The low performance of ChemLLM7B-Chat-1_5-DPO and Darwin 1.5-7B is associated with their poor instruction-following capability, leading to generating answers that are not formatted properly (i.e.,<answer>Option</answer>). Current general LLMs exhibit superior instruction-following, generalization capabilities and broader 5 knowledge coverage for materials simulation tools compared to domain-specific models. The overall performance of open-source LLMs (e.g., the Qwen 2.5 series) improves with increasing model size. Overall, these results highlight the clear advantages of general-purpose LLMs in materials simulation tool knowledge QA tasks. Based on this, we focus exclusively on general-purpose LLMs in the following testing based on the real-world tool-usage benchmark. Table 1: Performance of different LLMs on code and document QA benchmarks. Model Closed-source LLMs Gemini-1.5-Flash Gemini-1.5-Pro Gemini-2.0-Flash Open-source LLMs Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-Coder-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-Coder-32B-Instruct Qwen2.5-72B-Instruct Materials chemistry LLMs Accuracy (%) Code Doc 75.59 80.60 73.85 76.37 82.90 82.76 74.37 76.72 77.44 80.03 79.18 81.36 75.79 80.26 79.44 82.01 79.85 81. ChemDFM-v1.5-8B ChemLLM-7B-Chat-1_5-DPO Darwin 1.5-7B 32.35 0.18 0.01 30.20 0.13 0.01 Figure 4: Comparison of the performance of different LLMs on the real-world tool-usage benchmark. Error bars indicate standard deviation across three independent experiments. The displayed values represent the mean performance metrics from these trials. 3.2 Real-world tool-usage benchmark results To assess LLM performance on the real-world tool-usage benchmark, we designed three types of tests. The first involves directly querying the LLM with questions from the real-world tool-usage benchmark. We found that the function runnable rate and task success rate were both very low (<50%). Next, we examined if the RAG method improves LLM performance. Testing of four different retrieval 6 sources (lower panel of Figure 2) demonstrated that using the LLM-generated document as the RAG retrieval source yielded the best results; therefore, we designed simple agent system using this RAG retrieval source. The system generates reflective results based on the execution of each round of generated code (see 2.2), then iterates to generate the next round of code. The system, showed up to 149% improvement over the SOTA GraphRAG method (LightRAG), 58.8% improvement over the agentic RAG system with task decomposition, NER and reranking, and 201% improvement over the GPT-4o model (see Appendix A.2.3 for more details and examples). Figure 5: Comparative performance analysis of single RAG agent using different LLMs and retrieval sources on the real-world tool-usage benchmark. Retrieval sources include: (1) pymatgen codebase, (2) pymatgen official document split by recursively looking at characters, (3) LLMgenerated document split based on semantic similarity, and (4) LLM-generated document split based on function and class. Error bars indicate standard deviation across three independent experimental runs; displayed values represent mean performance metrics from these trials. Results of testing single LLM system Figure 4 compares the performance of different LLMs on real-world tool-usage benchmark. GPT-3.5 achieves function runnable rate of only 20.41% and task success rate of 3.62%. Even the top-performing model GPT-4o achieves function runnable rate of only 45.58% and task success rate of 18.36% . The reasoning model, Gemini-2.0-flash-thinkingexp-01-21, achieves the highest task success rate (25.63%), but function runnable rate of only 42.86%. All tested models demonstrate relatively low runnable function rates and task success rates, indicating that current mainstream LLMs, even reasoning models, struggle to effectively complete materials science tool usage tasks. The low function runnable rates suggest that codes generated by LLMs are often not executable without modification, while the low task success rates demonstrate 7 that even when the code runs successfully it is unreliable. To address these two challenges, we tested the RAG method in the next section to enhance LLM materials science tool usage capabilities. Results of testing single RAG agent with different retrieval sources Figure 5 compares the performance of single RAG agent using different LLMs and retrieval sources on the real-world tool-usage benchmark. Among the four retrieval sources, the LLM-generated document split based on function and class for retrieval content yielded the best performance for the RAG agent. GPT-4o with the LLM-generated document split based on function and class achieved the highest function runnable (67.35%) and task success (39.61%) rates; this is an improvement of 47.8% and 115.7% respectively compared to GPT-4o alone and 19.3% and 67.3% compared to GPT-4o with the official document. This indicates that LLM-generated information for the RAG leads to improved content retrieval and improved overall performance. Figure 6: Comparative performance analysis of advanced RAG agent systems on the real-world tool-usage benchmark. All systems used GPT-4o as the base model to generate code. Results of testing advanced RAG agents Based on these results, we design simple agent system with LLM-generated document split based on function and class as the retrieval source and apply the reflection method to provide LLM feedback on the generated code. Figure 6 compares the performance of our self-reflection LLM-doc RAG agent system with other mainstream RAG agent systems on the real-world tool-usage benchmark (we use GPT-4o in the single RAG agent system as the base model for all advanced RAG agent systems). Our self-reflection LLM-doc RAG agent system led to 26.3% improvement in function runnable rate and 39.6% improvement in task success rate, compared to the results without self-reflection. It is interesting to note that the agentic RAG system with task decomposition, NER and reranking achieved task success rate lower than that from GPT-4o with LLM-doc RAG. The GraphRAG method (LightRAG) performed even worse than the agentic RAG system. This suggests that LLMs utilizing only LLM-generated documentation as the retrieval source, combined with self-reflection, outperform mainstream approaches on materials science tool usage tasks (even though LightRAG and agentic RAG approaches typically perform better in other application domains). Compared to the single LLM only using GPT-4o, our self-reflection LLM-doc RAG system demonstrated significant improvements (86.6%) in function runnable rate and task success rate (201.3%) compared with GPT-4o alone."
        },
        {
            "title": "4 Conclusion",
            "content": "We introduced MatTools, benchmark for evaluating LLM capabilities in materials science tool usage. Our experiments, using two complementary benchmarks (a materials simulation tool QA benchmark and real-world tool-usage benchmark) led to three key insights. First, general-purpose LLMs outperform domain-specific materials science models in understanding simulation tools, with top 8 models achieving over 80% accuracy, compared to specialized models 32% or lower. Second, LLMgenerated documentation outperforms both codebases and human-written official documentation as retrieval source for RAG systems, improving function runnable rates to 67.35% and task success rates to 39.61% with GPT-4oan improvement of 47.8% and 115.7% compared to using GPT-4o alone (45.58% and 18.36%). Using LLM-generated documentation in the RAG outperforms the same system with the official documentation-based RAG by 19.3% and 67.3%. Third, our self-reflection LLM-doc RAG agent system led to task success rate that surpassed that using agentic RAG and LightRAG by 58.8% and 149%, respectively. Even the single RAG agent with GPT-4o and LLMgenerated document as the retrieval source led to task success rate improvement of 13.7% and 78.3% compared with agentic RAG and LightRAG. Our self-reflection LLM-doc RAG system showed 201.3% improvement over GPT-4o in task success rate. These findings highlight the current state and limitations of LLMs in materials science applications while providing standardized framework for future improvements. MatTools offers valuable resource for researchers to systematically evaluate and enhance LLM capabilities for scientific tool applications, potentially accelerating materials discovery and design through more effective and intelligent AI-assisted computational approaches."
        },
        {
            "title": "5.1 Materials science LLM benchmarks",
            "content": "There are many benchmarks for evaluating the understanding of scientific knowledge in LLMs, such as MaScQA [18], SciEval [19], SciAssess [20], SciQAG [21] and SciFIBench [33]. These benchmarks generally use QA or multiple-choice formats to evaluate the ability of LLMs to understand scientific literature or images, in uni-modal or multi-modal context. Most of these benchmarks are general and treat materials science as subfield, with limited number of benchmark tasks. There are additional benchmarks for evaluating LLM performance for materials property prediction (e.g., LLM4Mat-Bench [22], MatText [34] and those from Joren et al. [23] and Li et al. [24]). Compared with these benchmarks, we focus on applications that combine LLMs with physically-based materials science tools and design benchmark that evaluates the capabilities of LLMs to work with existing materials science tools as well as the performance of such combined LLM/materials science tool systems. In this way, our goal and benchmark are unique. 5.2 Evaluation of LLM coding performance for materials science SciCode [35] established benchmark for code generation across multiple scientific disciplines, encompassing mathematics, physics, chemistry, biology, and materials science. This benchmark consists of 80 primary problems further subdivided into 338 sub-problems; materials science represents only 16% of these tasks. HoneyComb [36] transformed scientific knowledge QA pairs into code completion questions (similar to Toolformer [37]) and demonstrated improved LLM performance on the MaScQA dataset. This approach does not assess real-world tool-usage capabilities (the focus of our work). Shi et al. [38] developed the LAMMPS-Expert Question with Score Dataset (LEQS-Dataset), which benchmarks scripts that run widely-used molecular dynamics simulation package (LAMMPS [39]). However, this benchmark is limited, since it only considers defined set of performance metrics for LAMMPS scripts for single tasks (e.g., grammatical errors, logical errors, and parameter errors) based on human-curated datasets. Our work, on the other hand, focuses on materials science tool-usage capabilities and employs multi-level testing frameworks to evaluate the performance of LLMs in materials science tool usage."
        },
        {
            "title": "References",
            "content": "[1] John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S. Rosen, Gerbrand Ceder, Kristin A. Persson, and Anubhav Jain. Structured information extraction from scientific text with large language models. Nature Communications, 15:1418, 2024. https://doi.org/10.1038/s41467-024-45563-x. [2] Yang Jeong Park, Sung Eun Jerng, Sungroh Yoon, and Ju Li. 1.5 million materials narratives generated by chatbots. Scientific Data, 11:1060, 2024. https://www.nature.com/articles/s41597024-03886-w. [3] Maciej Polak and Dane Morgan. Extracting accurate materials data from research papers with conversational language models and prompt engineering. Nature Communications, 15:1569, 2024. https://www.nature.com/articles/s41467-024-45914-8. [4] Kamal Choudhary. AtomGPT: Atomistic generative pretrained transformer for forJ. Phys. Chem. Lett., 15:69096917, 2024. ward and inverse materials design. https://pubs.acs.org/doi/10.1021/acs.jpclett.4c01126. [5] Kishalay Das, Pawan Goyal, Seung-Cheol Lee, Satadeep Bhattacharjee, and Niloy representation for crystal property predicIntelligence, 2023. In The 39th Conference on Uncertainty in Artificial Ganguly. tion. https://dl.acm.org/doi/10.5555/3625834.3625882. Crysmmnet: Multimodal [6] Siyu Liu, Tongqi Wen, Beilin Ye, Zhuoyuan Li, and David J. Srolovitz. Large language models for material property predictions: elastic constant tensor prediction and materials design, 2024. https://arxiv.org/abs/2411.12280. [7] Shaohan Tian, Xue Jiang, Weiren Wang, Zhihua Jing, Chi Zhang, Cheng Zhang, Turab Lookman, and Yanjing Su. Steel design based on large language model. Acta Materialia, 285:120663, 2025. https://www.sciencedirect.com/science/article/pii/S1359645424010115. [8] Bo Ni, Benjamin Glaser, and S. Mohadeseh Taheri-Mousavi. Alloygpt: End-to-end prediction and design of additively manufacturable alloys using an autoregressive language model, 2025. https://www.researchsquare.com/article/rs-6067058/v1. [9] Bo Hu, Siyu Liu, Beilin Ye, Yun Hao, and Tongqi Wen. multi-agent framework for materials laws discovery, 2024. https://arxiv.org/abs/2411.16416. [10] Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, and Dongxiao Zhang. Large language models for automatic equation discovery of nonlinear dynamics. Physics of Fluids, 36:097121, 2024. https://pubs.aip.org/aip/pof/article/36/9/097121/3312108/Large-languagemodels-for-automatic-equation. [11] Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses, 2024. https://arxiv.org/abs/2410.07076. [12] Atilla Kaan Alkan, Shashwat Sourav, Maja Jablonska, Simone Astarita, Rishabh Chakrabarty, Nikhil Garuda, Pranav Khetarpal, Maciej Pióro, Dimitrios Tanoglidis, Kartheik G. Iyer, Mugdha S. Polimera, Michael J. Smith, Tirthankar Ghosal, Marc Huertas-Company, Sandor Kruk, Kevin Schawinski, and Ioana Ciuca. survey on hypothesis generation for scientific discovery in the era of large language models, 2025. https://arxiv.org/abs/2504.05496. [13] Alireza Ghafarollahi and Markus Buehler. Automating alloy design and discovery with physics-aware multimodal multiagent ai. Proceedings of the National Academy of Sciences, 122:e2414074122, 2025. https://doi.org/10.1073/pnas.2414074122. [14] Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6:525535, 2024. https://doi.org/10.1038/s42256-024-00832-8. 10 [15] Mehrad Ansari and Seyed Mohamad Moosavi. als datasets from the scientific literature. http://dx.doi.org/10.1039/D4DD00252K. Agent-based learning of materiDigital Discovery, 3:26072617, 2024. [16] OpenAI. Hello gpt-4o, 2024. https://openai.com/index/hello-gpt-4o/. [17] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Wanli Ouyang, Dongzhan Zhou, Shufei Zhang, Mao Su, Han-Sen Zhong, and Yuqiang Li. Chemllm: chemical large language model, 2024. https://arxiv.org/abs/2402.06852. [18] Mohd Zaki, Jayadeva, Mausam, and N. M. Anoop Krishnan. Mascqa: question answering dataset for investigating materials science knowledge of large language models, 2023. https://arxiv.org/abs/2308.09115. [19] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. Scieval: multi-level large language model evaluation benchmark for scientific research. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, 2024. https://doi.org/10.1609/aaai.v38i17.29872. [20] Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Wang Changxin, Zhifeng Gao, Hongshuai Wang, Li Yongge, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Xi Fang, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, and Guolin Ke. SciAssess: Benchmarking LLM proficiency in scientific literature analysis. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 23352357, 2025. https://aclanthology.org/2025.findings-naacl.125/. [21] Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster. Sciqag: framework for auto-generated science question answering dataset with fine-grained evaluation, 2024. https://arxiv.org/abs/2405.09939. [22] Andre Niyongabo Rubungo, Kangming Li, Jason Hattrick-Simpers, and Adji Bousso Dieng. Llm4mat-bench: Benchmarking large language models for materials property prediction. Machine Learning: Science and Technology, 2025. http://iopscience.iop.org/article/10.1088/26322153/add3bb. [23] Joren Van Herck, María Victoria Gil, Kevin Maik Jablonka, Alex Abrudan, Andy S. Anker, Mehrdad Asgari, Ben Blaiszik, Antonio Buffo, Leander Choudhury, Clemence Corminboeuf, Hilal Daglar, Amir Mohammad Elahi, Ian T. Foster, Susana Garcia, Matthew Garvin, Guillaume Godin, Lydia L. Good, Jianan Gu, Noémie Xiao Hu, Xin Jin, Tanja Junkers, Seda Keskin, Tuomas P. J. Knowles, Ruben Laplaza, Michele Lessona, Sauradeep Majumdar, Hossein Mashhadimoslem, Ruaraidh D. McIntosh, Seyed Mohamad Moosavi, Beatriz Mouriño, Francesca Nerli, Covadonga Pevida, Neda Poudineh, Mahyar Rajabi-Kochi, Kadi L. Saar, Fahimeh Hooriabad Saboor, Morteza Sagharichiha, K. J. Schmidt, Jiale Shi, Elena Simone, Dennis Svatunek, Marco Taddei, Igor Tetko, Domonkos Tolnai, Sahar Vahdatifar, Jonathan Whitmer, D. C. Florian Wieland, Regine Willumeit-Römer, Andreas Züttel, and Berend Smit. Assessment of fine-tuned large language models for real-world chemistry and material science applications. Chem. Sci., 16:670684, 2025. http://dx.doi.org/10.1039/D4SC04401K. [24] Kangming Li, Andre Niyongabo Rubungo, Xiangyun Lei, Daniel Persaud, Kamal Choudhary, Brian DeCost, Adji Bousso Dieng, and Jason Hattrick-Simpers. Probing out-of-distribution generalization in machine learning for materials. Communications Materials, 6:9, 2025. [25] Shyue Ping Ong, William Davidson Richards, Anubhav Jain, Geoffroy Hautier, Michael Kocher, Shreyas Cholia, Dan Gunter, Vincent L. Chevrier, Kristin A. Persson, and GerPython materials genomics (pymatgen): robust, open-source python brand Ceder. Computational Materials Science, 68:314319, 2013. library for materials analysis. https://www.sciencedirect.com/science/article/pii/S0927025612006295. [26] Jimmy-Xuan Shen. pymatgen-analysis-defects, 2024. https://github.com/materialsproject/pymatgen-analysis-defects. 11 [27] Dirk Merkel. Docker: lightweight linux containers for consistent development and deployment. Linux J., 2014, 2014. https://dl.acm.org/doi/10.5555/2600239.2600241. [28] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. https://arxiv.org/abs/2412.15115. [29] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-augmented generation, 2024. https://arxiv.org/abs/2410.05779. [30] tree sitter. Tree-sitter, 2024. https://github.com/tree-sitter/tree-sitter. [31] Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun. RepoAgent: An LLM-powered open-source framework for repository-level code documentation generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 436464, 2024. https://aclanthology.org/2024.emnlp-demo.46/. [32] Google. Gemini 2.0 flash, 2025. https://cloud.google.com/vertex-ai/generativeai/docs/models/gemini/2-0-flash. [33] Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. Scifibench: Benchmarking large multimodal models for scientific figure interpretation, 2024. https://arxiv.org/abs/2405.08807. [34] Nawaf Alampara, Santiago Miret, and Kevin Maik Jablonka. Mattext: Do language models need more than text & scale for materials modeling?, 2024. https://arxiv.org/abs/2406.17295. [35] Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: research coding benchmark curated by scientists, 2024. https://arxiv.org/abs/2407.13168. [36] Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, and Bang Liu. Honeycomb: flexible llm-based agent system for materials science, 2024. https://arxiv.org/abs/2409.00135. [37] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. https://arxiv.org/abs/2302.04761. [38] Zhuofan Shi, Chunxiao Xin, Tong Huo, Yuntao Jiang, Bowen Wu, Xingyue Chen, Wei Qin, Xinjian Ma, Gang Huang, Zhenyu Wang, and Xiang Jing. fine-tuned large language model based molecular dynamics agent for code generation to obtain material thermodynamic parameters. Scientific Reports, 15, 2025. https://www.nature.com/articles/s41598-025-92337-6#article-info. [39] A. P. Thompson, H. M. Aktulga, R. Berger, D. S. Bolintineanu, W. M. Brown, P. S. Crozier, P. J. in Veld, A. Kohlmeyer, S. G. Moore, T. D. Nguyen, R. Shan, M. J. Stevens, J. Tranchida, C. Trott, and S. J. Plimpton. LAMMPS - flexible simulation tool for particle-based materials modeling at the atomic, meso, and continuum scales. Comp. Phys. Comm., 271:108171, 2022. https://doi.org/10.1016/j.cpc.2021.108171. [40] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. https://dl.acm.org/doi/10.1145/3600006.3613165. [41] Vasilios Mavroudis. Langchain: framework for building llm-based applications, 2024. https://hal.science/hal-04817573/."
        },
        {
            "title": "A Technical appendices",
            "content": "A.1 Details of materials simulation tool QA benchmark and results A.1.1 Method, prompt and generated document example of RepoAgent We used RepoAgent to extract the code segments and generate the documentation for the pymatgen software. We keep the same prompt template (see Figure 8) and settings as RepoAgent for the global structure analysis and documentation generation (see Figure 7). The model used to generate the documentation is Gemini-2.0-flash. The example of extracted code segment and generated documentation is shown in Figure 9. Figure 7: Three steps of RepoAgent: global structure analysis, documentation generation, and documentation update. This figure is from [31]. We used the first two steps of RepoAgent to extract the code segments and generate the documentation for the pymatgen software. 13 Figure 8: Prompt template of RepoAgent. This figure is from [31]. Figure 9: Example of extracted code segment and generated documentation for pymatgen. 14 A.1.2 QA benchmark prompt templates Figure 10: Prompt template for generating the QA pairs for pymatgen code. 15 Figure 11: Prompt template for generating the QA pairs of pymatgen documentation. 16 Figure 12: Prompt template for instructing the LLM to answer questions in the QA benchmark. A.1.3 Examples of generated QA pairs and testing results To extract answers from LLM responses, we implemented parsing mechanism that identifies content enclosed within <answer> and </answer> tags. This approach ensures standardized answer extraction and evaluation. If model failed to adhere to the required format (see Figure 12) in its response, we considered it as failed attempt and marked the answer as incorrect. extracted_answer = llm_answer.strip().replace(\"<answer>\", \"\") .replace(\"</answer>\", \"\").strip() Here are some examples (Figure 13 and 14) of generated QA pairs and testing results from different models. Figure 13: One example of generated QA pairs and testing results from different models. The Darwin 1.5-7B model failed to follow the required format in its response, so we considered it as failed attempt and marked the answer as incorrect. A.1.4 Model list and settings for materials simulation tool QA benchmark Model list Gemini-1.5-Flash: use API with model name gemini-1.5-flash. Gemini-1.5-Pro: use API with model name gemini-1.5-pro. Gemini-2.0-Flash: use API with model name gemini-2.0-flash. Qwen2.5-7B-Instruct: use local vllm [40] server and download the model from https: //huggingface.co/Qwen/Qwen2.5-7B-Instruct. 17 Figure 14: One example of generated QA pairs and testing results from different models. Both the closed-source and materials chemistry LLMs failed to output correct answers. The ChemDFM-v1.58B model even generated garbled characters and Chinese text. Qwen2.5-14B-Instruct: use local vllm server and download the model from https:// huggingface.co/Qwen/Qwen2.5-14B-Instruct. Qwen2.5-Coder-14B-Instruct: use local vllm server and download the model from https: //huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct. Qwen2.5-32B-Instruct: use local vllm server and download the model from https:// huggingface.co/Qwen/Qwen2.5-32B-Instruct. Qwen2.5-Coder-32B-Instruct: use local vllm server and download the model from https: //huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct. Qwen2.5-72B-Instruct: use local vllm server and download the model from https:// huggingface.co/Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4. Here we used the gptq version and int4 quantization. ChemDFM-v1.5-8B: use local vllm server and download the model from https:// huggingface.co/OpenDFM/ChemDFM-v1.5-8B. ChemLLM-7B-Chat-1_5-DPO: use local vllm server and download the model from https: //huggingface.co/AI4Chem/ChemLLM-7B-Chat-1_5-DPO. Due to the slow running speed and low instruction-following ability, we only test 6705 code QA pairs and 32006 documentation QA pairs. Darwin 1.5-7B: use local vllm server and download the model from https://github. com/MasterAI-EAM/Darwin. Settings The versions used in our codebase are 2024.8.9 for pymatgen and 2024.7.19 for pymatgenanalysis-defects. All tests were conducted on an NVIDIA A100 80GB GPU. For both QA generation and testing, we maintained consistent temperature setting of 0.7 across all models while keeping other parameters at their default values in the OpenAI API client. We verified all generated QA pairs for proper formatting and content completeness, ensuring each pair contained question, four answer choices, and the correct answer. For the Gemini series models, we utilized the Gemini API in conjunction with the OpenAI API client. For the Qwen series and materials chemistry models, we deployed vllm to emulate the OpenAI API client interface. 18 A.2 Details for real-world tool-usage benchmark and results A.2.1 Example prompt and generated triplet for the benchmark evaluation Figure 15: Prompt template for extracting the materials properties from the unit test code. Figure 16: Prompt template for proposing the problem statement from the unit test code. 19 Figure 17: Prompt template for generating the verification code for the problem statement. Figure 18: Example of the problem statement. 20 Figure 19: Example of the property dictionary. Figure 20: Example of the verification code. 21 Figure 21: Overview of the functions of pymatgen-analysis-defects [26] package. Benchmark scope derived from pymatgen-analysis-defects package Our benchmark is derived from the specific classes and functions of these APIs (Figure 21). A.2.2 Models, methods, and their configuration details Models and methods list GPT-3.5: use API with model name gpt-3.5-turbo-0125. GPT-4o-mini: use API with model name gpt-4o-mini-2024-07-18. GPT-4o: use API with model name gpt-4o-2024-08-06. Gemini-2.0-Flash: use API with model name gemini-2.0-flash. Gemini-2.0-Thinking: use API with model name gemini-2.0flash-thinking-exp-01-21. Retrieval sources 1: The implementation utilizes LangChain [41] RecursiveCharacterTextSplitter with chunk size 1000 and chunk overlap 50 to segment the pymatgen package codebase. For embedding, the system employs OpenAIs text-embedding-3-large model, with langchain-chroma serving as the vector store (these embedding and vector store settings remain consistent across all retrieval sources). Retrieval sources 2: This approach applies LangChains RecursiveCharacterTextSplitter with chunk size 1000 and chunk overlap 50 to divide the official pymatgen package documentation into chunks. Retrieval sources 3: This configuration leverages LangChains SemanticChunker to partition the documentation generated by Gemini-2.0-Flash into semantically coherent chunks. Retrieval sources 4: This method directly utilizes the unmodified documentation generated by Gemini-2.0-Flash as retrieval source, where each datapoint corresponds to documentation for specific function or class. LightRAG: use the default configuration of LightRAG source code to generate the knowledge graph based on the pymatgen package codebase, answer questions with the gpt-4o-2024-08-06 model. the system employs OpenAIs text-embedding-3-large model. For embedding, 22 Agentic RAG: and text-embedding-3-large model for embedding. The retrieval source is the same as the retrieval source 4. gpt-4o-2024-08-06 model questions answer use to Our proposed method: use gpt-4o-2024-08-06 model to answer questions and text-embedding-3-large model for embedding. The retrieval source is the same as the retrieval source 4. Configuration details All models and methods use API to get responses with the same temperature setting of 0.7 (except for LightRAG, which uses its own temperature setting). For RAG retrieval, we set the top-k value to 5 (except for LightRAG as it uses knowledge graph). A.2.3 Detailed results for the real-world tool-usage benchmark Table 2: Performance comparison of different LLMs on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific model."
        },
        {
            "title": "Models",
            "content": "gpt-3.5-turbo-0125 gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gemini-2.0-flash gemini-2.0-flash-thinking-exp-01-"
        },
        {
            "title": "Absolute Performance",
            "content": "Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%) Functions Tasks 6 3 6 17 19 25 26 25 15 20 18 36 39 30 22.45 18.37 20.41 42.86 44.9 46.94 46.94 53.06 36. 20.41 18.37 18.37 44.9 40.82 42.86 4.35 2.17 4.35 12.32 13.77 17.39 18.12 18.84 18.12 10.87 14.49 13. 26.9 28.26 21.74 11 9 10 21 22 23 23 26 18 10 9 9 22 20 23 Table 3: Performance comparison of different LLMs with retrieval source 1 on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific model."
        },
        {
            "title": "Models",
            "content": "gpt-3.5-turbo-0125 gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gemini-2.0-flash gemini-2.0-flash-thinking-exp-01-"
        },
        {
            "title": "Absolute Performance",
            "content": "Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%)"
        },
        {
            "title": "Functions Tasks",
            "content": "8 6 9 21 17 22 34 28 28 27 24 27 25 12 25 11 5 20 17 22 46 29 31 41 33 43 30 26 40 16.33 12.24 18.37 42.86 34.69 44. 69.39 57.14 57.14 55.10 48.98 55.10 51.02 44.90 51.02 7.97 3.62 5.80 14.49 12.32 15.94 33.33 21.01 22. 29.71 23.91 31.16 21.74 18.84 28.99 Table 4: Performance comparison of different LLMs with retrieval source 2 on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific model. Models gpt-3.5-turbo-0125 gpt-4o-mini-2024-07gpt-4o-2024-08-06 gemini-2.0-flash gemini-2.0-flash-thinking-exp-01-21 Absolute Performance Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%) Functions Tasks 6 3 8 24 15 23 42 32 24 45 51 26 48 48 22.45 18.37 20.41 42.86 38.78 40.82 63.27 53.06 53.06 48.98 53.06 57.14 38.78 48.98 48. 4.35 2.17 5.80 17.39 10.87 16.67 30.43 23.19 17.39 32.61 36.96 28.99 18.84 34.78 34.78 11 9 21 19 20 31 26 26 24 26 28 19 24 24 24 Table 5: Performance comparison of different LLMs with retrieval source 3 on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific model."
        },
        {
            "title": "Models",
            "content": "gpt-3.5-turbo-0125 gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gemini-2.0-flash gemini-2.0-flash-thinking-exp-01-"
        },
        {
            "title": "Absolute Performance",
            "content": "Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%)"
        },
        {
            "title": "Functions Tasks",
            "content": "10 9 9 26 28 23 32 31 35 27 24 24 28 28 27 8 11 36 31 33 47 50 49 52 44 48 45 40 47 20.41 18.37 18.37 53.06 57.14 46. 65.31 63.27 71.43 55.10 48.98 48.98 57.14 57.14 55.10 5.80 7.97 6.52 26.09 22.46 23.91 34.06 36.23 35. 37.68 31.88 34.78 32.61 28.99 34.06 Table 6: Performance comparison of different LLMs with retrieval source 4 on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific model. Models gpt-3.5-turbo-0125 gpt-4o-mini-2024-07gpt-4o-2024-08-06 gemini-2.0-flash gemini-2.0-flash-thinking-exp-01-21 Absolute Performance Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%) Functions Tasks 13 8 25 47 38 30 54 57 53 51 41 56 49 55 20.41 22.45 34.69 42.86 51.02 48.98 59.18 73.47 69.39 65.31 57.14 59.18 55.10 57.14 59. 9.42 5.80 18.12 34.06 27.54 21.74 39.13 41.30 38.41 36.96 29.71 33.33 40.58 35.51 39.86 10 11 21 25 24 29 36 34 32 28 29 27 28 29 25 Table 7: Performance comparison of different RAG agent systems on our real-world tool-usage benchmark. Each row represents an independent evaluation run of specific method."
        },
        {
            "title": "Absolute Performance",
            "content": "Runnable Functions (out of 49) Successful Tasks (out of 138) Success Rate (%)"
        },
        {
            "title": "Functions Tasks",
            "content": "21 29 27 32 33 35 42 42 41 29 27 36 37 55 47 70 84 42.86 59.18 55.10 65.31 67.35 71.43 85.71 85.71 83.67 21.01 19.57 26.09 30.61 39.86 34.06 50.72 60.87 54. Figure 22: Example of the wrong result for the problem statement shown in Figure 18. A.3 Limitations Despite the strengths of our automated pipeline, several limitations exist. First, real-world toolusage tasks still require expert review to ensure scientific validity, as LLMs may generate physical descriptions that do not fully align with human conventions of expression. Second, the relatively small scale of our benchmark limits statistical robustness, though we mitigated this with multiple evaluation runs. Finally, the computational cost associated with evaluating large-scale models may restrict broader participation. Future work should expand the benchmark, improve physical validation automation, and develop more resource-efficient evaluation methods. A.4 Broader impacts Positive impacts This work constructs comprehensive, two-level benchmark for tool usage in materials science and systematically evaluates LLMs across multiple dimensions of tool use. The potential societal benefits include: Accelerating scientific discovery: By providing standardized and automated framework for evaluating and improving the ability of LLMs to use materials science tools, MatTools can accelerate the pace of materials discovery, design, and innovation, ultimately benefiting fields such as energy, electronics, and healthcare. Enabling fair and comprehensive benchmarking: By including both knowledge QA and real-world tool-usage tasks, MatTools enables more holistic and fair assessment of the scientific reasoning and practical coding abilities of LLMs, guiding the development of more robust and generalizable AI systems for scientific applications. Lowering entry barriers for non-experts: The automated pipeline for dataset generation and the use of LLM-generated documentation as retrieval source can empower non-expert users and smaller research groups to more easily access, understand, and utilize complex materials science tools. 26 Figure 23: Example of the correct result for the problem statement shown in the figure. Driving advances in AI for science: The insights derived from MatToolssuch as the superiority of generalist LLMs and the effectiveness of LLM-generated documentation and self-reflectioncan inform the development of next-generation AI systems for broader scientific applications beyond materials science. Negative impacts While MatTools advances LLM evaluation for scientific tool usage, potential negative impacts include computational resource inequalities between institutions, risks of errors in automatically generated tasks and accessibility limitations due to the focus on English-language and Python-based tools. Broad community engagement and responsible usage guidelines remain essential to mitigate these risks while ensuring benefits of the benchmark are widely distributed."
        }
    ],
    "affiliations": [
        "Center for Structural Materials, Department of Mechanical Engineering, The University of Hong Kong, Hong Kong SAR, China",
        "Materials Innovation Institute for Life Sciences and Energy (MILES), HKU-SIRI, Shenzhen, China"
    ]
}