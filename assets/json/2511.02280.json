{
    "paper_title": "SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning",
    "authors": [
        "Fangxun Shu",
        "Yongjie Ye",
        "Yue Liao",
        "Zijian Kang",
        "Weijie Yin",
        "Jiacong Wang",
        "Xiao Liang",
        "Shuicheng Yan",
        "Chao Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SAIL-RL, a reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with a dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as a principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 8 2 2 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "SAIL-RL: GUIDING MLLMS IN WHEN AND HOW TO THINK VIA DUAL-REWARD RL TUNING Fangxun Shu1 Yongjie Ye1 Yue Liao2 Zijian Kang1 Weijie Yin1 Xiao Liang1 Shuicheng Yan2 Chao Feng1 1Douyin SAIL Team 2National University of Singapore Jiacong Wang"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce SAIL-RL, reinforcement learning (RL) post-training framework that enhances the reasoning capabilities of multimodal large language models (MLLMs) by teaching them when and how to think. Existing approaches are limited by outcome-only supervision, which rewards correct answers without ensuring sound reasoning, and by uniform thinking strategies, which often lead to overthinking on simple tasks and underthinking on complex ones. SAIL-RL addresses these challenges with dual reward system: the Thinking Reward, which evaluates reasoning quality through factual grounding, logical coherence, and answer consistency, and the Judging Reward, which adaptively determines whether deep reasoning or direct answering is appropriate. Experiments on the state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieving competitive performance against commercial closed-source models such as GPT-4o, and substantially reduces hallucinations, establishing it as principled framework for building more reliable and adaptive MLLMs. The code will be available at https://github.com/BytedanceDouyinContent/SAIL-RL."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal Large Language Models (MLLMs) (Team et al., 2023; Hurst et al., 2024; Bai et al., 2025; Liu et al., 2023; Chen et al., 2024) are continuously advancing, evolving from elementary visual description toward complex reasoning and comprehensive visual understanding. major driver of this progression is the development of training paradigms and optimization strategies that determine how MLLMs acquire such capabilities from large-scale multimodal data. Supervised fine-tuning (SFT) with teacher-forcing laid the foundation for pre-training by aligning models with multimodal corpora. Building on this foundation, recent studies (Team et al., 2025a; Yang et al., 2025b) have increasingly emphasized post-training, where hybrid frameworks integrate SFT with reinforcement learning (RL). RL post-training has undergone paradigm shift: early approaches (Ouyang et al., 2022; Rafailov et al., 2023; Ethayarajh et al., 2023; Shu et al., 2024) primarily focused on aligning models with human preferences, whereas recent methods (Yang et al., 2025b; Chen et al., 2025a; Team et al., 2025b) emphasize step-by-step thinking and iterative self-improvement. This transition enables models to refine their cognitive processes more autonomously, making RL post-training particularly promising pathway for further advancing MLLM capabilities. RL in MLLMs (Deng et al., 2025; Chen et al., 2025a; Team et al., 2025b) commonly follows the paradigm of thinking before speaking. Guided by special token think, the model first generates structured reasoning trace before producing the final answer. Leveraging long reasoning chains as an internal knowledge source allows the model to extract salient cues that improve answer accuracy and strengthen overall capability. Nevertheless, despite these advances, current RL post-training methods still face several fundamental challenges: Answers without sound reasoning: Conventional methods rely on outcome-only supervision, where rewards are determined by the correctness of the final answer while the quality of reasoning is Equal Contribution Project Lead Corresponding Author"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Performance comparison between SAIL-VL2-Thinking (SAIL-VL2 post-trained with our SAIL-RL) and other LVMs. SAIL-VL2-Thinking achieves clear advantages on both general understanding and mathematical reasoning benchmarks, surpassing open-source baselines at the 8B scale and delivering competitive performance against large-scale closed-source models. disregarded. This paradigm introduces two critical issues: first, as the intuition think well to answer right suggests, incoherent or redundant reasoning traces hinder the model from extracting useful cues, leading to inaccurate answers and exacerbating hallucinations. As shown in Figure 2, conventional MLLMs (Team et al., 2025b) can produce correct answers despite factual errors in reasoning, highlighting how outcome-only rewards compromise robustness and trustworthiness. Second, during optimization, models may occasionally reach correct answers through flawed or fabricated reasoning paths. Such spurious alignments are nevertheless reinforced as positive outcomes, fostering form of false correctness that undermines both robustness and reliability. Overthinking the easy, underthinking the hard: Most approaches apply the same reasoning process to all tasks, regardless of complexity. This uniformity often leads to overthinking on simple problems, introducing unnecessary cost and noisy reasoning chains. As illustrated in Figure 2, models frequently generate redundant reasoning for trivial queries (e.g., object color recognition), highlighting the inefficiency of static strategies. Conversely, on complex problems, the same rigidity causes underthinking, producing shallow reasoning and inaccurate answers. The lack of adaptive control prevents models from allocating cognitive resources efficiently, unlike humans who naturally adjust their effort based on task difficulty. To address these challenges, we propose SAIL-RL, novel RL-based post-training framework for MLLMs. SAIL-RL follows the standard two-stage paradigm, consisting of CoTaugmented SFT stage and an RL-tuning stage, but introduces dual reward system that explicitly supervises both the quality of reasoning and the adaptivity of thinking strategies. The Thinking Reward moves beyond outcome-only supervision by directly assessing the reasoning process. It evaluates factual grounding to penalize hallucinations, logical coherence to ensure consistency across steps, and answer consistency to guarantee that outputs are faithfully derived from the reasoning trace. This process-level supervision explicitly couples sound reasoning with correct answers. The Judging Reward enhances adaptivity by enabling models to decide when deep reasoning is necessary. The"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Limitations of current MLLMs in reasoning. Left: Lucky success where the model reaches the correct answer through flawed reasoning process. Right: Overthinking where the model applies needlessly complex reasoning process to simple problem, resulting in an incorrect answer. model learns to adopt direct-answer mode for simple tasks and full reasoning mode for complex ones, improving efficiency while aligning cognitive resource allocation more closely with human behavior. Together, these two reward systems allow SAIL-RL to strengthen both the reliability and efficiency of MLLMs in reasoning and comprehensive tasks. We conduct extensive experiments to evaluate the effectiveness of SAIL-RL. Building on the stateof-the-art MLLM SAIL-VL2, we develop SAIL-VL2-Thinking through our RL-based post-training strategy. As shown in Fig. 1, with the dual reward system, SAIL-VL2-Thinking delivers consistent gains over the baseline and conventional RL-based approaches, achieving state-of-the-art results on multiple reasoning benchmarks at 8B scales. It also reaches leading performance on OpenCompass, maintains competitive accuracy on general multimodal understanding tasks, and substantially reduces hallucinations, highlighting the robustness and reliability introduced by SAIL-RL. Together, these contributions establish SAIL-RL as principled post-training framework that strengthens both the quality and adaptivity of reasoning in MLLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 MULTIMODAL LARGE LANGUAGE MODELS. Recent Multimodal Large Language Models (MLLMs) have demonstrated strong visual understanding capabilities through supervised fine-tuning (Liu et al., 2023; Chen et al., 2024; Yin et al., 2025). Following these successes, the research focus is now shifting towards enhancing performance on more complex reasoning tasks, such as mathematics and intricate visual analysis (Lu et al., 2023; Zhang et al., 2024; Wu & Xie, 2024). While these models show potential, unlocking their full reasoning abilities often requires more advanced methods than instruction tuning alone. To this end, Reinforcement Learning (RL) has emerged as promising approach to significantly advance reasoning capabilities of MLLMs. 2.2 REINFORCEMENT LEARNING. The successful application of reinforcement learning to text-based reasoning in models like OpenAIo1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025) offers compelling blueprint for enhancing Multimodal Large Language Models (MLLMs). common paradigm in these approaches, as well as in recent multimodal reasoning efforts (Chen et al., 2025a; Team et al., 2025b; Deng et al., 2025; Wang et al., 2025), is the reliance on outcome-based supervision, where the reward is contingent on the correctness of the final answer. While this method has achieved impressive results, it provides limited direct feedback on the quality of the intermediate reasoning steps. This highlights critical research gap: an over-reliance on final outcomes may not be sufficient to cultivate robust reasoning processes, suggesting that more holistic reward system is necessary next step for the field."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "In this section, we review the reasoning-incentivized MLLM post-training pipeline of existing RLtuning methods, which serves as the preliminaries for SAIL-RL. The reasoning-incentivized MLLM post-training pipeline typically consists of two stages: supervised fine-tuning (SFT) on complex reasoning datasets to elicit long chain-of-thought (CoT) capability, followed by reinforcement learning (RL) with verifiable rewards to further strengthen reasoning."
        },
        {
            "title": "Technical Report",
            "content": "Stage 1: LongCoT SFT. The first stage trains the model to learn step-by-step reasoning structures through SFT. Each sample includes reasoning trace (T ) leading to final answer (A), and the objective is standard next-token prediction over the concatenated sequence: LSFT = E(I,T,A)DCoT (cid:2) log Pθ(T AI)(cid:3). This equips the model with the ability to articulate reasoning paths. Stage 2: RL with Verifiable Rewards (RLVR). The second stage enhances reasoning with reinforcement learning. The reward combines final-answer accuracy and output-format compliance: = α Ranswer + (1 α) Rformat. Policy optimization is performed with Group Relative Policy Optimization (GRPO), which computes normalized advantages across peer responses and updates the policy via clipped surrogate objective regularized by KL term: JGRPO(θ) = E(cid:104) min (cid:0)rt(θ) ˆAt, clip(rt(θ), 1 ε) ˆAt (cid:105) (cid:1) βDKL(πθπref) , . This stage directly aligns responses with both structural and answer correctness. where rt(θ) = πθ πθold"
        },
        {
            "title": "4 SAIL-RL",
            "content": "In this section, we introduce SAIL-RL, reasoning-incentivized tuning framework to improve both the effectiveness and efficiency of RL post-training for MLLMs. SAIL-RL introduces dual-reward mechanism that guides models on what to think and when to think, thereby enhancing the quality and efficiency of reasoning. By directly addressing the above two limitations of existing RL-tuning approaches, SAIL-RL establishes new paradigm for MLLM post-training. 4.1 THINKING REWARD: WHAT TO THINK As the saying goes, sound reasoning leads to correct answers. To improve response quality, model is required to learn what to think by constructing clear and coherent reasoning paths. Beyond outcomeonly supervision in conventional RL-tuning, we introduce Thinking Reward that comprehensively evaluates reasoning quality with LLM-based judge models. This reward is integrated into RL tuning to guide models toward producing higher-quality reasoning across multiple dimensions. Logical Coherence Reward. We first introduce the Logical Coherence Reward, which evaluates whether model can think clearly. This dimension measures the internal logical integrity of the reasoning process, ensuring arguments are both well-structured and correctly executed. To this end, the judge model applies two sequential checks: (i) Structural Soundness, assessing whether the problem is properly formulated (e.g., into valid equation); and (ii) Deductive Soundness, verifying that subsequent steps are free of contradictions, calculation errors, or logical fallacies. Failure in either check yields score of d1 = 0, and success in both yields d1 = 1. Factual Grounding Reward. We then introduce the Factual Grounding Reward to evaluate whether the model is thinking truthfully rather than hallucinating. This reward penalizes unsupported statements by requiring each step in the reasoning process to be factually grounded. To this end, the judge model performs hierarchical fact-check across three sources: (i) Visual Grounding, verifying claims against the provided image; (ii) Textual Grounding, checking consistency with the input query; and (iii) World Knowledge, consulted only when verification is not possible from the first two sources. Any contradiction at any stage yields score of d2 = 0, and otherwise d2 = 1. Answer Consistency Reward. We further introduce the Answer Consistency Reward to evaluate whether the model thinks correctly. This reward ensures that the final answer is direct and faithful conclusion derived from the preceding reasoning. The judge model verifies that the reasoning trace fully justifies the answer, checking for contradictions, reliance on unstated information, or unsupported claims. Any failure results in score of 0; otherwise, the score is 1. Finally, the overall thinking reward Rthink is computed as the average of the three binary dimensions evaluated by the judge model, i.e., Rthink = 1 3 i=1 di. (cid:80)"
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: An overview of the SAIL-RLs multi-dimensional reward system. The system evaluates models response across four dimensions: Format, Answer, Thinking, and Judging. The nuanced semantic rewards for Thinking and Judging are provided by Gemini acting as reward-judger. 4.2 JUDGING REWARD: WHEN TO THINK We further explore how to guide MLLMs on when to think: applying thorough reasoning for complex problems while giving direct responses to simple ones. The central challenge is to assess task difficulty and adapt the reasoning strategy accordingly, ensuring efficiency without sacrificing quality. To this end, we introduce Judging Reward, which incentivizes the model to first determine whether reasoning is necessary before generating response, thereby balancing effectiveness with efficiency. Specifically, the model is required to decide whether given question necessitates detailed reasoning process (think) or can be answered directly (direct). The reward depends on the appropriateness of this decision, measured against ground-truth labels indicating task complexity. For complex questions, choosing think yields score of djudge = 1, and otherwise 0. For simple questions, choosing direct is rewarded with djudge = 1, while initiating unnecessary reasoning results in 0. The final judging reward is therefore defined as Rjudge = djudge. By optimizing this reward, SAIL-RL trains the model to assess the necessity of reasoning before responding, thereby improving efficiency without sacrificing accuracy on complex tasks. 4.3 REWARD SYSTEM In addition to our dual-reward mechanism, we also incorporate traditional RL-tuning rewards (Shao et al., 2024). The answer reward Ranswer evaluates the correctness of the final response with binary score (1 if correct, 0 otherwise). For verifiable tasks (e.g., math problems), correctness is determined programmatically through exact matching, numerical evaluation with tolerance, or execution-based validation. For open-ended tasks (e.g., multimodal benchmarks), correctness is judged by strong LLM along with factual accuracy, semantic relevance, and completeness. The format reward enforces machine-parsable outputs, requiring judgement to be enclosed within <judge> tags, reasoning within <think> tags and the final answer in boxed{} tag; any violation yields score of 0. The overall reward signal unifies these components into single formulation: Rtotal = α (Rjudge Rthink Ranswer) + (1 α) Rformat, (1) Here, Rjudge, Rthink, Ranswer, and Rformat denote the respective rewards. The combination of Rjudge, Rthink, and Ranswer is designed as cascading product. An additive approach is susceptible to reward hacking, where model could be rewarded for success in one component despite critical failures in another. For example, this could reward lucky guess (a correct answer from flawed reasoning) or overthinking (generating high-quality but unnecessary reasoning for simple question). In contrast, our cascading product formulation functions as logical AND gate. It ensures that reward is given only when the initial judgment, the reasoning process, and the final answer are all correct. This design"
        },
        {
            "title": "Technical Report",
            "content": "effectively mitigates these undesirable behaviors, thereby enforcing strong link between sound reasoning and correct outcomes. The Rformat serves as lighter regularizer to encourage structural compliance. We set α = 0.9 to emphasize correctness and reasoning quality."
        },
        {
            "title": "4.4.1 LONGCOT SFT",
            "content": "The first stage builds the models foundational ability to sequentially judge problems complexity, generate step-by-step reasoning process, and derive final answer. This is achieved by fine-tuning the base model on large-scale, high-quality LongCoT dataset. Data Curation. The dataset is built through comprehensive pipeline. We aggregate data from diverse sources (Jia et al., 2025; Shi et al., 2024; Xu et al., 2024), followed by rigorous cleaning and deduplication. We unify the data into structured format that explicitly includes judgment, thinking, and answering tags. To generate this data, powerful teacher model is prompted to first produce judgment (<judge>) on whether the problem requires complex reasoning. It then generates detailed thinking process (<think>) that logically leads to the ground-truth answer, which is enclosed in the boxed{} tag. The generated samples are then filtered through multi-stage checks for redundancy, correctness, and reasoning complexity. This process yields 400K high-quality LongCoT samples (details in the Appendix). Training Objective. We fine-tune the model with AdamW (batch size 1024) and cosine learning rate peaking at 1e-6. The objective is standard next-token prediction loss over the full sequence of judgment, reasoning, and answer: LLongCoT-SFT = 1 DCoT (cid:88) (I,J,T,A)DCoT log Pθ(J AI) (2) where is the input, is the judgment text, the reasoning process, the final answer, and denotes concatenation. This training objective explicitly teaches the model to first judge the problems nature, then produce corresponding reasoning trace, and finally output the answer in the correct format. 4.4.2 RL TUNING WITH REWARD SYSTEM LongCoT SFT provides strong generative template, RL incentivizes reasoning capabilities. It focuses on not only what to think (reasoning quality) but also when to think (efficiency), thereby ensuring sound reasoning consistently leads to correct answers. Data Curation. We curate data from diverse sources, including Math (Sun et al., 2024; Meng et al., 2025), Puzzle (Chia et al., 2024), Science (Wang et al., 2025; Lu et al., 2022), OCR (Chen et al., 2025b), and Counting (Johnson et al., 2017). two-stage filtering pipeline is applied: (i) multiple-choice questions are converted into free-response format to prevent reward hacking, and (ii) difficulty-based filtering with the previous-stage models pass@4 score removes trivial and unsolvable samples. This yields high-quality dataset of 70K problems for stable RL training. Training Objective. We optimize the SFT model using DAPO (Yu et al., 2025) on the curated dataset with our proposed reward system. We remove the KL loss and set the learning rate to 1e-6. The clipping value ε is dynamically adjusted within the range of [0.20, 0.28] to encourage exploration Further implementation details and hyperparameters are provided in the Appendix."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENT SETUP This section details the methodology for our experiments. including training datasets, implementation details, and evaluation benchmarks."
        },
        {
            "title": "5.1.1 TRAINING DATASETS",
            "content": "We prepare two distinct datasets for the two-stage training pipeline. First, for LongCoT SFT, we generate 400K high-quality CoT data from diverse sources. All samples are unified into the structured sequence format <judge>...</judge><think>...</think> and boxed{...} to explicitly teach the model to judge first, then think, and finally answer. Second, for the RL, we curate mixed RL dataset, which consists of two parts: 50K from STEM for precise reward signals, and 50K from general tasks to enhance generalization. LongCoT Data Curation. We develop holistic data pipeline to build our high-quality LongCoT dataset. The goal is to instill meta-cognitive skill: first judging problems complexity, then executing the appropriate reasoning path. To this end, every sample in our dataset is meticulously structured in judge-think-answer format. We first collect diverse datasets, ranging from complex, logical problems (e.g., VisualWebInstruct (Jia et al., 2025), MathV360K (Shi et al., 2024)) to simple, perception-based questions (e.g., LLaVA-CoT (Xu et al., 2024)). All collected data is then processed through unified pipeline to fit our judge-think-answer format. The key steps are as follows: 1) Data Cleaning: We perform cleaning pass to remove extraneous content, such as system prompts and conflicting hints. We then deduplicate the entire dataset based on unique image and question pairs to ensure diversity; 2) Conditional Annotation: Each sample is annotated based on its complexity. For complex problems requiring reasoning, we use guided-prompting strategy to generate detailed chain-of-thought for the <think> section, and the <judge> tag is set to indicate that thinking is necessary. For simple perceptual tasks, the <judge> tag is set to indicate that the question can be answered directly, and the <think> section is intentionally populated with an empty string (nn). The final answer for all samples is standardized within the <boxed> tag; 3) Quality Filtering: Finally, the annotated dataset undergoes rigorous filtering workflow. This includes redundancy filter, which penalizes trivial reasoning by measuring the token overlap between the thought process and the final answer, and length-balancing step on the reasoning chains to ensure varied representation of complexity. This pipeline results in 400K high-quality longCoT samples, each designed to train the model on when and how to reason. RL Data Curation. We construct diverse dataset for the RL stage, balanced between specialized STEM problems and general-purpose QA. The STEM domain is curated from wide array of public benchmarks in fields like Math (Sun et al., 2024; Meng et al., 2025), Puzzles (Chia et al., 2024), Science (Wang et al., 2025; Lu et al., 2022), OCR (Chen et al., 2025b), and Counting (Johnson et al., 2017). This data undergoes rigorous two-stage filtering pipeline to optimize for training stability. The first stage mitigates reward hacking by reformatting multiple-choice questions into an open-ended, free-response format. The second stage implements difficulty-based curriculum filtering, using our SFT models pass@4 score to retain only problems within an optimal difficulty range by removing the easiest (pass@4=1) and hardest (pass@4=0) instances. To ensure broad capabilities, we incorporate with 50K General QA samples from LLaVA-OneVison (Li et al., 2024). This subset is filtered primarily for quality and diversity, preserving wide range of conversational and factual questions. The RL dataset comprises 100K samples, evenly split between 50K challenging STEM problems and 50K General QA instances, creating comprehensive training environment for both specialized reasoning and general interaction. 5.1.2 IMPLEMENTATION DETAILS Our model, based on SAIL-VL (Dong et al., 2025) (integrating AimV2 (Fini et al., 2025) and Qwen3 (Yang et al., 2025a)), is trained in two stages. We first conduct full-parameter SFT for one epoch on our LongCoT dataset. This is followed by three epochs of RL using the DAPO algorithm, guided by our SAIL-RL reward system which leverages Gemini-2.5-Pro as the VLM-Judge. LongCoT SFT Stage. In the first stage, we fine-tune all parameters of the model for one epoch on our 400K-sample LongCoT dataset. For this SFT stage, we set the maximum sequence length to 20K, the global batch size to 1024, and the learning rate to 1e-6. RL Stage. Subsequently, we optimize the SFT model for three epochs on our 100K-sample mixed RL dataset using the DAPO (Yu et al., 2025) algorithm, guided by our proposed SAIL-RL reward system. We set the maximum sequence length to 20K, consisting of 16K for the input and 4K for the output. The policy learning rate is set to 1e-6 with global PPO batch size of 256. For each sample, we rollout 5 times to estimate the advantage. To encourage exploration and stabilize training, we"
        },
        {
            "title": "Technical Report",
            "content": "Table 1: Evaluation results on OpenCompass multimodal reasoning benchmarks. The best results among open-source models are bolded and the second-best results are underlined. Model DynaMath LogicVista MathVerse MathVision MathVista WeMath Average Gemini-2.0-Pro GPT-4o-latest InternVL3-2B Qwen2.5-VL-3B WeThink-7B InternVL3-8B Qwen2.5-VL-7B VL-Rethinker-7B VLAA-Thinker-7B Keye-VL-8B-Thinking Kimi-VL-A3B-Thinking SAIL-VL2-2B-Instruct SAIL-VL2-2B-Thinking SAIL-VL2-8B-Instruct SAIL-VL2-8B-Thinking 43.3 48.5 14.0 11.0 24.4 25.7 21.8 17.8 22.4 37.3 29.1 10.2 25.7 17.8 38.3 Close-source Models 53.2 64.4 67.3 49. Open-source Models 33.6 36.0 53.0 44.5 47.9 42.7 48.5 54.8 47.2 36.2 45.4 45.0 63.8 20.6 29.3 44.7 38.5 41.1 46.4 48.2 59.8 55.2 22.6 50.5 32.9 65.1 48.1 43. 20.2 18.1 27.2 30.0 25.4 28.4 26.4 46.0 53.6 23.4 30.5 27.6 49.4 71.3 71.6 57.3 60.2 70.9 70.5 68.1 73.7 68.0 80.7 79.5 71.1 73.6 76.4 80.9 56.5 50. 13.0 20.7 48.0 39.5 36.2 36.3 41.5 60.7 47.9 22.7 42.1 35.8 58.2 56.6 54.8 26.5 29.2 44.7 41.5 40.1 40.9 42.5 56.6 52.1 31.0 44.6 39.3 59.3 Table 2: Evaluation on multimodal understanding benchmarks. HallBench denotes HallusionBench. The best results among open-source models are bolded and the second-best results are underlined. Model Gemini-2.0-Pro GPT-4o-latest InternVL3-2B Qwen2.5VL-3B WeThink-7B InternVL3-8B Qwen2.5-VL-7B VL-Rethinker-7B VLAA-Thinker-7B Keye-VL-8B-Thinking* Kimi-VL-A3B-Thinking* SAIL-VL2-2B-Instruct SAIL-VL2-2B-Thinking SAIL-VL2-8B-Instruct SAIL-VL2-8B-Thinking General VQA OCR & Chart Hallucination Average MMMUval MMBenchv1.1 MME ChartQAtest AI2D HallBench 72.6 70.7 47.1 48.1 50.9 57.3 50.3 54.8 51.9 63.4 60. 47.7 51.2 55.4 66.1 Close-source Models 83.0 84.3 86.1 84.2 Open-source Models 84.3 82.4 87.8 87.7 86.7 88.2 86.9 81.7 89. 86.8 87.2 90.2 90.4 77.4 77.5 82.9 85.2 82.2 82.9 83.3 83.5 87.0 76.6 78.4 84.5 86.0 91.2 91.5 80.4 87.0 90.8 89.6 89.5 91.5 89.5 88.0 92.1 89.1 92.2 90.3 93. 84.8 86.3 78.7 80.7 84.5 85.2 84.0 83.6 78.9 86.4 83.1 83.0 84.1 87.7 87.4 49.8 57.0 41.4 48.3 55.1 53.7 56.0 55.1 51.5 62.7 58.3 51.7 53.1 55.1 61. 77.9 79.0 68.2 70.7 75.3 76.5 74.8 76.0 73.7 77.6 78.4 72.5 74.1 77.2 80.8 remove the standard KL divergence and dynamically adjust the clipping value ε within the range of [0.20, 0.28]. 5.1.3 EVALUATION BENCHMARKS We conduct comprehensive evaluation using VLMEvalKit (Duan et al., 2024), with GPT-4o-Mini as the judge for fairness. We assess two primary categories of abilities. First, we evaluate advanced reasoning on benchmarks focused on mathematical and logical analysis (Zou et al., 2024; Zhang et al., 2024; Lu et al., 2023; Fang et al., 2024). Second, we evaluate general multimodal understanding using comprehensive benchmarks covering from general VQA, chart comprehension to hallucination detection (Yue et al., 2024; Liu et al., 2024)."
        },
        {
            "title": "5.2.1 BENCHMARK PERFORMANCE",
            "content": "Multimodal Reasoning Benchmarks. As shown in Tab. 1, SAIL-VL2-8B-Thinking sets new stateof-the-art among open-source models with an average score of 59.3. This gain is largely attributed to the thinking reward, which supervises the reasoning process rather than only the final answer, leading to +20.0 improvement over the baseline SAIL-VL2-8B (39.3). The model achieves top-tier results on complex tasks such as DynaMath (38.3), LogicVista (63.8), MathVerse (65.1), and MathVista (80.9), surpassing even closed-source systems like GPT-4o (54.8) and Gemini-2.0-Pro (56.6). Multimodal Understanding Benchmarks. As shown in Tab. 2, SAIL-VL2-8B-Thinking achieves an open-source state-of-the-art with an average score of 80.4. The thinking quality reward improves factual grounding, reducing hallucinations as reflected in the 61.5 score on HallusionBench, while the judge reward allocates cognitive effort effectively, achieving 93.6 on ChartQA. The synergy of these mechanisms enhances both reasoning quality and efficiency, ensuring robust performance. 5.2.2 THINKING ANALYSIS Thinking Quality. To validate the effectiveness of the proposed thinking reward, we analyze the reasoning chains generated by SAIL-VL2-8B-Thinking and compare them with leading open-source models that also produce explicit thinking steps, such as Keye-VL-8B-Thinking. As shown in Tab. 3, we achieves higher thinking quality across multiple dimensions, with average scores of 83.2 on LogicVista and 95.5 on OCRBench, surpassing Keye-VL-8B-Thinking. These results indicate that the thinking quality reward not only encourages models to generate reasoning steps but also guides them to reason correctly, leading to more reliable and accurate outcomes. Table 3: Evaluation results on thinking quality. We use the proposed reward system to compare SAIL-VL2-8B-Thinking and Keye-VL-8B-Thinking. Benchmark Model Logic Hallucination Consistency Score (%) Score (%) Score (%) Average LogicVista OCRBench SAIL-VL2-8B-Thinking Keye-VL-8B-Thinking (Improvement) SAIL-VL2-8B-Thinking Keye-VL-8B-Thinking (Improvement) 80.3 55.3 +25. 95.1 89.9 +5.2 73.8 61.7 +12.1 94.0 87.6 +6.4 95.3 78.8 +16.5 97.4 87.4 +10.0 83.2 65.3 +17. 95.5 88.3 +7.2 Thinking Trigger. To validate the effectiveness of our proposed judging reward, we analyze the adaptive allocation of reasoning resources in the model. As shown in Fig. 4, our model intelligently adjusts its thinking trigger rate based on the task type, demonstrating notable efficiency and rationality. For example, on tasks like OCRBench where complex reasoning is often unnecessary, the trigger rate is merely 7.5%. In contrast, for tasks requiring deep mathematical or logical inference like MathVista and WeMath, the rate significantly increases to 94% and 99.1%, respectively. Crucially, this efficiency does not come at the cost of performance. As demonstrated in Tab. 1 and Tab. 2, SAIL-VL28B-Thinking achieves superior results while being significantly more efficient than models with non-adaptive thinking mechanisms. This provides strong evidence that our Thinking Judge can effectively discern task complexity, activating deep reasoning only when necessary to achieve an optimal balance between performance and efficiency. Figure 4: Evaluation results on thinking trigger."
        },
        {
            "title": "5.3 ABLATION STUDY",
            "content": "Due to the computational cost of the full training, we conduct our experiments on smaller model, SAIL-VL2-2B. We also adopt shortened training schedule, training for single epoch at each stage."
        },
        {
            "title": "5.3.1 EFFECTIVENESS OF THINKING REWARD",
            "content": "To validate the effectiveness of Thinking Reward, we compare two variants: (1) baseline trained with an answer reward that only considers final correctness, and (2) our model trained with the proposed reward, which additionally supervises intermediate reasoning steps. Performance Comparison. As shown in Tab. 4, Thinking Reward yields +1.3% average gain across eight benchmarks. We show significant improvement in STEM tasks that require multi-step reasoning, for example, achieving +2.5% gain on WeMath. Although MME has -0.4% minor drop, the overall positive results demonstrate that complementing answer rewards with thinking reward produces more reliable reasoning and accurate answers. Table 4: Ablation on Thinking Reward. We compares answer reward against thinking reward. Reward STEM General Hallucination Average WeMath MathVerse LogicVista DynaMath MMMUval MMBenchv1.1 MME HallBench Answer Answer + Thinking (Improvement) 38.7 41. +2.5 47.8 49.5 +1.7 45.0 47.1 +2.1 22.6 25. +2.8 46.9 47.4 +0.5 84.5 85.3 +0.8 77.5 77. -0.4 51.7 52.1 +0.4 51.8 53.1 +1.3 Training Dynamics. Fig. 5 shows the evolution of three quality metricslogic, consistency, and hallucinationduring training. Our model steadily improves across all dimensions, while the baseline stagnates, and even degrades on answer consistency. This provides direct evidence that optimizing for final answers alone does not guarantee, and may even harm, coherent reasoning. Figure 5: Ablation on training dynamics of thinking reward. Our method (blue) consistently improves all three thinking score over the answer-only baseline (orange), which stagnates or degrades. Case Study. As shown in Fig. 7, the thinking reward significantly improve the cognitive depth. The baseline model (blue), guided by an answer-only reward, adopts brute-force strategy. It correctly computes the series term-by-term until the condition is met at n=4 and then immediately terminates its reasoning. While effective for finding the answer, this approach demonstrates superficial understanding of the problem. In contrast, our model (orange), trained with the thinking reward, exhibits more advanced analytical process. After finding the solution, it continues to analyze the series, successfully identifying the underlying pattern of alternating signs and increasing magnitudes. This demonstrates that by rewarding the quality of the reasoning process itself, our approach encourages the model to move beyond simple procedural execution towards deeper, more human-like understanding of the problems structure. 5.3.2 EFFECTIVENESS OF JUDGING REWARD We conduct an ablation study comparing two reward strategies: Forced Thinking baseline, which consistently engages in step-by-step reasoning, and Judge Reward approach, which dynamically decides whether to activate the thinking process."
        },
        {
            "title": "Technical Report",
            "content": "Performance Comparison. As shown in Tab. 5, our Judge Reward yields +0.6% average gain. The model maintains competitive performance on STEM benchmarks, confirming its core reasoning abilities are unaffected. The advantage is concentrated in general and perception-heavy tasks where avoiding overthinking is crucial. For example, we achieves significant gains on MMBench (+1.3%), and MME (+0.5%), reducing factual errors and analytical confusion. These results validate that Judging Reward dynamically allocates resources to achieve robust reasoning. Table 5: Ablation on Judging Reward. We compare forced thinking against judging reward. Method STEM General Hallucination Average WeMath MathVerse LogicVista DynaMath MMMUval MMBenchv1.1 MME HallBench Forced Thinking Judge Reward (Improvement) 38.7 38.5 -0. 47.8 47.9 +0.1 45.0 45.3 +0.3 22.6 22.7 +0. 46.9 48.3 +1.4 84.5 85.8 +1.3 77.5 78.0 +0. 51.7 53.1 +1.4 51.8 52.4 +0.6 Case Study. Fig. 6 highlights how our Judging Reward prevents errors from overthinking. The baseline model (blue), forced to generate reasoning chain, over-analyzes simple OCR task, leading to hallucinations (tongue) and an incorrect answer. In contrast, our model (orange box) uses the Judge Reward to identify the task as straightforward, bypass the thinking process, and answer correctly. This demonstrates the rewards critical role in dynamically allocating cognitive resources to avoid errors on simple perception tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose SAIL-RL, reinforcement learning post-training framework that advances the reasoning capability of MLLMs by teaching them when and how to think. Unlike outcome-only supervision and uniform reasoning strategies, SAIL-RL introduces dual reward system that evaluates reasoning quality and adaptively controls the depth of thinking. Experiments on SAIL-VL2 show that SAIL-RL improves reasoning and multimodal understanding benchmarks at both 4B and 8B scales, achieves state-of-the-art results among models of comparable size, and delivers competitive performance with GPT-4o and Gemini-2.0-Pro. These findings establish SAIL-RL as foundation that supports the design of more reliable and adaptive MLLMs through scalable thinking RL."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models. arXiv preprint arXiv:2504.11468, 2025a. Yang Chen, Yufan Shen, Wenxuan Huang, Shen Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Botian Shi, and Yu Qiao. Learning only with images: Visual reinforcement learning with reasoning, rendering, and visual feedback. arXiv preprint arXiv:2507.20766, 2025b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024. Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. arXiv preprint arXiv:2403.13315, 2024. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025."
        },
        {
            "title": "Technical Report",
            "content": "Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, and Jiao Ran. Scalable vision language model training via high quality data curation. arXiv preprint arXiv:2501.05952, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1119811201, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. URL https://arxiv. org/abs/2402.01306, 2023. Zhen Fang, Jiacheng Li, Lichan Zhang, Pan Lu, Bodhisattwa Prasad Majumder, Tony Xia, Rami Al-Rfou, and Swaroop Mishra. Wemath: well-crafted dataset for mathematical reasoning with weak supervision. arXiv preprint arXiv:2405.19228, 2024. Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Turrisi da Costa, Louis Bethune, Zhe Gan, et al. Multimodal autoregressive pretraining of large vision encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 96419654, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yiming Jia, Jiachen Li, Xiang Yue, Bo Li, Ping Nie, Kai Zou, and Wenhu Chen. Visualwebinstruct: Scaling up multimodal instruction data through web search. arXiv preprint arXiv:2503.10582, 2025. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. OpenAI. Openai-o1, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. Fangxun Shu, Yue Liao, Le Zhuo, Chenning Xu, Lei Zhang, Guanghao Zhang, Haonan Shi, Long Chen, Tao Zhong, Wanggui He, et al. Llava-mod: Making llava tiny via moe knowledge distillation. arXiv preprint arXiv:2408.15881, 2024. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. Mm-math: Advancing multimodal math evaluation with process evaluation and fine-grained classification. arXiv preprint arXiv:2404.05091, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025a. Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025b. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning. arXiv preprint arXiv:2506.07905, 2025b. Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, et al. Sail-vl2 technical report. arXiv preprint arXiv:2509.14033, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025."
        },
        {
            "title": "Technical Report",
            "content": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        },
        {
            "title": "A CASE STUDY",
            "content": "We present two case studies that highlight the distinct benefits of our proposed reward components. The first case, math problem, demonstrates how our Thinking Reward encourages the model to move beyond superficial, procedural answer-finding towards deeper, analytical understanding of problems structure. The second case, simple OCR task, illustrates how the Judging Reward provides the model with critical meta-cognitive ability to avoid overthinking by dynamically assessing task complexity and allocating its reasoning resources effectively. Detailed visualizations and analyses for both cases are provided in the Fig. 6 and Fig. 7, respectively. Figure 6: Visualizing behavior on an OCR task under two different reasoning strategies. Orange: The output from baseline that is forced to think. Blue: The output from our model guided by the proposed Judge Reward, which dynamically decides when to think. Figure 7: Visualizing reasoning on math problem under two different reward systems. Orange: The output from baseline trained with an answer-only reward. Blue: The output from our model trained with the proposed Thinking Quality Reward."
        },
        {
            "title": "THINK JUDGE PROMPT",
            "content": "CONTEXT You are an AI logic analyst. Your task is to evaluate [MODEL JUDGMENT] to determine if it correctly judged whether [QUESTION] (based on [CONTEXT]) requires reasoning. OBJECTIVE You will perform two core tasks: 1. Independent Assessment (Set requires reasoning): First, independently analyze the [QUESTION] and [CONTEXT]. Simple (false): The answer is from direct observation or retrieval (e.g., What color is...?, Who is...?). Complex (true): The answer requires inference, calculation, or synthesis (e.g., Why...?, Which is better...?, What is the final cost...?). 2. Evaluation (Set is judgment correct): Second, compare your assessment from Step 1 with the [MODEL JUDGMENT]. If your conclusion and the models judgment align, is judgment correct is true. If they do not align, it is false. ATTENTION: JSON Output Format Your output MUST be single JSON object. Do not add any other text. The JSON must contain these two fields: 1. is judgment correct (boolean): Is the [MODEL JUDGMENT] correct? 2. requires reasoning (boolean): Does the question itself actually require reasoning? (Your independent assessment). CONSISTENCY REWARD PROMPT CONTEXT You are rigorous evaluator. Your task is to verify whether given conclusion (Answer) is direct and logically consistent result of the preceding reasoning (Thinking Process). You focus solely on the consistency between the reasoning and the conclusion. OBJECTIVE You need to determine if the final Answer is logically consistent conclusion derived from the Thinking Process. Your tasks include: 1. Analyze Thinking Process: Carefully read and understand the steps and conclusions within the Thinking Process. 2. Analyze Answer: Read the final Answer. 3. Check for Consistency: Determine if the Answer directly and logically follows from the Thinking Process. The Answer should not introduce new information or contradict the Thinking Process. TONE Analytical, precise, direct. RESPONSE: Consistency Judgment [Is the Answer consistent with the Thinking Process? (TRUE or FALSE)] ATTENTION Your evaluation must focus exclusively on the consistency between the Thinking Process and the Answer. You are not evaluating the correctness of the Thinking Process itself. The consistency judgment is FALSE if the Answer contradicts, is not supported by, or introduces new reasoning not present in the Thinking Process. Do not provide any extra explanation."
        },
        {
            "title": "LOGIC REWARD PROMPT",
            "content": "CONTEXT You are professor of logic and applied reasoning. You are tasked with evaluating the structural and deductive soundness of students reasoning. Below, you are provided with problem (which may include an image) and the students step-by-step reasoning. Your job is to assess the validity of the entire reasoning process, from how the problem is modeled to how the conclusion is reached. OBJECTIVE You must judge if the students reasoning process is logically sound. This soundness depends on two distinct criteria: the structural model and the deductive execution. Your evaluation must perform two checks. failure in EITHER check means the entire reasoning process is logically unsound. 1. Check 1: Structural Soundness. First, evaluate if the student correctly translates the problems components, relationships, and constraints into valid mathematical or logical model. Does the chosen formula correctly represent the principles described in the problem? Are the variables in the reasoning correctly mapped to the components of the problem? This is about the setup of the reasoning. 2. Check 2: Deductive Soundness. Second, given the students model from Check 1 (even if its structurally flawed), is the subsequent chain of reasoning free of self-contradictions, mathematical errors, or invalid logical steps? Are the calculations correct? Is the algebraic manipulation valid? This is about the execution of the reasoning. TONE Professional, analytical. RESPONSE: Logical Validity Judgement [Is the reasoning process logically sound? (TRUE or FALSE)] ATTENTION The reasoning is logically unsound (FALSE) if it fails EITHER the structural check OR the deductive check. You are NOT fact-checking the initial numbers from the problem description. However, if the student misrepresents the fundamental relationship between radius, span, and arch height, that IS structural logic error and must be marked FALSE. The Logical Validity Judgement is FALSE for even single, minor flaw in either structure or deduction. Do not give any extra explanation."
        },
        {
            "title": "HALLUCINATION REWARD PROMPT",
            "content": "CONTEXT You are meticulous, multi-modal fact-checker. Your job is to assess if the provided model response contains any hallucinations by cross-referencing it against three sources: the image, the text, and world knowledge. OBJECTIVE You must judge whether the models response is completely free of hallucinations. Your evaluation must perform up to three checks. failure in ANY check means the response contains hallucination. 1. Visual Grounding Check: Cross-reference every claim about visual elements against the Input Image. Check for contradictions in objects, attributes (color, count, position), relationships, or scenes. 2. Textual Grounding Check: Cross-reference every claim against the Text Question. Check for contradictions in facts, numbers, conditions, or constraints in the text. 3. World Knowledge Check: For any claim of fact that cannot be verified by the image or text, cross-reference it against established, verifiable world knowledge. This includes historical facts, scientific principles, geographical locations, etc. TONE Professional, factual, objective. RESPONSE: Hallucination Judgement [Is the model response free of hallucinations? (TRUE or FALSE)] ATTENTION The Hallucination Judgement is FALSE if there is even single claim that contradicts the image, the text, OR world knowledge. If the provided text or image deliberately presents hypothetical or counter-factual scenario (e.g., If the capital of China were Nanjing...), you must evaluate based on that context, not world knowledge. In such cases, the provided context takes precedence. Do not give any extra explanation."
        }
    ],
    "affiliations": [
        "Douyin SAIL Team",
        "National University of Singapore"
    ]
}