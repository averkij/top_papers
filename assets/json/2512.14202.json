{
    "paper_title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning",
    "authors": [
        "Timo Klein",
        "Thomas Lang",
        "Andrii Shkabrii",
        "Alexander Sturm",
        "Kevin Sidak",
        "Lukas Miklautz",
        "Claudia Plant",
        "Yllka Velaj",
        "Sebastian Tschiatschek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 0 2 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "UNDERSTANDING AND IMPROVING HYPERBOLIC DEEP\nREINFORCEMENT LEARNING",
            "content": "Timo Klein,1,2, Thomas Lang,1,2, Andrii Shkabrii1,2, Alexander Sturm1,2, Kevin Sidak1,2 Lukas Miklautz3, Claudia Plant1,4, Yllka Velaj1,4, Sebastian Tschiatschek1,4 1 Faculty of Computer Science, University of Vienna, Vienna, Austria 2 Doctoral School Computer Science, University of Vienna, Vienna, Austria 3 Department of Machine Learning and Systems Biology, Max Planck Institute of Biochemistry, Martinsried, Germany 4 ds:UniVie, University of Vienna, Vienna, Austria Joint first authors firstname.lastname@univie.ac.at"
        },
        {
            "title": "ABSTRACT",
            "content": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincare Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce HYPER++, new hyperbolic PPO agent that consists of three components: (i) stable critic training through categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that HYPER++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, HYPER++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl."
        },
        {
            "title": "INTRODUCTION",
            "content": "Consider chess-playing agent facing difficult moment in its game. As it maps out future scenarios, these unfold into tree of possible future states. Each action commits to one branch and rules out others, and the number of reachable positions grows exponentially with depth. Playing chess can thus be viewed as traversing this expanding tree of possible states. The same structure appears in other sequential decision-making benchmarks such as ProcGen BIGFISH (Cobbe et al., 2020). Here, the agent grows by eating smaller fish, and growth cannot be undone, inducing natural order. In both cases, the data are inherently hierarchical: each state depends on its predecessors, and future states branch from the present one. Tree-structured data from sequential decision problems like chess or BIGFISH cannot be embedded in Euclidean space without large distortion: Euclidean volume grows only polynomially in radius, whereas tree size grows exponentially 1 Figure 1: Baseline improvement on ProcGen. We compare mean test rewards for our agent (HYPER++), Euclidean agent, and an unregularized hyperbolic agent (Hyper) with Cetin et al. (2023)s agent (Hyper+S-RYM). (Sarkar, 2011; Gromov, 1987). This creates mismatch between the hierarchical data produced by decision-making agents and the Euclidean representations used by modern deep networks (Cetin et al., 2023). We hypothesize that this mismatch contributes to the data-inefficiency and deployment challenges of deep RL despite impressive successes (Silver et al., 2016; Schrittwieser et al., 2020; Ouyang et al., 2022). But what if there were representations that better match the geometry of sequential decision making? Hyperbolic geometry (Bolyai, 1896; Lobachevskiı, 1891) offers an appealing solution to the limitations of Euclidean representations: Unlike Euclidean space, its exponential volume growth makes it natural fit for embedding hierarchical data. Since its inception, hyperbolic deep learning has achieved strong results in classification (Ganea et al., 2018), graph learning (Chami et al., 2019), unsupervised representation learning (Mathieu et al., 2019), deep metric learning (Ermolov et al., 2022), and image-text alignment (Pal et al., 2025). Despite its conceptual appeal, the broader adoption has been hampered by significant optimization challenges (Guo et al., 2022; Mishne et al., 2023). Thus, while hyperbolic geometry is inherently well-suited for RL, its broader adoption in deep RL hinges on thorough understanding of the associated optimization challenges and potential failure modes. To this end, we study the heuristic trust-region algorithm proximal policy optimization (PPO) with hybrid Euclideanhyperbolic encoder, which is commonly used architecture for Deep RL (Cetin et al., 2023; Salemohamed et al., 2023). Despite the trust region, hyperbolic PPO agents face policy-learning issues from unstable encoder gradients, further amplified by nonstationary data and targets in deep RL (Cetin et al., 2023). In this paper, we take step towards more principled understanding of the underlying optimization issues in hyperbolic deep RL. We start by analyzing key derivatives of mathematical operations of hyperbolic deep learning, which we link to trust-region issues of hyperbolic PPO. We show that neither the Poincare Ball nor the Hyperboloid common models for hyperbolic geometry is immune to gradient instability. Grounded in this analysis, we propose principled regularization approach to stabilize the training of hyperbolic agents. The resulting agent, HYPER++, ensures stable learning by pairing Euclidean feature regularization on the Hyperboloid with categorical value loss to handle target nonstationarity. HYPER++ improves PPOs performance and wall-clock time on ProcGen (Figure 1) by approximately 30% over existing hyperbolic agents. We further show that our regularization approach generalizes beyond on-policy methods: applying the same ideas to DDQN (van Hasselt et al., 2016) and the Atari-3 benchmark (Aitchison et al., 2023) also yields strong performance improvements. Our code will be made available. Our Key Contributions 1. Characterization of training issues. For both the Poincare Ball and Hyperboloid, we derive gradients of key operations and link them to training instability in deep RL. 2. Principled regularization. We study the weaknesses of current approaches and propose improvements rooted in our insights into hyperbolic deep RL training. 3. HYPER++, strong hyperbolic agent with stable training. We integrate categorical value loss, RMSNorm, and novel scaling layer for the Hyperboloid model."
        },
        {
            "title": "2 BACKGROUND",
            "content": "This section first reviews Markov decision processes (MDPs) and the PPO optimization procedure (Section 2.1), then presents the mathematical foundations of hyperbolic representation learning in Section 2.2. more thorough overview of the Poincare Ball and Hyperboloid models can be found in Ganea et al. (2018); Shimizu et al. (2021); Bdeir et al. (2024). 2.1 REINFORCEMENT LEARNING We formalize RL as discrete MDP = S, A, P, R, γ with state space and action space A. At each time step t, the agent observes state and selects an action with its policy π : [0, 1]A. The environment generates reward via its reward function : and transitions to the next state according to the transition kernel : [0, 1]. The agent maximizes discounted future rewards J(π) = Eπ [(cid:80) t=0 γtr(st, at) π], where γ [0, 1) is discount factor determining how much the agent values future rewards (Sutton & Barto, 2018). 2 PPO Proximal Policy Optimization (PPO) (Schulman et al., 2017) is an actor-critic algorithm directly maximizing cumulative reward via gradient ascent on surrogate objective. It replaces the hard trust-region constraint of Trust-Region Policy Optimization (TRPO) (Schulman et al., 2015) with the clipped objective (cid:104) CLIP(θ) = ˆEt min(rt(θ)At , clamp(rt(θ), 1 ϵ, 1 + ϵ) At (1) πθold (atst) are importance sampling ratios of policies parameterized by θ, and ˆEt is the where rt(θ) = πθ(atst) empirical mean with respect to the samples generated in episode t. The min-clamping in Equation 1 truncates the incentive to move probability ratios beyond [1 ϵ, 1 + ϵ], acting as an unconstrained proxy for TRPOs KL-divergence trust region (see Appendix B.1). , (cid:105)"
        },
        {
            "title": "2.2 HYPERBOLIC REPRESENTATION LEARNING",
            "content": "Hyperbolic Geometry In this work, we employ two common models of hyperbolic space: the Poincare Ball and the Hyperboloid. The two isometrically equivalent (distance-preserving) models are d-dimensional simply-connected Riemannian submanifolds (M, g) with constant negative sectional curvature (see Figure 4), with R>0. Poincare Ball The d-dimensional Poincare Ball is defined as the Riemannian submanifold (Pd with Pd of inner products u, vx : TxPd tangent spaces TxPd preserving) to the Euclidean space with conformal factor λc ), is given by the collection u, that smoothly varies between . That is, the Poincare Ball is conformal (anglec = (cid:8)(x1, . . . , xd) Rd : x2 < 1 TxPd (cid:9). Its Riemannian metric gPd R, (u, v) (cid:55) λc with base points Pd , gPd = 2 1c x2 . (Hd the forward sheet The d-dimensional Hyperboloid, often called Lorentz manifold, Hyperboloid fined as (cid:8)(x0, . . . , xd) Rd+1 : , x0 > 0(cid:9) and x, xL = x2 Minkowski inner product. It is endowed with the Riemannian metric gHd ing the Minkowski inner product to the tangent spaces TxHd R, (u, v) (cid:55) u, vL. In this work, we frequently refer to the first component x0 of Hd component and to the other components x1:d as space component. is dec = is the that arises when restrictc as time ) of two-sheeted Hyperboloid, where Hd , i.e. u, vx : TxHd x, xL = 1 1 + + x2 TxHd 0 + x2 , gHd c Hyperbolic Encoding In our experiments, we retrieve hyperbolic latent representations by first mapping Euclidean vectors Rd to the tangent space at the manifolds origin 0, followed by applying the exponential map at the origin exp0, to project it onto the manifold M. This process can exp0 M. The exponential map at the origin exp0 : T0M be summarized as Rd ϕ maps vectors T0M to the manifold such that the curve [0, 1] (cid:55) exp0(tv) is geodesic (shortest path) joining the manifolds origin 0 and exp0(v). The specific mapping functions are: T0M Poincare Ball: The origin 0 is the Euclidean origin 0, i.e. ϕ is the identity function and the exponential map at the origin is exp0 : (cid:55) tanh( v) v. Hyperboloid: The origin is 0 = (1/ c, 0, . . . , 0). The map ϕ projects Euclidean vector v, 0L = 0(cid:9) by setting its first = (cid:8)v Rd+1 : Rd onto the tangent space T0Hd coordinate to zero, i.e. ϕ : (cid:55) (0, v). The exponential map at the origin is exp0 : (cid:55) cosh (cid:17) 0 + sinh (cid:16)(cid:112)c v, vL (cid:16)(cid:112)c v, vL (cid:17) . v,vL Hyperbolic Multinomial Logistic Regression For the policy and value function of our PPO agent, we compute the Multinomial Logistic Regression (MLR) (Lebanon & Lafferty, 2004; Shimizu et al., 2021; Bdeir et al., 2024) in hyperbolic space. The method computes the probability p(y = x) of an input Rd belonging to specific class {1, . . . , K}: p(y = x) exp(vzk,rk (x)), vzk,rk (x) = zkTpk dM(x, Hzk,rk ). (2) Here, exp(vzk,rk (x)) is the logit for class and vzk,rk (x) the signed distance to the margin hyperplane Hzk,rk with learnable parameters zk Rd, rk specifying the normal and shift vector pk, respectively. The specific definitions for these parameters and the hyperplane itself depend on the hyperbolic model. We expand on this further in Appendix B.3. 3 Hyper++ (Ours) Hyper + S-RYM"
        },
        {
            "title": "Euclidean",
            "content": "(a) (e) (b) (f) (c) (g) (d) (h) Figure 2: PPO training metrics. Unregularized agents (Hyper, Euclidean) lose entropy and show unstable updates (higher update KL and clip fraction), with lower returns and larger gradients (BigFish). Hypers conformal factor explodes. In contrast, HYPER++ uses the Hyperboloid, which has no conformal factor. Metrics are means over six seeds with one standard deviation."
        },
        {
            "title": "3 DIAGNOSING ISSUES WITH HYPERBOLIC PPO AGENTS",
            "content": "In this section, we analyze training issues of hyperbolic PPO agents (Section 3.1). We link these issues to the gradients of common hybrid neural network architectures as used in Cetin et al. (2023) in Sections 3.2, 3.3, and 3.4. These networks consist of shared Euclidean encoder with only the last layers for the actor and the critic being hyperbolic (cf. Figure 11 in the Appendix). Appendices B.3, B.3.1, and B.3.2 contain additional background on the MLR formulations of the Poincare Ball and the Hyperboloid. 3.1 PPO OPTIMIZATION PPOs clipped surrogate objective (Eq. 1) restricts the per-sample importance sampling ratios and acts as heuristic trust region (Schulman et al., 2017). high clipping fraction indicates many samples are at the trust region boundary. Crucially, PPO constrains ratios only on the sampled states in batch. Gradient steps leading to large policy changes on unseen states remain unconstrained, so the heuristic trust region can fail. This cross-state interference can produce large unintended policy shifts beyond the sampled states in the batch (Moalla et al., 2024). Figure 2 shows key training metrics for hyperbolic PPO training in the BIGFISH environment (top row). As noted by Cetin et al. (2023), unregularized hyperbolic PPO is prone to early entropy collapse in Plot 2a. This coincides with rapid rise in entropy variance across batch states, producing large policy updates that potentially interfere (Figure 2b). Figures 2c and 2d confirm: unregularized agents experience larger update KL-divergence and more trust-region violations. Cetin et al. (2023) propose to mitigate this with S-RYM, combination of Euclidean embeddings scaled by 1/ and SpectralNorm to bound the encoders Lipschitz constant (Hyper+S-RYM in Fig. 2). In comparison, our method (Section 4) achieves lower update KL and markedly less clipping while avoiding the overhead of SpectralNorm and instabilities from the conformal factor. 3.2 GRADIENT ANALYSIS PRELIMINARIES To explain the trust-region instability of the hyperbolic agent, we follow Cetin et al. (2023) and analyze the gradients of the last encoder layer (Fig. 2g). Figure 2f shows that the conformal factor of the Poincare Ball λc 1c x2 is key driver for inducing instability. In the following, we derive closed-form, curvature-aware gradients for core hyperbolic layers and maps to study optimization failure points, extending Guo et al. (2022); Mishne et al. (2023) with new expressions for PPO. Below, = 2 4 we present the gradient with respect to the last Euclidean layer weights WE for generic loss L. W = vz,r(xH) vz,r(xH) xH xH xE xE , (3) where vz,r denotes the score function of any hyperbolic multinomial regression (MLR) layer, xE are the Euclidean embeddings from the encoder, and xH = exp0(xE) are the embeddings represented as tangent vectors mapped to hyperbolic space. For the Poincare MLR layer used by Cetin et al. (2023), Guo et al. (2022) have shown that backpropagating through the exponential map yields vanishing gradients near the boundary because the Riemannian gradient scales with the inverse conformal factor gradient: xH λc xH = 4c xH (cid:0)1 cxH2(cid:1)2 . (4)"
        },
        {
            "title": "3.3 GRADIENT ANALYSIS FOR HYPERBOLIC NETWORK++ MLR",
            "content": "The derivative (Appendix A.2) of the HNN++ MLR formulation (Shimizu et al., 2021) with respect to its input xH is: xH vHNN++ z,r (xH) = 2z 1 (cid:112)1 + (xH)2 2 (xH) = xH cosh(2 1 cxH2 cr) ˆz + xH (xH) , where (cid:16) 4 xH sinh( cr) + cosh(2 (cid:17) cr) ˆz, xH (1 cxH2)2 (5) . where ˆz = z/z is the (normalized) Euclidean weight vector of the layer and is scalar bias term. The problematic term is the denominator (1 cxH2)2 in (xH)/xH. It arises from the gradient of the conformal factor (Eq. 4), which causes gradient explosion near the Poincare Ball boundary as xH 1/ xH and alter the hyperbolic geometry by shifting decision boundaries, leading to performance plateaus. Hence, while HNN++ removes over-parameterization (Shimizu et al., 2021), it does not, by itself, resolve PPO training instabilities. xH is undesirable because HNN++ MLR logits depend on λc c. Clipping λc Next, we analyze the Jacobian of the Poincare Ball exponential map xH xE (Appendix A.1): similar to Guo et al. (2022) xH xE = xE exp0(xE) = tanh( cxE) cxE (cid:18) sech2( cxE) xE + tanh( cxE2 cxE) (cid:19) xEx xE . Although the exponential map Jacobian decays like O(xE1), the directional term (second summand) is highly sensitive to growing xE. Figures 2g and 2h show how volatile layer-wise gradients can get during training without proper handling. Cetin et al. (2023)s S-RYM scaling factor xE (cid:55) xE/ keeps xE moderate, preventing exp0(xE)/xE from destabilizing the learning signal fed back to the encoder (Eq. 3) while reducing directional variability. Hence, regularizing Euclidean embeddings before the hyperbolic layers is necessity for stable hyperbolic PPO agents. 3.4 GRADIENT ANALYSIS FOR HYPERBOLOID MLR Prior work establishes that the Hyperboloid trains more stably than the Poincare Ball (Mettes et al., 2024; Mishne et al., 2023; Bdeir et al., 2024) for two reasons. First, the Hyperboloid MLR score (Eq. 26) contains no conformal factor as it is not conformal to Euclidean space. Second, it neither multiplies nor divides by the Euclidean feature norm. As result, its gradients avoid the instabilities of the Poincare Ball. However, we will show in the following that the Jacobian xH 0(v) of the Hyperboloids exponential may still destabilize training. We denote = [0, xE] T0M as the Euclidean embeddings mapped into the tangent space of the Hyperboloid (cf. Section 2): = expc (cid:34) xH = 0 0 sinh( cxE) cxE sinh( cxE cosh( cxE) xE cxE)sinh( cxE3 (cid:35) . (6) cxE) xEx Id + cxE) grow exponentially, i.e., faster rate than cxE) Equation 6 is (1+s)(1+d) matrix, where the first column is zero. For large xE, sinh( and cosh( cxE. Thus, the Hyperboloid exponential map can destabilize gradients when Euclidean feature norms grow, requiring regularization of xE. Summarizing the findings in this section, we arrive at more nuanced understanding of the training issues of hyperbolic deep RL agents: Policy breakdown and large-norm gradients in the encoder are function of the hyperbolic layers used in the actor and the critic. The conformal factor, in particular, is source of numerical instability in Riemannian optimization methods (Guo et al., 2022; Mishne et al., 2023). This numerical instability gets exacerbated by noisy gradients in actor-critic training, particularly from the critics side (Sutton & Barto, 2018; Nauman et al., 2024a). In the next section, we will show how our method HYPER++ deals with these issues."
        },
        {
            "title": "4 STABILIZING HYPERBOLIC DEEP RL",
            "content": "In this part, we establish the components of our agent HYPER++: Section 4.1 proposes RMSNorm (Zhang & Sennrich, 2019) as an alternative to SpectralNorm. Section 4.2 introduces novel feature scaling layer. Section 4.3 discusses how these components relate to the Hyperboloid. Beyond these design choices, we use categorical loss to stabilize critic gradients (Imani & White, 2018; Farebrother et al., 2024) and to resolve an architectural mismatch in hyperbolic value learning. While Euclidean linear layers naturally support MSE regression over continuous values, hyperbolic MLR layers output classification-oriented hyperplane distances, making the categorical loss over discrete bins better geometrical fit. Collectively, our components target complementary sources of instability in Equation 3: the categorical loss stabilizes the loss derivative (first term), Hyperboloid MLR stabilizes the hyperbolic layer Jacobian (second term), and RMSNorm with feature scaling stabilizes the Jacobian of the exponential map (third term). Figure 11 illustrates the underlying hybrid network architecture (Guo et al., 2022; Cetin et al., 2023) analyzed in the following. 4.1 REGULARIZATION Here, we study how SpectralNorm (Miyato et al., 2018) affects the Euclidean embeddings produced by the encoder (cf. Figure 11). To this end, consider Lemma 4.1 which provides bound on the norm of the embeddings computed by single layer, depending on the input norm: Lemma 4.1. Let Rn, Rdn and Rd. Then, for any function : Rd Rd with Lipschitz constant L, it holds that (W + b)2 (0)2 + LW 2x2 + Lb2 . In particular, for ReLU activation functions and any normalized weight matrix ˆW , we have (cid:13) (cid:13)ReLU( ˆW + b) (cid:13) (cid:13) (cid:13) (cid:13)2 x2 + b2 . (7) (8) Lemma 4.1 shows that for multi-layer encoders such as the one used by Cetin et al. (2023), applying SpectralNorm only to the last (linear) layer of the encoder is not sufficient to prevent the Euclidean embedding norms from growing via the preceding layers. To tangibly affect these norms, SpectralNorm must be applied to every layer of the encoder (Cetin et al., 2023). This constrains the Lipschitz constant of all layers and reduces expressivity by globally enforcing smoothness (Rosca et al., 2020; Cetin et al., 2023). Additionally, SpectralNorm incurs computational overhead from the power-iteration steps needed at each forward pass. Ideally, we want to use regularization via spectral normalization only where needed and such that we can guarantee stable training, without limiting the expressivity of the entire Euclidean encoder. Proposition 4.2 shows that applying RMSNorm (Zhang & Sennrich, 2019) before the activation of the encoders last linear layer achieves stability without overly restricting its representational capacity (if the other layers are not regularized, their expressivity is not limited). Proposition 4.2. Let Rd and : Rd Rd with Lipschitz constant L. Then, for ˆx = 1 f (RMS(x)), it holds that: ˆx2 < 1 (0)2 + L, λexp0( ˆx) < 2 cosh2 (cid:18) (cid:18) 1 (cid:19)(cid:19) (0)2 + . (9) 6 tanh( Proposition 4.2 ensures stable hyperbolic operations for broad class of activation functions. For common 1-Lipschitz activations such as TanH and ReLU, the bounds reduce to ˆx2 < 1 and exp0( ˆx) < 1 c). Unlike SpectralNorm, which constrains every encoder layer, we only require applying RMSNorm to the pre-activation output embeddings of the final linear layer. This retains the expressivity of each encoder layer. We use RMSNorm (Zhang & Sennrich, 2019) rather than LayerNorm (Ba et al., 2016) because we do not want the mean-centering in LayerNorm to distort the hierarchical structure of the hyperbolic embeddings. Additionally, RMSNorm brings three further advantages: it smoothes gradients, prevents dead ReLU or saturated TanH units (Zhang & Sennrich, 2019; Xu et al., 2019; Lyle et al., 2024), and supports arbitrary embedding dimensions d, since the bound in Proposition 4.2 is dimension-independent for activation functions with fixed point 0."
        },
        {
            "title": "4.2 LEARNED EUCLIDEAN FEATURE SCALING",
            "content": "y Poincare Ball = 1 Proposition 4.2 guarantees stability by bounding both Euclidean embedding norms and the conformal factor. However, this may still affect representational capacity in the hyperbolic layers of the agent. For example, with ReLU as the last encoder layers activation function and curvature = 1, the bound restricts the Poincare Ball radius to xH2 0.76 (see the proof of Proposition 4.2). Since the volume of d-ball scales as Vd(r) = πd/2 rd rd, 2 +1) even modest restriction of the radius causes an exponential loss of available volume in d. To mitigate this, we rescale the Euclidean tangent embeddings obtained after application of Proposition 4.2 ˆxE by learnable scalar ξθ: Figure 3: Learned scaling effect. RMS+1/ Learned Γ( ˆxrescale = ρmax σ(ξθ) ˆxE, ρmax = atanh(α) , (10) )2 α/ where σ denotes the sigmoid function. By choosing this particular form for ρmax , we have that exp0( ˆxrescale ρmax) = α. Setting α = 0.95 (and = 1) expands the usable ball radius from 0.76 to 0.95, i.e., volume gain of (0.95/0.76)d. For = 32, this is approximately 1.2 103 more volume while still preventing the explosion of the conformal factor according to Proposition 4.2. Figure 3 illustrates the effect in 2D. since tanh( 4.3 HYPERBOLOID MODEL"
        },
        {
            "title": "Hn\nc",
            "content": "Isometry = Learned boundary Section 3.4 shows that the Hyperboloid avoids conformal factor instabilities and is therefore more robust against large norms. Yet, operations can become ill-conditioned far from the origin, i.e., when the sheet approaches the asymptotic null cone, and the Jacobian of the exponential map in Equation 6 gets more sensitive to large Euclidean norms. Since the Poincare Ball and the Hyperboloid are isometric  (Fig. 4)  models, our stabilization strategy transinstead of capping the Poincare Ball radius, fers: Figure 4: Isometry between Poincare Ball we propose to apply RMSNorm and feature scaling and Hyperbolid. before the last Euclidean activation to bound the Hyperboloid through its time component x0. Corollary 4.3 formalizes this insight by combining Proposition 4.2 with the Poincare Ball-Hyperboloid isometry (Chami et al., 2021; Mishne et al., 2023). Corollary 4.3. Let ˆxE Rn be point regularized by RMSNorm with learnable scaling, and xH = exp0( ˆxE) Pn. Then, the maximum value of the time component x0 of that point on the Hyperboloid is"
        },
        {
            "title": "Pn\nc",
            "content": "xmax 0 = 1 + cxH2 (1 cxH2) = 1 + tanh2( (cid:0)1 tanh2( ˆxE) ˆxE)(cid:1) . Figure 5: Normalized test rewards on ProcGen. HYPER++ outperforms baselines for all aggregation methods without increasing variance (as measured by the bootstrap confidence interval). We report median, interquartile mean (IQM), mean, and optimality gap, which is 1 IQM. 0 Since the time and space components are dependent (cf. Section 2.2), bounding the maximum norm of xmax also ensures that the space component xs remains bounded. Therefore, we also apply regularization with RMSNorm and learned scaling when training agents using the Hyperboloid. In Section 5.2, we show that this approach works well empirically. Our proposed agent HYPER++ is visualized in Figure 11 (Appendix) consists of the following components: HYPER++ HYPER++ tackles optimization issues in hyperbolic deep RL through formal and empirical analysis of training dynamics: 1. RL nonstationarity = Categorical value function. 2. Growing Euclidean feature norms = RMSNorm + Feature scaling. 3. Conformal factor instability = Hyperboloid model."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate HYPER++ on ProcGen (Cobbe et al., 2020) with PPO (Schulman et al., 2017) in Section 5.1 and provide ablation studies in Section 5.2. We test performance with the off-policy algorithm DDQN (van Hasselt et al., 2016) on subset of Atari games (Bellemare et al., 2015; Towers et al., 2024; Aitchison et al., 2023). Unless stated otherwise, error bands show one standard deviation. Wall-clock times are reported in Appendix D.1. 5.1 PROCGEN Figure 5 shows aggregate test rewards using normalized test rewards for 25M time steps on ProcGen. We normalize using random performance as the minimum and either theoretical or empirically determined maximum (Cobbe et al., 2020). We use the rliable library (Agarwal et al., 2021) to compute aggregate metrics such as the interquartile-mean (IQM) and the optimality gap with bootstrap confidence intervals. HYPER++ outperforms Poincare agents with and without S-RYM, as well as the Euclidean baseline. The per-game curves in Figure 6 match our findings: hyperbolic agents beat Euclidean ones on BIGFISH and DODGEBALL, while performance on STARPILOT and FRUITBOT saturates near the ceiling for all methods. Tables 6 and 7 show HYPER++ winning head-to-head vs. Hyper+S-RYM in 8/16 games on the train set and 11/16 games on the test set. Figure 6: Learning curves for PPO on ProcGen. We report the mean test rewards over six seeds on the same environments as Cetin et al. (2023)."
        },
        {
            "title": "5.2 ABLATION STUDIES",
            "content": "Figure 7 presents ablations of HYPER++s components using test IQM with bootstrapped confidence intervals. We begin with the most critical component: normalization. Removing RMSNorm (Zhang & Sennrich, 2019) and 1/ feature scaling causes complete learning failure (-RMSNorm), confirming the predictions of Proposition 4.2. This failure manifests as large embedding norms and near-zero gradients in the encoders final layer (Figure 14), providing empirical support for the theoretical analysis in Section 3 and Proposition 4.2. The next most important architectural choice is learned scaling (-Scaling), which we attribute to its synergy with RMSNorm. Among the loss function variants, replacing the categorical HLGauss loss (Imani & White, 2018) with MSE (+MSE) degrades performance, though not uniformly across all games. This aligns with the findings of Farebrother et al. (2024), who similarly observe that HL-Gauss does not consistently improve performance on all environments. Interestingly, substituting the C51 (Bellemare et al., 2017) distributional loss for HL-Gauss performs even worse than MSE. Using the Poincare ball instead of the hyperboloid model (+Poincare) leads to modest drop in performance, which is expected given their isometry (Corollary 4.3). Ablation studies on ProcGen with Figure 7: Hyperbolic geometry. We report interquartile mean (IQM) across six seeds with bootstrap confidence intervals. indicates that component is removed from HYPER++, + indicates component replacing its analog. the test We further validate Lemma 4.1 by testing SpectralNorm (Miyato et al., 2018) as an alternative to RMSNorm in two configurations: applying SpectralNorm to the complete Euclidean encoder (HYPER++ (SN Full)) and applying it only to the penultimate layer (HYPER++ (SN Penultimate)). In both cases, the agent fails to learn entirely. This underscores the critical importance of RMSNorm for obtaining the bounded feature norms guaranteed by Proposition 4.2. Finally, Figure 8 isolates the contribution of hyperbolic representations by evaluating Euclidean agents equipped with HL-Gauss, RMSNorm, and our full regularization combination. For Euclidean representations, the HL-Gauss loss (Euclidean+Categorical) performs worse than MSE. Adding RMSNorm to Euclidean agents improves performance, and equipping Euclidean agents with our full method yields an IQM of 0.35, which is slightly better than HYPER++ with the Poincare ball (IQM=0.34). However, HYPER++ with the Hyperboloid achieves the best overall performance (IQM=0.40). The underperformance of Euclidean+HL-Gauss relative to Euclidean+MSE indicates that categorical losses are particularly well-suited for hyperbolic agents, due to being better geometric fit for the signed distances produced by the critics hyperbolic MLR layer. Overall, our results demonstrate that hyperbolic representations can benefit deep RL agents, but require an optimization-friendly approach to hyperbolic geometry to realize these benefits. We present complete ablation results in Tables 9 and 10. In summary, every ablation underperforms HYPER++ with the Hyperboloid, confirming the synergistic interactions between hyperbolic geometry and our methods components. Figure 8: Ablation studies on ProcGen with Euclidean geometry. We report the test interquartile mean (IQM) across six seeds with bootstrap confidence intervals. 9 Figure 9: Human-normalized performance for DDQN on Atari-5. All agents are trained for 10M steps and five seeds. HYPER++ strongly improves over the baselines."
        },
        {
            "title": "5.3 ATARI DDQN",
            "content": "We evaluate our algorithm using the value-based off-policy algorithm DDQN (van Hasselt et al., 2016) (Appendix B.2). We focus on the Atari-5 subset of games (Aitchison et al., 2023), which consists of NAMETHISGAME, PHOENIX, BATTLEZONE, DOUBLE DUNK, and Q*BERT. This subset has been shown to be the most predictive of overall performance across all Atari environments (Bellemare et al., 2015; Towers et al., 2024). We train each agent for 10M steps and five random seeds. Figure 9 shows the final episode rewards achieved by each method. HYPER++ substantially outperforms the baselines across all five games in all metrics. Appendix E.5 provides the full learning curves for each individual game. We find that performance varies across games: our method achieves its strongest gains on NAMETHISGAME and Q*BERT. On PHOENIX, HYPER++ exhibits strong initial performance but subsequently plateaus, mirroring the behavior of the baseline agents. This plateauing is consistent with plasticity loss being confounding factor on this particular game (Klein et al., 2024). To further assess the generality of our modifications, we conduct an ablation study on NAMETHISGAME using Polyak updates for the target network instead of the standard hard replacement updates. We perform this ablation on NAMETHISGAME, as it is the single most representative game in the Atari benchmark according to Aitchison et al. (2023). As shown in Figure E.6, Polyak updates introduce only minor differences in performance, suggesting that our method is robust to this design choice."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Hyperbolic deep learning has progressed quickly from early hyperbolic neural networks and Riemannian optimization (Ganea et al., 2018; Becigneul & Ganea, 2019), which Cetin et al. (2023) adapt to RL. Parameter redundancy in Poincare Ball MLR was reduced by Shimizu et al. (2021). Fully hyperbolic architectures on the Hyperboloid now include transformers and convolutional networks, as well as Hyperboloid MLR layer (Chen et al., 2022; Bdeir et al., 2024). Mettes et al. (2024) survey this literature from vision perspective. Optimization and numerical stability have been analyzed independently (Mishne et al., 2023; Guo et al., 2022). We study optimization problems of hyperbolic networks within RL and propose principled solution. Reinforcement learning. We focus on PPO (Schulman et al., 2017), which remains under active study (Andrychowicz et al., 2021; Moalla et al., 2024) because of its strong performance. Several works show that regularization can improve deep RL training; with LayerNorm being widely adopted in the deep RL (Henderson et al., 2018; Ba et al., 2016; Lyle et al., 2023; Nauman et al., 2024b; Lee et al., 2025; Gallici et al., 2025). Instead, we regularize our agent using RMSNorm (Zhang & Sennrich, 2019), preventing interference with hyperbolic representations. separate line of research is stabilizing value function learning via categorical objectives (Bellemare et al., 2017; Schrittwieser et al., 2020; Imani & White, 2018; Farebrother et al., 2024), which we extend to hyperbolic PPO."
        },
        {
            "title": "7 LIMITATIONS AND CONCLUSION",
            "content": "Limitations and Future Work. Our analysis takes an optimization-centric view, focusing on training dynamics and the question of how hyperbolic deep RL learns, rather than what structures their representations capture. We also do not address which environments are most suited to hyperbolic representations. Moreover, the interaction between geometric choices and the design of different deep RL algorithms, remains unexplored. Each of these directions is an exciting avenue for future work. Conclusion. Our work analyzes gradients in the Poincare Ball and Hyperboloid, linking large-norm embeddings to PPO trust-region breakdowns. Based on these insights, we introduce HYPER++, which combines RMSNorm with learned feature scaling and categorical value loss to stabilize hyperbolic 10 deep RL. On ProcGen, HYPER++ improves performance and substantially reduces wall-clock time compared to existing hyperbolic PPO agents. Our findings transfer to Atari and DDQN with strong gains, indicating broader applicability beyond PPO."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "We will make our code publicly available on GitHub. Appendix contains the derivations for our gradient analysis and proofs for our theoretical results. In Appendix D, we state agent architecture, hyperparameters, relevant implementation details, and hardware used. In Appendix C.3 we discuss differences in results to existing works. USAGE OF LARGE LANGUAGE MODELS (LLMS) During this project, we used LLMs as an assistive tool. In the early stages of our project, we used LLMs for literature search and paper summarization. During the implementation phase, we used code assistants to support repetitive coding tasks such as Matplotlib figure generation. For the paper, LLMs were used as tool to iterate on our writing. An example use case is paragraph shortening with Shorten this paragraph. All LLM outputs used in this paper were thoroughly reviewed to ensure accuracy. LLMs were not used for idea generation, experimental design, or for proofs. Mathematical expressions were derived independently. ETHICS STATEMENT Our work advances the fundamental capabilities of hyperbolic deep RL agents and has no direct ethical implications by itself. We cannot rule out that unethical uses could occur in downstream applications because RL and PPO, in particular, are used to train LLMs, and hyperbolic embeddings are well-suited for text data. However, such uses would require significant extensions and modifications beyond the work submitted here. AUTHOR CONTRIBUTIONS If youd like to, you may include section for author contributions as is done in many journals. This is optional and at the discretion of the authors. ACKNOWLEDGMENTS This work has been funded in parts by the Vienna Science and Technology Fund (WWTF) [10.47379/ICT20058]. We acknowledge EuroHPC JU for awarding the project ID EHPC-DEV2025D08-024 access to the Luxembourg national supercomputer, MeluXina, and the Spanish supercomputer, MareNostrum. The authors gratefully acknowledge the LuxProvide and Barcelona Supercomputing Center for their expert support. Without Nikolaus Suß tireless work maintaining our research groups computing infrastructure, this work would not have been possible. We are deeply grateful."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C. Courville, and Marc G. Bellemare. Deep reinforcement learning at the edge of the statistical precipice. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 2930429320, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ f514cec81cb148559cf475e7426eed5e-Abstract.html. Matthew Aitchison, Penny Sweetser, and Marcus Hutter. Atari-5: Distilling the arcade learning environment down to five games. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 421438. PMLR, 2023. URL https://proceedings.mlr. press/v202/aitchison23a.html. 11 Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem. What matters for on-policy deep actor-critic methods? large-scale study. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=nIAxjsniDzg. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450. Ahmad Bdeir, Kristian Schwethelm, and Niels Landwehr. Fully hyperbolic convolutional neural networks for computer vision. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=ekz1hN5QNh. Gary Becigneul and Octavian-Eugen Ganea. Riemannian adaptive optimization methods. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= r1eiqi09K7. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents (extended abstract). In Qiang Yang and Michael J. Wooldridge (eds.), Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 41484152. AAAI Press, 2015. URL http://ijcai.org/Abstract/15/585. Marc G. Bellemare, Will Dabney, and Remi Munos. distributional perspective on reinforceIn Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th Internament learning. tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 449458. PMLR, 2017. URL http://proceedings.mlr.press/v70/bellemare17a.html. Janos Bolyai. The Science Absolute of Space..., volume 3. The Neomon, 1896. Edoardo Cetin, Benjamin Paul Chamberlain, Michael M. Bronstein, and Jonathan J. Hunt. Hyperbolic deep reinforcement learning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=TfBHFLgv77. Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 48694880, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 0415740eaa4d9decbc8da001d3fd805f-Abstract.html. Ines Chami, Albert Gu, Dat Nguyen, and Christopher Re. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In International Conference on Machine Learning, pp. 14191429. PMLR, 2021. Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 56725686. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.389. URL https://doi.org/10.18653/v1/2022.acl-long.389. Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 20482056. PMLR, 2020. URL http://proceedings. mlr.press/v119/cobbe20a.html. 12 Aleksandr Ermolov, Leyla Mirvakhabova, Valentin Khrulkov, Nicu Sebe, and Ivan V. Oseledets. Hyperbolic vision transformers: Combining improvements in metric learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 73997409. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00726. URL https://doi.org/10.1109/CVPR52688.2022.00726. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: scalable distributed deep-rl with importance weighted actor-learner architectures. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 14061415. PMLR, 2018. URL http: //proceedings.mlr.press/v80/espeholt18a.html. Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taıga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep RL. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=dVpFKfqF3R. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=7IzeL0kflu. Octavian-Eugen Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada, pp. 53505360, 2018. URL https://proceedings.neurips.cc/ paper/2018/hash/dbab2adc8f9d078009ee3fa810bea142-Abstract.html. Gromov. Hyperbolic groups. Essays in Group Theory, pages/Springer-Verlag, 1987. Yunhui Guo, Xudong Wang, Yubei Chen, and Stella X. Yu. Clipped hyperbolic classifiers In IEEE/CVF Conference on Computer Vision and Pattern are super-hyperbolic classifiers. Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 110. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00010. URL https://doi.org/10.1109/CVPR52688. 2022.00010. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Sheila A. McIlraith and Kilian Q. Weinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 32073214. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11694. URL https://doi.org/10.1609/aaai.v32i1.11694. Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Joao G. M. Araujo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. J. Mach. Learn. Res., 23:274:1274:18, 2022. URL https://jmlr.org/papers/v23/21-1342.html. Ehsan Imani and Martha White. Improving regression performance with distributional losses. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 21622171. PMLR, 2018. URL http://proceedings.mlr.press/v80/imani18a.html. Isay Katsman and Anna Gilbert. Shedding light on problems with hyperbolic graph learning. Trans. Mach. Learn. Res., 2025, 2025. URL https://openreview.net/forum?id= rKAkp1f3R7. 13 Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Timo Klein, Lukas Miklautz, Kevin Sidak, Claudia Plant, and Sebastian Tschiatschek. Plasticity loss in deep reinforcement learning: survey. CoRR, abs/2411.04832, 2024. doi: 10.48550/ARXIV. 2411.04832. URL https://doi.org/10.48550/arXiv.2411.04832. Guy Lebanon and John D. Lafferty. Hyperplane margin classifiers on the multinomial manifold. In Carla E. Brodley (ed.), Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004), Banff, Alberta, Canada, July 4-8, 2004, volume 69 of ACM International Conference Proceeding Series. ACM, 2004. doi: 10.1145/1015330.1015333. URL https://doi.org/ 10.1145/1015330.1015333. Hojoon Lee, Dongyoon Hwang, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter R. Wurman, Jaegul Choo, Peter Stone, and Takuma Seno. Simba: Simplicity bias for scaling up parameters in deep reinforcement learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=jXLiDKsuDo. Nikolaı Ivanovich Lobachevskiı. Geometrical researches on the theory of parallels. University of Texas, 1891. Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 2319023211. PMLR, 2023. URL https: //proceedings.mlr.press/v202/lyle23b.html. Clare Lyle, Zeyu Zheng, Khimya Khetarpal, James Martens, Hado Philip van Hasselt, Razvan Pascanu, and Will Dabney. Normalization and effective learning rates in reinforcement learning. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/c04d37be05ba74419d2d5705972a9d64-Abstract-Conference.html. Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hierarchical representations with poincare variational auto-encoders. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1254412555, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 0ec04cb3912c4f08874dd03716f80df1-Abstract.html. Pascal Mettes, Mina Ghadimi Atigh, Martin Keller-Ressel, Jeffrey Gu, and Serena Yeung. HyInt. J. Comput. Vis., 132(9):3484 perbolic deep learning in computer vision: survey. 3508, 2024. doi: 10.1007/S11263-024-02043-5. URL https://doi.org/10.1007/ s11263-024-02043-5. Gal Mishne, Zhengchao Wan, Yusu Wang, and Sheng Yang. The numerical stability of hyperbolic representation learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 2492524949. PMLR, 2023. URL https://proceedings.mlr.press/ v202/mishne23a.html. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=B1QRgziT-. 14 Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nat., 518(7540):529533, 2015. doi: 10.1038/NATURE14236. URL https://doi.org/10.1038/nature14236. Skander Moalla, Andrea Miele, Daniil Pyatko, Razvan Pascanu, and Caglar Gulcehre. No representation, no trust: Connecting representation, collapse, and trust issues in PPO. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 81166fbd9cc5adf14031cdb69d3fd6a8-Abstract-Conference.html. Michal Nauman, Michal Bortkiewicz, Piotr Milos, Tomasz Trzcinski, Mateusz Ostaszewski, and Marek Cygan. Overestimation, overfitting, and plasticity in actor-critic: the bitter lesson of reinforcement learning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview. net/forum?id=5vZzmCeTYu. Bigger, regularized, optimistic: Michal Nauman, Mateusz Ostaszewski, Krzysztof Jankowski, Piotr Milos, and Marek Cyscaling for compute and sample efficient congan. tinuous control. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http://papers.nips.cc/paper_files/paper/2024/hash/ cd3b5d2ed967e906af24b33d6a356cac-Abstract-Conference.html. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ b1efde53be364a73914f58805a001731-Abstract-Conference.html. Avik Pal, Max van Spengler, Guido Maria DAmely di Melendugno, Alessandro Flaborea, Fabio Galasso, and Pascal Mettes. Compositional entailment learning for hyperbolic vision-language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/ forum?id=3i13Gev2hV. Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. case for new neural network smoothness constraints. In Jessica Zosa Forde, Francisco J. R. Ruiz, Melanie F. Pradier, and Aaron Schein (eds.), Cant Believe Its Not Better! at NeurIPS Workshops, Virtual, December 12, 2020, volume 137 of Proceedings of Machine Learning Research, pp. 2132. PMLR, 2020. URL https://proceedings.mlr.press/v137/rosca20a.html. Omar Salemohamed, Edoardo Cetin, Sai Rajeswar, and Arnab Kumar Mondal. Hyperbolic deep reinforcement learning for continuous control. In Krystal Maughan, Rosanne Liu, and Thomas F. Burns (eds.), The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum? id=Mrz9PgP3sT. Rik Sarkar. Low distortion delaunay embedding of trees in hyperbolic plane. In International symposium on graph drawing, pp. 355366. Springer, 2011. 15 Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with learned model. Nat., 588(7839):604609, 2020. doi: 10.1038/S41586-020-03051-4. URL https://doi.org/10. 1038/s41586-020-03051-4. John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd policy optimization. International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 18891897. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/schulman15.html. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. Ryohei Shimizu, Yusuke Mukuta, and Tatsuya Harada. Hyperbolic neural networks++. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=Ec85b0tUwbA. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nat., 529(7587):484489, 2016. doi: 10.1038/NATURE16961. URL https://doi.org/10.1038/nature16961. Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction, 2nd Edition. MIT Press, 2018. URL http://www.incompleteideas.net/book/the-book-2nd. html. Mark Towers, Ariel Kwiatkowski, Jordan K. Terry, John U. Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierre, Sander Schulhoff, Jun Jet Tai, Hannah Tan, and Omar G. Younis. Gymnasium: standard interface for reinforcement learning environments. CoRR, abs/2407.17032, 2024. doi: 10.48550/ ARXIV.2407.17032. URL https://doi.org/10.48550/arXiv.2407.17032. Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp. 2094 2100. AAAI Press, 2016. doi: 10.1609/AAAI.V30I1.10295. URL https://doi.org/10. 1609/aaai.v30i1.10295. Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 43834393, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 2f4fe03d77724a7217006e5d16728874-Abstract.html. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1236012371, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 1e8a19426224ca89e83cef47f1e7f53b-Abstract.html."
        },
        {
            "title": "Appendix",
            "content": "Table 1 summarizes the contents of our Appendix: Table 1: Structure of our appendix. Appendix Section Content Appendix Appendix Appendix Appendix Appendix Proofs & Derivations Additional Background for RL and hyperbolic MLR Layers Environment descriptions Compute details and hyperparameters Complete results and additional metrics"
        },
        {
            "title": "A DERIVATIONS AND PROOFS",
            "content": "This section contains derivations and proofs for the results in A.1 POINCAR EXPONENTIAL MAP GRADIENTS We build on the analysis by Guo et al. (2022) and derive the Jacobian of the Poincare Ball exponential map at the origin. DxH = = = xE xE exp0(xE) tanh( cxE) cxE tanh( cxE) Id + cxE xE (cid:32) xE tanh( cxE) cxE (cid:33) xE , (11) where Id denotes the identity matrix. Deriving the second term yields: xE cxE) tanh( xE cxE tanh( cxE) cxE) xE cxE sech2( cxE) ˆxE cxE) ˆxE cxE tanh( ( cxE)2 cxE tanh( cxE2 sech2( cxE) cxE tanh( cxE) ˆxE sech2( cxE) xE cxE2 tanh( cxE) xE cxE3 (cid:32) sech2( xE2 xE cxE) tanh( (cid:33) cxE) cxE3 xE , (12) = (i) = = = = where we use ˆxE = xE xE = xE xE in (i). Putting Equation 11 and 12 together yields: xE exp0(xE) = tanh( cxE) + cxE (cid:18) sech2( cxE) xE tanh( cxE) cxE2 (cid:19) xEx xE . (13) We can see that the Jacobian of the exponential map decays with O(cid:0)xE1(cid:1), although the important directional term (second summand) can vanish faster with O(cid:0)xE2(cid:1). 17 A.2 HYPERBOLIC NETWORKS++ GRADIENTS Let us first re-state the forward pass for the hyperbolic networks++ formulation (Shimizu et al., 2021) of the Poincare multinomial logistic regression (MLR) layer: vHNN++ z,r (xH) = 2 sinh1 (cid:0)(1 λc xH ) sinh(2 r) + λc xH cosh(2 r) ˆz, xH(cid:1) . (14) Here xH = exp0(xE), λc 1cxH2 is the conformal factor of the Poincare Ball, is the weight xH vector of the layer, ˆz = are the weights normalized to unit length, and scalar bias term. We omit the class index as in the main paper to simplify the notation. = 2 We can re-state Equation 14 as vHNN++ z,r (xH) = 2z sinh1 (cid:0)F (xH)(cid:1) , with (xH) = (1 λc xH ) sinh(2 r) + λc xH cosh(2 r) ˆz, xH . We first calculate the outer derivative (Equation 15): xH vHNN++ z,r (xH) = 2z 1 (cid:112)1 + (xH)2 xH (xH) . The gradient of Equation 16 xHF (xH) is: (15) (16) (17) xH (xH) = sinh(2 cr)xH λc xH + xH (cid:16) cosh( cr) ˆz + = cλc xH (cid:104) xH cλc cosh(2 cr) ˆz, xH sinh(2 (cid:16) cr) + cosh(2 cr) ˆz, xH 4 xH sinh(2 cr) + cosh(2 cr) ˆz, xH (cid:17) xH λc xH (cid:17) (cid:105) 2 cosh(2 1 cxH2 We plug in the definition of the conformal factor λc ˆz + cr) = (1 cxH2)2 . (18) = 2 1cxH2 and its derivative in the last step. xH 1 The term grow with O(cid:0)(1 cxH2)2(cid:1) for samples xH close to the boundary of the Poincare Ball. 1 in Equation 17 cannot blow up. However, the gradients in Equation 18 1+F (xH)2 A.3 HYPERBOLOID EXPONENTIAL MAP GRADIENTS The exponential map of the Hyperboloid at the origin 0 = (1/ = [0, xE] T0M to the Hyperboloid (Bdeir et al., 2024): c, 0, . . . , 0) maps tangent vector exp0(v) = (cid:34) 1 cosh( cxE), sinh( cxE) (cid:35) , xE xE (19) where the first element is scalar time component and the remaining elements constitute the space component. The derivative of the time component is + 1-dimensional vector whose first element is zero: cosh( cxE) = (cid:20) 0, sinh( cxE) (cid:21) . xE xE (20) For the derivative of the space component, we start by reformulating it as: sinh( cxE)xE cxE (cid:20) = 0, (cid:21) (cid:20) (xE)xE = 0, (xE)Id + (xE) xE (cid:21) xEx , (21) 18 where (xE) = sinh( cxE) cxE , Id is the identity matrix, and 0 Rd. For (xE), we have: (xE) = cxE cosh( cxE) sinh( cxE cxE) . (22) Plugging Equation 22 into Equation 21 yields: sinh( cxE) cxE Id + cxE cosh( cxE) sinh( cxE3 cxE) xEx (23) We arrive at the Jacobian of the exponential map by putting Equation 20 and Equation 23 together: 0 0 sinh( cxE) cxE sinh( cxE cosh( cxE) xE cxE)sinh( cxE3 (cid:35) . (24) cxE) xEx Id + xH = exp0(v) = (cid:34) A.4 PROOFS Lemma 4.1. (W + b)2 (0)2 (W + b) (0)2 (i) LW + b2 (ii) LW 2x2 + Lb2, where (i) uses the lipschitz property of and (ii) follows from the definition of the induced matrix norm. The special case follows directly by observing that ReLU is 1-Lipschitz with ReLU(0) = 0. Proposition 4.2. First, we bound the norm of the normalized feature vector. Recall that RMS(x) = x/µ(x) with µ(x) = and (cid:113) ε + x2 2. Then, RMS(x)2 2 = x2 2 x2 2 ε + < x2 2 1 x2 2 = d, ˆx2 = (cid:13) (cid:13) (cid:13) (cid:13) 1 f (cid:0) RMS(x)(cid:1) (cid:13) (cid:13) (cid:13) (cid:13)2 (i) 1 (f (0)2 + LRMS(x)2) < 1 f (0)2 + L, where (i) follows from Lemma 4.1, conclude the first part. Second, we bound the conformal factor. Let = exp0( ˆx) = tanh( ˆx) ˆx ˆx . We have: v2 = tanh( (cid:13) (cid:13) (cid:13) (cid:13) ˆx2) ˆx ˆx 2 (cid:13) (cid:13) (cid:13) (cid:13)2 = tanh( ˆx2) ˆx2 ˆx2 = tanh( ˆx2) . Applying the previous equality gives λc = 2 1 cv2 2 = 2 1 tanh( ˆx2) which can be further bounded by combining it with the bound on ˆx and using the fact that cosh is monotonically increasing function on R>0: ˆx2)2 = 2 cosh2(cid:0) 2 1 tanh( ˆx(cid:1) 2 cosh2 (0)2 + (cid:18) 1 λc = (cid:18) (cid:19)(cid:19) ."
        },
        {
            "title": "B ADDITIONAL BACKGROUND",
            "content": "B.1 TRUST-REGION POLICY OPTIMIZATION (TRPO) TRPO maximizes cumulative reward through gradient ascent on surrogate objective (Equation equation 25); max θ Et subject (cid:21) (cid:20) πθ(at st) πθold (at st) (cid:104) to Et DKL[πθ(at st) θold] πθold (25) (cid:105) δ . Additionally, it enforces an average KL-divergence constraint to keep the new policy close to the data-generating policy. Theoretically, optimizing this objective guarantees monotonic improvement (Schulman et al., 2015). In practice, several approximations are used for deep neural networks. Nevertheless, TRPO tends to retain the monotonic improvement of its theory. B.2 DOUBLE DEEP Q-NETWORK Deep Network (DQN) (Mnih et al., 2015) learns the optimal Q-function for discrete action spaces by minimizing mean-squared error loss against an off-policy bootstrap target while reusing replayed transitions. The standard target is Qπ tar:DQN(s, a) = + γ maxa Qπ(s, a), which, together with experience replay and periodically updated target network, stabilizes training. However, because the same function approximator effectively selects both the maximizing action and evaluates its value under noise and approximation error, the max operator induces systematic overestimation bias (Sutton & Barto, 2018). Double DQN (DDQN) (van Hasselt et al., 2016) reduces the overestimation bias that arises when the same network both selects and evaluates the maximized next-state value. DDQN uses the online network with parameters θ to select the greedy next action, and target network with parameters φ to evaluate that action when calculating the TD target: Qπ tar:DDQN(s, a) = θ (s, a)(cid:1). This decorrelates action selection from evaluation, effectively + γ Qπ φ mitigating overestimation bias. (cid:0)s, arg maxa Qπ B.3 HYPERBOLIC MULTINOMIAL LOGISTIC REGRESSION Multinomial Logistic Regression (MLR) in hyperbolic space is defined as the log-linear model with parameters zk Rd, rk that predicts the probability p(y = x) of an input Rd belonging to specific class {1, . . . , K}: p(y = x) exp(vzk,rk (x)), vzk,rk (x) = zkTpk dM(x, Hzk,rk ). Here, exp(vzk,rk (x)) is the logit for class and vzk,rk (x) the signed distance to the margin hyperplane Hzk,rk . To prevent over-parametrization, each hyperplane is characterized by aligning its normal vector ak and shift pk, requiring only + 1-parameters per hyperplane (Shimizu et al., 2021; Bdeir et al., 2024). To leverage established Euclidean optimization algorithms, all parameters are maintained in Euclidean space and mapped to their hyperbolic counterparts. The normal vector ak TpkM is obtained by parallel transporting the Euclidean paramter zk Rd to the origin 0: ak = T0p(zk) with zk T0M. The hyperplanes shift pk is defined as scalar multiple of (cid:17) the same unit tangent vector pk = exp0 with rk R. (cid:16) rk zk zk 20 B.3.1 POINCAR BALL MLR For the Poincare Ball we have: (cid:19) (cid:18) rk pk = exp0 zk ak = T0p(zk) = (cid:0)1 tanh2 (cid:0) tanh ( zk = rk) zk, zk rk : ak, pk = 0(cid:9) , (cid:1)(cid:1) zk, Hzk,rk = Hak,pk = (cid:8)x Pd dPd (x, Hzk,rk ) = dP (x, Hak,pk ) = 1 sinh1 (cid:18) 2 ak, pk (1 pk x2)ak (cid:19) , where and denote the Mobius addition and subtraction (Ganea et al., 2018). The Poincare Ball MLR layer (Shimizu et al., 2021) can then be summarized as vHNN++ zk,rk (x) = 2 zk (cid:18) sinh1 (1 λc x) sinh(2 rk) + λc cosh(2 rk) (cid:28) zk zk (cid:29)(cid:19) , . B.3.2 HYPERBOLOID MLR For the Hyperboloid, the tangent vectors zk are represented in Rd rather than the full Rd+1 space that would typically characterize tangent vectors at the origin of the Hyperboloid. However, to preserve the correct number of degrees of freedom, we omit the time component, which is constrained to be zero. That said, we have: pk = exp0 (cid:18) rk zk (cid:19) zk = (cid:34) ak = T0p(zk) = (cid:34) 1 cosh( rk), sinh( rk) (cid:35) zk zk (cid:35) sinh( rk)zk, cosh( rk)zk , dHd (x, Hzk,rk ) = : ak, xL = 0(cid:9) , Hzk,rk = Hak,pk = (cid:8)x Hd 1 (cid:0)x0 sinh( zk rk) zk + cosh( sinh1 (cid:18) rk)zk, xs(cid:1) (cid:19) . The Hyperboloid MLR layer (Bdeir et al., 2024) can then be summarized as vHB zk,rk (x) = zk sinh1 (cid:18) zk (cid:0)x0 sinh( rk) zk + cosh( rk)zk, xs(cid:1) (cid:19) , (26) where xs = (x1, . . . , xd) denotes the space component and x0 the time component. 21 Figure 10: Visualization of all ProcGen environments."
        },
        {
            "title": "C ENVIRONMENTS",
            "content": "In this Section, we review the environments used in our paper and discuss evaluation differences to existing methods. C.1 PROCGEN ProcGen (Cobbe et al., 2020) uses RGB frames of size 64 64 3 as observations. We visualize the 16 games in Figure 10. The action space is discrete with 15 actions. For training, we follow the protocol by (Cetin et al., 2023): fix difficulty to easy and train on the first 200 levels (seeds 0199). For testing, we evaluate on all levels of the easy distribution. For the table, we run single end-of-training evaluation on 100 parallel environments sampled from the train and test distribution, respectively. We then normalize the scores for each individual run before aggregating. C.2 ATARI The Arcade Learning Environment (Bellemare et al., 2015; Towers et al., 2024) provides standardized interface for deep RL research based on dozens of Atari 2600 games. Of these, 57 are commonly used in evaluation. The observations are RGB frames 210 160 3, which are preprocessed via grayscaling, downsampling to 84 84, and frame stacking. The action space consists of up to 18 discrete joystick/button combinations, with most games using subset and action repeat (frame skipping) to help with jittering. The rewards are clipped to {1, 0, +1}. As the game dynamics are naturally deterministic, the benchmark uses randomized no-op resets as outlined in the original DQN paper (Mnih et al., 2015) and cleanRL (Huang et al., 2022). C.3 EVALUATION DIFFERENCES WITH EXISTING WORKS Our paper builds on the seminal work by Cetin et al. (2023). However, we struggled to reproduce their results, which we believe is mainly due to three reasons. First, their source code does not use seeding. As deep RL is notoriously seed-dependent (Henderson et al., 2018; Agarwal et al., 2021), we find exact reproduction impossible. Second, we use different implementation for the mathematical operations of hyperbolic geometry, which possibly affects the results. This issue is known within the hyperbolic deep learning community (Katsman & Gilbert, 2025). Third, we use different versions of PyTorch and Python. Additionally, our evaluation follows slightly different protocol (see Appendix C.1), and we use Pytorchs evaluation mode before generating results for our agents. We hope that by releasing our complete code, we can take step towards more reproducible research in hyperbolic deep RL. 22 Table 2: Wall-clock results. ProcGen forward NameThisGame Euclidean Hyper+S-RYM (Cetin et al., 2023) HYPER++ 14ms 19.3ms 14.7ms 17h52m 58h21m 35h25m"
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 HARDWARE & RUNTIME For our experiments, we used Nvidia A100 GPUs. For ProcGen, we can train up to four agents in parallel on single GPU with 40GB. We report wall-clock times for forward passes on ProcGen and for full runs on NameThisGame in Table 2. Note that agent performance can be confounding factor for results when timing on full experiments because agent performance can either be positively or negatively correlated with episode length. For NamethisGame, e.g., better agents generate longer episodes. We average over 100 passes for the forward pass results and five seeds for the NameThisGame results. D.2 NETWORK ARCHITECTURE For ProcGen, we use the same Impala-ResNet (Espeholt et al., 2018) as (Cetin et al., 2023), which we visualize in Figure 11. Our modifications are shaded in purple. They consist of 1. using RMSNorm (Zhang & Sennrich, 2019) before scaling the Euclidean features, 2. using TanH instead of ReLU as penultimate activation, 3. applying learned feature scaling before the exponential map, and 4. using the Hyperboloid instead of the Poincare Ball. For Atari, we use the NatureCNN (Mnih et al., 2015) architecture with the same modifications applied as for ProcGen. 23 fH π(a xH) Hyperboloid MLR HL-Gauss(xH) Hyperboloid MLR fE exp0(xE) Scaling TanH 1/ RMS(xE) Fully connected ReLU Impala residual block Impala residual block 3 [16, 32, 32] channels Max pooling 3 3, stride 2 Convolution 3 3 Figure 11: Hybrid neural network architecture. fH denotes hyperbolic layers, fE Euclidean layers. Components that are specific to HYPER++ are shaded in purple. 24 Table 4: PPO hyperparameters for ProcGen. PPO hyperparameters Parallel environments Stacked input frames Steps per rollout Training epochs per rollout Batch size Normalize rewards Discount γ GAE λ (Schulman et al., 2015) PPO clipping Entropy coefficient Value coefficient Shared network Impala stack filter sizes Default latent representation size Optimizer Optimizer learning rate Optimizer stabilization constant ( ϵ ) Maximum gradient norm. 64 1 16384 3 2048 True 0.999 0.95 0.2 0.01 0.5 True 16, 32, 32 32 Adam (Kingma & Ba, 2015) 5 104 1 105 0.5 Table 5: DDQN hyperparameters for Atari. Atari hyperparameters Environment steps Discount γ ϵ start ϵ end Exploration fraction Replay buffer size Target network update frequency Default latent representation size Batch size Training frequency Optimizer Optimizer learning rate Optimizer stabilization constant ( ϵ ) 10M 0.99 1 0.01 10% of steps 1M 1000 512 32 4 Adam (Kingma & Ba, 2015) 1 104 2.5 105 D.3 HYPERPARAMETERS Table 3: HYPER++ hyperparameters. We specify the hyperparameters for all PPO agents in Table 4 and for the DDQN agents in Table 5. For DDQN, we use the hyperparameters and preprocessing steps from cleanRL (Huang et al., 2022). Additional parameters for our method are in Table 5. On ProcGen, our agent uses latent dimension = 64 compared to = 32 for Hyper+S-RYM. For HL-Gauss (Imani & White, 2018), we use the default parameters specified by Farebrother et al. (2024). We set the learned scaling hyperparameter to α = 0.95 without tuning for all experiments. When using RMSNorm or LayerNorm, we do not use affine parameters, because they can overfit (Xu et al., 2019) to the training set. 51 Loss: Number of bins -10.0 Loss: Min clip Loss: Max clip +10.0 Last Euclidean Activation TanH Learned scaling α 0.95 HYPER++"
        },
        {
            "title": "E ADDITIONAL RESULTS",
            "content": "E.1 FULL TRAIN RESULTS Figure 12: ProcGen Train Learning curves. Table 6: ProcGen Train Results (mean std). Hyper Euclidean Hyper + S-RYM Hyper++ (Ours) bigfish starpilot dodgeball coinrun leaper ninja fruitbot jumper bossfight miner chaser climber caveflyer maze plunder heist 7.121.7 20.431.5 5.420.9 6.251.4 1.651.5 4.630.8 27.090.9 8.270.3 8.490.7 6.860.5 3.850.5 6.720.7 5.810.5 8.880.5 4.990.9 7.570.6 7.815.0 31.144.4 3.280.7 7.371.2 3.351.0 5.650.9 29.400.8 8.170.6 9.400.5 8.520.8 5.040.9 7.790.5 6.410.3 8.880.7 5.300.5 6.781.2 17.383.3 38.921.9 6.260.7 5.531.0 3.232.0 4.370.5 29.360.7 8.170.7 9.380.7 8.531.0 7.600.8 7.430.8 6.130.6 7.800.8 8.811.5 6.680.8 25.662.8 43.433.7 10.281.0 9.670.2 5.250.9 6.670.6 29.890.4 8.530.3 9.270.7 8.101.4 7.120.9 7.270.7 5.040.6 6.901.4 6.230.9 3.951.5 26 E.2 FULL TEST RESULTS Figure 13: ProcGen Learning curves. Table 7: ProcGen Test Results (mean std). Hyper Euclidean Hyper + S-RYM Hyper++ (Ours) bigfish starpilot dodgeball coinrun leaper ninja chaser miner jumper climber bossfight fruitbot caveflyer maze heist plunder 4.801.9 22.214.0 2.380.2 5.071.3 1.381.4 4.080.5 3.340.6 5.781.1 5.280.6 5.410.7 8.500.7 26.372.0 4.910.5 6.380.6 3.580.5 4.580. 3.603.4 30.443.1 1.790.3 6.131.2 3.581.3 4.650.5 4.551.0 7.150.6 5.080.5 5.890.7 9.500.8 27.880.7 5.220.3 6.430.5 3.680.5 4.710.5 11.882.7 35.592.8 3.950.8 5.571.1 3.332.0 4.270.4 6.670.7 7.071.1 5.280.4 5.520.9 9.030.8 28.440.5 4.830.5 5.850.3 3.380.5 7.450.7 20.014.8 42.492.6 9.411.2 8.720.4 5.131.0 5.680.4 7.270.9 7.210.7 5.300.4 5.861.2 9.400.7 28.280.9 4.120.4 5.170.8 2.281.0 5.821.6 27 E.3 ADDITIONAL ABLATION METRICS Figure 14: Additional training metrics of hyperbolic deep RL agents. Agents w/o RMSNorm (Zhang & Sennrich, 2019) (RMSNorm) suffer from growing embedding norms and vanishing gradients in the encoder. Using MSE instead of HL-Gauss (Imani & White, 2018) (+MSE) leads to larger initial encoder gradients due to gradients scaling proportional to the loss for MSE. Not using learned feature scaling (Scaling) has the largest embedding norms and gradients, which are quickly compensated by RMSNorms gradient variance normalization (Zhang & Sennrich, 2019). E.4 GRADIENT AND LOSS VARIANCE ANALYSIS Figure 15 shows the evolution of the loss and the loss variance over the course of the training, averaged over all runs. Compared to MSE, the categorical loss is both higher and has higher variance. However, our paper argues that not the loss values matter, but the gradients instead. Table 8 shows the L1 and L2 gradient norms of the penultimate layer (last FC layer in the encoder) using the final 25% of training steps. Despite higher loss values and variance, using the HL-Gauss loss yields smaller, lower-variance gradients. Table 8: Penultimate layer layer gradient statistics. Agent L2 Grad Norm L1 Grad Norm runs Euclidean Euclidean+Categorical Hyper++ Hyper++-mse 0.0788 0.0276 0.0695 0.0228 0.0258 0.0099 0.0327 0.0102 0.0506 0.0256 0.0460 0.0240 0.0294 0.0155 0.0346 0.0134 96 96 96 96 28 Figure 15: Critic loss and variance. We plot the critic loss and variance for our method when using MSE and the categorical loss, averaged over all runs and environments. The categorical HL-Gauss loss (Imani & White, 2018) has higher loss values and variance than MSE. E.5 LEARNING CURVES FOR ATARI GAMES Figure 16: Atari-5 learning curves. HYPER++ outperforms the baselines on all environments with particularly strong gains in NAMETHISGAME and QBERT. RESULTS ARE AVERAGED OVER FIVE SEEDS. 29 E.6 POLYAK AVERAGING EXPERIMENT Figure 17: Polyak averaging (NAMETHISGAME). Polyak averaging refers to exponential moving average updates for the target network instead of hard replacement updates. Algorithm performance is not meaningfully affected by the form of the target network update. Runs are averaged over five seeds, with one standard deviation as error. E.7 OFF-BATCH PPO METRICS Figure 18: Off-batch PPO stability metrics. We track the update KL divergence and PPO clipping fraction for the batch that has currently been updated (left column) and for batch of randomly sampled on-policy states (right column). The figures show high level of similarity for the evolution of both metrics. For off-batch data, the update KL divergence has noticeably higher variance. 30 E.8 FULL ABLATION RESULTS 31 . + O - a + . E R + . ) . n ( + O ) F ( + O R / u 1 5 + O + O l o / u . o + O u . ) a ( u n T t a c P : 9 a 6 . 1 7 8 . 4 2 3 . 1 2 7 . 9 0 . 1 8 3 . 4 2 . 0 5 5 . 9 . 0 9 4 . 7 2 6 . 0 9 2 . 8 0 . 1 8 6 . 6 4 . 0 7 0 . 5 6 . 0 7 3 . 8 3 . 0 1 8 . 6 . 0 8 5 . 6 3 . 3 8 7 . 8 3 7 . 0 7 8 . 4 1 . 2 8 6 . 7 0 . 2 5 9 . 6 5 . 1 2 1 . 9 . 9 4 1 . 1 1 8 . 3 9 5 . 1 1 5 . 1 3 6 . 4 9 . 0 3 1 . 4 2 . 0 7 2 . 9 7 . 1 0 7 . 9 2 . 1 6 4 . 6 0 . 2 5 6 . 4 3 . 0 2 3 . 4 4 . 1 5 5 . 7 3 . 0 4 4 . 4 4 . 1 1 1 . 7 . 4 5 9 . 5 0 . 1 8 0 . 5 1 . 1 3 3 . 3 4 . 1 5 4 . 8 1 . 1 8 0 . 8 2 5 . 0 4 1 . 5 . 2 9 0 . 6 3 . 0 5 9 . 5 3 . 0 7 5 . 8 8 . 0 1 7 . 5 8 . 0 4 6 . 7 4 . 7 5 9 . 2 5 . 3 2 2 . 5 2 4 . 1 0 7 . 4 1 . 1 6 2 . 5 3 . 1 2 5 . 8 6 . 0 8 8 . 2 0 . 9 0 2 . 0 . 2 3 7 . 4 5 . 0 6 4 . 5 3 . 0 8 3 . 9 7 . 1 2 6 . 7 1 . 7 5 1 . 9 2 . 0 4 0 . 1 . 0 3 7 . 1 3 . 0 5 3 . 3 2 . 0 7 0 . 6 2 . 0 4 5 . 1 - 3 . 0 3 4 . 0 2 . 0 7 2 . 5 . 0 2 3 . 3 3 . 0 0 1 . 3 5 . 0 7 9 . 3 4 . 0 6 1 . 2 3 . 0 6 8 . 2 0 . 0 6 6 . 5 . 0 8 5 . 4 3 . 0 2 1 . 5 7 . 0 3 3 . 3 9 . 1 9 5 . 2 1 . 0 4 9 . 0 3 . 0 4 6 . 4 . 0 7 5 . 3 4 . 0 8 7 . 5 3 . 0 2 8 . 1 - 2 . 0 8 4 . 0 3 . 0 3 4 . 1 6 . 0 7 1 . 7 . 0 5 0 . 3 4 . 0 7 8 . 3 2 . 0 7 8 . 1 4 . 0 8 7 . 2 0 . 0 9 6 . 0 2 . 0 4 7 . 7 . 0 2 4 . 5 4 . 0 5 1 . 3 0 . 2 5 5 . 2 2 . 0 4 0 . 1 3 . 0 6 8 . 1 6 . 0 7 5 . 2 . 0 8 6 . 5 3 . 0 9 4 . 1 - 2 . 0 8 4 . 0 4 . 0 4 5 . 1 4 . 0 3 4 . 3 6 . 0 8 0 . 5 . 0 5 4 . 3 2 . 0 3 6 . 2 4 . 0 1 7 . 2 0 . 0 3 6 . 0 3 . 0 1 6 . 4 3 . 0 7 1 . 4 . 0 3 7 . 3 9 . 1 3 6 . 2 3 . 3 3 9 . 9 1 3 . 4 2 7 . 1 2 4 . 1 5 9 . 7 2 . 1 2 2 . 2 . 0 5 5 . 9 5 . 1 6 0 . 8 5 . 0 0 8 . 2 3 . 0 2 2 . 8 9 . 0 3 2 . 7 2 3 . 1 3 8 . 9 6 . 0 2 6 . 8 5 . 1 3 5 . 4 9 . 0 3 1 . 5 4 . 0 5 7 . 7 4 . 0 7 2 . 4 8 . 0 9 3 . 4 . 0 4 9 . 9 2 . 1 2 3 . 7 6 . 0 7 2 . 5 3 . 0 7 8 . 8 8 . 0 7 9 . 5 2 . 0 5 6 . 0 . 4 1 7 . 7 3 6 . 4 8 5 . 0 4 6 . 0 2 0 . 4 6 . 0 7 3 . 6 2 . 0 0 6 . 5 7 . 0 5 2 . 7 . 9 7 9 . 9 9 . 0 3 2 . 6 1 . 1 7 4 . 8 1 . 0 2 7 . 9 8 . 0 8 3 . 8 1 . 3 8 2 . 0 5 . 0 2 1 . 8 8 . 0 3 8 . 3 3 . 0 8 4 . 9 8 . 0 4 0 . 0 3 7 . 0 6 8 . 8 2 . 1 2 2 . 7 . 0 2 6 . 5 3 . 0 7 5 . 8 3 . 0 9 2 . 5 4 . 1 0 0 . 5 0 . 2 7 9 . 2 4 8 . 0 3 0 . 5 . 0 9 8 . 6 5 . 0 8 2 . 9 1 . 1 2 2 . 4 0 . 4 5 3 . 7 1 0 . 1 5 1 . 8 9 . 0 7 4 . 1 . 0 2 5 . 9 6 . 0 9 8 . 9 2 2 . 1 6 3 . 9 8 . 0 3 9 . 7 9 . 0 5 4 . 6 4 . 0 8 6 . 4 . 0 5 0 . 5 4 . 1 2 2 . 6 9 . 1 2 0 . 7 2 0 . 1 6 4 . 0 1 7 . 0 7 8 . 4 2 . 0 8 6 . 2 . 1 3 1 . 0 3 9 . 0 5 5 . 9 6 . 0 9 4 . 7 7 . 0 0 9 . 5 3 . 0 0 2 . 8 5 . 0 5 8 . 6 . 1 2 5 . 6 0 . 2 2 2 . 0 4 8 . 3 0 8 . 1 4 9 . 0 1 5 . 7 7 . 1 7 7 . 7 4 . 1 8 4 . 5 . 0 0 7 . 3 0 . 1 6 6 . 6 7 . 1 0 1 . 6 1 . 2 8 7 . 6 5 . 0 5 6 . 3 b o h fi n i o u e l fi b fl c b c i t p r u e c n j e t h 0 . 0 1 1 8 . 1 1 5 . 0 1 8 4 . 1 1 8 . 9 3 2 . 1 1 8 . 0 1 5 8 . 1 1 . ) a ( l r T t a c : 0 1 a . + O - a + . S + . ) . n ( + O ) u ( + O o / u 1 5 + O + O l o / u . o + O O 7 . 2 1 3 . 9 1 7 . 0 8 1 . 6 0 . 1 4 0 . 7 2 6 . 0 7 7 . 7 . 0 3 7 . 7 3 . 0 8 3 . 8 7 . 0 7 6 . 4 7 . 1 5 3 . 5 3 3 . 1 1 2 . 9 5 . 0 4 8 . 7 . 0 2 2 . 5 4 . 0 5 5 . 5 9 . 0 8 4 . 5 4 . 0 2 3 . 4 4 . 0 8 6 . 2 3 . 1 9 3 . 6 . 3 0 6 . 7 1 . 2 0 1 . 4 9 . 0 7 1 . 8 2 1 . 1 0 2 . 3 3 . 1 0 1 . 6 2 . 0 5 0 . 8 . 0 8 8 . 3 1 . 7 2 1 . 2 3 3 . 1 2 9 . 2 1 . 1 4 2 . 4 6 . 0 3 0 . 5 0 . 1 5 3 . 3 . 1 3 2 . 4 5 . 0 9 9 . 3 6 . 0 8 2 . 2 1 . 1 3 3 . 4 9 . 2 1 7 . 3 0 . 2 5 2 . 3 . 1 2 8 . 6 2 4 . 0 3 1 . 5 8 . 0 9 1 . 9 2 . 1 3 6 . 7 1 . 1 7 3 . 3 9 . 2 1 6 . 5 9 . 0 4 7 . 2 1 . 2 7 6 . 4 1 . 0 7 3 . 5 3 . 0 2 9 . 6 4 . 0 3 4 . 6 5 . 0 5 4 . 9 . 0 2 9 . 3 5 . 0 3 8 . 4 1 . 0 6 9 . 0 2 . 0 3 3 . 1 3 . 0 5 6 . 1 - 2 . 0 0 4 . 3 . 0 9 5 . 0 5 . 0 0 4 . 5 3 . 0 7 2 . 3 3 . 0 8 5 . 2 2 . 0 1 6 . 1 0 . 0 4 6 . 4 . 0 0 6 . 2 5 . 0 7 2 . 5 8 . 0 9 3 . 2 5 . 0 5 9 . 3 3 . 0 7 2 . 3 3 . 0 5 4 . 1 . 0 1 0 . 1 3 . 0 8 4 . 1 3 . 0 4 7 . 1 - 6 . 0 7 3 . 3 2 . 0 2 6 . 0 3 . 0 5 3 . 5 . 0 8 1 . 3 3 . 0 0 8 . 2 2 . 0 4 5 . 1 0 . 0 5 6 . 0 4 . 0 0 4 . 2 7 . 0 8 0 . 3 . 0 4 4 . 2 5 . 0 7 5 . 3 4 . 0 5 9 . 2 3 . 0 2 4 . 4 1 . 0 5 0 . 1 4 . 0 2 6 . 2 . 0 9 6 . 1 - 5 . 0 8 2 . 3 3 . 0 8 7 . 0 6 . 0 5 2 . 5 5 . 0 0 3 . 3 3 . 0 6 7 . 3 . 0 9 5 . 1 0 . 0 5 6 . 0 3 . 0 0 5 . 2 4 . 0 2 2 . 5 4 . 0 7 4 . 2 7 . 0 2 6 . 4 . 0 0 8 . 2 3 . 0 4 4 . 4 7 . 4 7 7 . 4 1 4 . 1 0 4 . 4 3 . 1 1 2 . 7 2 4 . 0 5 3 . 9 . 0 8 6 . 8 3 . 0 7 6 . 8 4 . 1 0 2 . 4 6 . 4 7 4 . 6 1 0 . 1 3 2 . 6 2 . 1 5 2 . 8 2 . 0 8 7 . 4 7 . 0 0 5 . 9 7 . 0 5 7 . 7 2 . 0 7 4 . 2 7 . 4 8 6 . 3 3 8 . 2 4 1 . 0 3 . 1 0 7 . 5 5 . 0 9 9 . 3 6 . 0 3 2 . 5 9 . 0 2 2 . 5 5 . 0 6 4 . 3 3 . 0 7 8 . 9 . 0 5 5 . 2 6 . 0 4 9 . 5 3 . 1 2 4 . 5 8 . 0 4 8 . 5 4 . 0 2 5 . 5 8 . 0 8 1 . 3 . 0 9 7 . 5 7 . 0 1 4 . 5 7 . 0 3 2 . 4 3 . 1 9 5 . 7 3 . 2 2 0 . 5 1 4 . 1 4 4 . 3 . 1 6 7 . 7 2 5 . 0 0 7 . 4 8 . 0 6 4 . 8 5 . 0 5 7 . 8 1 . 1 2 4 . 4 5 . 3 6 6 . 9 0 . 1 8 2 . 5 6 . 0 4 9 . 6 5 . 0 8 3 . 5 6 . 0 7 8 . 5 3 . 1 3 1 . 4 7 . 0 5 8 . 6 . 0 0 0 . 2 0 . 1 6 1 . 6 8 . 3 2 5 . 2 1 5 . 0 2 0 . 7 3 . 1 4 0 . 8 2 8 . 0 3 3 . 0 . 1 0 6 . 8 4 . 0 8 5 . 8 9 . 0 8 5 . 3 5 . 3 6 5 . 1 2 2 . 1 2 4 . 7 9 . 0 2 6 . 8 6 . 0 7 5 . 5 7 . 0 1 5 . 9 4 . 0 5 7 . 8 0 . 1 2 6 . 4 4 . 2 4 3 . 7 3 5 . 2 5 8 . 9 1 . 1 1 6 . 5 2 . 1 8 0 . 7 2 . 0 3 7 . 5 7 . 0 0 4 . 5 2 . 1 4 1 . 5 7 . 0 5 0 . 7 . 0 5 3 . 2 6 . 1 8 8 . 6 7 . 0 1 9 . 8 8 . 0 5 7 . 6 3 . 0 7 3 . 5 5 . 1 8 5 . 1 . 1 5 9 . 4 2 . 0 1 9 . 3 4 . 0 3 4 . 2 7 . 0 8 0 . 5 fi r m b r n h fi b n r e a d t p s m e c m fl c m c n t h 33 E.9 ARCHITECTURE ABLATIONS Table 11: Latent dimension and learned scaling α ablation studies. Environment = 32 = 64 = 128 = 512 = 64, α = 0.9 = 64, α = 0. starpilot leaper ninja chaser coinrun bossfight dodgeball caveflyer heist fruitbot maze miner climber jumper plunder bigfish IQM (normalized) 40.07 4.1 4.10 1.6 5.07 0.4 6.42 0.7 8.60 0.4 9.23 1.2 9.02 1.0 4.33 0.9 2.67 1.1 27.65 1.8 5.68 0.4 7.76 0.6 5.10 0.9 5.15 0.4 6.40 1.0 20.77 2.0 0.38 42.49 2.6 5.13 1.0 5.68 0.4 7.27 0.9 8.72 0.4 9.40 0.7 9.41 1.2 4.12 0.4 2.28 1.0 28.28 0.9 5.17 0.8 7.21 0.7 5.86 1.2 5.30 0.4 5.82 1.6 20.01 4.8 0.41 41.49 2.0 4.27 1.4 5.68 0.4 7.30 0.7 8.68 0.4 8.94 1.0 9.15 1.9 3.98 0.5 2.42 0.5 28.81 0.8 5.70 1.2 6.63 0.9 6.54 1.0 5.22 0.6 6.17 1.2 20.96 2.4 0.42 39.07 2.9 4.35 1.6 5.40 0.3 6.63 0.6 8.82 0.3 9.55 0.7 7.16 2.3 4.08 0.5 2.60 0.6 28.75 0.8 5.57 0.4 6.46 0.7 6.42 1.1 5.90 0.4 5.85 0.7 19.84 4.2 0.39 39.55 4.1 4.75 1.3 5.45 0.4 6.74 0.9 8.67 0.2 9.00 0.7 9.62 1.4 3.93 0.6 2.73 0.7 27.87 0.5 5.40 0.7 7.11 1.1 6.20 0.9 5.32 0.7 6.71 0.6 19.55 2.9 0. 35.61 2.7 4.90 1.0 5.68 0.9 6.40 1.2 8.67 0.4 9.43 0.9 9.07 1.5 3.96 0.6 2.20 0.4 28.01 1.5 5.48 0.7 7.44 0.4 5.70 0.6 5.98 0.4 6.01 0.7 20.89 3.3 0."
        }
    ],
    "affiliations": [
        "Department of Machine Learning and Systems Biology, Max Planck Institute of Biochemistry",
        "Doctoral School Computer Science, University of Vienna",
        "Faculty of Computer Science, University of Vienna",
        "ds:UniVie, University of Vienna"
    ]
}