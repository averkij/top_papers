{
    "paper_title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders",
    "authors": [
        "Siyu Chen",
        "Heejune Sheen",
        "Xuyuan Xiong",
        "Tianhao Wang",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \\highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \\highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 0 0 4 1 . 6 0 5 2 : r Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders Siyu Chen Heejune Sheen Xuyuan Xiong Tianhao Wang Zhuoran Yang Department of Statistics and Data Science, Yale University Antai College of Economics and Management, Shanghai Jiao Tong University Toyota Technological Institute at Chicago {siyu.chen.sc3226, heejune.sheen, zhuoran.yang}@yale.edu xxy2021@sjtu.edu.cn tianhao.wang@ttic.edu Abstract We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose novel statistical framework for the feature recovery problem, which includes new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce new SAE training algorithm based on bias adaptation, technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically prove that this algorithm correctly recovers all monosemantic features when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters. This work represents foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability. TL;DR Existing Sparse Autoencoder (SAE) training algorithms often lack rigorous mathematical guarantees for feature recovery. Empirically, methods such as ℓ1 regularization and TopK activation are sensitive to hyperparameter tuning and can exhibit inconsistency. Our work addresses these theoretical and practical issues with the following contributions: 1. new statistical framework that formalizes feature recovery by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts and establishes rigorous notion of feature identifiability. 2. novel training algorithm, Group Bias Adaptation (GBA), that uses bias adaptation\" to directly control neuron sparsity, overcoming the limitations of conventional regularization and achieving better control over neuron activation frequency. 3. The first provable recovery guarantee for an SAE training algorithm, where GBA recovers all monosemantic features when data is sampled from the statistical model. 4. Superior empirical performance on LLMs up to 1.5B parameters, where GBA achieves the sparsity-loss frontier while learning more consistent features than benchmarks."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Related Works . 2 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 6 3 Algorithm: Group Bias Adaptation"
        },
        {
            "title": "3.1 Algorithm Motivations .\nImplementation Details .\n3.2",
            "content": ". . 9 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments on Qwen2.5-1.5B LLM 5 Identifiability of Features 20 5.1 Main Results on Identifiability of Features . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.2 Dicussion on Feature Co-occurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6 Dynamics Analysis: SAE Provably Recovers True Features",
            "content": "24 6.1 Simplification for Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 6.2 Main Theorem on Training Dynamics 6.3 Key Conditions for Reliable Feature Recovery . . . . . . . . . . . . . . . . . . . . . . . 28 6.3.1 Bias Range: Implications on Target Activation Frequency . . . . . . . . . . . . 28 Feature Balance and Network Width . . . . . . . . . . . . . . . . . . . . . . . . 30 6.3."
        },
        {
            "title": "7 Proof Overview",
            "content": "31 7.1 Good Initialization with Wide Network . . . . . . . . . . . . . . . . . . . . . . . . . . 31 7.2 Pre-activations are Approximately Gaussian . . . . . . . . . . . . . . . . . . . . . . . 32 7.3 Weight Decomposition and Concentration under Sparsity . . . . . . . . . . . . . . . . 32 State Recursion and Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 7."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": ""
        },
        {
            "title": "40\nA.1 Details on Other Training Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\nA.2 Evaluation Metrics .\nA.3 Equivalance between Row Normalization of X and H . . . . . . . . . . . . . . . . . . 44\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nA.4 Omitted Details in §6 .",
            "content": ". . . . ."
        },
        {
            "title": "45\nB.1 Additional Experimental Details for §6 . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nB.1.1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nB.1.2 Comparing with TopK Activation . . . . . . . . . . . . . . . . . . . . . . . . . 46\nB.1.3 Additional Details on Figure 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47",
            "content": "B.2 Additional Details for 4 . Synthetic Data . . . Identifiability of Features: Proof of Theorem 5.3 2 Good Initialization and Gaussian Conditioning 54 D.1 Initialization Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 D.2 Rewriting the Gradient Descent Iteration . . . . . . . . . . . . . . . . . . . . . . . . . 56 . D.3 Gaussian Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 . D.4 Additional Proofs . . . . . . . . Concentrations Results for the SAE Dynamics 64 E.1 Isolation of Gaussian Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 E.2 Sparse Activation . E.3 Second Order Concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 E.4 First Order Concentration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 E.5 Non-Gaussian Error Propogation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 . . . . . . . SAE Dynamics Analysis: Proof of Theorem 6.1 74 F.1 General Version of the Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 F.2 Concentration Results Combined . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 F.3 Two-State Alignment Recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 Simplifying the Conditions of Lemma F.13 . . . . . . . . . . . . . . . . . . . . . . . . . 82 F."
        },
        {
            "title": "84\nG.1 Proofs for Non-Gaussian Components . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\nG.1.1 Proof of Lemma E.1 .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nG.1.2 Additional Proofs for Lemma E.1 . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nG.2 Proofs for Concentration Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\nG.2.1 Concentration for Ideal Activations\n. . . . . . . . . . . . . . . . . . . . . . . . 87\nG.2.2 Activations with Non-Gaussian Component: Proof of Lemma E.3 . . . . . . . 91\nG.2.3 Concentration for ∥E⊤φ(Ey⋆\n. 92\nG.2.4 Concentration for ∥F ⊤φ(F yt + θ · v⊤ swt−1; bt)∥2\n2: Proof of Lemma E.6 . . . . . 102\nG.2.5 Concentration for ⟨zτ , E⊤φ(Ey⋆\nt ; bt)⟩: Proof of Lemma E.7 . . . . . . . . . . . 103\nG.2.6 Concentration for ⟨zτ , F ⊤φ(F yt + θ · v⊤ swt−1; bt)⟩: Proof of Lemma E.9 . . . . 105\nt + θv⊤ swt−1; bt): Proof of Lemma E.10 . . . . . . . 108\nG.2.7 Concentration for θ⊤φ(F y⋆\nG.3 Propogation of the Non-Gaussian Error . . . . . . . . . . . . . . . . . . . . . . . . . . 109\nG.3.1 Error Analysis for ∆Et: Proof of Lemma E.12 . . . . . . . . . . . . . . . . . .\n. 109\nG.3.2 Error Analysis for ∆Ft: Proof of Lemma E.13 . . . . . . . . . . . . . . . . . .\n. 110\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n. 111\n. 112\n. 113",
            "content": "G.4.1 Proof of Lemma E.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.2 Proof of Lemma E.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.4.3 Proof of Lemma E.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2: Proof of Lemma E.4 . . . . . . . . . . . G.4 Proofs for Technical Lemmas ; bt)"
        },
        {
            "title": "H Proofs for SAE Dynamics Analysis",
            "content": "."
        },
        {
            "title": "113\nH.1 Proof of Proposition F.2 .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\nH.2 Proofs for Concentration Results Combined . . . . . . . . . . . . . . . . . . . . . . . . 115\n. 115\nH.2.1 Proof of Lemma F.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. 116\nH.2.2 Proof of Lemma F.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. 117\nH.2.3 Proof of Lemma F.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. 119\nH.2.4 Proof of Lemma F.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nH.3 Proofs for Recursion Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n. 119\nH.3.1 Proof of Lemma F.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. 122\nH.3.2 Proof of Lemma F.12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 H.4.1 Proof of Lemma F.13 . H.4.2 Proof of Lemma F.14 . H.4.3 Proof of Lemma F.15 . H.5 Proofs for Technical Lemmas H.4 Proofs for Condition Simplification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 H.5.1 Proof of Lemma H.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 H.5.2 Proof of Lemma H.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 H.5.3 Proof of Proposition H.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 Auxiliary Lemmas 132 I.1 Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 I.2 Efron-Stein Inequalities . ."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks. It is found that LLMs encode vast amounts of information by superposition (Bengio et al., 2013; Elhage et al., 2022; Lu et al., 2024; Xiong et al., 2024) packing multiple concepts into the same weight or activation directions to maximize capacity. This efficiency comes at cost: individual neurons (or activation vectors) become polysemantic (Scherlis et al., 2022), meaning they respond to several monosemantic features at once. While superposition boosts representational power, it also makes interpreting what any single neuron means much harder. Dictionary learning has recently been applied to disentangle Figure 1: Illustration of SAE. polysemantic LLM representations, with Sparse Autoencoders (SAEs) emerging as leading approach (Bricken et al., 2023a; Cunningham et al., 2023; Gao et al., 2024; Rajamanoharan et al., 2024b; Templeton et al., 2024). An SAE encodes an LLMs internal activation Rd into highdimensional, sparse code = fenc(x) RM with d, and then decodes px = fdec(z) x. By enforcing sparsity, such that only few components of are nonzero, each active neuron ideally reflects single interpretable feature. Empirically, SAEs have revealed such monosemantic features in models like Pythia-70M (Cunningham et al., 2023) and Claude 3.5 Sonnet (Templeton et al., 2024). Despite these promising empirical advances, existing studies on SAEs often lack mathematically rigorous guarantees regarding feature recovery. Furthermore, popular training algorithms frequently suffer from practical limitations, including instability and strong sensitivity to hyperparameter tuning. For instance, methods employing ℓp regularization require careful tuning of the regularization parameter. The widely used ℓ1 regularization often leads to activation shrinkage\", where the magnitudes of the learned features are systematically underestimated (Tibshirani, 1996) The ℓ0 variant, while directly penalizing the number of non-zero activations, is computationally difficult to optimize. Other prominent approaches, like TopK activation (Gao et al., 2024; Makhzani and Frey, 2013), also depend on manually set hyperparameter the threshold K. While enforcing hard sparsity constraint, TopK can yield sets of learned features that are highly sensitive to the random seed used during initialization (Paulo and Belrose, 2025), raising concerns about the reliability of the discovered features. This landscape motivates us to address fundamental questions concerning the reliability and theoretical underpinnings of feature recovery with SAEs. We aim to tackle the following two primary questions:"
        },
        {
            "title": "Method",
            "content": "Sparsity-Loss Frontier Consistency Tuning-free L1 (Templeton et al., 2024) TopK (Gao et al., 2024) GBA (ours) Table 1: Comparison of SAE Training Methods. The table summarizes the properties of different SAE training methods. The first column indicates whether the method achieves sparsity-loss frontier. The sparsity-loss frontier is the curve for the best trade-off between ℓ2 reconstruction loss and neuron activation sparsity. The second column indicates whether the method ensures feature consistency, meaning it learns consistent features across different runs. The third column indicates whether the method is tuning-free, meaning it does not require hyperparameter tuning for optimal performance. 1. How can statistical model be formulated to formally study the feature recovery problem in the context of polysemantic representations in LLMs? Specifically, what constitutes an appropriate generative model for such polysemantic representations? What are suitable mathematical notions of feature identifiability under such framework? 2. Is it possible to design an SAE training algorithm that satisfies two crucial properties: (a) Under the proposed statistical framework, can the algorithm provably and efficiently recover the underlying ground-truth monosemantic features? (b) Can this algorithm be effectively applied to modern LLMs, overcoming the practical drawbacks associated with methods like TopK and ℓ1 regularization, such as intricate hyperparameter tuning and sensitivity to random initialization? We provide affirmative answers to these two questions. First, building on sparse dictionary learning and matrix factorization principles (Agarwal et al., 2016; Arora et al., 2014; Barak et al., 2015; Gribonval and Schnass, 2010; Spielman et al., 2012), we introduce formal statistical model for the feature recovery problem. We model polysemantic representation vector as random sparse linear combination hV of set of unknown monosemantic features from dictionary = [v1, . . . , vn] Rnd where Rn is sparse coefficient vector. We define precise notion of feature identifiability and cast the feature recovery problem as the reconstruction of the monosemantic features in from observation x. Second, we introduce novel SAE training algorithm based on the principle of bias adaptation: mx + bm) with wm Rd being the weight vector and bm the each neuron computes zm = σ(w bias, and we adaptively adjust the bias bm to enforce sparse activations while avoiding dead units. Theoretically, we prove that our bias adaptation algorithm correctly recovers all true monosemantic features when data are sampled according to our proposed statistical model under specific, welldefined conditions. Empirically, we develop an improved variant of our algorithm Group Bias Adaptation (GBA) and scale it to LLMs with up to 1.5B parameters. Our experiments demonstrate that GBA achieves superior performance in terms of reconstruction quality, activation sparsity, and feature consistency compared to benchmark methods. The significance of our work is multifaceted. It serves as foundational step in demystifying the training dynamics and empirical successes of SAEs by bridging the gap between their practical application and theoretical understanding. We propose what we believe to be the first SAE training algorithm accompanied by provable theoretical guarantees for feature recovery. By enhancing the reliability and robustness of SAEs, our contributions advance the broader objective of constructing transparent and trustworthy Artificial Intelligence systems through the continued development of mechanistic interpretability tools."
        },
        {
            "title": "1.1 Related Works",
            "content": "SAE training methods. Many methods have been proposed to train SAEs, addressing the trade-off between reconstruction fidelity and sparsity-induced interpretability from various perspectives. One canonical approach is imposing an ℓ1 penalty on the activations Bricken et al. (2023a). Although ℓ1 penalty is natural surrogate for enforcing ℓ0 sparsity, it typically suffers from activation shrinkage Tibshirani (1996). Several works have attempted to overcome this drawback through alternative techniques Konda et al. (2014); Rajamanoharan et al. (2024a); Taggart (2024); Wright and Sharkey (2024). In particular, Rajamanoharan et al. (2024b) proposed the JumpReLU activation, which achieves state-of-the-art performance. This method requires backpropagation with pseudoderivatives due to the non-smooth nature of JumpReLU and the need for tuning the kernel density estimation bandwidth. Another representative example is the use of TopK activation Makhzani and Frey (2013), which has proven effective when scaled to large models Gao et al. (2024). However, it has been observed that features learned via TopK activation are quite sensitive to the random seed Paulo and Belrose (2025), raising concerns about their reliability. Sparse dictionary learning. Beyond SAE training methods, there is long history of research on sparse dictionary learning (SDL) dating back to Kreutz-Delgado et al. (2003); Olshausen and Field (1996). Numerous techniques have been developed for applications in signal processing and computer vision (Bruckstein et al., 2009; Rubinstein et al., 2010). For example, Spielman et al. (2012) proposed polynomial-time algorithm that can accurately recover both the dictionary and its coefficient matrix, under the assumption of sparsity in the coefficients. Using SAEs for model interpretation. In recent years, SAEs have gained attention for model interpretation, particularly in the context of large language models (LLMs) (Bricken et al., 2023a; Paulo and Belrose, 2025). Notably, Ameisen et al. (2025); Bricken et al. (2023a); Dunefsky et al. (2024) have identified several interesting features and circuit patterns learned by SAEs or their variants. Beyond detecting monosemantic features, Papadimitriou et al. (2025) found that groups of SAE-learned features remain remarkably stable across different training runs and encode crossmodal semantics. Additionally, the potential of SAE activations for steering model behavior has been explored (Ameisen et al., 2025; Shu et al., 2025). Notation. Let R+ denote the set of non-negative real numbers. We use standard Big-O, Big-Θ and small-o notation and use to denote that + O(log log(n)/ log n) for sufficiently large n. For two sets and B, we denote by the disjoint union of and B. We denote by [n] the set {1, 2, . . . , n} for positive integer n. We use to denote the minimum of and b, and to denote the maximum of and b. We denote by 1 the all-ones vector, whose dimension will be clear from context. For finite set A, we denote by the cardinality of A. For two matrices and of the same shape Rnm, we denote by cos(A, B) the vector of cosine similarities between corresponding rows of and B, i.e., cos(A, B) = (cid:32) i,:Bi,: Ai,:2Bi,:2 (cid:33)n . i=1 (1.1)"
        },
        {
            "title": "2 Preliminaries",
            "content": "For trained deep neural network, the learned features often represent mixture of multiple underlying concepts, which can be viewed as linear combination of monosemantic features 6 (Bricken et al., 2023a; Cunningham et al., 2023). As motivating example, consider the following sentence: The detective found muddy footprint near the broken window , leading him to suspect ? When the model encounters this sentence, and in order to predict the missing word burglary, the model needs to mix the two monosemantic features footprint and window at some layer to form mixed representation as = h1 muddy footprint (cid:125) (cid:124) (cid:123)(cid:122) v1 + h2 broken window (cid:125) (cid:124) (cid:123)(cid:122) v2 + . . . , and then use the mixed representation to predict the missing word through the Feed Forward Neural Network (FFN) layer. In this example, we have two monosemantic features v1 and v2 corresponding to the concepts muddy footprint and broken window, respectively, and the coefficients h1 and h2 represent the contribution of each feature to the mixed representation x. In particular, the coefficients h1 and h2 are nonnegative for this mixing, as the opposite direction of feature would often lead to totally different or even contradictory concept. model for feature recovery. In what follows, we formalize the motivating example into statistical framework for feature recovery. Assume that the models hidden representation at specific layer encodes distinct features in d-dimensional space. We aggregate these features in the feature matrix Rnd, where each row vi is d-dimensional feature vector. Suppose we have training set with tokens. For each token, we extract the hidden representation as d-dimensional vector, resulting in data matrix RN d. Each row xℓ of corresponds to mixture of the features in with nonnegative coefficients, i.e., xℓ = (cid:80)n i=1 hℓ,ivi for all ℓ [N ]. We collect these coefficients into coefficient matrix RN , where each row Hℓ,: contains the coefficients for the corresponding data point xℓ. We then have the following data model: + = HV RN d. (2.1) In particular, we assume that every coefficient vector, i.e., each row of H, is s-sparse with all entries nonnegative. The goal is to recover the feature matrix from the training data X, even though the coefficient matrix is not known. In practice, both the embedding dimension and the number of features can be very large. In state-of-the-art LLMs, typically ranges from 210 to 212, while ninterpreted as the number of distinct concepts model can represent at token positionvaries with the architecture and where the model is probed. For generality, we restrict to be polynomial in d, i.e., ω(1) < < poly(d), and we focus in particular on the superposition regime where > d. In this regime, the feature vectors must be linearly dependent (Arora et al., 2018; Elhage et al., 2022; Olah et al., 2020). In the context of LLMs, is often vector computed internally at specific layer and token position. This vector encodes the semantic meaning of the sentence, which is typically mixture of multiple underlying concepts. For example, consider passing the sentence in the motivating example in 2 through transformer model. The hidden representation at specific layer at token a, used to predict the next word, will incorporate concepts from both muddy footprint and broken window, leading to polysemantic representation. The model in (2.1) explicitly describes superposition as the phenomenon where polysemantic vector is sparse linear combination of multiple monosemantic features in . However, given data matrix X, the factorization in (2.1) is not unique. That is, single data point (a row of X) may admit multiple valid representations through different feature combinations. This potentially raises the issue of identifiability, which will be addressed in 5. 7 SAE architecture. At the core of our feature recovery method is the Sparse Autoencoder (SAE) (Vincent et al., 2010), which is neural network architecture designed to learn sparse representations of input data through an unsupervised self-reconstruction process. We follow Cunningham et al. (2023); Gao et al. (2024) and use three-layer neural network for SAE with pre-bias and tied encoding and decoding weights. Let be the width of the SAE, i.e., the number of neurons. Let ϕ : be nonlinear activation function, such as ReLU or JumpReLU (Erichson et al., 2019; Rajamanoharan et al., 2024b). The parameters of the SAE are denoted by Θ = {(wm, am, bm)M m=1, bpre}, where {wm}m[M ] Rd are the shared weight vectors for both encoding and decoding, am is the output scale for neuron m, bm is the bias for neuron m, and bpre Rd is the pre-bias vector that centers the input data. For any input Rd, the output of the SAE with parameters Θ is defined as (x; Θ) = (cid:88)M m=1 am wm ϕ(cid:0)w (cid:1) + bpre. m(x bpre) + bm (cid:124) (cid:125) (cid:123)(cid:122) pre-activation ym (2.2) In particular, in each neuron, we center the input by subtracting the pre-bias bpre, which is then added back to the output. In each neuron [M ], we first compute the pre-activation ym = m(x bpre) + bm, and then apply the activation function ϕ to obtain the neuron activation. The activation function ϕ brings nonlinearity to the model. We say neuron is activated if ym > 0. When neuron is activated, its contribution to the output is proportional to wm, with scaling factor am. The symmetry introduced by tying the encoder and decoder weights endows the SAE with more interpretable structure. In this configuration, the encoder weight wm serves as detector that activates the neuron when its designated feature is present, while the same weight in the decoder acts as reconstructor that regenerates the input from the neuronal activations. During training, the SAE is optimized to reconstruct the input with reconstruction loss Lrec(x; Θ) = 1 2 (x; Θ) x2 2, (2.3) Reconstructing monosemantic features. Under the statistical model in (2.1), we can train the SAE to reconstruct the data matrix by minimizing the loss function L(Θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) ℓ=1 Lrec(xℓ; Θ) = 1 2N (cid:88) ℓ= (xℓ; Θ) x2 2. (2.4) By minimizing this loss function over Θ with suitable optimization algorithm, in addition to being able to reconstruct the input data, the SAE can also learn to recover the underlying monosemantic features in . Particularly, we aim to answer the following question: Can we design an SAE training algorithm that recovers the monosemantic features in from the training data X? That is, the parameters {wm}m[M ] recover all the monosemantic features in . In prior literature, sparsity-inducing penalty, typically the ℓ1 regularization, Baseline methods. is applied to the neuron activations (Bricken et al., 2023b; Templeton et al., 2024) to encourage the learning of sparse input representations. This leads to the loss function L1 method: L(x; Θ) = Lrec(x; Θ) + λ (cid:124) (cid:88) m=1 wm2 ϕ(w , m(x bpre) + bm) (2.5) (cid:123)(cid:122) ℓ1 regularization (cid:125) where λ is regularization hyperparameter. Here, the wm2 term gives higher weight to neurons whose weight vectors has larger magnitude. By applying the ℓ1 regularization, only subset of neurons are activated for each input, i.e., ϕ(w m(x bpre) + bm) is nonzero for only small number of neurons. Alternatively, another line of work (Gao et al., 2024; Makhzani and Frey, 2013) replaces the ReLU activation with TopK activation function while still using the reconstruction loss Lrec as the training objective. Here, the TopK activation function retains only the top largest neuron activations for each input in (2.2). That is, the top values of {w m(xbpre)+bm}m[M ] are kept and multiplied by the decoding weights, which effectively enforces sparsity in the neuron activations. In particular, exactly neurons are activated for each input. Nonetheless, the performance of these methods is often highly sensitive to hyperparameter tuning, such as the regularization parameter λ in the ℓ1 loss or the threshold in TopK activations. Furthermore, these techniques have their own limitations. For instance, while ℓ1 loss encourages sparsity, it often triggers activation shrinkage during training by continuously diminishing neuronal pre-activations. This, in turn, can cause dead neurons and an underestimation of true feature magnitudes (Tibshirani, 1996). In addition, TopK methods, although enforcing strict sparsity constraint on the number of activated neurons, can produce feature sets that are overly sensitive to the randomness inherent in the initialization process (Paulo and Belrose, 2025)."
        },
        {
            "title": "3 Algorithm: Group Bias Adaptation",
            "content": "To address the limitations of existing Sparse Autoencoder (SAE) training methods, we propose new algorithm called Grouped Bias Adaptation (GBA). Our algorithm has two main components: (i) bias adaptation subroutine that controls the activation frequency of each neuron, and (ii) neuron grouping strategy that allows us to assign different target activation frequencies to different groups of neurons. In the following, we first introduce the considerations behind these two components, and then give detailed illustrations of the algorithm. The algorithm is shown in Algorithm 1 with bias adaptation subroutine shown in Algorithm 2."
        },
        {
            "title": "3.1 Algorithm Motivations",
            "content": "In what follows, we recall the SAE definition in (2.2) and rewrite the pre-activation ym(x) = m(x bpre) + bm (3.1) for neuron as function of input x. We say that neuron is activated on input if ym(x) > 0. Let denote buffer of data, which stores subset of the data points in X. We define the activation frequency for each neuron over the buffer as the fraction of inputs for which neuron is activated: ppm = 1 (cid:88) xB 1(ym(x) > 0). (3.2) Ideally, if each neuron captures monosemantic feature and data in is sufficiently diverse, each monosematic feature should not appear too often, and thus the activation frequency ppm should be 9 Figure 2: Illustration of neuron grouping and bias adaptation. Neurons are assigned to different groups Gk with distinct Target Activation Frequencies (TAFs). (Left) Features with varying occurrence frequencies are captured by the group with designated TAFs. (Right) The bias adaptation mechanism shifts the neurons bias to achieve TAF over buffer of data, where each colored bar represents the neurons pre-activation on data point. small. Thus, we can use the activation frequency ppm to control the sparsity of the neuron activations. This consideration leads to the two main components of our algorithm. Component 1: Bias adaptation. An effective sparse autoencoder must strike careful balance between activating neurons often enough to learn meaningful feature directions, but not so often that every neuron responds indiscriminately. In particular, we identify two key desiderata for neuron activation frequency: Ensure sufficient sparsity. To discover feature direction, the neuron targeting this feature must focus. That is, activate preferentially on the subset of inputs that actually contain that feature. Suppose neuron targets monosemantic feature v, then it should only activate on an input if has linear component in the direction of v. Avoid over-sparsity. If neurons activate too rarely, they receive almost no gradient signal and can become permanently inactive (dead neurons), wasting representational capacity. In other words, for each m, the activation frequency ppm over the whole dataset should not be too small. Most existing SAE methods impose sparsity penalty on the hidden activations. While this encourages many activations to be zero, the exact activation frequency then depends critically on the balance between the reconstruction loss Lrec and the sparsity penalty strength (e.g., the ℓ1 regularization in (2.5)). Tuning that trade-off is cumbersome and data-dependent, and it becomes even more difficult when the relative scale of Lrec and the ℓ1 regularization is dynamically changing during training due to both sampling random minibatches and the evolution of the neuron weights and biases. Instead, we seek direct control over each neurons long-term activation frequency. As the preactivation ym(x) can be decomposed into projection term m(x bpre) and bias term bm, we can control the activation frequency by simply adjusting the bias bm of each neuron to hit Target Activation Frequency (TAF). In particular, we aim to decouple sparsity control from reconstruction loss minimization and at the same time avoid dead units motivating the use of bias adaptation subroutine. 10 Implementing bias adaptation. Consider simple example where we set the TAF to number (0, 1) for all neurons. Given buffer of data B, we can compute the activation frequency ppm defined in (3.2) for each neuron m. If ppm > p, neuron activates too frequently, and thus we reduce its bias bm to encourage it to activate less often. This essentially ensures that the neurons are sparsely activated. Conversely, if ppm < ϵ (where ϵ is small positive number), it suggests that neuron is rarely activating, and we can increase its bias bm to encourage more frequent activation. This avoids over-sparsity in the neuron activations. Component 2: Neuron grouping with different TAFs. While bias adaptation offers direct control over each neurons activation frequency without complex tuning, using uniform TAF ignores the diversity of features. model might need to capture both common features (e.g., cat) and rare ones (e.g., marmoset), and single TAF could either suppress rare features or overwhelm common ones. To overcome this limitation, we propose neuron grouping strategy that assigns distinct TAFs to different groups of neurons, thereby accommodating the varied occurrence rates of the underlying features. Implementing neuron grouping. We partition the neurons into groups, denoted by {Gk}K k=1, where each group Gk contains M/K neurons. Here is the number of total neurons. Each group Gk is assigned distinct TAF pk, which is set to be exponentially decaying, e.g., pk+1 = pk/2. The first group G1 has the highest TAF p1 set to some reasonably large value (e.g., 0.1). The exponential decay helps to cover wide range of feature frequencies with manageable number of groups. This grouping strategy allows us to capture features with varying occurrence frequencies. Alternative grouping strategies with variable group sizes and custom TAFs are possible; however, we find that partitioning the neurons into equally sized groups with exponentially decaying TAFs not only yields robust performance in practice but also simplifies implementation. 3."
        },
        {
            "title": "Implementation Details",
            "content": "Combining the above two components, we arrive at the Grouped Bias Adaptation (GBA) algorithm. We present the details of this algorithm and discuss the practical considerations of the algorithm as follows. GBA algorithm overview. The input of the GBA algorithm includes the training data X, the initial parameters Θ(0) of the SAE, the neuron groups {Gk, pk}K k=1 with their target activation frequencies, and general first-order optimization method, denoted by Opt. Recall that each group has M/K neurons and {pk}k[K] is an exponentially decreasing sequence. In addition, the hyperparameters include (i) the total number of iterations , (ii) the batch size L, (iii) designated buffer size B, and (iv) parameters {γ+, γ, ϵ} for bias adaptation. The output of the algorithm is the final parameters Θ(T ) of the SAE. The algorithm runs for iterations. Within each iteration, we sample new mini-batch from the training data and use this mini-batch to update the weights of the SAE using standard optimizer Opt, such as AdamW (Loshchilov and Hutter, 2017), except that we do not update the biases by default. The biases {b(t)}t[T ] are only updated periodically via bias adaptation subroutine At (Algorithm Algorithm 2) that is triggered when the first buffer reaches its capacity B. We present the details of this algorithm in Algorithm 1. Implementation of GBA. For any t, within the t-th iteration, we first sample mini-batch Xt RLd from the training data X, where is the batch size. Then we normalize each row of Xt to unit ℓ2 norm, i.e., for each row of Xt, we set x/x2 (Line 6). This normalization is 11 beneficial for training stability across different data points. Then we compute the pre-activations y(t) RM for all neurons across the batch (Line 7). Recall that the pre-activation for neuron is defined in (3.1). Using data Xt and the current parameters Θ(t1), the pre-activations form matrix. In particular, (t1) RM is the weight matrix whose rows are {w(t1) }m[M ], is an all-one vector in RL. Based on these pre-activations, we can evaluate the outputs of and 1 the SAE according to (2.2) and the reconstruction loss in (2.3). In particular, the loss L(t) in Line 8 is computed as the average reconstruction loss over the normalized mini-batch Xt: L(t)(Θ) = 1 (cid:88) xXt Lrec(xℓ; Θ). (3.3) )M , a(t1) m=1, b(t1) pre }, but not the biases {b(t1) Then, we compute the gradient of this loss function and pass the gradient to the optimizer Opt to update the parameters Θ(t1) {b(t1)} (Line 9). In particular, we only update parameters {(w(t1) m=1. Here Opt(, ) is general first-order optimization algorithm where the first argument is the parameters to be updated and the second argument is the gradient. It may contain additional hyperparameters such as the learning rate, which we omit for simplicity. Furthermore, we use the buffer Bm to store the pre-activations of each neuron m. In the t-th iteration, we add the newly computed pre-activations {y(t) m,L} to Bm for each neuron (Line 10). Once the first buffer reaches its capacity (thus all buffers are of size B), the bias adaptation subroutine (Algorithm 2) is triggered to update the biases. We release all buffers when b(t) is updated (Line 12). m,1, . . . , y(t) }M Algorithm 1 Group Bias Adaptation (GBA) 1: Input: data X, initialization Θ(0), neuron groups and desired TAFs {Gk, pk}K k=1, first-order optimization algorithm Opt. 2: Hyperparameters: , L, B, γ+, γ, and ϵ. 3: For all [M ], initialize buffer Bm . 4: For = 1, . . . , : 5: 6: 7: 8: 9: 10: 11: 12: Update weights using standard optimization method, except for bias parameters. Sample mini-batch of training data Xt RLd and normalize each row to unit ℓ2 norm. Compute pre-activation: y(t) (t1)(X Compute loss using mini-batch: L(t) L(Xt, Θ(t1)) as defined in (3.3). Update all parameters except for biases: Θ(t) Opt(Θ(t1) {b(t1)}, L(t)). Update buffers: Bm Bm {y(t) m,1, . . . , y(t) m,L} for all m. Update the bias parameters when the buffer sizes reach B. If B1 B, update biases b(t) At(b(t1), B) according to Algorithm 2 and empty all ) + b(t1)1 RM L. b(t1) pre 1 buffers by setting Bm for all m. 13: Return the final SAE parameters Θ(T ). Efficient bias adaptation. The bias adaptation subroutine is outlined in Algorithm 2. Note that the neurons are split into groups, denoted by {Gk}K k=1, and each group Gk has designated target activation frequency (TAF) pk. In this subroutine, based on buffer Bm of pre-activations for each neuron m, we periodically adapt the bias bm based on the information stored in Bm to make sure that the activation frequency ppm of each neuron closely tracks the target TAF. As mentioned in 3.1, to achieve such goal we need to simultaneously ensure sufficient sparsity and 12 avoid over-sparsity of the neuron activations. In particular, in this subroutine, we propose to decrease the bias for overly active neurons and increase the bias for inactive neurons. Specifically, given the buffer = {Bm}m[M ] containing stored pre-activations for each neuron m, we first compute the activation frequency ppm as in (3.2). Suppose neuron belongs to group k. Then, we adjust the bias bm to let ppm move closer to the target TAF pk. We introduce two quantities: Maximum pre-activation rm = max{maxyBm y, 0} for neuron m. This value represents the highest pre-activation that neuron attains over the buffer Bm, with the truncation at zero ensuring non-negativity. Intuitively, rm reflects the strongest activation of neuron m, and it is used to guide the bias adaptation: if we aim to reduce bm such that the neuron barely activates (i.e., nearly dead) for all inputs in Bm, then the ideal new bias would be bm rm. In practice, we update the bias as bm γrm (with γ (0, 1)) to gradually decrease the activation frequency without completely deactivating the neuron. 1(rm > 0))1 (cid:80) rm for group k, which mGk is the average of the maximum pre-activations of all neurons in group k. This quantity effectively captures the aggregated response of neuron group to the data points in the buffer B. The computation of srk excludes dead neurons (i.e., those with rm = 0) and involves the information from all neurons in group k. Average maximum pre-activation srk = ((cid:80) mGk Based on these two quantities, we adapt the bias bm of each neuron in group as follows. Note that we initialize each bias bm to zero. We consider following two cases: (i) Decrease bias for overly active neurons: If neuron activates too often (ppm > pk), we reduce bm by γ rm, where γ (0, 1) is small hyperparameter. Here rm plays the same role as the gradient in standard optimization methods, providing sufficient amount of adjustment that drives the activation frequency ppm toward the target TAF pk. Multiplying by small factor γ, which plays the role of learning rate, ensures the bias is reduced gradually, preventing the neuron from becoming completely inactive. We further clamp bm at 1 to avoid overly large decreases in the bias. See Line 7 in Algorithm 2. (ii) Increase bias for inactive neurons: If neuron rarely activates (i.e., if ppm < ϵ, with ϵ = 106 in our experiments), we increase its bias bm by adding γ+ srk, where γ+ (0, 1) is the learning rate. This adjustment encourages the neuron to activate more frequently. In particular, when ppm is too small, we use the pre-activation information from the entire group Gk to increase the bias, which helps to avoid dead neurons. Moreover, to make sure that the bias does not become too large, we clamp bm at 0. See Line 8 in Algorithm 2. Practical implementation of bias adaptation. In practice, we perform bias adaptation every 50 gradient steps using the largest batch size permitted by the training hardware trade-off that balances memory consumption against the accuracy of neuron activation frequency estimation. Furthermore, we clamp each bias to the interval [1, 0], where the upper bound (0) maintains sparsity and the lower bound (1) prevents over-sparsification. While from the description of the algorithm, it seems that we need to store all the pre-activations in the buffer Bm for each neuron m, this is not necessary in practice. In particular, we only need to track ppm and rm as the buffer Bm increases, which can be iteratively updated as new mini-batches are processed. To see this, suppose that we have buffer Bm and then receive mini-batch of size with pre-activations 13 Algorithm 2 GBA Subroutine At(b, B) 1: Input: current bias RM , buffer = {Bm}m[M ] containing stored pre-activations for each neuron, neuron groups and desired TAFs {Gk, pk}K k=1, and hypterparameters γ+, γ, and ϵ. 2: Compute activation frequency, maximum pre-actions and their group average. 3: Compute the activation frequency ppm and maximum pre-activation rm for each neuron [M ]: ppm Bm1 (cid:80) yBm 1(y > 0), rm max(cid:8)maxyBm y, 0(cid:9). 4: Compute the average maximum pre-activation srk for each group [K]: srk (cid:16)(cid:80) mGk 1(rm > 0) (cid:17)1 (cid:80) mGk rm. 5: Adapt the bias for each neuron based on its activation frequency. 6: For each group = 1, . . . , and each neuron in group Gk: If ppm > pk then set bm max{bm γrm, 1}. 7: If ppm < ϵ then set bm min{bm + γ+srk, 0}. 8: 9: Return updated bias vector b. {y(t) m,1, . . . , y(t) m,L} for neuron m. Then, we can update ppm and rm as follows: ppm 1 Bm + (cid:16) Bm ppm + 1(y(t) (cid:17) m,i > 0) , (cid:88) i=1 rm max(cid:8)y(t) m,1, . . . , y(t) m,L, rm, 0(cid:9). This allows an efficient practical implementation of Algorithm 2 without the need to store all pre-activations in the buffer Bm for each neuron m. Although our algorithm involves few hyperparameters, they are easy to set and do not require much tuning as we have demonstrated above. The effectiveness of this grouping strategy is further demonstrated in 4. Our results indicate that it achieves comparable sparsity-loss trade-offs to state-of-the-art methods (e.g., TopK SAE) while yielding more consistent learned features across different random initializationshighlighting its robustness and reliability. We conclude this section by discussing two practical considerations regarding the grouping strategy. What if feature occurs more often than the largest TAF? Although some features may occur even more frequently than p1, we expect these cases to be rare, so dedicated neuron group is unnecessary. Instead, we rely on bias-clamping strategy that permits neurons to have ppm exceeding their TAFs when needed. For example, if neuron in the first group aligns with feature occurring more frequently than p1, by construction, the bias adaptation subroutine will continue to reduce its bias because the ppm > p1 condition is triggered. This process will continue to decrease bm until the neuron becomes completely inactive, which is undesirable. To avoid this case, we always clamp bm to be no less than 1. This strategy is effective in practice, as we observe in the real-world experiments, there are indeed several neurons that are enabled to learn features with frequencies exceeding the largest TAF. We defer the details to 4. Compared to directly setting group with very large TAF, this strategy is more suitable for capturing few extremely frequent features while ensuring better sparsity overall. Could the early groups dominate training? potential concern is that neurons in the early groups being activated more frequently might dominate training and reduce overall sparsity. Although this is reasonable concern, it is mitigated by the inherent selectivity of the neurons. In 6, we show via both theory and experiments that feature with occurrence frequency is primarily captured by neurons whose TAF pk falls within range close to . See also Figure 2 for an illustration. In other words, each neuron group is tuned to extract features whose frequencies lie within its designated exponential interval."
        },
        {
            "title": "4 Experiments on Qwen2.5-1.5B LLM",
            "content": "Training data and architecture of SAE. To demonstrate the effectiveness of our proposed method, we conduct series of experiments on the Qwen2.5-1.5B base model (Yang et al., 2024) using two datasets, Pile Github and Pile Wikipedia (Gao et al., 2020) with the first 100k tokens from each dataset. We attach an SAE to the output of the LLMs MLP block at layers 2, 13, and 26 with = 66k hidden neurons. That is, we train three separate SAEs for each of the datasets. The input and output dimensions of the SAEs are set to = 1536. To achieve optimal performance, we adopt the JumpReLU activation (Erichson et al., 2019; Rajamanoharan et al., 2024b) for all training methods (In B.2, we also provide detailed comparison between the use of ReLU and JumpReLU.). After preprocessing the data, we use = 100m tokens to train the SAEs. In other words, the training data of the SAEs are obtained by feeding the tokens into the LLM and collecting the MLP outputs at the specified layers. Implementations of four methods. We compare the performance of our proposed Grouped Bias Adaptation (GBA) method with the following baselines: the TopK, L1, and Bias Adaptation (BA) method. Here, the BA method is simply GBA using only one group with hyperparameter p. In other words, our GBA can be viewed as hyperparameter-free version of the BA method without explicitly setting the value of p. All these algorithms are trained using AdamW (Loshchilov and Hutter, 2017) with the same set of hyperparameters, including learning rate, weight decay, and batch size. In the implementation of the GBA algorithm, by default, we set the number of groups to = 10 and the target frequencies to form geometric sequence with the highest target frequency (HTF) set to p1 = 0.1 and the lowest target frequency (LTF) set to p10 = 0.001. The other hyperparameters of GBA, hyperparameters of the other methods, and details of AdamW training can be found in B.2. Evaluation metrics. We evaluate each method based on two metrics: (i) ℓ2 reconstruction loss, and (ii) the average fraction of activated neurons (equivalent to the average ℓ0 loss divided by the total number of neurons) in the trained SAE. Ideally, good SAE should probe the pareto frontier of these two metrics, achieving minimal reconstruction loss while maintaining low fraction of activated neurons. Through extensive experiments, we would like to answer the following questions: Q1 How does the proposed GBA method compare with the TopK, L1, and BA methods in terms of ℓ2 reconstruction loss and activation sparsity? Q2 How robust is the GBA method to the choice of hyperparameters, such as the number of groups and target frequencies? Q3 How consistent are the features learned by the GBA method across different runs with distinct random seeds? 15 Reconstruction loss and activation sparsity frontier. We first compare the normalized ℓ2 reconstruction loss and the average fraction of activated neurons across different methods The experiment results are presented in Figure 3, where we plot the average ℓ2 reconstruction loss against the average fraction of activated neurons for each method. We plot the results for the SAEs trained on data from Github dataset and the MLP outputs of layers 2, 13, and 26, and Wikipedia dataset with MLP layer 26. We note that there are two ways to measure the fraction of activated neurons for the TopK method: one based on the pre-activation values and another based on the post-activation values. See A.1 for details. For the other methods, measuring sparsity using preor post-activation values yields the same results. Without specification, we use the post-activation for the TopK method by default in the sequel when we say activation frequency/fraction. We summarize the key findings as follows, which answer Q1: (1) (GBA comparable to TopK) Our method performs comparably to the best-performing benchmark, TopK with post-activation sparsity. In addition, GBA outperforms TopK with preactivation sparsity. Specifically, when these methods have the same average fraction of activated neurons, the reconstruction of GBA (green star) is comparable to that of TopK with post-activation sparsity and significantly better than that of TopK with pre-activation sparsity. (2) (GBA outperforms L1) GBA is significantly better than the L1 method. When they have the same average fraction of activated neurons, GBA achieves lower reconstruction loss. (3) (GBA outperforms BA) Moreover, we compare our method with its non-grouped counterpart BA and observe that GBA consistently outperforms the non-grouped version across all experiments. This provides strong evidence that the grouping mechanism enhances both sparsity and reconstruction performance. Figure 3: The reconstruction error with respect to the average fraction of neurons activated per data point. All experiments are conducted using an SAE with 66k neurons. For the TopK method, we vary within {50, 100, 200, 300, 400, 500, 600}. For the L1 method, we vary the penalty coefficient within {0.1, 0.03, 0.01, 0.003, 0.001}. neuron is considered active in the pre-activation if 1(ym > 0), and in the post-activation if 1(SK(ϕ(ym)) > 0), where SK is an operator that selects the largest values and sets the remaining to zero across all {ϕ(ym)}M m=1. This distinction in sparsity between preand post-activation is relevant only for the TopK activation. See A.1 for details. An ablation study with varying configurations for the GBA method is presented in Figure 4. To see these findings, for any fixed value of the average fraction of activated neurons, TopK with post-activation sparsity achieves the lowest reconstruction loss among the benchmark methods. Note that all these methods involve sparsity-related tuning parameters, namely, in TopK, λ in L1, and in BA. Varying these parameters, we obtain the curves in Figure 3. It is clear that the curve for TopK with post-activation sparsity is the lowest, indicating that it achieves the best reconstruction loss for given fraction of activated neurons. In contrast, we do not explicitly tune the hyperparameters in our GBA method. Rather, we fix the number of groups and series of 16 target frequencies {pk}k[K] for the different neuron groups. We only report the results for default setting of these grouping parameters, and thus the results are shown as single point in Figure 3. Vertically comparing the results, we see that the GBA method achieves similar reconstruction losses as the TopK method with post-activation sparsity, best among the benchmark methods, and is strictly better than the rest of these methods. Robustness and nearly tuning-free. Given the results in Figure 3, it is unclear whether GBA is sensitive to the choices of and {pk}k[K], as raised by Q2. To address this question, we perform an ablation study on neuron grouping and target frequency (see Figure 4). Specifically, we vary the number of groups K, the HTF p1 and LTF pK for the GBA method. For the other parameters, we use the default configurations as in the loss-sparsity experiment. The detailed results are presented in Figure 4. In the left plot of Figure 4, we plot the ℓ2 reconstruction losses against the average fraction of activated neurons for different choices of HTF, LTF, and the number of groups. We group these results according to the value of HTF, and for each HTF, we plot the results for each value of with different color. As result, for each fixed HTF and K, there are three scatter points, each corresponding to different LTF. We also plot the curve corresponding to the TopK method with post-activation sparsity for comparison, which is obtained by varying the value of the number of activated neurons in TopK. Figure 4: Ablation study illustrating the impact of neuron grouping and the highest target frequency for Github-Layer 26. For each run, we partition the neurons into #Groups (K) groups and set the target frequencies as geometric sequence between the Highest Target Frequency (HTF) and Lowest Target Frequency (LTF). The LTFs are chosen from {1 103, 5 103, 1 104} and the HTFs are chosen from {0.05, 0.1, 0.3, 0.5}. (Left) Illustrations for the ℓ2 reconstruction loss versus the average fraction of activated neurons, grouped by HTF. For each HTF, different colors represent different values of K, while dots of the same color correspond to different LTFs. (Middle & Right) Illustrations for ℓ2 reconstruction loss and the average fraction of activated neurons for different choices of with HTF and LTF fixed. Overall, these plots show that GBA is nearly tuning free, meaning that the performance is robust to the variations in the grouping parameters as long as HTF is sufficiently high (e.g., 0.5) and is sufficiently large (e.g., 10 or 20). From this plot, we observe general pattern: as HTF increases, the scatter points with the same color converge together and they overall move downwards. This means that increasing HTF stabilizes performance, and that variations in the LTF have relatively minor effect when HTF is sufficiently high, e.g., HTF = 0.5. low HTF may hinder the recovery of frequent features (e.g., HTF = 0.05 results in higher reconstruction loss). Thus, we recommend higher HTF, ideally within the range of 0.1 0.5. This is far less restrictive than tuning TopKs [50, 600] across 66k neurons. In fact, 0.5 represents the upper limit for HTF since, without sparsification (i.e., setting bm = 0), the average activation frequency would approximate coin flip (yielding roughly equal proportions of negative and positive pre-activations). Moreover, we observe that the scatter points roughly align with the curve of TopK, especially for = 10 and = 20. This validates that with adequate grouping, the GBA method achieves performance comparable to TopK. We also observe that scatter plots with different colors converge together as HTF increases. This indicates that the performance of GBA is also insensitive to the choice of K, which matches what we observe in Figure 4 (middle & right) that both the reconstruction loss and the fraction of activated neurons stabilize when exceeds 10. These ablation studies confirm that our GBA method is nearly tuning-free. We only need sufficiently high HTF (e.g., 0.5) to capture the maximum feature activation frequency, modestly low LTF to establish lower bound and sufficiently large number of groups. We summarize the key findings as follows. (4) When HTF and LTF are properly chosen (e.g., high HTF and modestly low LTF), with an adequate number of groups, the GBA method achieves performance comparable to TopK, and the performance becomes largely insensitive to the specific choices of these parameters. Consistency of recovered features. Furthermore, we answer Q3 by assessing the consistency of the learned features across independent runs with different random seeds. Since the ground truth features are unavailable, consistency serves as proxy for the reliability of the training method. We measure the consistency of features using the Maximum Cosine Similarity (MCS), which is defined in A.2 mathematically. neuron from one run is considered to have an MCS of at least τ if similar neuron (cosine similarity τ ) can be found in every other run. If large percentage of neurons exceeds this MCS threshold, the features learned by the SAE are consistent across different runs. Such percentage is referred to as the neuron percentage, which is defined in (A.3). To avoid the influence of rarely activated neurons, we compute the neuron percentage only on subsets of the total neurons. In particular, we restrict to the top-α proportion of neurons, based on the maximum activation or neuron Z-score computed across the validation set. These two metrics are defined in (A.1) and (A.2), respectively. In particular, the maximum activation of neuron is defined as the maximum value of the pre-activation of the neuron across the validation set. The Z-score of neuron is defined as the largest Z-score of the post-activation values of neuron across the validation dataset. higher Z-score indicates that the neuron has higher maximum activation relative to its mean and variance over the whole set of validation tokens, suggesting that it is more specific to particular subset of input tokens. Figure 5: Percentage of neurons attaining Maximum Cosine Similarity (MCS) above specified threshold for Github-Layer 26. The MCS is computed across three independent runs with distinct random seed initializations. neuron is considered to have an MCS exceeding threshold if its pairwise MCS with neurons from every other run surpasses that threshold. higher percentage of neurons exceeding given MCS threshold indicates more consistent feature recovery across different runs. The percentage is computed based on top-α neurons selected by either maximum activation or Z-score, where α is chosen from {0.3%, 0.05%}. higher curve indicates better consistency. In Figure 5, we plot the percentage of neurons with MCS above threshold τ , restricted to the top-α neurons in terms of either maximum activation or Z-score. Here we set α = 0.3% and 0.05%, 18 respectively. We plot the results for the SAEs trained with methods such as GBA, TopK, and L1 on the MLP output of layer 26 of the Github dataset. The curves are generated by varying the threshold τ from 0.6 to 0.9. As shown in Figure 5, we observe that the TopK method has the lowest MCS overall, and is thus seed-dependent. GBA clearly outperforms TopK in terms of consistency, achieving higher percentage of neurons with MCS above given threshold. The L1 method is more consistent than TopK uniformly, and also more consistent than GBA in three of the four cases. However, when focusing on the top-0.05% neurons selected by the maximum activation criterion, GBA surpasses L1 in terms of consistency. We summarize the key findings as follows, which answer Q3: (5) As the TopK method is shown to be seed-dependent (Paulo and Belrose, 2025), it has the lowest MCS overall. Our GBA method outperforms TopK in achieving higher percentage of neurons with high MCS. (6) The L1 method has been shown to be more consistent than TopK (Paulo and Belrose, 2025) uniformly and also more consistent than GBA in three of the four cases. However, when focusing on neurons with the top-0.05% activations, our GBA method surpasses the L1 method. Additional results. We further provide additional studies on the neurons learned by the GBA and TopK methods in terms of the three metrics used above: maximum activation, Z-score, and maximum cosine similarity across different runs with different random seeds. These metrics are computed based on the validation part of Github dataset, with the hook position at the MLP output of layer 26. For the Z-score, we compute the largest value among the tokens in the validation set, and for the maximum cosine similarity, we compute the smaller value among the two additional runs. See A.2 for rigorous definitions of these metrics. In addition, for each neuron m, we also compute the activation fraction (or activation rate), which is defined as the fraction of tokens where pre-activations of neuron are non-negative. Thus, for each neuron m, we have four metrics: maximum activation, Z-score, maximum cosine similarity, and activation fraction. We generate scatter plots by plotting the Z-score against the other three metrics. The results for GBA and TopK are presented in Figure 6 and Figure 7, respectively. Z-score v.s. maximum activation. In Figure 6 (left), we present the scatter plot of the Z-score versus the maximum activation of neurons, which is shown in the logarithmic scale with base 10. We observe an almost linear relationship between the two metrics, indicating that neurons with higher Z-scores also exhibit higher maximum activations. Notably, at the upper end of the distribution, subset of neurons attains even higher Z-scores. This behavior suggests that these neurons capture cleaner feature and fire exclusively when the feature is present. By the definition of the Z-score, for neurons with the same maximum activation, higher Z-score implies lower variance. In other words, these neurons activations tend to be bimodalpredominantly near baseline when the feature is absent and significantly higher when the feature is present. This is consistent with the dashboard results for individual neurons as we shown in Figure 8. Z-score v.s. activation fraction. In Figure 6 (middle), we present scatter plot of the Z-score versus the activation fraction, which is shown in the logarithmic scale with base 10. Neurons with higher Z-scores generally exhibit an activation fraction near 0.01 (around 2 in the figure). This suggests that they predominantly capture infrequent yet salient features. Moreover, the neuron grouping mechanism effectively adapts to diverse feature occurrence frequencies, underscoring the adaptivity of our approach. Additionally, we observe several neurons with activation frequencies 19 Figure 6: Scatter plots illustrating neuron properties for the GBA method: Z-score versus Maximum Activation, Fraction of Non-negative Pre-Activations (i.e., activation frequency), and Maximum Cosine Similarity across different runs with different random seed. The 66k-neuron SAE is trained on the GitHub dataset with hook at the MLP output of layer 26. exceeding the HTF of 0.1 (by our default configurations). This behavior is facilitated by the biasclamping mechanism, which prevents biases from becoming excessively negative, as discussed in 3. Z-score v.s. MCS. In Figure 6 (right), we present scatter plot of the Z-score versus maximum cosine similarity across different runs with distinct random seeds. Recall that higher maximum cosine similarity indicates more consistent feature recovery, and we observe that neurons with higher Z-scores tend to exhibit higher levels of consistency. This result supports the effectiveness of GBA in reliably extracting salient features. Figure 7: Scatter plots illustrating neuron properties trained using TopK with = 300. The other configurations are the same as Figure 6 We further analyze the neurons obtained using the TopK method with = 300, chosen because its reconstruction loss is comparable to that of the GBA method. In particular, as shown in Figure 7 (middle panel), neurons from TopK are generally less sparse than those from GBA. Moreover, the proportion of neurons achieving maximum cosine similarity above 0.9 is lower for TopK, echoing the consistency results presented in Figure 5."
        },
        {
            "title": "5 Identifiability of Features",
            "content": "In addition to empirical results, we establish theoretical guarantees for the proposed algorithm under the statistical model in (2.1). In particular, as we will show in the next section, the singlegroup variant of Algorithm 1, i.e., the Bias Adaptation algorithm, can successfully recover all monosemantic features in , thus providing positive answer to the question raised in 2. 20 Figure 8: Feature dashboard for neuron 4688 in the GBA-SAE model trained on Pile Github at layer 26s MLP output position. This neuron exhibits clear bimodal activation pattern, and is activated before outputting the class token. Before establishing the feature recovery results, an immediate question is whether the true features are statistically identifiable under the statistical model in (2.1), especially under the superposition regime where the number of features is larger than the embedding dimension d. Without answering this question, it would be vacuous to claim that the desired features can be learned from the data. We examine the identifiability of the monosemantic features as follows."
        },
        {
            "title": "5.1 Main Results on Identifiability of Features",
            "content": "Below, we discuss three intrinsic ambiguities in the representation of features: Feature permutation: The order in which features appear is arbitrary. For example, if we swap two rows of the feature matrix and adjust the corresponding entries in the coefficient matrix accordingly, the overall product HV remains unchanged. This demonstrates that features can be permuted without affecting the data representation, which captures the homogeneity of features. Feature scaling: Individual features can be scaled arbitrarily while compensating with inverse scaling in the coefficients. That is, if one multiplies feature vector by constant and divides the corresponding coefficient by the same constant, the resulting product still represents the same underlying concept. This rescaling ambiguity means that only the direction of the feature vector (and not its magnitude) is uniquely determined. Linear combination ambiguity: given feature may be equivalently represented as positive linear combination of other features. Under this scenario, different decompositions of the data are possible that yield the same aggregated representation. This ambiguity reflects the possibility that one feature could be fragmented into several components, each capturing part of the original concept, complicating the identification of minimal and unique set of features. In light of these ambiguities, we introduce the notion of ε-identifiability of features, which is relaxed version of strict identifiability that allows for small perturbations in the feature directions. Definition 5.1 (ε-identifiability). Let be class of pairs of matrices (H , ) where RN has nonnegative entries and Rnd where hidden dimension is an arbitrary positive integer. pair (H, ) with hidden dimension is said to be ε-identifiable within if for any other pair (H , ) with hidden dimension (not necessarily the same as n) satisfying HV = , there exists permutation matrix Rnn such that 1 cos(V, ΩQV ) ε. Here, each ωi is nonnegative row vector and the cosine similarity is defined as in (1.1). and nonnegative block-diagonal matrix Ω = diag(ω1, . . . , ωn) Rnn The notion of ε-identifiability implies that if feature matrix is ε-identifiable with respect to coefficient matrix H, then for any alternative factorization (H , ) of the dataset X, the matrix must either be equivalent to (up to permutation and rescaling) or represent refined splitting of each feature in , with deviations bounded by ε. To see this formally, we note that if the feature matrix is εidentifiable, then for any other feature matrix that factorizes the dataset X, there exists permutation matrix and block-diagonal matrix Ω such that ΩQV (up to scaling). The block-diagonal matrix Ω naturally partitions the rows of QV into disjoint groups. Each group is then linearly combined by ωi to form single feature in this is the feature splitting phenomenon, which was previously observed in Bricken et al. (2023a) and is particularly pronounced when the neuron size is large. In addition, in the special case where = n, is square permutation matrix and Ω is diagonal scaling matrix, and thus and are essentially identical up to permutation and rescaling. Therefore, our definition of feature identifiability captures all the three ambiguities discussed above. Figure 9: Illustration of the feature splitting phenomenon. Since always holds (due to the definition of the block-diagonal matrix Ω), the identifiable feature matrix is minimal without any redundant features. Moreover, is also unique in the sense that any alternative feature matrix with the same minimal set of features is equivalent to up to permutation and rescaling. Since there is inherent ambiguity in the scaling of features, our focus is solely on recovering the direction of these features. Next, we detail the conditions on the data that guarantee the identifiability of features under our framework. Definition 5.2 (Decomposable Data). We say that the data matrix RN is decomposable if there exists positive integer N, nonnegative matrix RN and feature matrix Rnd such that = HV. Moreover, each row of has unit ℓ2 norm and the ℓ2 norm of each row of is Θ( d). Furthermore, the coefficient matrix RN satisfies the following three conditions: + (H1) Row-wise sparsity: maxℓ[N ] Hℓ,:0 = with = Θ(1). (H2) Non-degeneracy: For every [n], H:,i1/H:,i0 = Θ(1). (H3) Low co-occurrence: ρ2 := maxi=j 1{H:,i = 0}, 1{H:,j = 0}/H:,i0 n1/2. In addition, we further assume that the feature matrix Rnd satisfies: (V1) Incoherence: For all = j, vi, vj/(vi2 vj2) = o(1). The nonnegativity condition on is natural and removes the ambiguity regarding the sign of the features, as the opposite direction of feature would often lead to totally different or even contradictory concept. The Row-wise Sparsity (H1) assumption ensures each data point cannot contain more than features, and it is essential for sparse recovery. The Non-degeneracy (H2) condition ensures that when feature is present in data point, its average magnitude is sufficiently large. When either of these conditions is violated, accurately isolating individual features from the data becomes significantly more challenging, and even impossible. Finally, the 22 Low Co-occurrence (H3) and Incoherence (V1) assumptions ensure that two different features are distinct and well separated either in their occurrence or in the feature directions. We provide more discussions on (H3) in 5.2. One thing to note is that the Incoherence (V1) condition can be viewed as generalization of the orthogonality condition to almost orthogonal features, which is common structural assumption in the sparse recovery literature (Candès and Plan, 2009; Marques et al., 2018). With the necessary definitions and conditions established, we now present the main theorem that establishes the identifiability of the features. Theorem 5.3. For decomposable dataset RN d, define to be the class of pairs (H , ) that satisfy conditions in Definition 5.2 with arbitrary hidden dimension n. Then there exists pair (H, ) such that is ε-identifiable within with ε = o(1). See for proof. The key step is to show that under Conditions (H1) to (H3), is approximately pseudo-invertible. Hence, we can construct suitable linear transformation such that AX = (AH)V for identifying the features. In fact, to satisfy all the conditions in Definition 5.1, the number of data must be no less than the number of features (i.e., n). Otherwise, the data matrix would not have enough information to recover the features. Related work on identifiability. We compare our identifiability results with those in the literature of sparse dictionary learning. We find the following salient differences: Model-free: Our identifiability result does not require any probabilistic structure on either the coefficient matrix or the feature matrix . In contrast, the work of Spielman et al. (2012) assumes the coefficient matrix is drawn from Bernoulli-Gaussian/Rademacher model, while Schnass (2014) assume to come from unit norm tight frame. variety of probabilistic assumptions can also be found in Cohen and Gillis (2019); Gribonval et al. (2015). Unknown number of features: Our identifiability result covers the case where the number of features is unknown, and we are essentially identifying the minimal set of features. All the works mentioned above assume the number of features is known and fixed for identifying the set of features only up to permutation and scaling. Therefore, the previous results do not capture the interesting phenomenon of feature splitting in the notion of identifiability, as discussed before. Full spectrum: Our identifiability result holds for the full spectrum in terms of the relative scale of the number of features versus the dimension d, and also covers the interesting superposition regime where > d. In contrast, Cohen and Gillis (2019) focus on the case = where the feature matrix is square matrix, while Gribonval and Schnass (2010) show the uniqueness of the factorization only when the feature matrix is overcomplete d. Given these differences, we believe our identifiability result still holds significant novelty for better understanding the identifiability of features in the SAE framework, particularly when the number of features is also unknown."
        },
        {
            "title": "5.2 Dicussion on Feature Co-occurrence",
            "content": "Large feature co-occurrence often happens under mixture of concepts or when one concept is highly correlated with another. From the perspective of data graph where the nodes are data points and each edge represents that two data points share at least one common feature (as shown implies that each clique corresponding to feature has in Figure 10), the condition ρ2 1/ 23 significantly sparser connections to nodes in other cliques. Hence, all cliques are well separated from each other. Indeed, if one wants to run our SAE training algorithm to successfully recover all the features, 1/ is also the critical threshold of co-occurrence, as we will show in 6. This fact demonstrates fundamental connection between feature identifiability and SAE learnability. In the following, we conduct experiments using the proposed Algorithm 1 (with one group only) on datasets with Gaussian features and different levels of feature co-occurrence quantified by ρ2. The algorithm for training the SAE is described in 3. Figure 11 (left) shows that the Feature Recovery Rate (FRR) sharply declines when ρ2 approaches 1/ n, which is in line with our theoretical predictions. Furthermore, the histograms in Figure 11 (middle & right) demonstrate that clear peak in the maximum cosine similarity between feature vectors and neuron weights in the SAEindicative of successful feature learningappears only when ρ2 < 1/ n. Figure 10: An illustration of data graph that contains cliques with sparse interconnections. Figure 11: Illustration of the feature co-occurrence condition. (Left) Feature Recovery Rate (FRR) versus ρ2. (Middle & Right) Histograms of the maximum cosine similarity between neurons and features for ρ2 < 1/ n. Here, we take (n, d, M, s, p) = (256, 48, 2048, 3, 0.01) and feature learning threshold 0.946. Additional details on the experimental settings can be found in 6.3, and the feature learning threshold is detailed in A.2. The Random Avg is the averaged max cosine similarity between features and randomly initialized neurons. and ρ2 = 1/"
        },
        {
            "title": "6 Dynamics Analysis: SAE Provably Recovers True Fea-\ntures",
            "content": "In this section, we aim to provide theoretical guarantees of feature recovery for the SAE trained with simplified version of the GBA algorithm. We focus on the case where the data matrix admits factorization = HV with identifiable monosemantic features (Definition 5.2). On high level, our goal is to prove that the SAE trained with our algorithm provably recovers all the monosemantic features in under proper conditions. We will further examine the implications of these conditions from both theoretical and practical perspectives. In the following, we first introduce Modified Bias Adaptation (BA) algorithm, which is simplified version of the GBA algorithm with only one group of neurons and fixed TAF. Then, we provide theoretical results on the training dynamics of Modified BA, which is accompanied by synthetic experiments to validate the theoretical findings."
        },
        {
            "title": "6.1 Simplification for Theoretical Analysis",
            "content": "We make several simplifications to the setup of SAE to facilitate theoretical analysis. 24 Decomposible data with Gaussian features. We assume that the data matrix RN is decomposable in the sense of Definition 5.2. Moreover, we assume that the feature matrix Rnd has i.i.d. entries following (0, 1). Such choice of satisfies the incoherence condition (V1). Simplified SAE model. We consider simplified version of the SAE model (x; Θ) in (2.2), where the only trainable parameters are the weights {wm}M m=1. (Small output scale) We assume that the output scale am = and is sufficiently small. When computing the gradient, we rescale the L(Θ) back to its original scale by multiplying a1. (Fixed pre-bias) We fix the pre-bias bpre = 0, as the data matrix is centered. (ReLU-like smooth activation) We use smooth, ReLU-like activation function ϕ (see Definition A.2 for details). One example is the softplus activation ϕ(x) = log(1 + exp(x)). (Fixed bias) For each neuron [M ], we fix the bias bm = < 0 throughout training, where is negative scalar whose value will be specified later. Besides, as shown in Definition 5.2, each data point xℓ is combination of at most monosemantic features, where = Θ(1). As we will show below, we further assume that the occurrence of each monosemantic feature is relatively balanced. Thus, it is reasonable to try using version of GBA algorithm with single group to recover . We introduce the algorithm as follows. by + η g(t) w(t1) + η g(t) w(t1) 2 Modified Bias Adaptation (BA) algorithm. Recall that Bias Adaptation (BA) algorithm is special case of GBA algorithm with only one group of neurons and fixed TAF p. Here we determine the value of implicitly by choosing fixed bias < 0, and they are related by = Φ(b), where Φ() is the tail probability function for Gaussian distribution. That is, Φ(t) = P(Z t) for (0, 1). Given the data matrix and the SAE model (x; Θ), we can compute the loss function L(Θ) m=1. Since only the directions of the i=1 matter, we adopt spherical gradient descent to update the weights. That is, starting }m[M ] uniformly sampled from the unit sphere Sd1, for any 1, in as in (2.4) and its gradient with respect to the weights {wm}M features {vi}n from the initial weights {w(0) the t-th iteration, we update each w(t1) Modified BA: w(t) = , where g(t) = lim a0 a1wmL(Θ(t1)). (6.1) Here, g(t) is the rescaled negative gradient of the loss function L() in (2.4) with respect to the weight wm of neuron at iteration t. We will show that, under proper conditions, for any feature vi, there exists at least one neuron mi [M ] such that the alignment between w(T ) mi and vi is arbitrarily close to one when is sufficiently large. Before we proceed to the main theoretical results, we make several remarks on the above simplifications for theoretical analysis and their implications. Fixed bias is without loss of generality. As we consider Gaussian features and always normalize w(t) to the unit sphere, it can be shown using the Gaussian conditioning technique that the preactivations remain approximately Gaussian, i.e., ym(xl) = w(t) , xℓ + (b, 1) for constant number of iterations t. See 7 for details. Therefore, to achieve the desired TAF p, it is without loss of generality to fix the bias < 0 such that Φ(b) = p, which means that the pre-activations of each neuron will be non-negative for approximately fraction of the data points throughout the training. 25 Smooth ReLU-like activation approximates ReLU. We choose smooth activation function for technical convenience. These activations can be viewed as smooth approximation to the ReLU function, as illustrated in Figure 12. This class of activations encompasses functions like Softplus and shifted ELU, and closely resembles the standard ReLU activation function. We believe that more refined analysis can also be applied to the standard ReLU activation, but we leave this as future work. Small output scale decouples neuron dynamics. Following common paradigm in the literature (see e.g. Chen et al. (2025); Lee et al. (2024)), we assume that the output scale of the SAE is sufficiently small. The benefit of this condition is that it decouples the dynamics among the neurons, making the analysis more tractable. Specifically, the rescaled negative gradient of the loss L(Θ) is given by Figure 12: Smooth ReLU-like activations gm = a1wmL(Θ) = (cid:88) ℓ=1 (cid:0) φ(w mxℓ; b)xℓ ψm(xℓ; Θ) (cid:1) a0= (cid:88) ℓ= φ(w mxℓ; b)xℓ , (6.2) where we define φ(, ) and ψm(; Θ) as φ(u, v) = ϕ(u + v) + ϕ(u + v) , ψm(x; Θ) = ϕ(w mx + b) mf (x; Θ) + ϕ(w mx + b) (x; Θ) . Here, φ : (cid:55) is decoupled term that depends only on each individual neurons weight and bias, while ψm : Rd (cid:55) Rd is coupling term that captures the interaction between the neuron and the rest of the network. Since the scale of (x; Θ) is proportional to a, this coupling term is negligible when is small. As result, when is infinitesimally small, each neuron evolves independently of the other neurons. Furthermore, thanks to the decoupled dynamics, the restriction to single group with fixed TAF does not result in any loss of generality, as the analysis of multiple groups is straightforward extension. Limitations: overlooking benefits of neuron correlation. While having small decouples the dynamics and thus simplifies the analysis, allowing neuron correlation can in fact be beneficial for feature learning. As we show in Figure 15, when setting = 1 in simulation experiments, thus allowing neuron correlation, the SAE can successfully recover all features with much smaller than the theoretical requirement. To see this benefit, we rewrite the gradient in (6.2) as gm = (cid:88) (cid:16) ℓ=1 ϕ(w mxℓ + bm)Id + ϕ(w mxℓ + bm) xℓw (cid:17) (cid:0) xℓ (xℓ; Θ) (cid:1). Once feature is learned by the network, the term (xℓ; Θ) , containing the feature cancels out the contribution of from xℓ . That is, xℓ (xℓ; Θ) no longer contains the feature v. As result, under the incoherence condition (V1), the correlation between gm and becomes negligible for each m, thus preventing any neuron from learning the same feature again and improving neuron utilization efficiency. We will revisit this point in 6.3.2 with simulation results. Our current theoretical analysis does not capture this benefit, which is left as an open question for future work to explore."
        },
        {
            "title": "6.2 Main Theorem on Training Dynamics",
            "content": "Intuitively, to recover feature v, it has to appear in sufficiently many data points with sufficiently large coefficients. To characterize this intuition, we introduce two key quantities based on the coefficient matrix H. First, for each feature index [n], let Di = {l [N ] : Hl,i = 0} be the set of data indices that contain feature vi. The occurrence of the feature vi is thus given by Di/N . We define the maximum feature occurrence as the largest occurrence among all features, i.e., ρ1 = max i[n] (cid:8)Di/N (cid:9) = max i[n] (cid:26) 1/N (cid:88) l[N ] (cid:27) 1{Hl,i = 0} . (6.3) To ensure each feature vi appears in sufficiently many data points, we require that the occurrence of each feature is comparable to ρ1, i.e., Di/(ρ1N ) is not too small for each [n]. Second, to measure the magnitude of coefficients associated with each feature, we define the cut-off level for the feature as Figure 13: Relationship between s, and hi with different concentration level in Hs nonzero entries empirical distribution (shadow). less concentrated leads to larger and hi. (cid:110) hi := max 1 : 1 Di (cid:88) lDi 1{Hl,i h} polylog(n)1(cid:111) . (6.4) Intuitively, hi is critical threshold such that, among all data points containing vi, at least polylog(n)1 fraction of them have coefficients no smaller than hi. In other words, hi reflects the magnitude of coefficients associated with feature vi, within the subset of data points where vi is present. Thus, hi can effectively be viewed as notion of signal strength for feature vi, and we should require that hi is not too small for each [N ]. Furthermore, we additionally introduce global quantity called the concentration coefficient = h(H), whose definition is technical and deferred to (F.1) in the appendix. Intuitively, characterizes the global concentration level of nonzero entries in H. For now we can intuitively understand it as the variance of the nonzero entries in H, and thus will increase when the nonzero entries in are less concentrated. With these definitions, we are now ready to state the main theorem on the training dynamics. Theorem 6.1. Let = HV be decomposable in the sense of Definition 5.2 with RN satisfying all the conditions therein, and further assume that Rnd has i.i.d. entries following (0, 1). For this X, we train the SAE with Modified BA given in (6.1). Let ς, ε (0, 1) be any small constants. We assume that the number of neurons is sufficiently large: Network Width: (cid:26) log log max i[n] b2 2(1 ε)2h2 log (cid:27) + 1 . (6.5) Moreover, we assume that the learning rate η satisfies log η (b2/2 log ) and that the bias < 0 is set to satisfy the following condition: Bias Range: 1 b2 2 log max (cid:110) 1 2 + h2 2 , 2(1 + ε)2h2 , 1 (1 ς) log log (cid:111) . (6.6) Furthermore, we assume the coefficient matrix satisfies the following feature balance condition: 27 Feature Balance: Di ρ1N polylog(n)1, h2 log log(n) log(n) , [n]. (6.7) Then, it holds with probability at least 1 n4ε over the randomness of that for any feature [n], there exists at least one unique neuron mi such that after at most = ς 1 iterations, the alignment between the weights of neuron mi and the feature vector vi satisfies w(T ) mi , vi/vi2 1 o(1). See for detailed proof of this theorem. Theorem 6.1 shows that under appropriate conditions, Modified BA provably recovers all monosemantic features within constant number of iterations. These conditions include that (i) the network is sufficiently wide compared to the number of features as specified in (6.5), (ii) the bias is chosen within certain range as specified in (6.6), and (iii) the coefficient matrix satisfies the feature balance condition in (6.7), ensuring that each feature appears frequently enough with sufficiently large coefficients. We revisit the theoretical and empirical implications of these conditions in 6.3. To our best knowledge, this theorem is the first theoretical result that proves SAE training algorithm can provably recover all monosemantic features."
        },
        {
            "title": "6.3 Key Conditions for Reliable Feature Recovery",
            "content": "We examine the three key conditions in Theorem 6.1 that ensure reliable feature recovery from both theoretical and empirical perspectives. Simulation setup. We generate synthetic data = HV satisfying the assumptions in Theorem 6.1. In the default setting, each row of contains exactly nonzero entries, each with value 1/ s, and the support of each row is chosen independently at random. We implement the BA algorithm with fixed TAF p, where the SAE adopts the ReLU activation. We fix the output scale am = 1 for all [M ] and the pre-bias bpre = 0, and initialize the weights w(0) uniformly on the unit sphere Sd1 with bias b(0) = 0. The details of the simulation setup are deferred to B.1. To evaluate feature learning of neuron m, we use the Max Cosine Similarity (MCS) metric. For any neuron m, MCS is defined as maxi[n] wm/wm2, vi/vi2. Thus, MCS measures how well neuron aligns with the most aligned feature in . We say neuron is aligned with some feature if the MCS for that neuron exceeds certain threshold. To evaluate overall feature recovery, we use the Feature Recovery Rate (FRR) metric, defined as the proportion of features that are aligned with at least one neuron. See A.2 for more details on these metrics and the choice of thresholds."
        },
        {
            "title": "6.3.1 Bias Range: Implications on Target Activation Frequency",
            "content": "High and low superposition regimes. Recall from 6.1 that the pre-activation is approximately Gaussian, i.e., w(t) , xℓ + (b, 1). As result, the TAF should satisfy = Φ(b) exp(b2/2) by the Gaussian tail estimate. Combining this with the Bias Range condition in (6.6), we conclude that this condition on effectively implies that should satisfy n1 min(cid:8)n(1+h2 )/2, dn1(cid:9). (6.8) Here the lower bound n1 captures the ideal activation frequency. To see this, note by the way is generated, each feature appears in roughly s/n fraction of data points, and thus the optimal TAF should be Θ(n1). This feasible range of TAF is visualized in Figure 14 (Right) with different values of h, where the yellow area represents the feasible region. We discuss two regimes based on the relative scale between and n: (High superposition regime). When is relatively small compared to n, the upper bound in (6.8) is attained at d/n, while the lower bound is n1. Thus, the feasible TAF is between 1/n and d/n, as shown in the left half of Figure 14 (Right). We call this the high superposition regime because larger number of features are stored in relatively low-dimensional space, leading to high superposition. Notably, in this regime, the upper bound for the feasible TAF grows linearly with d. (Low superposition regime). When is relatively large compared to n, the upper bound in (6.8) is attained at n(1+h2 )/2, while the lower bound is still n1, as is shown in the right half of Figure 14 (Right). Thus, the feasible TAF is between 1/n and n(1+h2 )/2. We call this the low superposition regime because smaller number of features are stored in relatively highdimensional space, leading to low or negligible superposition. Notably, in this regime, the feasible TAF is upper bounded by flat line that does not depend on d. Moreover, in the extreme case where is super small, the transition between the two regimes occurs at = by equating the two terms in the upper bound of (6.8). Therefore, we can roughly view = as the transition point between the two regimes. Also, the upper bound for the feasible TAF will approach n1/2 as approaches zero in the low superposition regime. Empirical results. Our empirical results corroborate the above theoretical insights. We plot the heatmap of FRR under various values of and in the left two plots of Figure 14, where we show the results for the low and high superposition regimes, respectively. Moreover, in the high superposition regime, (6.8) asserts that valid is under linear function of d. This is exactly the case in Figure 14 (Middle), where the region with high values of FRR is under line on the (d, p) plane. We also notice some learnable regimes that are not captured by our theory in the low superposition regime > n. This could be due to two facts: (i) the algorithm used for the synthetic experiment is slightly different from what is proved by theory (ii) our theory is positive result in nature, and our condition does not rule out the possibility that there are other learnable regimes. We leave the investigation to future work. Figure 14: (Left & Middle) FRR heatmaps for the BA algorithm under various TAFs and dimensions with both axes in log scale. Here, we set (n, M, s, h) as (128, 512, 3, 1/ 3) and (65536, 2.62 105, 3, 1/ 3) respectively. (Right) Theoretical learnable region of (6.8) (marked in yellow) for different h. (Left) Under low superposition, the learnable region is (n1, n2/3). (Middle) Under high superposition, the learnable region expands as grows. Both match our theory. In our synthetic experiments, as we regard as constant, each feature Neuron selectivity. appears in the data points at Θ(1/n) frequency. Thus, we can rewrite (6.8) in terms of feature frequency = 1/n. That is, when each feature appears in the data points with frequency , 29 for the BA algorithm to recover the features, the desired TAF should be chosen in the interval (f, min{f (1+h2 )/2, df }). The result implies that feature learning is selective: feature with frequency is more likely to be learned by neuron with TAF that is higher than and roughly less than . This fact explains why GBA training is not dominated by the first group with the highest TAFa feature tends to be learned by neuron group with matching TAF. Interestingly, this is analogous to the physical phenomenon of resonance. Each neuron group acts as resonator tuned to specific intrinsic frequency (TAF), while each feature acts like sound with its own frequency (occupancy frequency). feature is therefore preferentially learned by the group whose TAF resonates with the features natural occurrence frequency. Take-away from Bias Range Condition (i) In high superposition regime where is small, lower TAF is required for feature recovery. (ii) Neurons are selective by aligning only with features whose occurrence frequencies fall within their designated target range."
        },
        {
            "title": "6.3.2 Feature Balance and Network Width",
            "content": "In Theorem 6.1 we present sufficient conditions for recovering all monosemantic features. If we only care about recovering particular feature vi, as we show in Theorem F.3 in appendix, it suffices to replace (6.5) and choose smaller such that log b2 2(1 ε)2h2 + log n, (6.9) where ε is small constant. In addition, we also need the Feature Balance condition in (6.7). As result, the learnability of each individual feature involves three key factors: (1) the network width , (2) the feature cut-off level hi, defined in (6.4), and (3) the relative occurrence of the feature defined in (6.3) We conduct controlled experiments to assess the impact of these factors. Network width and feature cut-off level hi. We test how scales with hi in Figure 15 (Left). We observe an exponential increase in the required network width as the 1/h2 increases, which again matches our theoretical prediction. In particular, we plot the heatmap of FRR with respect to and 1/h2 , where is shown in logarithmic scale. We observe that the region with high values of FRR is above line in terms of (1/h2 , log ), which is consistent with (6.9). Moreover, in the special case where Hℓ,i {0, 1/ = s. This relationship also suggests that the required network width might scale exponentially with the sparsity s. However, if feature vi has Significant Strengthe.g., if for proportion of data points Hℓ,i > 1/2then the cutoff hi will be much larger, thereby alleviating the exponential dependency on s. This message is intuitive: as signal strength of the feature becomes stronger, we need fewer parameters to learn it. s}, we have by definition that 1/h2 Observant readers may have noticed discrepancy between the theoretical requirement for the network width and the values used in our experiments. The requirement in (6.9) translates to exp (cid:17) (cid:16) b2 2h2 Φ(b)1/h2 = (cid:17)1/h2 , (cid:16) 1 where the holds by using Gaussian tail estimate. In contrast, our experiments use much smaller 3n. This difference arises because the theory assumes independently evolving, randomly initialized neurons, while in practice the neurons are correlated during training. That 30 is, the discrepancy between setting an infinitesimally small scale in theory versus setting = 1 in experiments. When neuron successfully learns feature, the coupling term ψm in (6.2) will prevent other neurons from going in the same directions, effectively improving the efficiency of the neuron utilization. Figure 15: (Left) Heatmap of FRR with respect to (M, 1/h2 ) for the GBA algorithm with axis in log scale. (Middle) FRR vs. relative occurrence Di/(N ρ1) with = 1024, = 3. (Right) FRR vs. cut-off hi with = 512, = 7. All the experiments are for (n, d) = (384, 100) and = 0.01. Strong vs. weak features. We now investigate what happens if the feature balance condition is violated, that is, some features are much stronger than others. feature can be considered strong if it either has high occurrence frequency or large cut-off level. To this end, we conduct additional experiments where in data generation, we split the features into two halves, which correspond to the strong and weak features. Let vi and vj be representatives of the strong and weak features. We numerically test how the relative occurrence Di/Dj = Di/(ρ1N ) and relative cut-off hj/hi affect the FRR, when we train the SAE using BA or GBA. In Figure 15 (Middle), we show how FRR is affected by the value of occurrence frequency. We see that both BA and GBA struggle with weak features, compared to results with strong features. In particular, FRR of BA drops significantly as the relative occurrence is less than 0.3. However, with grouping, GBA still learns quite portion of the weak features even when the relative occurrence is less than 0.1. From Figure 15 (Right), we show how FRR depends on the relative cut-off level. We observe that strong features with larger cut-off are learned more reliably than the weak features for both methods. More interestingly, in this case, grouping does not offer an evident advantage. Take-away from Feature Balance and Network Width Conditions (i) feature with significant strength, i.e., large cut-off value as defined in (6.4), can be learned with exponentially less compute. (ii) GBA can tolerate more feature imbalance in terms of the occurrence frequency."
        },
        {
            "title": "7 Proof Overview",
            "content": "In the following, we provide an overview of the key steps in the proof of Theorem 6.1."
        },
        {
            "title": "7.1 Good Initialization with Wide Network",
            "content": "By planting large pool of i.i.d. random neurons at initialization, we canwith overwhelming probability(1) assign to each feature vi one neuron mi whose inner product with vi is already very large, and (2) simultaneously ensure that this same neuron has only weak correlations with all the 31 other features. Concretely, we prove that if grows fast enough relative to n, then there exists choice of distinct neurons {mi}n i=1 such that InitCond-1: InitCond-2: max j=i mi (1 ε) (cid:112)2 log(M/n), vi, w(0) mi (cid:12) (cid:12) (cid:12)vj, w(0) 2(1 + ε) (cid:112)2 log n. (cid:12) (7.1) These two properties together ensure good initialization for the neuron mi dedicated to feature vi. With n3, we deduce that the weight vector wmi aligns exclusively with feature vi. In fact, as increases the separation between the two thresholds also increases, so w(0) mi is ever more strongly aligned with its own feature vi than with any other vj at the start. This widening margin precisely captures the benign over-parameterization effect: having many neurons actually promotes clean, feature-specific initialization. See Lemma D.1 for more details."
        },
        {
            "title": "7.2 Pre-activations are Approximately Gaussian",
            "content": "We give brief overview of how we deal with the challenge of tracking the highly nonlinear dynamics in (6.1). With an abuse of notation, let us denote by wt and bt one neurons weight and bias after iteration t. For the first step, the pre-activations are Gaussian, i.e., 0 xℓ + (b, 1). For later steps, we expand the gradient descent update for the neuron weights wt at iteration t. Let us denote by φt = (φ(w t1xℓ; b))ℓ[N ] and gt = φt the gradient computed in (6.2) at iteration t. By the gradient formula in (6.2), we have wt = (cid:88) τ =1 λτ φτ + λ0 w0, and Xwt = (cid:88) τ =1 λτ Xgτ + λ0 Xw0, (7.2) for some coefficient λτ . Let us recall the decomposition = HV . The first equality in (7.2) indicates that wt1 only contains information of through the (t 1)-dimensional projection Φ = span{φ τ =1. For the second equality, the most recent component Xgt = HV gt in the pre-activations contains new gradient component that is not captured by the previous steps projection of gt onto the orthogonal space of = {w0, g1, . . . , gt1}, which we denote as . Data Xs projection onto this new direction can be decomposed as τ H}t Xgt = (ΦV + ΦV ), Rd only contains information from subblock of that is orthogonal to both where ΦV Φ in the row space and in the column space. Notably, all the previous updates only contain information of limited to the row space Φ, i.e., φτ = φτ for τ 1 or the column space G, i.e., Xgτ = HV gτ for τ 1. Thus, the first term ΦV is independent of all the history updates, and is high-dimensional independent Gaussian vector plus low-dimensional coupling term ΦV . The argument holds true for all iteration steps, and if n, we approximately have xℓwt + (b, 1) thanks to the normalization of the weight wt. This argument can be made rigorous by using the Gaussian conditioning technique (Bayati and Montanari, 2011; Montanari and Wu, 2023; Wu and Zhou, 2023) in the formal proof. See D.3 for details."
        },
        {
            "title": "7.3 Weight Decomposition and Concentration under Sparsity",
            "content": "32 For one neuron dedicated to the target feature vi and satisfying the initialization conditions in (7.1), we decompose the weight wt into two directions: (1) the projection of wt onto the twodimensional subspace spanned by w0 and vi; (2) the projection of wt onto the orthogonal space . We define αt = wt, vi vi2 , βt = 2. The visualization of the projection is shown in Figure 16. Using αt and βt, one can compute the first and second moments of the post-activation φt under the decomposition of the preactivations (into high-dimensional Gaussian component and low-dimensional coupling term) obtained by the Gaussian conditioning technique. The post-activation φt then gives rise to the next-step wt+1, and we thus obtain an induced recursion over αt and βt. As more concrete example, let us take learning rate η = , and we can express αt as Figure 16: Visualization of the projection of wt onto 1) : the projection of wt onto the subspace spanned by w0 and vi and 2) : the projection of wt onto the subspace orthogonal to vi and w0. αt = wt, vi vi = φt vi2 φt2 Recall that = HV . By splitting into rows as [Vi; = [Hi, Hi], we have ] and splitting column-wise into αt = vi 2 φt + vi2 φt2 iφt = φt vi2 2 φt2 (cid:123)(cid:122) Signal (cid:124) (cid:125) + V iφt vi2 φt2 (cid:123)(cid:122) (cid:125) (cid:124) Noise . Here, we explicitly separate the signal from the noise. Our goal is to steer the neuron toward the direction of vi. Therefore, we treat the first term involving Hi and vi as the signal while treating gradient contributions from other features as noise. Controlling Moments of Activations. To proceed, we must tightly control both the signal and noise terms in the numerator and the denominator. Concretely, this means bounding the first moment of the activation φt (which enters the numerator) and its second moment (which controls the denominator), all while respecting the sparsity structure of φt. core difficulty stems from the pre-activation Xwt = wt, whose entries are not independenteven when has i.i.d. Gaussian entriesbecause different data points may share the same features. To handle this challenge, we employ refined Efron-Stein inequality (Boucheron et al., 2003) to tightly bound the moments related to the post-activation φt."
        },
        {
            "title": "7.4 State Recursion and Convergence",
            "content": "We track at iteration the alignment α1,t and the orthogonal component βt of the neuron weight wt. By exploiting the Gaussian-like concentration of the pre-activation Xwt = wt and applying 33 the refined Efron-Stein inequality to handle both feature correlations and the nonlinearity, one obtains the coupled recurrences 1 α1,t (1 + o(1)) + λt (cid:16) Φ(b) ρ1d 1 α1,t1 + rξt (cid:17) , βt α1,t λt (cid:16) βt1 α1,t1 + rξt (cid:17) . Here, λt ρ1N/Di, and Φ(b) denotes the Gaussian tail probability beyond the threshold b, which captures the activation sparsity. For clarity, we focus on the noiseless regime (i.e., assume rξt = 0) so that all noise contributions are neglected. We now elaborate on these recursions in detail: 1. Recall that we require βt α1,t since the neuron should eventually converge exclusively in the direction of the target feature. In our framework, the minimal growth rate of the ratio βt/α1,t is intrinsically controlled by λt = rO(ρ1N/Di), which characterizes how often the target feature vi appears in the training data relative to the most frequent feature. By the definition of ρ1, this ratio is inherently larger than 1. Thus, to prevent an unbounded escalation of βt/α1,t, we should restrict λt to, at most, polylogarithmic scale, i.e., λt = rO(1). 2. If we additionally set Φ(b)/(ρ1d) < dς for some ς (0, 1), then the map α1 1,t (cid:55) α1 1,t+1 rΘ(d1/2) to 1 o(1) in O(1) steps, is contractive. Hence α1,t grows from its initialization and the growth rate is much faster than that of βt/α1,t thanks to the sparsity condition Φ(b)/(ρ1d) 1. Occurrence condition Di 1 (1 ς) log From the above discussions, we have already justified the inclusion of the Individual Feature log Φ(b) d1ς /n = rO(dς (ρ1d)) in (6.6). The remaining conditions can rξt and the initialization ρ1N polylog(n)1 in (6.7) and part of the Bias Range condition b2 be derived based on more careful analysis, including the noise term conditions (7.1). 2 log n"
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "Conclusion. This research tackles the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for Large Language Models (LLMs). Traditional SAE training algorithms often lack rigorous guarantees and suffer from issues such as hyperparameter sensitivity and instability. To address these limitations, we introduce novel statistical framework that redefines feature identifiability by modeling polysemantic representations as sparse mixtures of underlying monosemantic features. Within this framework, we propose an SAE training algorithm based on bias adaptation, which adaptively adjusts bias parameters to maintain appropriate activation sparsity while preventing neuron death. Rigorous theoretical analysis and experiments on synthetic data validate the methods ability to recover monosemantic features effectively, particularly when the frequency of feature occurrence aligns with the neurons target range. Building on this, we develop an enhanced empirical variant named Group Bias Adaptation (GBA). By partitioning neurons into groups and assigning distinct target frequencies, GBA offers more flexible approach to feature recovery. Our analysis of group dynamics further refines the method for diverse network architectures. Experiments on 1.5-B causal LLM show that GBA outperforms benchmark methods in reconstruction fidelity, activation sparsity, and feature consistency. Additionally, our ablation study demonstrates the robustness and largely tuning-free nature of GBA. 34 Limitations and future work. While our work lays solid foundation for understanding SAE training, several limitations remain. The theoretical guarantees on the dynamics are established under the assumption of Gaussian-distributed features, which may not hold universally. Moreover, simplifying assumptions related to model architecture and algorithmic complexity were made to facilitate analysis, though our synthetic experiments indicate that the key findings remain valid without these simplifications. Notably, in the identifiability results, the only assumption we need for the features is incoherence, which is weaker and more natural assumption than Gaussianity. This means that the dynamics analysiswhich relies heavily on the Gaussian conditioning techniquemay be potentially extended to more general distributions, provided that the incoherence assumption holds. Moreover, verifying the incoherence assumption in real-world datasets is challenging, and we believe that closer examination and evaluation of the learned features is necessary for improving the robustness of current feature recovery methods. Additionally, we plan to undertake intervention studies to validate the causal impact of the learned features on model behavior. Beyond feature recovery, the emerging field of circuit discovery offers promising opportunities for enhancing LLM interpretability. We anticipate that integrating our approach with existing circuit discovery techniques will further illuminate the inner workings of large-scale models."
        },
        {
            "title": "References",
            "content": "Agarwal, A., Anandkumar, A., Jain, P. and Netrapalli, P. (2016). Learning sparsely used overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26 27752799. 5 Ameisen, E., Lindsey, J., Pearce, A., Gurnee, W., Turner, N. L., Chen, B., Citro, C., Abrahams, D., Carter, S., Hosmer, B. et al. (2025). Circuit tracing: Revealing computational graphs in language models. Transformer Circuits Thread. 6 Arora, S., Ge, R. and Moitra, A. (2014). New algorithms for learning incoherent and overcomplete dictionaries. In Conference on Learning Theory. PMLR. 5 Arora, S., Li, Y., Liang, Y., Ma, T. and Risteski, A. (2018). Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6 483495. 7 Barak, B., Kelner, J. A. and Steurer, D. (2015). Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing. Bayati, M. and Montanari, A. (2011). The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57 764785. 32, 58 Bengio, Y., Courville, A. and Vincent, P. (2013). Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35 17981828. 4 Boucheron, S., Lugosi, G. and Massart, P. (2003). Concentration inequalities using the entropy method. The Annals of Probability, 31 15831614. 33, 67, 133, 134, 136 Boucheron, S., Lugosi, G. and Massart, P. (2013). Concentration Inequalities: Nonasymptotic Theory of Independence. Oxford University Press. 132 Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B. and Burke, J. E. (2023a). Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. https://transformer-circuits.pub/2023/monosemantic-features.html 4, 6, 7, 22 Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N. L., Anil, C., Denison, C., Askell, A. et al. (2023b). Decomposing language models with dictionary learning. 8 Bruckstein, A. M., Donoho, D. L. and Elad, M. (2009). From sparse solutions of systems of equations to sparse modeling of signals and images. SIAM review, 51 3481. Candès, E. J. and Plan, Y. (2009). Near-ideal model selection by ℓ1 minimization. The Annals of Statistics, 37. 23 Chen, S., Wu, B., Lu, M., Yang, Z. and Wang, T. (2025). Can neural networks achieve optimal computational-statistical tradeoff? an analysis on single-index model. In The Thirteenth International Conference on Learning Representations. 26 36 Cohen, J. E. and Gillis, N. (2019). Identifiability of complete dictionary learning. SIAM Journal on Mathematics of Data Science, 1 518536. Cunningham, H., Ewart, A., Riggs, L., Huben, R. and Sharkey, L. (2023). Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600. 4, 7, 8 Dunefsky, J., Chlenski, P. and Nanda, N. (2024). Transcoders find interpretable llm feature circuits. arXiv preprint arXiv:2406.11944. 6 Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S., Hatfield-Dodds, Z., arXiv preprint Lasenby, R., Drain, D., Chen, C. et al. (2022). Toy models of superposition. arXiv:2209.10652. 4, Erichson, N. B., Yao, Z. and Mahoney, M. W. (2019). Jumprelu: retrofit defense strategy for adversarial attacks. arXiv preprint arXiv:1904.03750. 8, 15, 48 Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N. et al. (2020). The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. 15, 47 Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J. and Wu, J. (2024). Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093. 4, 5, 6, 8, 9 Gribonval, R., Jenatton, R. and Bach, F. (2015). Sparse and spurious: dictionary learning with noise and outliers. IEEE Transactions on Information Theory, 61 62986319. 23 Gribonval, R. and Schnass, K. (2010). Dictionary identificationsparse matrix-factorization via ℓ1 -minimization. IEEE Transactions on Information Theory, 56 35233539. 5, 23 Konda, K., Memisevic, R. and Krueger, D. (2014). Zero-bias autoencoders and the benefits of coadapting features. arXiv preprint arXiv:1402.3337. 6 Kreutz-Delgado, K., Murray, J. F., Rao, B. D., Engan, K., Lee, T.-W. and Sejnowski, T. J. (2003). Dictionary learning algorithms for sparse representation. Neural computation, 15 349396. 6 Laurent, B. and Massart, P. (2000). Adaptive estimation of quadratic functional by model selection. Annals of statistics 13021338. 132 Ledoux, M. and Talagrand, M. (2013). Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media. Lee, J. D., Oko, K., Suzuki, T. and Wu, D. (2024). Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit. Advances in Neural Information Processing Systems, 37 5871658756. 26 Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101. 11, 15 Lu, K., Yu, B., Zhou, C. and Zhou, J. (2024). Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. arXiv preprint arXiv:2401.12474. 4 Makhzani, A. and Frey, B. (2013). K-sparse autoencoders. arXiv preprint arXiv:1312.5663. 4, 6, Marques, E. C., Maciel, N., Naviner, L., Cai, H. and Yang, J. (2018). review of sparse recovery algorithms. IEEE access, 7 13001322. 23 37 McDiarmid, C. et al. (1989). On the method of bounded differences. Surveys in combinatorics, 141 148188. 67 Montanari, A. and Wu, Y. (2023). Adversarial examples in random neural networks with general activations. Mathematical Statistics and Learning, 6 143200. 32, 58 Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S. (2020). Zoom in: An introduction to circuits. Distill, 5 e00024001. 7 Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning sparse code for natural images. Nature, 381 607609. 6 Papadimitriou, I., Su, H., Fel, T., Saphra, N., Kakade, S. and Gil, S. (2025). Interpreting the linear structure of vision-language model embedding spaces. arXiv preprint arXiv:2504.11695. 6 Paulo, G. and Belrose, N. (2025). Sparse autoencoders trained on the same data learn different features. arXiv preprint arXiv:2501.16615. 4, 6, 9, 19 Rajamanoharan, S., Conmy, A., Smith, L., Lieberum, T., Varma, V., Kramár, J., Shah, R. and Nanda, N. (2024a). Improving dictionary learning with gated sparse autoencoders. arXiv preprint arXiv:2404.16014. 6 Rajamanoharan, S., Lieberum, T., Sonnerat, N., Conmy, A., Varma, V., Kramár, J. and Nanda, N. (2024b). Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435. 4, 6, 8, 15, 48 Rubinstein, R., Bruckstein, A. M. and Elad, M. (2010). Dictionaries for sparse representation modeling. Proceedings of the IEEE, 98 10451057. 6 Scherlis, A., Sachan, K., Jermyn, A. S., Benton, J. and Shlegeris, B. (2022). Polysemanticity and capacity in neural networks. arXiv preprint arXiv:2210.01892. 4 Schnass, K. (2014). On the identifiability of overcomplete dictionaries via the minimisation principle underlying k-svd. Applied and Computational Harmonic Analysis, 37 464491. 23 Shu, D., Wu, X., Zhao, H., Du, M. and Liu, N. (2025). Beyond input activations: Identifying influential latents by gradient sparse autoencoders. arXiv preprint arXiv:2505.08080. 6 Spielman, D. A., Wang, H. and Wright, J. (2012). Exact recovery of sparsely-used dictionaries. In Conference on Learning Theory. JMLR Workshop and Conference Proceedings. 5, 6, 23 Taggart, G. (2024). Prolu: nonlinearity for sparse autoencoders. In AI Alignment Forum. 6 Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Jermyn, A., Carter, S., Olah, C. and Scaling monosemanticity: Extracting interpretable features from Ameisen, E., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Henighan, T. (2024). claude 3 sonnet. Transformer Circuits Thread. https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html 4, 5, Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B: Statistical Methodology, 58 267288. 4, 6, 9 38 Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A. and Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in deep network with local denoising criterion. Journal of machine learning research, 11. 8 Wainwright, M. J. (2019). High-dimensional statistics: non-asymptotic viewpoint, vol. 48. Cambridge university press. Wright, B. and Sharkey, L. (2024). Addressing feature suppression in saes. In AI Alignment Forum. 6 Wu, Y. and Zhou, K. (2023). Lower bounds for the convergence of tensor power iteration on random overcomplete models. In The Thirty Sixth Annual Conference on Learning Theory. PMLR. 32, 58 Xiong, Z., Cai, Z., Cooper, J., Ge, A., Papageorgiou, V., Sifakis, Z., Giannou, A., Lin, Z., Yang, L., Agarwal, S. et al. (2024). Everything everywhere all at once: Llms can in-context learn multiple tasks in superposition. arXiv preprint arXiv:2410.05603. 4 Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H. et al. (2024). Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. 15,"
        },
        {
            "title": "A Supplementary Discussions",
            "content": "A.1 Details on Other Training Methods We provide here more details on the training methods used in our experiments, including the Sparse Autoencoder (SAE) with TopK activation and SAE with ℓ1 regularization. Sparse Autoencoder (SAE) with TopK activation. In an SAE with TopK activation, sparsity is enforced by selecting only the neurons with the highest activation values in the hidden layer. Let = (x bpre) + be the pre-activation values of the hidden layer. Let ϕ(y) be the activations after applying standard activation function. The TopK selection mechanism, denoted as SK(), operates on ϕ(y). For vector RM , SK(v) produces vector RM such that: = (cid:40)vj 0 if vj is among the largest values in v, otherwise for [M ]. The post-activation in TopK SAE is: = SK (cid:0)ϕ(W (x bpre) + b)(cid:1), which by definition is K-sparse. The reconstructed output is: px = diag(a) + bpre. Let Θ = (W, bpre, b, a) be the parameters of the SAE. The loss function for the TopK SAE is the reconstruction loss: Lrec(x; Θ) = px2 2. Sparse Autoencoder (SAE) with ℓ1 regularization. In an SAE with ℓ1 regularization, sparsity is encouraged by adding penalty term to the reconstruction loss, proportional to the sum of the absolute values of the hidden layer activations. Let = (x bpre) + be the pre-activation values of the hidden layer. Let = ϕ(y) = ϕ(W (x bpre) + b) be the activations after applying standard activation function; these are the hidden layer representations that will be encouraged towards sparsity. The reconstructed output is: px = diag(a) + bpre. The loss function for the L1 SAE, L(x; Θ), incorporates both the reconstruction error and the L1 penalty on the hidden activations z: L(x; Θ) = px 2 + λ (cid:88) j=1 zj wj2, where λ > 0 is the sparsity penalty parameter that controls the strength of the regularization, is the number of neurons in the hidden layer, and wj is the j-th row of the weight matrix . In our real-data experiments, we also consider the JumpReLU activation, nonJumpReLU. smooth, non-monotonic function. Conceptually, it behaves like ReLU for positive inputs but introduces sharp jump for sufficiently large inputs. In our implementation, we adopt simplified scalar form adapted to our neuron pre-activation mx + bm: JumpReLU(w mx; bm) = (cid:40) 0, mx, if if mx + bm < 0, mx + bm 0. This activation acts as hard thresholded identity: it passes the neurons response only when the pre-activation crosses bias-controlled threshold. Although JumpReLU does not satisfy the smoothness or Lipschitz conditions required in our theory (see Definition A.2), it is empirically effective and included in our experimental comparisons 4. Preand post-activation sparsity. For both the L1 and TopK SAE, we define the pre-activation sparsity as the number of non-zero entries in y, i.e., y0, and the post-activation sparsity as the number of non-zero entries in z, i.e., z0. Note that the post-activation sparsity is always no larger than while the pre-activation sparsity can be larger than K. This difference leads to two evaluation metrics for the TopK SAE: the pre-activation sparsity and the post-activation sparsity as used in Figure 3. Minor notational discrepancy. In the main text and above definition we express the activation as ϕ(w mx + bm), whereas in the definition above the JumpReLU activation is indeed as bivariate function of mx and bm. This slight difference is purely notational and does not affect the underlying functionality or the definition of preand post-activation sparsity. For simplicity, we always stick to ϕ(w mx + bm). A.2 Evaluation Metrics We explain here the details of the evaluation metrics used in our experiments to assess how well the GBA algorithm recovers the underlying features. We first introduce the maximum activation and neuron Z-score, which are used to measure the quality of the learned neurons. Then, we introduce the notion of Max Cosine Similarity (MCS) and Feature Recovery Rate (FRR), which are used to measure the quality of the alignment between the learned neurons and the ground-truth features, or the consistency of the learned features across different runs. We also introduce the neuron percentage, constructed from the MCS, which is used to generate Figure 5. We introduce maximum activation and neuron Z-score of neuron as follows. Maximum activation. Unless specified, we define the maximum activation of neuron as the maximum of its pre-activations over the validation set: Maximum Activation(m) = max xValidation Set ym(x), where ym(x) = m(x bpre) + bm. (A.1) Note that the maximum activation is computed based on the tokens in the validation set. It maps each neuron to scalar, characterizing the maximum pre-activation of the neuron across all validation tokens. Neuron Z-score. Let ϕ() denote the neurons activation function (e.g., ReLU, or JumpReLU). For each neuron and minibatch {xi}B ϕm,i = ϕ(cid:0)w i=1, we define its post-activation responses as = 1, . . . , B, m(xi bpre) + bm (cid:1) , 41 where wm Rd is the neurons weight vector and bm is its bias. We can compute the mean and standard deviation of these activations in the minibatch as µm = 1 (cid:88) i= ϕm,i, sm = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i= (cid:0)ϕm,i µm (cid:1)2 . The Z-score of neuron on data point xi is defined as Zm,i = (ϕm,i µm)/sm R. We can also take the maximum of the Z-scores over the batch: Zmax = (ϕm,max µm)/sm , where ϕm,max = max 1iB ϕm,i. (A.2) large value of Zm,i (or Zmax multiple standard deviations above its mean. Thus, when Zmax sensitively detect certain data points within the batch. More specifically, when Zmax two following conditions hold: Strong Selectivity: There exists some xi within the batch such that ϕm,i µm, i.e., the neurons 0) indicates that on some input xi, the neurons activation ϕm,i lies is large, neuron is well-learned to is large, the activation ϕm,i spikes for input xi. Low Baseline Variability: Within the whole batch, the neurons activation ϕm,i is relatively stable, i.e., the standard deviation sm is moderate. As result, Zmax serves as quantitative measure of the neurons specificity on the batch of data. When generating Figure 5, we use the maximum Z-score of each neuron across the whole validation set to select subset of neurons. Next, we introduce the Max Cosine Similarity (MCS) and Feature Recovery Rate (FRR) metrics, which are used to measure the quality of the alignment between the learned neurons and the ground-truth features, or the consistency of the learned features across different runs. Max Cosine Similarity (MCS) for synthetic data. For each neuron with weight vector wm Rd, we define MCS(m) = max i[n] wm, vi wm2 vi2 [1, 1]. By definition, MCS(m) = 1 if and only if wm coincides with one of the true features vi. Max Cosine Similarity (MCS) for real data. For real data, as we do not have access to the ground-truth features, we define the MCS as the maximum cosine similarity between neurons across different runs. This definition is used in Figure 5. Specifically, consider the trained neurons weights (j) RM for = 1, . . . , where is the number of runs with different random seeds. We fix the first run as the host run and compute the MCS for the m-th neuron in the host run with respect to the j-th run with 2 as follows: MCS(m, j) = max(cid:8)cos(W (j), w(1) )(cid:9). Here, the term inside the max is the cosine similarity between the m-th neuron in the host run and all neurons in the j-th run, which is an -dimensional vector. The maximum taken outside can be interpreted as finding the best match for the m-th neuron in the host run. Now, given threshold τ for the MCS value, i.e., the x-axis in Figure 5, we define neuron to have an MCS above the threshold if MCS(m, j) τ for all 2. We require this condition to hold for all runs 2 because if the 42 algorithm learns consistent feature, it should be present no matter which random seed is used. When this is the case, neuron in the host run can find corresponding neuron in each of the other runs that has cosine similarity above the threshold τ . Thus, by computing MCS for all the neurons in the host run, we evaluate the consistency of the learned features across different runs. Neuron percentage in Figure 5. Recall that we call the first run of the algorithm the host run. Under the definition of MCS, in Figure 5 we plot the neuron percentage as function of the MCS threshold τ . In particular, for any threshold τ (x-axis in Figure 5), we compute the fraction of neurons in the host run that have an MCS above the threshold across all runs. That is, we define Neuron Percentage(τ ) = 1 M (cid:88) m=1 1(cid:0)MCS(m, j) τ, 2(cid:1). (A.3) By definition, this quantity computes the fraction of neurons in the host run that have an MCS above the threshold τ across all runs 2. If this quantity is large, the algorithm is able to produce consistent results across different runs with different random seeds. Moreover, because considerable portion of the neurons of SAE are rarely activated, instead of enumerating over all neurons as in (A.3), we can also consider the neuron percentage over subset of neurons, denoted by [M ]. Then, focusing on M, we define the neuron percentage as Neuron Percentage(τ, M) = 1 (cid:88) mM 1(cid:0)MCS(m, j) τ, 2(cid:1). (A.4) In particular, in Figure 5, we choose to be the top-α subset of neurons in terms of the maximum activations or neuron Z-score in the host run, which are defined in (A.1) and (A.2), respectively. Note that these two metrics are computed based on the validation dataset. The y-axis in Figure 5 is computed as in (A.4) with these two versions of M. The notion of Feature Recovery Rate (FRR) is only used for synthetic data, where we have access to the ground-truth features. Feature Recovery Rate (FRR). For one monosemantic feature vi, we say it is recovered if there exists neuron [M ] such that the cosine similarity between the neuron and the feature is above certain threshold τalign: 1i = if [M ] such that (cid:40) 1 0 otherwise. (cid:12) pwm, vi(cid:12) (cid:12) (cid:12)/vi2 τalign,"
        },
        {
            "title": "Then the Feature Recovery Rate is",
            "content": "FRR = 1 (cid:88) i=1 1i [0, 1]. In words, FRR is the fraction of ground-truth features vi that have been recovered, i.e., aligned to at least one learned neuron. Here, we find the following way to define the threshold τalign useful: τalign = cos (cid:16) 1 3 arccos (cid:16) max i=j vi, vj vi2 vj (cid:17)(cid:17) . (A.5) Figure 17: An illustration of the learnable region surrounding the feature. Any neuron weight within the cone has cosine similarity above the threshold with the feature. 43 Intuitively, the angle given by arccos in (A.5) is the smallest angle among all pairs of features vi and vj in , which is denoted by θ in Figure 17. Then, if neuron exhibits cosine similarity above the threshold τalign with feature vi, then it lies within the cone centered at vi with angle θ/3. See Figure 17 for an illustration. By our choice of τalign, these cones associated to all monosemantic features lie in the 1-dimensional sphere without overlapping, ensuring that each neuron exceeding the threshold is uniquely aligned with single feature. A.3 Equivalance between Row Normalization of and In the following, we argue that normalizing the data is essentially the same as normalizing each row of the coefficient matrix under the Gaussian feature setting. We first invoke the following proposition to show that normalizing the rows of can approximate the row-normalization of H. Let hi be the i-th row of and xi be the i-th row of X. i.i.d. (0, 1). Then for any universal constant > 0, there Proposition A.1. Suppose log and Vij exists another universal constant > 0 such that with probability at least 1 nc, we have for all [N ] that (cid:12) (cid:12) (cid:12) xi2 2 dhi2 2 (cid:114) (cid:12) (cid:12) (cid:12) Cs 1 log . Proof of Proposition A.1. In the following proof, we denote by some universal constant that may vary from line to line. Note that the norm of xi is given by xi 2 = (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) j[n]:Hij ="
        },
        {
            "title": "Hijvj",
            "content": "(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 = (cid:88) 2 ijvj2 2 + (cid:88) HijHikv vk. j[n]:Hij =0 j=k:Hij ,Hik=0 By Lemma I.1 and applying union bound, we have with probability at least 1nc that vi2 C((cid:112)d log(n) + log(n)) for all [n] and some universal constant c, C. For = moment generating function of as 2d vk, we have the E[eλZ] = (cid:89) l=1 E[eλvj [l]vk[l]] = (cid:89) l=1 E[eλ2vk[l]2/2] = 1 (1 λ2)d/2 , where λ2 < 1. Using the Chernoff bound, we conclude that P(vj, vk t) inf 0<λ<1 eλt E[eλZ] = inf 0<λ<1 θλt (1 λ2)d/2 inf λ>0 (cid:16) exp λt + 2 λ2 1 λ2 (cid:17) . By setting λ = t/d 1, we get (cid:16) P(vj, vk t) exp t2 2d similar bound holds for the negative case and we thus conclude that P(vj, vk t) 2 exp( t2 5d ) for t/d 1. Consequently, with probability at least 1 nc, it holds for all pair of (i, j) [n]2 and = that vi, vj log for some universal constant C. Here, we are using the condition that log d. Combining the above two results, we obtain that 1 1 t2/d exp t2 5d t2 + (cid:17) (cid:16) (cid:17) . (cid:12) (cid:12)xi2 2 dhi2 2 (cid:12) C((cid:112)d log + log n) hi2 (cid:12) 2 + C(cid:112)d log (cid:88)"
        },
        {
            "title": "HijHik",
            "content": "C(cid:112)d log hi2 1 Cs(cid:112)d log hi2 2, j=k:Hij =0 44 where in the last step we use the hölders inequality and the fact that each row of has at most non-zero entries. Hence, we conclude the proof. A.4 Omitted Details in In this section, we provide the omitted details for 6. We give formal definition of ReLU-like activations. Definition A.2 (ReLU-like Activation). For the activation function ϕ : R, we define φ as φ(x) = φ(x; 0) = ϕ(x) + ϕ(x). We say that ϕ is ReLU-like if it satisfies the following: 1. (Lipschitzness) The activation function ϕ is continuously differentiable, 1-Lipschitz, and γ1-smooth with γ1 = O(polylog(n)). Furthermore, φ(x) is γ2-Lipschitz with γ2 = O(polylog(n)). 2. (Monotonicity) The activation function ϕ is non-decreasing, and moreover, ϕ(x) > C0 for some constant C0 > 0 and all 0. 3. (Diminishing Tail) There exists threshold κ0 = O((log n)1/2) and sufficiently large constant c0 > 0 such that for all < κ0, max{ϕ(x), ϕ(x), ϕ(x)} nc0. Lipschitzness. Under the above assumptions, we note that φ(x; b) is L-Lipschitz in with = (γ2 + bγ1) = O(polylog(n)) > 1. The Lipschitz property of the function φ is pivotal in our analysis since it enables control over error propagation across iterations. However, this property depends on the smoothness of the activation function ϕ, condition that the standard ReLU does not satisfy. Fortunately, many common activation functionssuch as softplus, noisy ReLU, and shifted ELU (with the limit at set to 0)do satisfy this smoothness requirement. In particular, with large smoothness parameter γ1 = polylog(n), we can use smooth activation function to well approximate the ReLU function. For instance, we can take ϕ(x) = γ1 log(1 + eγ1x) for some γ1 = polylog(n) as smooth approximation of the ReLU activation function. Monotonicity. The monotonicity property ensures that neurons with large pre-activations, which indicate good alignment with the underlying features, will also have large post-activations. This then guarantees continuous growth of the corresponding neuron weights. Diminishing Tail. The diminishing tail condition ensures that both the activation function ϕ and its derivative ϕ are negligibly small when the input is below the threshold κ0. This property suppresses unwanted neuron activations, thereby promoting sparsity in the activationsa key factor in the successful training of the SAE."
        },
        {
            "title": "B Additional Experiments Details",
            "content": "We provide additional experimental results and implementation details that complement the main findings presented in the paper. B.1 Additional Experimental Details for 6 In 6, we present the theoretical results for the feature recovery problem along with the experimental results on synthetic data. 45 B.1.1 Synthetic Data In synthetic experiments, we use Spherical Gaussian features. For each sample xj (j [N ]), we randomly sample indices (with replacement) from [n] to form multi-set Sj. The corresponding features are then combined with weight 1/ to construct the reconstruction target: s. vi/ xj = (cid:88) iSj B.1.2 Comparing with TopK Activation We compare our algorithm with the TopK Algorithm on synthetic data. Data Details. We generate synthetic data comprising = 105 samples, each formed by directly summing = 3 independent spherical Gaussian features. The dataset contains total of = 4096 features, each of dimension = 128. Each sample is constructed by summing = 3 independent Gaussian features. Comparison Details. We evaluate the TopK algorithm for the following values of k: {3, 6, 9, 14, 18, 22, 28, 34, 40, 50, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125}. For GBA, we set five groups and set the Highest Target Frequency (HFT) to be exponentially scattered within [104, 103]: {1.00 103, 5.62 104, 3.16 104, 1.78 104, 1.00 104}, while the Lowest Target Frequency (LTF) is fixed at 105. In both cases, we train SAE with = 16384 hidden neurons. Training Details. We use the AdamW optimizer with learning rate of 103, weight decay 0.01, batch size of 128, and maximum of 600 training epochs. The activation function is set to ReLU. Result. The results are shown in Figure 18. The GBA algorithm recovers nearly all features while maintaining an exceptionally low activation rate. Take-away from Figure 18 1. GBA achieves Feature Recovery Rate (FRR) close to 1, on par with the TopK algorithm, validating our theoretical results. 2. Moreover, GBA automatically identifies the correct number of active features per sample (s = 3) through its activation percentage. Consequently, only the neurons corresponding to the ground truth features are frequently activated, while the others remain largely inactive. This results in cleaner, more interpretable activation pattern. 46 Figure 18: Comparison of GBA (blue) against the standard TopK algorithm (orange). Parameters: (n, d, M, s) = (4096, 128, 16384, 3). We can see that GBA can automatically identify the ground truth number of active features per sample (K = = 3) in terms of activation percentage (the average percentage of neurons activated measured by post-activations for each data) B.1.3 Additional Details on Figure 15 Data details. We provide additional details on the experimental setup used in Figure 15. All experiments were conducted on synthetic data with spherical Gaussian features and = 107 data points. For Figure 15 (Left), we used the setup described earlier to evaluate the learned feature percentage across different model widths and sparsity levels s. Here, each data is obtained by s. By Proposition F.1, we know linear combination of random features with coefficients 1/ that 1/h2 = s. Therefore, it suffices to vary to achieve the same effect of varying 1/h2 . For Figure 15 (Middle), we randomly designated half of the features as high-occurrence and the other half as low-occurrence. To control the imbalance in feature frequency, fraction α of the data was generated using only high-occurrence features, while the rest was sampled uniformly from the full feature set. By tuning the parameter α, we achieve the effect of controlling the relative occurrence by identity Di/(N ρ1) = (1 α)/(1 + α). For Figure 15 (Right), we again split the features into two groups: high cut-off and low cut-off. For high cut-off feature i, we perturb the nonzero coefficients Hℓ,i using the transformation Hℓ,i = tanh(N (µ, σ)), where µ = arctanh(1/ 7) and σ {0.0, 0.3, 0.6, 0.9, 1.2, 1.5}. Low cut-off features retain the constant weight 1/ 7. By the perturbation, we almost retain the mean while making the cut-off larger. Each data point is constructed by randomly sampling 7 features (s = 7) and assigning their corresponding weights. Training details. We use the AdamW optimizer with learning rate of 104, batch size of 128, and maximum of 30 training epochs. The activation function is set to ReLU. The Lowest Target Frequency (LTF) is fixed at 106. For the BA method (GBA with one group), the Highest Target Frequency (HTF) is set to 102. For the GBA method with four groups, we set the HTFs to 5 103, 102, 2 102, and 5 102, respectively. B.2 Additional Details for 4 Data and model details. We choose the subsets of Github and Wikipedia_en of Pile (Gao et al., 2020) without copyright as our datasets. The Github dataset is collection of 1.2 billion tokens from public GitHub repositories, while the Wikipedia_en subset contains 1.5 billion tokens from English Wikipedia articles. We use the first 99.8k rows from each dataset for training 47 and the next 0.2k rows for validation. Each row in the dataset is truncated to the first 1024 tokens after tokenization. Therefore, the total number of tokens is roughly = 100m. We use the Qwen2.5-1.5B base model (Yang et al., 2024) as our LLM, which has 1.5 billion parameters and MLP output dimension 1536. We attach an SAE to the output of the LLMs MLP output at layer 2, 13, and 26 with = 66k neurons, resulting in three different SAEs for each dataset. The dimension of the input data points is equal to = 1536. We use the JumpReLU activation (Erichson et al., 2019; Rajamanoharan et al., 2024b) for all training methods. Training details. We train the SAEs using methods such as GBA, TopK, L1, and BA, where BA is simply GBA with one group. For all these methods, we use the AdamW optimizer with learning rate of 104 and weight decay of 102. Since the sentences are truncated to 1024 with padding token removed, we set the batch size to = 8192 tokens and buffer size of = 40k tokens. Each run can be completed using single NVIDIA A100 GPU with 80GB memory, and we train 8 epochs for each method. The hyperparameters of each method are set as follows: For GBA, we set Highest Target Frequency (HTF) and the Lowest Target Frequency (LTF) are set to 0.1 and 0.001, respectively. We set total number of = 10 groups with exponentially decaying target frequencies betweeen the HTF and LTF. We set γ = 0.01 and γ+ = 0.01. In other words, we have p1 = 0.1, p10 = 0.001, and {pk}k[10] form geometric sequence. For the BA method, we set the HTF to be from {101, 3 102, 102, 3 103} and vary the choice. The other parameters are the same as GBA. For TopK method, we implement two versions the pre-activation TopK and the postactivation TopK. See A.1 for details. We vary the value of in {50, 100, 200, 300, 400, 500, 600}. For L1 method, we vary the penalty parameter λ in {101, 3 102, 102, 3 103, 103}. Comparison between JumpReLU and ReLU activation. For the SAE trained on the Github dataset at layer 26, we compare the performance between JumpReLU and the standard ReLU activations across all methods considered in this paper. As shown in Figure 19, the sparsity-loss frontiers for TopK and L1 methods are nearly identical under both activations. However, the GBA method demonstrates marked improvement when using JumpReLU activation. With ReLU, decreasing the neuron bias also reduces the output magnitude. Thus more neurons are needed to compensate for the loss of output magnitude, which leads to less sparse model, which degrades the sparsity-loss frontier. In contrast, JumpReLU decouples the neuron output magnitude from its biasonly the activation frequency is influencedyielding more robust sparsity-loss performance. Improved explanation of bias clamping to prevent over-sparsification. During the bias scheduling subroutine of the GBA algorithm (Algorithm 1), we enforce clamp on the bias values, restricting them to the range [1, 0]. This constraint serves two primary purposes. The upper bound of 0 ensures that neuron is only activated when the input data exhibits sufficient alignment with the neurons weight vector. Consequently, allowing negative bias values (bm < 0) effectively prevents excessive or premature activation of neurons. The lower bound of 1 is implemented to avoid over-deactivation and the emergence of reinforcing loop. We have observed experimentally that when the pre-bias (bpre) significantly deviates from zero, certain neurons may develop weights that are in opposition to the pre-bias to compensate for this drift. As these compensatory neurons are more likely to be activated by the initial pre-bias, the GBA algorithm might inadvertently continue to deactivate them by further 48 Figure 19: Comparison of sparsity-loss frontier between JumpReLU and ReLU activations. The left and middle plots show the sparsity-loss frontier with JumpReLU and ReLU activations, respectively. The right plot is combination of the two, where the faded plots represent the sparsity-loss frontier of the ReLU activation. reducing their bias (bm). This deactivation would then necessitate an increase in the neurons weight to maintain its influence, leading to counterproductive cycle of deactivation and weight growth. By limiting the bias to be no less than 1, we effectively interrupt this reinforcing loop and promote training stability. The rationale behind choosing 1 as the lower bound stems from the fact that our input data is normalized. This normalization typically results in pre-activation values that are significantly smaller than 1, with values approaching 1 only when the data strongly activates specific neurons. Therefore, lower bias bound of 1 provides sufficient range for deactivation without causing the problematic feedback loop. This clamping strategy has been shown to significantly enhance the stability of the training process. Identifiability of Features: Proof of Theorem 5. In this section, we prove Theorem 5.3 under the general assumptions. In particular, we consider the following relaxed version of Definition 5.2 that accounts for variability in the scaling of the data matrix X. Assumption C.1 (Decomposable data). We say that dataset RN is decomposable if there exists positive integer N, diagonal scaling matrix RN with positive diagonal entries, nonnegative matrix RN and feature matrix Rnd such that DX = HV. In addition, each row of is normalized to one in ℓ2 norm and the ℓ2 norm each row of is normalized to Θ( d). Moreover, for every ℓ [N ] and all i, [n], the weight matrix RN obeys the following conditions: + + (H1) Row-wise sparsity: Each row has at most nonzero entries, i.e., Hℓ,:0 with = Θ(1). (H2) Non-degeneracy: For every [n], the average magnitude of the nonzero entries is constant, i.e., H:,i1/H:,i0 = Θ(1). (H3) Low co-occurrence: For any two distinct columns, the frequency of simultaneous nonzeros is small, i.e., ρ2 := maxi=j 1{H:,i = 0}, 1{H:,j = 0}/H:,i0 n1/2. We further assume that the feature matrix Rnd satisfies: (V1) Incoherence: Features are nearly orthogonal, i.e., vi, vj/vi2 vj2 = o(1) for all = j. Suppose that there exists another pair of RN , RN and Rnd such that all the conditions for and also hold for and , and = D1HV = (D)1H . Our goal 49 is to show that under these conditions, the alternative factorization must essentially recover the same feature structure as the original: specifically, each feature vi in can be expressed as linear combination of disjoint subset of the features in , thereby establishing the identifiability of the feature recovery. In the main text (Definition 5.2), we work with the normalized Reduction to the normalized case. factorization = HV, which corresponds to the special case = IN in Assumption C.1. Since all of our assumptions on and are identical in both settings, the arguments we present here for DX = HV apply equally to the normalized case. Moreover, allowing an arbitrary lets us accommodate datasets whose rows of may be arbitrarily large or small in norm. The identifiability conclusion remains unchanged if we redefine as the class of normalized pairs rH = D1H and (D, H, ) satisfying Assumption C.1. Consequently, Theorem 5.3 in ( rH, ), with the main text follows immediately from the more general result shown here. Proof outline. We proceed the proof in four main steps: 1. Row-scale consistency. Given two decompositions = and = , we show that any two row-scalings and differ only by constant factors (i.e., D(D)1 = Θ(1)). 2. Approximate pseudo-inverse and weight vector construction. To identify the features from HV = D(D)1H , we construct matrix such that AH In, and define weight vectors i,:, ω From AH In and bounded scaling of D(D)1, we derive that each ωi and ω ωi2, ω i2 = Θ(1) and ωi, ωj, ω = o(1) for all = j. = (cid:0)A D(D)1H (cid:1) i,:. ωi = (AH) i, ω satisfies 3. Disjoint support and feature reconstruction. Since weight vectors are nearly orthogonal, cannot overlap. Thresholding each ω 2a (with = at the level . Consequently, each original feature vi is the large entries of each ω maxi=jω associated with unique subset of primed features in . We then form j) produces disjoint support sets i, ω ui = ωii vi, = i,k ω k, (cid:88) kK where ui is the true feature vi scaled by ωii and and show ui i2 = o( d), so each original feature is well-recovered. is its reconstruction from the primed features, 4. From small residuals to cosine closeness. Since ui2, i2 = Θ( d) and ui i2 = o( it follows that 1 cos(vi, i) = o(1), and hence ε = o(1) identifiability. d), Now, we proceed with the proof of Theorem 5.3 by showing the above steps in detail. Step 1: Row-scale consistency. We show that the elements of the diagonal matrix D(D)1 are of constant order. By Assumption C.1, we have HV = D(D)1H which implies that ℓ,:V for all ℓ [N ]. Using the incoherence (V1) assumption, ℓℓ(D Hℓ,:V, Hℓ,:V = D2 we can expand the inner product as follows: ℓℓ)2H ℓ,:V , Hℓ,:V, Hℓ,:V = Hℓ,:diag(V )H ℓ,: + Hℓ,:(V diag(V ))H ℓ,: = Θ(Hℓ,:2 ℓ,:V = Θ(H 2 d) + o(Hℓ,:2 1 d). 2 d) + o(H ℓ,:V , ℓ,:2 = 1 ℓ,:1 = Θ(1) by Assumptions (H1) and (H2), by comparing Hℓ,:V, Hℓ,:V with 1 d). Since Hℓ,:2 = ℓ,:2 ℓ,:2 ℓ,:V , we conclude that Dℓℓ(D ℓℓ)1 = Θ(1) for all ℓ [N ]. Similarly, we have and Hℓ,:1, ℓ,:V , 50 Step 2: Approximate pseudo-inverse and weight vector construction. Next, we define the following nonnegative matrix RnN : + Ail = (cid:40) 0 H:,i1 if Hli = 0 if Hli = 0 for [n], ℓ [N ], Note that AH Rnn is square matrix. By the non-degeneracy and nonnegativity conditions, for the diagonal entries of AH, we have (AH)ii = H:,i1/H:,i0 = Θ(1) for all [n]. By the low co-occurrence condition, we have for the off-diagonal entry (AH)ij = 1(H:,i = 0), H:,j H:,i0 1(H:,i = 0), 1(H:,j = 0) H:,i0 ρ2 for all = j. The first inequality holds because all the elements in should be no larger than 1 in their absolute value. By the construction of A, we have AH In and apply on both sides of the equation HV = D(D)1H to identify the feature correspondence between and ."
        },
        {
            "title": "Let us define the weight vectors",
            "content": "ωi = (AH) i,: Rn +, = (cid:0)A D(D)1H (cid:1) ω i,: Rn + . To understand their behavior, we analyze their norms and pairwise inner products. For each [n], let Di = { ℓ [N ] : Hℓi = 0 } be the support of the i-th column of H. The ℓ1 norm of ωi can be written as ωi1 = (cid:88) (cid:88) ℓ=1 j=1 AiℓHℓj = 1 Di (cid:88) ℓDi Hℓ,:1 [1, s], where the second equality uses the nonnegativity of and the row-wise averaging in the cons], struction of A. Since Hℓ,:2 = 1 and each row is s-sparse, it follows that Hℓ,:1 [1, implying ωi1 = Θ(1) for all [n]. By similar argument and the results from the first step that Dℓℓ(D ℓℓ)1 = Θ(1) for all ℓ [N ], we obtain ω i1 = Θ(1) for all [n]. Next, for the ℓ2 norm of ωi, we have ωi2 = (cid:115) (AH)2 ii + (AH)2 ij (cid:20) (AH)ii, (cid:88) j=i (cid:113) (AH)2 ii + nρ2 2 (cid:21) . Given that (AH)ii = Θ(1) and ρ2 = o(n1/2), it follows that ωi2 = Θ(1). Also, since ωii = (AH)ii = Θ(1), ωij = (AH)ij ρ2 = o(n1/2), the inner product between ωi and ωj satisfies: ωi, ωj = (cid:40) Θ(1) + nρ2 O(ρ2 + nρ2 2 = Θ(1) 2) = o(1) if = j, if = j. To analyze the pairwise inner product of after applying them to the feature matrices. Specifically, we observe that (ω ω V, ω vi2 = Θ( d) and vi, vj = o(d) for all = j, we obtain: , which allows us to relate ω i, we examine the corresponding inner products j)V = through the known properties of ωi. Since i)V, (ω and ω ω V, ω = ω diag(V )ωj + ω (V diag(V ))ωj = Θ(ωi, ωj d) o(ωi1ωj1 d) = (cid:40) Θ(d) o(d) if = j, if = j. (C.1) 51 By similar calculation, we also have (ω i)V, (ω j)V = Θ(ω i, ω jd) o(ω i1ω j1d), i1 = Θ(1) for all [n]. To ensure the equality (ω where ω following properties must hold following the calculation in (C.1): i)V, (ω j)V = ω V, ω , the ω i, ω = (cid:40) Θ(1) o(1) if = j, if = j. are nearly orthogonal, i.e., ωi, ωj = o(1) for all = and ω Step 3: Disjoint support and feature reconstruction. We have established that the weight vectors ωi and ω = o(1) for all = j. In this step, we define set of coordinates for each ω that captures the significant entries, and show that these sets are disjoint for different and j. In particular, let := maxi=jω j, which follows that = o(1). We define the following set of coordinates for ω i: i, ω i, ω = {k [n] : ω ik 2a} [n]. The set set of sufficiently large coordinates of ω can be viewed as the support of ω and ω = . Lemma C.2. For all = j, we have K i. We claim that K are disjoint for = j. = for all = j, that is the j)V = ω i)V, (ω Proof of Lemma C.2. Recall that we define := maxi=jω (ω V, ω = j. By the definition of ω 2a. Then, we have ω jk i, ω ω that = o(1). Suppose that there exists , we have by the nonnegativity of the weights that ω i, ω and it holds from the relationship for some 2a and jk 2a. This contradicts with the definition of where for all = j. ik ω ikω i, ω This disjointness plays crucial role in showing that the feature vi is essentially linear , we define truncated version }. With the definition of k : combination of the features {v of ω as i(K ω )k = (cid:40) ω ik 0 if if / , and = ω i(K )V . = [n] Let us take is zero elsewhere. In other words, ω i(K and the residual We can write the residual as ω . We define similarly ω ) and ω i(K ), respectively. Then, we can show that ) the vector which equals i(K ) are the projection of ω i(K on the set and onto the top elements i)V . closely approximates (ω (ω i)V 2 ))V 2 2 = (ω i(K 2 ))diag(V )(ω = (ω i(K 2 d) o(ω = Θ(ω i(K )2 )) + ω i(K )2 1 d). i(K i(K )(V diag(V ))ω i(K ) Here, note that the squared ℓ2 norm can be upper bounded by the ℓ1 norm times the ℓ norm, which is (ω i)V 2 2 Θ(ω i(K )1ω i(K ) d) + o(ω i(K )2 1 d) = o(d), (C.2) 52 = [n] where implies that (ω bounded as follows: . In the last equality, we use the property that ω i1 = Θ(1). This i)V . Similarly, for ωi we define ui = ωiivi and the residual can be upper i(K )1 ω ui (ωi)V 2 2 (cid:13) (cid:13) (cid:88) ωikvk (cid:13) 2 2 ω (cid:13) i,idiag(V )ωi,i + ω i,i(V diag(V ))ωi,i k=i ωi,i2 2 Θ(d) + o(ωi,i2 1 d) = o(d), (C.3) where ωi,i is the vector of the i-th row of AH with the i-th entry removed. In the last inequality, we use the low co-occurrence condition such that ωi,i2 2 = ω2 i,j = (cid:88) j=i (cid:88) j=i (AH)2 ij nρ2 2 = o(1). Thus, we have ui (ωi)V . Finally, recall that ω (cid:1) = have ui + (cid:0)ω residual bounds from (C.2) and (C.3), we obtain = (ω + (cid:0)(ω ui i)V . By the definitions of ui = ωiivi and i)V i )V , we (cid:1) . Applying the triangle inequality together with the = ω i(K ui i2 (ωi)V ui2 + (ω i)V i2 = o( d), and since ui2 = ωiivi2 = Θ( d), we easily conclude that i2 = Θ( d). Step 4: From small residuals to cosine closeness. Now, we can apply the cosine similarity bound to show that the directions of ui and are close to each other: 1 cos(ui, i) i2 ui 2 2ui2u i2 = o(1). Next, recall that ui = ωiivi and conclude = ω i(K )V . Since cosine similarity is scale-invariant, we i(K )V ) = o(1). 1 cos(vi, ω i(K Since the ordering of the supports in ω ) is irrelevant and the feature order in is arbitrary, we can introduce permutation matrix that reorders the coordinates so that the indices in each {K come before those in be block-diagonal matrix with rows i(K Ωi := ω after reordering). In particular, let Ω Rnn )Q1. Then: }i[n] are arranged in increasing order (i.e., for < j, the indices in + 1 cos(V, ΩQV ) = max i[n] {1 cos(vi, ω i(K )V )} = o(1). This verifies that (H, ) is ε-identifiable within with ε = o(1), as defined in Definition 5.1. Furthermore, if = n, then each must be singleton, and 1 cos(vi, i) = o(1) i2 vi vi2 = o(1). Thus, and must match up to vanishing perturbation and permutation. This completes the proof."
        },
        {
            "title": "D Good Initialization and Gaussian Conditioning",
            "content": "In this section, we provide proofs for two important lemmas: Lemma D.1 on the initialization properties and Lemma D.2 on the Gaussian conditioning. These lemmas provide the necessary foundation for analyzing the SAE training dynamics, enabling us to isolate and control the relevant sources of randomness throughout the analysis. D."
        },
        {
            "title": "Initialization Properties",
            "content": "If we initialize the network with sufficiently large number of neurons , then for each neuron, there must exist feature that aligns well with it. However, the question is how many neurons we need to achieve sufficiently large alignment and with all features of interest simultaneously. Lemma D.1 provides an answer to this question. In particular, we prove that when is sufficiently large, for each feature vi, we can find neuron mi that aligns well with it (InitCond-1) while maintaining small alignment with all other features (InitCond-2). }M i=1 with vi (0, Id) and weights Lemma D.1 (Good initialization). Given i.i.d. {w(0) m=1 independently initialized from the uniform distribution on the unit sphere, then for any constants ε (0, 1) and > 0 such that nc upper bound exp(nO(ε)), with probability at least 1 nc over the randomness of both {vi}n i=1 satisfying the following properties: m=1, one can select sequence of neurons {mi}n i=1 and {w(0) }M features {vi}n 1. For any [n], we have InitCond-1 : vi, w(0) mi (1 ε)(cid:112)2 log(M/n). 2. For any [n], when conditioned on the selection of neuron mi, which aligns well with feature vi in the sense of InitCond-1, the distribution of the remaining features {vj}j=i remains unchanged, i.e., they are independently drawn from (0, Id). 3. For any [n], when conditioned on selecting neuron mi, with probability at least 1 n14ε over the randomness of {vj}j=i, we have InitCond-2 : vj, w(0) mi 2(1 + ε) (cid:112)2 log n, = Proof of Lemma D.1. We present the proof by constructing such m1, m2, . . . , mn explicitly. Suppose 2 , . . . , w(0) we are provided with features v1, v2, . . . , vn and neurons with initial weights w(0) . We first put all the pair-wise alignments vi, w(0) into matrix RnM , where Aim = vi, w(0) for [n] and [M ]. The algorithm execute as follows for going from 1 to n: 1 , w(0) 1. Randomly divide the neurons into disjoint groups M1, M2, . . . , Mn such that each group Mi contains M/n neurons. 2. For each Mi, find the neuron mi as the one that maximizes the alignment with feature vi, i.e., mi = argmax mMi Ai,m = argmax mMi vi, w(0) . By construction, we know that the selection of mi is independent of the selection of mj for = j. It is not hard to see that the distribution of vi, w(0) is the same (up to scaling) as the distribution 54 of the first coordinate of random vector uniformly distributed on the unit sphere. Therefore, for each [n], each group {Ai,m}Mi is iid sampled from the following distribution: Ai,m (cid:12) (cid:12) mMi d= (cid:113) Z1vi2 1 + . . . + Z2 Z2 , where Zk (0, 1), [d]. By the concentration for Chi-square distribution, Lemma I.1, we know that the denominator and also the norm of vi2 satisfies (cid:88) (cid:18)(cid:12) (cid:12) (cid:12) Z2 (cid:12) (cid:12) (cid:12) 2 k=1 (cid:18)(cid:12) (cid:12)vi2 (cid:12) (cid:12) (cid:12) (cid:12) 2 2 (cid:112) log δ1 + 2 log δ1 (cid:112) log δ1 + 2 log δ1 (cid:19) (cid:19) δ, δ. To proceed, we label each d-dimensional random vector as Z(i,m) = (Z(i,m) ), where the superscript (i, m) corresponds to feature and neuron m. Applying union bound over all M/n pairs of (i, m) and choosing δ = nc/M for some universal constant c, we deduce that with probability at least 1 nc, the following holds for all [n] and [M ]: , . . . , Z(i,m) Ai,m Z(i,m) 1 (cid:0)d C(cid:112)d log(nM ) log(nM )(cid:1)1/2 (cid:0)d + C(cid:112)d log(nM ) + log(nM )(cid:1)1/2 , where is universal constant. Moreover, by property of the maximum of Gaussian random variables in Lemma I.4, it holds that (cid:18) max mMi Z(i,m) 1 (cid:19) (1 ε/2)(cid:112)2 log(M/n) 1 exp (cid:16) (M/n)εε2/4 3(cid:112)π log(M/n) (cid:17) . (D.1) Here, we divide ε by 2 because Ai,m Z(i,m) 1 (cid:0)d C(cid:112)d log(nM ) log(nM )(cid:1)1/2 (cid:0)d + C(cid:112)d log(nM ) + log(nM )(cid:1)1/2 1 ε 1 ε/2 Z(i,m) 1 for small constant ε. Consequently, by multiplying both sides of the inequality inside P() in (D.1) by 1ε 1ε/2 , we can recast the probability statement so that the maximum of Ai,m over all Mi exceeds (1 ε)(cid:112)2 log(M/n). By taking union bound for [n], the probability of successfully finding sequence of neurons m1, m2, . . . , mn satisfying Ai,m > (1 ε)(cid:112)2 log(M/n) for all [n] and [M ] is at least (cid:16) [n] : max mMi Ai,m > (1 ε)(cid:112)2 log(M/n) (cid:17) 1 exp (cid:16) (M/n)εε2/4 3(cid:112)π log(M/n) (cid:17) 1 nc. where we can safely take to some constant as the failure probability is exponentially small in given that n2. To this end, we conclude that with probability at least 1 nc, we can find sequence of non-overlapping neurons m1, m2, . . . , mn such that Ai,mi > (1 ε)(cid:112)2 log(M/n) for all [n]. Observe that the selection of each neuron mi is done independently for each feature. Consequently, when we condition on the selection of mi, the distribution for the remaining features {vj}j=i remains unchanged. This proves the second statement. 55 It remains to analyze the probability that Aj,mi < 2 log for all [n] and = j. By the second statement, we know that when conditioned on neuron mi, the collection {Aj,mj }j=i (for any fixed i) consists of (n 1) independent and identically distributed random variables with distribution (0, 1). Thus, we can apply the tail probability for the maximum of Gaussian random variables in Lemma I.2 to obtain 2(1 + ε) (cid:16) max j[n]:j=i Aj,mi > 2(1 + ε) (cid:112)2 log (cid:17) n12(1+ε) n14ε. Thus, we prove the last argument for Lemma D.1. direct corollary of Lemma D.1 is that InitCond-1 and InitCond-2 hold simultaneously for all [n] and = with probability at least 1 nc n4ε 1 nε after taking union bound over the success of InitCond-2 for all [n]. These two conditions together imply that the neuron mi exclusively focuses on feature vi at initialization, which is crucial for developing 1 o(1) alignment with feature vi during training. D.2 Rewriting the Gradient Descent Iteration Single neuron analysis. In the previous Lemma D.1, we have shown correspondence between each feature vi and neuron mi such that the initial weight of neuron mi aligns well with feature vi while maintaining small alignments with all other features. In other words, mi is the neuron that is most likely to learn feature vi during training. As the neuron dynamics are decoupled under the small output scale assumption, we only need to analyze the dynamics of neuron mi to understand how feature vi is learned. In the following, we denote by the feature of interest and by wt the weight of the Notation. corresponding neuron at iteration t. Let be the maximum number of steps considered and the time step ranges from 0 to . For the sake of notational convenience, we also denote the feature of interest by w1 = and the normalization sw1 = v/v2. Meanwhile, w0 = sw0 is the initialization that is already normalized to unit length. Here, the bar notation indicates that the vector is normalized to unit length throughout the whole proof. Reformulating the iteration. In this section, we reformulate the gradient descent update (6.1) to isolate the contribution of specific feature from the remaining features. Recall that the data matrix is given by = HV , where RN is the weight matrix and Rnd is the feature matrix. The gradient descent update (6.1) with gradient explicit in (6.2) is Modified BA: wt = wt1 + η gt wt1 + η gt2 , where gt = (cid:88) ℓ=1 φ(w t1xℓ; bt)xℓ, which can be written in terms of and as: yt = swt1, bt = At(Hyt), wt = ut + η1 swt1, ut = φ(Hyt; bt), swt = wt/wt2. (D.2) Here, the meaning of these quantities are given as follows: yt Rd is the projection of the normalized weight vector onto all the features, which we refer to as the feature pre-activation. 56 bt is the bias term updated by bias adaptation algorithm At() that depends on the feature preactivation and time t. ut Rn is the feature post-activation that aggregates the post-activation information from all the data points back to the feature space. wt Rd is the unnormalized weight vector after one step of gradient descent update, and swt Rd is the normalized weight vector. In our analysis, as the bias is fixed, At() always returns the same bias value. However, we keep this general form which can be useful for adapting the current proof framework to handle more complex bias adaptation algorithms. Note that φ(Hyt; bt) RN obtained from the gradient calculation in (6.2) is not exactly the post-activation (recall definition φ(x; b) = ϕ(x + b) + ϕ(x + b)x, where ϕ is the actual activation function. ) However, in the following proof, we will abuse the notation and refer to φ(Hyt; bt) as the post-activation for brevity. (a) The weight matrix is splitted into matrices and by row according to whether the corresponding entries in the i-th column are zero or not. The nonzero entries in the i-th column of are collected as vector θ. (b) Isolating the i-th feature from feature matrix . Figure 20: Illustration of the split of matrices and . Without loss of generality, suppose is the i-th feature. To isolate the contribution from feature of interest from the remaining features, we decompose the weight matrix into three parts: (i) θ: the non-zero entries of the i-th column, (ii) : the rows with non-zero entries in the i-th column, and (iii) E: the remaining rows with zero entries in the i-th column. Formally, suppose is the i-th feature, then we decompose as follows: θ = (cid:0)Hki : Hki = 0(cid:1) k[N ], = (cid:0)Hkj : Hki = 0(cid:1) k[N ],j[n]{i}, = (cid:0)Hkj : Hki = 0(cid:1) k[N ],j[n]{i}. (D.3) Notably, the rows of and do not include the i-th column of H, as it is already isolated as vector θ. See Figure 20a for an illustration of this decomposition. Using the above decomposition, we can rewrite the actual projection of the weights swt1 on each data point as HV swt1 = Interleave(cid:0)[F ; E] Vi swt1 + [θ; 0] swt1 (cid:1) = Interleave(cid:0)[F ; E] yt,i + [θ; 0] swt1 (cid:1), where [E; ] is the vertical concatenation of and , Vi is the feature matrix with the i-th row removed, and yt,i = Vi swt1 is the vector yt with the i-th entry removed. The interleave operation 57 simply restores the original order of the rows in H. Therefore, we can rewrite the original ut in (D.2) as ut = φ(Hyt; bt) = Eφ(Eyt,i; bt) + φ(F yt,i + θ swt1; bt). (D.4) In order to avoid overcomplicated subscripts, we let denote the feature matrix Vi with the i-th row removed, and let refer to the original i-th row of . See Figure 20b for an illustration of this decomposition. We also rewrite yt,i as yt, and following the above notation, we still have yt = swt1. Now with (D.4), we can explicitly separate the contribution of feature from the remaining features in the gradient descent iteration (D.2) and obtain the following equivalent iteration:"
        },
        {
            "title": "Gradient Descent Iteration",
            "content": "feature pre-activation: bias scheduling: yt = swt1, swt1 = wt1/wt12, bt = At(bt1, Eyt, yt + θ swt1), feature post-activation: ut = Eφ(Eyt; bt) + φ(F yt + θ swt1; bt), weight update: wt = ut + vθφ(F yt + θ swt1; bt) + η1 swt1, (D.5) Note that the notation in (D.5) is self-consistent with E, F, θ defined in (D.3) and V, defined below (D.4). We will keep using this notation throughout the rest of the proof. D.3 Gaussian Conditioning Since both the feature of interest and each row of the feature matrix follow Gaussian distributions, we can leverage the properties of Gaussian distributions to simplify the dynamics. However, the coupling between different iterations prohibits direct application of Gaussian properties. This challenge motivates us to explicitly split the intermediate variables in (D.5) into two components: (i) coupling component that lies in the subspace spanned by the previous intermediate variables, and (ii) an independent component that is orthogonal to this subspace. We can then apply some Gaussian concentration arguments to the orthogonal component to simplify the dynamics. Additional notation. To achieve this, we introduce some additional notations. Let us define Pw1:t1x as the projection of onto the subspace spanned by {w1, . . . , wt1}, and w1:t1x = Pw1:t1x as the orthogonal projection. In the following, we use the notations = w1:t1wt to denote the new direction induced by wt, and we define u1:t1ut in similar manner (note that ut starts from = 1). Note that when < 2, u1:t1 is empty and u1:t1 becomes the identity mapping. Also, we enforce w1 = = 1 = v. In the following, we use the trick of Gaussian conditioning (Bayati and Montanari, 2011; Montanari and Wu, 2023; Wu and Zhou, 2023) to simplify the dynamics in (D.5). Specifically, we will define an alternative dynamics that is distributionally equivalent to the original one, where for each iteration, two new independent Gaussian vectors are introduced to replace the original Gaussian components coming from the matrix. To make the presentation clearer, we will denote the variables in the original dynamics in (D.5) by (yt, wt, ut, bt) and the variables in the alternative dynamics by (ryt, rwt, rut, rbt) in the following proofs. Lemma D.2 (Alternative dynamics). For any N, let z1, z0, . . . , zt and rz1, . . . , rzt be sequences of i.i.d. random vectors from (0, In1) and (0, Id1), respectively, with mutual independence. In addition z1:t 58 and rz1:t are also independent of the initialization sw0 and the feature of interest v. Consider the following alternative iteration for (ryt, rwt): ryt = t1 (cid:88) τ = rατ,t1 ru1:τ zτ + t1 (cid:88) τ =1 rατ,t1 rw ru τ 2 τ 2 ru τ ru τ 2 , (D.6) t1 (cid:88) rwt = ru1:τ zτ , rut τ =1 rw τ rw τ 2 + t1 (cid:88) τ =1 τ , rut ru ru τ 2 rw ru τ 2 τ rw τ rw τ 2 + rw1:t1 rzt ru 2 + θφ(F ryt + θ rswt1; bt) + η1 rswt1, where we define the alignment rατ,t = τ , rswt rw rw τ 2 with rswt = rwt rwt . In addition, (bt, rut) in the alternative dynamics are updated by the same formula as in (D.5): bt = At(bt1, Eryt, ryt + θ rswt1), rut = Eφ(Eryt; bt) + φ(F ryt + θ rswt1; bt). (D.7) Then, conditioned on rw1 = (the same as our previous definition of w1 = v) and rw0 = w0 being the initialization of the neuron weight, the alternative dynamics (ryτ , rwτ , ruτ , rbτ )t τ =1 from (D.6) and (D.7) and the original dynamics (yτ , wτ , uτ , bτ )t τ =1 from (D.5) follow the same distribution. Proof of Lemma D.2. To show that the trajectory from (D.6) and (D.7) follow the same distribution as the trajectory from (D.5), we first decompose the iteration in (D.5) in the following lemma. Lemma D.3 (Decomposition). For the iteration in (D.5), define the alignment between the weight vector swt and the weight direction τ /w τ 2, Then, we have the following decomposition for the preactivation vector yt Rn1: as ατ,t = swt, yt = t1 (cid:88) τ =1 ατ,t1 u1:τ τ τ 2 + t1 (cid:88) τ =1 ατ,t1 u τ 2 τ 2 τ τ , and the following decomposition for the unnormalized weight vector wt Rd: (cid:68) wt = t1 (cid:88) τ = , ut u1:τ τ τ 2 w1:t1V τ τ 2 + (cid:69) τ τ 2 + t1 (cid:88) τ =1 u τ , ut τ 2 u τ 2 τ 2 τ τ 2 + vθφ(F yt + θ swt1; bt) + η1 swt1, Proof. See D.4 for the proof of Lemma D.3. With the above decomposition, if we do the following substitution for yt and wt in the above lemma: u1:tzt u1:tV w 2 , w1:t1 rzt w1:t1V u 2 , the assertion in Lemma D.2 follows immediately. The following proof is devoted to showing that the substitution does not change the joint distribution of the whole dynamics. To show that, we 59 just need to verify that for each iteration t, when conditioned on all the history up to iteration 1, the two newly introduced vectors 2 still follow standard Gaussian distribution and are independent of all the history. w1:t1V t 2 and u1:tV /w /u To proceed, we denote the original iteration in (D.5) by (yt, wt, ut, bt) and the alternative iteration in (D.6) and (D.7) by (ryt, rwt, rut, rbt). Following explicitly from the decomposition in Lemma D.3 and the construction in (D.6), we can further derive the following dependency between the variables in both iterations. Lemma D.4. For each iteration (ut, wt) in (D.5), it holds for any 1 that w1:τ 1V τ τ 2 w1:τ 1V τ τ τ τ 2 τ τ 2 wt σ ut σ w1:0, w1:0, u1:τ u1:τ (cid:27)t1 (cid:27)t1 τ = (cid:26) (cid:26) (cid:26) (cid:26) (cid:18) (cid:18) , , τ =1 (cid:27)t1 τ =1 (cid:27)t (cid:19) , (cid:19) . τ =1 where σ(X) denotes the σ-algebra generated by the random variable X. For the Gaussian conditioning iteration (rut, rwt) in (D.6) and (D.7), it holds for any 1 that rut σ( rw1:0, {P ru1:τ zτ }t τ =1, {rzτ }t1 τ =1), rwt σ( rw1:0, {P ru1:τ zτ }t1 τ =1, {rzτ }t τ =1). Proof. See D.4 for proof of Lemma D.4. The message of the above lemma is intuitive: each iteration only inserts new randomness coming from u1:t1V t1 t1 and w1:t1V u 2 for the original iteration, and from ru1:t1 zt and rw1:t1 rzt for the alternative iteration. Using the dependency results, we next prove the equivalence between the trajectory { rw1, rw0, (ryτ , rwτ , ruτ , rbτ )t τ =1} from the Gaussian conditioning and the trajectory {w1, w0, (yτ , wτ , uτ , bτ )t τ =1} from the original iteration by considering the conditional distribution of the newly introduced randomness at each iteration. Let us define At as realization of the random variables ( rw1:0, z1:t1, rz1:t) or (cid:18) (cid:27)t1 (cid:27)t (cid:26) (cid:26) (cid:19) w1:t, u1:τ w τ τ 2 , τ =1 w1:τ 1V τ τ 2 . τ = By property of the Gaussian ensembles, it holds that (cid:40)(cid:18) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) w 2 u1:tV w w1:0, (cid:26) u1:τ τ τ 2 (cid:27)t (cid:26) , τ =1 (cid:26) (cid:12) (cid:40)(cid:18) (cid:12) (cid:12) (cid:12) (cid:12) w1:0, τ τ 2 (cid:9). zt (cid:8)( rw1:t, ru1:t, z1:t1, rz1:t) = At u1:τ d= u1:tVt d= ru1:t (cid:27)t w1:τ 1V τ τ 2 w1:τ 1V τ τ 2 τ =1 (cid:26) , τ = (cid:27)t1 (cid:19) (cid:41) = At (cid:41) = At (cid:27)t (cid:19) τ =1 (D.8) d= is an independent copy of and is independent of all the histories. Here, the first 2 is orthogonal to any of the previous row/column space where Vt equality holds because that we have conditioned on. In particular, u1:tV /w 60 u1:tV w 2 nal to τ for any τ < t. is orthogonal to {P u1:τ τ τ 2 }t1 τ =1 in the column space of since is orthogou1:tV w 2 ing to the row space orthogonal to is orthogonal to {P w1:τ 1V τ τ 2 τ for any τ < t. }t τ =1 in the row space of since u1:t is projectMoreover, is also independent of w1 = and the initialization w0. See Figure 21 for more intuitive explanation. Therefore, the conditional distribution of 2 is the same as that of an (n t)-dimensional Gaussian vectors. Hence, we are able to replace by an independent copy Vt. For the second equality, we can set zt = Vtw 2, which is again Gaussian vector independent of all the histories. Similarly, let Bt be realization of ( rw1:0, z1:t1, rz1:t1) or u1:tV /w /w (cid:18) (cid:26) w1:0, u1:τ w τ τ 2 (cid:27)t1 (cid:26) , τ =1 w1:τ 1V τ τ (cid:27)t1 (cid:19) τ =1 we similarly have for /u 2 that w1:t1V (cid:12) (cid:40)(cid:18) (cid:12) (cid:12) (cid:12) (cid:12) w1:0, (cid:26) w1:t1V w τ u τ 2 2 rzt (cid:8)( rw1:0, z1:t1, rz1:t1) = Bt d= u1:τ P rw1:t1 (cid:27)t1 τ =1 (cid:9). , (cid:26) w1:τ 1V τ τ 2 (cid:27)t1 (cid:19) τ =1 (cid:41) = Bt (D.9) To this end, it can be concluded that 1. The initializations (w1, w0) and ( rw1, rw0) are the same. 2. By (D.8) and (D.9), we have the same conditional distributions for the updates of (P 2) and those of (P w1:t1V ru1:t butions of (yt, wt) and (ryt, rwt) given the past are the same. 3. The updates of (bt, ut) and those of (rbt, rut) are also the same. /u zt, rw1:t1 rzt), which means the conditional distriu1:tV /w 2, We hence conclude that the joint distribution for the two iterations are the same for any time t. Consequently, we obtain that { rw1, rw0, (ryτ , rwτ , ruτ , rbτ )t τ =1} d= {w1, w0, (yτ , wτ , uτ , bτ )t τ =1}. This completes the proof. Since the alternative dynamics in Lemma D.2 are distributionally equivalent to the original dynamics, we work exclusively with the alternative formulation below. We emphasize the following key point when running the alternative dynamics for steps: The randomness in the alternative dynamics comes from the initialization sw0, the feature of interest v, and the random vectors z1:T and rz1:T . Since the system is rotation-invariant, without loss of generality, we fix the direction of the initialization sw0 in the following analysis, and only consider the randomness over v, z1:T , and rz1:T . 61 Remark. In fact, the iteration in (D.6) is reformulation of (D.5) obtained by decomposing the random matrix into its projections along the row spaces 2 , . . . and column spaces 2 , . . ., and then replacing the corresponding components by the following rules: 1 , 1 , u1:tzt ru1:t rw rw 2 , w1:t1 rzt rw1:t1 ru ru 2 . For detailed explanation, we refer interested readers to Lemma D.3 and its following discussions. In essence, the terms on the right-hand side combine to reconstruct the matrix , as illustrated in Figure 21. crucial property is that these terms are orthogonal in direction; within Gaussian ensemble, such orthogonality implies their mutual independence. This decoupling of randomness across iterations considerably simplifies the subsequent analysis. Figure 21: Illustration of the Gaussian conditioning. After removing the feature of interest v, the remaining part of are sliced into zt and u1:t rzt that are orthogonal to each other. w1:t1 Rewriting the initial conditions under the alternative dynamics. Let us now specify the randomness in equation (D.5) by describing the distributions of the vector and the matrix . In the absence of any conditioning on the initialization, and have i.i.d. standard normal entries. However, the neuron selected for analysis is not arbitrary; it must satisfy the initialization conditions detailed in Lemma D.1. We first restate these conditions in the following more concise form: v, rsw0 (1 ε)(cid:112)2 log(M/n) =: ζ0, ry1 = rsw0 2(1 + ε) (cid:112)2 log 1 =: ζ1 1, where indicates that every element of is no greater than the corresponding element of b. In fact, these two conditions induce correlation among v, , and the initialization sw0. Under the alternative dynamics in (D.6) and (D.7), we can reformulate these conditions without involving as follows: InitCond-1: α1,0 v2 ζ0, InitCond-2: y1 = α1,0z1 + α0,0z0 ζ1 1, (D.10) where ζ0 :=(1 ε)(cid:112)2 log(M/n), ζ1 := 2(1 + ε)(cid:112)2 log . (D.11) Here, we recall that α1,0 = v, sw0/v2 and α0,0 = 0 , sw0/w 0 2. Decoupling the randomness. In the following analysis, we can safely decouple the randomness in and w0 from the randomness in z1:T and rz1:T by definition of the alternative dynamics. Notably, the second initial condition in (D.10) only couples z1 and z0 if we treat α1,0 and α0,0 as deterministic quantities when conditioning on and w0. In fact, if we condition on and w0, the second condition can be satisfied with probability at least 1 nε by Lemma D.1. 62 Rewriting the alignment recurrence under the alternative dynamics. Under the reformulation (D.6), the alignment we are interested in is α1,t = v, wt/(v2wt2). Note that in the decomposition of wt, only the terms in the direction of 1 = w1 = contribute to the inner product v, wt. Therefore, the alignment can be expressed as α1,t = z1, ut + v2 θφ(F yt + θ swt1; bt) + η1α1,t1 wt2 . (D.12) This formula will be useful in the later proof. D.4 Additional Proofs Proof of Lemma D.3. The proof follows from direct decomposition of the preactivation vector yt and the unnormalized weight vector wt. By direct decomposition of sw , we have sw = u1:tV sw + (i)= u1:tV sw + (ii)= u1:tV sw + u 2 , sw 2 2 , sw u 2 2 2 2 1(t 1) + Pu1:t1V sw 1(t 1) sw 2 1(t 1). Here, (i) follows from the fact that for any τ = 1, . . . , 1, uτ = wτ vθφ(F yτ + θ swτ 1; bτ ) η1 swτ 1 span(w1:τ ), which is orthogonal to sw . In (ii), we use the fact that wt = ut wt Pu1:t1ut = vθφ(F yt + θ swt1; bt) η1 swt1 Pu1:t1ut span(w1:t1). , sw = Using the above result, we derive for the preactivation vector yt that = wt, sw 2 sw = , sw Therefore, u 2. yt = swt1 = t1 (cid:88) τ =1 sw τ , swt1 sw τ 2 2 sw τ = = t1 (cid:88) τ =1 t1 (cid:88) τ =1 ατ,t1 sw τ (cid:16) u1:τ sw τ + τ τ 2 u τ 2 τ 2 sw (cid:17) τ 2 1(τ 1) ατ,t1 u1:τ w τ τ + t1 (cid:88) τ =1 ατ,t1 u τ 2 τ 2 τ τ 2 . And also for the unnormalized weight vector wt, we have wt vθφ(F yt + θ swt1; bt) η1 swt1 = w1:t1V ut + t1 (cid:88) τ =1 sw τ sw τ 2 2 sw τ , ut = w1:t1V u 2 + t1 (cid:88) u1:τ τ =1 τ τ , ut sw τ sw τ 2 + t1 (cid:88) τ =1 u τ , ut τ 2 u τ 2 τ 2 sw τ sw τ 2 . Therefore, we complete the proof of Lemma D.3. Proof of Lemma D.4. Recall that ut = Eφ(Eyt; bt) + φ(F yt + θ swt1; bt). This implies that ut can be expressed as function of yt only. This also holds for rut. For each iteration (ut, wt) in (D.5), it holds by the explicit decomposition in Lemma D.3 that (cid:18) (cid:26) ut σ w1:t1, u1:t1, u1:τ (cid:18) wt σ w1:t1, u1:t, (cid:26) u1:τ (cid:27)t1 (cid:19) , τ =1 τ τ 2 τ τ 2 (cid:27)t1 , w1:t1V u 2 (cid:19) , (D.13) τ =1 where σ(X) denotes the σ-algebra generated by the random variable X. For the Gaussian conditioning iteration (rut, rwt) in (D.6) and (D.7), it also holds that rut σ(cid:0) rw1:t1, ru1:t1, {P ru1:τ rwt σ(cid:0) rw1:t1, ru1:t, {P ru1:τ τ =1, zτ }t1 zτ }t1 rw1:t1 τ =1 (cid:1), (cid:1). rzt Notably, for u1 (only depending on y1) we have y1 = α1,0 w1 w12 = w1, sw0 w1 w1 w12 (cid:16) σ w1:0, u1:1V (cid:17) 1 12 by the definition that by u1:1 is the identity mapping and 1 = w1. Similarly, w1 is also measurable (cid:16) w1 σ w1:0, u1:1V 1 12 , w1:0V 1 1 (cid:17) . This verifies the base case for = 1. Now we can recursively apply the dependency results in (D.13) for = 2, 3, . . . and obtain the desired conclusion. This completes the proof of Lemma D.4."
        },
        {
            "title": "E Concentrations Results for the SAE Dynamics",
            "content": "Notation. In the following proofs, we use the blue color box to highlight the definitions that are used in the proofs for readers convenience, and use the olive color box to highlight different versions of the conditions in (6.6) and (6.7) to inform the readers how the conditions evolve throughout the proof. We use N1 to denote the number of rows in matrix and N2 to denote the number of rows in matrix . In the statement of lemma, we use > 4, > 0 to denote some universal constants that may change from line to line. We redefine ρ1 := max ρ2 := max i=j (cid:110) max i[n] (cid:80)N l= H:,i0 , max i=j (cid:80)N l=1 1(Hl,j = 0) 1(Hl,i = 0) (cid:80)N 1(Hl,i = 0) l= (cid:111) , 1(Hl,i = 0) 1(Hl,j = 0) (cid:80)N 1(Hl,i = 0) l=1 . (E.1) Compared to the original definition in the main text, we add an additional term in the definition of ρ1. We remark that this is not an issue as (cid:80)N l=1 max i=j 1(Hl,j = 0) 1(Hl,i = 0) (cid:80)N 1(Hl,i = 0) l= max i=j H:,j0 H:,i0 maxj[n]H:,j0/N 1 maxi[n]H:,i0/N . Since we assume in the main theorem that maxi[n]H:,i0/N 1, we have (cid:80)N l=1 max i=j 1(Hl,j = 0) 1(Hl,i = 0) (cid:80)N 1(Hl,i = 0) l= (1 + o(1)) max i[n] H:,i0 . The two terms in the definition of ρ1 are only different up to factor of 1 + o(1), and hence we can safely stick to the new definition of ρ1 in the proof. Consequently, ρ1 maxi[n1]E:,i0/N1, ρ2 maxi[n1]F:,i0/N2. In addition, N1 (1 ρ1)N . By assuming ρ1 1/2, we have N1 N/2. We use notation = to indicate [x y, + y]. Initialization conditions. In the following analysis, we focus on single neuron whose initialization satisfies the conditions in (D.10) for given feature of interest, v. For clarity, we restate the initialization conditions: InitCond-1: α1,0 v2 ζ0, InitCond-2: y1 = α1,0z1 + α0,0z0 ζ1 1, where ζ0 :=(1 ε)(cid:112)2 log(M/n), ζ1 := 2(1 + ε)(cid:112)2 log . Once InitCond-1 is satisfied for fixed w0 and v, it remains to ensure that the Gaussian vectors z1 and z0 satisfy InitCond-2. In the subsequent analysis, we sometimes relax InitCond-2 so as to leverage the standard Gaussian properties of z1 and z0. In fact, if an event holds with probabiity 1 without enforcing InitCond-2, then the joint event that both InitCond-2 and hold occurs with probability at least 1 nε by union bound. For this reason, unless otherwise specified, we In E.1, we decompose the pre-activation yt into two parts: the Gaussian component Roadmap. , which aggregates independent Gaussian contributions and captures the nominal dynamics, and the non-Gaussian component yt, which accounts for deviations induced by cross-iteration coupling that is typically non-Gaussian. Using this decomposition, in E.2 we demonstrate that only small fraction of the training examples activate the neurona phenomenon we refer to as sparse activation. E."
        },
        {
            "title": "Isolation of Gaussian Component",
            "content": "As is discussed in D.3, the key step in our analysis is to isolate the Gaussian component from the non-Gaussian component. In the following, we decompose yt, which is the alignments between the weight and all features, into the Gaussian component that contains weighted sum of i.i.d. Gaussian vectors, and non-Gaussian part whose ℓ2-norm can be bounded by tracking the evolution of the dynamics. Recall the definition of yt in (D.6), we use the fact that u1:τ zτ = zτ Pu1:τ zτ to decompose yt as yt = = t1 (cid:88) τ =1 t1 (cid:88) τ =1 ατ,t1 u1:τ zτ + t1 (cid:88) τ =1 ατ,t1 u τ 2 τ 2 τ τ 2 ατ,t1 zτ + (cid:16) t1 (cid:88) τ =1 ατ,t1 u τ 2 τ 2 τ τ t1 (cid:88) τ =1 ατ,t1 Pu1:τ zτ (cid:17) . We can thus define the Gaussian component and the non-Gaussian component yt as 65 := t1 (cid:88) τ = ατ,t1 zτ , yt := t1 (cid:88) τ =1 ατ,t1 u τ 2 τ τ τ 2 t1 (cid:88) τ =1 ατ,t1 Pu1:τ zτ . (E.2) In the above, the Gaussian component = (cid:80)t1 τ =1 ατ,t1zτ is obtained by summing independent Gaussian vectors z1, z0, . . . , zt1 with weights ατ,t1. Conditional on these coefficients, is simply standard Gaussian vector independent of the learned directions w1:t1 and u1:t1. In contrast, the non-Gaussian component yt quantifies the deviation of the true feature pre-activation yt from due to cross-iteration coupling. In the sequel, let us recall the form of ατ,t1 in (D.12) and define βt1 as α1,t = z1, ut + v2 θφ(F yt + θ swt1; bt) + η1α1,t1 wt2 , βt1 := (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ = τ,t1 = α2 w1:0 swt12. (E.3) Here, α1,t is the alignment between swt and the feature of interest = w1, and βt is the norm of the projection of swt onto the subspace orthogonal to both sw1 and sw0. Tracking α1,t quantifies how far the neuron has progressed from its initialization sw0 toward the feature direction sw1. Ideally, we want α1,t 1, indicating strong alignment with the feature while remaining confined to the plane spanned by sw1 and sw0. In contrast, βt measures the extent to which the neuron drifts away from that plane due to the influence of irrelevant features. We can build an interesting connection between the non-Gaussian component yt and βt1 as stated in the following lemma. and (n1/c1, nc1) Lemma E.1 (Upper bound the non-Gaussian component yt). Suppose for some universal constant c1 > 1. For all = 1, . . . , , it holds with probability at least 1 nc for some universal constants c, > 0 that yt2 2 Cd β2 t1. Proof. See G.1.1 for detailed proof. E.2 Sparse Activation Before we move on to studying the evolution of α1,t and βt defined in (E.3), we first present concentration results for the neurons activation frequency. To leverage the benefits of sparse activation, we analyze how the scheduled bias bt induces sparsity in the neuron. Concentration for ideal activation. We will first study the ideal case where yt = 0, and then move on to the real case in Corollary E.2 where we replace with yt in Lemma E.3. For more generality, we present full version in Lemma G.4 and derive Corollary E.2 as direct corollary. In the following, recall that el is the l-th row of matrix E, which is submatrix of defined in (D.3). We study the activation frequency of the neuron on the set of data that does not contain the feature (i.e., the rows contained in E). Corollary E.2 (Concentration for ideal activation). Let el be the l-th row of matrix E. For κ0 as the τ =1 ατ,t1zτ with zτ being the threshold defined in Definition A.2, we denote by i.i.d. standard Gaussian vectors. It holds for all nc, αt1 = (α1,t1, . . . , αt1,t1) St, bt sbt = bt + κ0. Let = (cid:80)t1 and any δ (exp(n/4), 1) that with probability at least 1 δ over the randomness of z1:T , the following holds: 1 N1 N1(cid:88) l=1 1(e t + sbt > 0) (cid:0)Φ(sbt) + ρ1st log(n) + ρ1s log(δ1)(cid:1). (E.4) Proof. This is direct corollary of Lemma G.4. Here, neuron is considered active when its ideal pre-activation + bt exceeds the threshold κ0. In the idealized setting (i.e., as N1 , and (0, In1)), the expected activation frequency is exactly Φ(sbt), making the Φ(sbt) term tight. The additional terms in the bound capture the empirical fluctuations in the activation frequency due to data coupling. In particular, the parameter ρ1 quantifies the maximum fraction of data coupled through single feature, thereby governing the fluctuation term. key point to note is that αt1 St also depends on the randomness of z1:T , hence how to approximate with random Gaussian vector is not straightforward. In the proof, we decouple the dependence of on αt1 by proving concentration result for all αt1 that form covering net of St, and then take union bound over the covering net of size nO(t). This gives rise to the log factor in the bound when taking the logarithm of the covering number. Efron-Stein inequality for handling data correlation. In proving the lemma, we use refined version of the Efron-Stein inequality (Boucheron et al., 2003) to overcome challenges caused by data correlation. In our setting, two data points may be correlated if they share the same feature, which violates the independence assumption required by classical concentration results such as Bernsteins inequality. Traditional techniques based on the bounded-differences propertyfor example, McDiarmids inequality (McDiarmid et al., 1989)would treat the left-hand side (LHS) of (E.4) as function (cid:0)y (1), . . . , (n 1)(cid:1) of (n 1) variables, where . Since altering single coordinate has the same effect as modifying the projection of swt onto single feature, and because of each feature influences at most ρ1N1 fraction of the terms in the sum on the LHS, we obtain the bounded-differences property (i) is the i-th coordinate of f (y (1), . . . , (i), . . . , (n 1)) (y (1), . . . , (i), . . . , (n 1)) ρ1. Consequently, McDiarmids inequality would yield fluctuation bound of order (cid:118) (cid:117) (cid:117) (cid:116) n1 (cid:88) i=1 ρ2 1 ρ1 n, which is clearly suboptimal. Unlike McDiarmids bounded-differences inequality, which requires each individual input change to have uniformly small impact on , Efron-Stein only demands weaker bound on the variance incurred by altering one coordinate. We defer interested readers to G.2.1 for detailed proof. Concentration for original activation. To fully characterize the behavior of the activation, we also need to take into account the non-Gaussian component y. This gives rise to the following lemma. 67 Lemma E.3 (Activation with non-Gaussian component). Following the setup of Corollary E.2, suppose sbt < 2. Then for all nc, αt1 St and bt R, it holds with probability at least 1 nc over the randomness of z1:T that 1 N1 N1(cid:88) l=1 1(e yt + sbt > 0) (cid:0)Φ(sbt) + ρ1st log(n) + ρ1sbt2yt2 2 (cid:1). Proof. See G.2.2 for detailed proof. The fluctuation term in the upper bound now depends on both ρ1 and the ℓ2 norm of the non-Gaussian yt. This is because larger yt2 can shift the pre-activations further away from the ideal Gaussian case, thereby in the worst case, increasing the activation frequency. Concentration for α1,t and βt. We next aim to characterize the evolution of the parameters α1,t and βt defined in (E.3). Note that in the formula of α1,t α1,t1 = z1, ut + v2 θφ(F yt + θ swt1; bt) + η1α1,t1 wt2 , we can decompose the first term in the numerator as follows: z1, ut = z1, Eφ(Eyt; bt) + z1, φ(F yt + θ swt1; bt) according to the defintion of ut in (D.5). Here, and are the submatrices of defined in (D.3), where corresponds to the rows not containing the feature of interest v, and corresponds to the rows containing v. To this end, we just need to control zτ , Eφ(Eyt; bt), zτ , φ(F yt + θ swt1; bt), (E.5) for general τ [1 : ] and then specialize to τ = 1. Note that the above two terms for general τ will also be used in computing the norm of wt2 later. Let us just consider simplfied case where zτ is independent of yt (which does not hold in general). To control the fluctuation of the above terms, it is important to compute the second-order moments with respect to the randomness of zτ . As concrete example, for the first term, we have the second-order moment computed as Ezτ (0,In1) (cid:2)zτ , Eφ(Eyt; bt)2(cid:3) = Eφ(Eyt; bt)2 2. The second-order moment of the second term can be computed similarly. Therefore, as first step, we will focus on the follwoing two terms: Eφ(Eyt; bt)2 2, φ(F yt + θ swt1; bt)2 2. (E.6) In E.3, we will first present concentration results for the second-order terms in (E.6) and then use them to derive the concentration results for the two first-order terms in (E.5). In addition, we will also derive the concentration result for the term θφ(F yt + θ swt1; bt) as in the numerator of α1,t1. E.3 Second Order Concentration In this subsection, we present concentration results for the second-order terms with respect to the Gaussian component defined in (E.2): Eφ(Ey ; bt)2 2 and φ(F t + θ swt1; bt)2 2. (E.7) We will bridge the gap between these two terms and the original terms in (E.5) by using the analysis of the non-Gaussian component yt in E.5. For now, let us focus on the two terms in (E.7). We now present our concentration result formally in the following lemma. sbt = bt + κ0 < Lemma E.4 (Second-order concentration for E-related term). Under Definition A.2, let 0, and assume further that sbt = Θ(cid:0) log n(cid:1) and sbt < ζ1, with ζ1 defined in (D.11) as required by InitCond-2. Suppose ρ1 < 1 1/C1 for some universal constant C1 > 0. Then with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T , it holds for all with nc that 1 2 1 Eφ(Ey ; bt)2 2 1(E0) CL2 ρ2 1st2(log n)2 K2 + CL2 Φ(sbt) pEl,l (cid:34) Φ (cid:16) (cid:115) sbt 1 hl, hl 1 + hl, hl (cid:17) (cid:35) hl, hl . (E.8) pEl,l denotes the empirical average over l, [N ], hl denotes the l-th row of H, = γ2 + btγ1, and where E0 is the event such that z1 and z0 satisfy InitCond-2. Here we define Kt as (cid:18) sbt Φ Kt := (cid:18) (cid:16) Φ + (cid:113) 3 4 sbt ℏ2 4, + 1 4 sbt + ℏ4,tζt (cid:17) (cid:113) 1 ℏ2 4,t 1/4 (cid:19) ρ2snsbtΦ (cid:18) + sbt ℏ2 3, + 1 3 (cid:113) 2 3 1/ (cid:19) (cid:19) + (cid:0)ρ2s(cid:1)1/4 (cid:0)t log(n)(cid:1)1/4 + n1/4ρ2 log(n), (E.9) In the above definition, we let ℏq, and ℏq,t for any positive > 1 and time 1 be the smallest real values in [0, 1] such that the following inequalities hold: max j[n] 1 Dj (cid:88) lDj (cid:18) Φ (cid:113) q1 max j[n] 1 Dj (cid:88) (cid:16) Φ lDj sbt l,j + 1 2 sbt + Hl,jζt (cid:113) 1 2 l,j (cid:19) (cid:18) Φ (cid:17)q (cid:16) Φ (cid:113) q1 sbt ℏ2 q, + 1 sbt + ℏq,tζt (cid:17)q (cid:113) 1 ℏ2 q,t (cid:19) , . (E.10) (E.11) Here Dj = {l [N ] : hl,j = 0} is the set of row indices in matrix that has non-zero entries in the j-th column, and ζt = ζ1 + 1(t 2) C(βt1 + α1,t1 + α1,0)(cid:112)t log(nt) with the value ζ1 in InitCond-2 and βt1 = (cid:113)(cid:80)t1 τ =1 α2 τ,t1. Proof. See G.2.3 for detailed proof. Validity of the definition of ℏq,t and ℏq,. The definitions of ℏq, and ℏq,1 are valid as the righthand sides (RHSs) of the above two inequalities are strictly increasing in terms of ℏq, and ℏq,1, respectively, under the condition sbt < ζ1. 69 To see this for ℏq,, we note that Φ() is strictly decreasing function, while (cid:113) q1 sbt 2 l,j + is also strictly decreasing in terms of Hl,j. Therefore, the composition of the two functions is strictly increasing in terms of ℏq,. To see this for ℏq,t, observe that ζt ζ1 > c1 = sbt, since the bias is fixed at bt = in the current algorithm. Moreover, the derivative of the right-hand side of the inequality in (E.11) with respect to ℏq,t is sbt + xζ1 1 x2 ζ1 (sbt)x (1 x2)3/2 sbt + xζ1 1 sbt + xζ1 1 x2 dx (E.12) = qΦ (cid:17)q1 > 0. (cid:17)q Φ (cid:16) (cid:16) (cid:16) (cid:17) Therefore, the definitions of ℏq, and ℏq,t as the smallest real values satisfying the inequalities in (E.10) and (E.11) are valid. Heuristic derivation for Eφ(Ey 2. The first term involves the submatrix E. Before we present the concentration result, let us derive heuristically what the concentration result should look like. Let us denote by el the l-th row of matrix E. We can compute the expectation of the squared norm as ; bt)"
        },
        {
            "title": "1\nN 2\n1",
            "content": "E(cid:2)Eφ(Ey ; bt)2 2 (cid:3) ="
        },
        {
            "title": "1\nN 2\n1",
            "content": "N1(cid:88) l,l=1 E(cid:2)(cid:12) (cid:12)φ(e ; bt) φ(e y ; bt)(cid:12) (cid:12) (cid:3) el, el. If we assume α:,t1 are fixed, then is just standard Gaussian vector, and (e y , l ) (cid:21) (cid:18)(cid:20)0 , (cid:20) 1 el, el (cid:21)(cid:19) . el, el This fact enables direct upper bound on the expectation, as detailed in Lemma E.5. Lemma E.5. Let and c0 > 0 under Definition A.2. For two independent x, (0, 1) and ι (0, 1), it holds that sb = + κ0 < 0. Suppose φ(x; b) (n d)c0 + L(x + sb) 1(x > sb) for some > 0 E[φ(x; b)φ(ιx + (cid:112) (cid:16) 1 ι2 z; b)] CL(n d)c0 + C(L2 + 1) Φ(sb) Φ sb (cid:114) 1 ι 1 + ι (cid:17) . Proof. See G.4.1 for detailed proof. By relaxing the rows el, el of to the corresponding rows hl, hl of H, we derive the second term in the concentration result (E.8). The first fluctuation term is obtained again via the Efron-Stein inequality, which needs careful analysis up to the 4-th moment. In particular, we also apply uniform bound over the sphere St for αt1, which gives rise to the dependency on in the definition of Kt in (E.9). We now turn to the second term in (E.7), which is φ(F + θ swt1; bt)2 2. Lemma E.6 (Second-order concentration for -related term). Under Definition A.2, suppose bt κ0 and let = γ2 + btγ1. For all nc, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T that"
        },
        {
            "title": "1\nN 2\n2",
            "content": "F φ(F + θ swt1; bt)2 2 CL2ρ2 (cid:0)θ2v2 2α2 1,t1 + ρ2n + ρ2t log n(cid:1), where θ2 = θ 2/N2. Proof. See G.2.4 for detailed proof. 70 E.4 First Order Concentration In this subsection, we continue to present the concentration results on the first order terms specified in (E.5). Lets first consider the concentration for zτ , Eφ(Ey ; bt). Heuristic derivation for zτ , Eφ(Ey rewrite the term as ; bt). Let us recall that = (cid:80)t1 τ =1 ατ,t1zτ , and we can zτ , Eφ(Ey ; bt) = N1(cid:88) l=1 zτ φ(e y ; bt) for el being the l-th row of matrix E. Moreover, we have for any fixed αt1 = (α1,t1, . . . , αt1,t1) St and by the fact that el2 = 1 for all [N1], we have (cid:20) 1 ατ,t (cid:21) (cid:18)(cid:20)0 0 ατ,t1 1 zτ , ) (E.13) (e (cid:21)(cid:19) , where [n 1] is the entry index of the vectors. Hence, the term we are interested in should be close to N1(cid:88) l= ζ,ξ i.i.d. (0,1) (cid:2)(cid:0)ατ,t1ζ + where we define (cid:113) 1 α2 τ,t1 ξ(cid:1) φ(ζ; bt)(cid:3) = N1 ατ,t1 pφ1(bt), pφ1(b) = EuN (0,1)[φ(u; b)u]. Building on this intuition, the following lemma provides the concentration result in more detail. Lemma E.7 (First-order concentration for E-related term). Under the condition of Lemma E.4, let = γ2 + btγ1. For all nc, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T that (cid:12) (cid:12) (cid:12) ; bt) ατ,t1 pφ1(bt) 1 zτ , Eφ(Ey N1 CLατ,t1t log(n) (cid:0)(cid:113) sρ1Φ(sbt)t log(n) + sρ1t log(n)(cid:1) (cid:113) (cid:113) (cid:12) (cid:12) (cid:12) 1 α2 τ,t1 Eφ(Ey ; bt)2 2 log(n). + N1 Proof. See G.2.5 for detailed proof. In the above lemma, we bound the deviation of the first-order term zτ , Eφ(Ey ; bt) from its expectation ατ,t1 pφ1(bt) by some ρ1 and Φ(sbt)-dependent fluctuation terms. The dependence on Φ(sbt) is consistent with the intuition that sparser activation which avoids unnecessary activations on other features except the one of interest, often leads to less fluctuation. The following lemma provides upper and lower bound for pφ1(bt). Lemma E.8 (Upper and lower bounds for pφ1(bt)). Suppose Definition A.2 holds and let = γ2 + btγ1. If sbt = ω(1), and κ0sbt = O(1), then sbt = bt +κ0 < 0, C0 4 Φ(sbt) pφ1(bt) 2 C0LΦ(sbt). Proof. See G.4.2 for detailed proof. The message from Lemma E.8 is quite straightforward: the expectation term pφ1(bt) is on the same order as the activation sparsity level Φ(sbt). Heuristic derivation for zτ , φ(F the approximation in (E.13) except that this time each row fl of has norm + θ swt1; bt). Similar to the previous case, we still use 1 θ (cid:113) , and have (f zτ , ) (cid:21) (cid:18)(cid:20)0 0 , (1 θ2 ) (cid:20) 1 ατ,t1 (cid:21)(cid:19) . ατ,t1 1 This leads to the following approximation: zτ , φ(F + θ swt1; bt) N2(cid:88) l=1 ατ,t1 (cid:113) 1 θ2 ExN (0,1) xφ(cid:0)(cid:113) (cid:104) 1 θ2 + θlv swt1; bt (cid:1)(cid:105) . We now present the formal concentration result for zτ , φ(F lemma. + θ swt1; bt) in the following sbt = Lemma E.9 (First-order concentration for -related term). Under Definition A.2, suppose bt + κ0 0 and let = γ2 + btγ1. For all τ < with nc, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T that"
        },
        {
            "title": "1\nN2",
            "content": "t + θ swt1; bt) (cid:12) (cid:12)zτ , φ(F (cid:12) CLατ,t1 ((cid:112)t log(n) + v2α1,t1) l=1 N2(cid:88) ρ2s (t log(n))3/ ατ,t1 (cid:113) 1 θ2 ExN (0,1) xφ(cid:0)(cid:113) (cid:104) 1 θ + θlv swt1; bt (cid:1)(cid:105)(cid:12) (cid:12) (cid:12) + N2 (cid:113) 1 α τ,t1 (cid:113) φ(F + θ swt1; bt)2 2 log(n). Proof. See G.2.6 for detailed proof. Heuristic derivation for θφ(F yt + θ swt1; bt). The last term we need to control is θφ(F θ swt1; bt). Using the Gaussian approximation have + ) as in the previous case, we (0, 1 θ2 θφ(F + θ swt1; bt) N2(cid:88) l=1 θl ExN (0,1) For our convenience, let us define (cid:113) (cid:104) φ( 1 θ + θlv swt1; bt) (cid:105) . ψt := N2(cid:88) l=1 ExN (0,1) (cid:113) (cid:2)θl φ( 1 θ2 + θl swt1; bt)(cid:3), (E.14) and it follows that θφ(F + θ swt1; bt). θφ(F + θ swt1; bt) ψt/ d. Lastly, we present the concentration for sbt = bt +κ0 0 Lemma E.10 (First-order concentration for signal term). Under Definition A.2, suppose and let = γ2 + btγ1. For all nc, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T that (cid:12) (cid:12) (cid:12) 1 N2 θφ(F + θ swt1; bt) ψtN dN (cid:12) (cid:12) CL(cid:0)(cid:112)t log(n) + v2α1,t1 (cid:12) (cid:1) (cid:113) ρ2sθ2 log(n). Proof. See G.2.7 for detailed proof. Lastly, we provide useful bound for the term ψt defined in (E.14) in the following lemma, which is related to the strength of the weight vector θ for the feature of interest. To quantify the strength, we make the following definition Qt := 1 N2 N2(cid:88) (cid:16) 1 l= θl > bt dα1,t1 (cid:17) , θ2 := θ2 2 N2 . (E.15) Lemma E.11 (Bounds for the signal term). Under Definition A.2, it holds for ψt defined in Lemma E.10 that C1θ2Qt N2dα1,t1 ψt CLθ2 N2dα1,t1. Proof. See G.4.3 for detailed proof. E.5 Non-Gaussian Error Propogation In the following, let us define the following error terms Et = Eφ(Eyt; bt) Eφ(Ey Ft = φ(F yt + θ swt1; bt) φ(F ; bt), + θ swt1; bt), φF,t = φ(F yt + θ swt1; bt) φ(F t + θ swt1; bt). The last piece of the puzzle is to control the error propagation in the dynamics due to the nonGaussian component yt in the pre-activation. Let us recall the error terms Et = Eφ(Eyt; bt) Eφ(Ey Ft = φ(F yt + θ swt1; bt) φ(F ; bt) + θ swt1; bt). We are interested in how the error yt propagates through the nonlinear function φ in the update. sbt = sbt < 2. For all nc, it holds with probability at least 1 nc Lemma E.12 (Error propogation for Et). Under Definition A.2 on the activation function, let bt + κ0, = γ2 + btγ1 and suppose over the randomness of standard Gaussian vectors z1:T that Et1 CLN1 (cid:16)(cid:0)(cid:113) sρ1Φ(sbt) + sρ1 s(2 + bt) (n d)c0, + CN1 (cid:112)t log n(cid:1) yt2 + sρ1sbt yt2 2 (cid:17) and the ℓ2 norm of Et are bounded as Et2 (γ2 + btγ1) ρ1N1yt2. Proof. See G.3.1 for detailed proof. 73 In the above lemma, we incorporate the sparsity in the activation to obtain more refined bound for Et1. Next, we also present the error bound for Ft. Lemma E.13 (Error propogation for Ft). Define φF,t = φ(F yt + θ swt1; bt) φ(F v swt1; bt). The following bounds hold: + θ 1. Ft1 sN2L yt2. 2. Ft2 ρ2N2L yt2. 3. φF,t2 ρ2N2L yt2. Proof. See G.3.2 for detailed proof. SAE Dynamics Analysis: Proof of Theorem 6. In the sequel, we will first state more general version of Theorem 6.1, accompanied by the full details on the related definitions and assumptions that are mentioned in the main text. Then we will present the proof of the theorem. F.1 General Version of the Theorem In the follwoing, we first state the definition of the concentration coefficient and general version of the main theorem. Then, we present the rigorous definition of the ReLU-like activation function. Details on concentration parameters h. To measure the magnitude of coefficients associated with each feature, we recall in the definition of the cut-off level for feature in (6.4) as (cid:110) hi := max 1 : 1 Di (cid:88) lDi 1{Hl,i h} polylog(n)1(cid:111) . To measure the concentration level of the global coefficients across all features, we define the concentration coefficient as follows. We first recall the definitions of ℏq, and ℏq,t from Lemma E.4 (with = 1 for any > 1). In particular, ℏq, and ℏq,1 are defined as the smallest numbers satisfying the following inequalities: max j[n] 1 Dj (cid:88) lDj (cid:18) Φ (cid:113) q1 max j[n] 1 Dj (cid:88) (cid:16) Φ lDj sbt l,j + 1 2 sbt + Hl,jζ1 (cid:113) 1 2 l,j (cid:19) (cid:18) Φ (cid:17)q (cid:16) Φ (cid:19) , (cid:113) q1 sbt ℏ2 q, + 1 sbt + ℏq,1ζ1 (cid:17)q (cid:113) 1 ℏ2 q,1 . Here, Dj = {l [N ] : Hl,j = 0} is the set of row indices in matrix that has non-zero entries in log that is formally defined in (D.11). Here, Φ() is the tail the j-th column, and ζ1 = 2(1 + ε) probability function of the standard Gaussian distribution, i.e., Φ(x) = (cid:82) 2π du. The definitions of ℏq, and ℏq,1 are valid as the right-hand sides (RHSs) of the above two inequalities are strictly increasing in terms of ℏq, and ℏq,1, respectively. We defer readers to the discussion under Lemma E.4. We define the concentration coefficient for the weight matrix H, denoted by h, as the smallest number such that eu2/2/ max{ℏ2 4,, ℏ2 3,, ℏ2 4,1} h, (cid:88) j= 1 Dj2 (cid:88) l,lDj (cid:18) Φ (cid:115) sb (cid:19) 1 Hl,jHl,j 1 + Hl,jHl,j nΦ 74 (cid:115) (cid:18) sb (cid:19) , 1 h2 1 + h2 (F.1) In fact, the RHS of the last inequality in (F.1) is also strictly increasing in terms of h, and hence the definition is valid. In the extreme case where does not have any diversity in its nonzero entries, we have the following simple relationship between s, si and s: s} for all [N ] and [n], then Proposition F.1 (Concentrated coefficient H). If Hlj {0, 1/ = hi = 1/ s. In this extreme case, every row of has exactly non-zero entries, and the non-zero entries are 1(Hl,i x) = θi2 all equal to 1/ for [0, 1]. The following proposition relates si and to the sparsity through inequalities that must be satisfied. s. In the following, let us define θ2 pQi(x) = Di1 (cid:80) 2/N2 and lDi Proposition F.2 (General coefficient). Recall the definitions of hi in (6.4) and in (F.1). Suppose the > pQi(hi), 3, then for any feature [n] satisfying the conditions in (6.6) and (6.7) and that θ2 bias < we have the following inequalities: 1/ s, (cid:113) pQi(hi). θ2 hi Proof. See H.1 for detailed proof. General version of Theorem 6.1. . To ensure consistency in the notation, we will also define si = 1/h2 for hi defined in (6.4). We give more general version of Theorem 6.1 in the following theorem, which will be formally proved in the remaining part of this section. In the following, we will let = 1/h2 Theorem F.3. For feature [n], let us take some small constant ε (0, 1) and define Q(i) as Q(i) = pQ(i) (cid:18) Suppose b/ (1 ε)(cid:112)2(logn 1) log (cid:19) . log η log b2/2 log log . For any feature [i], consider the following joint conditions for ρ2, d, Q(i) and bias < 0 with respect to constant parameter ς (0, 1): Individual Feature Occurrence: polylog(n)1, Limited Feature Co-ocurrence: Bias Range: 1 max 2 log 2 ) max (cid:110) 4 logn Q(i), logn Q(i)(cid:111) 1 , 2 h2 4 (cid:0) 2h(1 + ε) + (cid:112) (1 h2 ) logn Q1 (1 + h2 1 ) logn Q(i), + (cid:1)2, 1 (1 ς) logn logn Q(i)(cid:111) . (3h2 + 1) logn Q(i), H:,i0 ρ1N logn(ρ1 h2 2 + (cid:110) 1 Here means + O(log log(n)/ log(n)). Then with probability at least 1 n4ε over the randomness of the features , for any feature such that there exists some constant ςi satisfying the above conditions, there exists at least one unique neuron mi and after at most Ti = max{(2ςi)1, 1} steps of training, we have wTi mi, vi/vi2 1 o(1). 75 Relationship between Theorem 6.1 and Theorem F.3. The main difference between Theorem 6.1 and Theorem F.3 is that the latter allows Q(i) to have larger range of values, while the former requires Q(i) = pQ(i)(hi) to be strictly larger than polylog(n)1. direct consequence of this restriction in Theorem 6.1 is that the range of is smaller compared to that in Theorem F.3. However, the conditions in Theorem F.3 have Q(i) and ρ2, coupled together, which makes it difficult to gain clear understanding, while in Theorem 6.1, we decouple the conditions by enforcing the range of Q(i). Specifically, 1. The condition Q(i) polylog(n)1 is equivalent to (1 ε)(cid:112)2(logn 1) hi by recalling the definition of hi. This gives the range of as in (6.5) if we require all the features to be learned simultaneously. In fact, if the condition is satisfied for only subset of features, our theorem still holds on that subset of features. 2. The individual feature occurrence condition is the same in both theorems, and the limited feature co-occurrence condition in Theorem F.3 will reduce to ρ2 n1/2o(1), which is already implied by the data condition in Definition 5.2. 3. The bias range condition in Theorem F.3 will reduce to the version in Theorem 6.1 by removing the terms that involve Q(i) as log log(n)/ log(n) gap is already enforced by the notation. Moreover, we assume that 3 as mandated in Theorem 6.1. Since by Proposition F.2, if 2 the following inequality 1 2 log 1 (cid:16) 2(1 + ε) + (cid:113) (s 1) logn Q(i)(cid:17) cannot hold, because the right-hand side would exceed 1. Roadmap for the proof of Theorem F.3. The remaining part of this section is organized as follows: Concentration simplification: In F.2, we will combine the concentration results derived in to derive explicitly the simplified concentration results for the atomic terms in (E.3) for the evolution of α1,t and βt. Conditions for strong alignment: In F.3, we formulate set of conditions Cond.(i) to Cond.(iii), Cond.(I) and Cond.(II) that will yield simple two-state recursion. Building upon these conditions, we further identify Cond.(iv) to Cond.(vi) that will guarantee strong alignment α1,T = 1 o(1) with only = O(1) steps of training. Conditions simplification: In F.4, we further simplified the series of conditions into more concise form as in (F.12), which yields the full list of conditions in Theorem F.3. Notation. Following the convention in E, we let in Definition A.2. Recall the definition ζ1 = 2(1 + ε) some small constant ε (0, 1). We let be universal constant that may vary from line to line. sbt = bt + κ0 where κ0 = O((log n)1/2) is defined 2 log in (D.11) for log and ζ0 = (1 ε) F.2 Concentration Results Combined We now combine the concentration results for the second-order terms in Lemma E.4 and Lemma E.6 under the assumption that log n. In particular, by taking the square root of the upper bounds 2 = O(d) holds with probability at least 1 nc (see Lemma I.1), in these lemmas and noting that v2 we can express the combined square-root upper bound as ξt = log Kt + ρ1 1 (cid:118) (cid:34) (cid:117) (cid:117) (cid:116)Φ(sbt) pEl,l Φ (cid:16) (cid:115) sbt 1 hl, hl 1 + hl, hl (cid:17) hl, hl (cid:35) + (cid:112)ρ2d α1,t1 + ρ2 . We formally state the combination of the above two lemmas in the following corollary. Corollary F.4 (Second-order concentration combined). Then under the conditions log n, sbt = log n) < ζ1, ρ1 1, it holds for all nc with probability at least 1 nc over the randomness Θ( of standard Gaussian vectors z1:T and that (cid:113) Eφ(Ey ; bt)2 2 + φ(F yt + θ swt1; bt)2 2 CLN ρ1ξt. + Here, the constant hides some factors from using the inequality (cid:112)2(a + b). We refrain from detailed proof here. With the second order concentration results in Corollary F.4, we can now derive the first-order concentration results for the terms zτ , ut based on Lemma E.7 and Lemma E.8. To further simplify the concentration bound, we impose the additional condition Φ(sbt) ρ1 (t log(n))3, which in particular holds if Φ(sbt) n1 polylog(n). This requirement is reasonable because it ensures that the neuron is not activated too rarely compared to the average occurrence frequency (s/n) of the features. Lemma F.5 (First-order concentration combined). If Φ(sbt) Lsρ1(t log(n))3, sbt = Θ( log n) < ζ1, κ0sbt = O(1), for all nc, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T that zτ , ut = ατ,t1 pφ1(bt) (1 o(1)) CN Lρ1 ρ2s(t log n)3/2 ατ,t1α1,t1 CN ρ1L(cid:112)t log ξt CLN (cid:112)log (cid:0)(cid:113) sρ1dΦ(sbt) + sρ1sbtd βt1 (cid:1) βt1, where ξt is defined in Corollary F.4. Proof. See H.2.1 for detailed proof. In order to derive the recursion for α1,t in (E.3), we need to control the numerator α1,twt2 = v, wt v2 = z1, ut + v2 θφ(F yt + θ swt1; bt) + η1α1,t1. Using Lemma F.5 and the concentration for the second term in Lemma E.10, we derive the following lemma for v, wt/v2. Lemma F.6 (Concentration for numerator in α-recursion). Suppose ρ1d(st log n)1 Φ(sbt) Lsρ1(t log(n))3, sbt = Θ( dα1,t1 1. Furthermore, assume that log n) < ζ1, κ0sbt = O(1), ts log nsbtβt1 1, and C0θ2Qt max (cid:110) Lρ1 ρ2s(t log n)3/2, Ld1Φ(sbt), L(cid:112)t log nρ1 N2 ξt dα1,t1 , Lρ1 βt1 α1,t1 (cid:111) . 77 If η1 N2dC0θ2Qt Then it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T and that v, wt v2 = (1 o(1))N ψt. Proof. See H.2.2 for detailed proof. Now that we have characterized the numerator for α-recursion. It remains to control the denumerator wt2. In what follows, we will decompose the norm wt2 into two parts: the projection onto the subspace spanned by w1:0 and the projection onto the orthogonal compliment of this subspace. For w1:0wt being the projection onto the orthogonal complement of the subspace spanned by w1:0, we have the following bound. Lemma F.7. Suppose ρ1d(st log n)1 Φ(sbt) Lsρ1(t log(n))3, sbt = Θ( O(1), dα1,t1 1, z1:T and that log n) < ζ1, κ0sbt = ρ2s(t log n)3/2 1, and η1 Φ(sbt). Then, for all d, it holds with probability at least 1 nc over the randomness of standard Gaussian vectors ts log nsbtβt1 1, w1:0wt2 CN Lρ1 d(cid:0)ξt + dβt (cid:1). Proof. See H.2.3 for detailed proof. For Pw1:0wt being the projection onto the subspace spanned by w1:0, we have the following bound. Lemma F.8. Suppose ρ1d(st log n)1 Φ(sbt) Lsρ1(t log(n))3, sbt = Θ( O(1), ts log nsbtβt1 1, and dα1,t1 1. Furthermore, assume for some constant C0 > 0 that log n) < ζ1, κ0sbt = C0θ2Qt max (cid:110) Lρ1 ρ2s(t log n)3/2, Ld1Φ(sbt), L(cid:112)t log nρ1 N2 ξt dα1,t1 , Lρ βt1 α1,t1 (cid:111) . If η1 N2dC0θ2Qt Φ(sbt), then it holds with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T and that (cid:13) (cid:13)Pw1:0wt (cid:13) (cid:13)2 = (1 o(1)) (cid:113) (N ψt)2 + (cid:0)N α0,t1 pφ1(bt)(cid:1)2. Proof. See H.2.4 for detailed proof. Combining the results from Lemmas F.7 and F.8, we obtain the upper bound for wt2. Lemma F.9. Suppose ρ1d(st log n)1 Φ(sbt) Lsρ1(t log(n))3, sbt = Θ( log n), κ0sbt = O(1), ρ2s(t log n)3/2 1. Furthermore, assume for some constant ts log nsbtβt1 1, dα1,t1 1 and C0 > 0 that C0θ2Qt max (cid:110) Lρ1 ρ2s(t log n)3/2, Ld1Φ(sbt), L(cid:112)t log nρ1 N2 ξt dα1,t , Lρ1 βt1 α1,t1 (cid:111) . If η1 N2dC0θ2Qt Φ(sbt), then it holds for all the randomness of standard Gaussian vectors z1:T and that with probability at least 1 nc over wt2 (1 o(1)) (cid:113) (N ψt)2 + (cid:0)N pφ1(bt)(cid:1)2 + CN Lρ1 dξt. Proof of Lemma F.9. By the triangle inequality, it holds that wt2 Pw1:0wt2 + w1:0wt2 (cid:113) (1 + o(1)) (N ψt)2 + (cid:0)N α0,t1 pφ1(bt)(cid:1)2 + CN Lρ d(cid:0)ξt + dβt1 (cid:1). C0θ2Qt Lρ βt1 By condition N2 α1,t1 and the lower bound ψt Cθ2QtN2dα1,t1 shown in Lemma E.11, we have ψt CN Lρ1dβt1 satisfied and can be absorbed into the upper bound of Pw1:0wt2. Hence, we conclude the proof. When we derive the above lemmas step by step, we collect all the conditions used in the final Lemma F.9. In the following proof, we will be focusing on the conditions listed in the statement of this lemma. F.3 Two-State Alignment Recursion From now on, we adhere to the fact that the bias remains fixed throughout the dynamics. Thus, sb = + κ0. To further simplify the we drop the time index in bt (writing it simply as b) and define conditions in the previous section, we have the following lemma. Lemma F.10. Consider fixing the bias to be < 0 and hold. With the following conditions at initialization: sb = b+κ0 < 0. Suppose InitCond-1 and InitCondρ2s(T log n)3/2 1, η1 N2dC0θ2Q1 Φ(sb). log n) < ζ1, κ0sb = O(1), (i) sb = Θ( (ii) ρ1d(sT log n)1 Φ(sb) Lsρ1(T log(n))3. (cid:110) (iii) N2 C0θ2Q1 max Lρ1 If for some time step (I) α1,t1 t2α1,0, we have log nsbβt1 1, ρ2s(T log n)3/2, Ld1Φ(sb), log nρ1 ξ1 dα1,0 (cid:111) . (II) N C0θ2Qt Lρ1 βt1 α1,t1 . Let us define λ0 = CLρ1 C0θ2 N2/N , λt = λ0 Qt , rξt = 1 (cid:16) ξ1 α1,0 (cid:17) st2(log n)3/2 1(t 2) + for some sufficiently large constant > 0. Under the above conditions, we have the following conclusions: 1. All the conditions in Lemma F.9 hold for ; 2. Then with probability at least 1 nc over the randomness of standard Gaussian vectors z1:T and v, we have the following two-state alignment recursion: Two-State Alignment Recursion βt α1,t λt (cid:16)rξt + βt1 α1,t1 (cid:17) , 1 α1,t (1 + o(1)) + λt (cid:16) Φ(sb) ρ1d 1 α1,t1 + rξt (cid:17) . 79 Proof. See H.3.1 for detailed proof. From the above lemma, we can obtain the following observations: The ratio λtΦ(sb)/ρ1d controls the growth of the alignment α1,t. In order for the alignment to grow faster, we need smaller activation frequency Φ(sb), i.e., larger bias sb in the absolute value. The term λt controls the growth of the ratio βt/α1,t. By definition, we know that λt 1. The maximum alignment achievable is 1 o(1). Therefore, the best we can do is to set λt as close to 1 as possible while exploiting small ratio Φ(sb)/ρ1d to ensure that the alignment α1,t goes to 1 before the ratio βt/α1,t blows up. Since rξ1 to avoid large β0 = 0, we have β1/α1,1 = λ1 rξ1. This means we also need small initial value ratio β1/α1,1 at the beginning. In the sequel, we quantitatively analyze the evolution of the above (cid:1), we note that Qt is recursions. Before we proceed, by definition Qt = 1 N2 nondecreasing in α1,t1. Therefore, we have the following fact: dα1,t1 1(cid:0)θl > (cid:80)N2 l=1 Fact F.11. If α1,t1 α1,1, then Qt Q2 and λt λ2. Expanding the recursions. Let us define T0 + 1 as the minimum of such that either of the following conditions fails: T0-Cond.(1). Cond.(I) or Cond.(II); T0-Cond.(2). α1,t1 α1,1; T0-Cond.(3). < log(n). In other word, T0 is the stopping time up to which all the conditions above hold. We have λt λ2 by Fact F.11 and the definition λt = λ0/Qt. To obtain simple recursion for α1,t, we take (cid:16) C1 = 1 + o(1) + λ2ξ1 dα1,0 + Cλ sT 2 0 (log n)3/2 (cid:17) 1 1 λ2Φ(sb)/ρ1d . (F.2) Here, we take the o(1) term above to be the maximum of all the o(1) terms in the recursion for α1,t for any T0. For 2 T0, we have from substracting C1 from both sides of the recursion for α1,t that 1 α1,t C1 (1 + o(1)) + λt (cid:16) 1 + o(1) + (cid:16) Φ(sb) ρ1d λ2ξ1 dα1,0 1 α1,t1 Cλ2 + (cid:16) ξ + 1 α1,0 sT 2 0 (log n)3/2 (cid:124) λ2Φ(sb) ρ1d (cid:123)(cid:122) (cid:16) 1 α1,t1 (cid:17) , 2 T0."
        },
        {
            "title": "Using the fact that",
            "content": "+ st2(log n)3/2 1(t 2) (cid:17)(cid:17) (cid:17) C1λ2Φ(sb) ρ1d (cid:125) 1 α1,1 C1 1 + o(1) + (cid:16) λ1Φ(sb) ρ1d (cid:16) λ1Φ(sb) ρ1d (cid:17) λ1ξ1 + + (cid:17) λ1ξ1 1 α1,0 C1 1 α1,0 , 80 (F.3) (F.4) we obtain that 1 α1,t (cid:16) λ2Φ(sb) ρ1d (cid:17)t (cid:16) λ1Φ(sb) ρ1d + (cid:17) λ1ξ1 1 α1,0 + C1, 1 T0. (F.5) In the above formula, we can extend to allow = 1 as 1 α1, 1 + o(1) + (cid:16) λ1Φ(sb) ρ1d + (cid:17) λ1ξ1 1 α1,0 C1 + (cid:16) λ1Φ(sb) ρ1d + (cid:17) λ1ξ1 1 α1,0 . For the ratio βt/α1,t, we use the fact that λt λ2 for 2 T0 and also that rξt 1 (cid:16) ξ1 α1,0 + sT 2 0 (log n)3/2(cid:17) , 2 T0 to expand the recursion for βt/α1,t as follows: + sT 0 (log n)3/2(cid:17) (cid:88) λtτ +1 2 + λt1 2 β1 α1,1 + sT 2 0 log(n)3/2(cid:17) τ =2 2 + λt λt1 2 λ1ξ1 dα1,0 βt α1,t = (cid:16) ξ1 α1,0 (cid:16) ξ α1,0 1 T0 λt1 2 (cid:16) (T0 + λ1) ξ1 α1, + sT 3 0 log(n)3/2(cid:17) , 1 T0, (F.6) where in the second inequality, we use the fact that λ2 1 and the recursion for the ratio that β1 α1,1 λ1 rξ1 = λ1ξ1 dα1,0 . (F.7) Also in the last equality of (F.6), we can relax the condition to allow = 1 as the right-hand side for = 1 clearly upper bounds the right-hand side of (F.7). Using the results derived in (F.5) and (F.6), we now have the following statement. Now, building upon the results derived in (F.5) and (F.6), we have the following lemma, which summarizes the additional conditions needed to ensure that the alignment α1,t can be driven to 1 o(1). Lemma F.12. Let ς (0, 1) be constant. Take ϵ = log log n/(ς log d) for some sufficiently large constant > 0. Suppose InitCond-1 and InitCond-2, Cond.(i) to Cond.(iii) hold. Under the following conditions (iv) λ0 = Θ(polylog(n)). (v) λ1 0 Q1 dς = Φ(sb)/ρ1d. log n). (vi) ξ1/Q1 dϵ/(λ0 there exists time ((2ς)1 1) T0 such that α1,t = 1 o(1), where T0 is the stopping time before and at which T0-Cond.(1) to T0-Cond.(3) hold. Proof. See H.3.2 for detailed proof of the lemma. Since T0, Cond.(I) and Cond.(II) hold for all automatically. In summary, in Lemma F.12, we have shown that under Cond.(i) to Cond.(vi), the alignment α1,t can be driven to 1 o(1) in constant time steps. 81 F.4 Simplifying the Conditions of Lemma F.13 To finish the proof of Theorem 6.1, it remains to simplify the conditions in Lemma F.12. As first step, we have the following lemma. Lemma F.13. Under InitCond-1, InitCond-2, and Definition A.2, Cond.(i) to Cond.(vi) hold upon the following conditions for some constant ς (0, 1) and ϵ = log log n/(ς log d) for some sufficiently large constant > 0: Q1 λ dς = Φ(sb) ρ1d max dϵς (cid:110) Ls log(n)3 (cid:111) , (F.8) log ξ1, η1 Φ(sb). λ0 = O(polylog(n)), Proof. See H.4.1 for detailed proof. Next, we will plug in the definition of ξ1 into the above condition to obtain the statement in Theorem 6.1. In what follows, let us define as the smallest number such that max{ℏ2 4,, ℏ2 3,, ℏ2 4,1} h2 , (cid:88) j= 1 Dj2 (cid:88) l,lDj (cid:18) Φ (cid:115) sb (cid:19) 1 Hl,jHl,j 1 + Hl,jHl,j nΦ (cid:115) (cid:18) sb (cid:19) , 1 h2 1 + h2 (F.9) 4,, ℏ2 3, and ℏ where ℏ2 4,1 are defined in Lemma E.4. The definition is valid as the right-hand sides of both inequalities are increasing in h. In addition, we notice that 1 always holds, as = 1 gives the trival upper bounds for all the inequalities in (F.9). In fact, the quantity characterize the concentration level for the empirical distribution of {Hl,j}lDj . Lemma F.14. If (1 h2 ) = Θ(1) for defined in (F.9), it holds that )/(1 + h2 (cid:115) (cid:20) (cid:16) sb Φ pEl,l 1 hl, hl 1 + hl, hl (cid:21) (cid:17) hl, hl Cnρ2 1 Φ(sb) 1h2 1+h2 + ρ1ρ2s2. (F.10) Proof. See H.4.2 for detailed proof. Next, we also upper bound K1 in terms of h. Lemma F.15. Under the conditions that ζ1h/sb < 1ν for some small constant ν (0, 1) and Φ(sb) ρ1, it holds for some sufficiently large constant > 0 that C1K1 (cid:0)n sb(cid:1)1/4 Φ(sb) 1 3h2 +1 + (ρ2snsb)1/4 Φ(sb) 3 8h2 +4 + (cid:0)Φ(sb) (1hζ1/sb)2 1h2 + (ρ2s)1/4(cid:1) (log n)1/4 + n1/4ρ2s log n. Proof. See H.4.3 for detailed proof. In the following, let us take = O(polylog(n)), = O(polylog(n)) and = nx0, ρ1 = nx1, ρ2 = nx2, Φ(sb) = n1+x3. (F.11) Using the above configurations, we have by the Mills ratio that sb = (cid:112)2(1 x3) log O(log log(n)). 82 In the following, we use the notation to denote that + O(log log(n)/ log n), and to denote that and x. Consequently, we have sb/ ζ1 sb 2(1 + ε) log (cid:112)2(1 x3) log = (1 + ε) 1 log (cid:112)2(1 x3), and (cid:114) 2 With Lemmas F.14 and F.15, we can now upper bound ξ1 as logn K1 max (cid:110) 1 4 + x3 1 3h2 + , 1 x2 4 + 3(x3 1) 8h2 + 4 , ( 1 x3 2h(1 + ε))2 1 h2 , x2 , 1 4 (cid:111) . x2 In addition, using Lemma F.14, the second term in the definition of ξ1 is upper bounded as (cid:32) logn ρ1 1 (cid:118) (cid:117) (cid:117) (cid:116)Φ(sb)pEl,l (cid:115) (cid:20) Φ (cid:16) sb 1 hl, hl 1 + hl, hl (cid:17) hl, hl (cid:21)(cid:33) max (cid:110) x3 + h2 1 + h2 1 2 , x3 x2 + x1 1 (cid:111) . Therefore, ξ1 is upper bounded as logn ξ1 max (cid:110) 1 4 + , x3 1 3h2 + 1 1 4 , x2 4 x2, 1 x2 4 x3 2 + ( + , 3(x3 1) 8h2 + 4 (1 h2 )(x3 1) 2(1 + h2 ) 1 x3 2h(1 + ε))2 1 h2 , , x3 x2 + x1 1 2 , x2 2 , 1 2 (cid:111) . x2 Plugging this bound into the first inequality in (F.8), we have the following reformulation: logn Q1 x3 (1 ς)x0 logn ξ1, x3 0. where we note that ϵ = O(log log(n)/ log n) and can be ignored in the context of notation. Therefore, we just need to solve the following inequality system: logn Q1 max , + (cid:110) 1 4 x3 1 3h2 + 1 x2 1 4 4 0 x3 (1 ς)x0 + logn Q1, 1 x2 4 x3 2 x2, + , + 3(x3 1) 8h2 + 4 (1 h2 , )(x3 1) 2(1 + h2 ) 0 x2 1, , ( 1 x3 2h(1 + ε)) , 1 h2 x3 x2 + x1 1 2 0 x1 1, , x0 0, x2 2 , 1 2 (cid:111) x2 Solving this inequality system, we arrive at the following conditions that ensures (F.8): 1 x2 max (cid:110) 4 logn Q1, logn Q1 (cid:111) , 1 2 logn (cid:17) (cid:16) N2 ρ1N 0, η1 Φ(sb) 0 x3 min + (1 + h2 h2 (cid:110) 1 2 1 (cid:0) 2h(1 + ε) + ) logn Q1, (cid:112) h2 3 4 4 (1 h2 ) logn Q1 + (3h + 1) logn Q1, (cid:1)2, (1 ς)x0 + logn Q1 (F.12) (cid:111) , Now, the first condition involving x2 = logn(ρ1 2 ) can be transformed into the Limited Feature Cooccurrence condition in Theorem F.3. The second condition logn(N2/ρ1N ) 0 can be transformed into the Individual Feature Occurrence condition in Theorem F.3 by noting that N2 = H:,i0 for feature of interest. The third condition η1 Φ(sb) can be transformed into log η log log Φ(sb) + O(log log n), where the second term on the right-hand side can be further upper bounded as log Φ(sb) sb2 2 + O(log log n) b2 2 + O(log log + sbκ0 + κ 0) b2 2 + O(log log n), where we use the Mills ration in the first inequality and the fact that κ0 = O((log n)1/2) and sb < 2 log in the second inequality. Therefore, sufficient condition will be log η log b2/2 log log ."
        },
        {
            "title": "The last condition involving",
            "content": "x3 = 1 b2 O(log log(n)) 2 log = 1 b2 O(cid:0)log log(n) + bκ0 + κ2 2 log 0 (cid:1) 1 b2 2 log can be transformed into the Limited Feature Co-occurrence condition in Theorem F.3. Lastly, we remind the readers that Q1 is also lower bounded as function of x3, which is shown in the following proposition. Proposition F.16. Under InitCond-1 and the reparameterization in (F.11), we have (cid:32) Q1 pQ b/ log (1 ε)(cid:112)logn 1 (cid:33) , where pQ(x) :="
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) l=1 1(θl x) is the tail function for the empirical distribution of θl. Proof of Proposition F.16. By InitCond-1, we have tion of Qt in (E.15), we have by the non-increasing property of dα1,0 (1 ε)(cid:112)2 log(M/n). Recall the defini- (cid:16) Q1 = pQ (cid:17) dα1,0 (cid:32) pQ This completes the proof of Proposition F.16. pQ() that (cid:33) (1 ε)(cid:112)2 log(M/n) . Note that using the lower bound on Q1 only strengthens the conditions in (F.12). Hence, we can directly plug in the lower bound of Q1 into all the conditions in (F.12), and this gives us the final statement of Theorem F.3."
        },
        {
            "title": "G Proofs for Concentration Results",
            "content": "In this section, we provide proof for the concentration results presented in the previous section. We first provide proofs for Lemma E.1 that controls the norm of the non-Gaussian component yt. Then we give the proof for the concentrations of the second-order and first-order terms in the decomposition of the alignment recursion. Finally, we provide the proof for the error propagation in the dynamics due to the non-Gaussian component yt. G.1 Proofs for Non-Gaussian Components In this subsection, we provide the proofs that are related to the Gaussian & non-Gaussian components. In particular, we provide the proof for Lemma E.1 that controls the norm of the non-Gaussian component yt. 84 G.1.1 Proof of Lemma E.1 By definition of yt in (E.2), we further define y(1) = t1 (cid:88) τ =1 ατ,t1 u τ 2 τ 2 τ τ 2 , y(2) = t1 (cid:88) τ =1 ατ,t1 Pu1:τ zτ , and thus yt = y(1) of y(1) ratio + y(2) respectively. To proceed with controlling the norm y(1) . The proof of Lemma E.1 is then based on the bounding the ℓ2 norm 2, we first control the and y(2) τ 2/u τ 2 via the following lemma. Lemma G.1 (Ratio and suppose (n1/c1, nc1) for some universal constant c1 (0, 1). For all = 1, . . . , , it holds with probability at least 1 nc for some universal constant c, > 0 that τ 2). Take some total step τ 2/u (cid:12) (cid:12) (cid:12) u 2 2 (cid:12) (cid:12) C(log n)1/2, (cid:12) (cid:12) (cid:12) (cid:12) u 2 2 2 2 (cid:12) (cid:12) C(cid:112)d log n. (cid:12) Proof. See G.1.2 for detailed proof. With Lemma G.1, we can now control the ℓ2 norm of y(1) and y(2) respectively with the following two lemmas. Lemma G.2 (ℓ2 norm of y(1) probability at least 1 nc for some universal constant c, > 0 that ). Under the conditions in Lemma G.1, for all = 1, . . . , , it holds with (d C(cid:112)d log n) w1:0 swt12 2 y(1) 2 2 (d + C(cid:112)d log n) w1:0 swt12 2. Proof. See G.1.2 for detailed proof. Lemma G.3 (ℓ2 norm of y(2) probability at least 1 nc for some universal constants c, > 0 that ). Under the conditions in Lemma G.1, for all = 1, . . . , , it holds with y(2) 2 2 C(t + log n) w1:0 swt12 2. Proof. See G.1.2 for detailed proof. Combining Lemma G.2 and Lemma G.3, we complete the proof of Lemma E.1 by additionally noting that yt2 2 2y(1) 2 2 + 2y(2) 2 2 2(d + C(cid:112)d log + C(t + log n)) swt12 2. w1:0 As the first term dP w1: swt12 2 = dβ2 t1 is the leading term, we conclude the proof of Lemma E.1. G.1.2 Additional Proofs for Lemma E.1 Proof of Lemma G.1. Recall from (D.6) that wt = t1 (cid:88) u1:τ zτ , ut τ τ 2 + vθφ(F yt + θ swt1; bt) + η1 swt1. τ , ut τ =1 + t1 (cid:88) τ =1 u τ 2 τ τ τ 2 + w1:t1 rzt 85 Applying projection w1:t1 to both sides, we have = w1:t1wt = w1:t1 rzt 2, 2/u which implies that follows standard Gaussian distribution. Therefore, the concentration in Lemma I.1 that with probability at least 1 nc for all [T ], rzt2. Note that rzt is independent of σ(w1:t1) and 2 χ2(d 1) and we have by 2 = 2/u w1:t1 2 2 (cid:12) (cid:12) (cid:12) u 2 2 2 2 (cid:12) (cid:12) + 2(cid:112)d log(T nc) + 2 log(T nc) C(cid:112)d log(n), (cid:12) where the last inequality holds by conditions that with probability at least 1 nc for all [T ], and (n1/c1, nc1). Therefore, we conclude (cid:12) (cid:12) (cid:12) u 2 2 (cid:12) (cid:12) C(log n)1/2. (cid:12) This completes the proof of Lemma G.1. Proof of Lemma G.2. By definition of y(1) , we have (cid:12) (cid:12)y(1) 2 2 dP w1:0 swt12 2 (cid:12) (cid:12) = α2 τ,t1 (cid:12) t1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:16) u C(cid:112)d log (cid:17)(cid:12) (cid:12) (cid:12) (cid:12) τ 2 2 τ 2 2 swt12 2, τ =1 w1: sup (cid:12) (cid:12) (cid:12) τ =1,...,t1 u τ 2 2 τ 2 2 (cid:12) (cid:12) (cid:12) t1 (cid:88) τ =1 α2 τ,t1 where the first equality holds by (cid:80)t1 second inequality holds by Lemma G.1 with probability at least 1 nc. τ =1 α2 τ,t1 = 1 according to the definition of ατ,t, and the Proof of Lemma G.3. By rewriting the definition of y(2) , we have y(2) = t1 (cid:88) t1 (cid:88) τ =1 j=τ αj,t1 τ τ 2 zj. We note that when conditioned on {ατ,T 1}T 1 zj}j,τ for any 1 τ 1 are i.i.d. standard Gaussian. Let us denote the filtration = σ({ατ,T 1}T 1 τ =1 and u1:T 1, the random variables { τ τ 2 τ =1, u1:T 1). Therefore, we have y(2) d= t1 (cid:88) (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ =1 j=τ α2 j,t1 τ τ 2 τ , where {z τ }t1 τ =1 are i.i.d. standard Gaussian independent of the filtration F. Hence, y(2) 2 2 d= t1 (cid:88) t1 (cid:88) τ =1 j=τ j,t1 (z α2 τ )2. Using the concentration of χ2 distribution in Lemma I.1 gives us (cid:32) (cid:12) (cid:12)y(2) (cid:12) 2 2 t1 (cid:88) t1 (cid:88) τ =1 j=τ α j,t1 (cid:12) (cid:12) (cid:12) (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) (cid:16) t1 (cid:88) τ = j=τ α2 j,t1 (cid:17)2 (cid:112)log(n) + α τ,t1 log(nT ) (cid:33) (cid:12) (cid:12) (cid:12) nc . (cid:88) τ ="
        },
        {
            "title": "Each term inside the probability can be upper bounded by",
            "content": "(cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) (cid:16) t1 (cid:88) τ =1 j=τ (cid:17)2 α2 j,t1 (1 α2 1,t1 α 0,t1) = w1:0 swt12 2, (cid:88) τ =1 τ,t1 = 1 α2 α2 1,t1 α2 0,t1 = w1:0 swt12 2, t1 (cid:88) t1 (cid:88) τ =1 j=τ j,t1 tP α2 w1: swt12 2. Therefore, we conclude that when conditioning on F, it holds with probability at least 1 nc and for all = 1, . . . , that y(2) 2 2 C(t + (cid:112)t log + log n) w1: swt12 2 C(t + log n) w1:0 swt12 2, where is universal constant that changes from line to line. Here, we also use the condition that n. Now, since for any event in the filtration F, the failure probability is at most nc, we can safely remove the conditioning and conclude the proof of Lemma G.3. G.2 Proofs for Concentration Lemmas In this subsection, we first provide formal lemma that characterizes the sparsity of the activations when tuning the bias bt to be some negative value. Building upon this result, we then provide the proofs for the concentration results concerning the recursion of the alignment. G.2.1 Concentration for Ideal Activations The statement of the following lemma slightly generalize beyond the settings in (E.2) for technical convenience. Specifically, we want to understand how the neurons activation frequency concentrates around Φ(bt). As we have the coefficient matrix decomposed into and , we want to have general result that can be applied to all of them. Therefore, we consider general sparse weight matrix in the following lemma. Lemma G.4 (Concentration for Activations). Let RLn (gl)l[L] satisfy gl2 = 1, and assume that is sparse in both rows and columns: + be nonnegative weight matrix whose rows For every coordinate [n], the ith column satisfies G:,i0 ρL for some ρ [n1, 1]. For every row [L], we have gl0 s. For any integer nc (with some fixed constant > 0), define yt = t1 (cid:88) τ = ατ,t1 zτ , where the vectors zτ Rn (for τ = 1, 0, . . . , 1) are independent standard Gaussian random vectors, and the coefficients αt1 = (ατ,t1)t1 τ =1 St belong to the unit sphere in Rt+1. Next, let bt be an arbitrary bias and let ϑt Rt+1 and ς = (ςl)l[L] RL + be fixed vectors. For each neuron [L], define its shifted bias by bt,l = bt ςl α t1ϑt. 87 Then, for any failure probability δ (cid:0)exp(n/4), 1(cid:1), there exists universal constant > 0 such that with probability at least 1 δ (over the randomness of the Gaussian vectors {zτ }t1 τ =1) the following holds simultaneously for all choices of αt1 St and bt R: 1 (cid:88) l=1 1(cid:8)g yt > bt,l (cid:9) (cid:32) 1 (cid:88) l=1 Φ(bt,l) + ρ log(cid:0)n(1 + ςϑt)(cid:1) + ρ log(δ1) (cid:33) , where Φ() denotes the standard Gaussian tail probability. In particular, if t, αt1 and bt are also fixed, then with probability at least 1 δ it holds that 1 (cid:88) l=1 1(cid:8)g yt > bt,l (cid:9) (cid:32) 1 (cid:88) l=1 Φ(bt,l) + ρ log(δ1) . (cid:33) Reduction to Corollary E.2. We remark that when take to be the weight matrix E, to be N1, sbt, and letting ϑt = 0, we directly obtain Corollary E.2 as special to be 1, ρ to be ρ1, bt to be case. In the remaining of this subsection, we will present the proof of this lemma. Proof of Lemma G.4. In the following proof, we will use to denote universal constants that change from line to line. Step I: Concentration for fixed αt1, bt and ϑt. When fixing αt1, bt and ϑt, note that bt,l = bt ςlα t1ϑt is also fixed and the only randomness comes from the Gaussian vectors z1, z0, . . . , zt1. In particular, yt (0, In) since αt12 = 1 by assumption. In the sequel, the discussion will be focused on one time step and we omit the subscript for simplicity. The following is table of the notations we will use in the proof: Notation (Simplified)"
        },
        {
            "title": "Definition",
            "content": "y yt bl bt,l α αt1 y(i) Z(i) τ =1 ατ,t1zτ yt = (cid:80)t1 bt,l = bt ςlα αt1 = (ατ,t1)t1 t1ϑt τ =1 St y(i) is the vector with the i-th coordinate yi replaced by an independent copy (0, 1) (cid:1) = L1 (cid:80)L Z(i) = L1 (cid:80)L l=1 l=1 1(cid:0)g 1(cid:0)g > bt,l y(i) > bt,l (cid:1) Table 2: Summary of notations used in the proof of Lemma G.4. Define = L1 (cid:80)L l=1 (cid:1). To study the concentration of Z, we need to analyze the fluctuations when we change one coordinate of y. This leads us to the definition of y(i) in > bl 1(cid:0)g 88 Table 2 with the corresponding Z(i) = L1 (cid:80)L l=1 Perturbed Variance (EPV) as follows: 1(cid:0)g y(i) > bt,l (cid:1). Let us also define the Exceedance- (cid:20) (cid:88) V+ = i= (cid:0)Z(i) Z(cid:1)2 1(Z > Z(i)) (cid:21) . (cid:12) (cid:12) (cid:12) In the definition of EPV, we only count the contribution from the i-th coordinate of when exceeds its perturbed counterpart Z(i). Next, we show that V+ is actually controlled by itself up to small factor. In particular, for the term inside the expectation in the definition of V+, we have (cid:88) i= where (cid:0)Z(i) Z(cid:1)2 1(Z > Z(i)) = (cid:88) (cid:0)Z(i) Z(cid:1)2 1(yi > i) i=1 1 ρ (cid:88) (cid:18) (cid:88) 1(gl,i = 0) 1(cid:0)g > bt,l (cid:19)2 (cid:1) i=1 (cid:88) l=1 (cid:88) 1(gl,i = 0) 1(cid:0)g > bt,l (cid:1) = ρsZ. i=1 l=1 in the first identity, we use the fact that is monotone in the i-th coordinate yi due to the nonnegativity of the weight matrix G. In the first inequality, we use the fact that 0 1(g > bt,l) 1(g y(i) > bt,l) 1(g > bt,l) thanks to the condition yi > i which is guaranteed by the condition > Z(i). In the last line, we use the Cauchy-Schwarz inequality with the fact that (cid:80)L l=1 = 1(gl,i = 0) ρL. Then, by also noting that each gl is also s-sparse, we 1(gl,i > bt,l) (cid:80)L 0) 1(g l=1 obtain the last equality. Meanwhile, the mean of is simply E[Z] = L1 (cid:80)L l=1 Φ(bt,l), where we use the fact that gl2 = 1 by assumption and gy (0, 1). Invoking Lemma I.5, we conclude that for fixed α and bt, we have with probability at least 1 δ, E[Z] + (cid:112) ρsE[Z] log δ1 + Cρs log δ1 (cid:18) 1 (cid:88) l= Φ(bt,l) + ρs log(δ1) (cid:19) (G.1) for some universal constant > 0. Here, we directly apply the inequality ab + for a, > 0 in the last inequality. In the following, we will apply union bound on αt1, bt to extend the above bound to arbitrary choices of αt1 and bt. Step II: Union bound over αt1 and bt. t. Since is function of α and b, we use the following notation: In the following argument, we will also drop the subscript Z(α, b) ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) 1 (cid:18) t1 (cid:88) l=1 τ =1 ατ l zτ > ςlαϑ (cid:19) . It is sufficient to construct covering net for the pair (α, b). Since the Gaussian vectors zτ are unbounded, we first introduce truncation step in our covering argument. By applying the 89 Chernoff bound for Gaussian tails and then taking union bound over all indices τ = 1, 0, . . . , t1, we deduce that with probability at least 1 (t + 1)n exp(n/2) 1 exp(n/4)/2, we have max τ =1,0,...,t1 zτ . In what follows we condition on this high-probability event. For α St, we take uniform covering net on the sphere, denoted by Nα, such that for any α, there exists α Nα satisfying α α ϵ. The covering number is upper bounded by Nα ϵt. See for example Example 5.8 in Wainwright (2019). To proceed, let us define µ = (t + 1) ( stn + ςϑ). The intuition for this definition is that µ represents the Lipschitz constant of (cid:80)t1 zτ + ςlαϑ with respect to any perturbation on α in the ℓ-norm. For b, leveraging the Gaussian tail property, we define the following covering net with size at most 4µϵ1 + 4: τ =1 ατ Nb = (cid:8)k µ ϵ (cid:12) (cid:12) Z, (cid:2)2ϵ1, 2ϵ1(cid:3)(cid:9) {}. There are three special points in Nb: , the minimal finite point bmin = 2ϵ1 µ ϵ, and the maximal point bmax = 2ϵ1 µ ϵ. For any α St and R, we pick pα = argminαNα α α pb = argmax{b Nb : < µ ϵ}. Therefore, we have by the monotonicity of the indicator and function that Z(α, b)"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) 1 (cid:18) t1 (cid:88) l=1 τ =1 pατ l zt > ςl pαϑ µ ϵ (cid:19) Z(pα, pb). (G.2) On the other hand, for pbl = pb ςl pαϑ, using the definition of pα and pb, it holds that Φ(pbl) Φ(bl 3µ ϵ) 1(bmin pbl < bmax) + 1(pbl = ) + Φ(2µ ςlαϑ) 1(pbl = bmax). The above inequality holds by considering three cases: When pbl [bmin, bmax), we have bl close to pbl up to an approximation error of 3µ ϵ, where one µ ϵ comes from the approximation between α and pα and the other 2µ ϵ comes from the approximation between bl and pbl. When When pbl = , we simply upper bound the tail probability by 1. pbl = bmax, we have Φ(pbl) = Φ(bmax ςlαϑ) Φ(2µ ςlαϑ). Next, we characterize in each case the approximation error between Φ(bl) and the bound given above, which are Φ(bl 3µ ϵ), 1, and Φ(2µ ςlαϑ) respectively. In particular, For the first case pbt [bmin, bmax), we have the approximation error Φ(bl 3µ ϵ) Φ(bl) directly bounded by 3µϵ by Lipschitz continuity of the Gaussian tail function. 90 For the second case pbt = , it must hold that bt < bmin + µϵ, and the approximation error is thus upper bounded by 1 Φ(bt,l) = 1 Φ(bt ςlαϑ) exp((bmin (1 + ϵ)µ)2/2) exp(µ2/4). For the third case pbt = bmax, it must hold that bt > bmax > 2µ. Hence, the approximation error is upper bounded by Φ(2µ ςlαϑ) Φ(bt,l) Φ(2µ ςlαϑ) exp((2µ µ)2/2) exp(µ2/4). Combining these three cases, we conclude that Φ(pbl) Φ(bl) + exp(µ2/4) + 3µ ϵ. (G.3) If we choose the covering net parameter ϵ = ρµ1, then the upper bound can be simplified as Φ(pbl) Φ(bl)+exp(n/4)+3ρ. Since ρ is at least 1/n, we can further conclude that Φ(pbl) Φ(bl)+4ρ given that exp(n/4) 1/n ρ. Lastly, note that the log cardinality of the joint covering net is upper bounded by log(Nα) + log(Nb) log(ϵ1) + log(4µϵ1) Ct log(n(1 + ςϑ)) (G.4) given that ϵ = ρµ1 > (nµ)1. Here, for the last inequality, we use the fact that log(µ) = log((t + stn + ςϑ)) log(n(1 + ςϑ)) since nc for some constant > 0 and n. 1)( We can also apply similar argument for every < nc. This only increases the size of the covering net by factor nc. Combining (G.1), (G.2) and (G.3) with the log cardinality (G.4), we conclude that with probability at least 1 δ for all α, bt and δ > exp(n/4) that Z(αt1, bt) Z(pαt1, pbt) (cid:18) 1 (cid:18) 1 (cid:88) l=1 (cid:88) l=1 (cid:19) Φ(pbl,t) + ρst log(n(1 + ςϑ)) + ρs log(δ1) Φ(bl,t) + ρst log(n(1 + ςϑ)) + ρs log(δ1) (cid:19) , where in the second inequality, we apply union bound on the joint covering net for α and and also for all nc. In the last inequality, we just need change in the constant factor to absorb the approximation error 4ρ for the approximation error Φ(bt,l) Φ(pbt,l). Here, the lower bound δ > exp(n/4) is to ensure that the good event maxτ =1,0,...,t1zτ tn holds true. This concludes the proof of Lemma G.4. G.2.2 Activations with Non-Gaussian Component: Proof of Lemma E. In the following proof, we will use to denote universal constants that change from line to line. sbt = bt + κ0 as the shifted bias. Let us pick ϱt > 0 to be specified later. For any Let us denote by [N1], the neuron is activated only if either of the following two conditions hold: 1. 2. y + sbt > ϱt; + sbt ϱt and yt > ϱt. For the first case, by Corollary E.2, we have with probability at least 1 δ that"
        },
        {
            "title": "1\nN1",
            "content": "N1(cid:88) l=1 1(e + sbt > ϱt) (cid:0)Φ(sbt ϱt) + ρ1st log(n) + ρ1s log(δ1)(cid:1). (G.5) 91 For the second case, we only need to control 1 upper bound 1 (cid:80)N1 l=1 1(e yt > ϱt). We have the following 1 N1 N1(cid:88) l=1 1(e yt > ϱt) = 1 N1 1 N1 l=1 N1(cid:88) (cid:16)n1 (cid:88) l=1 i=1 N1(cid:88) (cid:16) 1 el2 2 y2 t,i 1(El,i = 0) > ϱ2 (cid:17) n1 (cid:88) i=1 y2 t,i 1(El,i = 0) > ϱ2 (cid:17) 1 N1ϱ2 N1(cid:88) n1 (cid:88) l= i=1 y2 t,i 1(El,i = 0) ρ1 ϱ2 yt2 2, (G.6) where the first inequality holds by the Cauchy-Schwarz inequality and the following equality holds by the fact that el2 = 1. The second inequality follows from the fact that 1(x > a) x/a for any > 0 and > 0. The last inequality holds by noting that E:,i0 ρ1N1. Combining (G.5) and (G.6), we conclude that with probability at least 1 nc,"
        },
        {
            "title": "1\nN1",
            "content": "N1(cid:88) l=1 1(e yt > sbt) (cid:0)Φ(sbt ϱt) + ρ1st log(n)(cid:1) + ρ1 ϱ2 yt2 2. (G.7) Let us pick ϱt = sbt1. Note that by assumption sbt < 2, we have sbt ϱt > 3/2 and by the Mills ratio inequality (x1 x3) < Φ(x)/p(x) < x1 x3 + 3x5 for > 0, where p(x) = exp(x2/2)/ 2π is the density for standard Gaussian distribution, we have (sbt ϱt)2 1 + 3(sbt ϱt)4 2π (sbt ϱt) 2 (cid:16) exp (cid:16) (cid:17) (cid:17) sbt2 2 (1 + 3(sbt sbt1)4)sbt (sbt sbt1)(1 sbt2) 1 sbt2 2πsbt exp exp (cid:16) 2 sbt2 (G.8) (cid:17) CΦ(sbt), Φ(sbt ϱt) where in the last inequality, we note that the highlighted ratios are bounded by universal constant. Combining (G.7) and (G.8), we conclude the proof of Lemma E.3. G.2.3 Concentration for Eφ(Ey ; bt)2 2: Proof of Lemma E.4 When treating {ατ,t1}t1 tioned on the good event E, we always have we use to replace sbt = bt + κ0. τ =1 and bt to be deterministic, it follows that (0, 1). When condit (1 + c)(cid:112)2(t + 1) log(nt). In the following, for notation simplicity. We use yj to denote the j-th coordinate of y. Let Good event on bounded Gaussian vectors. Let E0 denote the event that InitCond-2 is satisfied by the vectors z1:0. Throughout the proof, will denote universal constant whose value may change from line to line. Fix time step 1 (we omit the subscript for notational simplicity). Define the good event (cid:110) E1 = max τ =1,0,...,t1 (cid:111) zτ (1 + c)(cid:112)2 log(nt) . Then, by Lemma I.2 (applied to the i.i.d. standard Gaussian vectors z1:t1), we have P(E1) 1 (nt)c 1 nc . 92 Good event on the activation sparsity. Let us define Sj = {l [N1] : El,j = 0}. It holds that Sj N1ρ1. In addition, we define event E2 as E2 = [n 1] αt1 St bt , (cid:88) lSj 1(e + sbt > 0) (cid:18)(cid:88) (cid:16) Φ lSj (cid:17) sbt + El,jyj (cid:113) 1 E2 l,j + Sjρ2st log(n) (cid:19) . To show that E2 holds with high probability, let us define rows indexed by Sj while removing the j-th column. We also normalize each row of ℓ2-norm equal to one. We then have rE as the submatrix of by keeping the rE to have 1. rEl,:2 = 1, rEl,:0 and rE:,k0 (cid:80)N l=1 inequality holds by definition of ρ2. 2. It holds that 1(Hl,j = 0) 1(Hl,k = 0) Sjρ2, where the last Sj1 (cid:88) 1(e + sbt > 0) = Sj1 (cid:88) (cid:16) 1 re yj + lSj lSj sbt + El,jyj (cid:113) 1 E2 l,j (cid:17) , > where rel is the l-th row of rE and yj is the vector with the j-th coordinate removed. In the following, we use zτ,j to denote the j-th coordinate of zτ , and yj = (cid:80)t1 τ =1 ατ,t1zτ,j. We denote by zτ,j the vector zτ with the j-th coordinate removed. Therefore, we can invoke Lemma G.4 with the configurations rE, ρ ρ2, ϑ (z1,j, z0,j, . . . , zt1,j), ςl El,j/ (cid:113) 1 E2 l,j, bt sbt/ (cid:113) 1 E2 l,j and zτ zτ,j to obtain that with probability at least 1 δ/n over the randomness of standard Gaussian vectors z1:t1,j, and for fixed t, αt1, bt and ϑ = (z1,j, z0,j, . . . , zt1,j), 1(e + sbt > 0) (cid:18)(cid:88) (cid:16) Φ (cid:17) + Sjρ2s log(nδ1) (cid:19) (cid:88) lSj sbt + El,jyj (cid:113) 1 E2 l,j sbt + Hl,jyj (cid:113) 1 2 l,j lSj (cid:18)(cid:88) lDj (cid:16) Φ (cid:17) + Sjρ2s log(nδ1) (cid:19) , (G.9) where is universal constant independent of t, αt1, bt and ϑ. Here, in the last inequality, we define Dj = {l [N ] : Hl,j = 0} as the set of rows in matrix that have nonzero j-th coordinate. Since is just submatrix of H, adding more rows to the summation does not decrease the target value in the second inequality. Note that z1:t1,j are independent of z1:t1,j. We thus conclude that the above bound holds with probability at least 1 δ/n over the randomness of z1:t1. Further applying the union bound for all [n 1], we conclude that (G.9) holds with probability at least 1 δ for all [n 1]. Note that the randomness discussed above is only over z1:t1. We invoke covering argument over αt1 St and bt similar to the proof of Lemma G.4. Since the argument is largely the same, we will not repeat it here. The size of the covering net is nO(t+1), and we can pick δ = ncO(t+1) in (G.9), which gives us the upper bound in the definition of E2 with probability at least 1 nc. 93 Refined upper bound on y. We work with fixed time step and aim to bound every coordinate yj for [n 1]. Here, we recall definitions yj = t1 (cid:88) τ = ατ,t1 zτ,j, βt1 = (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ =1 α τ,t1 where βt1 represents the ℓ2-norm of the component of swt1 in the subspace orthogonal to w1:0. (Recall that the coefficients {ατ,t1}t1 τ =1 arise when projecting swt1 onto the orthonormal basis (cid:110) sw1, 0 0 , 1 1 , . . . , w t1 t1 (cid:111) . To leverage InitCond-2, we make change of basis for the first two directions, namely, we replace (cid:110) sw1, 0 0 Note that rw is orthogonal to sw0. The projection of swt1 onto the direction rw satisfies with (cid:8) sw0, rw(cid:9), where rw = α0,0 sw1 α1, 0 0 (cid:111) . (cid:12) swt1, rw(cid:12) (cid:12) (cid:12) = (cid:12) (cid:12)α0,0 α1,t1 α1,0 α0,t (cid:12) (cid:12) α1,t1 + α1,0. Since sw0, rw, and {w τ =1 form an orthonormal basis, the component of swt1 orthogonal to sw0 is bounded by βt1 + α1,t1 + α1,0. Moreover, we can also decompose yt into the new basis as follows: τ /w τ }t1 yt = sw0, swt1 (cid:0)α1,0 z1 + α0,0 z0 (cid:1) + rw, swt1(cid:0)α0,0 z1 α1,0 (cid:1) + t1 (cid:88) τ =1 ατ,t1 zτ = sw0, swt1 y1 + rw, swt1(cid:0)α0,0 z1 α1,0 z0 (cid:1) + t1 (cid:88) τ =1 ατ,t1 zτ Under InitCond-2 the first term, sw0, swt1 y1, is bounded by ζ1. Moreover, since both α0,0 z1 α1,0 z0 and {zτ }t1 τ = have their entries bounded by 2(1 + c)(cid:112)log(nt) on the good event E1, the contribution from the subspace orthogonal to sw0 is bounded by (cid:0)βt1 + α1,t1 + α1,0(cid:1) (cid:112)t log(nt). Thus, by the triangle inequality, for every coordinate we have under event E0 and E1 that yj ζ1 + (cid:0)βt1 + α1,t1 + α1,0(cid:1) (cid:112)t log(nt) =: ζt. (G.10) Good event on the Bernstein concentration. In the following, we will use another good event to control the upper bound in the definition of E2. Consider the function Φ((sbt + xyj)/ 1 x2)q for 1. We demonstrate that this function is Lipschitz continuous and monotonically increasing on the interval [0, 1] if yj > sbt by taking the derivative with respect to x: (cid:16) Φ dx sbt + xyj 1 (cid:17)q = qΦ (cid:16) sbt + xyj 1 x2 (cid:17)q (cid:16) sbt + xyj 1 x2 (cid:17) yj (sbt)x (1 x2)3/2 > 0. 94 Using the upper bound for specified in (G.10), we can define the critical value ℏq,t as the smallest real number such that the following inequality holds: 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(E0 E1) max j[n] 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jζt (cid:113) 1 2 l,j (cid:17)q (cid:16) Φ sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t (cid:17)q . (G.11) As we will only be using {3, 4} in the following proof, we define the event E3 as the event such that for all {3, 4}, αt1 St, bt and [n 1], E3 : n1 (cid:88) j=1 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(E0) 1(E1) n1 (cid:88) j=1 1 Dj (cid:88) lDj (cid:20) Φ (cid:16) sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:21) (cid:16) + Φ (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t log(n) , where is universal constant independent of t, αt1, bt and ζt. To show the event E3 holds with high probability, we can apply the Bernstein concentration inequality in Lemma I.3 for the bounded random variables 1 Dj (cid:88) Φ lDj (cid:16) sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q . That is, for fixed αt1, bt and with probability at least 1 δ over the randomness of z1:t1, we have n1 (cid:88) j=1 1 Dj lDj (cid:118) (cid:117) (cid:117) (cid:117) (cid:116) (cid:88) (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(E0) 1(E1) 2 log δ1 n1 (cid:88) j= (cid:20)(cid:18) 1 Dj + n1 (cid:88) j=1 1 Dj (cid:88) lDj (cid:20) (cid:16) Φ (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:19)2 (cid:21) 1(E0 E1) (cid:17)q(cid:21) + (cid:16) Φ 1 3 (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t log(δ1). Moreover, we have for the second moment term that (cid:20)(cid:18) 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:19)2 (cid:21) 1(E0 E1) n1 (cid:88) j=1 n1 (cid:88) j=1 1 Dj2 1 Dj (cid:88) lDj (cid:88) lDj (cid:20) (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(E0 E1) (cid:21) (cid:88) (cid:16) Φ sbt + Hl,jζt (cid:113) 1 2 l,j (cid:17)q lDj sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t (cid:17)q , (cid:20) (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:21) Φ (cid:16) 95 where in the first inequality, we invoke the upper bound in (G.11). Using the fact that for a, 0, we derive that + n1 (cid:88) j=1 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(E0) 1(E1) n1 (cid:88) j=1 1 Dj (cid:88) lDj (cid:20) (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:21) + (cid:16) Φ 1 3 (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t log(δ1) . (G.12) Now, we apply the covering argument over αt1 St and bt similar to the proof of Lemma G.4. The size of the covering net is nO(t+1), and we can pick δ = ncO(t+1) in (G.12), which gives us the upper bound in the definition of E3 with P(E3) 1 nc. The Perturbed Variance. Given the good events E0, E1, E2, and E3, we define = 1 2 N1(cid:88) l,l=1 Zl,l, where Zl,l = φ(e y; bt) φ(e y; bt) el, el 1(E0 E1 E2 E3). (G.13) For concentration of Z, we consider the following Perturbed Variance (PV) defined as (cid:20)n1 (cid:88) := i=1 (Z Z(i))2 (cid:12) (cid:12) (cid:12) (cid:21) , where the perturbed term Z(i) is defined as follows: Z(i) ="
        },
        {
            "title": "1\nN 2\n1",
            "content": "N1(cid:88) l,l=1 l,l, where Z(i) Z(i) l,l = φ(e y(i); bt) φ(e y(i); bt) el, el 1( ι=0E (i) ι ). τ =1 ατ,t1z(i) Here, y(i) = (cid:80)t1 τ and z(i) τ dent (0, 1) random variable. In addition, the good events {E (i) using z(i) Thus, we obtain is given by replacing the i-th coordinate of zτ by an indepenι=0 are defined similarly to Eι, but 1:t1 instead of z1:t1. We begin by noting the elementary inequality (a b)2 2a2 + 2b2. ι } V"
        },
        {
            "title": "2\nN 4\n1",
            "content": "(cid:34)n1 (cid:88) (cid:32) N1(cid:88) i=1 l,l=1 Zl,l (cid:110) El,i = 0 El,i = 0 (cid:111) (cid:33)2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:35) (cid:125) (cid:124) +"
        },
        {
            "title": "2\nN 4\n1",
            "content": "(cid:123)(cid:122) (I) (cid:34)n1 (cid:88) (cid:32) N1(cid:88) Z(i) l,l 1 (cid:110) i=1 l,l=1 (cid:124) (cid:123)(cid:122) (II) El,i = 0 El,i = 0 (cid:111) (cid:33)2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:35) , (cid:125) where the upper bound is obtained by the following reasoning. For each perturbed quantity Z(i), we have Z(i) ="
        },
        {
            "title": "1\nN 2\n1",
            "content": "N1(cid:88) (cid:0)Zl,l Z(i) l,l (cid:1) 1(cid:8)El,i = 0 El,i = 0(cid:9). l,l=1 96 Note that the difference Zl,l Z(i) l,l is nonzero only when at least one of the vectors el or el has nonzero ith coordinate. The two terms (I) and (II) correspond to the contributions from the original and the perturbed parts, respectively. In what follows we focus on an upper bound for the term (I); the term (II) can be estimated by completely analogous argument. Controlling Term (I). Due to the L-Lipschitz continuity of φ with = γ2 + btγ1, on the good event E1, the absolute value of φ(e y; bt) is bounded by φ(e y, which can be further bounded as y; bt) φ(0; bt) + φ(e y; bt) (d n)c0 + L(cid:112)t log(n) := Bt, where we used that nc, el1 bound holds for φ(e first upper bound φ(e y(i); bt) on the corresponding good event (i) y; bt) φ(e s, and that (d n)c0 1 L(cid:112)t log(n). Note that the same 1 . For Zl,l defined in (G.13), we φ(e y; bt) φ(e y; bt) B2 + sbt > 0) + 2Bt(d n)c0 + (d n)2c0, y; bt) by + sbt > 0) 1(e 1(e + sbt > 0, the neuron is deemed activated and its output is bounded where we recall that if above by Bt. Otherwise, by Definition A.2, the activation is bounded by (d n)c0. Note that the term (d n)c0B1 can be made arbitrarily small as c0 is some large constant no less than 4. Therefore, we just keep the first term above. Secondly, the inner product el, el is upper bounded by (cid:80)n1 1(El,j = 0) 1(El,j = 0) as 1. Lastly, the indicator 1(El,i = 0 El,i = 0) can be j=1 upper bounded by 1(El,i = 0) + 1(El,i = 0). For (I), we then have (I) CB4 4 1 (cid:34)n1 (cid:88) (cid:32)n1 (cid:88) N1(cid:88) i=1 j=1 l,l=1 1(cid:0)e + sbt > 0(cid:1) 1(cid:0)e + sbt > 0(cid:1) (cid:16) 1(cid:8)El,i = 0(cid:9) + 1(cid:8)El,i = 0(cid:9)(cid:17) 1(cid:8)El,j = 0(cid:9) 1(cid:8)El,j = 0(cid:9) (cid:33)2 1(cid:0) ι=0Eι (cid:1) (cid:35) . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Due to symmetry in the indices and l, we can multiply the constant factor by 2 and obtain (I) CB4 4 1 (cid:20)n1 (cid:88) (cid:16)n1 (cid:88) N1(cid:88) 1(e + sbt > 0) 1(El,i = 0) 1(El,j = 0) i=1 j=1 l=1 N1(cid:88) l=1 1(El,j = 0) 1(e + sbt > 0) (cid:17)2 1( ι=0Eι) (cid:21) (cid:12) (cid:12) (cid:12) CB4 4 1 (cid:20)n1 (cid:88) n1 (cid:88) (cid:18) N1(cid:88) i=1 j=1 l= 1(e + sbt > 0) 1(El,i = 0) 1(El,j = 0) (cid:19)2 (cid:18) N1(cid:88) l= 1(El,j = 0) 1(e + sbt > 0) (cid:19)2 1(3 ι=0Eι) (cid:21) , (cid:12) (cid:12) (cid:12) where the last inequality holds by the Cauchy-Schwarz inequality. Note that for = j: N1(cid:88) l=1 1(El,i = 0) 1(El,j = 0) (cid:88) l=1 1(Hl,i = 0) 1(Hl,j = 0) ρ1ρ2N. 97 Using ρ1ρ2N to substitue one (cid:80)N1 l=1 1(e + sbt > 0) 1(El,i = 0) 1(El,j = 0) for = j, we obtain (I) B4 ρ1ρ2 4 1 (cid:20)n1 (cid:88) (cid:88) N1(cid:88) j=1 i=j l=1 1(e + sbt > 0) 1(El,i = 0) 1(El,j = 0) + B4 4 1 (cid:16) N1(cid:88) l= 1(El,j = 0) 1(e + sbt > 0) (cid:17)2 (cid:20)n1 (cid:88) (cid:16) N1(cid:88) 1(e + sbt > 0) 1(El,j = 0) (cid:17)2 j=1 l=1 (cid:16) N1(cid:88) l=1 1(El,j = 0) 1(e + sbt > 0) (cid:17)2 1( ι=0Eι) (cid:21) (cid:12) (cid:12) (cid:12) 1(3 ι=0Eι) (cid:21) . (cid:12) (cid:12) (cid:12) Rearranging the order of summation and using the fact that (cid:80) we can further simplify the terms as i=j 1(El,i = 0) for any fixed j, (I) 2B ρ1ρ2s 3 1 + 2B4 4 1 (cid:20)n1 (cid:88) (cid:16) N1(cid:88) j=1 (cid:20)n1 (cid:88) l=1 (cid:16) N1(cid:88) j=1 l= 1(e + sbt > 0) 1(El,j = 0) (cid:17)3 1(e + sbt > 0) 1(El,j = 0) (cid:17) 1(3 ι=0Eι) 1(3 ι=0Eι) (cid:21) (cid:12) (cid:12) (cid:12) (cid:21) . (cid:12) (cid:12) (cid:12) (G.14) Observe that the above two terms share common structure. We define the common structure as (III) :="
        },
        {
            "title": "1\nN q\n1",
            "content": "(cid:20)n1 (cid:88) (cid:16) N1(cid:88) j=1 l=1 1(e + sbt > 0) 1(El,j = 0) (cid:17)q 1(3 ι=0Eι) (cid:21) , (cid:12) (cid:12) (cid:12) where {3, 4}. Recall the definition Sj = {l [N1] : El,j = 0}. It holds that Sj N1ρ1. We aim to control N1(cid:88) 1(e + sbt > 0) 1(El,j = 0) = Sj1 (cid:88) 1(e + sbt > 0) l=1 lSj in the following. By the definition of the good event E2, we have (III) 1 n1 (cid:88) (cid:32)(cid:18)(cid:88) j=1 lDj (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17) + Sjρ2st log(n) (cid:19)q (cid:33) 1(3 ι=0Eι) 2q1C 1 (cid:32) n1 (cid:88) Djq 1 Dj j=1 (cid:32)n1 (cid:88) j=1 1 Dj (cid:88) (cid:16) Φ lDj Cρq 1 (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q 1(3 ι=0Eι) + (cid:0)ρ2st log(n)(cid:1)q (cid:33) (cid:17)q 1( ι=0Eι) + n(cid:0)ρ2st log(n)(cid:1)q (cid:33) . (G.15) where we use the Hölders inequality for the second line, and in the last line, we absorb the constant factor 2q1 into the universal constant and use the fact that Sj Dj ρ1 98 N1ρ1/(1 ρ1) C1N1ρ1 for all [n 1], where we also absorb the factor Cq constant C. By the definition of the good event E3, it holds that sbt + Hl,jyj (cid:113) 1 2 l,j 1 Dj ι=0Eι) 1(3 n1 (cid:88) (cid:88) lDj j=1 (cid:17)q Φ (cid:16) 1 into the universal (cid:32)n1 (cid:88) j=1 1 Dj (cid:88) lDj (cid:20) (cid:16) Φ sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:21) (cid:16) + Φ (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t (cid:33) . log(n) (G.16) To evaluate the expectation term, we use the Mills ratio Φ(x) Cp(x) for some universal constant > 0, > 0 and p(x) = exp(x2/2)/ (cid:20) Φ (cid:16) sbt + Hl,jyj (cid:113) 1 2 l,j (cid:17)q(cid:21) (cid:20) exp (cid:16) (cid:20) exp (cid:16) 2π to obtain q(sbt + Hl,jyj)2 2(1 2 l,j) q(sbt + Hl,jyj)2 2(1 2 l,j) (cid:17) (cid:21) 1(sbt + Hl,jyj 0) + P(sbt + Hl,jyj > 0) (cid:17)(cid:21) (cid:16) + Φ (cid:17) sbt Hl,j (cid:118) (cid:117) (cid:117) (cid:116) = 1 2 l,j 1 + (q 1)H 2 l,j (cid:16) exp sb2 l,j + 1 2 ) 2( q1 (cid:17) (cid:16) + Φ sbt Hl,j (cid:17) , (G.17) where the third equality holds by direct algebraic calculation for Gaussian integral. By the Mills ratio Φ(x)/p(x) x1 x3 = Cx1 for 1, and also the fact that Hl,j [0, 1], we conclude that the right-hand side of (G.17) is bounded by sbt + Hl,jyj (cid:113) 1 2 l,j CsbtΦ (cid:113) q1 (cid:19) . (cid:17)q(cid:21) (G.18) l,j + 1 (cid:18) Φ (cid:16) (cid:20) Similar to the previous argument, we also have Φ (cid:16) as non-decreasing function of sbt 2 (cid:17) sbt (cid:113) q1 x2+ 1 for [0, 1] by checking the derivative. We define ℏq, as the smallest real number such that the following inequality holds: (cid:88) j=1 1 Dj (cid:88) lDj (cid:18) Φ sbt 2 (cid:113) q1 l,j + 1 (cid:19) Φ (cid:18) sbt ℏ2 q, + 1 (cid:113) q1 (cid:19) . (G.19) Plugging (G.18) and (G.19) into (G.16), we have that n1 (cid:88) j=1 1 Dj (cid:88) (cid:16) Φ lDj sbt + Hl,jyj (cid:113) 1 2 l,j (cid:32) (cid:32) nsbt Φ (cid:17)q 1(3 ι=0Eι) sbt ℏ2 q, + 1 (cid:113) q1 (cid:33) (cid:16) + Φ (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t (cid:33) . log(n) (G.20) Combining (G.15) and (G.20), we obtain (III) Cρq 1 (cid:32) (cid:32) sbt Φ sbt ℏ2 q, + 1 (cid:113) q1 (cid:33) (cid:16) + Φ (cid:17)q sbt + ℏq,tζt (cid:113) 1 ℏ2 q,t log(n) + n(cid:0)ρ2 log(n)(cid:1)q (cid:33) . 99 Note that we always have ℏq, 1 and ℏq,t 1 for 1. As both Φ (when ζt > sbt) are non-decreasing functions with respect to x, for the first term in the right-hand side of (G.14), we take = 3 and ℏq,t = 1 to have the following upper bound: and Φ (cid:113) q1 (cid:16) (cid:17) sbt x2+ 1 (cid:16) (cid:17)q sbt+xζt 1x 2B4 ρ1ρ2s 3 1 (cid:20)n1 (cid:88) (cid:16) N1(cid:88) j= l=1 (cid:18) CB4 ρ4 1ρ2s 1(e + sbt > 0) 1(El,j = 0) (cid:17)3 1(3 ι=0Eι) (cid:21) (cid:12) (cid:12) (cid:12) (cid:18) nsbtΦ (cid:19) sbt ℏ2 3, + 1 3 (cid:113) 2 3 + log(n) + n(ρ2st log(n))3 (cid:19) . (G.21) For the second term on the right-hand side of (G.14), we take = 4 and obtain (cid:20)n1 (cid:88) (cid:16) N1(cid:88) 1(e + sbt > 0) 1(El,j = 0) (cid:17)4 1(3 ι=0Eι) (cid:21) (cid:12) (cid:12) (cid:12) 2B4 4 1 j=1 l=1 (cid:32) (cid:32) sbt Φ CB4 ρ4 1 sbt ℏ2 4, + 1 4 (cid:113) 3 4 (cid:33) (cid:16) + Φ (cid:17)4 sbt + ℏ4,tζt (cid:113) 1 ℏ2 4,t log(n) + n(cid:0)ρ2 log(n)(cid:1)4 (cid:33) . (G.22) We conclude by combining (G.21) and (G.22) that (I) CB4 ρ4 1 (cid:32) (cid:18) sbt Φ (cid:113) 3 4 (cid:18) (cid:19) sbt ℏ2 4, + 1 4 + ρ2snsbtΦ (cid:18) (cid:19) sbt ℏ2 3, + 1 (cid:113) 2 3 + Φ (cid:16) sbt + ℏ4,tζt (cid:113) 1 ℏ2 4,t (cid:17)4 (cid:19) + ρ2s log(n) + n(cid:0)ρ2 log(n)(cid:1)4 (cid:33) =: V0. Similarly, (II) can be bounded by V0. We are now ready to invoke Lemma I.9. Since 2V0 with probability 1, the final bound for E[Z] is then given by E[Z] (cid:112) V0 log(δ1), where the inequality holds with probability at least 1 δ over the randomness of standard Gaussian vectors z1:T . Plugging in the formula for V0, we obtain the following upper bound E[Z] CB ρ2 1 (cid:32) (cid:32) sbt Φ (cid:113) 3 4 (cid:18) (cid:16) Φ + (cid:33) sbt ℏ2 4, + 1 4 sbt + ℏ4,tζt (cid:113) 1 ℏ2 4,t + ρ2snsbtΦ (cid:18) (cid:19) sbt ℏ2 3, + 1 3 (cid:113) 2 3 (cid:17)4 (cid:19) log(n) + n(cid:0)ρ2 log(n)(cid:1) + ρ2s (cid:33)1/2 log δ1 with probability 1δ. For notational convenience, we define Kt as the 1/4 power of each term inside the bracket in the above equation (see (E.9) for the definition). The fluctuation of is controlled by E[Z] CL2ρ2 1ts log t log δ1, where we plug in the definition Bt = the activation function φ. ts log and = γ2 + btγ1 is the Lipschitz constant for 100 Expectation E[Z]. For E[Eφ(Ey ; bt)2 2], we have 1 2 1 E(cid:2)Eφ(Ey ; bt)2 2 (cid:3) 1 2 1 (cid:88) E(cid:2)(cid:12) (cid:12)φ(rh y; bt) φ(rh (cid:3) rhl, rhl y; bt)(cid:12) (cid:12) l,l=1 (cid:104) E(cid:2)(cid:12) 1 pEl,l C2 (cid:12)φ(rh y; bt) φ(rh (cid:105) (cid:3) rhl, rhl y; bt)(cid:12) (cid:12) , where in the first inequality, we obtain the upper bound by also adding the rows of that are not contained in the submatrix to the sum. Here, we use the notation rhl = (Hl,1, . . . , Hl,i1, Hl,i+1, . . . , Hl,n1) to denote the l-th row of with the i-th entry removed. This structure comes from the definition (D.3) where we decompose the matrix into submatrices E, and the column vector θ as the non-zero entries in H:,i if the feature of interest is the i-th feature. In the second inequality, we use pEl,l as the empirical expectation over l, [N ]2. Invoking the fact that N/N1 C1, and define Lemma E.5 with = γ2 + btγ1, sb = sbt = bt + κ0, we conclude that pEl,l (cid:104) E(cid:2)(cid:12) (cid:12)φ(rh y; bt) φ(rh (cid:105) (cid:3) rhl, rhl y; bt)(cid:12) (cid:12) CL (n d)c0 + CL2 Φ(sbt) pEl,l CL (n d)c0 + CL2 Φ(sbt) pEl,l (cid:34) Φ (cid:34) Φ (cid:16) (cid:16) (cid:115) sbt (cid:115) sbt 1 rhl, rhl 1 + rhl, rhl (cid:17) (cid:35) rhl, rhl 1 hl, hl 1 + hl, hl (cid:17) (cid:35) hl, hl , where in the first inequality, we directly apply Lemma E.5 to the expectation term, and in the second inequality, we use the fact that rhl, rhl hl, hl for l, [N1] and the fact that the term inside the expectation is non-decreasing when increasing the value of rhl, rhl. Just as before, since c0 > 4 is large enough, the first term is negligible, and we can absorb it into the constant and focus on the second term:"
        },
        {
            "title": "1\nN 2\n1",
            "content": "E(cid:2)Eφ(Ey ; bt)2 2 (cid:3) CL2 Φ(sbt) pEl,l (cid:115) (cid:34) Φ (cid:16) sbt 1 hl, hl 1 + hl, hl (cid:17) (cid:35) hl, hl . Since Eφ(Ey the indicator condition 1(3 ; bt)2 ι=0Eι). 2 is non-negative, the same upper bound applies to E[Z], where includes Finally, we plug in δ = nc to conclude that with probability at least 1 nc it holds that"
        },
        {
            "title": "1\nN 2\n1",
            "content": "Eφ(Ey ; bt)2 2 1(E0) 1(3 ι=0Eι) CL2 ρ2 1st2(log n)2 K2 + CL2 Φ(sbt) pEl,l (cid:34) (cid:16) Φ (cid:115) sbt 1 hl, hl 1 + hl, hl (cid:17) (cid:35) hl, hl . Note that the joint event 1(3 Therefore, we can safely drop the indicator 1(3 proof of Lemma E.4. ι=1Eι) holds with probability at least 1 nc as we discussed earlier. ι=1Eι) in the above inequality. This completes the 101 G.2.4 Concentration for φ(F yt + θ swt1; bt)2 2: Proof of Lemma E.6 In the following proof, we will use to denote universal constants that change from line to line. Let us fix {ατ,t1}t1 τ =1 and bt. Then by in the following. Let us define the good event (0, In1). For simplicity, we will denote E = (cid:8) max τ =1,0,...,t1 zτ (1 + c)(cid:112)2 log(nt)(cid:9). It then follows from Lemma I.2 that P(E) (nt)c nc, and also (1 + E. In particular, c)(cid:112)2t log(nt) on φ(f + θlv swt1; bt) 1(E) (γ2 + btγ1)((1 + c)(cid:112)2t log(nt) + θlv2α1,t1) + (n d)c0 := Bt, where the inequality holds by the Lipschitz continuity of φ in Definition A.2 and also the fact that bt + κ0 0 for the bias. Define ="
        },
        {
            "title": "1\nN 2\n2",
            "content": "N2(cid:88) fl, fl φ(cid:0)f + θlv swt1; bt (cid:1) φ(cid:0)f + θlv swt1; bt (cid:1) 1(E). l,l=1 Using the Cauchy-Schwarz inequality, we have"
        },
        {
            "title": "1\nN 2\n2",
            "content": "2ρ2 N2 N2(cid:88) (cid:0)φ(cid:0)f + θlv swt1; bt (cid:1)2 + φ(cid:0)f + θlv swt1; bt (cid:1)2(cid:1) 1(fl, fl = 0) 1(E) l,l=1 N2(cid:88) φ(cid:0)f + θlv swt1; bt l=1 (cid:1)2 1(E), (G.23) where the first inequality follows from ab a2 + b2, and the second inequality follows from the fact that fl, fl2 is nonzero for at most N2ρ2 terms when going over by definition (E.1). Next, we concentrate the right-hand side of (G.23). Note that by the Lipschitz continuity of φ, we have φ(f + θlv swt1; bt) (γ2 + btγ1)(f + θlv2α1,t1) + (n d)c0. By the Cauchy-Schwarz inequality, we further obtain + θlv swt1; bt)2 C(γ2 + btγ1)2(cid:0)(f φ(f y)2 + (θlv2α1,t1)2(cid:1) + C(n d)2c0. (G.24) To this end, we apply the Cauchy-Schwarz inequality again to obtain that"
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) (f y)2 l="
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) (cid:16)n1 (cid:88) l=1 j=1 (cid:17) y(j)2 1(fl(j) = 0) fl 2 ρ2 y2 2. Under the good event E, we have y2 (1 + n1, and we can apply the concentration inequality for the chi-squared distribution in Lemma I.1 to obtain that with probability at least 1 δ, it holds over the randomness of that c)(cid:112)2t log(nt). In fact, y2 2 χ"
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) l=1 (f y)2 1(E)"
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) (f y)2 Cρ2 (cid:0)n + log δ1(cid:1). l=1 102 Applying union bound over {ατ,t1}t1 bounded, we conclude that with probability at least 1 nc, it holds for all nc that τ =1 and bt similar to Lemma G.4, and since is uniformly 1 N2 N2(cid:88) l=1 (f y)2 1(E) Cρ2 (cid:0)n + log(n)(cid:1). (G.25) Combining (G.23), (G.24), and (G.25), we conclude that with probability at least 1 nc, it holds for all nc that C(γ2 + btγ1)2ρ2 (cid:16) 1 2 θ 2v2 2α2 1,t1 + ρ2n + ρ2t log (cid:17) . As the good event holds with sufficiently high probability if we choose large enough in the definition of E, similar bound holds for the original quantity φ(F yt + θ swt1; bt)2 2. This completes the proof of Lemma E.6. G.2.5 Concentration for zτ , Eφ(Ey ; bt): Proof of Lemma E.7 In the following proof, we will use to denote universal constants that change from line to line. When treating {ατ,t1}t1 τ,t1 z, where (0, In1) and is independent of , and α to replace ατ,t1 for notational simplicity. Therefore, the concentration we consider can be reduced to the concentration of . In the following, we use to replace τ =1 and bt to be deterministic, we have zτ d= ατ,t1y 1 α2 + (cid:113) α"
        },
        {
            "title": "1\nN1",
            "content": "y, Eφ(Ey; bt) + (cid:112) 1 α"
        },
        {
            "title": "1\nN1",
            "content": "z, Eφ(Ey; bt), Firstly, note that when conditioned on y, z, Eφ(Ey; bt) is gaussian random variable with mean zero and variance Eφ(Ey; bt)2 2, it holds with probability at least 1 δ over the randomness of that"
        },
        {
            "title": "1\nN1",
            "content": "z, Eφ(Ey; bt)"
        },
        {
            "title": "1\nN1",
            "content": "(cid:113) 2Eφ(Ey; bt)2 2 log δ1, where the second order term has already been handled in Lemma E.4. Similar to the proof of Lemma G.4, we can use covering argument over {ατ,t1}t1 τ =1 St+1, bt R, τ = 1, 0, . . . , 1 and nc to obtain that with probability at least 1 nc, it holds for all (τ, t) that"
        },
        {
            "title": "1\nN1",
            "content": "z, Eφ(Ey; bt) N1 (cid:113) Eφ(Ey; bt)2 2 log(n). Now it remains to control the first term. Define good event = (cid:8)y (1 + c)(cid:112)2t log(nt)(cid:9). In fact, the above good event can be directly implied by the following good event: (cid:110) = max τ =1,0,...,t1 zτ (1 + c)(cid:112)2 log(nt) (cid:111) . For notational simplicity, we will just focus on the latter definition of the good event. It follows from Lemma I.2 that P(E) 1 (tn)c 1 nc. Let us define ="
        },
        {
            "title": "1\nN1",
            "content": "y, Eφ(Ey; bt) 1(E), (cid:20)n1 (cid:88) (Z Z(i))2 (cid:12) (cid:12) (cid:12) (cid:21) , and := i= 103 where Z(i) = y(i), Eφ(Ey(i); bt) 1(E (i)) and y(i) is given by replacing the i-th coordinate yi with an independent copy (0, 1). Note that this is equivalent to replacing the i-th coordinate of each zτ with an independent copy z(i) τ . Thus, the good event can be also changed to (i) accordingly. Next, we show how to control the variance . Let us define Zl = y φ(e y; bt) 1(E) and Z(i) = y(i) φ(e y(i); bt) 1(E (i)) for any [N1]. On the joint event (1) . . . (n1), we have by the Lipschitzness of φ in Definition A.2 that Zl C(γ2 + btγ1)t log(nt) =: Bt, [N1]. (G.26) This bounds also holds for all Z(i) E1 . . . En1 that for [n 1]. By reformulation, we obtain for the joint event (Z Z(i))2 = 1 2 1 ρ1 N1 2ρ1 l=1 B2 (cid:18) N1(cid:88) (Zl Z(i) ) l=1 (cid:19) = 1 2 1 (cid:18) N1(cid:88) l=1 (Zl Z(i) ) 1(El,i = 0) (cid:19)2 N1(cid:88) (Zl Z(i) )2 1(El,i = 0) 2ρ1 N1(cid:88) l=1 (cid:0)Z2 + (Z(i) )2(cid:1) 1(El,i = 0) N1(cid:88) (cid:0)1(e + sbt > 0) + 1(e y(i) + sbt > 0) + 2B1 (n d)c0(cid:1) 1(El,i = 0), l=1 where the first inequality holds by the Cauchy-Schwarz inequality, the second one holds by (a b)2 2(a2 + b2), and the last line holds by Definition A.2 and the upper bound in (G.26). Since c0 is some sufficiently large constant, we can safely ignore the term involving (n d)c0 in the sequel (when invoking constant factor C). Taking summation over = 1, . . . , 1 on both sides and taking the conditional expectation, we obtain that Cρ1 N1 B2 n1 (cid:88) N1(cid:88) i=1 l=1 Let us define (cid:0)1(e + sbt > 0) + E(cid:2)1(e y(i) + sbt > 0) y(cid:3)(cid:1) 1(El,i = 0). g(y) = 2ρ1B2 N1 n1 (cid:88) N1(cid:88) i=1 l=1 1(e + sbt > 0) 1(El,i = 0). Therefore, the moment generating function of is controlled by E[exp(λV )] (cid:104) exp(λg(y)) exp(cid:0)λE[g(y(i)) y](cid:1)(cid:105) (cid:104) exp(λg(y)) exp(cid:0)λg(y(i))(cid:1)(cid:105) for λ > 0. Here, the last inequality follows from the Jensens inequality. To this end, we notice that is non-decreasing functions of y. Then by Lemma I.10, we have that E[exp(λg(y))exp(λg(y(i)))] E[exp(2λg(y))]. Therefore, we just need to focus on the moment generating function of g(y). Note that since el is s-sparse, with probability at least 1 δ over the randomness of y, we have g(y) 2sρ1B2 N1 N1(cid:88) l=1 1(e + sbt > 0) Csρ1B2 (cid:0)Φ(sbt) + ρ1s log δ1(cid:1). 104 where in the last inequality, we invoke Lemma G.4. This can be transformed into the following tail bound E(cid:2)exp(λV )(cid:3) E(cid:2)exp(2λg(y))(cid:3), where P(cid:0)g(y) > Csρ1B2 Φ(sbt) + v(cid:1) exp (cid:16) 1s2B2 Cρ2 (cid:17) , and any > 0. In particular, for V+ and defined in (I.2), we always have 0 V+ and 0 . With the sub-exponential tail bound, we now invoke Condition 1 of Lemma I.8 to conclude that with probability at least 1 δ over the randomness of y, E[Z] CBt (cid:0)(cid:113) sρ1Φ(sbt) log δ1 + ρ1s log δ1(cid:1). (G.27) τ =1 and {zτ }t1 Since is Lipschitz over {ατ,t1}t1 τ =1, we follow similar covering argument over the balls {St1}T t=1 with nc. Note that the failure probability of the joint event (1) . . . (n1) is at most n1c. In addition, we can set δ = nc(ncεnc) in (G.27), where ϵ is the approximation error in the covering argument in the infinity norm. By union bound of the covering net of size ncεnc , we will obtain failure probability at most nc as well. By decreasing the constant slightly (up to 2), we can combine the two failure probabilities to obtain that for all nc, it holds with probability at least 1 nc that E[Z] CBt (cid:0)(cid:113) sρ1Φ(sbt) log(n) + sρ1 log(n)(cid:1). Next, let us evaluate the expectation E[Z]. By definition, (cid:12) E[Z] (cid:12) (cid:12)"
        },
        {
            "title": "1\nN1",
            "content": "E(cid:2)y, Eφ(Ey; bt)(cid:3)(cid:12) (cid:12) (cid:12) ="
        },
        {
            "title": "1\nN1\n1\nN1",
            "content": "E(cid:2)y, Eφ(Ey; bt) 1(E)(cid:3) (cid:113) E(cid:2)y, Eφ(Ey; bt)2(cid:3) P(E). Since P(E) nc, while E(cid:2)y, Eφ(Ey; bt)2(cid:3) is at most C(sb2 + (γ1 + sbtγ2)2) for some universal constant by the Lipschitzness of φ given by Definition A.2. We can pick in the definition of to be sufficiently large, Thereby, the approximation error in the expectation is negligible. We thus just need to evaluate"
        },
        {
            "title": "1\nN1",
            "content": "E(cid:2)y, Eφ(Ey; bt)(cid:3) ="
        },
        {
            "title": "1\nN1",
            "content": "N1(cid:88) l=1 E(cid:2)e φ(e y; bt)(cid:3) = ExN (0,1)[xφ(x; bt)] =: pφ1(bt). Hence, we conclude that for all τ 1 and nc, it holds with probability at least 1 nc that (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN1",
            "content": "(cid:12) zτ , Eφ(Eyt; bt) ατ,t1 pφ1(bt) (cid:12) (cid:12) ατ,t1 CBt (cid:113) (cid:0)(cid:113) sρ1Φ(sbt) log(n) + sρ1 log(n)(cid:1) + 1 α2 τ,t1 (cid:113) 2Eφ(Ey; bt)2 2 log(n). N1 Plugging in the definition of Bt = C(γ2 + btγ1)t log(nt), we complete the proof of Lemma E.7. G.2.6 Concentration for zτ , φ(F yt + θ swt1; bt): Proof of Lemma E. In this proof, we will show the concentration for the term 1 Similar to the proof of Lemma E.7, when fixing {ατ,t1}t1 2 zτ , φ(F yt + θ swt1; bt). (0, In1). τ =1 and {bt,l}l=1, we have 105 For simplicity, we will denote τ,t1 where (0, In1) is independent of y. In the sequel, we also simplify ατ,t1 to α. Therefore, the concentration we consider can be reduced to by in the following. Note that zτ 1 α2 d= ατ,t1y + (cid:113) α 1 N2 y, φ(F + θ swt1; bt) + (cid:112) 1 α2 1 N2 z, φ(F + θ swt1; bt). The concentration for the second part follows directly from the Gaussian tail bound. That said, with probability at least 1 δ, it holds that 1 N2 (cid:12)z, φ(F + θ swt1; bt)(cid:12) (cid:12) 2F φ(F + θ swt1; bt)2 2 log δ1, 1 N2 (cid:12) (cid:113) where the right-hand side can be controlled by Lemma E.6. Then by covering argument over {ατ,t1}t1 τ =1 and bt similar to Lemma G.4 (with proper truncation of the random variables that yields sufficiently small error probability), we conclude that with probability at least 1 nc, it holds for all = 1, . . . , and τ = 1, 0, . . . , 1 that"
        },
        {
            "title": "1\nN2",
            "content": "(cid:12)z, φ(F + θ swt1; bt)(cid:12) (cid:12) (cid:12) (cid:113) N2 φ(F + θ swt1; bt) 2 log(n). To control the first term, define good event = (cid:8) max zτ (1 + c)(cid:112)2 log(nt)(cid:9). τ =1,0,...,t1 c)(cid:112)2t log(nt) and this good event holds with probability at On this good event, (1 + least 1 (tn)c 1 nc. We define ="
        },
        {
            "title": "1\nN2",
            "content": "y, φ(F + θ swt1; bt) 1(E), and = (cid:20)n1 (cid:88) (Z Z(i))2 (cid:12) (cid:12) (cid:12) (cid:21) , i=1 2 y(i), φ(F y(i) + θ swt1; bt) 1(E (i)). Here, we define y(i) = (cid:80)t1 j=1 αj,t1z(i) τ given by replacing the i-th coordinate of zj with an independent copy, and (i) is the event where Z(i) = 1 with z(i) defined with respect to z(i) τ . Let us define + θlv swt1; bt) 1(E), Z(i) Zl = φ(f = where fl is the l-th row of . On the joint event (1) (n1), we have by the Lipschitz continuity of φ in Definition A.2 that y(i) + θlv swt1; bt) 1(E (i)), y(i) φ(f Zl C(cid:0)(γ2 + btγ1) ((cid:112)t log(n) + v2α1,t1) + (n d)c0(cid:1) (cid:112)t log(n) := Bt, where we also use the fact that bt + κ0 0 for the bias. Note that the (n d)c0 term is negligible rbt,l = bt + κ0 + θlv swt1. This bound when c0 is sufficiently large. For notation simplicity, we define also holds for Z(i) . On the joint event (1) (n1), we have (cid:18) N2(cid:88) N2(cid:88) (cid:19)2 (Zl Z(i) )2 1(Fl,i = 0) (Zl Z(i) )"
        },
        {
            "title": "1\nN 2\n2",
            "content": "l=1 l=1 N2(cid:88) (Zl Z(i) )2 1(Fl,i = 0) 2ρ2 N2 N2(cid:88) l=1 (cid:0)Z2 + (Z(i) )2(cid:1) 1(Fl,i = 0) (Z Z(i))2 l"
        },
        {
            "title": "1\nN 2\n2",
            "content": "ρ2 N2 l=1 2ρ2B2 N2 N2(cid:88) (cid:0)1(f + rbt,l > 0) + 1(f y(i) + rbt,l > 0) + 2B (n d)c0(cid:1) 1(Fl,i = 0), l=1 106 where the first inequality holds by the Cauchy-Schwarz inequality, the second one holds by (a b)2 2(a2 + b2), and the last line holds by Definition A.2 and the upper bound for Zl and Z(i) (n d)c0 term by multiplying some universal constant. Taking summation over = 1, . . . , 1 on both sides with the conditional expectation, we obtain . We can also ignore the 2B Cρ2B2 N"
        },
        {
            "title": "Let us take",
            "content": "n1 (cid:88) N2(cid:88) (cid:0)1(f + rbt,l > 0) + E(cid:2)1(f y(i) + rbt,l > 0)(cid:3)(cid:1) 1(Fl,i = 0). i= l=1 g(y) := Cρ2B2 N2 n1 (cid:88) N2(cid:88) i= l=1 1(f + rbt,l > 0) 1(Fl,i = 0) = Cρ2sB2 N2 N2(cid:88) l=1 1(f + rbt,l > 0) Cρ2sB2 . Then we have by the monotonicity of and Lemma I.10 that E[exp(λV )] E[exp(2λg(y))] for all λ > 0. Invoking Lemma I.9 for this bounded variance, we obtain that with probability at least 1 δ over the randomness of y, it holds that E[Z] CBt ρ2s log δ1. By covering argument over {ατ,t1}t1 E[Z] CBt In addition, the approximation error τ =1 and bt similar to Lemma G.4, we conclude that ρ2st log(n) with probability at least 1nc for all = 1, . . . , and τ = 1, 0, . . . , t1. (cid:12) E[Z] (cid:12) (cid:12)"
        },
        {
            "title": "1\nN2",
            "content": "E[y, φ(F + θ swt1; bt)] (cid:113) (cid:12) (cid:12) (cid:12) P(E) + θlv swt1; bt) has bounded second by the Cauchy-Schwarz inequality and the fact that moment. Therefore, by taking sufficiently large in the definition of the good event E, we can make this approximation error negligible. Moreover, we also have yφ(f E[f yφ(f + θlv swt1; bt)] = (cid:113) 1 θ2 ExN (0,1) xφ(cid:0)(cid:113) (cid:104) 1 θ2 + θlv swt1; bt (cid:1)(cid:105) . Combining everything, we conclude that with probability at least 1nc, it holds for all = 1, . . . , and τ = 1, 0, . . . , 1 that"
        },
        {
            "title": "1\nN2",
            "content": "(cid:12) (cid:12)zτ , φ(F (cid:12) + θ swt1; bt) N2(cid:88) l=1 ατ,t1 (cid:113) 1 θ2 ExN (0,1) xφ(cid:0)(cid:113) (cid:104) 1 θ2 + θlv swt1; bt (cid:1)(cid:105)(cid:12) (cid:12) (cid:12) (cid:113) N2 (cid:113) τ,t1 φ(F + θ swt1; bt)2 1 α2 + Cατ,t1 (γ2 + btγ1) ((cid:112)t log(n) + v2α1,t1) 2 log(n) ρ2s (t log(n))3/2 This completes the proof of Lemma E.9. 107 G.2.7 Concentration for θφ(F t + θv swt1; bt): Proof of Lemma E.10 In the following, we will use to denote universal constants that change from line to line. Let fl denote the l-th row of . Let us first fix {ατ,t1}t1 (0, In1). In the sequel, we will simplify τ =1 and bt. Then . Let us define the good event = (cid:8) max τ =1,0,...,t zτ (1 + c)(cid:112)2 log(nt)(cid:9). It then follows from Lemma I.2 that P(E) (nt)c nc, and also (1 + E. In particular, c)(cid:112)2t log(nt) on φ(f + θlv swt1; bt) 1(E) (γ2 + btγ1)((1 + c)(cid:112)2t log(nt) + v2α1,t1) + (n d)c0 := Bt. where the last inequality holds by noting that φ(; bt) is γ2 + btγ1-Lipschitz by Definition A.2, and also the fact that sbt = bt + κ0 0. The target function to study is ="
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) l=1 θlφ(f y; rbt,l) 1(E), where rbt,l = bt,l + θlv2α1,t1. Let y(i) be the vector obtained by replacing the i-th element of yt with an independent standard t(i). The good event (i) is defined similarly. Define Z(i) as the Gaussian random variable correspondence of with y(i) and (i). Let us define variance = E[(cid:80)n1 i=1 (Z Z(i))2]. Notice that this upper bounds both V+ = E[(cid:80)n1 i=1 (ZZ(i))2 1(Z < Z(i))]. Note that when changing one coordinate in y, the total number of terms affected in is at most N2ρ2 by definition (E.1). It then holds by the Cauchy-Schwarz inequality that i=1 (ZZ(i))2 1(Z > Z(i))] and = E[(cid:80)n1 n1 (cid:88) N2(cid:88) E(cid:2)(cid:0)φ(f θ2 y; rbt,l) 1(E) φ(f y(i); rbt,l) 1(E)(cid:1)2 y(cid:3) Cρ2 N2 ρ2 CB2 N2 i= l=1 n1 (cid:88) N2(cid:88) i=1 l=1 θ2 1(fl(i) = 0), where in the second inequality, the indicator is included since the term will be zero if fl(i) = 0. Additionally, we invoke the bound Bt to upper bound the φ() term. Let us define g(y) := ρ2 CB2 N2 n1 (cid:88) N2(cid:88) i=1 l=1 θ2 1(fl(i) = 0) CB2 ρ2s N2 θ2 2. By Lemma I.10, we know that the MGF of can be upper bounded by E[exp(λV )] E(cid:2)exp(2λg(y))(cid:3). Thanks to the bounded variance, invoking Lemma I.9, we conclude that with probability at least 1 δ over the randomness of y, it holds that (cid:12)Z E[Z](cid:12) (cid:12) (cid:12) CBtθ2 (cid:114) ρ2s N2 log(δ1). Next, we invoke union covering argument over the ball St+1 for ατ,t1 and also for bt. Since is Lipschitz and bounded, the approximation error can be made sufficiently small. Therefore, we conclude that with probability at least 1 nc, it holds for all nc that (cid:12)Z E[Z](cid:12) (cid:12) (cid:12) CBtθ2 (cid:114) ρ2s N2 log(n). 108 Similar to previous proof, the error in E[Z] and 1 E[θφ(F yt + θ swt1; bt)] can be made sufficiently small if we choose large in the definition of the good event E. Consequently we just need to plug in the expectatin 1 N2 E[θφ(F yt + θ swt1; bt)] = 1 N2 N2(cid:88) l=1 ExN (0,1) (cid:2)θl φ( This completes the proof of Lemma E.10. G.3 Propogation of the Non-Gaussian Error (cid:113) 1 θ2 + θl swt1; bt)(cid:3). In this subsection, we analyze how to Non-Gaussian error yt propagates through the nonlinear activation. G.3.1 Error Analysis for Et: Proof of Lemma E.12 In the following proof, we will use to denote universal constants that change from line to line. Bounding Et1. By definition of Et, we have Et1 = Eφ(E(y + yt); bt) Eφ(Ey ; bt)1 φ(E(y + yt); bt) φ(Ey ; bt)1 s(γ2 + btγ1) N1(cid:88) l=1 yt 1(e yt + sbt > 0 y + sbt > 0) + N1(cid:88) l=1 2(2 + bt) (n d)c0 1(e yt + sbt 0 y + sbt 0). yt + sbt > 0 sbt = bt + κ0 is the shifted bias. The first inequality follows from the fact that el1 as where each row el is s-sparse. The second inequality holds by splitting the summation into two parts. + sbt > 0} where the neuron is activated, we have the For the first part {l : term bounded by the Lipschitz continuity of φ times the pre-activation difference yt. Here, we recall from Definition A.2 that φ is (γ2 + btγ1)-Lipschitz continuous. For the second part + sbt 0} where the neuron is inactive, we simply apply the upper bound {l : on φ in Definition A.2 as (2 + bt) (n d)c0. Note that c0 can be chosen to be sufficiently large constant. Thus, we just need to focus on the first part. Using the Cauchy-Schwarz inequality twice, we have yt + sbt 0 y N1(cid:88) l=1 yt 1(e yt + sbt > 0) N1(cid:88) el2 yt 1(el = 0)2 1(e yt + sbt > 0) l=1 (cid:118) (cid:117) (cid:117) (cid:116) N1(cid:88) l=1 1(e yt + sbt > 0) N1(cid:88) yt 1(el = 0)2 2, (G.28) l= where is the Hadamard product between two vectors and y. Note that the second term on the right hand side can be further bounded by N1(cid:88) yt 1(el = 0)2 2 = l=1 N1(cid:88) n1 (cid:88) l=1 i=1 y2 t,i 1(El,i = 0) ρ1N1 yt2 2. (G.29) 109 Plugging (G.29) back into (G.28), and invoking Lemma E.3, we conclude that with probability at least 1 nc for all nc, yt 1(e yt + sbt > 0) CN1 (cid:113)(cid:0)Φ(sbt) + ρ1st log(n) + ρ1sbt2yt 2 (cid:1)ρ1yt2 2 N1(cid:88) l=1 CN1 (cid:16)(cid:0)(cid:113) ρ1Φ(sbt) + ρ1 (cid:112)st log n(cid:1) yt2 + ρ1sbt yt2 2 (cid:17) . + sbt > 0) has an upper bound in Corollary E.2 even Note that the ideal activation (cid:80)N1 l=1 tighter than the one we use above. Therefore, we just need to double the above error term. Thereby, we conclude that 1(e Et1 CN1(γ2 + btγ1) (cid:16)(cid:0)(cid:113) sρ1Φ(sbt) + sρ (cid:112)t log n(cid:1) yt2 + sρ1sbt yt2 2 (cid:17) + CN1 s(2 + bt) (n d)c0. Bounding Et2 vector RN1, 2. The proof is similar to bounding Et1. Again, we notice that for any test Ex2 2 = n1 (cid:88) (cid:16) N1(cid:88) i=1 l=1 (cid:17)2 El,ixl n1 (cid:88) (cid:16) N1(cid:88) i=1 l=1 (cid:17) 1(El,i = 0) (cid:16) N1(cid:88) l=1 (cid:17) E2 l,ix2 ρ1N1x2 2. Here, the first inequality holds by the Cauchy-Schwarz inequality while the second inequality holds by the sparsity assumption on the columns of and also the fact that (cid:80)n1 2 = 1. Thereby, it holds for Et2 l,i = el2 i=1 E2 2 that Et 2 ρ1N1φ(E(y + yt); bt) φ(Ey ; bt)2 2 ρ1N1(γ2 + btγ1)2 N1(cid:88) l= yt2 ρ1N1(γ2 + btγ1)2 N1(cid:88) l=1 el 2 yt 1(el = 0)2 2 (γ2 + btγ1)2 (ρ1N1)2yt2 2, where the second inequality holds by the Lipschitz continuity of φ and the third inequality follows from the Cauchy-Schwarz inequality. The last inequality holds by invoking (G.29). Hence, we complete the proof of Lemma E.12. G.3.2 Error Analysis for Ft: Proof of Lemma E.13 In the following proof, we will use to denote universal constants that change from line to line. Let fl be the l-th row of matrix . Note that Ft1 φ(F (y + yt) + θ swt1; bt) φ(F + θ swt1; bt)1 s(γ2 + btγ1) yt N2(cid:88) l=1 sN2(γ2 + btγ1) yt2, s(γ2 + btγ1) yt2 N2(cid:88) l=1 fl where the first inequality follows from the fact that fl1 by the Hölders inequality for s-sparse fl with fl2 1, the second inequality follows from the Lipschitzness of φ and the third 110 inequality follows from the Cauchy-Schwarz inequality. In the last inequality, we use the fact that fl2 1. Next, we turn to the bound for Ft2. For any test vector RN2, we have x2 2 = n1 (cid:88) (cid:16) N2(cid:88) i=1 l=1 (cid:17)2 Flixl n1 (cid:88) F:,i2 2 x2 2 ρ2N2x2 2, i= (G.30) where we recall that ρ2 = maxi[n1]F:,i0/N2. Since Ft = φF,t, we have Ft2 2. Next, we use the same Lipschitzness of φ to upper bound φF,t2 ρ2N2φF,t2 2 as 2 φF,t2 2 (γ2 + btγ1)2 (γ2 + btγ1)2 N2(cid:88) l=1 n1 (cid:88) yt2 (γ2 + btγ1)2 N2(cid:88) fl2 2 l=1 n1 (cid:88) i=1 y2 t,i 1(Fl,i = 0) N2(cid:88) y2 t,i 1(Fl,i = 0) N2ρ2(γ2 + btγ1)2 yt2 2, (G.31) i=1 l= where we use the Cauchy-Schwarz inequality in the second inequality, the fact that fl2 1 in the third inequality, and the definition of ρ2 in the last inequality. Combining (G.30) and (G.31), we conclude that Ft2 ρ2N2(γ2 + btγ1) yt2. This completes the proof of Lemma E.13. G.4 Proofs for Technical Lemmas G.4.1 Proof of Lemma E.5 We invoke the upper bound φ(x; b) (n d)c0 + L(x + sb) 1(x > sb) to obtain that E[φ(x; b)φ(ιx + (cid:112) 1 ι2 z; b)] (n d)2c0 + 2(n d)c0 E[(x + sb) 1(x > sb)] + L2 E[(x + sb) 1(x > sb) (ιx + (cid:112) 1 ι2z + sb) 1(ιx + (cid:124) (cid:123)(cid:122) (I) (cid:112) 1 ι2z > sb)] (cid:125) . Note that E[x 1(x > sb)] = p(sb) for any is the standard Gaussian density function. Therefore, we have sb by explicit calculation, where p(x) = exp(x2/2)/ 2π E[(x + sb) 1(x > sb)] = E[x 1(x > sb)] + sb P(x > sb) = p(sb) sbΦ(sb) = (sb), where we define (x) = p(x) xΦ(x). We note that the function (x) is monotonically decreasing for all R. To see this, we take the derivative of (x) and using the fact that p(x) = xp(x) and Φ(x) = p(x), which gives us (x) = Φ(x) xΦ(x) xp(x) = Φ(x) + xp(x) xp(x) = Φ(x) < 0. (G.32) In particular, function (x) is always positive for any as limx (x) = 0 by the Mills ratio limx xΦ(x)/p(x) = 1. Therefore, (sb) (0) = 1/2 and the first two terms involving (n d)c0 are negligible. For the last term, by marginalizing z, we have (cid:112) (cid:112) E[(x + sb) 1(x > sb) (ιx + 1 ι2z + sb) 1(ιx + 1 ι2z > sb)] sb + ιx 1 ι2 (cid:17) + (cid:16) (cid:16) sb + ιx 1 ι2 (cid:17)(cid:19)(cid:21) (cid:20) (cid:20) = = (x + sb) 1(x > sb) (x + sb) 1(x > sb) (cid:112) (cid:112) 1 ι2 (cid:18) ιx + sb 1 ι2 1 ι2 (cid:16) 111 Φ sb + ιx 1 ι2 (cid:17)(cid:21) . Since (x) is monotonically decreasing, we can upper bound the expectation by just plugging in = sb to obtain that (I) (cid:2)(x + sb) 1(x > sb)(cid:3) (cid:112) 1 ι2 (cid:16) sb (cid:114) 1 ι 1 + ι (cid:17) = (cid:112) 1 ι2 (sb) (cid:16) sb (cid:114) 1 ι 1 + ι (cid:17) . Next, we prove that (x) 2Φ(x) for all > 0. For any > 0, we have (x) = Φ(x) by (G.32), and Φ(x) = p(x). Therefore, (x) Φ(x) = Φ(x) p(x) Φ(0) p(0) (cid:114) π 2 = 2, where we use the fact that Φ(x)/p(x) is monotonically decreasing. Noting that limx (x) = 0 and limx Φ(x) = 0, we thus conclude that (x) 2Φ(x) for all > 0. Consequently, (cid:114) 1 ι 1 + ι 1 ι2 Φ(sb) Φ 1 ι2 (sb) (cid:114) 1 ι 1 + ι (I) 2 4 sb sb (cid:112) (cid:112) (cid:17) (cid:16) (cid:16) (cid:17) . Therefore, we conclude the proof of this proposition. G.4.2 Proof of Lemma E.8 Proof of Lemma E.8. Note that pφ1(bt) = ExN (0,1)[φ(x; bt)x] E[1(x + sbt > 0)(x + sbt)x] + E[x 1(x + sbt 0)] (d n)c0 (cid:16) sbt 2π exp(sb2 /2) + Φ(sbt) + sbt 2π exp(sb2 (cid:17) /2) + C(d n)c0. Here, the last inequality holds by the following integral calculation: (cid:90) sb xp(x)dx = p(sb), (cid:90) sb x2p(x)dx = sbp(sb) + Φ(sb) sbt < 0, the first and the last term for the standard normal distribution p(x) = exp(x2/2)/ cancel in the bracket, and we conclude that pφ1(bt) 2C0LΦ(sbt) as (d n)c0 can be sufficienlty small. On the other hand, using the condition φ(x; bt) xϕ(x + b) C0x(x + bt) for bt by Definition A.2, we have 2π. For pφ1(bt) C0E[1(x + bt > 0)(x + bt)x] + E[φ(x; bt)x 1(sbt bt)] (n d)c0E[x]. Here, we recall definition φ(x; b) = ϕ(x + b) + ϕ(x + b). Therefore, φ(x; b) ϕ(x + b) for > 0. By Definition A.2, we know that ϕ(x + b) 0 for all x. Since sbt > 0, we have for [sbt, bt] that φ(x; bt) ϕ(x + bt) (n d)c0, where the last inequality holds by the monotonicity of ϕ. Therefore, we conclude that pφ1(bt) C0E[1(x + bt > 0)(x + bt)x] (n d)c0 C0 2 Φ(bt). Since we can make κ0 = bt sbt log-polynomially small, e.g., κ0 = (log(n d))C, for sbt = Θ(log(n d)C), we have 2Φ(sbt) Φ(bt) Φ(sbt) . This completes the proof. 112 G.4.3 Proof of Lemma E.11 Proof of Lemma E.11. Lower bounding the signal term. Let us lower bound the signal term. Note that by the monotonicity assumption in Definition A.2, φ(x; bt) x>bt = ϕ(x + bt) + xϕ(x + bt) x>bt C0x. For (sbt, bt), we have φ(x; bt) φ(sbt; bt) (d n)c0. Together, we conclude that (cid:104) ExN (0,1) θl φ(cid:0)(cid:113) 1 θ2 + θl dα1,t1; bt (cid:1)(cid:105) N2(cid:88) l=1 N2(cid:88) l=1 (cid:16) θl 1 + θl ExN (0,1) dα1,t1 + bt (cid:113) 1 θ2 (cid:17) > 0 (cid:16)(cid:113) 1 θ2 + θl dα1,t1 (cid:17) N2(d n)c0 N2(cid:88) l= Φ (cid:16) bt θl (cid:113) 1 θ2 dα1,t1 (cid:17) C0θ2 dα1,t1 N2(d n)c0 1 o(1) 2 N2(cid:88) (cid:16) 1 l=1 θl > (cid:17) bt dα1,t C0θ2 dα1,t1, where in the second inequality, it follows from the direct calculation of the integral of the Gaussian that ExN (0,1)[1(x > a)x] = p(a) > 0 with p(a) being the density of (0, 1) at a. The (d n)c0 on the right-hand side is negligible. Note that the indicator is selecting the larger half of θl, and we can thereby obtain the following lower bound C1N2 dα1,t1 θ2Qt, where Qt ="
        },
        {
            "title": "1\nN2",
            "content": "N2(cid:88) (cid:16) 1 l=1 θl > bt dα1,t1 (cid:17) , θ2 = θ2 2 N2 . Upper bounding the signal term. To arrive at an upper bound, we use the fact that φ(x; bt) (d n)c0 1(x < sbt) + Lx 1(x sbt) to obtain that θl φ(cid:0)(cid:113) (cid:104) dα1,t1; bt ExN (0,1) 1 θ2 N2(cid:88) (cid:1)(cid:105) + θl l=1 N2(cid:88) l= (cid:16) θl 1 θl + ExN (0,1) + N2(d n)c0 dα1,t1 + bt (cid:113) 1 θ2 (cid:17) > 0 (cid:16)(cid:113) 1 θ2 + θl dα1,t1 (cid:17) CL N2(cid:88) l=1 (cid:0)θl (cid:113) 1 θ2 + θ2 dα1,t1 (cid:1) CLN2θ dα1,t1, where the last second inequality holds by noting that E[1(x > a)x] = p(a) 1, and the last one holds by noting that dα1,t1 1."
        },
        {
            "title": "H Proofs for SAE Dynamics Analysis",
            "content": "In this section, we provide supplementary proofs for the results used in the proof of the main theorem in F. 113 H.1 Proof of Proposition F.2 Let us first prove that there must exists some [n] such that θ2 (cid:80) 1/s. Since the total sum 2 = , and there are at most non-zero entries in the weight l,j = (cid:80)N j[n] (cid:80) l=1hl2 2 matrix H, we have the average lDj 2 := (cid:80)N l=1 On the other hand, we also have (cid:80)N (cid:80)n l=1 (cid:80)n j= j=1 2 l,j 1(Hl,j > 0) = 1 . 2 = (cid:80)n j=1 Dj θ2 (cid:80)n j=1 Dj max j[n] θ2 . It thus follows that there exists some [n] such that θ 1/s. Proof of the first inequality. By definition of h, we have h2 bound on h, we just need to show that ℏ2 in the definition of ℏq,: q, θ2 q, for = 4. To prove the upper for any [n]. Let us consider the kernel function ℏ2 (x) = Φ (cid:16) sb (cid:113) q1 + 1 (cid:17) . In particular, we aim to show that () is convex for [0, 1]. The second derivative of (x) is given by (x) = (cid:16) sb (cid:113) q1 + 1 (cid:17) (cid:1)2 sb(cid:0) q1 + 1 4(cid:0) q1 (cid:104) 3 (cid:16) 1 + 1 (cid:17) sb2(cid:105) . (cid:1)7/2 3, we conclude that (x) 0 for [0, 1], and is convex. Now, sb < Using the property that by definition of ℏq,, we have (ℏ2 q,) max j[n] 1 Dj (cid:88) lDj (H 2 l,j) max j[n] (cid:16) 1 (cid:88) Dj lDj (cid:17) 2 l,j = max j[n] (θ2 ), (H.1) where the second inequality follows from the convexity of (x) and Jensens inequality. Moreover, the first derivative of (x) is given by (x) = (cid:16) sb (cid:113) q1 + 1 (cid:17) (cid:1) sb(cid:0) q1 + 2(cid:0) q1 > 0. (cid:1)3/2 Therefore, we have by (H.1) that ℏ2 maxj[n] θ2 1/s. This proves the first inequality. q, θ2 for any [n] and = 4. Consequently, h2 ℏ2 4, 114 Proof of the second inequality. Since we have by definition of θ that θ2 = θi2 2 Di (1 pQi(hi)) h2 + pQ(hi) 1 pQi(hi) + h2 , it follows from the condition θ > pQi(hi) that (cid:113) hi pQ(s1/2 θ2 ). This completes the proof of the third inequality. Hence, we have completed the proof of Proposition F.2. H.2 Proofs for Concentration Results Combined In the following, we present the proofs of the lemmas and propositions used in F.2. H.2.1 Proof of Lemma F.5 From Φ(sbt) Lsρ1(t log n)3, we deduce that log n, since Lsρ1n3 1 Φ(sbt) (recalling that ρ1 n1). Hence, we can directly apply Corollary F.4 in what follows. Using the bound in Lemma E.7 together with Lemma E.8, if we further assume Φ(sbt) Lsρ1(t log n)3, then the desired concentration result is obtained as follows: zτ , Eφ(Ey ; bt) = (1 o(1)) ατ,t1 pφ1(bt) (cid:113) 1 α2 τ,t1 (cid:113) Eφ(Ey ; bt)2 2 log(n). Here, we use the fact that N1/N 1 ρ1 1, where ρ1 1 can also be deduced from the condition Φ(sbt) Lsρ1(t log(n))3. For the concentration result for zτ , φ(F yt + θ swt1; bt) in Lemma E.9, we use the Steins lemma to derive that (H.2) N2 N2(cid:88) l=1 (cid:113) ατ,t1 1 θ2 ExN (0,1) xφ(cid:0)(cid:113) (cid:104) 1 θ2 + θlv swt1; bt (cid:1)(cid:105) N2ατ,t1 l=1 N2(cid:88) (1 θ ) ExN (0,1) (cid:104) φ( (cid:113) 1 θ2 (cid:105) + θlv swt1; bt) ρ1ατ,t1L = o(Φ(sbt) ατ,t1) (H.3) where in the second inequality we use the Lipschitzness of φ and in the last inequality we use Lsρ1(t log n)3 Φ(sbt). Moreover, we have Lατ,t1 N2 ((cid:112)t log(n) + v2α1,t1) ρ2s(t log n)2 + ρ1 Lατ,t1 ρ1 o(Φ(sbt) ατ,t1) + ρ1 ρ2s (t log(n))3/2 ρ2s(t log n)3/2 dα1,t1ατ,t1 ρ2s(t log n)3/2 dα1,t1ατ,t1. (H.4) where in the first inequality, we use N2/N ρ1 by definition and in the second inequality, we ρ2s(t log n)2 ρ1(t log n)2 Φ(sbt) under the condition Lsρ1(t log n)3 Φ(sbt). use the fact ρ1 115 Moreover, by Lemma E.8, we know that pφ1(bt) = Ω(Φ(sbt)). Consequently, by combining (H.3) and (H.4) with the upper bound in Lemma E.9, we have (cid:12)zτ , φ(F yt + θ swt1; bt)(cid:12) (cid:12) (cid:12) (cid:113) (N ατ,t1 pφ1(bt)) + + CN Lρ1 1 α2 τ,t1 ρ2s(t log n)3/2 dατ,t1α1,t1. (cid:113) φ(F + θ swt1; bt)2 2 log(n) (H.5) Let us consider the good event with respect to some universal constant > 0: (cid:110) zτ C(cid:112)log(tn), : τ (cid:111) . As we increase the constant C, the failure probability of the event can be made polynomially small, e.g., 1 nc for some other constant > 0 (See Lemma I.2). Conditioned on the success of this event, we have for the non-Gaussian components that zτ , Et + zτ , Ft C(cid:112)log(tn) (cid:0)Et1 + Ft1 (cid:1) CLN (cid:112)log(n) (cid:0) sρ1( + CLN (cid:112)log(n) ρ CLN (cid:112)log (cid:0)(cid:113) (cid:113) Φ(sbt) + (cid:112)sρ1t log n) sdβt1 sρ1dΦ(sbt)βt1 + sρ1sbtdβ2 dβt1 + sρ1sbtdβ2 t1 (cid:1) (cid:1), (H.6) t1 where in the second inequality, we invoke Lemma E.12 and Lemma E.13 to bound the ℓ1 norm of the error terms, and also the fact that is at most polynomial in n. In the last inequality, we use the dβt1 by Lemma E.1. Now, we combine the derived concentration results in fact that yt2 (H.2), (H.5) and (H.6) with 1 α2 2 + φ(F yt + θ swt1; bt)2 τ,t1 1 and the upper bound for Eφ(Ey 2 in Corollary F.4 to obtain that ; bt)2 zτ , ut = zτ , Eφ(Ey ; bt) + zτ , φ(F yt + θ swt1; bt) + zτ , Et + zτ , Ft = ατ,t1 pφ1(bt) (1 o(1)) CN Lρ1 CN ρ1L(cid:112)t log ξt CLN (cid:112)log (cid:0)(cid:113) ρ2s(t log n)3/2 dατ,t1α1,t1 sρ1dΦ(sbt)βt1 + sρ1sbtdβ2 t1 (cid:1). Hence, we complete the proof of the Lemma F.5. H.2.2 Proof of Lemma F.6 Recall by definition of wt, v, wt/v2 can be decomposed into v, wt v2 = z1, ut + v2 θφ(F yt + θ swt1; bt) + η1α1,t1. (H.7) Taking τ = 1 in Lemma F.5, we have z1, ut = α1,t1 pφ1(bt) (1 o(1)) CN Lρ1 CN ρ1L(cid:112)t log ξt CLN (cid:112)log (cid:0)(cid:113) ρ2s(t log n)3/2 dα1,t12 sρ1dΦ(sbt)βt1 + sρ1sbtdβ t1 (cid:1). (H.8) Moreover, by direct decomposition of the second term, we have v2θφ(F yt + θ swt1; bt) = v2θφ(F = v2θφ(F + θ swt1; bt) + v2θφF,t + θ swt1; bt) v2θ2 φF,t2. 116 Notice that v2 = χ2 random variables (see Lemma I.1). By Lemma E.13, we have φF,t2 (1 C(cid:112)log(n)/d) with probability at least 1 nc by concentration of ρ2N2L yt2 ρ2N2dLβt1. Therefore, v2θ2 φF,t2 (cid:113) N2θ2 L(cid:112)ρ2N2dLβt1 CLN ρ1d ρ2βt1. Now, combining the concentration results for θφ(F that + θ swt1; bt) in Lemma E.10, we obtain v2 θφ(F yt + θ swt1; bt) = (1 o(1))N ψt ρ1 ρ2 (t log n)3/2 α1,t1 ρ1d ρ2βt1. (H.9) Furthermore, we have by Lemma E.11 that ψt C0θ2Qt N2dα1,t1. Under the conditions C0θ2Qt max (cid:110) Lρ1 ρ2s(t log n)3/2, Ld1Φ(sbt), L(cid:112)t log nρ1 N2 ξt dα1,t1 (cid:111) , we conclude by also noting that dα1,t1 1 that ψt max (cid:110) CN Lρ ρ2s(t log n)3/2 dα1,t1, α1,t1 pφ1(bt), CN ρ1L(cid:112)t log ξt (cid:111) . Now we plug (H.9) and (H.8) into (H.7) to obtain v, wt v2 = (1 o(1))N ψt + η1α1,t1 CLN (cid:112)dρ1s log (cid:0)(cid:113) Φ(sbt) + (cid:112) ρ1dρ2s1 + (cid:112)ρ1dsbtβt1 (cid:1) βt1. (H.10) Finally, under the conditions ts log nsbtβt1 1, st log Φ(sbt) ρ1d, we have (cid:112)dρ1s log (cid:0)(cid:113) Φ(sbt) + (cid:112) ρ1dρ2s1 + (cid:112)ρ1dsbtβt1 (cid:1) Cρ1d. Here, we use the fact that ρ2 log 1, which can be deduced from the following inequality under the condition N2 ρ2s(t log n)3/2: C0θ2Qt Lρ1 ρ1 N2 C0θ2Qt Lρ1 ρ2s(t log n)3/2 ρ1 (cid:112)ρ2 log n. Moreover, under the condition N2 C0θ2Qt CLρ1 βt1 α1,t , we conclude that the second line of (H.10) can be upper bounded by o(N ψt). Hence, the proof of Lemma F.6 is completed. H.2.3 Proof of Lemma F."
        },
        {
            "title": "Recall from the definition of wt that",
            "content": "P w1:0(wt η1 swt1)2 2 = t1 (cid:88) (cid:16) τ = zτ , ut Pu1:τ zτ , ut + u τ , ut τ 2 u τ 2 τ (cid:17)2 rzt2 2 2 2. + w1:t 117 (H.11) and (n1/c1, nc1) for some universal constant c1 (0, 1). Then Lemma H.1. Assume that there exist universal constants c, > 0 such that with probability at least 1 nc over the randomness of i.i.d. standard Gaussian vectors z1:T , for all [T ], t1 (cid:88) Pu1:τ zτ , ut2 + τ =1 t1 (cid:88) τ =1 (cid:16) u τ , ut τ 2 u τ 2 τ 2 (cid:17)2 + w1:t1 Proof. See H.5.1 for detailed proof. rzt2 2 2 2 Cd ut2 2. Lemma H.2 (Upper Bound for ut2 probability at least 1 nc for all < that 2). If log n, sbt = Θ( log n), ρ1 1, it holds with u2 CN Lρ1(ξt + dβt1). Proof. See H.5.2 for detailed proof. Combining Lemmas H.1 and H.2, it holds with probability at least 1 nc for all d, (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ =1 Pu1:τ zτ , ut2 + t1 (cid:88) τ =1 (cid:16) u τ , ut τ 2 τ 2 τ 2 d(cid:0)ξt + ut2 CN Lρ (cid:17)2 + w1:t1 rzt2 2 2 dβt1 (cid:1). (H.12) It remains to upper bound (cid:80)t1 Using Lemma F.5, we conclude that τ =1zτ , ut2. Recall that βt1 = (cid:113) 1 α2 1,t1 α2 0,t1 = (cid:113)(cid:80)t1 τ =1 α2 τ,t1. (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ =1 zτ , ut2 CN βt1 pφ1(bt) (1 o(1)) + CN Lρ1 ρ2s(t log n)3/2 α1,t1βt + CN ρ1Lt(cid:112)log ξt + CLN (cid:112)t log (cid:0)(cid:113) sρ1dΦ(sbt) + sρ1sbtd βt1 (cid:1) βt1 CN ρ1Lt(cid:112)log ξt + CLN ρ1dβt1, (H.13) where in the first inequality, the βt1 terms in the first line is obtained by the Pythagorean sum with respect to ατ,t1 for τ = 1, . . . , 1. In the second line, an additional 1 factor is added to the (cid:113)(cid:80)t1 maxτ =1,...,t1 xτ . In the last inequality, we use upper bound for zτ , ut since st log nsbtβt1 1 to upper the conditions bound all the terms containing βt1 by CLN ρ1dβt1. Plugging (H.12) and (H.13) into (H.11), we obtain τ ρ2s(t log n)3/2 1, Φ(sbt) ρ1d(st log n)1, and τ =1 x2 w1:0wt2 ut2 + (cid:118) (cid:117) (cid:117) (cid:116) t1 (cid:88) τ =1 zτ , ut2 + η1βt1 CN Lρ1 d(cid:0)ξt + dβt1 (cid:1) + η1βt1. d, which is implied by the condition ρ1d(st log n)1 Here, we use the fact that Φ(sbt) Lsρ1(t log(n))3. Lastly, by condition η1 Φ(sbt) and the fact that Lρ1d Φ(sbt) by assumption, we can absorb the η1βt1 term into the CN Lρ1dβt1 term. Hence, we complete the proof of Lemma F.7. log 118 H.2.4 Proof of Lemma F."
        },
        {
            "title": "Recall by definition of wt that",
            "content": "Pw1:0wt2 = (cid:115) v, wt2 v2 2 + (cid:0)z0, ut + η1α0,t1 (cid:1)2. By Lemma F.6, we already have v, wt/v2 = (1 o(1))N ψt. It remains to characterize z0, ut. We have by Lemma F.5 that z0, ut = α0,t1 pφ1(bt) (1 o(1)) CN Lρ1 CN ρ1L(cid:112)t log ξt CLN (cid:0)(cid:113) = α0,t1 pφ1(bt) (1 o(1)) CN Lρ1 CN ρ1L(cid:112)t log ξt CLN ρ1dβt1 ρ2s(t log n)3/2 α0,t1α1,t1 log(n)ρ1dΦ(sbt) + (cid:112)s log(n)ρ1sbtdβt1 ρ2s(t log n)3/2 α0,t1α1,t1 (cid:1) βt1 log nsbtβt1 Here, in the last term we use the condition 1, and ρ1d(st log n)1 Φ(sbt) to upper bound log(n)ρ1dΦ(sbt) Cρ1d. Note that the fluctuation terms are similar to the one for z1, ut in the proof of Lemma F.6. Specifically, under the same conditions ts log nsbtβt1 1 to upper bound (cid:112) C0θ2Qt max (cid:110) Lρ ρ2s(t log n)3/2, L(cid:112)t log nρ1 N2 ξt dα1,t1 , Lρ1 (cid:111) βt1 α1,t1 we have ψt CN max (cid:110) ρ ρ2s(t log n)3/2 dα0,t1α1,t1, ρ1 (cid:112)t log ξt, ρ1dβt1 (cid:111) . Thus, we conclude that z0, ut = α0,t1 pφ1(bt) (1 o(1)) o(N ψt). Thus, Pw1:0wt2 = (cid:115) v, wt2 v2 2 + (z0, ut + η1α0,t1)2 (cid:113)(cid:0)N ψt (1 o(1))(cid:1)2 + (cid:0)N α0,t1 pφ1(bt) (1 o(1)) o(N ψt) + η1α0,t1 (cid:1)2 = = (1 o(1)) (cid:113) (N ψt)2 + (N α0,t1 pφ1(bt))2. Here, the last inequality holds by also noting that η1 Φ(sbt) CN pφ1(bt). This completes the proof. H.3 Proofs for Recursion Analysis H.3.1 Proof of Lemma F.10 What we need to prove here is that all the conditions in Lemma F.9 hold for the current time step if the conditions in Lemma F.10 hold. This is because the conditions in Lemma F.9 are the union of the conditions in Corollary F.4 and Lemmas F.5 to F.8. In the following, we check all the listed conditions one by one. 119 Step I: Checking all conditions in Lemma F.9. For the first step, we divide the conditions in Lemma F.9 into three groups. Group 1: Implication of Cond.(i) and Cond.(I). We first notice that since , conditions sbt = Θ((cid:112)log n) < ζ1, η1 Φ(sbt) N2dC0θ2Qt ρ2s(t log n)3/2 1, κ0sbt = O(1), are guaranteed by Cond.(i). Here, we need to be more careful about condition η1 N2dC0θ2Qt, as Qt is function of t, and what we directly have in Cond.(i) is for Q1 only. By definition (cid:1), we note that Qt is nondecreasing in α1,t1. Therefore, we have Qt = 1 N2 the following fact: dα1,t1 1(cid:0)θl > (cid:80)N2 l=1 Fact H.3. If α1,t1 α1,0, then Qt Q1. In fact, the condition α1,t1 α1,0 is automatically guaranteed by Cond.(I). Therefore, the condition η1 N2dC0θ2Qt will hold for all successive as long as it holds for = 1 and α1,t1 α1,0. Meanwhile, we also have by the same reasoning that dα1,t1 dα1,0 1 where the last inequality is guaranteed by InitCond-1. The condition guaranteed by Cond.(I) as well. ts log nsbtβt1 1 is Group 2: Implication of Cond.(ii) to Cond.(iii). The direct implication of Cond.(ii) is that ρ1d(st log n)1 Φ(sbt) Lsρ1(t log(n))3. Similarly, the direct implication of Cond.(II) and Cond.(iii) is that C0θ2Qt max (cid:110) Lρ1 ρ2s(t log n)3/2, Ld1Φ(sbt), Lρ1 N2 βt1 α1,t1 (cid:111) . ξt log nρ1 dα1,t1 holds. Here, we use the fact that and the monotonicity of Qt in Fact H.3. It remains to check whether N2 C0θ2Qt Group 3: Implication of Cond.(ii),Cond.(iii), Cond.(I) and Cond.(II). To verify this inequality N2 C0θ2Qt dα1,t1 , we just need to show that ξt/α1,t1 Cξ1/α1,0 for some universal constant > 0, as the corresponding inequality for the latter is already guaranteed by Cond.(iii). Recall the definition of ξt in Corollary F.4, the ratio ξt/α1,t1 is given by log nρ1 ξt log(n) Kt + ρ1 (cid:114) (cid:104) Φ(sbt) pEl,l (cid:16) Φ sbt (cid:113) 1hl,hl 1+hl,hl (cid:17) (cid:105) hl, hl + ρ2 α1,t ξt α1,t = + (cid:112)ρ2d. (H.14) We obtain the above formula by the nonnegativity of α1,t1 guaranteed by Cond.(I). Proposition H.4. If sbt 2 log for some universal constant κ > 0, then for 2, Kt (cid:0)K1 + C(cid:112)log (βt1 + α1,t1 + α1,0)(cid:1). Proof. See H.5.3 for detailed proof. 120 Combining (H.14), Proposition H.4 and the fact that α1,t1 t2α1,0 α1,0 by Cond.(I), we have ξt α1,t st2 log(n)K1 α1,t1 (cid:114) + ρ1 (cid:104) Φ(sbt) pEl,l st2 log(n)3/2(cid:16) βt1 α1,t1 (cid:17) (cid:16) (cid:113) 1hl,hl 1+hl,hl sbt Φ (cid:17) + 2 + α1,t1 (cid:105) hl, hl + ρ2 + (cid:112)ρ2d ξ1 α1,0 + st2 log(n)3/2(cid:16) βt1 α1,t1 (cid:17) , + 2 (H.15) where in the second inequality, we directly plug in the definition of ξ1 with = 1 in (H.14) and use the fact that α1,t1 t2α1,0 to upper bound the first term in the right-hand side. Furthermore, for each term in Cond.(II), we have the following relationship: N2 ρ1, C0θ2Qt = O(1), = Ω(1), where the first inequality holds by direct definition of ρ1 in (E.1), the second equality holds by noting that θ2 1, Qt 1 and C0 is universal constant, and the last inequality holds by Assumption C.1. Together, we have the following implication: N2 C0θ2Qt Lρ βt1 α1,t1 βt1 α1,t1 1 Therefore, we can further simplify the upper bound in (H.15) to ξt α1,t ξ1 α1,0 + st2 log(n)3/2 1(t 2). (H.16) Using (H.16), in order for condition N2 ensure C0θ2Qt log nρ1 ξt dα1,t1 to hold, we just need to N2 C0θ2Qt L(cid:112)t log nρ1 ξ1 dα1,0 , N2 C0θ2Qt CLd1ρ st5(log n)2. The first one is clearly given by Cond.(iii), and the second one is satisfied because we have by using Cond.(ii) and Cond.(iii) that C0θ2Qt Ld1Φ(sb) L2d1ρ1s(T log(n))3 CLd1ρ1 st5(log n)2. N2 Here, the first inequality holds by the second condition in Cond.(iii), the second inequality holds by Cond.(ii), and the last inequality holds by noting that we are considering any . The last inequality shows that the last condition N2 C0θ2Qt dα1,t1 also holds automatically under the conditions in Lemma F.10. To this end, we have shown that all the conditions in Lemma F.9 hold for if the conditions in Lemma F.10 are satisfied. log nρ1 ξt Step II: Deriving the recursion. As we have shown in the previous step, all the conditions in Lemma F.9 hold for if the conditions in Lemma F.10 hold. Therefore, we can safely apply all the concentration results derived in F.2. We next show how to use the previous derived concentration result on v, wt/v2, Pw1:0wt2, and w1:0wt2 to control the recursion of βt/α1,t and 1/α1,t. 121 Since βt is the projection of wt onto the Pw1:0 direction, and α1,t is the projection of wt onto the direction, we have βt α1,t = v2 w1:0wt2 v, wt CLρ1 C0θ2Qt N2/N (cid:18) 1 (cid:1) dβt1 d(cid:0)ξt + CLρ1 C0θ2Qt N2/N dα1,t1 (cid:16) ξ1 α1,0 + (cid:17) st2 log(n)3/2 1(t 2) + βt1 α1,t1 (cid:19) . where in the first inequality, we use the upper bound for w1:0wt2 in Lemma F.7 and the lower bound for v, wt/v2 in Lemma F.6 as v, wt/v2 (1 o(1))N ψt C0θ2Qt N2/N dα1,t1 by the lower bound of ψt in Lemma E.11. The second inequality holds by plugging in the upper bound for ξt/α1,t1 in (H.16). Similarly, we have by definition of α1,t that (1 + o(1)) (cid:112)ψ + pφ1(b)2 + CLρ1 dξt 1 α1,t v2 wt2 v, wt (1 o(1)) ψt = (1 + o(1)) (cid:113) (C0θ2Qt N2/N dα1,t1)2 + (cid:0)CLΦ(sb)(cid:1)2 + CLρ1 dξt C0θ2Qt N2/N dα1,t1 (cid:18) Φ(sb) ρ1d 1 α1,t1 + 1 (cid:16) ξ1 α1,0 + st2 log(n)3/2 1(t 2) (cid:17)(cid:19) CLρ1 C0θ2Qt N2/N + (1 + o(1)). where in the second inequality, we plug in the lower bound for ψt and the upper bound for pφ1(bt) in Lemma E.8. The last inequality holds by the triangle inequality and the upper bound for ξt/α1,t1 in (H.16). This completes the proof of Lemma F.10. H.3.2 Proof of Lemma F.12 In the following proof, let us take T1 = max{(2ς)1, 1}. As our goal is to establish that (F.5) and (F.6) holds for all T1, we just need to show that Cond.(I) and Cond.(II) hold for all T1, as they are the only conditions that might be violated over time, and the other conditions only depend on the initial conditions. Initial step. For = 1, we have α1,t1 = α1,0 and βt1 = β0 = 0. Hence, Cond.(I) and Cond.(II) hold trivially. Before we start the proof, we first derive some useful inequalities. Useful inequalities. For λ1, we have by Cond.(v) and Cond.(vi) that λ1 = CLρ1 C0θ2Q1 N2/N = ρ1d1ς Φ(sb) , λ1ξ1 = λ0ξ1 Q1 dϵ log 1. (H.17) Using the above two inequalities, we have by (F.4) that (cid:16) λ1Φ(sb) ρ1d 1 + o(1) + 1 α1,1 1 α1,0 λ1ξ1 + (cid:17) 1 + o(1) + (d1/2ς + 1) 3 + d1/2ς . In fact, we have the ratio α1,0/α1,1 as α1,0 α1,1 (3 + d1/2ς ) α1,0 (3d1/2 + dς ) C(cid:112)log 1. (H.18) log ) with sufficiently high probability 1 nc, and Here, we use the fact that α1,0 = O( = poly(n). The above inequality demonstrates that α1,1 is guaranteed to grow in the first step. Thus, by definition of Qt in (E.15), we conclude that Q2 = 1 N2 N2(cid:88) (cid:16) 1 l=1 θl dα1,1 (cid:17) 1 N2 N2(cid:88) l=1 (cid:16) (cid:17) θl b(3d1/2 + dς ) 1 =: Qν 2, where we take ν = b(3d1/2 + dς ) = O( above inequality as Qν 2. Since θl [0, 1], we have log dς1/2) and denote the right-hand side of the θ2 = 1 N2 N2(cid:88) l=1 Qν θ2 2 12 + (1 Qν 2) ν2 = Qν 2(1 ν2) + ν2 Qν 2 θ2 ν2 1 ν2 θ2 2 , where the last inequality holds from Assumption C.1 that θ2 = Ω(polylog(n)1) ν2. In the sequel, we will use Q2 θ2/2 as the lower bound for Q2. By definition of T1, we have T1 = (2ς)1 1 = Θ(1). In addition, for λ2, we have λ2 = λ0 Q2 2λ0 θ2 = O(polylog(n)), (H.19) where in the inequality we use the lower bound for Q2 and in the last equality we use θ2 = Ω(polylog(n)1) in Assumption C.1 and λ0 = O(polylog(n)) in Cond.(iv). We now have for the coefficient λ2Φ(sb)/(ρ1d) that λ2Φ(sb) ρ1d = λ0Φ(sb) Q2ρ1d = Q1dς Q2 dς , where the second identity holds from Cond.(v) and the last inequality holds by noting that α1,1 α1,0 by the first steps calculation in (H.18) and using the monotonicity of Qt in Fact H.3. Next, we upper bound the quantity C1 in (F.2): (cid:16) (cid:16) (cid:16) C1 = 1 + o(1) + λ2ξ1 dα1,0 1 + o(1) + λ1ξ1 + + Cλ2 0 (log n)3/2 sT 2 polylog(n) (cid:17) (cid:17) 1 1 λ2Φ(sb)/ρ1d 1 1 dϵ 1 + o(1) + (cid:17) (1 + o(1)) = 1 + o(1), dα1,0 dϵ log where in the first inequality, we use the fact that λ2 λ1 by the fact Q2 Q1, and we invoke the upper bound T0 log and λ2 = O(polylog(n)) in (H.19). In the last inequality, we use the previous bound for λ1ξ1 in (H.17) together with the fact that dα1,0 1 by InitCond-1. Induction step. Suppose the induction hypothesis holds for 1, 2, . . . , t. We will show that Cond.(I) and Cond.(II) hold for + 1 T1 as well. To this end, it is evident that α1,t is always growing before reaching C1, which is evident from (F.3) by noting that λ2Φ(sb)/ρ1d dς < 1. We first look at the recursion of α1,t. By (F.5), the ratio α1,0/α1,t is bounded by α1,0 α1,t (cid:16) λ2Φ(sb) ρ1d (cid:17)t1 dς(t1) (cid:16) dς + (cid:16) λ1Φ(sb) ρ1d dϵ sd log + (cid:17) (cid:17) λ1ξ1 + C1α1,0 + (1 + o(1)) log Cdς(t1)(ς1/2) + d1/2+ϵ. The first term on the right-hand side is decaying exponentially fast with respect to t. The second term is much smaller than 1/T 2 0 given that T0 log by definition. Therefore, both terms are much smaller than 1/T 2 0 . This implies the first condition in Cond.(I) holds for + 1. Next, we look at the conditions involving βt. By previous analysis on T1 and the upper bound in (H.19), we obtain λT11 2 (polylog(n))(2ς)11 = O(polylog(n)). By recursion of βt/α1,t in (F.6), we have βt α1,t λt1 2 (cid:16) (T0 + λ1) + ξ1 α1,0 polylog(n) polylog(n) + (cid:16) λ1ξ1 α1,0 (cid:16) dϵ log sT 3 0 log(n)3/2(cid:17) (cid:17) polylog(n) + polylog(n) (cid:17) dϵ polylog(n) . Here, the second inequality holds by the upper bound for λT11 and also the fact that T0 + λ1 2λ1 log since λ1 1 and T0 log n. In the second inequality, we use the upper bound for λ1ξ1 in dα1,0 1 by InitCond-1. The last inequality holds because ϵ < 1/2 by (H.17) and the fact that definition. Using the above inequality with the fact that α1,t 1, we obtain 2 βt dϵ polylog(n)"
        },
        {
            "title": "1\nT0s log n|sb|",
            "content": ", where the last inequality holds by noting that both T0 and sb are at most O(polylog(n)). This implies that the second condition in Cond.(I) holds for + 1. Eventually, for Cond.(II), we have C0θ2Qt N2/N CLρ1 = Qt λ Q2 λ0 θ2 2λ0 = Ω(polylog(n)1). Therefore, the left-hand side of the above inequality is also much larger than βt/α1,t. To this end, we have finished the induction step and proved that Cond.(I) and Cond.(II) hold for all T1. Final step. According to the recursion in (F.5), let us consider the real value that satisfies (cid:16) λ2Φ(sb) ρ1d (cid:17)t1 (cid:16) λ1Φ(sb) ρ1d + (cid:17) λ1ξ1 1 α1,0 = log(d)c0 (H.20) for some small constant c0 > 0 to be determined later. We first note that we can obtain the ς 1/2 factor by the inequality for λ1 in (H.17) that λ1Φ(sb) ρ1d λ1Φ(sb) ρ1d λ2Φ(sb) ρ1d λ1Φ(sb) ρ1d d1/2ϵ log dς + λ1ξ1 λ1ξ1 (H.21) and + + , Using the above inequality (H.21), and taking logarithm of both sides with base for (F.5), we have for that logd (cid:16) dς + (cid:17) d1/2ϵ log + logd (cid:16) 1 (cid:17) α1,0 c0 log log log , 124 which implies that logd (cid:18) 1 dς + d1/2ϵ log (cid:19)1 (cid:18) logd (cid:16) 1 (cid:17) α1,0 + c0 log log log (cid:19) 1/2 ς 1/2 = (2ς)1 1 = T1. (H.22) In the second inequality, we use the fact that by InitCond-1, logd (cid:16) 1 (cid:17) α1,0 = logd(v2) logd (cid:0)(1 ε)(cid:112)2 log(M/n)(cid:1) 1 2 log log(M/n) 2 log . Therefore, we can take c0 to be small enough but still on constant level such that 1 log log(M/n) 2 log 1 2 c0 log log 2 log . This justifies the second inequality in (H.22). Thus, there must exists some time T1 such that (H.20) holds. For this time t, we already have 1 α1,t (cid:16) λ2Φ(sb) ρ1d (cid:17)t (cid:16) λ1Φ(sb) ρ1d + (cid:17) λ1ξ1 1 α1,0 + C1 dς + C1 1 + o(1). This implies that α1,t = 1 o(1). Checking α1,t1 α1,1. An additional step is needed to show that α1,t1 α1,1 for all 2 and before is reached. This is required because we want to ensure that before time t, we always have α1,t1 α1,1, and the stopping time T0 will not prohibit us from reaching t. In fact, we have by (F.3) that 1 α1,t (cid:16) λ2Φ(sb) ρ1d (cid:17)t2 (cid:16) 1 α1,1 (cid:17) C1 + C1. Therefore, the ratio α1,t1/α1,1 is bounded by α1,1 α1,t1 (cid:16) λ2Φ(sb) ρ1d (cid:17)t2 (cid:0)1 C1α1,1 (cid:1) + C1α1,1. We consider two cases. If C1α1,1 1, we can just stop the gradient at = 1 and obtain α1,1 = 1 o(1) since C1 = 1 + o(1). In this case, we reach strong alignment in just one step. In another case where C1α1,1 < 1, since λ2Φ(sb)/(ρ1d) dς , we have the above ratio strictly upper bounded by 1. Hence, the condition α1,t1 α1,1 holds for all 2 and before is reached. In both cases, we have shown that α1,t1 α1,1 hold for 2 t. As we have shown that Cond.(I) and Cond.(II) hold for all T1 from the induction step, T1 from the final step, and T1 log(n) by definition, we conclude that T0-Cond.(1) to T0-Cond.(3) in the definition of the stopping time T0 hold for all t. In other words, we have shown that T0 t. Thus, we complete the proof of Lemma F.12. H.4 Proofs for Condition Simplification H.4.1 Proof of Lemma F.13 Let us take as the maximum number of iterations considered. In the following, we first provide sufficient condition for Cond.(iii), Cond.(v) and Cond.(vi) to hold. Then, we give reformulation of Cond.(i), Cond.(ii) and Cond.(iv). 125 sufficient condition for Cond.(iii) to hold is given by Φ(sb) ρ1d ρ2s(log n)3/2, max Q1 λ0 (cid:110) , (cid:112)log ξ1 (cid:111) (H.23) under the condition dα1,0 1. On the other hand, we note that Cond.(v) and Cond.(vi) can be reformulated as Q1 λ0 dς = Φ(sb) ρ1d dϵς log ξ1. (H.24) log ξ1 Since dϵ log ξ1, we can safely delete the last term in (H.23). Also by noting that dς 1, we can safely delete the second term in (H.23). Furthermore, by definition of ξ1, which we recall as follows: ξ1 = log K1 + ρ1 1 (cid:118) (cid:34) (cid:117) (cid:117) (cid:116)Φ(sb) pEl,l Φ (cid:16) sb (cid:115) 1 hl, hl 1 + hl, hl (cid:17) hl, hl (cid:35) + (cid:112)ρ2d α1,0 + ρ2 , we conclude that ξ1 ρ2. Therefore, ρ2dα1,0 dϵ log ξ1 dϵ ρ2s log ρ2s(log n)3/2, where in the last inequality, we use the definition ϵ = log log n/(ς log d) log log n/ log d. Therefore, the first term in (H.23) can also be deleted. In summary, Cond.(iii) is automatically implied by (H.24). reformulation of Cond.(ii) gives 1 log Φ(sb) ρ1d Ls log(n)3 . (H.25) In the following, we will simplify the above condition. Note that Φ(sb)/(ρ1d) = Q1/λ0dς dς (s log n)1 holds by using λ0 = Θ(polylog(n)) according to Cond.(iv) and λ1 to Cond.(v). Therefore, we can safely remove the first inequality in (H.25). dϵ directly implies that ρ2 n1/2. Therefore, we can safely delete the condition in Cond.(i). ρ2s(t log n)3/2 1 in Cond.(i). As Q1/λ0 In the following, we aim to remove the condition log ξ1 by Cond.(vi), we conclude that ξ1 Q1/λ0 < 1. By definition of ξ1, this condition ρ2s(t log n)3/2 1 0Q1 dς = Φ(sb)/(ρ1d) according To this end, we can summarize Cond.(ii), Cond.(iii), Cond.(v) and Cond.(vi) into one condition as follows: Q1 λ0 dς = Φ(sb) ρ1d max dϵς (cid:110) log ξ1, Ls log(n)3 (cid:111) , and Cond.(i) and Cond.(iv) can be summarized into λ0 = O(polylog(n)), κ0 = O((log n)1/2), η1 (cid:16) ρ1d λ0 (cid:17) Φ(sb) . Note that in the last condition, we have ρ1d/λ0 Φ(sb) according to the first equality in (H.4.1). Hence, we only need to keep η1 Φ(sb). This completes the proof of Lemma F.13. 126 H.4.2 Proof of Lemma F.14 To prove this lemma, we need to upper bound the expectation term on the left-hand side of (F.10). pEl,l is given by uniformly samples l, from [N ], and that hl, hl 1 always holds. We Recall that can upper bound the expectation term as follows: (cid:20) (cid:16) Φ pEl,l (cid:115) sbt 1 hl, hl 1 + hl, hl (cid:17) (cid:21) hl, hl 1 2 (cid:88) (cid:88) j=1 l,lDj (cid:115) sb (cid:16) Φ 1 hl, hl 1 + hl, hl (cid:17) = 1 2 (cid:88) (cid:88) j=1 l,lDj (cid:115) (cid:16) sb Φ 1 hl, hl 1 + hl, hl (cid:17) 1(hl hl = 1) (cid:88) (cid:88) (cid:115) sb (cid:16) Φ + 1"
        },
        {
            "title": "1\nN 2",
            "content": "n (cid:88) (cid:88) j=1 l,lDj l,lDj (cid:115) j= (cid:16) Φ sb 1 hl, hl 1 + hl, hl (cid:17) 1(hl hl 2) 1 Hl,jHl,j 1 + Hl,jHl,j (cid:17) + 1 2 (cid:88) (cid:88) j=1 l,lDj 1(hl hl 2), where in the identity, we split the summation according to whether how many non-zero entries are shared between two rows hl and hl in the matrix. In the last inequality, we drop the indicator for the case hl hl = 1 and use the fact that Φ() 1 for the case hl hl 2. For the first term, we use the fact that Dj/N ρ1 for all [n] to obtain"
        },
        {
            "title": "1\nN 2",
            "content": "n (cid:88) (cid:88) j=1 l,lDj (cid:115) sb (cid:16) Φ 1 Hl,jHl,j 1 + Hl,jHl,j (cid:17) ρ2 1 (cid:88) j=1 1 Dj2 (cid:88) (cid:16) Φ (cid:115) sb l,lDj 1 Hl,jHl,j 1 + Hl,jHl,j (cid:17) nρ2 (cid:16) 1 Φ (cid:115) sbt 1 h2 1 + h2 (cid:17) , where the last inequality holds by the definition of in (F.9). In addition, the second term is upper bounded by"
        },
        {
            "title": "1\nN 2",
            "content": "n (cid:88) (cid:88) j=1 l,lDj 1(hl hl 2)"
        },
        {
            "title": "1\nN 2",
            "content": "N (cid:88) (cid:88) (cid:88) l=1 j[n]: Hl,j =0 i=j: Hl,i= (cid:88) l=1 1(Hl,i = 0) 1(Hl,j = 0) max l[N ] max l[N ]"
        },
        {
            "title": "1\nN",
            "content": "(cid:88) i,j[n]:i=j Hl,i=0,Hl,j =0 (cid:88) i,j[n]:i=j Hl,i=0,Hl,j =0 (cid:88) l=1 1(Hl,i = 0) 1(Hl,j = 0) ρ1 ρ2 s2ρ1ρ2. In the first inequality, we notice that if hl hl 2, then there must exist two different feature indices = such that both hl, hl are non-zero at these two indices. This is indeed reflected in the constraints Hl,j = 0, Hl,i = 0 and the two indicators 1(Hl,i = 0) 1(Hl,j = 0). Therefore, summing over all posible (i, j) pairs gives an upper bound for the second term. In the second inequality, we change the average over to be the maximum over l, and in the third inequality, we use the 127 definition of ρ2 and ρ1 in (E.1) to upper bound sum of the double indicator term. The last inequality holds by noting that each row hl is s-sparse. Combining the above two bounds, we obtain that (cid:115) (cid:20) Φ (cid:16) sbt pEl,l 1 hl, hl 1 + hl, hl (cid:17) hl, hl (cid:21) (cid:16) nρ2 1 Φ (cid:115) sbt (cid:17) 1 h2 1 + h2 + ρ1ρ2s2 Cnρ2 1 Φ(sbt) 1h2 1+h2 + ρ1ρ2s2, where in the last inequality, we use the Mills ratio (cid:115) sbt (cid:16) Φ (cid:17) 1 h2 1 + h2 (cid:16) (cid:115) sbt (cid:17)1 1 h2 1 + h2 Csbt1 1 2π exp (cid:16) 1 2π sb2 2 (cid:16) exp 1 h2 1 + h2 (cid:17) sb2 2 (cid:17) 1h2 1+h CΦ(sbt) 1h2 1+h2 , and the above inequalities hold as long as (1 h2 have proved Lemma F.14. )/(1 + h2 ) is on constant level. Therefore, we H.4.3 Proof of Lemma F. Recall the definition Kt in (E.9) as (cid:18) sb Φ K1 := (cid:19) 1/ ρ2snsbΦ (cid:18) + sb ℏ2 3, + 1 3 (cid:113) 2 1/4 (cid:19) (cid:18) (cid:16) Φ + (cid:19) + (cid:0)ρ2s(cid:1)1/4 (cid:0)log(n)(cid:1)1/4 + n1/4ρ2 log(n). (cid:113) 3 4 sb ℏ2 4, + 1 4 sb + ℏ4,1ζ1 (cid:17) (cid:113) 1 ℏ2 4,1 To upper bound the above terms, let us consider the following inequality for any τ (0, 1) and sb 1: Φ(τ sb) (cid:16) exp 1 2π 1 2π)τ 2 exp ( τ 2sb2 (cid:17) (cid:16) τ 2sb2 2 1 τ sb (cid:17) 1 2π (cid:16) sb sb2 + 1 exp (cid:16) (cid:17) τ 2sb2 2 (cid:17)τ 2 2τ 1 2 τ sb sb2 + 1 Φ(sb)τ 2 , 2τ 1 (H.26) where in the first and the last inequalities, we use the Mills ratio bound that x/(x2 + 1) Φ(x)/p(x) x1 for all > 0. Now, we can apply (H.26) to upper bound the first term in Kt as sb Φ (cid:18) sb ℏ2 4, + 1 4 (cid:113) 3 4 1/4 (cid:19) C(cid:0)n sb(cid:1)1/4 Φ(sb) 1 4,+1 C(cid:0)n sb(cid:1)1/4 Φ(sb) 3ℏ2 1 3h2 +1 , (H.27) where the last inequality holds because ℏ4, by definition. Similarly, we can upper bound the second term as ρ2snsbΦ (cid:18) sb ℏ2 3, + 1 3 (cid:113) 2 3 1/4 (cid:19) C(ρ2snsb)1/4 Φ(sb) 128 8ℏ2 3,+4 C(ρ2snsb)1/4 Φ(sb) 3 8h2 +4 . (H.28) Here, the third term also follows from the above inequality as (cid:16) Φ sb + ℏ4,1ζ1 (cid:113) 1 ℏ2 4,1 (cid:17) (cid:16) Φ (cid:17) sb + hζ1 (cid:112)1 h2 Φ(sb) (1hζ1/sb)2 1h2 , (H.29) where in the first inequality, we use the derivative in (E.12) and the fact that ζ1/sb = Θ(1) > 1, log in (D.11), to conclude that increasing ℏ4,1 to which is given by the definition ζ1 = (1 + ε)2 will only increase the value of the whole term. In the second inequality, we apply (H.26) with 1 hζ1/sb (cid:112)1 h2 (0, 1). τ = Here, we claim τ (0, 1) because by condition ζ1h < 1 ν for some constant ν > 0, we have 1 hζ1/sb > 0, and by noting that ζ1/sb > 1, we have τ < 1 1 = (cid:112) 1 1. In addition, since sb quently, we obtain that 2 log by condition Φ(sb) ρ1 n1, we also have ζ1/sb > 1. Conseζ1/sb h1 (cid:112) h2 1 h1 1 (cid:112) h2 1 h2 as < 1. Therefore, we can apply (H.26) with τ = (h1 1 (0, 1) in the last inequality in (H.29). Now, we can combine (H.27), (H.28) and (H.29) to obtain the desired result in Lemma F.15. ζ1/sb)/ (cid:112) H.5 Proofs for Technical Lemmas H.5.1 Proof of Lemma H.1 By Cauchy-Schwartz, it holds that t1 (cid:88) Pu1:τ zτ , ut2 t1 (cid:88) Pu1:τ zτ 2 2 ut2 2. τ = τ =1 One thing to be noted is that zτ is independent of the filtration σ(u1:τ ). Consiquently, when τ . By the concentration of χ2 distribution in Lemma I.1 with conditioned on u1:τ , Pu1:τ zτ 2 union bound over all τ [T ], we obtain that with probability at least 1 nc for some universal constant c, > 0 that 2 χ2 Pu1:τ zτ 2 2 τ + C(cid:112)τ log(nT ) + log(nT ) C(t + log n), τ [t 1], [T ]. Therefore, we have that with probability at least 1 nc: t1 (cid:88) Pu1:τ zτ , ut2 C(t2 + log n) ut2 2, τ =1 [T ]. (H.30) 129 For the second term, it follows from Lemma G.1 that with probability at least 1 nc: t1 (cid:88) τ =1 (cid:16) u τ , ut τ 2 (cid:17)2 u τ 2 τ 2 Cd t1 (cid:88) τ =1 τ , ut2 τ 2 2 = Cd Pu1:t1ut2 2, [T ]. (H.31) For the last term Therefore, with probability at least 1 nc that rzt2 2 χ2 w1:t1 rzt2 w1:t1 2 2, we also note that rzt is independent of the filtration σ(w1:t1). 2 dt+1. We have by Lemma I.1 with union bound over all [T ], and w1:t1 rzt2 2 + 1 + C(cid:112)(d + 1) log(nT ) + log(nT ) Cd, [T ]. (H.32) Combining (H.31) and (H.32), we have with probability at least 1 nc and for all [T ]: t1 (cid:88) τ =1 (cid:16) u τ , ut τ 2 (cid:17) u τ 2 τ 2 + w1:t1 rzt 2 2 2 Cd (cid:0)Pu1:t1ut2 2 + 2 2 (cid:1) = Cd ut2 2. (H.33) Now we combine (H.30) and (H.33) to obtain the desired result in Lemma H.1. H.5.2 Proof of Lemma H."
        },
        {
            "title": "Recall that",
            "content": "ut = Eφ(Ey ; bt) + φ(F + θ swt1; bt) + Et + Ft. By the triangular inequality, we have ut2 2 (cid:113) Eφ(Ey ; bt)2 2 + φ(F + θ swt1; bt)2 2 + 2 (cid:113) Et2 2 + Ft2 2 By Corollary F.4, we have (cid:113) Eφ(Ey ; bt) 2 + φ(F yt + θ swt1; bt)2 2 CLN ρ1ξt. By Lemmas E.12 and E.13, and the fact that N1 and N2 ρ1, we derive that (cid:113) Et2 2 + Ft 2 CLρ1N dβt1 + CLρ1ρ2 dβt1 CLN ρ1 dβt1. This completes the proof of Lemma H.2. H.5.3 Proof of Proposition H."
        },
        {
            "title": "We recall from the definition of Kt that",
            "content": "(cid:18) sb Φ Kt = (cid:113) 3 4 sb ℏ2 4, + 1 4 sb + ℏ4,tζt (cid:113) 1 ℏ2 4,t + Φ (cid:16) 1/4 (cid:19) ρ2snsbΦ (cid:18) + sb ℏ2 3, + 1 3 (cid:113) 2 1/4 (cid:19) (cid:17) + (cid:0)ρ2s(cid:1)1/ (cid:0)t log(n)(cid:1)1/4 + n1/4ρ2 log(n), 130 The terms that implicitly change with is ζt and ℏ4,t. Recall that ζt = ζ1 + 1(t 2) C(βt1 + α1,t1 + α1,0) with ζ1 = 2(1 + ε) 2 log n. Moreover, by definition of ℏ4,t, we can rewrite the term as (cid:16) Φ (cid:17) sb + ℏ4,tζt (cid:113) 1 ℏ2 4,t 1 Dj = max j[n] (cid:88) (cid:16) Φ lDj sb + Hl,jζt (cid:113) 1 2 l,j (cid:17)4 1/4 . (H.34) To understand how the change in ζt affects the term, we take the derivatives for positive power q: (cid:16) Φ dζ sb + xζ 1 (cid:17)q (cid:12) (cid:12) (cid:12) (cid:12) ζ=ζt = qΦ (cid:16) sb + xζ 1 x2 (cid:17)q (cid:16) sb + xζ 1 x2 (cid:17) ζx 1 x2 > 0. 2 log > sb by Here, we have the second derivative larger than 0 since ζt ζ1 = assumption. Now, our goal is to upper bound the derivative with respect to ζ. We discuss in two cases: 2(1 + ε) sb + xζ [(sb + ζ)/2, sb + ζ], (1 + 1/ 2)/2 and thus For (cid:2)(1 + sb/ζ)/2, 1(cid:3), we have sb + xζ 1 x2 d dζ (cid:17)q Φ (cid:16) (cid:16) sb + xζ 1 x2 (cid:17) sb + xζ 1 x2 ζx sb + ζx ζ sb + ζ sup z0 p(z) = O(1). sup 1(1+1/ 2)2/4 p(z) For [0, (1 + sb/ζ)/2], we have 1 x2 (cid:112) 1 (1 + sb/ζ)2/4 Ω(1). Thus (cid:113) 1 (1 + 1/ 2)2/4 = (cid:16) Φ dζ sb + xζ 1 x2 (cid:17)q ζ (cid:113) 1 (1 + 1/ 2)2/4 = O((cid:112)log n). Therefore, we conclude that for = 1, it holds for any Hl,j [0, 1] that (cid:16) Φ sb + Hl,jζt (cid:113) 1 2 l,j (cid:17) (cid:16) Φ (cid:17) sb + Hl,jζ1 (cid:113) 1 2 l,j C(cid:112)log (βt1 + α1,t1 + α1,0). (H.35) Since x4 y4 y4 m1/4x for any x, Rm by the triangle inequality, we conclude that the same upper bound in (H.35) holds for each in (H.34) as well. Therefore, the same upper bound also holds after taking the maximum over [n] in (H.34). Therefore, we obtain that Kt (cid:0)K1 + C(cid:112)log (βt1 + α1,t1 + α1,0)(cid:1). This completes the proof of Proposition H.4."
        },
        {
            "title": "I Auxiliary Lemmas",
            "content": "I.1 Concentration Inequalities Lemma I.1 (Chi-square concentration, Lemma 1 in Laurent and Massart (2000)). Let X1, . . . , Xn be independent random variables such that Xi (0, 1) for all i. Let Rn + be vector with nonnegative entries. Then the following holds for any δ (0, 1): (cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:88) i= aiX 2 a1 (cid:12) (cid:12) (cid:12) (cid:12) 2 (cid:113) 2 log δ1 + 2a log δ1 (cid:33) δ. Lemma I.2 (Tail probability for the maximum Gaussian random variables). Let X1, . . . , Xn be σ2-subgaussian random variables with mean 0. Then for any > 0, (cid:18) max i=1,...,n Xi (cid:112) 2σ2 log + (cid:19) (cid:18) exp (cid:19) . t2 2σ2 In particular, if X1, . . . , Xn are independent standard normal random variables, then for any > 1, (cid:18) max i=1,...,n Xi c(cid:112)2 log (cid:19) n1c . Lemma I.3 (Bernsteins inequality). Let X1, . . . , Xn be independent random variables with XiE[Xi] for all [n]. Then for any δ (0, 1), (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) i=1 Xi EXi (cid:114) (cid:12) (cid:12) (cid:12) (cid:12) 2 n1 (cid:80)n i=1 Var[Xi] log δ1 (cid:19) + log δ1 3n 1 δ. Lemma I.4. Let X1, . . . , Xn be independent standard normal random variables and define Mn = max1in Xi. Then for any fixed ϵ (0, 1) and all sufficiently large with 2(1 ϵ)2 log 1 that (cid:16) Mn (1 ϵ)(cid:112)2 log (cid:17) (cid:32) exp n2ϵϵ2 π log 3 (cid:33) . Proof. Since X1, . . . , Xn i.i.d. (0, 1), it holds for any P(Mn x) = (1 Φ(x))n, where Φ(x) is the standard normal tail distribution function. In order to upper bound (1 Φ(x))n when = (1 ϵ) 2 log n, we use well-known lower bound for the Gaussian tail. Specifically, for all > 0 (see, e.g., Ledoux and Talagrand (2013) or Boucheron et al. (2013)), Φ(x) (x) := 1 + x2 1 2π ex2/2. Hence, further applying the fact that 1 (x) exp((x)), we get P(Mn x) (cid:0)1 (x)(cid:1)n exp(cid:0)n(x)(cid:1). Now, for = (1 ϵ) 2 log n, we have 1 + x2 = (1 ϵ) 2 log 1 + 2(1 ϵ)2 log 2 3(1 ϵ) , log 132 where the inequality holds for sufficiently large such that 2(1 ϵ)2 log 1. Thus, (x) 1 3(1 ϵ) π log n(1ϵ)2 ."
        },
        {
            "title": "Substituting this lower bound into our earlier inequality gives",
            "content": "(cid:16) Mn (1 ϵ)(cid:112)2 log (cid:17) (cid:33) n1(1ϵ) (cid:32) exp (cid:32) = exp 1 3(1 ϵ) n2ϵϵ2 π log 3 π log (cid:33) . This completes the proof. I.2 Efron-Stein Inequalities Let be function of independent random variables X1, . . . , Xn with domain : = (X1, . . . , Xn), (I.1) where : is measurable function. Let Define the modified versions of where one coordinate is replaced by its independent copy: be independent copies of X1, . . . , Xn. 1, . . . , Z(i) = (X1, . . . , Xi1, i, Xi+1, . . . , Xn). Define the deviation terms: V+ = = (cid:34) (cid:88) i=1 (cid:34) (cid:88) i=1 (Z Z(i))2 1{Z > Z(i)} (Z Z(i))2 1{Z < Z(i)} X1, . . . , Xn X1, . . . , Xn (cid:35) (cid:35) , . (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (I.2) The following lemma is borrowed from Theorem 5 in Boucheron et al. (2003) for the case where V+ is dominated by some linear transformation of Z. Lemma I.5 (Efron-Stein for dominated variance). For and V+ defined in (I.1) and (I.2), respectively, suppose that there exist positive constants and such that V+ aZ + b. Then there is universal constant > 0 such that for any δ (0, 1), with probability at least 1 δ, E[Z] + (cid:112)(a E[Z] + b) log(1/δ) + log(1/δ). The following lemma is borrowed from Theorem 2 in Boucheron et al. (2003). Lemma I.6 (Efron-Stein for the moment generating function). For all θ > 0 and λ (0, 1/θ), log [exp (λ(Z E[Z]))] λθ 1 λθ log (cid:20) exp (cid:18) λV+ θ (cid:19)(cid:21) . On the other hand, for all θ > 0 and λ (0, 1/θ), log [exp (λ(Z E[Z]))] λθ 1 λθ log (cid:20) exp (cid:18) λV θ (cid:19)(cid:21) . The following lemma is borrowed from Lemma 11 in Boucheron et al. (2003) for transforming the upper bound on moment generating function (MGF) bound into an exponential tail bound. Lemma I.7. Suppose for any λ (0, 1/a), there exists constant > 0 such that: log E[exp(λ(Z E[Z]))] λ2V 1 λa . Then there exists some universal constant such that for any δ (0, 1), with probability at least 1 δ: E[Z] (cid:112)V log(1/δ) + log(1/δ). With the above lemmas, we can derive the following Efron-Stein inequality for sub-exponential variance. Lemma I.8 (Efron-Stein inequality for sub-exponential variance). Suppose either of the following two conditions is satisfied: 1. The variance V+ for satisfies that E[exp(λV+)] E[exp(λV +)] for any λ > 0, where + is a-subexponential with (0, 1): Q(v) := P(V + > + v) exp(v/a). when V+ exceeds some threshold > 0. 2. The moment generating function of V+ satisfies log E[exp(λV+)] λV + λa 1 aλ for some > 0, 0 < < 1 and any 0 < λ < a1/2. Then, with probability at least 1 δ, it holds that E[Z] (cid:112) log(δ1) + log(δ1). Similarly, if satisfies either of the two conditions, then with probability at least 1 δ, it holds that E[Z] (cid:112) log(δ1) + log(δ1). Proof. We just prove the first condition and the second condition can be implied by the proof. We explicit calculate the MGF for V+. The case for can be handled similarly. Take parameter λ (0, a1/2), we have for the moment generating function of V+ that E[exp(λV+)] = exp(λV ) (cid:18) lim v0 Q(v) + λ (cid:90) 0+ exp(cid:0)λ v(cid:1) Q(v)dv (cid:19) (cid:18) exp(λV ) 1 + λ (cid:90) exp(cid:0)(a1 λ) v(cid:1)dv (cid:19) (cid:18) = exp(λV ) 1 + 0+ λ a1 λ (cid:19) (cid:18) = exp(λV ) 1 + (cid:19) . λ 1 aλ where we use 0+ and 0 to denote the limit from the right and left side of 0, respectively. Here, in the first line, we use integration by parts to obtain an integration term with respect to the tail 134 probability Q(v). In the final line, we have the denominator 1 aλ > 0 since λ < a1/2 < a1 for (0, 1). Taking the logarithm on both side, we obtain that log E[exp(λV+)] λV + log(1 + λa/(1 aλ)) λV + λa 1 aλ . Now, we apply Lemma I.6 with λ replaced by λ/θ for some θ (aλ, λ1) to obtain that log E[exp(λ(Z E[Z]))] λθ 1 λθ (cid:104) log exp (cid:16) λV+ θ (cid:17)(cid:105) λ2 1 λθ (cid:16) + (cid:17) 1 aλ/θ λ2(V + a) (1 λθ)(1 aλ/θ) . Note that such θ exists since λ < a1/2. In particular, we have by the constraint on λ that and further restrict ourselves to λ < a1/2/2 to obtain that aλ < < λ1. Let us just pick θ = log E[exp(λ(Z E[Z]))] λ2(V + a) (1 λ a)2 λ2(V + a) a) (1 2λ . Now, we invoke Lemma I.7 and conclude that there exists universal constant > 0 such that E[Z] (cid:112) (V + a) log(δ1) + log(δ1) 2C (cid:0)(cid:112) log(δ1) + log(δ1)(cid:1). similar bound holds for the lower tail with the condition on V. Hence, we complete the proof. Lemma I.9 (Efron-Stein inequality for bounded variance). Suppose that max{V+, V} V0 with probability at least 1 exp(a) for some > nc1 and V0 > nc2 for some universal constant c1, c2 > 0. Also assume that max{V+, V} is uniformly bounded by V1 with V1 nc3 for some universal constant c3 > 0. Then, with probability at least 1 δ, it holds that E[Z] (cid:0)(cid:112) V0 log(δ1) + (cid:112) a1V1 log(δ1)(cid:1). Proof. By Lemma I.6, we have for the moment generating function (MGF) of V+ that log E[exp(λ(Z E[Z]))] λθ 1 λθ λθ 1 λθ λ 1 λθ (cid:104) log exp (cid:16) exp log (cid:18) (cid:17)(cid:105) (cid:16) λV+ θ (cid:17) (cid:16) λV0 θ λV0 + θ exp (cid:17) + exp (cid:16) λV1 θ (cid:16) λ(V1 V0) θ (cid:17) P(V+ V0) (cid:17)(cid:19) . In the following, we take θ = 2λ(V1 V0)/a, and the above upper bound can be simplified as log E[exp(λ(Z E[Z]))] λ2 1 2λ2(V1 V0)/a λ2 1 λ(cid:112)2(V1 V0)/a (cid:18) V0 + (cid:18) V0 + (cid:19) exp(a/2) 2λ2(V1 V0)/a exp(a/2) 2λ2(V1 V0)/a (cid:19) . Similarly for V, we also have log E[exp(λ(Z E[Z]))] λ2 1 λ(cid:112)2(V1 V0)/a (cid:18) V0 + exp(a/2) 2λ2(V1 V0)/a (cid:19) . 135 Therefore, in the following, we only need to consider the upper tail and the lower tail can be directly implied. As long as 1/(2λ2(V1 V0)/a) is polynomially in n, we will have exp(a/2)/(2λ2(V1 V0)/a) V0. Take to be the deviation of from its mean, i.e., = E[Z], we have By Lemma 11 of Boucheron et al. (2003), we conclude by using the Chernoff bound that log P(Z E[Z] t) (cid:26) λ2 2V0 1 λ(cid:112)2(V1 V0)/a (cid:27) tλ λ(0, inf a/2(V1V0)) t2 2(4V0 + t(cid:112)2a1(V1 V0)/3) , where the last inequality holds as long as satisfies (cid:16) 1 + 1 t(cid:112)2a1(V1 V0) 2V (cid:17)1/2 exp(a/4) V0 . (I.3) The lower bound holds similarly. sufficient condition for (I.3) to hold is 8 exp(a/4) (cid:112)2a1(V1 V0) . This condition will be automatically satisfied if we pick = ((cid:112)V0 log(δ1) + (cid:112)a1(V1 V0) log(δ1). Therefore, we conclude that with probability at least 1 δ, it holds that E[Z] (cid:0)(cid:112) V0 log(δ1) + (cid:112) a1V1 log(δ1)(cid:1). This completes the proof. Lemma I.10. Let = (w1, w2, . . . , wd) be random vector, and let w(i) denote the vector where the i-th coordinate wi is replaced by an independent copy i, while all other coordinates remain unchanged. Suppose that : Rd and : Rd are both nondecreasing/nonincreasing functions with respect to the coordinate wi. Then, we have the inequality: E(cid:2)f (w)g(w)(cid:3) E(cid:2)f (w)g(w(i))(cid:3). Proof of Lemma I.10. By the monotonicity of and g, we have: (f (w) (w(i))) (g(w) g(w(i))) 0. Expanding the product and taking expectations, we obtain: E(cid:2)f (w)g(w)(cid:3) E(cid:2)f (w)g(w(i))(cid:3) E(cid:2)f (w(i))g(w)(cid:3) + E(cid:2)f (w(i))g(w(i))(cid:3) 0. By the symmetry of expectations, the first and last terms are equal, and the second and third terms are also equal, so we obtain the desired inequality: E(cid:2)f (w)g(w)(cid:3) E(cid:2)f (w)g(w(i))(cid:3). This completes the proof."
        }
    ],
    "affiliations": [
        "Antai College of Economics and Management, Shanghai Jiao Tong University",
        "Department of Statistics and Data Science, Yale University",
        "Toyota Technological Institute at Chicago"
    ]
}