{
    "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
    "authors": [
        "Yong Lin",
        "Shange Tang",
        "Bohan Lyu",
        "Jiayun Wu",
        "Hongzhou Lin",
        "Kaiyu Yang",
        "Jia Li",
        "Mengzhou Xia",
        "Danqi Chen",
        "Sanjeev Arora",
        "Chi Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves a 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 4 6 7 0 . 2 0 5 2 : r Technical Report Goedel-Prover: Frontier Model for Open-Source Automated Theorem Proving Yong Lin Shange Tang Bohan Lyu Jiayun Wu Hongzhou Lin Kaiyu Yang Jia Li Mengzhou Xia Danqi Chen Sanjeev Arora Chi Jin"
        },
        {
            "title": "Abstract",
            "content": "We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build large dataset of formal proofs by training series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. The final prover outperforms all existing open-source models in whole-proof generation. On the miniF2F benchmark, it achieves 57.6% success rate (Pass@32), exceeding the previous best open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in reasoning tasks, especially in solving mathematical problems (Guo et al., 2025; Yang et al., 2024a). These models excel at reasoning through natural language, which we refer to informal reasoning. However, natural language-based reasoning is difficult to automatically verify by machines, which undermines the reliability of informal reasoning in practical applications. This also makes it more difficult to further improve the reasoning capabilities of language models. In contrast to informal reasoning, formal reasoning allows reasoning in machine-verifiable format, opening up new possibilities for verification and automation. In particular, proof assistants such as Lean (De Moura et al., 2015; Moura & Ullrich, 2021), Isabelle (Paulson, 1994), and Coq (Barras et al., 1997) provide formal languages that can express reasoning in way that can be mechanically verified. Thus, it is of great interest to train LLMs to write proofs in these formal languages. significant challenge in training LLMs for theorem proving in formal languages is the scarcity of formalized math statements and proofs. Writing proofs for theorems expressed in formal languages is highly demanding and necessitates considerable domain expertise. YL and ST contribute equally to this work. {yong.lin,shangetang}@princeton.edu Princeton Language and Intelligence, Princeton University. Tsinghua University. Amazon. This work is independent of and outside of the work at Amazon. Meta FAIR. KY served an advisory role. All experiments were conducted on PLI servers. Numina. 1 Technical Report (Left) The performance of Pass@32 for whole-proof generation on miniF2F Figure 1: comparing with previous SOTA models. (Middle) comparison of Goedel-Prover-SFT and DeepSeek-Prover-V1.5 in terms of miniF2F performance across different inference budgets, ranging from Pass@32, 64, 128, ..., to 4 6400. (Right) Goedel-Prover-SFT solve 29.7K problems in Lean Workbook. Previously, InternLM2.5-Step-Prover (Wu et al., 2024) and InternLM-Math-Plus (Ying et al., 2024b) collectively solved 15.7K samples. Therefore, existing publicly available datasets for formal languages are limited in size. For example, the Lean Workbook (including Lean Workbook Plus) dataset (Ying et al., 2024a; Wu et al., 2024) comprises total of 140K formal statements, where formal statements refer to problem statements in Lean without proofs. However, only 15.7K of these statements come with formal proofs, which were found by InternLM2.5-StepProver and InternLM-Math-Plus (Ying et al., 2024a; Wu et al., 2024; Ying et al., 2024b). Additionally, the Open Bootstrapped Theorems dataset (Wang et al., 2024) includes 107K statements with proofs sourced from Mathlib44 (mathlib4, 2023). However, we observe notable difference in the distribution of Mathlib4 compared to that of general problem-solving benchmarks, such as the widely used miniF2F (Zheng et al., 2021). For instance, statements in miniF2F are primarily from high school and require complex reasoning abilities to solve, whereas those in Mathlib4 focus on the simple manipulation of advanced mathematical concepts. (see Figure 7 and 8 in the appendix for comparison). Furthermore, we find that incorporating Mathlib4 data into training does not consistently improve performance on miniF2F (Section 4). In contrast to the scarcity of data in formal languages, there is vast amount of math problems and solutions written in informal language. For example, Numina (Li et al., 2024a) includes 860K high-quality question and answer pairs sourced from MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC (aop), AIME (MAA, 2024), the AoPS Forum (aop), Chinese K-12 Exams (Shao et al., 2024), World Olympiads, and synthetic data (Mitra et al., 2024). We start by training LLMs to formalize the problem statements in these datasets into Lean language. To increase the diversity of the formalization styles, we train two formalizers. One is trained on informal and formal (I-F) statement pairs from Lean Workbook, while the other is trained on I-F statement pairs annotated by Claude-sonnet-3.5 (Anthropic, 2024). We use these two formalizers to formalize the statements and then employ LLMs to ensure that the formal statements preserve the content of the informal statements. Our efforts result in 1.64 million formal statements. Using this extensive dataset of formal statements, we employ expert iteration (Polu et al., 2022) to train the prover to generate proofs. Notably, we train model to generate complete proofs solely based on statements, without interacting with the Lean compiler during the generation process. This approach is referred to as the whole-proof generation method (Jiang et al., 2022; Wang et al., 2024; Xin et al., 2024a;b). At the beginning of the expert iteration, we generate 16 proof candidates using DeepSeek-Prover-V1.5-RL (the previous SOTA) for each formal statement in our dataset, and then we verify the correctness of each candidate using Lean compiler. The correct proofs are then collected to train our iter-1 prover based on DeepSeek-Prover-V1.5-Base. In subsequent rounds, we utilize our iter-k prover to collect new proofs and add them to the training data. We then perform supervised fine-tuning starting from DeepSeek-Prover-V1.5-Base for another round, resulting in the iter-(k + 1) 2 Technical Report prover. We conduct total of 8 iterations and observe consistent improvement starting from the first iteration. With our large-scale formalized statements, we demonstrate that expert iteration can lead to SOTA performance in whole-proof generation. Specifically, Our model outperforms DeepSeek-Prover-V1.5-RL (the previous SOTA model) by 7.6% on miniF2F, achieving Pass@32 score of 57.6% compared to DeepSeek-ProverV1.5-RLs 50.0%, as shown in Figure 1 (left). It consistently surpasses DeepSeekProver-V1.5-RL across all sampling budgets, including Pass@32, 64, and up to 25600, as shown in Figure 1 (middle). We have cumulatively solved 29.7K problems in Lean Workbook, significantly increasing the existing 15.7K proofs found by InternLM2.5-StepProver and InternLMMath-Plus (Wu et al., 2024; Ying et al., 2024b), as shown in Figure 1 (right). Our model solves 7 problems on PutnamBench by Pass@5121, securing the #1 position on the leaderboard  (Table 4)  . We open source our code2, model3, and the new proofs discovered4 in the Lean Workbook to facilitate future research."
        },
        {
            "title": "2 Related Work",
            "content": "Automated theorem proving. Automated theorem proving (ATP) is long-standing problem in symbolic AI (Robinson & Voronkov, 2001). Traditional approaches represent theorems in first-order logic and prove them using decision procedures (De Moura & Bjørner, 2008; Barbosa et al., 2022) and search (Kovacs & Voronkov, 2013; Schulz et al., 2019). The proof search has been enhanced by replacing handcrafted heuristics with machine learning techniques (Urban et al., 2011; Kaliszyk et al., 2018). However, approaches based on firstorder logic struggle to scale to complex theorems and often do not yield human-readable proofs. In recent years, learning-based theorem proving has undergone significant transformation. notable approach, introduced by Polu & Sutskever (2020), involves leveraging large language models to assist in theorem proving with proof assistants such as Lean (De Moura et al., 2015; Moura & Ullrich, 2021) and Isabelle (Paulson, 1994). Follow-up research has explored various avenues, such as retrieving useful lemmas (Irving et al., 2016; Mikuła et al., 2024; Yang et al., 2024b), utilizing Monte Carlo tree search for proof discovery (Lample et al., 2022), and harnessing the capabilities of large language models (LLMs) for natural language reasoning (Jiang et al., 2022; Lin et al., 2024). Notably, Polu et al. (2023) was the first to apply expert iteration (Anthony et al., 2017) to theorem proving. This method alternates between two phases: (1) attempting to prove unsolved theorems and (2) enhancing the prover by incorporating newly discovered proofs into its training data. Expert iteration has yielded significant improvements in several recent provers (Wu et al., 2024; Xin et al., 2024b), including our Goedel-Prover. Most automated theorem provers operate in stepwise manner, generating individual proof steps that are then assembled into complete proofs using proof search algorithms. Recently, researchers have shown that generating entire proofs is feasible (First et al., 2023; Xin et al., 2024a; Wang et al., 2024). This approach avoids the costly search process, resulting in lower latency and potentially offering more efficient use of computational resources during testing. While Goedel-Prover also generates whole proofs, our data and methodology can, in principle, be adapted to develop stepwise provers as well. 1We initially solved 8 problems on PutnamBench. However, after discussing with the authors of PutnamBench, we discovered that one of the problems was mis-formalized. Therefore, this problem is not included in our count, and we report total of 7 problems here. 2https://github.com/Goedel-LM/Goedel-Prover 3https://huggingface.co/Goedel-LM/Goedel-Prover-SFT 4https://huggingface.co/datasets/Goedel-LM/Lean-workbook-proofs Technical Report Figure 2: This figure illustrates the training of the formalizers. The term F-I statement pairs refers to pairs consisting of Formal and Informal (F-I) statements. An example is shown on the right part. We train two formalizers, Formalizer and B, using F-I statement pairs sourced from various origins. Autoformalization and synthetic data generation. The shortage of high-quality formal mathematical data poses significant bottleneck in training theorem-proving models. While techniques like reinforcement learning may reduce the reliance on human-written proofs (Google DeepMind, 2024), there remains need for substantial number of formal theorem statements. promising approach is to synthesize formal statements through autoformalization, where large language models (LLMs) translate informal mathematical statements into formal ones (Wu et al., 2022; 2024; Xin et al., 2024a;b). DeepSeek-Prover (Xin et al., 2024a) and InternLM2.5-StepProver (Wu et al., 2024) have successfully implemented this strategy to formalize large volume of statements into Lean for expert iteration. We adopt similar approach. The difference is: while Liu et al. (2024) focuses on formalizing their internal dataset, we concentrate on formalizing the Numina dataset (Li et al., 2024a) alongside privately collected dataset. Additionally, we train two formalizers to enhance the diversity of formalization styles, which we demonstrate to be beneficial in Section 4."
        },
        {
            "title": "3 Method",
            "content": "We begin by translating informal statements (expressed in natural language) into formal statements (represented in Lean). Using these formal statements, we iteratively train our prover with proofs generated by the prover and verified by the Lean compiler. The details of each step are elaborated in the following parts. 3.1 Statement Formalization We first train the statement formalizers to translate informal statements in Numina into formal statements as shown in Figure 2. To enhance the diversity of formalized statements, we train two models to formalize informal statements. Formalizer A: We train the Formalizer model using Formal and Informal (F-I) statement pairs sourced from Lean Workbook. Formalizer B: We employ Claude-sonnet-3.5 to formalize 230K statements from Numina. From this set, we extract 170K statements that successfully passed Lean compilation. These 170K F-I statement pairs are then used to train Formalizer B. Both Formalizer and are trained using supervised fine-tuning with Qwen2.5-Coder-32B 5. The training of these two formalizers takes less than 24 hours on 8 H100 GPUs. Table 1 presents two examples in which both Formalizer and Formalizer yield reasonable formalizations. However, our final prover exhibits varying performance on these formalized statements, highlighting the influence of formalization style on model effectiveness. 5https://huggingface.co/Qwen/Qwen2.5-Coder-32B 4 Technical Report Figure 3: This figure illustrates the process of expert iteration. Each time, we utilize our iter-k prover to collect new proofs and add them to the training data. We then conduct supervised fine-tuning starting from DeepSeek-Prover-V1.5-Base for another round, resulting in the iter-(k + 1) prover."
        },
        {
            "title": "Informal\nStatement",
            "content": "Example 1 The function (x) = 2x + 3x2 + ax + 1 is an even function, then equals = 0. Example 2 If and log10 are real numbers and log10 < 0, show that 1 < < 0."
        },
        {
            "title": "Formalizer A\nOutput",
            "content": "Pass rate: 14/16 Pass rate: 0/"
        },
        {
            "title": "Formalizer B\nOutput",
            "content": "Pass rate: 0/16 Pass rate: 5/16 Table 1: Comparison of formalizer outputs for two examples. Comparison of Formalizer Outputs for Two Examples. In Example 1, Formalizer defines the even function directly by stating (x) = (x). In contrast, Formalizer first introduces function called IsEven and then defines the even function using IsEven. Notably, our prover successfully solves the statements provided by Formalizer but fails with those from Formalizer B. Example 2 is similar; however, our prover fails to solve the statement provided by Formalizer but succeeds with the one from Formalizer B. Quality assessment. We employ two tests to assess the quality of the formalized statements. First, the formalized statement must conform to Lean syntax and can successfully compile, with the potential proof replaced by the placeholder := by sorry. This syntax check is known as the Compiling Correctness (CC) Test in the literature (Ying et al., 2024a). Second, the formalized statement must accurately capture the original informal problem, incorporating all assumptions, conditions, and implicit definitions. We refer to this second test as the Faithfulness and Completeness (FC) Test. For the FC test, we use Qwen2.5-72BInstruct6 with the prompts shown in Figure 4. For each formalized statement, we generate four independent judgments, and the FC score is calculated as #{Appropriate in four Judgments}/4. For example, if the four judgments produced by Qwen2.5-72B-Instruct 6https://huggingface.co/Qwen/Qwen2.5-72B-Instruct Technical Report Figure 4: Prompts for Faithfulness and Completeness (FC) Test. Pass Formalizer Formalizer Model CC Test CC Test FC Test FC Test Pass@1 Pass@8 Pass@1 Pass@8 CC + FC Test Pass@1 CC + FC Test Pass@8 76.74% 95.93% 48.06% 88.01% 45.72% 82.33% 88.48% 98.59% 80.42% 97.22% 76.41% 95.78% Table 2: Quality assessment of the formalized statement include three Appropriate and one Inappropriate, the overall FC score is calculated as 0.75. We filter out formalized statements with an FC score less than 0.5. For each informal statement in Numina, we generate eight formalized statements from each formalizer, resulting in 16 formalized statements per problem. Each statement undergoes the CC and FC Test, and we retain only those valid statements. We then randomly select one valid statement from each formalizer. For example, if five out of eight statements from Formalizer and three from Formalizer are valid, we randomly choose one from each. If formalizer produces no valid statements, we exclude all its statements for that problem. The statistics for each test conducted on both formalizers are summarized in Table 2. In addition to formalizing the 860K open-sourced Numina (Li et al., 2024a) datasets, we also formalize private 68K collection of math problems from Art of Problem Solving (AOPS), which has been collected and processed by the Numina group (Li et al., 2024a). Out of total of 928K informal statements, 760K have two valid formalized statements generated by Formalizer and B, while 123K contain only one valid formalized statement. After formalizing both the Numina and AOPS datasets, we further incorporate 140K statements from Lean Workbook, including Lean Workbook Plus. As result, we have total of 1.78M formal statements. 6 Technical Report 3.2 Expert Iteration After obtaining large collection of formalized statements in Section 3.1, we employ expert iteration to train the prover (Liu et al., 2024; Wu et al., 2024; Li et al., 2024b), which is illustrated in Figure 3. Specifically, we first utilize DeepSeek-Prover-V1.5-RL7 to generate 16 proofs for each statement. We then verify these proofs with the Lean compiler. If at least one proof solves the statement, we retain one proof per statement. In cases where multiple proofs are available, we randomly sample one solution. These collected proofs are used for supervised fine-tuning (SFT) based on DeepSeek-Prover-V1.5-Base8, resulting in the iter-1 prover. We continue this expert iteration process; each time, we use the iter-k prover to generate answers and cumulatively collect correct solutions to train DeepSeek-Prover-V1.5Base for the next iteration, the iter-(k + 1) prover. Refer to Appendix for more details on each iteration. We experiment with learning rates of 1 104 and 5 105, training for either 1 or 2 epochs. We use the packing trick (Tunstall et al., 2022) with small batch size of 8 to speed up the training. In each iteration, the training time for 1 epoch is approximately 12 hours using 4 H100 GPUs. The inference time for the 1.78M statements set by Pass@16 is 6 hours, utilizing 64 H100 GPUs. Additionally, the verification time for these proofs requires 10 hours with 8,000 CPUs."
        },
        {
            "title": "4 Results",
            "content": "Benchmarks. Following the works of (Wang et al., 2024; Xin et al., 2024a; Wu et al., 2024; Li et al., 2024b), we primarily use miniF2F (Zheng et al., 2021) as our main evaluation benchmark. We also track the problems solved by our prover in Lean Workbook (Ying et al., 2024a) and investigate the performance on ProofNet (Azerbayev et al., 2023) and PutnamBench (Tsoukalas et al., 2024). Additionally, we uniformly sample subset from our formalized dataset to create held-out evaluation dataset. Below, we provide descriptions of each dataset. miniF2F (Zheng et al., 2021) is formal theorem proving benchmark, consisting of 488 problem statements (244 validation and 244 test problems) in Lean. The problems are drawn from high-school exercises, as well as high-school level competitions including the AIME, AMC, and the International Mathematical Olympiad (IMO). The original benchmark was released in Lean 3, and for our analysis, we use the version of miniF2F in Lean 4.9.0 provided by Xin et al. (2024a). ProofNet (Azerbayev et al., 2023) is formal theorem proving benchmark of undergraduate-level mathematics, consisting of 371 problem statements in Lean (185 validation and 186 test problems). The problems are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology. The original benchmark was released in Lean 3, and for our analysis, we use the version of ProofNet in Lean 4.9.0 provided by Xin et al. (2024a). Lean Workbook (Ying et al., 2024a) is large-scale Lean 4 problem set formalized from natural language math problems (mainly from the forum AOPS), which consists of 140K statements in Lean 4. We also monitor the problems solved by our model during the expert iteration process. Notably, the problem set from Lean Workbook is included in this training, which is consistent with DeepSeek-ProverV1.5 (Xin et al., 2024a) and InternLM2.5-StepProver (Wu et al., 2024). PutnamBench (Tsoukalas et al., 2024) is formal theorem proving benchmark on competition mathematics problems sourced from the William Lowell Putnam Mathematical Competition years 1962 - 2023. PutnamBenchcomprises 644 Lean 4 statements, covering algebra, analysis, number theory, geometry, combinatorics, probability and set theory. 7https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL 8https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base 7 Technical Report Whole-Proof Generation Model Pass Performance TheoremLamma (Wang et al., 2024) Deepseek-Prover-V1 (Xin et al., 2024a) DeepSeek-Prover-V1.5-SFT (Xin et al., 2024b) DeepSeek-Prover-V1.5-RL (Xin et al., 2024b) Goedel-Prover-SFT DeepSeek-Prover-V1.5-SFT (Xin et al., 2024b) DeepSeek-Prover-V1.5-RL (Xin et al., 2024b) Goedel-Prover-SFT DeepSeek-Prover-V1.5-SFT (Xin et al., 2024b) DeepSeek-Prover-V1.5-RL (Xin et al., 2024b) Goedel-Prover-SFT 128 32 32 32 32 33.6% 46.1% 0.5% 48.2% 0.6% 50.0% 0.5% 57.6% 0.7% 3200 3200 3200 4 6400 4 6400 4 6400 53.3% 54.9% 62.7% 55.8% 58.5% 64.7% Table 3: Whole-proof generation performance on miniF2F. Figure 5: The figures show the performance of our model on the four datasets at each iteration. We gradually increase the size of the problem set and add more training data. The details of each iteration are shown in Table 8. NuminaTest. We randomly sample 250 statements from our formalized Numina dataset and use it as held-out testing set. We refer to this subset as NuminaTest. Main results. The performance on miniF2F is shown in Table 3. The Pass@32 performance of our Goedel-Prover-SFT is 57.6%, surpassing the previous SOTA open source model, DeepSeek-Prover-V1.5-RL, by 7.6%. We observe that our Goedel-Prover-SFTs Pass@32 is even better than DeepSeek-Prover-V1.5-RLs Pass@3200 by 2.7%. Furthermore, when both evaluated by Pass@3200, our model achieves 62.7%, surpassing DeepSeek-Prover-V1.5RLs 54.9% by 7.8%. Figure 1 illustrates the inference time scaling curve for our GoedelProver-SFT, DeepSeek-Prover-V1.5-RL and DeepSeek-Prover-V1.5-SFT. Goedel-Prover-SFT demonstrates significant improvements over both DeepSeek-Prover-V1.5-RL and DeepSeekProver-V1.5-SFT across all inference compute budgets. Figure 5 illustrates the performance of our model during each iteration. Overall, we observe relatively consistent improvement in performance across iterations. Technical Report Figure 6: This figure illustrates the correlation of model performance across four datasets. lower value indicates weaker correlation between the models performance on two datasets. Notably, ProofNet shows very low correlation with the other three datasets. PutnamBench performance. Goedel-Prover-SFT solves 7 out of 644 problems in PutnamBench (Pass@512), achieving the first place on the PutnamBench leaderboard. The previous SOTA method ABEL (Gloeckle et al.) solves 7 with slightly higher inference budget (Pass@596) and InternLM2.5-Step-Prover (Wu et al., 2024) solves 6 (Pass@2 32 600). The performance is summarized in Table 4. ProofNet performance. We observe that the performance on ProofNet exhibits very low correlation with miniF2F, Lean Workbook and NuminaTest. Specifically, Figure 6 presents the correlation of model performance across the four datasets. The models performance on NuminaTest, miniF2F, and Lean Workbook exhibits strong positive correlations, whereas its performance on ProofNet shows notably low correlation with the other three datasets. We also observe that the data distribution in ProofNet is significantly different from that of the other three datasets. The problems in ProofNet are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology. These topics largely rely on the abstract and general formulations of mathematical definitions in Mathlib4 (mathlib4, 2023). Other datasets, e.g., miniF2F, largely consist of competition and Olympic-style problems, which require complex reasoning, while only depending on relatively small set of elementary facts about integers, real numbers, counting, and geometry. We show two examples in Table 5 to illustrate the style difference between ProofNet and miniF2F. Ranking Model Type Num-solved Compute (Pass) 1 1 3 3 5 6 7 8 9 Goedel-Prover-SFT Whole-Proof Generation Tree Search Method ABEL (Gloeckle et al.) Goedel-Prover-SFT Whole-Proof Generation InternLM2.5-StepProver (Wu et al., 2024) Tree Search Method InternLM 7B (Ying et al., 2024b) Whole-Proof Generation Whole-Proof Generation GPT-4o COPRA (GPT-4o) (Thakur et al., 2023) Whole-Proof Generation ReProver w/ retrieval (Yang et al., 2024b) Whole-Proof Generation ReProver w/o retrieval (Yang et al., 2024b) Whole-Proof Generation 7 7 6 6 4 1 1 0 0 512 596 32 232600 4096 10 1 1 Table 4: Number of problems solved on PutnamBench statements (out of 644). The performance numbers for existing works are taken from the leaderboard. Here inidicates open-source models. Including Mathlib4 in the training data. Given the close connection between Mathlib4 and ProofNet, we investigate the inclusion of Mathlib4 in the training dataset, following the approaches of Xin et al. (2024b) and Wang et al. (2024). Our results in Table 6 indicate 9 Technical Report Example from ProofNet Example from miniF2F Informal Statement Prove that no order can be defined in the complex field that turns it into an ordered field. Show that for any natural number n, 7 does not divide 2n + 1. Formal Statement Comments This problem involves the notion of order, which is undergraduate level. Its formal statement uses the definition IsLinearOrder in Mathlib4. This problem comes from IMO but only involves division. Table 5: Comparison of Examples from ProofNetand miniF2F. ProofNet largely relies on the abstract and general formulations of mathematical results in Mathlib4. In contrast, miniF2F largely consists of high-school competition and Olympic style problems, which require complex reasoning. that while adding Mathlib4 enhances the models performance on ProofNet, it also leads to decrease in miniF2F performance. Specifically, we analyze the impact of incorporating Mathlib4 into the training set during the sixth iteration, with the proofs generated by the 5th iteration referred to as Iter-5 proofs. We observe that including Mathlib4 increases ProofNet performance from 13.3% to 15.6%, but results in decline in miniF2F performance from 56.6% to 54.1%, as well as decrease in NuminaTest performance from 59.2% to 58.8%. We continue to include Mathlib4 in the training dataset from the 6th iteration 6 onward, consistent with DeepSeek-Prover-V1.5-RL (Xin et al., 2024b) and TheoremLamma (Wang et al., 2024). Additional details can be found in the Appendix A. Model Training Dataset miniF2F ProofNet NuminaTest Average Deepseek-RL Iter-6 prover Iter-6 prover Iter-5 proofs Iter-5 proofs + Mathlib4 50.0% 56.6% 54.1% 16.0% 13.3% 15.6% 53.6% 59.2% 58.8% 39.9% 43.0% 42.8% Table 6: Investigation on ProofNet at the 6th iteration. The results indicate that incorporating Mathlib4 into the training data enhances performance on ProofNet but reduces performance on miniF2F and NuminaTest. We continue to include Mathlib4 in the training dataset from iteration 6 onward, in line with Xin et al. (2024a). Details are in Appendix A. Proofs found in Lean Workbook. The Lean Workbook, which includes Lean Workbookplus (Ying et al., 2024a; Wu et al., 2024), formalizes 140K high-quality problems sourced from AOPS and the Compfiles data. Currently, proofs for only 15.7K statements in Lean Workbook have been found and made open-source by InternLM2.5-StepProver (Wu et al., 2024) and InternLM-Math-Plus (Ying et al., 2024b). In contrast, our model has discovered significantly larger set of proofs within Lean Workbook, cumulatively solving 29.7K problems. Figure 1 (right) shows comparison of the number of proofs found by our model and prior works. We open-source all the proofs found by our model to benefit the research community. Ablation on two formalizers. In Section 3.1, we discuss that the style of statement formalization can impact the provers performance. Therefore, we propose generating statements using two formalizers trained on data from different sources. In this section, we compare prover trained on single statement style (using only one formalizer) with prover trained on both styles (utilizing statements from both Formalizer and B). We conducted 10 Technical Report experiments by training the prover at the 8th iteration on statements with proofs collected by the iter-7 prover, and the results are presented in Table 7. The findings indicate that using both formalizers leads to improved performance. Formalization Model miniF2F ProofNet NuminaTest Average Formalizer only Formalizer only Formalizer and 56.5% 56.2% 57.6% 13.8% 15.2% 15.2% 59.6% 60.0% 61.2% 43.3% 43.8% 44.7% Table 7: An ablation study on using two formalizers to formalize the statements. The term Formalizer and refers to using statements formalized by both and B. In contrast, Formalizer only or Formalizer only indicates that we include statements formalized exclusively by Formalizer or B, respectively. Notably, in these experiments, we all add the Mathlib4 training data."
        },
        {
            "title": "5 Discussion",
            "content": "We delve into the characteristics of proofs generated by Goedel-Prover-SFT and discuss potential directions for improvement, particularly regarding the proof style adopted by the model, the role of search as well as online interaction in proof generation, and the integration of external symbolic computation tools such as SymPy. The Proof Style. We observe that the proofs provided by Goedel-Prover-SFT often rely on high-level tactics such as nlinarith, simp all, and norm num, among others. These high-level tactics handle multiple reasoning steps internally, delegating the resolution of intermediate steps to their built-in automation. For example, the nlinarith tactic can automatically solve certain linear and non-linear equalities and inequalities. Figure 9 shows typical proof generated by our prover. The first several steps involve only trivial transformations of the original problem, whereas the final line uses nlinarith to immediately achieve the goal. Whether this style of proof is sufficient for complex reasoning remains an important area for exploration. Search and online interaction. Currently, Goedel-Prover-SFT generates the entire proof for the problem at once, without receiving further feedback. While our current approach is appealing in terms of computation, incorporating search and interaction in future work could enhance performance. For example, once tactic is generated by our prover, it can interact with the Lean compiler to receive feedback on how the goal changes after the tactic is applied. This information can then be utilized in generating the next tactic, potentially improving the overall proof strategy (Wu et al., 2024). SymPy. Future work may aim to leverage other software packages to enhance Leans capabilities. For instance, Leans ring tactic can handle algebraic simplifications by applying axioms such as distributivity, associativity, and commutativity. However, combination of tactics is required for non-algebraic transformations of transcendental functions, such as logarithmic and trigonometric functions, and other advanced simplifications beyond commutative rings. We explored using Python-based computer algebra system, SymPy (Meurer et al., 2017), to simplify complex expressions in theorem statements and feed the simplified form into the prover. Specifically, we parse equations of the form = within the goals of Lean theorem statements, construct the SymPy expression B, and then apply the simplify method in Lean. This procedure directly solves 9.4% of miniF2F by simplifying the statements to 0 = 0. In addition, it solves 0.8% of the problems in miniF2F that were unsolved by Goedel-Prover-SFT with Pass@32, but did not improve Goedel-Prover-SFT with Pass@3200. Thus, SymPy simplification is not part of any of our reported results. However, we think such procedures need further exploration. 11 Technical Report"
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Haoyu Zhao, Hubert Strauss and Suozhi Huang for their helpful discussions."
        },
        {
            "title": "References",
            "content": "Art of problem solving wiki. https://artofproblemsolving.com/wiki/. Accessed: 2025-0124. Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Neural Information Processing Systems (NeurIPS), 2017. Anthropic. Claude 3.5 sonnet, 2024. claude-3-5-sonnet. URL https://www.anthropic.com/news/ Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward Ayers, Dragomir Proofnet: Autoformalizing and formally proving Radev, and Jeremy Avigad. undergraduate-level mathematics. arXiv preprint arXiv:2302.12433, 2023. Haniel Barbosa, Clark Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres otzli, et al. cvc5: versatile and industrial-strength smt solver. In International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 415442. Springer, 2022. Bruno Barras, Samuel Boutin, Cristina Cornes, Judicael Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria, 1997. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient smt solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems, pp. 337340. Springer, 2008. Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pp. 378388. Springer, 2015. Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation In ACM Joint European Software Engineering and repair with large language models. Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE), 2023. Fabian Gloeckle, Jannis Limperg, Gabriel Synnaeve, and Amaury Hayat. Abel: Sample efficient online reinforcement learning for neural theorem proving. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24. Google DeepMind. AI achieves mathematical olympiad problems. ai-solves-imo-problems-at-silver-medal-level/, 2024. silver-medal standard solving international https://deepmind.google/discover/blog/ Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 12 Technical Report Geoffrey Irving, Christian Szegedy, Alexander Alemi, Niklas Een, Francois Chollet, and Josef Urban. Deepmath-deep sequence models for premise selection. Advances in neural information processing systems, 29, 2016. Albert Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothee Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. arXiv preprint arXiv:2210.12283, 2022. Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and Miroslav Olˇsak. Reinforcement learning of theorem proving. In Neural Information Processing Systems (NeurIPS), volume 31, 2018. Laura Kovacs and Andrei Voronkov. First-order theorem proving and vampire. In International Conference on Computer Aided Verification, pp. 135. Springer, 2013. Guillaume Lample, Timothee Lacroix, Marie anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=J4pX8Q8cxHH. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13, 2024a. Yang Li, Dong Du, Linfeng Song, Chen Li, Weikang Wang, Tao Yang, and Haitao Mi. Hunyuanprover: scalable data synthesis framework and guided tree search for automated theorem proving. arXiv preprint arXiv:2412.20735, 2024b. Haohan Lin, Zhiqing Sun, Yiming Yang, and Sean Welleck. Lean-STaR: Learning to interleave thinking and proving. arXiv preprint arXiv:2407.10040, 2024. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL: https://maa.org/ math-competitions/american-invitational-mathematics-examination-aime. mathlib4. mathlib4: The math library of lean 4, 2023. URL https://github.com/ leanprover-community/mathlib4. Accessed: 2025-01-14. Aaron Meurer, Christopher Smith, Mateusz Paprocki, Ondˇrej ˇCertık, Sergey Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason Moore, Sartaj Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017. Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuci nski, Piotr Miłos, and Yuhuai Wu. Magnushammer: transformer-based approach to premise selection. In International Conference on Learning Representations (ICLR), 2024. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem prover and programming language. In International Conference on Automated Deduction (CADE), 2021. Lawrence Paulson. Isabelle: generic theorem prover. 1994. Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020. 13 Technical Report Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. arXiv preprint arXiv:2202.01344, 2022. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. In International Conference on Learning Representations (ICLR), 2023. Alan JA Robinson and Andrei Voronkov. Handbook of automated reasoning, volume 1. 2001. Stephan Schulz, Simon Cruanes, and Petar Vukmirovic. Faster, higher, stronger: 2.3. In International Conference on Automated Deduction (CADE), 2019. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Amitayush Thakur, Yeming Wen, and Swarat Chaudhuri. language-agent approach to formal theorem-proving. arXiv preprint arXiv:2310.04353, 2023. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. Putnambench: Evaluating neural theoremprovers on the putnam mathematical competition. arXiv preprint arXiv:2407.11214, 2024. Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. Natural language processing with transformers. OReilly Media, Inc., 2022. Josef Urban, Jiˇrı Vyskoˇcil, and Petr ˇStˇepanek. MaLeCoP machine learning connection prover. In International Conference on Automated Reasoning with Analytic Tableaux and Related Methods, 2011. Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, and Tong Zhang. Theoremllama: Transforming general-purpose llms into lean4 experts. arXiv preprint arXiv:2407.03203, 2024. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id= IUikebJ1Bf0. Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems. arXiv preprint arXiv:2410.15700, 2024. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024a. Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024b. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems, 36, 2024b. 14 Technical Report Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: large-scale lean problem set formalized from natural language math problems. arXiv preprint arXiv:2406.03847, 2024a. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024b. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110, 2021. 15 Technical Report"
        },
        {
            "title": "A Expert Iteration Details",
            "content": "The main training pipeline is illustrated in Section 3.2. When we implement the expert iteration algorithm, we gradually add the data. From iter-0 to iter-3, we gradually add the statements formalized by Claude-sonnet-3.5. At iter-3, we train the Formalizer and add the formalized statements generated by Formalizer for iter-4 to iter-6. At iter-7, we begin to add the statements generated by Formalizer A. We also add Mathlib4 data into the training dataset for better ProofNet performance when starting from iter-6. Iteration Statements Training Data Lean Workbook Formalized Lean Workbook Solved Formalized Solved Mathlib Iter-0 Iter-1 Iter-2 Iter-3 Iter-4 Iter-5 Iter-6 Iter-7 Iter-8 Iter-9 140K 140K 140K 140K 140K 140K 140K 140K 140K 140K 0 140K 270K 270K 882K 882K 882K 1.64M 1.64M 1.64M 20.6K 20.6K 23.0K 24.4K 25.4K 27.0K 27.8K 28.8K 29.7K 30.3K 0 72.4K 128.7K 161.2K 425.8K 436.5K 443.2K 887.7K 915.7K 928.2K 0 0 0 0 0 0 104K 104K 104K 104K Table 8: Expert iteration details."
        },
        {
            "title": "B More Examples",
            "content": "B.1 Mathlib4 example Figure 7 and 8 show the statement and proof in mathlib4 and miniF2F respectively. It can be easily seen that both the statement and proof rely on pre-defined objects. Unlike miniF2F statements, the example in Figure 7 can not even pass the lean compilation, given that pre-defined objects are missing. Figure 7: Mathlib4 example which relies on pre-defined objects @Acc.ndrec and @Acc.ndrecC Figure 8: miniF2F example which does not rely on pre-defined objects 16 Technical Report B.2 Proof Style The proofs from Goedel-Prover-SFT often rely on high-level tactics like nlinarith, simp all, and norm num, which handle complex equalities and inequalities while leaving intermediate steps to powerful methods. As shown in Figure 9, initial steps typically involve trivial transformations, with the final step quickly resolving the goal using nlinarith. The effectiveness of this proof style for complex reasoning is still under exploration. Figure 9: Example of proof style, where intermediate steps are absorbed in high-level tactics"
        }
    ],
    "affiliations": [
        "Amazon",
        "Meta FAIR",
        "Numina",
        "Princeton Language and Intelligence, Princeton University",
        "Tsinghua University"
    ]
}