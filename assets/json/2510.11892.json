{
    "paper_title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
    "authors": [
        "Kai Mei",
        "Jiang Guo",
        "Shuaichen Chang",
        "Mingwen Dong",
        "Dongkyu Lee",
        "Xing Niu",
        "Jiarong Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs' tendency toward hallucination and their reliance on static training knowledge, which can lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs' limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 2 9 8 1 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "R-WOM: RETRIEVAL-AUGMENTED WORLD MODEL FOR COMPUTER-USE AGENTS Kai Mei Rutgers University kai.mei@rutgers.edu Jiang Guo, Shuaichen Chang, Mingwen Dong AWS Agentic AI {gujiang, cshuaich, mingwd}@amazon.com Dongkyu Lee, Xing Niu & Jiarong Jiang AWS Agentic AI {dkleekr, xingniu, jiarongj}@amazon.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs tendency to hallucination and their reliance on static training knowledge, which could lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models future state prediction and reward estimation through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantage in longer-horizon simulations."
        },
        {
            "title": "INTRODUCTION",
            "content": "World models have evolved from early symbolic planning systems to sophisticated neural architectures that learn latent representations of environment dynamics. Model-based reinforcement learning (MBRL) approaches, such as Dreamer v1-3 (Hafner et al., 2019; 2020; 2023) and MuZero (Schrittwieser et al., 2020), learn latent world models to imagine trajectories before selecting actions. More recently, Large Language Model (LLM)-based world models (Hao et al., 2023; Wang et al., 2024; Zhang et al., 2024) have emerged as new paradigm, leveraging large-scale pretraining to reason about action consequences in realistic digital environments. They show particular promise for long-horizon planning for browser and computer-use agents, where mentally simulating future states can mitigate irreversibility and reduce costly trial-and-error. However, due to their inherent tendency toward hallucination and reliance on static parametric knowledge, LLMs perform world modeling in fundamentally ungrounded manner. In complex, multi-step tasks, this detachment from the environments real-time state can trigger cascading errors: the imagined trajectory gradually diverges from actual dynamics, producing simulations that appear coherent but are ultimately unexecutable. This limitation becomes particularly evident in realistic computer-use environments, as illustrated in Figure 1. To systematically investigate whether LLMs can serve as effective world models, we probe two core capabilities: future state prediction and reward estimation. We design three evaluation tasks: Corresponding Author. Work done when Kai is an intern at AWS Agentic AI."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Example task: Copy the screenshot 1.png from the desktop to where my cursor is located. (Left:) Using only internal world knowledge, the agent loses cursor location and gets stuck. (Right:) With grounded world knowledge from tutorials, the agent uses the correct Insert Image operation while maintaining cursor position. This illustrates how grounding with external knowledge enables more reliable decision-making in realistic environments. next-state prediction and full-procedure planning alignment to assess LLMs future state prediction capability; and milestone transition recognition to assess LLMs reward estimation capability. Our analysis reveals that while LLMs demonstrate strong short-term dynamics understanding such as identifying state changes and recognizing transition outcomes they fail to maintain accuracy in full-procedure planning. This performance degradation over longer-horizon simulations highlights fundamental limitations of LLM-based world modeling. Motivated by these findings, we propose the Retrieval-augmented World Model (R-WoM) framework, which enhances LLM-based simulations by grounding them in external knowledge drawn from environment-specific tutorials. The core insight behind R-WoM is that while LLMs possess broad world knowledge from pretraining, they lack the specific, up-to-date procedural knowledge required for accurate simulation in dynamic digital environments. Recent work suggests that tutorials can function as high-level abstractions of environment dynamics (Xu et al., 2024; Zhang et al., 2025a; Su et al., 2025). However, standard retrieval pipelines often surface noisy or tangential information, which undermines the alignment between retrieved tutorials and the world-modeling process. For instance, query about fork chatgpt might retrieve general Git forking tutorials rather than specific procedures for the current application context. To mitigate this, R-WoM incorporates reasoning-based RAG pipeline that combines query rewriting with LLM-based reranking to improve the relevance of retrieved tutorials. In contrast to prior approaches that rely on computationally expensive iterative rollouts between policy and world models (Gu et al., 2024; Fang et al., 2025), R-WoM leverages the more lightweight long chain-of-thought (CoT) (Guo et al., 2025) reasoning mechanism for multi-step simulation. Moreover, we observe that the use of absolute reward estimation in existing works (Chae et al., 2024; Gu et al., 2024; Fang et al., 2025) could introduce biases and lead to unstable action scoring. To address this limitation, we employ listwise reward estimation strategy that ranks simulation rollouts relative to each other rather than assigning absolute scores, leading to more robust and consistent action selection. Our key contributions are as follows: Systematic probing of LLMs as world models. We conduct comprehensive evaluation revealing that while LLMs excel at understanding immediate state changes and local transitions, they critically fail in producing procedures aligned to the environments over long horizons."
        },
        {
            "title": "Preprint",
            "content": "Retrieval-augmented world modeling framework. We propose R-WoM, retrieval-augmented framework that grounds LLM-based world models with external tutorials, enabling environmentspecific adaptation through retrieval-augmented simulation and listwise reward estimation. Empirical validation on realistic benchmarks. We demonstrate R-WoMs effectiveness on two challenging computer-use benchmarks, WebArena (Zhou et al., 2023) and OSWorld (Xie et al., 2024), achieving consistent and substantial improvements (i.e., 7.2% to 25.3%) over competitive baselines, with particular advantages in longer-horizon scenarios."
        },
        {
            "title": "2.1 PROBLEM FORMALIZATION",
            "content": "Given an initial task goal g, computer-use agent interacts with the environment by iteratively receiving observations and executing actions to accomplish the task. Following the notation of prior work (Qin et al., 2025; Fang et al., 2025), we also introduce an intermediate reasoning component thought t, to capture thinking process. The resulting interaction trajectory can be expressed as (g, (o1, t1, a1), (o2, t2, a2), . . . , (on, tn, an)), (1) where oi is the observation at step i, ti is the reasoning thought generated before action selection, and ai is the executed action. At each step i, the LLM-based policy model produces thoughtaction pair conditioned on the task goal, the current observation, and the prior interaction history: (ti, ai) πp (cid:0) g, oi, {(oj, tj, aj)}i j=v (cid:1) , [1, 1] (2) 2.2 WORLD MODEL ROLLOUT In realistic environments, many actions are irreversible or costly to undo, which makes naive trialand-error exploration infeasible. To address this challenge, researchers explore using world model (Hafner et al., 2019; 2020; 2023) that can simulate possible futures to be aware of the action outcomes before executing. Formally, at each decision step i, given the set of candidate actions along with their thoughts Ac = {(t(1) )} proposed by policy model in Equation 2, the world model performs k-step lookahead rollouts to estimate the potential outcomes of each action candidate {1, 2, . . . , m}: ), . . . , (t(m) ), (t(2) , a(m) , a(1) , a(2) i i+1 πw(g, oi, t(j) o(j) i+1) πw(g, oi+1, t(j) i+1, a(j) , a(j) ) , a(j) ) (t(j) ... i+k πw(g, o(j) o(j) i+k1, t(j) i+k1, a(j) i+k1) (3) For each k-step rollout trajectory ˆτ (j) i+k), the corresponding rewards are estimated using model-based (Li et al., 2023; Mahan et al., 2024) or program-based (Lambert et al., 2024; Guo et al., 2025) reward function: r(aj) = R(ˆτ (j) i+1, . . . , o(j) = (o(j) i+1, a(j) i+1, t(j) , a(j) , t(j) , o(j) , g) (4) The optimal action is then selected from Ac based on the highest estimated reward. = arg max (ti,ai)Ac r(ai) (5)"
        },
        {
            "title": "3 PRELIMINARY ANALYSIS",
            "content": "We focus on two fundamental capabilities of world models that are critical for computer-use tasks: future state prediction, which supports anticipating environment dynamics, and reward estimation, which underpins evaluating the outcomes of actions (Hafner et al., 2019; 2020; 2023). Recent"
        },
        {
            "title": "Preprint",
            "content": "work such as WMA (Chae et al., 2024) explores these aspects mainly through next-state identification and immediate reward estimation. However, such analyses do not fully account for the importance of reasoning across extended horizons. To address this, we design probing tasks tailored to these two capabilities by considering longer planning horizon. Specifically, for future state prediction, we design the task of next-state identification and full-procedure planning alignment, which together capture both short and long horizon dynamics; For reward estimation, we design the task of milestone transition recognition, which assesses models ability to anticipate the outcomes of intermediate transitions. We apply these probes to three state-of-the-art LLMs, Qwen-2.5-VL72B (Bai et al., 2025), Claude-3.5-Sonnet1, and Claude-3.7-Sonnet2 by sampling trajectories on two challenging browser/computer-use benchmarks: WebArena (Zhou et al., 2023) and OSWorld (Xie et al., 2024). In the following, we introduce these tasks and present the probing analysis, while more details with illustrative examples are provided in Appendix A.1."
        },
        {
            "title": "3.1 NEXT-STATE IDENTIFICATION",
            "content": "To assess the most basic requirement of future state prediction, we follow WMA (Chae et al., 2024) to design this task where models are asked to predict the correct subsequent observation given current state and action. Given current observation oi and action ai, the model predicts the correct subsequent observation from two candidates: ˆoi+1 = arg max i+1,ofalse i+1} o{otrue (ooi, ti, ai) (6) Setup: Given the n-step trajectory, we extract intermediate steps from successful and failed trajectories where [2, 2] to avoid trivial predictions from initial or terminal states. For each (oi, ai, oi+1) triplet, we create negative sample by selecting the most lexically similar observation from the same trajectory. The lexical analysis is conducted using difflib3, Pythons built-in library. This requires LLMs to distinguish the true next observation otrue i+1 from distractor ofalse i+1. Results: As shown in Table 1, models achieve relatively strong accuracy overall, i.e., exceeding 75%, indicating they can capture short-term state changes under various lexical similarity levels. 3.2 FULL-PROCEDURE PLANNING ALIGNMENT While next-state identification evaluates whether an LLM can capture immediate state transitions, effective world models must also reason over longer horizons. To probe this ability, we design plan alignment task, where models are asked to generate execution plans and these plans are evaluated for consistency with realistic environment dynamics. Formally, given task goal and an initial observation o1, the model produces an execution plan ˆP = (a1, a2, . . . , aT ). The LLM judge then evaluates whether the execution plan conforms to the standard procedure defined in Equation 7. (cid:40) = Φ g, o1, ˆP , (cid:17) (cid:16) = True, False, if ˆP aligns with , otherwise. (7) where denotes the reference procedure derived from environment tutorials. The judgement is based on element attributes (e.g., location, text description, visibility) and operation logic (e.g., feasibility, ordering) with respect to . Setup: We sample tasks from WebArena and OSWorld benchmarks. For each task, we manually annotate reference document chunk that is directly relevant to accomplishing the task under the corresponding environment (e.g., website or software). More annotation details are in Appendix A.2. Models are then prompted to generate execution plans without access to tutorials, and the generated plans are evaluated by an LLM judge (Claude-3.7-Sonnet by default) for alignment against the reference procedures. Details of the evaluation prompt are provided in Appendix A.1. Results: Table 1 shows that alignment remains moderate across all models, rarely exceeding 65%. This reveals clear limitation: while LLMs can list plausible actions, they often fail to maintain procedural coherence or respect environment-specific constraints. 1https://www.anthropic.com/news/claude-3-5-sonnet 2https://www.anthropic.com/news/claude-3-7-sonnet 3https://docs.python.org/3/library/difflib.html"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Probing results across three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. All values are percentages. Model Next-state identification (by lexical similarity) Full-procedure planning alignment Milestone transition recognition [0, 0.8) [0.8, 0.9) [0.9, 1] Overall Accuracy Accuracy Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet 61.1 72.2 88. 84.8 84.8 87.9 77.6 81.6 83.7 77.0 81.0 86.0 50.0 55.0 65.0 83.7 85.7 86."
        },
        {
            "title": "3.3 MILESTONE TRANSITION RECOGNITION",
            "content": "Aside from probing LLMs capability of capturing future states, we also probe whether models can recognize task-relevant progress, an essential skill for reward estimation in world models. The task evaluates whether models can distinguish promising transition sequences from unproductive ones: ˆS = arg max S{Strue, Sfalse} (success S, g) (8) where = {oi, oi+h, oi+2h, . . . , oi+(l1)h} denotes subsequence of length sampled at interval from the full trajectory. Setup: We sample sequences of = 3 consecutive transitions with interval = 2 from both successful and failed trajectories, where the intervals are used to avoid repeated states. Same as next state identification, we also sample steps from steps within [2, 2] to avoid trivial predictions. For each objective g, we annotate pairs where Strue represents more promising subsequence drawn from successful trajectory, and Sfalse represents less effective subsequence from failed trajectory. More task details can be found in Appendix A.1. Results: Table 1 shows that all models perform strongly. Claude-3.7-Sonnet achieves the highest accuracy (86.7%), followed by Claude-3.5-Sonnet (85.7%) and Qwen-2.5-VL-72B (83.7%). The consistently high performance across models suggests that LLMs possess reasonable ability to evaluate which transitions are conducive to task progress. 3.4 DISCUSSION Overall, our probing analysis reveals that modern LLMs demonstrate relatively good short-term predictive and local evaluative capabilities: they can reliably identify next states and recognize taskrelevant transitions. However, these strengths do not extend to long-horizon planning, where performance deteriorates sharply in aligning its knowledge to specific environments. This suggests that LLMs might inherently lack robust generalization for world modeling across dynamic environments, thus may require external guidance to sustain accurate simulations over extended horizons."
        },
        {
            "title": "4 R-WOM FRAMEWORK",
            "content": "From the probing analysis in Section 3, we identify grounding as key mechanism for improving the alignment of LLMs to specific environments, which motivates the design of our R-WoM framework. 4.1 OVERVIEW As illustrated in Figure 2, the R-WoM framework employs the retrieval-augmented way to ground world modeling during simulation. Given the task objective and current observation, relevant documentation and tutorials are retrieved and reranked to form the grounding evidence set. This evidence is used to condition the world model during both state transition prediction and reward estimation process. Algorithm 1 summarizes the complete R-WoM pipeline, which iteratively applies this process until task completion or termination."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the R-WoM pipeline. At each time step i, the policy model generates candidate actions. For each candidate, the world model grounded by retrieved tutorials performs k-step rollouts to simulate possible future trajectory. The rewards of rollout trajectories are finally estimated by world models to select the best action. Algorithm 1 The Pipeline of R-WoM Require: Task objective g, initial observation o1 1: Retrieve and rerank tutorials relevant to the objective 2: 1 3: while task not completed do 4: Ac {(t(1) for each (t(j) 5: , a(2) ), (t(2) ) Ac do , a(1) , a(j) ), . . . , (t(m) , a(m) i )} πp(g, oi) Generate rollout trajectory ˆτ (j) = πLongCoT , a(j) ; E) (oi, t(j) , g, E)(cid:1)(cid:105) (cid:104) fw (cid:0)R(ˆτ (j) )Ac 6: 7: end for (t , Execute + 1 8: 9: 10: end while ) = arg max(t(j) , observe oi+1 ,a(j) 4.2 DESIGN DETAILS RAG design. We adopt reasoning-based retrieval design to enhance relevance of retrieved document chunks to the given query. Given the task goal g, we construct query = fenc(g) and retrieve top-k tutorial chunks Ck based on cosine similarity. An LLM-based reranker (i.e., policy model here) then conducts list-wise reranking of candidates based on contextual relevance: = rank (C, q) (9) yielding the final evidence set E. The world model conditions on for grounded future state prediction and reward estimation. , a(j) R-WoM design. At step i, with tutorial evidence E, for each candidate thought and action pair (t(j) ) Ac, the rollout produces predicted trajectory of steps. Unlike the iterative rollout explored in previous works (Gu et al., 2024; Fang et al., 2025), which requires multiple rounds of LLM calls and thus suffers from efficiency limitations, we adopt reasoning-based LongCoT rollout inspired by Deepseek-R1 (Guo et al., 2025), enabling the world model to unfold the entire multi-step imagination trajectory within single forward reasoning sequence. ˆτ (j) = πLongCoT (oi, t(j) , a(j) ; E) (10)"
        },
        {
            "title": "Preprint",
            "content": "Table 2: End-to-end performance on OSWorld and WebArena across three runs. Best in bold; second-best underlined. R-WoM cells include relative improvement over the second-best. Model Method OSWorld (Xie et al., 2024) WebArena (Zhou et al., 2023) Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Vanilla RAG WoM R-WoM Vanilla RAG WoM R-WoM Vanilla RAG WoM R-WoM 26.36 2.32 30.84 1.07 28.37 2.01 38.05 2.29 (+23.4%) 22.43 2.25 22.19 0.92 23.48 2.14 26.41 0.44 (+12.5%) 28.47 2.27 27.76 0.75 31.24 2.88 39.13 1.92 (+25.3%) 21.84 0.42 22.42 0.42 24.50 0.84 28.92 0.43 (+18.1%) 27.74 0.43 30.70 0.41 29.82 0.41 33.65 0.01 (+9.6%) 28.92 0.41 32.75 0.72 31.86 0.01 35.11 1.10 (+7.2%) We observe that absolute sparse reward used in previous works (Chae et al., 2024; Gu et al., 2024; Fang et al., 2025) might not effectively distinguish more meaningful rollouts. Therefore, inspired by recent advances in relative reward design (Liu et al., 2024; Choi et al., 2024; Guo et al., 2025), we employ list-wise ranking mechanism to evaluate simulated trajectories in relative way. (t , i ) = arg (cid:104) max ,a(j) )Ac (t(j) fw (cid:0)R(ˆτ (j) , g, E)(cid:1)(cid:105) (11) As is shown in Equation 11, each rollout trajectory is scored relatively in the comparative context of all candidates. In this way, we aim to reduce potential bias from absolute reward signals and stablize the selection of most promising action candidate."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "To evaluate the effectiveness of R-WoM, we propose the following research questions: RQ1: Does R-WoM improve the performance of computer-use agents compared to established baselines in realistic environments such as browsers and operating systems? RQ2: How do external tutorials contribute to grounding world models, and to what extent do agents benefit from incorporating this information from tutorials? RQ3: Can tutorial-grounded world models support longer imagination horizons more effectively than ungrounded counterparts over multi-step rollouts? 5.1 SETUP We evaluate R-WoM against three baselines: Vanilla: The vanilla approach is adapted from the official implementations: the screenshot-only version for OSWorld provided by GTA-1 (Yang et al., 2025), and the screenshot+accessibilty tree version for WebArena provided by WMA (Chae et al., 2024). This approach relies solely on the task objective, current observation (represented as screenshots and accessibilty trees) and prior interaction history. RAG: retrieval-augmented generation pipeline that retrieves relevant documentation and augments the LLM before action prediction, which is built upon the vanilla approach. WoM: variant of the WebDreamer (Gu et al., 2024) where we implement world model-based dreaming and reward assignment in LongCoT way as the original iterative way of generating rollouts takes much more LLM calls, which will be discussed with more details in Appendix A.4."
        },
        {
            "title": "Preprint",
            "content": "(a) OSWorld (b) WebArena Figure 3: Performance under different grounding settings, where we compare ungrounded world model: WoM, world model grounded with retrieved tutorials: R-WoM, and world model grounded with oracle tutorials: R-WoM (oracle). We conduct experiments on two comprehensive benchmarks designed for multi-round interactions in realistic computer-use environments: WebArena (Zhou et al., 2023), which spans web-based tasks across domains such as e-commerce, social forums, and collaborative platforms; and OSWorld (Xie et al., 2024), which covers diverse desktop tasks including file management, terminal commands, and productivity applications. Specifically, we sample subset from these two benchmarks for our experiments where tutorials are available for retrieval purpose and we collect tutorials from both online websites and offline documents. The details of the subsets and tutorial collection can be found in Appendix A.2. We test three popular LLM backbones: Qwen-2.5-VL-72B (Instruct version) (Bai et al., 2025), Claude-3.5-Sonnet, and Claude-3.7-Sonnet, serving as both the policy and world model. For methods requiring retrieval, we build the RAG pipeline with Langchain4, FAISS (Douze et al., 2024) as the vector store, and Qwen-3-Embedding-8B (Zhang et al., 2025b) as the embedding model. More implementation details can be found in Appendix A.3. 5.2 RQ1: END-TO-END PERFORMANCE Table 2 reports the overall end-to-end performance. It shows that R-WoM consistently outperforms all alternatives, with improvements of +23.4% on OSWorld and +18.1% on WebArena for Qwen2.5, +12.5% and +9.6% for Claude-3.5, and +25.3% and +7.2% for Claude-3.7 over the strongest non-R-WoM baselines. These results reveal that the improvements remain stable across different backbones, highlighting that R-WoM provides more consistent benefits compared with retrieval alone or ungrounded world modeling. 5.3 RQ2: THE ROLE OF TUTORIALS IN GROUNDING WORLD MODELS To further assess the role of tutorials, we compare three settings: no grounding (WoM), grounding with R-WoM using retrieved tutorials, and grounding with R-WoM using oracle tutorials. Similarly as the full-procedure alignment task in Section 3.2, we also manually annotate document chunks that are relevant to the task from humans perspective. More annotation details and the performance of retrieval can be found in Appendix A.2 and A.4, respectively. As shown in Figure 3, performance consistently improves with the grounding levels, from no grounding to grounding with retrieved tutorials, then to grounding with oracle tutorials. It indicates that access to external procedural knowledge helps models in world modeling. These findings underscore that R-WoMs effectiveness is tightly coupled with tutorial fidelity, and the advances in retrieval and resource curation represent critical levers for future progress. 4https://github.com/langchain-ai/langchain"
        },
        {
            "title": "Preprint",
            "content": "(a) OSWorld (b) WebArena Figure 4: Success rates (%) across imagination horizons on OSWorld (a) and WebArena (b). R-WoM (green, solid) consistently outperforms WoM (red, dashed) and reaches its peak at larger imagination horizon (at horizon around 3), indicating that grounding benefits world models in simulations over longer horizons. 5.4 RQ3: ABLATION STUDIES OF IMAGINATION HORIZON To examine the effect of imagination horizon on end-to-end performance, we vary the horizon from 1 to 4 for both ungrounded (WoM) and grounded (R-WoM) world models, as shown in Figure 4. WoM, the world model without grounding during rollouts, shows modest initial gains but quickly plateaus and even declines beyond 2 steps, reflecting its susceptibility to compounding prediction errors. In contrast, R-WoM maintains consistently higher success across horizons on both OSWorld and WebArena, with improvements lasting up to horizon three before tapering off. These results suggest that tutorial-guided grounding helps stabilize rollouts over longer horizon simulations."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "6.1 COMPUTER-USE AGENT One line of works focuses on exploring how to improve agents understanding of computer-use actions, such as building end-to-end agent frameworks (Agashe et al., 2024; 2025; Song et al., 2025), and training native agent models (Qin et al., 2025; Wang et al., 2025; Lai et al., 2025) or specific action grounding models (Wu et al., 2024; Xie et al., 2025; Yang et al., 2025). Another line of works explores treating LLMs as world models to simulate the computer-use environments. WoM (Gu et al., 2024) pioneers this direction by using LLMs to simulate the outcome of candidate actions, and evaluate these imagined states with discrete reward given by LLM judge (Gu et al., 2024). Subsequent works such as WMA (Chae et al., 2024) adapt this idea to improve planning by abstracting state transitions into natural language summaries. WKM (Qiao et al., 2024) and WebEvolver (Fang et al., 2025) develop co-evolving world models and policies to progressively refine both simulation and planning, moving beyond one-horizon imagination. 6.2 TUTORIAL-USE Parallel developments leverage tutorials or indirect knowledge to train digital agents. Synatra (Ou et al., 2024) converts human-oriented tutorials into 100k synthetic demonstrations to fine-tune 7B CodeLLaMA model. Other frameworks generate trajectories guided by tutorial completion or replay (e.g., AgentTrek (Xu et al., 2024), TongUI (Zhang et al., 2025a)) to teach GUI navigation and tool use from multimodal resources. Learn-by-interact (Su et al., 2025) synthesizes trajectories by leveraging tutorials and interaction with the environments. These approaches focus on offline trajectory generation by referring to tutorials while our approach focuses on tutorial-guided grounding of LLMs as world models at inference time."
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORK",
            "content": "We presented systematic study of LLM-based world models for computer-use tasks, revealing that while they can model state transitions and recognize task-relevant progress, they fail to reliably adapt to unfamiliar environments in long-term planning without grounding. To address this, we proposed the Retrieval-augmented World Model (R-WoM), which incorporates environment-specific tutorial knowledge during the imagination rollouts and reward prediction procedures to reduce hallucinations and stale knowledge. Evaluations on WebArena and OSWorld show that R-WoM consistently outperforms competitive baselines, demonstrating the efficacy of retrieval-augmented grounding for LLM agents in dynamic browser-use and computer-use scenarios. While R-WoM shows promises in improving LLM as world models, some bottlenecks still remain. First, the grounding stage requires availability of online tutorials for the target environment, which has limits in tutorial-scarce domains, or when documentation is outdated. Synthesizing tutorials from tutorial-scarce environments is one of the future directions we aim to explore. Second, despite the efficiency optimizations in R-WoMs rollout simulation and reward estimation, the computational cost is still non-trivial. Conducting world modeling in an agentic way to further reduce costs can be our future work."
        },
        {
            "title": "REFERENCES",
            "content": "Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Hyungjoo Chae, Namyoung Kim, Kai Tzu-iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. arXiv preprint arXiv:2410.13232, 2024. Heewoong Choi, Sangwon Jung, Hongjoon Ahn, and Taesup Moon. Listwise reward estimation for offline preference-based reinforcement learning. arXiv preprint arXiv:2408.04190, 2024. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, PierreEmmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint arXiv:2504.21024, 2025. Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. Is your llm secretly world model of the internet? modelbased planning for web agents. arXiv preprint arXiv:2411.06559, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023."
        },
        {
            "title": "Preprint",
            "content": "Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, and Jie Tang. Computerrl: Scaling end-to-end online reinforcement learning for computer use agents. arXiv preprint arXiv:2508.14040, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023. Tianqi Liu, Zhen Qin, Junru Wu, Jiaming Shen, Misha Khalman, Rishabh Joshi, Yao Zhao, Mohammad Saleh, Simon Baumgartner, Jialu Liu, et al. Lipo: Listwise preference optimization through learning-to-rank. arXiv preprint arXiv:2402.01878, 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Tianyue Ou, Frank Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale. Advances in Neural Information Processing Systems, 37:9161891652, 2024. Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843114871, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Arık. Learn-byinteract: data-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025. Ruoyao Wang, Graham Todd, Ziang Xiao, Xingdi Yuan, Marc-Alexandre Cˆote, Peter Clark, and arXiv preprint Peter Jansen. Can language models serve as text-based world simulators? arXiv:2406.06485, 2024. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024."
        },
        {
            "title": "Preprint",
            "content": "Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. arXiv preprint arXiv:2412.09605, 2024. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. Alex Zhang, Khanh Nguyen, Jens Tuyls, Albert Lin, and Karthik Narasimhan. Language-guided world models: model-based approach to ai control. arXiv preprint arXiv:2402.01695, 2024. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, Song-Chun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025a. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "Roadmap: Section A.1 introduces the design details of our probing task. Section A.2 introduces the tutorial collection, annotation and retrieval approach for our experiments. Section A.3 presents the implementation details of R-WoM, including action space definition and prompt design. Section A.4 presents additional experimental results. A.1 DETAILS OF PROBING TASK"
        },
        {
            "title": "PROMPT FOR NEXT STATE IDENTIFICATION",
            "content": "Given the previous state of the web page: {previous state} and the current action: {current action}, please reason about the next state. The next state can be one of the following: {state a}, {state b}. Please reason about the next state and return the rationale and the choice. The choice should be one of the following: A, B. Output the choice in the following JSON format: { } \"rationale\": \"...\", \"choice\": \"...\" Task 1: Next-state identification. To assess whether the world model can predict the immediate outcome of an action given the current state, the model is asked to discriminate between the true next observation and lexically similar distractor, as illustrated in Figure 5. In this way, we aim to probe LLMs sensitivity to environment changes. We construct 100 samples drawn from trajectories in WebArena for this task. Task 2: Full-procedure planning alignment. Moving beyond identifying next state, we would like to probe whether LLM can reason about longer steps of future states. As shown in A.1, given task objective, the model is asked to generate multi-step plan, which is then validated against tutorials describing environment dynamics. The evaluation measures whether the models procedure aligns with realistic element locations, operation sequences, and interaction methods. To assess this capability, we construct 40 samples from trajectories in both OSWorld and WebArena. PROMPT FOR FULL-PROCEDURE PLANNING ALIGNMENT You are grounding validation assistant that verifies whether tutorial-referenced operations in plan are accurately grounded in the provided documentation. Evaluation criteria 1. Element Text Accuracy: Exact text matches between plan and tutorial for referenced elements. 2. Location Consistency: Location indicators (position, context) align with tutorial descriptions. 3. Operation Sequence: Prerequisites and dependencies match tutorial methodology. 4. Interaction Method: Specified actions (click, input, select) align with tutorial instructions. 5. Attribute Precision: Element types, properties, and characteristics match tutorial specifications. Evaluation principle 1. Accept: Plan steps that extend beyond tutorial scope (additional operations are allowed). 2. Reject: Any tutorial-referenced operation with misaligned text, location, or method. Output Format Output your response in the following JSON format: { } \"rationale\": \"Your rationale of your evaluation\", \"answer\": \"yes/no\""
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Illustration of the next-state identification probing task. Given current state and an action, the model must choose between two candidate next states: (A) the ground-truth state, and (B) lexically similar distractor. This task evaluates whether the world model can correctly predict the true next observation rather than being misled by textual similarity. Figure 6: Illustration of the full-procedure planning alignment probing task. Given task objective (top), the model generates multi-step plan (left), which is then compared against environmentspecific tutorials (right). The evaluation checks whether the generated procedure aligns with the tutorials in terms of navigation logic, element selection, and operation feasibility. This task assesses the world models ability to sustain long-horizon procedural reasoning in realistic environments. PROMPT FOR MILESTONE TRANSITION RECOGNITION You are evaluating web automation trajectories to identify which one is more likely to succeed in completing the given task. The following two trajectories show segments from different agent attempts at the same task. Both agents were following the same initial steps, but diverged when they chose different actions at critical decision point. Your task is to determine which trajectory segment demonstrates better progress toward completing the task objective. You need to output in the following JSON format as: { } \"answer\": \"A/B\", \"rationale\": \"xxx\""
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Illustration of the milestone transition recognition probing task. Given sequence of transitions, the model must identify whether they reflect meaningful progress toward the goal. In this example, the top path shows an unproductive transition where the agent gets stuck trying to directly select books as forum type, failing to proceed. The bottom path shows more promising milestone transition: the agent first enters the books forum and then successfully fills out the submission form. The task evaluates whether the world model can distinguish between effective and ineffective procedural progress. Task 3: Milestone transition recognition. To probe reward estimation capability of LLMs, we design this task to assess whether LLMs have the capability to capture meaning state transitions. As shown in Figure 7, the LLM is presented with pairs of trajectory segments that diverge at decision point, one representing promising milestone transition and the other an unproductive path. The LLM needs to identify which trajectory is more conducive to task success. This setting is evaluated on 98 samples drawn from both successful and failed trajectories in WebArena. A.2 TUTORIAL PROCESSING Our framework relies on tutorials as external grounding for browserand computer-use tasks. To construct comprehensive knowledge base, we gather tutorials from both general-purpose and environment-specific resources. For cross-domain instructional guidance, we include WikiHow, which provides structured, step-by-step content spanning broad range of tasks. For environmentspecific domains, we incorporate official documentation from the corresponding software or websites. The complete list of tutorial sources is as follows: WikiHow: https://www.wikihow.com/Main-Page Google Chrome Help: https://support.google.com/chrome"
        },
        {
            "title": "Preprint",
            "content": "GIMP 3.0 User Manual: https://docs.gimp.org/3.0/en/ Visual Studio Code Documentation: https://code.visualstudio.com/docs Ubuntu Help: https://help.ubuntu.com/22.04/ubuntu-help/ Mozilla Thunderbird Support: https://support.mozilla.org/en-US/products/ thunderbird/learn-basics-get-started VLC Media Player User Guide: https://docs.videolan.me/vlc-user/desktop/ 3.0/en/ LibreOffice Help: https://help.libreoffice.org/latest/en-US/ GitLab Documentation: https://docs.gitlab.com/ Adobe Commerce Admin User Guides: https://experienceleague.adobe.com/en/ docs/commerce-admin/user-guides/home From these sources, we construct knowledge base of over 30k chunked tutorial documents that collectively support tasks across diverse software and website environments. Since our framework requires tutorial availability to provide concrete grounding, we sample task subsets from OSWorld and WebArena that can be partially mapped to tutorial examples. Specifically, we select 85 tasks from OSWorld, covering domains such as Chrome, GIMP, VSCode, VLC, Thunderbird, and Ubuntu OS, and 113 tasks from WebArena, covering CMS and GitLab domains and we annotate one or two document chunks that are most relevant to each task from humans perspective. To retrieve useful tutorials at inference time, we adopt reasoning-based retrieval strategy. This involves query rewriting to anonymize and generalize task queries, followed by LLM-based reranking to reduce false negatives that may arise when relying solely on cosine similarity. The detailed prompts used for query rewriting and reranking are provided below, and the results comparing retrieval strategies are reported in Appendix A.4. PROMPT FOR QUERY REWRITING You are an AI assistant that rewrite original query into comprehensive, searchable queries that are easier to retrieve answers from documents. You must follow these rules: 1. Organize the original query to be well-structured and clear with details: Try to make the query detailed and clear. For example, instead of title like Fork ChatGPT, good rewritten query would be, How could fork the ChatGPT repository in the gitlab? 2. Generalize Personal Details: Replace all specific, personal information (like user names, file names, file location) with general descriptions (like user, xxx format file, at desktop). PROMPT FOR RERANKING Your task is to re-rank list of documents based on their relevance to given task. Carefully analyze the task and each numbered document. Your goal is to identify which documents are helpful for completing the task and order them accordingly. Your output must be single JSON object with one key: reranked indexes. The value for this key must be list of the original document indexes, sorted from most relevant to least relevant. Example format: { } \"reranked_indexes\": [0, 2, 1] A.3 IMPLEMENTATION DETAILS OF R-WOM To enable automation in browser and computer-use environments, we adopt the official action space definitions provided by WebArena5 and OSWorld6, as summarized in Table 3. In practice, we find that direct action coordinate mapping in OSWorld poses challenges for models such as the Qwen 5https://github.com/web-arena-x/webarena 6https://github.com/xlang-ai/OSWorld"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Action space for WebArena and OSWorld. Environment Action Definition WebArena OSWorld click type hover press scroll new tab tab focus close tab goto go back go forward stop Clicks webpage element identified by its id. Types text into webpage element; may submit if appropriate. Moves the cursor over webpage element. Presses key or key combination. Scrolls the page up or down. Opens new browser tab. Focuses specific browser tab. Closes the active browser tab. Navigates the current tab to URL. Navigates to the previous page. Navigates to the next page. Terminates the task and returns an answer (use N/A if unknown). click drag and drop highlight text span hold and press hotkey open scroll set cell values switch applications type wait done fail Clicks described UI element in the desktop environment. Drags from one described UI location to another. Highlights text between two provided phrases. Holds keys and presses sequence of keys. Presses hotkey combination. Opens an application or file by name. Scrolls within described element. Sets specified cells in spreadsheet. Switches focus to another open application. Types text into described element. Pauses execution for short duration. Ends the task successfully and returns the final answer if any. Ends the task with failure and stop. series and Claude-3.5-Sonnet. To address this and enable the policy model to generate more effective actions during world model rollouts, we employ GTA-1-7B (Yang et al., 2025) as an auxiliary action grounding model to assist in action generation when evaluating on OSWorld. For retrieval-related approach (i.e., RAG and R-WoM), we use top-5 retrieved document chunks by default to put them into the LLMs context. PROMPT FOR GENERATING ACTION CANDIDATES You are reasoner that analyzes the current state, previous actions, and task progress to determine the next required action. Available actions # Action space definition Rules for success 1. When pressing keys, ensure held/pressed keys are within {KEYBOARD KEYS}. 2. Output single action at each step; do not bundle multiple intents into one step. 3. Only issue actions that are valid for the current observation (e.g., do not type into buttons or click static text). 4. Strictly avoid repeating the same action if the interface state is unchanged. Response JSON schema { \"observation\": \"Description of current state and any changes observed\", \"action_candidates\": [ { \"thought_and_action\": \"Why this action is appropriate given the observation\", \"action_code\": { \"action_type\": \"action_type\", \"parameters\": { \"param1\": \"value1\","
        },
        {
            "title": "Preprint",
            "content": "\"param2\": \"value2\" } } } ] } Output requirements observation: provide detailed description of the current computer state based on the full screenshot, noting any state changes. action candidates: include {branching factor} candidates, ordered by confidence (most confident first). For each candidate, include: thought and action: rationale for the proposed action. action code: the concrete action with its required parameters. PROMPT FOR RETRIEVAL-AUGMENTED FUTURE STATE ROLLOUTS You are world-model assistant with extensive knowledge of desktop and web UIs. Given the previous observations, the task objective, and candidate action, you must simulate the future and describe the plausible future states. Available actions # Action space definition Tutorial usage guideline 1. Use tutorials to identify efficient workflow patterns that should be predicted as likely outcomes. 2. Provide reference to the tutorial if the current situation matches the standard operations in the tutorials. If the current situation does not align with tutorials, rely on internal world knowledge instead. Environment awareness checklist Visible UI elements: text, icons, menus, modals, tooltips Element states: enabled/disabled, focused/hovered, loading progress Hidden or off-screen affordances revealed by scrolling or clicking Cursor position, caret position, selection highlights Global context: file system changes, network requests, OS dialogs Output Format Produce an ordered chain from STATE 0 (current) up to STATE (1 {k}); you may stop early if no further prediction is useful. PROMPT FOR RETRIEVAL-AUGMENTED REWARD ESTIMATION You are an agent that evaluates actions by considering previous observations and the potential outcomes of these actions. Tutorial Grounding Guidance Priorize action sequences that follow the standard operations in the tutorials and have captured the milestones and conditions to make more meaningful progress to achieve the task objective. Output Format Output your response in the following JSON format: { } \"ranking\": [x, x, x] # \"indexes of the action candidates, most promising first\", \"thought\": \"your rationale for the ranking result\""
        },
        {
            "title": "Preprint",
            "content": "Table 4: Domain-level performance. Best in bold; second-best underlined. Benchmark Domain Model Vanilla RAG WoM R-WoM chrome (17) gimp (22) OSWorld thunderbird (11) vlc (5) os (15) vs code (15) shopping admin (57) WebArena gitlab (56) Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet Qwen-2.5-VL-72B Claude-3.5-Sonnet Claude-3.7-Sonnet 9.95 0.02 5.28 0.45 5.00 0. 7.33 0.47 5.33 0.47 5.00 0.82 1.33 0.47 2.00 0.00 2.67 0.47 0.33 0.47 1.33 0.47 1.67 0.47 3.33 0.47 2.33 0.47 5.33 0.47 2.33 0.47 2.67 0.47 4.67 0.47 8.93 0.02 5.27 0.46 6.97 0. 7.33 0.47 5.33 0.47 5.33 0.47 2.67 0.47 2.33 0.47 2.33 0.47 1.33 0.47 1.33 0.47 1.00 0.00 1.33 0.47 2.00 0.00 3.33 0.47 5.33 0.47 3.33 0.47 5.33 0.47 6.29 0.47 4.95 0.02 7.31 0. 9.33 0.47 3.33 0.47 9.67 0.47 2.33 0.47 2.33 0.47 2.00 0.00 0.33 0.47 1.67 0.47 0.33 0.47 4.33 0.47 4.67 0.47 6.33 0.47 3.67 0.47 2.33 0.47 4.33 0.47 9.95 0.02 5.92 0.00 8.95 0. 11.33 0.47 6.00 0.00 10.67 0.47 3.67 0.47 2.00 0.00 4.00 0.00 1.00 0.00 2.00 0.00 0.33 0.47 4.33 0.47 3.00 0.00 6.67 0.47 4.33 0.47 3.00 0.00 5.33 0.47 Qwen-2.5-VL-72B 11.33 0.47 14.33 0.47 Claude-3.5-Sonnet 15.33 0.47 Claude-3.7-Sonnet Qwen-2.5-VL-72B 13.33 0.47 17.00 0.82 Claude-3.5-Sonnet 17.67 0.47 Claude-3.7-Sonnet 12.33 0.47 15.00 0.00 17.33 0.47 13.00 0.00 19.67 0.47 19.67 0.47 12.33 0.47 14.67 0.47 18.33 0.47 15.33 0.47 19.00 0.00 17.67 0.47 15.33 0.47 17.67 0.47 19.00 0. 17.33 0.47 20.33 0.47 20.67 0.47 (a) OSWorld (b) WebArena Figure 8: Retrieval performance under different retrieving strategies. A.4 ADDITIONAL EXPERIMENTAL RESULTS Breakdown of end-to-end performance. Table 4 provides domain-level view of performance. R-WoM consistently achieves the best results across most of the domains, but the relative magnitude of improvement varies. In domains such as chrome and gimp, where tasks involve longer dependencies and compounding errors, R-WoM exhibits the largest margins over WoM. By contrast, in lighter workloads such as vlc or thunderbird, the absolute gains are smaller and sometimes comparable to RAG, suggesting that grounding might bring limited additional benefit when task horizons are short. These results imply that grounding is most critical in environments requiring extended planning. Ablation studies of retrieval performance. Figure 8 shows that retrieval performance improves most when query rewriting and reranking are combined, indicating their complementary effects. Query rewriting is more beneficial in diverse environments like WebArena, while reranking offers steadier gains across settings by filtering irrelevant matches. The overall trend suggests that single strategies yield uneven improvements depending on domain structure, but their integration consistently delivers more robust retrieval."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Cost statistics of running different methods across benchmarks. Benchmark Model Method Avg Turns Per Task Total # LLM Calls Total Time OSWorld WebArena Qwen-2.5-VL-72B Greedy Qwen-2.5-VL-72B RAG Qwen-2.5-VL-72B WoM Qwen-2.5-VL-72B WebDreamer Qwen-2.5-VL-72B R-WoM Claude-3.5-Sonnet Greedy RAG Claude-3.5-Sonnet Claude-3.5-Sonnet WoM Claude-3.5-Sonnet WebDreamer Claude-3.5-Sonnet R-WoM Claude-3.7-Sonnet Greedy RAG Claude-3.7-Sonnet Claude-3.5-Sonnet WoM Claude-3.7-Sonnet WebDreamer Claude-3.7-Sonnet R-WoM Qwen-2.5-VL-72B Greedy Qwen-2.5-VL-72B RAG Qwen-2.5-VL-72B WoM Qwen-2.5-VL-72B WebDreamer Qwen-2.5-VL-72B R-WoM Claude-3.5-Sonnet Greedy Claude-3.5-Sonnet RAG Claude-3.5-Sonnet WoM Claude-3.5-Sonnet WebDreamer Claude-3.5-Sonnet R-WoM Claude-3.7-Sonnet Greedy Claude-3.7-Sonnet RAG Claude-3.7-Sonnet WoM Claude-3.7-Sonnet WebDreamer Claude-3.7-Sonnet R-WoM 23.80 22.30 25.50 25.70 27.00 22.30 18.40 23.10 24.60 22.80 21.20 21.00 24.10 23.60 22.00 12.57 12.11 13.28 12.53 12.99 13.37 13.11 11.89 11.73 12.26 14.49 14.64 16.34 16.87 16. 2,028 1,984 10923 41,658 11,515 1,984 1,683 9903 39,747 9,778 1,889 1,947 10328 38,162 9,460 1,544 1,596 7508 26,948 7,459 1,624 1,642 6718 25,213 7,049 1,754 1,668 9345 36,176 9, 2.1h 2.1h 11.4h 43.6h 12.0h 0.8h 0.7h 4.0h 15.9h 3.9h 0.8h 0.8h 4.1h 15.3h 3.8h 1.6h 1.7h 7.9h 28.2h 7.8h 0.7h 0.7h 2.7h 10.1h 2.8h 0.7h 0.7h 3.8h 14.5h 3.7h Cost comparison. In this section, we compare the computational cost of different methods in terms of the number of LLM calls and total inference time. As discussed in Section 4.2, iterative rollout introduces significant inefficiency due to multiple rounds of policyworld model interactions. To quantify this effect, we reimplement the iterative rollout used by WebDreamer and measure its cost alongside other methods. Although R-WoM remains more expensive than single-pass approaches such as Greedy or RAG, it offers more favorable trade-off between efficiency and stability, enabling longer and more consistent rollouts without the extreme overhead of full iterative reasoning. Looking forward, we would explore incorporating agentic calling of world models to further eliminate redundant calls and improve cost efficiency. A.5 USE OF LARGE LANGUAGE MODELS We utilized Large Language Models (LLMs), such as Claude, exclusively for ancillary support in two main areas: (i) language editing and polishing of the manuscript, and (ii) coding assistance for minor boilerplate tasks, such as generating plotting scripts and small utilities. All model-generated outputs were thoroughly reviewed, modified, and rigorously tested by the authors to ensure their accuracy and appropriateness."
        }
    ],
    "affiliations": [
        "AWS Agentic AI",
        "Rutgers University"
    ]
}