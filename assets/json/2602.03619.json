{
    "paper_title": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation",
    "authors": [
        "Changze Lv",
        "Jie Zhou",
        "Wentao Zhao",
        "Jingwen Xu",
        "Zisu Huang",
        "Muzhao Tian",
        "Shihan Dou",
        "Tao Gui",
        "Le Tian",
        "Xiao Zhou",
        "Xiaoqing Zheng",
        "Xuanjing Huang",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models."
        },
        {
            "title": "Start",
            "content": "Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Changze Lv 1 2 Jie Zhou 1 Wentao Zhao 1 Jingwen Xu 2 Zisu Huang 2 Muzhao Tian 2 Shihan Dou 2 Tao Gui 2 Le Tian 1 Xiao Zhou 1 Xiaoqing Zheng 2 Xuanjing Huang 2 Jie Zhou 1 6 2 0 2 3 ] . [ 1 9 1 6 3 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Nowadays, training and evaluating DeepResearchgenerated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models. 1. Introduction Large language models (LLMs) (Achiam et al., 2023; Guo et al., 2025; Yang et al., 2025) have recently enThe work was conducted during the internship of Changze Lv (czlv24@m.fudan.edu.cn) at Tencent. 1Pattern Recognition Center, WeChat AI, Tencent Inc. 2College of Computer Science and Artificial Intelligence, Fudan University. Correspondence to: Xiaoqing Zheng <zhengxq@fudan.edu.cn>. Preprint. abled DeepResearch systems (Qwen Team, 2025; Google, 2025; OpenAI, 2025; Anthropic, 2025) that can synthesize evidence from large-scale document collections and produce long-form analytical reports for complex, openended queries. Unlike short-form DeepResearch tasks like BrowseComp (Wei et al., 2025; Zhou et al., 2025), GAIA (Mialon et al., 2023), and HLE (Phan et al., 2025), report generation requires models to reason, retrieve, and integrate over diverse sources and multiple turns, while presenting results in coherent and well-structured manner. However, training and evaluating DeepResearch report generators remain fundamentally challenging. key difficulty lies in the absence of verifiable rewards. Human evaluation, while reliable, is costly and difficult to scale (Krishna et al., 2021; Xu et al., 2023; Shao et al., 2025b), motivating the widespread adoption of rubric-based evaluation (Gunjal et al., 2025; Huang et al., 2025; Viswanathan et al., 2025) as practical alternative. In principle, expert-designed, queryspecific rubrics, like ResearchRubrics (Sharma et al., 2025), could serve as faithful proxy for human judgments when evaluating DeepResearch reports. However, authoring such rubrics requires substantial domain expertise and effort for each query, making this approach difficult to scale to large and diverse training corpora. Prior work has explored pre-defined generic rubrics (Que et al., 2024; Hashemi et al., 2024; Shao et al., 2024a) or LLM-generated query-specific rubrics (Xie et al., 2025; Du et al., 2025) to provide structured feedback for report generation tasks. However, those methods suffer from two limitations: First, pre-defined rubrics are necessarily generic and lack the granularity needed to distinguish subtle quality differences across diverse research queries. Second, LLMgenerated query-specific rubrics are typically not grounded in human preference data, making them prone to misalignment with how humans actually compare and judge research reports. These issues may lead to weak supervision signals, reward hacking, and inefficient learning dynamics. To address these limitations, it is crucial to reconsider the source of supervision for evaluating DeepResearch reports. We argue that one of the most direct supervision signals for assessing report quality is human preference (Dai Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation et al., 2023; Zheng et al., 2023b; Wang et al., 2024a; Liu et al., 2024b) over candidate reports. Given these limitations, natural question arises: rather than applying generic or human-annotated rubrics, can we learn to evaluate reports in way that is both scalable on large training data and aligned with human preferences? In this paper, we propose pipeline to effectively train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We construct preference dataset of over 5, 000 DeepResearch-style queries, each paired with two candidate reports and annotated with human preference judgments. Then we train the rubric generator using Group Relative Policy Optimization (GRPO) (Shao et al., 2024b) with hybrid reward that integrates two complementary signals. The first component is the preference consistency reward, which encourages the generated rubric to be discriminative by correctly ranking human-preferred reports above less-preferred ones, using the annotated preference dataset to guide scoring. For the second reward, we leverage LLMs to assess rubric quality, providing feedback on whether the generated criteria are coherent, applicable, and discriminative. Once trained, the rubric generator is integrated into the training of DeepResearch systems. For each input query, it automatically produces query-level rubrics that are used to evaluate rollout samples from the policy model, assigning fine-grained reward scores that guide the optimization process. Whats more, to address the long-context dependencies in the ReAct workflow (Yao et al., 2022), we further propose the Multi-agent Markov-state (MaMs) workflow. Empirically, we demonstrate that our proposed rubric generators deliver more discriminative and human-aligned supervision signals than pre-defined or LLM-generated alternatives. Furthermore, when employed as training signals for DeepResearch agents, these generated rubrics consistently enhance performance across multiple baselines on DeepResearch Bench (Du et al., 2025), surpassing all opensource systems while achieving results approaching those of closed-source models. These results demonstrate that learning to generate query-specific rubrics offers principled and effective path toward scalable, preference-aligned training for DeepResearch systems. To sum up, our contributions can be summarized as follows: We construct large-scale human preference dataset and train query-specific rubric generators using GRPO with hybrid reward combining human preferences and LLM evaluation. We introduce the Multi-agent Markov-state (MaMs) workflow for DeepResearch report generation, addressing the long-context dependencies of ReAct workflow. We demonstrate that the rubric generators yield stronger and more reliable supervision for DeepResearch report generation, leading to consistent improvements over existing rubric-based baselines. 2. Related Work 2.1. DeepResearch Agent Recent progress in LLMs has spurred the development of agentic systems for complex tasks, yet their reliance on static internal knowledge motivates deep research agents that combine planning, retrieval, and evidence-grounded synthesis. Existing DeepResearch agents can be broadly categorized according to their primary task domains. Short-Form Question Answering. In this setting, DeepResearch agents primarily target retrieval-based short-form question answering tasks. Benchmarks such as GAIA (Mialon et al., 2023; Russell et al., 2025), BrowseComp (Wei et al., 2025; Zhou et al., 2025), and HLE (Phan et al., 2025) provide verifiable targets, enabling agent training via Reinforcement Learning with Verifiable Rewards (RLVR) (Jin et al., 2025; Liu et al., 2025a). Several systems leverage this paradigm to enhance search and reasoning capabilities. For instance, Search-R1 (Jin et al., 2025) and WebExplorer (Liu et al., 2025a) adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024b) to improve retrieval effectiveness in short-form QA tasks with explicit correctness signals. In contrast, WebThinker (Li et al., 2025a) employs Direct Preference Optimization (DPO) (Rafailov et al., 2023) to equip LLMs with DeepResearch capabilities without relying on verifiable rewards. Meanwhile, Tongyi DeepResearch (Tongyi et al., 2025) is specifically designed to support long-horizon information-seeking behaviors. Long-Form Report Generation. By contrast, long-form report generation requires agents to synthesize evidence from large, heterogeneous document collections and to produce coherent, well-structured reports that address complex, open-ended queries. Beyond retrieving isolated facts, agents must perform multi-step reasoning, reconcile conflicting evidence, and organize information at the document level. Because evaluating long-form outputs is inherently difficult due to lack of reference answers, benchmarks in this regime (e.g., DeepResearch Bench (Du et al., 2025) and ResearchQA (Yifei et al., 2025)) commonly use LLM-as-aJudge applied to human-annotated general or query-specific rubrics. Recent studies have focused on designing end-toend workflows for report synthesis. For example, WebWeaver (Li et al., 2025b) develops dual-agent framework that emulates collaborative human research processes. Dr Tulu (Shao et al., 2025a) is among the first fully open-source DeepResearch agents for long-form tasks. 2 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Figure 1. Overview of our method. (a) We construct diverse reporting queries and generate candidate reports. Human experts provide pairwise preference annotations based on usefulness, coherence, completeness, and alignment. (b) Given query and its preferred and rejected reports, we train rubric generator via GRPO to produce weighted evaluation rubrics, with rewards based on preference consistency, LLM-as-a-judge scores, and format validity. (c) Multi-Agent Markov-state (MaMs) workflow. shared policy executes structured workflow with search, chunking, state update, and report generation, interacting with external tools. Query-specific rubrics are used to compute rewards of the rollout reports under an individual query. 2.2. Rubrics for Reward Modeling Training agents for long-form report generation inherently involves weak supervision, as explicit ground-truth reports are rarely available, and correctness cannot be reduced to verifiable targets. Consequently, prior work commonly relies on human preference annotations or rubric-based evaluation to assess report quality, which introduces challenges for both training stability and alignment with human judgments. Existing approaches explore the use of fixed rubrics (Hashemi et al., 2024; Que et al., 2024; Shao et al., 2024a) as well as query-specific rubrics (Shao et al., 2025a; Xie et al., 2025) to provide evaluative feedback for longform outputs. More recently, several studies (Wang et al., 2024b; Liu et al., 2025b; Viswanathan et al., 2025; Gunjal et al., 2025) have further treated rubrics as reward models within reinforcement learning frameworks. In this work, we focus on providing principled reward signals, via rubric generators, for training DeepResearch agents to generate long-form reports using RL. In particular, we aim to learn rubric generators that produce query-aware evaluation criteria, enabling more stable optimization and better alignment with human preferences by providing finegrained and interpretable reward signals during RL training. 3. Method 3.1. Motivations Although expert annotators can provide high-quality evaluation rubrics, manually designing query-specific rubrics at scale is fundamentally impractical, as even highly trained experts struggle to produce consistent and fine-grained criteria across large and diverse set of queries. While such approaches are effective and have been explored in prior work (Yao et al., 2025; Sharma et al., 2025), their reliance on intensive expert effort limits their applicability to largescale DeepResearch training. Motivated by this limitation, our objective is to develop query-specific rubric generator aligned with human preferences, enabling the training of DeepResearch systems through reinforcement learning with more reliable and human-aligned reward signals. 3.2. Creation of the Preference Dataset Stage 1: Query Construction We begin by constructing diverse set of research-oriented queries that reflect realistic information needs in DeepResearch scenarios. Each query is formulated as an open-ended research prompt that requires multi-step reasoning, evidence synthesis, and structured long-form reporting, rather than short factual answers."
        },
        {
            "title": "Our original queries are automatically generated from a",
            "content": "3 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation and an auxiliary LLM-based verifier. This step removes reports with evident factual errors, disorganized or inconsistent citations, or content that exhibits superficial aggregation without coherent reasoning, ultimately retaining only the two highest-quality reports for annotation. Stage 3: Preference Annotation by Human Experts For human annotation, we recruit 16 human experts, each holding at least masters degree and capable of critically reading and evaluating long-form research reports. The experts perform pairwise comparisons between candidate reports generated for the same query. Given query and two candidate reports ra, rb R(q), annotators are asked to select the report they prefer overall, considering factors such as usefulness, coherence, completeness, and alignment with the information need expressed in q. Each comparison results in preferred report racc and less preferred report rrej, forming preference triple (q, racc, rrej). Aggregating all annotated comparisons yields the final human preference dataset = {(q, racc, rrej)}, which is used as supervision for modeling and evaluating preference-aligned report generation. By relying on expert relative judgments rather than absolute ratings, the dataset captures fine-grained human preferences that are difficult to express with generic or LLMgenerated evaluation metrics. 3.3. Training Rubric Generators with Hybrid Rewards To generate evaluation rubrics that are well aligned with human preferences, we train the rubric generator using Group Relative Policy Optimization (GRPO) (Shao et al., 2024b). Given query q, the policy model πθ samples group of rubric candidates {y1, y2, . . . , yG}, where each candidate yi specifies structured set of evaluation criteria. Following the rubric specification proposed by Gunjal et al. (2025), each rubric item is represented with three key fields: title, description, and an associated importance weight, forming weighted list of assessment dimensions (e.g., in JSON format). To robustly guide learning, we design hybrid reward function Rtotal that integrates three complementary signals: preference consistency reward, format reward, and an LLM-as-a-Judge quality reward. Formally, the overall training signal is computed as weighted combination of the above components: Rtotal = λprefRpref + λllmRllm + Rfmt (1) Preference Consistency Reward (Rpref). An effective rubric must be discriminative, i.e., capable of reflecting human preferences when applied to real reports. To this end, we leverage the preference dataset created in Section 3.2 that consists of triplets (q, racc, rrej), where racc is preferred by human annotators over rrej for the same query q. Given generated rubric y, we score report by computing the Figure 2. Topic distribution of our created human preference dataset for DeepResearch reports. knowledge graph constructed over entities from diverse domains. By leveraging the relational structure of the graph, we sample multi-hop entity paths and prompt an LLM to synthesize corresponding natural-language questions. This ensures each query is grounded in entity relations while requiring reasoning across multiple facts, making it suitable for evaluating deep research and synthesis. Figure 2 summarizes the topic distribution of our query dataset, which spans diverse domains commonly encountered in DeepResearch scenarios, such as Law, Business, Science, and Health, along with long tail of other types. We additionally rewrite queries with GPT-5 (OpenAI, 2025) to diversify their phrasing and naturalness, aligning them with realistic user questioning styles and naturally inducing variation in report quality. Formally, we denote the dataset with constructed queries as = {qi}N i=1, where each qi serves as the conditioning input for generating subsequent candidate reports. The case study of rewriting queries and detailed categories is shown in Appendix A. Stage 2: Candidate Report Generation via Multi-Agent Markov State Framework Given fixed query Q, we generate multiple candidate reports by varying hyperparameters across multiple LLMs, including DeepSeek V3.1 (Liu et al., 2024a) and TongyiDeepResearch (Tongyi et al., 2025), all of which have been trained on agentic data and have abilities of tool calling. To address the challenges arising from long-context dependencies in ReAct-style reasoning (Yao et al., 2022) and the multi-step nature of automated DeepResearch, we draw inspiration from prior work (Li et al., 2025b; Yu et al., 2025; Chen et al., 2025) and propose Multi-Agent MarkovState (MaMs) workflow, described in detail in Section 3.4. Using this workflow, report candidates are generated independently and without access to any human annotations. Before being submitted for human annotation, all candidates undergo filtering process involving both human reviewers 4 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation weighted average of item-level ratings: S(r y) = (cid:80)K k=1 wk vk (cid:80)K k=1 wk , (2) where wk denotes the weight of the k-th rubric item and vk is the corresponding conformity score assigned by judge LLM. Each vk is rated on 1-10 Likert scale (Zheng et al., 2023a; Kim et al., 2024) and linearly normalized to the range [0, 1] for aggregation. The preference consistency reward is determined by whether the rubric correctly ranks the human-preferred report above the rejected one: Rpref(y) = (cid:40) if S(racc y) > S(rrej y); +1, 1, otherwise. (3) Format Reward (Rfmt). Secondly, we enforce structural validity as hard constraint. Since the downstream evaluation pipeline requires machine-parsable rubric representations, each generated candidate is checked for compliance with the required JSON schema (including mandatory fields such as title, description, and weight). Candidates that fail this check receive penalty of 1, while structurally valid rubrics incur no additional reward. LLM-as-a-Judge Reward (Rllm). To assess the intrinsic quality of the generated rubrics, we further adopt an LLM-as-a-Judge mechanism that serves as semantic metaevaluator. Rather than relying on pre-defined rules, the judge evaluates rubric in the context of the query q, focusing on its logical coherence, coverage comprehensiveness, and the relevance of its evaluation dimensions, synthesizing these factors into scalar quality score (e.g., scaled to [0, 4]): These criteria are aggregated into scalar quality score Rllm = Judge(q, y). The specific prompt can be found in Appendix C.3. 3.4. Multi-Agent Markov-State Workflow with Rubric-Based GRPO After obtaining well-trained rubric generator, we leverage it to provide reliable and query-specific reward signals for training DeepResearch systems via GRPO. The DeepResearch framework in this paper is called the Multi-Agent Markov-State (MaMs) workflow. Unlike monolithic architectures that are prone to context saturation and error accumulation, our framework explicitly separates highlevel reasoning, information acquisition, and report synthesis into specialized agents that interact through an iterative state transition loop. Additionally, to improve the efficiency of the multi-agent system, we propose concurrent execution scheme, with details in Appendix F. State Abstraction and Iterative Transitions. We model the deep research process as sequential decision-making 5 problem over an abstract state space. For user query q, the research state at iteration turn is defined as st = mt, pt, rt. Here, mt represents the structured memory, pt denotes the dynamic execution plan, and rt is the incrementally evolving report. Unlike standard RAG systems that condition on raw retrieved context, our framework operates on this compact abstraction, ensuring scalability across long-horizon workflows. The transitions follow hierarchical structure: high-level search action triggers low-level state processing. Formally, st+1 = (st, at), where encapsulates the tool execution and the subsequent multiagent processing pipeline described below. For detailed algorithm description, please refer to Appendix B. Agent Modules and Chunk-based Process. The MaMs workflow consists of 3 specialized agents with clearly defined responsibilities, shown in Figure 1. Search Agent: Acting as the high-level controller, the search agent observes the current state st and determines the optimal next step. It generates search action at (e.g., generating <tool call></tool call>) and refines the global plan: at, = Asearch(q, st). This agent is responsible for identifying information gaps in mt and driving the exploration process. If sufficient information is gathered, the search loop terminates, and the output is finalized. State Agent: Upon execution of action at, the environment returns raw observation Ot (e.g., long search content). critical challenge is that Ot often exceeds the context window limits of the LLM. To address this, we follow MemAgent (Yu et al., 2025) to implement chunk-based processing mechanism. The raw text Ot is segmented into sequence of smaller chunks {c1, c2, . . . , cK} using text splitter that respects semantic boundaries (e.g., paragraphs). The state agent processes these chunks sequentially to update the memory and plan while minimizing information loss. Let mt,0 = mt and pt,0 = t. For each chunk {1, . . . , K}, the agent performs an incremental update: mt,k, pt,k = Astate(q, ck, mt,k1, pt,k1). (4) The prompt logic explicitly enforces an incremental fusion strategy: existing knowledge in mt,k1 is preserved, while new facts from ck are compressed and merged. After processing all chunks, the final state for the next iteration is established as mt+1 = mt,K and pt+1 = pt,K. Report Agent: The report agent incrementally refines the research report alongside state updates: rt,k = Areport(q, ck, mt,k1, rt,k1). (5) This design effectively decouples information compression (handled by state agent) from narrative generation (handled by report agent). The report agent uses the streaming evidence ck to draft, correct, and expand sections of the report Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Table 1. Evaluation results on the test set of our human preference dataset D. Baselines are introduced in Section 4.2 (c). We always choose GRPO as the algorithm in RL. Bold font indicates the best performance, whereas underline font denotes the second-best performance. Pref. Acc. denotes preference accuracy, i.e., the fraction of pairs where the accepted report receives higher score than the rejected one (equivalently, AUC). larger paired Cohens indicates stronger advantage of the chosen score over the rejected score. Model N/A GPT-5 Gemini-2.5-Pro Qwen3-14B Qwen3-30B-A3B Method Human-defined General Rubrics Pointwise Preference Scoring Pairwise Preference Judgment Generated Rubrics Pointwise Preference Scoring Pairwise Preference Judgment Generated Rubrics Pointwise Preference Scoring Pairwise Preference Judgment Generated Rubrics Supervised Fine-tuning RL with LLM-as-a-Judge Reward RL with Preference Reward RL with Hybrid Reward Pointwise Preference Scoring Pairwise Preference Judgment Generated Rubrics Supervised Fine-tuning RL with LLM-as-a-Judge Reward RL with Preference Reward RL with Hybrid Reward Query-Specific Rubrics Pref. Acc./AUC (%) 48.78 60.28 59.93 60.80 57.49 57.17 59.23 53.83 53.66 56.09 59.76 60.98 64.63 65.16 54.01 54.53 58.54 59.58 61.50 64.81 65.68 Paired Cohens 0.192 0.315 0.328 0.297 0.302 0.254 0.246 0.260 0.303 0.359 0.366 0.246 0.314 0.317 0.296 0.384 0.376 rt, ensuring global consistency and reducing the hallucination risk associated with generating long reports in single pass. Once termination conditions, maximum turns or no further tool calls, are met, the final report rfinal is produced. Although the system adopts multi-agent architecture at the functional level, all agents are instantiated from the same LLM. Specifically, these three agents share single policy model πθ and differ only in their role-specific prompts, action spaces, and state interfaces. As result, MaMs can also be viewed as structured single-agent formulation with modularized behaviors, rather than multi-agent learning system with independently optimized policies. This design isolates architectural benefits from model heterogeneity, ensuring that observed behaviors arise from agent specialization rather than differences in model capacity. Reward Assignment with Weighted Rubrics. For each query q, the rubric generator produces list of evaluation rubrics with associated weights, capturing query-specific notions of report quality. After the system generates set of candidate reports for q, we employ an LLM-as-a-Judge to score each generated report according to these weighted rubrics. The resulting scalar reward is computed following the same weighted aggregation scheme defined in Equation (2), and is used to supervise policy optimization. Please refer to Appendix for all prompts. 6 4. Experiments In this section, we conduct series of experiments to address the following research questions: RQ1: Can rubric generator trained with GRPO effectively capture human preferences over generated reports? RQ2: Does the rubric generator provide more reliable reward signals when used to train DeepResearch agents? RQ3: Does the proposed multi-agent Markov-state workflow outperform the conventional ReAct-style framework in complex DeepResearch tasks? 4.1. Experimental Settings Datasets. To address RQ1, we partition the constructed human preference dataset into training, validation, and test splits with ratio of 8:1:1, where 10% of the data is reserved for testing. The split is performed in topic-balanced manner, ensuring that all topics are represented across the training, validation, and test sets. To address RQ2 and RQ3, we evaluate our method on the widely adopted DeepResearch Bench (Du et al., 2025), which comprises 100 queries, 50 in Chinese and 50 in English, spanning diverse range of research topics. Implementation Details. All experiments are performed on dedicated large-scale GPU cluster equipped with Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Table 2. Evaluation Results on DeepResearch Bench. The rubric generator trained by RL refers to the Qwen3-30B-A3B model trained in the previous step using GRPO with hybrid rewards. Performance of closed-source models is sourced from the official leaderboard, while WebThinker-32B-DPO and DRTulu-Qwen3-8B-RL are obtained from DRTulu. Bold font indicates the best performance, whereas underline font denotes the second-best performance. All our models are trained on the queries in the training set of our created dataset D. Model Workflow Rubric Strategy During RL DeepResearch Bench (Du et al., 2025) Comp. Depth Inst. Read. Overall OpenAI DeepResearch Claude Research Gemini DeepResearch 46.5 45.3 49.5 43.7 42.8 49.5 Closed-Source DeepResearch WebThinker-Qwen2.5-32B-DPO WebThinker DRTulu-Qwen3-8B-RL WebWeaver-Qwen3-30B-A3B ReAct WebWeaver Qwen3-30B-A3B Tongyi-DeepResearch ReAct ReAct MaMs ReAct ReAct MaMs MaMs MaMs MaMs MaMs Open-Source DeepResearch N/A Self-Evolving Rubrics N/A Ours N/A Rubric Generator Trained by RL Rubric Generator Trained by RL N/A Rubric Generator Trained by RL N/A Human-defined General Rubrics GPT-5 Generated Rubrics Rubric Generator Trained by SFT Rubric Generator Trained by RL 39.4 41.7 45.2 33.8 38.5 41.5 39.5 43.4 38.9 40.5 41.1 40.5 48.3 35.4 41.8 45.8 29.9 37.9 41.8 34.4 42.5 38.5 39.8 39.7 40.9 48. 49.4 47.6 50.1 46.0 48.2 49.2 39.1 46.0 49.0 46.2 48.8 47.3 48.2 48.5 48.2 50.7 47.2 44.7 50.0 43.5 41.3 47.3 40.0 43.4 46.9 44.3 48.0 44.5 45.4 46.5 45.7 50. 46.5 45.0 49.7 40.6 43.4 46.8 35.0 41.0 44.3 40.5 45.2 41.8 42.9 43.4 43.4 49.3 NVIDIA H20 GPUs. Rubric generators are trained on 8 H20 GPUs, while training of DeepResearch agents within the MaMs workflow utilizes 32 H20 GPUs. To enable largescale, high-concurrency inference for both rubric scoring and LLM-as-a-Judge evaluation during reinforcement learning, we further deploy 192 H20 GPUs using the vLLM framework (Kwon et al., 2023) to run Qwen3-235B-A22B, ensuring that all requests are processed end-to-end within 4 minutes. Full implementation, infrastructure details, and hyperparameters are provided in Appendix D. Metrics. We evaluate the performance of preference modeling using two complementary metrics. Preference accuracy measures how often the model assigns higher score to the preferred response than to the rejected one, and is equivalent to the AUC in pairwise preference evaluation, reflecting ranking correctness. To further assess the magnitude and stability of preference separation, we report the paired Cohens d, which quantifies the standardized difference between accepted and rejected responses across queries. While preference accuracy captures whether the ordering is correct, Cohens (Diener, 2010) characterizes how strongly and consistently the model distinguishes preferred outputs, providing more nuanced view of preference quality. In DeepResearch Bench, comprehensiveness, Depth, instruction following, and readability are predefined evaluation dimensions, each assessed by LLM judges using the official benchmark prompts. Baselines. For human preference evaluation, we consider the following baselines: (1) Human-defined General Rubrics, which adopt manually specified evaluation rubrics following the general report rubrics proposed in Yao et al. (2025); (2) Pointwise Preference Scoring, where the accepted and rejected reports (racc, rrej) in each triplet (q, racc, rrej) are scored independently by the model, and preference is determined by score comparison; (3) Pairwise Preference Judgment, where (racc, rrej) are jointly provided to the model for direct preference judgment; (4) Generated Rubrics, which prompt the model to generate queryspecific rubrics from q, followed by LLM-based evaluation; (5) Supervised Fine-Tuning (SFT), which uses GPT-5generated rubrics as supervision targets; and (6) Reinforcement Learning with Various Rewards, where GRPO is applied with different reward weight configurations in Equation (1). For DeepResearch Bench, closed-source baselines are reported directly from the official leaderboard. Due to resource constraints, we are unable to reproduce the DRTulu system with Qwen3-30B-A3B as the backbone model. Besides, we also compare the ReAct with our MaMs workflow. The tool performs keyword-based search and returns full retrieval results, while for the ReAct (search-then-generate) framework, we provide summarized version of the results to prevent output length issues and ensure effective reasoning. For fairness, all results are obtained using the checkpoint with the best validation performance. Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation 4.2. Evaluation on Human Preferences We report the results on the test set of the human preference dataset in Table 1, which directly addresses RQ1. Several key observations can be drawn: (1) Methods based on query-specific rubrics consistently outperform those relying on human-defined general rubrics. As shown in the first block of Table 1, general rubrics yield near-random preference accuracy and small effect size, whereas generated, query-conditioned rubrics substantially improve both preference accuracy and paired Cohens d. This confirms that incorporating query-specific evaluation criteria is crucial for providing reliable reward signals in downstream training. (2) Directly applying strong LLMs (e.g., GPT-5) to generate rubrics, or performing supervised fine-tuning on such rubrics, does not sufficiently capture fine-grained human preferences. Although these approaches achieve gains in preference accuracy, their paired Cohens remains relatively small and does not exhibit clear separation between accepted and rejected reports, indicating limited alignment with human preference margins. (3) Reinforcement learning with preference-based rewards leads to pronounced improvement in paired Cohens across both Qwen backbones. The increasing Cohens reflects growing score margin between accepted and rejected reports, suggesting that the model becomes progressively better at discriminating reports preferred by humans. Moreover, RL with hybrid rewards achieves the best overall performance, combining strong preference accuracy with consistently large effect sizes, demonstrating the complementary benefits of integrating preference signals with auxiliary reward components. 4.3. Results on DeepResearch Bench Through Table 2, we can answer RQ2 and RQ3. First, comparing different rubric strategies under the same TongyiDeepResearch backbone, we observe that the rubric generator trained with RL consistently achieves the best performance across all evaluation dimensions. It clearly outperforms human-defined general rubrics, GPT5-generated rubrics, and the SFT-trained generator, indicating that the proposed rubric generator provides more reliable and informative reward signals for training DeepResearch agents. This validates RQ2, demonstrating that learning rubrics through reinforcement learning yields more effective supervision compared to static or purely supervised alternatives. We note that Tongyi-DeepResearch exhibits stronger toolcalling and execution abilities than Qwen3-30B-A3B, while being less specialized in report generation than WebWeaver agents, as reflected in the table. This makes it suitable backbone for our study, as it allows us to better examine the (a) (b) Figure 3. Comparison between GSPO and GRPO during training rubric generators (Qwen3-30B-A3B). (a) Reward curves of generated rollouts under the two algorithms, showing nearly identical reward values. (b) Entropy of generated rollouts, where GSPO consistently exhibits higher entropy than GRPO. effectiveness of rubric learning and workflow design, rather than relying on strong inherent generation capability. Second, models equipped with the proposed MaMs consistently outperform their ReAct-style counterparts under the same rubric strategy. In particular, Tongyi-DeepResearch, equipped with the MaMs workflow and an RL-trained rubric generator, achieves the strongest overall performance among all open-source methods, with clear gains in comprehensiveness, instruction following, readability, and overall score. These results demonstrate that explicitly structuring the research process as state-conditioned workflow enables more effective reasoning and information integration than the conventional ReAct paradigm, thereby answering RQ3. 4.4. Analysis on Entropy over Two RL Algorithms Since Qwen3-30B-A3B is Mixture-of-Experts model, applying GRPO during training may introduce mismatch between the expert routing used for optimization and that employed during rollout. To mitigate this issue, we additionally explore GSPO (Zheng et al., 2025) for training the rubric generator. Although GSPO and GRPO share identical training configurations, we observe that rubric generators trained with GSPO consistently produce rollouts with higher entropy, as shown in Figure 3, despite achieving nearly identical reward values on the generated samples. We attribute this behavior to GSPOs sequence-level optimization scheme, where importance weighting and clipping are performed over entire responses rather than individual tokens. This design reduces sensitivity to local token-level deviations and allows multiple realizations with similar global rewards to coexist, leading to increased output diversity. In contrast, GRPO applies group-wise relative advantages over complete rollouts but relies on token-level likelihood ratios, which implicitly impose stronger structural constraints Given that rubric generation prioritizes stability, consistency, Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation and preference alignment rather than linguistic diversity, we adopt GRPO as it better matches the mode-seeking nature of the task. The performance of rubric generators trained with GSPO is reported in Appendix G. Additional analyses on tool-calling behavior are provided in Appendix H. 5. Conclusion In this work, we address core challenge in DeepResearch report generation: obtaining scalable, reliable, and humanaligned supervision without explicit golden signals. Instead of relying on predefined or human-annotated rubrics, we learn query-specific rubric generators from human preferences. Combined with LLM-based rubric evaluation under GRPO, our method produces discriminative and adaptable rubrics that provide stronger training and evaluation signals, leading to consistent improvements in report generation. More broadly, our results highlight learning evaluative criteria as promising direction for preference-aligned training in complex tasks. Limitations and future work are discussed in Appendix J."
        },
        {
            "title": "Accessibility",
            "content": "We have made efforts to ensure that this submission is as accessible as possible to broad audience, including readers with disabilities or sensory and neurological differences."
        },
        {
            "title": "Software and Data",
            "content": "We have shown all the prompts in the Appendix, and we will release the weights of our models on Huggingface upon acceptance."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. Claude takes research to new places. https: //www.anthropic.com/news/research, 2025. Accessed: 2025-04. Chen, G., Qiao, Z., Chen, X., Yu, D., Xu, H., Zhao, W. X., Song, R., Yin, W., Yin, H., Zhang, L., et al. Iterresearch: Rethinking long-horizon agents via markovian state reconstruction. arXiv preprint arXiv:2511.07327, 2025. Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe rlhf: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2023. Diener, M. J. Cohens d. The Corsini encyclopedia of psychology, pp. 11, 2010. Du, M., Xu, B., Zhu, C., Wang, X., and Mao, Z. Deepresearch bench: comprehensive benchmark for deep research agents. ArXiv, abs/2506.11763, 2025. Google. Gemini deep research your personal research assistant. https://gemini.google/overview/ deep-research/, 2025. Accessed: 2025-03. Gunjal, A., Wang, A., Lau, E., Nath, V., He, Y., Liu, B., and Hendryx, S. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hashemi, H., Eisner, J., Rosset, C., Van Durme, B., and Kedzie, C. Llm-rubric: multidimensional, calibrated approach to automated evaluation of natural language In Proceedings of the 62nd Annual Meeting of texts. the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1380613834, 2024. Huang, Z., Zhuang, Y., Lu, G., Qin, Z., Xu, H., Zhao, T., Peng, R., Hu, J., Shen, Z., Hu, X., et al. Reinforcement learning with rubric anchors. arXiv preprint arXiv:2508.12790, 2025. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S. O., Wang, D., Zamani, H., and Han, J. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 43344353, 2024. Krishna, K., Roy, A., and Iyyer, M. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 49404957, 2021. 9 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Li, X., Jin, J., Dong, G., Qian, H., Wu, Y., Wen, J.-R., Zhu, Y., and Dou, Z. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025a. Li, Z., Guan, X., Zhang, B., Huang, S., Zhou, H., Lai, S., Yan, M., Jiang, Y., Xie, P., Huang, F., Zhang, J., and Zhou, J. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025b. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, J., Li, Y., Zhang, C., Li, J., Chen, A., Ji, K., Cheng, W., Wu, Z., Du, C., Xu, Q., et al. Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501, 2025a. Liu, T., Xu, R., Yu, T., Hong, I., Yang, C., Zhao, T., and Wang, H. Openrubrics: Towards scalable synthetic rubric generation for reward modeling and llm alignment. arXiv preprint arXiv:2510.07743, 2025b. Liu, W., Wang, X., Wu, M., Li, T., Lv, C., Ling, Z., JianHao, Z., Zhang, C., Zheng, X., and Huang, X. Aligning large language models with human preferences through representation engineering. In Association for Computational Linguistics, 2024b. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. OpenAI. Gpt-5 system card. 2025. OpenAI. Introducing deep research. https://openai. com/index/introducing-deep-research/, 2025. Accessed: 2025-02. Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2024. Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Que, H., Duan, F., He, L., Mou, Y., Zhou, W., Liu, J., Rong, W., Wang, Z. M., Yang, J., Zhang, G., et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. Qwen Team. Qwen DeepResearch: When inspiration becomes its own reason. https://qwen.ai/blog? id=qwen-deepresearch, November 2025. Accessed: 2025-12-23. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. Russell, L., Hu, A., Bertoni, L., Fedoseev, G., Shotton, J., Arani, E., and Corrado, G. Gaia-2: controllable multiview generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. Shao, R., Asai, A., Shen, S. Z., Ivison, H., Kishore, V., Zhuo, J., Zhao, X., Park, M., Finlayson, S. G., Sontag, D., et al. Dr tulu: Reinforcement learning with evolving rubrics for deep research. arXiv preprint arXiv:2511.19399, 2025a. Shao, Y., Jiang, Y., Kanell, T., Xu, P., Khattab, O., and Lam, M. Assisting in writing wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 62526278, 2024a. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024b. Shao, Z., Luo, Y., Lu, C., Ren, Z., Hu, J., Ye, T., Gou, Z., Ma, S., and Zhang, X. Deepseekmath-v2: Towards self-verifiable mathematical reasoning. arXiv preprint arXiv:2511.22570, 2025b. Sharma, M., Zhang, C. B. C., Bandi, C., Wang, C., Aich, A., Nghiem, H., Rabbani, T., Htet, Y., Jang, B., Basu, S., et al. Researchrubrics: benchmark of prompts and rubrics for evaluating deep research agents. arXiv preprint arXiv:2511.07685, 2025. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Tongyi, D., Li, B., Zhang, B., Zhang, D., Huang, F., Li, G., Chen, G., Yin, H., Wu, J., Zhou, J., et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36: 4659546623, 2023a. Zheng, L., Yin, L., Xie, Z., Sun, C. L., Huang, J., Yu, C. H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J. E., et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024. Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023b. Zhou, P., Leon, B., Ying, X., Zhang, C., Shao, Y., Ye, Q., Chong, D., Jin, Z., Xie, C., Cao, M., et al. Browsecompzh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. Zhu, Z., Xie, C., Lv, X., and slime Contributors. slime: An llm post-training framework for rl scaling. https:// github.com/THUDM/slime, 2025. GitHub repository. Corresponding author: Xin Lv. Viswanathan, V., Sun, Y., Ma, S., Kong, X., Cao, M., Neubig, G., and Wu, T. Checklists are better than reward models for aligning language models. arXiv preprint arXiv:2507.18624, 2025. Wang, B., Zheng, R., Chen, L., Liu, Y., Dou, S., Huang, C., Shen, W., Jin, S., Zhou, E., Shi, C., et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv preprint arXiv:2401.06080, 2024a. Wang, H., Xiong, W., Xie, T., Zhao, H., and Zhang, T. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Conference on Empirical Methods in Natural Language Processing, 2024b. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Xie, L., Huang, S., Zhang, Z., Zou, A., Zhai, Y., Ren, D., Zhang, K., Hu, H., Liu, B., Chen, H., et al. Auto-rubric: Learning to extract generalizable criteria for reward modeling. arXiv preprint arXiv:2510.17314, 2025. Xu, F., Song, Y., Iyyer, M., and Choi, E. critical evaluation of evaluations for long-form question answering. In Association for Computational Linguistics, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Yao, Y., Wang, Y., Zhang, Y., Lu, Y., Gu, T., Li, L., Zhao, D., Wu, K., Wang, H., Nie, P., et al. rigorous benchmark with multidimensional evaluation for deep research agents: From answers to reports. arXiv preprint arXiv:2510.02190, 2025. Yifei, L. S., Chang, A., Malaviya, C., and Yatskar, M. Researchqa: Evaluating scholarly question answering at scale across 75 fields with survey-mined questions and rubrics. ArXiv, abs/2509.00496, 2025. Yu, H., Chen, T., Feng, J., Chen, J., Dai, W., Yu, Q., Zhang, Y.-Q., Ma, W.-Y., Liu, J., Wang, M., et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 11 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation A. Case Study on Query Rewriting Our created query set covers broad range of domains relevant to DeepResearch scenarios. High-frequency categories include Law & Regulation, Business & Finance, Science & Technology, and Health & Medical Care, reflecting common research-oriented information needs that require multi-step reasoning and evidence synthesis. The dataset also contains diverse set of mediumand low-frequency topics, such as Media & Entertainment, Daily Life, Education, Arts, and Trending News, as well as long-tail domains including Academic Literature and Job & Career. This distribution mirrors realistic usage patterns of DeepResearch systems, supporting the study of human preferences across heterogeneous report-generation tasks. The following is case study of the query rewriting: Original Query: In Alices Adventures in Wonderland, what is the most common eye color corresponding to the real-life cat breed that inspired the Cheshire Cat? DeepResearch-style Query: Please conduct study on the Cheshire Cat from Alices Adventures in Wonderland: identify the most likely real-world cat breed that served as its inspiration, and summarize the breeds most common coat colors along with the typical eye color associated with each coat. B. Global Algorithm of the MaMs workflow We show the detailed algorithm description in Algorithm 1. Algorithm 1 Multi-agent Markov-state (MaMs) Workflow Execute action at and obtain raw observation Ot Split Ot into semantically coherent chunks {c1, . . . , cK} // chunking handles long context and enables incremental processing Initialize chunk-level states: mt,0 mt, pt,0 State Agent update: // incremental memory & plan update Report Agent update: // incremental report generation t, rt,0 rt t: refined plan for = 1 to do Search Agent: // high-level controller Generate action and updated plan at, = Asearch(q, st) // at: search action, 1: Input: User query q, maximum iterations 2: Initialize: memory m0, plan p0, report r0 3: for = 0 to 1 do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: end for 27: Return: Final report rfinal // st+1 = mt+1, pt+1, rt+1 if termination condition is satisfied then end if rt,k = Areport(q, ck, mt,k1, rt,k1) end for Update global state after all chunks processed mt+1 mt,K, pt+1 pt,K, rt+1 rt,K mt,k, pt,k = Astate(q, ck, mt,k1, pt,k1) // stop if max turns reached, or plan indicates no further search needed break 12 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation The termination condition is triggered when either (i) the maximum number of interaction turns is reached, and the system is forced to stop and produce final report, or (ii) the Search Agent determines that no further information acquisition is required according to the current plan. Therefore, the final report rfinal is conditionally equal to rT . C. Prompts Used in MaMs Workflow and LLM-as-a-Judge For each prompt, we have both Chinese and English versions, as the question dataset is bilingual. In this section, we present the English version, while the corresponding Chinese version is included in the supplementary material. C.1. Prompts for Generating Query-Specific Rubrics You are professional rubric-writing expert. Your task is to generate coherent and self -contained set of evaluation rubrics based on given **report-generation query**, which will be used to assess the quality of generated response (i.e., report). Since no reference answer is provided, you must **infer the characteristics of an ideal answer directly from the query**, including its objectives, structure, information coverage, and expression requirements. The evaluation rubrics should include, but are not limited to, the following aspects: * Factual relevance and accuracy of the content * Structure and logical organization of the report * Completeness and depth of information * Soundness of reasoning and argumentation * Clarity and coherence of expression * Appropriateness of tone and style with respect to the reports intent (e.g., summary, analysis, recommendation) Each rubric item must be **self-contained**, so that non-expert reader can understand it independently without additional context. Each description must begin with one of the following prefixes: - Key Criterion: ... - Important Criterion: ... - Optional Criterion: ... - Error Criterion: ... --- ### **Input:** * query: the full text of the report-generation request ### **Number of Rubric Items:** * Select between 7 and 20 rubric items depending on the complexity of the query. ### **Each rubric item must include:** * title (2-6 words) * description: one sentence, starting with category prefix and clearly stating what should be observed in the generated report * weight: numeric value * Key / Important / Optional criteria take values from 1-5 (5 = most important) * Error criteria take values of -1 or -2 (indicating penalties) --- ### **Category Definitions:** * **Key Criterion**: Core facts, structure, or objectives that must be present; missing them makes the answer invalid (weight = 5) * **Important Criterion**: Critical reasoning, completeness, or clarity that significantly affects quality (weight = 3-4) Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation * **Optional Criterion**: Stylistic or depth-related enhancements (weight = 1-2) * **Error Criterion**: Common mistakes or omissions, explicitly indicating missing or incorrect elements (weight = -1 or -2) --- ### **Additional Guidelines:** * If the report should include conclusions or recommendations, include: Key Criterion: Includes clear conclusion supported by evidence. * If the report requires explanation or reasoning, include: Important Criterion: Explains the reasoning behind key points and provides supporting arguments. * If the report requires clear structure, include: Key Criterion: Organizes content with clear sections and logical flow. * If the report has specific tone (e.g., academic, policy-oriented, business), include: Important Criterion: Maintains professional and objective tone consistent with the report context. * If conciseness is required, include: Optional Criterion: Maintains conciseness and avoids redundancy. --- ### **Output Requirements:** * Output JSON array in the format: [{...}, {...}, ...], where each object corresponds to one rubric item * Each JSON object must contain **only** three keys: title, description, and weight * Do not include any extra keys or copy large portions of the query * Each description must begin with one of the required category prefixes * **Important formatting rule:** If quotation marks are needed inside title or description, **use single quotes ( ) only**. Do NOT use double quotes (\" \"), as they will break the JSON format. Example: use Michelin star instead of \"Michelin star\". --- ### **Summary:** Your task is to **infer the essential qualities of an ideal report solely from the given query**, and construct structured, weighted rubric in JSON format to evaluate reportgeneration quality. Return **only** the requested JSON array. Do not include any additional explanations or text. C.2. Prompts for Scoring Report by Single Rubric through LLM-as-a-Judge You are precise and impartial scoring model. Your task: evaluate the degree to which report aligns with given single rubric description, based solely on that rubric. **Input Information** Query: {query} Rubric: {rubric} Report to be scored: {report} **Scoring Instructions** - You only need to judge how well the report \"matches\" the description of the rubric. - Do not judge whether the rubric represents positive goal or negative constraint. - Do not attempt to reverse or correct the semantic direction of the rubric. - Do not introduce any additional evaluation criteria. 14 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation **Scoring Requirements** - Output an integer score from 1 to 10: - 10 = report fully aligns with the rubric description - 7-9 = largely aligns - 4-6 = partially aligns - 1-3 = largely does not align **Output Format** (strict, single line, no punctuation): rating: <integer from 1 to 10> C.3. Prompts for LLM-based Judgement of Rubrics You are an accurate and impartial scoring model (Reward Model). Your task is to evaluate the quality of **rubrics** (evaluation criteria). rubric is set of standards used to assess the quality of model-generated answers. You need to determine whether the given rubric is reasonable, comprehensive, and aligned with the task objective. Based on the following information, you should assess how well the rubric generated by the policy model (response) matches your criteria. **[Input Information]** Question: {question} Rubric to be evaluated (response): {response} **[Scoring Requirements]** You must output three items: 1. **[reward]**: decimal number ranging from 0.00 to 4.00 (up to two decimal places). * 4.00 = High quality: clear structure, comprehensive dimensions, rigorous logic, and strong alignment with the question. * 3.00 = Generally reasonable: covers key dimensions but with minor omissions or less concise expression. * 2.00 = Partially reasonable: covers some important aspects, but lacks key elements or has notable logical flaws. * 1.00 = Weakly related: low relevance to the task or serious format issues. * 0.00 = Completely irrelevant or meaningless: does not meet the evaluation purpose or is empty/garbled. 2. **[confidence]**: Your confidence in the score (0%-100%). higher value indicates greater certainty. 3. **[reason]**: brief explanation of the scoring rationale. **[Important Note]** You are evaluating the *design quality of the rubric itself*, not the quality of any report or answer. **[Output Format]** (strictly three lines, no punctuation): reward: <decimal between 0.00 and 4.00> confidence: <integer percentage between 0% and 100%> reason: <brief explanation in English> C.4. System Prompt of Search Agent in MaMs workflow You are an intelligent assistant capable of generating high-quality deep research reports. Your goal is to solve complex user problems through multiple cycles of \"Plan-ExecuteObserve.\" 15 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation ### Core Process 1. **Analyze State**: Review the current <memory> (information obtained so far) and < plan> (current progress). 2. **Develop Strategy**: - If information is insufficient or the plan is incomplete -> update the Plan and use tools (e.g., search) to gather information. - If information is sufficient and the plan is complete -> organize your thoughts and output the final report. 3. **Output Specifications**: - Update the plan table <plan>...</plan>: mark completed items and list remaining tasks. - Final action: either invoke tool or output <answer>...</answer>. ### Notes - **Plan**: must be Markdown list, clearly showing current and upcoming steps. - **Answer**: generate <answer>...</answer> only when you are confident all necessary information has been collected. **Tool Instructions**: {tool description} C.5. User Prompt of Search Agent in MaMs workflow <user_input> {{ query }} </user_input> <memory> {{ memory }} </memory> <plan> {{ plan }} </plan> <report> {{ report }} </report> Remaining tool call chances: {{ tool_call_chance }}. Based on the current state (Memory/Plan) and the completeness of <report>, plan the next action. If the current <report> is unsatisfactory, continue updating <plan> and use tools to search. If the <report> is deemed complete, directly output <answer>...</answer> to finish. Strictly follow the output format: <plan>Updated execution plan</plan> <tool_call>Tool invocation details (if any)</tool_call> or <answer>End</answer> When the count of the tool calling action meets the threshold (default 10), then we will change the user prompt as: Tool call chances have been exhausted. Based on the following information: <user_input> {{ query }} </user_input> <memory> {{ memory }} </memory> <plan> {{ plan }} 16 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation </plan> List your final plan. Do not call tools again. C.6. System Prompt of State Agent in MaMs workflow You are an information processing expert responsible for maintaining \"long-term memory\" database. You are currently in multi-step process of reading long text in chunks. ### Task Objective After reading the current \"Observation Fragment,\" you need to **incrementally merge** newly discovered information into the existing <memory>. Note: the input <memory> contains all previously accumulated key information. **When updating via compression, details are easily lost, so you must take all measures to prevent this.** ### Core Principles 1. **Preserve Old Memory (Most Important)**: - Information in the input <memory> that is not mentioned in the current fragment ** must be retained** in the output. - Do not remove information from Memory just because it is absent in the current fragment. 2. **Incremental Integration**: . - Only add facts, data, or insights that are **new** from the current fragment to Memory - If new information corrects old information, modify it; if it is redundant, ignore it. 3. **Maintain High Density**: - Memory should be \"pile of facts,\" not an article summary. - Preserve specific numbers, names, dates, and references. Do not write \"a detailed discussion about XX\"; instead, write \"XX stated that YYY.\" ### Steps 1. Read the input <memory> (old knowledge). 2. Read the Tool Output below (new fragment). 3. Output new <memory>: it = old memory + new knowledge from the fragment. ### Output Format Strictly follow this format for use in the next decision step: <memory>Updated memory integrating old and new information</memory> C.7. User Prompt of State Agent in MaMs workflow <user_input> {{ query }} </user_input> <memory> {{ memory }} </memory> <plan> {{ plan }} </plan> Please read the following tool output fragment. Task: extract key information to update <memory>. Strictly follow the output format: <memory>Updated key retrieved information summary</memory> C.8. System Prompt of Report Agent in MaMs workflow Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation You are professional structured analysis report writing assistant, responsible for maintaining <report> that is continuously updated based on user input. Your goal is to incrementally update the existing <report> based on the tool-provided information, **without introducing external information**. ### Workflow When you receive the user query <user_input>, key information summary <memory>, execution plan <plan>, the current round report <report>, and new information from tool calls, perform the following steps: 1. Analyze the type of new information. 2. Decide whether the new information should be included in the updated report. 3. If it should be included, update the original report: - Do not simply append new information; instead, supplement, correct, or replace content while maintaining logical flow. - Avoid expanding the scope of content unnecessarily. ### Core Principles 1. Update the report solely based on user-provided information: - Do not add external facts, speculative information, fabricated data, or extrapolated scenarios. Do not infer information not present in reality. - Do not add uncertainty disclaimers in the report. 2. **Do not simply append new information**: - Assess whether new information is relevant; if so, integrate it into the corresponding section. Otherwise, omit it. - Structure may be optimized if necessary, but core content must remain stable. 3. Maintain logical consistency: - If new information conflicts with the existing report, carefully decide whether to replace the old information based on current knowledge. - The report must not contain contradictory statements. ### Report Requirements 1. Output <report> in Markdown format. 2. Ensure <report> has clear structure, rigorous logic, and high readability. 3. At the end of <report>, list all necessary references or sources (each numbered, with full citation), avoiding duplicates. 4. Citation formatting rules: - In the report body, superscript citations may be used, e.g., <sup>[1]</sup>. - If superscripts are used, the corresponding entry must be included in the \"References\" section. - Superscripts must immediately follow the cited noun or term, not at the beginning of sentence. Correct examples: ...the law<sup>[1]</sup> states..., Article 1 of the Civil Code<sup>[4]</sup> stipulates.... ### Output Format Strictly follow this format: <report>Complete report content</report> C.9. User Prompt of Report Agent in MaMs workflow <user_input> {{ query }} </user_input> <memory> {{ memory }} </memory> <report> {{ report }} </report> C.10. System Prompt in ReAct workflow Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation You are deep research expert. You need to use search tools to investigate the question posed by the user and eventually produce comprehensive and in-depth report. Your research process follows the steps below: **Research Process** 1. Carefully read and analyze the users question, considering what information the user needs. 2. Develop detailed research plan by breaking the users question into multiple subquestions. If necessary, further decompose sub-questions until each is simple enough. For each decomposed question, create search plan. 3. In the same round as planning, perform the first round of tool calls. To increase efficiency, you may generate at most {{max_tool_call_cnt_per_round}} tool calls per message. 4. Enter the \"Plan Revision - Search\" loop. In each iteration: (1) Organize the results returned by the search tools. Consider what information is still missing and whether new leads need to be explored. If needed, revise your search plan and ensure it covers all potential user concerns, adding supplementary searches as necessary. (2) Check whether the latest search plan still contains questions that need searching. If so, generate new round of tool calls, again limited to {{ max_tool_call_cnt_per_round}} per message. Then wait for the search results. (3) If in step (2) you determine that the search plan is complete and you have enough information to write the report, synthesize the search results into comprehensive and insightful report through logical inference rather than listing facts. Do not perform further tool calls; the process will automatically end. **Requirements** 1. Mandatory Tool Calls: While research is ongoing, every assistant message **must include tool_calls**. If reply contains only text and no tool calls, the task is considered complete. 2. Multi-Round Limit: Complete all research within {{max_turn}} rounds, i.e., maximum of {{max_turn}} messages. 3. Per-Round Call Limit: Each message may generate at most {{max_tool_call_cnt_per_round}} tool calls. 4. Search Breadth: When developing or revising the search plan, consider all possible directions relevant to the research question and collect as much information and detail as possible. 5. Search Depth: Do not only search for \"what it is\"; focus on \"why\" and \"how it works.\" For key phenomena, explore underlying mechanisms or deeper causes. If current search result mentions critical concept, technology, or contradiction, prioritize investigating it in the next round rather than switching to parallel topic. Avoid wasting too many rounds on deep tracing. 6. Report Requirements: - Avoid information dumping: The report may be divided into sections, but strictly avoid listing retrieved facts without synthesis. The core value of the report is transforming fragmented information into logically connected, systematic discussion. - Logical Completeness: Every main point must have full argument arc: state the core conclusion -> provide concrete evidence (e.g., data, cases, details) -> explain underlying mechanisms or relevance (i.e., why or what it implies). - Substantive Content: Avoid empty adjectives (e.g., \"highly effective,\" \"promising\"). Use concrete technical parameters, quantitative metrics, regulatory details, or expert opinions from search results. - Multi-Dimensional Perspective: For complex issues, analyze from multiple dimensions (e .g., cause analysis, risk assessment, long-term impact, technical path comparison) ensuring each dimension is sufficiently supported. **Citation Standards** 1. **In-text citations**: Use superscript format in the report body, e.g., \"This is an important conclusion<sup>[1]</sup>.\" 2. **Reference List**: At the end of the report, list all references. Include **full article title and URL**. If the search result does not provide URL, only include the title. Format: [1] Article Title - URL [2] Article Title - URL 19 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation 3. **Ordering**: Number references in the order of their first appearance in the text. 4. **Deduplication**: If the same source is cited multiple times (even across rounds), merge into single entry with the same number; do not duplicate. 5. **Source Extraction**: Use the titles from search result summaries formatted as [Title: xxxx] directly as reference names. **Tool Instructions**: {tool description} C.11. Prompt for Pairwise Preference Judgment Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the users instructions and answers the users question better. Your evaluation should consider factors such as helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide short explanation. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. User Question: {question} [The Start of Assistant As Answer] {answer_a} [The End of Assistant As Answer] [The Start of Assistant Bs Answer] {answer_b} [The End of Assistant Bs Answer] Please output your final verdict by strictly following this format: \"[[A]]\" if Assistant is better, \"[[B]]\" if Assistant is better. C.12. Prompt for Pointwise Preference Scoring Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You should give score between 1 and 10, where 1 is the worst and 10 is the best. User Question: {question} [The Start of Assistants Answer] {answer} [The End of Assistants Answer] Please output your final verdict by strictly following this format: \"[[score]]\", for example \"[[8]]\". C.13. Prompts of DeepResearch Bench The prompts used to evaluate generated reports on the DeepResearch Bench are directly adopted from the official prompts released on GitHub1. 1https://github.com/Ayanami0730/deep_research_bench/tree/main/prompt 20 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation D. Implementation Details Our training code is based on the post-training framework slime2 (Zhu et al., 2025), which leverages Megatron (Shoeybi et al., 2019) for the training backend and SGlang (Zheng et al., 2024) for the inference backend. Note that there is crucial update3 on the Megatron config of optimizers to this framework, ensuring the correct training of MoE models when reinforcement learning. We show hyperparameters for training rubric generators in Table 3 and hyperparameters for training DeepResearch Agents based on Tongyi-DeepResearch in Table 4. For evaluation, we follow the official DeepResearch Bench protocol and adopt Gemini-2.5-Pro as the LLM-as-a-Judge. During reinforcement learning, rubric scoring is conducted using Qwen3-235B-A22B with temperature of 0.3, top-p of 0.95, and maximum context length of 131,072 tokens enabled by Yarn RoPE scaling (Peng et al., 2024). Unless otherwise specified, the weighting coefficients λpref and λllm in Equation (1) are both set to 1. All policy models, including rubric generators and DeepResearch agents, are trained with context length of 64k tokens, temperature of 1.0, and top-p of 1.0. For DeepResearch agents, the maximum number of interaction turns is set to 10, with up to 5 tool invocations allowed per turn. Table 3. Hyperparameters for Training Rubric Generators based on Qwen3-30B-A3B with Hybrid Reward Hyperparameter Value Hyperparameter Value Optimization Config Optimizer Learning Rate Weight Decay Global Batch Size LR Schedule System & Parallelism Tensor Parallel (TP) Expert Parallel (EP) Context Parallel (CP) Max Tokens/GPU Adamw 1 106 0.1 256 Constant 4 8 1 30,000 GRPO Strategy Algorithm Group Size (G) KL Coefficient Clip Ratio (ϵ) Advantage GRPO 8 0.0 0.2 Group Relative Generation & Data Max Response Len Temperature Rollout Batch Size 8,192 1.0 32 Table 4. Hyperparameters for Training DeepResearch Agents Hyperparameter Value Hyperparameter Value Optimization Config Optimizer Learning Rate Weight Decay Global Batch Size LR Schedule System & Parallelism Tensor Parallel (TP) Expert Parallel (EP) Context Parallel (CP) Max Tokens/GPU Adam 1 106 0.1 64 Constant 4 8 2 6,000 GRPO Strategy Algorithm Group Size (G) KL Coefficient Clip Ratio (ϵ) Advantage Generation & Data Max Response Len Temperature Rollout Batch Size Observation Window GRPO 8 0.0 0.2 Group Relative 16,384 1.0 8 24,000 E. Metrics for Human Preference We evaluate the performance of preference modeling using two complementary metrics. Given preference dataset {(qi, r(i) i=1 with scalar scores S(), we first report preference accuracy, defined as acc, r(i) rej )}N Pref.Acc. ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:16) (cid:104) r(i) acc (cid:17) > (cid:17)(cid:105) (cid:16) r(i) rej , (6) which is equivalent to the area under the ROC curve (AUC) for pairwise preference judgments. To further quantify the magnitude and stability of preference separation, we report the paired Cohens (Diener, 2010). Let = S(r(i) acc) S(r(i) rej ) 2https://github.com/THUDM/slime 3https://github.com/THUDM/slime/issues/958 21 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation denote the score difference for query qi; the paired effect size is defined as Cohens = E[] (cid:112)Var() . (7) While preference accuracy (AUC) reflects ranking correctness, paired Cohens captures the standardized strength of score separation at the query level, providing complementary view of preference quality. F. Rollout Speed-up for MaMs workflow Figure 4. Speed-up achieved by overlapping multiple micro-batches using the asynchronous event loop. The concurrency-limited scheduling allows high-latency API calls to run in parallel, maximizing resource utilization and reducing the effective runtime of the stage from linear in the dataset size to approximately D/C, where is the concurrency limit. To address the latency bottlenecks inherent in sequential processing, specifically within I/O-bound Large Language Model (LLM) interactions, we introduce parallel execution mechanism in the MaMs workflow. The baseline implementation, referred to as the Naive Linear Pipeline, processes the entire dataset sequentially through chain of agents. In this mode, the total execution time Tnaive is the summation of the processing time for all samples, where network latency accumulates linearly. To optimize efficiency, we developed the Linear Concurrent Pipeline, which implements data parallelism via asynchronous micro-batching. The pipeline divides the agent execution flow into three stages: pre-processing, concurrent execution, and post-processing. The acceleration focuses on the concurrent stage, where the input dataset is partitioned into sequence of micro-batches = {b1, b2, . . . , bm}, each with configurable size Smicro. We leverage an event loop to manage pool of asynchronous tasks subject to concurrency limit C. The scheduling algorithm operates as follows: 1. sliding window maintains set of active tasks , ensuring C. 2. As long as the active task slots are available (T < C), new micro-batches are dequeued, and corresponding agent tasks are spawned immediately using asyncio. 3. Upon task completion, results are collected via callbacks into synchronized queue, and the window slides forward to admit pending micro-batches. As shown in Figure 4, by overlapping the high-latency API calls across multiple micro-batches, the framework significantly maximizes resource utilization. This approach effectively safeguards against blocking operations, reducing the theoretical time complexity for the concurrent stage from O(D) to approximately O(D/C), bounded primarily by external API rate limits rather than local execution speed. 22 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation G. Preference Performance of Rubric Generator trained with GSPO In this section, we will show the preference accuracy (AUC) and paired Cohens of the rubric generator trained by GSPO in Table 5. Table 5. Preference performance of rubric generators trained by GSPO."
        },
        {
            "title": "Model",
            "content": "Qwen3-30B-A3B"
        },
        {
            "title": "Method\nGRPO with Hybrid Reward\nGSPO with Hybrid Reward",
            "content": "Pref. Acc./AUC (%) 65.68 62.02 Paired Cohens 0.376 0.337 As discussed in Section 4.4, the rubric generator trained with GSPO exhibits substantially higher rollout entropy than its GRPO-trained counterpart, which is undesirable for our setting. Consequently, we adopt GRPO for training the rubric generator. H. Analysis on Tool Calling As detailed in Appendix D, during the training of DeepResearch agents under both the ReAct and MaMs frameworks, the maximum number of interaction turns is set to 10, with up to five tool invocations permitted per turn. Statistics of interaction turns and tool invocations per sample for our trained DeepResearch systems on DeepResearch Bench are reported in Table 6. Table 6. Interaction turns and tool calls per sample of our DeepResearch systems on DeepResearch Bench. In the proposed MaMs workflow, an interaction turn is defined as one complete sequential conversation conducted by the three agents."
        },
        {
            "title": "Workflow Model\nReAct\nReAct\nMaMs\nMaMs",
            "content": "Qwen3-30B-A3B Tongyi-DeepResearch Qwen3-30B-A3B Tongyi-DeepResearch Tool Calling per Sample 6.05 8.10 19.70 39.23 Interaction Turns per Sample 2.21 3.02 7.74 9.40 As expected, the Tongyi-DeepResearch model, trained on agentic data (Tongyi et al., 2025), exhibits substantially stronger tool-use and interaction capabilities than the vanilla Qwen3-30B-A3B (Instruct) model. Moreover, DeepResearch systems trained under the MaMs workflow demonstrate superior interaction performance compared to those following the ReAct (search-then-generate) paradigm. I. Case Study of Rubric List We show case about the rubric list of question in the following: { \"question\": \"Please generate an analysis report on common network failures.\", \"rubrics\": [ { \"title\": \"Coverage of Common Failures\", \"description\": \"Key criterion: The report must identify and describe multiple common types of network failures, such as DNS issues, IP address conflicts, or physical connection interruptions.\", \"weight\": 5 }, { \"title\": \"Inclusion of Core Analysis\", \"description\": \"Key criterion: The report must analyze each mentioned network failure rather than merely listing their names.\", \"weight\": 5 }, { \"title\": \"Clear Structure\", \"description\": \"Key criterion: The report should have clear organizational structure, such as an introduction, categorized analysis of different failures, 23 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation and conclusion.\", \"weight\": 5 }, { \"title\": \"Analysis of Causes and Symptoms\", \"description\": \"Important criterion: The report should explain the typical symptoms and possible causes of each network failure, establishing clear causal relationship.\", \"weight\": 4 }, { \"title\": \"Provision of Troubleshooting Methods\", \"description\": \"Important criterion: The report should provide concrete and actionable troubleshooting steps or solution suggestions for each type of failure .\", \"weight\": 4 }, { \"title\": \"Clear and Understandable Explanation\", \"description\": \"Important criterion: When explaining technical concepts (such as DNS or IP addresses), the report should strive to be clear and accurate so that nonexpert readers can understand it.\", \"weight\": 3 }, { \"title\": \"Professional and Objective Tone\", \"description\": \"Important criterion: The report should maintain professional and objective tone, avoiding overly colloquial or subjective expressions.\", \"weight\": }, { \"title\": \"Systematic Classification of Failures\", \"description\": \"Optional criterion: The report may systematically categorize network failures based on their nature (e.g., hardware, software, configuration issues) to enhance clarity.\", \"weight\": 2 }, { \"title\": \"Inclusion of Preventive Measures\", \"description\": \"Optional criterion: The report may further propose preventive measures and best practices to avoid common network failures.\", \"weight\": 2 }, { \"title\": \"Use of Concrete Examples\", \"description\": \"Optional criterion: The report may use concrete scenarios or cases to illustrate failure phenomena and solutions, improving readability.\", \"weight\": 1 }, { \"title\": \"Technical Errors\", \"description\": \"Error criterion: The report provides incorrect technical explanations, causes, or solutions that may mislead readers.\", \"weight\": -2 }, { \"title\": \"Listing Without Analysis\", \"description\": \"Error criterion: The report merely lists failure names without providing any analysis of causes, symptoms, or solutions.\", \"weight\": -2 }, { \"title\": \"Inclusion of Irrelevant Information\", \"description\": \"Error criterion: The report includes content unrelated to common network failures, such as in-depth discussion of unrelated software programming 24 Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation errors.\", \"weight\": -1 } ], \"topic\": \"Science & Technology\", \"rubric_count\": 13 } J. Limitations and Future work J.1. Limitations While the proposed query-specific rubric generator demonstrates strong alignment with human preferences in the DeepResearch setting, several limitations remain. First, the construction of the preference dataset relies on pairwise comparisons between two candidate reports, which may not fully capture more fine-grained or multi-way preference structures that arise in real-world evaluation scenarios. Second, although the hybrid reward incorporates both human supervision and LLM-based rubric assessment, it may still struggle to fully evaluate aspects such as novelty, creativity, or reasoning depth, as these qualities are inherently subjective and largely dependent on the calibration of the underlying human-LLM feedback. Finally, our experiments primarily focus on our created fixed set of DeepResearch-style tasks and document collections, and the generalization of the learned rubric generation policy to substantially different report formats or domains has not been extensively explored. J.2. Future Work Several directions offer promising opportunities for future research. First, the preference formulation could be extended beyond pairwise comparisons to leverage richer preference signals, such as rankings or graded scores, enabling more fine-grained learning of human preferences. Second, future work could focus on improving the assessment of novelty, creativity, and reasoning depth, for example, by combining more sophisticated LLM evaluations with targeted human feedback to reduce subjectivity and increase reliability. Finally, developing more principled approaches to reduce dependence on LLM-based evaluation, potentially through self-consistency checks or hybrid human-LLM validation, could enhance the stability and interpretability of the training process."
        }
    ],
    "affiliations": [
        "College of Computer Science and Artificial Intelligence, Fudan University",
        "Pattern Recognition Center, WeChat AI, Tencent Inc."
    ]
}