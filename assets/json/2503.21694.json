{
    "paper_title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data",
    "authors": [
        "Zhiyuan Ma",
        "Xinyue Liang",
        "Rongyuan Wu",
        "Xiangyu Zhu",
        "Zhen Lei",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$ trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 9 6 1 2 . 3 0 5 2 : r Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data Zhiyuan Ma1,2 Xinyue Liang1,2 Rongyuan Wu1 Xiangyu Zhu3,4 Zhen Lei1,2,3,4* Lei Zhang1* 1The Hong Kong Polytechnic University 2Center for Artificial Intelligence and Robotics, HKISI CAS 3State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA 4School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS Figure 1. Our method adapts Stable Diffusion [89] to generate high-fidelity textured meshes in 1.2 seconds."
        },
        {
            "title": "Abstract",
            "content": "It is highly desirable to obtain model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained textto-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD *Corresponding authors. to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just few steps. With PRD, we train Triplane generator, namely TriplaneTurbo, which adds only 2.5% trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at github.com/theEricMa/TriplaneTurbo. 1. Introduction Text-to-3D aims to create 3D content faithful to the given textual descriptions. The optimization-based text-to-3D methods can achieve high generation fidelity by using image diffusion priors [53, 63, 68, 70, 73, 86, 96, 113] and score distillation techniques [43, 57, 75, 84, 114, 119, 148, 158] to optimize 3D representations [10, 30, 60, 61, 108]. However, these approaches encounter bottlenecks in computational efficiency since they take hours to generate 3D textured meshes. Recent researches have shifted towards learning-based methods [7, 25, 38, 50, 82], which generate 3D content through feedforward networks, reducing the latency to few seconds per output. Unfortunately, existing 3D datasets [1618, 83, 127] are much smaller compared to those used in training text-to-image generation models, while the 3D data therein suffer from texture quality and inconsistent object poses [62]. Consequently, these approaches struggle to produce high-quality 3D outputs. As promising alternative to the above mentioned methods, one can adapt pretrained text-to-image models, such as Stable Diffusion (SD) [89], into generators of 3D representations [55, 67, 78]. Recent studies have enabled the use of SD to generate 2D planes for 3D representations such as Triplane [8]. PI3D [67] adapts SD to generate six 2D planes pre-optimized for 3D object reconstruction. HexaGen [78] first trains VAE to compress 3D objects into latent space, then adapts SD to generate the desired latents from text prompts. These methods leverage SDs prior knowledge for diverse text prompts while reducing training costs. However, their reliance on data-driven training limits the generalization performance. In this paper, we propose to address the data shortage problem by using high-quality multi-view diffusion models [68, 86, 96] as teachers and performing multi-view distillation to adapt SD into an instant text-to-mesh generator without using 3D ground-truth data. Data-free distillation techniques [71, 159] have been applied in previous methods [52, 72, 75, 134] to train native 3D generators from scratch, whereas they struggle to produce high-quality 3D outputs. We argue that if we can adapt SD for native 3D generation, instead of training from scratch, not only the training efficiency can be improved but also the generation quality can be enhanced. To our best knowledge, however, no such attempt has been made before. The primary obstacle lies in the training scheme of SD, which requires 3D ground-truth data to supervise the denoising process. This requirement, unfortunately, conflicts with our goal to eliminate the dependency on 3D training data. To address the above challenges, we propose Progressive Rendering Distillation (PRD), which enables 3Ddata-free distillation. PRD achieves this goal by denoising the latent from random noise rather than 3D groundtruths. In each training iteration, PRD uses few steps to progressively reverse noise to latent space using the adapted U-Net of SD. Then the denoised latent of each step is decoded to 3D content via the adapted VAE decoder. Multiview teacher models are used to distill high-quality renderings through efficient score distillation techniques such as ASD [75]. An additional benefit of PRD is that by adapting the SD to generate 3D content in just four steps, the overall generation process can be much accelerated. PRD is flexible in the choices of multi-view teacher models and the types of 3D representations. Unlike existing approaches using SD alone as the teacher [60, 107, 119, 144, 158], which risk the multi-view inconsistency and the Janus problem [4], we employ multiple teachers in training. Specifically, we employ MVDream [96] and SD to suppress the Janus problem and ensure text consistency in 3D content supervision. Additionally, we use RichDreamer [86] for geometry supervision through normal and depth. With PRD, we adapt SD into native 3D generator to produce geometry and texture Triplanes in 4 steps, which is named as TriplaneTurbo in the following context. To enhance TriplaneTurbos textured mesh quality, we employ SDF-based volumetric rendering [142] and mesh rasterization [122] for multi-view teacher supervision. In addition, to address the high GPU memory usage caused by multiple teachers and multi-view renderings in the training process, we introduce the Parameter-Efficient Triplane Adapter (PETA), which adds only 2.5% trainable parameters to the frozen SD and effectively adapts it for 3D generation. Some results are shown in Fig. 1. To our knowledge, this is the first parameter-efficient training method for direct 3D content generation from 2D diffusion models, departing from full-parameter adaptation approaches. Our key contributions are summarized as follows: We make the first attempt to adapt the pretrained 2D SD model into native 3D generator without 3D data. With the proposed PRD scheme, we use multi-view diffusion models as teachers and distill SD into four-step native 3D generator, namely TriplaneTurbo. TriplaneTurbo adds only 2.5% additional trainable parameters to the frozen SD for Triplane adapation. It marks the first use of parameter-efficient training to adapt 2D diffusion models for native 3D generation. TriplaneTurbo surpasses existing text-to-3D models not only in quality but also in speed, reducing text-to-mesh generation time to just 1.2 seconds. In addition, by scaling up the text training data, the model can generalize much better to complex text input. 2. Related Work Data-Driven Models for 3D Generation. Employing 3D data to train generators has shown its effectiveness for single-category 3D generation such as human faces [2, 8, 32, 74, 101, 129, 132], body shapes [35, 58, 91, 138], everyday objects [3, 40, 66, 80] , and structured room layouts [5, 21, 77, 88, 110, 130]. This is mainly because for these specific categories, the training data are relatively 2 easy to collect. However, text-to-3D generation requires open-category generation, bringing rather different challenge. The difficulties in obtaining sufficient 3D training data largely limit the model generalizability across diverse text prompts [75], regardless of the type of used generation models (e.g., GAN [24, 81, 93] or diffusion models [29, 41, 99]) or 3D representations (e.g., NeRF [6, 79] or Gaussian Splatting [45, 149]). The publicly available 3D datasets [1618, 83, 127] mostly contain only hundreds or thousands of samples, which are very hard, if not impossible, to train generalized text-to-3D generation model. 2D Diffusion Models for 3D Generation. The difficulty of text-to-3D generation can be alleviated by leveraging 2D diffusion models as priors, thanks to their training on vast text-image pairs. Early approaches like SDS [107] and VSD [13] pioneered zero-shot text-to-3D generation by optimizing 3D representations (NeRF [60, 63, 84, 114], mesh [11, 60, 119, 140], 3DGS [57, 108, 144, 145]) through score distillation [1, 37, 42, 56, 57, 75, 84, 106, 117, 119, 121, 131, 147, 155, 157, 160], which serves as bridge to transfer the generative capability of 2D diffusion models to 3D representations through rendered views. However, pre-trained text-to-image models like SD lack multiview consistency, leading to the Janus problem [4]. MVDream [96] and subsequent works [36, 69] address this issue by camera-aware adaptations and synchronized multiview generation, and they incorporate additional modalities such as normal [86], depth [22, 44], and CCM [65, 135] to enhance geometric quality. While achieving improved 3D generation results, these methods require computationally intensive score distillation [12, 86, 92, 96] or 3D reconstruction [33, 54, 64, 70, 95, 109, 120, 139, 151]. Native 3D generators [17, 26, 35, 50, 51, 59, 76, 111, 126, 133, 150, 153, 153, 156, 156] can reduce the generation time to seconds by directly producing 3D content without view rendering as proxies. For example, LN3Diff [50] and GVGEN [26] compress NeRF [79] and 3DGS [45] into latent spaces using VAEs [48], then train text-conditioned latent diffusion models [29]. However, these methods show limited generalizability across text prompts, as their performance is constrained by the insufficient text-3D training pairs. Some methods [67, 78, 86, 96] leverage SD as their backbone to transfer text-to-image knowledge into textto-3D generation. SD provides strong generative prior and improves the model generalizability [9, 19, 59, 102 105, 124, 125, 141, 152]. Approaches like PI3D [67] adapt SD to generate multiple planes for constructing 3D space, yet their performance remains limited due to the insufficient 3D training data. Instead of adapting SD for multiview generation, native 3D generation requires substantially more 3D data for effective adaptation. We propose to address the data insufficiency challenge by distilling knowledge from multi-view diffusion models into an SD-adapted native 3D generator, eliminating the need for 3D training data. While previous works like ATT3D [72] and ScaleDreamer [75] have investigated such data-free training strategy, they employ multi-view distillation to train generators from scratch, and show limited performance due to the insufficient training scale. To overcome the huge training cost, we propose cost-effective solution that combines multi-view distillation with SD-based native 3D generation. fundamental challenge to achieve this goal is how to adapt SD for 3D generation without ground-truth data. We address this by proposing novel Progressive Rendering Distillation scheme, which not only eliminates the need of 3D ground-truths but also enables few-step generation. 3. Method 3.1. Preliminary Stable Diffusion (SD) performs diffusion in latent space for efficient text-to-image generation. Its VAE encoder EϕSD compresses an image into latent code z, while its decoder DϕSD reconstructs the image. Given text prompt y, U-net ϵϕSD predicts noise ϵ, which is added to zt = αtz + σtϵ, where timestep 1, . . . , controls noise level via scalars αt and σt. Generation proceeds by iteratively denoising from zT to prompt-aligned z0. At each step, the U-net estimates noise ˆϵ = ϵϕSD(zt; t, y) to compute ˆz0 = ztσtˆϵ , denoted as ˆz0 = zϕSD (zt; t, y). Results can be refined through additional diffusion steps < t. The final latent is decoded to an image via ˆx = DϕSD( ˆz0). Score Distillation. 2D diffusion models can optimize 3D representations θ through differentiable rendering xπ = g(θ, π) [46, 94, 97], which produces images xπ from camera view π. Here 2D diffusion models serve as metric L(xπ; π, y) that measures the consistency between xπ and the text conditions y. The 3D representation is optimized using gradient θL(xπ; π, y) = xπL(xπ; π, y) xπ θ , which also trains the native 3D generator. The computation of xπL depends on the chosen score distillation method. αt 3.2. Progressive Rendering Distillation We now detail our proposed training scheme for adapting SD as native 3D generator. Traditional adaptation approaches require preparing ground-truth 3D representations θ and their corresponding latents for each 3D sample in the dataset. The U-net is adapted to denoise diffused latents zt by minimizing the noise prediction mean squared error (MSE). Fig. 2(a) illustrates this paradigm, which has been used by several native 3D generators [67, 118, 126, 153]. However, this paradigm faces limitations in both the quantity and quality of available 3D representations θ, as existing 3D datasets lack sufficient high-quality data for training text-to-3D generators. Actually, the pretrained SD models already possess denoising capabilities for image generation. 3 Figure 2. Comparison between (a) traditional SD adaptation and (b) our proposed progressive rendering distillation (PRD) for native 3D generation. Traditional approach requires ground-truth 3D representations θ and their latents z0 for each 3D sample to generate z0. Our proposed PRD scheme progressively denoises latents zt initialized from random noise into z0, which are decoded to θ, using multi-view diffusion models as teachers for distillation, eliminating the need for 3D data during adaptation and overcoming data scarcity. In other words, pretrained SD is well-trained for Markov Chain to reverse zT = ϵ (0, I) to z0 with its Unet zϕSD, and decode z0 to image with its decoder DϕSD. Our goal is to modify the Markov chain by transforming zϕSD and DϕSD into 3D generators zϕ3D and Dϕ3D , from which the 3D representations θ can be decoded. Note that our modification of the Markov chain differs from the traditional diffusion model adaptation objectives, as it requires neither ground-truth latents z0 nor their noise-diffused variants zt in the training process. Specifically, at the beginning of the Markov chain, the network takes random noise ϵ as input to represent zT . At each step, the current state zt is used to estimate ˆz0 through zϕSD , which is then decoded to 3D output ˆθ with DϕSD. The 3D output ˆθ is used to render images xπ1, xπ2, . . . at camera views π1, π2, . . . and receive supervision from multiview teachers via score distillation, while the estimated ˆz0 is diffused to the next timestep as zt = αt ˆz0 + σtϵ for subsequent operations. We name this training scheme Progressive Rendering Distillation (PRD), as illustrated in Fig. 2(b). From the total timesteps, we select decreasing sequence of timesteps = t1 > t2 > > tK = /K with uniform spacing to perform score distillation from multi-view teachers. The gradient at each step is not backpropagated to previous steps; therefore, we can largely reduce the GPU memory usage and prevent gradient explosion [15, 85, 128, 137]. Since ϕ3D is initialized from ϕSD, this specialized gradient detachment strategy still maintains good convergence. The pseudo code of our algorithm is provided in Algorithm 1. While the time cost of our training strategy increases with the increase of K, we can ensure the model generates high-quality results in just few steps, thereby accelerating inference. We set = 4 to balance quality and speed. 3.3. Parameter-Efficient Triplane Adaptation While various 3D representations could be employed by our PRD scheme, in this paper we demonstrate an exemplar solution using Triplanes as the representation. We denote our adapted model as TriplaneTurbo with parameters ϕ3D. Specifically, TriplaneTurbo adapts SD to generate 3D representation consisting of two Triplanes [8] θ = (θgeo, θtex): geometry Triplane θgeo storing Signed Distance Function (SDF) and deformation values for mesh extraction, and texture Triplane θtex containing RGB attributes for painting texture on the mesh. For each point in 3D space, we use two-layer MLP to decode its SDF value. The same process applies to texture and mesh deformation [94, 122]. This separation of geometry and texture planes follows the work in [23, 100, 123]. We set the Triplane resolution as 256 256, and replace the last convolution in SDs decoder to output 32 channels. The Triplanes output has dimension of 6 256 256 32 in feature space. Due to the 8 compression of the VAE, this corresponds to 6 32 32 4 in latent space, distinct from the 1 64 64 4 latent generated by pretrained SD. To enable interaction between the six feature planes, we follow existing approaches [70, 95, 96, 115] to adapt the U-nets self-attention [89, 112] to allow cross-plane attention. Unlike existing works that fully retrain SD [12, 55, 67, 78, 96], which can lead to catastrophic forgetting [67, 96], we propose parameter-efficient adaptation approach. The core of our design lies in the fact that each of the six feature planes maintains its own unique feature distribution. Therefore, plane-specific characteristics must be incorporated into the adaptation process. As illustrated in Fig. 3, our adaptation modifies the convolution, self-attention, and cross-attention layers. We name our approach Parameter-Efficient Triplane Adaptation (PETA). 4 Figure 3. Illustration of TriplaneTurbo: an SD-adapted native 3D generator using our PRD scheme. Our model generates six feature planes comprising geometry Triplane θgeo and texture Triplane θtex in 4 steps. We introduce Parameter Efficient Triplane Adaptation (PETA), which requires only 2.5% additional parameters for adaptation. The parameter arrangement is illustrated in the figure. for As shown in Fig. 3, the convolution blocks (Res-Blocks) and cross-attention layers, we implement LoRA [34, 143] for parameter-efficient adaptation, and process the six planes uniformly. The plane-specific adaptations are then applied to the self-attention. For the selfattention blocks, we apply distinct LoRA layers [34] to the to Q, to K, to V, and to Out linear layers when processing each of the six feature planes. In each linear transformation within self-attention block, the linear projection with multiple LoRAs is implemented in two steps. First, the frozen linear layer batch processes the features extracted from all the six planes together. Then, separate LoRA transformations are applied independently to the features of each plane. This adaptation maintains low computational overhead while effectively introducing plane-specific processing during attention calculations across the six feature planes. It can be applied with other techniques like AdaLoRa [154] and Vera [49]. We leave this for further exploration. We set the LoRA rank to 16 by default. While this adaptation adds only 2.5% of the parameters to the SD model, it effectively enables native 3D generation. 3.4. Distillation Details Since PRD eliminates the need for 3D data by referring to multi-view teachers for distillation, using multiple teachers allows us to combine their strengths while mitigating individual biases. Most previous works [14, 60, 72, 84, 108] use SD model (parametrized by ϕSD) as the teacher for its ability to generate high-fidelity, text-consistent images. However, SD lacks camera-awareness, which can lead to the Janus problem [4]. MVDream [96] (MV, parametrized by ϕMV) addresses this by generating four camera-conditioned views simultaneously, but at the cost of reduced prompt consistency [115]. While SD and MV complement each other, both of them focus on RGB rendering and provide no direct supervision on geometry. We further incorporate RichDreamer [86] (RD, parameterized by ϕRD), model that generates four-view normal and depth maps based on text prompts. The score distillation guidance (xπ; π, y) in our implementation thus integrates SD, MV, and RD. At each PRD step (see Sec. 3.2 and Fig. 2), given text prompt and generated 3D representation ˆθ from zϕ3D and Dϕ3D , we sample four views π1, . . . , π4 at uniform azimuth intervals. We sample one view π from π1, . . . , π4 to compute LSD = LϕSD (xπ; π, y). For MV, all the four views are used to compute LMV = LϕMV (xπ1 , . . . , xπ4 ; π1, . . . , π4, y). RD operates on con- , . . . , catenated normal and depth renderings π4, and π1 ; π1, . . . , π4, y). , . . . , its objective is LRD = LϕRD (x π4 Among existing score distillation methods [37, 42, 56, 57, 75, 84, 119, 121, 131, 147], we adopt Asynchronous Score Distillation (ASD) [75] for its pioneered efficiency in training deep text-to-3D generators. For 3D rendering, as illustrated in Fig. 3, we combine volumetric rendering [142] with mesh rasterization [122] to overcome the instability of pure mesh supervision [136]. See Sec. for more details. π 4. Experiments 4.1. Experimental Settings Implementation Details. For fair comparison with existing methods, we use captions of the 3D objects [17] provided by [31] to train our model, which comprises total of 360K text prompts. We employ SD v2.1-base [90] as the 5 Figure 4. Qualitative comparison of text-to-mesh generation results by competing methods. Please refer to Sec. 4.2 for details. base model. Our model is trained for 15K iterations with learning rate of 2e-4, costing 40 hours on 8 NVIDIA H20 GPUs. We further collect 1.6 million text prompts to evaluate the benefit of data scaling supported by our method; the detail is provided in Sec. D. Additional training details and loss weights are provided in Sec. B. Compared Methods. Our proposed PRD aims for fast text-to-mesh generation. Therefore, we compare it against state-of-the-art approaches that can generate textured meshes within one minute, including Shape-E [39], Direct3D [62], 3DTopia [31], GVGEN [26], LN3Diff [50], LGM [109] and PI3D [67]. Note that we do not compare with some relevant works [78, 134] since their codes/models are not publicly available and they employ different evaluation protocols. In our experiments, the results of Shape-E, Direct3D, 3DTopia, GVGEN, LN3Diff and LGM are obtained by running their publicly available models. For PI3D, we can only perform quantitative comparisons with it by copying its results from the original paper, but we cannot perform visual comparisons with it since its code/model is not yet publicly available. Evaluation Protocol. We employ the protocol used in our competing methods [26, 31, 39, 50, 67, 109] to evaluate our PRD model. Specifically, we use the ViT-B/34 CLIP model [87] to evaluate the test prompts from the DreamFusion gallery [84]. As in [67], we render the generated 3D results at 512 resolution from four viewpoints at 15 elevation across four azimuth angles: 0, 90, 180 and 270. Under these views, we evaluate the performance of competing methods using CLIP Score [27] (C.S.) and CLIP Rprecision (R@1). To ensure fair comparison for instant text-to-mesh generation, we exclude the post-processing steps such as SDS refinement [84], as they will prolong the generation time by several minutes. For models [26, 109] using Gaussian Splatting representations, we use the conversion script from [109] to generate textured meshes. 6 4.2. Experimental Results We showcase qualitative comparisons of the competing methods in Fig. 4 using challenging test prompts. One can see that most of the existing methods struggle to generate satisfactory outputs. Our method, in comparison, produces better quality results with complete and vivid meshes. The quantitative results are reported in Tab. 1. Again, our method demonstrates superior performance to its competitors in all the metrics. Existing methods fall short in the consistency of object poses and in the capability of handling complex prompts. Our PRD method addresses these challenges, enabling faster inference speed and improving the model performance by data scaling. Handling Inconsistent Generation Poses. As shown in Fig. 4, existing models often generate objects with incorrect poses. For prompts like teal moped, the competing methods like Direct3D [126], Shape-E [39] and GVGEN [26] generate objects with misaligned orientations, such as facing sideways or backwards. This issue stems from the inconsistent object orientations in 3D training datasets, which are difficult to detect and correct automatically. Current methods struggle with noisy training data, leading to pose ambiguity and geometric defects. Our approach addresses this challenge by learning from multiview teachers. Though our teachers are trained on noisy 3D datasets and may occasionally provide incorrect directional guidance, our PRD scheme exposes the 3D outputs to multi-view teachers times per iteration (see Sec. 3.2). Through the iterative distillation process, we substantially decrease the impact of incorrect directional guidance, ensuring consistent pose alignment in the 3D outputs. Handling Complex Text Prompts. The failure of our competing methods mainly stems from their reliance on the existing text-3D paired datasets, which are not comprehensive enough compared to the diverse user inputs. Therefore, trained on these data, existing methods fail to produce good results when the input is complex. For example, existing methods might perform well when the input prompt is robot but fail when the prompt becomes robot tiger. The quality of existing 3D datasets is also compromised by the absence of creative concepts. Creating 3D models for imaginative concepts like robot tiger demands significant time and expertise from 3D artists. As result, existing 3D datasets are largely limited to common everyday objects. Models trained on these limited datasets fail to generalize effectively to imaginative or complex prompts. We solve this problem by adapting SD as 3D generator and inheriting its generative power, and more crucially, by introducing training scheme that completely eliminates the need of 3D data. Our training scheme not only improves the quality of results on the existing training corpus, as shown by the qualitative and quantitative results in Fig. 4 and Tab. 1, but also enhances the capability to handle complex prompts by C.S. R@1 Latency (/sec) Shape-E [39] Direct3D 3DTopia [31] PI3D [67] GVGEN [26] LN3Diff [50] LGM [109] Ours +More Text Data 55.1 60.8 59.7 65.9 51.1 55.9 67.4 68.2 75.1 27.1 4.33 11.2 25.2 2.44 5.09 28.3 32.3 46.0 13.0 16.0 23.7 3.00 49.2 8.16 56.1 1.23 1. Table 1. Quality and speed comparison for text-to-mesh generation. indicates that the values are quoted from the original papers. expanding the available training data, as detailed below. Fast Inference Speed. Our PRD method also demonstrates superior computational efficiency. As shown in Tab. 1, we evaluate the average inference latency from text input to textured mesh output across all test prompts on the H20 GPU device. While some methods [26, 109] require dozens of steps, our PRD approach enables the native 3D generator to produce quality results in just steps. With the suggested setting of = 4, as shown in Tab. 1, our model achieves sub-second latency for text-to-mesh generation, significantly outperforming previous methods. Scaling Up Training Corpus. Since our method is free from the constraints of 3D training data, it can be easily scaled to accommodate more complex and creative text prompts during training. It preserves the SD models ability to handle creative concepts throughout the 3D adaptation process, generating 3D outputs that faithfully represent the input prompts. As can be seen in the +More Text Data column, when we scale up the training text data from 360K to 1.6M, the CLIP similarity improves by as much as 7%, leading to more consistent generation for challenging text prompts such as lion reading the newspaper and tray of Sushi containing pugs. This is because our collected text data covers wider range of creative concepts than the existing 3D datasets [17, 127], thus providing more sufficient training and improving the generalizability of the model. More visual examples are partially presented in Fig. 1 and detailed in Fig. 5 and Fig. 8. 4.3. Ablation Study The Effectiveness of PRD. We validate our PRD algorithm by testing simplified configuration with K=1, which reduces our method to single-step generation process that is equivalent to vanilla native 3D generator trained by score distillation [75]. As shown in Fig. 6 and quantified in Tab. 2, this configuration fails to generate proper 3D structures because it needs to simultaneously handle two complex challenges: adapting SD for 3D generation and performing single-step generation. In contrast, our PRD 7 Figure 5. More results of our model trained with expanded corpus. 8 Figure 6. Visualizations on the ablation studies of PRD algorithm. Figure 7. Visualizations on the ablation study of PETA. C.S. R@1 K=1 K=2 K=4 (Proposed) 50.9 62.6 68.2 14.4 22.4 32.3 C.S. R@1 Full Param Tuning w/ Basic LoRA w/ PETA (Proposed) 35.8 54.2 68. 0.35 11.1 32.3 Table 2. Ablation study on the hyper-parameters of PRD. Table 3. Ablation study on the effectiveness of PETA. scheme can use alternative step configurations such as K=2, which produces suboptimal yet acceptable results. the configuration of K=4 provides the We found that best trade-off between output quality and computational efficiency, which is used as the default setting of PRD. The Effectiveness of PETA. We first compare our proposed PETA method with conventional full parameter finetuning (shown as Full Param Tuning in Fig. 7 and Tab. 3). We see that full parameter fine-tuning exhibits training instability and catastrophic forgetting, resulting in textinconsistent outputs. We then conduct additional ablation studies using vanilla LoRA tuning, maintaining an equivalent parameter size (22.6M), shown as the configuration of w/ Basic LoRA. We see that this vanilla adaptation produces degraded geometric structures and textures, and lacks the capability to handle the unique characteristics of each plane. Since different geometry and texture planes (see Fig. 3) exhibit distinct feature distributions, they require specialized consideration. Our solution (denoted as w/ PETA) considers plane dependency using multiple LoRAs in self-attention blocks, enabling each plane to maintain its unique representation while allowing cross-plane interactions during self-attention computation. As shown in Fig. 6 and Tab. 2, PETA achieves enhanced 3D generation quality with good text consistency. We also perform ablation studies to investigate the impact of multiple multi-view teachers in training, the choice of LoRA rank, our hybrid rendering approach that combines volumetric rendering [116] and mesh rasterization [122] for multi-view distillation in PRD training scheme. The details can be found in Sec. E. 5. Conclusion and Limitation In this paper, we presented Progressive Rendering Distillation (PRD), novel training scheme that adapts Stable Diffusion (SD) for instant text-to-mesh generation without relying on 3D data. We also introduced PETA (Parameter-Efficient Triplane Adaptation), parameterefficient method that introduces only 2.5% additional parameters to effectively enable SD for instant text-to-mesh generation. Our model, namely TriplaneTurbo, can produce text-consistent textured meshes in only 1.2 second. Through comprehensive experiments, we validated the effectiveness of our approach. Our methodology has the potential to be extended to 3D scene generation and image-to3D tasks. While currently implemented with SD, the PRD approach can also be applied to other pre-trained models like DiT [20]. We hope our work can inspire new directions in 3D generation to overcome the dependency on 3D data. Limitations. One limitation of our method lies in the generation of precise numbers of multiple 3D objects, which may require more sophisticated multi-view teachers, potentially enhanced with layout guidance. Besides, our results for full-body humans might exhibit limited facial and hand details, which can be improved by extending SD adaptation to more advanced 3D structures than Triplane."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by the InnoHK program."
        },
        {
            "title": "References",
            "content": "[1] Thiemo Alldieck, Nikos Kolotouros, and Cristian Sminchisescu. Score distillation sampling with learned manifold corrective, 2024. 3 [2] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, and Linjie Luo. Panohead: Geometry-aware 3d fullhead synthesis in 360deg. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2095020959, 2023. 2 [3] Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1260812618, 2023. 2 [4] Mohammadreza Armandpour, Ali Sadeghian, Huangjie Zheng, Amir Sadeghian, and Mingyuan Zhou. Re-imagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus problem and beyond. arXiv preprint arXiv:2304.04968, 2023. 2, 3, 5 [5] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, and Andrea Tagliasacchi. Cc3d: Layout-conditioned generaIn Proceedings of the tion of compositional 3d scenes. IEEE/CVF International Conference on Computer Vision, pages 71717181, 2023. 2 [6] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58555864, 2021. [7] Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, and Ziwei Liu. Large-vocabulary 3d diffusion model with transformer. arXiv preprint arXiv:2309.07920, 2023. 2 [8] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16123 16133, 2022. 2, 4 [9] Bin Chen, Gehui Li, Rongyuan Wu, Xindong Zhang, Jie Chen, Jian Zhang, and Lei Zhang. Adversarial diffusion compression for real-world image super-resolution. arXiv preprint arXiv:2411.13383, 2024. 3 [10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2 [11] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2224622256, 2023. 3 [12] Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views. arXiv preprint arXiv:2312.04424, 2023. 3, [13] Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, and Li Yuan. Progressive3d: Progressively local editing for text-to-3d content creation with complex semantic prompts, 2023. 3 [14] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384, 2023. 5 [15] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. 4 [16] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2112621136, 2022. 2, 3 [17] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: universe of 10m+ 3d objects. In Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 3, 5, 7, 19, 20, 22 [18] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 2, [19] Slava Elizarov, Ciara Rowles, and Simon Donne. Geometry image diffusion: Fast and data-efficient text-to-3d with image-based surface representation. arXiv preprint arXiv:2409.03718, 2024. 3 [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 9 [21] Chengzeng Feng, Jiacheng Wei, Cheng Chen, Yang Li, Pan Ji, Fayao Liu, Hongdong Li, and Guosheng Lin. Prim2room: Layout-controllable room mesh generation from primitives. arXiv preprint arXiv:2409.05380, 2024. 2 [22] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. arXiv preprint arXiv:2403.12013, 2024. 3 10 [23] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. In Advances In Neural Information Processing Systems, 2022. [24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 3 [25] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation, 2023. 2 [26] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. arXiv preprint arXiv:2403.12957, 2024. 3, 6, 7 [27] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Clipscore: reference-free arXiv preprint Bras, and Yejin Choi. evaluation metric for image captioning. arXiv:2104.08718, 2021. 6 [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 17 [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 17, 19 [30] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 79097920, 2023. [31] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, Dahua Lin, et al. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234, 2024. 5, 6, 7 [32] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: real-time nerf-based parametric head In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 20374 20384, 2022. 2 [33] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d, 2023. 3 [34] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 5 [35] Tao Hu, Fangzhou Hong, and Ziwei Liu. Structldm: StrucarXiv tured latent diffusion for 3d human generation. preprint arXiv:2404.01241, 2024. 2, [36] Zhipeng Hu, Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Changjie Fan, Xiaowei Zhou, and Xin Yu. Efficientdreamer: High-fidelity and robust 3d creation via orthogonal-view diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49494958, 2024. 3 [37] Shuo Huang, Shikun Sun, Zixuan Wang, Xiaoyu Qin, Yanmin Xiong, Yuan Zhang, Pengfei Wan, Di Zhang, and Jia Jia. Placiddreamer: Advancing harmony in text-to-3d genIn Proceedings of the 32nd ACM International eration. Conference on Multimedia, pages 68806889, 2024. 3, 5 [38] Tianyu Huang, Yihan Zeng, Bowen Dong, Hang Xu, Songcen Xu, Rynson WH Lau, and Wangmeng Zuo. Textfield3d: Towards enhancing open-vocabulary 3d generation with noisy text fields. arXiv preprint arXiv:2309.17175, 2023. 2 [39] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions, 2023. 6, 7 [40] Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy Mitra. Holodiffusion: Training 3D diffusion model In Proceedings of the IEEE/CVF conusing 2D images. ference on computer vision and pattern recognition, 2023. [41] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 3 [42] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. arXiv preprint arXiv:2310.17590, 2023. 3, 5 [43] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation, 2023. 2 [44] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. 3 [45] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [46] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. 3 [47] Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, and Sanja Fidler. Neuralfield-ldm: Scene genIn Proeration with hierarchical latent diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84968506, 2023. 21 [48] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 3 [49] Dawid Kopiczko, Tijmen Blankevoort, and Yuki Asano. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023. 5 [50] Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation, 2024. 2, 3, 6, 7 [51] Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change 11 Loy. Gaussiananything: Interactive point cloud latent diffusion for 3d generation. arXiv preprint arXiv:2411.08033, 2024. 3 [52] Ming Li, Pan Zhou, Jia-Wei Liu, Jussi Keppo, Min Lin, Shuicheng Yan, and Xiangyu Xu. Instant3d: Instant text-to3d generation. International Journal of Computer Vision, pages 117, 2024. 2 [53] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arxiv:2310.02596, 2023. 2 [54] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2024. 3 [55] Xinyang Li, Zhangyu Lai, Linning Xu, Jianfei Guo, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Dual3d: Efficient and consistent text-to-3d generation with dual-mode multi-view latent diffusion. arXiv preprint arXiv:2405.09874, 2024. 2, [56] Zongrui Li, Minghui Hu, Qian Zheng, and Xudong Jiang. Connecting consistency distillation to score distillation for In European Conference on Comtext-to-3d generation. puter Vision, pages 274291. Springer, 2025. 3, 5 [57] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching, 2023. 2, 3, 5 [58] Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xiangyu Zhu, and Zhen Lei. High-fidelity clothed avatar reIn Proceedings of the construction from single image. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86628672, 2023. 2 [59] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. Diffsplat: Repurposing image diffusion models for scalable gaussian splat generation. arXiv preprint arXiv:2501.16764, 2025. 3 [60] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: HighIn IEEE Conferresolution text-to-3d content creation. ence on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, [61] Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, and Qifeng Chen. Headartist: Textconditioned 3d head generation with self score distillation. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2 [62] Qihao Liu, Yi Zhang, Song Bai, Adam Kortylewski, and Alan Yuille. Direct-3d: Learning direct text-to-3d genIn Proceedings of the eration on massive noisy 3d data. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68816891, 2024. 2, 6 [63] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. 2, 3 multi-view refinement. arXiv preprint arXiv:2411.17772, 2024. 3 [65] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [66] Yibo Liu, Zheyuan Yang, Guile Wu, Yuan Ren, Kejian Lin, Bingbing Liu, Yang Liu, and Jinjun Shan. Vqa-diff: Exploiting vqa and diffusion for zero-shot image-to-3d vehicle asset generation in autonomous driving. arXiv preprint arXiv:2407.06516, 2024. [67] Ying-Tian Liu, Yuan-Chen Guo, Guan Luo, Heyi Sun, Wei Yin, and Song-Hai Zhang. Pi3d: Efficient text-to-3d genIn Proceedings of eration with pseudo-image diffusion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1991519924, 2024. 2, 3, 4, 6, 7, 19, 20 [68] Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion priors for relightable text-to-3d generation, 2023. 2 [69] Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, and Wanli Ouyang. Unidream: Unifying diffusion In European priors for relightable text-to-3d generation. Conference on Computer Vision, pages 7491. Springer, 2025. 3 [70] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion, 2023. 2, 3, 4 [71] Raphael Gontijo Lopes, Stefano Fenu, and Thad Starner. Data-free knowledge distillation for deep neural networks. arXiv preprint arXiv:1710.07535, 2017. 2 [72] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: AmorIn Proceedings of the tized text-to-3d object synthesis. IEEE/CVF International Conference on Computer Vision, pages 1794617956, 2023. 2, 3, 5 [73] Baorui Ma, Haoge Deng, Junsheng Zhou, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Geodream: Disentangling 2d and geometric priors for high-fidelity and consistent 3d generation. arXiv preprint arXiv:2311.17971, 2023. [74] Zhiyuan Ma, Xiangyu Zhu, Guo-Jun Qi, Zhen Lei, and Lei Zhang. Otavatar: One-shot talking face avatar with controllable tri-plane rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116910, 2023. 2 [75] Zhiyuan Ma, Yuxiang Wei, Yabin Zhang, Xiangyu Zhu, Zhen Lei, and Lei Zhang. Scaledreamer: Scalable textto-3d synthesis with asynchronous score distillation. arXiv preprint arXiv:2407.02040, 2024. 2, 3, 5, 7, 17 [64] Xiangyu Liu, Xiaomei Zhang, Zhiyuan Ma, Xiangyu Zhu, and Zhen Lei. Mvboost: Boost 3d reconstruction with [76] Zhiyuan Ma, Xiangyu Zhu, Guojun Qi, Chen Qian, Zhaoxiang Zhang, and Zhen Lei. Diffspeaker: Speech-driven 3d 12 facial animation with diffusion transformer. arXiv preprint arXiv:2402.05712, 2024. [77] Quan Meng, Lei Li, Matthias Nießner, and Angela Dai. Lt3sd: Latent trees for 3d scene diffusion. arXiv preprint arXiv:2409.08215, 2024. 2 [78] Antoine Mercier, Ramin Nakhli, Mahesh Reddy, Rajeev Yasarla, Hong Cai, Fatih Porikli, and Guillaume Berger. Hexagen3d: Stablediffusion is just one step away from fast and diverse text-to-3d generation, 2024. 2, 3, 4, 6, 19, 20 [79] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [80] Norman Muller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Diffrf: Rendering-guided 3d radiance field Nießner. In Proceedings of the IEEE/CVF Conference diffusion. on Computer Vision and Pattern Recognition, pages 43284338, 2023. 2 [81] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature In Proceedings of the IEEE/CVF Conference on fields. Computer Vision and Pattern Recognition, pages 11453 11464, 2021. [82] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and Sergey Tulyakov. Autodecoding latent 3d diffusion models, 2023. 2 [83] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Carl Yuheng Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception, 2023. 2, 3 [84] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2, 3, 5, 6, 17 [85] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-image diffusion arXiv preprint models with reward backpropagation. arXiv:2310.03739, 2023. 4 [86] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. arXiv preprint arXiv:2311.16918, 2023. 2, 3, 5, 17, 20 [87] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. [88] Barbara Roessle, Norman Muller, Lorenzo Porzi, Samuel Rota Bul`o, Peter Kontschieder, Angela Dai, and Matthias Nießner. L3dg: Latent 3d gaussian diffusion. arXiv preprint arXiv:2410.13530, 2024. 2 [89] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 4, 17, 19, 20 [90] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Stable-diffusion-v2.1base. https://huggingface.co/stabilityai/ stable-diffusion-2-1-base, 2022. 5 [91] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for In Proceedings of high-resolution 3d human digitization. the IEEE/CVF conference on computer vision and pattern recognition, pages 8493, 2020. 2 [92] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. Zeronvs: Zero-shot 360-degree view synthesis from single real image, 2023. 3 [93] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in Neural Information Processing Systems, 33:2015420166, 2020. [94] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. 3, 4 [95] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multiview diffusion base model, 2023. 3, 4 [96] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv:2308.16512, 2023. 2, 3, 4, 5, 17, 20 [97] Sanghyun Son, Matheus Gadelha, Yang Zhou, Zexiang Xu, Ming C. Lin, and Yi Zhou. Dmesh: differentiable representation for general meshes, 2024. 3 [98] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 19 and Stefano Ermon. arXiv preprint [99] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [100] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. Ide-3d: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis. ACM Transactions on Graphics (ToG), 41(6):110, 2022. 4 [101] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. Next3d: Generative neural texture rasterization for 3d-aware head avatars. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2099121002, 2023. 2 [102] Lingchen Sun, Rongyuan Wu, Jie Liang, Zhengqiang Zhang, Hongwei Yong, and Lei Zhang. Improving the stability and efficiency of diffusion models for content consistent super-resolution. arXiv preprint arXiv:2401.00877, 2023. 3 13 [103] Lingchen Sun, Rongyuan Wu, Zhiyuan Ma, Shuaizheng Liu, Qiaosi Yi, and Lei Zhang. Pixel-level and semanticlevel adjustable super-resolution: dual-lora approach. arXiv preprint arXiv:2412.03017, 2024. 17 [104] Yichun Tai, Zhenzhen Huang, Tao Peng, and Zhijiang Zhang. Deffiller: Mask-conditioned diffusion for arXiv preprint salient steel surface defect generation. arXiv:2412.15570, 2024. [105] Yichun Tai, Kun Yang, Tao Peng, Zhenzhen Huang, and Zhijiang Zhang. Defect image sample generation with difIEEE fusion prior for steel surface defect recognition. Transactions on Automation Science and Engineering, 22: 82398251, 2025. [106] Boshi Tang, Jianan Wang, Zhiyong Wu, and Lei Zhang. Stable score distillation for high-quality 3d generation, 2023. 3 [107] Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with stable-diffusion, 2022. https://github.com/ashawkey/stabledreamfusion. 2, 3 [108] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatarXiv preprint ting for efficient 3d content creation. arXiv:2309.16653, 2023. 2, 3, 5 [109] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation, 2024. 3, 6, 7 [110] Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias Nießner. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2050720518, 2024. 2 [111] Zhicong Tang, Shuyang Gu, Chunyu Wang, Ting Zhang, Jianmin Bao, Dong Chen, and Baining Guo. Volumediffusion: Flexible text-to-3d generation with efficient volumetric encoder. arXiv preprint arXiv:2312.11459, 2023. 3 [112] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [113] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitrii Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. SV3D: Novel multiview synthesis and 3D generation from single image using latent video diffusion. arXiv, 2024. 2 [114] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. arXiv preprint arXiv:2212.00774, 2022. 2, 3 [115] Peng Wang and Yichun Shi. Imagedream: Image-prompt arXiv preprint multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 4, 5 [116] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 9, 17 [117] Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, et al. Steindreamer: Variance reduction for text-to-3d score distillation via stein identity. arXiv preprint arXiv:2401.00604, 2023. 3 [118] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45634573, 2023. 3 [119] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 5 [120] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model, 2024. 3 [121] Min Wei, Jingkai Zhou, Junyao Sun, and Xuesong Zhang. Adversarial score distillation: When score distillation In Proceedings of the IEEE/CVF Conference meets gan. on Computer Vision and Pattern Recognition, pages 8131 8141, 2024. 3, 5 [122] Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, and Hao Su. Neumanifold: Neural watertight manifold reconstruction with efficient arXiv preprint and high-quality rendering support. arXiv:2305.17134, 2023. 2, 4, 5, 9, [123] Bin-Shih Wu, Hong-En Chen, Sheng-Yu Huang, and YuChiang Frank Wang. Tpa3d: Triplane attention for fast textIn European Conference on Computer to-3d generation. Vision, pages 438455. Springer, 2024. 4 [124] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Advances in Neural Information Processing Systems, 37:9252992553, 2024. 3, 17 [125] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semanticsaware real-world image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2545625467, 2024. 3 [126] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 3, 7 [127] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. 2, 3, 7 [128] Xiaoshi Wu, Yiming Hao, Manyuan Zhang, Keqiang Sun, Zhaoyang Huang, Guanglu Song, Yu Liu, and Hongsheng Li. Deep reward supervisions for tuning text-to-image diffusion models. arXiv preprint arXiv:2405.00760, 2024. 4 [129] Yue Wu, Sicheng Xu, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, and Xin Tong. Aniportraitgan: 14 animatable 3d portrait generation from 2d image collections. In SIGGRAPH Asia 2023 Conference Papers, pages 19, 2023. 2 [130] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation. ACM Transactions on Graphics (TOG), 43(4):117, 2024. 2 [131] Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, and Hanwang Zhang. Consistent3d: Towards consistent highfidelity text-to-3d generation with deterministic sampling prior. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98929902, 2024. 3, 5 [132] Jianfeng Xiang, Jiaolong Yang, Yu Deng, and Xin Tong. Gram-hd: 3d-consistent image generation at high resolution with generative radiance manifolds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21952205, 2023. 2 [133] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [134] Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, and Xiaohui Zeng. Latte3d: Large-scale amortized text-to-enhanced3d synthesis, 2024. 2, 6, 19, 20 [135] Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, and Minghua Liu. Sparp: Fast 3d object reconstruction and pose estimation from sparse views. In European Conference on Computer Vision, pages 143163. Springer, 2025. 3 [136] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 5, 17 [137] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36, 2024. 4 [138] Yinghao Xu, Wang Yifan, Alexander Bergman, Menglei Chai, Bolei Zhou, and Gordon Wetzstein. Efficient 3d articulated human generation with layered surface volumes. arXiv preprint arXiv:2307.05462, 2023. 2 [139] Yinghao Xu, Zifan Shi, Wang Yifan, Sida Peng, Ceyuan Yang, Yujun Shen, and Wetzstein Gordon. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. arxiv: 2403.14621, 2024. [140] Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, and Tao Mei. Jointly manipulating and texturing trianDreammesh: arXiv preprint gle meshes for text-to-3d generation. arXiv:2409.07454, 2024. 3 [141] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong Xie, and Lei Zhang. Pixel-aware stable diffusion for realistic image super-resolution and personalized stylization. In European Conference on Computer Vision, pages 7491. Springer, 2024. 3 [142] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34:48054815, 2021. 2, 5, 17, 21 [143] Shih-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard BW Yang, Giyeong Oh, and Yanmin Gong. Navigating textto-image customization: From lycoris fine-tuning to model In The Twelfth International Conference on evaluation. Learning Representations, 2023. 5 [144] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arxiv:2310.08529, 2023. 2, [145] Taoran Yi, Jiemin Fang, Zanwei Zhou, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Xinggang Wang, and Qi Tian. Gaussiandreamerpro: Text to manipulable 3d gaussians with highly enhanced quality. arXiv:2406.18462, 2024. 3 [146] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 17 [147] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, SongHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023. 3, 5 [148] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, SongHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation, 2023. 2 [149] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19447 19456, 2024. 3 [150] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance reparXiv preprint resentation for 3d generative modeling. arXiv:2403.19655, 2024. [151] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. arXiv, 2024. 3 [152] Lvmin Zhang and Maneesh Agrawala. Transparent image layer diffusion using latent transparency. arXiv preprint arXiv:2402.17113, 2024. 3 [153] Longwen Zhang, Ziyu Wang, Qixuan Zhang Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. arXiv preprint arXiv:2406.13897, 2024. 3 15 [154] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Adalora: Adaptive budget allocaand Tuo Zhao. arXiv preprint tion for parameter-efficient fine-tuning. arXiv:2303.10512, 2023. 5 [155] Chenxi Zheng, Yihong Lin, Bangzhen Liu, Xuemiao Xu, Yongwei Nie, and Shengfeng He. Recdreamer: Consistent text-to-3d generation via uniform score distillation. In The Thirteenth International Conference on Learning Representations, 2025. [156] Junsheng Zhou, Weiqi Zhang, and Yu-Shen Liu. Diffgs: Functional gaussian splatting diffusion. Advances in Neural Information Processing Systems, 37:3753537560, 2024. 3 [157] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 3 [158] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to3d with advanced diffusion guidance, 2023. 2 [159] Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learnIn International conference on machine learning, ing. pages 1287812889. PMLR, 2021. 2 [160] Wenjie Zhuo, Fan Ma, Hehe Fan, and Yi Yang. Vividdreamer: invariant score distillation for hyper-realistic textIn European Conference on Computer to-3d generation. Vision, pages 122139. Springer, 2024. 3 Supplementary Material to Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data The contents of this supplementary file include: Progressive Rendering Distillation pseudo code (referring to Sec. 3.2 in the main paper). More implementation details (referring to Sec. 4.1 in the main paper). Additional qualitative comparisons (referring to Sec. 4.2 in the main paper). Additional results with expanded training corpus (referring to Sec. 4.2 in the main paper). Additional ablation experiments (referring to Sec. 4.3 in the main paper). A. Pesudo Code The pseudo-code of our Progressive Rendering Distillation (PRD) training scheme is appended in Algorithm 1. B. More Implementation Details Dual rendering. We integrate DiffMC [122] for mesh rasterization and NeuS [116] for volume rendering to supervise the generation of 3D outputs. Such dual rendering approach can ensure the training stability: when SDF values are all positive or all negative throughout the 3D space and thus the mesh extraction fails, volume rendering can still guide the training process to optimize the 3D space. Due to memory constraints, volume rendering is limited to low resolution (128 128). We complement this with high-resolution (512 512) mesh rasterization. To handle mesh extraction failures caused by the uniformly distributed SDF signs, we implement the method proposed in [136] to artificially enforce the position of the zero-level set in the 3D space. We manually control gradient magnitudes during backpropagation. The gradient of volume rendered multi-views with respect to the texture decoding MLP starts at 1.0 and linearly decreases to 0.01 at the end of training, preventing blurry textures caused by low-resolution volume rendering supervision. The gradient of mesh rasterized multi-views with respect to both the SDF decoding MLP and deformation decoding MLP is fixed at 0.001 throughout training, which stabilizes training and improves generation performance. Training objective. With the multi-view teacher [86, 89, 96], we decode the multi-views xπ to latent zπ, which are diffused by adding Gaussian noise at timestep [29], denoted by zπ,t. We write the diffusion module of the multi-view teacher as zϕ2D (zπ,t; t, π, y) to represent the process of noise prediction and latent denoising, where is the text prompt. With ASD [75], the derivative of the objective with respect to the 3D generator ϕ3D is: ϕ3DLϕ2D (xπ; π, y) = Et,ϵ,t (cid:20) ω(t) (cid:16) zCls ϕ2D (zπ,t; t, π, y) zϕ2D (zπ,t+t; + t, π, y) (cid:17) zπ ϕ3D (cid:21) , (1) where ϕ2D denotes the teacher model parameters, is sampled from U[TMin, TMax] with 0 < TMin < TMax < = 1000, and Cls indicates classifier-free guidance (CFG) [28]. By introducing timestep shift [75] sampled from uniform distribution U[0, η(t TMin)], ASD achieves more effective training of the native 3D generator. We utilize the timestepdependent weighting factor from DMD [146], as implemented in [103, 124]. We let ω(t) = NoGrad(Mean(zπ zCls 1 ϕ2D (zπ,t; t, π, y))) + δ , (2) where NoGrad detaches gradients for loss back-propagation, and Mean applies L1-norm across height, width, channel dimensions and all rendered views. Unlike [103, 124, 146], we add constant δ = 0.1 to the denominator, which stabilizes training and improves generation performance. We apply this objective function to supervise 3D outputs using three teacher models (SD, MV, RD) and two rendering pipelines (volume rendering and mesh rasterization). Regarding the sampling range of timestep t, Max = 980 throughout training, while TMin starts at 500 and linearly decreases to 20. Teacher-specific hyperparameters vary: RD uses CFG=20 and η = 0.1; MV uses CFG decreasing from 20 to 10 and η = 0; SD uses CFG=5 and η = 0. Setting η = 0 for multi-view teachers that supervise RGB renderings aligns with the findings in PiSA-SR [103]. Additionally, we incorporate regularization terms during training, such as sparsity loss [84] and eikonal loss [142]. We linearly reduce the sparsity and eikonal loss weights from 1 to 0 throughout the training process. Figure 8. More results of our model trained with expanded corpus. 18 Algorithm 1 Progressive Rendering Distillation (PRD) Input: SD-based native 3D generator with zϕ3D and Dϕ3D ; score distillation objective Lϕ2D parameterized by multi-view diffusion model ϕ2D; prompt corpus Sy; number of rendered views ; number of steps 1 Initialize optimizer Opt for zϕ3D and Dϕ3D 2 Define fixed timesteps = t1 > t2 > tK > 0 3 while not converged do 4 Sample text prompt Sy Sample ˆz0 (0, I) for t1 to tK do Sample ϵ (0, I) zt αt ˆz0 + σtϵ ˆz0 zϕ3D(zt; t, y) ˆθ Dϕ3D ( ˆz0) Sample camera poses π1, . . . , πN for 1 to do xπi g(ˆθ, πi) end Lϕ2D (xπ1 , . . . , xπN ; π1, . . . , πN , y) Save 1 ϕ3DL in Opt 5 6 8 9 10 11 12 14 15 16 17 end Update zϕ3D and Dϕ3D with gradient saved in Opt 18 19 end 20 return zϕ3D and Dϕ3D Figure 9. Qualitative comparison with LATTE3D [134]. Noise schedule. The PRD training incorporates progressive noise addition to the denoised latents (see Line 8 in Algorithm 1). Being adapted from SD [89], our native 3D generator follows the DDPM [29] noise schedule in training. During inference, we employ DDIM [98]. C. More Qualitative Comparison Results Comparison with methods adapting SD as native 3D generators. Since the codes or trained models of current SD-based native 3D generators [67, 78] are not publicly available, we conduct our comparisons by using their visual results presented in the original publications. The qualitative comparisons with PI3D [67] and HexaGen3D [78] are presented in Fig. 10 and Fig. 11, respectively. As both the two compared methods employ data-driven training, they inherit pose inconsistencies existed in the 3D training datasets [17], leading to the issue of occasional pose misalignment. This can be clearly observed from PI3Ds result of dalmatian wearing firemans hat shown in Fig. 10, where the dog is oriented sideways. The comparison results demonstrate our methods superior visual fidelity with the input prompts. These improvements are attributed 19 Figure 10. Qualitative comparison with PI3D [67]. Figure 11. Qualitative comparison with HexaGen3D [78]. to our proposed Progressive Rendering Distillation (PRD) scheme, which utilizes multi-view teachers in training without requiring 3D training data. Comparison with native 3D generators trained with score distillation. We further compare our approach with existing methods that employ score distillation for native 3D generator training. Specifically, we compare against the current state-ofthe-art method, LATTE3D [134]. Since the code or model of LATTE3D is unavailable, we conduct qualitative comparisons using their published results. The visual comparisons are presented in Fig. 9. It can be seen that our method demonstrates notable improvements in both texture fidelity and geometric accuracy. For example, in blue tulip, our model captures more natural flower textures, while in pile of dice on green tabletop, our model achieves more precise geometric structures. These improvements can be attributed to our strategic adaptation of SD as the backbone architecture, which allows our model to leverage its powerful generative capabilities. D. Expanding Training Corpus Since our proposed training scheme does not require 3D ground truth data, it can be easily up-scaled to large amount of text prompts. We collect total number of 1.7 million text prompts from HuggingFace that were used to generate images by DALL-E and Midjourney. This corpus has more unnatural prompts than the Objaverse [17], and it is more challenging. To the best of our knowledge, our work is the first that can process more than 1 million training data. Our model, trained on this expanded dataset, achieves enhanced visual quality, as demonstrated in Fig. 1 in the main paper and Fig. 5, Fig. 8 in this supplementary file. E. More Ablation Studies The necessity of multiple teachers. We employ SD [89], MV [96] and RD [86] as teachers for multi-view supervision of RGB, normal and depth maps. Here we perform ablation studies by systematically removing individual components. First, as visualized by w/o SD in Fig. 12, when SD is removed, leaving only MV and RD as teachers, the model can collapse to generate results inconsistent with text prompts. For example, given the prompt DSLR photo of cracked egg with the yolk spilling out on wooden table, the model collapses to generating stack of discs. This occurs because training for multi-view generation may impair MV and SDs text understanding capabilities, resulting in outputs that diverge from the specified text descriptions. SD can prevent from training collapse and improve the generation stability. Second, the importance of MV is demonstrated by the visualizations of w/o MV in Fig. 12. Without multi-view RGB supervision, the generated results tend to show repetitive and redundant contents across different viewpoints. For instance, multiple egg yolks might appear in the results of DSLR photo of cracked egg with the yolk spilling out on wooden table. Finally, the importance of RD is validated by the visualizations of w/o RD. We can see that adding normal and depth constraints enhances text consistency in the outputs, such as the generated wooden table in the results of DSLR photo of cracked egg with the yolk spilling out on wooden table. Overall, the combination of SD, MV, and RD as teachers achieves the best results, as validated by the visualization of w/ All and the metrics shown in Tab. 4. 20 Figure 12. Visualizations for the ablation study on jointly using SD, MV and RD as multi-view teachers. w/o SD w/o MV w/o RD w/ All (Proposed) C.S. R@1 63.0 67.4 41.5 68.2 20.1 25.9 11.4 32.3 Table 4. Ablation study on jointly using SD, MV and RD as multi-view teachers. The necessity of dual rendering. We use dual rendering framework that integrates mesh rasterization [47] and volume rendering [142] for 3D output supervision, as detailed in Sec. B. The effectiveness of this dual approach is demonstrated through quantitative and qualitative evaluations in Tab. 5 and Fig. 13, respectively. Without volume rendering, relying solely on mesh rasterization leads to training collapse and invalid mesh extraction. The results labeled as w/o Volume Rendering in Fig. 13 demonstrate that training converges to state where the SDFs zero-level set vanishes, resulting in mesh extraction failure and empty space. Conversely, using only volume rendering, which is constrained to low-resolution training, fails to produce high-quality mesh geometry, leading to rough and coarse textural details, as shown by w/o Mesh Rasterization in Fig. 13. For example, it fails to produce the shining gold texture for the prompt DSLR photo of toilet made out of gold. Moreover, without direct mesh supervision, volume rendering-based methods may produce geometrically invalid structures. This limitation is evident in the result of DSLR photo of aerial view of ruined castle, where the extracted meshes exhibit incorrect structural features and poor textures, manifesting as gray regions in parts of the mesh. As shown by w/ Both in Fig. 13 and supported by the superior metrics in Tab. 5, our dual rendering approach enables stable training while producing meshes with detailed textures and well-defined geometric structures. The choice of LoRA rank. We demonstrate the significance of using LoRA rank of 16 in our Parameter-Efficient Triplane Adaption (PETA). With lower rank of 8, shown as Rank=8 in Fig. 14, the model exhibits insufficient learning capacity, as evidenced by its failure to generate the top hat structure for the prompt capybara wearing top hat, low poly. However, setting higher rank, such as 32, can also lead to unreasonable geometric outputs. As shown in Rank=32 in Fig. 14, some unwanted platform structures appear at the bottom of results of capybara wearing top hat, low poly and zoomed out DSLR photo of baby dragon. Such artifacts stem from the inherent generation biases in both MV 21 Figure 13. Ablation study on dual rendering. The cross mark means the model fails to generate mesh due to training instability. Figure 14. Visualization for the ablation study on the LoRA rank in PETA. and SD, as their training dataset [17] contains numerous examples where objects rest on square platforms. As result, the multi-view teachers are fitted to generate outputs with similar structures. When the LoRA rank is set too high, the native 3D generator tends to learn and reproduce the biases from the teachers during the distillation. Setting the rank to balanced value of 16 enables the model to generate text-aligned 3D results while avoiding the incorporation of undesirable biases into the 3D generation model. Denoted as Rank=16, both qualitative results in Fig. 14 and quantitative results in Fig. 14 show that rank of 16 yields the best performance. w/o Volume Rendering w/o Mesh Rasterization Joint (Proposed) C.S. R@1 25.1 67.4 68.2 0.01 25. 32.3 C.S. R@1 rank=8 rank=16 (Proposed) rank=32 62.9 68.2 66.2 15.6 32.3 26.6 Table 5. Ablation study on the dual renders. Table 6. Ablation study on the LoRA rank in PETA."
        }
    ],
    "affiliations": [
        "Center for Artificial Intelligence and Robotics, HKISI CAS",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences, UCAS",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA",
        "The Hong Kong Polytechnic University"
    ]
}