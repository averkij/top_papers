{
    "paper_title": "Challenges and Paths Towards AI for Software Engineering",
    "authors": [
        "Alex Gu",
        "Naman Jain",
        "Wen-Ding Li",
        "Manish Shetty",
        "Yijia Shao",
        "Ziyang Li",
        "Diyi Yang",
        "Kevin Ellis",
        "Koushik Sen",
        "Armando Solar-Lezama"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI for software engineering has made remarkable progress recently, becoming a notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difficult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. In this paper, we aim to discuss progress towards this in a threefold manner. First, we provide a structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, we outline several key bottlenecks that limit current approaches. Finally, we provide an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing field."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 2 6 2 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Challenges and Paths Towards\nAI for Software Engineering",
            "content": "Alex Gu MIT CSAIL Naman Jain University of California, Berkeley Wen-Ding Li Cornell University Manish Shetty University of California, Berkeley"
        },
        {
            "title": "Kevin Ellis\nCornell University",
            "content": "Koushik Sen University of California, Berkeley Armando Solar-Lezama MIT CSAIL gua@mit.edu naman jain@berkeley.edu wl678@cornell.edu manishs@berkeley.edu shaoyj@cs.stanford.edu liby99@seas.upenn.edu diyiy@cs.stanford.edu kellis@cornell.edu ksen@berkeley.edu asolar@csail.mit.edu"
        },
        {
            "title": "Abstract",
            "content": "AI for software engineering has made remarkable progress recently, becoming notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. It should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difﬁcult tradeoffs while most routine development effort is automated away. Reaching this level of automation will require substantial research and engineering efforts across academia and industry. In this paper, we aim to discuss progress towards this in threefold manner. First, we provide structured taxonomy of concrete tasks in AI for software engineering, emphasizing the many other tasks in software engineering beyond code generation and completion. Second, we outline several key bottlenecks that limit current approaches. Finally, we provide an opinionated list of promising research directions toward making progress on these bottlenecks, hoping to inspire future research in this rapidly maturing ﬁeld. Equal contribution, ordered alphabetically by last name"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Tasks in AI Software Engineering 2.1 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Code Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Code Refactoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Code Migration and Translation . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.3 Code Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Software Testing and Program Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Software Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Program Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.3 Program Repair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Software Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Code Documentation and Summarization . . . . . . . . . . . . . . . . . . . . 2.4.2 Pull Request (PR) Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.3 Code Understanding, Navigation, and Question Answering . . . . . . . . . . 2.5 Scaffolding and Meta-Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.6 Formal Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Challenges 3.1 Evaluation and Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Effective Tool Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Human-AI Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Long-Horizon Code Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Large Scope and Long Contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Semantic Understanding of Codebases . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Low-Resource Languages and Specialized Libraries . . . . . . . . . . . . . . . . . . . 3.8 Library and API Version Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.9 High Logical Complexity and OOD Domains . . . . . . . . . . . . . . . . . . . . . . . 2 4 6 7 7 7 9 9 10 11 11 11 11 12 13 14 15 16 18 19 21 24 26 4 Paths Forward 4.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Automatic Data Curation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Human-Centric Data Curation . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Environment Design for Code RL . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Adapting to Specialized and Quickly Changing Codebases . . . . . . . . . . 4.2.3 Training Code LLMs to Collaborate with Humans . . . . . . . . . . . . . . . . 4.3 Inference Time Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Semantic-Aware Embeddings and Retrieval . . . . . . . . . . . . . . . . . . . 4.3.2 Integration with SWE Development Frameworks . . . . . . . . . . . . . . . . 4.3.3 Incorporating SWE Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.4 Scaffolding Human Supervision . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Limitations 6 Conclusion 7 Acknowledgements Survey of Related Work: Tasks in AI Software Engineering A.1 Code Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Code Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Software Testing and Program Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Software Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Scaffolding and Meta-Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Formal Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 28 29 30 30 31 35 35 37 38 39 41 41 70 70 71 74 75"
        },
        {
            "title": "1 Introduction",
            "content": "AI for software engineering has made remarkable progress recently, becoming notable success within generative AI. Despite this, there are still many challenges that need to be addressed before automated software engineering reaches its full potential. With additional efforts, it should be possible to reach high levels of automation where humans can focus on the critical decisions of what to build and how to balance difﬁcult tradeoffs while most routine development effort is automated away. Reaching this level of automation, however, will require substantial research and engineering efforts across academia and industry. This paper provides an opinionated view of the tasks, challenges, and promising directions towards achieving this goal. Many existing surveys overlap with the topics that are discussed in this paper. Liang et al. (2024) and Sergeyuk et al. (2025) survey the successes and challenges of AI programming assistants, (Wang et al., 2024c) survey using LLMs for software testing, and Joel et al. (2024) survey using LLMs in low-resource and domain-speciﬁc languages, and Zhang et al. (2023c) focus on automated program repair, both with and without LLMs. Finally, Yang et al. (2024d) is roadmap for formal mathematical reasoning and has some overlap with our discussion on software veriﬁcation. In addition, many papers discuss the current state, challenges, and future of AI for software engineering (Fan et al., 2023; Ozkaya, 2023; Wong et al., 2023; Zheng et al., 2023; Hou et al., 2024; Jin et al., 2024; Wan et al., 2024b; Roychoudhury et al., 2025a). Our work draws inspiration from them, and we recommend that the reader consult with them for alternative perspectives. In Sec. 2, we provide structured taxonomy of concrete In this paper, our goal is threefold. tasks in AI for software engineering. In particular, we emphasize that there are many other tasks in software engineering beyond code generation and code completion, encouraging research in these areas. We provide three measures for categorizing concrete realizations of each task: the scale of the problem, the logical complexity, and the level of human intervention. Moving forward to Sec. 3, we highlight nine challenges in the ﬁeld that todays models face, each cross-cutting and applicable to several tasks. In Sec. 4, we posit set of promising research directions to tackle the challenges above, with Fig. 1 summarizing which directions correspond to each challenge. We hope that through our paper, the reader can appreciate the progress the ﬁeld has made, understand the shortcomings of todays state-of-the-art models, and take inspiration from our suggested future ideas for tackling these challenges."
        },
        {
            "title": "2 Tasks in AI Software Engineering",
            "content": "We ﬁrst provide taxonomy of tasks in AI software engineering. To provide structured way to consider concrete realizations of each task, we deﬁne three measures that apply across them: scope, logical complexity, and level of human intervention. To achieve an AI software engineer, we strive for AI to be capable across the board for all three measures. Scope Measure: We deﬁne three levels of scope, the extent of changes that the AI makes to the codebase. Function-level scope refers to single, self-contained functions such as in HumanEval (Chen et al., 2021a) and MBPP (Austin et al., 2021). Self-contained unit scope goes beyond singular functions and to larger chunks of code such as entire ﬁles and classes, such as FullStackBench (Liu et al., 2024d) and BigCodeBench (Zhuo et al., 2024). Finally, project-level scope refers to larger 4 Semantic Understanding of Codebases (Sec. 3.6) Automatic Data Curation (Sec. 4.1.1) Evaluation and Benchmarks (Sec. 3.1) Human-AI Collaboration (Sec. 3.3) Low-Resource Languages and Specialized Libraries (Sec. 3.7) Library and API Version Updates (Sec. 3.8) Human-Centric Data Curation (Sec. 4.1.2) Training to Collaborate with Humans (Sec. 4.2.3) Scaffolding Human Supervision (Sec. 4.3.4) Fast Specialized Codebase Adaptation (Sec. 4.2.2) Effective Tool Usage (Sec. 3.2) Incorporating SWE Tools (Sec. 4.3.3) High Logical Complexity and OOD Domains (Sec. 3.9) Reinforcement Learning for Code (Sec. 4.2.1) Long-Horizon Code Planning (Sec. 3.4) Integration with SWE Development Frameworks (Sec. 4.3.2) Large Scope and Long Contexts (Sec. 3.5) Semantic-Aware Embeddings and Retrieval (Sec. 4.3.1) Figure 1: Overview of Challenges (Sec. 3) and Paths Forward (Sec. 4) in AI for Software Engineering codebases such as entire repositories, such as in Commit0 (Zhao et al., 2024) and SWE-Bench (Jimenez et al., 2024). Logical Complexity Measure: Tasks require wide range of reasoning abilities when it comes to devising algorithms to solve them. Low logical complexity tasks require little to no reasoning, such as writing CRUD (create, read, update, delete) applications or using APIs. Medium logical complexity tasks include most LeetCode problems, ﬁnding inputs to fuzz simple programs, and reasoning about execution behavior of multithreaded programs. High logical complexity tasks require meticulous and challenging levels of algorithmic and logical reasoning, either because the algorithm is complex or because the problem requires clever thinking and insights. This includes difﬁcult competition programming problems, writing large thread-safe concurrent programs, cracking cryptographic ciphers, and solving SMT-like problems. Many popular coding benchmarks are function-level, medium-high logical complexity, such as APPS (Hendrycks et al., 2021), CodeContests (Li et al., 2022a), and LiveCodeBench (Jain et al., 2024b). 5 Level of Human Intervention Measure: AI coding is collaborative task. Treude and Gerosa (2025) categorize interactions between developers and AI. Each interaction progresses through four phases: the trigger for the interaction, the AI response describing the systems output, the developer response capturing how developers react to the AI response, and the output of the interaction, the exact result. They characterize these developer-AI interactions into eleven types, including autocomplete code suggestions, conversational assistance (e.g., asking question about codebase), selection-based enhancements (e.g., refactoring selected chunk of code), commentguided prompts (e.g., natural language to code), check correctness, and more. We map these interactions to the autonomy taxonomy outlined in Morris et al. (2023)1 to deﬁne three levels of human intervention. We distill their six levels of autonomy into three levels: low (No AI and AI as Tool), medium (AI as Consultant and AI as Collaborator), and high (AI as an Expert and AI as an Agent). Low autonomy is when the human has full control over the task and uses AI to automate simple sub-tasks. This might look like writing codebase with tests while leaving small function-level snippets for the AI to ﬁll in. Medium autonomy is when there is similar amount of human-AI collaboration, with interactive coordination of goals and tasks. Here, the human and AI might both suggest refactorings, optimizations during the development cycle. High autonomy is when AI drives the interaction and tasks, identifying required changes and the changing demands of the user. The AI would defer to the human only when needed or for ﬁnal check, write the code and tests autonomously. Next, with our taxonomy of measures in place, we turn to the set of tasks that are reﬂective of the tasks and capabilities of human software engineer. We give brief description of each task in this section, deferring more extensive survey to Appendix A."
        },
        {
            "title": "2.1 Code Generation",
            "content": "Code generation is the task of generating code from speciﬁcation. In code completion, the speciﬁcation takes the form of preexisting code snippet, and the goal is to complete the snippet. The most popular form of code completion is tab completion, where the user can press the tab key to complete block of code (e.g. GitHub Copilot). Tab completion is often done at line-level or function-level scopes but needs to be fast to provide users with seamless experience. Another paradigm is natural language to code, where the speciﬁcation is natural language description with requirements such as the task description, input-output examples, or libraries to use. Recently, AI-driven IDEs, such as Cursor Composer and Codeiums Windsurf Editor, have blurred the lines between the two paradigms. With the ultimate goal of decreasing the burden of human programmers, they aim to automatically infer the users intent from the code context and user behavior (e.g. keystrokes, user edits, ﬁle navigation patterns). However, when intent is vague, they allow users to specify desired functionality via chat interfaces. Depending on scope and logical complexity, code generation can vary highly in difﬁculty. Reliable code generation in large codebases is still challenge for state-of-the-art AI systems today. 1We follow page 9, Table 2 from their paper"
        },
        {
            "title": "2.2 Code Transformation",
            "content": "2.2.1 Code Refactoring In code refactoring, the goal is to take working implementation of piece of software and rewrite parts of it while maintaining correctness. One challenge with this task is that success extends beyond functional correctness or metrics. The goal is often to improve code maintainability, readability, or extensibilityqualities that can be inherently difﬁcult to quantify and highly context-dependent. For instance, extracting shared functionality into helper methods presents trade-offs between modularity and cognitive complexity (Parnas, 1972). While there are no hard rules for when to extract functionality, one heuristic adopted by software engineers is the rule of three (three strikes and you refactor)abstractions should only be used when piece of code has been duplicated thrice. Because it can often be unclear what level of abstraction refactorings should be done at, completing refactoring at high autonomy level is also difﬁcult. These challenges are further compounded by the need to understand implicit trade-offs customized to speciﬁc codebases, respect conventions, and reason about the long-term maintenance implications of structural changes. While code refactoring often has low logical complexity, it can be laborious in practice due to scope, as seemingly small refactors can propagate across the entire codebase. Example: React Fiber architecture refactor: Reacts major refactoring was motivated by performance limitations in the original engine, particularly for complex UIs with animations and dynamic updates. Beyond challenges related to optimized implementation, major challenge was providing backward compatibility while completely rewriting Reacts core algorithm. Being an open source tool, this refactor also required educating developers about new concepts without disrupting existing mental models highlighting nuances in real-world software system design. 2.2.2 Code Migration and Translation An incredibly resource-intensive (time and manual effort) task frequently affecting companies is migrating large amounts of code while preserving all the original functionality and semantics. Such high-value migrations present opportunities for AI-assisted automation to reduce cost and manual effort. Code migration often has very high scope (many ﬁles and systems affected alongside their interdependencies) and high logical complexity (semantic depth of required transformations, constructs in different languages may be different). Current solutions may excel at migrations with high scope but modest logical demands (API migrations, type conversions) but struggle with changes across component boundaries (Nikolov et al., 2025). special case of code migration is code translation (transpilation): rewriting code from source language to target language. In industry, this task can be motivated by several reasons, such as security and scalability concerns in legacy languages, avoiding the technical debt project has accumulated over the years, and improving the performance of codebase. Due to the safetycritical and cross-system nature of many migrations, this task often requires substantial human oversight in practice and cannot be done fully autonomously. 7 Example: Scala version migration: recent Scala 2.13 to 3 migration (Ricadat, 2025) illustrates these challenges, documenting year-long effort. Critical issues included the loss of macro annotations, broken type projections, incompatible libraries, and compiler performance degradationall requiring innovative workarounds and architectural changes. There have been many similar language migrations with analogous problems, famously Python 2 to 3 and Swift 4 to 5. Example: COBOL: COBOL powers 80% of in-person ﬁnancial services transactions and 95% of ATM swipes while processing $3 trillion in commerce day, with over 220 billion lines of COBOL code in production (Taulli, 2020). However, there are less and less COBOL programmers, leading to the desire to migrate out of COBOL and into modern language like Java (Sneed, 2001; Sellink et al., 2002; Sneed, 2010). However, because of the large scope and high logical complexity of existing COBOL code, migrating from COBOL to Java would be monumental undertaking and many companies opt to continue using COBOL. These companies are still forced to migrate to newer versions like COBOL V6, because eearly versions of COBOL were gradually withdrawn from service. This task still requires skilled COBOL engineers and high precision, as it can often be difﬁcult to understand the business logic of legacy code and introducing bugs can have dangerous implications. Example: Twitter migration to improve latency: Twittera built its initial platform using Ruby on Rails, facilitating rapid development. However, as the user base expanded, performance and scalability issues arose. They migrated key components to Java and Scala, achieving 3X latency drop. This transition required re-architecting the system to adapt Rubys dynamic features to the statically typed environments of Java and Scala, exemplifying the complexities of large-scale code translation. ahttps://www.infoq.com/news/2012/11/twitter-ruby-to-java/ Example: to Rust: There has been push to use translation as proactive approach to eliminate memory safety vulnerabilities in C-based systems. This has even garnered attention from the US Department of Defensea, which has long-lived systems that disproportionately depend on C, supporting programs to translate codebases to Rust (TRACTOR). Recent efforts like Syzygy (Shetty et al., 2024), C2SaferRust (Nitin et al., 2025), and AlphaTrans (Ibrahimzada et al., 2024) have shown the potential for hybrid approaches combining LLMs with traditional program analysis techniques. However, some signiﬁcant challenges remain, including ensuring correctness in large codebases while maintaining desirable attributes such as speed, reduced vulnerabilities, and idiomaticity. ahttps://www.darpa.mil/news/2024/memory-safety-vulnerabilities 2.2.3 Code Optimization Transforming programs to improve performance characteristics while maintaining functional correctness is critical software task. Optimizing real-world systems poses signiﬁcant challenges due to the large scope and high logical complexity of the task, as performance bottlenecks must be identiﬁed and new algorithms to mitigate them must be proposed. Code optimization often has 8 large search and solution space with competing objectives like speed, memory efﬁciency, and readability, for example when optimizing kernel code at the PTX level for GPU-based AI model optimization (Zhao et al., 2025; Ouyang et al., 2024). In many scenarios, high levels of autonomy may not be desirable, as tradeoffs can depend heavily on external factors such as hardware, and the best optimizations may ultimately affect readability. Example: Google Chrome performance improvements: For over two decades, changes to the Chrome web browser have been an exemplar of optimization affecting real-world code (Chromium, 2018). Their V8 engine achieved 20x performance improvement through coordinated optimizations across multiple layers - from implementing concurrent garbage collection that reduced bloat by 100x to developing specialized compilers like TurboFan that improved performance by 5-10%, to enabling background parsing and compilation that reduced compile time by 20%. The demand for cross-layer and low-level code changes (e.g., writing new JavaScript interpreter) and building tools to measure and test representative performance metrics are key challenges for achieving this sort of real-world impact with LLMs."
        },
        {
            "title": "2.3 Software Testing and Program Analysis",
            "content": "In the process of software development, there will inevitably be bugs. The difﬁculty of detecting these bugs can vary depending on their scope and logical complexity. For LLMs, minor typos or correctness bugs (small scope, low logical complexity) are easier to spot (Mosolyg et al., 2021) while complex concurrency bugs and security vulnerabilities (large scope, high logical complexity) can be tricky because they can be hidden deep in the call stack, contain subtle logic errors, or be hard to isolate due to the large scope (Trent and Li, 2025). 2.3.1 Software Testing Software testing is practical approach to prevent bugs, both during development and production. There are several popular approaches to software testing, some short-term and others longer-term. Unit testing refers to using input-output style tests that exercise the functionality of piece of code. Property-based testing is based on formal speciﬁcations and relies on specifying test cases that ensure that known properties of the code hold. Mutation testing modiﬁes program subtly and ensures that the test suite can detect errors in these mutations. Fuzzing refers to executing programs with unexpected inputs and monitoring for exceptions, usually over more extended time period. 9 Example: OSS-Fuzz on FreeType: OSS-Fuzz (Chang et al., 2024), Googles automated fuzzing infrastructure, has proven its value by swiftly uncovering security ﬂaws in critical software. For instance, when recent source change was made to FreeTypea font rendering library deployed on over billion devicesOSS-Fuzz detected heap-buffer-overﬂow within hours: ERROR: AddressSanitizer: heap-buffer-overflow on address 0x615000000ffa READ of size 2 at 0x615000000ffa thread T0 SCARINESS: 24 (2-byte-read-heap-buffer-overflow-far-from-bounds) #0 0x885e06 in tt_face_vary_cvtsrc/truetype/ttgxvar.c:1556: The goal of software testing is to design tests that can surface bugs reliably. This is evaluated through metrics such as code coveragehow much of the source code is executed when the test suite is run. An alternate to code coverage is mutation score, where mutants are generated, and the score is deﬁned as the percentage of mutants causing the suite to fail. While practical, software testing faces challenges such as the scalability limits of traditional tools and the difﬁculty of manually designing tests with good coverage. As LLMs continue to improve at coding, they present promising avenue for automatically generating high-quality tests. Example. Fault-based test generation at Meta: Metas Automated Compliance Hardening (ACH) system (Foster et al., 2025) is system that generates tests aiming to catch realworld bugs. ACH works in three steps: ﬁrst, the engineer describes the bugs they are worried about. Second, ACH combines LLMs with mutation testing to generate code with those bugs. Finally, these mutants were used to develop unit tests capturing them. ACH was used to generate tests for Messenger and WhatsApp, where engineers accepted 73% of its tests. 2.3.2 Program Analysis While testing catches bugs, the most challenging software issues are security vulnerabilities and zero-day exploits, from memory corruption to privilege escalation. This requires deep program understanding, that testing/fuzzing often misses. For instance, zero-day is vulnerability unknown to the software developers that is found by an attacker, and there is no patch available from the vendor. In such cases, the only practical approach is offensive security research, manual source code audits, and root cause analysis of prior vulnerabilities to harden codebases. Example: Variant Analysis: Project Zeros (Hawkes, 2019) investigations at Google reveal that many in-the-wild 0-day exploits arent entirely newtheyre often variants of vulnerabilIn their analysis of recent 0-day reports, nearly half ities that had been patched before. of the issues were closely related to earlier bugs (such as those affecting Windows win32k and iOS IOMobileFrameBuffer). This ﬁnding underscores the importance of performing rigorous root cause and variant analyses. Instead of just ﬁxing single exploit path, security teams must comprehensively address the underlying bug class, ensuring that alternate exploitation routes are closed off for goodmaking this task more challenging. Another example of valuable but challenging analysis is invariant detection. program invariant is property of piece of code that is guaranteed to be true at speciﬁed program point, no matter what the input is. simple example is that after the line int = * c; is executed, 10 must be nonnegative. Identifying invariants in program can be useful when testing, debugging, and modifying code. This task can be challenging because it requires reasoning abstractly about code execution across many different potential inputs and execution paths to determine what properties must hold for all possible inputs. 2.3.3 Program Repair Bug localization is signiﬁcant challenge in program repair, as pinpointing the exact site of bug can be challenging, especially in large codebases. Issues like out-of-memory accesses often manifest themselves further downstream, making it difﬁcult to identify the root cause. Once the bug is localized, the next step is to repair the bug. LLMs can be an ideal tool for this because they have seen wide variety of bugs during training. Function-level, low-logical complexity bugs can often be easily ﬁxed by feeding back error information to the model. It can be tricker to perform repair in larger scopes (e.g. repositories) where the code has higher logical complexity. This can often require several steps, including designing and implementing new algorithms or making complex refactorings across multiple ﬁles."
        },
        {
            "title": "2.4 Software Maintenance",
            "content": "2.4.1 Code Documentation and Summarization To ensure maintainability, readability, and ease of collaboration, code must be well documented. Good documentation needs to be written cleanly and crisply, describing what the function does and how the function works. It must also anticipate and address any misunderstandings that programmer might have, such as potential side effects or special cases. Humans often see documentation as chore and neglect it, leading to code and documentation frequently being out of sync. This has led to the concept of self-documenting code, code that clearly conveys its purpose. As documentation is generally task that has low logical complexity and does not require too much human intervention, LLMs can help ensure that documentation is continuously updated artifact in sync with the code. 2.4.2 Pull Request (PR) Review Reviewing pull requests is an integral aspect of the software development cycle. While the most essential requirement for PRs is that new feature is implemented correctly, other important considerations include checking whether the repositorys style conventions are satisﬁed, ensuring that the PR does not introduce any new bugs, verifying that program invariants and guarantees still hold, and inspecting whether tests are robust. Generally, reviewing PRs is task requiring low logical complexity and can be automated relatively easily. 2.4.3 Code Understanding, Navigation, and Question Answering When encountering codebase for the ﬁrst time, developers often ﬁnd it challenging to understand and develop good mental model of the code. This can be due to many reasons: too many wrapper functions, excessive error-handling boilerplate, deep call stacks, or poor code cleanliness. 11 One important challenge in code understanding is code navigation: ﬁnding where relevant functionality is implemented. Doing this well requires understanding the high-level layout of where every functionality lies in the codebase and the low-level understanding of which helper functions are used to implement each functionality. Another challenge is code question answering: answering complex questions about codebase, which requires sophisticated code understanding and reasoning abilities. Models should not hallucinate or give incorrect information that skews developers mental model of the code. Beyond other tasks mentioned in this section, developers might commonly ask questions related to data ﬂow (when and where data structures get mutated), code functionality (whether there are any side effects), performance characteristics (determining the runtime and memory complexity of function), or error handling (whether certain corner cases are handled)."
        },
        {
            "title": "2.5 Scaffolding and Meta-Code",
            "content": "For software system to work, the core logic must be written well, but that is not enough. Many infrastructural aspects must be in place to support the software. We group these into two main categories: scaffolding and meta-code. We deﬁne scaffolding as task outside of the code that must be done to get the software running properly. Examples of scaffolding include setting up Google authentication, subscribing to APIs, managing ﬁle storage, and generating API tokens. In contrast, we deﬁne meta-code to be code that is important to make the system work but does not actually participate in the execution of its main logic. Examples of meta-code include test harnesses, conﬁguration ﬁles, CI/CD code, Makeﬁles, Dockerﬁles, sandbox databases, and preprocessors. Scaffolding and meta-code often are small in scope and have low logical complexity but can require lot of domain-speciﬁc knowledge about the application, requiring human intervention. Example. Conﬁguration validation: Ciri (Lian et al., 2024) is tool that uses LLMs for conﬁguration validation on open-source projects including Django, PostgreSQL, and Redis. They ﬁnd that while Ciri excels at detecting misconﬁgurations of syntax and range violations, it struggles to detect dependency and version violations and is limited to narrow range of misconﬁgurations. They also ﬁnd that LLMs are biased towards more popular conﬁguration parameters, which may lead to hallucinations in out-of-domain scenarios. Infrastructure-as-code and Security. particularly challenging case is generating Infrastructure-ascode such as Terraform, where you specify the type of infrastructure speciﬁcations (such as AWS EC2 instances, Kubernetes clusters, S3 buckets, VPC buckets) as code and execute it to create the infrastructure. When generating such code, LLMs struggle with security conﬁgurations due to the complex interplay between service-level permissions (e.g., AWS resource access), resourcelevel permissions (e.g., speciﬁc allowed actions), and provider-speciﬁc security primitives like IAM roles, security groups, and network access controls. Example. Distinguishing permission levels in cluster setup: Terrateam (2024) show that on task of bringing up cluster, models fail to distinguish between ECS (Amazon Elastic Container Service) Task Execution Roles (for container operations) and Task Roles (for applicationlevel permissions). This resulted in overly permissive policies where single role was granted both image pull permissions and DynamoDB table access, violating principles of least privilege."
        },
        {
            "title": "2.6 Formal Veriﬁcation",
            "content": "The task of formal veriﬁcation involves generating checkable, mechanized proofs that can guarantee that piece of code works as intended. There are two major classes of formal veriﬁcation: full functional veriﬁcation (FFV) and property veriﬁcation (PV). In FFV, the goal is to design complete and precise formal speciﬁcation that captures the desired behavior of the implementation, such as fully veriﬁed data structures (mutable lists, trees, graphs, hash tables) (Zee et al., 2008). The main challenge in full functional veriﬁcation is in correctly writing the speciﬁcation so that all desired properties are speciﬁed. FFV generally has high scope and medium logical complexity, as the properties to verify are often straightforward to write once the correct abstractions are found. While FFV provides complete set of guarantees, it is usually sufﬁcient to opt for PV, where few key properties of system are proven correct. Examples include: ensuring that two threads do not simultaneously enter critical section of program, verifying that complex program will always terminate, proving the absence of security vulnerabilities like buffer overﬂows, and guaranteeing memory safety. One challenge that makes PV difﬁcult to use in practice is the issue of false positives, where functionally correct code often does not pass property checks. prime example is Rust: while the powerful type system enforces many desired guarantees, code with correct semantics often does not pass type checks. Another challenge is that many standalone tools for PV are often semantics-dependent, which can make them hard to maintain as language semantics change. Example. Costly disasters: Formal veriﬁcation of software is important in mission-critical applications such as aircraft software, as software bugs may lead to costly disasters. In the maiden ﬂight of the Ariane 5 rocket, ﬂoating-point conversion error caused it to explode forty seconds after liftoff. Another case is with the computer-controlled radiation therapy machine Therac-25, where concurrency bugs led to six people being massively overdosed, leading to serious injury and deaths. Example. Veriﬁed Compiler: CompCert (Leroy et al., 2016) is formally veriﬁed optimizing compiler that supports restricted subset of including most of the ISO 99 language. CompCert has been formally veriﬁed using the Coq proof assistant (The Coq Development Team, 2024), eliminating the potential for compiler bugs. While formal veriﬁcation tools have begun to see adoption in industry, they has not yet become mainstream because of these challenges. Code LLMs could greatly ease this burden and make it more feasible to verify code at larger scales, especially verifying properties requiring lower logical complexity. Example. Property Veriﬁcation: Coverity: Coverity is static analysis tool meant to ﬁnd generic errors (memory corruption, data races) and system-speciﬁc violations (e.g. functionordering constraints). In their report (Bessey et al., 2010), they highlight two issues mentioned earlier: churn and false positives. The ﬁrst issue, churn, deals with ensuring that the tool produces the same result both when the code base is modiﬁed and across different versions of the tool, making upgrades constant headache. The second issue is that when the false positive rate is more than 30%, users ignore the tool and real bugs get lost among these false positives."
        },
        {
            "title": "3 Challenges",
            "content": "While the ﬁeld of AI for code has made fruitful progress, cutting-edge AI still struggles with SWE tasks, especially at larger scopes and higher levels of logical complexity. Next, we discuss ten key challenges in AI for code. Each challenge spans multiple tasks, and progress on any can lead to improvements on many tasks at once."
        },
        {
            "title": "3.1 Evaluation and Benchmarks",
            "content": "Todays code LLM evaluations focus on narrow set of tasks, suffer from potential contamination, and do not reliably measure real-world software engineering abilities. Potential solutions: 4.1 Our taxonomy of tasks and measures highlights some of the shortcomings of todays evaluations and benchmarks. For example, the majority of todays coding evaluations have no level of human intervention, with few, such as Copilot-Arena (Chi et al., 2025), having low to medium autonomy. HumanEval, MBPP, APPS, CodeContests, and LiveCodeBench are all at function-level scope, with low to medium-high logical complexity. Commit0 (Zhao et al., 2024), SWE-Bench (Jimenez et al., 2024), TestGenEval (Jain et al., 2024a), RefactorBench (Gautam et al., 2024), SWELancer (Miserendino et al., 2025) are at project-level scope with low to medium logical complexity. Task Diversity and Capability Isolation: Current coding evaluations primarily focus on the code generation task, while most of the tasks discussed in Section 2 are either not studied such as Code QA or only studied in limited scopes like EvalPerf (Liu et al., 2024c), vulnerability detection (Mei et al., 2024), formal veriﬁcation (Sun et al., 2024a). As more agent-based approaches are introduced for software engineering (e.g. pairing code generation model with debugging model), these engineering-related capabilities beyond just code generation will be important towards designing maximally performant system. Solely relying on end-to-end coding evaluations that focus on the overall correctness of codebase makes it difﬁcult to precisely measure progress and learn from the failure modes on individual tasks. Contamination: Data contamination is serious issue that, if not taken into account, can affect the soundness of various conclusions drawn from set of benchmark results. In coding, the performance of LLMs on competitive programming (Xu et al., 2024a; Jain et al., 2024b) and SWE-Bench (Aleithan et al., 2024) tasks has been shown to degrade over time, indicating the possibility of older problems being contaminated due to public exposure on the internet. For simpler function-level HumanEval style problems, Matton et al. (2024) suggest three potential causes of contamination: direct data leakage (benchmarks are on GitHub), synthetic data leakage (there are only limited number of interview problems), and overﬁtting to test sets (benchmark hacking). In addition, for code, contamination can be hard to detect, as semantically equivalent code that is syntactically distinct could be thought of as contamination (Riddell et al., 2024). recent benchmark, the Konwinski Prize2, is promising way to fairly evaluate SoTA LLM models by only using new GitHub issues. 2https://www.kprize.ai/ 14 Construct Validity: Construct validity refers to how closely measurement reﬂects the underlying concept. Given the implications of rapid performance improvement in the AI for the code domain, it is essential to have high-construct validity benchmarks evaluating how well programming agents can perform. While benchmarks like SWE-Bench come close, user experiences do not currently match rapid performance gains obtained from them. This is partially because many desiderata in software engineering cannot be described cleanly via automated unit testing. Things like multi-turn code generation, designing an UI, and writing clean and idiomatic code are all difﬁcult to quantitatively measure with precision. Designing reliable proxy metrics for these desired goals remains challenge."
        },
        {
            "title": "3.2 Effective Tool Usage",
            "content": "While software engineers use wide suite of programming tools when programming, most of todays AI coding systems do not invoke tools. AI needs to be able to select which tool to use, decide how to use it, and interpret the outputs in order to continue making progress on the task. Potential solutions: 4.3.3 Software engineering has witnessed the development of various open and proprietary tooling support for programming, debugging, analysis, and code management over time. For example, program analysis tools provide static and dynamic assurances on code correctness. Print statements and debuggers are used for dynamically analyzing and debugging programs at ﬁne-grained level. Beyond programming, such tools are richly integrated into the entire software development lifecycle, e.g., code navigation or search, reviewing code, CI testing. There have been efforts combining LLMs with tools such as calculators and search engines (Schick et al., 2023; Patil et al., 2023). However, effective integration of LLMs with software engineering tools is more challenging problem. Several early works have incorporated such tool feedback in code generation in an automated fashion, for example, linter or execution feedback in (Olausson et al., 2023; Zhong et al., 2024b; Gehring et al., 2024). However, these works do not actively interact with tools. More recently, programming agents have started incorporating tool use within their workﬂows termed as Agent-Computer-Interface (Yang et al., 2024b). These tools range from aiding in general search (grep), providing code editor for making changes (Wang et al., 2024g; Anthropic, 2024), language server for static analysis (Liu et al., 2024f), dependency analyzer (Bairi et al., 2024), terminal access for bash commands including code execution (Yang et al., 2024b), debugger (BigSleep, 2024). Dynamic and Effective Tool Usage: While many efforts combine LLMs and agents with tools, they do not achieve fully dynamic and effective software engineering tool usage. This involves an AI system seamlessly and proactively integrating appropriate tools depending on the task at hand. There are few challenges towards achieving this goal. First, the AI system must identify which tools could potentially be useful for the task at hand. Second, the system then needs to decide when to invoke the tool. complex debugging task might require the use of pdb or gdb to track intermediate program states, while looking at input-output pairs may be sufﬁcient for simple debugging tasks. Third, the agent then must ﬁgure out how to invoke the tool. If the agent knows that certain function in program has an error, it may wish to walk through only that function instead of the entire code from start to ﬁnish. Finally, the agent needs to incorporate the output provided by the tool in order to inform its next steps, e.g. edit the code if bug was uncovered or run the tool again otherwise. Example: Performance Instrumentation: common way to instrument software systems is known as compiler-inserted program instrumentation. CSI (Schardl et al., 2017) is tool that inserts instrumentation hooks to track objects such as memory loads/stores, function entry/exits, and basic blocks. CSI contains tools like code coverage tools, memoryoperations counter, performance proﬁler, and call-graph generator. To use the tool, the user must follow the API in order to write hooks so the correct aspects can be proﬁled. Tools like CSI are very valuable when trying to improve the performance of piece of code, but are not trivial to use. In order for an LLM agent to use CSI effectively, it must ﬁrst familiarize itself with the CSI API. Then, it needs to know exactly which aspects of the code to instrument, such as placing hooks before and after function suspected to be bottleneck. Finally, the agent needs to learn how to use the output of the tool to inform its approach to the task, such as deciding whether block of code can be further optimized after seeing its performance proﬁle."
        },
        {
            "title": "3.3 Human-AI Collaboration",
            "content": "Human-AI collaboration is still far from seamless. First, speciﬁcations written by humans are often vague and leave out many details, leading LLMs to produce code misaligned with humans. There is also very little controllability with coding LLMs, and todays human-AI collaboration interfaces are still limited. Potential solutions: 4.1.2, 4.2.3, 4.3.4 While AI coding systems are increasingly more powerful, the majority of them are still at low to medium autonomy level, serving as engineer assistance rather than achieving high or full automation. We identify few key challenges of todays AI coding systems that prevent these systems from working with humans effectively at higher levels of autonomy. Vague Speciﬁcations and User Misalignment: When using code LLMs or coding agents, we typically prompt them with natural language speciﬁcation. This can include natural language description of the desired code, input-output examples, relevant code snippets, and other functional requirements. However, there is gap in the level of abstraction between English and code, leading to incomplete or ambiguous speciﬁcations. This issue becomes more pronounced in longer programs, where the number of ambiguous decision points increases, and choices traditionally made by humans are instead implicitly embedded in the LLMs generated code. Consequently, users often experience misalignment due to vague speciﬁcations. While many code LLMs support multi-turn interactions, it remains inherently challenging for users to articulate their thought processes into follow-up natural language instructions. Speciﬁcations beyond text: While todays speciﬁcations predominantly rely on text, there are many domains for which pure text is insufﬁcient as speciﬁcation. In domains like robotics, virtual reality, embedded devices, and user interfaces, speciﬁcations often need to be multi-modal (e.g. showing the model picture of an UI to create) and world-interfacing (e.g. providing simulation code describing robot will interact with its environment). Inherent trade-offs in software development: Designing large software systems always surfaces tradeoffs between various desiderata such as readability, scalability, performance, maintainability, reliability, security, etc. These trade-offs are often context-dependent. long-term and rapidly moving project may be willing to trade off some performance to have simplicity and readability. Performance-critical applications may completely sacriﬁce readability to eke out every millisecond of speed (such as using bit-twiddling hacks). Finding the sweet spot among these trade-offs can often involve extensive prototyping and benchmarking to understand the performance characteristics of different approaches. However, user speciﬁcations in the initial prompt rarely include details about these trade-offs, nor do models often take them into account. Implicit constraints: Aside from functional/semantic correctness, there are also often implicit constraints in writing code. For example, many companies such as Jane Street and Google have style guides, and many GitHub repositories explicitly outline style elements that new code ought to follow. Zou et al. (2019) ﬁnd that GitHub pull requests that are more consistent with the style of the existing code get merged faster. Additionally, corporations may enforces codes of conduct or compliance requirements at the code level. Furthermore, codebases follow common programming patterns or system design patterns that are implicitly speciﬁed by the way the current code is written. However, when using code LLMs, these constraints are often inferred incorrectly (Wang et al., 2024h). Example: Serializer-Deserializer pattern for objects: Consider the issue astropy-#14181 from the astropy Python library. The issue requests support for new input ﬁle format (reStructuredText) to load astronomical data into the codebase more ﬂexibly. While the issue does not mention it explicitly, as per common practices, developers implement read (deserializer) and write (serializer) operations when implementing support for new ﬁle format. This ensures data can ﬂow bidirectionally between the ﬁle format and the applications internal data structures. However, models evaluated on this issue, as part of the SWEBench benchmark, only implemented the read method. Lack of Controllability: When using AI coding systems, programmers often seek speciﬁc functionality, yet they lack reliable ways to steer LLMs toward generating precisely the desired code. Instead, they typically rely on trial-and-error approach, repeatedly sampling outputs or providing feedback until the AI produces an acceptable solution. Consequently, signiﬁcant human effort is still required to review and modify the code to ensure it meets the intended functionality (Weisz et al., 2024). way to improve controllability is for AI coding systems to recognize when human input is needed and communicate effectivelyyet this remains the top-reported challenge in human-agent collaboration (Shao et al., 2024a). LLMs rarely defer to humans for clariﬁcation, while developers often ask questions to clarify the description of task provided by their peers. For example, when product manager reﬁnes requirements document, developers who are unclear about the scope or speciﬁcations ask questions and leave comments, which the manager resolves iteratively to disambiguate requirements (Nahar et al., 2022). Based on its knowledge of existing software, AI should be able to incorporate implicit priors from speciﬁcation while keeping the user in the loop. For instance, when designing an academic website, certain expectationssuch as including list of publications and contact informationare implicit. However, whether to include persons GPA would require explicit clariﬁcation. Restricted Human-AI Interface: as intelligence features embedded within integrated development environments Existing interfaces for code LLMs primarily manifest (IDEs). 17 Treude and Gerosa (2025) establishes taxonomy of developer-AI tool interactions, emphasizing low-level support mechanisms such as auto-complete suggestions, selection-based enhancements, and conversational assistance within the codebase context. While this taxonomy comprehensively covers existing AI coding systems that function primarily as engineering assistants, its applicability becomes questionable as these systems advance toward higher levels of automation. For instance, the ubiquitous Tab interaction paradigm prevalent in intelligent IDEs may prove inadequate when AI systems transition from completing developer-scaffolded functions to authoring the majority of the codebase autonomously. Current interfaces for coding agents, such as Devin, typically stream raw actions without adequate context or explanation. Given that these agents can execute numerous actions rapidly, developers face signiﬁcant challenges in effectively monitoring the process, implementing timely interventions, or reasserting control when necessary. This lack of transparency can also undermine trust in AI-generated code (Wang et al., 2024e). While human-AI interface design has received extensive attention in autonomous vehicle research (Benderius et al., 2017; Tinga et al., 2022), similar consideration for AI coding systems remains notably absent."
        },
        {
            "title": "3.4 Long-Horizon Code Planning",
            "content": "Large software engineering projects often require long-term planning about the design and structure of the code. LLMs struggle at two key aspects of this: designing good, lasting abstractions and respecting modularity and code quality principles. Potential solutions: 4.2.1, 4.3.2 When working on large projects, engineers and tech leads often make complex decisions about how to design and structure the code to best support the various functionalities that will eventually be needed. To build long-lasting software system, an engineer must know the potential paths that the systems evolution might take. This requires domain expertise and experience in how different code structures require different forms of extension. We believe that todays language models are unable to perform this level of sophisticated planning. Designing Good Abstractions: One instance of long-horizon code planning is choosing the right abstractions from the outset. An API designed with good abstractions will allow new features to be implemented seamlessly with minimal user overhead, while an API designed with poor abstractions may lead to excessive code duplication, refactoring, or even debugging. We discuss two examples of this, library learning and data representation. Library learning: Designing APIs and libraries are designed with useful abstractions often leads to more code reuse and more intuitive interfaces. The challenge of library learning is to derive library of useful abstractions from corpus of programs by abstracting out common reusable features (Ellis et al., 2021; Stengel-Eskin et al., 2024; Bowers et al., 2023). While the traditional library learning literature has focused primarily on code reuse, truly effective library must also prioritize ease of use and maintainability, as well as be robust and adaptable to future extensions. Data representation: The choice between data structures leads to variety of trade-offs when it comes to performance aspects such as memory usage and processing speed. For example, database engineers need to decide between various data models, storage formats, and indexing methods to balance performance. 18 Example: Database Design for Web Applications: Database engineers strive to design their databases in way that minimizes memory usage and maximizes query performance (speed). To achieve this goal, the databases community has spent considerable efforts optimizing both the high-level data representation and the underlying data structures (Kraska et al., 2018; Hawkins et al., 2011). Consider the task of designing database schema for restaurant owner to manage their business: keeping track of customers, managing rewards program, maintaining the restaurants inventory of ingredients, etc. One important design decision to make is deciding on schema: while having reservation and customer table is fairly straightforward, should we include table for customer reviews or simply add rating and review ﬁelds in the customer table? Another important design decision is choosing which database indexes to include. While choosing the correct indexes can speed up queries signiﬁcantly, indexes cost additional memory and must be kept updated. Making decisions like these requires knowledge of the application, context, and the effects of each option. Modularity and Code Quality: LLMs are trained and optimized primarily for code correctness with insufﬁcient focus on other aspects of code like quality and maintainability. This is further exacerbated with large scale reinforcement learning being performed using test cases which can lead to unintended consequences regarding code quality, as correct but poor quality code is still often given high reward. Empirically, it has been observed that LLM written solutions are often more complex than human-written counterparts. For example, Jain et al. (2024c) identiﬁed that LLMs prefer to repeat existing code instead of making use of abstractions in the existing code. One aspect of code quality is modularity, ensuring that code does not get duplicated too often. Here, Berlot-Attwell et al. (2024) identiﬁed that library or tool reuse is non-trivial for LLMs in coding and formal math domains."
        },
        {
            "title": "3.5 Large Scope and Long Contexts",
            "content": "Coding is domain where required context lengths can be very long, as codebases can consist of millions of lines of code, posing challenges to todays models. In addition, todays retrieval-based methods are still limited: models often retrieve incorrect information and can still struggle with leveraging retrievals effectively due to the difﬁculty of code reuse. Potential solutions: 4.3.1, 4.3.2 Large Scopes: At the repository level, the tasks in Sec. 2 become signiﬁcantly more difﬁcult and require many steps. In code generation, user alignment can be an issue because there are many decision points and tradeoffs that can compound. In code refactoring, modiﬁcations will touch multiple parts of the codebase, and it can be tricky to keep the repository consistent. In code debugging, functions can be large and bugs can be nested deeply within stacks of function calls. In code navigation, because there are so many functions interacting in various ways, it can be difﬁcult to know where each piece of functionality is implemented and how the code is pieced together. Another issue with large scopes is large context lengths. Software engineering often requires dealing with very large codebasesfor example, Google has repositories with over billion lines 19 of code (Potvin and Levenberg, 2016). As this is far too large for modern-day LLMs, choosing the correct context to include when using LLMs is important. Example: Debugging Cloud Applications: Organizations often rely on monitoring and observability tools to track the performance of their applications. One such tool is Datadog, an observability service for cloud applications that can monitor infrastructure, detect security anomalies, and track database performance. For larger applications with more moving parts, these logs can consist of thousands of lines of JSON payloads. For humans, sifting through these logs is usually matter of searching for certain keywords that they know will appear in the logs. However, LLMs often have hard time interpreting large amounts of logs like these. Limits of Retrieval-Augmented Generation (RAG): Retrieval-based algorithms have been the predominant way to deal with long-context coding issues. First, the retriever retrieves relevant functions. Then, the generator leverages the retrieval to improve generation. While RAG has proven effective in many NLP tasks such as question answering (Gao et al., 2023; Lewis et al., 2020), the code domain provides new challenges for these methods. Retrieval: In most NLP tasks, the retrieval step can be done relatively well because keywords that are in the query are often keywords that need to be retrieved. Unlike answering NL questions, writing code often requires drawing inspiration from code snippets that may be completely different syntactically. This can include programs with similar semantics, algorithms, or API calls, all of which potentially have very little in common when it comes to syntax. For example, the implementation of Dijkstras algorithm in GPS navigation application can guide the implementation of shortest-path algorithm in social media application. Because retrievers often rely on syntactic matching, these relevant programs can be hard to retrieve (Ma et al., 2024; Utpala et al., 2023). When deciding what to retrieve, it is also necessary to have sufﬁcient awareness of other parts of the codebase so that you know which building blocks are necessary to construct the new function. This can make the retrieval task relatively tricky, as shown by failure modes on two benchmarks, CodeRAGBench (Wang et al., 2024i) and BRIGHT (Su et al., 2024). Example: Failure Case of Finding Relevant Files When Resolving Issues: BM25, despite its widespread use in code retrieval, demonstrates limitations in scenarios that involve large and complex codebases. For instance, in chartjs Chart.js-7951 from SWE-bench Multimodal (Yang et al., 2024c), BM25 retrieval using the issue description returns suboptimal results. The top-3 retrieved ﬁles from src/ are src/scales/scale.radialLinear.js, src/scales/scale.linearbase.js, However, in src/elements/element.bar.js and src/controllers/controller.bar.js. This retrieval failure impedes the effectiveness of coding agents, many of which are augmented with code retrieval systems. When agents focus their attention on irrelevant ﬁles, their ability to resolve the issue successfully becomes substantially compromised. required to resolve the issue should occur and src/helpers/helpers.canvas.js. the critical modiﬁcations Generation: In NLP tasks, the generation step often is straightforward application of the retrieved information. However, in code, writing new function requires more than copy and paste. This is closely tied to the problem of code reuse: piecing together relevant snippets of code in precise and productive way to ﬁt the current context. Depending on what is retrieved, each piece of re20 trieved content provides different information. This can include information about the languages syntax, documentation about the API, clues about the algorithm to be written, or examples of similar functionality being written. Ding et al. (2023) ﬁnd that even when the oracle context is retrieved, LLMs tend to misuse it, highlighting lack of semantic understanding, which we discuss in the next section. Example: Bad Generation Despite Identifying the Correct Context: Ding et al. (2023) highlights failure case where code LLM fails to complete Python test case correctly, even though it has the correct context. The function name from the context, test case convert camel to snake, suggests that the function being completed is test case for convert camel to snake. With the given context, the model generates the function as convert camel to snake, which however does not match the larger codebase as other pieces of code expect this function name to be camel to snake. While this issue can partly be attributed to incomplete retrieval of relevant information, it also presents challenge for code LLMs, as they must recognize such inconsistenciesespecially when the immediate context is correctly providedthereby avoiding high-conﬁdence errors."
        },
        {
            "title": "3.6 Semantic Understanding of Codebases",
            "content": "Being able to effectively write code relies on having strong semantic understanding (somewhat like world model) of the entire codebase: structurally seeing how various parts of the code go together, knowing what is implemented where, understanding how the algorithms work, and keeping track of program invariants at certain program points. LLMs struggle with this global semantic understanding Potential solutions: 4.1 global and holistic semantic understanding of codebase is important for performing almost all code-related tasks. For example, lets say an engineer is asked to improve the runtime performance of query. To do so, they must ﬁrst understand the codebases structure well enough to know where all the pieces of the algorithm are implemented. Then, they need to understand the algorithm and implementation in detail. This includes both the high-level algorithm (including its time complexity) and the low-level implementation details to identify both algorithmic and implementation bottlenecks. Finally, after coming up with solution, they must then return to their understanding of the global code structure so that they can integrate their new algorithm without introducing new bugs. LLMs struggle at semantic understanding of codebases for several reasons. First, the way that code is pieced together can be relatively intricate, and understanding all these complex relationships can be difﬁcult. Second, code can often have units with high logical complexity that contain custom algorithms that may never have appeared anywhere in the training data. Finally, because disproportionately large number of LLM training tokens are spent on code generation rather than other coding tasks, models may lack holistic awareness and world model of code. One desiderata is that models can generalize knowledge across various coding tasks (Roychoudhury and Zeller, 2025). However, this may not be straightforward as just training on more tasks: Gu et al. (2024) found that coding models ﬁne-tuned on additional natural language/- code pairs saw signiﬁcant improvements on code generation but did not transfer to improving code understanding and execution reasoning. While there have been successful efforts to augment code LLM training with execution information to improve general coding capabilities (Ni et al., 2024; Ding et al., 2024c), imbuing code LLMs with general and holistic understanding of code remains an important challenge today."
        },
        {
            "title": "3.7 Low-Resource Languages and Specialized Libraries",
            "content": "Models struggle with low-resource languages and codebases with specialized libraries. Because of the limited exposure they have in these contexts, models fail in variety of ways including generating syntactically incorrect code, misunderstanding the semantics of certain constructs, and using libraries improperly. Potential solutions: 4.2.2 As we adapt code LLMs to individual codebases, generating correct code in out of distribution (OOD) scenarios becomes crucial. Much of software development in business contexts revolves around proprietary codebases, which is distribution shift from the open-source code that dominates LLM training data (Ahmed et al., 2024a). These OOD scenarios include domain-speciﬁc languages (DSLs), custom internal libraries, low-resource APIs, and company-speciﬁc coding styles and conventions. Syntactic Failures: Models have been shown to hallucinate constructs from higher resource languages when working in low-resource languages. Blinn et al. (2024) remark that contemporary LLMs fail to follow Hazels syntax and semantics, often borrowing syntactic forms and library functions from [higher-resource languages like] OCaml and Elm. 22 Example. Syntax error in Triton: In Listing 3.7, we show an attempt from Gemma-3 27B to write dot-product kernel in low-resource GPU programming language called Triton (part of docstring omitted for brevity). Gemma uses indexing notation such as a[index], which is not valid Triton construct. Models like o1 and o3, however, do not make this mistake. @triton.jit def dot_product_indexed_kernel (a, b, indexes, out, N): \"\"\" Computes the dot product of two vectors and using an index vector. ... \"\"\" block_size = 64 grid_size = (N + block_size - 1) // block_size # Adjust block size for optimal performance block_id = tl.program_id(0) start = block_id * block_size end = min(start + block_size, N) accumulator = tl.zeros(1, dtype=tl.float32) for in range(start, end): index = indexes[i] accumulator += a[index] * b[index] tl.store(out, 0, accumulator) Poor Semantic Understanding: In low-resource languages, models have less exposure to the various language constructs. Therefore, they have weaker semantic understanding of the language. Many studies reveal that code LLMs perform poorly when asked to write code in low-resource languages. Due to the lack of training data in these OOD domains, models may struggle to write common primitives or piece together functionality coherently. On HumanEval, Qwen 2.5 Coder Instruct (32B) (Hui et al., 2024) has an accuracy of 83% in Python but only 27% in D.3 Library Usage Failures: In OOD scenarios, LLMs lack awareness of the libraries and functions In new codebases using custom libraries, many functions appear only few available for use. times, providing limited training data for AI models to learn their usage. This scarcity can lead to overﬁtting, where models fail to recognize an effective use-case of these functions. Models also frequently hallucinate non-existent functions based on patterns that it infers. 3As reported by the BigCode Models Leaderboard on the MultiPL-E benchmark (Cassano et al., 2023) 23 Example. Hallucinating theorems in Lean: In Listing 1, the LLM makes up theorem named infinitelyManyPrimes arithmeticSequence. While the model understands that it should use Dirichlets theorem, the actual theorem is named forall exists prime gt and eq mod. This theorem occurs very few times in all of the currently available Lean code on GitHub, so it is likely that LLMs did not train on it many times. theorem infinitely_many_primes_of_form_6k_plus_1 : : N, : N, > Prime : N, = 6 * + 1 := by intro -- Use Dirichlets theorem for arithmetic progressions have := Nat.infinitelyManyPrimes_arithmeticSequence 6 1 Listing 1: LLM-generated Lean code snippet that incorrectly uses indexing notation"
        },
        {
            "title": "3.8 Library and API Version Updates",
            "content": "Software engineering has unique property that repositories and libraries are constantly changing. Code LLMs have trouble adapting to these rapid changes, often struggling to use the correct version of libraries and ignoring new paradigms and features. Potential solutions: 4.2.2 Continual learning, the idea of training an AI system to take in new information continually, has been long-standing challenge in AI and NLP (Wu et al., 2024; Wang et al., 2024d). In software engineering, codebases are continuously changing as new features are supported and awkward design patterns are reworked. While backwards compatibility is often prioritized in software design, it inevitably becomes broken as codebases evolve further. Therefore, programming libraries have version releases, each release supporting and deprecating features in the last version. There have been few works exposing this issue. For example, CodeUpdateArena (Liu et al., 2024g) and GitChameleon (Islah et al., 2024) are two benchmarks exploring the ability of LLMs to write version-speciﬁc code, examining this issue at the function and ﬁle level. They ﬁnd that language models struggle to adapt to these changes even with this limited scope. In theorem proving (Lean), Kumarappan et al. (2024) try to mitigate this by developing lifelong learning framework that continuously learns and uses new theorems. In real-world engineering, the challenge of library and API versioning generally spans across an entire repository, as everything must be kept consistent. To our knowledge, there are no techniques that successfully deal with this challenge at such large scale. This problem is difﬁcult for few reasons, which we discuss below. Version Identiﬁcation: In order to to successfully deal with version changes, LLM must ﬁrst identify which version of each library is being used in codebase. This may often be quite difﬁcult, because versioning information can be hidden deeply within codebase. Sometimes, it can be found in comments or conﬁguration ﬁles, but in the worst case, it must be inferred from the library calls being used. To make things worse, some code may be compatible across multiple versions, while other code will cause errors only in speciﬁc versions. Therefore, the model will often require deep understanding of both the codebase and the nuances between different versions in order to infer the version at hand. Example: Debugging Frontend Code: Frontend framework usually has more frequent versions update, making it hard for code LLMs to work with. For example, when helping user debug the NextRouter was not mounted issue, Claude 3.7 tries various solutions without recognizing that the core problem requires importing useRouter from next/navigation instead of next/router, crucial distinction since the users codebase leverages App Router in Next.js 13. Version Adaptation: Many fast-changing libraries are not backward compatible as older features become deprecated. It can be difﬁcult for LLMs to implicitly keep track of which constructs and patterns are associated with each version. Therefore, consistently using constructs from the right version can be difﬁcult. As we will see in the examples below, LLMs often write code that mixes and matches API constructs from different versions of the same library. Example. Typing Hints: While Python 3.5 required importing types from the typing module, Python 3.9s PEP 585 enabled direct use of built-in types for generics (e.g., list[int] vs typing.List[int]). However, language models tend to default to the older typing module syntax. Continuous Adaptation to Paradigms, Features, and APIs: New styles, patterns, and paradigms are often introduced to replace older, more cumbersome ways to write code. For example, React came out with its Hooks paradigm in version 16.8 (2019). Over the next few years, developers transitioned from the old class components paradigm to using hooks, as hooks made code cleaner and more maintainable. Only in early 2023, with the launch of react.dev, were Hooks the default paradigm in the documentation. For language models, incorporating these features can take long time, because code in these new paradigms are initially completely absent in the training In Kharma et al. (2025), the authors ﬁnd that data and inherently in the low-resource regime. LLMs fail to utilize security features in compiler and toolkit updates (such as in Java 17), still relying on legacy methods such as insecure random number generation. While it is possible to use retrieved examples and documentation in order to coerce language models to write code using new and updated features, we should strive to create AI coding assistants that can quickly internalize new changes and be able to naturally incorporate new features and paradigms, even without an abundance of training data. For each task, the language model should be able to reason about the best way to write the code, independently of the number of occurrences seen in the training data. 25 Example. Lean 3 vs. Lean 4: Lean (De Moura et al., 2015) is programming language that allows users to write formal proofs of mathematical theorems. In 2017, using Lean 3, enthusiasts implemented library for mathematics called mathlib, with over half million lines of code. Because Lean 3 had many shortcomings, Lean 4 (Moura and Ullrich, 2021) was initiated at the beginning of 2021 to address many of these issues. There was massive undertaking to port all of the mathlib code over to Lean 4, and only in September 2023 was there stable release of Lean 4, the version of Lean that is predominantly used today. The two versions are generally incompatible. We hypothesize that, due to the recency of Lean 4, most language models have been trained on much more Lean 3 code compared to Lean 4 code. When asking models to generate code in Lean 4, it sometimes generates code with Lean 3 coding conventions. Other times, it uses theorems and lemmas from Lean 3 that are deprecated in Lean 4. In Listing 3.8, we show an example of prompting o3-mini with Lean 4 problem, where it generates Lean 3 syntax (e.g. begin). Prompt: Write and prove the following in Lean 4: the sum of the first positive integers is *(n+1)/2. Begin with \"import Mathlib\". Surround your Lean 4 code with lean4 and /-- The sum of the first positive integers (i.e. 0 + 1 + ... + n) is * (n + 1) / 2. -/ theorem sum_first_n (n : N) : (range (n + 1)).sum id = * (n + 1) / 2 := begin induction with ih, { -- Base case: when = 0, we have range (0 + 1) = {0} and so the sum is 0, which matches 0 * (0 + 1) / 2. simp [range], }, { -- Inductive step: assume the formula holds for n, and prove it for + 1. -- First, note that range (n + 2) = insert (n + 1) (range (n + 1)). rw [range_succ], -- Since (n + 1) / range (n + 1), we can express the sum as: rw [sum_insert (not_mem_range_self (n + 1))], -- Substitute the induction hypothesis: rw [ih], -- We now have to verify: -- * (n + 1) / 2 + (n + 1) = (n + 1) * (n + 2) / 2. -- This equality follows by algebra (ring calculation). ring, } end"
        },
        {
            "title": "3.9 High Logical Complexity and OOD Domains",
            "content": "Tasks such as writing highly concurrent code or discovering performance optimizations have high logical complexity, often proving difﬁcult for even the best human coders. Similar to solving research-level math problems, these out-of-distribution domains are very hard for LLMs. Potential solutions: 4.2.1 Some programming tasks are challenging for even the best human programmers, requiring approaches with very high logical complexity. Examples of tasks that fall into this category include superoptimizing programs, discovering attacks for purportedly secure code, writing performant 26 compilers, optimizing GPU kernels (Ouyang et al., 2024), and writing very error-prone and very technical code. Example. Synthesis of Sorting Kernels: An example of an out-of-distribution domain is synthesizing fast assembly code for sorting kernels. In 2023, AlphaDev (Mankowitz et al., 2023b) used reinforcement learning to ﬁnd SoTA kernel for sorting length 3-5 arrays. While this appeared to be superhuman performance, shortly after, Neri (2023) hand-wrote kernel shorter and faster than the one found by AlphaDev. Later, (Ullrich and Hack, 2025) developed an algorithm based on enumeration and intelligent heuristic-based sampling that beat both of these. In addition, the algorithm ran faster than AlphaDev by two orders of magnitude. In this case, while AI was able to achieve an impressive performance, humans were able to discover better algorithms. Example: Verifying File System Properties: In formal veriﬁcation, when working with new domains, it is necessary to devise new theories to faithfully represent desired properties. For example, FSCQ is formally certiﬁed crash-proof ﬁle system with the provable guarantee that under any sequence of crashes followed by reboots, FSCQ will recover the ﬁle system correctly without losing data (Chen et al., 2015). In this domain, one challenge is that proving safety cannot be done at the source code levelbecause instructions are not atomic, data may be lost if the crash occurs within non-atomic instruction. Instead, new logic known as the Crash Hoare logic (CHL) needed to be developed, and constructs representing crash condition and recovery procedure needed to be described. Constructing logic like this would be very difﬁcult for AI systems. Limits of Symbolic Techniques: When it comes to applying symbolic techniques to these tasks, there are few limiting factors that make them difﬁcult to tackle. First, for synthesis-style tasks, the search space can be very large. Deductive and rewrite-based synthesis techniques are unable to explore majority of the search space. Second, veriﬁers can be limited in power, such as when dealing with properties in concurrency or weak memory models. Third, many domains lack clean models to reason about properties, such as dealing with memory bandwidth in GPU kernels. Because they are hard for humans, these tasks are very rarely in the training data of todays language models. They have unique, domain-speciﬁc, challenges that making generalizing from existing data difﬁcult. For these problems, language models rely heavily on feedback-driven search algorithms (Mankowitz et al., 2023b), and it can be difﬁcult to navigate the search space effectively. In addition, many of these tasks lack feedback mechanisms, which is crucial for AI to pick up learning signals. When designing complex algorithm or data structure, it is often hard to know if you are on the right track until you get to the correct result. When writing code for large multithreaded operation, it may be hard to know if the algorithm has concurrency issues until all the parts are fully ﬂeshed out. Without feedback, incremental improvement is nearly impossible."
        },
        {
            "title": "4.1 Data Collection",
            "content": "One bottleneck in the development of AI for SWE in the open-source community is the lack of access to ﬁne-grained and high-quality code data. In Sec. 4.1.1, we discuss how automated techniques can mitigate this by augmenting existing programs with symbolic information and generating synthetic data with symbolic veriﬁers. However, there are other crucial signals in programming that can be hard to automate. We envision that large community-based coding data curation effort will be very impactful. In Sec. 4.1.2, we discuss examples of datasets that such community could create that would unlock new capabilities in code LLMs. Challenges addressed: 3.1, 3.3, 3.6 4.1.1 Automatic Data Curation Augmenting Data with Program Information: One challenge in enabling LLMs to develop world model of code is that programs are often treated like text: as tokens with no semantic information. However, modern programming tools allow us to extract rich semantic and structural information about code. By leveraging these tools, we can augment training datasets with detailed annotations describing various properties of programs. We hypothesize that this augmentation will signiﬁcantly improve models understanding of code, leading to better generalization and stronger coding capabilities. Information can include: Static analysis: the syntactic structure of program (abstract syntax trees, control ﬂow graphs), information about the type of each variable, data ﬂow analysis (reachability, liveness analysis) Program instrumentation: memory consumption, runtime analysis, aliasing, and code coverage (like statement or branch coverage) Dynamic analysis: program states at various points in the program, call stacks, dynamically curated properties (often relies on instrumentation) Formal veriﬁcation: concurrency analysis, program invariants, loop invariants, memory safety There have been few examples of this in the literature: Ouyang et al. (2024) leverage proﬁler feedback to improve GPU kernel generation, Ding et al. (2024c,b); Ni et al. (2024) incorporate execution trace information, Pei et al. (2023) train with program invariants, GraphCodeBERT (Guo et al., 2020) incorporate data ﬂow information, and Shypula et al. (2023) train on dataset of performance-improving edits. High-quality, Veriﬁable Synthetic Data: The advantage of code is it is possible to achieve strong, veriﬁable feedback with test cases, program execution engines, and other symbolic tools. This makes high-quality synthetic data generation viable, as it is possible to generate large batch of 28 data and ﬁlter out low-quality samples. For example, to generate code with interesting program invariants, we can sample large batch of programs, run an invariant detection engine, and retain only programs with interesting invariants. While synthetic data in code has mostly been at the function-level scope, there are no fundamental bottlenecks to expanding to larger scopes. As code is quite compositional, individual building blocks can be combined to generate complex synthetic data at the repository-level scope, which can be very helpful in both training and evaluation. While the importance of having high-quality data vs. high quantities of data is debated, using veriﬁed data has proven to be useful. For example, Liu and Zhang (2025) shows that simply removing bugs in existing datasets such as TACO (Li et al., 2023) can lead to signiﬁcant boosts. KodCode (Xu et al., 2024a) also showed that ﬁne-tuning on veriﬁed synthetic data also leads to signiﬁcant improvements. However, these works work with programs at the function-level scope with low to medium logical complexity, and we imagine that general SWE abilities can improve with synthetic data across scopes and logical complexities. In DSLs, where programs can be cleanly described with semantics and rewrite rules, one can symbolically generate programs with desired properties via sampling, drawing on enumeration techniques from program synthesis (Gulwani et al., 2017). This technique has been successfully applied to make considerable progress in difﬁcult reasoning tasks such as ARC-AGI (Li et al., 2024d) and math olympiad problems (Trinh et al., 2024; Google, 2024; Chervonyi et al., 2025). 4.1.2 Human-Centric Data Curation Below, we list three classes of human-annotated data that would be invaluable for the next generation of coding LLMs. Fine-Grained Data of the Developmental Process: Many code LMs are trained on datasets such as the Stack (Kocetkov et al., 2022; Lozhkov et al., 2024), consisting of trillions of tokens sourced from GitHub. However, training on raw GitHub tokens omits many crucial human signals in the process of software development. For example, companies such as Google rely on internally captured logs of high-quality SWE data. This includes ﬁne-grained code edits, build outcomes, edits to resolve build issues, code copy-paste actions, ﬁxes of pasted code, code reviews, edits to ﬁx reviewer issues, and change submissions to repository (Chandra, 2024). Similarly, Meta and GitHub Copilot use telemetry with their AI coding assistants to track and leverage signals from AI-generated code (Murali et al., 2024; Ziegler et al., 2024). These tools, along with coding IDEs like Cursor, could provide treasure trove of reward data for RL-based methods. With direct access to the full history and evolution of codebase, they can track which suggestions are adopted over time. However, collecting data from human usage also raises critical privacy and intellectual property concerns. Data for Diverse SWE Tasks: Most of todays code LLM training recipes still focus primarily on code generation because large-scale datasets are mostly in continuous, tokenized format. However, as described in Sec. 2), there are many tasks involved in software engineering which models lack exposure to. Training on broader set of tasks would also incentivize models to learn general capabilities of programs beyond just generation (e.g. better understanding of program semantics). As initial evidence, Li et al. (2025a) ﬁnd that training models on input-output prediction data leads to consistent improvements on reasoning tasks. The lack of high-quality data on these tasks makes it hard to train on them. It can also be hard to automatically curate them on GitHub. For example, for code refactoring (Sec. 2.2.1), we need paired repositories before and after refactoring, ideally with the refactoring changes described. While some signal such as commit messages and version releases can be used, many repositories lack clean commit histories and releases conﬂate many features at once. Therefore, to mitigate this, we envision large community-based efforts curating task-speciﬁc data on these diverse challenges. Human-Centric Data: Code LLMs are typically trained and evaluated on carefully curated datasets with clear instructions and veriﬁable test cases. However, as discussed in Sec. 3.3, these models are often deployed in real-world scenarios where users provide vague speciﬁcations or incomplete requirements in their queries. Collecting human-centric data that reﬂects real-world model usage is promising approach to bridging the gap between model development and deployment. Recent efforts, such as Copilot Arena (Chi et al., 2025) and WebDev Arena, have explored gamiﬁed arenas to gather data on human preferences, offering an alternative to purposefully curated datasets. However, such data collection methods may introduce noise, and arena-style approaches are not well-suited for long-horizon, interactive tasks. One potential approach is to leverage existing coding tools and environments, such as developing plugins for GitHub Copilot (Bajpai et al., 2024) or open-source IDEs, to capture real-world interactions. Unlike static datasets, human-centric data can also be collected encompassing diverse interaction modalities, such as users providing sketches to AI coding systems for web development (Li et al., 2024c). As AI coding systems continue to emerge and evolve, launching data initiatives focused on human-centric SWE data is also crucial direction for advancing human-AI collaboration in software development."
        },
        {
            "title": "4.2 Training",
            "content": "4.2.1 Environment Design for Code RL Reinforcement Learning from Veriﬁable Rewards (RLVR) (Lambert et al., 2025) has emerged as powerful paradigm in math and coding domains where model outputs can be evaluated against ground truth outcome such as exact match and passing set of unit-tests. Towards this direction, promising avenues include collecting executable codebase environments, sourcing task prompts/rewards from GitHub, and designing non-execution based rewards based on program syntax and semantics. Challenges addressed: 3.4, 3.9 Collecting executable codebases: In recent months, RLVR has seen success in solving algorithmic programming problems through DeepSeek-R1 (DeepSeek-AI et al., 2025) and OpenAI o1. Recently, on SWE-Bench, SWE-RL (Wei et al., 2025) use RL on rule-based reward to improve performance on SWE-Bench. We ﬁnd it promising to continue scaling the RL approach to problems collected from real-world software engineering repositories. Towards this, we believe that collecting execution-assisted gym-like reinforcement-learning environments will lead to further performance improvements. These environments can be used further to improve reasoning skills, environment-interaction capabilities and tool usage. Several prior works (Jain et al., 2024c; Pan et al., 2024; Guo et al., 2025; Xie et al., 2025) curate executable environments for programming agents by supporting CI/heuristic-based repository installations. However, these works are at relatively small scale and limited in scope, offering only few thousand tasks from maximum of thousand repositories and more importantly, limited to the Python language. Scaling this up signiﬁcantly requires solving several research and engineering problems. First, installing arbitrary repositories from Github, even using CI is challenging and we require smarter solutions potentially involving LLM-based installation agents. Next, setting up execution infrastructure would require storing installed repository images in something akin to docker for efﬁcient storage and fast container startup times (Team et al., 2025). Notably, combined docker images can grow massively large and often grow at hundreds of gigabytes even at modest scale of few hundred repositories. They require engineering support for efﬁcient storage and serving of such images. Sourcing task prompts and rewards: Beyond environments, performing large-scale reinforcement learning would require collecting diverse challenging problems with an appropriate way to compute the rewards. These task prompts can be collected from Github (Pan et al., 2024) or generated synthetically from problems on Github. Moreover, assuming access to many executable repositories, we can source various end-to-end problems for tasks beyond bug-ﬁxing such as optimization, fuzzing, etc. Access to pre-existing or generated test cases allows for measuring correctness and providing rewards. However, we envision many practical challenges to remain. For example, longer-horizon tasks are usually more ambiguous and approaches may require multi-turn interactions beyond autonomous coding agents. This would pose considerable challenge during reinforcement learning where ambiguity resolution might need to be modeled in the reinforcement learning process itself. We elaborate on human collaboration further in Section 4.2.3. Reward hacking Skalse et al. (2022) poses another challenge as we build more real-world coding challenges. Test cases often suffer from coverage issues and can grade correct solutions as incorrect. For example, Baker et al. (2025); Denison et al. (2024) identiﬁed that models attempt to bypass or cheat against the testing harness when optimized using reinforcement learning. Rewards without execution: As setting up execution environments can lead to considerable overhead, another potential strategy is to use proxy metrics and trained language models to judge correctness. This was common in the pre-LLM era, researchers often used BLEU/CodeBLEU (Papineni et al., 2002; Ren et al., 2020) and BERTScore/CodeBERTScore (Zhang et al., 2019; Zhou et al., 2023) to assess correctness of text and code. In code, semantic and structural properties can be used to improve similarity metrics. Two examples of this are Dolos (Maertens et al., 2022), an AST-aware plagiarism detector, and difflib.SequenceMatcher, which can be used to compute the similarity between two patches (Wei et al., 2025; Ma et al., 2025b). Beyond rule-based rewards, LLMs-as-a-judge approaches can also be used as reward functions, possibly in conjunction with other execution-based or execution-free approaches. 4.2.2 Adapting to Specialized and Quickly Changing Codebases 3.7), custom APIs, 3.8), Low-resource languages (Sec. large codebases (Sec. 3.5), and custom coding styles all surface the fact that code LMs struggle to adapt to unseen specialized contexts. Customization can be achieved through test-time training, keeping specialized information in an information bank. cheaper and alternative approach to test-time training is to apply prompt and preﬁx tuning, where codebase-speciﬁc embeddings are learned and applied depending on the context. library version updates (Sec. Challenges addressed: 3.7, 3.8 31 Test-time training (TTT) to custom codebases: TTT is the recent paradigm of adapting to speciﬁc problem instance by training on narrow set of in-distribution examples (Aky urek et al., 2024; Sun et al., 2020). This can be used when working in low-resource context, for example training on speciﬁc codebase, new domain, or unseen API. One challenge in this setting is customizing the model to the particular codebase while retaining general coding knowledge, potentially by using algorithms that can induce controllable forgetting (Wu et al., 2024). To get data in specialized contexts, we envision two mitigation strategies: generating synthetic data and collecting trajectories. In-distribution synthetic data can be generated in large quantities and then ﬁltered and annotated with symbolic (e.g. compiler) information to gain more global understanding of the current environment and setting. To gather agentic trajectories, we can keep track of previous model attempts and failures to learn from past successes and avoid making repeated mistakes. This will steer the model closer to the desired distributionfor example, to generate code in the speciﬁed version of libraries being used in the current context. Keeping an information bank of code information: For library and versioning issues, retrieval (Sec. 4.3.1) can be very effective for preventing hallucinations of wrong versions of libraries, which can inherently lead to better synthetic data and agentic trajectories. During the TTT process, we can also keep large growing memory bank of code, documentation, synthetic code, and agentic trajectories in the specialized context. Retrieving from the memory bank would improve the success of generating code, which can then be augmented to the memory bank, and so on, continuously increasing the amount of data and knowledge available. Prompt and preﬁx tuning for specialized code contexts: One issue that makes it difﬁcult to continuously keep up with library updates is that doing full ﬁnetuning every time something changes is very expensive. Because only small amount of knowledge needs to be learned compared to that of the pre-trained model, we believe less expensive approaches such as prompt tuning (Lester et al., 2021) and preﬁx-tuning (Li and Liang, 2021) could sufﬁce. Both these methods append set of learned task-speciﬁc vectors to the input sequence in order to model speciﬁed context, though prompt tuning only modiﬁes the input and preﬁx-tuning modiﬁes the input at each layer. These methods have also been shown to have good OOD performance, and we believe they present promising approach to dealing with multiple library versions. separate prompt/preﬁx can be trained for each version and then applied according to the context. When an API has new updates, the prompt/preﬁx can then be cheaply re-tuned to reﬂect the new updates without undergoing full ﬁne-tuning. This approach also applies to adhering to speciﬁc coding styles, where codebase-speciﬁc prompts/preﬁxes can also be learned. Learning on the ﬂy: When humans are faced with task they have never seen before, they are often able to draw from past experiences and quickly adapt and generalize to the new domain. This is one of the big unsolved challenges of todays LLMs: given an OOD coding task, how can models get up to speed and productively work on the task with few samples? On toy domains, an example of this is DreamCoder (Ellis et al., 2021), system that learns to solve problems by writing programs and automatically discovering domain concepts and composing concepts together. Designing such approaches for more practical applications is an exciting research direction that will have drastic implications for coding and reasoning. 32 4.2.3 Training Code LLMs to Collaborate with Humans Training the next generation of code LLMs needs to account for human-AI collaboration, as these models will likely be deployed in ambiguous and interactive scenarios. We highlight two key directions for improving collaboration: First, learning to leverage speciﬁcations beyond natural language through formal methods and user-speciﬁed tests can mitigate vague speciﬁcations. Second, improving uncertainty quantiﬁcation and proactive communication through post-training has the potential to prevent hallucination and misalignment. Challenges addressed: 3. Learning to Leverage Speciﬁcations Beyond Natural Language: As discussed in Section 3.3, while natural language prompts offer intuitive and ﬂexible ways to express requirements, they often suffer from ambiguity and incompleteness. One direction to address this limitation is to train models to leverage enhanced speciﬁcations with more precise and veriﬁable representations, such as formal speciﬁcations and test-based speciﬁcations. Formal speciﬁcations: To mitigate underspeciﬁcation issues, one solution is to develop systems that can translate user intent into formal speciﬁcations (Szegedy, 2020; Endres et al., 2024). While current autoformalization approaches face challenges in accurately capturing user intent (see example below), we envision next-generation systems that will iteratively reﬁne formal speciﬁcations through interactive veriﬁcation with human feedback. These systems would present intermediate formalizations in accessible notation, enabling non-expert users to verify correctness before code generation. Example: Incomplete speciﬁcation in Verus: Here, we show failure mode of LLMs when writing speciﬁcations and proofs in Verus. The LLM is asked to write the ensures postcondition clause for ring buffer enqueue functiona. Here, the postcondition is incomplete: it does not check, for example, that the original elements were maintained in the ring buffer. fn enqueue(&mut self, val: T) -> (ret: bool) ensures ret == !old(self).is_full(), self.inv(), if ret { self.view() === old(self).view().push(val) } else { self.view() === old(self). { } view() } if self.is_full() { false } else { self.ring.set(self.tail, val); self.tail = (self.tail + 1) % self.ring.len(); true } aFull example here Tests as speciﬁcations: Another approach to specify software behavior is through tests. These range from input-output examples and assertions to property-based tests. However, in practice, hand33 crafted test suites are often incomplete, failing to capture the full intended behavior, particularly edge cases. This can lead to misalignment, where AI-generated code passes tests but does not genuinely meet functional requirements, potentially misleading users. Moving forward, direction is training models to generate high-quality test cases based on the users initial query, ensuring more comprehensive speciﬁcation coverage. Example: For instance, in release of AI CUDA Engineer by Sakana AI, an AI-generated CUDA kernel for lower triangular matrix multiplicationpurportedly achieving signiﬁcant speedupswas later found to exploit out-of-bounds memory access to bypass correctness checksa. Advancing research on frameworks that facilitate test generation and automated adversarial testing represents an important direction. aThe full LLM-generated kernel code can be found in Listing 3, pg. 46-47 of Lange et al. (2025) Learning to Quantify Uncertainty and Communicate Proactively: As AI coding systems are increasingly deployed to complex software engineering tasks, they encounter more ambiguous and uncertain scenarios compared to traditional benchmarks for coding models. Ideally, in such situations, these systems should proactively communicate with users to clarify tasks and acknowledge its own limitations rather than becoming stuck in endless failure loops or generating buggy code. key challenge is enabling models to distinguish between well-speciﬁed and ambiguous instructions while quantifying uncertainty in robust manner. While early studies, such as Vijayvargiya et al. (2025) and the example below, demonstrate that interactive LLMs can improve performance through clariﬁcation-seeking behavior, current models still struggle with uncertainty estimation. Equipping models with the ability to quantify uncertainty will likely require incorporating corresponding reasoning data into the post-training stage. Besides uncertainty quantiﬁcation, Shao et al. (2024b) identify communication as primary challenge in human-agent collaboration, highlighting the need for improving models proactive communication capability. Current models often fail to ask meaningful questions when user input is ambiguous or insufﬁcient, and they struggle to provide progress updates or verify plans in interactive settings. Enhancing models proactive communication abilities requires innovative approaches to reward behaviors that yield beneﬁts over multiple steps. Since communication with users does not immediately resolve the task at hand but may improve long-term outcomes, effective strategies must account for delayed rewards in training. Example: Discussion Helps Coding Agents Resolve Github Issues: In SWE-bench (Jimenez et al., 2024) pydata xarray-4750, the original issue description requests limiting the number of data rows displayed in repr. While it suggests maximum of 25 rows, it does not specify whether this number should be conﬁgurablea key requirement that emerged during the issue discussion. When SWE-Agent (Yang et al., 2024b), powered by GPT-4o, uses only the issue description as the problem statement, it generates function that hardcodes the maximum at 25, causing the solution to fail the test. However, incorporating the issue discussion allows the agent to produce correct, test-passing implementation (see Listing 2). This suggests that enabling coding agents to engage in discussions with users could potentially improve the issue solving rate. Listing 2: SWE-Agent improves when incorporating issue discussions # Use only the issue description def _mapping_repr(mapping, title, summarizer, col_width=None): if col_width is None: col_width = _calculate_col_width(mapping) summary = [f\"{title}:\"] if not mapping: summary += [EMPTY_REPR] else: items = list(mapping.items()) if len(items) > 25: # Show first 12 and last 12 items first = items[:12] last = items[-12:] summary += [summarizer(k, v, col_width) for k, in first] summary += [\" summary += [summarizer(k, v, col_width) for k, in last] ...\"] else: summary += [summarizer(k, v, col_width) for k, in items] return \"n\".join(summary) # Use the issue description and discussion def _mapping_repr(mapping, title, summarizer, col_width=None): col_width = _calculate_col_width(mapping) summary = [f\"{title}:\"] if mapping: items = list(mapping.items()) if title == \"Data variables\" and len(items) > OPTIONS[\"display_max_rows\"]: # Show first and last variables if there are too many first_n = OPTIONS[\"display_max_rows\"] // 2 last_n = OPTIONS[\"display_max_rows\"] - first_n selected_items = items[:first_n] + [(\"...\", \"...\")] + items[-last_n:] else: selected_items = items summary += [summarizer(k, v, col_width) if != \"...\" else \" ...\" for k, in selected_items] else: summary += [EMPTY_REPR] return \"n\".join(summary)"
        },
        {
            "title": "4.3 Inference Time Approaches",
            "content": "4.3.1 Semantic-Aware Embeddings and Retrieval In contrast to text, embeddings for code should incorporate execution and semantic information, improving retrieval. RAG beneﬁts from both context-aware retrievals and explicit training on how to use them, enhancing code reuse across languages and APIs. Beyond static retrieval, AI agents could also dynamically navigate codebases using command-line tools and IDE functions. Challenges addressed: 3.5 Semantic and execution aware code embeddings: When training LLMs, code is often treated as pure tokens (just like text) rather than explicitly incorporating code-speciﬁc information such as 35 program execution and semantics. As result, code that is close in embedding space is more often syntactically similar than semantically similar (Utpala et al., 2023; Zhao et al., 2023), and there are few reliable methods today to retrieve semantically similar code. However, before the LLM era, there were variety of efforts to incorporate code properties when training embeddings. For example, Nye et al. (2020) train neural modules to represent program operations, leading to compositional program representations that encode the semantics of the underlying programming language. Many other works (Zohar and Wolf, 2018; Ellis et al., 2019; Chen et al., 2021b) attempt to learn execution-aware latent representations for partial and full programs, taking semantics into account. We speculate that incorporating these techniques to train models to have better and more semantically aware representations may lead to models with more general understanding of code (Sec. 3.6). For example, if correct and buggy programs could hypothetically be separated in embedding space, then models could be steered away from the incorrect program space. While such clean separation might not be possible, we believe that training embeddings to have interesting semantic properties is worth exploring. Better retrieval-augmented code generation: When retrieval-augmented language models were ﬁrst introduced, they often relied on training the retriever and language model jointly, as in FiD (Izacard and Grave, 2020), RETRO (Borgeaud et al., 2022), and Atlas (Izacard et al., 2023). As language models increased in size, the ﬁeld shifted to black-box setting (Shi et al., 2023), where the retrieval module is tuned independently to adapt to the pretrained black-box LLM. This setting is much more cost-effective, but the language model is not explicitly trained on how to use its retrievals. The black-box setting is ideal for challenges such as low-resource languages or specialized contexts. In these situations, the model has not seen enough training data to fully grasp the context, and the challenge is often syntactic rather than algorithmic. For example, when adapting to domain or codebase where the relevant API functionality or code style, retrievals can be very instructive. When using APIs with multiple versions, providing retrievals in the correct version can inform the model of how to use the API. When writing code in completely new language, showing examples of for loops and while statements will teach the model the syntax of these constructs. Retrievals should be diverse and given in multiple forms, including documentation, function deﬁnitions of APIs that are used, and example use cases of target functions. In many other cases, however, we believe that black-box setting is insufﬁcient. As described in Sec. 3.5, there are two challenges: 1) knowing what to retrieve and 2) using the retrieval. The ﬁrst challenge relies on retrieving relevant examples, both syntactically and semantically. We believe that having more semantically aware embeddings, as mentioned above, will drastically improve this. For example, embeddings can be trained contrastively to minimize the distance between semantically similar programs. Another potential direction is to consider diverse set of potential retrievals and then train the retriever to prefer samples that help during generation, as in Atlas (Izacard et al., 2023). The second challenge, using the retrieval, is code reuse task, which requires complex reasoning and code understanding. Algorithms provided in retrievals may often need to be modiﬁed and adapted signiﬁcantly to adapt to the current setting. An example of this might be writing C++ version of shortest path algorithm when the retrieval is Java version, translation task that models may not have been trained for explicitly. Long chunks of retrieved documentation may need to be understood precisely so that correct hyperparameters and ﬂags can be used. Yet, in black-box setting, models have not been explicitly trained to leverage this information. Therefore, just as training on incorrect-correct code pairs can improve program repair, we believe that direct training can be very beneﬁcial for code reuse and retrieval-augmented generation. Execution information could also be useful, as code reuse often requires understanding the situation well enough to identify subtle differences between the context of the retrieved code and the current context. Retrieving via code navigation on the ﬂy: Standard retrieval-augmented methods keep large retrieval index containing millions of embeddings, which can require high one-time cost to create. As the codebase evolves, these embeddings may also need to be continuously updated. Instead of keeping track of embeddings, another approach is to ﬁnd retrievals on the ﬂy by navigating the codebase. We can imagine an agent that learns to use command line functions such as cd, ls, and grep, as well as IDE functions such as jumping to function deﬁnitions or ﬁnding all references of function. Static analysis tools can also be paired with the agent to improve code navigation, such as providing the abstract syntax tree (AST) or ﬁle structure of codebase. 4.3.2 Integration with SWE Development Frameworks Integrating AI with SWE development frameworks is critical for practical applications and impact on developer workﬂows. While software development is inherently integrated with tools, workﬂows, scaffolding, and meta-code, these are often absent from source code and scarce in AI training data. Ensuring that AI deeply understands software deployment beyond code editing is crucial, as writing code is only small part of the development cycle. These can include automated reviews, deployment risk assessments, and documentation generation. We can also ﬁne-tune LLMs to recognize and avoid known software anti-patterns such as CWEs. Challenges addressed: 3.4, 3.5 Incorporating AI into the CI/CD process: In continuous integration and continuous deployment (CI/CD), automated pipelines are the backbone for building, testing, and deploying code changes. CI/CD accelerates feedback cycles and minimizes integration issues. AI offers several integration points within CI/CD. AI-powered code review tools can be incorporated into CI pipelines to automatically identify and ﬂag style violations, potential security vulnerabilities, and code smells before human reviewers are involved. Furthermore, AI can provide intelligent deployment risk assessments. By analyzing code changes, test outcomes, and historical deployment data, AI can predict the likelihood of deployment issues, informing decisions about whether to proceed with automated deployment or mandate manual veriﬁcation steps. Finally, AI can automate the generation of release notes by summarizing commit messages, issue tracker data, and relevant code modiﬁcations within the CI/CD process. Steering away from software anti-patterns: In software engineering, certain anti-patterns frequently lead to bugs. For example, common weakness enumeration (CWE) is categorization of software and hardware weaknesses often leading to vulnerabilities. Because publicly available GitHub code often contains code with anti-patterns, bugs, and CWE vulnerabilities, LLMs often write code susceptible to these issues (Asare et al., 2023; Fu et al., 2023). We hypothesize that explicitly steering models against these vulnerabilities will lead to more secure and correct code. One way to do this is to collect large number of program samples violating each CWE 37 (either synthetically or on GitHub) and then use these samples as negative signal during further supervised ﬁne-tuning or RL stages. 4.3.3 Incorporating SWE Tools Software engineers integrate variety of domain-speciﬁc tools when writing code. By repeatedly interacting with tools in an RL-style manner, AI can develop the ability to do the same. Beyond tool use, using neurosymbolic approaches such as incorporating program analysis and type-checking can also help enhance LLM capabilities. Challenges addressed: 3. Learning to use SWE Tools: As mentioned in Sec. 3.2, we believe SWE agents should understand the intricacies of programming tools and be able to autonomously invoke them as needed. There are three skills to learn: which tool to use, how to use the tool, and how to incorporate the results of the tool. Similar to how models learn to play complicated games, we believe that intelligent tool integration can be learned through repeated interactions with the tool in RL-style manner. One way we envision this is as follows: ﬁrst, the interface of the tool must be precisely speciﬁed. Next, data containing repeated interactions from the tool (with varying degrees of success) should be collected. Finally, multiple rounds of RL and expert iteration can be done to improve understanding of the tool and learn from misuses. Evidence that learning higher-level strategies might be possible is that through test-time techniques, OpenAIs o3 model learned to write brute-force solutions to verify the correctness of more complicated solutions (El-Kishky et al., 2025). We envision that after learning to use tools, AI coding agents can autonomously invoke tools as needed to improve its overall world model of the code and hence its software engineering capabilities. Neurosymbolic Approaches: Code is unique domain because there is vast body of techniques from programming languages (PL) research to build off of, but the majority of AI for code research today does not leverage the symbolic properties of code. Some of these PL techniques are as follows: abstract interpretation (Cousot and Cousot, 1977) is technique to compute overapproximations of program state in order to prove the soundness of program properties at points in the code. Concolic testing (Godefroid et al., 2005; Sen et al., 2005) ﬁnds bugs in software by combining concrete and symbolic execution. Model checking (Clarke, 1997) is way to prove properties of execution traces via temporal logic. Linting and type-checking (Cardelli, 1996) provide static check to ensure that variables, expressions, and functions adhere to programming languages rules. Finally, many other program analysis algorithms leveraging these tools have been designed to prevent bugs and ensure code correctness properties. Traditional PL approaches have few common shortcomings, which overlap with some of the issues mentioned in Sec. 2.6. First, they often require very complete and precise speciﬁcations. Many tools need to have speciﬁcations for all library functions, need to specialize to precise version of the language, and need to specialize to the build system. Second, there is often high computational cost due to the large search space. Third, there can be many false positives due to the limitations of the tool. We believe that deeply integrating these symbolic tools with LLMs can partially mitigate these challenges. 38 We provide few examples of this potential integration. When generating code, program analysis techniques could be applied on shorter snippets of AI-generated code to surface potential bugs or prove properties of the generated code. To improve general code understanding, LLMs can be trained with information about program structure such as abstract syntax trees (Gong et al., 2024). When debugging large codebase, when the scale is too large to directly apply PL techniques, AI could be ﬁrst used to narrow down potentially problematic sections of the code which are then handed off to PL tools for debugging. During code generation in DSLs, LLMs can leverage the grammar of the programming language to do constrained decoding (Poesia et al., 2022; Geng et al., 2023; Wei et al., 2023b) to mitigate syntactic errors. During code refactoring, abstract interpretation and static analysis can be used to identify whether new errors have been introduced and preemptively cut off unpromising search paths. Deductive Synthesis and Intermediate Languages: Early program synthesis relied on deductive synthesis approaches (Burstall and Darlington, 1977), where programmers would write clean simple implementation and then apply transformation rules to convert it into more efﬁcient one. The appeal of deductive approaches is that because these rewrite rules are semantics preserving, there is correct-by-construction guarantee. One success story of deductive synthesis is Spiral (Puschel et al., 2005), DSL for signal processing kernels that takes advantage of domain-speciﬁc transformation rules to produce implementations beating expert hand-crafted code. Another example is Halide (Ragan-Kelley et al., 2013), DSL for high-performance image and array processing code. Due to the difﬁculty of writing optimized code, humans generally opt for writing code in these intermediate DSLs, and we ﬁnd it promising for LLMs to do the same. Example. LLM-aided Compilation for Tensor Accelerators: As an example, Hong et al. (2024) consider the task of generating highly optimized, hardware-efﬁcient code for tensor accelerator from high-level speciﬁcation of computation (e.g. C++ code). Their pipeline works in two steps: ﬁrst, the high-level speciﬁcation is translated to DSL. Then, the DSL code is symbolically compiled to hardware-speciﬁc instructions. The LLM is also used to optimize the DSL code via cost model driven search, where it suggests rewrites and scheduling operations (e.g. loop reordering) that guarantee semantic equivalence. 4.3.4 Scaffolding Human Supervision At inference time, most machine-generated code will be presented to humans in format shaped by the human-AI interface design. Since AI may be responsible for generating the majority of the code within human-AI team, it is important to ensure human control and oversight. By scaffolding human supervision with techniques like summarization and interactive veriﬁcation, we could potentially improve trust in AI-generated code. Challenges addressed: 3.3 Once code LLMs are deployed for inference, it is crucial to scaffold human supervision of AIgenerated code. This goes beyond merely enhancing the accuracy of AI-generated code, as humans often still need to make the ﬁnal decision on whether to accept the code or understand it for future integration and maintenance. study on Github Copilot usage (Al Madi, 2023) revealed that programmers tend to allocate less visual attention to AI-generated code. While one solution is to train humans to better identify issues in AI-generated code (Singhal and Kumar, 2023), 39 more desirable approach is to design AI systems that scaffold human supervision, reducing their cognitive load when reviewing generated code. One way to achieve this is by enriching AI-generated content with additional contextual information. Modern LLM chatbots now routinely generate text with citations for knowledge-intensive queries. In Collaborative STORM (Jiang et al., 2024), researchers demonstrated that dynamically presenting hierarchical mind maps alongside the actual collected information signiﬁcantly enhanced human-AI collaboration, particularly in long sessions. In software engineering speciﬁcally, Sun et al. (2024b) highlighted the beneﬁts of high-quality source code summarization in aiding software developers in understanding and maintaining machine-generated code. Second, interactive approaches can also enhance supervision. One example is Live Programming (Ferdowsi et al., 2024), continuous display of the runtime values of program, as means of lowering the cost of validating AI-generated code. However, these existing studies are largely limited to speciﬁc programming languages and small codebases. Finally, improving the readability and interpretability of AI-generated code itself presents promising direction. For example, Pu et al. (2020) showed that modeling program synthesis as rational communication improved end-user interpretation and subsequent communication of code. Expanding on these ideas, future research should prioritize human interpretability in the design and optimization of AI coding systems, fostering greater trust and control in AI-assisted software development."
        },
        {
            "title": "5 Limitations",
            "content": "We identify few limitations below: Speculative nature of future work: The ideas we list in the future work section are opinionated directions we believe have high chance of success. Many draw upon insights from related work in the literature, but many lack strong and concrete evidence. We encourage further research validating or disproving the effectiveness of these ideas. Limited scope of future work: We also do not include any novel moonshot ideas, and many of the directions we propose have their roots in existing code LLM literature. Our future work section is also relatively general and applies holistically to AI for code. However, the ﬁeld has many tasks and challenges that can beneﬁt from using domain-speciﬁc knowledge and insights, and we do not touch on these. Finally, this paper is written by people primarily in the academic community, who may not know the details of cutting-edge methods employed in frontier industry labs. We cater this paper towards areas we have more expertise in, and thus leave out many promising directions such as novel architectures. Focus towards code-speciﬁc challenges: In this paper, we mostly focus on code-speciﬁc challenges and techniques. However, there are many techniques that apply to general LLM reasoning and development that could be directly applied to code. We believe many of these methods can be used in synergy with code-speciﬁc techniques. Quickly changing nature of the ﬁeld: The ﬁeld of LLM for software engineering is progressing very rapidly, with new innovations released weekly. It is possible that reader reading this paper few months down the line will ﬁnd that several of the mentioned challenges will have been partially or entirely resolved."
        },
        {
            "title": "6 Conclusion",
            "content": "In this position paper, we have identiﬁed key tasks at the heart of AI for software engineering as well as set of three measures to classify different realizations of these tasks. We have also highlighted critical cross-cutting challenges that permeate throughout many tasks. Finally, to drive progress in AI for code, weve pinpointed set of exciting and promising research directions for alleviating these challenges and advancing AI towards being more capable software engineer. We hope this work provides valuable insights about the current landscape of AI for software engineering and encourages future research in these directions. By building on these insights, we are optimistic that the community can work toward developing AI-driven solutions that better support software engineers in real-world settings."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We thank Alex Polozov, Baptiste Roziere, Daya Guo, Jenny Liang, Jiawei Liu, Justin Chiu, Kexun Zhang, Leonardo Hernandez Cano, Li Zhong, Michael Wang, Silas Alberti, Theo Olausson, Valerie Chen, Xingyao Wang, Yangruibo Ding, Yuxiang Wei, Zhiruo Wang, and several anonymous workshop reviewers for providing valuable feedback regarding various stages of the draft. We also thank the following people for bringing up illustrative examples mentioned in this paper: Silas Alberti (debugging cloud applications), Chuyue Sun (incomplete speciﬁcation in Verus), MITs 6.172 Course (performance instrumentation), Theo Olausson (costly disasters), Songlin Yang (syntax error in Triton). A. Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship under Grant No. 2141064. N. Jain is supported by NSF grants CCF:1900968, CCF:1908870, and by SKY Lab industrial sponsors and afﬁliates. A. Solar-Lezama is supported by the National Science Foundation (NSF) and Intel Corporation through NSF Grant CCF:2217064. D. Yang is supported by the ONR YIP Award N000142412532."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal, Bryan Parno, and Sean Welleck. Alphaverus: Bootstrapping formally verarXiv preprint iﬁed code generation through self-improving translation and treeﬁnement. arXiv:2412.06176, 2024. (Cited on pg. 76) Lakshya Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K. Lahiri, and Sriram K. Rajamani. Guiding language models of code with global context using monitors, 2023. URL https://arxiv.org/abs/2306.10763. (Cited on pg. 70) Touﬁque Ahmed, Christian Bird, Premkumar Devanbu, and Saikat Chakraborty. Studying llm performance on closed-and open-source data. arXiv preprint arXiv:2402.15100, 2024a. (Cited on pg. 22) Touﬁque Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl Barr. Automatic semantic augmentation of language model prompts (for code summarization). In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113, 2024b. (Cited on pg. 74) Ekin Aky urek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. URL test-time training for abstract reasoning, 2024. The surprising effectiveness of https://arxiv.org/abs/2411.07279. (Cited on pg. 32) Naser Al Madi. How readable is model-generated code? examining readability and viIn Proceedings of the 37th IEEE/ACM International Consual inspection of github copilot. ference on Automated Software Engineering, ASE 22, New York, NY, USA, 2023. Association 10.1145/3551349.3560438. URL for Computing Machinery. https://doi.org/10.1145/3551349.3560438. (Cited on pg. 39) ISBN 9781450394758. doi: Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024. (Cited on pg. 14) Anthropic. Raising the bar on swe-bench veriﬁed with claude 3.5 sonnet. 2024. (Cited on pg. 15) Owura Asare, Meiyappan Nagappan, and Asokan. Is githubs copilot as bad as humans at introducing vulnerabilities in code? Empirical Software Engineering, 28(6):129, 2023. (Cited on pg. 37) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. (Cited on pg. 4, 70) Pavel Avgustinov, Oege de Moor, Michael Jones Peyton, and Max Schafer. Ql: object-oriented queries on relational data. ECOOP, 2016. (Cited on pg. 74) Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. (Cited on pg. 15) Yasharth Bajpai, Bhavya Chopra, Param Biyani, Cagri Aslan, Dustin Coleman, Sumit Gulwani, Chris Parnin, Arjun Radhakrishna, and Gustavo Soares. Lets ﬁx this together: Conversational debugging with github copilot. In 2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC), pages 112. IEEE, 2024. (Cited on pg. 30) 42 Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, and David Farhi. Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. arXiv preprint arXiv:2503.11926, 2025. (Cited on pg. 31) Ola Benderius, Christian Berger, and Victor Malmsten Lundgren. The best rated humanmachine interface design for autonomous vehicles in the 2016 grand cooperative driving challenge. IEEE Transactions on intelligent transportation systems, 19(4):13021307, 2017. (Cited on pg. 18) Ian Berlot-Attwell, Frank Rudzicz, and Xujie Si. Library learning doesnt: The curious case of the single-use library. arXiv preprint arXiv:2410.20274, 2024. (Cited on pg. 19) Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak, and Dawson Engler. few billion lines of code later: using static analysis to ﬁnd bugs in the real world. Communications of the ACM, 53(2):6675, 2010. (Cited on pg. 13) Dirk Beyer. Competition on software veriﬁcation and witness validation: Sv-comp 2023. In Tools and Algorithms for the Construction and Analysis of Systems: 29th International Conference, TACAS 2023, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2023, Paris, France, April 2227, 2023, Proceedings, Part II, page 495522, Berlin, Heidelberg, 2023. Springer-Verlag. ISBN 978-3-031-30819-2. (Cited on pg. 76) Dirk Beyer and Alexander K. Petrenko. Linux driver veriﬁcation. In Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Veriﬁcation and Validation: Applications and Case Studies - Volume Part II, ISoLA12, page 16, Berlin, Heidelberg, 2012. SpringerVerlag. ISBN 9783642340314. (Cited on pg. 76) Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama cyberseceval: secure coding benchmark for language models, 2023. URL https://arxiv.org/abs/2312.04724. (Cited on pg. 71) Google BigSleep. to From naptime vulnerabilities to big real-world sleep: catch models https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html. (Cited on pg. 15, 73) code. 2024. in Using large language URL Andrew Blinn, Xiang Li, June Hyung Kim, and Cyrus Omar. Statically contextualizing large language models with typed holes. Proceedings of the ACM on Programming Languages, 8(OOPSLA2): 468498, 2024. (Cited on pg. 22) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. (Cited on pg. 36) Islem Bouzenia and Michael Pradel. You name it, run it: An llm agent to execute tests of arbitrary projects, 2024. URL https://arxiv.org/abs/2412.10133. (Cited on pg. 75) Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. Repairagent: An autonomous, llmbased agent for program repair. arXiv preprint arXiv:2403.17134, 2024. (Cited on pg. 74, 75) 43 Matthew Bowers, Theo X. Olausson, Lionel Wong, Gabriel Grand, Joshua B. Tenenbaum, Kevin Ellis, and Armando Solar-Lezama. Top-down synthesis for library learning. Proc. ACM Program. Lang., 7(POPL), January 2023. doi: 10.1145/3571234. URL https://doi.org/10.1145/3571234. (Cited on pg. 18) Rod Burstall and John Darlington. transformation system for developing recursive programs. Journal of the ACM (JACM), 24(1):4467, 1977. (Cited on pg. 39) Jialun Cao, Zhiyong Chen, Jiarong Wu, Shing chi Cheung, and Chang Xu. Javabench: benchmark of object-oriented code generation for evaluating large language models, 2024. URL https://arxiv.org/abs/2406.12902. (Cited on pg. 70) Luca Cardelli. Type systems. ACM Computing Surveys (CSUR), 28(1):263264, 1996. (Cited on pg. 38) Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. (Cited on pg. 23, 71) Federico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Anders Freeman, Carolyn Jane Anderson, Molly Feldman, Michael Greenberg, Abhinav Jangda, and Arjun Guha. Knowledge transfer from high-resource to low-resource programming languages for code llms. Proceedings of the ACM on Programming Languages, 8(OOPSLA2):677708, 2024. (Cited on pg. 71) Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, et al. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436, 2024. (Cited on pg. 71) Saikat Chakraborty, Rahul Krishna, Yangruibo Ding, and Baishakhi Ray. Deep learning based vulnerability detection: Are we there yet? IEEE Transactions on Software Engineering, 48:3280 3296, 2020. (Cited on pg. 73) Saikat Chakraborty, Shuvendu Lahiri, Sarah Fakhoury, Madanlal Musuvathi, Akash Lal, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma, and Nikhil Swamy. Ranking llm-generated loop invariants for program veriﬁcation. arXiv preprint arXiv:2310.09342, 2023. (Cited on pg. 73) Arifa I. Champa, Md Fazle Rabbi, Costain Nachuma, and Minhaz F. Zibran. Chatgpt in action: Analyzing its use in software development. In 2024 IEEE/ACM 21st International Conference on Mining Software Repositories (MSR), pages 182186, 2024. (Cited on pg. 71) Satish Chandra. Ai in software engineering at google: Progress and the path ahead (invited talk). In Proceedings of the 1st ACM International Conference on AI-Powered Software, pages 182182, 2024. (Cited on pg. 29) Oliver Chang, Dongge Liu, Jonathan Metzman, curity Team. https://security.googleblog.com/2024/11/leveling-up-fuzzing-finding-more.html, 2024. (Cited on pg. 10, 73) Leveling up fuzzing: Finding more and Google Open Source vulnerabilities with Seai. Daksh Chaudhary, Sri Lakshmi Vadlamani, Dimple Thomas, Shiva Nejati, and Mehrdad Sabetzadeh. Developing llama-based chatbot for ci/cd question answering: case study at ericsson, 2024. URL https://arxiv.org/abs/2408.09277. (Cited on pg. 75) 44 Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu arXiv preprint arXiv:2207.10397, 2022. Chen. Codet: Code generation with generated tests. (Cited on pg. 73) Haogang Chen, Daniel Ziegler, Tej Chajed, Adam Chlipala, Frans Kaashoek, and Nickolai Zeldovich. Using crash hoare logic for certifying the fscq ﬁle system. In Proceedings of the 25th Symposium on Operating Systems Principles, pages 1837, 2015. (Cited on pg. 27) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a. (Cited on pg. 4, 70) Xiancai Chen, Zhengwei Tao, Kechi Zhang, Changzhi Zhou, Wanli Gu, Yuanpeng He, Mengdi Zhang, Xunliang Cai, Haiyan Zhao, and Zhi Jin. Revisit self-debugging with self-generated tests for code generation. arXiv preprint arXiv:2501.12793, 2025. (Cited on pg. 73) Xinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis beyond domain-speciﬁc languages. Advances in Neural Information Processing Systems, 34:22196 22208, 2021b. (Cited on pg. 36) Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. In International Conference on Learning Representations (ICLR), 2024. (Cited on pg. 75) Xiao Cheng, Guanqin Zhang, Haoyu Wang, and Yulei Sui. Path-sensitive code embedding via contrastive learning for software vulnerability detection. In International Symposium on Software Testing and Analysis, 2022. (Cited on pg. 73) Yuri Chervonyi, Trieu Trinh, Miroslav Olˇsak, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXiv preprint arXiv:2502.03544, 2025. (Cited on pg. 29) Wayne Chi, Valerie Chen, Anastasios Nikolas Angelopoulos, Wei-Lin Chiang, Aditya Mittal, Naman Jain, Tianjun Zhang, Ion Stoica, Chris Donahue, and Ameet Talwalkar. Copilot arena: platform for code llm evaluation in the wild, 2025. URL https://arxiv.org/abs/2502.09328. (Cited on pg. 14, 30) Chromium. 10 years of speed in chrome. Chromium Blog, 2018. https://blog.chromium.org/2018/09/10-years-of-speed-in-chrome_11.html. on pg. 9) URL (Cited Edmund Clarke. Model checking. In Foundations of Software Technology and Theoretical Computer Science: 17th Conference Kharagpur, India, December 1820, 1997 Proceedings 17, pages 5456. Springer, 1997. (Cited on pg. 38) Patrick Cousot and Radhia Cousot. Abstract interpretation: uniﬁed lattice model for static In Proceedings of the 4th analysis of programs by construction or approximation of ﬁxpoints. ACM SIGACT-SIGPLAN symposium on Principles of programming languages, pages 238252, 1977. (Cited on pg. 38) Patrick Cousot, Radhia Cousot, Jer ˆome Feret, Laurent Mauborgne, Antoine Mine, David Monniaux, and Xavier Rival. The astree analyzer. In Programming Languages and Systems: 14th European Symposium on Programming, ESOP 2005, Held as Part of the Joint European Conferences on Theory 45 and Practice of Software, ETAPS 2005, Edinburgh, UK, April 4-8, 2005. Proceedings 14, pages 2130. Springer, 2005. (Cited on pg. 76) Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The lean theorem prover (system description). In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pages 378 388. Springer, 2015. (Cited on pg. 26, 76) DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. (Cited on pg. 30) Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Sycophancy arXiv preprint Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, et al. to subterfuge: arXiv:2406.10162, 2024. (Cited on pg. 31) Investigating reward-tampering in large language models. Colin Diggs, Michael Doyle, Amit Madan, Siggy Scott, Emily Escamilla, Jacob Zimmer, Naveed Nekoo, Paul Ursino, Michael Bartholf, Zachary Robin, et al. Leveraging llms for legacy code modernization: Challenges and opportunities for llm-generated documentation. arXiv preprint arXiv:2411.14971, 2024. (Cited on pg. 74) Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang. Hoppity: Learning graph transformations to detect and ﬁx bugs in programs. In International Conference on Learning Representations, 2020. (Cited on pg. 73) 46 Elizabeth Dinella, Shuvendu Lahiri, and Mayur Naik. Program structure aware precondition generation, 2024a. URL https://arxiv.org/abs/2310.02154. (Cited on pg. 73) Elizabeth Dinella, Shuvendu K. Lahiri, and Mayur Naik. Inferring natural preconditions via program transformation. In Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, FSE 2024, page 657658, New York, NY, USA, 2024b. Association for Computing Machinery. ISBN 9798400706585. (Cited on pg. 73) Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: diverse and multilingual benchmark for cross-ﬁle code completion. Advances in Neural Information Processing Systems, 36:4670146723, 2023. (Cited on pg. 21) Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, and Yizheng Chen. Vulnerability detection with code language models: How far are we? arXiv preprint arXiv:2403.18624, 2024a. (Cited on pg. 73) Yangruibo Ding, Jinjun Peng, Marcus Min, Gail Kaiser, Junfeng Yang, and Baishakhi Ray. arXiv preprint Semcoder: Training code language models with comprehensive semantics. arXiv:2406.01006, 2024b. (Cited on pg. 28) Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution-aware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pages 112, 2024c. (Cited on pg. 22, 28) Craig Disselkoen, John Kastner, shaobo-he aws, Kesha Hietala, Andrew Wells, Aaron Eline, Victor Moreno, Adrian Palacios, Magnus Markling, Nicholas Szegheo, yuan, Mats Jun Larsen, Saurav Sharma, B-Lorentz, Naomi Smith, Sandy Vanderbleek, Anwar Mamat, Andrew Banchich, Kevin Hakanson, vasumv, Sarah Cecchetti, Rin Arakaki, Oliver Flatt, Christian Meissl, Bhakti, Brandon Rozek, Juan V. Garcıa, ozsa Tamas, and Lucas Jones. cedar-policy/cedar, 2025. URL https://github.com/cedar-policy/cedar. (Cited on pg. 76) Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation, 2023. (Cited on pg. 70) Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, et al. Competitive programming with large reasoning models. arXiv preprint arXiv:2502.06807, 2025. (Cited on pg. 38) Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with repl. Advances in Neural Information Processing Systems, 32, 2019. (Cited on pg. 36) Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua Tenenbaum. Dreamcoder: Bootstrapping inIn Proceedings of the 42nd acm ductive program synthesis with wake-sleep library learning. sigplan international conference on programming language design and implementation, pages 835850, 2021. (Cited on pg. 18, 32) Madeline Endres, Sarah Fakhoury, Saikat Chakraborty, and Shuvendu Lahiri. Can large language models transform natural language intent into formal method postconditions? Proceedings of the ACM on Software Engineering, 1(FSE):18891912, 2024. (Cited on pg. 33) 47 Hasan Ferit Eniser, Hanliang Zhang, Cristina David, Meng Wang, Maria Christakis, Brandon Paulsen, Joey Dodds, and Daniel Kroening. Towards translating real-world code with llms: study of translating to rust, 2024. (Cited on pg. 72) Andres Erbsen, Samuel Gruetter, Joonwon Choi, Clark Wood, and Adam Chlipala. Integration veriﬁcation across software and hardware for simple embedded system. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, pages 604619, 2021. (Cited on pg. 76) Andres Erbsen, Jade Philipoom, Dustin Jamner, Ashley Lin, Samuel Gruetter, Clement Pit-Claudel, and Adam Chlipala. Foundational integration veriﬁcation of cryptographic server. Proceedings of the ACM on Programming Languages, 8(PLDI):17041729, 2024. (Cited on pg. 76) Michael Ernst, Jake Cockrell, William Griswold, and David Notkin. Dynamically discovering likely program invariants to support program evolution. In Proceedings of the 21st international conference on Software engineering, pages 213224, 1999. (Cited on pg. 73) Michael Ernst, Jeff Perkins, Philip Guo, Stephen McCamant, Carlos Pacheco, Matthew Tschantz, and Chen Xiao. The daikon system for dynamic detection of likely invariants. Science of computer programming, 69(1-3):3545, 2007. (Cited on pg. 73) Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty, and Shuvendu Lahiri. Llm-based test-driven interactive code generation: User study and empirical evaluation. IEEE Transactions on Software Engineering, 2024. (Cited on pg. 71) Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie Zhang. Large language models for software engineering: Survey and open problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE), pages 3153. IEEE, 2023. (Cited on pg. 4) Kasra Ferdowsi, Ruanqianqian Huang, Michael James, Nadia Polikarpova, and Sorin Lerner. Validating ai-generated code with live programming. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 18, 2024. (Cited on pg. 40) Andreas Florath. Enhancing formal theorem proving: comprehensive dataset for training ai models on coq code. arXiv preprint arXiv:2403.12627, 2024. (Cited on pg. 71) Christopher Foster, Abhishek Gulati, Mark Harman, Inna Harper, Ke Mao, Jillian Ritchey, Herve arXiv Robert, and Shubho Sengupta. Mutation-guided llm-based test generation at meta. preprint arXiv:2501.12862, 2025. (Cited on pg. 10) Michael Fu and Chakkrit Tantithamthavorn. LineVul: transformer-based line-level vulnerability prediction. In International Conference on Mining Software Repositories, 2022. (Cited on pg. 73) Yujia Fu, Peng Liang, Amjed Tahir, Zengyang Li, Mojtaba Shahin, Jiaxin Yu, and Jinfu Chen. Security weaknesses of copilot generated code in github. arXiv preprint arXiv:2310.02059, 2023. (Cited on pg. 37) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. (Cited on pg. 20) Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refactorbench: Evaluating stateful reasoning in language agents through code. In NeurIPS 2024 Workshop on Open-World Agents, 2024. (Cited on pg. 14, 71) Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. (Cited on pg. 15) Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. Grammar-constrained decoding for structured nlp tasks without ﬁnetuning. arXiv preprint arXiv:2305.13971, 2023. (Cited on pg. 39) Patrice Godefroid, Nils Klarlund, and Koushik Sen. Dart: Directed automated random testing. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation, pages 213223, 2005. (Cited on pg. 38) Shilpi Goel, Alex Keizer, Siddharth, Yan Peng, Kim Morrison, Nathan Wetzler, Leonardo de Moura, Nevine Ebeid, Juneyoung Lee, Austin Letson, Luisa Cicolini, and Soonho Kong. leanprover/lnsym, 2024. URL https://github.com/leanprover/LNSym. (Cited on pg. 76) Jingzhi Gong, Vardan Voskanyan, Paul Brookes, Fan Wu, Wei Jie, Jie Xu, Rafail Giavrimis, Mike Basios, Leslie Kanthan, and Zheng Wang. Language models for code optimization: Survey, challenges and future directions. arXiv preprint arXiv:2501.01277, 2025. (Cited on pg. 72) Linyuan Gong, Mostafa Elhoushi, and Alvin Cheung. Ast-t5: Structure-aware pretraining for code generation and understanding. arXiv preprint arXiv:2401.03003, 2024. (Cited on pg. 39) Google. Ai achieves silver-medal standard solving international mathematical olympiad problems. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/, 2024. (Cited on pg. 29) Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: Benchmark for Code Reasoning, Understanding and Execution. arXiv preprint arXiv:2401.03065, 2024. (Cited on pg. 21) Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations and Trends in Programming Languages, 4(1-2):1119, 2017. (Cited on pg. 29) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data ﬂow. arXiv preprint arXiv:2009.08366, 2020. (Cited on pg. 28) Xuehang Guo, Xingyao Wang, Yangyi Chen, Sha Li, Chi Han, Manling Li, and Heng Ji. Syncmind: arXiv preprint Measuring agent out-of-sync recovery in collaborative software engineering. arXiv:2502.06994, 2025. (Cited on pg. 30) Hossein Hajipour, Keno Hassler, Thorsten Holz, Lea Sch onherr, and Mario Fritz. Codelmsec benchmark: Systematically evaluating and ﬁnding security vulnerabilities in black-box code language models, 2023. URL https://arxiv.org/abs/2302.04012. (Cited on pg. 71) Rajarshi Haldar and Julia Hockenmaier. Analyzing the performance of large language models on code summarization. arXiv preprint arXiv:2404.08018, 2024. (Cited on pg. 74) Sudheendra Hangal and Monica Lam. Tracking down software bugs using automatic anomaly detection. In Proceedings of the 24th international conference on Software engineering, pages 291301, 2002. (Cited on pg. 73) Ben Hawkes. 0day in the wild. https://googleprojectzero.blogspot.com/p/0day.html, 2019. (Cited on pg. 10) 49 Peter Hawkins, Alex Aiken, Kathleen Fisher, Martin Rinard, and Mooly Sagiv. Data representation synthesis. In Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation, pages 3849, 2011. (Cited on pg. 19) Jingxuan He, Mark Vero, Gabriela Krasnopolska, and Martin Vechev. Instruction tuning for secure code generation, 2024. URL https://arxiv.org/abs/2402.09497. (Cited on pg. 71) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021. (Cited on pg. 5) David Hin, Andrey Kan, Huaming Chen, and Muhammad Ali Babar. Linevd: Statement-level vulnerability detection using graph neural networks. In International Conference on Mining Software Repositories, 2022. (Cited on pg. 73) Charles Hong, Sahil Bhatia, Altan Haan, Shengjun Kris Dong, Dima Nikiforov, Alvin Cheung, and Yakun Sophia Shao. Llm-aided compilation for tensor accelerators. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 114. IEEE, 2024. (Cited on pg. 39) Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: systematic literature review. ACM Transactions on Software Engineering and Methodology, 33(8):179, 2024. (Cited on pg. 4) Xueyu Hu, Kun Kuang, Jiankai Sun, Hongxia Yang, and Fei Wu. Leveraging print debugging to improve code generation in large language models. arXiv preprint arXiv:2401.05319, 2024. (Cited on pg. 75) Li Huang, Sophie Ebersold, Alexander Kogtenkov, Bertrand Meyer, and Yinling Liu. Lessons from formally veriﬁed deployed software systems (extended version). arXiv preprint arXiv:2301.02206, 2023. (Cited on pg. 76) Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. (Cited on pg. 23) Ali Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid, Rangeet Pan, Saurabh Sinha, and Reyhaneh Jabbarvand. Repository-level compositional code translation and validation. arXiv preprint arXiv:2410.24117, 2024. (Cited on pg. 8, 72) Nizar Islah, Justine Gehring, Diganta Misra, Eilif Muller, Irina Rish, Terry Yue Zhuo, and Massimo Caccia. Gitchameleon: Unmasking the version-switching capabilities of code generation models. arXiv preprint arXiv:2411.05830, 2024. (Cited on pg. 24) Mohayeminul Islam, Ajay Kumar Jha, Sarah Nadi, and Ildar Akhmetov. Pymigbench: benchmark for python library migration. In 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR), 2023. (Cited on pg. 72) Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020. (Cited on pg. 36) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. (Cited on pg. 36) 50 Kush Jain, Gabriel Synnaeve, and Baptiste Rozi`ere. Testgeneval: real world unit test generation and test completion benchmark. arXiv preprint arXiv:2410.00752, 2024a. (Cited on pg. 14) Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Livecodebench: HolisWang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. tic and contamination free evaluation of large language models for code, 2024b. URL https://arxiv.org/abs/2403.07974. (Cited on pg. 5, 14, 75) Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: TurnIn Forty-ﬁrst International ing any github repository into programming agent environment. Conference on Machine Learning, 2024c. (Cited on pg. 19, 30) Yanjie Jiang, Hui Liu, Nan Niu, Lu Zhang, and Yamin Hu. Extracting concise bug-ﬁxing In IEEE/ACM 43rd Inpatches from human-written patches in version control systems. ternational Conference on Software Engineering (ICSE 2021), pages 686698, Los Alamitos, CA, USA, may 2021. IEEE Computer Society. doi: 10.1109/ICSE43902.2021.00069. URL https://doi.ieeecomputersociety.org/10.1109/ICSE43902.2021.00069. (Cited on pg. 74) Yanjie Jiang, Hui Liu, Xiaoqing Luo, Zhihao Zhu, Xiaye Chi, Nan Niu, Yuxia Zhang, Yamin Hu, Pan Bian, and Lu Zhang. Bugbuilder: An automated approach to building bug repository. IEEE Transactions on Software Engineering, pages 122, 2022a. doi: 10.1109/TSE.2022.3177713. (Cited on pg. 74) Yanjie Jiang, Hui Liu, Yuxia Zhang, Weixing Ji, Hao Zhong, and Lu Zhang. Do bugs lead to unnaturalness of source code? In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, page 10851096, New York, NY, USA, 2022b. Association for Computing Machinery. ISBN 9781450394130. doi: 10.1145/3540250.3549149. URL https://doi.org/10.1145/3540250.3549149. (Cited on pg. 74) Yucheng Jiang, Yijia Shao, Dekun Ma, Sina Semnani, and Monica Lam. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. arXiv preprint arXiv:2408.15232, 2024. (Cited on pg. 40) Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Oﬁr Press, and SWE-bench: Can language models resolve real-world github isIn The Twelfth International Conference on Learning Representations, 2024. URL Karthik Narasimhan. sues? https://openreview.net/forum?id=VTF8yNQM66. (Cited on pg. 5, 14, 34) Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llm-based agents for software engineering: survey of current, challenges and future. arXiv preprint arXiv:2408.02479, 2024. (Cited on pg. 4) Xin Jin, tion: https://arxiv.org/abs/2312.09601. (Cited on pg. 74) Jonathan Larson, Weiwei Yang, and Zhiqiang Lin. Benchmarking chatgpt/gpt-4 and other large language models, Binary code summarizaURL 2023. Sathvik Joel, Jie JW Wu, and Fatemeh Fard. survey on llm-based code generation for lowresource and domain-speciﬁc programming languages. arXiv preprint arXiv:2410.03981, 2024. (Cited on pg. 4, 71) Rene Just, Darioush Jalali, and Michael Ernst. Defects4j: database of existing faults to enable controlled testing studies for java programs. In Proceedings of the 2014 international symposium on software testing and analysis, pages 437440, 2014. (Cited on pg. 74) 51 Adharsh Kamath, Aditya Senthilnathan, Saikat Chakraborty, Pantazis Deligiannis, Shuvendu Lahiri, Akash Lal, Aseem Rastogi, Subhajit Roy, and Rahul Sharma. Finding inductive loop invariants using large language models. arXiv preprint arXiv:2311.07948, 2023. (Cited on pg. 73) Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara Ericson, David Weintrop, and Tovi Grossman. Studying the effect of ai code generators on supporting novice learners in introductory programming. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 123, 2023a. (Cited on pg. 71) Majeed Kazemitabaar, Xinying Hou, Austin Henley, Barbara Jane Ericson, David Weintrop, and Tovi Grossman. How novices use llm-based code generators to solve cs1 coding tasks in self-paced learning environment. In Proceedings of the 23rd Koli calling international conference on computing education research, pages 112, 2023b. (Cited on pg. 71) Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev Alur, and Mayur Naik. Understanding the effectiveness of large language models in detecting security vulnerabilities. arXiv preprint arXiv:2311.16169, 2023. (Cited on pg. 73) Mohammed Kharma, Soohyeon Choi, Mohammed AlKhanafseh, and David Mohaisen. Security arXiv preprint and quality in llm-generated code: multi-language, multi-model analysis. arXiv:2502.01853, 2025. (Cited on pg. 25) Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski, Michael Norrish, et al. sel4: Formal In Proceedings of the ACM SIGOPS 22nd symposium on Operating veriﬁcation of an os kernel. systems principles, pages 207220, 2009. (Cited on pg. 76) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu noz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. (Cited on pg. 29) Tim Kraska, Alex Beutel, Ed Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489504, 2018. (Cited on pg. 19) Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, and Anima Anandkumar. Leanagent: Lifelong learning for formal theorem proving. arXiv preprint arXiv:2410.06209, 2024. (Cited on pg. 24) Shuvendu Lahiri, Sarah Fakhoury, Aaditya Naik, Georgios Sakkas, Saikat Chakraborty, Madanlal Musuvathi, Piali Choudhury, Curtis von Veh, Jeevana Priya Inala, Chenglong Wang, arXiv preprint et al. arXiv:2208.05950, 2022. (Cited on pg. 71) Interactive code generation via test-driven user-intent formalization. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. (Cited on pg. 30) Leslie Lamport. Introduction to tla. 1994. (Cited on pg. 76) 52 Robert Tjarko Lange, Aaditya Prasad, Qi Sun, Maxence Faldor, Yujin Tang, and David Ha. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. 2025. (Cited on pg. 34) Andrea Lattuada, Travis Hance, Jay Bosamiya, Matthias Brun, Chanhee Cho, Hayley LeBlanc, Pranav Srinivasan, Reto Achermann, Tej Chajed, Chris Hawblitzel, et al. Verus: practical foundation for systems veriﬁcation. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles, pages 438454, 2024. (Cited on pg. 76) Cheryl Lee, Chunqiu Steven Xia, Longji Yang, Jen-tse Huang, Zhouruixin Zhu, Lingming Zhang, and Michael Lyu. uniﬁed debugging approach via llm-based multi-agent synergy. arXiv preprint arXiv:2404.17153, 2024. (Cited on pg. 75) Rustan Leino. Dafny: An automatic program veriﬁer for functional correctness. In International conference on logic for programming artiﬁcial intelligence and reasoning, pages 348370. Springer, 2010. (Cited on pg. 76) Caroline Lemieux, Jeevana Priya Inala, Shuvendu Lahiri, and Siddhartha Sen. Codamosa: EsIn 2023 caping coverage plateaus in test generation with pre-trained large language models. IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 919931. IEEE, 2023. (Cited on pg. 72) Xavier Leroy, Sandrine Blazy, Daniel Kastner, Bernhard Schommer, Markus Pister, and Christian In ERTS 2016: Embedded Real Ferdinand. Compcert-a formally veriﬁed optimizing compiler. Time Software and Systems, 8th European Congress, 2016. (Cited on pg. 13, 76) Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. (Cited on pg. 32) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich uttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. (Cited on pg. 20) Haonan Li, Yu Hao, Yizhuo Zhai, and Zhiyun Qian. Enhancing static analysis for practical bug detection: An LLM-integrated approach. Proc. ACM Program. Lang., 8(OOPSLA1), 2024a. (Cited on pg. 73) Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025a. (Cited on pg. 29) Kefan Li and Yuan Yuan. Large language models as test case generators: Performance evaluation and enhancement, 2024. URL https://arxiv.org/abs/2404.13340. (Cited on pg. 72) Keqin Li, Armando Zhu, Peng Zhao, Jintong Song, and Jiabei Liu. Utilizing deep learning to optimize software development processes. arXiv preprint arXiv:2404.13630, 2024b. (Cited on pg. 74) Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. (Cited on pg. 29) 53 Ruishi Li, Bo Wang, Tianyu Li, Prateek Saxena, and Ashish Kundu. Translating to rust: Lessons from user study. In Proceedings 2025 Network and Distributed System Security Symposium, NDSS 2025. Internet Society, 2025b. (Cited on pg. 72) Ryan Li, Yanzhe Zhang, and Diyi Yang. Sketch2code: Evaluating vision-language models for interactive web design prototyping. arXiv preprint arXiv:2410.16232, 2024c. (Cited on pg. 30) Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer Dunn, Hao Tang, Michelangelo Naim, Dat Nguyen, et al. Combining induction and transduction for abstract reasoning. arXiv preprint arXiv:2411.02272, 2024d. (Cited on pg. 29) Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. (Cited on pg. 32) Yi Li, Shaohua Wang, and Tien Nhut Nguyen. Vulnerability detection with ﬁne-grained interIn Joint Meeting on European Software Engineering Conference and Symposium on the pretations. Foundations of Software Engineering, 2021. (Cited on pg. 73) Yue Chen Li, Stefan Zetzsche, and Siva Somayyajula. Dafny as veriﬁcation-aware intermediate language for code generation. arXiv preprint arXiv:2501.06283, 2025c. (Cited on pg. 76) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022a. (Cited on pg. 5) Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie Si. survey on deep learning for theorem proving. arXiv preprint arXiv:2404.09939, 2024e. (Cited on pg. 76) Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, et al. Automating code review activities by largescale pre-training. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 10351047, 2022b. (Cited on pg. 74) Ziyang Li, Saikat Dutta, and Mayur Naik. Llm-assisted static analysis for detecting security vulnerabilities. arXiv preprint arXiv:2405.17238, 2024f. (Cited on pg. 73) Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar, Minjia Zhang, and Tianyin Xu. Large language models as conﬁguration validators. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pages 204216. IEEE Computer Society, 2024. (Cited on pg. 12, 75) Jenny Liang, Chenyang Yang, and Brad Myers. large-scale survey on the usability of ai programming assistants: Successes and challenges. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pages 113, 2024. (Cited on pg. 4) Chang Liu, Xiwei Wu, Yuan Feng, Qinxiang Cao, and Junchi Yan. Towards general loop invariant generation: benchmark of programs with memory manipulation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. (Cited on pg. 74) Dongge Liu, Jonathan Metzman, Oliver Chang, and Google Open Source Team. curity https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html, 2023a. (Cited on pg. 73) Ai-powered Breaking fuzzing: hunting bug the Sebarrier. 54 Dongge Liu, Oliver Chang, Jonathan metzman, Martin OSS-Fuzz-Gen: Automated Fuzz Target Generation, May 2024b. Maruseac. https://github.com/google/oss-fuzz-gen. (Cited on pg. 73) Sablotny, and Mihai URL Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. (Cited on pg. 29) Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efﬁcient code generation. In First Conference on Language Modeling, 2024c. URL https://openreview.net/forum?id=IBCBMeAhmC. (Cited on pg. 14) Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD), pages 18. IEEE, 2023b. (Cited on pg. 71) Peipei Liu, Jian Sun, Li Chen, Zhaoteng Yan, Peizheng Zhang, Dapeng Sun, Dawei Wang, and Dan Li. Control ﬂow-augmented decompiler based on large language model, 2025a. URL https://arxiv.org/abs/2503.07215. (Cited on pg. 74) Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhi Li, and Limin Sun. Harnessing the power of llm to support binary taint analysis. arXiv preprint arXiv:2310.08275, 2023c. (Cited on pg. 74) Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, Z. Y. Peng, Shukai Liu, Zhaoxiang Zhang, Ge Zhang, Wenhao Huang, Kai Shen, and Liang Xiang. Fullstack bench: Evaluating llms as full stack coders, 2024d. URL https://arxiv.org/abs/2412.00535. (Cited on pg. 4) Vivian Liu, Rubaiat Habib Kazi, Li-Yi Wei, Matthew Fisher, Timothy Langlois, Seth Walker, and Lydia Chilton. Logomotion: Visually-grounded code synthesis for creating and editing animation, 2025b. URL https://arxiv.org/abs/2405.07065. (Cited on pg. 70) Ye Liu, Rui Meng, Shaﬁq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Codexembed: generalist embedding model family for multiligual and multi-task code retrieval, 2024e. (Cited on pg. 74) Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, and Chao Peng. Marscode agent: Ai-native automated bug ﬁxing. arXiv preprint arXiv:2409.00899, 2024f. (Cited on pg. 15) Zeyu Leo Liu, Shrey Pandit, Xi Ye, Eunsol Choi, and Greg Durrett. Codeupdatearena: Benchmarking knowledge editing on api updates. arXiv preprint arXiv:2407.06249, 2024g. (Cited on pg. 24) Evan Lohn and Sean Welleck. minicodeprops: minimal benchmark for proving code properties. arXiv preprint arXiv:2406.11915, 2024. (Cited on pg. 76) Chloe Loughridge, Qinyi Sun, Seth Ahrenbach, Federico Cassano, Chuyue Sun, Ying Sheng, Anish Mudide, Md Rakib Hossain Misu, Nada Amin, and Max Tegmark. Dafnybench: benchmark for formal software veriﬁcation. arXiv preprint arXiv:2406.08467, 2024. (Cited on pg. 76) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. (Cited on pg. 29) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021. (Cited on pg. 75) Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repositorylevel code documentation generation. arXiv preprint arXiv:2402.16667, 2024. (Cited on pg. 74) Jenny Ma, Karthik Sreedhar, Vivian Liu, Pedro Alejandro Perez, Sitong Wang, Riya Sahni, and Lydia B. Chilton. Dynex: Dynamic code synthesis with structured design exploration for accelerated exploratory programming, 2025a. URL https://arxiv.org/abs/2410.00400. (Cited on pg. 71) Wei Ma, Shangqing Liu, Mengjie Zhao, Xiaofei Xie, Wenhang Wang, Qiang Hu, Jie Zhang, and Yang Liu. Unveiling code pre-trained models: Investigating syntax and semantics capacities. ACM Transactions on Software Engineering and Methodology, 33(7):129, 2024. (Cited on pg. 20) Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, and Bing Xie. Sorft: Issue resolving with subtask-oriented reinforced ﬁne-tuning. arXiv preprint arXiv:2502.20127, 2025b. (Cited on pg. 31) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-reﬁne: Iterative reﬁnement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. (Cited on pg. 75) Rien Maertens, Charlotte Van Petegem, Niko Strijbol, Toon Baeyens, Arne Carla Jacobs, Peter Dawyndt, and Bart Mesuere. Dolos: Language-agnostic plagiarism detection in source code. Journal of Computer Assisted Learning, 38(4):10461061, 2022. (Cited on pg. 31) Daniel Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257263, 2023a. (Cited on pg. 72) Daniel Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618(7964):257263, 2023b. (Cited on pg. 27) Alexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Raymond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, and Matthias Galle. On leakage of code generation evaluation datasets. arXiv preprint arXiv:2407.07565, 2024. (Cited on pg. 14) Xiang Mei, Pulkit Singh Singaria, Jordi Del Castillo, Haoran Xi, Tiffany Bao, Ruoyu Wang, Yan Shoshitaishvili, Adam Doupe, Hammond Pearce, Brendan Dolan-Gavitt, et al. Arvo: Atlas of reproducible vulnerabilities for open source software. arXiv preprint arXiv:2408.02153, 2024. (Cited on pg. 14) Barton Miller, Lars Fredriksen, and Bryan So. An empirical study of the reliability of unix utilities. Communications of the ACM, 33(12):3244, 1990. (Cited on pg. 73) Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can arXiv preprint frontier llms earn $1 million from real-world freelance software engineering? arXiv:2502.12115, 2025. (Cited on pg. 14) 56 Md Rakib Hossain Misu, Cristina Lopes, Iris Ma, and James Noble. Towards ai-assisted synthesis of veriﬁed dafny methods. Proceedings of the ACM on Software Engineering, 1(FSE):812835, 2024. (Cited on pg. 76) Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462, 2023. (Cited on pg. 6) Balazs Mosolyg o, Norbert Vandor, Gabor Antal, and Peter Heged us. On the rise and fall of simple stupid bugs: life-cycle analysis of sstubs. In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pages 495499. IEEE, 2021. (Cited on pg. 9) Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In Automated DeductionCADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 1215, 2021, Proceedings 28, pages 625635. Springer, 2021. (Cited on pg. 26) Niels undler, Mark uller, Jingxuan He, and Martin Vechev. Swt-bench: Testing and validating real-world bug-ﬁxes with code agents. Advances in Neural Information Processing Systems, 37: 8185781887, 2025. (Cited on pg. 72) Vijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel Cheng, Negar Ghorbani, Renuka Fernandez, Nachiappan Nagappan, and Peter Rigby. Ai-assisted code authoring at scale: Fine-tuning, deploying, and mixed methods evaluation. Proceedings of the ACM on Software Engineering, 1(FSE):10661085, 2024. (Cited on pg. 29) Nadia Nahar, Shurui Zhou, Grace Lewis, and Christian Kastner. Collaboration challenges in In building ml-enabled systems: Communication, documentation, engineering, and process. Proceedings of the 44th international conference on software engineering, pages 413425, 2022. (Cited on pg. 17) Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113, 2024. (Cited on pg. 75) Chandrakana Nandi, Max Willsey, Adam Anderson, James R. Wilcox, Eva Darulova, Dan Grossman, and Zachary Tatlock. Synthesizing structured cad models with equality saturation and inverse transformations. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2020, page 3144, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450376136. (Cited on pg. 70) Cassio Neri. Shorter and faster than sort3alphadev. arXiv preprint arXiv:2307.14503, 2023. (Cited on pg. 27) Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. Next: Teaching large language models to reason about code execution. arXiv preprint arXiv:2404.14662, 2024. (Cited on pg. 22, 28) Stoyan Nikolov, Daniele Codecasa, Anna Sjovall, Maxim Tabachnyk, Satish Chandra, Siddharth Taneja, and Celal Ziftci. How is google using ai for internal code migrations? arXiv preprint arXiv:2501.06972, 2025. (Cited on pg. 7, 72) Tobias Nipkow, Markus Wenzel, and Lawrence Paulson. Isabelle/HOL: proof assistant for higherorder logic. Springer, 2002. (Cited on pg. 76) 57 Vikram Nitin, Rahul Krishna, Luiz Lemos do Valle, and Baishakhi Ray. C2saferrust: Transforming projects into safer rust with neurosymbolic techniques. arXiv preprint arXiv:2501.14257, 2025. (Cited on pg. 8, 72) Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua Tenenbaum, and Armando Solar-Lezama. Representing partial programs with blended abstract semantics. arXiv preprint arXiv:2012.12964, 2020. (Cited on pg. 36) Theo Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando SolarLezama. Is self-repair silver bullet for code generation? In The Twelfth International Conference on Learning Representations, 2023. (Cited on pg. 15) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando SolarIn International Conference on Is Self-Repair Silver Bullet for Code Generation? Lezama. Learning Representations (ICLR), 2024. (Cited on pg. 75) Behrooz Omidvar Tehrani and Anmol Anubhai. Evaluating human-ai partnership for llm-based code migration. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 18, 2024. (Cited on pg. 72) OpenAI. Gpt-4 demo: From sketch to website. https://www.youtube.com/watch?v=outcGtbnMuQ, 2023a. Accessed: 2025-03-26. (Cited on pg. 70) OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2023b. (Cited on pg. 70) Gabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishabh Singh, and Michele Catasta. Measuring the impact of programming language distribution. In International Conference on Machine Learning, pages 2661926645. PMLR, 2023. (Cited on pg. 71) Anne Ouyang, Simon Guo, and Azalia Mirhoseini. Kernelbench: Can llms write gpu kernels?, 2024. URL https://scalingintelligence.stanford.edu/blogs/kernelbench/. (Cited on pg. 9, 27, 28, 72) Ipek Ozkaya. Application of large language models to software engineering tasks: Opportunities, risks, and implications. IEEE Software, 40(3):48, 2023. (Cited on pg. 4) Oded Padon, Neil Immerman, Sharon Shoham, Aleksandr Karbyshev, and Mooly Sagiv. Decidability of inferring inductive invariants. ACM SIGPLAN Notices, 51(1):217231, 2016. (Cited on pg. 73) Shankara Pailoor, Yuepeng Wang, and Isil Dillig. Semantic code refactoring for abstract data types. Proc. ACM Program. Lang., (POPL), 2024. (Cited on pg. 71) Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Training software engineering agents and veriﬁers with swe-gym, 2024. URL Zhang. https://arxiv.org/abs/2412.21139. (Cited on pg. 30, 31) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. (Cited on pg. 31) Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, and Loris DAntoni. (Cited on pg. Grammar-aligned decoding, 2024. URL https://arxiv.org/abs/2405.21047. 70) 58 David Lorge Parnas. On the criteria to be used in decomposing systems into modules. Communications of the ACM, 15(12):10531058, 1972. (Cited on pg. 7) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. (Cited on pg. 15) Indraneil Paul, Goran Glavaˇs, and Iryna Gurevych. Ircoder: Intermediate representations make language models robust multilingual code generators. arXiv preprint arXiv:2403.03894, 2024. (Cited on pg. 71) Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao, David WilliamsKing, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, and Suman Jana. Stateformer: ﬁnegrained type recovery from binaries using generative state modeling. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2021. Association for Computing Machinery, 2021. ISBN 9781450385626. (Cited on pg. 74) Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. Can large language models reason about program invariants? In International Conference on Machine Learning, pages 2749627520. PMLR, 2023. (Cited on pg. 28, 73) Zehua Pei, Hui-Ling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. Betterv: Controlled verilog generation with discriminative guidance. arXiv preprint arXiv:2402.03375, 2024. (Cited on pg. 71) Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, and Baishakhi Ray. Cweval: OutcomeURL llm code generation, 2025. driven evaluation on functionality and security of https://arxiv.org/abs/2501.08200. (Cited on pg. 71) Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, and Michael Lyu. Generative type In 2023 38th IEEE/ACM International Conference on Automated Software inference for python. Engineering (ASE), pages 988999. IEEE, 2023. (Cited on pg. 74) Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, Reliable code generation from pre-trained lanURL and Sumit Gulwani. guage models. https://openreview.net/forum?id=KmtVD97J43e. (Cited on pg. 39) In International Conference on Learning Representations, 2022. Synchromesh: Gabriel Poesia, Chloe Loughridge, and Nada Amin. dafny-annotator: Ai-assisted veriﬁcation of dafny programs. arXiv preprint arXiv:2411.15143, 2024. (Cited on pg. 76) Rachel Potvin and Josh Levenberg. Why google stores billions of lines of code in single repository. Communications of the ACM, 59(7):7887, 2016. (Cited on pg. 20) James Prather, Brent Reeves, Paul Denny, Brett Becker, Juho Leinonen, Andrew Luxton-Reilly, Garrett Powell, James Finnie-Ansley, and Eddie Antonio Santos. its weird that it knows what want: Usability and interactions with copilot for novice programmers. ACM transactions on computer-human interaction, 31(1):131, 2023. (Cited on pg. 71) Yewen Pu, Kevin Ellis, Marta Kryven, Josh Tenenbaum, and Armando Solar-Lezama. Program synthesis with pragmatic communication. Advances in neural information processing systems, 33: 1324913259, 2020. (Cited on pg. 40) 59 Markus Puschel, Jose MF Moura, Jeremy Johnson, David Padua, Manuela Veloso, Bryan Singer, Jianxin Xiong, Franz Franchetti, Aca Gacic, Yevgen Voronenko, et al. Spiral: Code generation for dsp transforms. Proceedings of the IEEE, 93(2):232275, 2005. (Cited on pg. 39) Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, and Bernhard Sch olkopf. Can large language models understand symbolic graphics programs?, 2024. URL https://arxiv.org/abs/2408.08313. (Cited on pg. 70) Alec Radford, Sutskever. Language models are unsupervised multitask learners. https://api.semanticscholar.org/CorpusID:160025533. (Cited on pg. 70) Rewon Child, David Luan, Dario Amodei, Jeff Wu, and Ilya URL 2019. Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Fredo Durand, and Saman Amarasinghe. Halide: language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines. Acm Sigplan Notices, 48(6):519530, 2013. (Cited on pg. 39) Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. Codebleu: method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297, 2020. (Cited on pg. 31) Pierre Ricadat. the https://blog.pierre-ricadat.com/scala-3-migration-report-from-the-field, (Cited on pg. 8) migration: Report Scala from ﬁeld. 2025. Martin Riddell, Ansong Ni, and Arman Cohan. Quantifying contamination in evaluating code generation capabilities of language models. arXiv preprint arXiv:2403.04811, 2024. (Cited on pg. 14) Abhik Roychoudhury and Andreas Zeller. Will ai replace software engineers? hold your breath. arXiv preprint arXiv:2502.20429, 2025. (Cited on pg. 21) Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, and Baishakhi Ray. Ai software engineer: Programming with trust. arXiv preprint arXiv:2502.13767, 2025a. (Cited on pg. 4) Abhik Roychoudhury, Corina Pasareanu, Michael Pradel, and Baishakhi Ray. Ai software engineer: Programming with trust, 2025b. URL https://arxiv.org/abs/2502.13767. (Cited on pg. 73) Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. Specrover: Code intent extraction via llms, 2024. URL https://arxiv.org/abs/2408.02232. (Cited on pg. 73) Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, and Baishakhi Ray. Code-aware prompting: study of coverage-guided test generation in regression setting using llm. Proceedings of the ACM on Software Engineering, 1(FSE): 951971, 2024. (Cited on pg. 72) Tao Schardl, Tyler Denniston, Damon Doucet, Bradley Kuszmaul, I-Ting Angelina Lee, and Charles Leiserson. The csi framework for compiler-inserted program instrumentation. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 1(2):125, 2017. (Cited on pg. 16) Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. (Cited on pg. 15) Alex Sellink, Harry Sneed, and Chris Verhoef. Restructuring of cobol/cics legacy systems. Science of Computer Programming, 45(2-3):193243, 2002. (Cited on pg. 8) Koushik Sen, Darko Marinov, and Gul Agha. Cute: concolic unit testing engine for c. ACM SIGSOFT Software Engineering Notes, 30(5):263272, 2005. (Cited on pg. 38) Agnia Sergeyuk, Yaroslav Golubev, Timofey Bryksin, and Iftekhar Ahmed. Using ai-based coding assistants in practice: State of affairs, perceptions, and ways forward. Information and Software Technology, 178:107610, 2025. (Cited on pg. 4) Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, and Diyi Yang. Collaborative gym: framework for enabling and evaluating human-agent collaboration. arXiv preprint arXiv:2412.15701, 2024a. (Cited on pg. 17) Yijia Shao, Vinay Samuel, Yucheng Jiang, Collaborative gym: framework for enabling and evaluating human-agent collaboration, 2024b. URL https://arxiv.org/abs/2412.15701. (Cited on pg. 34) John Yang, and Diyi Yang. Brad Sheese, Mark Lifﬁton, Jaromir Savelka, and Paul Denny. Patterns of student help-seeking when using large language model-powered programming assistant. In Proceedings of the 26th Australasian computing education conference, pages 4957, 2024. (Cited on pg. 71) Manish Shetty, Naman Jain, Adwait Godbole, Sanjit Seshia, and Koushik Sen. Dual code-test to (safe) rust translation using llms and dynamic analysis. arXiv:2412.14234, 2024. (Cited on pg. 8, 72) Syzygy: arXiv preprint Kensen Shi, Deniz Altınb uken, Saswat Anand, Mihai Christodorescu, Katja Gr unwedel, Alexa Koenings, Sai Naidu, Anurag Pathak, Marc Rasi, Fredde Ribeiro, et al. Natural language outlines for code: Literate programming in the llm era. arXiv preprint arXiv:2408.04820, 2024. (Cited on pg. 74) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke ZettlearXiv moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. preprint arXiv:2301.12652, 2023. (Cited on pg. 36) Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023. (Cited on pg. 28, 72) Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. Learning loop invariants for program veriﬁcation. Advances in Neural Information Processing Systems, 31, 2018. (Cited on pg. 73) Mohammed Latif Siddiq and Joanna C. S. Santos. Securityeval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques. page 2933, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450394574. (Cited on pg. 71) Alvaro Silva, Alexandra Mendes, and Joao Ferreira. Leveraging large language models to boost dafnys developers productivity. arXiv preprint arXiv:2401.00963, 2024a. (Cited on pg. 76) 61 Andre Silva, Nuno Saavedra, and Martin Monperrus. Gitbug-java: reproducible benchmark of recent java bugs. In Proceedings of the 21st International Conference on Mining Software Repositories, pages 118122, 2024b. (Cited on pg. 74) Shreya Singhal and Viraj Kumar. Creating thorough tests for ai-generated code is hard. In Proceedings of the 16th Annual ACM India Compute Conference, COMPUTE 23, page 108111, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400708404. doi: 10.1145/3627217.3627238. URL https://doi.org/10.1145/3627217.3627238. (Cited on pg. 39) Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Deﬁning and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. (Cited on pg. 31) Harry Sneed. Extracting business logic from existing cobol programs as basis for redevelopment. In Proceedings 9th International Workshop on Program Comprehension. IWPC 2001, pages 167175. IEEE, 2001. (Cited on pg. 8) Harry Sneed. Migrating from cobol to java. In 2010 IEEE International Conference on Software Maintenance, pages 17. IEEE, 2010. (Cited on pg. 8) Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in lean. arXiv preprint arXiv:2404.12534, 2024. (Cited on pg. 76) Benjamin Steenhoek, Hongyang Gao, and Wei Le. Dataﬂow analysis-inspired deep learning for efﬁcient vulnerability detection. arXiv preprint arXiv:2212.08108, 2023. (Cited on pg. 73) Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza Sanjida Alam, Earl Barr, and Wei Le. comprehensive study of the capabilities of large language models for vulnerability detection. arXiv preprint arXiv:2403.17218, 2024. (Cited on pg. 73) Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal. Regal: refactoring programs to discover generalizable abstractions. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. (Cited on pg. 18) Chia-Yi Su and Collin McMillan. Distilled gpt for source code summarization. Automated Software Engineering, 31(1):22, 2024. (Cited on pg. 74) Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, et al. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. arXiv preprint arXiv:2407.12883, 2024. (Cited on pg. 20) Chuyue Sun, Ying Sheng, Oded Padon, and Clark Barrett. Clover: Closed-loop veriﬁable code generation. In International Symposium on AI Veriﬁcation, pages 134155. Springer, 2024a. (Cited on pg. 14, 76) Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. Evaluating large language models on controlled generation tasks, 2023. URL https://arxiv.org/abs/2310.14542. (Cited on pg. 70) Tao Sun, Jian Xu, Yuanpeng Li, Zhao Yan, Ge Zhang, Lintao Xie, Lu Geng, Zheng Wang, Yueyan Chen, Qin Lin, et al. Bitsai-cr: Automated code review via llm in practice. arXiv preprint arXiv:2501.15134, 2025. (Cited on pg. 74) 62 Weisong Sun, Yun Miao, Yuekang Li, Hongyu Zhang, Chunrong Fang, Yi Liu, Gelei Deng, Yang Liu, and Zhenyu Chen. Source code summarization in the era of large language models. arXiv preprint arXiv:2407.07959, 2024b. (Cited on pg. 40, 74) Yu Sun, Xiaolong Wang, Liu Zhuang, John Miller, Moritz Hardt, and Alexei A. Efros. Test-time training with self-supervision for generalization under distribution shifts. In ICML, 2020. (Cited on pg. 32) Nigar Shaﬁq Surameery and Mohammed Shakor. Use chat gpt to solve programming bugs. International Journal of Information Technology and Computer Engineering, (31):1722, 2023. (Cited on pg. 71) Christian Szegedy. promising path towards autoformalization and general artiﬁcial intelligence. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 2631, 2020, Proceedings 13, pages 320. Springer, 2020. (Cited on pg. 33) Shin Hwei Tan, Jooyong Yi, Sergey Mechtaev, Abhik Roychoudhury, et al. Codeﬂaws: proIn 2017 gramming competition benchmark for evaluating automated program repair tools. IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C), pages 180 182. IEEE, 2017. (Cited on pg. 75) Shin Hwei Tan, Zhen Dong, Xiang Gao, and Abhik Roychoudhury. Repairing crashes in android apps. In Proceedings of the 40th International Conference on Software Engineering, pages 187198, 2018. (Cited on pg. 74) Hao Tang, Keya Hu, Jin Zhou, Si Cheng Zhong, Wei-Long Zheng, Xujie Si, and Kevin Ellis. Code repair with llms gives an exploration-exploitation tradeoff. Advances in Neural Information Processing Systems, 37:117954117996, 2025. (Cited on pg. 75) Tom Taulli. it https://www.forbes.com/sites/tomtaulli/2020/07/13/cobol-language-call-it-a-comeback/. (Cited on pg. 8) comeback?, language: Cobol 2020. URL Call Jul Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. (Cited on pg. 31) Terrateam. Using llms to generate terraform code. https://terrateam.io/blog/using-llms-togenerate-terraform-code/, 2024. (Cited on pg. 12) The Coq Development reference manual https://coq.inria.fr/doc/V8.19.0/refman, 2024. (Cited on pg. 13, 76) The Coq Team. release 8.19.0. Angelica Tinga, Diane Cleij, Reinier Jansen, Sander van der Kint, and Nicole van Nes. Human machine interface design for continuous support of mode awareness during automated driving: An online simulation. Transportation research part F: trafﬁc psychology and behaviour, 87:102119, 2022. (Cited on pg. 18) David Tomassi, Naji Dmeiri, Yichen Wang, Antara Bhowmick, Yen-Chuan Liu, Premkumar Devanbu, Bogdan Vasilescu, and Cindy Rubio-Gonzalez. Bugswarm: Mining and continuously growing dataset of reproducible failures and ﬁxes. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pages 339349. IEEE, 2019. (Cited on pg. 75) 63 Benjamin Trent and Ao Li. Concurrency bugs in lucene: How to ﬁx optimistic concurrency failures. https://www.elastic.co/search-labs/blog/optimistic-concurrency-lucene-debugging, 2025. (Cited on pg. 9) Christoph Treude and Marco Gerosa. How developers interact with ai: taxonomy of humanai collaboration in software engineering. arXiv preprint arXiv:2501.08774, 2025. (Cited on pg. 6, 18) Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. (Cited on pg. 29) Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. ToIn 2021 IEEE/ACM 43rd International Conference on wards automating code review activities. Software Engineering (ICSE), pages 163174. IEEE, 2021. (Cited on pg. 74) Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and Gabriele Bavota. Using pre-trained models to boost code review automation. In Proceedings of the 44th international conference on software engineering, pages 22912302, 2022. (Cited on pg. 74) Marcel Ullrich and Sebastian Hack. Synthesis of sorting kernels. In Proceedings of the 23rd ACM/IEEE International Symposium on Code Generation and Optimization, CGO 25, page 114, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400712753. doi: 10.1145/3696443.3708954. URL https://doi.org/10.1145/3696443.3708954. (Cited on pg. 27) Saiteja Utpala, Alex Gu, and Pin Yu Chen. Language agnostic code embeddings. arXiv preprint arXiv:2310.16803, 2023. (Cited on pg. 20, 36) Alexandre Verdet, Mohammad Hamdaqa, Leuson Da Silva, and Foutse Khomh. Exploring security practices in infrastructure as code: An empirical study, 2023. URL https://arxiv.org/abs/2308.03952. (Cited on pg. 75) Mark Vero, Niels undler, Victor Chibotaru, Veselin Raychev, Maximilian Baader, Nikola Jovanovic, Jingxuan He, and Martin Vechev. Baxbench: Can llms generate correct and secure backends?, 2025. URL https://arxiv.org/abs/2502.11844. (Cited on pg. 71) Sanidhya Vijayvargiya, Xuhui Zhou, Akhila Yerukola, Maarten Sap, and Graham NeuURL Interactive agents to overcome ambiguity in software engineering, 2025. big. https://arxiv.org/abs/2502.13069. (Cited on pg. 34, 71) Vasudev Vikram, Caroline Lemieux, Joshua Sunshine, and Rohan Padhye. Can large language models write good property-based tests? arXiv preprint arXiv:2307.04346, 2023. (Cited on pg. 72) Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, et al. Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large language models. arXiv preprint arXiv:2408.01605, 2024a. (Cited on pg. 71) Yao Wan, Zhangqian Bi, Yang He, Jianguo Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin, and Philip Yu. Deep learning for code intelligence: Survey, benchmark and toolkit. ACM Computing Surveys, 2024b. (Cited on pg. 4) Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie, and Xiangyu Zhang. Llmdfa: Analyzing dataﬂow in code with large language models, 2024a. URL https://arxiv.org/abs/2402.10754. (Cited on pg. 73) 64 Chong Wang, Jianan Liu, Xin Peng, Yang Liu, and Yiling Lou. Boosting static resource leak detection via llm-based resource-oriented intention inference. arXiv preprint arXiv:2311.04448, 2023. (Cited on pg. 73) Chong Wang, Jian Zhang, Yiling Lou, Mingwei Liu, Weisong Sun, Yang Liu, and Xin Peng. Tiger: generating-then-ranking framework for practical python type inference. arXiv preprint arXiv:2407.02095, 2024b. (Cited on pg. 74) Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. Software testing with large language models: Survey, landscape, and vision. IEEE Transactions on Software Engineering, 2024c. (Cited on pg. 4) Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. comprehensive survey of continual IEEE Transactions on Pattern Analysis and Machine learning: theory, method and application. Intelligence, 2024d. (Cited on pg. 24) Ruotong Wang, Ruijia Cheng, Denae Ford, and Thomas Zimmermann. Investigating and designing for trust in ai-powered code generation tools. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, page 14751493, New York, NY, USA, 2024e. Association for Computing Machinery. ISBN 9798400704505. doi: 10.1145/3630106.3658984. URL https://doi.org/10.1145/3630106.3658984. (Cited on pg. 18) Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. Oop: Objectoriented programming evaluation benchmark for large language models, 2024f. URL https://arxiv.org/abs/2401.06628. (Cited on pg. 70) Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024g. URL https://arxiv.org/abs/2407.16741. (Cited on pg. 15, 74) Yanlin Wang, Tianyue Jiang, Mingwei Liu, Jiachi Chen, and Zibin Zheng. Beyond functional correctness: Investigating coding style inconsistencies in large language models. arXiv preprint arXiv:2407.00456, 2024h. (Cited on pg. 17) Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank Xu, Yiqing Xie, Graham Neubig, arXiv preprint and Daniel Fried. Coderag-bench: Can retrieval augment code generation? arXiv:2406.14497, 2024i. (Cited on pg. 20) Jiayi Wei, Greg Durrett, and Isil Dillig. Typet5: Seq2seq type inference using static analysis. arXiv preprint arXiv:2303.09564, 2023a. (Cited on pg. 74) Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. Copiloting the copilots: Fusing large language models with completion engines for automated program repair. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 172184, 2023b. (Cited on pg. 39) Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, 2025. URL https://arxiv.org/abs/2502.18449. (Cited on pg. 30, 31) 65 Justin Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. Examining the use and impact of an ai code assistant on developer productivity and experience in the enterprise. arXiv preprint arXiv:2412.06603, 2024. (Cited on pg. 17) Sean Welleck and Rahul Saha. Llmstep: Llm proofstep suggestions in lean. arXiv preprint arXiv:2310.18457, 2023. (Cited on pg. 76) Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi Qi, Jack Phan, Qijin Tay, Constance Tan, Fiona Wee, Jodie Ethelda Tan, Yuheng Yieh, et al. Bugsinpy: database of existing bugs in python programs to enable controlled testing and debugging studies. In Proceedings of the 28th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering, pages 15561560, 2020. (Cited on pg. 74) Man-Fai Wong, Shangxin Guo, Ching-Nam Hang, Siu-Wai Ho, and Chee-Wei Tan. Natural language generation and understanding of big code for ai-assisted programming: review. Entropy, 25(6):888, 2023. (Cited on pg. 4) Yuk Wah Wong and Raymond Mooney. Learning for semantic parsing with statistical machine translation. In Robert C. Moore, Jeff Bilmes, Jennifer Chu-Carroll, and Mark Sanderson, editors, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 439446, New York City, USA, June 2006. Association for Computational Linguistics. (Cited on pg. 70) Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: survey. arXiv preprint arXiv:2402.01364, 2024. (Cited on pg. 24, 32) Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024a. (Cited on pg. 74, 75) Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. Fuzz4all: Universal fuzzing with large language models. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113, 2024b. (Cited on pg. 73) Tao Xiao, Christoph Treude, Hideaki Hata, and Kenichi Matsumoto. Devgpt: Studying developerchatgpt conversations. In Proceedings of the 21st International Conference on Mining Software Repositories, pages 227230, 2024. (Cited on pg. 71) Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable repository-level coding environment construction with sandbox testing. arXiv preprint arXiv:2503.07358, 2025. (Cited on pg. 30) Cheng Xu, Shuhao Guan, Derek Greene, Kechadi, et al. Benchmark data contamination of large language models: survey. arXiv preprint arXiv:2406.04244, 2024a. (Cited on pg. 14, 29) Zhixing Xu, Shengjian Guo, Oksana Tkachuk, Saeed Nejati, Niloofar Razavi, and George Argyros. Cloud resource protection via automated security property reasoning. In Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, pages 21702175, 2024b. (Cited on pg. 76) Hao Yan, Thomas Latoza, and Ziyu Yao. Intelliexplain: Enhancing conversational code generation for non-professional programmers. arXiv preprint arXiv:2405.10250, 2024. (Cited on pg. 71) 66 Chenyuan Yang, Zijie Zhao, and Lingming Zhang. Kernelgpt: Enhanced kernel fuzzing via large language models. arXiv preprint arXiv:2401.00563, 2023a. (Cited on pg. 73) Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Jianan Yao, Weidong Cui, Yeyun Gong, Chris Hawblitzel, Shuvendu Lahiri, Jacob Lorch, Shuai Lu, et al. Autoverus: Automated proof generation for rust code. arXiv preprint arXiv:2409.13082, 2024a. (Cited on pg. 76) Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, et al. Code to think, think to code: survey on code-enhanced reasoning and reasoning-driven code intelligence in llms. arXiv preprint arXiv:2502.19411, 2025. (Cited on pg. 75) John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Oﬁr Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://arxiv.org/abs/2405.15793. (Cited on pg. 15, 34, 74) John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, et al. Swe-bench multimodal: Do ai systems generalize to visual software domains? arXiv preprint arXiv:2410.03859, 2024c. (Cited on pg. 20) Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Animashree Anandkumar. Leandojo: Theorem proving with retrievalaugmented language models. Advances in Neural Information Processing Systems, 36:2157321612, 2023b. (Cited on pg. 71) Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn Song. Formal mathematical reasoning: new frontier in ai. arXiv preprint arXiv:2412.16075, 2024d. (Cited on pg. 4, 76) Zuoning Yin, Xiao Ma, Jing Zheng, Yuanyuan Zhou, Lakshmi N. Bairavasundaram, and Shankar Pasupathy. An empirical study on conﬁguration errors in commercial and open source systems. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles, SOSP 11, page 159172, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450309776. (Cited on pg. 75) Shiwen Yu, Ting Wang, and Ji Wang. Loop invariant inference through smt solving enhanced In Proceedings of the 32nd ACM SIGSOFT International Symposium on reinforcement learning. Software Testing and Analysis, pages 175187, 2023. (Cited on pg. 74) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task, 2019. (Cited on pg. 70) Karen Zee, Viktor Kuncak, and Martin Rinard. Full functional veriﬁcation of linked data structures. ACM SIGPLAN Notices, 43(6):349361, 2008. (Cited on pg. 13) Luke S. Zettlemoyer and Michael Collins. Learning to map sentences form: https://arxiv.org/abs/1207.1420. (Cited on pg. 70) Structured classiﬁcation with probabilistic categorial grammars, 2012. to logical URL 67 Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 769787, Toronto, Canada, July 2023a. Association for Computational Linguistics. (Cited on pg. 75) Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, and Lei Li. Algo: Synthesizing algorithmic programs with generated oracle veriﬁers. arXiv preprint arXiv:2305.14591, 2023b. (Cited on pg. 73) Quanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen. survey of learning-based automated program repair. ACM Transactions on Software Engineering and Methodology, 33(2):169, 2023c. (Cited on pg. 4) Quanjun Zhang, Chunrong Fang, Yang Xie, YuXiang Ma, Weisong Sun, Yun Yang, and Zhenyu Chen. systematic literature review on large language models for automated program repair. arXiv preprint arXiv:2405.01466, 2024a. (Cited on pg. 75) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. (Cited on pg. 31) Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: AuIn Proceedings of the 33rd ACM SIGSOFT International Symtonomous program improvement. posium on Software Testing and Analysis, pages 15921604, 2024b. (Cited on pg. 74) Chenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi Li, and Liang Zhao. Deepep: an efﬁcient expert-parallel communication library. https://github.com/deepseek-ai/DeepEP, 2025. (Cited on pg. 9) Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias Galle, and Alexander Rush. Commit0: Library generation from scratch. arXiv preprint arXiv:2412.01769, 2024. (Cited on pg. 5, 14) Yu Zhao, Lina Gong, Haoxiang Zhang, Yaoshen Yu, and Zhiqiu Huang. How to get better embeddings with code pre-trained models? an empirical study. arXiv preprint arXiv:2311.08066, 2023. (Cited on pg. 36) Dan Zheng and Koushik Sen. Dynamic inference of likely symbolic tensor shapes in python machine learning programs. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice, pages 147156, 2024. (Cited on pg. 73) Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. survey of large language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372, 2023. (Cited on pg. 4) Li Zhong, Zilong Wang, and Jingbo Shang. Debug like human: large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024a. (Cited on pg. 75) Li Zhong, Zilong Wang, and Jingbo Shang. Ldb: large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024b. (Cited on pg. 15) Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. 2017. (Cited on pg. 70) Shuyan Zhou, Uri Alon, Frank Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig. Docprompting: Generating code by retrieving the docs. arXiv preprint arXiv: 2207.05987, 2022. (Cited on pg. 71) Shuyan Zhou, Uri Alon, Sumit Agarwal, and Graham Neubig. Codebertscore: Evaluating code generation with pretrained models of code. arXiv preprint arXiv:2302.05527, 2023. (Cited on pg. 31) Yaqin Zhou, Shangqing Liu, J. Siow, Xiaoning Du, and Yang Liu. Devign: Effective vulnerability In identiﬁcation by learning comprehensive program semantics via graph neural networks. Neural Information Processing Systems, 2019. (Cited on pg. 73) Chang Zhu, Ziyang Li, Anton Xue, Ati Priya Bajaj, Wil Gibbs, Yibo Liu, Rajeev Alur, Tiffany Bao, Hanjun Dai, Adam Doupe, Mayur Naik, Yan Shoshitaishvili, Ruoyu Wang, and Aravind In 33rd Machiry. TYGR: Type inference on stripped binaries using graph neural networks. USENIX Security Symposium (USENIX Security 24), 2024. ISBN 978-1-939133-44-1. (Cited on pg. 74) Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. (Cited on pg. 4) Albert Ziegler, Eirini Kalliamvakou, Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. Measuring github copilots impact on productivity. Communications of the ACM, 67(3):5463, 2024. (Cited on pg. 29) Amit Zohar and Lior Wolf. Automatic program synthesis of long programs with learned garbage collector. Advances in neural information processing systems, 31, 2018. (Cited on pg. 36) Weiqin Zou, Jifeng Xuan, Xiaoyuan Xie, Zhenyu Chen, and Baowen Xu. How does code style inconsistency affect pull request integration? an exploratory study on 117 github projects. Empirical Software Engineering, 24:38713903, 2019. (Cited on pg. 17) 69 Survey of Related Work: Tasks in AI Software Engineering In this section, we brieﬂy survey some of the relevant works for each of the tasks we mention in Sec. 2. These works are by no means complete, and we encourage the reader to check out the survey works mentioned in the introduction and in this section for further references. A.1 Code Generation Code Completion: Completion typically happens in conjunction with live programming or within an IDE, helping developers write code faster by suggesting relevant continuations. Traditional code completion systems rely heavily on syntactic and type-aware models (e.g., AST-based models), but recent advances leverage LLMs trained on code corpora to offer semantically rich and context-aware suggestions, naturally following the next-token prediction task in language modeling (Radford et al., 2019). Tools like GitHub Copilot and Codex exemplify this trend (Chen et al., 2021a), and are followed by commercial tools such as Cursor4 and Tabnine5. Recent advances in context-aware (Agrawal et al., 2023), grammar-aligned (Park et al., 2024), and constraint-based decoding (Sun et al., 2023) have improved the quality of local completions, particularly for shorter code snippets. For longer code snippets, the typical task formulation is method implementation synthesis given function signature. This setup is commonly evaluated using benchmarks such as MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021a). Natural Language to Code Generation: Translating natural language into code has long been central challenge in AI for programming. Early attempts at code generation involved semantic parsing (Zettlemoyer and Collins, 2012; Wong and Mooney, 2006), where natural language is translated into logical forms or domain-speciﬁc languages. prominent example is SQL query synthesis from natural language questions, as seen in systems like Seq2SQL (Zhong et al., 2017) and Spider (Yu et al., 2019), where the target language is constrained, small, and domainspeciﬁc. Recent work demonstrates that large language models (LLMs) can generalize to generalpurpose programming languages, enabling the generation of larger and more complex code snippets (OpenAI, 2023b). When applied to code completion, users often begin with natural language instructions in the form of comments, which LLMs use as context for code synthesis. Beyond function-level code generation (Austin et al., 2021; Chen et al., 2021a), recent work has extended to class-level generation (Du et al., 2023), which targets classes in object-oriented programming, and even project-level code generation (Cao et al., 2024; Wang et al., 2024f), which involves generating or completing entire multi-ﬁle codebases. Multimodal Code Generation: While text can describe most cases of code generation, certain instructions are better deﬁned visually. For example, in graphics applications, visual context such as trajectory or 3D model is essential to synthesize the correct code. Demonstrations of GPT-4s multi-modal capabilities have shown that models can generate functional webpage code directly from paper sketches, translating visual layouts into HTML and CSS (OpenAI, 2023a). LogoMotion (Liu et al., 2025b) explores visually grounded code synthesis for animations and motion graphics in JavaScript. The system leverages vision-language models (VLMs) to incorporate both visual inputs and user instructions, enabling code generation that aligns with spatial and temporal visual cues. Other works, such as SynthesizeCAD (Nandi et al., 2020) and SGP-Bench (Qiu et al., 4https://www.cursor.so 5https://www.tabnine.com 2024), explore how LLMs can interface with visual and 3D modalities by generating code in languages like SVG and CAD. Code Generation in Low-Resource Languages: As discussed in Sec. 3.7, one major challenge is writing code in low-adoption general purposed language and domain speciﬁc languages (DSLs). Benchmarks for this include MultiPL-E (Cassano et al., 2023), McEval (Chai et al., 2024), and VerilogEval (Liu et al., 2023b). popular method to improve performance is to train on manually curated and processing data in low-resource languages such as Coq (Florath, 2024) and Verilog (Pei et al., 2024). Another line of work aims to achieve transfer between different low-resource languages (Paul et al., 2024; Cassano et al., 2024; Orlanski et al., 2023). Finally, since the lack of data is large bottleneck, another popular direction is using relevant retrievals such as useful functions and library documentation (Yang et al., 2023b; Zhou et al., 2022; Yang et al., 2023b). For recent survey of code generation for low-resource languages and DSLs, see (Joel et al., 2024). Security Concerns Surrounding Code Generation: Despite the growing power of LLMs for code generation, their outputs often remain insecure, incorrect, or misaligned with user intent. For instance, BaxBench (Vero et al., 2025) evaluates LLMs on generating secure and correct back-ends, revealing that while the average functional correctness is already modest ( 60%), the rate of secure outputs is even lower (< 35%). To better understand and quantify these limitations, several benchmarks and evaluation suites have been proposed. SecurityEval (Siddiq and Santos, 2022), SafeCoder (He et al., 2024), CodeLMSec (Hajipour et al., 2023), CWEval (Peng et al., 2025), and CyberSecEval (Bhatt et al., 2023; Wan et al., 2024a) each provide distinct lenses on evaluating vulnerabilities, unsafe API usage, or compliance with common weakness enumerations (CWEs). In response, several approaches introduce human-in-the-loop guardrails, where developers can interactively guide, inspect, or constrain the generation process. Dynex (Ma et al., 2025a), for instance, supports dynamic, step-wise code synthesis with user feedback, enabling real-time correction and iterative reﬁnement before errors can accumulate. Human Interaction in Code Generation: Modern code LLMs typically support interactive code generation through conversational interfaces. Champa et al. (2024) conducted quan- (Xiao et al., titative analysis of developer-ChatGPT interactions using the DevGPT dataset 2024), examining how the quality of the initial prompt inﬂuences conversation length. Code LLMs can be further optimized for various interactive scenarios, including debugging environments (Surameery and Shakor, 2023), educational settings (Kazemitabaar et al., 2023a,b; Prather et al., 2023; Sheese et al., 2024), and use by non-professional programmers Yan et al. (2024). Beyond human-driven interactions in chat-based setups, more advanced code generation systems such as coding agents can proactively ask clarifying questions (Vijayvargiya et al., 2025) or generate test cases for users to validate (Lahiri et al., 2022; Fakhoury et al., 2024) before generating the actual code, helping to resolve ambiguities. A.2 Code Transformation Code Refactoring: Code refactoring aims to simplify and remove repetitions in complex repositories without altering high-level program intent. While there have been traditional methods (Pailoor et al., 2024) that refactor data structures, Aider AI introduces refactoring benchmark6 evaluating LLMs ability to output long chunks of code that simplify complex programs without changing its behavior. More recently, RefactorBench (Gautam et al., 2024) introduced 6https://github.com/Aider-AI/refactor-benchmark 71 more complex benchmark with natural language refactor requests, as well as an LLM agent that can perform refactoring. Code Migration: Compared to code refactoring, code migration typically refers to mid-scale modiﬁcations that affect programs interface, dependencies, or underlying architecture. Common examples include switching the back-end database from MySQL to PostgreSQL, migrating machine learning model from TensorFlow to PyTorch, or upgrading the Java version from legacy Java 8 to more modern Java 17. While recent work has introduced benchmark designed to evaluate library migrations (Islam et al., 2023), works at Google (Nikolov et al., 2025) and Amazon (Omidvar Tehrani and Anubhai, 2024) have explored LLM-driven solutions for simple but vast migrations. Googles system identiﬁes locations for changes, generates edits with LLMs ﬁne-tuned on internal code, and automatically validates changes through compilation and test execution. Code Translation (Transpilation): Moving beyond code migration, transpilation involves largescale transformation of programs underlying programming language. Transpilation serves not only to modernize outdated codebases but also to eliminate classes of safety issues inherent to older languages. particularly active area of research involves transpiling C-based systems to Rust, systems-level language that offers strong memory and concurrency safety guarantees. This direction has garnered attention, including from the U.S. Department of Defense7, which maintains critical infrastructure built on aging code. An end-to-end LLM-based approach, such as Flourine (Eniser et al., 2024), has been proposed for real-world code translation, but it has achieved only limited success due to frequent compilation errors. Recent efforts like Syzygy (Shetty et al., 2024), C2SaferRust (Nitin et al., 2025), and AlphaTrans (Ibrahimzada et al., 2024) have shown the potential for hybrid approaches combining LLMs with traditional program analysis techniques. However, some signiﬁcant challenges remain, as identiﬁed by Li et al. (2025b), including ensuring correctness in large codebases while maintaining desirable attributes such as speed, reduced vulnerabilities, and idiomaticity. Speciﬁcally, We anticipate that the techniques discussed in Section A.3 may help address these remaining challenges. Code Optimization: Certain refactoring or transpilation tasks are speciﬁcally aimed at optimizing code performance. Prior work has explored the use of LLMs for optimizing standalone programs, such as PIE (Shypula et al., 2023), which targets C++ functions, and AlphaDev (Mankowitz et al., 2023a), which discovers more efﬁcient sorting algorithms at the assembly level. These tasks are particularly challenging due to the vast search space of possible code transformations. More recently, KernelBench (Ouyang et al., 2024) introduced benchmark focused on optimizing machine learning models written in high-level PyTorch code into low-level, high-performance CUDA GPU kernels. For broader overview of language models applied to code optimization, see the survey by Gong et al. (2025). A.3 Software Testing and Program Analysis Short-horizon Testing: For short-horizon testing such as unit tests (Lemieux et al., 2023) and property-based tests (Vikram et al., 2023), LLMs are employed to automatically generate targeted test cases (Li and Yuan, 2024; undler et al., 2025), and even hill-climb on code coverage to improve test effectiveness (Ryan et al., 2024). At the granularity of individual functions, LLMgenerated tests have also been employed to support downstream tasks such as ﬁltering implemen7https://www.darpa.mil/news/2024/memory-safety-vulnerabilities 72 tations based on behavioral correctness (Chen et al., 2022; Zhang et al., 2023b), as well as assisting in program debugging by surfacing inputs that expose incorrect behavior (Chen et al., 2025). Long-horizon Testing: Long-horizon testing involves evaluating system behavior across extended executions, complex interactions, or multiple components, potentially embedded within CI/CD (Continuous Integration or Delivery) pipeline. Fuzzing (Miller et al., 1990) is longhorizon testing approach that continuously generates novel random input. Recent works such as Fuzz4All (Xia et al., 2024b), KernelGPT (Yang et al., 2023a), and OSS-Fuzz (Liu et al., 2023a; Chang et al., 2024) have shown that LLMs can signiﬁcantly improve effectiveness through better input generation and exploration strategies. Speciﬁcatlly, OSS-Fuzz-Gen (Liu et al., 2024b) employs diverse LLMs for fuzzing harness generation, helping to ﬁnd novel and complex crashing interactions. Static Analysis for Vulnerability Detection: Vulnerability Detection refers to the task of identifying weaknesses or ﬂaws in software code that could be exploited to compromise the systems security, stability, or correctness. wide range of prior work leverages machine learning models such as Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) to detect software vulnerabilities (Zhou et al., 2019; Chakraborty et al., 2020; Dinella et al., 2020; Hin et al., 2022; Li et al., 2021). While some recent methods pre-train or ﬁne-tune LLMs on code-speciﬁc datasets (Fu and Tantithamthavorn, 2022; Steenhoek et al., 2023; Cheng et al., 2022) to improve vulnerability classiﬁcation, several studies have highlighted the limitations of LLMs in real-world software (Steenhoek et al., 2024; Ding et al., 2024a; Khare et al., 2023). To combat such limitations, works like Li et al. (2024a), IRIS (Li et al., 2024f), LLMDFA (Wang et al., 2024a), and InferROI (Wang et al., 2023) explored augmenting static analysis tools (e.g., CodeQL) with LLMs for taint and resource leak analyses. More recently, BigSleep (2024) demonstrated the potential of using LLMs at much bigger scale by ﬁnding real SQLite vulnerability through exploratory variant analysis. Specialized Program Analysis: Beyond long-running analysis to identify vulnerabilities, several traditional program analyses have struggled to scale in practice despite their theoretical promise. For instance, inferring program invariants (properties deemed to always be true at program point) has been challenging with traditional symbolic methods such as Daikon (Ernst et al., 2007; Padon et al., 2016) while being valuable for exposing bugs (Hangal and Lam, 2002) and aiding software evolution (Ernst et al., 1999). Similarly, type inference for dynamically typed languages suffers from coverage limitations of rule-based approaches and requires specialized tools like ShapeIt (Zheng and Sen, 2024) for domain-speciﬁc challenges such as inferring symbolic tensor shapes. Speciﬁcation Inference: Speciﬁcation inference is the task of automatically recovering formal description of programs expected behavior, including pre-conditions, post-conditions, or invariants. The availability of speciﬁcation is at the core of establishing trust (Roychoudhury et al., 2025b), and existing works (Dinella et al., 2024b; Ruan et al., 2024) have shown that LLMs can help the inference of such speciﬁcations. For instance, Dinella et al. (2024a) presents program structure aware technique for synthesizing pre-conditions for arbitrary code snippets, and have established dataset of 18K LLM generated pre-conditions on real Java projects. Invariant Inference: As subtask of speciﬁcation inference, invariant inference aims at inferring loop, function, or class invariants, which are greatly helpful in automatic program veriﬁcation. There have been several LLM-based approaches for invariant identiﬁcation. They enhance traditional approaches through structured representations (Si et al., 2018), LLM-based prompting (Kamath et al., 2023; Pei et al., 2023) and re-ranking (Chakraborty et al., 2023), and 73 reinforcement learning (Yu et al., 2023). Similarly, works have used sequence-to-sequence models (Wei et al., 2023a), few-shot LLM approaches like TypeGen (Peng et al., 2023), and generatethen-rank methods like TIGER (Wang et al., 2024b) for type inference. Consequently, we observe new benchmarks emerging in the space such as LIG-MM (Liu et al., 2024a) for loop-invariant detection. Binary Analysis: While the aforementioned tasks primarily focus on human-readable programming languages, many can also be extended to operate on compiled machine code, or binaries. One prominent example is binary type inference, which aims to recover high-level type information from low-level binary code. It has seen signiﬁcant improvements with deep learning models and LLMs (Pei et al., 2021; Zhu et al., 2024). These advancements, alongside other LLM-based analyses, have enhanced the capabilities of decompilers, enabling them to synthesize humanreadable code from binaries (Liu et al., 2025a). Beyond decompilation, LLMs have also been applied to detect security vulnerabilities in binaries (Liu et al., 2023c) and to generate semantic summaries that capture the high-level intent of binary code (Jin et al., 2023). A.4 Software Maintenance Code Navigation: Code navigation refers to the task of locating speciﬁc position within code repository based on either natural language description (Liu et al., 2024e) or programmatic speciﬁcation (Avgustinov et al., 2016). Common use cases include identifying where particular functionality is implemented, tracing the origin of user input that leads to vulnerability, or locating relevant ﬁles when starting work on new feature. This capability underpins many downstream tasks such as software testing, vulnerability detection, program repair, and code question answering. Code navigation or code search modules are integral components of modern code agents (Yang et al., 2024b; Bouzenia et al., 2024; Xia et al., 2024a), often implemented using ﬁnd commands, embedding-based similarity search, or query-based tools like CodeQL and Semgrep. Code Documentation and Summarization: Several works have used LLMs for code summarization invoking techniques like prompting (Sun et al., 2024b; Su and McMillan, 2024; Haldar and Hockenmaier, 2024; Ahmed et al., 2024b). RepoAgent (Luo et al., 2024) is framework that analyzes global contextual relationships in source code to generate ﬁne-grained documentation. Shi et al. (2024) show that LMs are capable of generating good natural language outlines text descriptions alongside code to partition it into semantically coherent sections. One challenge is that the evaluation of this task is very tricky: the academic community currently lacks datasets and benchmarks that contain good documentation and the automatic evaluation metrics do not align well with human metrics (Diggs et al., 2024). Pull Request (PR) Review: In industry, autonomous software agents such as OpenHands (Wang et al., 2024g) and Devin have been able to automatically review and even ﬁx PRs. At ByteDance, BitsAI-CR (Sun et al., 2025) is code review system that identiﬁes issues based on manually crafted taxonomy of review rules. In the academic community, there have been several works studying the ability of AI systems to automatically review PRs (Tufano et al., 2021, 2022; Li et al., 2022b, 2024b). Recently, AutoCodeRover (Zhang et al., 2024b) combines LLMs with code search to automatically ﬁx GitHub issues. Program Repair: Automated program repair has had long history, with many benchmarks covering different scopes and languages. These include DroixBench (Tan et al., 2018) for android apps; Defects4J (Just et al., 2014), GitBug-Java (Silva et al., 2024b), and growingBugs (Jiang et al., 2021, 2022a,b) for real-world Java; Bugsinpy (Widyasari et al., 2020) for Python; BugSwarm 74 (Tomassi et al., 2019) for multilingual; DebugBench (Hu et al., 2024), LiveCodeBench (Jain et al., 2024b), and Codeﬂaws (Tan et al., 2017) for LeetCode-style problems; and many more. Historically, there have been many techniques for this task, including heuristic-based APR (using genetic programming to explore the search space of the correct patch), constraint-based APR (treating repair as constraint-solving task), pattern-based APR (apply expert hand-crafted repair templates), and learning-based APR (using language models) (Zhang et al., 2024a). More recently, with LLMs, there have been agent-based approaches such as FixAgent (Lee et al., 2024) using agents specializing in different aspects of debugging, and RepairAgent (Bouzenia et al., 2024) that invokes suitable tools. On the other hand, Agentless (Xia et al., 2024a) uses three-phase process of localization, repair, and patch validation. Finally, program repair has also been used as tool to improve code generation, where error messages and incorrect test cases are fed back into the model to improve code generation (Madaan et al., 2023; Chen et al., 2024; Zhang et al., 2023a; Olausson et al., 2024; Zhong et al., 2024a; Tang et al., 2025). This is also known as self-repair or self-debugging. For much more comprehensive survey of automated program repair, we recommend the reader check out this website8. Code Understanding and Question Answering: Code understanding with language models has been studied for many years. In earlier days, researchers used the CodeXGLUE (Lu et al., 2021) benchmark containing tasks such as clone detection, code search, code summarization, and so on. Nam et al. (2024) create an IDE plugin containing features that help users understand code through explaining highlighted sections of code and explaining domain-speciﬁc code. Yang et al. (2025) present survey touching on reasoning-enhanced code intelligence. A.5 Scaffolding and Meta-Code Beyond code generation, the broader software engineering ecosystem includes DevOps workﬂows, CI/CD pipelines, and Infrastructure-as-Code (IaC). LLMs have shown particular promise in generating, debugging, and explaining CI/CD conﬁgurations (e.g., GitHub Actions, Jenkinsﬁles), assisting with environment setup, test orchestration, and deployment logic. case study at Ericsson (Chaudhary et al., 2024) demonstrates how an LLM-based chatbot can support CI/CD question answering, enabling engineers to better understand and manage deployment pipelines. LLMs are also being explored for automated testing across heterogeneous software environments. ExecutionAgent (Bouzenia and Pradel, 2024) presents language model-driven agent that autonomously installs, conﬁgures, and runs test suites for arbitrary projects. Beyond CI/CD and testing, LLMs are increasingly used to reason about conﬁguration logic and scaffolding code, which is critical but often overlooked layer of modern software systems. For instance, Yin et al. (2011) conducted an empirical study of real-world conﬁguration errors, identifying systemic causes of failure such as external dependencies, inter-parameter violations, and overlooked default parameters. Building on this line of work, Ciri (Lian et al., 2024) conﬁrms the feasibility of using LLMs for conﬁguration validation. Further, in the domain of IaC, an empirical study of 812 open-source Terraform projects found that while access policies are commonly adopted, critical practices like encryption at rest are often neglected (Verdet et al., 2023). This highlights the opportunity for LLMs to assist practitioners in detecting and enforcing security best practices in IaC conﬁgurations. 8https://program-repair.org/ 75 A.6 Formal Veriﬁcation There are variety of programming languages designed with different principles to support formal veriﬁcation. the popular ones include TLA (Lamport, 1994), Coq (The Coq Development Team, 2024), Lean (De Moura et al., 2015), Dafny (Leino, 2010), Isabelle (Nipkow et al., 2002), and Verus (Lattuada et al., 2024). Some of Formal software veriﬁcation has seen few great successes in the last few years: Astree (Cousot et al., 2005) was able to completely verify that Airbus A340s primary ﬂight-control software had no run-time errors, verifying 132,000 lines of code. More recently, formal methods have been applied to verify cryptographic server (Erbsen et al., 2024) and an IoT lightbulb at both hardware and software level (Erbsen et al., 2021). CompCert (Leroy et al., 2016), veriﬁed compiler and seL4 (Klein et al., 2009), veriﬁed microkernel are demonstrations that formal methods could be promising for veriﬁable code. At Amazon, formal methods been used to verify and protect cryptographic software (Goel et al., 2024), cloud resources (Xu et al., 2024b), and authorization (Disselkoen et al., 2025). Notably, SV-COMP (Beyer, 2023) is an annual competition designed to evaluate program veriﬁers using curated benchmark of veriﬁable and Java code. It even includes samples from the Linux Driver Veriﬁcation (LDV) project (Beyer and Petrenko, 2012), aiding the veriﬁcation of Linux kernel device drivers. For more applications, we refer the reader to the survey in Huang et al. (2023). Recently, the ability of LLMs to write formal veriﬁcation code. Benchmarks like DafnyBench (Loughridge et al., 2024) and miniCodeProps (Lohn and Welleck, 2024) were designed to measure the ability of LLMs to write software proofs in Dafny and Lean, respectively. In Dafny, Poesia et al. (2024) use combination of search and prompting to create synthetic dataset of annotations greatly improving performance on DafnyBench. Clover (Sun et al., 2024a) generates code alongside consistency checks (like Dafny annotations), Li et al. (2025c) employ Dafny as an intermediate language to improve code generation, and Misu et al. (2024) explore prompting and retrieval to generate Dafny. In Rust, Verus is popular formal veriﬁcation language, with AutoVerus (Yang et al., 2024a) and AlphaVerus (Aggarwal et al., 2024) generating veriﬁed speciﬁcations and proofs for Rust functions. There are also many IDE plugins designed to help humans to write code in formal languages such as Dafny and Lean such as Silva et al. (2024a), Lean Copilot (Song et al., 2024), and llmstep (Welleck and Saha, 2023). Finally, there is growing interest of work in using formal languages like Lean for mathematical theorem proving, which is covered comprehensively in Li et al. (2024e) and Yang et al. (2024d)."
        }
    ],
    "affiliations": [
        "Cornell University",
        "MIT CSAIL",
        "University of California, Berkeley"
    ]
}