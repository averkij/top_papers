{
    "paper_title": "CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing",
    "authors": [
        "Jianxiong Gao",
        "Yichang Liu",
        "Baofeng Yang",
        "Jianfeng Feng",
        "Yanwei Fu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEG's high temporal resolution and fMRI's deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates a Multi-Modal Fusion Encoder with a diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex audiovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, a comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github.io/CineBrain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 4 9 6 0 . 3 0 5 2 : r CineBrain: Large-Scale Multi-Modal Brain Dataset During Naturalistic"
        },
        {
            "title": "Audiovisual Narrative Processing",
            "content": "Jianxiong Gao, Yichang Liu, Baofeng Yang, Jianfeng Feng, Yanwei Fu Fudan University Figure 1. Overview of CineBrain. To leverage the complementary strengths of fMRI and EEG, CineBrain provides simultaneous audiovisual stimuli to participants while recording their EEG and fMRI signals. Engaging narrative-driven content from the television series The Big Bang Theory is utilized to facilitate the study of complex brain dynamics and multimodal neural decoding."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce CineBrain, the first large-scale dataset featuring simultaneous EEG and fMRI recordings during dynamic audiovisual stimulation. Recognizing the complementary strengths of EEGs high temporal resolution and fMRIs deep-brain spatial coverage, CineBrain provides approximately six hours of narrative-driven content from the popular television series The Big Bang Theory for each of six participants. Building upon this unique dataset, we propose CineSync, an innovative multimodal decoding framework integrates Multi-Modal Fusion Encoder with diffusion-based Neural Latent Decoder. Our approach effectively fuses EEG and fMRI signals, significantly improving the reconstruction quality of complex auEmail: jxgao22@m.fudan.edu.cn, yanweifu@fudan.edu.cn. diovisual stimuli. To facilitate rigorous evaluation, we introduce Cine-Benchmark, comprehensive evaluation protocol that assesses reconstructions across semantic and perceptual dimensions. Experimental results demonstrate that CineSync achieves state-of-the-art video reconstruction performance and highlight our initial success in combining fMRI and EEG for reconstructing both video and audio stimuli. Project Page: https://jianxgao.github. io/CineBrain. 1. Introduction The human brain, often described as the most complex and enigmatic organ, has long intrigued researchers. Functional Magnetic Resonance Imaging (fMRI) and Electroencephalography (EEG) are two widely employed noninvasive techniques to capture neural responses, facilitating the decoding of brain activities evoked by various stimuli. While fMRI measures blood oxygenation changes in the brain induced by neuronal activity, EEG captures electrical signals resulting directly from neuronal firing. Previous research [6, 13, 14, 22, 47] has demonstrated that both modalities effectively decode stimuli encompassing visual, linguistic, and auditory information. offer However, these modalities complementary strengths while also suffering from intrinsic limitations. Specifically, fMRI can probe deep-brain neural activities but is hindered by relatively low temporal resolution, potentially overlooking transient but critical neural dynamics. Conversely, EEG provides superior temporal resolution suitable for capturing rapid neural oscillations, yet its signals predominantly reflect superficial cortical activities. Consequently, decoding continuous neural responses, such as those elicited by audiovisual stimuli, solely through either modality inevitably results in significant informational loss. Motivated by these complementary characteristics, natural question emerges: Can we leverage the high temporal resolution of EEG to compensate for the inherent temporal limitations of fMRI? This paper presents an initial exploration into addressing this challenge by reconstructing video and audio from human brain activity. Currently, no publicly available dataset provides simultaneous EEG and fMRI recordings during temporally dynamic stimulation such as video or audio presentations. To bridge this gap, we introduce the cureated CineBrain, the first large-scale multimodal stimulation dataset that captures synchronized fMRI and EEG signals. Literature [16, 18, 43] highlights that engaging narratives can naturally capture audience attention, sustain neural engagement, and provide deep insights into complex brain dynamics. Additionally, neuroscientific findings [33, 41] emphasize that different regions of the human brain are highly interconnected and interact dynamically during perceptual processing. Motivated by these insights, we designed the CineBrain dataset to incorporate audiovisual stimuli closely resembling real-world experiences, where participants simultaneously watched and listened to dynamic content. Specifically, we selected episodes from the popular humancentric television series The Big Bang Theory, providing approximately six hours of narrative-driven audiovisual content for each of six participants. Recordings were conducted in 3T MRI scanner, with participants wearing customized non-magnetic headphones and EEG caps. As illustrated in Fig. 2, participants 14 exhibited consistent activation patterns in both fMRI and EEG modalities when presented with identical stimuli. Further details about the CineBrain dataset are provided in Sec. 3. Leveraging the CineBrain dataset, we propose an innovative framework called CineSync, aiming to reconstruct 2 both video and audio stimuli from brain signals. We introduce Multi-Modal Fusion Encoder that effectively integrates information from fMRI and EEG. Initially, the encoder applies dual transformer-based architecture to separately encode multi-frame signals from each modality. To ensure accurate and meaningful multimodal feature integration, both modalities are jointly aligned with visual and textual representations using combined contrastive loss. Subsequently, these fused brain representations serve as inputs to our Neural Latent Decoder. To guarantee both semantic coherence and perceptual fidelity in reconstructions, we train diffusion-based Neural Latent Decoder that integrates brain-derived features with latent noise. Given that CineBrain stimuli are human-centric audiovisual materials, reconstructing such complex content is challenging even for advanced generative models. Therefore, selecting robust and suitable evaluation metrics is crucial. To address this need, we introduce the Cine-Benchmark, which comprehensively assesses video reconstructions on semantic and perceptual dimensions, as well as audio reconstructions. Experimental results confirm that our approach achieves state-of-the-art performance in brain-signal-based visual reconstruction, particularly by effectively integrating EEG and fMRI modalities to improve the reconstruction quality of temporally dynamic stimuli within the challenging CineBrain dataset. In summary, our contributions are as follows: We introduce CineBrain, For the first time, we provide an initial exploration and validation demonstrating that combining the complementary strengths of fMRI and EEG significantly enhances the reconstruction quality of temporally dynamic stimuli. the first multimodal audiovisual stimulation dataset featuring meticulously paired modalities, including fMRI, EEG, video, audio, and captions. CineBrain provides challenging tasks suitable for diverse experimental setups and fosters research in multimodal brain decoding. We propose CineSync, versatile multimodal brain decoding framework comprising Multi-Modal Fusion Encoder and diffusion-based Neural Latent Decoder. CineSync achieves state-of-the-art video reconstruction performance and can be readily adapted for audio reconstruction tasks. We develop Cine-Benchmark, comprehensive evaluation protocol designed to effectively assess the reconstruction quality of video and audio from brain signals. 2. Related Work 2.1. Neuro-Stimulus Dataset Data is fundamental to deep learning. To facilitate neural decoding experiments, various datasets have been proposed. For visual stimuli, NSD dataset [1] is widely used 2.3. Diffusion Models Diffusion models [20, 35, 38] are powerful generative framework known for their capability to produce highquality images and can also be extended to other data modalities. The fundamental principle involves defining forward diffusion process, in which clean data is progressively corrupted by adding Gaussian noise. neural network, typically U-Net architecture, is then trained to reverse this diffusion process, iteratively removing the noise to reconstruct high-quality data. DiT [34] has successfully scaled up diffusion models through transformer-based architectures, significantly improving their generative capabilities. Consequently, DiT has been effectively applied across multiple modalities, including text-to-image generation [10, 25, 34], text-to-video generation [3, 19, 45], and text-to-audio generation [5, 9]. In our study, motivated by the strong distribution modeling capabilities of diffusion models, we leverage diffusion models specifically trained on video and audio data to reconstruct these modalities from multimodal brain signals, including fMRI and EEG. 3. Experimental Designs and Curated Dataset In this section, we introduce CineBrain, the first multimodal brain dataset featuring synchronized EEG and fMRI recordings collected during story-based audiovisual stimulation. Tab. 1 summarizes key statistics of CineBrain and contrasts it with existing datasets primarily employing visual stimuli. In comparison, CineBrain provides richer data suitable for broader range of downstream tasks. The dataset will be publicly available to promote research in story decoding and multimodal neural decoding. 3.1. Subjects and Experiments Six participants (ages 2126; two males, four females), who are unaware of the studys objectives, contributed to the CineBrain dataset. All participants have normal or corrected-to-normal vision and hearing. Written informed consent was obtained, and the experimental protocol received approval from the appropriate ethics committee. We select episodes from the popular TV series The Big Bang Theory as audiovisual stimuli due to its engaging nature, relevance to everyday experiences, and its effectiveness in sustaining participants attention over the 18-minute viewing periods. To enhance the diversity of stimuli, participants collectively view 20 different episodes. All participants watch the first 10 episodes of Season 7; additionally, Participants 1, 2, and 6 watch 10 episodes from Season 9, while the remaining participants view 10 episodes from Season 11. During the experiment, the video resolution is downsampled to 720p to match the MRIs in-bore LED screen. Episodes of varying lengths are standardized to 18 minutes Figure 2. Visualization of fMRI and EEG Responses in CineBrain. fMRI and EEG responses of subjects 14 to identical stimuli, illustrating individual differences in brain activation. large-scale fMRI dataset collected from image-based stimuli, while Wen dataset [44] and fMRI-Video-WebVid [27] contain fMRI data acquired from video stimuli. Additionally, the fMRI-Shape [13] and fMRI-Objaverse [14] datasets provide fMRI recordings from 3D video stimuli. For EEG data, the SEED-DV [30] dataset includes EEG recordings from video stimuli. Tangs dataset [42] focuses specifically on auditory stimulation. However, these existing datasets are limited because each dataset only captures single-modality brain signals responding to singlemodality stimuli. To address this limitation, in this paper, we propose multimodal audiovisual stimulus dataset named CineBrain and conduct experiments based on it. 2.2. Neural Decoding Neural decoding refers to the process of reconstructing external stimuli perceived by the human brain from brain signals that reflect underlying neural activity. This process poses significant challenges for both encoding brain signals and decoding the corresponding stimuli. Functional magnetic resonance imaging (fMRI), known for its excellent spatial resolution and non-invasive characteristics, has demonstrated effectiveness in reconstructing visual stimuli. Methods such as MinD-Vis [6] and MindEye [39] have successfully achieved image reconstruction. Techniques including Mind-Video [7], GLFA [27], and NeuroClips [15] have further extended these accomplishments into video reconstruction. More recently, MinD3D [13] and MinD-3D++ [14] have explored reconstructing three-dimensional visual content. Electroencephalography (EEG), characterized by high temporal resolution and continuous signals, has also shown promise in neural decoding. EEG-based methods have successfully reconstructed various stimuli, including videos [31], 3D structures [17], and auditory signals [8, 23]. However, existing methods have not considered combining different types of brain signals, such as fMRI and EEG, which possess significantly different characteristics. In this paper, we aim to explore the effectiveness of integrating fMRI and EEG signals to enhance stimulus reconstruction."
        },
        {
            "title": "Participants",
            "content": "Gender (M/F)"
        },
        {
            "title": "EEG",
            "content": "fMRI Wen [44] fMRI-Video-FCVID [27] fMRI-Video-WebVid [27] SEED-DV [30]"
        },
        {
            "title": "Video\nVideo\nVideo\nVideo",
            "content": "CineBrain (Ours) Audio+Video 3 3 5 20 6 0/3 1/1 2/2 10/10 2/ 3.07 1.11 2.67 0.76 6 5520 400 1200 1400 5400 Table 1. Overview of the CineBrain Dataset. We present detailed statistics of our proposed CineBrain dataset and compare it with other existing video-based brain datasets. CineBrain provides comprehensive multimodal brain recordings during audiovisual stimulation. Each participant watched total of 6 hours of audiovisual stimuli, corresponding to approximately 27,000 frames of fMRI data. of viewing time, easing subsequent data processing. Each 18-minute viewing constitutes one run, yielding 20 runs per participant. To maintain data quality and reduce fatigue, runs are grouped into sessions of two or three episodes each, with no more than five runs conducted on single day. In total, each participant contributes approximately 6 hours of concurrent fMRI and EEG alongside ECG. Visual stimuli are presented on an LCD screen (8 8) positioned at the head of the scanner bed. Participants viewed the screen through mirror attached to the RF coil, maintaining fixation on central red dot (0.4 0.4). CineBrain experimental setup features simultaneous fMRI and EEG recording, harnessing their complementary strengthshigh spatial resolution from fMRI and high temporal resolution from EEG. Given the noisy MRI environment, participants use sponge earplugs and customdesigned non-magnetic headphones with soft padding to ensure comfort and clear audio delivery. An MRI-compatible EEG cap is also worn, enabling simultaneous collection of these complementary neural data types, significantly broadening datasets potential for diverse downstream analyses. 3.2. Data Acquisition and Preprocessing fMRI. The fMRI acquisition protocol follows established experimental standards [13, 14, 27]. 3T scanner equipped with 32-channel RF head coil is employed to obtain highresolution T1-weighted structural images and functional data for the decoding experiments. T1-weighted images were obtained using an MPRAGE sequence (0.8-mm isotropic resolution, TR = 2500 ms, TE = 2.22 ms, flip angle 8). Functional data were recorded using gradient-echo EPI sequence with whole-brain coverage (2-mm isotropic resolution, TR = 800 ms, TE = 37 ms, flip angle 52, multi-band factor = 8). With sampling frequency of 1.25 Hz, each video run generated 1350 functional MRI frames. Preprocessing is conducted using the widely adopted fMRIPrep pipeline [11, 12]. Unlike prior studies focused solely on visual decoding [6, 7, 13], our experimental design also incorporated auditory stimuli, leading us to select both visual and auditory regions of interest (ROIs). For the selection of visual regions of interest (ROIs), Figure 3. ROIs from the fMRI signals used in our experiments. we follow the regions commonly reported in [13, 14]. The ROIs were defined using the parcellation provided by the Human Connectome Project Multi-Modal Parcellation (HCP-MMP) in the 32k_fs_LR space. These ROIs included areas such as V1, V2, V3, V3A, V3B, V3CD, V4, LO1, LO2, LO3, PIT, V4t, V6, V6A, V7, V8, PH, FFC, IP0, MT, MST, FST, VVC, VMV1, VMV2, VMV3, PHA1, PHA2, PHA3. For the auditory cortex, we selected regions corresponding to 4, 7AL, 7Am, 7m, 7PC, 7PL, 7Pm, 8Ad, 8Av, 8BM, 8C, 9a, 9p, 10d, 10v, 44, 45, 46, 47m, A1, IPS1, p32, PGp, s32, STGa, STSda, STSdp, STSva, STSvp, TPOJ1, TPOJ2, TPOJ3. The selection of these ROIs is illustrated in Fig. 3. The visual ROIs contain 8,405 voxels, and the auditory ROIs contain 10,541 voxels. To account for the inherent delay in BOLD responses, fMRI signals underwent z-scoring across vertices within each run, incorporating 4-second lag. EEG. EEG data are captured using an MRI-compatible 64channel cap at 1000 Hz, simultaneously recording ECG signals. Precise synchronization between EEG and fMRI data is ensured by logging fMRI TR timings. EEG preprocessing involves multi-step artifact removal approach, targeting scanner-induced noise and biological interference while preserving neural activity. This includes bandpass filtering (0.130 Hz) to remove baseline drift and muscle artifacts, and 50 Hz notch filter to reduce powerline interference [4, 26, 28, 29]. ECG artifacts are initially mitigated using QRS-based techniques, followed by independent component analysis (ICA) to isolate residual artifacts. ECG recordings enable correlation analyses to adaptively refine artifact removal, culminating in clean EEG data suitable for subsequent analyses. 4 Figure 4. Overview of the CineSync Framework. CineSync first employs Multimodal Fusion Encoder to extract features from fMRI and EEG data, with modality alignment module to align these features with semantic information. Subsequently, it utilizes LoRA-tuned neural latent decoder to reconstruct videos based on the fused brain features. Note: The gray box is used only during training. Video Stimuli. Original video stimuli consist of twenty 18minute episodes at 24 fps and 1080p. For our experiments, we resize and crop these videos to resolution of 480720 and segment each into 4-second clips (33 frames per clip). Thus, each participant generates 5400 clips, partitioned into 4860 training clips (first 18 episodes) and 540 testing clips (final 2 episodes). Audio. Audio from each video episode is segmented into corresponding 4-second clips, resulting in the same number of samples as the video clips, maintaining consistency for training and evaluation. Text Descriptions. To support multimodal decoding experiments, textual descriptions are generated separately for each audio and video clip. Video descriptions are generated using vision-language models (Qwen2.5-VL [2] and LlavaVideo [46]) to facilitate contrastive learning. Audio clips are transcribed using Whisper-large-v3 [37], providing rich textual inputs aligned with auditory data for enhanced multimodal analyses. 4. Methods (MFE): This module extracts semantically aligned features from fMRI and EEG signals and subsequently fuses these modalities to produce effective representations for downstream tasks. 2) Neuro Latent Decoder (NLD): Utilizing fused signals from the MFE, this module employs LoRAtuned, diffusion-based decoder to reconstruct corresponding stimuli (e.g., video or audio) from brain signals. 4.1. Multi-Modal Fusion Encoder Inspired by the success of contrastive learning [36] in neuro-decoding applications [7, 14, 27] and multi-modal tasks, and considering that fMRI and EEG signals significantly differ in their characteristicswe design dualtransformer-based encoder, termed the Multi-Modal Fusion Encoder (MFE). The MFE utilizes ViT architecture with class token to semantically align information, serving as the backbone for each modality. Formally, we denote fMRI signals as Bf , EEG signals as Be, and the MFE as = {Ef , Ee}. Through the MFE, we obtain: ff , fe, cf , ce = E(Bf , Be), Overall. To evaluate the effectiveness of our proposed CineBrain framework and the integration of fMRI and EEG signals to enhance downstream decoding tasks, we introduce an innovative and versatile approach comprising two main components: 1) Multi-Modal Fusion Encoder where ff and fe represent features extracted from fMRI and EEG, respectively, and cf and ce are the corresponding class tokens. Next, we apply fusion MLP ψ to integrate features 5 from Ef and Ee: fb = ψ(ff , fe), where fb drives the decoder to reconstruct the stimulus. Simultaneously, we utilize pre-trained contrastive encoders Ev and Et to extract embeddings from videos and text generated by the VLM, as illustrated in the modality alignment in Fig. 4. Given video clips with frames = {I1, . . . , In}, we first extract embeddings for all frames using Ev and then apply an aggregation module φ to produce unified video embedding cv. For text, we directly use the encoder Et: cv = φ(cid:0){Ev(Ii)}n i=1 (cid:1) ct = Et(Text) We then compute the contrastive losses as follows: Lf = Lclip(cf , cv); Lf = Lclip(cf , ct) Lev = Lclip(ce, cv); Let = Lclip(ce, ct) Therefore, the total contrastive loss for the encoder is defined as: Lc = Lf + Lf + Lev + Let During training, we optimize the MFE, including the aggregation module φ, while keeping the pre-trained image encoder Ev and text encoder Et frozen. After extracting the fused brain feature fb, we feed it into the next module for video reconstruction. 4.2. Neuro Latent Decoder To effectively reconstruct video stimuli and advance video decoding, our framework leverages CogVideoX [45], which generates videos at 8 fps and 480720 resolution. Compared to existing approaches [15, 27], our model generates higher-resolution videos with more frames, substantially enhancing brain-based representations. CogVideo employs causal 3D VAE to encode latent video representations while maintaining temporal causality, thereby preserving the videos spatiotemporal dynamics. Specifically, we adapt CogVideo-5B-T2V as the primary diffusion model within our Neuro Latent Decoder (NLD). Given the central role of textual conditioning in video generation models, we replace the original text embedding with our fused brain feature fb. During training, we encode video clip using 3DVAE E, then introduce noise into the latent representation: x0 = E(V ); xt = αtx0 + 1 αtϵ ϵ (0, I) where is uniformly sampled between 1 and 1000. Following CogVideoX, we adopt explicit uniform sampling of timestamps during training. As depicted in Fig. 4, we concatenate xt with the fused brain feature fb for input to the diffusion model. We apply LoRA to fine-tune attention and feed-forward layers within the DiT Blocks of NLD, enabling effective integration of brain-derived signals into video reconstruction The training objective for NLD is formulated as: = EV,ϵ,t ϵ ϵθ (xt, fb, t)2 , ϵ (0, I) 4.3. Extension to Audio Reconstruction Our framework can readily extend to audio reconstruction. Specifically, to accurately reconstruct audio from brain signals, we leverage the DiT-based audio generation model F5TTS [5] as our decoder. Since participants simultaneously listened to audio while viewing video stimuli, we fully utilize available audio information. Unlike video reconstruction, we convert pre-segmented audio clips into text using Whisper-large-v3 [37] for contrastive learning. We extract text embeddings using Et, computing contrastive loss exclusively on text modality to avoid visual-audio misalignment noise. Similar to video reconstruction, we apply LoRA to fine-tune attention layers within F5-TTS. Additionally, we adapt the original F5-TTS training strategy by introducing noise to the entire audio segment, aligning with standard diffusion model practices. Further architectural details are provided in the supplementary materials. 5. Experiments 5.1. Cine-Benchmark Video Metrics. Reconstructing human-centric videos from multimodal brain signals is challenging task, even within text-to-video frameworks. Thus, establishing effective evaluation metrics is crucial for comprehensively assessing model performance. We evaluate the reconstructed videos on two levels: semantic and perceptual. 1) Semantic-Level: Semantic similarity is essential for evaluating reconstructed videos. Following existing neurodecoding literature [7, 13, 24, 27, 39], we calculate N-way top-K accuracy across entire videos and individual frames to measure feature similarity. Additionally, we compute the Fréchet Video Distance (FVD) to quantify the distributional similarity between reconstructed and ground-truth videos. 2) Perceptual-Level: Beyond semantics, visual quality is also critical. Therefore, we employ DINO temporal consistency (DTC) [32] and CLIP temporal consistency (CTC) [36] metrics to evaluate video temporal coherence. Additionally, structural similarity (SSIM) and peak signalto-noise ratio (PSNR) are used to assess frame-level image quality. Audio Metrics. For audio reconstruction, we similarly adopt metrics at both semantic and perceptual levels: 1) Semantic-Level: Rather than employing the conventional N-way top-K, we utilize Whisper [37] to transcribe 6 Figure 5. Qualitative comparison of our method with baselines. We compare the results of CineSync, CineSync-fMRI, and CineSyncEEG with the ground truth (GT). CineSync demonstrates higher accuracy, greater temporal consistency, and improved video quality. the audio into text, enabling the calculation of Word Error Rate (WER) and Character Error Rate (CER) against ground-truth text. We also calculate the Fréchet Audio Distance (FAD) to measure distributional similarity between reconstructed and ground-truth audio signals. 2) Perceptual-Level: We employ Log-Spectral Distance (LSD) and Mel Cepstral Distortion (MCD) to evaluate audio quality by analyzing their spectral characteristics. Additionally, SSIM and Root Mean Squared Error (RMSE) are computed on mel spectrograms to assess perceptual. 5.2. Implementation Details Video Reconstruction. We employ the AdamW optimizer with β = (0.9, 0.95) and an initial learning rate of 1104. Due to GPU memory constraints, CineSync is trained using two-stage approach. In the first stage, only the multimodal fusion encoder is trained using contrastive loss with batch size of 16 on single H100 GPU. To enhance the effectiveness of contrastive learning, we augment the training dataset with diverse textual captions generated by Qwen2.5VL-7B [2] and Llava-Video-7B [46]. This pretraining phase lasts approximately 50 epochs. In the second stage, the pretrained encoder is integrated into the full model, which is finetuned for 5000 steps on four H100 GPUs using batch size of 2 per GPU. The fMRI and EEG transformers each consist of 24 layers with transformer dimension of 2048 and token length of 227 (226 spatial tokens plus one class token). The LoRA configuration uses rank of 128 and scaling factor α=64. Input data consist of 4-second multimodal brain signals, yielding standard inputs of 58405 fMRI voxels and 644000 EEG data points. Audio Reconstruction. We use the AdamW optimizer with β=(0.9, 0.95) and an initial learning rate of 1104. Given the strict alignment between text and audio , we reduce the vision feature and utilize transcriptions generated by Whisper [37] to facilitate contrastive learning. We use F5-TTSBase [5] as the diffusion model in the neural latent decoder and train the model for 30,000 steps using batch size of 8 on single A6000 GPU. Both fMRI and EEG transformers comprise 12 layers, each with transformer dimension of 1920 and token length of 377 (376 spatial tokens plus one class token). The LoRA configuration uses rank of 64 and scaling factor α=32. Similar to video reconstruction, each input contains standard 4-second segments, resulting in 5 10541 fMRI voxels and 64 4000 EEG data points. 7 METHODS 2-way Video-based 50-way Frame-based Video-based Frame-based FVD 2-way 50-way DTC CTC SSIM PSNR Semantic-level Perceptual-level GLFA [27] CineSync-EEG CineSync-fMRI CineSync CineSync 0.801 0.891 0.893 0.909 0.926 0.167 0.304 0.307 0.319 0.336 128.76 53.75 57.47 52.78 44. 0.847 0.918 0.926 0.937 0.954 0.225 0.349 0.358 0.398 0.423 0.706 0.899 0.907 0.915 0. 0.735 0.937 0.945 0.967 0.953 0.123 0.231 0.240 0.262 0.297 7.526 11.75 11.92 11.99 12. Table 2. Performance comparison of CineSync with baselines. The average metrics across all subjects are reported. CineSync indicates the experiment that includes audio-related ROIs in fMRI. Bold denotes the best performance, while underlined denotes the second-best. 5.3. Results of Video Reconstruction 5.5. Exploration of Brain ROIs As the first attempt to combine fMRI and EEG signals for video decoding, there are no directly comparable baselines. To validate the effectiveness of our proposed CineSync and assess the quality of the CineBrain dataset, we select two baselines: CineSync-fMRI and CineSync-EEG, which remove the EEG or fMRI branches from our model, respectively. Additionally, we include GLFA [27], strong endto-end baseline based solely on fMRI signals. All models are trained and evaluated on the same dataset. Tab. 2 presents the averaged metrics across all subjects. The results demonstrate that CineSync-fMRI significantly outperforms GLFA across all metrics by substantial margin, confirming the effectiveness of our framework. Moreover, CineSync surpasses both CineSync-fMRI and CineSyncEEG, clearly indicating that our framework successfully integrates fMRI and EEG signals to enhance video reconstruction quality. This demonstrates the superiority of CineSync and establishes new state-of-the-art in reconstructing videos from human brain signals. Additionally, qualitative results of subject 3, shown in Fig. 5, indicate that CineSync captures more temporal details compared to the two baselines, demonstrating that our framework effectively leverages EEGs temporal resolution to complement fMRIs spatial characteristics. Furthermore, the experimental results affirm the high quality of our dataset, demonstrating its value in supporting multimodal brain-signal research. We hope our dataset will facilitate further advancements in computer vision and cognitive neuroscience. 5.4. Results of Audio Reconstruction To verify the flexibility and robustness of our framework, we also conduct experiments reconstructing audio signals from multimodal brain data. In this setting, we again select CineSync-fMRI and CineSync-EEG as baselines due to their strong performance in video reconstruction. Qualitative and quantitative results are provided in the supplementary material. These results indicate that our framework can be easily adapted to audio reconstruction tasks, further validating its effectiveness and the high quality of our dataset. As discussed in the introduction, different brain regions work collaboratively. Based on this insight, we conducted an exploratory study to investigate the effectiveness of joint brain-region interactions. Specifically, we incorporated auditory ROIs into the fMRI signals for video reconstruction tasks and visual ROIs into the fMRI signals for audio reconstruction tasks while retaining all the EEG data, resulting in total of 18,946 voxels per frame. We refer to this modified model as CineSync. As shown in Tab. 2 and the supplementary material, CineSync achieves better results than CineSync in both video and audio reconstruction. These experimental findings demonstrate that utilizing jointly stimulated brain regions can effectively enhance the performance of video and audio reconstruction from brain signals. 6. Conclusion In this paper, we present the first large-scale multimodal audiovisual stimulation dataset, CineBrain, supporting various reconstruction tasks. Building on this dataset, we develop an innovative framework, CineSync, successfully fuses fMRI and EEG signals by leveraging the high spatial resolution of fMRI and the high temporal resolution of EEG to enhance both video and audio decoding performance. In the first stage, the multimodal fusion encoder extracts features from both fMRI and EEG signals and aligns them with visual and textual representations at the semantic level. In the second stage, the framework employs neuro latent decoder, wherein LoRA-tuned diffusion decoder reconstructs the stimuli based on these extracted multimodal features. We also introduce Cine-benchmark to evaluate our models performance in both video and audio reconstruction. CineSync achieves SOTA performance in video reconstruction and demonstrates sufficient flexibility and robustness for audio reconstruction. Our experiments empirically validate the effectiveness of our dataset, highlighting its potential to support future research. We hope that our framework will serve as strong baseline for multimodal brain decoding and that our dataset will pave the way for further advancements in computer vision and cognitive science."
        },
        {
            "title": "References",
            "content": "[1] Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1):116126, 2022. 2 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 5, 7, 14 [3] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 3 [4] Jin Chen, Tony Ro, and Zhigang Zhu. Emotion recognition with audio, video, eeg, and emg: dataset and baseline approaches. IEEE Access, 10:1322913242, 2022. 4 [5] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. 3, 6, 7 [6] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2271022720, 2023. 2, 3, [7] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou. Cinematic mindscapes: High-quality video reconstruction from brain activity. NeurIPS, 2023. 3, 4, 5, 6 [8] Ian Daly. Neural decoding of music from the eeg. Scientific Reports, 13(1):624, 2023. 3 [9] Sefik Emre Eskimez, Xiaofei Wang, Manthan Thakker, Canrun Li, Chung-Hsien Tsai, Zhen Xiao, Hemin Yang, Zirun Zhu, Min Tang, Xu Tan, Yanqing Liu, Sheng Zhao, and Naoyuki Kanda. E2 tts: Embarrassingly easy fully nonautoregressive zero-shot tts. 2024. 3 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 3 [11] Oscar Esteban, Ross Blair, Christopher J. Markiewicz, Shoshana L. Berleant, Craig Moodie, Feilong Ma, Ayse Ilkay Isik, Asier Erramuzpe, Mathias Kent, James D. andGoncalves, Elizabeth DuPre, Kevin R. Sitek, Daniel E. P. Gomez, Daniel J. Lurie, Zhifang Ye, Russell A. Polfmriprep. Software, drack, and Krzysztof J. Gorgolewski. 2018. 4 [12] Oscar Esteban, Christopher Markiewicz, Ross Blair, Craig Moodie, Ayse Ilkay Isik, Asier Erramuzpe Aliaga, James Kent, Mathias Goncalves, Elizabeth DuPre, Madeleine Snyder, Hiroyuki Oya, Satrajit Ghosh, Jessey Wright, Joke Durnez, Russell Poldrack, and Krzysztof Jacek Gorgolewski. fMRIPrep: robust preprocessing pipeline for functional MRI. Nature Methods, 16:111116, 2019. 4 [13] Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng Feng, and Yanwei Fu. Mind-3d: Reconstruct high-quality 3d objects in human brain. arXiv preprint arXiv:2312.07485, 2023. 2, 3, 4, 6 [14] Jianxiong Gao, Yuqian Fu, Yun Wang, Xuelin Qian, Jianfeng fmri-3d: comprehensive dataset Feng, and Yanwei Fu. for enhancing fmri-based 3d reconstruction. arXiv preprint arXiv:2409.11315, 2024. 2, 3, 4, 5 [15] Zixuan Gong, Guangyin Bao, Qi Zhang, Zhongwei Wan, Duoqian Miao, Shoujin Wang, Lei Zhu, Changwei Wang, Rongtao Xu, Liang Hu, et al. Neuroclips: Towards higharXiv fidelity and smooth fmri-to-video reconstruction. preprint arXiv:2410.19452, 2024. 3, 6 [16] Melanie Green and Timothy Brock. The role of transportation in the persuasiveness of public narratives. Journal of personality and social psychology, 79(5):701, 2000. 2 [17] Zhanqiang Guo, Jiamin Wu, Yonghao Song, Jiahui Bu, Weijian Mai, Qihao Zheng, Wanli Ouyang, and Chunfeng Song. Neuro-3d: Towards 3d visual decoding from eeg signals, 2024. 3 [18] Uri Hasson, Ohad Landesman, Barbara Knappmeyer, Ignacio Vallines, Nava Rubin, and David Heeger. Neurocinematics: The neuroscience of film. Projections, 2(1):126, 2008. 2 [19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. 3 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 11 [22] Zhicheng Jiao, Haoxuan You, Fan Yang, Xin Li, Han Zhang, and Dinggang Shen. Decoding eeg by visual-guided deep neural networks. In IJCAI, pages 13871393. Macao, 2019. 2 [23] Jaswanth Reddy Katthi and Sriram Ganapathy. Deep correlation analysis for audio-eeg decoding. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:2742 2753, 2021. 3 [24] Gautam Krishna, Co Tran, Yan Han, Mason Carnahan, and Ahmed Tewfik. Speech synthesis using eeg. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12351238. IEEE, 2020. [25] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [26] Min-Ho Lee, Adai Shomanov, Balgyn Begim, Zhuldyz Kabidenova, Aruna Nyssanbay, Adnan Yazici, and SeongWhan Lee. Eav: Eeg-audio-video dataset for emotion recognition in conversational contexts. Scientific data, 11(1):1026, 2024. 4 9 [39] Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, and Tanishq Mathew Abraham. Reconstructing the minds eye: fmri-to-image with contrastive learning and diffusion priors, 2023. 3, 6 [40] Hubert Siuzdak. Vocos: Closing the gap between timedomain and fourier-based neural vocoders for high-quality audio synthesis. arXiv preprint arXiv:2306.00814, 2023. 11 [41] Olaf Sporns. Brain connectivity. Scholarpedia, 2(10):4695, 2007. 2 [42] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, 26(5): 858866, 2023. 3 [43] Mitsuko Watabe-Uchida, Neir Eshel, and Naoshige Uchida. Neural circuitry of reward prediction error. Annual review of neuroscience, 40(1):373394, 2017. 2 [44] Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, and Zhongming Liu. Neural encoding and decoding with deep learning for dynamic natural vision. Cerebral cortex, 28(12):41364160, 2018. 3, 4 [45] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 3, 6 [46] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 5, 7, [47] Xiao Zheng, Wanzhong Chen, Mingyang Li, Tao Zhang, Yang You, and Yun Jiang. Decoding human brain activity with deep learning. Biomedical Signal Processing and Control, 56:101730, 2020. 2 [27] Chong Li, Xuelin Qian, Yun Wang, Jingyang Huo, Xiangyang Xue, Yanwei Fu, and Jianfeng Feng. Enhancing cross-subject fmri-to-video decoding with global-local functional alignment. 3, 4, 5, 6, 8 [28] Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Haoyang Qin, and Quanying Liu. Visual decoding and reconstruction via eeg embeddings with guided diffusion. arXiv preprint arXiv:2403.07721, 2024. 4 [29] Yamin Li, Ange Lou, Ziyuan Xu, Shengchao Zhang, Shiyu Wang, Dario Englot, Soheil Kolouri, Daniel Moyer, Roza Bayrak, and Catie Chang. Neurobolt: Resting-state eegto-fmri synthesis with multi-dimensional feature mapping. Advances in Neural Information Processing Systems, 37: 2337823405, 2025. 4 [30] Xuan-Hao Liu, Yan-Kai Liu, Yansen Wang, Kan Ren, Hanwen Shi, Zilong Wang, Dongsheng Li, Bao-Liang Lu, and Wei-Long Zheng. EEG2video: Towards decoding dynamic In The Thirty-eighth visual perception from EEG signals. Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. 3, 4 [31] Xuan-Hao Liu, Yan-Kai Liu, Yansen Wang, Kan Ren, Hanwen Shi, Zilong Wang, Dongsheng Li, Bao-Liang Lu, and Wei-Long Zheng. EEG2video: Towards decoding dynamic In The Thirty-eighth visual perception from EEG signals. Annual Conference on Neural Information Processing Systems (NeurIPS), 2024. [32] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 6 [33] Hae-Jeong Park and Karl Friston. Structural and functional brain networks: from connections to cognition. Science, 342 (6158):1238411, 2013. 2 [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 3, 11 [35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 3 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 5, 6 [37] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. 5, 6, 7 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 3 A. Additional Experimental Results A.1. More Results of Video Reconstruction To demonstrate the quality and temporal consistency of the reconstructed videos generated by our method, we present additional examples containing 12 frames each in Figures B.2, B.3, and B.4. Overall, our reconstructions exhibit strong semantic and perceptual performance. A.2. Architecture of CineSync for Audio The detailed architecture of CineSync, employed for the audio reconstruction task, is depicted in Fig. A.1. To simplify the illustration, we omit the contrastive loss of the MultiModal Fusion Encoder and focus on the differences from the video reconstruction pipeline. Specifically, we first convert an audio file into mel spectrogram. We then use Vocos [40] as encoder VE to obtain latent representation of the mel spectrogram , and subsequently add noise to it for training: x0 = VE (M ); xt = αt x0 + 1 αt ϵ, ϵ (0, I) Then, we concatenate the fused brain features with xt to form the training input for our neural latent decoder. Unlike the strategy used for the video decoder, we train the neural latent decoder using flow-matching approach. A.3. Results of Audio Reconstruction The quantitative results, averaged across all participants, are summarized in Table B.1. Our CineSync approach outperforms both baselines, indicating that our framework effectively leverages the strengths of fMRI and EEG signals for audio reconstruction. Additionally, qualitative comparisons between CineSync and the baseline methods are shown in Figure B.5. From the mel spectrograms, it is evident that CineSync achieves superior performance compared to baselines relying on single modality. These experimental results in audio reconstruction further demonstrate that integrating fMRI and EEG improves performance, consistent with our findings on video reconstruction. Furthermore, we also explored incorporating fMRI voxels from visual areas into the audio reconstruction task, which similarly improved reconstruction quality. This outcome suggests that different brain regions activations are interrelated. Overall, our experiments empirically validate the high quality of the CineBrain dataset, which can support various experimental settings. B. EEG Experimental Device We illustrate the electrode montage of 64-channel EEG cap configured according to the GSN-HydroCel-64_1.0 layout in Fig. B.1. Figure A.1. Architecture of CineSync for audio reconstruction. We concatenate the fused brain features with the noised latent representation of the mel spectrogram as the training input. We then apply LoRA [21] to the attention and feed-forward layers in the DiT [34] blocks of our neural latent decoder. Figure B.1. Electrode montage of 64-channel EEG cap using the GSN-HydroCel-64_1.0 layout. Sensor positions are annotated with their corresponding channel labels. 11 Figure B.2. More Results of CineSync: We present 12 frames with timestamps compared with the ground truth (GT). Figure B.3. More Results of CineSync: We present 12 frames with timestamps compared with the ground truth (GT). 12 Figure B.4. More Results of CineSync: We present 12 frames with timestamps compared with the ground truth (GT)."
        },
        {
            "title": "METHODS",
            "content": "CineSync-EEG CineSync-fMRI CineSync CineSync WER Semantic-Level CER FAD LSD MCD SSIM RMSE Perceptual-Level 1.802 1.776 1.476 1. 1.924 1.835 1.559 1.506 4.221 4.032 3.811 3.625 0.949 0.951 0.927 0. 0.710 0.704 0.683 0.671 0.777 0.783 0.814 0.819 0.239 0.248 0.221 0. Table B.1. Performance comparison of CineSync with baselines. The average metrics across all subjects are reported. CineSync indicates the experiment that includes vision-related ROIs in fMRI. Bold denotes the best performance Figure B.5. Qualitative results of CineSync and the two strong baselines are presented. GT indicates the ground-truth mel spectrogram. 13 C. Prompts for Generating Text Descriptions Textual descriptions are crucial for our training procedure. Prompts utilized for generating descriptions with Qwen2.5VL [2] and Llava-Video [46] are presented in Fig. C.1 and Fig. C.2, respectively. does not explicitly provide additional fMRI data. This limitation restricts broader applications of our dataset and represents an area for future exploration. Furthermore, our CineBrain dataset supports synchronized audiovisual stimuli. However, we have independently evaluated our primary contributions solely through separate video and audio reconstruction tasks. If we aim to expand into more complex applications such as embodied intelligence, it will be necessary to reconstruct multiple modalities simultaneously. Therefore, joint audiovisual reconstruction represents another significant direction for our future research. Figure C.1. Prompt utilized by Qwen2.5-VL for generating video descriptions. Figure C.2. Prompt utilized by Llava-Video for generating video descriptions. D. Limitations and Future Work Although our CineSync successfully leverages the spatial resolution of EEG to compensate for the temporal resolution limitations of fMRI, substantially improving video and audio reconstruction performance, our dataset currently"
        }
    ],
    "affiliations": [
        "Fudan University"
    ]
}