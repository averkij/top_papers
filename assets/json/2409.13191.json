{
    "paper_title": "An adapted large language model facilitates multiple medical tasks in diabetes care",
    "authors": [
        "Lai Wei",
        "Zhen Ying",
        "Muyang He",
        "Yutong Chen",
        "Qian Yang",
        "Yanzhe Hong",
        "Jiaping Lu",
        "Xiaoying Li",
        "Weiran Huang",
        "Ying Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diabetes is a chronic disease that poses a significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but their effectiveness across a diverse range of diabetes tasks remains unproven. In this study, we introduced a framework to train and validate diabetes-specific LLMs. We first developed a comprehensive data processing pipeline that includes data collection, filtering, augmentation and refinement. This approach contributes to creating a high-quality, diabetes-specific dataset, and several evaluation benchmarks entirely from scratch. Utilizing the collected training dataset, we fine-tuned a diabetes-specific LLM family that demonstrated state-of-the-art proficiency in understanding and processing various diabetes tasks compared to other LLMs. Furthermore, clinical studies showed the potential applications of our models in diabetes care, including providing personalized healthcare, assisting medical education, and streamlining clinical tasks. In conclusion, our study introduced a framework to develop and evaluate a diabetes-specific LLM family, and highlighted its potential to enhance clinical practice and provide personalized, data-driven support for diabetes support when facing different end users. The code is provided via GitHub at https://github.com/waltonfuture/Diabetica."
        },
        {
            "title": "Start",
            "content": "Lai Wei1#, Zhen Ying1#, Muyang He1#, Yutong Chen1, Qian Yang2, Yanzhe Hong1, Jiaping Lu3, Xiaoying Li1, Weiran Huang1*, Ying Chen1* 1 MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China; Ministry of Education Key Laboratory of Metabolism and Molecular Medicine, Department of Endocrinology and Metabolism, Zhongshan Hospital, Fudan University, Shanghai, China; 2 Department of Endocrinology, Fifth Peoples Hospital of Shanghai Fudan University, Shanghai, China 3 Department of Endocrinology and Metabolism, Qingpu Branch of Zhongshan Hospital Affiliated to Fudan University, Shanghai, China # Co-first authors * Correspondence to: Prof. Weiran Huang, weiran.huang@outlook.com or Dr. Ying Chen, chen.ying4@zs-hospital.sh.cn; MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China; Ministry of Education Key Laboratory of Metabolism and Molecular Medicine, Department of Endocrinology and Metabolism, Zhongshan Hospital, Fudan University, Shanghai, China Figure 1 Study design. (a) Training data was collected from public datasets (MCQ banks, and SFT datasets) and private in-house datasets (guidelines, textbooks, drug labels, and real-world dialogues). Data preprocessing was then conducted to get the final diabetes-related, formatted, and high-quality dataset. (b) Fine-tuning was applied for developing Diabetica. (c) We compared the performance of the different LLMs on three benchmarks, including MCQ benchmark, FB benchmark, and dialogue benchmark. (d) Our model was then evaluated in three clinical applications, including medical consulting, examination education, and clinical record summarization. 1 Abstract Diabetes is chronic disease that poses significant global health burden, and optimizing diabetes management requires multi-stakeholder collaboration. Large language models (LLMs) have shown promise in various healthcare scenarios, but their effectiveness across diverse range of diabetes tasks remains unproven. In this study, we introduced framework to train and validate diabetes-specific LLMs. We first developed comprehensive data processing pipeline that includes data collection, filtering, augmentation and refinement. This approach contributes to creating high-quality, diabetes-specific dataset, and several evaluation benchmarks entirely from scratch. Utilizing the collected training dataset, we fine-tuned diabetes-specific LLM family that demonstrated state-of-the-art proficiency in understanding and processing various diabetes tasks compared to other LLMs. Furthermore, clinical studies showed the potential applications of our models in diabetes care, including providing personalized healthcare, assisting medical education, and streamlining clinical tasks. In conclusion, our study introduced framework to develop and evaluate diabetes-specific LLM family, and highlighted its potential to enhance clinical practice and provide personalized, data-driven support for diabetes support when facing different end users. The code is provided via GitHub at https://github.com/waltonfuture/Diabetica. 2 Introduction Diabetes mellitus, affecting 10% of the global population, stands as one of the most prevalent chronic diseases worldwide1. Despite global efforts, challenges such as shortage of diabetes specialists, uneven distribution of medical resources, low diabetes knowledge awareness, and inadequate self-management capabilities persist, leading to poor glycemic control and substantial mortality and social burden2. With diabetes prevalence projected to rise to 643 million by 2030 and 783 million by 20451, current diabetes care systems would not be able to scale to meet the increasing demand. Optimizing diabetes management requires multi-stakeholder collaboration to strengthen specialist training and improve patient selfmanagement capabilities. Therefore, there is an urgent need for novel diabetes management instrument with accessibility, reliability and efficiency. The advancement of artificial intelligence (AI) technology presents significant opportunity to enhance diabetes care efficiency. Various AI-based tools for diabetes care, such as those for diagnosis3,4, insulin titration4,5, and retinal image analysis6,7, have demonstrated impressive performance in diabetes care. However, previous AI models in diabetes management, albeit advantageous in certain aspects, are so far predominantly single-task oriented and face challenges in comprehending and generating natural language. These limitations narrow down their potentials to offer comprehensive and easily understandable healthcare supports across diverse user groups. Recent developments in large language models (LLMs) have shown rapid progress, equipped with advanced language comprehension capabilities and the ability to handle complex linguistic tasks. Commercial models like GPT-48 and Claude-3.59, leveraging expansive datasets and refined training methods, have demonstrated high efficacy in healthcare applications, even among experts. However, their proprietary and closed-source nature limits accessibility and raises concerns about patient privacy, which may hinder their widespread adoption in diverse medical settings. In contrast, open-source LLMs like Llama310, Yi-1.511 and Qwen212 enhance healthcare by providing tailored solutions and transparent structures. Recent research shows that general models fine-tuned with medical datasets can yield performance on par with commercial models of larger scales, offering viable method for delivering cost-effective and transparent clinical support13,14. Additionally, the medical field can be further divided into departments with unique disease spectrums, general medical LLMs trained on broad medical data may fail to capture in-depth domainspecific knowledge so that perform inadequately when confronted with specialized clinical questions. While several open-source model architectures were proposed for specialized medical domain15,16, models specifically addressing diabetes are rarely reported17, primarily due to the lack of high-quality datasets and appropriate paradigms. Therefore, it is crucial to develop tailored LLM for diabetes, which holds remarkable promise in advancing personalized, data-driven support for both patients and healthcare professionals. Due to the life-critical nature of healthcare applications, using medical large language models necessitates objective and comprehensive evaluation of the models performance and capabilities. While several medical benchmarks exist, their objectivity is not always assured due to potential data contamination risks associated with expanded training datasets. Moreover, there is still lack of benchmarks for diabetes specialties. Additionally, clinical practice is not the same as answering examination questions correctly, and finding appropriate benchmarks to gauge the clinical potential of LLMs is substantial challenge18. Therefore, to validate the effectiveness and utility of specific models, there is an urgent need to provide comprehensive diabetes assessment framework that balances both laboratory and clinical practice performance. In this study, we introduced reproducible paradigm that includes data processing, model construction, benchmark assessment and clinical evaluation to develop specialized large language model that could handle wide range of diabetes-related tasks. In particular, our study makes three key contributions to the field of AI-assisted diabetes care and medical LLMs. First, we present reproducible paradigm for developing specialized medical LLMs. Our approach demonstrated how to effectively leverage open-source models, curate high-quality disease-specific datasets, and fine-tune models for optimal performance in particular medical domain. Second, we have independently designed and created comprehensive evaluation benchmarks specifically for the diabetes field. These benchmarks encompass wide range of tasks with diverse formats, and the assessment results demonstrated the state-of-the-art performance of our models in comprehending and executing diabetes-related tasks. Third, we conducted series of clinical studies to evaluate the models efficacy in real-world settings. These studies showed the potential applications of our model in diabetes care and how they could contribute to providing personalized healthcare, assisting medical education, and streamlining clinical tasks. Collectively, our work not only offers an effective framework for diverse diabetes-related tasks, but also provides feasible blueprint for developing specialized LLMs in other medical domains (Figure 1). Results Benchmark assessment In this section, we present the performance results of Diabetica-7B and different LLMs on several diabetesrelated benchmarks. The results show that Diabetica-7B outperforms other open-source models of similar size, demonstrating its high performance and robustness in handling diabetes-related tasks. First, we compared our Diabetica-7B and other baseline models against multiple-choice-questions set. We report the zero-shot performance of wide range of models as shown in Figure 2a and Supplementary Table 1. Diabetica-7B had an 87.2% accuracy level (272 correct responses of 312 questions), significantly surpassing all the other models. In addition, Diabetica-7B was even better than state-of-the-art close-source models, such as GPT-4 and Claude-3.5. Upon analyzing the performance based on the question type, Diabetica-7B achieved the highest accuracy level of 88.09% and 84.42% among the models, followed by GPT4 with an accuracy level of 82.98% and 67.53%, as well as Claude-3.5 with 82.55% and 72.73%. Notably, Diabetica showed similar accuracy on type A1 and type A2, suggesting balanced proficiency in both basic knowledge and case study analysis. To further explore the ability to recall medical knowledge and identify critical points, we then examined the proficiency of our Diabetica-7B and other baseline models in fill-in-the-blank set. The results presented in Figure 2b and Supplementary Table 1 show the performance of Diabetica-7B (BERTScore of 0.9298; ROUGE-L of 0.7828; ROUGE-1 of 0.7876, ROUGE-2 of 0.6952, and BLEU of 0.5143) was superior to all other open-source models with similar sizes across all metrics. In addition, Diabetica-7B was also comparable with state-of-the-art close-source models, such as GPT-4 and Claude-3.5, showcasing its exceptional ability in diabetes context understanding. Figure 2. Performance in the multiple-choice questions (MCQ) and fill-in-the-blanks (FB) benchmarks. Performance of different LLMs in diabetes-related benchmarks, including (a) multiple-choice questions and (b) fill-in-the-blank questions. These opensource LLMs include medical LLMs (Meditron, MMedLM, Apollo), generic LLMs (Internlm2, Llama3, Yi-1.5), our model (Diabetica7B) and its base model (Qwen2-7B). The performance of advanced proprietary LLMs (Claude-3.5, and GPT-4) is also displayed. In addition, we evaluated Diabetica-7B's ability to address practical and open-ended questions using single-round and open-ended dialogue evaluation set that includes medical consultations and interactive diagnoses. In this dialogue benchmark, we utilized GPT-4 and Claude-3.5, which are state-of-art proprietary LLMs, to judge19 and provide quantitative scores for LLM responses based on specific questions and manually collected evaluation rules. Our experiments showed that by only conducting fine-tuning using selfdistillation pipeline without RLHF20, Diabetica-7B outperformed other similarly sized open-source LLMs. Remarkably, Diabetica-7B achieved scores of 7.81 from GPT-4 and 7.96 from Claude-3.5, improved significantly from Qwen2-7B-Instruct's 7.50 from GPT-4 and 7.74 from Claude-3.5. As for different domains of diabetes care, our model consistently showed greater performance compared with other baseline models (Figure 3 and Supplementary Figure 1). The scores given by GPT-4 and Claude-3.5 were consistent (ICC=0.852), indicating the robustness of LLM-as-Judge. We also provide further analysis of the selfdistillation method in Supplementary information. In summary, our assessment validated Diabetica-7B's ability to recall medical knowledge, identify critical points, and address practical and open-ended questions across various diabetes-related tasks, showcasing its robustness and effectiveness as high-performance diabetes-specialized LLM. Figure 3. Performance in the dialogue benchmark. Results in the dialogue benchmark. (a) Format of the dialogue benchmark. Each instance in the benchmark consists of question and the associated rules, and the proprietary LLMs (GPT-4 and Claude-3.5) are used to rate LLM responses based on rules. (b) GPT-4 judged scores of different LLMs in the dialogue benchmark. In summary, our assessment validated Diabetica-7B's ability to recall medical knowledge, identify critical points, and address practical and open-ended questions across various diabetes-related tasks, showcasing its robustness and effectiveness as high-performance diabetes-specialized LLM. Diabetica family To test our data on smaller model, we also trained Diabetica-1.5B (based on Qwen2-1.5B-Instruct) using the same training configuration and dataset of Diabetica-7B. These two models make up the Diabetica family. We observed that Diabetica-1.5B significantly outperformed its base models across all evaluation metrics. Notably, Diabetica-1.5B achieved scores of 6.20 and 6.58 in dialogue evaluation from Claude-3.5 and GPT-4 judges, respectively, which were higher than the 5.33 and 5.79 scores received by Qwen2-1.5B (Supplementary Table 2). Furthermore, Diabetica-1.5B achieved competitive results compared to several larger models, like InternLM2-7B-Chat, Llama3-8B-Instruct, and Yi-1.5-9B-Chat, in many cases. In particular, Diabetica-1.5B outperformed all of these three LLMs in fill-in-the-blank questions, with BERTScore of 0.9034, ROUGE-L of 0.6448, ROUGE-1 of 0.6496, ROUGE-2 of 0.5620, and BLEU of 0.4017. Diabetes-1.5B also achieved the highest accuracy of 75.32% and 66.23% in multiple-choice-questions among these models (Supplementary Table 2). This suggests that our training approach is effective not only for large models but also for smaller ones, potentially making high-quality medical AI more accessible for resource-constrained applications. Moreover, the Diabetica family offers range of deployment options across different hardware configurations. Diabetica-7B is best suited for desktops with GPUs of at least 16GB memory (e.g., NVIDIA RTX 4060 Ti), while Diabetica-1.5B is optimized for more modest setups, such as laptops with CPUs or GPUs of at least 4GB of memory. This range of options ensures that the Diabetica family can accommodate various computational resources, demonstrating its strong applicability. Alleviating catastrophic forgetting We conducted additional experiments to assess how our methodology helps alleviate catastrophic forgetting using range of general benchmarks. Results showed that our approach significantly reduced forgetting, with the fine-tuned model retaining up to 99.6% of their initial capability on GSM8K21 while achieving high performance on diabetes-specific tasks. Surprisingly, Diabetica-7B achieved an average score of 68.62 on MMLU22, surpassing the 67.08 before fine-tuning. It also excelled on the C-Eval23 benchmark, reaching an 6 average score of 78.11, substantial improvement from the pre-fine-tuning score of 73.01. This demonstrates the robustness of our method in maintaining comprehensive knowledge base while adapting to new specialized domains (Supplementary Table 3). Ablations We performed several ablation studies across different benchmarks to better understand our results and identify the key components contributing to Diabeticas performance. Our analysis focused on three main areas: (1) Fine-tuning from different base LLMs; (2) Fine-tuning the LLM with the original self-distillation method or without any self-distillation; (3) Fine-tuning the LLM on existing open-source medical datasets. The evaluation method for these ablation studies followed the same procedure for Diabetica evaluation, as described above. The robustness of Diabetes-QA dataset To validate that our carefully collected Diabetes-QA dataset can improve LLMs diabetes knowledge in different scenarios, we conducted fine-tuning on Diabetes-QA from different popular base LLMs, such as Qwen2-7B-Instruct, Llama3-8B-Instruct10, Yi-1.5-9B-Chat11, and InternLM2-7B-Chat24. Across these base LLMs with different sizes and structures, we observed significant performance improvements in all benchmarksmultiple-choice questions (MCQ), fill-in-the-blank (FB), and open-ended dialogueafter tuning (Figure 4, Supplementary Table 4). Note that Qwen2-7B-Instruct achieved the highest performance both before and after training, and therefore we chose Qwen2-7B-Instruct as our base LLM. These results indicated that our Diabetes-QA dataset effectively enhanced the diabetes-related knowledge and performance of various large language models. It also demonstrated the strong benefits and robustness of our fine-tuning pipeline despite different base LLMs. d h Figure 4. Performance improvement of fine-tuning from different LLMs. Ablation studies based on different LLMs showed performance improvement in (a) accuracy of A1 type multiple-choice questions, (b) accuracy of A2 type multiple-choice questions, (c) Bert score of fill-in-the-blank questions, (d) Rouge of fill-in-the-blank questions, (e) Rouge 1 of fill-in-the-blank questions, (f) Rouge 2 of fill-in-the-blank questions, (g) BLEU of fill-in-the-blank questions, (h) GPT-4 judge score of dialogue benchmark, (i) Claude-3.5 judge score of dialogue benchmark. Base, performance of base models; DM, performance of fine-tuned models. 7 Response quality improvement from self-distillation We proposed self-distillation method, inspired by previous work25, as part of the data refining process. This method is effective in reducing the data distribution shift relative to the knowledge contained in the LLM, thereby improving the response quality of the LLM after fine-tuning on such data. Specifically, we conducted additional experiments to demonstrate that our self-distillation method can enhance model performance on the dialogue evaluation. Self-distillation fine-tuning outperformed vanilla fine-tuning by delivering scores of 7.81 (from GPT-4's judgement) and 7.80 (from Claude-3.5s judgement), compared to 6.32 and 6.71. Besides, our proposed method showed improved results compared to the original approach, with scores of 7.81 and 7.80 versus 7.29 and 7.53 (Supplementary Table 4). This advancement revealed the potential to significantly improve the quality and relevance of AI-generated responses in diabetes management applications, ultimately providing better support for healthcare providers and patients alike. The importance of careful dataset collection Although many open-source medical datasets26,27 contain diabetes-related content, they often suffer from low quality. This is primarily because they are mostly collected from the web without adequate cleaning or refinement. To address this issue, we manually collected high-quality data from various sources and performed comprehensive data processing to create the Diabetes-QA dataset. To demonstrate the superiority of the Diabetes-QA dataset over existing open-source medical datasets with diabetes-related content, we fine-tuned models on both types of datasets and compared their performance. The model tuned on our Diabetes-QA achieves superior performance in all benchmarks by showcasing relative 10% average increase on the multiple-choice questions, 33% average increase on the fill-in-the-blanks task, and 34% improvement on the single-round dialogue evaluation (Supplementary Table 4). These significant performance improvements underscored the value of our meticulously curated Diabetes-QA dataset. By prioritizing data quality and relevance, we have created resource that enables more accurate and effective diabetes-specific language models, potentially leading to improved traditional diabetes management. Clinical evaluation In this section, we explored three potential clinical applications, including providing healthcare consulting advice, assisting medical education, and streamlining clinical tasks. Performance on medical counseling We first explored the potential of Diabetica in medical consulting using 20 online patient cases. Three endocrinology specialists were asked to rate the readability, relevance, correctness, completeness, helpfulness, and empathy of responses from Diabetica and doctors using 5-point Likert scale. Regarding the ordinal ratings associated with the quality dimensions mentioned above, Diabeticas responses significantly exceeded human responses with mean (and the corresponding standard deviation SD) values of 4.78 (0.42) for readability, 4.95 (0.22) for relevance, 4.78 (0.45) for correctness, 4.80 (0.40) for completeness, 4.82 (0.39) for safety, and 5.00 (0) for empathy (all values <0.001, Figure 5). Supplementary Table 5 contains scores separated by individual readers and affirms the reliability of scores across readers by displaying positive intra-reader correlation values. Additionally, the percentage of selected superior Diabetica responses was 80.0%, suggesting that the Diabetica model was superior to doctor responses based on expert evaluations. There are some example questions with doctor and Diabetica response in Supplementary Figure 2. These results demonstrated the potential of Diabetica in providing high-quality 8 healthcare consulting. We also presented an example video of conversation between patients and Diabetica (Supplementary information). d g Figure 5. Performance on medical counseling. Performance comparison of the AI-generated and doctor-delivered responses of online patient cases (n=20). Evaluation was based on the expert panel review including (a) readability, (b) relevance, (c) correctness, (d) completeness, (e) safety, (f) empathy, and (g) selected superior responses. Bar graphs indicate the mean s.e.m., ***P < 0.001, calculated by paired-Wilcox test. Performance on medical education Furthermore, we evaluated the model performance in medical education by recruiting medical students and doctors with different levels of clinical experience for human-machine comparisons. Diabetica achieved an accuracy of 84.4% on type A2 multiple-choice questions, outperforming medical students (53.7%), junior physicians (69.7%), and intermediate physicians (74.0%), and slightly surpassing senior physicians (83.5%) (Figure 6a). These results suggested that our Diabetica model achieved comparable, and even superior proficiency with human physicians on diabetes specialist exams. To move beyond statistical measures on exams, we explored the capability of Diabetica in the medical education scenario by having it explain incorrect answers to medical students. Three medical students reviewed the explanation for their previously incorrect answers from both reference textbook and Diabetica, and scored their readability and helpfulness using 5-point Likert scale. As shown in Figure 6b, among the 107 questions, Diabeticas explanations were considered helpful (71.96%) and readable (65.42%) by the medical students, with quality comparable to that of the reference answers. The difference of the mean readability and helpfulness score between Diabetica and reference explanations is not significant (readability: 3.67 vs 3.85; helpfulness: 3.89 vs 3.94, all values > 0.05, Figure 6c). An example of the explanation generated by Diabetica is presented in Supplementary Figure 3, showing comparable expertise and greater empathy than reference explanation. 9 b Figure 6. Performance on medical education. (a) Accuracy of medical students, physicians with different levels, and LLMs in the MCQ examination. The accuracy here refers to the correctness rate of A2 type multi-choice questions. (b) Student evaluation of the helpfulness and readability of answer explanations from Diabetica and reference. (c) The readability and (d) helpfulness scores of answer explanations from Diabetica and reference. ns, no significant difference, calculated by paired-Wilcox test. Performance on record summarization Another helpful application of LLM is assisting doctors in summarizing patient records, which can streamline clinical tasks and reduce the burden on physicians. Here we presented an example of record generated by our Diabetica model. Supplementary Figure 4 shows that our model can reorganize plain language medical history into structured data, including disease course, symptoms, signs, blood glucose, complications and past treatment. This structured format enhances the records readability, making it more accessible for patients and later analysis. The model also provides thorough medical advice, including diagnosis, rationale, further examinations and treatment suggestions, all presented in concise, web-friendly format for clarity and sharing. Additionally, we conducted cross-over AI-assistance study to explore the potential of Diabetica as clinical support tool. Our results showed that the time usage of records written with Diabetica assistance 10 was about 23% shorter than that without assistance (750 seconds/case vs. 976 seconds/case, value < 0.05) Meanwhile, the completeness score of records written by intern doctors with Diabetica assistance was significantly higher than that without assistance (4.88 vs. 4.38, value < 0.001). Whereas there were no statistical differences in conciseness and correctness between the two groups (Figure 7a-e). Finally, to capture the interns perceptions and satisfaction towards the Diabetica system, the eight participated interns were also asked to complete user satisfaction questionnaire. Results revealed that the Diabetica system obtained an average score of 3.75 for providing complete and accurate summary (out of 5.00), 4.13 for time-saving, and 4.00 for being used in future clinical practice. Five of eight intern doctors indicated that they preferred to have AI assistance when writing medical records (Figure 7f). d Figure 7. Performance on clinical record summarization. (a-e) Comparison of patient records summarized by doctors with/without Diabetica assistance. Evaluation metrics include (a) usage time, (b) completeness, (c) conciseness, (d) correctness and (e) selected preferred responses. (f) Satisfaction of participated doctors (score ranges from 1-5). Bar graphs indicate the mean s.e.m. , *P<0.05, ***P < 0.001, ns, no significant difference. The results suggested that Diabetica, as an assistant tool for summarizing clinical records, can streamline clinical workflows and was well-accepted by most physicians. Discussion In this study, we developed diabetes-specific LLM by fine-tuning the open-source Qwen2 model using carefully curated specialized datasets. Our model demonstrated superior performance on various diabetesrelated assessment benchmarks, including multiple-choice questions, fill-in-the-blank questions, and dialogue tasks, surpassing other open-source models of similar size and even matching or exceeding stateof-art proprietary LLMs. Furthermore, clinical evaluations have confirmed the effectiveness of our model in patient consulting, medical education, and optimizing clinical workflows, showcasing its potential for diverse applications in diabetes management facing different end users. Our study provides feasible framework to develop domain-specific large language model. Data privacy and quality are significant constraints in the development of large language models (LLMs)28,29. 11 Although proprietary model performance such as GPT-4 has demonstrated superior performance, their adoption in real-world clinical settings is constrained by cost considerations and data privacy regulations. In addition, while open-source models can be deployed locally, their effectiveness in medical specialties is hampered by the scarcity of high-quality specialist data. In contrast, our study is more clinically appropriate in terms of base model selection, dataset curation, and training algorithms. First, we opted for Qwen2 as our base model, leveraging its robust performance and suitable size for hospital deployment. Second, by collecting both open-source and proprietary data, we contributed diabetes specialty dataset. Through targeted optimization of various data types, our dataset showcases its capacity to bolster the performance of diverse base models. Furthermore, we introduced an innovative approach combining self-distillation with supervised fine-tuning during training. Note that Reinforcement Learning from Human Feedback (RLHF)20 is frequently used to improve the LLM alignment with human preference while vanilla fine-tuning often struggles. However, RLHF always requires expensive preference-labeling process30. By only conducting finetuning, our self-distillation method has proven effective in facilitating models to acquire new knowledge while mitigating forgetfulness. In general, our approach is feasible and favorable for generalization. Our study contributes three diabetes-related evaluation datasets to fully assess the model performance. Evaluating large language models remains challenging, with the selection of appropriate evaluation datasets and methods being crucial31. Previous studies have primarily relied on public evaluation sets, which may suffer from data leakage and often focus on single evaluation dimension, thereby limiting comprehensive assessment of model performance. In our study, we proposed three diabetes-related evaluation datasets: multiple-choice questions, fill-in-the-blank questions, and open-ended dialogues. The fill-in-the-blank and multiple-choice datasets include standard answers to assess the model's knowledge accuracy, simulating medical exam scenarios. For the open-ended dialogues benchmark, each question was annotated by physicians with specific guidelines, against which state-of-the-art LLMs (i.e., GPT-4 and Claude-3.5) scored the responses. This method incorporates human annotations with medical expertise and thus provides more comprehensive and accurate evaluation compared to direct scoring by state-of-the-art LLMs. Furthermore, it reduces the workload on physicians and minimizes human bias inherent in individual model assessments. Through these benchmarks, we conducted detailed comparison of our model against other existing modelsproprietary, generic, and specialistacross broad spectrum of dimensions, affirming the superior performance of our model and establishing new benchmark for future diabetes-related evaluations. Our study provides specialized LLM that can address various clinical applications in diabetes management. Specialized large models in the field of diabetes have numerous potential applications32. Traditional diabetes management models often struggle with natural language understanding and human interaction. recent study introduced diabetes model, which integrated image-based deep learning and Llama and showed good performance in primary diabetes care17. However, this model mainly focused on singular tasks and the end users were only physicians. Unlike previous single-task oriented medical LLMs, our specialized LLMs, including Diabetica-1.5B and Diabetica-7B, are designed to handle wide range of diabetes-related task. Our research demonstrates that our diabetes-specific model excels across multiple medical tasks and could provide help to various populations. During patient consulting, it offers patients more comprehensive information and greater empathy compared to online doctors. In medical education, the model's examination ability reaches the expert level and can provide students with readable and useful explanations of topics. In clinical assistance, our model can assist trainee doctors in writing medical records, significantly reducing writing time and potentially streamlining clinical workflows to alleviate doctors' burdens. Overall, our model has shown robust performance across various aspects of diabetes management and is poised to further enhance diabetes care as datasets continue to expand. However, our study has several limitations. First, our dataset primarily consists of Chinese data as we are from Chinese hospitals, and we have not yet evaluated its performance on English datasets. Second, our clinical validation remains limited to offline simulation studies. Future research should include larger-scale evaluations in real clinical settings to assess the practical applicability of our model. Third, as medical knowledge evolves, ongoing iterative optimization of our model is necessary. Future enhancements could integrating methods such as retrieval-enhanced generation (RAG)33,34 to enhance the involve professionalism and quality of responses. In summary, we have developed an open-source, high-performance, and diabetes-specialty LLM family, showcasing its potential clinical applications and establishing research framework for constructing similar specialty models. Moving forward, continuous optimization and broader clinical evaluations are essential to validate the effectiveness and reliability of future models. Methods Overall study design We describe the details of our methods in four main sections, aligning with the study aims and the results section. The first section describes the dataset collection and processing (Figure 1a). The second section describes the development of Diabetica (Figure 1b). The third section describes the benchmark assessment of Diabetica performance and ablation studies (Figure 1c). The fourth section describes the clinical applications of Diabetica, including examination education, medical consulting, and clinical record summarization (Figure 1d). Additional contexts of ethics approval and statistical analyses are detailed at the end. Dataset collection Our datasets include public multi-choice questions and medical SFT datasets, as well as our private in-house dataset derived from guidelines, textbooks, drug labels and real-world dialogues. Public multi-choice questions banks To enhance the model's ability to recognize key information, series of open-source multiple-choice question banks were incorporated into our training, including MedQA35, MedMCQA36, MMLU22, CMMLU37, CMB38 and CMExam39. detailed description of these banks can be found in the Supplementary information. Public medical SFT datasets In order to make open-source models aligned with humans in medical area, some teams have constructed and open-source parts of their SFT datasets for public use. We collected these public medical SFT datasets from various open-source platforms, including CMtMedQA40, Qizhen, ChatMed41, cMedQA242, and DISCMed-SFT27. 13 Endocrinology guidelines and textbooks To enable the model to have comprehensive understanding of diabetes domain knowledge, we collected series of guidelines and textbooks on diabetes. We also utilized the DiaKG43 dataset, high-quality Chinese Diabetes knowledge graph derived from 41 diabetes guidelines and expert consensus, which encompasses wide spectrum of diabetes-related topics from clinical research, pharmacology, and case studies to diagnostic and treatment protocols. Drug label In addition to general diabetes knowledge, we collected labels of anti-diabetic medications to reinforce the model's knowledge of drug therapy. The instructions, derived from Chinese drug label site, cover the indications, dosage, adverse reactions, contraindications, precautions, uses in special populations, drug interactions, pharmacology and toxicology, pharmacokinetics, and storage. Real-world dialogues To further enhance the model's understanding of diabetes specialty knowledge, we also collected 100 diabetes-related specialty questions covering diabetes prevention, diagnosis, treatment, education, blood glucose monitoring, and so on. Endocrine specialists then answered these questions in detail, based on guidelines and their clinical experience. Data processing Data filtering We first conducted data filtering, including keywords filtering and deduplication, to construct diabetesrelated dataset. Keywords filtering. To extract diabetes-related questions from our endocrinology MCQ dataset, we developed keyword filtering system that incorporated both positive and negative matching. For positive matching, we identified and used keywords such as diabetes, DKA (diabetic ketoacidosis), blood sugar, HbA1c (hemoglobin A1c), pancreas, as well as the names of commonly prescribed diabetes medications. For negative matching, we crafted specific list of exclusion keywords after thoroughly reviewing the dataset content. These exclusion keywords included terms like insulinoma, short bowel syndrome, and hypopituitarism, which are not directly related to diabetes. After the initial filtering process, we conducted manual review to ensure the accuracy and relevance of the selected questions. This combination of automated keyword filtering and manual revision helped us accurately identify and curate comprehensive set of diabetes-related datasets from the original dataset. Deduplication. As training LLMs on duplicates and near-duplicates is harmful to the performance44-46, its crucial to apply suitable deduplication method to remove redundant data points from the collected dataset. To achieve this, we utilized SemDeDup45, deduplication method which leverages embeddings from pretrained model to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. In particular, we firstly embed each data point using pre-trained embedding model (bge-large-zhv1.547). Then, we clustered the embeddings into clusters via k-means. Within each cluster, we computed all pairwise cosine similarities to measure the semantic distance and set threshold cosine similarity above which data pairs are considered semantic duplicates. Finally, from each group of semantic duplicates within 14 cluster, we kept the data points with longer lengths and removed the rest, which is based on the assumption that longer data may naturally contain more detailed information48. Data augmentation To make the data format meet the subsequent training requirements and construct formatted dataset, we performed data augmentation for datasets with different formats. Data augmentation from long textual data. For long textual data (like guidelines, textbooks, and drug labels), we first divided these texts into entries based on knowledge points, and then employed GPT-4 to create dialogues from each section, utilizing two-step augmentation strategy detailed in the Supplementary information. total of 2538 dialogues were created. Meanwhile, we employed GPT-4 to create fill-in-theblank data, using another prompt in Supplementary information. Data augmentation from multi-choice questions. For multi-choice question banks, we refined the method by Quzhe Huang et al49 to generate instruction-response pairs. First, we used regular expressions to integrate each question with its four options into unified, coherent question in Chinese. Then, we utilized ChatGPT3.5 to make these new questions more fluent, using the prompt described in Supplementary information. Subsequently, these modified questions were inputted into GPT-4, which was tasked with generating reasoning explanations via chain-of-thought approach, followed by giving the answers (refer to Prompt in Supplementary information). To ensure accuracy, only instruction-response pairs with verified correct answers were retained. This methodology resulted in collection of 6592 samples. Data refinement Given that data quality is key determinant of model performance, we further conducted data refinement to construct high-quality dataset. Motivated by previous research25 that designs self-distillation method to enhance model performance during the continual fine-tuning, we apply an improved self-distillation pipeline. This approach is effective in our case for reducing the data distribution shift relative to the knowledge contained in the LLM. Though LLMs showcase outstanding performance in various language tasks, they often face limitations with downstream tasks that require continual fine-tuning. Specifically, we refer to an LLM in need of finetuning as seed LLM, denoted as 洧녭 and parameterized by 洧랚. The seed LLM typically undergoes vanilla finetuning to map any natural language instruction 洧논 洧녦 to its corresponding output 洧녽 洧녧 (i.e., 洧녭洧랚: 洧녦 洧녧) by updating the model parameters. This update aims at minimizing the disparity between the data distribution and the LLM distribution: 洧쯨anilla(洧랚) = log 洧녭洧랚 ( 洧녽 洧논 ), (1) which seeks to minimize the negative log likelihood of the target output 洧녽 given the input 洧논 with the model parameters 洧랚. 洧 converges when the generated response 洧녽 matches 洧녽 , i.e., the distribution of fine-tuned LLM aligns with the task data distribution. This process can inject the knowledge contained in the data into the LLM. However, vanilla fine-tuning an LLM on collected dataset, whose distribution is far from the LLMs, may be harmful to the LLMs original alignment with human preference and lead to catastrophic forgetting in general instruction-following capabilities, which consequently results to the decrease of LLMs response quality50. 15 To address these issues in vanilla fine-tuning, we propose modified self-distillation (SD) pipeline to make the LLM better align the distribution of the collected dialogue dataset as depicted in Supplementary Figure 5. In particular, the self-distillation pipeline contains two steps, which impose minimal requirements on the seed LLM. Firstly, we collect the seed LLMs own response 洧녽 of each instruction 洧논 in our dataset. Secondly, we simply use specific prompt 洧녷 (shown in Supplementary information) to let the seed LLM generate refined response 洧녽 based on the instruction 洧논 , the original response 洧녽 and its own response 洧녽. The original response is accurate, reflecting the intended diabetes knowledge and information. The subsequent seed LLMs own response aligns with the internal distribution of the seed LLM. Note that including the seed LLM generated response in the self-distillation pipeline is the main difference between our improved method and the raw one25. Rewriting based on these two responses, the seed LLM can create refined response, ensuring its accuracy and alignment with the LLMs distribution. These steps mark the primary distinction between our method and vanilla fine-tuning, as it involves mapping the original response into refined response within the seed LLM's distribution. Finally, the rewritten response 洧녽 is used to replace the original response 洧녽洧노 in the fine-tuning stage, and the loss of self-distillation becomes: 洧쯉D(洧랚) = log 洧녭洧랚 ( 洧녽 洧논 ). (2) Hence, the distribution gap between the model and dataset is mitigated by utilizing the distilled dataset instead of the original dataset, and the loss function in Equation (2) converges more efficiently than that in Equation (1). This newly generated dataset from self-distillation can not only help model learn new knowledge, but also restore the models generic knowledge distribution. Modelling Architecture The Diabetica-7B (based on Qwen2-7B-Instruct12) is built upon the foundational Transformer architecture51. The models core consists of stack of Transformer layers, each incorporating self-attention mechanisms with causal masks and feed-forward neural networks (FFNs). Notably, it uses Grouped Query Attention (GQA)52 in place of the traditional multi-head attention (MHA). GQA optimizes the utilization of the keyvalue (KV) cache during inference, resulting in substantial improvements in throughput. Furthermore, Diabetica-7B employs several architectural enhancements to boost performance and training stability. It utilizes SwiGLU53 as the activation function, which has demonstrated superior performance in language modeling tasks. Rotary Positional Embeddings54 are incorporated to effectively capture positional information, while QKV bias is applied to the attention mechanism, enhancing the model's ability to extrapolate to longer sequences. To ensure training stability, Diabetica-7B also adopts RMSNorm55 and pre-normalization. The detailed architecture of Diabetica is shown in Supplementary Figure 6. Supervised fine-tuning We trained Diabetica-7B from the Qwen2-7B-Instruct weights12, and applied supervised fine-tuning pipeline. We followed the default chat template of Qwen2 in finetuning with system prompt You are helpful assistant at the beginning of the (instruction, response) pair. Instead of updating full parameters of the model during its training, we utilize LoRA56 training as parameter-efficient fine-tuning method. LoRA 16 training involves freezing the model weights and incorporate trainable rank decomposition matrices, called LoRA adapters, into different layers of the transformer architecture. In our experiments, LoRA adapters were integrated into the attention and MLP layers, with additional training on embeddings and all linear layers. We utilized 4 24GB-NVIDIA-4090 GPUs for two epoch fine-tuning. The AdamW optimizer was used with 1e-5 learning rate and the LoRA parameters dimension, alpha, and dropout are set to 64, 16, and 0.1, with batch size of 64. Diabetica family To test our data on smaller size of model and offer more deployment options, we also trained Diabetica1.5B (based on Qwen2-1.5B-Instruct) using the same training configuration and dataset of Diabetica-7B. Consequently, the Diabetica family is comprised of 7B and 1.5B models, with Diabetica-7B suitable for GPUequipped desktops and Diabetica-1.5B for laptops, ensuring wide applicability across different hardware configurations. Benchmark assessment To comprehensively assess the potential of LLMs in diabetes management, we chose three distinct tasks: multiple-choice questions, fill-in-the-blank questions, and open-ended questions. Multiple-choice questions tested the ability to recall medical knowledge and identify critical points. Fill-in-the-blank tasks assessed contextual understanding and text generation. Open-ended dialogue responses gauged reading comprehension, knowledge manipulation, and empathy. We describe each task and dataset below. We also compared our model with other LLMs. Benchmarks and evaluation metrics Multiple choices questions. The benchmark for multiple choices questions was comprised of 312 multiplechoice questions, specifically 235 Type A1 and 77 Type A2 questions, extracted from the Advanced Health Professional Technical Qualification Examination. Type A1 questions were designed to assess the examinee's foundational knowledge in endocrinology, encompassing broad range of topics from the pathophysiology of various diabetes forms to the pharmacological fundamentals of antidiabetic medications. Conversely, Type A2 questions were crafted within specific clinical contexts, challenging examinees to apply their knowledge in diagnosing and making evidence-based medical decisions. We used accuracy that measures the percentage of correct answers given by model for multiple-choice questions. In addition, we also conduct experiments to test for benchmark memorization in the Diabetica-7B model (Supplementary information). Fill-in-the-blanks task. Besides the Multiple-choices questions, fill-in-the-blanks task is another popular exam type in medical education. Therefore, we manually created set of fill-in-the-blanks questions. The fill-inthe-blanks benchmark includes 35 questions from the guideline and textbook. We used five evaluation metrics: BERTScore57, ROUGE-L58, ROUGE-158, ROUGE-258, and BLEU59, to assess the performance in fill-in-the-blank tasks. BERTScore is used to evaluate the similarity between the predicted text and the reference text. It compares the semantic meaning of sentences rather than just matching exact words, providing more nuanced measure of performance. ROUGE-L measures the longest common subsequence between the predicted text and the reference text. This metric helps to assess the quality of the predicted text in terms of its similarity to the reference text, particularly focusing on how well the sequences align. ROUGE-1 quantifies the overlap of unigrams between the generated content and the reference content, while ROUGE-2 evaluates the overlap of bigrams between the generated content and the reference content. BLEU is another commonly used metric that measures the precision of n-grams (usually up to 4) in the generated text against one or more reference texts. All metrics range from 0 to 1, higher score indicates higher similarity with reference. Open-ended dialogue evaluation. To evaluate the model's dialogue capabilities in real world applications, we constructed single-round and open-ended dialogue evaluation dataset containing 120 questions covering various aspects of diabetes. In the benchmark, each instance consists of three elements: category, question, and the associated rules, as depicted in Figure 3a. For each instance, physicians annotated comprehensive set of rules that define the criteria for evaluating the quality of an answer. Note that evaluating LLM based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on these open-ended questions. Since GPT-4 and Claude-3.5 are the stateof-art proprietary LLMs with level of intelligence close to that of humans, we utilized them to judge the performance of the model's answers19. These two LLMs were asked to review the instructions carefully, and rate each answer on scale of 1-10 based on the human rule (see detail prompt in Supplementary information). Comparison with other large language models We compared Diabetica to large amount of models as our baselines, including proprietary LLMs like GPT-4 and Claude-3.5, open-source general LLMs like Qwen2-7B, InternLM2-7B, Llama3-8B and Yi-1.5-9B, as well as open-source medical LLMs like Meditron-7B, MMedLM-7B and Apollo-7B. Detailed descriptions of these large language models are presented in Supplementary Table 6. Alleviating catastrophic forgetting Catastrophic forgetting60 is common issue when fine-tuning the LLM, where the LLM loses previously acquired knowledge while learning new information. To mitigate this, we utilized LoRA56 training and selfdistillation25 strategy in our fine-tuning stage. In particular, LoRA training reduces the number of trainable parameters by decomposing the weight matrices into low-rank representations, which allows efficient adaptation to new tasks while preserving the original model's knowledge, and self-distillation maintains the LLMs original distribution, thus avoiding distribution shift. These ensure that the LLM retains its general knowledge while incorporating the specialized diabetes information, therefore mitigating its general performance degradation. In particular, we evaluated the effectiveness of our strategy using suite of general benchmarks that measure the general language understanding abilities, including MMLU22, GSM8K21, and C-Eval23. Ablations To gain deeper understanding of our results, we conducted series of ablation studies across various benchmarks. Our investigation concentrated on three primary areas, allowing us to systematically evaluate the contributions of each component as follows. Robustness of Diabetes-QA dataset. We conducted fine-tuning on the Diabetes-QA dataset using various popular base LLMs, such as Qwen2-7B-Instruct, Llama3-8B-Instruct, Yi-1.5-9B-Chat, and InternLM2-7B-Chat. This was done to validate that our dataset can improve diabetes knowledge across different models. 18 Response quality improvement from self-distillation. Inspired by previous work, we proposed selfdistillation method as part of the data refining process. We aim to verify that our method helps reduce data distribution shifts relative to the knowledge contained in the LLM, thereby improving response quality after fine-tuning. The importance of careful dataset collection. We compared the performance of models fine-tuned on our manually collected and refined Diabetes-QA dataset with those fine-tuned on existing public medical datasets containing diabetes-related content. This was done to demonstrate the importance of high-quality, curated data. The evaluation method for the ablation studies followed the same procedure as the evaluation of Diabetica. Clinical evaluation To explore the performance of LLM in diabetes care clinical scenarios, we conducted clinical evaluations in three distinct settings: online patient consulting, medical exam education, and assisting doctors with record summary. Online medical consulting compared with doctors We curated dataset comprising 20 cases of diabetes-related inquiries from Chinese online consulting platform between July 1, 2024, and July 3, 2024. Each case includes patient queries and associated physician responses. Informed consent was not required because the data were public and did not contain identifiable information. The full text of the case was put into Diabetica and the chatbot response was saved. An expert panel of three licensed healthcare professionals independently reviewed each case, consisting of the patient's inquiry, the physician's response, and the chatbot's reply. Responses were anonymized, randomized, and labeled as Response 1 or Response 2 to ensure evaluator blinding. Evaluators assessed responses based on readability, relevance, correctness, completeness, safety, and empathy using predefined criteria detailed in Supplementary Table 7. Ratings were conducted on 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). Evaluators were also asked to compare these two responses and select the superior one. MCQ examination compared with students and doctors In the medical education scenario, we initially compared the accuracy of LLM responses with those of medical students and doctors at different experience levels. The study involved 12 participants divided into four groups of three individuals each: medical students, junior doctors, mid-level doctors, and senior doctors. Considering the workload and difficulty of the questions, we selected the A2-type questions as the evaluation dataset. Each participant independently completed 67 A2 type multiple-choice questions, and their accuracy was recorded and compared with Diabeticas responses. Subsequently, we investigated the model's ability to provide explanations for incorrect answers. Using specific prompts, the model explained questions previously answered incorrectly by students, which were then evaluated for readability (The explanation is easy to understand) and helpfulness (The explanation is helpful) by the respective students using 5-point Likert scale. Students also need to rate the reference explanations from textbooks. 19 importance AI-assistance study in the clinical summarization task To evaluate the effectiveness and efficiency of Diabetica, we assembled dataset comprising five real-life cases involving various aspects of diabetes. Eight intern physicians were involved in the multi-reader multicase (MRMC) study and were asked to write records from five patients based on multi-turn dialogues with doctors. Using crossover design, we randomly and equally divided the interns into group (first read cases without Diabetica assistance) and group (first read cases with Diabetica assistance). After washout period of 1 week, the arrangement was reversed. The overall time of each intern for reading these cases was recorded and the quality of records was accessed by three experts. The evaluation metrics of quality include completeness (containing all clinical information), conciseness (without superfluous information), and correctness (without any errors), using predefined criteria detailed in Supplementary Table 8. Ratings were conducted on 5-point Likert scale, ranging from 1 (strongly disagree) to 5 (strongly agree). We then compared the record quality and time usage of doctors in scenarios with and without Diabetica assistance. Furthermore, interns were invited to complete satisfaction questionnaire within one weeks after the conclusion of the study. The questionnaire included four-item questions assessing these interns views regarding the integration of Diabetica into clinical practice. The study design is shown in Supplementary Figure 7. Statistical analysis In all our studies, categorical result values were expressed as frequencies (percentages) and were compared with chi-square tests for value. Continuous result values were expressed as mean (SD) and were compared with MannWhitney test or paired Wilcox test for value. p-value <0.05 was considered statistically significant and significances were indicated as < 0.05 (*), < 0.01 (**), and < 0.001 (***). Ethics approval This study adhered to the principles outlined in the Declaration of Helsinki. This study used only retrospective, de-identified data that fell outside the scope of institutional review board oversight. Data availability Interested investigators can obtain and certify the data transfer agreement and submit requests to Weiran Huang (weiran.huang@outlook.com) or Ying Chen (chen.ying4@zs-hospital.sh.cn). Investigators who consent to the terms of the data transfer agreement, including, but not limited to, the use of these data only for academic purposes, and to protect the confidentiality of the data and limit the possibility of identification of patients, will be granted access. Requests will be evaluated on case-by-case basis within one month before receipt of response. All data shared will be deidentified. Acknowledgements This study was funded by National Natural Science Foundation of China (62406192) to W. H, the Shanghai Municipal Health Commission (2022JC015) to X.L. and Clinical innovation project of Shanghai Science and Technology Commission (23Y11904800) to Y.C.. Author contributions Y. C., W. H. and X. L. conceptualized and led the research project. L. W., Z. Y., M. H., YT. C., Q. Y., Y. H., and J. L. performed the experiments. L. W., Z. Y., and M. H. analyzed the results, plotted the figures, and drafted 20 the manuscript. Y. C., and W. H. supervised the projects, approved the submission and accepted responsibility for the overall integrity of the paper. Competing interests The authors declare no competing interests."
        },
        {
            "title": "Reference",
            "content": "1. Sun, H., et al. IDF Diabetes Atlas: Global, regional and country-level diabetes prevalence estimates for 2021 and projections for 2045. Diabetes Res Clin Pract 183, 109119 (2022). 2. Guan, Z., et al. Artificial intelligence in diabetes management: Advancements, opportunities, and challenges. Cell Rep Med 4, 101213 (2023). 3. da Silva Santos, T., et al. MODY probability calculator utility in individuals' selection for genetic testing: Its accuracy and performance. Endocrinol Diabetes Metab 5, e00332 (2022). 4. Rabie, O., Alghazzawi, D., Asghar, J., Saddozai, F.K. & Asghar, M.Z. Decision Support System for Diagnosing Diabetes Using Deep Neural Network. Front Public Health 10, 861062 (2022). 5. Wang, G., et al. Optimized glycemic control of type 2 diabetes with reinforcement learning: proof-ofconcept trial. Nature Medicine 29, 2633-2642 (2023). 6. Arcadu, F., et al. Deep learning algorithm predicts diabetic retinopathy progression in individual patients. NPJ Digit Med 2, 92 (2019). 7. Dai, L., et al. deep learning system for detecting diabetic retinopathy across the disease spectrum. Nat Commun 12, 3242 (2021). 8. Lee, P., Bubeck, S. & Petro, J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. Engl Med 388, 1233-1239 (2023). 9. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet. 10. Dubey, A., et al. The Llama 3 Herd of Models. Preprint at https://arxiv.org/pdf/2407.21783 (2024). 11. Ai, et al. Yi: Open Foundation Models by 01.AI. Preprint at https://arxiv.org/pdf/2403.04652 (2024) 12. Yang, A., et al. Qwen2 Technical Report. Preprint at https://arxiv.org/pdf/2407.10671(2024) 13. Zhang, G., et al. Closing the gap between open source and commercial large language models for medical evidence summarization. NPJ Digit Med 7, 239 (2024). 14. Van Veen, D., et al. Adapted large language models can outperform medical experts in clinical text summarization. Nat Med 30, 1134-1142 (2024). 15. Chen, X., et al. FFA-GPT: an automated pipeline for fundus fluorescein angiography interpretation and question-answer. NPJ Digit Med 7, 111 (2024). 16. Zhou, J., et al. Pre-trained multimodal large language model enhances dermatological diagnosis using SkinGPT-4. Nat Commun 15, 5649 (2024). 17. Li, J., et al. Integrated image-based deep learning and language models for primary diabetes care. Nat Med (2024). 18. Thirunavukarasu, A.J., et al. Large language models in medicine. Nat Med (2023). 19. Zheng, L., et al. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. in Proceedings of the 37th International Conference on Neural Information Processing Systems Article 2020 (Curran Associates Inc., New Orleans, LA, USA, 2024). 20. Ouyang, L., et al. Training language models to follow instructions with human feedback. Vol. 35 (eds. Koyejo, S., et al.) 27730-27744 (Curran Associates, Inc.). 21 21. Cobbe, K., et al. Training Verifiers to Solve Math Word Problems. Preprint at https://arxiv.org/pdf/2110.14168. 22. Hendrycks, D., et al. Measuring Massive Multitask Language Understanding. (International Conference on Learning Representations, 2021). 23. Huang, Y., et al. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Fo undation Models. Vol. 36 (eds. Oh, A., et al.) 62991-63010 (Curran Associates, Inc.). 24. Cai, Z., et al. InternLM2 Technical Report. Preprint at https://arxiv.org/html/2403.17297v1 25. Yang, Z., et al. Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuni ng. (eds. Ku, L.- W., Martins, A. & Srikumar, V.) 1028-1043 (Association for Computational Linguistics). 26. Li, J., et al. Huatuo-26M, Large-scale Chinese Medical QA Dataset. Preprint at https://arxiv.org/pdf/2305.01526. 27. Bao, Z., et al. DISC-MedLLM: Bridging General Large Language Models and Real-World Med ical Consultation. Preprint at https://arxiv.org/pdf/2308.14346 28. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E.J. AI in health and medicine. Nat Med 28, 31-38 (2022). 29. Hathaliya, J.J. & Tanwar, S. An exhaustive survey on security and privacy issues in Healthcare 4.0. Comput. Commun. 153, 311-335. 30. Xu, S., et al. Is DPO Superior to PPO for LLM Alignment? Comprehensive Study. in Proceedings of the 41st International Conference on Machine Learning, Vol. 235 (eds. Ruslan, S., et al.) 54983--54998 (PMLR, Proceedings of Machine Learning Research, 2024). 31. Chang, Y., et al. Survey on Evaluation of Large Language Models. ACM Trans. Intell. Syst. Technol. 15, 39:31-39:45. 32. Sheng, B., et al. Large language models for diabetes care: Potentials and prospects. Science Bulletin, S2095-9273. 33. Lewis, P., et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Vol. 33 (eds. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F. & Lin, H.) 9459-9474 (Curran Associates, Inc.). 34. Hiesinger, W., et al. Almanac: Retrieval-Augmented Language Models for Clinical Medicine. Research Square. 35. Jin, D., et al. What disease does this patient have? large-scale open domain questio answering dataset from medical exams. Applied Sciences 11, 6421. 36. Pal, A., Umapathi, L.K. & Sankarasubbu, M. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. 248-260 (PMLR). 37. Li, H., et al. CMMLU: Measuring massive multitask language understanding in Chinese. (eds. Ku, L.- W., Martins, A. & Srikumar, V.) 11260-11285 (Association for Computational Linguistics). 38. Wang, X., et al. CMB: Comprehensive Medical Benchmark in Chinese. (eds. Duh, K., Gomez, H. & Bethard, S.) 6184-6205 (Association for Computational Linguistics). 39. Liu, J., et al. Benchmarking Large Language Models on CMExam-A Comprehensive Chinese edical Exam Dataset. Advances in Neural Information Processing Systems 36. 40. Yang, S., et al. Zhongjing: Enhancing the chinese medical capabilities of large languag model through expert feedback and real-world multi-turn dialogue. Vol. 38 19368-19376. 41. Zhu, W. & Wang, X. ChatMed: Chinese Medical Large Language Model, (GitHub https://github.com/michael-wzhu/ChatMed). 42. Zhang, S., Zhang, X., Wang, H., Guo, L. & Liu, S. Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection. IEEE Access 6, 74061-74071 (2018). 43. Chang, D., et al. DiaKG: an annotated diabetes dataset for medical knowledge graph const ruction. 308-314 (Springer). 22 44. Tirumala, K., Simig, D., Aghajanyan, A. & Morcos, A.S. D4: improving LLM pretraining via document deduplication and diversification. in Proceedings of the 37th International Conference on Neural Information Processing Systems Article 2348 (Curran Associates Inc., New Orleans, LA, USA, 2024). 45. Abbas, A., Tirumala, K., Simig, D., Ganguli, S. & Morcos, A.S. SemDeDup: Data-efficient learning at webscale through semantic dedupl ication. Preprint at https://arxiv.org/pdf/2303.09540. 46. Sachdeva, N., et al. How to Train Data-Efficient LLMs. Preprint at https://arxiv.org/pdf/2402.09668. 47. Xiao, S., et al. C-Pack: Packed Resources For General Chinese Embeddings. in Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval 641649 (Association for Computing Machinery, Washington DC, USA, 2024). 48. Zhao, H., Andriushchenko, M., Croce, F. & Flammarion, N. Long Is More for Alignment: Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning. in Proceedings of the 41st International Conference on Machine Learning, Vol. 235 (eds. Ruslan, S., et al.) 60674--60703 (PMLR, Proceedings of Machine Learning Research, 2024). 49. Huang, Q., et al. Lawyer LLaMA Technical Report. Preprint at https://arxiv.org/pdf/2305.15062. 50. Ren, M., et al. Learning or Self-aligning? Rethinking Instruction Fine-tuning. (eds. Ku, L.-W., Martins, A. & Srikumar, V.) 6090-6105 (Association for Computational Linguistics). 51. Vaswani, A., et al. Attention is all you need. in Proceedings of the 31st International Conference on Neural Information Processing Systems 60006010 (Curran Associates Inc., Long Beach, California, USA, 2017). 52. Ainslie, J., et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-He ad Checkpoints. (eds. Bouamor, H., Pino, J. & Bali, K.) 4895-4901 (Association for Computational Linguistics). 53. Dauphin, Y.N., Fan, A., Auli, M. & Grangier, D. Language Modeling with Gated Convolutional Networks. in Proceedings of the 34th International Conference on Machine Learning, Vol. 70 (eds. Doina, P. & Yee Whye, T.) 933--941 (PMLR, Proceedings of Machine Learning Research, 2017). 54. Su, J., et al. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing 568, 127063 (2024). 55. Jiang, Z., Gu, J., Zhu, H. & Pan, D. Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pr e-LN Transformers. Vol. 36 (eds. Oh, A., et al.) 45777-45793 (Curran Associates, Inc.). 56. Hu, E.J., et al. LoRA: Low-Rank Adaptation of Large Language Models. (International Conference on Learning Representations,2022) 57. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q. & Artzi, Y. BERTScore: Evaluating Text Generation with BERT. in International Conference on Learning Representations (2020). 58. Lin, C.-Y. ROUGE: Package for Automatic Evaluation of Summaries. 74-81 (Association for Computational Linguistics). 59. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: Method for Automatic Evaluation of Machine Translation. (eds. Isabelle, P., Charniak, E. & Lin, D.) 311-318 (Association for Computational Linguistics). 60. Ren, W., Li, X., Wang, L., Zhao, T. & Qin, W. Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning. Preprint at https://arxiv.org/pdf/2402.18865. Supplementary Tables Supplementary Table 1. Performance of different LLMs in the FB and MCQ benchmarks. Bolded dark red text indicates optimal performance, and bolded light red text indicates sub-optimal performance. 24 Supplementary Table 2. Performance of models with smaller sizes. Supplementary Table 3. Alleviating catastrophic forgetting. 25 Supplementary Table 4. Ablation studies. DM SFT means utilizing our collected diabetes-related dataset to fine-tune the base model. Bold indicates optimal performance. 26 Supplementary Table 5. Performance on medical consulting across different readers. Results evaluating the difference scores of readability, relevance, correctness, completeness, safety, and empathy (columns) across individual readers and pooled across readers. The scores are calculated by subtracting the human scores from the LLM scores, where positive scores denote that the LLM is preferred to the medical expert. Intra-class correlation (ICC) values across readers are on range of [1, 1] where 1, 0 and +1 correspond to negative, no and positive correlations, respectively. value was calculated by paired-Wilcox test. 27 Supplementary Table 6. Model information. 28 Supplementary Table 7. Evaluation metrics in the medical consulting task. Supplementary Table 8. Evaluation metrics in the clinical record summarization task. Supplementary Figures Supplementary Figure 1. Claude 3.5 judged scores of different LLMs in the dialogue benchmark. 30 Supplementary Figure 2. Examples of the LLM and physician responses of an online consulting case. 31 Supplementary Figure 3. Examples of the LLM and textbook explanations of the wrong answer. 32 Supplementary Figure 4. Example of record summary. 33 d 34 Supplementary Figure 5. Technical routes for model training. (a) The overall pipeline of self-distillation. Firstly, we collect the seed LLM's responses to each instruction in the dataset. Secondly, we use specific prompt to let the seed LLM generate refined response based on the instruction, the original response and its own response. Finally, the refined responses are combined into distilled dataset, which is subsequently used for supervised finetuning to develop Diabetica; (b) The original task dataset's distribution is far from the LLMs, while the distilled dataset can align with the seed LLMs generic knowledge distribution. 35 Supplementary Figure 6. Model Architecture. 36 Supplementary Figure 7. Design of the LLM-assistance study. 37 Supplementary Note 1. Description of multiple-choice question datasets The MedQA1 dataset is large-scale open-domain question-answering dataset from medical exams. We selected USMLE-style and MCMLE-style questions with four or five possible answers from this dataset. The MedMCQA2 dataset consists of more than 194,000 four-option multiple-choice questions from Indian medical entrance examinations (AIIMS/NEET). The CMB3 is medical benchmark in Chinese that contains comprehensive multi-level assessment for physicians, nurses, technicians, pharmacists, undergraduate disciplines, and graduate entrance exam medical knowledge. MMLU4 is an English dataset including exam questions from 57 domains, and we selected the subtasks most relevant to medical knowledge: anatomy, clinical knowledge, college biology, college medicine, medical genetics, nutrition, and professional medicine. CMMLU5 is comprehensive Chinese benchmark that covers various subject, and we select subtasks of anatomy, clinical knowledge, college medicine, genetics, nutrition, traditional Chinese medicine, and virology. CMExam6 is dataset from the Chinese National Medical Licensing Examination. It consists of 60K+ multiple-choice questions and five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Reference Jin, D., et al. What disease does this patient have? large-scale open domain questio answering 1. dataset from medical exams. Applied Sciences 11, 6421. 2. Pal, A., Umapathi, L.K. & Sankarasubbu, M. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. 248-260 (PMLR). 3. Wang, X., et al. Cmb: comprehensive medical benchmark in chinese. arXiv preprint arXiv:2308.08833. 4. Hendrycks, D., et al. Measuring Massive Multitask Language Understanding. (International Conference on Learning Representations, 2021). 5. Li, H., et al. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212. 6. Liu, J., et al. Benchmarking Large Language Models on CMExam-A Comprehensive Chinese edical Exam Dataset. Advances in Neural Information Processing Systems 36. 38 Supplementary Note 2. Prompts. Prompt 1: Prompt for generating QA pairs from guidelines and textbooks using two-step strategy 1. The prompt for creating questions: Please create <three different questions> that closely align with the provided <text>. Ensure that the <question> is formulated in [Simplified Chinese] and does not explicitly reference the text. You may incorporate specific scenarios or contexts in the <question>, allowing the <text> to serve as comprehensive and precise answer. Separate each question with ';.' <text>: 2. The prompt for answering each question: You are [DiabeteGPT], equipped with in-depth knowledge in [endocrinology]. Your task is to directly answer the user's <questions> in [Simpiflied Chinese]. In formulating your response, you must thoughtfully reference the <reference text>, ensuring that your reply does not disclose your reliance on <reference text>. Aim to provide comprehensive and informative response, incorporating relevant insights from <reference text> to best assist the user. Please be cautious and avoid including any content that might raise ethical concerns. Prompt 2: Prompt for generating fill-in-the-blank from guidelines and textbooks Create three 'fill in the blank' type of test questions from the given test as well as the answer. The answer should be excerpted from the original text. The length of the blank should be shorter than10 Chinese character. The answer should contain endocrinology terms. <text>: Prompt 3: Prompt for generating QA pairs from MCQ datasets 1. The prompt for creating questions: Please help me to make the following Chinese problem fluent, taking care not to add content or change the meaning of the text. Don't include special characters. <problem>: {question} Please output the modified Chinese question directly: 2. The prompt for answering each question: You are an endocrinologist. The following input is medical problem, please generate an elaborate step-bystep explanation to the problem and answer the problem with \"Yes\" or \"No\". Ensure that the <explanation> is formulated in Chinese <problem>: {question} Output format: <explanation> <answer> Prompt 4: Prompt for self-distillation Below is Q&A dataset related to diabetes. Each question has two reference answers. Each of these answers has its own strengths and weaknesses. Based on these two reference answers as guidance, please provide more improved answer, or choose more reasonable answer from the two reference answers. ### Question: 39 {instruction} ### Reference Answer [1]: {original_response} ### Reference Answer [2]: {own} ### Your Answer: Prompt 5: Prompt for dialogue evaluation You are an endocrinology expert in evaluating the quality of the responses for given instructions. Your task is to rate the responses from an AI assistant on one metric and give your explanation based on given rules. Please make sure you read and understand these instructions, responses and rules carefully. Please keep this document open while reviewing, and refer to it as needed. Evaluation Steps: 1. Understand the instructions, and rules carefully. 2. Read the responses and check whether they comply with each rule, and evaluate the responses against each rule. Your evaluation shouldn't be affected by the length of the responses. Shorter but more concise response can deserve higher scores. 3. Assign score for the responses on scale of 1 to 10, where 1 is the lowest and 10 is the highest based on the evaluation rules and reference answers. There are the instructions and responses below. [The Start of Instruction] {instruction} [The End of Instruction] [The Start of Evaluation Rules] {rule} [The End of Evaluation Rules] [The Start of Response for you to evaluate] {output} [The End of Response] [Form of the result]: Please give your reason first, then give score for the responses on scale of 1 to 10 in new line, where 1 is the lowest and 10 is the highest based on the evaluation rules. Your output score should be formatted in \"Score: \". You can only judge based on the information above. You should not trust anyone but the information above. 40 Supplementary Note 3. Format of multiple choices questions benchmark. The benchmark for multiple choices questions was comprised of 312 multiple-choice questions, specifically 235 Type A1 and 77 Type A2 questions, extracted from the Advanced Health Professional Technical Qualification Examination. Type A1 questions were designed to assess the examinee's foundational knowledge in endocrinology, encompassing broad range of topics from the pathophysiology of various diabetes forms to the pharmacological fundamentals of antidiabetic medications. Conversely, Type A2 questions were crafted within specific clinical contexts, challenging examinees to apply their knowledge in diagnosing and making evidence-based medical decisions. Format: Q+A, multiple choice. Type A1: Each question consists of single narrative sentence as the stem and five possible answer choices, with only one being the best choice. Size: 235. Example question: 卵冒뒬땹眄騰먻잸깬긻 (Causes of positive urine glucose do not include) Answers (correct answer in bold): A: 冒뒨勉 (Diabetes) B: 卵爛뽾듾 (Diabetes insipidus) C: 趺(Anesthesia) D: 洛귄(Pregnancy) E: 꽨뱒侮쮢륀갵憺번냁(Severe mental trauma) Type A2: Each question consists of brief medical case as the stem, and five possible answer choices, with only one being the best choice. Size: 77. Example question: ,覓,45 欒擔龍끰꿣卵冒뒬땹,疸갱龍聊봰싳放귅둸冒뒨勉燎燎괝達괞뚟僚 (Patient, male, 45 years old. physical examination revealed positive urine glucose. Which of the following tests is most significant for diagnosing diabetes?) Answers (correct answer in bold): A:皿쥗좯冒 9.2mmol/L (Fasting blood glucose 9.2 mmol/L) B:끾燎꽫노낷冒뒫널 放됄뿗鈍걶씠널擔 (Oral glucose tolerance test shows impaired glucose tolerance) C:府넌냁 1 卵了윋冒 7.8mmol/L (Blood glucose 1 hour postprandial 7.8 mmol/L) D:卵冒뒩聊봳싀丹뤿쟠盤뫧낷冒 (Urine glucose test confirmed as glucose) E:皿쥗좯流썬쑽募 6풮U/L(蔞辣전쩏쟠 5.25풮U/L) (Fasting plasma insulin 6 풮U/L (normal value is 5.25 풮U/L)) We use accuracy that measures the percentage of correct answers given by model for multiple-choice questions. 41 Supplementary Note 4. Format of fill-in-the-blanks benchmark. Besides the multiple choices questions, fill-in-the-blanks task is another popular exam type in human education. Therefore, we manually create set of fill-in-the-blanks questions. The fill-in-the-blanks benchmark includes 35 questions from the guideline and textbook. Format: Fill in the blank Type: fill-in-the-blank question consists of sentence with blanks, requiring the examinee to insert the correct words or phrases to complete the sentence. Size: 35. Example question: 蓼쩐갱긾쒽땶_____袂傅꽪씠꼺(Glibenclamide is _____ class of hypoglycemic drug.) Correct answers: 名쥗 (Sulfonylurea). We used five evaluation metrics: BERTScore, ROUGE-L, ROUGE-1, ROUGE-2 and BLEU, to assess the performance in fill-in-the-blank tasks. BERTScore is used to evaluate the similarity between the predicted text and the reference text. It compares the semantic meaning of sentences rather than just matching exact words, providing more nuanced measure of performance. Rouge-L measures the longest common subsequence between the predicted text and the reference text. This metric helps to assess the quality of the predicted text in terms of its similarity to the reference text, particularly focusing on how well the sequences align. ROUGE-1 quantifies the overlap of unigrams between the generated summary and set of reference summaries, providing straightforward metric of content similarity. ROUGE-2 evaluates the overlap of bigrams between the system-generated summary and the reference summaries, offering insight into the preciseness and continuity of the generated text. BLEU is another commonly used metric that compares candidate translation with one or more reference translations based on n-gram precision. 42 Supplementary Note 5. Evaluating benchmark memorization in LLMs. In this experiment, we aim to evaluate the memorization capabilities of models on the benchmark. In particular, we choose to analyze the multiple-choice-question benchmark by splitting each problem into two parts: the initial segment (A) and the true continuation (B). We then provide the initial segment (A) to each model and let it directly generate its own continuation (C), ensuring that the model operates at temperature setting of 0 to produce the most likely and deterministic output. The generated continuation (C) ends when the model produces an answer to the question. To assess the similarity of the modelgenerated continuation (C) compared to the true continuation (B), we performed an analysis, analogous to method introduced by Biderman et al1. This metrics collectively measure the degree of ordered token matching between the true continuation and the model's output. Our findings revealed that the scores of Qwen2-7B-Instruct and Diabetica-7B were equally poor with no significant difference (Qwen2: mean = 0.27, SD = 0.09, = 312, Diabetica: mean = 0.28, SD = 0.13, = 312; paired t-test, p=0.12, t-statistic=1.554, 95 % CI [-0.002, 0.020], = 312, mean of the differences: 0.009), , suggesting that Diabetica-7B does not exhibit benchmark memorization. 1.Biderman, S., et al. Emergent and Predictable Memorization in Large Language Models. Vol. 36 (eds. Oh, A., et al.) 28072-28090 (Curran Associates, Inc.). 43 Supplementary Note 6. Validation the effectiveness of self-distillation method. To further validate the effectiveness of our proposed self-distillation method, we conducted three additional experiments: 1. Data Length Analysis We analyzed the length of data samples before and after self-distillation. The results show that self-distilled data (mean = 598.00, SD = 177.45) is longer than the raw data (mean = 299.20, SD = 115.69). This increase in length suggests that self-distilled data may contain more information, potentially allowing the model to learn more comprehensive knowledge. 2. Comparative Quality Assessment Motivated by LLM-as-judge1, we employed GPT-4 to conduct pairwise comparisons between the original and self-distilled versions of each data sample. The prompt for comparison was designed as: Given question and two responses (A and B), please select better response. You output should be or B. Please directly output your selection. Question: {question} Response A: {A} Response B: {B}. We randomly selected 100 samples and repeated this process three times. To mitigate potential order bias, we also conducted comparisons by changing the orderings of each pair. Averaging across all experiments, self-distilled data was preferred in 65.7% of comparisons, while the original data was preferred in 34.3%. This experiment suggests significant improvement in overall data quality after self-distillation. 3. Training Dynamics Analysis We compared the evaluation loss curves during training for models using self-distilled data versus those using the original data. Models trained on self-distilled data consistently exhibited lower loss values throughout the training process, indicating superior convergence and fitting (Figure 1). This improved training dynamics can be attributed to the self-distilled data distribution being more closely aligned with the target LLM's distribution. Figure 1. Comparison of Vanilla SFT Loss and Self-Distillation SFT Loss These additional experiments provide further evidence of the efficacy of our self-distillation method, demonstrating improvements in data length, quality, and training dynamics. Here, we present an example of responses before and after self-distillation. Table 1: An example of responses before and after self-distillation. 洛瞭럱갬跋放귅둸疸쥖씠卵勉갬擥덛신끴坍達걵꿝雷쫹론俯쒽속嵐윈끰勉眄끰覓? If am diagnosed with diabetes, what steps should take to prevent complications? Response before distillation self洛瞭럱뻗뭚放귅둸疸쥖씠卵勉扶뒫부眄坍궈僚椧瞭酩뫦낹뻖뚟發冒뒩썂嵐坍봴쒽속갰攬윊쨉嵐윈끰勉眄끰覓謗뙌깬긻丹뛵眠놵옒發冒뒨롘깨봰줭 眄復쉴達먽濫眄謗넌궏坍봰끩깰꺝藤쐕랤覓뻗꼺골蔞번뱄丹뛵謗發껂럎椧놵聊봳쮠쫹聊봯진끩磅뻕聊봰낊坍봰쟕궐了뽽끰꿣 Response after distillation self嵐윈밳낹낊쫺뚟嵐윈끰勉洛瞭럱랠纜魃뻔낊쫼魃擔覓뻓啖꼺골療봱댳登籃쉰쭭꿢坍봳찉疸蔞봴묷俯쒼좄끰勉眄끰覓放房썫쪮椧瞭眄覓 榴雷좬쨆賚좬끶得껁딜燎眄嵐윈끰勉螟聊봱땶俯쒽속冒뒨勉嵐윈끰勉眄鋒 If you have been diagnosed with diabetes, your first priority is to actively manage your blood glucose levels to prevent or delay complications. This includes regular blood glucose monitoring, adhering to healthy diet, exercising properly, and taking medications as prescribed. In addition, regular eye exams, kidney function tests, and foot exams can help detect and manage possible complications early. If necessary, you may need to use some medications to improve microcirculation to further prevent complications. Remember, positive lifestyle changes and early screening for complications are key to preventing diabetes complications. 喇뻗뭚放귅둸疸쥖씠卵勉냁끴疸冒갱某쩐낻꿝雷쫹론孵俯쒼좄끰勉꽫부坍봯잶僚疸疸쀥띲疸쥔뻘록眄攬쥗쉁 1. 椧瞭酩뫦낹發冒뒩썂嵐丹뛵眠놵옒發冒뒪몵淡뢘숚淡뢙깨喇뻖띭蓼깪騰언왠쭭꺝覓갰냔辣걶뚟깮亂쩓쌖賴얼봁富得껃쯿궏房뫤뉜쯴魃了 擔覓뻗꼺골坍봲짒깨溟丹뛶뚟發冒뒩썂嵐 2. 봰줭復쉴괝길亂껀냚뻗썅골磻섣끺陋얹뤂得껃띎沔쫻얃眄富골傅넌걌冒뒨갡侮쮠걌冥陋언깼낻골得껄봄得껃쀦뚟牢봰롆發뫨봁富燎괜궐 啖뀑꿚걌發冒뒨눏擔꽨卵놴쯲發酩뫦쮢듼眄富뀔똘 3. 倣籃謗넌궏鏤鈍뻗卵놷찉發 150 갡蜂眄疸쇊셸欖쥔줝燎괝쌶謗넌궏洛纜滂썭쟞柳갰溥놷쀧몗蚌뷙솏邏뒨먼봳껃괜쨬깼榴궏洛疸쮤꽩갰 盟럯쨯燎괜궐啖뀑끯父떧줢擔亂좯썬쑽募먾뚟賴 4. 某얹깨봰줭擔꽬뛷쯶봰줭復쉴得껃븫籃謗넌궏療봯찋깨낹眄擔꽫봳뒩땶冒뒨勉끰覓纜發酩뫦쮢듼得껀윇즸嵐윈끰勉眄疸魃富뀔똘 騰먾먻졞疸 5. 뉞傅널뉠꽨냦得껃쯶復쉴뉜먻쟠謗뙋줕達먽憺뛴륀먼쯲發酩뫦쮢듼眄富뀔똘亂좯冒뒩꿚걌疸꽨걀 6. 丹뛵擔龍謗發껂럎椧놵聊봳쮠쫹옒放됃웝뻕聊봯진끩윇즸纜魃眄봰줭放憺썫진曇了뽽끰꿣嵐윈밳낹낊쫺뚟嵐윈끰勉 7. 酩뫦낹꿂궅駱뷗먽랠賴걶뚟꿂궅酩뫦낹剌洛봱盟럯쨯갰瀏쐔높냦某達먻진蚌纜낹꿂궅付얹꿘辣쉰궐꿚걌發冒뒩썂嵐 8. 깰了윉랤꼺깰꺝覓眄攬쥗쉁擔覓뻘똀冒뒫꼺골갰윇즸邏雷좯꼺疸꽫부쀧몗럳꼺갰料얹댳곅 9. 쀥갬賴뙏쒻줁倣冒뒨勉眄癩쥕랿麵봳싅깬긻洛擔됂숴낹發冒뒫싅걂擔뀓冒뒪뚟勉궜坍봰끩洛擔됀줎亂좮뿌끰왠끡먾씠卵勉賴뙏쒿쮢뺆 갰得뻗식疸疸뛳줰裸꿩끴料언뱈賚깨得꺿찌 10. 椧瞭旒뛳잹꺝覓淡뢙깨亂갢덚앁丹뛵邏꽫싉房뻗쉌旒冕賴걵뤂得꺿짙擔됃쌖賴얼劉椧瞭끡疸뀒씠卵勉酩뫦낹卵某갰命쮠꺜疸뀐윇즸 갡啖某溥껀눏滂留낥n 뛷쯶疸귇찛某쩐낻꿝雷쫸낊坍봱랠賴건럃孵俯쒾씠卵勉끩윈좄끰勉眄끰覓得껀끰欄됄꽫부眄僚魃疸뀐꺝冕疸疸뛳줰鈍떦얂 亂낻擔런걌丹뛸낻疸쀣줰왢뚟봰줭酩뫦낹房뫤뉜좄깨達坍봱뉜럃곦發 After being diagnosed with diabetes, it is vital to take comprehensive range of measures to prevent complications. The following is more comprehensive set of recommendations: 1. Actively manage blood glucose levels: Monitor your blood glucose regularly to ensure that it stays within your target range. Follow your doctor's or dietitian's instructions to adjust your diet and exercise programme, and use medication if necessary to maintain stable blood glucose level. 2. Healthy Eating: Choose foods rich in whole grains, vegetables, fruits and proteins, and limit your intake of sugar, refined carbohydrates and saturated fats. balanced diet helps control blood sugar and weight and reduces the risk of cardiovascular disease. 3. Regular Exercise: Perform at least 150 minutes of moderate-intensity aerobic exercise, such as brisk walking, swimming or cycling, each week. In addition, incorporating muscle-strengthening activities, such as weight lifting or yoga, can help improve your body's sensitivity to insulin. 4. Maintaining Healthy Weight: Maintain an ideal body weight through healthy diet and regular exercise. Obesity is one of the major risk factors for cardiovascular disease and other complications in people with diabetes. 45 5. Quit Smoking and Limit Alcohol: Avoid smoking and excessive alcohol consumption as these habits increase the risk of cardiovascular disease and are detrimental to blood glucose control. 6. Periodic Physical Examinations: Perform eye exams, kidney function tests, foot exams, and other health assessments as necessary for early detection and management of possible complications. 7. Manage stress: Learn effective stress management techniques such as meditation, yoga or deep breathing exercises to reduce psychological stress and indirectly help control blood glucose levels. 8. Take medication on time: Use glucose-lowering medication or other prescribed medication according to your doctor's advice, and do not stop or change the dosage on your own. 9. Educate yourself: Learn the basics of diabetes, including how to manage your blood sugar, recognise the symptoms of hypoglycaemia, and how to cope with unexpected situations. Attend diabetes education course or consult professional for more support and information. 10. Active Communication: Keep in close contact with your doctor and have regular follow-up appointments to discuss the effectiveness of your treatment and any adjustment needs. Participate actively in diabetes management group or community to share experiences and resources with other patients. With the above combination of measures, the onset and progression of diabetes and its complications can be effectively prevented. It is important to work closely with healthcare professionals to develop health management plan that is appropriate for your individual situation and to implement it consistently. Zheng, L., et al. Judging LLM-as-a-judge with MT-bench and Chatbot Arena. in Proceedings of the 37th International 1. Conference on Neural Information Processing Systems Article 2020 (Curran Associates Inc., New Orleans, LA, USA, 2024)."
        }
    ],
    "affiliations": [
        "Department of Endocrinology and Metabolism, Qingpu Branch of Zhongshan Hospital Affiliated to Fudan University, Shanghai, China",
        "Department of Endocrinology, Fifth Peoples Hospital of Shanghai Fudan University, Shanghai, China",
        "MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University, Shanghai, China",
        "Ministry of Education Key Laboratory of Metabolism and Molecular Medicine, Department of Endocrinology and Metabolism, Zhongshan Hospital, Fudan University, Shanghai, China"
    ]
}