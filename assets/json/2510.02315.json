{
    "paper_title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity",
    "authors": [
        "Eric Tillmann Bill",
        "Enis Simsar",
        "Thomas Hofmann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 1 3 2 0 . 0 1 5 2 : r OPTIMAL CONTROL MEETS FLOW MATCHING: PRINCIPLED ROUTE TO MULTI-SUBJECT FIDELITY Eric Tillmann Bill ETH Zurich Enis Simsar ETH Zurich Thomas Hofmann ETH Zurich"
        },
        {
            "title": "Base Models",
            "content": "Base Models + FOCUS (Ours) Figure 1: Optimal control makes flow matching models reliable on multi-subject prompts. Using FOCUS at test time or via fine-tuning yields faithful multi-subject compositions with correct attributes, minimal leakage, and no omissions, while preserving base style."
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over trained FM sampler. This yields two architecture-agnostic algorithms: (i) training-free test-time controller that perturbs the base velocity with single-pass update, and (ii) Adjoint Matching, lightweight fine-tuning rule that regresses control network to backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via flowdiffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models. Contacts: erbill@ethz.ch"
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-image (T2I) generators have made substantial progress in visual fidelity and prompt adherence, yet they remain brittle on multi-subject prompts. Typical failure modes include attribute leakage (an attribute intended for one subject propagates to others), identity entanglement (multiple subjects merged into hybrid), and subject omission (Chefer et al., 2023; Liu et al., 2022; Bar-Tal et al., 2023; Dahary et al., 2024). These limitations hinder downstream applications such as story illustration, multi-panel composition, and scientific communication, where preserving subject identity and attribute binding is essential. unifying theoretical perspective on modern T2I generators is flow matching (FM), which parameterizes generation as time-dependent flow from base distribution to the data distribution via learned vector field (Lipman et al., 2023; Liu et al., 2023; Albergo et al., 2023). This framework encompasses both rectified-flow (RF) models used in recent large-scale systems such as Stable Diffusion 3.5 (Esser et al., 2024), FLUX (Labs et al., 2025), and earlier denoising-diffusion architectures such as Stable Diffusion 1.5 (Rombach et al., 2022), Stable Diffusion XL (Podell et al., 2024), enabling statements that transfer across architectures and training choices. We leverage this common ground to analyzeand improvemulti-subject fidelity in FM models. Prior work has attempted to mitigate entanglement through test-time heuristics that reshape crossattention (Meral et al., 2024) or adjust guidance (Feng et al., 2023), including token amplification (Chefer et al., 2023), constraint-based binding (Li et al., 2023b), and structure-aware attention editing (Hertz et al., 2023; Dahary et al., 2024). While effective in specific settings, these methods are heuristic and lack unifying optimization objective, making it unclear when and why they succeed. Furthermore, most were developed for Stable Diffusion 1.x backbones, and their portability to RF and modern FM models remains limited. In this work, we show that multi-subject disentanglement can be formulated as stochastic optimal control (SOC) problem for trained FM-based samplers. Concretely, augmenting the base dynamics with small control that balances proximity to the original generator against differentiable disentanglement objective yields principled formulation and two complementary algorithms: (i) Test-time controller. lightweight single-pass controller derived from the optimality conditions of the SOC objective that steers sampling toward disentangled renderings without retraining. The formulation accepts any differentiable cost, thereby providing principled path to adapt existing heuristics to modern FM models. (ii) Fine-tuning via Adjoint Matching. stable, low-cost update rule based on Adjoint Matching (Domingo-Enrich et al., 2025) that regresses control network onto backward adjoint signal under memoryless noise schedule, directly minimizing the disentanglement objective while preserving the base models style and support. Empirically, our methods improve multi-subject fidelity across both modern FM models (Stable Diffusion 3.5, FLUX) and earlier diffusion backbones (Stable Diffusion XL). The test-time controller provides consistent gains with negligible overhead, while fine-tuning further reduces entanglement without degrading style or generalization beyond the training prompts. Building on these insights, we introduce FOCUS (Flow Optimal Control for Unentangled Subjects), which consolidates our framework into practical algorithm and achieves the strongest results in our experiments. To foster transparency and reproducibility, we release code, the curated dataset, and checkpoints of the best-performing fine-tuned models at https://github.com/ericbill21/FOCUS/."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "Flow Matching (FM) (Lipman et al., 2023; Liu et al., 2023; Albergo et al., 2023) trains timedependent vector field vθ : Rd [0, 1] Rd that transports base distribution π0 (e.g., (0, I)) to target distribution (e.g. Pdata), without simulating forward noising process during training. Given reference path = (X t)t[0,1] with 0 π0 and 1 π1, FM regresses the conditional velocity ut(X 0, 1) := dt 2 (1) so that vθ(x, t) matches its conditional expectation E[ut = x] (Lipman et al., 2023). Reference paths. standard choice is the linear (Gaussian) interpolant = βtX 0 + αtX 1, α0 = 0, β0 = 1, α1 = 1, β1 = 0, (2) where (αt, βt)t[0,1] is differentiable scheduler with αt strictly increasing, and βt strictly decreasing. The pathwise derivative is then ut(X 0, 1) = βtX 0 + αtX 1.1. widely used instance is rectified flow (RF) with αt = and βt = 1 (Liu et al., 2023). Training objective. FM is trained with the conditional flow matching loss (Lipman et al., 2023) (cid:104)(cid:13) (cid:13)vθ(X t, t) ut(X 0, 1)(cid:13) 2 (cid:13) LCFM(θ) = EtU [0,1]E (cid:105) , 0π0 1π1 which regresses the pathwise velocity toward its conditional mean at uniformly sampled times. Sampling. After training, sample X0 π0 and evolve the learned flow by solving the ODE dXt = vθ(Xt, t) dt, (3) (4) which produces path (Xt)t[0,1] whose marginals match those of the reference path (X t)t[0,1] under standard existenceuniqueness conditions; in particular X1 π1 Lipman et al. (2023). More generally, FM admits stochastic formulation (Domingo-Enrich et al., 2025) in which the drift is augmented by arbitrary schedule-dependent correction with diffusion coefficient σ(t) 0: dXt = vθ(Xt, t) + (cid:124) σ(t)2 (cid:16) 2βt (cid:16) αt αt (cid:17) βt βt (cid:123)(cid:122) :=b(Xt,t) vθ(Xt, t) αt αt Xt (cid:17) (cid:125) dt + σ(t) dBt, (5) where (Bt)t0 is standard Brownian motion in Rd. We will refer to b(Xt, t) as the (base) drift. Setting σ 0 recovers the deterministic ODE. Connection to denoising diffusion. Classical denoising diffusion models arise as special cases of FM when their discrete procedures are lifted to continuous time; refer to Appendix for details."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We formulate disentanglement as optimal control over flow-matching dynamics, derive single-pass test-time and fine-tuned controllers, and introduce probabilistic attention loss, FOCUS."
        },
        {
            "title": "3.1 STOCHASTIC OPTIMAL CONTROL",
            "content": "Our goal is to reduce multi-subject entanglement while remaining close to the base model. To this end, we introduce small control : Rd [0, 1] Rd into the drift and pose generation as quadratic, control-affine SOC problem: (cid:20)(cid:90) 1 min uU s.t. dX u(X 1 2 0 , t) + σ(t)u(X = (b(X t , t)2 2 + (X (cid:21) , t)dt + g(X 1 ) , t)) dt + σ(t)dBt, , 0 π0, (6) (7) where is the latent state, is the base FM drift, σ(t) 0 is scalar diffusion schedule, and (Bt)t[0,1] is Brownian motion. The running cost : Rd [0, 1] will measure subject entanglement (e.g. FOCUS), and we set the terminal cost 0 in all derivations and experiments. For control-affine dynamics with ℓ(x, u, t) = 1 2 u2 2 + (x, t), the Hamiltonian of the SOC is H(x, u, a, t) = 1 2 u2 2 + (x, t) + (b(x, t) + σ(t)u) , (8) 1Over-dot denotes the time derivative, i.e., xt = dt xt. 3 where a(t) Rd is the co-state (adjoint). Since is strictly convex in u, the first-order optimality condition yields = σ(t)a(t), (9) with adjoint dynamics dt a(t) = (cid:2)X b(X , t)a(t) + (X , t)(cid:3) , a(1) = g(X 1 ). (10)"
        },
        {
            "title": "3.2 ON-THE-FLY DISENTANGLEMENT (TEST-TIME CONTROL)",
            "content": "t on-the-fly and steer the sampling process at each timestep t. Directly computing At inference, we solve Equation (6) per trajectory with frozen model parameters. The idea is to compute requires the adjoint a(t) in Equation (9), which is defined along the controlled path via Equation (10). This is impractical because a(t) depends on the terminal condition a(1) = g(X 1 ), which depends on the endpoint τ )τ [t,1] ; coupling backward solve to the forward pass at every step. To obtain single-pass controller, we approximate a(t) locally at the current state. Concretely, we linearize Equation (10) around , freeze 0, and treat the future state as locally constant: 1 , which in turn depends on the future segment (X a(t) (cid:90) 1 X (X , τ )dτ (1 t)X (X , t), (11) where the last step uses left-Riemann approximation. Substituting into Equation (9) yields the instantaneous control σ(t)(1 t)X (X t , t). (12) The approximation 0 is common in online control settings (Havens et al., 2025). Velocity reparameterization (SDE). Let vbase denote the base FM velocity. We adopt the memoryless diffusion schedule, which makes the stochastic interpolant endpoints independent (X0 X1) and yields simple driftvelocity identity: (cid:115) σmem(t) = 2 βt βt βt (cid:19) . (cid:18) αt αt (13) Under this choice, the drift-velocity relation from Equation (5) simplifies to b(Xt, t) = 2vθ(Xt, t) αt 2 σmem(t)ut. Therefore the conαt trolled velocity is Xt. Adding +σmem(t)ut to the drift shifts the velocity by + 1 = vbase(Xt, t) + σmem(t) 2 vbase(Xt, t) σ2 mem(t) (1 t)X (Xt, t) , (14) which can be passed to any SDE solver without modifying the integrator.2 Deterministic alternative (ODE). Many off-the-shelf T2I models are optimized for ODE sampling (σ 0). Decoupling σ from the control gives min (cid:20)(cid:90) 1 0 1 2 u(Xt, t)2 (cid:21) 2 + (Xt, t)dt s.t. dXt = (vbase(Xt, t) + u(Xt, t)) dt, X0 π0. (15) (16) The Hamiltonian = 1 approximation: 2 u2 + + a(vbase + u) yields = a(t), and with the same local = vbase(Xt, t) a(t) vbase(Xt, t) (1 t)X (Xt, t), (17) 2If desired, the factor 1 2 σmem(t)2 can be absorbed into the weight of , yielding schedule-invariant update."
        },
        {
            "title": "3.3 FINE-TUNING FOR DISENTANGLEMENT",
            "content": "Our goal is to learn control network uθ that remains close to the base dynamics and generalizes beyond the specific trajectories used during training. Adjoint Matching (AM). Directly solving Equation (10) during training is prohibitive because the adjoint a(t) depends on the controlled path . Instead, we use Adjoint Matching (Domingo-Enrich et al., 2025), regressing uθ to cheaper lean adjoint computed along frozen forward trajectories (Xt)t[0,1] while dropping u-dependent Jacobian terms: dt a(t) = (cid:2)X b(Xt, t)a(t) + (Xt, t)(cid:3) , a(1) = g(X1). (18) Memoryless training. To ensure that the learned control generalizes beyond the specific trajectories used in training, we follow Domingo-Enrich et al. (2025) and train under memoryless generative process where X0 X1, i.e., p(X0, X1) = p(X0)p(X1). For linear (Gaussian) FM paths with scheduler (αt, βt)t[0,1], the diffusion coefficient σmem from Equation (13) achieves this independence and makes the regression target trajectory-stationary. Training objective. Each iteration proceeds as follows: (i) sample forward trajectories (Xt)t[0,1] under σmem with the current model frozen via Equation (5); (ii) integrate the lean adjoint (a(t))t[0,1] backward with Equation (18); (iii) regress the control toward the stationary target σmem(t)a(t) by minimizing LAM(θ) := 1 2 (cid:90) 1 uθ(Xt, t) + σmema(t)2dt. (19) The memoryless schedule is only required during fine-tuning. At inference σ(t) can be set to zero, allowing to use faster off-the-shelf ODE samplers. 3.4 MEASURING MULTI-SUBJECT ENTANGLEMENT At each sampling step, T2I backbones compute crossattention from image-space queries to text tokens. Empirically, these token-wise cross-attention maps correlate with the eventual spatial placement of the corresponding entities (Hertz et al., 2023; Chefer et al., 2023). This enables us to diagnose and mitigate subject entanglement during generation by measuring spatial interactions among subject-specific attention maps, rather than relying solely on post-hoc image encoders (see Figure 2). Dachshund Corgi Image dachshund and corgi sitting together on cozy rug Figure 2: Extracted cross-attention maps for both subjects in FLUX.1 [dev]. Most prior work that optimizes multi-subject behavior via cross-attention treats these maps as generic similarity scores (e.g., maximizing cosine similarity (Meral et al., 2024) or activation differences (Chefer et al., 2023)). However, cross-attention arises from softmax: each map is probability distribution over spatial locations. Ignoring this structure discards principled probabilistic footing and can induce artifacts such as over-concentration. We instead treat attention maps as distributions and optimize them accordingly. FOCUS. Let denote the number of spatial locations and let d1 be the probability simplex. For finite set = { vp1, . . . , pn} d1 of distributions, define the JensenShannon divergence DJS(P ) = 1 n (cid:88) i=1 DKL (pim) , = 1 (cid:88) j=1 pj, with DKL(pq) = (cid:80)d being the Kullback-Leibler divergence. Since DJS(P ) [0, log n], we normalize by dividing with log to obtain (cid:98)DJS(P ) [0, 1], which makes scores comparable across different set sizes; see Theorem B.1 for proof of this upper bound. i=1 pi log pi qi We introduce FOCUS (Flow Optimal Control for Unentangled Subjects) to encourage, for each subject, unimodal, spatially localized, and nonoverlapping attention. Let be the set of subjects in Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) An astronaut, violin, and sunflower floating inside space station 5 . 3 1 . F swan, goose, and duck drifting past lily pads Figure 3: Qualitative results with test-time control on Stable Diffusion 3.5 and FLUX.1. Each heuristic is shown at its optimal λ. Additional examples appear in Figures 9 and 10 of the Appendix. the prompt. For each subject S, collect its attention maps at the current step into Ps d1 (e.g., across layers or heads), and define the subject mean ms = 1 p. Let = {ms Ps S} be the set of subject means. Our FOCUS loss combines within-subject agreement and between-subject separation: pPs (cid:80) FOCUS(S) = (cid:32) 1 2 1 (cid:88) sS (cid:33) (cid:98)DJS(Ps) + 1 (cid:98)DJS(M ) (cid:17) (cid:16) 1 2 (20) The first term penalizes dispersion within each subjects maps (encouraging consistent binding, and for multi-encoder models such as SD 3.5, agreement across encoders). The second term rewards separation among subjects by maximizing divergence between their mean attention distributions. By construction, focus [0, 1]: 0 indicates perfect disentanglement (low intra-subject dispersion and maximal inter-subject separation), while larger values indicate greater entanglement."
        },
        {
            "title": "4 RELATED WORK",
            "content": "We review approaches to multi-subject T2I generation. We first cover training-free attention-space interventions that operate at inference time. We then discuss methods that enforce regional/layout constraints or combine multiple diffusion paths. Finally, we survey training-time objectives that strengthen subjectattribute binding. Attention-space interventions (training-free). large body of work steers pre-trained generators at inference by manipulating cross-attention. At each sampling step, the model produces attention weights from spatial queries to text tokens; selecting the column for token and normalizing over space yields token-conditioned spatial map. Methods then assess entanglement (e.g., by measuring overlap across subjects) and modify attention or latents to promote coverage and separation. Attend&Excite amplifies token activations to enforce entity coverage and reduce neglect or leakage (Chefer et al., 2023). Divide&Bind adds an inference-time objective that separately enforces subject coverage and attribute binding, optimizing latents during sampling (Li et al., 2023b). Structured Diffusion Guidance injects linguistic structure (e.g., dependency trees) to guide attention manipulation for multi-object composition (Feng et al., 2023). Prompt-to-Prompt locks cross-attention correspondences to preserve wordsubject alignments across edits, often used to maintain multisubject layouts (Hertz et al., 2023). CONFORM formulates contrastive, InfoNCE-style objective that separate different subjects while pulling subjectattribute pairs together (Meral et al., 2024). While effective in specific setups, these methods are heuristic and lack unifying optimization principle; moreover, many were developed for Stable Diffusion 1.x backbones, limiting portability to modern flow-matching models. In contrast, our method derives controller from single SOC 6 Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) flamingo, yoga mat, and gramophone on rooftop at sunset 5 . 3 1 . F macaw, cockatoo, and an Amazon parrot perched on jungle vine Figure 4: Qualitative results after fine-tuning Stable Diffusion 3.5 and FLUX.1. Each heuristic uses its optimal hyperparameters. Additional examples appear in Figures 11 and 12 of the Appendix. objective at the FM level, yielding architecture-agnostic updates. We also instantiate our controller with costs derived from several of the above heuristics to demonstrate principled portability. Regional/layout composition and multi-path fusion. complementary direction constrains where subjects appear. MultiDiffusion fuses multiple diffusion trajectories under shared spatial constraints (e.g., boxes or masks), enabling faithful multi-subject placement without retraining (Bar-Tal et al., 2023). Related systems extend this idea to interactive, region-based workflows. GLIGEN augments frozen backbone with grounding layers and conditions on bounding boxes or phrases to place multiple objects precisely (Li et al., 2023a). More recently, Be Decisive leverages the layout implicitly encoded in the initial noise and refines it during denoising, avoiding conflicts with externally imposed layouts and improving prompt alignment while preserving model priors (Dahary et al., 2025). These approaches disentangle primarily via spatial decoupling but often require user-specified or learned layouts, which increases user effort and restricts spontaneous subject interaction. Our method reduces entanglement without explicit spatial annotations, requiring only the text prompt (and its subjects). Training-time objectives for multi-subject fidelity. Some works alter training signals to strengthen subjectattribute binding. TokenCompose introduces token-level supervision to improve consistency for prompts with multiple categories and attributes (Wang et al., 2024b). Region-aware objectives decompose complex prompts into per-region descriptions and enforce alignment, reducing crossentity leakage. Such methods typically assume curated supervision and substantial retraining. In contrast, our fine-tuning objective is lightweight: it adapts pre-trained models via Adjoint Matching and requires only text prompts, while our test-time controller operates with zero parameter updates."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate our approach in three stages. We first describe datasets, metrics, models, and baselines. We then present test-time (on-the-fly) results, followed by fine-tuning results. All experiments ran on NVIDIA A100/H100 GPUs. While the test-time controller runs on commodity GPUs with as little as 12 GB VRAM, fine-tuning experiments fit within the VRAM of H100 GPUs. Base Models. We report results on two open-source flow-matching models: Stable Diffusion 3.5 (SD 3.5) (Esser et al., 2024) and FLUX.1 [dev] (FLUX.1) (Labs et al., 2025). Dataset. We create 150-prompt corpus with 24 subjects per prompt using GPT-5. Half the prompts contain similar subjects (e.g., black bear and brown bear[...]); the rest contain dissimilar subjects (e.g., snowboard, telescope, and husky[...]). For each prompt, we annotate subject token indices for both CLIP and T5 encoders to extract cross-attention maps for the heuristics. Such per-subject annotations are typically absent from existing corpora. 7 Table 1: Test-time control results at the optimal λ for each heuristic. We report mean std over all prompts and seeds; the top three per metric are highlighted (gold/silver/bronze). Heuristic CLIP I-T SigLIP-2 I-T BLIP T-T Qwen2 T-T PickScore I-T ImgRew I-T Composite 5 . 3 1 . F 0.34740.03 Base 0.34840.03 Attend&Excite 0.34810.03 CONFORM 0.34890.03 Divide&Bind FOCUS (Ours) 0.34830.03 0.34490.03 Base 0.34300.03 Attend&Excite 0.34360.03 CONFORM 0.34530.03 Divide&Bind FOCUS (Ours) 0.34460.03 0.23090.05 0.23260.05 0.23230.05 0.23160.05 0.23440. 0.22710.05 0.22420.05 0.22520.05 0.22720.05 0.22680.05 0.57310.15 0.57520.15 0.57730.15 0.57420.14 0.57510.15 0.57390.15 0.57160.14 0.57260.15 0.57220.15 0.57410.14 0.64020.08 0.64040.08 0.64210.08 0.63990.08 0.63850.08 0.63000.09 0.63040.09 0.63210.09 0.63300.08 0.63260.08 22.69400.99 22.69501.01 22.71880.99 22.67791.03 22.74991. 23.42341.03 23.25491.11 23.35741.03 23.43951.02 23.42741.02 1.31750.68 1.35450.66 1.36840.64 1.34930.67 1.40030.62 1.29700.66 1.24940.70 1.24610.70 1.29390.67 1.29130.67 0.00000.00 3.17140.53 3.43360.45 3.93730.78 4.28650.86 0.00000.00 1.75950.67 1.51140.26 1.63520.44 1.97120.31 Metrics. To quantify multi-subject fidelity, we follow Yu & Chien (2025) and report two alignment groups. For imagetext (IT) alignment we compute CLIP (Radford et al., 2018) and SigLIP2 (Tschannen et al., 2025) cosine similarities between image and prompt embeddings. For captionbased texttext (TT) faithfulness, we caption each image with BLIP (Li et al., 2022) and Qwen2VL (Wang et al., 2024a) and measure semantic similarity to the prompt. We additionally report preference-trained scores, PickScore (Kirstain et al., 2023) and ImageReward (Xu et al., 2023), as proxies for human preference. For model selection, we compute composite score per hyperparameter combination by macroaveraging baseline-relative gains across metrics; see Appendix D.2 for the formula. Because we aim to preserve base style and subject depiction, global alignment scores may shift modestly even when multi-subject fidelity improves. Unless noted otherwise, we generate five images per prompt (distinct seeds) per hyperparameter setting, fixing sampler, steps, guidance, and resolution allowing to make direct comparisons between test-time control and fine-tuning. Full details are in Appendix D. Baselines and heuristics. To demonstrate portability across FM models and legacy U-Net heuristics, we evaluate Attend&Excite (Chefer et al., 2023), CONFORM (Meral et al., 2024), Divide&Bind (Li et al., 2023b), and our heuristic FOCUS. Because cost magnitudes differ, we optimize scaled running cost λ (Xt, t) with λ > 0. The effect of λ at test time is shown in Figure 7. Human study. Automated metrics struggle to detect attribute leakage reliably (Dahary et al., 2025), so we conducted prompt-conditioned, pairwise preference study with 50 participants. In each trial, participants viewed two images from our evaluation suite alongside the prompt and selected the image that better matched the prompt, yielding 2,000 pairwise judgments. From these outcomes we computed Elo ratings (across-method comparability) and win rates (fraction of pairwise wins). 5.1 ON-THE-FLY DISENTANGLEMENT (TEST-TIME CONTROL) We sweep ten λ values per heuristic and select the best via the composite score defined above. Table 1 report per-heuristic, per-model results at the optimal λ, qualitative examples are shown in Figures 9 and 10, and Human Study results are summarized in Table 2. Table 2: Human preference study for testtime control. Report pairwise win rate and Elo rating; see Appendix for details. Heuristic SD3.5 FLUX.1 Win[%] Elo Win[%] Elo indicating that All heuristics outperform the base sampler on SD 3.5 and FLUX.1, the SOC formulation yields principled route to port legacy heuristics to modern FM models. Qualitatively, outputs show higher multi-subject fidelity: subjects are more often present and better separated than in the base model. Our human study shows similar trends, with higher win-rates and Elo ratings. While FOCUS is not best on every metric, it attains the highest composite score across all heuristics and achieves almost all best scores in our human study. Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) 1517 45% 1500 53% 1373 42% 50% 1562 58% 1548 1464 46% 1526 49% 1498 50% 50% 1450 54% 1562 5.2 FINE-TUNING FOR DISENTANGLEMENT We insert LoRA layers (Hu et al., 2022) into self-attention blocks and freeze all base parameters. We use rank r=4 (training < 0.1% of parameters). We sweep λ and other hyperparameters, including 8 Table 4: Fine-tuning results at the set of hyperparameters for each heuristic. We report mean std over all prompts and seeds; the top three per metric are highlighted (gold/silver/bronze). Heuristic CLIP I-T SigLIP-2 I-T BLIP T-T Qwen2 T-T PickScore I-T ImgRew I-T Composite 5 . 3 1 . F 0.34740.03 Base 0.34690.03 Attend&Excite CONFORM 0.34780.03 Divide&Bind 0.34860.03 FOCUS (Ours) 0.34950.03 0.34490.03 Base 0.34680.03 Attend&Excite 0.34580.03 CONFORM Divide&Bind 0.34450.03 FOCUS (Ours) 0.34680.03 0.23090.05 0.22810.04 0.22940.05 0.22660.05 0.23310.04 0.22710.05 0.23200.05 0.23050.04 0.22960.05 0.23280.05 0.57310.15 0.57470.15 0.56460.15 0.58700.14 0.57440. 0.57390.15 0.58760.15 0.58000.15 0.57050.15 0.57800.15 0.64020.08 0.64250.08 0.63930.09 0.63580.08 0.63830.08 0.63000.09 0.63820.08 0.63690.08 0.62460.09 0.63860.08 22.69400.99 22.84291.01 22.59620.99 22.34010.99 22.64450.97 23.42341.03 23.33331.01 23.37241.00 23.19091.06 23.32781.01 1.31750.68 1.44600.60 1.37820.63 1.35240.68 1.44950. 1.29700.66 1.38060.62 1.36310.63 1.22690.70 1.38990.61 0.00000.00 5.71811.21 3.45831.05 0.80060.69 5.91741.19 0.00000.00 2.34770.79 1.95910.83 0.20020.47 2.58810.79 dataset choice; see Appendix D. With the best settings, fine-tuning takes 17 min on SD 3.5 and 79 min on FLUX.1. Table 4 reports metrics across all heuristics and models; qualitative results appear in Figures 4, 11 and 12; human preferences are summarized in Table 3. Training data comprise two small subsets of our is single prompt, The first evaluation dataset. horse and bear in forest, where SD 3.5 fails reliably (HORSE&BEAR). The second contains 15 prompts with two semantically similar subjects (TWOOBJECTS). Despite their size, both subsets yield gains on diverse unseen prompts, including different subject categories, prompts with more than two subjects, and different subject token positions, suggesting that our method targets core failure mode in multisubject composition. Table 3: Human preference study for finetuned models. Report pairwise win rate and Elo rating; see Appendix for details. Heuristic SD3.5 FLUX.1 Win[%] Elo Win[%] Elo Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) 1355 39% 1584 56% 1520 49% 48% 1436 57% 1605 1462 51% 1476 50% 1620 50% 43% 1442 54% 1500 Across the board, the fine-tuned models outperform their test-time controlled counterparts. This matches our theory, since during fine-tuning the adjoint signal is computed explicitly over the full trajectory, whereas test-time control relies on single-pass approximation. Between the two training sets, HORSE&BEAR yields the strongest gains with an 85% relative improvement in the composite scores in contrast to TWOOBJECTS for SD 3.5 and about 5% for FLUX.1. Across heuristics, FOCUS attains the highest composite score, indicating the largest average improvement across metrics. Consistently, in the human study FOCUS achieves the highest win rates against competing heuristics and among the highest Elo ratings for both models. 5.3 CLASSICAL DENOISING DIFFUSION Although our algorithms are derived for flow matching, Appendix establishes correspondence denoising diffusion. To test the theory, we apply the test-time controller to Stable Diffusion XL, U-Net based denoising diffusion model. As shown in Figure 5, FOCUS improves on the prompt lion and tiger resting side by side[...] by reducing attribute leakage. SDXL SDXL + FOCUS"
        },
        {
            "title": "6 DISCUSSION AND FUTURE WORK",
            "content": "Figure 5: Transfer to SDXL. We propose control-theoretic framework for multi-subject fidelity, instantiated either as singlepass test-time controller or as lightweight fine-tuned controller. The formulation accommodates existing attention-based heuristics, and our FOCUS yields the most consistent gains across settings. The two realizations offer complementary trade-offs: test-time control applies directly to frozen model given subject tokens, at the cost of roughly 2 longer inference, whereas fine-tuning requires subject tokens only during training and matches the base models inference speed during inference. Finally, the strong generalization of fine-tuningeven from single promptsuggests an underlying attention-level failure mode in current T2I models; future work should probe this mechanism and develop annotation-free proxies and automated subject tokenization."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Brian D.O. Anderson. Reverse-time diffusion equation models. ISSN 0304-4149. Stochastic Processes and their Applications, 12(3):313326, 1982. https://doi.org/10. 1016/0304-4149(82)90051-5. URL https://www.sciencedirect.com/science/ article/pii/0304414982900515. doi: Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. MultiDiffusion: Fusing diffusion paths for controlled image generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 17371752. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/ bar-tal23a.html. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention In European Conference on Computer Vision, pp. for multi-subject text-to-image generation. 432448. Springer, 2024. Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be decisive: Noise-induced layouts for multi-subject generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 112, 2025. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky T. Q. Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=xQBRrtQM8u. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1260612633. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/esser24a.html. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion In The Eleventh International Conferguidance for compositional text-to-image synthesis. ence on Learning Representations, 2023. URL https://openreview.net/forum?id= PUIqjT4rzq7. Aaron Havens, Benjamin Kurt Miller, Bing Yan, Carles Domingo-Enrich, Anuroop Sriram, Daniel S. Levine, Brandon Wood, Bin Hu, Brandon Amos, Brian Karrer, Xiang Fu, GuanHorng Liu, and Ricky T. Q. Chen. Adjoint sampling: Highly scalable diffusion samplers via adjoint matching. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=6Eg1OrHmg2. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=_CDixzkzeyb. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. URL https://proceedings.neurips.cc/paper_files/paper/2020/ Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Tero Karras, Miika Aittala, Timo Aila, Elucidating the deIn S. Koyejo, S. Mohamed, InforInc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ sign space of diffusion-based generative models. and A. Oh (eds.), Advances in Neural A. Agarwal, D. Belgrave, K. Cho, mation Processing Systems, volume 35, pp. 2656526577. Curran Associates, 2022. file/a98846e9d9cc01cfb87eb694d946ce6b-Paper-Conference.pdf. and Samuli Laine. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2417424184, June 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3665236663. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/73aacd8b3b05b4b503d58310b523553c-Paper-Conference.pdf. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 1288812900. PMLR, 1723 Jul 2022. URL https: //proceedings.mlr.press/v162/li22n.html. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2251122521, June 2023a. Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. Divide & bind your attention for improved generative semantic nursing. In BMVC, pp. 366, 2023b. URL http://proceedings. bmvc2023.org/366/. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models. In Shai Avidan, Gabriel Brostow, Moustapha Cisse, Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision ECCV 2022, pp. 423439, Cham, 2022. Springer Nature Switzerland. ISBN 978-3-031-19790-1. Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=XVjTT1nw5z. 11 Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 90059014, June 2024. Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7298373007. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/e6d58fc68c0f3c36ae6e0e64478a69c0-Paper-Conference.pdf. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic modIn Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conferels. ence on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 81628171. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/ nichol21a.html. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41954205, October 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, 2024. URL synthesis. https://openreview.net/forum?id=di52zR8xgf. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. understanding by generative pre-training. https://cdn.openai.com/research-covers/language-unsupervised/ language_understanding_paper.pdf. Improving language arXiv preprint arXiv:1801.06146, 2018. URL Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised In Francis Bach and David Blei (eds.), Prolearning using nonequilibrium thermodynamics. ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 22562265, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/sohl-dickstein15.html. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben In InternaPoole. Score-based generative modeling through stochastic differential equations. tional Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=PxTIG12RRHS. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. URL https://arxiv.org/abs/2502.14786. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024a. URL https://doi. org/10.48550/arXiv.2409.12191. Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-toimage diffusion with token-level supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 85538564, June 2024b. 12 Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1590315935. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/33646ef0ed554145eab65f6250fab0c9-Paper-Conference.pdf. Hsiang-Chun Yu and Jen-Tsung Chien. Attention disentanglement for semantic diffusion modeling in text-to-image generation. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15, 2025. doi: 10.1109/ICASSP49660.2025. 10888688."
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Use of Large Language Models FOCUS Denoising Diffusion as Flow Matching C.1 VP chain to SDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Reverse-time dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Time change to FM . C.4 Score relations . . . . . Hyperparameters D.1 Sampling Parameters . D.2 Metrics . . . . . . D.3 Test-Time Control . D.4 Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Additional Metric: Open-Vocabulary Detection . . . . . . . . . . . . . . . . . . . Human Study E.1 Elo Rating Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Extra Samples F.1 Test-Time Control: Stable Diffusion 3.5 . . . . . . . . . . . . . . . . . . . . . . . F.2 Test-Time Control: FLUX.1 [dev] . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Fine-tuned: Stable Diffusion 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Fine-tuned: FLUX.1 [dev] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 16 17 17 18 18 18 18 18 20 20 21 21 22"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "We used large language model (OpenAI GPT-5 via ChatGPT) for two purposes: (i) expanding small, human-written set of text prompts to create additional prompts for our synthetic dataset, and (ii) polishing writing (grammar, clarity, and tone). For dataset construction, the model generated semantically similar prompt variants; all outputs were screened and curated by the authors. For writing, the authors drafted all sections and used the model only for copy-editing, not for introducing technical content."
        },
        {
            "title": "B FOCUS",
            "content": "This appendix details FOCUS, our probabilistic attention heuristic used as running cost for disentanglement. We emphasize three practical design choices: (i) encoding spatial proximity before measuring divergence, (ii) aggregating attention maps prior to scoring, and (iii) omitting an explicit collapse regularizer. Spatially aware divergence. We promote separation of subjects by maximizing Jensen Shannon divergence (JSD) defined over attention distributions. naıve computation on flattened maps discards locality, allowing distant activations to interact as if adjacent. To preserve spatial structure, we (i) reshape token-embedding maps to the target aspect ratio, (ii) apply light 2D Gaussian smoothing, and only then (iii) flatten for scoring. This encodes proximity and mitigates grid-like artifacts during optimization. Block selection and aggregation. Modern T2I backbones follow Diffusion Transformer designs (Peebles & Xie, 2023). Rather than computing scores per block and averaging their scores which can result in conflicting update directions, we first aggregate attention and then score. Concretely, we average cross-attention maps over all blocks that process text and image tokens separately, producing single map per token and consistent optimization direction. Blocks that jointly process text and image tokens are excluded from this aggregation for compatibility. No explicit collapse regularizer. We experimented with an entropy-based regularizer aimed at discouraging overly concentrated (collapsed) attention. Let H(p) = (cid:80) pi log pi denote the Shannon Entropy and (cid:98)H(p) = H(p)/ log [0, 1] its normalized version, where is the number of spatial locations. For each subject we form its mixture distribution ms and added γreg 1 (cid:88) (cid:16) sinS 1 (cid:98)H(m) (cid:17) , (21) scaling by γreg > 0 to control its effect. In our experiments, small γreg made the term largely inactive, while larger γreg pushed mass away from the subject rather than stabilizing it, see Figure 6 for an example. We therefore omit this term in FOCUS and rely on the probabilistic objective described above. Base γreg = 0 γreg = 0.01 γreg = γreg = 10 Figure 6: Ablation of regularizer strength γreg for the test-time controller on Stable Diffusion 3.5. Lemma B.1 (Upper Bound of JensenShannon Divergence). Let = {p(1), . . . , p(n)} d1 be set of probability distributions. Then, DJS(P ) is upper bounded by log n. 15 Proof. Define as in Theorem B.1, then the JSD is defined as follows: DJS(P ) = 1 (cid:88) k=1 (cid:16)"
        },
        {
            "title": "DKL",
            "content": "p(k) (cid:17) , = 1 (cid:88) k= p(k). We can upper bound each DKL-term as follows: DKL(p(k) m) = = = (cid:88) i=1 (cid:88) i=1 (cid:88) i= (cid:88) p(k) log p(k) mi p(k) log p(k) log 1 (cid:32) p(k) (cid:80)n ℓ=1 p(ℓ) p(k) ℓ=1 p(ℓ) (cid:80)n n (cid:33) p(k) log Plugging this bound back into the definition of the JSD, yields the desired results: i=1 = log n. 1 (cid:88) k=1 DKL (cid:16) p(k) (cid:17) 1 (cid:88) k=1 log = log Normalization. Because DJS(P ) [0, log n], we use the normalized score (cid:98)DJS(P ) = DJS(P )/ log [0, 1], which makes values comparable across different set sizes."
        },
        {
            "title": "C DENOISING DIFFUSION AS FLOW MATCHING",
            "content": "This section makes precise how classical denoising diffusion (score-based) models arise as special case of the flow-matching (FM) framework. We first derive the continuous-time SDE limit of the variance-preserving (VP) family (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021); after we express reverse-time generation; and finally show an explicit parameterization that uses diffusion model as an FM velocity field. Analogous statements hold for VE and EDM variants (Nichol & Dhariwal, 2021; Karras et al., 2022; 2024). C.1 VP CHAIN TO SDE Let X0 pdata. The standard K-step VP forward noising chain is Xk = αkXk1 + 1 αkϵk, ϵk (0, I), = 1, . . . , K, (22) where αk := 1 βk (0, 1) with βk (0, 1) typically increasing over (Ho et al., 2020). This yields the closed-form marginal Xk X0 (cid:0) αk X0, (1 αk) I(cid:1), αk := (cid:89) i=1 αi. (23) For sufficiently large K, XK is approximately standard normal (Ho et al., 2020). We lift this formulation to continuous time by defining uniform grid τk := k/K on [0, 1], so every increment is τ = 1/K. Define piecewise-constant rate β(τ ) via β(τ ) := βk/τ for τ [τk1, τk). Then by using the first-order Taylor approximation of 1 + x, we can rewrite 16 αk = 1 βk 1 1 2 βk + O(β2 k), and obtain Xk := Xk Xk1 1 = βkXk1 + (cid:112)βkϵk + O(β2 k) 1 2 β(τk1)Xk1 (cid:19) τ + (cid:112)β(τk1) (cid:18) = τ ϵk + (cid:16) (τ ) (cid:17) 3 2 . This is the EulerMaruyama discretizations of the forward/diffusion VP-SDE: dXτ = 1 β(τ )Xτ dτ + (cid:112)β(τ )dBτ , τ [0, 1], (24) (25) (26) (27) and the discrete chain converges to this SDE as . Moreover, the SDE has Gaussian marginals Xτ X0 (cid:16) α(τ )X0, (1 α(τ )) (cid:17) , with α(τ ) := exp (cid:18) (cid:19) β(u)du , (28) (cid:90) τ 0 which matches Equation (23) at the grid points if we choose α(τk) = αk (Song et al., 2021). C.2 REVERSE-TIME DYNAMICS We now reverse time to generate from noise to data. Let τ = 1 τ denote the generative time. By classical time reversal diffusion (Anderson, 1982) the reverse-time process satisfies (cid:18) dXτ = 1 2 βτ Xτ β(τ )X log pτ (Xτ ) (cid:19) dτ + (cid:112)β(τ )d Bτ , with dτ = dτ, (29) where pτ are the forward-time marginals and log pτ is the score (Song et al., 2021). In practice, most diffusion architectures parameterize the model via noise prediction ϵθ (Ho et al., 2020; Karras et al., 2022; 2024), which is related to the score by: log pτ (x) = ϵθ(x, τ ) (cid:112)1 α(τ ) . (30) C.3 TIME CHANGE TO FM To embed VP diffusion into FM, we reparameterize time so that FM runs from noise to data, setting := 1 τ , which yields the following FM schedules: αFM := (cid:112)α(1 t), and βFM := (cid:112)1 α(1 t). (31) C.4 SCORE RELATIONS For linear Gaussian reference paths, the score s(x, t) := log pt(x) and the FM vector field vθ(x, t) are linked by schedule-dependent affine map (Lipman et al., 2023; Albergo et al., 2023; Liu et al., 2023): s(x, t) = (cid:16) vθ(x, t) κt (cid:17) , 1 ηt κt := αFM αFM , ηt := βFM (cid:18) αFM αFM t βFM βFM (cid:19) . Combining the noisescore relation with the time change τ = 1 gives: s(x, t) = log pt(x) = ϵθ(x, 1 t) βFM , (32) (33) = (cid:112)1 α(1 t). Substituting this into the scorevelocity map yields the corresponding since βFM FM velocity prediction induced by an ϵ-parameterized diffusion model: vθ(x, t) = κtx ηt ϵθ(x, 1 t) βFM . (34) This identity lets an ϵ-trained diffusion model be used directly as an FM velocity field for the VPinduced schedules above; plugging vθ into the FM SDE recovers the reverse-time VP sampler (and setting σ 0 recovers the probability-flow/DDIM ODE) under the change of variables = 1 τ ."
        },
        {
            "title": "D HYPERPARAMETERS",
            "content": "D.1 SAMPLING PARAMETERS For Stable Diffusion 3.53 and FLUX.1 [dev]4, we follow the official sampling recommendations. Unless stated otherwise, we use the deterministic Euler scheduler with 28 inference steps for both models and generate images at 512 512 resolution. The classifier-free guidance scale is set to 4.5 for SD3.5 and 3.5 for FLUX. To ensure consistent extraction of cross-attention maps, we cap the maximum tokenized sequence length at 77 for SD3.5 and 256 for FLUX, and we verify that all prompts in our dataset fall within these limits. Models are loaded and all computations are performed in bfloat16 to reduce memory usage. D.2 METRICS To summarize each hyperparameter setting with single scalar, we macro-average the relative improvement over the base model across prompts, seeds, and metrics. Let Xp,s be the image produced by the current setting for prompt and seed S, and let ˆXp,s be the corresponding image from the base model. Let denote the set of evaluation metrics. Since in our settings all metrics are increasing, the composite score for hyperparameter setting is the macro-average 1 (cid:88) sS 1 (cid:88) pP 1 (cid:88) mM m(Xp,s) m( ˆXp,s) m( ˆXp,s) , (35) such that value larger than 0 indicates an average improvement over the base model, while values smaller than 0 indicate degradation. D.3 TEST-TIME CONTROL In the deterministic (ODE) variant, the single-pass update does not inherit the timeweighting 1 2 σ2 mem(t) that appears in the SDE case. Since σmem(t) is large at early times and decays rapidly as 1, we reintroduce this desirable earlystrong / lateweak behavior in the ODE setting by reweighting the running cost: (Xt, t) = λ σ2 mem(t) Heuristic(Xt), (36) where λ > 0 is the earlier introduced hyperparameter to account for different heuristic magnitudes. Throughout our test-time control experiments, we use this time-weighted running cost variant and sweep over λ {0.1, 0.5, 1, 2, 3, 4, 8, 12, 16, 32}. Values below 0.1 have negligible effect across heuristics, while values above 32 tend to produce artifacts (over-sharpening, texture noise) or occasional numerical instabilities (NaNs). See Figure 7 for qualitative trends. Base λ = 0.1 λ = 0.5 λ = 1 λ = 2 λ = 3 λ = λ = 8 λ = 16 λ = 32 Figure 7: Effect of the control parameter λ on test-time control with Stable Diffusion 3.5. D.4 FINE-TUNING We initialize the memoryless schedule from each models ODE 28-step inference schedule (same time steps), do not use classifier-free guidance, and for FLUX.1 apply its native guidance scale (not CFG). Following Appendix D.1, we cap tokenized sequence length for cross-attention extraction to 77 (SD 3.5) and 256 (FLUX.1). Models are loaded in bfloat16; forward/backward passes run in 3https://huggingface.co/stabilityai/stable-diffusion-3.5-medium 4https://huggingface.co/black-forest-labs/FLUX.1-dev 18 Figure 8: User interface for the prompt-conditioned, pairwise preference study. BF16 and the final loss reduction is computed in FP32 to avoid numerical issues. To reduce memory, at each iteration we subsample 16 of the 28 steps to be used in our loss calculation. We further use batch sizes of 5 trajectories for SD 3.5 and 2 trajectories for FLUX.1. We use two small prompt sets: HORSE&BEAR (single prompt: horse and bear) and TWOOBJECTS (15 prompts, each with two semantically similar subjects). Optimization uses AdamW with weight decay of 0.01 and β0 = 0.95, β1 = 0.999. In addition, we also employ Accelerate to lower peak memory consumption. Table 5 lists the hyperparameter grids we sweep per heuristic; best settings are bold. Table 5: Hyperparameter grids for fine-tuning; best settings per row in bold. Heuristic Lambda λ Learning rate Checkpoint Dataset 5 Attend&Excite CONFORM Divide&Bind FOCUS . 3 . 1 Attend&Excite CONFORM Divide&Bind FOCUS {0.1, 1, 10} {0.1, 1, 10} {0.1, 1, 10} {0.01, 0.1, 1, 10, 100} 5e5 5e5 5e5 {1e4, 5e5, 1e5} {0.1, 1, 10} {0.1, 1, 10} {0.1, 1, 10} {0.01, 0.1, 1, 10, 100} 5e5 5e5 5e5 {1e4, 5e5, 1e5} {100, 150 } {100, 150 } {100, 150 } {100, 150, 200} {200, 250 } {200, 250 } {200, 250 } {200, 250, 300} HORSE&BEAR HORSE&BEAR HORSE&BEAR {HORSE&BEAR, TWOOBJECT} HORSE&BEAR HORSE&BEAR HORSE&BEAR {HORSE&BEAR, TWOOBJECT} D.5 ADDITIONAL METRIC: OPEN-VOCABULARY DETECTION As complementary metric, we assess subject presence with OWL-V2 open-vocabulary detection (Minderer et al., 2023). For each prompt, we pass the subject strings as class queries and count an image as correct if all subjects are detected at least once. We report the fraction of images meeting this criterion. Results for test-time control and fine-tuned models are shown in Tables 6 and 7. Both control algorithms increase subject presence over the base model. However, OWL-V2 is blind to attribute leakage and subject numerosity (it does not verify attributes or counts), so we exclude it from the main evaluation and report it only as supportive metric here. 19 Table 6: OWL-V2 subject presence in percentage for test-time control. Table 7: OWL-V2 subject presence in percentage for fine-tuned models. Heuristic SD3. FLUX Heuristic SD3.5 FLUX Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) 69.33% 66.93% 72.13% 66.80% 77.20% 67.87% 70.80% 68.53% 74.27% 68.27% Base Attend&Excite CONFORM Divide&Bind FOCUS (Ours) 69.33% 66.93% 80.40% 74.93% 77.73% 72.53% 73.33% 63.87% 78.53% 74.66%"
        },
        {
            "title": "E HUMAN STUDY",
            "content": "We evaluate whether metric gains translate to human preferences. Fifty participants each completed 40 prompt-conditioned, pairwise trials, resulting in 2,000 total judgments. In every trial, two images generated from the same prompt were shown side by side with the prompt; participants selected the image that better matched the prompt. The instruction shown was: Which image renders all subjects of the prompt correctly? If both do an equivalent good job, please pick the one you prefer visually. To ensure sufficient rating density, we fixed the sampling seed to 0, yielding one image per methodprompt pair (pool of 150 prompts). Trials were balanced across backbone and setting: SD 3.5 vs. FLUX.1 and test-time control vs. fine-tuning each accounted for one quarter of the comparisons per participant. screenshot of the interface is shown in Figure 8. E.1 ELO RATING COMPUTATION We compute Elo ratings from the pairwise outcomes to obtain an across-method ranking, alongside win rates (fraction of pairwise wins). Elo is initialized at 1500 for all candidates and updated after each comparison with K=32. For candidate with rating RA matched against with RB, the expected score is and the update is EA = 1 1 + 10(RB RA)/400 , = RA + K(SA EA) (37) (38) where SA = 1 for win, 0 for loss, and 0.5 for draw. Higher Elo indicates stronger preference relative to alternatives. Win rate is reported as the proportion of head-to-head wins."
        },
        {
            "title": "F EXTRA SAMPLES",
            "content": "F.1 TEST-TIME CONTROL: STABLE DIFFUSION 3.5 Base Attend&Excite CONFORM Divide &Bind FOCUS (Ours) puffin and penguin standing on windswept shoreline fox, lantern, and teapot in misty forest clearing jellyfish, lighthouse, and pocket watch suspended in seawater sailboat, bicycle, and stack of books beside canal bluetit, croissant, and porcelain cup on balcony rail violin, raven, and pocket watch on stone windowsill Figure 9: Stable Diffusion 3.5 samples with test-time control. All evaluated heuristics are shown at their optimal λ. 21 F.2 TEST-TIME CONTROL: FLUX.1 [DEV] Base Attend&Excite CONFORM Divide &Bind FOCUS (Ours) chameleon, wristwatch, and paper crane on mossy rock peacock, fountain pen, and silk scarf on marble table hammerhead shark and great white shark circling over coral shelf windmill, picnic blanket, and bicycle with basket of tulips quartz crystal, an amethyst, and citrine displayed on black velvet chefs knife, santoku, and paring knife laid on cutting board Figure 10: FLUX.1 dev samples with test-time control. All evaluated heuristics are shown at their optimal λ. 22 F.3 FINE-TUNED: STABLE DIFFUSION 3.5 Base Attend&Excite CONFORM Divide &Bind FOCUS (Ours) Siberian Husky, an Alaskan Malamute, and Samoyed trotting through fresh snow magician, white rabbit, and deck of cards on velvet stage horse and bear in forest robin, bluebird, and warbler perched on garden fence at dawn painter, foxglove, and an easel by cliffside path black cat, an orange cat, and white cat lounging on windowsill Figure 11: Sample results from Stable Diffusion 3.5 fine-tuned with each heuristic. Prompts were not seen during training to evaluate generalization. All images are generated with identical settings (seed, sampler, steps, guidance); each heuristic is shown at its optimal trained λ. F.4 FINE-TUNED: FLUX.1 [DEV] Base Attend&Excite CONFORM Divide &Bind FOCUS (Ours) red fox and an arctic fox sitting side by side in tall grass mooncake, teapot, and jade rabbit under paper lanterns lighthouse, cello, and red scarf beside crashing waves lynx, bobcat, and cougar stepping across rocky ledge bluetit, croissant, and porcelain cup on balcony rail jellyfish, seashell, and glass bottle drifting in turquoise water Figure 12: Sample results from FLUX.1 [dev] fine-tuned with each heuristic. Prompts were not seen during training to evaluate generalization. All images are generated with identical settings (seed, sampler, steps, guidance); each heuristic is shown at its optimal trained λ."
        }
    ],
    "affiliations": [
        "ETH Zurich"
    ]
}