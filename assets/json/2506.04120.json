{
    "paper_title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data",
    "authors": [
        "Ben Moran",
        "Mauro Comi",
        "Steven Bohez",
        "Tom Erez",
        "Zhibin Li",
        "Leonard Hasenclever"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce a novel real-to-sim framework tackling all these challenges at once. Our key insight is a hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within a single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components - from object geometry and appearance to robot poses and physical parameters - directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve high-fidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-to-simulation pipelines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 2 1 4 0 . 6 0 5 2 : r Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data Ben Moran*,1,2 Mauro Comi*,1,3 Steven Bohez1 Tom Erez1 Zhibin Li Leonard Hasenclever1 1Google DeepMind, 2University College London, 3University of Bristol Figure 1: Overview of our framework. Starting from an uncalibrated robot and real-world sensor data (RGB images and proprioception), we perform robot calibration, reconstruct both the geometry and appearance of scene elements. Abstract: Creating accurate, physical simulations directly from real-world robot motion holds great value for safe, scalable, and affordable robot learning, yet remains exceptionally challenging. Real robot data suffers from occlusions, noisy camera poses, dynamic scene elements, which hinder the creation of geometrically accurate and photorealistic digital twins of unseen objects. We introduce novel real-to-sim framework tackling all these challenges at once. Our key insight is hybrid scene representation merging the photorealistic rendering of 3D Gaussian Splatting with explicit object meshes suitable for physics simulation within single representation. We propose an end-to-end optimization pipeline that leverages differentiable rendering and differentiable physics within MuJoCo to jointly refine all scene components from object geometry and appearance to robot poses and physical parameters directly from raw and imprecise robot trajectories. This unified optimization allows us to simultaneously achieve highfidelity object mesh reconstruction, generate photorealistic novel views, and perform annotation-free robot pose calibration. We demonstrate the effectiveness of our approach both in simulation and on challenging real-world sequences using an ALOHA 2 bi-manual manipulator, enabling more practical and robust real-tosimulation pipelines. *Equal contribution. Correspondence to benmoran@google.com Keywords: 3d Gaussian Splatting, Robotics, Differentiable rendering, Differentiable simulation"
        },
        {
            "title": "Introduction",
            "content": "Creating physically accurate and visually realistic simulations directly from real-world robot interactions is crucial for scalable robotics, yet bridging the visual and physical sim-to-real gap remains major hurdle, especially with imperfect data. While recent advances like Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting (3DGS) [2] generate high-quality photorealistic novel views [1, 2], they face significant challenges in dynamic robotic settings: they are sensitive to noisy camera poses common in real trajectories, and produce representations ill-suited for direct use in physics simulators like MuJoCo [3]. Extracting usable simulation assets often requires laborious post-processing [4] or separate geometry estimation pipelines, breaking the link between visual input and physical behaviour. This disconnect critically limits the automated creation of high-fidelity digital twins for robot learning and planning. We argue that overcoming this requires tackling scene appearance reconstruction, object geometry extraction, and robot/camera calibration jointly within single, end-to-end differentiable framework. This paper introduces such framework, enabling the learning of explicit simulation scenes directly from imperfect, dynamic robot trajectory data. Our key innovation is the development of single representation that combines SplatMesh, hybrid scene representation that tightly couples 3DGS for appearance with explicit mesh geometry, with differentiable physics states. Our end-to-end optimization pipeline leverages differentiable rendering and differentiable physics using MuJoCo MJX [5]. This allows visual discrepancies to directly propagate gradients back through the entire system, simultaneously refining not only the 3DGS appearance but also the underlying mesh geometry, estimated robot poses, and camera parameters. This unified approach eliminates the need for separate processing steps and leverages visual feedback to ground the physical reconstruction, proving effective even with imperfect real robot platforms. We demonstrate the effectiveness of our framework on object reconstruction with the low-cost ALOHA bi-manual manipulator using only onboard RGB sensors and pre-existing, imperfect robot model. Our contributions are as follows: End-to-End Real-to-Sim Pipeline: fully differentiable framework jointly optimizing scene appearance, object geometry, robot poses, and camera parameters directly from raw RGB sequences. Scene Reconstruction from Imperfect Data: Demonstrated reconstruction of dynamic robot scenes, including novel objects, from noisy monocular trajectories captured by real robots. Simulation-Ready Asset Generation: Controllable reconstruction of object meshes suitable for direct integration into physics engines, driven by visual consistency."
        },
        {
            "title": "2 Related work",
            "content": "2.1 3D scene reconstruction Radiance fields represent 3D scene in terms of rendering function which determines the viewdependent color and opacity of each point (x, y, z) in the 3D space of the scene. Evaluating this along the 3D rays for each pixel enables the scene to be consistently rendered from arbitrary camera viewpoints. This function can be parameterized in different ways - implicitly via Neural Networks as in such as Neural Radiance Fields [1], or explicitly as in 3D Gaussian Splatting (3DGS) [2]. The key assumption of these methods is knowledge of the 3D structure of the generation process of the multi-view image data, usually by requiring scene to be static across the image views. The camera parameters must also be known precisely, typically by preprocessing with COLMAP [6, 7] which also performs best on static scenes. 2 Feature Table 1: Feature Comparison of 3DGS robotics simulators DRRobot RL-GSBridge RoboGSim RoboStudio Render robot kinematics Learn novel object mesh Physics simulation Differentiable Physics Learning from dynamic scenes 3D asset generation [8] [9] [10] [11] 1SR2S [12] SplatSim Ours [13] 2.2 Real2Sim robotics with radiance fields Beyond the computer graphics community, there has been considerable research into radiance fields applied to robotics [14, 15, 16]. Several recent works obtain 3DGS representations able to render robot arm and its articulated poses, such as [8, 9, 10, 11, 12]. 3DGS representation of the robot is learned either from its CAD model or from manually collected posed photographs, and the Gaussians then segmented according to the robot model. learned mapping between the kinematic configurations and part poses is used to render the robot in arbitrary joint configurations. Articulated 3DGS models must also be learned for each simulated interaction object. Such models can be applied to trajectory tracking via inverse video [8], sim-to-real policy learning in visually realistic simulations (e.g. [10]), or physical property estimation [12]. We compare the features of these works with ours in table 1. We additionally focus on the ability to fit models to imprecise low-cost robots without separate data collection steps. 2.3 Object-based scene decomposition Many applications require compositionally structured scenes with objects. Pre-trained instance segmentation models enable masking the objects of interest, before fitting the masked images separately with local radiance fields. For example, [17] uses the Detic model [18] combined with NeRF, while [19] combines SAM [20] with 3DGS, and [4] considers various methods together with CropFormer [21]."
        },
        {
            "title": "3 Problem setting",
            "content": "Given reasonable but inaccurate simulation of our robot, and samples of real observation data, we wish to recover an accurate simulation of our scene, including new scene geometry. 3.1 Fitting models with prediction errors For any simulation of system we can always consider prediction error: how well do the simulator predicted observations agree with data from the real world? Formally, given multi-modal robot observation composed of modality specific components, = (y1, . . . , yI ), we choose divergence functions di( , ) and weights βi R. Our idealized simulator is model that generates observations from physics states S: = m(s). As the physics state may be large and is not necessarily differentiable, we will further consider perturbations from base state = (s0, θ) with respect to selected parameter set θ Θ. We can then finally define our loss as the weighted sum of the divergence terms over each observation component: L(θ) = (cid:88) iI βidi(Y, m(f (s0, θ))). 3.2 Real-world constraints in robotics This high-level scheme is very general, so to ground our investigation, we consider realistic realto-simulation problem: access to simulator with reasonably accurate geometry and kinematics, 3 Figure 2: Fitting with differentiable rendering. We optimize the scene parameters θ, consisting of object vertices, 3D Gaussian parameters, camera poses and robot joint angles, uniquely using real-world data acquired by robot sensors. but not perfect system identification or camera calibration. The scene also contains an unmodelled novel object, and must be learned from on-board robot sensors without additional data collection. We consider the ALOHA2 low-cost bi-manual tabletop manipulation platform [22][23], together with its associated open-source MuJoCo model [24]. This limits us to four RGB cameras, two fixed and two mounted on the moving wrists. The robot has 6 degrees of freedom in each arm and 1 in each gripper, with Dynamixel actuators. This setting poses obstacles for standard 3DGS data collection and scene modelling: small number of cameras enables only constrained range of viewpoints; the motion of the robot arms makes the scenes dynamic; camera position estimates are noisy due to timing, backlash, imperfect encoder calibration etc. These challenging conditions mean several popular pipeline components are not applicable here. We tested COLMAP on both masked object images, and on the full image trajectories, but could not obtain coherent estimates across the cameras. Moreover, as shown in the Appendix, object segmentation models like SAM2 can provide good segmentations for semantic scene objects, but are not effective for segmenting elements like the robot body that have little texture, lack clear semantic descriptions, and are easily confounded with similar distractor elements in the scene background. To evaluate the recovered simulation, we perform (1) calibration of camera extrinsics and robot pose, (2) novel view synthesis, and (3) novel object geometry reconstruction."
        },
        {
            "title": "4 Method",
            "content": "4.1 System overview We propose general framework that solves diverse range of tasks in real, low-cost, bi-manual robot setting via end-to-end optimization of all the components in our scene. To achieve our goal, we implement prediction error optimization scheme using automatic differentiation and GPU acceleration. Specifically, we first collect real RGB images and robot states from recorded robot trajectories to build model of our scene. For this purpose, we propose novel scene representation that enables end-to-end optimization of all the represented elements. Then, we optimize the required components via this robot trajectory data alone, using differentiable physics simulation and differentiable rendering. Please refer to the Appendix for full details. 4.2 Scene representation 4.2.1 SplatMesh We represent scene objects using SplatMesh, hybrid representation combining triangle mesh for geometry with 3D Gaussians for appearance  (Fig. 1)  . 3D Gaussians are constrained to lie on the surface of the mesh faces, and their orientations transform as the underlying mesh moves. Decoupling visual and geometry information allows flexibility to learn the appearance, pose, and/or shapes of each element, or to treat these as fixed by the original model. To optimize the geometry, we deform the vertices while preserving the underlying connectivity. This approach maintains consistent mesh topology, ensuring fixed and controllable number of vertices and faces. The optimization of the explicit underlying geometry offers two key advantages. First, it enables the direct incorporation of mesh regularization terms into the optimization objective, promoting desirable properties such as smoothness and mesh uniformity. Second, it provides precise control over mesh complexity, resulting in computationally efficient simulations of the reconstructions. The mean µ of each Gaussian is initialized using weighted barycentric coordinate approach. Given face defined by the vertices v1, v2, v3 R3, we randomly sample barycentric weights (implicitly summing to 1) to determine the Gaussians positions: µ = w1 v1 + w2 v2 + w3 v3. Each 3D Gaussian is further parameterized by covariance Σ R33, view-dependent spherical harmonics coefficients representing color, and an opacity term [0, 1]. Object appearance is rendered through the differentiable rasterization of the 3D Gaussians. All SplatMesh parameters, encompassing both appearance and geometry, can be optimized via supervision from RGB images thanks to the differentiable pipeline connecting geometry, Gaussians, and the final rendering. 4.3 End-to-end optimization Given set of input images and an initial scene composed of coarse mesh, e.g. sphere, and set of 3D Gaussians, our framework iteratively refines this representation by minimizing the photometric error between the rendered and the ground-truth images. This enables the optimization of both the scene geometry, represented by the underlying mesh, and the appearance defined by the 3D Gaussian representation. We adopt the differentiable rasterization pipeline from [2], including custom CUDA kernel for efficient forward and backward passes. Additionally, we leverage recent advancements in 3DGS-based surface reconstruction, specifically the use of surface elements or surfels [25]. Surfels, similar to 3DGS, are geometric primitives associated with 2D covariance matrix. In contrast to [25, 26] we simultaneously optimize Gaussians constrained to the mesh while clamping the covariance in the normal direction to tiny constant, rather than learning unconstrained surfels and subsequently inferring mesh. The precise objective function we optimize can be varied to suit the specific task, and we typically include weighted sum of terms from three broad families: Photometric losses: We use L1 or L2 losses between predicted and ground truth RGB pixel values to supervise the optimization of the 3D Gaussians. Geometric regularization: Explicitly optimizing geometry jointly with appearance, we can incorporate Laplacian regularization LLL which penalizes deviations of vertex from the centroid of its nearest neighbors, thus promoting smooth surface. We can also use estimates of geometry from other models, e.g. surface normals [27]. Object segmentation: An L2 silhouette masking loss. This loss compares the predicted silhouette, obtained by adapting the 3D Gaussian rasterizer to object-identity values, against ground truth object mask obtained using SAM2 [28]. However, the binary nature of the ground-truth mask presents challenge as non-overlapping regions provide no gradient information. To address this, we smooth the binary mask with the Euclidean Distance Transform. This smoothing ensures that gradients can propagate throughout the image, even in areas where the predicted and ground-truth silhouettes do not initially overlap."
        },
        {
            "title": "5 Results",
            "content": "To highlight the contribution of our proposed general framework, we demonstrate its capabilities in novel-view synthesis, geometry reconstruction, and 3D asset generation on two datasets: 5 Figure 3: Recovering real world assets from robot data only. Simulation: synthetic dataset, generated using the YCB objects [29], consists of 50 posed images for each of 64 objects. This dataset was divided 80%/20% into train and test sets. Real-to-sim: novel dataset captured on the ALOHA2 platform [30] consisting of 6 observation trajectories (approx. 800 frames in total) including multiview RGB from 4 cameras together with recorded joint angles. 16 frames from the moving cameras were held out for evaluation. All our experiments are run on single GPU NVIDIA H100 (80GB VRAM). Please refer to the Appendix for more details. 5.1 Simulation 5.1.1 Geometry reconstruction We assess the quality of SplatMesh-based object reconstruction on the Simulated YCB dataset. Reconstruction quality was evaluated using the Chamfer Distance (CD) computed on 10 000 points sampled uniformly on both the ground truth and reconstructed meshes. Our full framework obtains CD = 0.073 mm2. Without Laplacian mesh regularization we see increased geometric error (CD = 0.237 mm2) due to high frequency artifacts. An alternative ablation without the surfacealigned Gaussians (surfels) constraint obtains CD = 0.122 mm2. While our method directly optimize mesh, NeRFacto produces radiance field without explicit geometry. The meshes we recovered with NeRFactos Poisson Surface reconstruction pipeline displayed significant artifacts, e.g. floaters and reconstruction of the background, and we were not able to compare them. 5.1.2 Novel-view synthesis We benchmark our method in simulation against two radiance-field techniques: NeRFacto, stateof-the-art implementation of NeRF in the nerfstudio framework [31], and 3DGS, also implemented in nerfstudio. The strong performance in novel-view synthesis and fast reconstruction make these approaches well-suited for robotics applications. All methods are optimized for 15000 iterations. Additionally, we conduct an ablation study to investigate the influence of mesh reconstruction regularization terms on novel-view synthesis. Specifically, the performance of the proposed reconstruction method is assessed both with and without the inclusion of Laplacian and average edge length loss functions, and without constraining the covariance. Tab. 2 shows our results when compared to benchmarks and ablations. We report the standard photometric quality metrics commonly utilized in radiance-field techniques: Peak Signal-to-Noise-Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) between the predicted and ground truth images across all YCB objects. PSNR 0.961 0.972 30.29 26.97 0.057 0.083 SSIM LPIPS NeRFacto 3DGS Table 2: Novel-view synthesis metrics on the simulated YCB object dataset after 15K iterations. Unlike NeRF, which often exhibits artifacts like floaters due to unconstrained volumetric density, our method bounds Gaussians to desired surfaces, leading to higher photometric quality. 3DGS relies on iterative heuristic procedures, such as pruning, cloning, and splitting Gaussians, which necessitate careful tuning and often require several iterations. In contrast, our method benefits from streamlined optimization process, achieving high-quality reconstructions within fixed budget of 15000 iterations ( 3-4 minutes). The inclusion of mesh regularization terms results in an increased visual quality, with PSNR of 30.91, compared to PSNR of 30.70 when regularization was not applied. Without regularization, uniform weight (λL1 = 0.1) was assigned to the reconstruction loss. When regularization terms were incorporated, object-specific weighting (λLL [0.1, 1.0] and λE [0.01, 0.1]) are employed to accommodate varying levels of guidance required by different objects. Additionally, an ablation study on surfel constraints shows the importance of restricting the 3D Gaussian covariance matrices. Unconstrained Gaussians can expand arbitrarily in 3D space, artificially achieving desired colors or opacities by adopting background colors or near-full transparency. This behavior, while potentially leading to overfitting on training views, results in poor generalization to unseen views. Ours Ours w/o mesh reg. Ours w/o surfels 30.91 30.70 25.82 0.044 0.045 0.071 0.970 0.970 0. 5.2 Real Table 3: Real object recovery metrics for individual YCB prop objects. Geometry ( CD, mm) Novel view synthesis (PSNR, dB) Object name Proprio-only Aligned TRELLIS Ours Proprio-only Banana Blue tuna can Lemon Peach Red Apple Strawberry 11.67 13.92 17.55 17.37 17.16 18.92 10.43 16.17 17.89 2.70 21.40 1. 7.35 3.31 5.03 4.42 5.09 3.07 16.96 20.17 14.18 14.00 18.22 16.77 Ours 24.49 25.45 21.62 21.81 24.94 22.73 We test the reconstruction of six YCB objects from real robot trajectory data (segmented with text prompts as described in the Appendix). We additionally estimate surface normals for each RGB camera frame independently using the pre-trained model from [27]. The objective is weighted sum of L1 RGB loss, L2 masks smoothed with 2d distance transforms, and L2 between the predicted and estimated surface normals, with mesh Laplacian regularization. We optimize the mesh vertices, Gaussian parameters, and camera extrinsic rotation parameters. We run the optimization for 40000 steps, and report in Table 3 the Chamfer Distance values for geometry reconstruction and PSNR for novel view synthesis on held-out views of the asset. We compare with the Proprio-only ablation, the same model with the camera extrinsics frozen at the nominal values. The geometry fails to converge effectively, demonstrating that without further optimization this low-cost robot platform lacks the high precision required for the reconstruction task. We also compare shape reconstruction against output from TRELLIS, [32]. This pre-trained 3D foundation model produces 3D appearance and geometry from one or few image views. We provide it with the best hand-chosen masked image from each dataset, and did not find that additional views improved performance. The model does not predict metric scales or object poses, so to enable comparison we optimize the SE3 pose and scale of the TRELLIS mesh using privileged information (Chamfer Distance to the ground truth asset mesh), shown as Aligned TRELLIS in the Tab. 3. We observe that while it can sometimes produce high quality shape predictions, on this real 7 Figure 4: 3D asset generation with SplatMesh. Assets, which are generated from text or single view image, can be imported in any simulator. robot dataset it can sometimes introduce anisotropic scale distortions, add spurious geometry like additional ground planes, or fail to capture the 3D structure of simple shapes - see the Appendix. 5.3 3D asset generation Our framework extends beyond calibration and object reconstruction to enable the generation of 3D objects from single images or text prompts. The incorporation of single-image or text-based object reconstruction provides streamlined approach for the rapid creation of diverse object instances with varying geometries and appearances. This capability is particularly valuable for training robust robotic policies by augmenting datasets and enriching synthetic environments. Given text prompt or single image, we leverage CAT3D [33] to predict geometrically consistent multi-views from single-view input. Then, we follow the procedures outlined in Sec. 4.2.1 for novel-view synthesis and geometry reconstruction using SplatMesh. This approach yields accurate geometry as mesh and visual appearance as 3DGS representation. However, as traditional simulators and rendering engines cannot directly render 3DGS representation, we further optimize texture map using inverse rendering, supervised on the images produced by our 3DGS differentiable renderer. Fig. 4 presents qualitative result of our generated assets within the MuJoCo simulation. Additional results are reported in the Appendix."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper we explored the the feasibility of tackling broad range of model identification and scene estimation problems by directly applying end-to-end optimization, fitting standard MuJoCo models to noisy real-world robot data drawn from existing platforms. We developed novel explicit 3D scene representation, SplatMesh, which enables gradient signals from RGB pixels to propagate through to arbitrary model parameters including geometry, appearance and camera pose. We used this framework to refine the calibration of noisy robot and camera poses, and to show the reconstruction of 3D objects from this real data. This conceptually streamlined but powerful framework is able to leverage existing models to efficiently extract precise information about key simulation quantities from scarce and noisy real world robot data. It opens broad spectrum of possibilities for future work enriching and refining physical models with real world data."
        },
        {
            "title": "7 Limitations",
            "content": "While our framework demonstrates promising results in reconstructing dynamic robot scenes and novel object geometries, we identify limitations of the present version that motivate future extensions. Making use of gradient descent to fit parameters is straightforward way to leverage the differentiability of the model, but is applicable only to smooth parameters and can only provide local information. In particular this limits meshes learned in this way to those homeomorphic to the topology fixed at initialization (e.g. if the initial geometry is topologically equivalent to sphere, the refined mesh will also be topologically equivalent to sphere). This limitation can be mitigated through the choice of initialization mesh structure, but future work will explore more robust and flexible solutions. More generally, gradient descent finds only local minima and so is sensitive to the choice of initialization for non-convex problems. We can consider several possible ways to handle this: In our real robot setting, with existing but noisy data, in practice it is often possible to constrain many parameters to small region of interest for initialization. Using more general uncertainty-aware inference methods rather than simply optimization is an interesting direction for future work Complementing gradient optimization, which excels at high precision and local refinement, with initialization proposed by data-driven learning based approaches Visually, the rendering model used in 3DGS does not enable relighting, so cannot represent effects like reflections and shadows if the dynamic scene elements are moved. This restriction has been overcome in some later works [34, 35] but the data gathering requirements may prove challenge given the real robot constraints. Finally, since we base our differentiable physics simulation on MuJoCo, we are limited to simulation features supported in its JAX based MJX implementation. For now this restricts us to rigid objects, although the framework is open source and still under development, so could in principle be extended to support deformable objects as is the case in the C++ Mujoco implementation. Acknowledgments Thanks to Nicolas Heess and Norman Di Palo for valuable discussions."
        },
        {
            "title": "References",
            "content": "[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. URL https://arxiv. org/abs/2003.08934. [2] B. Kerbl, G. Kopanas, T. Leimkuehler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), July 2023. ISSN 0730-0301. doi:10. 1145/3592433. URL https://doi.org/10.1145/3592433. [3] E. Todorov, T. Erez, and Y. Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026 5033. IEEE, 2012. doi:10.1109/IROS.2012.6386109. [4] A. Dogaru, M. Ozer, and B. Egger. Generalizable 3D scene reconstruction via divide and conquer from single view. [5] C. D. Freeman, E. Frey, A. Raichuk, S. Girgin, I. Mordatch, and O. Bachem. Brax - differentiable physics engine for large scale rigid body simulation, 2021. URL http: //github.com/google/brax. [6] J. L. Schonberger and J.-M. Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 9 [7] J. L. Schonberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. [8] R. Liu, A. Canberk, S. Song, and C. Vondrick. Differentiable robot rendering. [9] Y. Wu, L. Pan, W. Wu, G. Wang, Y. Miao, and H. Wang. Rl-gsbridge: 3d gaussian splatting based real2sim2real method for robotic manipulation learning, 2024. URL https://arxiv. org/abs/2409.20291. [10] X. Li, J. Li, Z. Zhang, R. Zhang, F. Jia, T. Wang, H. Fan, K.-K. Tseng, and R. Wang. RoboGSim: Real2Sim2Real robotic gaussian splatting simulator. [11] H. Lou, Y. Liu, Y. Pan, Y. Geng, J. Chen, W. Ma, C. Li, L. Wang, H. Feng, L. Shi, L. Luo, and Y. Shi. Robo-GS: physics consistent spatial-temporal model for robotic arm with hybrid representation. [12] Y. Zhu, T. Xiang, A. Dollar, and Z. Pan. One-shot real-to-sim via end-to-end differentiable simulation and rendering, 2024. URL https://arxiv.org/abs/2412.00259. [13] M. N. Qureshi, S. Garg, F. Yandun, D. Held, G. Kantor, and A. Silwal. Splatsim: Zero-shot sim2real transfer of rgb manipulation policies using gaussian splatting, 2024. URL https: //arxiv.org/abs/2409.10161. [14] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori, T. Haarnoja, B. Moran, S. Bohez, F. Sadeghi, B. Vujatovic, and N. Heess. NeRF2Real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields. [15] Y. Sato and Y. Yaguchi. Rapidsim: Enhancing robotic simulation with photorealistic 3d environments via smartphone-captured nerf and ue5 integration. In 2024 International Conference on Image Processing and Robotics (ICIPRoB), pages 16, 2024. doi:10.1109/ICIPRoB62548. 2024.10543811. [16] A. Rosinol, J. J. Leonard, and L. Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance fields, 2022. URL https://arxiv.org/abs/2210.13641. [17] X. Kong, S. Liu, M. Taher, and A. J. Davison. vmap: Vectorised object mapping for neural field slam, 2023. URL https://arxiv.org/abs/2302.01838. [18] X. Zhou, R. Girdhar, A. Joulin, P. Krahenbuhl, and I. Misra. Detecting twenty-thousand classes using image-level supervision, 2022. URL https://arxiv.org/abs/2201.02605. [19] M. Ye, M. Danelljan, F. Yu, and L. Ke. Gaussian grouping: Segment and edit anything in 3D scenes. 2023. [20] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick. Segment anything, 2023. URL https: //arxiv.org/abs/2304.02643. [21] L. Qi, J. Kuen, W. Guo, T. Shen, J. Gu, J. Jia, Z. Lin, and M.-H. Yang. High-quality entity segmentation, 2023. URL https://arxiv.org/abs/2211.05776. [22] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware, 2023. URL https://arxiv.org/abs/2304.13705. [23] A. . Team, J. Aldaco, T. Armstrong, R. Baruch, J. Bingham, S. Chan, K. Draper, D. Dwibedi, C. Finn, P. Florence, S. Goodrich, W. Gramlich, T. Hage, A. Herzog, J. Hoech, T. Nguyen, I. Storz, B. Tabanpour, L. Takayama, J. Tompson, A. Wahid, T. Wahrburg, S. Xu, S. Yaroshenko, K. Zakka, and T. Z. Zhao. Aloha 2: An enhanced low-cost hardware for bimanual teleoperation, 2024. URL https://arxiv.org/abs/2405.02292. 10 [24] K. Zakka, Y. Tassa, and MuJoCo Menagerie Contributors. MuJoCo Menagerie: collection of high-quality simulation models for MuJoCo, 2022. URL http://github.com/ google-deepmind/mujoco_menagerie. [25] P. Dai, J. Xu, W. Xie, X. Liu, H. Wang, and W. Xu. High-quality surface reconstruction using gaussian surfels. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [26] A. Guedon and V. Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. [27] G. M. Garcia, K. A. Zeid, C. Schmidt, D. de Geus, A. Hermans, and B. Leibe. Fine-tuning image-conditional diffusion models is easier than you think, 2025. URL https://arxiv. org/abs/2409.11355. [28] SAM 2: Segment anything in images and videos. [29] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and A. M. Dollar. Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols. arXiv preprint arXiv:1502.03143, 2015. [30] J. Aldaco, T. Armstrong, R. Baruch, J. Bingham, S. Chan, K. Draper, D. Dwibedi, C. Finn, P. Florence, S. Goodrich, et al. Aloha 2: An enhanced low-cost hardware for bimanual teleoperation. arXiv preprint arXiv:2405.02292, 2024. [31] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja, D. McAllister, and A. Kanazawa. Nerfstudio: modular framework In ACM SIGGRAPH 2023 Conference Proceedings, for neural radiance field development. SIGGRAPH 23, 2023. [32] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. URL https://arxiv.org/abs/ 2412.01506. [33] R. Gao, A. Holynski, P. Henzler, A. Brussee, R. Martin-Brualla, P. Srinivasan, J. T. Barron, and B. Poole. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024. [34] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam. Relightable gaussian codec avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 130141, 2024. [35] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic. 3d gaussian ray tracing: Fast tracing of particle scenes. ACM Transactions on Graphics (TOG), 43(6):119, 2024. [36] A. Guedon and V. Lepetit. SuGaR: Surface-aligned gaussian splatting for efficient 3D mesh reconstruction and high-quality mesh rendering. [37] D. P. Kingma and J. Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [38] M. Minderer, A. Gritsenko, and N. Houlsby. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36:7298373007, 2023. [39] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [40] Dawson-Haggerty et al. trimesh. URL https://trimesh.org/."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Hardware and Software All experiments detailed in the main paper were conducted on single machine equipped with an NVIDIA H100 GPU possessing 80GB of VRAM, using JAX version 0.6.0 and MuJoCo 3.3.2. The baselines were computed using nerfstudio 1.1.5. single real-world object reconstruction (Section 5.2) over 40,000 steps takes approximately 157 seconds on this machine, after 60s JAX JIT compilation time, and the full experiment takes 3-4 minutes with additional evaluation steps. The optimizaiton loop on the simulated YCB object reconstruction (Section 5.1) over 15,000 steps took less than minute, and approximately 3-4 minutes for the entire process (dataset generation + optimization) as stated in the main paper. We used the ALOHA2 model from [24]."
        },
        {
            "title": "B Method Details",
            "content": "B.1 Scene Representation: SplatMesh As described in the main paper, SplatMesh combines an explicit triangle mesh for geometry with 3D Gaussians for appearance. Geometry: Represented by triangle mesh (vertices and faces). For these experiments we initialized each object with discretized sphere mesh (642 vertices, 5cm radius). We optimize all 642 vertex translations. We also added global 3D translation vector which, although redundant with mesh vertex positions, aids fitting with Adam especially early on when moving the mesh to the object mask. Mesh connectivity remains fixed throughout optimization. Appearance: Represented by 3D Gaussians. Each Gaussian is parameterized by: Barycentric weights (µ R3): position on the associated triangle face, given as logits of weights for the vertex positions. Scales (R3): the covariance Σ R33 of each Gaussian is parameterized as face axisaligned lengths in log scales. Color: as view-dependent Spherical Harmonics (SH) coefficients, with degree between 0 and 3. Opacity (α [0, 1]) Coupling: Gaussians are constrained to the surface of the mesh. We initialize Gaussian means (µ) by sampling barycentric coordinates on the mesh faces, as described in Eq. 1 of the main paper. We found that sampling between 6 and 20 Gaussians per mesh face, while redistributing the particles proportional to face area with stochastic rounding, produces the best result. The final position and orientation of each Gaussian then evolves through the optimization according to the mesh vertices and barycentric coordinates (see Section 4.2.1). Surface Alignment (Surfel Constraint): As mentioned in Section 4.3, we adopt strategy inspired by prior works [25, 36]. However, our formulation differs from these methods in that we maintain mesh at all times, rather than learning unconstrained Gaussians/surfels before inferring mesh in post-processing. We constrain the Gaussian covariance to be axis aligned with the mesh face normal, and the scale along this normal direction is clamped to small constant value, float32 machine epsilon 1.2 107. This encourages Gaussians to represent surface patches rather than volumetric elements. B.2 End-to-End Optimization Our framework utilizes an end-to-end optimization approach based on minimizing prediction error, as outlined in Section 3.1. The core idea is to minimize the discrepancy between real-world observations (Y ) and simulated observations (Y = m(f (s0, θ))) rendered from the current estimate of the 12 scene state and parameters θ. The optimization relies on the differentiability of the entire simulation and rendering pipeline: Differentiable Physics: We use MuJoCo MJX [5], the JAX-native version of the MuJoCo physics simulator [3]. This allows us to compute gradients through the physics state (s = (s0, θ)), including kinematics (mapping joint angles to Cartesian poses). The physics state is represented by the mjx.Data structure, containing generalized coordinates, velocities, Cartesian poses of bodies/cameras, contact forces, etc. although in the present work we focus on object reconstruction and kinematics. Differentiable Rendering: We employ the differentiable rasterizer from 3D Gaussian Splatting [2]. This allows gradients from pixel-level losses to flow back to the parameters of the 3D Gaussians (position, covariance, SH coefficients, opacity) and, crucially in our SplatMesh representation, to the underlying mesh vertex positions. We utilize the publicly available implementation based on [2] with custom interface to JAX. B.2.1 Optimization Objectives The total loss L(θ) is weighted sum of several terms, customized for each task. The parameters θ include mesh vertex positions, Gaussian parameters, camera poses, robot joint angles, etc. The losses are defined in terms of rendered image pixel values, with height, width, and channels = 480, = 848, = 3. We render several different modalities: RGB images ˆx(θ) RHW C, using the conventional Gaussian Splatting rasterizer; surface normal images ˆn(θ), where we instead define the RGB colors of each Gaussian to be the X, Y, surface normal unit vectors, rescaled to (0, 1) and rotated from the world to the camera frame; and mask images ˆm(θ), where we take the colors to be (1, 1, 1) for all Gaussians and (0, 0, 0) on the background. We supervise these with corresponding data observations x, and from datasets described Sec. below. Photometric Loss (Lphoto): The mean absolute difference between rendered RGB images and masked ground truth images: Lphoto(θ) = ˆx(θ) m1 (1) Hard mask Loss (Lmask): Encourages the rendered silhouette of an object to match ground truth segmentation mask. We use an L2 loss on the difference between the rendered segmentation and ground truth binary mask: Lmask(θ) = ˆm(θ) m2 2 (2) Soft Mask Loss (Lsmask): We apply the squared 2D Euclidean Distance Transform (EDT) as described in Section 4.3 to the ground truth mask before taking the L2 norm: Lsmask(θ) = ˆm(θ) [EDT(m)]22 2 (3) Normal Consistency (Lnormal(θ)): Penalizes the difference between predicted surface normals given by the orientations of mesh faces and normals estimated from RGB ground truth. Lnormal(θ) = ˆn(θ) n2 2 (4) Laplacian Smoothing (Llaplacian): Encourages smooth surfaces, by penalizing the deviation of each vertex position vi(θ) from the average position of its edge-connected neighboring vertices Ni. Llaplacian(θ) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) vi(θ) 1 (i) (cid:88) jN (i) vj(θ) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) (5) We use the Adam optimizer [37]. 13 B.2.2 Real shape recovery parameters When optimizing real shapes, we did not optimize these but clamped to full opacity, α = 1.0, finding that this more strongly couples the geometry to the RGB image appearance. We sampled an average 12 Gaussians per face. We used Adam learning rates 0.0005 for the Gaussian parameters, 0.0001 for the mesh vertex deltas and camera poses, and 0.001 for the mesh object translation parameters. See Table 4 for comprehensive breakdown of the loss terms. The target for Lnormal was predicted surface normals derived from from RGB using [27]. Table 4: Detailed breakdown of the loss terms used in the optimization process. Each term targets specific aspect of the reconstruction or generation. Loss term Weight"
        },
        {
            "title": "Lphoto\nLmask\nLsmask\nLnormal\nLlaplacian",
            "content": "1 10 102"
        },
        {
            "title": "C Dataset Details",
            "content": "C.1 Simulation Dataset (YCB Objects) As mentioned in the main paper, our simulation dataset uses YCB objects [29]. 64 YCB objects were used. 50 posed images were generated per object, 80% for training and 20% for testing Camera poses were sampled uniformly on the upper hemisphere around the object center at fixed distance. The fixed distance is automatically computed to be 2.5 times the spatial extent of the geometry. Images rendered using MuJoCos native EGL renderer at 512 512 resolution. Ground truth segmentation masks were also generated. C.2 Real-World Dataset (ALOHA 2) Data was collected using the ALOHA 2 platform [23]. Platform: Low-cost, bi-manual manipulator with 6 DoF per arm + 1 DoF grippers (14 DoF total). Sensors: 4 RealSense D405 cameras (2 fixed cameras positioned overhead and at the front of the table, and one camera on each wrist), proprioceptive joint angle sensors (Dynamixel actuators). Automatic white balance and gain were disabled for consistency between cameras. Resolution was 848x480 pixels, downsampled to 5 fps. In this work we did not use depth images, only RGB data. Data: 6 observation trajectories around 40 seconds each with the arms moving around YCB objects, totalling 1168 frames across all cameras before filtering. Test data: 8 frames from each of the moving wrist cameras per trajectory were held out for novel-view synthesis evaluation (Section 5.2). C.2.1 Real-World Object Segmentation Pipeline To obtain object masks for the real-world YCB objects (Section 5.2), we followed this procedure: 1. Text Prompting: For each target object (e.g., banana, blue tuna can), textual description was used as prompt. 14 (a) Overhead camera (b) Table camera (c) Left wrist camera (d) Right wrist camera Figure 5: Example images from dataset. 2. Bounding Box Proposal: We used OWL-ViT [38] to generate bounding box proposals conditioned on the text prompt for each camera frame. The model outputs bounding boxes and associated confidence scores, and we discarded all but the highest confidence box. 3. Frame Filtering: Frames where the maximum detection confidence score for the target object fell below 0.01 were discarded. 4. Instance Segmentation: The selected bounding box was passed as prompt to SAM2 [28] in multimask mode, generating three candidate segmentation masks within that box. 5. Mask Postprocessing: The SAM2 masks were averaged together pixel-wise, and thresholded at 0.5 to obtain binary masks. This pipeline provided the ground truth masks used for the mask loss (Lmask) during real-world object reconstruction. C.2.2 Segmentation Challenges on Robot Hardware As stated in the main paper (Section 3.2), standard segmentation models face difficulties with robot hardware like the ALOHA 2 arms, especially compared to distinct semantic objects. Figure 6 shows examples of semantic segmentation obtained following the approach outlined in C.2.1 The are multiple reasons for the poor results obtained in segmenting our robot platform. First, large parts of the robot arm have simple, uniform coloration, making it hard to distinguish from similarly colored backgrounds or other objects based solely on appearance. Moreover, the robot is an articulated object and its shape changes drastically with its articulation poses, requiring robustness to wide range of configurations. Grippers, in particular, are often small, complex, and change appearance significantly when opening/closing. This makes mask-based supervision for the robot itself unreliable, motivating our approach of using known (but potentially inaccurate) robot model and refining its pose via visual feedback from the entire scene, rather than relying on direct segmentation of the robot. 15 Figure 6: Examples of segmentation difficulties on the ALOHA 2 robot arm using text prompts (robot arm, gripper) with OWL-ViT2 for bounding box proposals followed by SAM2 for segmentation. The labels below indicate the OWL-ViT2 proposal text. Masks often miss parts (especially grippers), include background elements, or are inconsistent across views and poses due to lack of distinct texture, uniform color, and complex articulation."
        },
        {
            "title": "D Additional Experimental Results",
            "content": "D.1 Robot Calibration To validate the calibration capability, we performed simulation-to-simulation experiments. We rendered synthetic observations from the ALOHA2 MuJoCo model [24] with the standard OpenGL renderer and took these images and joint angles as ground truth. Noise sampled from Gaussian distribution with 0 mean and standard deviation σ {0.005, 0.01, 0.02, 0.03} radians was added to the ground truth joint angles to create dataset with noisy poses. We test calibration by recovering the true poses, optimizing the joint angles and camera perturbations in our framework using SplatMesh for differentiable rendering, and MJX for kinematics. We alternate two Adam optimization steps: Firstly, keeping the pose fixed, we optimize the color of each geometric primitive in the scene by minimizing the L1 photometric loss with respect to the Gaussian color parameters. Secondly, holding the color constant, we optimize the joint angles and fixed camera pose perturbations to maximize the Structural Similarity Index Measure (SSIM) [39] between the rendered SplatMesh and the ground truth images. We took weighted average over the multi-view images in each batch, with weights wf ixed = 1.0 for the two fixed cameras and wwrist = 0.1 for the two wrist cameras. Evaluation: We measured the error reduction by comparing the initial pose error (due to added noise) and the final pose error after optimization. Errors were measured as average Euclidean distance between the estimated and ground truth Tool Center Points (TCPs) of the two grippers (mm). Results are shown in Tab. 5 and visually in Figure 7 and Figure 8. Our method consistently reduces pose error across different noise levels. Table 5: Robot joint pose calibration results. Mean tool pose error (average Euclidean distance between estimated and ground truth TCPs for both arms) before and after optimization via visual feedback. Added noise (radians std. dev.) Initial mean tool pose error (mm) Final mean tool pose error (mm) 0.005 0.01 0.02 0.03 5.43 10.9 21.7 32. 2.90 3.79 12.6 18.5 Figure 7: Effect of calibration on wrist camera view for sample pose with 0.01 radians noise. (Left) Reference image (ground truth rendering). (Center) Pixel-wise MSE error map between the rendering from the noisy pose (using estimated flat colors) and the reference. (Right) Pixel-wise MSE error map between the rendering from the optimized pose and the reference. Brighter colors indicate larger errors. Calibration significantly reduces the visual discrepancy. Figure 8: Distribution of TCP errors (mm) before (Initial) and after (Calibrated) optimization for different levels of added joint noise (radians std. dev.). Box plots show median (orange line), quartiles (box), and range (whiskers). Calibration reduces the error across multiple runs with different initializations. D.2 3D Asset Generation Examples Section 5.3 described our pipeline for generating simulation-ready assets from text prompts or single images using CAT3D [33] for multi-view generation followed by SplatMesh optimization and texture map baking. Figure 9 shows additional examples beyond Figure 4 in the main paper. The process involves: 17 Figure 9: Additional details on 3D asset generation. For each object: (Left) Input text prompt or single image. (Center) Final textured mesh generated using SplatMesh optimization and texture baking, shown imported into the MuJoCo simulator. (Right) Mesh normals rendered with Trimesh [40] Generating consistent multi-view images from the input prompt using CAT3D. This process generates 40 images from predefined trajectory which revolves around the object. Optimizing SplatMesh model (geometry and Gaussians) to fit these multi-view images using the photometric and geometric losses described in Section B.2. Once the SplatMesh is optimized, we render its appearance from multiple viewpoints onto the unwrapped UV coordinates of the optimized mesh using inverse rendering to optimize texture map. This produces standard textured mesh usable in conventional simulators like MuJoCo. D.3 YCB Object Reconstruction Figure 10: Reconstruction of YCB objects in simulation. 18 Section 5.1.2 in the main paper shows results for novel-viewy synthesis on the YCB dataset. Figure 10 shows additional qualitative results on the simulated reconstruction in terms of nove-view synthesis and geometry reconstruction. D.4 Real-World Object Reconstruction We show the six YCB objects reconstructed in the real world and described in the main paper in Figure 12. (a) Real RGB (b) Render RGB (c) Learned mesh (d) Ground Truth YCB Figure 11: Object reconstruction from real robot data fails in the ablation when camera pose is not calibrated jointly with the shape reconstruction. We also show an example failure mode from the no-calibration ablation in Figure 11. Using the ground truth camera poses inferred from the robots nominal joint angles, it is not possible to correctly reconstruct the image or 3D shape. Because of these non-negligible errors for the uncalibrated camera poses we additionally note that the PSNR values reported in the novel-view synthesis results in the main text are calculated after an additional optimization step, to align the camera poses for the held-out views. This step happens after training and does not affect the learned textures or geometry: it is done to enable evaluation of the reconstruction for unseen views at our best estimate of the ground truth camera poses. As mentioned in Section 5.3, we tested the recent 3D reconstruction method TRELLIS [32] on our dataset. This data-driven method is effective at single-view reconstruction and can infer complex unseen geometry based on its training set. However it exhibited some failure modes on our specific robot data, which may be out of its training distribution (see Figure 13). In some cases, the reconstructed object mesh was disproportionately scaled along one axis, not matching the true object shape. Moreover, extra geometry was sometimes generated, such as the additional ground plane in the Lemon example. Finally, For some simple objects such as the Tuna Can, the model failed to capture the basic 3D structure correctly from the provided view(s). We ran TRELLIS with the default settings. We manually selected masked image of each object from our dataset, taking the least occluded, clearest view we could find. We also tried the experimental multi-view feature in TRELLIS but did not find that this improved the results. Note also that the output from TRELLIS does not give scale or pose for the objects, whereas our method is metrically accurate and positions the object with its 6D pose accurately within the robot workspace. These issues highlight the difficulty of obtaining accurate, metric-scale geometry directly from limited, potentially noisy views captured by low-cost robot, motivating our approach that leverages multi-view consistency and end-to-end optimization. 19 (a) Real RGB (b) Render RGB (c) Learned mesh (d) Ground Truth YCB (e) Real RGB (f) Render RGB (g) Learned mesh (h) Ground Truth YCB (i) Real RGB (j) Render RGB (k) Learned mesh (l) Ground Truth YCB (m) Real RGB (n) Render RGB (o) Learned mesh (p) Ground Truth YCB (q) Real RGB (r) Render RGB (s) Learned mesh (t) Ground Truth YCB (u) Real RGB (v) Render RGB (w) Learned mesh (x) Ground Truth YCB Figure 12: Grid of real and learned YCB object images. Our method recovers metrically accurate posed objects despite the challenging conditions under which the robot operates. 20 (a) Real RGB (b) TRELLIS (no scale) (c) Ground Truth YCB (d) Real RGB (e) TRELLIS (no scale) (f) Ground Truth YCB (g) Real RGB (h) TRELLIS (no scale) (i) Ground Truth YCB (j) Real RGB (k) TRELLIS (no scale) (l) Ground Truth YCB (m) Real RGB (n) TRELLIS (no scale) (o) Ground Truth YCB (p) Real RGB (q) TRELLIS (no scale) (r) Ground Truth YCB Figure 13: Real object image prompts, ground truth meshes and TRELLIS meshes. While TRELLIS sometimes infers plausible shapes from single image, it sometimes distorts or includes additional geometry."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "University College London",
        "University of Bristol"
    ]
}