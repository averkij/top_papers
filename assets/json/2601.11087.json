{
    "paper_title": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models",
    "authors": [
        "Qiyuan Zhang",
        "Biao Gong",
        "Shuai Tan",
        "Zheng Zhang",
        "Yujun Shen",
        "Xing Zhu",
        "Yuyuan Li",
        "Kelu Yao",
        "Chunhua Shen",
        "Changqing Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness."
        },
        {
            "title": "Start",
            "content": "PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models Qiyuan Zhang1,2*, Biao Gong2, Shuai Tan2, Zheng Zhang2, Yujun Shen2, Xing Zhu2, Yuyuan Li1, Kelu Yao3, Chunhua Shen1, Changqing Zou1,3 1Zhejiang University 2Ant Group 3Zhejiang Lab 6 2 0 2 6 1 ] . [ 1 7 8 0 1 1 . 1 0 6 2 : r Figure 1. Samples generated by PhysRVG. Our model produces videos with physically plausible rigid body dynamics. Rows14 display four fundamental types of motion addressed in our work, row5 validates the models generalization in out-of-distribution scenarios."
        },
        {
            "title": "Abstract",
            "content": "Physical principles are fundamental to realistic visual simulation, but remain significant oversight in transformerbased video generation. This gap highlights critical limitation in rendering rigid body motion, core tenet of classical mechanics. While computer graphics and physicsbased simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints *Work done during internship at Ant Group. Project lead. Corresponding author. are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to unified framework, termed Mimicry-Discovery Cycle (MDcycle) , which allows substantial fine-tuning while fully preserving the models ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness. Our code and ckpt will be released publicly soon. Project page: https://lucaria-academy.github.io/PhysRVG/ 1. Introduction Physical principles form the foundation of realistic visual simulation, governing how objects move, interact, and respond to forces in the real world [3, 17, 29, 36]. From rigid body motion to the deformation of soft materials, physical laws establish the continuity and coherence that make dynamic scenes appear physically plausible. In traditional computer graphics and simulation pipelines [21, 53], such principles are explicitly encoded through Newtonian mechanics and numerical solvers [35, 37], ensuring that visual outcomes remain physically consistent. However, recent advances in transformer-based video generation have shifted the focus toward data-driven synthesis, where realism emerges statistically from large-scale training rather than from explicit physical modeling [1, 8, 9, 34, 48]. While this paradigm has enabled remarkable progress in semantic understanding and visual fidelity [26, 36], it remains inherently limited in representing the underlying dynamics of physical real motion. The absence of embedded physical constraints leads to inconsistencies such as unstable trajectories [22], implausible collisions [1], and lack of temporal coherence [4], especially when modeling rigid body motion governed by classical mechanics [23, 31]. Despite the importance of physical laws, transformerbased video generators lack explicit structural grounding. Their pretrain-finetune paradigm prioritizes pixellevel reconstruction and perceptual quality, disregarding the constraints imposed by object rigidity. In addition, current video generative models and training frameworks [10, 18, 59] treat physical laws as auxiliary constraints rather than core principles, causing the network to prioritize statistical alignment over physical consistency. During scalingup [9, 22, 49], such alignment is inherently fragile and can be easily compromised, as optimization objectives tend to favor distributional similarity over physical realism. Motivated by these insights, as illustrated in Fig. 2, we for the first time introduce PhysRVG, physics-aware reinforcement learning paradigm that enforces physical collision rules directly in high-dimensional spaces for video generative models. In this work, we focus on Rigid Body Motion [31] as the core representation of physical behavior. To capture the physical dynamic, we introduce physicsgrounded reward function that integrates motion masks, trajectory offsets, and collision detection into reinforcement learning, enabling models to internalize accurate physical knowledge. Subsequently, we extend PhysRVG into unified framework through the proposed Mimicry-Discovery Cycle (MDcycle). It allows the model to undergo subFigure 2. The core idea of PhysRVG. DiT-based video generative models reconstruct noisy videos in latent space using Flow Matching loss, which only captures data distributions () but discards essential spatio-temporal physical cues during conditional alignment and feature extraction ((cid:37)), thereby hindering stable learning of physical knowledge ((cid:37)). While reinforcement learning with subjective ratings can train on physics-rich video data using RLbased feedback (), its evaluation remains perceptually biased and fails to provide stable physical supervision ((cid:37)). In contrast, our PhysRVG leverages the MDcycle to fully utilize data for visual refinement () and enforces physical knowledge injection through the Physics-Grounded Metric (), enabling stable retention and active discovery of physical principles for truly physics-aware learning and generation (). stantial parameter adaptation in the early stages, leveraging the Mimicry phase to capture visual patterns from the data and address unreliable reward signals in standard reinforcement learning. detailed analysis of these reward inconsistencies is provided in Sec. 3.4. As training progresses, the Discovery phase enables the model to progressively internalize physical rules, facilitating smooth transition from data-driven learning to physics-consistent generation. The Cycle mechanism continuously alternates between Mimicry and Discovery, allowing the model to dynamically adjust their balance during training and ultimately converge to stable physics-aware state. For comprehensive training and evaluation of PhysRVG, we construct PhysRVGBench, benchmark comprising 700 videos collected from game footage, online sources, and in-house recordings. These videos cover four types of rigid body motion: collision, pendulum, free fall, and rolling. For each video, we manually annotate the subjects coordinates in the first frame and leverage SAM2 to generate motion masks across the entire video. Based on these masks, we compute two key evaluation metrics, Intersection over Union (IoU) [42] and Trajectory Offset (TO), which provide precise, quantitative measures of physical plausibility in rigid body dynamics. Experimental results in Fig. 1 and Sec. 4 validate the effectiveness of our approach across wide range of physical and visual settings. 2. Related Work 3. Methodology 2.1. Physics-Aware Video Generation. While modern video generative models [9, 14, 22, 38, 45, 46, 51, 58] can render highly realistic frames, their grasp of physical laws remains insufficient [6, 19, 26, 52]. Methods to enhance physical fidelity in video generation can be categorized by how they inject knowledge [2, 7, 15, 25, 27, 47, 61]. The first class conditions generation on explicit knowledge. PhysGen [31] derives motion sequences from rigid body dynamics simulation, GPT4Motion [33] leverages GPT-4o planning and Blender simulation to obtain edge maps and depth maps, NewtonGen [60] and PhysAnimator [54] use optical flow as input guidance. These methods depend heavily on the quality and coverage of the simulator. The second approach expands training data with more physics related examples. Wisa [50] builds dataset containing multi-level basic physics knowledge and finetune the model using MoE. Pisa [23] collects free fall videos and applies post-training to teach the model specific physical behaviors. However, such methods inherently confine the models capabilities to the motion modes represented in the curated datasets. The third class learns from feedback. PhyT2V [55] employs MLLM to iteratively evaluate generations and refine textual prompts, while PhysMaster [20] uses human feedback to rank samples and applies DPO [40] for preference optimization. These pipelines depend on subjective feedback, while ours couples Trajectory Offset (TO) with GRPO [44] to provide physics-aware rewards. 2.2. RL for Generative Model. Reinforcement learning has achieved strong success in large language models [11, 39, 57]. RLHF [13] uses human ranking and preferences as feedback and optimizes the model with Proximal Policy Optimization(PPO) [43]. DeepSeek -R1 [12] uses verifiable outcomes as feedback and applies Group Relative Policy Optimization(GRPO) [44] to greatly improve reasoning. Compared to PPO, GRPO is more efficient because it does not require training separate value model. Motivated by these successes, DDPO [5] brings reinforcement learning into diffusion-based generative models and formulates the denoising process as Markov Decision Process(MDP). Recently, Flow-GRPO [30] and DanceGRPO [56] push GRPO into flow matching models by converting ODE sampling into equivalent SDE sampling and adding tunable exploration noise. For faster training, MixGRPO [24] proposes hybrid ODE-SDE strategy. The model reaches competitive results with very few optimization steps and short training time. TempFlowGRPO [16] and G2RPO [62] leverage the intrinsic temporal structure of flow-based models and address uneven credit assignment under sparse rewards. In this section, we first present the necessary background on reinforcement learning and flow matching in Sec. 3.1. In Sec. 3.2, we define our task and articulate its relationship with physical knowledge. Subsequently, Sec. 3.3 details the design of our reward function. We then present PhysRVG, unified post-training method in Sec. 3.4. Finally, Sec. 3.5 outlines the overall training pipeline. The overall framework of PhysRVG is illustrated in Fig. 3. 3.1. Preliminary Flow Matching. Flow Matching [28, 32] treats generation as denoising process from noise to data. Assume x0 is sampled from real data and x1 is sampled from Gaussian noise distribution. The intermediate sample xt is linear interpolation of x0 and x1, defined as: xt = (1t)x0 +tx1. generative model is trained to predict the velocity field = x1 x0. The optimization objective for this process can be formulated as: L(θ) = Et,x0X0,x1X1 [v vθ(xt, t)2]. (1) Reinforcement Learning. According to Flow-GRPO [30] and DanceGRPO [56], the ODE governing the Flow Matching denoising trajectory can be convert to SDE while preserving the same marginal distributions, the SDE denoising process can be formulated as: dxt = (cid:20) vt + σ2 2t (cid:21) (xt + (1 t)vt) dt + σt dw (2) where vt is the predicted velocity, σt is hyperparameter controlling the strength of stochastic noise (i.e., the intensity of exploration in RL), and represents standard Brownian motion. GRPO estimates the advantage value for each sample by comparing it against other samples within the same group. Given group of samples {xi i=1 generated from the same condition c, the advantage corresponding to the i-th sample is formulated as: 0}G Ai = r(xi 0, c) mean({r(xj 0, c)}G std({r(xj 0, c)}G j=1) j=1) (3) The GRPO algorithm then optimizes the policy model by maximizing the following objective: J(θ) = 1 (cid:88) i=1 1 (cid:88) (cid:16) min(cid:0)ri 1 t(θ) ˆAi t, DKL(πθ πref)(cid:1)(cid:17) t=0 , (4) clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi where ri t1xi t,c) t,c) serves as an importance t1xi sampling ratio, correcting for the bias of evaluating the pθ(xi pθold (xi t(θ) = Figure 3. The framework of PhysRVG. Given text prompt and context frames, the model generates future video frames. For both the groundtruth and sampled frames, we derive motion masks by prompting SAM2 [41] with object coordinates p1 from the first frame, which are manually annotated during data preprocessing. We then compute object trajectories and perform collision detection. The trajectory offset between the sampled and groundtruth trajectories is calculated and reweighted by the collision signal wt to yield weighted trajectory offset Oc, which serves as the per-sample score. All transformer blocks are trained with full parameters. current policy using data generated by an older one. The DKL term acts as regularizer to promote training stability. well the generated videos aligns with physical laws. The details of the reward modeling are described in Sec. 3.3. 3.2. Physics-Grounded Task Definition 3.3. Reward Modeling Physical laws provide natural foundation for defining verifiable tasks in video generation. Among them, rigidbody motion serves as fundamental and well-defined phenomenon that enables quantitative evaluation. It exhibits two intrinsic properties: (1) Observability: the motion of rigid body can be precisely represented through coordinate transformations, and its position is directly measurable from video frames; (2) Determinism: given the initial position the subsequent trajectory of rigid body and velocity, is uniquely determined by Newtonian mechanics. These properties make rigid-body motion suitable for verifying the physical plausibility of generated videos. In this work, we consider four representative types of motion, including collision, pendulum, free fall, and rolling. Specifically, our task can be formulated as simulating the rigid body dynamics by generating future frames based on the observed initial frames. Given the initial Tobs frames {I1, I2, . . . , ITobs} and text prompt c, we aim to generate the subsequent Tpred frames {ITobs+1, . . . , ITobs+Tpred }. This can be expressed as: ˆI1:Tobs+Tpred = Fθ (I1:Tobs, c) (5) Building on this definition, we further formulate physicsgrounded reward function that quantitatively measures how To enable physics-aware reinforcement learning, we design new reward function guided by the principles established in the Physics-Grounded Task Definition. In reinforcement learning, the key challenge lies in designing reliable feedback for generated outputs, which directly determines the models stability and learning direction. Many previous works [20, 55] rely on MLLM or human evaluations as the feedback signals, which are inherently subjective and coarse-grained. Such feedback cannot precisely verify the physical plausibility of generated videos and also fails to capture the relative quality among samples. Therefore, in this paper, we propose new Physics-Grounded Metric for reward modeling, which consists of two core components: the Trajectory Offset and the Collision Detection. Trajectory Offset. Since we establish Physics-grounded Task Definition, where the motion of rigid body is represented by the variation of its center coordinates, we then denote the trajectory of the object by p1:T = {p1, p2, . . . , pT }, where pt R2 indicates the center position of the object at frame t. Because rigid-body motion does not involve deformation, the coordinate sequence p1:T can precisely reflect both positional and velocity changes over time. During data preparation, we manually annotate the objects center coordinate p1 in the first frame. In collision scenarios, two objects are annotated, corresponding to the active and passive bodies, while in other scenarios only one object is annotated. As shown in Fig. 3, given the annotated initial coordinate p1, both the groundtruth and generated videos are processed by SAM2 [41] to extract motion mask sequences gt 1:T and sample . We then compute the mean pixel of each mask as the object center: 1:T 1:T = Center(M gt pgt 1:T ), psample 1:T = Center(M sample 1:T ) (6) Lastly, we quantify the Trajectory Offset(TO) between the generated and ground-truth trajectories by computing the frame-wise deviation: (a) RL Training loss for samples of varying quality. Figure 4. (b) Number of samples assigned to the Mimicry and Discovery branches throughout MDcycle. ="
        },
        {
            "title": "1\nN T",
            "content": "N (cid:88) (cid:88) s=1 t=Tobs (cid:13) (cid:13)pgt (cid:13) t,s psample t,s (cid:13) (cid:13) (cid:13)2 (7) 3.4. Mimicry-Discovery Cycle t,s t,s and psample where denotes the number of annotated rigid bodies in the scene and pgt represent the 2D coordinates of the s-th object at frame in the ground-truth and generated videos, respectively. Collision Detection. Optimizing the model only with the offset in Eq. 7 causes reward hacking, where it favors simple linear motions and avoids complex collisions (Sec. 4.3). To address this, we upweight losses near collision frames, guiding the model to focus on critical physical interactions. Specifically, given the object trajectory p1:T = {p1, p2, . . . , pT }, we compute the velocity sequence v2:T , where each term is defined as vn = pnpn1 , Then, the acceleration sequence a3:T is derived by an = vnvn1 . According to Newtons second law = ma, the occurrence of collision can be detected by sudden changes in acceleration. The detection method is provided in our Appendix. We identify the set of collision timestamps and their adjacent timestamps Cadj = {t 1 C} C. To emphasize these critical moments, temporal weight wt is assigned to each frame as follows: t wt = wcol, wadj, w, if if Cadj otherwise (8) wcol,wadjand are hyperparameters, we provide an ablation study of their effects in Sec. 4.3 and Fig. 7. Finally, the weighted trajectory offset is expressed as: Oc = 1 N (cid:88) (cid:88) s=1 t=Tobs wt (cid:13) (cid:13)pgt (cid:13) t,s psample t,s (cid:13) (cid:13) (cid:13)2 , (9) The offset Oc measures trajectory discrepancies between generated and groundtruth motions, where smaller values indicate higher accuracy and stronger physical consistency. During reinforcement learning, the reward is defined as its negative, = Oc, so that higher rewards correspond to smaller trajectory errors, guiding the policy toward physically accurate and temporally coherent motion generation. Despite being capable of reinforcement learning, the model encounters two practical challenges: (1) difficulty in convergence with small batches and (2) instability in early training stages even with large ones  (Fig. 7)  . As shown in Fig. 4, when trained purely with reinforcement learning, the model tends to perform worse on challenging samples even as it continues to improve on easier ones. This phenomenon arises because the model fails to generate high-quality outputs within limited number of exploratory attempts, leaving it insufficiently prepared for effective exploration in reinforcement learning. To address this issue, we apply the Flow Matching loss to these challenging cases, providing fine-grained pixel-level supervision that stabilizes early training and improves convergence. o}G Specifically, given textual condition and context video I1:Tobs, the model generates group of video samples {xi i=1. Each sample is evaluated by the evaluate function Oc described above, and we compute per-sample advantage value {Ai}G i=1 using Eq. (3). Additionally, we compute the group-average trajectory offset Oc = 1 0, c) to assess the performance on this group. We introduce hyperparameter Threshold to control the strategy switch: when Oc > Threshold, it indicates that the model performs poorly on this case, and we add an additional Flow Matching loss term LM to guide optimization. Otherwise, the training relies primarily on the RL objective LD = J(θ) Eq. (4). Formally, the unified optimization objective is defined as: i=1 Oc(xi (cid:80)G = LD + αLM , (10) where α = (cid:40) 1, 0, if Oc > Threshold otherwise. MDcycle essentially operates under reinforcement learning framework, which is more complex than standard fully finetune or PEFT such as LoRA. Due to space limitations, we provide only brief description here and include the full pseudocode in the Appendix. Model I2V models Wan2.2 5B Wan2.2 14B Kling2.5 Cogvideox HunyuanVideo V2V models Magi-1 PhysRVG Subj. Cons. Back. Cons. Image Qual. Moti. Smoo. Dyna. Degr. Aest. Qual. Temp. Flic. Total Score SA PC IoU TO 86.64 94.75 95.32 92.28 95.99 97.03 96.97 90.35 95.94 95.56 95.87 96.75 97.40 97. 59.06 60.66 64.85 52.49 59.80 60.66 64.96 97.87 99.21 99.47 99.10 99.47 99.57 99. 56.00 50.00 57.14 46.00 46.00 48.28 52.00 37.02 38.61 40.29 38.89 41.28 37.67 41. 97.11 98.64 98.76 98.66 99.06 99.55 99.58 74.86 76.83 78.77 74.76 76.97 77.16 78. 0.57 0.64 0.70 0.56 0.60 0.67 0.76 0.21 0.34 0.41 0.19 0.32 0.38 0. 0.15 0.12 0.23 0.12 0.10 0.27 0.64 162.78 162.40 103.22 158.01 181.62 113.42 15. Table 1. Quantitative comparisons with existing methods on Vbench, VideoPhy-2 (SA, PC) and PhysRVGBench (IoU, TO). The best results are shown in bold, and the second best are underlined. 3.5. Training Pipeline To implement MDcycle in practice, we follow twostage training pipeline that gradually transitions the model from conventional image-to-video generation to physicsaware video-to-video generation. We start from pretrained diffusion transformer with strong visual priors and adapt it to V2V in the first stage. Specifically, we replace the original image condition with the first Tobs = 5 frames of the input video and perform full-parameter finetuning on our training set using the Flow Matching loss, enabling basic temporal generation capability. In the second stage, we focus on improving physical consistency through training under the MDcycle framework, using parameter-efficient weight setup inspired by LoRA and reinforcement feedback guided by the Physics-Grounded Metric. This staged pipeline ensures stable optimization while introducing physics-aware objectives. More details and ablation studies of the training schedule are provided in Appendix, Tab. 3d, and Sec. 4.3. 4. Experiment 4.1. Experimental Settings Dataset. Our training data consist of both open-source and proprietary video collections, totaling approximately 10M samples. The open-source portion includes Panda70M, InternVid, and WebVid-10M, providing diverse motion scenes and visual contexts. To supplement these, we additionally collect proprietary video data, including realworld competition footage, synthetic videos recorded in video games, and real experiments captured using our own recording equipment. For high-quality rigid-body motion data, which are relatively scarce, we manually curate and annotate about 700 video samples covering various motion types such as collision, rolling, pendulum, and free fall. Among them, around 50 videos are separated as the benchmark evaluation set, which is completely excluded from training. Representative samples and annotation examples are provided in the Appendix. Evaluation Metrics. We adopt VBench [19] to evaluate visual fidelity and VideoPhy-2 [4] to evaluate physical realism in generated videos. To further evaluate physical realism in the rigid body motion, we employ PhysRVGBench, which measures the discrepancy between generated and ground-truth motions using Intersection-over-Union (IoU) and Trajectory Offset (TO) to capture spatial overlap and trajectory deviation. Details of our newly proposed IoU and TO are provided in Appendix. 4.2. Experimental Results Quantitative Comparisons. Table 1 shows that PhysRVG delivers high visual quality on VBench and achieves clear gains on VideoPhy-2 and PhysRVGBench. It consistently outperforms existing methods in both IoU and TO, highlighting two key observations. First, V2V models generally achieve better physical realism than I2V models, since video inputs provide more reliable motion cues than text descriptions. Second, PhysRVG further improves over the best V2V baseline, confirming its superior capability in simulating rigid body motions. Qualitative Comparisons. As shown in Fig. 5, competing methods fail to reproduce stable rigid body motions. Even in simple linear rolling, objects often follow incorrect trajectories or remain static (column-3,5), and in collision scenes the outputs exhibit tearing, overlap, and unnatural merging (column-1,2,4,6). We observe characteristic failure cases: an unexpected human appears in row-5-column-1, domino pieces overlap and cross in row-7-column-4, and the glass ball changes color and becomes distorted in row-2column-4. These errors are consistent with human-centric dataset bias that favors generating people over enforcing physical plausibility. We include the original videos for all cases in the Supplementary Materials, and all results are produced with single random seed without manual selecIn contrast, PhysRVG preserves physical integrity tion. and motion coherence across these settings, accurately capturing rolling and collision behaviors and reflecting sound grasp of rigid body physics. Figure 5. Qualitative comparisons with existing methods. Each sample in the figure corresponds to the final frame of generated video. We include the original videos for all cases in the Supplementary Materials. IoU Method 0.15 Baseline 0.41 Baseline+LoRA 0.38 Baseline+FT Baseline+FT+RL 0.61 Baseline+FT+MD 0.64 TO 162.78 48.60 46.27 17.25 15. (w, wadj , wcol) IoU TO 0.64 15.41 0.64 15.03 0.64 15.08 0.63 16.17 (1,1,1) (1,2,3) (1,2,4) (1,2,5) (a) (b) (a) Ablation Study of Training Strategies. Table 2. FT denotes full-parameter fine-tuning, and MD denotes MDcycle. (b) Hyperparameter Analysis of the Collision Weight. The best configuration is marked in gray . 4.3. Ablation Study MDcycle and Other Training Strategies. We conduct an ablation study to evaluate the core objective of MDcycle, which is to stabilize reinforcement learning and improve the overall quality of generated samples. As shown in Tab. 2a and Fig. 7, MDcycle achieves smoother convergence and higher final performance than other strategies. The reward curves in Fig. 7(a) show that MDcycle converges steadily Figure 6. Ablation Study of the Collision Detection. and reaches higher rewards, while pure RL training remains unstable during the initial 50 steps and finally settles at lower level. This instability arises because the model cannot produce reliable outputs within limited exploratory attempts, which restricts effective learning in early stages. similar pattern is observed in the metric distributions of sampled results, as shown in Fig. 7(b,c,d). Compared with FT and FT+RL, MDcycle increases the proportion of high-quality samples, such as those with TO in [0, 5] and IoU in [0.8, 1.0], and significantly reduces lowFigure 7. (a) Reward Curve of RL and MDcycle. (b)(c)(d) Metric Distribution under different training strategies. SDE Window Sample Step 75 % 100 % 50 % 100 % 25 % 100 % 0 % 100 % 2 2 2 16 IoU TO 15.03 0.64 15.58 0.63 17.07 0.61 27.24 0.55 σt 0.2 0.6 1.0 1.4 IoU TO 17.31 0.61 15.83 0.64 15.03 0.64 16.91 0.62 Threshold IoU TO 22.39 15.03 16.97 0.57 0.64 0. 4 8 12 FT steps, MD steps (6000, 250) (16000, 250) (30000, 250) IoU TO 37.23 0.49 15.03 0.64 15.08 0.64 (a) (b) (c) (d) Table 3. Hyperparameter Analysis. All comparisons are conducted under the same data and GPU-hours budget. (a) SDE Interval. (b) Noise Intensity σt. (c) Threshold in Eq. (10). (d) Training Steps. The best configuration is marked in gray . quality cases, such as those with TO in [15, 25] and IoU in [0.0, 0.4]. FT+RL already performs better than FT, and MDcycle achieves the best results across all metrics. Together, these results demonstrate that MDcycle provides greater training stability and stronger physical consistency than other optimization strategies. Collision Detection in Reward Function. We conduct an ablation study to evaluate the effect of collision-aware design in the reward model. Fig. 6 presents the qualitative results, where the FT model produces artifacts such as object disappearance and inaccurate trajectories, and RL improves motion smoothness but fails to handle collisions correctly. Incorporating collision-aware rewards eliminates these issues and yields physically consistent motions. Tab. 2b presents the results of varying the weighting factor w. The performance remains stable within reasonable range, and we select the best value based on this study for all subsequent experiments. SDE Interval. We conduct hyperparameter study on the hybrid SDE-ODE sampling strategy to evaluate its effect on RL training efficiency. Within each sampling window, two consecutive steps use an SDE sampler, and all remaining sampling steps use an ODE sampler. Results in Tab. 3a show that SDE sampling performs best in high-noise regions, indicating that stochastic exploration at early noisy stages enhances semantic learning and overall stability. Noise Intensity. σt in Eq. (2) controls the noise intensity, which determines how extensively the model adjusts its parameters. In most cases, the value is set lower than the optimal 1.0 reported in Tab. 3b (e.g., 0.3 in DanceGRPO), since excessive noise may degrade the models original In PhysRVG, we enperformance and training stability. courage active exploration of physical knowledge, making higher noise ratio desirable. Stable training under this setting is achieved because the V2V process provides an unbiased initialization, ensuring reliable convergence. Threshold. We analyze the Threshold in Eq. (10), which regulates the balance between mimicry and discovery. As shown in Tab. 3c, small threshold causes over-mimicry and limits exploration, whereas large one weakens guidance and destabilizes training. proper threshold allows the model to adaptively transition from mimicry-driven stabilization to discovery-driven exploration, achieving balanced and self-regulated learning process. Training Steps. As shown in Tab. 3d, the benefit of fullparameter fine-tuning before the MDcycle quickly saturates, with the optimal point around 16k steps. This indicates that the V2V process mainly performs data alignment at the visual level, ensuring lower bound of visual quality, while the upper bound of physical knowledge learning still relies on the MDcycle. 5. Conclusion We present PhysRVG, unified reinforcement learning framework that enhances the physical modeling ability of video generative models. Conventional pretrainfinetune paradigms focus on pixel reconstruction and perceptual quality while neglecting physical realism. To overcome this, we introduce physics-grounded metric that measures motion fidelity and integrate abstract physical knowledge through MDcycle. We also develop PhysRVGBench, benchmark for evaluating rigid-body motion quality. Extensive experiments demonstrate the effectiveness of our approach. Future works, limitations, and ethical considerations are provided in Appendix."
        },
        {
            "title": "References",
            "content": "[1] Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale, 2025. 2 [2] Luca Savant Aira, Antonio Montanaro, Emanuele Aiello, Diego Valsesia, and Enrico Magli. Motioncraft: Physicsbased zero-shot video generation, 2024. 3 [3] Chayan Banerjee, Kien Nguyen, Clinton Fookes, and George Karniadakis. Physics-informed computer vision: review and perspectives, 2024. 2, 8 [4] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation, 2025. 2, 6 [5] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. 3 [6] Florian Bordes, Quentin Garrido, Justine Kao, Adina Williams, Michael Rabbat, and Emmanuel Dupoux. Intphys 2: Benchmarking intuitive physics understanding in complex synthetic environments, 2025. 3 [7] Junyi Cao, Shanyan Guan, Yanhao Ge, Wei Li, Xiaokang Yang, and Chao Ma. Neural material adaptor for visual grounding of intrinsic dynamics, 2024. [8] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model, 2025. 2 [9] Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, and Enze Xie. Sana-video: Efficient video generation with block linear diffusion transformer, 2025. 2, 3 [10] Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, and Cho-Jui Hsieh. Self-forcing++: Towards minute-scale high-quality video generation. arXiv preprint arXiv:2510.02283, 2025. 2 [11] DeepSeek-AI. Deepseek-v3 technical report, 2024. 3 [12] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 3 [13] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. 3 [14] Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, and Yu Liu. Check locate rectify: trainingfree layout calibration system for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66246634, 2024. 3 [15] Biao Gong, Shuai Tan, Yutong Feng, Xiaoying Xie, Yuyuan Li, Chaochao Chen, Kecheng Zheng, Yujun Shen, and Deli Zhao. Uknow: unified knowledge protocol with multimodal knowledge graph datasets for reasoning and visionAdvances in Neural Information language pre-training. Processing Systems, 37:96129633, 2024. [16] Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. 3 [17] Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, and Hui Xiong. Simulating the real world: unified survey of multimodal generative models, 2025. 2, 8 [18] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Self forcing: Bridging the trainand Eli Shechtman. test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. 2 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 3, 6 [20] Sihui Ji, Xi Chen, Xin Tao, Pengfei Wan, and Hengshuang Zhao. Physmaster: Mastering physical representation for video generation via reinforcement learning, 2025. 3, 4 [21] Chenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey Stomakhin, and Andrew Selle. The material point method In ACM SIGGRAPH for simulating continuum materials. 2016 Courses, New York, NY, USA, 2016. Association for Computing Machinery. 2 [22] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, [23] Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike Roberts, and Saining Xie. Pisa experiments: Exploring physics post-training for video diffusion models by watching stuff drop. 2025. 2, 3 [24] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flowbased grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. 3 [25] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander Holynski. Generative image dynamics, 2024. 3 [26] Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, Cunxiang Wang, Zhengrong Zuo, Nong Sang, Siteng Huang, and Donglin Wang. Exploring the evolution of physics cognition in video generation: survey, 2025. 2, 3 [27] Yuchen Lin, Chenguo Lin, Jianjin Xu, and Yadong Mu. Omniphysgs: 3d constitutive gaussians for general physicsbased dynamics generation, 2025. [28] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. 3 [43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. 3 [29] Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Ajmal Mian, Mubarak Shah, and Chang Xu. Generative physical ai in vision: survey, 2025. 2, 8 [30] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. 3 [31] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision (ECCV), 2024. 2, 3 [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. 2022. 3 [33] Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, and Shifeng Chen. Gpt4motion: Scripting physical motions in text-to-video generation via blender-oriented gpt planning, 2024. 3 [34] Guoqing Ma, Haoyang Huang, Kun Yan, and Liangyu Chen. Step-video-t2v technical report: The practice, challenges, and future of video foundation model, 2025. 2 [35] Miles Macklin, Matthias Muller, and Nuttapong Chentanez. Xpbd: position-based simulation of compliant constrained dynamics. In Proceedings of the 9th International Conference on Motion in Games, page 4954, New York, NY, USA, 2016. Association for Computing Machinery. 2 [36] Siwei Meng, Yawei Luo, and Ping Liu. Grounding creativity in physics: brief survey of physical priors in aigc, 2025. 2, [37] Matthias Muller, Bruno Heidelberger, Marcus Hennix, and J. Vis. Comun. John Ratcliff. Position based dynamics. Image Represent., 18(2):109118, 2007. 2 [38] NVIDIA, :, Niket Agarwal, and Arslan Ali. Cosmos world foundation model platform for physical ai, 2025. 3 [39] Long Ouyang, Jeff Wu, and Xu Jiang. Training language models to follow instructions with human feedback, 2022. 3 [40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. 3 [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4, 5, [42] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression, 2019. 2 [44] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. 3 [45] Shuwei Shi, Biao Gong, Xi Chen, Dandan Zheng, Shuai Tan, Zizheng Yang, Yuyuan Li, Jingwen He, Kecheng Zheng, Jingdong Chen, et al. Motionstone: Decoupled motion intensity modulation with diffusion transformer for imageto-video generation. CVPR, 2025. 3 [46] Shuai Tan, Biao Gong, Yutong Feng, Kecheng Zheng, Dandan Zheng, Shuwei Shi, Yujun Shen, Jingdong Chen, and Ming Yang. Mimir: Improving video diffusion models for precise text understanding. CVPR, 2025. 3 [47] Xiyang Tan, Ying Jiang, Xuan Li, Zeshun Zong, Tianyi Xie, Yin Yang, and Chenfanfu Jiang. Physmotion: Physicsgrounded dynamics from single image, 2024. 3 [48] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 2 [49] Team Wan, Ang Wang, and Baole Ai. Wan: Open and advanced large-scale video generative models, 2025. 2 [50] Jing Wang, Ao Ma, Ke Cao, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng, Yuhui Yin, and Xiaodan Liang. Wisa: World simulator assistant for physics-aware text-to-video generation, 2025. 3 [51] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, et al. Dreamrelation: Relation-centric video customization. ICCV, 2025. 3 [52] Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, and Xiaodan Liang. Aligning perception, reasoning, modeling and interaction: survey on physical ai, 2025. 3 [53] Tianyi Xie, Zeshun Zong, Yuxing Qiu, Xuan Li, Yutao Feng, Yin Yang, and Chenfanfu Jiang. Physgaussian: Physicsintegrated 3d gaussians for generative dynamics. In CVPR, 2023. 2 [54] Tianyi Xie, Yiwei Zhao, Ying Jiang, and Chenfanfu Jiang. Physanimator: Physics-guided generative cartoon animation. In CVPR, 2025. 3 [55] Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physicsgrounded text-to-video generation, 2025. 3, [56] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 3 [57] An Yang, Baosong Yang, Binyuan Hui, and Bo Zheng. Qwen2 technical report, 2024. 3 [58] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2025. 3 [59] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 2 [60] Yu Yuan, Xijun Wang, Tharindu Wickremasinghe, Zeeshan Nadir, Bole Ma, and Stanley Chan. Newtongen: Physicsconsistent and controllable text-to-video generation via neural newtonian dynamics. arXiv preprint arXiv: 2509.21309, 2025. [61] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Y. Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William T. Freeman. PhysDreamer: Physics-based interIn European action with 3d objects via video generation. Conference on Computer Vision. Springer, 2024. 3 [62] Yujie Zhou, Pengyang Ling, Jiazi Bu, Yibin Wang, Yuhang Zang, Jiaqi Wang, Li Niu, and Guangtao Zhai. G2rpo: Granular grpo for precise reward in flow models. arXiv preprint arXiv:2510.01982, 2025. 3 PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Mimicry-Discovery Cycle Details MDcycle builds on the reinforcement learning paradigm and is structured around two complementary components, the Mimicry Branch and the Discovery Branch. These components are integrated into coherent framework that defines how the model alternates between data-driven imitation and physics-aware exploration. full description of this process, including step-by-step operations, is given in the pseudocode presented in Algorithm 1. B. PhysRVG Architecture & Training Details PhysRVG Architecture. PhysRVG is built on the pretrained video generation model Wan2.2 5B (TI2V). Leveraging its strong visual generative prior, we design twostage training pipeline. In Stage-1, we fully fine-tune the Wan2.2 5B TI2V model into video-to-video model using mixture of open-source and proprietary video collections. We keep the context length fixed at 5 frames. In Stage2, we enhance the model with Physics-Aware generation capability by training MDcycle on high-quality rigid-body motion data. During this stage, MDcycle operates without full-parameter optimization to maintain stable learning. Why We Avoid Full-Parameter Training in MDcycle. Training in RL ranges from full-parameter updates to PEFTstyle strategies (e.g., LoRA), and in our framework parameter efficient fine-tuning offers substantially better stability. As shown in Fig. 8, full-parameter RL produces severe instability, and the reward curve collapses early and never recovers. This behavior is rooted in the high dimensionality of video generation, where reward signals are weak and ambiguous and the optimization landscape contains many misleading directions that push the model toward incorrect behaviors. Stable RL for video requires extremely large batch sizes, and even with our current configuration of 32 GPUs, one group per GPU, and 20 samples per group (effective batch size 640), the signal remains insufficient for reliable full-parameter training. Exploring larger-scale configurations is left for future work. Hyperparameters and Environment. All experiments are conducted on 4 nodes, each equipped with 8 H20 GPUs. We train the model for 16,000 steps in Stage 1 and 250 steps in Stage 2. It is worth noting that we do not use classifier-free guidance (CFG) during the sampling phase of MDcycle. The main reasons are that CFG roughly doubles the computational cost and can introduce training In our V2V setting, the model benefits from instability. Figure 8. Reward curve of RL trained with full-parameter. Parameter Value Video Resolution Input Video Frames Input Video FPS Optimizer Learning Rate Weight Decay Training Device Train steps /stage1 Train steps /stage2 Batchsize /stage1 Batchsize /stage2 Group per GPU Samples per Group Noise intensity σt (w, wadj, wcol) SDE window size SDE window interval Threshold Sampling Steps Same initial noise CFG 480*832 49 30 Adam; β1 = 0.9, β2 = 0.95 1 105 0.0001 32 80G H20 GPUs 16000 250 32 32(gpu) 20(sample) = 640 1 20 1.0 1,2,3 2 75% - 100% 8 16 True False Table 4. Training Configuration rich contextual signals in the input video, which allows us to discard CFG without significantly harming visual quality. However, we do not recommend removing CFG in T2V tasks, where such strong contextual information is absent and turning off CFG typically leads to substantial degradation in sample quality. Within each group, we use the same noise initialization for all samples. Under the constraint of relatively small batch size, this shared-noise strategy further contributes to stabilizing RL training. All of our training configurations are listed in Tab. 4. Algorithm 1 MDcycle Training Algorithm Require: Video generative policy model πθ; trajectory offset reward fucntion R; paired video-text datasets = {vi, ci}M i=1. SDE window timesteps = {si}K i=1; coordinates i=1; total sampling timestep , threshold for Mimicry branch of the first frame = {pi}M Threshold; Generate samples: {xi}G for each sample x: do Ensure: Optimized policy model πθ 1: for training iteration = 1 to do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: end for LM 0 end for else Sample batch Db and correspond coordinates Update old policy: πθold πθ for each context vobs, Db and correspond coordinates P: do i=1 πθold (vobs, c) with the same random initialization noise i=1 using SAM2 and Extract motion mask {mi}T Extract trajectory {pi}T Collision detect and get collision weight {wt}T Compute weighted trajectory offset (TO) oc Get Reward oc i=1 using Center operation t=1 end for Gather group rewards {ri}G Compute group-average trajectory offset Oc 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . if Oc > Threshold then Mimicry Branch for each sample x: do i=1 and group trajectory Oc {oc,i}G (cid:80)G i=1 oc,i i=1 Calculate mimicry loss lM using Flow Matching algorithm end for Gather group mimicry loss LM 1 (cid:80)G i=1 lM end if . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Discovery Branch for each sample do: Calculate advantage: ai riµ for SDE timestep window = {si}K σ i=1 do Calculate discovery loss lD using GRPO algorithm end for Gather group discovery loss LD 1 (cid:80)G i=1 lD end for Update policy via gradient descent: θ θ ηθ(LD + LM ) C. Benchmark Details C.1. Video Samples In this section, we present detailed description of the dataset construction process. Representative samples are provided in Fig. 9. Our PhysRVGBench focuses on four types of rigid-body motion: collision, free fall, rolling, and pendulum motion. The data are collected from several sources, including existing datasets (e.g., PISA [23]), the Internet, video game recordings, and real-world experiments captured using our own devices. For every sample, the text prompt is fixed to The video shows rigid body motion, ensuring that the model relies primarily on the context frames rather than textual variation. We manually annotate object coordinates in the first frame. In collision scenes, annotations include both the active object and the passive object, while in other scenarios we annotate only the primary moving object. Based on the first-frame coordinates, we use SAM2 [41] to extract motion masks. In collision scenes, although two objects are involved, we segment them in two separate passes because generating both masks at once often produces noticeably less accurate results. C.2. Evaluation Metrics In PhysRVGBench, we employ two metrics to evaluate the physical realism of rigid-body motion: interaction over Unions (IoU) and our newly proposed trajectory offset (TO). The IoU metric measures the overlap between the predicted and ground-truth interaction regions, while the Figure 9. Videos in the PhysRVGBench. trajectory offset evaluates the discrepancy between the generated motion trajectory and the ground-truth trajectory. By combining these two metrics, we obtain more comprehensive assessment of the physical plausibility of rigidbody motion. In the following, we describe in detail how these two metrics are computed. Algorithm 2 Collision Detection Algorithm from scipy.signal import find_peaks import numpy as np # Require: # Video: # Coordinate: # Motion Mask masks = SAM2(v,p) # Coordinates list coords = Center(masks) # Velocity,Acceleration vels = np.diff(coords, axis=0) accs = np.diff(vels, axis=0) Figure 10. Evaluation Metric. # Identify acceleration peaks peaks, properties = find_peaks(accs,prominence, distance=distance) # Output Collision indexs collision_frames = peaks+2 # We cannot obtain the acceleration for the first two frames, so we add an offset of 2 here. return collision_frames interaction over Unions (IoU): As illustrated in Fig. 10, Intersection over Union (IoU) is metric used to measure the overlap between two regions. It is defined as the area of their overlap So divided by the area of their union Su. Formally, the IoU is given by: IoU = So Su Trajectory Offset (TO): As illustrated in Fig. 10, For each timestamp Ti, we extract the ground-truth object coordinates pgt and the generated object coordinates psample , and compute the distance between the two (cid:13) psample (cid:13)pgt (cid:13) ot = . We then average these per-frame distances over all frames to obtain the final trajectory offset = 1 psample (cid:13) (cid:13) (cid:13)2 (cid:80)T t=Tobs . . (cid:13) (cid:13)pgt (cid:13) (cid:13) (cid:13) (cid:13) D. Detection Method In this section, we describe how we perform collision detection. Our goal is to identify the time indices of collisions based on the motion mask extracted by SAM2 [41]. The core idea is inspired by Newtons second law, = ma: assuming the objects mass remains constant, the moment when an external force is applied corresponds to change in acceleration. Therefore, we detect collisions by locating the time points at which the acceleration increases sharply. In practice, we detect abrupt changes in acceleration using functions from the SciPy Python library. The corresponding pseudocode is provided in Algorithm 2. E. More Generated Results. Figure 11. Reward curve of different Threshold settings. similar scenes such as swinging playground swing. In free fall scenarios, the model not only produces realistic falling motion but also reasonably captures the subsequent impact with the ground. In rolling scenarios, the model is able to perform repeated acceleration and deceleration along the track, consistent with the track geometry and energy conservation. Fig. 12 and Fig. 13 present additional results generated by our method. All context frames are out-of-distribution: they are either taken from the evaluation set or generated by Kling2.5 Turbo. In collision scenarios, we observe that the model has learned to produce complex motions involving multiple objects and multiple collision events. In pendulum scenarios, although the training data only contain simple pendulum setups (e.g., clock pendulum or single swinging ball), the model can generalize this behavior to F. More analysis on Threshold. In this section, we further analyze the Threshold. The Threshold controls the degree to which the Mimicry branch participates in training. Although involving the Mimicry branch generally improves training stability, more participation is not always better. As shown in Fig. 11, when the Threshold is very small (e.g., threshold = 4 in the reward curve), the Mimicry branch helps stabilize training in the Figure 12. More results generated by PhysRVG Figure 13. More results generated by PhysRVG Figure 14. Failure cases generated by PhysRVG creators of this technology, we acknowledge our responsibility to mitigate these risks. Our reinforcement learning framework, while optimizing for physical accuracy, does not inherently encode ethical or safety constraints, making it agnostic to the content it generates. Therefore, we are committed to responsible release, which includes clearly labeling our models for research purposes, documenting potential misuse scenarios, and encouraging the adoption of content provenance and watermarking technologies. early stage, but ultimately leads to premature convergence to lower reward. We believe this happens because, in the later stages of the MDcycle, the model needs more freedom for RL-based exploration, and excessive pixel-level supervision from the Mimicry branch hampers exploration and constrains the models performance ceiling. Conversely, when the threshold is very large, the convergence behavior becomes almost identical to pure RL, as illustrated in the figure. When the threshold is set to moderate, appropriate value, the model not only converges more stably but also achieves higher final reward. Although this hybrid training strategy is effective, the choice of hyperparameters is largely empirical and must be tuned In future work, we plan to design an for different tasks. automatic adjustment mechanism that can balance training stability and exploration capability. G. Discussion G.1. Limitation Although our model can generate videos with highly realistic motion trajectories, it still makes mistakes in aspects that are weakly correlated with the primary motion. For example, as shown in Fig. 14, the objects in row 1 and row 2 change color after collision, and in row 3, an extra ball appears in the frame when the original ball turns. We attribute these issues to inherent limitations of the model, as these undesirable cases are not supervised by our reward. Our reward is solely related to the motion trajectory of the moving object and is not directly concerned with its color, the presence or behavior of other objects in the scene, or object shape. As result, the model is not penalized when such bad cases occur. This suggests that we need more comprehensive, multi-scale evaluation framework for assessing the physical realism of object motion, which is extremely challenging. G.2. Ethical Considerations Our work is motivated by improving the physical realism of generative models [3, 17, 29, 36], enabling constructive applications from scientific simulation and robotics to enhanced creative pipelines in film and games. However, we recognize that any advance that makes synthetic media more plausible also heightens the potential for misuse. system capable of generating physically consistent videos could be exploited to create highly convincing disinformation, such as fabricated accidents or false evidence, that circumvents detection methods relying on physical inconsistencies. Furthermore, while our work focuses on rigid objects, the underlying techniques are transferable and could be misapplied to create defamatory or privacyinvasive content by placing real individuals into synthetically generated, compromising physical situations. As"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang Lab",
        "Zhejiang University"
    ]
}