{
    "paper_title": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction",
    "authors": [
        "Yifan Wang",
        "Peishan Yang",
        "Zhen Xu",
        "Jiaming Sun",
        "Zhanhua Zhang",
        "Yong Chen",
        "Hujun Bao",
        "Sida Peng",
        "Xiaowei Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, a novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by a large margin."
        },
        {
            "title": "Start",
            "content": "FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction Yifan Wang1* Peishan Yang1* Yong Chen2 Hujun Bao1 Zhen Xu1* Jiaming Sun1 Sida Peng1 Xiaowei Zhou1 Zhanhua Zhang 5 2 0 2 5 ] . [ 1 8 4 3 5 0 . 6 0 5 2 : r 1Zhejiang University 2Geely Automobile Research Institute Figure 1. Photorealistic and real-time rendering of dynamic 3D scenes. Our method achieves the best performance on challenging dynamic scenes with fast and complex motion. Compared with current state-of-the-art methods 4DGS [49] and STGS [21], our PSNR is improved by 2.4dB and 1.4dB on the SelfCap dataset. For dynamic regions, our PSNR is improved by 4.1dB and 2.6dB. Whats more, our method supports real-time rendering at 1080p resolution with speed of 450 FPS using single RTX 4090 GPU."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction This paper addresses the challenge of reconstructing dynamic 3D scenes with complex motions. Some recent works define 3D Gaussian primitives in the canonical space and use deformation fields to map canonical primitives to observation spaces, achieving real-time dynamic view synthesis. However, these methods often struggle to handle scenes with complex motions due to the difficulty of optimizing deformation fields. To overcome this problem, we propose FreeTimeGS, novel 4D representation that allows Gaussian primitives to appear at arbitrary time and locations. In contrast to canonical Gaussian primitives, our representation possesses the strong flexibility, thus improving the ability to model dynamic 3D scenes. In addition, we endow each Gaussian primitive with an motion function, allowing it to move to neighboring regions over time, which reduces the temporal redundancy. Experiments results on several datasets show that the rendering quality of our method outperforms recent methods by large margin. Equal contribution. Corresponding author: Xiaowei Zhou. Dynamic view synthesis aims to produce novel views of dynamic 3D scene from captured multi-view videos, which has wide range of applications, such as movie production, video games, and virtual reality. Traditional approaches [5, 7, 8, 12, 31, 32, 51] use sequences of textured meshes to represent dynamic 3D scenes. Such representation requires complicated hardware setups for highquality reconstruction, thus making traditional approaches limited to controlled environments. NeRF-based methods [10, 19, 30] have achieved impressive results in dynamic view synthesis by modeling scenes as neural implicit representations. However, these representations are computationally expensive, leading to slow rendering speed and hindering practical applications. Recently, popular paradigm for dynamic view synthesis is to equip 3D Gaussian primitives [2, 11, 16, 22, 25, 27, 37, 44, 48, 53] with deformation fields to model dynamic scenes. These methods model the geometry and appearance of the scene based on Gaussian primitives in canonical space and then use MLP networks to model scene motions for deforming the canonical-space scene to observationspace scenes at particular moments. Although these methods achieve real-time and high-quality rendering performance on scenes with small motions, they often struggle to handle scenes with complex motions. plausible reason is that when objects move much in the scene, these methods need to build long-range correspondences between canonical space and observation spaces, which is difficult to be recovered from RGB observations, as discussed in [20, 33]. In this paper, we propose novel 4D representation, named FreeTimeGS, for reconstructing dynamic 3D scenes with complex motions. In contrast to previous methods [44, 48] that define Gaussian primitives at canonical space only, FreeTimeGS allows Gaussian primitives to appear at arbitrary positions and time steps, possessing the strong flexibility. In addition, we assign an explicit motion function to each Gaussian primitive, allowing it to move to neighboring regions over time, which facilitates the reuse of Gaussian primitives along the temporal dimension and reduces the representation redundancy. By endowing Gaussian primitives high degrees of freedom, our representation has two advantages. First, this significantly improves the ability to model dynamic 3D scenes and the rendering quality, as demonstrated in our experiments. Second, we only need to model short-range motions between Gaussian primitives and observed scenes, compared with deformationbased methods [44, 48]. Therefore, our motion function can be implemented as linear function, alleviating the illposed optimization problem. During experiments, we find that optimizing FreeTimeGS with only rendering loss tend to get trapped in local minima on fast-moving regions, leading to poor rendering quality. To tackle this issue, we examine the opacity distribution of Gaussian primitives and discover that considerable part of it approaches 1. The result suggests that high opacity of some Gaussian primitives may prevent the gradient from back-propagating to all Gaussian primitives and block the optimization process. Motivated by this observation, we design simple regularization strategy to penalize the high opacity of Gaussian primitives in the early stage of optimization, which effectively mitigates the local minima problem and improves our rendering quality. To validate the effectiveness of our method, we evaluate FreeTimeGS on multiple widely used datasets for multiview dynamic novel view synthesis, including Neural3DV [18] and ENeRF-Outdoor [23]. Our method achieves highest quality on these public datasets compared with existing state-of-the-art methods. To further evaluate and demonstrate the capability of our method on challenging scenes, we also collect dataset with much faster and more complex motions compared with Neural3DV [18]. Our method achieves best quality on it compared with the SOTA methods by large margin on both quality and efficiency. 2. Related Work Dynamic scene reconstruction with RGB-D cameras. In the early days, dynamic scene reconstruction leverages RGB-D cameras to capture the dynamic geometry and appearance of scene. Methods pioneered by DynamicFusion [31] use depth sensors to build TSDF model in canonical space. Each frame applies non-rigid tracking to create deformation graph, enabling TSDF fusion in canonical space to integrate reconstruction results across frames. Fusion4D [8] and Holoportation [32] further refined this system, achieving highly impressive dynamic reconstruction results. Subsequent works, such as DoubleFusion [51] and BodyFusion [50], introduced human body priors, enhancing the robustness of non-rigid tracking. The main bottleneck of the aforementioned methods lies in the lack of robustness and inaccuracy of non-rigid tracking. Data-driven approaches, such as DeepDeform [3] and neural non-rigid tracking, address this by combining pre-trained optical flow networks and using CNNs to predict non-rigid correspondences, achieving promising results. However, since these methods cannot optimize visual appearance in an end-to-end manner, they still suffer from issues with robustness and poor visual quality. Additionally, they rely on extra depth sensors, which increases the system complexity and overall cost. NeRF-based dynamic scene reconstruction. With the rise of NeRF [30] and differentiable rendering, neural scene representations have gradually become the mainstream approach for dynamic scene reconstruction. Similar to the RGB-D methods discussed above, methods like [33, 34, 36] build canonical NeRF model and use an MLP to capture the deformation field from each frame to the canonical model. Another line of research, including NeuralBody [35] and subsequent works [6, 15, 38], utilizes body shape priors, storing per-frame structured latent codes on the body parametric model to model the frame-dependent dynamic appearances and geometries. Neural3DV [18] opts to use time-conditioned neural representations to directly model dynamic scenes in the 4D space. This approach provides high representational capacity, but modeling dynamics directly in 4D introduces considerable computational costs, leading to long training times and high resource demands To address efficiency and scalability, hybrid representations such as K-Planes [10] and HEX-Plane [4] combine voxel grids with neural fields, substantially reducing training and inference times. Meanwhile, data-efficient methods [40, 41] utilize tensor factorization to enable compact, long-sequence storage. Despite the impressive progress achieved by NeRFbased methods, challenges in dynamic scene reconstruction remain on slow rendering speeds, poor rendering quality and high storage requirements. In contrast, our proposed 2 Figure 2. Pipeline Overview. We represent dynamic scene using Gaussian primitives that can appear anytime anywhere. Each Gaussian is assigned with motion function to to model its movement. And its opacity is modulated by the temporal opacity function which control the impact of the Gaussian primitive over time. With this 4D representation, we further regularize the Gaussians with 4D regularization loss and optimize the rasterization result with rendering loss for reconstructing dynamic 3D scene from multi-view videos. approach demonstrates clear advantages in these aspects, making it more feasible solution for scalable dynamic scene reconstruction. Gaussian-based dynamic scene reconstruction. Recently, 3D Gaussian Splatting (3DGS) [14] has gained popularity as mainstream approach due to its real-time rendering speed and sharp rendering quality. Dy3DGS [28] first adapts 3DGS to dynamic scenes by tracking Gaussian primitives on frame-by-frame basis. Similar to RGB-D and NeRF-based methods, [2, 11, 16, 22, 25, 27, 37, 44, 48, 53] model the geometry and appearance of the scene in canonical space and then use deformation MLP networks to model scene motions. Notably, instead of using deformation to model dynamic, 4DGS [49] and STGS [21] leverage 4D Gaussian primitives to represent dynamic scenes. However, these method still struggle to handle scenes with complex motions due to their motion representation. 4DGS entangles geometry and velocity, making it difficult to optimize both. Specifically, its spatial scale can be rotated by the velocity vector, converted into temporal scale and vice versa. Whats more, instead of Euclidean space, they optimize the velocity in angular space, where in the case of fast motion, small angular changes can lead to large Euclidean velocity changes, making it difficult to converge. STGS explicitly models motion using polynomials and angular velocity, but this method has too many parameters and is difficult to optimize in complex motion scenarios, leading to overfitting. LongVolCap [47] also uses 4D Gaussian primitive representations, but it focuses on modeling long volumetric videos, which is orthogonal to our work and can be further integrated. 3. Method primitives at any time and location to faithfully represent the content of the dynamic scene. Figure 2 presents the In Sec. 3.1, we first overview of the proposed approach. introduce the design details of our 4D representation. Then, Sec. 3.2 describes the optimization strategies to train the 4D representation, which includes 4D regularization, periodic relocation, and 4D initialization. 3.1. Gaussian primitives at anytime anywhere To represent the content of dynamic 3D scenes, we define Gaussian primitives that can appear at any spatial position and time step. In addition, our approach assigns motion function to each Gaussian primitive, allowing it to dynamically adjust its position over time to the neighboring region, thereby enhancing its ability of representing the geometry and appearance of the dynamic scene. In addition, our approach assigns motion function to each Gaussian primitive, allowing it to dynamically adjust its position over time to the neighboring region to simulate the physical movement of real-world dynamic objects in the scene, thereby enhancing its ability to represent the evolving geometry and appearance of the dynamic scene. Specifically, each Gaussian primitive consists of eight learnable parameters: position, time, duration, velocity, scale, orientation, opacity and spherical harmonics coefficients. To calculate the opacity and color of the Gaussian primitive at any (x, t), we first move the Gaussian primitive to obtain its actual spatial position µx(t) at time according to its motion function, which is defined as: µx(t) = µx + (t µt), (1) Given multi-view videos that capture dynamic scene, our goal is to generate novel views of the target scene at arbitrary time. Our key idea is designing novel 4D representation, named FreeTimeGS, which leverages Gaussian where R3 is the velocity of Gaussian primitive, and µx and µt are the original position and time of Gaussian primitive, respectively. Based on the moved Gaussian primitive, we calculate its 3 color at position through the spherical harmonics model: (cid:88) (cid:88) = l=0 m=l clmYlm(d(µx(t))), (2) where is the color of Gaussian primitive, is the degree of spherical harmonics, clm is the spherical harmonics coefficients, d(µx(t)) is the view direction at position µx(t), and Ylm(d(µx(t))) is the spherical harmonics basis function with direction d(µx(t))."
        },
        {
            "title": "The opacity of the moved Gaussian primitive at position",
            "content": "x and time is defined as: σ(x, t) = σ(t) σ exp (cid:16) 1 2 (x µx(t))T Σ1 (x µx(t)) (cid:17) , (3) where σ is the original opacity of Gaussian primitive. Σ is the covariance matrix defined by the scale and orientation of Gaussian primitive: Σ = RSST RT . σ(t) is the temporal opacity that aims to control the impact of the Gaussian primitive over time. To enable the time and duration of Gaussian primitives to be automatically adjusted by rendering gradients, the temporal opacity should be unimodal function with scaling parameter. Therefore, our approach models σ(t) as Gaussian distribution: (cid:32) σ(t) = exp (cid:18) µt 1 2 (cid:19)2(cid:33) , (4) where µt and is the time and the duration of the Gaussian primitive. 3.2. Training Similar to 3DGS [14], our approach optimizes the parameters of Gaussian primitives by minimizing the rendering loss between the observed images and the rendered images: Lrender = λimgLimg + λssimLssim + λpercLperc, (5) where Limg, Lssim, and Lperc are the image loss, SSIM loss [43], and perceptual loss [52], respectively. However, we find that simply optimizing the proposed representation with only rendering loss often leads to poor rendering quality in fast-moving or complex motion regions. To address this problem, we analyze the distribution of Gaussian primitives opacity and find that significant portion of it is near 1. Therefore, plausible reason for the poor quality is that high opacity of some Gaussian primitives can prevent the gradient from backpropagating to all Gaussian primitives, hindering the optimization process. 4D regularization. Based on the observation, our approach designs regularization loss to constrain high opacity values of Gaussian primitives, which is defined as: Lreg(t) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (σ sg [σ(t)]) , (6) where is the time of observed images in each training iteration, is the number of Gaussian primitives, and sg[] is the stop-gradient operation. Here we introduce the temporal opacity σ(t) as the weight of the regularization loss, which represents the impact of Gaussian primitives at particular time. For Gaussian primitives with less impact, we reduce the penalty on them. Note that the stop-gradient operation is applied to prevent the regularization loss from minimizing the temporal opacity. Periodic relocation of primitives. Although the regularization loss can effectively improve the rendering quality, it causes dramatic increase in the number of Gaussian primitives needed to represent the same scene. To mitigate this, we design periodic relocation strategy to move Gaussian primitives with low opacity to the region with high opacity. Specifically, we design sampling score for each Gaussian primitive to measure the region that requires more primitives: = λgg + λoσ (7) where and σ are the spatial gradient and opacity of the Gaussian primitive, and λg and λo are the weights of the gradient and opacity, respectively. For every iterations, we move the Gaussian primitives with opacity below threshold to the region with high sampling score. Initialization of our representation. To further improve the rendering quality, our approach proposes strategy to initialize the position, time, and velocity of Gaussian primitives. For each video frame, we first use ROMA [9] to obtain 2D matches across multi-view images and then calculate 3D points through 3D triangulation. These 3D points and the corresponding time step are used to initialize the position and time of Gaussian primitives. Subsequently, 3D points of two video frames are matched by k-nearest neighbor algorithm, and the translation between the point pairs are taken as the velocity of Gaussian primitives. During the optimization process, we further anneal the optimization rate of velocity according to λt = λ1t 0 + λt 1, where goes from 0 to 1 during training. This annealing motion scheduler helps to model the fast motions in the early stage and the complex motion in the later stage. 3.3. Implementation Details We implement our approach using PyTorch. We use the Adam optimizer for optimization with the same settings as 3DGS. The model is trained for 30k iterations for sequence length of 300 frames, which takes around 1 hour on an RTX 4090 GPU. The weight of the 4D regularization loss is λreg set to 1e2, and the weights of the image, SSIM and perceptual loss λimg, λssim, λperc are set to 0.8, 0.2 and 0.01, respectively. The weights of the gradient and opacity in the sampling score λg, λo are set to 0.5 and 0.5, 4 respectively. And the periodic relocation is performed every = 100 iterations. 4. Experiments Datasets. We conducte our experiments using three datasets: Neural3DV [19], ENeRF-Outdoor [23], and our self-collected SelfCap dataset. Neural3DV contains six scenes, each captured by 19-21 cameras with resolution of 2704 2028 at 30 FPS. For each scene, we use the first 300 frames for training and evaluation, with an image resize ratio of 0.5. ENeRF-Outdoor is dynamic outdoor dataset with three scenes, captured by 18 synchronized cameras at resolution of 1920 1080 and 60 FPS. We also use the first 300 frames for training and evaluation, with an image resize ratio of 1. As the Neural3DV and ENeRF-Outdoor datasets lack significant motion scenes, we collect our own dataset, SelfCap, to test performance in large-motion scenarios. SelfCap contains eight scenes, each with 60 frames captured by 22-24 cameras, including daily life scenes such as dancing, playing with pets, and repairing bicycles. Compared with existing datasets, SelfCap contains more challenging scenes with fast and complex motion. The resolution is 3840 2160 (1080 1080 for the bike scene) at 60 FPS, with an image resize ratio of 0.5 (1 for the bike scene). Metrics. We use PSNR, DSSIM [43] and LPIPS [52] to access the quality of the rendered images in our method and baselines. PSNR measures the l2 difference between reconstructed image and its ground truth, with higher values indicating less disparity. DSSIM1 and DSSIM2 measure the structural similarity, where higher values denote greater similarity. DSSIM1 sets data range to 1.0, while DSSIM2 sets to 2.0. LPIPS evaluates perceptual similarity, aligned with human perception. Lower LPIPS values indicate better perceptual similarity. 4.1. Comparison Experiments Neural3DV. Qualitative and quantitative comparisons are shown in Figure 4 and Table 1, respectively. As evident in Table 1, our method outperforms all baselines in all metrics. Figure 4 illustrates our methods ability to more accurately capture details in dynamic regions, such as the tail section of the flamethrower. Additionally, our method can achieved better results in static background regions, exemplified by clearer rendering of the forest visible through the window. ENeRF-Outdoor. Qualitative and quantitative comparisons are shown in Figure 3 and Table 2, respectively. This dataset introduces increased dynamic motion, as the actors exhibit extensive arm movements and manipulate doll with wide range of motion. As indicated in Table 2, our method achieves the highest performance across metrics. Figure 3 further demonstrates our methods ability to capTable 1. Quantitative comparison on the Neural 3D Video [19] Dataset. We report PSNR, DSSIM1, DSSIM2, and LPIPS to evaluate the rendering quality. 1: only includes the Flame Salmon scene. 2: excludes the Coffee Martini scene. PSNR DSSIM1 DSSIM2 LPIPS Neural Volume1 [26] LLFF1 [29] DyNeRF1 [19] HexPlane2 [4] K-Planes [10] MixVoxels-L [42] MixVoxels-X [42] HyperReel [1] NeRFPlayer [39] Deformable-3DGS [44] C-D3DGS [13] SWinGS [37] Ex4DGS [17] 4DGS [49] STGS [21] Ours 22.80 23.24 29.58 31.71 31.63 31.34 31.73 31.10 30.96 31.15 30.46 31.10 32.11 32.01 32.05 33.19 - - - - - - - 0.036 0. 0.030 - 0.030 0.030 - 0.026 0.026 0.062 0.076 0.020 0.014 0.018 0.017 0.015 - - - 0.022 - 0.015 0.014 0.014 0.013 0.295 0.235 0.083 0.075 - 0.096 0.064 0.096 0. 0.049 0.150 0.096 0.048 0.055 0.044 0.036 Table 2. Quantitative comparison on the ENeRF-Outdoor [23] Dataset. Green and yellow cell colors indicate the best and the second best results, respectively. PSNR DSSIM2 LPIPS FPS ENeRF [23] 4K4D [45] 4DGS [49] STGS [21] Ours 24.96 25.28 24.82 24.93 25.36 0.107 0.096 0.089 0. 0.077 0.299 0.379 0.317 0.297 0.244 3 220 90 226 454 Table 3. Quantitative comparison on our SelfCap Dataset. We include quantitative results for both the entire image and only dynamic regions (entire/dynamic). For 4DGS and STGS, we traversed different camera near plane settings during testing to maximize floater removal. Green and yellow cell colors indicate the best and the second best results, respectively. PSNR DSSIM2 LPIPS FPS STGS [21] 4DGS [49] 24.97/25.32 25.98/26. 0.048/0.029 0.036/0.019 0.273/0.123 0.237/0.104 Ours 27.41/29.38 0.024/0.013 0.204/0. 142 65 467 ture fine details in rapid motion, particularly in objects such as the doll held by the actors and the text on the T-shirt. SelfCap. Qualitative and quantitative comparisons are shown in Figure 5 and Table 3, respectively. Our method achieves the best performance on the SelfCap dataset, which contains challenging dynamic scenes with fast and complex motion. As shown in Table 3, our method outperforms all baselines in all metrics. In Figure 5, our method can better capture the details of the dynamic regions, like the fastmoving hands and the complex motion of the dancers body. 5 Figure 3. Qualitative comparison on the ENeRF-Outdoor Dataset. Our method achieves higher quality for fast-moving objects and regions, such as the swinging arms and dolls in hands, and clearer text details on the clothes. Figure 4. Qualitative comparison on the Neural 3D Video Dataset. Our method achieves the best rendering quality compared with baseline methods, especially for distant static regions and fast-moving dynamic regions. 6 Figure 5. Qualitative comparison on our SelfCap Dataset. Our method achieves significantly higher rendering quality than other methods. For example, in the dance sequence, other methods struggle to handle fast-moving regions, such as fingers, faces, and texture details on clothes, while our method retains their details. In the bike sequence, other methods failed to model the complex motions of hands and rapidly rotating pedals, while our method maintain high-quality rendering results. 7 Figure 6. Ablation study of proposed components on dance1 sequence of our SelfCap Dataset. Removing our proposed components leads to visible artifacts in the rendered results, especially in the dynamic regions with fast motion. Our method produce high-quality results even in the challenging dynamic scenes. Table 4. Ablation studies. We include quantitative results for both entire sequence and the subsequence with fastest motion (entire/fastest) . PSNR DSSIM2 LPIPS 28.10/26.92 0.024/0.031 0.165/0.161 w/o our motion 28.68/29.09 0.023/0.024 0.159/0.150 w/o 4d regularization w/o periodic relocation 29.07/29.15 0.020/0.021 0.155/0.146 28.33/27.06 0.023/0.030 0.162/0.158 w/o 4d initialization Ours 29.74/30.75 0.018/0.017 0.152/0. Table 5. Ablation studies. We include quantitative results for both entire sequence and the subsequence with fastest motion (entire/fastest) . PSNR DSSIM2 LPIPS λreg = 0 λreg = 1e3 λreg = 1e2 λreg = 1e 28.68/29.09 29.10/29.79 29.74/30.75 26.43/27.33 0.023/0.024 0.021/0.021 0.018/0.017 0.035/0.036 0.159/0.150 0.154/0.139 0.152/0.133 0.198/0.183 4.2. Ablation Studies We perform ablation studies on the 60-frame dance1 sequence of the SelfCap dataset and further report the results on 10-frame with the fastest motion to evaluate the performance of our method in high-motion scenarios. Quantitative and qualitative results are shown in Table 4, Table 5, and Figure 6. Motion representation. The w/o our motion variant removes the proposed motion representation and replace it with the motion representation used in 4DGS [49]. As shown in Table 4 and Figure 6, our motion representation significantly improves the ability to model dynamic 3D scenes, especially in region with fast and complex motion. 4D regularization. The w/o 4d regularization variant removes the 4D regularization loss Lreg in Eq. 6. Without the 4D regularization, gaussians with large opacity will hinder the optimization process, leading to suboptimal results on detailed region. We also ablate the effect of different λreg values in Table 5 and choose λreg = 1e2 for all of the experiments. Periodic relocation. The w/o periodic relocation variant use the same densify strategy as the 3DGS [14] and do not perform the proposed periodic relocation. We also control the number of gaussians to be the same as the proposed method for fair comparison. Without the periodic relocation, the model tend to use more gaussians with lower opacity to model the scene, leading to suboptimal results when the number of gaussians is limited. 4D initialization. The w/o 4d initialization variant removes the proposed velocity initialization method and use zero velocity initialization instead. As shown in Table 4 and Figure 6, the proposed 4D initialization method significantly improve the models ability to model the fast motion in the scene. 5. Conclusions This paper introduces FreeTimeGS, novel 4D representation method for dynamic 3D scenes. FreeTimeGS allows Gaussian primitives to emerge at any time and location, offering enhanced flexibility to model complex motions. By assigning an optimizable explicit motion function and temporal opacity function to each Gaussian primitive, our representation can more faithfully and flexibly represent the dynamic scene. Furthermore, we propose simple regularization strategy that penalizes high opacity in Gaussian primitives, effectively mitigating the local minima problem during optimization. Experimental results demonstrate that FreeTimeGS achieves higher rendering quality and rendering speed on multiple widely-used datasets for multi-view dynamic novel view synthesis. Notably, our method still has few limitations. For one, our method still requires length reconstruction process for each dynamic scene. Future work could potentially mitigate this by incorporating generative priors on the proposed representation for optimization-free reconstruction. Another limitation of our method is that the current representation doesnt support relighting, only focusing on novel view synthesis. Future work could extend the current representation with surface normal and material properties to extend its applicability for relighting. 8 Acknowledgement This work was partially supported by NSFC (No. 62172364, No. U24B20154, No. 62402427), Zhejiang Provincial Natural Science Foundation of China (No. LR25F020003), and Information Technology Center and State Key Lab of CAD&CG, Zhejiang University. We also acknowledge the EasyVolcap [46] codebase. 9 FreeTimeGS: Free Gaussians at Anytime and Anywhere for Dynamic Scene Reconstruction"
        },
        {
            "title": "Supplementary Material",
            "content": "A. More Experiments Results A.1. More Quantitative Comparison on Neural3DV In Table 6, we provide additional quantitative comparisons on the Neural3DV dataset including rendering speed and storage cost. In Table 7, we further demonstrate the trade-off between rendering quality and storage cost by controlling the number of primitives of our method. A.2. More Quantitative Comparison on SelfCap In Table 8, we provide additional quantitative comparisons on the SelfCap dataset including rendering speed and storage cost. For 4DGS, we provide results with and without 4DSH. We find that 4DSH introduces additional storage cost without improving rendering quality on the SelfCap dataset. Therefore, we do not use 4DSH in the SelfCap experiments presented in the main paper. We also add the results of Deformable-3DGS [14] on the SelfCap dataset. As stated in the main paper, the difficulty of building long-range correspondences makes it challenging to handle complex dynamic scenes, as shown in Table 8, Figure 8, Figure 9, and Figure 10. A.3. More Qualitative Results More qualitative comparisons on the SelfCap dataset are shown in Figure 8 and Figure 9. A.4. Per Scene Breakdown In Table 11, Table 9, and Table 10, we provide the per-scene breakdown of the quantitative results on the Neural3DV, ENeRF-Outdoor, and SelfCap datasets, respectively. B. Implementation Details B.1. Evaluation details on SelfCap We report quantitative results for both the entire image and only dynamic regions on the SelfCap dataset in the main paper and supplementary material. For entire image evaluation, we use the full image to calculate the metrics. For dynamic regions, we first use the ground truth background image and Background Matting V2 [24] to extract the mask of dynamic regions. Then we crop the full image using the bounding box of the dynamic mask, and fill the region outside the mask with black color. An example of the ground truth for dynamic region is shown in Figure 7. Table 6. Quantitative comparison on the Neural 3D Video [19] Dataset. We report PSNR, DSSIM1, DSSIM2, and LPIPSAlex to evaluate the rendering quality. 1: only includes the Flame Salmon scene. 2: excludes the Coffee Martini scene. : Our method with number of primitives controlled to no more than 500k. PSNR DSSIM1 DSSIM2 LPIPS Size (MB) Neural Volume1 [26] LLFF1 [29] DyNeRF1 [19] HexPlane2 [4] K-Planes [10] MixVoxels-L [42] MixVoxels-X [42] HyperReel [1] NeRFPlayer [39] Deformable-3DGS [44] C-D3DGS [13] Ex4DGS [17] 4DGS [49] STGS-Lite [21] STGS [21] Ours Ours 22.80 23.24 29.58 31.71 31.63 31.34 31.73 31.10 30.96 31.15 30.46 32.11 32.01 31.59 32.05 33.19 32.97 - - - - - - - 0.036 0.034 0.030 - 0.030 - 0.027 0.026 0.026 0.028 0.062 0.076 0.020 0.014 0.018 0.017 0.015 - - - 0.022 0.015 0.014 0.015 0.014 0.013 0.014 0.295 0.235 0.083 0.075 - 0.096 0.064 0.096 0.111 0.049 0.150 0.048 0.055 0.047 0.044 0.036 0.043 - - 28 200 311 500 500 360 5130 90 338 115 3128 103 200 125 41 Table 7. Ablation studies on the number of primitives on Neural3D Video [19] Dataset.We report PSNR, DSSIM1, DSSIM2, and LPIPSAlex to evaluate the rendering quality. Our method achieve high-quality results even when the storage cost is reduced to as low as 8.3MB. # Gaussians = 70k = 91k = 165k = 347k = 618k = 1060k = 2042k . PSNR DSSIM1 DSSIM2 LPIPS Size (MB) 32.39 32.54 32.66 32.97 32.94 33.19 32.94 0.031 0.030 0.029 0.028 0.027 0.026 0. 0.016 0.015 0.015 0.014 0.014 0.013 0.013 0.052 0.051 0.047 0.043 0.039 0.036 0.035 8.3 11 20 41 73 125 240 Table 8. Quantitative comparison on our SelfCap Dataset. We report PSNR, DSSIM2, and LPIPSV GG to evaluate the rendering quality, and include quantitative results for both the entire image and only dynamic regions (entire/dynamic). The FPS is measured on an NVIDIA RTX 4090 GPU. Green and yellow cell colors indicate the best and the second best results, respectively. : Our method with number of primitives controlled to no more than 500k. PSNR DSSIM2 LPIPS FPS Size (MB) Deformable-3DGS [44] STGS [21] 4DGS [49] Ours Ours 25.95/25.27 24.97/25.32 25.98/26.75 27.41/29.38 27.27/28.87 0.037/0.026 0.048/0.029 0.036/0.019 0.024/0.013 0.025/0. 0.298/0.139 0.273/0.123 0.237/0.104 0.204/0.080 0.217/0.081 57 142 65 467 664 73 77 827"
        },
        {
            "title": "References",
            "content": "[1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil 1 Table 9. Quantitative comparison of view synthesis results on ENeRF-Outdoor [23] Dataset. We report PSNR, SSIM2, and LPIPSV GG to evaluate the rendering quality. Green and yellow cell colors indicate the best and the second best results, respectively. actor1 4 actor5 6 actor2 3 Avg. actor1 4 actor5 6 actor2 3 Avg. actor1 4 actor5 6 actor2 3 Avg. PSNR SSIM2 ENeRF [23] 4K4D [45] 4DGS [49] STGS [21] Ours 25.65 25.69 24.78 25.25 25.56 24.53 25.02 24.77 25.04 25.46 24.71 25.13 24.91 24.49 25. 24.96 25.28 24.82 24.93 25.36 0.779 0.809 0.782 0.816 0.836 0.767 0.801 0.785 0.830 0.845 0.808 0.810 0.794 0.822 0.858 0.785 0.807 0.787 0.823 0.846 0.305 0.384 0.326 0.315 0. LPIPS 0.303 0.374 0.320 0.282 0.235 0.290 0.378 0.306 0.294 0.236 0.299 0.379 0.317 0.297 0.244 Davidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Taylor, et al. Fusion4d: Real-time performance capture of challenging scenes. ACM TOG, 2016. 1, 2 [9] Johan Edstedt, Qiyu Sun, Georg Bokman, Marten Wadenback, and Michael Felsberg. RoMa: Robust Dense IEEE Conference on Computer Vision Feature Matching. and Pattern Recognition, 2024. [10] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1247912488, 2023. 1, 2, 5, 4 [11] Zhiyang Guo, Wen gang Zhou, Li Li, Min Wang, and Houqiang Li. Motion-aware 3d gaussian splatting for efficient dynamic scene reconstruction. ArXiv, abs/2403.11447, 2024. 1, 3 [12] Anna Hilsmann, Philipp Fechteler, Wieland Morgenstern, Wolfgang Paier, Ingo Feldmann, Oliver Schreer, and Peter Eisert. Going beyond free viewpoint: creating animatable volumetric video of human performances. IET Computer Vision, pages 350358, 2020. 1 [13] Kai Katsumata, Duc Minh Vo, and Hideki Nakayama. compact dynamic 3d gaussian representation for real-time dynamic view synthesis. In European Conference on Computer Vision, 2023. 5, 1 [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (TOG), 42(4):114, 2023. 3, 4, 8, 1 [15] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. Neural human performer: Learning generalizable radiance fields for human performance rendering. Advances in Neural Information Processing Systems, 34, 2021. [16] Isaac Labe, Noam Issachar, Itai Lang, and Sagie Benaim. Dgd: Dynamic 3d gaussians distillation. ArXiv, abs/2405.19321, 2024. 1, 3 [17] Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic gaussian splatting. 2024. 5, 1, 4 [18] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv. Neural 3d video synthesis. CVPR, 2022. 2 Figure 7. Dynamic Region Mask. An illustration of the use of dynamic region masking during evaluation. (a) Ground Truth image from the SelfCap dataset. (b) Masked Ground Truth, generated by cropping the Ground Truth image using dynamic region mask extracted by Background Matting V2 [24], with the outer area filled in black. Hyperreel: High-fidelity 6-dof video with rayKim. In Proceedings of the IEEE/CVF conditioned sampling. Conference on Computer Vision and Pattern Recognition, pages 1661016620, 2023. 5, 1, [2] Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Per-gaussian embedding-based deformation for deformable 3d gaussian splatting. ArXiv, abs/2404.03613, 2024. 1, 3 [3] Aljaˇz Boˇziˇc, Michael Zollhofer, Christian Theobalt, and Matthias Nießner. Deepdeform: Learning non-rigid rgb-d reconstruction with semi-supervised data. 2020. 2 [4] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. arXiv, 2023. 2, 5, 1, 4 [5] Dan Casas, Marco Volino, John Collomosse, and Adrian Hilton. 4d video textures for interactive character appearance. In Computer Graphics Forum, pages 371380. Wiley Online Library, 2014. 1 [6] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798, 2022. 2 [7] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, and Steve Sullivan. High-quality streamable free-viewpoint video. ACM Transactions on Graphics (ToG), 34(4):113, 2015. [8] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip 2 Table 10. Quantitative comparison of view synthesis results on SelfCap Dataset. We report PSNR, SSIM2, and LPIPSV GG to evaluate the rendering quality, and include quantitative results for both the entire image and only dynamic regions. Green and yellow cell colors indicate the best and the second best results, respectively. dance 1 dance 2 dance Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours Deformable-3DGS [44] 4DGS-4DSH [49] 4DGS [49] STGS [21] Ours 26.82 26.49 26.40 25.26 27.66 0.940 0.937 0.938 0.933 0.953 0.259 0.246 0.244 0.237 0.234 25.30 27.10 27.26 26.95 29.74 0.925 0.945 0.945 0.949 0.963 0.232 0.188 0.187 0.177 0. 26.54 24.99 25.53 23.74 27.85 0.936 0.924 0.929 0.916 0.953 0.269 0.269 0.257 0.245 0.232 24.64 22.17 26.27 25.00 28.91 0.948 0.938 0.959 0.944 0.970 0.155 0.155 0.125 0.122 0. 25.69 26.26 26.35 25.69 27.00 0.943 0.937 0.937 0.939 0.955 0.254 0.221 0.219 0.222 0.213 26.18 27.47 27.54 27.05 29.24 0.962 0.968 0.966 0.966 0.975 0.138 0.103 0.100 0.097 0. dance 4 corgi 1 Entire PSNR 24.15 27.08 26.95 27.55 28.90 26.80 26.42 26.33 25.41 27.25 Entire SSIM2 0.940 0.936 0.937 0.936 0.954 0.900 0.920 0.919 0.933 0. Entire LPIPS 0.256 0.225 0.223 0.217 0.218 0.357 0.212 0.211 0.219 0.158 Dynamic PSNR 25.88 29.32 26.62 28.54 31.20 24.17 25.84 26.08 25.63 27.33 Dynamic SSIM2 0.936 0.955 0.957 0.956 0. 0.957 0.976 0.977 0.967 0.982 Dynamic LPIPS 0.182 0.124 0.121 0.116 0.099 0.113 0.064 0.062 0.084 0.045 corgi 2 bike bike 2 Avg. 28.04 27.56 27.50 28.52 29.96 0.902 0.924 0.924 0.934 0.959 0.365 0.210 0.212 0.235 0.160 24.72 27.52 27.72 26.81 32.62 0.950 0.972 0.973 0.966 0. 0.112 0.073 0.072 0.090 0.031 24.30 24.61 24.04 21.46 24.96 0.920 0.922 0.919 0.822 0.946 0.312 0.256 0.259 0.406 0.199 25.31 26.39 26.05 21.15 28.89 0.950 0.957 0.956 0.890 0. 0.102 0.090 0.090 0.152 0.067 25.25 24.42 24.77 22.09 25.67 0.924 0.912 0.916 0.827 0.937 0.310 0.275 0.269 0.402 0.221 25.98 25.58 26.45 21.41 27.07 0.960 0.962 0.962 0.896 0. 0.075 0.076 0.074 0.145 0.065 25.95 25.98 25.98 24.97 27.41 0.926 0.927 0.927 0.905 0.952 0.298 0.239 0.237 0.273 0.204 25.27 26.42 26.75 25.32 29.38 0.949 0.959 0.962 0.942 0. 0.139 0.109 0.104 0.123 0.080 [19] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55215531, 2022. 1, 5, 4 [20] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 2 [21] Zhan Li, Zhang Chen, Zhong Li, and Yinghao Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 85088520, 2023. 3 Table 11. Quantitative comparison of view synthesis results on Neural3D Video [19] Dataset. We report PSNR, DSSIM1, DSSIM2, and LPIPSAlex to evaluate the rendering quality. Green and yellow cell colors indicate the best and the second best results, respectively. : Our method with number of primitives controlled to no more than 500k. Coffee Martini Cook Spinach Cut Roasted Beef Flame Salmon Flame Steak Sear Steak Avg. PSNR Neural Volume [26] LLFF [29] DyNeRF [19] HexPlane [4] K-Planes [10] MixVoxels-L [42] MixVoxels-X [42] HyperReel [1] NeRFPlayer [39] Deformable-3DGS [44] Ex4DGS [17] 4DGS [49] STGS [21] STGS-Lite [21] Ours Ours HyperReel [1] NeRFPlayer [39] Deformable-3DGS [44] Ex4DGS [17] STGS [21] STGS-Lite [21] Ours Ours Neural Volume LLFF DyNeRF K-Planes MixVoxels-L MixVoxels-X Ex4DGS STGS STGS-Lite Ours Ours Neural Volume LLFF DyNeRF HexPlane MixVoxels-L MixVoxels-X HyperReel NeRFPlayer Ex4DGS STGS STGS-Lite Ours Ours - - - - 29.99 29.63 30.39 28.37 31.53 27.34 28.79 28.33 28.61 27.49 30.63 30.31 0.0540 0.0245 0.0475 0.0425 0.0415 0.0437 0.0369 0.0395 - - - 0.0235 0.0244 0.0232 0.0245 0.0250 0.0270 0.0198 0. - - - - 0.106 0.081 0.127 0.085 0.070 0.069 0.075 0.052 0.065 - - - 32.04 32.60 32.25 32.31 32.30 30.56 32.46 33.23 32.93 33.18 32.92 33.80 33.52 0.0295 0.0355 0.0255 0.0265 0.0215 0.0218 0.0233 0.0242 - - - 0.0170 0.0162 0.0160 0.0120 0.0113 0.0118 0.0112 0.0117 - - - 0.082 0.099 0.062 0.089 0.113 0.042 0.037 0.038 0.031 0.036 22.80 23.24 29.58 29.47 30.44 29.81 30.60 28.26 31.65 29.20 29.29 29.38 29.48 28.67 31.18 30. 0.0590 0.0300 0.0415 0.0415 0.0375 0.0387 0.0347 0.0369 0.0620 0.0760 0.0200 0.0235 0.0255 0.0233 0.0220 0.0224 0.0244 0.0186 0.0197 0.295 0.235 0.083 0.078 0.116 0.078 0.136 0.098 0.066 0.063 0.068 0.050 0.060 DSSIM1 DSSIM2 LPIPS - - - 32.55 31.82 32.40 32.63 32.92 29.35 32.90 33.73 33.85 33.52 33.72 34.52 34.13 0.0275 0.0460 0.0215 0.0260 0.0205 0.0209 0.0220 0.0238 - - - 0.0170 0.0157 0.0146 0.0115 0.0105 0.0112 0.0101 0.0110 - - - 0.080 0.088 0.057 0.084 0.144 0.040 0.036 0.038 0.030 0.037 4 - - - 32.08 32.38 31.83 32.10 32.20 31.93 32.51 33.91 34.03 33.64 33.28 34.98 34. 0.0255 0.0250 0.0230 0.0220 0.0176 0.0179 0.0194 0.0208 - - - 0.0150 0.0144 0.0137 0.0100 0.0087 0.0097 0.0087 0.0095 - - - 0.066 0.088 0.051 0.078 0.088 0.034 0.029 0.031 0.026 0.030 - - - 32.39 32.52 32.10 32.33 32.57 29.13 32.49 33.69 33.51 33.89 33.47 34.06 34.56 0.0240 0.0460 0.0215 0.0205 0.0174 0.0177 0.0198 0.0199 - - - 0.0130 0.0122 0.0121 0.0105 0.0085 0.0095 0.0091 0. - - - 0.070 0.080 0.053 0.077 0.138 0.035 0.030 0.031 0.026 0.029 22.80 23.24 29.58 31.71 31.63 31.34 31.73 31.10 30.69 31.15 32.11 32.01 32.05 31.59 33.19 32.97 0.037 0.035 0.030 0.030 0.026 0.027 0.026 0.028 0.062 0.076 0.020 0.018 0.018 0.017 0.015 0.014 0.016 0.013 0.014 0.295 0.235 0.083 0.075 0.096 0.064 0.099 0.111 0.048 0.044 0.047 0.036 0.043 Figure 8. Qualitative comparison on our SelfCap Dataset. 5 Figure 9. Qualitative comparison on our SelfCap Dataset. 6 Figure 10. Qualitative comparison on the Neural 3D Video Dataset. 1, 3, 5, 2, 4 [22] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis. ArXiv, abs/2312.11458, 2023. 1, [23] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance In SIGGRAPH fields for interactive free-viewpoint video. Asia Conference Proceedings, 2022. 2, 5 [24] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steven M. Seitz, and Ira KemelmacherShlizerman. Real-time high-resolution background matting. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 87588767, 2020. 1, 2 [25] Qingming Liu, Yuan Liu, Jie-Chao Wang, Xianqiang Lyv, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos. 2024. 1, 3 [26] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM Trans. Graph., 38(4):65:165:14, 2019. 5, 1, 4 [27] Zhicheng Lu, Xiang Guo, Le Hui, Tianrui Chen, Min Yang, Xiao Tang, Feng Zhu, and Yuchao Dai. 3d geometry-aware deformable gaussian splatting for dynamic view synthesis. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 89008910, 2024. 1, 3 [28] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking arXiv preprint Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. Dynamic 3d gaussians: [29] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. 5, 1, 4 [30] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 1, 2 7 [45] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2002920040, 2023. 5, 2 [46] Zhen Xu, Tao Xie, Sida Peng, Haotong Lin, Qing Shuai, Zhiyuan Yu, Guangzhao He, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Easyvolcap: Accelerating neural volumetric video research. 2023. 9 [47] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. ACM Transactions on Graphics, 43(6), 2024. [48] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. 1, 2, 3 [49] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv 2310.10642, 2023. 1, 3, 5, 8, 2, 4 [50] Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo Li, Qionghai Dai, and Yebin Liu. Bodyfusion: Real-time capture of human motion and surface geometry using single depth camera. In The IEEE International Conference on Computer Vision (ICCV). IEEE, 2017. 2 [51] Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li, Gerard Pons-Moll, and Yebin Liu. Doublefusion: Real-time capture of human performances with inner body shapes from single depth sensor. In CVPR, 2018. 1, 2 [52] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 4, 5 [53] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance arXiv preprint for deformable 3d gaussian splatting. arXiv:2410.07707, 2024. 1, 3 [31] Richard Newcombe, Dieter Fox, and Steven Seitz. Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time. In CVPR, 2015. 1, [32] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip Davidson, Sameh Khamis, Mingsong Dou, et al. Holoportation: Virtual 3d teleportation in real-time. In UIST, 2016. 1, 2 [33] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. 2 [34] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinHypernerf: higherBrualla, and Steven Seitz. dimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021. 2 [35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes In Proceedfor novel view synthesis of dynamic humans. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90549063, 2021. 2 [36] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In ICCV, 2021. 2 [37] Richard Shaw, Jifei Song, Arthur Moreau, Michal Nazarczuk, Sibi Catley-Chandar, Helisa Dhamo, and Eduardo Perez-Pellitero. Swings: Sliding windows for dynamic 3d gaussian splatting. 2023. 1, 3, 5 [38] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In SIGGRAPH Conference Proceedings, 2022. 2 [39] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer: streamable dynamic scene representation with decomposed neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 29(5):27322742, 2023. 5, 1, 4 [40] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas Muller, Morgan McGuire, Alec Jacobson, and Sanja Fidler. Variable bitrate neural fields. ACM SIGGRAPH 2022 Conference Proceedings, 2022. 2 [41] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng. Compressible-composable nerf via rank-residual decomposition. arXiv preprint arXiv:2205.14870, 2022. 2 [42] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, and Huaping Liu. Mixed neural voxels for fast multi-view video synthesis. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1964919659, 2022. 5, 1, 4 [43] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004. 4, 5 [44] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 1, 2, 3, 5,"
        }
    ],
    "affiliations": [
        "Geely Automobile Research Institute",
        "Zhejiang University"
    ]
}