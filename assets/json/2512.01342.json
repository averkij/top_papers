{
    "paper_title": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision",
    "authors": [
        "Chenting Wang",
        "Yuhan Zhu",
        "Yicheng Xu",
        "Jiange Yang",
        "Ziang Yan",
        "Yali Wang",
        "Yi Wang",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale video-text pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physical cues. In contrast, masked video modeling (MVM) directly exploits spatiotemporal structures but trails text-supervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoder-decoder design into an Encoder-Predictor-Decoder (EPD) framework, where the predictor acts as a latent world model, and propose InternVideo-Next, a two-stage pretraining scheme that builds a semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictor output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes a conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-of-the-art results across benchmarks and provides a scalable path toward general video representation learning."
        },
        {
            "title": "Start",
            "content": "InternVideo-Next: Towards General Video Foundation Models without Video-Text Supervision Chenting Wang1,2 Yuhan Zhu2,5 Yicheng Xu2 Jiange Yang2,5 Ziang Yan2 Yali Wang2,4 Yi Wang2,3 Limin Wang1,2,5 2Shanghai AI Laboratory 1Shanghai Jiao Tong University 3Shanghai Innovation Institute 4Shenzhen Institutes of Advanced Technology, China 5State Key Laboratory for Novel Software Technology, Nanjing University 5 2 0 2 1 ] . [ 1 2 4 3 1 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large-scale videotext pretraining achieves strong performance but depends on noisy, synthetic captions with limited semantic coverage, often overlooking implicit world knowledge such as object motion, 3D geometry, and physIn contrast, masked video modeling (MVM) ical cues. directly exploits spatiotemporal structures but trails textsupervised methods on general tasks. We find this gap arises from overlooked architectural issues: pixel-level reconstruction struggles with convergence and its low-level requirement often conflicts with semantics, while latent prediction often encourages shortcut learning. To address these, we disentangle the traditional encoderdecoder design into an EncoderPredictorDecoder (EPD) framework, where the predictor acts as latent world model, and propose InternVideo-Next, two-stage pretraining scheme that builds semantically consistent yet detail-preserving latent space for this world model. First, conventional linear decoder in pixel MVM enforces the predictors output latent to be linearly projected to, thus separable in pixel space, causing the conflict with semantic abstraction. Our Stage 1 proposes conditional diffusion decoder and injects reliable image-level semantic priors to enhance semantics and convergence, thus bridging pixel-level fidelity with high-level semantic abstraction. Stage 2 further learns world knowledge by predicting frozen Stage 1 targets within this space, mitigating shortcut learning. Trained on public, unlabeled videos, InternVideo-Next achieves state-ofthe-art results across benchmarks and provides scalable path toward general video representation learning. 1. Introduction Video, as one of the most dynamic and information-rich modalities, offers window into the physical world. It emCorresponding author. Figure 1. Comparison with previous SOTA methods of size ViT-Large. With only public sources, our model excels at general video benchmarks within probing setting where ViTs are frozen to directly show representations quality. The benchmarks involve scene-related, motion-related, complex video-language related and implicit world knowledge (3D geometric prior, causal relations and fine-grained object motion) related tasks. beds not only spatiotemporal dynamics [20, 36, 58, 64] but also causal relationships, 3D geometrical priors and physical cues [2, 10]. These are all essential components for building genuine video understanding models. Such models are crucial for advancing embodied AI [23, 71], procedural reasoning [83], and the next generation of multimodal large language models [31, 32, 35, 38, 39]. However, learning such knowledge from raw data, especially in scalable and unbiased manner, remains challenge. Recent progress in large-scale video representation learning often falls into two categories. 1) Text-supervised methods, leveraging large-scale video-text alignment [33, 42, 70, 75, 82], have achieved impressive performance on semantic, human-centric tasks like action recognition. However, due to expensive annotation costs, currently their reliance on noisy, synthetic annotations with limited seman1 Figure 2. Our two-stage self-supervised video pretraining framework. For general video understanding and building real worldunderstanding video foundation models, we propose InternVideo-Next, which is simple, scalable, efficient and reproducible. tic coverage introduces external biases, making it difficult to capture non-semantic vision features. They often fall short in tasks that require detailed object motion, causal structure or 3D geometrical priors. 2) Self-supervised video pretraining [2, 5, 57], in contrast, holds the promise of learning directly from the spatiotemporal structure of videos, offering path towards truly unbiased and scalable world modeling. However, existing methods still lag behind textsupervised counterparts on benchmarks requiring strong semantics about main subject like Kinetics-400 [28]. We contend that this performance gap of self-supervised video pretraining is not an intrinsic limitation, but rather stems from fundamental architectural challenges unsolved in current methods: 1) Bridge pixel-level fidelity with high-level semantic abstraction 2) Learn robust spatiotemporal dynamics, causal relations and 3D geometric priors from prediction without shortcut solutions. Popular MVM methods, like MAE [25], take conventional Encoder-Decoder paradigm, where ViT-Decoder [19] directly takes Encoder output and generates reconstructed pixels. We explicitly decouple it into Predictor and Decoder, and consider the whole MVM paradigm as new unified EncoderPredictorDecoder (EPD) overview. Such disentanglement enables clearer inspection of the Predictors output latent space that is often neglected, which we find is essential in dealing with the unsolved challenges. It also reveals key insight overlooked by previous works: the Encoder and Predictor should share semantically rich yet detail-faithful output latent space, making the predictor possible Latent World Model. Enforcing this compels the Predictor to complete missing content using genuine spatiotemporal relationships and implicit world knowledge rather than trivial correlations, further boosting semantic abstraction with implicit world knowledge like geometry and motion in the Encoders representation. In light of this, we propose InternVideo-Next, novel two-stage video pretraining framework, as Figure 2: In Stage 1, we construct such semantically grounded, detail-preserving and structurally consistent latent space by: 1) leveraging pixel reconstruction framework as foundation to learn fine-grained pixel details and spatiotemporal priors. 2) using condition-based diffusion generative decoder in place of the popular Linear Decoder head in turning Predictor output latent to pixels. Such diffusion decoder is helpful to maintain high semantics in the Predictor output space as common linear decoder may require the output to be linearly separable to pixels, which is harmful to balance semantic information with fine-grained details. 3) integrating semantic priors from pretrained Image semantic model. Unlike videotext pretraining, which is limited by the sparsity and noisiness of video captions, imagetext pretraining benefits from massive, high-quality corpora whose captions more comprehensively describe visual content. Such priors are cleaner and can accelerate models convergence by focusing on more temporal-centric information, meanwhile boosting multi-modal friendliness. Building upon this, Stage 2 then focuses on learning spatiotemporal dynamics and causal relations within this already coherent latent space. We employ latent-space prediction objective, where student model learns to predict the representations generated by frozen teacher, where the two models are both initialized with weights learned in Stage 1. Crucially, the semantically rich and detailpreserving latent space established in Stage 1 prevents the 2 shortcut solutions that plague traditional latent-prediction methods, pushing the model to learn genuine predictive world knowledge rather than superficial temporal statistics. As shown in Figure 1, InternVideo-Next achieves stateof-the-art results across wide range of video understanding tasks. Remarkably, it is the first video model trained without explicit videotext supervision that surpasses videotext pretrained counterparts on both Kinetics-400 (action recognition) and SSv2 (fine-grained motion recognition). The learned representations also exhibit strong generalization to tasks requiring physical and 3D spatial intelligence, such as depth estimation and object-centric tracking, which are implicit world knowledge inherent in videos. In addition, with lightweight LiT-style [78] fine-tuning, InternVideo-Next delivers competitive zero-shot videotext retrieval performance, highlighting our model representations multimodal friendliness. Also, with preliminary exploration in chat-centric tasks in video, it holds the potential to be the foundation Encoder of next-generation multimodal video chat models. Our framework provides unified and scalable pathway toward general-purpose worldunderstanding video models. All the code and model will be released upon publication. 2. Related Works Text Supervised Pre-training. Clip-style pretraining has dominated current mainstream image pretraining frameworks [50, 59, 79], benefiting from large-scale web-crawled image-text pair data. Videotext pretraining methods [13, 15, 34, 45, 75] also achieve strong performance on benchmarks relying heavily on the main subject semantics like Kinetics-400. However, video captions are more difficult to be directly crawled from internet and are often generated by summarizing title and ASR information. Such synthetic annotations are typically noisier than images and lack sufficient motion or spatial diversity, considering that videos are harder to fully describe than images. As can be seen with Figure1, although video-text models excel at semanticheavy tasks, they often fall short in tasks related to nonsemantic information like depth, fine-grained motion and causal relation. Our framework focuses more on unbiased learning from video itself, retaining high semantics, lowlevel details and world knowledge priors. Masked Video Modeling. Masked video modeling (MVM) is the mainstream self-supervised video pretraining method, inspired by masked image modeling [4, 25, 49, 67]. VideoMAE [57] and MaskFeat [73] reconstruct masked video patches in the pixel domain, achieving strong performance but primarily capturing low-level appearance cues. Subsequent works such as VideoMAE-V2 [65] improve masking strategies and decoder designs but still struggle to preserve high-level semantics. Latent-space prediction approaches, such as V-JEPA [5], move toward semantic abstraction by predicting feature representations instead of pixels, yet symmetric teacherstudent structures often lead to shortcut learning or semantic drift. As seen in Figure 1, it falls short in appearance-intensive tasks, depth estimation tasks that require low-level details, and high-semantic tasks, meaning that such shortcut learning leads to missing details and prevents higher-level information. Our approach unifies these paradigms under an EncoderPredictorDecoder (EPD) formulation and emphasizes maintaining semantic coherence between encoder and predictor latent spaces, combining the strengths of both. The InternVideo Series. The InternVideo series [62, 68, 70] has explored multiple ways in integrating unsupervised video learning with text alignment to bridge pixel-level fiInternVideo delity with high-level semantic abstraction. merges weights from pretrained VideoMAE and CLIP models using model ensemble, while InternVideo2 introduces video-only stage that aligns unmasked representations from VideoMAE and CLIP encoders. These methods can be viewed as weightor embedding-level fusion of video priors and language knowledge. However, as will be verified in the experiment section, they still can not fully solve the conflict between fine-grained details and high-level semantics. InternVideo-Next revisits video-only pretraining from task-level perspective by integrating CLIP-level semantic priors into an augmented video reconstruction framework, resolving such conflict. 3. Method 3.1. The EPD Disentanglement We formulate our unified EncoderPredictorDecoder (EPD) overview as follows: E: Vision Transformer that extracts spatiotemporal representations from an input video. : lightweight transformer that predicts latent representations for masked regions based on visible tokens. D: reconstruction module that maps the predictors output latent to target space, either pixels or target latent. 3.2. S1: Semantic-Guided Pixel Reconstruction Semantic Alignment Loss. Our approach leverages semantic knowledge from the image domain via frozen SigLIP encoder. This allows us to inject strong imagelevel semantic prior into our video-only pre-training, offloading the need for noisy video-text annotations while focusing the models learning on spatiotemporal dynamics: Lsem = cos(cid:0)E(Xvis), vis(SigLIP(X))(cid:1), (1) We use cosine similarity loss between the students embedding of masked video and the corresponding visible 3 (a) Stage-1 module effect (b) Predictor size & init (c) Different Decoder settings Variant K400 SSv Predictor K400 SSv2 Decoder K400 SSv2 Pixel Recon. Baseline SigLIP Align only Pixel Rec. + Align + Diffusion Decoder + Text-Decoder Init + Keep Both 47.2 70.7 69.8 74.2 69.4 75.8 28.1 32.1 31.8 35.4 31.3 36.9 ModernBert-L last2 ModernBert-L last5 last5 w/o init ModernBert-L last8 Depth-5 ViT Depth-12 ViT 73.2 75.8 74.2 75.6 74.1 73.2 34.8 36.9 35.4 37.0 34.9 34.4 Linear Head 3-Layer MLP Head DiffMLP D3 W1024 DiffMLP D6 W1536 DiffMLP D9 69.4 69.7 73.4 75.8 75.5 31.3 31.2 33.2 36.9 36.4 Table 1. Ablations of Stage 1 design. Linear probing results are shown. (a) Adding semantic alignment boosts overall performance, while directly merging it into pixel reconstruction framework causes performance drop. Combining our diffusion decoder and text decoder augmentation make them fuse perfectly and results in accuracy gain. (b) Moderate predictor depth with initialization performs best in our scheme, outperforming popular results with Depth-12 ViT. (c) for number of MLP layers. and for network width. region of the teachers embedding of the full video. Stage 1 jointly optimizes pixel reconstruction and semantic alignment with the same loss weight. Semantic-Aware Masking. Semantic mask prioritizes temporally informative regions using attention scores derived from the semantic teacher with top-k selection. the popular Diffusion Decoder. Unlike linear decoder [25], we adopt lightweight conditional diffusion [27] decoder to model the distribution of each patch independently. The diffusion process and loss function follow [46], where noise schedule has cosine shape, with 1000 steps at training time. We use small MLP consisting of few residual blocks [24] for denoising. The denoising MLP is conditioned on vector produced by the Predictor and outputs corresponding pixels. Since it only models patchs latent distribution, small MLP works perfectly and does not involve much overhead. Text-Decoder Initialization. Conventional pixel reconstruction framework uses zero-init ViT. Our Predictor is initialized with weights from pretrained text decoder [42, 72], which provides better semantic priors and the smooth translation of high semantics between the two latent spaces, and proved to demand fewer layers than common methods. 3.3. S2: Semantically Coherent Latent Prediction Stage 2 builds upon the semantically aligned latent spaces obtained in Stage 1 to further enhance temporal abstraction and representation generalization. Initialization. Both the student and teacher networks are initialized with the weights learned in Stage 1. And the Stage 1 predictor is also kept in Stage 2. Multi-Block Masking. To strengthen temporal reasoning, we apply multi-block masking [5], where large contiguous spatiotemporal blocks in video are masked. This strategy increases prediction difficulty and mitigates information leakage, which is better for Stage 2s target of implicit world knowledge by prediction. Latent Prediction Objective. The student predicts the teachers latent representations for masked regions. This objective enforces consistency in the latent space without direct pixel reconstruction, enabling the model to focus on abstract semantic and temporal patterns. Frozen Teacher Target. Unlike V-JEPA, the teacher network is frozen with Stage 1 initialization to prevent shortcut learning, as the Stage 1 initialization is already detailpreserving and of high semantics. 4. Experiments Implementation. For our pretraining, we utilize the last five layers of ModernBert-Large [72] as our Predictor. We use SigLIP2-1B [59] as our semantic teacher in our final version, and use SigLIP2-Large in the Ablation study. For Stage 1 pre-training, we use mask ratio of 80% and learning rate of 1e-3. For ablation study, we use 32 A100 with batch size of 1024 for 30 epochs in both Stage 1 and Stage 2. And for final training, we use 64 A100 with batch size of 2048 for 50 epochs in Stage 1 and 100 epochs in Stage 2. For more, please refer to the Supplement. Datasets. Unless otherwise stated, we use K-Mash [70] for training, including 1.1M video data from K400 [28], K600 [8], K700 [9], SSv2 [22], ANet [26], HACS [81], and MiT [44] with duplicates removed. For ablation, we use subset K710 containing only the Kinetics datasets. 4 SSv2 K400 Acc1 Acc1 Scannet δ1 Stage Mask Type #F SSv2 Acc1 Acc1 Scannet δ1 Stage1-Variant DinoV2 Align only Clip-ViT Align only SigLIP Align only InternVideo2 Stage1 Strategy SigLIP+VideoMAE align Ours w/ DinoV2 align Ours w/ Clip-ViT align Ours w/ SigLIP align 68.4 69.1 70.7 70.4 69.3 73.2 75.8 29.5 31.2 32.1 32.5 31.5 34.6 36. 43.3 40.6 42.1 50.1 58.3 57.8 59.4 Table 2. Comparison with different supervision signals. Also, we compare results with InternVideo2-S1, which directly aligns with two teachers as first attempt to balance pixel-level detail and semantic abstraction, in fair training recipe. Stage-2 Variant Stage Our Stage 2 setting replace w/ Zero-Init V-JEPA Predictor replace w/ Momentum 0.9998 Target replace w/ Frozen SigLIP2 Target replace w/ Frozen InternVideo2 Target if w/ Unmasked Token Alignment Loss if w/ Pixel Reconstruction Loss K400 SSv2 75.8 76.9 74.8 74.1 75.4 74.3 75.7 76.8 36. 56.9 53.8 54.3 45.7 47.4 51.1 57.0 Table 3. Stage-2 ablation. Our full configuration yields the strongest temporal abstraction and overall accuracy. 4.1. Ablation Study We conduct comprehensive ablations to analyze performance of our training framework on appearance-based, motion-centric and depth-estimation benchmarks, including K400 [28], SSv2 [22], and ScanNet [17]. We leverage Linear Probing results on action recognition tasks for clear presentation of representation quality improvement. Results on ScanNet are based on probing results using learnable queries and single-layer trainable cross-attention head. Effects of Stage 1 Settings. Table 1 presents detailed ablations on the Stage 1 design. As shown in (a), the pixel reconstruction baseline achieves 47.2% on K400, confirming its limited semantic abstraction ability. Using SigLIP alignment alone without reconstruction boosts recognition accuracy. However, naively combining pixel reconstruction and alignment hurts overall performance due to optimizaIntroducing the diffusion decoder reverses tion conflicts. such degradation into +4.4% performance gain, demonstrating strong complementarity between the two supervision signals, if not hurt by the original Linear Decoder. It Effect of Masking Strategy (ViT-B) Stage-1 Stage-2 Semantic Multi-block Semantic Multi-block 8 8 8 75.8 74.4 75.2 76.9 Effect of Temporal Frame Count (ViT-B) Stage-1 Semantic Stage-2 Multi-block 8 16 32 8 16 32 75.8 76.4 76.6 77.0 77.4 78.1 36.9 36. 52.3 56.9 36.9 38.4 38.6 57.5 58.4 59.4 59.4 58.1 61.1 66.1 59.4 59.8 59. 67.1 68.7 70.1 Table 4. Impact of masking and frame settings. Each Stage-2 model is based on the highlighted Stage-1 model in the same table. highlights the effect of our EPD disentanglement, which provides novel reconsideration and inspires us to re-use the now seldom-utilized pixel reconstruction framework. (b) studies the impact of predictor architecture and initialization. The vanilla ViT predictor without initialization, as used in the original setting, underperforms our solution. Panel (c) compares different decoder designs. lightweight linear head underfits complex spatial relations, while our diffusion-based MLP decoders progressively improve as capacity grows. The medium-sized Depth 6, Width 1536 variant achieves the best trade-off between representation richness and computational cost. Different supervision for Stage 1s semantic prior. Table 2 reveals that our framework shows generalizable performance gain with different semantic alignment targets, whether on common recognition tasks or 3D understanding tasks with low-level details. Supervision from semantic image models performs best. It also compares our unified Stage 1 strategy with the multi-teacher alignment scheme in InternVideo2, which aligns with both CLIP and VideoMAE to balance pixel-level detail and semantic abstraction. Effects of Stage 2 Settings. Stage 2 further boosts temporal abstraction and causal relations via masked latent prediction. As shown in Table 3, our configuration achieves the strongest results. Replacing our predictor with zeroinitialized V-JEPA variant, unfreezing the target encoder or using other initialization of the target encoder all lead to degradation, confirming the importance of semantically coherent initialization and stable target that encodes both ViT Size Data GPU-hrs K400 SSv2 Coin Model Name Image Models SigLIP2 [59] DinoV2 [7] Giant Giant 10B 142M - - Models Trained with Video-Text Pairs InternVideo2s2 [70] InternVideo2s2 [70] InternVideo2s2 [70] InternVideo2s2 [70] VideoPrism [82] VideoPrism [82] Base Large 1B 6B Base 1B 25.5M 25.5M 25.5M 400M 618M 618M Models Trained with Video Data Only Large 1B Large Large 1B 6B VideoMAEv2 [65] VideoMAEv2 [65] V-JEPAv1 [5] V-JEPAv2 [2] InternVideo2s1 [70] InternVideo2s1 [70] InternVideo-Nexts1 Base InternVideo-Nexts2 Base InternVideo-Nexts1 Large InternVideo-Nexts2 Large 1.35M 1.35M 2M 22M 1.1M 2.1M 1.1M 1.1M 1.1M 1.1M - - 30K 200K - 250K - 15K 8K 10K 15K 110K 1.5K 3.4K 3.6K 9.7K 85.6 83. 84.9 86.0 87.9 88.8 84.2 87.2 80.9 82.8 80.8 83.3 85.6 86.0 84.8 85.9 87.1 88.4 47.9 50.0 64.7 65.9 67.3 67.7 63.6 68.5 54.9 56.1 69.5 72.0 57.4 59.0 58.6 70.1 65.5 73.0 91.8 87. 88.7 90.1 91.7 92.6 - - 83.2 84.6 83.0 85.9 89.6 90.3 89.9 91.4 92.0 93.6 Table 5. Attentive Probing Results on Video Action Recognition Datasets K400, SSv2 and COIN. We report top@1 results with single probing head trainable. The training cost is estimated with equivalent A100 GPU Hours. means estimated result, as not included in the paper. Models are tested with 16 frames, and with 1 clips 3 crops. Our models achieve the best performance across model sizes and with the smallest, public and annotation-free training data, highlighting our methods efficiency. high semantics and pixel-level details. Adding unmasked token alignment as in Stage 1 introduces noise to latent targets and harms motion modeling. Pixel-space reconstruction during this stage provides marginal benefit to SSv2, as the targets, produced by our Stage 1 encoder, already contain enough pixel-level details. Masking Strategy and Temporal Window. Table 4 analyzes masking and temporal length configurations. Semantic masking in Stage 1 helps the model attend to spatially meaningful regions and bridge high-level semantic abstraction with pixel-level fidelity. Multi-block masking in Stage 2 makes the prediction task harder and forces robust spatiotemporal dynamics and causal relations learned in the Predictor. Moreover, increasing the number of input frames consistently enhances performance, demonstrating that richer temporal diversity strengthens temporal abstraction. We finally chose F16 for Stage 1 and F32 for Stage 2 for balance between accuracy and efficiency. 4.2. Single-modality Tasks We evaluate our method on several single-modality tasks: including the Kinetics-400 [28], Something-Something V2 [22] for action recognition, COIN [56] for long video classification, ScanNet [17] and KITTI [6] for monocular depth estimation, WayMo[54] for object tracking and EK100[18] for action prediction. Video Classification. Table 5 reports probing results with different frozen backbones on general video classification tasks. We test the model in an Attentive Probing setting where the encoders are frozen and single-layer attention pooling head is trained. Such Frozen Encoder settings can test representations quality in an unbiased way. Our methods achieve the best results with only public data and less computation cost on these foundation tasks. Depth Estimation. As discussed above, 3D geometric prior is also part of the world knowledge that can be acquired from videos. Table 6 tests the Encoders probing performance on 3D-related Monocular Video Depth Estimation tasks Scannet [17] and KITTI[6] in two settings. One uses single-layer cross-attention based Probing Head with learnable queries, as to directly show models spatiotemporal embedding quality regarding Video Depth. Our InternVideo-Next achieves far better results than image models as it learns more direct 3D spatial information in the backbone. And another uses complex VDA head designed specifically for Image models with additional temporal fusion modules [14]. Our InternVideo-Next excels at both settings, even achieving nearly SoTA performance in 6 Figure 3. Video Depth estimation visualization. All models are trained with frozen setting with only probing head (VDA head from Video Depth Anything [14]) module trainable. Graphs are from the first video of the ScanNet Dataset. Our model shows potential for building next-generation 3D-spatial intelligence models from video sources. Videos are resized with spatial resolution of 224. Model Name Scannet KITTI ARel δ1 ARel δ1 8.7 92. 94.6 74.9 81.8 19.3 66.9 11.9 23.4 57.0 12.9 20.9 63.1 10.9 9.9 13.7 81.5 SOTA solution with VDA head and specific training VideoDepthAnything [14] 8.3 Image Models w/ simple probing head [10] 26.4 50.4 16.8 SigLip2-L [59] 24.0 59.1 13.5 DinoV3-L [53] Video Models w/ simple probing head [10] VideoMAEv2-L[65] 84.6 83.0 InternVideo2-L [70] 86.8 V-JEPA2-L [2] InternVideo-Next-L 88.6 Image Models w/ VDA head with temporal fusion [14] 84.2 SigLip2-L [59] 93.0 DinoV3-L [53] Video Models w/ VDA head with temporal fusion [14] VideoMAEv2-B [65] 89.7 91.3 InternVideo2-B [70] 93.2 InternVideo-Next-B 89.7 VideoMAEv2-L [65] InternVideo2-L [70] 92.3 91.2 V-JEPA2-L [2] 94.6 InternVideo-Next-L 21.8 60.6 15.7 76.2 11.3 87.3 17.9 70.4 15.1 77.8 14.4 79.6 92.2 9.2 19.9 65.6 12.2 7.0 91.2 9. 9.6 8.8 7.5 9.5 8.1 8.7 6.7 Model Name Image Models SigLip2-L [59] DinoV3-L [53] Video Models InternVideo2-L [70] V-JEPA2-L [2] InternVideo-Next-L Waymo Open mean IOU 52.3 59.7 63.0 68.9 72. Table 7. Probing Results on Video Object Tracking Task Waymo Open. We follow the task setting in Scaling4D [10] to test representations capability in capturing object-level motion. Object Tracking. We follow the setting in Scaling 4D representations [10] to test model embeddings quality in capturing object-level motion with Object Tracking task on Waymo Open [54]. The task requires the model to predict Bounding Box given the coordinates of an object in the first frame. We use learnable queries and single-layer crossattention pooling head with the frozen ViT backbone for testing. We will leave more details in the Supp. Our model achieves the best result in capturing object-level motion. Table 6. Probing Results on Video Depth-Estimation Datasets Scannet and KITTI. Our InternVideo3 model with such simple probing setting can achieve comparable results with the carefully designed and trained Video Depth Anything model. ARel for absolute relative error and δ1 for relative accuracy [14]. such Probing Setting compared with the specifically designed and trained Video Depth Anything [14] model. Detailed test settings are left in the Supplement. Video Prediction. Table 8 shows video prediction results on Epic-Kitchens-100 [18]. The model predicts the future action given contextual video clip. The outputs of the predictor and encoder are concatenated along the token dimension and fed to an attentive probe layer and three classification heads. We follow most of the settings in V-JEPA2 [2] and report the mean-class recall-at-5 result. Our model performs better at predicting future actions. Verb Noun Action Method MLLM MCQ result VideoLLaMa-7B [80] 52.9 52.0 Frozen Backbone Evaluation 57.8 53.8 V-JEPA2-L [2] InternVideo-Next-L 58.9 56.4 26.0 32.7 34.0 Table 8. Results of Video Action Prediction task EK100. We report mean-class recall-at-5 for verb, noun and action on the validation set of EK100, following the test pipeline in V-JEPA2. K400 K700 SSv2-MC MiTv Method InternVideo2s2-B [70] 67.7 57.9 InternVideo-Next-B 68.9 59.0 V-JEPA2-L [2] 60.2 51.4 InternVideo2s2-L [70] 70.7 61.9 InternVideo-Next-L 72.1 63.0 55.9 61.2 53.0 59.6 64.2 27.9 29.0 25.4 30.6 32.0 Table 9. Results of Zero-shot Action Recognition on K400, K700, SSv2-MC and MiT. Both models use MobileCLIP-B text encoder and train with ViT frozen to test their representations separability, alignment and completeness in text space. 4.3. Multi-modality Tasks In this section, we evaluate the potential of our model in multi-modal learning scenarios through two representative experimental setups. First, following the LiT [78] methodology, we freeze the ViT backbone and train only text encoder, initialized with MobileCLIP-B [60], on 25.5M [3, 11, 30, 37, 43, 47, 52] publicly available videotext pairs from UMT [33] for 5 epochs. This setup evaluates the models zero-shot video-text retrieval performance, serving as direct probe into the completeness and correctness of the visual embeddings in the semantic space. We choose InternVideo2s2 and V-JEPA2 as our baseline model. Second, to assess the models applicability as video foundation model for high-level dialogue tasks, we connect the frozen ViT with frozen large language model, Qwen27B [66], via trainable MLP connector. Only the MLP is fine-tuned. This is consistent with the common Stage 1 training protocol for video-based chat models to validate the compatibility and transferability of our visual representations in generative video-LLM framework. Zero-shot Action Recognition. Table 9 shows results on zero-shot action recognition tasks including K400 [28], K700 [9], SSv2 [22] and MiTv1 [44]. Our model outperforms InternVideo2s2, which is the previous SoTA encoder for multimodal retrieval tasks, especially on motionintensive task SSv2-Multiple Choice. Zero-shot Retrieval. Table 10 shows results on zeroshot text-to-video retrieval tasks, including MSRVTT [76], [29], LSMDC [51] and DiDeMo [1], ActivityNet MSR DDM ANet LSMDC MSVD Method Popular methods with full video-text training - VINDLU-L [15] 32.0 36.9 30.9 17.6 InternVideo-L [68] 40.7 31.5 30.7 24.9 40.7 48.6 41.9 UMT-L [33] 20.1 42.4 18.4 15.1 ViClip-L [69] LanguageBind-L [84] 42.8 39.7 38.4 - Frozen ViT with LiT Training using MobileCLIP-B 19.2 34.4 36.3 35.7 V-JEPA2-L [2] 21.4 InternVideo2s2-L [70] 42.1 42.8 43.6 20.8 InternVideo-Next-L 43.2 43.7 43.4 - 43.4 49.0 49.1 54.1 40.1 44.5 46. Table 10. Results of zero-shot text-to-video retrieval on MSRVTT, DiDeMo, ActivityNet, LSMDC and MSVD. We only report the T2V R@1 accuracy here. Encoder Clip-L [50] SigLIP336-L [79] SigLIP2336-L [59] UMT-L [33] V-JEPA2-L [33] InternVideo2-L [70] InternVideo-Next-L MVBench Percept Test Dream1k 45.1 44.1 45.8 44.5 44.2 46.7 49.2 45.6 46.7 46.9 45.0 44.3 47.0 50.6 28.4 29.2 29.6 24.6 24.3 28.7 29.8 Table 11. Results on Chat-Centric benchmarks MVbench [74], Perception Test [48] for General Perception and Dream1k [63] for Detailed Caption. Models are trained in multi-modal linear prob setting where both the Encoder and the LLM are frozen, which is common setting in the Stage 1 training for multi-modal chat models. For Dream1k we report the F1-score. MSVD [12]. Our model shows comparable performance. This highlights our models embedding of the video is highly aligned with the semantic text space implicitly. Chat-centric Tasks. We further evaluate our models potential as foundation Encoder for generative video-LLM frameworks. We follow the recipe of VideoChat-Flash [35] Stage 1 and utilize large-scaled train-set including LLava558K[39], S-MiT [43], 700k filtered subset of WebVid10M [3], VidLN[61], and SSv2-open-ended [22] to fully train the connector. Such Linear Probing setting is widely used in the Stage-1 training of multi-modal chat models. Our model achieves better results on general spatiotemporal perception Video QA tasks MVBench [74] and Perception Test [48], and caption task Dream1K [63]. 5. Conclusion and Future Work In this paper, we present InternVideo-Next, two-stage video pretraining framework without video-text supervision that unifies pixel-level reconstruction and latent prediction under coherent semantic space. By enforcing semantic alignment between encoder and predictor, our method effectively learns structured spatiotemporal pri8 Memory(G) K400 Method 70.7 22 SigLip Align Only. 69.8 Align + Linear Recon. 41 75.8 Align + Diffusion Recon. Table 12. GPU memory utilization comparison. Models are trained with the Stage 1 settings using 16 frames and mask ratio of 80%. We use batch size of 16 and report the memory consumption on single A100 GPU. ors from raw videos, surpassing previous video-only and videotext methods on general video tasks, especially those requiring implicit world knowledge. For future works, we currently rely on large-scale image-text model. While this strategy is highly effective, in that it provides clean semantic signals and boosts multi-modal alignment without noisy video captions, it also represents certain dependency. We here do not much see it as limitation, but possibly essential way in building video understanding models. The model may need to understand images first for fast convergence before understanding the complex video modality with an additional time dimension. Also, we plan to extend this framework toward multimodal and interactive world modeling, exploring efficient scaling and open-world generalization for more versatile video foundation models. A. Training GPU utilization Table 12 shows memory consumption using different training strategies in Stage 1. Utilizing patch diffusion decoder increases the memory consumption to an acceptable level, considering the performance boost it brings. We further explain the process of diffusion loss in Algorithm 1. B. More implementation details In this section, we introduce the model architectures, training hyperparameters, and test settings in our experiments. B.1. Model architecture and training details We use patch of size 14 for our InternVideo-Next models. We report the detailed architecture of Base-scaled model in Table 13, and the pre-training details in Tables 14, 15. B.2. Action Recognition We show the detailed hyperparameters for action recognition probing in Table 16. We use an attention pooling head that averages the input tokens into single token. And the single token is utilized as the common [CLS] token for classification, as shown in Figure 4 (a). B.3. Depth Estimation Experiment Setup. To assess the spatiotemporal, lowlevel, and 3D-geometry related capabilities of InternVideoStage Data Patch Embedding Position Embedding Mask Encoder Projection ViT-B sparse sampling 11414, 768 stride 11414 sine-cosine 7682048 semantic mask mask ratio = ρ (cid:21) (cid:20) MHSA(768) MLP(3072) (cid:20) LN(768) MLP(1408) Output Size 38224224 7688256 7682048 7682048(1-ρ) 12 7682048(1-ρ) (cid:21) K14082048(1-ρ) Table 13. Architecture of video encoder. We take ViT-B with 8-frame input as an example. MHSA, MLP and LN refer to spatiotemporal multi-head self-attention, multi-layer perceptron and layer normalization. means the layer number for unmasked token alignment. We mark the channel number, frame number, spatial size and token number by different colors. config optimizer optimizer momentum weight decay learning rate schedule learning rate batch size warmup epochs [21] total epochs input frame spatial resolution flip augmentation augmentation SthSth Others AdamW [40] β1, β2=0.9, 0.98 0.05 cosine decay [41] 1e-3 2048 5 50 16 224 no yes MultiScaleCrop [0.66, 0.75, 0.875, 1] Table 14. InternVideo-Next Stage 1 pre-training settings. config optimizer optimizer momentum weight decay learning rate schedule learning rate batch size warmup epochs [21] total epochs input frame spatial resolution flip augmentation augmentation SthSth V2 Others AdamW [40] β1, β2=0.9, 0.98 0.05 cosine decay [41] 1e-4 2048(Base) 1024(Large) 0 100 32 224 no yes MultiScaleCrop [0.66, 0.75, 0.875, 1] Table 15. InternVideo-Next Stage 2 pre-training settings. Next, we evaluate it on two widely used monocular video depth-estimation benchmarks: ScanNet and KITTI. ScanNet is large-scale RGB-D video dataset of indoor scenes, containing over 2.5 million views across more than 1500 scans with rich annotations. KITTI is an outdoor dataset collected using an autonomous-driving platform, where stereo videos are recorded by high-resolution color cameras 9 Figure 4. Explanations of different probing settings in our paper. class DiffusionLoss(nn.Module) def __init__(depth = 6, width = 1536): # SimpleMLP takes in x_t, timestep, and condition, and outputs predicted noise. self.net = SimpleMLP(depth, width) # GaussianDiffusion offers forward and backward functions q_sample and p_sample. self.diffusion = GaussianDiffusion() # Given condition and ground truth token x, compute loss def loss(self, z, x): # sample random noise and timestep noise = torch.randn(x.shape) timestep = torch.randint(0, self.diffusion.num_timesteps, x.size(0)) # sample x_t from x_t = self.diffusion.q_sample(x, timestep, noise) # predict noise from x_t noise_pred = self.net(x_t, timestep, z) # L2 loss loss = ((noise_pred - noise) ** 2).mean() return loss Algorithm 1 Pseudo-code illustrating the concept of Diffusion Loss. Here, the conditioning vector is the output from the Predictor. The gradient is backpropagated to z. The GT is generated by patching the video input with pixel shuffle function and patch size of 14. And then we input the masked part of and the predicted part into the model. and depth ground truth is captured via Velodyne LiDAR sensor. Throughout experiments, we adopt head-probing setup in which the backbone network remains frozen. Training Details. To provide comprehensive evaluation, we employ two representative prediction heads: (1) Simple probing head, following Scaling4d[10], implemented as lightweight cross-attention pooling layer with an embedding dimension of 1536 (see Figure 4(b)); (2) Temporally Aware head [14] based on VideoDepthAnything, consisting of four stacked temporal modules specifically designed to adapt image models such as DINO to video depth-estimation tasks (see Figure 4(c)). As it contains spatial modules tailored for depth prediction, we also apply it to video models. We use learning rate of 3 103 with batch sizes of 128 (KITTI) and 256 (ScanNet). Models are trained for 30 epochs, which we find sufficient for convergence. sliding-window strategy is applied to segment input videos, using window sizes of 32 (stride 10) for KITTI and 16 (stride 5) for ScanNet. Following standard practice, the valid depth range is set to [0.1, 80.0] for KITTI and [0.001, 10.0] for ScanNet, with values outside these intervals masked or clamped. 10 SthSth v2 Kinetics Coin AdamW [40] β1, β2=0.9, 0.999 cosine decay [41] 1e-3 3e-4 2e-4 1024 2 20 30 0.75 (B), 0.85 (L) no yes 20 yes config optimizer optimizer momentum learning rate schedule learning rate batch size repeated augmentation warmup epochs [21] total epochs layer-wise lr decay [4] flip augmentation label smoothing [55] cutmix [77] augmentation 0.1 1.0 RandAug(9, 0.5) [16] Table 16. Action recognition fine-tuning settings. Competitive Methods. We compare InternVideo-Next with state-of-the-art image and video representation models, re-implementing all baselines under unified training and evaluation protocol for fairness. For image-based models, we include SigLip2 and DINOv3, known for strong semantic alignment and high-quality visual representations. For video-based models, we evaluate VideoMAEv2, InternVideo2, and V-JEPA, representing the current state of the art in video representation learning. Evaluation Metrics. Following standard monocular depth-estimation protocols, we report two metrics that jointly assess geometric accuracy. 1. Absolute Relative Error (AbsRel): AbsRel = 1 (cid:32) (cid:88) i=1 Di ˆDi Di (cid:33) , where Di and ˆDi denote the ground-truth and predicted depths, respectively. This metric reflects the average relative deviation of predictions from the ground truth. 2. δ1: The proportion of predictions within factor of 1.25 of the ground truth, providing measure of prediction robustness and can be viewed as the accuracy rate. B.4. Object Tracking Setup. To evaluate the models capability to capture object-level motion, we conduct experiments on the Waymo Open dataset, following the protocol of Scaling4D [10]. The videos include 2D and 3D bounding-box annotations. We use only the RGB frames as model input and the 2D bounding boxes for computing losses and evaluation metrics. Because the original Scaling4D benchmark construction code is not publicly available, we reconstruct the training and test sets based on descriptions provided in their paper. Given the target objects bounding boxes in the first 11 frame, the models are required to output their bounding boxes in the subsequent frames. Preprocessing. Raw 1280 1920 RGB videos are spatially downsampled to 224336 and then centrally cropped to 224 224. All bounding boxes are remapped to the cropped coordinate space, and boxes occupying less than 0.5% of the cropped frame area are filtered out. Temporally, the original 20-second sequences recorded at 10 fps are further downsampled to 5 fps. Dataset Construction. Training samples are generated via sliding window of 16 frames (length = 16, stride = 1) applied to each downsampled video. To avoid excessive redundancy, the starting indices for extracted windows must be at least three frames apart. We impose several object-consistency constraints: each window must contain 125 objects in the first frame; each first-frame object must appear in at least 70% of frames within the window; all first-frame objects must co-occur in at least 10 frames; and each object may have at most continuous 5-frame missing gap. No further area-based filtering beyond the initial 0.5% threshold is applied. Training and Evaluation. During training, the backbone is frozen and only the probing head is fine-tuned. The head encodes the initial bounding boxesspecifying the objects to trackusing query encoder. The resulting query tokens are fed into one-layer cross-attention module to obtain pooled tokens, which are then decoded linearly to regress bounding boxes for all frames except the first. The training objective is weighted sum of Smooth L1 loss (1.0) and GIoU loss (0.5). For evaluation, the Intersection-overUnion (IoU) is averaged across all objects and frames."
        },
        {
            "title": "References",
            "content": "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, 2017. 8 [2] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. ArXiv, 2506.09985, 2025. 1, 2, 6, 7, 8 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In ICCV, 2021. 8 [4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, 2021. 3, 11 [5] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. ArXiv, 2404.08471, 2024. 2, 3, 4, 6 [6] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. Semantickitti: dataset for semantic scene understanding of lidar sequences. ArXiv, 2019. 6 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. ICCV, 2021. [8] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about kinetics600. ArXiv, abs/1808.01340, 2018. 4 [9] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. short note on the kinetics-700 human action dataset. ArXiv, abs/1907.06987, 2019. 4, 8 [10] Joao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Velez, Luisa Polanıa, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Patraucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, and Andrew Zisserman. Scaling 4d representations. arXiv, 2025. 1, 7, 10, 11 [11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. 8 [12] David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011. 8 [13] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. In NIPS, 2023. 3 [14] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In CVPR, 2025. 6, 7, [15] Feng Cheng, Xizi Wang, Jie Lei, David J. Crandall, Mohit Bansal, and Gedas Bertasius. Vindlu: recipe for effective video-and-language pretraining. ArXiv, abs/2212.05051, 2022. 3, 8 [16] Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment: Practical automated data augmentation with reduced search space. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2020. 11 [17] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. ArXiv, 2017. 5, 6 [18] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The epic-kitchens dataset: Collection, challenges and baselines. TPAMI, 43, 2020. 6, 7 [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [20] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and In Kaiming He. Slowfast networks for video recognition. ICCV, 2019. 1 [21] Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv, abs/1706.02677, 2017. 9, 11 [22] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Ingo Fruend, Peter Yianilos, Moritz Valentin Haenel, Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In ICCV, 2017. 4, 5, 6, 8 [23] Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang. Vision-and-language navigation: survey of tasks, methods, and future directions. In ACL, 2022. 1 [24] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016. 4 [25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2, 3, 4 [26] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In CVPR, benchmark for human activity understanding. 2015. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NIPS, 2020. 4 [28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Apostol Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. ArXiv, abs/1705.06950, 2017. 2, 4, 5, 6, 8 [29] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, 2017. 8 [30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017. 8 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2022. 1 [32] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. ArXiv, abs/2305.06355, 2023. 1 [33] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, 2023. 1, 8 [34] Tianhao Li and Limin Wang. Learning spatiotemporal features via video and text pair discrimination. arXiv, 2021. 3 [35] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. Videochatflash: Hierarchical compression for long-context video modeling. arXiv, 2025. 1, 8 [36] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In ICCV, 2019. 1 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023. 1 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 8 [40] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017. 9, 11 [41] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017. 9, 11 [42] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NeurIPS, 2019. 1, [43] Mathew Monfort and SouYoung Jin. Spoken moments: Learning joint audio-visual representations from video descriptions. In CVPR, 2021. 8 [44] Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex Andonian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown, Quanfu Fan, Dan Gutfreund, Carl Vondrick, and Aude Oliva. Moments in time dataset: One million videos for event understanding. TPAMI, 2020. 4, 8 [45] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. ArXiv, abs/2208.02816, 2022. 3 [46] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. arXiv, 2021. 4 [47] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In NeurIPS, 2011. [48] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adri`a Recasens Continente, Larisa Markeeva, Dylan, Banarse, Mateusz Malinowski, Yezhou Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine, Miech, Skanda Koppula, Alexander Frechette, Hanna Klimczak, R. Koster, Junlin Zhang, Stephanie, Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Joao Carreira. Perception test : diagnostic benchmark for multimodal models. In NeurIPS, 2023. 8 [49] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling with vector-quantized visual tokenizers. ArXiv, abs/2208.06366, 2022. 3 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 3, 8 [51] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Joseph Pal, H. Larochelle, Aaron C. Courville, and Bernt Schiele. Movie description. International Journal of Computer Vision, 2016. 8 [52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. [53] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. Dinov3. arXiv, 2025. 7 [54] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception In CVPR, for autonomous driving: Waymo open dataset. 2020. 6, 7 [55] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inIn CVPR, 2016. ception architecture for computer vision. 11 [56] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In CVPR, 2019. 6 [57] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In NeurIPS, 2022. 2, 3 [58] Du Tran, Hong xiu Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. closer look at spaIn CVPR, tiotemporal convolutions for action recognition. 2018. 1 [59] Michael Tschannen, Alexey Gritsenko, Xiao Wang, MuhamIbrahim Alabdulmohsin, Nikhil mad Ferjad Naeem, Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. ArXiv, 2025. 3, 4, 6, 7, [60] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobile13 clip: Fast image-text models through multi-modal reinforced training. In CVPR, 2024. 8 coder for fast, memory efficient, and long context finetuning and inference. ArXiv, 2412.13663, 2024. 4 [73] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, 2022. 3 [74] Longhui Wei, Lingxi Xie, Wen gang Zhou, Houqiang Li, and Qi Tian. Mvp: Multimodality-guided visual pre-training. In ECCV, 2022. [75] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Videoclip: Contrastive preChristoph Feichtenhofer. ArXiv, training for zero-shot video-text understanding. abs/2109.14084, 2021. 1, 3 [76] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In CVPR, 2016. 8 [77] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Young Joon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 11 [78] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. In Lit: Zero-shot transfer with locked-image text tuning. CVPR, 2022. 3, 8 [79] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. CVPR, 2023. 3, 8 [80] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. ArXiv, abs/2306.02858, 2023. [81] Hang Zhao, Antonio Torralba, Lorenzo Torresani, and Zhicheng Yan. Hacs: Human action clips and segments dataset for recognition and temporal localization. In ICCV, 2019. 4 [82] Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, et al. Videoprism: foundational visual encoder for video understanding. In PMLR, 2024. 1, 6 [83] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv, 2025. 1 [84] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Wang HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to nmodality by language-based semantic alignment. In ICLR, 2024. 8 [61] Paul Voigtlaender, Soravit Changpinyo, Jordi Pont-Tuset, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with video localized narratives. CVPR, 2023. 8 [62] Chenting Wang, Kunchang Li, Tianxiang Jiang, Xiangyu Zeng, Yi Wang, and Limin Wang. Make your training flexible: Towards deployment-efficient video models. arXiv, 2025. 3 [63] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv, abs/2407.00634, 2024. 8 [64] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, 2016. 1 [65] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR, 2023. 3, 6, [66] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv, abs/2409.12191, 2024. 8 [67] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as foreign language: Beit pretraining for all vision and visionlanguage tasks. ArXiv, abs/2208.10442, 2022. 3 [68] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. ArXiv, abs/2212.03191, 2022. 3, 8 [69] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Jian Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Y. Qiao. Internvid: large-scale video-text dataset for multimodal understanding and generation. ArXiv, 2023. 8 [70] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Chenting Wang, Guo Chen, Baoqi Pei, Ziang Yan, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2: Scaling foundation models for multimodal video understanding. In ECCV, 2024. 1, 3, 4, 6, 7, 8 [71] Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language navigation. In ICCV, 2023. [72] Benjamin Warner, Antoine Chaffin, Benjamin Clavie, Orion Weller, Oskar Hallstrom, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. Smarter, better, faster, longer: modern bidirectional en-"
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Shenzhen Institutes of Advanced Technology, China",
        "State Key Laboratory for Novel Software Technology, Nanjing University"
    ]
}