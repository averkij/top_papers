{
    "paper_title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction",
    "authors": [
        "Changjian Jiang",
        "Kerui Ren",
        "Xudong Li",
        "Kaiwen Song",
        "Linning Xu",
        "Tao Lu",
        "Junting Dong",
        "Yu Zhang",
        "Bo Dai",
        "Mulin Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ ."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 6 4 0 2 2 . 1 0 6 2 : r PLANING: Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction Changjian Jiang1,2* Kerui Ren3,2* Xudong Li2 Kaiwen Song4,2 Linning Xu5,2 Tao Lu2 Junting Dong2 Yu Zhang1 Bo Dai6 Mulin Yu2 1Zhejiang University 2Shanghai Artificial Intelligence Laboratory 3Shanghai Jiao Tong University 4The University of Science and Technology of China 5The Chinese University of Hong Kong 6The University of Hong Kong Figure 1. PLANING introduces loosely coupled triangle-Gaussian representation for streaming 3D reconstruction, balancing geometric accuracy, high-fidelity rendering, and computational efficiency. Building upon this hybrid representation, we further adapt it to an efficient streaming reconstruction framework for monocular image sequences, enabling effective modeling of both scene geometry and appearance in streaming setting. Leveraging the inherent edge-preserving property of triangle primitives, our method allows for the explicit extraction of compact planar structures, which can serve as high-performance simulation environment for locomotion training in embodied AI."
        },
        {
            "title": "Abstract",
            "content": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on hybrid representation that Equal contribution. Corresponding author. loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, 1 and reconstructs ScanNetV2 scenes in under 100 seconds, over 5 faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of PLANING make it well suited for broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/. 1. Introduction 3D scene reconstruction is core capability for embodied intelligence, autonomous driving, and AR/VR, providing the spatial understanding required for perception and interaction [30, 43]. While offline reconstruction methods following capture-then-process paradigm have reached high level of maturity, their reliance on time-intensive post-processing limits scalability and responsiveness in timecritical scenarios. This has driven growing demand for real-time, on-the-fly reconstruction frames. central challenge in on-the-fly 3D reconstruction is scene representation that jointly achieves high geometric accuracy and real-time efficiency. Recently, 3D Gaussian Splatting (3DGS) [18] has emerged as compelling explicit representation, offering high visual fidelity with efficient rendering, and has therefore been widely adopted in streaming reconstruction methods [5, 17, 20, 25, 26, 50]. Despite their success, existing streaming 3DGS-based methods share fundamental limitation: the absence of explicit, compact, and stable geometry. While Gaussian primitives are effective for appearance modeling, they lack well-defined structural boundaries, making it difficult to recover coherent and editable surface geometry without sacrificing rendering quality. Moreover, optimizing Gaussians to reproduce input views inherently biases learning toward appearance, often at the expense of geometric consistency especially under sparse observations or novel viewpoints. To compensate, these methods rely on large number of primitives, leading to significant redundancy, increased computational cost, and limited scalability in streaming settings. To address this challenge, (1) we propose hybrid representation that decouples geometry from appearance, enabling both efficient geometric reconstruction and high-fidelity rendering. For geometry, we introduce learnable triangle primitives. Triangles provide well-defined edges and explicitly model surface structures, making them particularly effective for capturing the planar layouts prevalent in indoor environments. For appearance modeling, we adopt neural-Gaussian formulation inspired by Scaffold-GS [23], in which Gaussian attributes are decoded from fused feature representation that combines triangle features with perGaussian features, encouraging local rendering consistency and smoothness. This design establishes synergistic couFigure 2. PLANING consistently outperforms existing streaming and per-scene reconstruction methods across geometry accuracy, rendering quality, computational efficiency, and memory usage, while maintaining clear and well-structured planar geometry. pling: triangles act as stable structural anchors that mitigate drift and redundancy, while rendering gradients propagated through neural Gaussians refine the underlying geometry in controlled way, allowing appearance cues to guide surface optimization without conflicting against the structural constraints. Building upon this hybrid representation, (2) we introduce PLANING, framework for efficient monocular 3D reconstruction in streaming setting. Our framework leverages feed-forward models as learned priors to enable robust camera pose estimation and to provide stable geometric guidance for scene modeling. To achieve both high efficiency and global consistency, we adopt tailored initialization strategy that applies photometric and spatial filtering to reduce redundant primitives, and perform global map adjustment to keep the reconstructed 3D model aligned with continually optimized camera poses. Extensive experiments across diverse indoor and outdoor benchmarks demonstrate that our method outperforms stateof-the-art approaches in geometric accuracy, rendering quality, training efficiency, and primitive count, as illustrated in Fig. 2. By preserving salient structures while removing redundant geometry, our representation enables the export of compact and consistent 3D planes. This highly compressed geometric output, characterized by significantly reduced triangle count, shows strong potential for enhancing large-scale scene reconstruction and improving the global consistency of pose estimation. Additionally, the structural clarity and computational efficiency of our model make it well suited for simulation-ready scene modeling, such as supporting local motion policy training in embodied AI. Our main contributions can be summarized as follows: Decoupled Geometry and Appearance Modeling. We introduce hybrid scene representation that loosely couples explicit, learnable triangle primitives for geometry with neural Gaussians for appearance, enabling compact, stable, and editable structure while preserving high-fidelity 2 rendering. principled, explicit decoupling of geometry and appearance. Efficient Streaming Reconstruction Framework. We develop an efficient on-the-fly monocular reconstruction framework that leverages the proposed representation together with streaming-aware initialization and global map adjustment. State-of-the-Art Results and Broad Applicability. We demonstrate state-of-the-art performance in both geometric accuracy and rendering quality across diverse indoor and outdoor benchmarks, and showcase the versatility of our approach for downstream tasks including planeguided pose refinement, large-scale scene reconstruction, and simulation-ready environments for embodied AI. 2. Related Work 3D Reconstruction. Reconstructing 3D geometry from multi-view images is long-standing and fundamental problem in computer graphics. Traditional methods [34] transform calibrated images into point clouds and optimize them into implicit fields, followed by mesh extraction using Marching Cubes [22]. More Recently, Neural Radiance Fields (NeRF) [27] established neural rendering milestone by using MLPs for ray-based synthesis. However, NeRFbased methods are limited by their implicit nature and costly per-ray sampling, which hinders scalability and geometric control. To address these limitations, 3D Gaussian Splatting (3DGS) [18] employs explicit anisotropic Gaussian primitives, leveraging efficient rasterization to enable real-time reconstruction [16, 32]. Nevertheless, the emphasis on rendering efficiency in 3DGS-based methods often compromises geometric consistency, making it difficult to recover intricate structural details without robust geometric constraints. 3DGS Variants. Various extensions have explored alternative primitives to better align with scene geometry. 2DGS [14], GSS [7], and Quadratic Gaussian Splatting [52] replace anisotropic Gaussians with ellipsoidal or quadric forms for superior surface alignment. Other works incorporate explicit geometric elements, such as the 3D convexes [12] and triangles [1, 11, 15], to compactly model hard-edged scenes. Similarly, PlanarSplatting [37] utilizes rectangular primitives to achieve structured and efficient indoor planar reconstructions. Despite these advances, singlerepresentation methods often struggle to balance geometric precision with rendering fidelity. To bridge this gap, recent dual-branch approaches such as GSDF [47] and 3DGSR [24] integrate neural signed distance fields (SDFs) with 3DGS. While this enables partial geometryappearance decoupling, it introduces significant computational overhead and optimization complexity. Alternatively, 3D-GES [44] adopts bi-scale formulation using 2D surfels for coarse structure and 3D Gaussians for fine detail. However, this design primarily targets appearance enhancement rather than achieving 3 Streaming Reconstruction. Classical visual SLAM frameworks provide robust online tracking and mapping but often lack the fidelity required for high-quality rendering [3, 28, 31]. To address this, recent works have integrated volumetric rendering into SLAM pipelines to enable online novel view synthesis [2, 49, 51, 54]. While NeRF-based SLAM achieves photorealistic results, the high computational cost of per-ray volumetric rendering limits its suitability for real-time applications. In contrast, 3DGS has attracted increasing attention for SLAM integration due to its explicit representation and efficient rendering, with some methods directly propagating gradients from rendering losses to optimize camera poses [10, 17, 25, 50]. However, monocular frameworks often struggle to simultaneously balance robustness, reconstruction accuracy, and efficiency. Recent on-the-fly NVS approaches [26] show that GPU-friendly mini-bundle adjustment combined with incremental 3DGS updates can enable interactive reconstruction, yet they remain fragile on casual, unposed sequences. Meanwhile, feed-forward models [21, 29, 38, 39] pretrained on large-scale datasets have emerged as an alternative paradigm, reconstructing 3D scenes directly without per-scene optimization. These methods fall into two categories: pose-aware approaches, which leverage camera poses for rapid reconstruction, and pose-free approaches, which perform end-to-end reconstruction from raw images using point maps or 3DGS. While these methods offer strong robustness and fast inference across diverse scenarios, they generally underperform optimization-based approaches in accuracy and struggle with global consistency, high-resolution inputs, and long-sequence scalability. 3. Method In this section, we first introduce our dual scene representation that combines learnable triangles with neural Gaussians (Sec. 3.1). We then describe how we adapt this representation into an on-the-fly reconstruction framework, achieving both efficiency and high-quality 3D reconstruction (Sec. 3.2). 3.1. Loosely-coupled Triangle-Gaussian Representation We first detail the triangle primitives and our differentiable rasterizer. Subsequently, we explain the interaction between neural Gaussians and their corresponding triangles, followed by the integrated rendering process. 3.1.1. Learnable Triangles for Geometry We propose learnable triangle primitives based on vertexbased formulation and differentiable triangle rasterizer. Figure 3. Pipeline of PLANING. PLANING adopts hybrid representation in which triangles explicitly model scene geometry, while neural Gaussians decoded from these triangles render appearance. Built upon this representation, we develop streaming reconstruction framework that takes unposed monocular image sequences as input and comprises frontend for camera tracking, backend for global pose optimization, and mapper for scene reconstruction. Specifically, the mapper incorporates an efficient primitive initialization strategy to reduce redundancy. The recontructed triangle soup further enables efficient planar abstraction, facilitating range of downstream tasks. σ > 0, to control edge sharpness and boundary smoothness. Each triangle is also associated with learnable opacity parameter α, analogous to 3DGS [18]. Differentiable Triangle Rasterizer. We implement an efficient differentiable triangle rasterizer that enables direct supervision of triangles using prior normals and depths. To obtain unbiased depth rendering, we adopt an explicit raytriangle intersection strategy, similar in spirit to 2DGS [14]. We further introduce the edge-preserving contribution function as: w(ˆx) = Sigmoid σ log 2 (cid:88) j= exp (δ dist(ˆx, ej)) α, (2) where dist(ˆx, ej) denotes the distance from the intersection point ˆx to the j-th triangle edge in the local tangent plane. Thanks to the local frame parameterization, these distances can be computed analytically as: dist(ˆx, e0) = + (1 a)v 1, dist(ˆx, e1) = 2u + (2a + 1)v 1, dist(ˆx, e2) = + (2 a)v 1, (3) where ˆx = (u, v)T . This closed-form formulation significantly simplifies both forward evaluation and gradient propagation. Notably, our contribution computation differs from 3DCS, where contributions are computed directly on the image plane rather than in the local surface domain. Finally, triangles are rendered into depth and normal maps Figure 4. Definition of the local frame and results of forward rendering. Our triangle rasterizer enables correct and reliable forward rendering of triangles. Vertix-based Primitive Definition. As illustrated in Fig. 4(a), we parameterize each triangle primitive by its three learnable vertices {p0, p1, p2}. To facilitate efficient and differentiable rendering, we define local coordinate frame for each triangle: tu = p0 µ p0 µ2 su = p0 µ2 , , tv = tu, sv = tv (p1 µ) , (1) , = (p1 p0) (p2 p0) (p1 p0) (p2 p0)2 where the barycenter µ is set as the origin of the local frame. Under this construction, the three vertices can be expressed in the local tangent plane as {p 2} = {(0, 1)T , (a, 1)T , (1 a, 1)T }, where = tu (p1 µ) is the only degree of freedom in the local frame. 1, 0, Following 3D Convex Splatting (3DCS) [12], we further introduce two learnable triangle-wise parameters, δ > 0 and 4 using front-to-back alpha compositing: N(x) = D(x) = (cid:88) i=1 (cid:88) i=1 niw (ˆxi) diw (ˆxi) i1 (cid:89) j=1 i1 (cid:89) j=1 (1 (ˆxj)) , (1 (ˆxj)) , (4) where di denotes the distance from the i-th intersection point to the pixel. The ordered intersection points {ˆxi} between the triangles and pixel are computed using our custom CUDA-based rasterizer. To enable accurate differentiable rendering of triangles, we define new criterion for visibility determination and design triangle-subdivision-based primitive depth sorting algorithm in the rasterizer to address rendering issues introduced by the edge-preserving contribution function, as illustrated in Fig. 4(b). The detailed forward rendering pipeline is described in the Appendix A.1. 3.1.2. Neural Gaussians for Appearance Modeling To achieve decoupled yet consistent representation of geometry and appearance, we introduce neural Gaussians to flexibly encode view-dependent appearance. Inspired by Scaffold-GS [23], neural Gaussians are anchored to the triangles and used for appearance. Specifically, each learnable triangle is associated with context feature ft R24. Each Gaussian is parameterized by learnable position offset og R3, spherical harmonics (SH) coefficients, opacity αg R, base scale sg R3, base quaternion qg R4, and an individual feature fg R8. In addition, each Gaussian maintains the index it of its corresponding triangle as the geometric association. During rendering, the position of each Gaussian µg = og +µt, where µt denotes the barycenter of the associated triangle. Then we predict the final scale = sgMLPs(ftfg) and rotation = ϕ(qg MLPq(ft fg)), where denotes element-wise multiplication, denotes feature concatenation and ϕ() denotes ℓ2 normalization to ensure valid rotation quaternions. Through this design, geometry and appearance are represented in consistent and coherent manner. Notably, each triangle hosts flexible number of Gaussians, enabling the representation to adapt to local scene details. 3.2. Streaming Reconstruction Framework 3.2.1. Overview As shown in Fig. 3, we design streaming reconstruction framework built upon our hybrid representation, leveraging its capacity for high-fidelity modeling. Following [20], our framework takes unposed monocular image sequences as input and comprises three main components: frontend for camera tracking, backend for global pose optimization, and mapper for scene reconstruction. The frontend processes incoming frames in streaming manner to estimate camera motion, select keyframes, and predict per-frame dense point maps using feed-forward models [19]. The backend subsequently performs loop closure detection [39] and global bundle adjustment [29] over keyframes to improve global pose consistency, which is critical for accurate geometry reconstruction. The mapper reconstructs scene geometry and appearance by integrating posed images and dense point maps provided by the backend. Unlike previous streaming methods that rely on single representation [20, 26], our mapper utilizes loosely coupled triangleGaussian representation to decouple geometry from appearance modeling, thus mitigating mutual interference. Guided by geometric priors from the backend, we introduce novel primitive initialization and optimization strategy. To maintain global geometric consistency, we perform global map adjustment whenever the backend updates the global camera poses. Following streaming reconstruction, planar structures can be directly extracted from the triangle soup via coarseto-fine plane extraction algorithm. Furthermore, our framework supports dense mesh reconstruction through depth fusion. Additional implementation details are provided in Appendix A.3. 3.2.2. Primitive Initialization Upon the arrival of keyframe from the backend, the framework determines the optimal locations for instantiating new primitives. To maintain compact global map and mitigate structural redundancy, triangle insertion is restricted to regions exhibiting insufficient geometric coverage or high reconstruction error, guided by image-level priors. Specifically, we first apply photometric filter, which prioritizes highfrequency regions and poorly reconstructed areas by computing an insertion probability Pa(u, v) at each pixel (u, v) using the Laplacian of Gaussian (LoG) operator Φ()[26] to measure the discrepancy between the ground truth and rendered images: Pa(u, v) = max (cid:16) Φ(I) Φ( I), 0 (cid:17) , (5) where Φ(I) = min(2(Gσg ) I(u, v), 1), and represent the ground-truth and rendered images, respectively, and Gσg denotes Gaussian smoothing kernel. new geometric primitive is considered only when Pa(u, v) exceeds predefined threshold τa. To further suppress structural redundancy, we apply spatial filter to candidates passing the photometric filter. For each candidate pixel, we compute its back-projected 3D center ci and prune it if any existing triangles fall within its local vicinity of size (di): (di) = Vmin + (Vmax Vmin) (cid:18) di dmin dmax dmin (cid:19)p , (6) 5 di denotes observation and depth, the where {Vmin, Vmax, dmin, dmax, p} are hyperparameters that modulate the vicinity scale. This depth-adaptive spatial filter ensures map compactness by preventing redundant primitive growth in already-reconstructed regions. Once candidate pixel (u, v) is selected, triangle is initialized. Each triangle is parameterized by its vertices pt, opacity αt, sharpness δt, smoothness σt, and feature vector ft. Following geometric scaling principles, the world- (cid:14)2f (cid:112)Φ(I), where is the focal length. space scale st = 3di The triangle orientation is determined by the normal prior at (u, v). Specifically, three unit vectors vt,k are sampled on the local tangent plane, and the vertex positions pt = st vt. The opacity is initialized as αt = 0.2 C(u, v) to downweight low-confidence regions, where C(u, v) is the backend confidence score. Then, neural Gaussians are initializeded at triangle barycenters for appearance modeling. We adaptively set the number of Gaussians per triangle to Kmax if Φ(I) > 0.4, and Kmin otherwise. Here, the hyperparameters Kmax and Kmin define the bounds of the representational capacity based on scene detail. For primitive attributes, we initialize offsets og, rotation qg, and features fg to zero, while Gaussian opacity αg is synchronized with αt. The base scale is (cid:14)2f (cid:112)Φ(I) 1 to align with local geomedefined as sg = di try. Crucially, the zero-order spherical harmonic coefficient SH0 is extracted from the pixel color at (u, v), with higher coefficients zero-initialized. 3.2.3. Training We supervise the triangles and Gaussians with separate geometric and appearance losses for decoupled optimization: = Lgeo + Lrgb. (7) For geometry, we leverage multi-view depth Dp and normal Np priors from MASt3R [19] to supervise triangles, penalizing deviations from the rendered depth Dt and normals Nt: Lgeo = λdDt Dp1 + λnNt Np1 + λoLo, (8) where λd and λn are user-prescribed weights. Lo is an entropy loss on triangle opacity α, following [9]. We regularly prune triangle primitives with α < 0.5, which removes redundant geometry and maintains compact representation. The appearance loss supervises neural Gaussians via: Lrgb = (1 λc)Cgt Cgs1 + λcSSIM(cid:0)Cgt, Cgs where Ls is volume regularization term adopted from Scaffold-GS [23]. Notably, appearance gradients from Gaussians are back-propagated to the triangles, enabling implicit refinement of the underlying geometry. More details are provided in Appendix A.2. (cid:1) + λsLs, (9) 3.2.4. Global Map Update In our streaming framework, camera poses are continuously refined within the backend, while primitives in the mapper are initialized and optimized using the poses available at that timestamp. This asynchronous update can lead to posemodel misalignment. To maintain consistency between the refined poses and the 3D model, we explicitly transform the primitives after the pose optimization. Specifically, we record the source keyframe for each primitive and apply relative transformation = TnT1 to its attributes when the corresponding keyframe pose changes from To to Tn: = Tpt, = R1(RR(qg)), g = T(og + µt) µ t, (10) where is the rotation component of T, and R() maps t, quaternions to rotation matrices. Here, {p g} denote the updated triangle and Gaussian parameters. g, t, µ 4. Experiments 4.1. Experimental Setup Datasets. We evaluate PLANING on 56 real-world scenes from diverse benchmarks: 20 from ScanNet++ [45], 10 from ScanNetV2 [6], 6 from VR-NeRF [42], 4 from FASTLIVO2 [53], 8 from KITTI [8], and 8 from Waymo [36], covering wide range of indoor and outdoor environments. Baselines. We compare PLANING with state-of-the-art methods across three categories. For per-scene reconstruction, we evaluate 2DGS [14], PGSR [4], and MeshSplatting [11]. For streaming reconstruction, we select ARTDECO [20], OnTheFly-NVS [26], S3PO-GS [5], and MonoGS [25]. For planar reconstruction, we include PlanarSplatting [37] and AirPlanes [41]. To ensure fair comparison, all per-scene reconstruction baselines are augmented with the same MASt3R geometric priors used in ours. For methods requiring poses, we provide our estimated poses for fair comparison. Metrics. We conduct comprehensive evaluation of our framework across three tasks. For planar reconstruction, following PlanarSplatting [37], we evaluate plane geometry using Chamfer Distance and F-score. For datasets with ground-truth plane annotations, we further assess the top-20 largest planes using Planar Fidelity, Planar Accuracy, and Planar Chamfer metrics. For dense mesh reconstruction, we report Chamfer Distance and F-score, while for novel view synthesis (NVS), we use standard metrics including PSNR, SSIM [40], and LPIPS [48]. In addition, we report training time and the number of primitives to quantify computational efficiency. 6 Table 1. Quantitative comparison of planar reconstruction. We evaluate the geometric and planar metrics on the ScanNet++, ScanNetV2, and FAST-LIVO2 datasets. Ours achieves top-tier performance in most categories while significantly reducing primitive count and runtime (reported in minutes). Method Geometry ScanNet++ Planar Ch-L2 F-score Fidelity Acc Ch-L2 Time #Prim. Geometry ScanNetV2 Planar Ch-L2 F-score Fidelity Acc Ch-L2 FAST-LIVO2 Time #Prim. Geometry Acc Comp Ch-L2 F-score Time #Prim. 2DGS 3.89 PGSR 3.87 MeshSplatting 9.13 AirPlanes PlanarSplatting 25.19 7.27 81.64 81.98 47.19 19.21 49.78 7.19 7. 16.1 415.3k 6.48 8.16 7.44 31.2 353.4k 6.59 37.87 10.71 24.29 38.5 1825k 11.15 7.67 7.33 47.10 25.97 36.53 13.35 11.50 9.64 3.7 8.8 / 1.0k 6.34 6. 53.73 54.28 30.73 55.33 51.67 15.56 15.88 40.16 11.45 25.81 8.12 11.84 10.9 1196.8k 14.11 48.17 53.45 8.46 12.17 21.3 629.1k 13.95 49.16 54.13 9.7 291.3k 14.52 66.68 62.97 60.47 60.75 47.31 35.8 3197.0k 25.5 1065.8k 26.3 2505.1k 9.68 9.72 8.90 9.29 10.77 10.24 3.5 3.1 / 1.76k - - - - - - - - - - - - ARTDECO 3. 83.08 15.84 7.92 11.88 5.6 478.3k 6.05 57.58 19. 8.73 14.18 2.2 621.5k 14.17 63.63 57.19 54.23 6.5 501.6k Ours 3. 86.88 7.24 6.95 7.09 5.5 61.6k 5. 62.15 10.55 7.58 9.07 2.1 56.1k 12.58 30.60 36. 65.89 3.6 101.4k /: w/o explicit geometric primitives, : beyond the scope (indoor scenes) of the method, : leveraging geometric priors. Table 2. Quantitative comparison of appearance rendering. We evaluate the rendering quality metrics across six diverse indoor and outdoor datasets. Our method achieves state-of-the-art performance in most categories while significantly reducing the runtime (reported in minutes). Method ScanNetV VR-NeRF ScanNet++ Waymo FAST-LIVO2 KITTI PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS Time 2DGS 27.74 0.873 0.234 PGSR 27.73 0.880 0.233 MeshSplatting 25.64 0.830 0.351 29.61 0.905 0.200 29.37 0.903 0.201 25.23 0.819 0.352 32.00 0.937 0.129 31.38 0.937 0.133 27.71 0.876 0.294 26.97 0.854 0.324 27.42 0.865 0.306 23.10 0.781 0.424 29.24 0.867 0.287 29.22 0.870 0.280 25.78 0.789 0. 22.29 0.751 0.338 31.9 22.88 0.785 0.284 39.9 17.35 0.551 0.499 24.6 MonoGS S3PO-GS 22.17 0.806 0.542 24.37 0.829 0.476 15.30 0.583 0.655 24.00 0.810 0.371 17.08 0.708 0.632 23.34 0.820 0.444 19.06 0.744 0.639 25.33 0.821 0. 19.80 0.694 0.649 24.99 0.776 0.419 14.56 0.489 0.767 8.3 19.23 0.622 0.430 24.5 OnTheFly-NVS 23.33 0.823 0.376 28.44 0.877 0.232 ARTDECO 29.10 0.895 0.237 30.02 0.911 0.230 21.54 0.794 0.357 31.64 0.941 0.140 27.22 0.848 0.300 26.59 0.869 0. 21.92 0.735 0.443 32.86 0.926 0.210 17.17 0.584 0.427 22.99 0.777 0.282 1.3 6.9 Ours 28.83 0.882 0.222 32.59 0.933 0. 31.91 0.941 0.133 29.24 0.887 0.278 33.97 0.938 0.180 23.82 0.793 0.253 7.4 : leveraging geometric priors. Table 3. Quantitative comparison of dense mesh reconstruction. Method ScanNet++ ScanNetV2 Ch-L2 F-score Ch-L2 F-score Ch-L2 F-score FAST-LIVO2 2DGS 3.95 PGSR 3.92 MeshSplatting 9. 80.90 81.47 46.30 6.45 6.55 11.05 53.11 53.89 31.22 52.83 53.56 61.03 61.06 60.99 51.61 ARTDECO 3.87 82.34 6.00 57.61 36.99 61. Ours 3.76 84.81 5.87 59.93 38. 64.36 : leveraging geometric priors. Implementation Details. Following standard novel view synthesis practice, every eighth frame is held out for evaluation, which are excluded from the mapper while their poses are optimized for evaluation. Following [20, 26], our method, ARTDECO [20], and OnTheFly-NVS [26] perform 15k-iteration global optimization after the streaming stage, whereas per-scene baselines are trained for 30k iterations. More implementation details are provided in Appendix A.4. 4.2. Results Analysis Geometry Results. We first evaluate our method on planar reconstruction, comparing it with six baselines spanning diverse set of learnable scene representations, including triangles, 3D Gaussians, surfels, rectangles, and implicit embedding-based planar representations. Quantitative results in Tab. 1 show that our method consistently achieves superior geometric accuracy, attaining the lowest Chamfer Distance and highest F-score, while maintaining compact primitive count and the shortest training time. As shown in Fig. 5, our hybrid representation preserves planar regularity and sharp geometric features by explicitly modeling planar structures with triangles. In contrast, rectangle-based representations, despite their compactness, lack the flexibility to capture fine-grained geometry, limiting their ability to model complex scene structures. Surfel-based methods, which tightly couple geometry and appearance, often suffer from appearance-induced distortions, resulting in uneven or erroneous surfaces even when geometric priors are applied. We also evaluate our method on dense mesh reconstruction, with all meshes extracted via depth fusion for fair comparison. As reported in Tab. 3, our method achieves higher geometric accuracy while requiring less than 20% of the training time compared to per-scene optimization methods. Figure 5. Qualitative comparison of geometric reconstruction. We visualize planar reconstruction and geometric modeling across different primitives, with 2DGS shown as dense mesh for comparison. Overall, our method preserves planar structures while capturing fine geometric details. Rendering Results. Our method achieves state-of-the-art rendering performance, outperforming both per-scene optimization and streaming reconstruction baselines, as shown in Tab. 2. In particular, it demonstrates clear advantages in texture-less and low-light regions  (Fig. 6)  . In these challenging scenes, per-scene optimization models are prone to overfitting or Gaussian instability due to poor initialization, while streaming approaches frequently suffer from pose drift that manifests as rendering artifacts. By contrast, our approach mitigates these issues through precise and consistent geometric model. Furthermore, the integration of feed-forward model ensures robust pose estimation, further driving the improvement in rendering fidelity. 4.3. Applications era poses via online plane extraction and point-to-plane alignment loss, improving global consistency  (Fig. 9)  . Due to the geometric regularity and structural sparsity of planar primitives, these constraints provide strong and stable geometric supervision for pose estimation. Details are provided in Appendix B.1. Large Scale Scene Reconstruction. Although our hybrid representation is compact, large-scale reconstruction remains challenging under limited GPU memory. We therefore adopt dynamic loading strategy that swaps primitive parameters between the GPU and CPU, enabling our framework to scale to large environments  (Fig. 8)  . Additional details are provided in the Appendix B.2. Plane-Guided Camera Pose Optimization. Most streaming reconstruction frameworks decouple pose estimation from mapping, preventing effective use of the global scene map and often resulting in drift. We instead feed back the reconstructed planar map to the frontend and refine camEfficient Locomotion Strategy Training. Our method produces compact, simulation-ready scenes composed of planar primitives. By preserving the geometric correctness and consistency of large-scale structures, the reconstructed environments provide reliable contact geometry for phys8 Figure 6. Qualitative comparison of appearance rendering. We evaluate our method against state-of-the-art approaches. White wireframes highlight regions where our method excels, faithfully reconstructing fine structures and complete surface. Figure 7. Locomotion. To demonstrate the utility of our geometric output as robust simulation environment, we trained two motion policies using Proximal Policy Optimization (PPO) within the Isaac Lab framework: (a) indoor walking with Unitree H1 humanoid, and (b) stair climbing with Unitree A1 quadruped. These experiments validate that our reconstructed geometry provides high-fidelity foundation for reinforcement learning. ical simulation. The resulting scenes are lightweight, enabling fast asset conversion and scalable training pipelines, as shown in Fig. 7. Additional details are provided in the Appendix B.3. 4.4. Ablation Studies We conduct ablation studies to systematically evaluate the contributions of our representation and framework design. pixels than 2D Gaussians, which stabilizes parameter optimization. We further ablate the hybrid representation by replacing it with unanchored neural Gaussians. As shown in Tab. 4, the hybrid representation improves both geometric accuracy and rendering quality. Moreover, the proposed representation reduces redundancy and encourages Gaussians to concentrate around the underlying surface, as shown in Fig. 11. Representation Design. We replace triangles with 2D Gaussians to ablate their contribution. As shown in Fig. 10, triangles offer two advantages: (i) higher-quality geometry with sharp boundaries; and (ii) improved rendering, since their clear boundaries cause them to be influenced by fewer Framework Design. We conduct ablation studies on the mapping module of our on-the-fly reconstruction framework. Disabling spatial filtering substantially increases the number of primitives (+200% on ScanNetV2 and +245% on ScanFigure 8. Large-scale indoor reconstruction. We captured over 2000 monocular images of an indoor corridor using mobile phone. Leveraging our dynamic loading strategy, our method achieves high-quality dense mesh reconstruction and rendering. Figure 9. Effect of plane-guided camera pose optimization. Feeding back planar map constraints into pose estimation effectively reduces drift. Figure 11. Ablation on hybrid representation. Our design effectively reduces representation redundancy and mitigates the geometric inconsistencies commonly observed in depth predicted by feed-forward methods. The point clouds visualize the centers of Gaussians. Figure 10. Ablation on triangle representation. Compared to surfels, our representation produces clearer, opaque surfaces and enables finer rendering details. Net++), confirming its effectiveness in reducing redundancy. As shown in Fig. 12, disabling the global map update improves geometric consistency and, consequently, rendering quality. More ablation results are provided in Appendix C.2. Figure 12. Ablation on global map update. Our framework effectively improves the global consistency. 5. Limitations PLANING is modular framework whose components can benefit from future advances in scene representation and rendering. Our current formulation inherits limitations from the chosen primitives and scene assumptions. In particular, neural Gaussian primitives are not well suited for modeling semi-transparent or transparent objects, where unreliable appearance gradients may adversely affect geometry optimization. Moreover, the framework focuses on surface modeling Table 4. Ablation studies on the ScanNetV2 dataset. We conduct ablation studies on the hybrid representation and framework design, evaluating performance across both geometric and appearance metrics. Setting Ours Geometry # Primitives Ch-L2 F-score PSNR SSIM LPIPS (#Geo/#GS) Rendering 5.68 62.15 28.83 0.882 0.222 56.1k/222.2k w/o triangles w/o hybrid 5.90 6.06 w/o spatial filtering 6.01 w/o global map update 6.20 59.85 57.54 58.86 56.00 28.44 0.876 0.232 52.8k/157.3k 28.48 0.877 0. -/621.5k 28.66 0.880 0.213 211.5k/625.7k 28.33 0.877 0.229 55.3k/166.6k : w/o geometric primitives. and does not explicitly handle sky or distant background regions in outdoor scenes, which can lead to inconsistent initialization and degraded appearance quality. Addressing these limitations is significant bonus in practice and left as future work. 6. Conclusion PLANING addresses fundamental limitation of existing streaming Gaussian-based reconstruction frameworks: the absence of robust and compact anchoring geometry that does not compromise appearance modeling. By introducing loosely coupled triangleGaussian representation together with streaming-aware optimization framework, PLANING decouples geometry from appearance while preserving highfidelity rendering. This design help resolve long-standing issues of geometric drift, redundancy, and instability in onthe-fly reconstruction that arise from conflicts between accurate geometry and appearance modeling. PLANING enables efficient, structurally robust streaming reconstruction, and further showcases its potential for simulation-ready 3D scene assets suitable for wide range of downstream applications."
        },
        {
            "title": "References",
            "content": "[1] Nathaniel Burgdorfer and Philippos Mordohai. Radiant triangle soup with soft connectivity forces for 3d reconstruction and novel view synthesis. arXiv preprint arXiv:2505.23642, 2025. 3 [2] Roberto Caldara and Sébastien Miellet. map: novel method for statistical fixation mapping of eye movement data. Behavior research methods, 43(3):864878, 2011. 3 [3] Carlos Campos, Richard Elvira, Juan Gómez Rodríguez, José MM Montiel, and Juan Tardós. Orb-slam3: An accurate open-source library for visual, visualinertial, and multimap slam. IEEE transactions on robotics, 37(6):18741890, 2021. 3 [4] Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, and Guofeng Zhang. Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction. IEEE Transactions on Visualization and Computer Graphics, 2024. 6, 15 [5] Chong Cheng, Sicheng Yu, Zijian Wang, Yifan Zhou, and Hao Wang. Outdoor monocular slam with global scale-consistent 3d gaussian pointmaps. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26035 26044, 2025. 2, 6 [6] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 6, 16 [7] Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, and Weiwei Xu. High-quality surface reconstruction using gaussian surfels. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 3 [8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark In 2012 IEEE conference on computer vision and suite. pattern recognition, pages 33543361. IEEE, 2012. 6, 16 [9] Antoine Guédon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53545363, 2024. 6 [10] Seongbo Ha, Jiung Yeon, and Hyeonwoo Yu. Rgbd gs-icp slam. In European Conference on Computer Vision, pages 180197. Springer, 2024. 3 [11] Jan Held, Sanghyun Son, Renaud Vandeghen, Daniel Rebain, Matheus Gadelha, Yi Zhou, Anthony Cioppa, Ming Lin, Marc Van Droogenbroeck, and Andrea Tagliasacchi. Meshsplatting: Differentiable rendering with opaque meshes. arXiv preprint arXiv:2512.06818, 2025. 3, 6, [12] Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deliege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, and Marc Van Droogenbroeck. 3d convex splatting: Radiance field rendering with 3d smooth convexes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2136021369, 2025. 3, 4 [13] SA Hojjatoleslami and Josef Kittler. Region growing: new IEEE Transactions on Image processing, 7(7): approach. 10791084, 1998. 15 [14] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 3, 4, 6, 14, 15 [15] Changjian Jiang, Kerui Ren, Linning Xu, Jiong Chen, Jiangmiao Pang, Yu Zhang, Bo Dai, and Mulin Yu. Halogs: Loose coupling of compact geometry and gaussian splats for 3d scenes. arXiv preprint arXiv:2505.20267, 2025. 3 [16] Lihan Jiang, Kerui Ren, Mulin Yu, Linning Xu, Junting Dong, Tao Lu, Feng Zhao, Dahua Lin, and Bo Dai. Horizon-gs: Unified 3d gaussian splatting for large-scale aerial-to-ground scenes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2678926799, 2025. 3 [17] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat track & map 3d gaussians for dense rgb-d slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2135721366, 2024. 2, 3 [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 4 [19] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 5, 6, 15 [20] Guanghao Li, Kerui Ren, Linning Xu, Zhewen Zheng, Changjian Jiang, Xin Gao, Bo Dai, Jian Pu, Mulin Yu, and Jiangmiao Pang. Artdeco: Towards efficient and high-fidelity on-the-fly 3d reconstruction with structured scene representation. arXiv preprint arXiv:2510.08551, 2025. 2, 5, 6, 7, 14, 15 [21] Haotong Lin, Sili Chen, Junhao Liew, Donny Chen, Zhenyu Li, Guang Shi, Jiashi Feng, and Bingyi Kang. Depth anything 3: Recovering the visual space from any views. arXiv preprint arXiv:2511.10647, 2025. 3 [22] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347 353. 1998. [23] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 2, 5, 6 [24] Xiaoyang Lyu, Yang-Tian Sun, Yi-Hua Huang, Xiuzhe Wu, Ziyi Yang, Yilun Chen, Jiangmiao Pang, and Xiaojuan Qi. 3dgsr: Implicit surface reconstruction with 3d gaussian splatting. ACM Transactions on Graphics (TOG), 43(6):112, 2024. 3 [25] Hidenobu Matsuki, Riku Murai, Paul HJ Kelly, and Andrew Davison. Gaussian splatting slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1803918048, 2024. 2, 3, 6 [26] Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, and George Drettakis. On-the-fly reconstruction for large-scale novel view synthesis from unposed images. ACM Transactions on Graphics (TOG), 44(4):114, 2025. 2, 3, 5, 6, 7 [27] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [28] Raul Mur-Artal and Juan Tardós. Orb-slam2: An opensource slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics, 33(5):12551262, 2017. [29] Riku Murai, Eric Dexheimer, and Andrew Davison. Mast3rslam: Real-time dense slam with 3d reconstruction priors. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1669516705, 2025. 3, 5 [30] Zhenghao Qi, Shenghai Yuan, Fen Liu, Haozhi Cao, Tianchen Deng, Jianfei Yang, and Lihua Xie. Air-embodied: An efficient active 3dgs-based interaction and reconstruction framework with embodied large language model. arXiv preprint arXiv:2409.16019, 2024. 2 [31] Tong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: robust and versatile monocular visual-inertial state estimator. IEEE transactions on robotics, 34(4):10041020, 2018. 3 [32] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 3 [33] Xuqian Ren, Matias Turkulainen, Jiepeng Wang, Otto Seiskari, Iaroslav Melekhov, Juho Kannala, and Esa Rahtu. Ags-mesh: Adaptive gaussian splatting and meshing with geometric priors for indoor room reconstruction using smartphones. In International Conference on 3D Vision (3DV), 2025. [34] Johannes Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In European conference on computer vision, pages 501518. Springer, 2016. 3 [35] Christian Sigg, Tim Weyrich, Mario Botsch, and Markus Gross. Gpu-based ray-casting of quadratic surfaces. In PBG@ SIGGRAPH, pages 5965, 2006. 14 [36] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. 6 [37] Bin Tan, Rui Yu, Yujun Shen, and Nan Xue. Planarsplatting: Accurate planar surface reconstruction in 3 minutes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 11901199, 2025. 3, 6 [38] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 3 [39] Yifan Wang, Jianjun Zhou, Haoyi Zhu, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Jiangmiao Pang, Chunhua Shen, and Tong He. π3: Permutation-equivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025. 3, [40] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [41] Jamie Watson, Filippo Aleotti, Mohamed Sayed, Zawar Qureshi, Oisin Mac Aodha, Gabriel Brostow, Michael Firman, and Sara Vicente. Airplanes: Accurate plane estimation via 3d-consistent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52705280, 2024. 6 [42] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Aljaž Božiˇc, et al. Vr-nerf: Highfidelity virtualized walkable spaces. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 6, 16 12 [43] Yandan Yang, Baoxiong Jia, Peiyuan Zhi, and Siyuan Huang. Physcene: Physically interactable 3d scene synthesis for emIn Proceedings of the IEEE/CVF Conference bodied ai. on Computer Vision and Pattern Recognition, pages 16262 16272, 2024. 2 [44] Keyang Ye, Tianjia Shao, and Kun Zhou. When gaussian meets surfel: Ultra-fast high-fidelity radiance field rendering. ACM Transactions on Graphics (TOG), 44(4):115, 2025. 3 [45] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 6, [46] Mulin Yu and Florent Lafarge. Finding good configurations of planar primitives in unorganized point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63676376, 2022. 14 [47] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. Gsdf: 3dgs meets sdf for improved neural rendering and reconstruction. Advances in Neural Information Processing Systems, 37:129507129530, 2024. 3 [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 6 [49] Wei Zhang, Tiecheng Sun, Sen Wang, Qing Cheng, and Norbert Haala. Hi-slam: Monocular real-time dense mapping with hybrid implicit fields. IEEE Robotics and Automation Letters, 9(2):15481555, 2023. 3 [50] Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, and Norbert Haala. Hi-slam2: Geometry-aware gaussian slam for fast monocular scene reconstruction. IEEE Transactions on Robotics, 41:64786493, 2025. 2, 3 [51] Youmin Zhang, Fabio Tosi, Stefano Mattoccia, and Matteo Poggi. Go-slam: Global optimization for consistent 3d instant reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 37273737, 2023. 3 [52] Ziyu Zhang, Binbin Huang, Hanqing Jiang, Liyang Zhou, Xiaojun Xiang, and Shuhan Shen. Quadratic gaussian splatting: High quality surface reconstruction with second-order geometric primitives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2826028270, 2025. 3 [53] Chunran Zheng, Wei Xu, Zuhao Zou, Tong Hua, Chongjian Yuan, Dongjiao He, Bingyang Zhou, Zheng Liu, Jiarong Lin, Fangcheng Zhu, et al. Fast-livo2: Fast, direct lidar-inertialvisual odometry. IEEE Transactions on Robotics, 2024. [54] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin Oswald, and Marc Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1278612796, 2022. 3 13 The following appendices provide additional technical details and experimental results that support the main findings of this work. They include descriptions of the technical details of our method (Sec. A), application details and implementation (Sec. B), and additional experimental results (Sec. C). A. Technical Details A.1. Differentiable Triangle Rasterizer To enable unbiased depth and normal rendering with triangle primitives, we adopt an explicit rayprimitive intersection strategy [35], following 2DGS [14]. We define the transformation from triangles local coordinate system to world space as = (cid:20)sutu 0 svtv 0 (cid:21) 0 µ 1 0 , (11) where µ, su, tu, sv, and tv follow the definition of the local triangle frame in Eq. 1. When combined with the edge-preserving contribution function (Eq. 2), this formulation leads to two practical challenges: (i) inaccurate depth sorting for large triangles whose barycenters deviate from true raytriangle intersections; and (ii) incorrect visibility estimation when triangle barycenters are occluded while portions of the triangle remain visible. To address these issues, we propose subdivision-aware forward rendering pipeline that integrates adaptive triangle subdivision for robust depth sorting and vertex-based visibility criterion for accurate occlusion handling. The complete procedure is summarized in Algorithm 1. A.2. Training Strategy In our streaming reconstruction system, we adopt staged training strategy to balance efficiency and reconstruction quality, following [20]. Specifically, when keyframe is encountered, new primitives are initialized and the scene is optimized for iterations (set to 20 in our implementation), while common frames are optimized for only M/2 iterations without adding new Gaussians. Training frames are sampled with probability of 0.2 from the current frame and 0.8 from past frames to mitigate local overfitting. After processing the sequence in streaming fashion, global optimization is performed over all frames, prioritizing those with fewer prior updates. A.3. Planar Primitive Extraction Planar primitives provide an efficient structural abstraction of the scene and can be directly leveraged in downstream tasks, such as robot local motion training. To extract these planes, we adopt coarse-to-fine strategy based on GoCoPP [46], where the method is applied iteratively with progressively ALGORITHM 1: Subdivision-aware Forward Rendering Input: Triangle soup , camera pose W, screen resolution Output: Rendered depth and normal maps Triangle Preprocessing: Initialize visible triangle set Tv foreach triangle do if at least one vertex of is visible then Construct local triangle frame and transformation ; Subdivide recursively until all edges are shorter than threshold ϵ ;"
        },
        {
            "title": "Assign parent triangle ID to all subdivision",
            "content": "triangles ; Add subdivision triangles to Tv ; end end Subdivision Processing: foreach subdivision triangle ts Tv do if at least one vertex of ts is visible then Project vertices to image plane ; Determine overlapped tiles ; Compute view-space depth using barycenter of ts ; Generate sorting key (depth, tile ID) ; end end Depth Sorting: Perform GPU-based radix sort on all subdivision triangles using sorting keys ; Rendering: foreach pixel = (x, y)T do Define the camera ray using two orthogonal homogeneous planes; Transform rays into local triangle coordinates using (WH)T ; Compute raytriangle intersection ˆx on the original triangle ; Evaluate rendering contribution using Eq. 2 ; end Render depth and normal images following Eq. 4 in the main text ; finer parameters to detect smaller planes from the residual points remaining after coarser planes are extracted. A.4. More Implementation Details For our method, we set Kmin = 4 and Kmax = 8, with loss weights λd = 10.0, λn = 3.0, λo = 0.2, λc = 0.2, and λs = 0.01. For dense mesh extraction, our method fuses triangle14 28.8 28.7 28.6 P 0.8820 0.8815 0.8810 0.8805 S PSNR SSIM 0. 28.5 100 150 200 250 300 350 400 Number of Gaussians (k) Figure 13. Effect of the number of Gaussians on rendering quality. PSNR and SSIM improve initially and then saturate as Gaussian count increases. rendered depth maps into meshes using TSDF, following the procedure in 2DGS [14]. For per-scene methods, geometric priors are incorporated according to the parameterization in AGS-Mesh [33], which provides comprehensive study of geometric prior integration. For planar primitive extraction, since baseline methods, including 2DGS [14], PGSR [4], MeshSplatting [11], and ARTDECO [20], typically output dense meshes, we extract multi-level planar shapes from their results using the same strategy and parameters applied to our method to ensure fair comparison. All experiments are performed on an Intel Core i9-14900K CPU and an NVIDIA RTX 4090 GPU. B. Application Details B.1. Plane-Guided Camera Pose Optimization In our streaming reconstruction system, we optionally feed back the reconstructed planar map to the frontend to refine camera poses via point-to-plane alignment loss, improving global consistency. Specifically, in the mapper, we maintain voxel map using spatial hash to manage triangle primitives. During training, planar primitives are regularly extracted via region growing [13]. In our implementation, the voxel size is set to 3 cm, and plane extraction is performed every 10 frames. The extracted plane parameters and associated voxel keys are then shared with the frontend. In the frontend, high-confidence points predicted by MASt3R [19] are associated with the planar map via the voxel grid. For each point and its corresponding plane, we adopt simple yet effective point-to-plane alignment loss: Lp = (p c) n1, (12) where and denote the planes normal and center, respectively. Table 5. Comparison of scene import and conversion time under non-headless (GUI-based) and headless Isaac Sim pipelines. Both settings perform the same sequence of operations, including mesh import, collision geometry construction, and USD packaging, but differ in execution mode. Setting Non-headless (s) Headless (s) # Primitives Ours (Plane) 89.73 2DGS 2DGS 120.00 5.27 657 37.21 17k 277k 17k : impractical runtime (> 30,min), *: 16 mesh simplification. B.2. Large Scale Scene Reconstruction To enable large-scale scene reconstruction and alleviate GPU memory limitations, we introduce dynamic loading strategy that swaps primitive parameters between the GPU and CPU. Specifically, we periodically evaluate the projected scale of each neural Gaussian on the most recent image plane. neural Gaussian is marked as invisible if its projected scale is smaller than pixel, and triangle is marked as invisible when all its associated neural Gaussians are invisible. Invisible triangles and their corresponding Gaussians are then offloaded from the GPU to the CPU. Upon detecting loop closure in the Backend module, we reload the primitives initialized from the associated images from the CPU back to the GPU. This dynamic loading strategy allows our framework to efficiently scale to large environments. B.3. Locomotion Strategy Training Beyond visual fidelity, our hybrid representation facilitates downstream embodied tasks by providing the geometric consistency essential for stable contact dynamics in physicsbased locomotion. Unlike appearance-driven methods that generate redundant primitives, our approach prioritizes largescale, load-bearing structures such as floors and walls. This results in highly compact representation that significantly reduces triangle counts while preserving the structural integrity required for high-fidelity simulation and reinforcement learning. While 2DGS [14] serves as strong baseline, it produces highly complex meshes that incur significant preprocessing overhead in Isaac Sim. In practice, 2DGS scene (277k faces) requires over 30 minutes for standard import and conversion. By contrast, our lower mesh complexity circumvents these bottlenecks, consistently leading to faster processing in both standard and headless (convert_mesh) pipelines. Quantitative comparisons are reported in Tab. 5. To evaluate the impact of geometric reconstruction quality on policy learning, we conduct locomotion experiments in Isaac Lab using the Unitree H1 humanoid and the Unitree A1 quadruped. We first consider setting without height scanner to enforce reliance on the physical correctFigure 14. More geometric comparison results. We visualize planar reconstruction and geometric modeling across different primitives, with 2DGS shown as dense mesh for comparison. Overall, our method preserves planar structures while capturing fine geometric details. ness of the simulated geometry. Under this configuration, policies trained in 2DGS scenes after aggressive mesh simplification fail to converge due to degraded planar geometry, whereas policies trained in our reconstructed scenes consistently achieve stable locomotion under identical observation settings. Overall, our method enables the construction of simulation environments that are both geometrically accurate and compact. By reducing the real-to-sim gap, our approach provides practical foundation for efficient downstream locomotion policy training and deployment. Table 6. Ablation studies on the ScanNet++ dataset. Our ablations are divided into two categories: representation design and framework design. Setting Ours w/o triangles w/o hybrid w/o spatial filtering w/o global map update Geometry # Primitives Chamfer F-score PSNR SSIM LPIPS (#Geo/#GS) Rendering 3.53 3.63 3. 3.71 3.59 86.88 31.91 0.941 0.133 61.6k/291.0k 84.91 83.01 84.94 85.07 31.05 0.932 0.150 39.4k/201.9k 31.60 0.940 0. -/478.3k 31.95 0.942 0.126 233.6k/981.6k 31.78 0.941 0.131 61.5k/291.4k C. Supplementary Experiments : w/o geometric primitives. C.1. Supplementary Comparison Experiments In Fig. 14, we provide additional reconstruction comparisons on the ScanNet++ [45] and ScanNetV2 [6] datasets, showing that our method more faithfully preserves the scenes geometric structures. Fig. 15 presents rendering quality comparisons on the KITTI [8] and VR-NeRF [42] datasets, demonstrat16 Figure 15. More rendering comparison results. White boxes highlight artifacts and fine-grained details from baseline methods. Our approach yields significantly sharper results on intricate structures, such as text, while achieving superior overall rendering quality. ing our methods robustness and applicability across diverse scenarios, including both indoor and outdoor environments. C.2. Supplementary Ablation Studies We also conduct ablation studies on ScanNet++, using the same experimental setup as in the main text. As shown in Tab. 6, our representation design achieves the best performance in both rendering quality and geometric accuracy. Regarding system design, by enabling spatial filtering, our method can achieve higher geometric accuracy and comparable rendering quality with less than one-third of the primitives, highlighting the efficiency of our framework. Furthermore, we investigate the effect of the number of Gaussians on rendering quality. As shown in Fig. 13, PSNR and SSIM initially improve with increasing Gaussian count and then saturate."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong",
        "The University of Science and Technology of China",
        "Zhejiang University"
    ]
}