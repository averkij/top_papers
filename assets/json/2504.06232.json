{
    "paper_title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance",
    "authors": [
        "Jiazi Bu",
        "Pengyang Ling",
        "Yujie Zhou",
        "Pan Zhang",
        "Tong Wu",
        "Xiaoyi Dong",
        "Yuhang Zang",
        "Yuhang Cao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, a training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes a virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlow's superiority in achieving superior high-resolution image quality over current state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance Jiazi Bu1,5 Pengyang Ling2,5 Yujie Zhou1,5 Pan Zhang5 Xiaoyi Dong3,5 Yuhang Zang5 Yuhang Cao5 Tong Wu4 Dahua Lin3,5,7 Jiaqi Wang5,6 1Shanghai Jiao Tong University 2University of Science and Technology of China 3The Chinese University of Hong Kong 4Stanford University 5Shanghai AI Laboratory 6Shanghai Innovation Institute 7CPII under InnoHK https://github.com/Bujiazi/HiFlow 5 2 0 2 8 ] . [ 1 2 3 2 6 0 . 4 0 5 2 : r Figure 1: Gallery of HiFlow. The proposed HiFlow enables pre-trained text-to-image diffusion models (Flux.1.0-dev in this figure) to synthesize high-resolution images with high fidelity and rich details in training-free manner. Best view zoomed in. Equal contribution. Corresponding author. Preprint. Under review."
        },
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion/flow models have drawn considerable attention recently due to their remarkable ability to deliver flexible visual creations. Still, high-resolution image synthesis presents formidable challenges due to the scarcity and complexity of high-resolution content. To this end, we present HiFlow, training-free and model-agnostic framework to unlock the resolution potential of pre-trained flow models. Specifically, HiFlow establishes virtual reference flow within the high-resolution space that effectively captures the characteristics of low-resolution flow information, offering guidance for high-resolution generation through three key aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. By leveraging this flow-aligned guidance, HiFlow substantially elevates the quality of high-resolution image synthesis of T2I models and demonstrates versatility across their personalized variants. Extensive experiments validate HiFlows superiority in achieving superior high-resolution image quality over current state-of-the-art methods."
        },
        {
            "title": "Introduction",
            "content": "The text-to-image (T2I) diffusion/flow models [38; 34; 33; 4; 8; 32; 33; 39; 26; 43; 2], which allow for flexible content creation from textual prompts, have recently achieved landmark advancement. Despite considerable improvements, existing T2I models are typically confined to restricted resolution (e.g., 1024 1024) and experience notable quality decline and even structural breakdown when attempting to generate higher-resolution images, as illustrated in Fig. 2. Such shortcoming limits the utility and diminishes their appeal to contemporary artistic and commercial applications, in which detail and precision are paramount. As initial efforts, several methods [15; 20; 30; 37; 44; 50] suggest fine-tuning T2I models on higherresolution samples to enhance the adaptability to large-scale images. However, this straightforward approach entails significant costs, primarily the burden of high-resolution image collection and the necessity for model-specific fine-tuning. Therefore, recent studies have investigated training-free strategies [16; 10; 22; 3; 23; 49; 25; 24; 35; 11; 42] to harness the inherent potential of pre-trained T2I models in high-resolution image synthesis. The majority of these methods [16; 22; 49; 35; 23] involve manipulating the internal features within models, exhibiting restricted transferability across architectures, such as applying methods tailored for U-Net architecture in DiT-based models. Some methods [10; 24; 42] suggest fusing the synthesized low-resolution images into the denoising target during high-resolution synthesizing for structure guidance. Despite their simplicity, these methods only utilize low-resolution sampling endpoints, and the potential of sampling trajectory is underexplored, producing suboptimal productions that lack in detail. Recently, I-Max [11] introduces projected flow to guide the high-resolution generation of Rectified Flow Transformers. Nevertheless, it treats the upsampled low-resolution image as the endpoint of the ideal projected flow at each denoising timestep, imposing overly restrictive conditions on high-resolution generation that severely limit detail creation. In this work, we introduce novel training-free and model-agnostic generation framework, termed HiFlow, which is designed to advance high-resolution T2I synthesis of Rectified Flow models and can be seamlessly extended to diffusion models by modifying the denoising scheduler. Specifically, HiFlow involves cascade generation paradigm: First, virtual reference flow is constructed in the high-resolution space based on the step-wise estimated clean samples of the low-resolution sampling flow. Then, during high-resolution synthesizing, the reference flow offers guidance from sampling initialization, denoising direction, and moving acceleration, aiding in achieving consistent lowfrequency patterns, preserving structural features, and maintaining high-fidelity details. Such flowaligned guidance from the sampling trajectory facilitates better merging of the structure synthesized at the low-resolution scale and the details synthesized at the high-resolution scale, enabling superior generation. Furthermore, it exhibits broad generalizability across U-Net and DiT architectures, owing to its independence from internal model characteristics. Extensive experiments demonstrate that the proposed method surpasses state-of-the-art baselines on the latest Rectified Flow T2I model and is 2 Figure 2: T2I models suffer significant quality degradation in high-resolution generation. even comparable to some training-based methods [37] and leading closed-source T2I commercial models like DALLE 3 [1] and Flux-Pro [13]. The main contribution can be summarized as follows: We introduce HiFlow, novel training-free and model-agnostic framework to unlock the resolution potential of pre-trained Rectified Flow models via flow-aligned guidance in the high-resolution space. We present an innovative perspective of flow guidance that constructs virtual reference flow in the high-resolution space and aligns the actual high-resolution sampling flow with it from three unique aspects: initialization, direction, and acceleration. We conduct comprehensive experiments, including comparison with variety of state-ofthe-art baselines, along with applications including LoRA, ControlNet, and Quantization, demonstrating the effectiveness and versatility of HiFlow."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Text-to-image generation. Text-to-image (T2I) synthesis [38; 34; 33; 4; 8; 32; 39; 26; 43; 2] has witnessed significant advancements with the advent of diffusion models, which have demonstrated remarkable capabilities in producing high-quality images based on given textual prompts. Denoising Diffusion Probabilistic Models (DDPM) [18] and Guided Diffusion [7] showcased the potential of diffusion processes to generate high-fidelity images. Subsequently, the introduction of latent space diffusion [38] marked revolutionary advancement in the field, which significantly reduced computational demands and enabled more efficient training, giving rise to pioneering models such as Stable Diffusion and Stable Diffusion XL [34]. Recently, the integration of transformer-based architectures [5; 4; 12; 14; 51; 2] into diffusion models has also led to improvements in both image quality and computational efficiency. In this work, we primarily construct our method upon Flux.1.0-dev [2], an advanced Rectified Flow T2I backbone renowned for its superior generation quality. 2.2 High-resolution image generation. High-resolution image generation with diffusion models has gained increasing popularity in recent years. Several studies [15; 20; 30; 37; 44; 50] have proposed training or fine-tuning existing T2I diffusion models to enhance their capability for high-resolution image generation. However, it remains challenging task due to the scarcity of high-resolution training data and the substantial computational resources required for modeling such data. Another line of research [47; 9; 28; 45; 6; 46] employs super-resolution techniques to upscale the resolution of generated base-resolution images in workflow manner. Nevertheless, the quality of generated images via this approach heavily depends on the initial quality of the base-resolution images and the performance of the super-resolution models. Recent efforts have focused on training-free strategies [16; 10; 22; 3; 23; 49; 25; 24; 35; 11; 42] that modify the inference strategies of diffusion models for high-resolution generation. For instance, 3 HiDiffusion [49] suggests reshaping features in the outermost blocks of the U-Net architecture to match the training size in deeper blocks. DiffuseHigh [24] upscales the base-resolution generation result and re-denoises it under the structural guidance of the Discrete Wavelet Transform (DWT). FreeScale [35] combines cascade upscaling strategy with scale attention fusion to achieve ultrahigh-resolution generation. I-Max [11] projects the high-resolution flow into the low-resolution space, enabling training-free high-resolution image generation with Rectified Flow Transformers for the first time. However, their results exhibit unsatisfactory visual quality or limited performance on the Rectified Flow T2I models due to the lack of effective flow guidance during high-resolution generation. Our method effectively aligns the high-resolution sampling flow with the reference flow, guiding the high-resolution flow to synthesize rich and reasonable details while maintaining coherent structure, thereby achieving high-resolution generation with superior image quality."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Flow Matching [29] and Rectified Flow [31] aim at streamlining the formulation of Ordinary Differential Equation (ODE) models by establishing linear transition between two distinct distributions. Consider clean image samples X0 π0 and Gaussian noise X1 π1. Rectified Flow delineates linear trajectory from X1 to X0, with the intermediate state Xt defined as: in which [0, 1] denotes continuous time interval. By taking the derivative of on both sides of Eq. 1, its linear progression gives the following equation: Xt = tX1 + (1 t)X0, (1) dXt = (X1 X0)dt. (2) During the denoising phase, given Xt and time t, neural network vθ is encouraged to estimate the vector of flow, i.e., X1 X0), which can be expressed as: vθ(Xt, t, c) = X1t X0t, (3) in which represents optional control conditions such as textual or image prompts, and X1t and X0t denote the predicted noisy and clear component within Xt, respectively. Since Xt and are known, from Eq. 1, vθ(Xt, t, c) is essentially determined by the predicted clean component X0t, which can be expressed as: vθ(Xt, t, c) = Xt X0t . The progressive denoising of flow models can be formulated as follows: Xti1 = Xti + vθ(Xti, ti, c)(ti1 ti), (4) (5) in which the movement of ti from 1 to 0 represents the trajectory of Xti from gaussin noise X1 to clean sample X0. During denosing, the predicted vθ(Xti, ti, c) determines the denosing direction at each time ti. In the following sections, vt is used to denote vθ(Xti, ti, c) for simplicity. 3.2 Virtual Reference Flow 0 for guidance by fusing its upsampled variant into predicted clean sample high Rectified flow models have shown great promise in advancing high-quality image generations. Yet, these models experience critical quality drop when attempting high-resolution synthesizing, as shown in Fig. 2. To this end, previous work typically leverages the synthesized low-resolution sample low 0t to modify the denoising direction. However, for given flow models, it is observed that there are ignored distribution discrepancies between the predicted clean sample X0t at different t, as shown in Fig. 4 (a). Therefore, the fusion between upsampled low 0t may cause artifacts and thus lead to sub-optimal results, as shown in Fig. 4 (b). To this end, in this work, virtual reference flow is constructed in the high-resolution space that can fully characterize the information of the low-resolution sampling trajectory. Specifically, for reference flow, its noisy image ref and predicted clear image at time are defined as: (cid:8)X ref (cid:9) = (cid:8)X vrt and high , ϕ(X low 0t)(cid:9) , , ref 0t (6) 4 Figure 3: Pipeline of HiFlow. HiFlow constructs reference flow from low-resolution sampling trajectory to offer initiation alignment, direction alignment, and acceleration alignment, enabling flow-aligned high-resolution image generation. ϕ(X low is virtual noisy image in high-resolution space, and ϕ(X low in which vrt 0t) is the upsampled low 0t ref by interpolation function ϕ(). For each time t, the vector of reference flow is (X ref 0t)/t = (X vrt 0t))/t. Indeed, the reference flow employs virtual vrt to construct an imaginary sampling trajectory in high-resolution space that can produce the upsampled low-resolution image low 0 . Such reference flow, while lacking in details, is proficient in content preservation within high-resolution synthesizing, which could serve as valuable guidance. 3.3 Flow-Aligned Guidance Given that rectified flow models prioritize low-frequency components before synthesizing highfrequency details and the low-frequency components between high-resolution images and upsampled low-resolution exhibit little difference, we follow the cascade generation pipeline [24; 35], i.e., first synthesize low-resolution image and mapping its sampling trajectory to high-resolution space to obtain reference flow. This reference flow is then used for flow-guided high-resolution generation, which is analyzed from the following three aspects. The pipeline of HiFlow is illustrated in Fig. 3. Initialization alignment. For given virtual reference flow, the sampling of high-resolution generation starts from the noisy variant of ref 0τ , which can be expressed as: 1 + (1 τ )X ref τ = τ high 0τ , high (7) τ in which high is the sampling initialization of high-resolution generation, high is gaussian noise, and τ is the noise addition ratio. Such initialization alignment allows skipping the early stage in high-resolution generation, thereby maintaining the consistency of high-resolution results and low-resolution images in low-frequent components, while also facilitating higher processing speeds. 1 Direction alignment. While initialization alignment enables low-frequent consistency at the beginning of high-resolution generation, such low-frequent information may be destroyed at subsequent denoising steps due to the limited stability of models in synthesizing high-resolution content (as shown in Fig. 2), resulting in broken structures. Therefore, the direction alignment is designed to preserve structure while adding details. This is achieved by modifying the denoising direction based on reference flow, which can be expressed as: 0t = high 0t + αt[ (cid:101)F(F(X ref 0t) L(D)) (cid:101)F(F(X high 0t) L(D))], ˆX high (8) 5 Figure 4: Observations regarding direction and acceleration. (a) Distribution discrepancy of predicted clean sample X0t from different t. (b) Comparison with constant and time-dependent direction guidance, in which the former exhibits notable artifacts while the latter demonstrates better structure preservation. (c) Visualization of acceleration. (d) Effect of acceleration alignment, validating its role in facilitating high-fidelity details generation. in which and (cid:101)F denote 2D Fast Fourier Transform and its inverse operation, respectively, L() denotes butterworth low-pass filter and is the normalized cutoff frequency, and αt is the direction guidance weight. Essentially, Eq. 8 replaces the low-frequent component of high 0t with that in ref 0t, thus rejecting the updating on low-frequent structures when synthesizing high-frequent details. Different from previous work [24; 10; 42] that rely on single guidance anchor ϕ(X low 0 ), the proposed direction alignment manipulates high 0t by using time-dependent ref 0t, such fusion between samples from the same helps avoids artifacts caused by distribution discrepancy (see Fig. 4 (a)), facilitating superior visual quality. Acceleration alignment. Although the above strategies allow for rich details generation while maintaining structure preservation, the fidelity of the synthesized details risks dropping in some cases, in which unrealistic contents appear, such as repetitive patterns and abnormal texture, as shown in Fig. 4 (d). To this end, acceleration alignment is proposed to enhance detail fidelity. Specifically, the acceleration, which is defined as the second-order derivative of movement Xt, and also denotes the first-order derivation of vector vt, can be expressed as: dvt dt d2Xt dt2 = Furthermore, based on Eq. 4 and Eq. 5, the acceleration in Eq. 9 can be simplified as follows: vti1 vti ti1 ti ati1ti = (9) = . ati1ti = 1 ti1 (Xti1 X0ti1) 1 ti (Xti X0ti) ti1 ti = tiXti + (ti1 ti)(Xti X0ti) tiX0ti1 ti1(Xti X0ti) ti1ti(ti1 ti) (10) = 1 ti1 X0ti1 X0ti ti1 ti . It can be concluded from Eq. 10 that the acceleration depicts the variance of the predicted clean sample X0t between adjacent time t, with time-dependent term 1/t. As shown in Fig. 4 (c), the acceleration primarily captures texture and contour information while also indicating the sequence of content synthesis at each t, i.e., it showcases what content the model is responsible for adding at different t. Therefore, we propose aligning the acceleration of high-resolution generation with that of reference flow to synchronize the models preference for the content synthesis order, facilitating guided detail synthesis both in content and timing. Mathematically, the acceleration alignment is modeled as: (11) in which βt is the acceleration guidance weight. In practical, from Eq. 9, this guidance process can be implemented as: ti1ti), ti1ti + βt(aref ti1ti = ahigh ˆahigh ahigh ti1ti ti1 = vhigh ˆvhigh ti1 + βt(vref ti1 vref ti vhigh ti1 + vhigh ti ). (12) 6 Figure 5: Visual comparison of synthesized 2K and 4K images. The proposed HiFlow yields highresolution images characterized by high-fidelity details and coherent structure. Best view zoomed in. Then, the updated ˆvhigh is substituted into Eq. 5 to sample the next noisy latent."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Details Experimental settings. If not specified, the generated images are based on Flux.1.0-dev [2], an advanced open-sourced Rectified Flow model based on DiT architecture. The sampling steps are set as 30, with classifier-free guidance [19] 4.5 for 2K generation and 7.5 for 4K generation, respectively. For initialization alignment, the noise-adding ratio τ is [0.6, 0.3, 0.3] for 1K 2K 3K 4K cascade generation, and the normalized cutoff frequency is = 0.4. For direction and acceleration alignment, the guidance weight is set as αt = βt = t/τ , enabling gradually weakening control. All experiments are produced on single NVIDIA A100 GPU. Baselines. The compared methods consist of various types of methods that enable high-resolution image generation, encompassing training-free approaches (DemoFusion [10], DiffuseHigh [24], I-Max [11]), training-based approaches (UltraPixel [37], Flux-Pro [13], DALLE 3 [1]), and an image 7 Figure 6: Image qualitative comparison to training-based methods. HiFlow demonstrates the capability to generate high-resolution images with quality comparable to leading training-based models (UltraPixel, DALLE 3, and Flux-Pro). Table 1: Quantitative comparison with other baselines. The best result is highlighted in bold, while the second-best result is underlined. * indicates methods migrated from U-Net architecture. ISpatch CLIP Score Resolution (height width) Method IS 2048 2048 (2K) 4096 4096 (4K) DemoFusion* [10] DiffuseHigh* [24] I-Max [11] Flux + BSRGAN [47] UltraPixel [37] HiFlow (Ours) DemoFusion* [10] DiffuseHigh* [24] I-Max [11] Flux + BSRGAN [47] UltraPixel [37] HiFlow (Ours) FID FIDpatch 56.07 61.62 57.57 60.25 59.67 55.39 51.69 50.25 54.56 52.06 49.02 47.70 56.72 62.01 53.27 59.53 60.90 52.55 49.48 50.98 52.93 54.12 47.19 45.01 27.23 26.76 28.84 25.85 28.49 28. 21.17 20.60 22.21 19.32 24.73 24.62 13.48 13.10 12.07 13.39 13.74 13.86 8.49 8.09 7.65 8.87 9.47 9.73 35.05 34.83 34.96 35.34 35.16 35.32 35.27 34.98 35.05 35.37 35.01 35.40 super-resolution method, BSRGAN [47]. For fair comparison, the training-free methods are tested on the same Flux model according to the official implementations. Evaluation. We collect 1K high-quality captions across various scenarios for diverse image generation. Based on these generated images, the CLIP [36] score is used to access the prompt-following capability. Meanwhile, the Frechet Inception Distance [17] (FID) and Inception Score [40] (IS) are calculated between these generated images and 10K real high-quality images (with resolution of at least 1024 1024) sourced from the LAION-High-Resolution dataset [41] to measure image quality. Furthermore, the patch-version FIDpatch and ISpatch are calculated based on local image patches to quantify the quality of synthesized details. 4.2 Comparison to State-of-the-Art Methods Qualitative comparison. Fig. 5 presents the visual comparison of generated 2K and 4K images by different methods. It is observed that DemoFusion and DiffuseHigh, while capable of preserving overall image structure, commonly exhibit lack of detail and diminished contrast, which can be attributed to the absence of guidance during high-resolution generation. I-Max sometimes produces blurred results because it uses upsampled low-resolution images as the ideal projected flow endpoints, leading to an over-alignment with these lower-quality images. Although BSRGAN, an image superresolution method, enhances image clarity to some extent, it struggles to synthesize finer details. In comparison, the proposed HiFlow consistently yields aesthetically pleasing and semantically coherent outcomes. Furthermore, as depicted in Fig. 6, HiFlow delivers high-resolution images of exceptional quality, matching the performance of leading training-based models such as UltraPixel, DALLE 3, and Flux-Pro, underscoring its outstanding generative capabilities. Quantitative comparison. The quantitative results are presented in Tab. 1. It is observed that the proposed HiFlow achieves competitive performance in terms of both image quality (FID, FIDpatch, IS and ISpatch) and image-text alignment (CLIP score) under different resolutions, validating its stable and superior performance in high-resolution image generation. Moreover, as presented in Tab. 2, HiFlow surpasses its training-free competitors in inference speed, offering higher efficiency. 8 Figure 7: Sequential ablation experiment. Best view zoomed in. Table 2: Comparison in latency. Latency (sec.) Resolution Method 20482 40962 DemoFusion* [10] DiffuseHigh* [24] I-Max [11] HiFlow (Ours) DemoFusion* [10] DiffuseHigh* [24] I-Max [11] HiFlow (Ours) 106 59 94 56 972 533 735 379 Table 3: Quantitative results of ablation study. Resolution Method 40962 ga, gs, gi ga, gs ga HiFlow (Ours) ga, gs, gi ga, gs ga HiFlow (Ours) FID 67.87 58.26 58.40 55.39 234.38 56.20 55.36 52. FIDpatch 71.79 54.22 50.38 47.70 198.21 48.30 51.79 45.01 IS 23.47 27.46 28.92 28.67 9.33 19.85 22.78 24.62 ISpatch CLIP 33.81 34.47 34.90 35. 9.16 12.58 13.14 13.86 3.31 6.51 8.77 9.73 11.42 33.64 35.44 35.40 4.3 Ablation Study HiFlow performs guided high-resolution generation by aligning its flow with the reference flow in three aspects: initialization alignment (gi), direction alignment (gd), and acceleration alignment (ga). As can be observed in Fig. 7, initialization alignment helps avoid semantic incorrectness by facilitating low-frequent consistency with low-resolution images. Furthermore, direction alignment enhances structure preservation in the generation process by suppressing the updating in the lowfrequent component. Moreover, acceleration alignment contributes to high-fidelity detail synthesizing, eliminating the production of repetitive patterns in the garden, as shown in the realistic girls face and the appearance of her clothing. The quantitative results of ablation study are shown in Tab. 3. 4.4 Applications Application on customization T2I models. Customization T2I models, which allow tailored generation to match users specific requirements, have recently garnered increasing attention. As illustrated in Fig. 8 (a), the proposed HiFlow is compatible with various customization models based on LoRA [21] or ControlNet [48], highlighting its potential in facilitating personalized high-resolution image generation. Application on quantized T2I models. Given the substantial rise in computational complexity introduced by advanced T2I models, model quantization techniques have been widely explored to decrease computing resource requirements. As shown in Fig. 8 (b), the training-free and modelagnostic attributes of HiFlow allow it to be directly employed with the 4-bit version of Flux (quantized by SVDQuant [27]), thus substantially accelerating high-resolution image generation. Application on U-Net based T2I Models. U-Net-based T2I models, such as SD1.5 and SDXL [34], constitute pivotal parts of T2I diffusion models and have been extensively developed by the community. As depicted in Fig. 8 (c), the integration of HiFlow and SDXL [34] enables the synthesis of realistic high-resolution images, demonstrating its broader applications. 4.5 Limitation and Discussion Despite the advancements of HiFlow in enhancing high-resolution image generation, it faces certain constraints. As training-free method, the generation capability of HiFlow is highly dependent on the quality of the reference flow, which provides flow-aligned guidance in each sampling step. Therefore, the structural irregularities within the reference flow might compromise the quality of the 9 Figure 8: Versatile applications of HiFlow. final production. Fortunately, given its model-agnostic nature, improvements can be anticipated when combining it with more advanced models, bypassing the need for fine-tuning or adjustment."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present HiFlow, tuning-free and model-agnostic framework to enable pre-trained Rectified Flow models to generate high-resolution images with high fidelity and rich details. HiFlow involves novel cascade generation paradigm: (i) virtual reference low is constructed in the high-resolution space based on the step-wise estimated clean samples of the low-resolution sampling trajectory; and (ii) the high-resolution flow is aligned with the reference flow via flow-aligned guidance, which encompasses three aspects: initialization alignment for low-frequency consistency, direction alignment for structure preservation, and acceleration alignment for detail fidelity. Extensive experiments demonstrate that HiFlow outperforms its competitors in high-resolution generation quality and highlight its broad applicability across different model architectures."
        },
        {
            "title": "References",
            "content": "[1] Dalle 3. OpenAI, 2024. [2] Flux.1 ai model. BlackForestLabs, 2024. [3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 10 [4] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1106511074, 2019. [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [8] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [9] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2):295307, 2015. [10] Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. Demofusion: Democratising high-resolution image generation with no $$$. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61596168, 2024. [11] Ruoyi Du, Dongyang Liu, Le Zhuo, Qin Qi, Hongsheng Li, Zhanyu Ma, and Peng Gao. I-max: Maximize the resolution potential of pre-trained rectified flow transformers with projected flow. arXiv preprint arXiv:2410.07536, 2024. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [13] FluxPro. Fluxpro: Ai-powered image generation, 2024. [14] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [15] Lanqing Guo, Yingqing He, Haoxin Chen, Menghan Xia, Xiaodong Cun, Yufei Wang, Siyu Huang, Yong Zhang, Xintao Wang, Qifeng Chen, et al. Make cheap scaling: self-cascade diffusion model for higher-resolution adaptation. In European Conference on Computer Vision, pages 3955. Springer, 2024. [16] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, and Ying Shan. Scalecrafter: Tuning-free higher-resolution visual generation with diffusion models. In The Twelfth International Conference on Learning Representations, 2023. [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [20] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pages 1321313232. PMLR, 2023. [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [22] Linjiang Huang, Rongyao Fang, Aiping Zhang, Guanglu Song, Si Liu, Yu Liu, and Hongsheng Li. In European Fouriscale: frequency perspective on training-free high-resolution image synthesis. Conference on Computer Vision, pages 196212. Springer, 2024. [23] Zhiyu Jin, Xuli Shen, Bin Li, and Xiangyang Xue. Training-free diffusion model adaptation for variablesized text-to-image synthesis. Advances in Neural Information Processing Systems, 36, 2024. [24] Younghyun Kim, Geunmin Hwang, Junyu Zhang, and Eunbyung Park. Diffusehigh: Training-free progressive high-resolution image synthesis through structure guidance. arXiv preprint arXiv:2406.18459, 2024. [25] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. Advances in Neural Information Processing Systems, 36:5064850660, 2023. [26] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [27] Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [28] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 18331844, 2021. 11 [29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [30] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [33] Pablo Pernias, Dominic Rampas, Mats Richter, Christopher Pal, and Marc Aubreville. Würstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv preprint arXiv:2306.00637, 2023. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [35] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via tuning-free scale fusion. arXiv preprint arXiv:2412.09626, 2024. [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [37] Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks. arXiv preprint arXiv:2407.02158, 2024. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [40] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [42] Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. Resmaster: Mastering high-resolution image generation via structural and fine-grained guidance. arXiv preprint arXiv:2406.16476, 2024. [43] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu. Freeu: Free lunch in diffusion u-net. In CVPR, 2024. [44] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. [45] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind superresolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 19051914, 2021. [46] Tong Wu, Jiaqi Wang, Xingang Pan, Xudong Xu, Christian Theobalt, Ziwei Liu, and Dahua Lin. Voxurf: Voxel-based efficient and accurate neural surface reconstruction. In International Conference on Learning Representations (ICLR), 2023. [47] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 47914800, 2021. [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [49] Shen Zhang, Zhaowei Chen, Zhenyu Zhao, Zhenyuan Chen, Yao Tang, Yuhao Chen, Wengang Cao, and Jiajun Liang. Hidiffusion: Unlocking high-resolution creativity and efficiency in low-resolution trained diffusion models. arXiv preprint arXiv:2311.17528, 2023. [50] Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, and Hang Xu. Anysize-diffusion: Toward efficient text-driven synthesis for any-size hd images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 75717578, 2024. [51] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024."
        }
    ],
    "affiliations": [
        "CPII under InnoHK",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}