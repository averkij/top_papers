{
    "paper_title": "GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching",
    "authors": [
        "Guinan Su",
        "Li Shen",
        "Lu Yin",
        "Shiwei Liu",
        "Yanwu Yang",
        "Jonas Geiping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in deployment and inference. While structured pruning of model parameters offers a promising way to reduce computational costs at deployment time, current methods primarily focus on single model pruning. In this work, we develop a novel strategy to compress models by strategically combining or merging layers from finetuned model variants, which preserves the original model's abilities by aggregating capabilities accentuated in different finetunes. We pose the optimal tailoring of these LLMs as a zero-order optimization problem, adopting a search space that supports three different operations: (1) Layer removal, (2) Layer selection from different candidate models, and (3) Layer merging. Our experiments demonstrate that this approach leads to competitive model pruning, for example, for the Llama2-13B model families, our compressed models maintain approximately 97.3\\% of the original performance while removing $\\sim25\\%$ of parameters, significantly outperforming previous state-of-the-art methods. The code is available at https://github.com/Guinan-Su/auto-merge-llm."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 0 8 4 0 2 . 6 0 5 2 : r GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching Guinan Su1, Li Shen5, Lu Yin6, Shiwei Liu7, Yanwu Yang4, Jonas Geiping1,2,3 1Max Planck Institute for Intelligent Systems, 2ELLIS Institute Tübingen 3Tübingen AI Center, 4University of Tübingen, 5Sun Yat-sen University 6University of Surrey, 7University of Oxford"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with substantial model size, which presents significant challenges in deployment and inference. While structured pruning of model parameters offers promising way to reduce computational costs at deployment time, current methods primarily focus on single model pruning. In this work, we develop novel strategy to compress models by strategically combining or merging layers from finetuned model variants, which preserves the original models abilities by aggregating capabilities accentuated in different finetunes. We pose the optimal tailoring of these LLMs as zero-order optimization problem, adopting search space that supports three different operations: (1) Layer removal, (2) Layer selection from different candidate models, and (3) Layer merging. Our experiments demonstrate that this approach leads to competitive model pruning, for example, for the Llama213B model families, our compressed models maintain approximately 97.3% of the original performance while removing 25% of parameters, significantly outperforming previous state-of-the-art methods. The code is available at."
        },
        {
            "title": "Introduction",
            "content": "The unique strengths of modern Large Language Models (LLMs) in language understanding, generation, and reasoning [1, 2, 3] are inextricably linked to their immense size. Research in this field has generally followed trajectory of scaling model parameters and data to enhance performance, guided by two fundamental principles: scaling laws, which establish that performance improves predictably with increased parameters [4, 5, 6], and over-parameterization theory, which demonstrates that models with excess parameters achieve better optimization and generalization [7, 8, 9]. These principles have led researchers to develop billion-parameter architectures delivering unprecedented performance across diverse language tasks. Despite these impressive capabilities, deploying LLMs presents significant challenges due to their substantial computational demands. Various post-training techniques have been proposed to address the issues faced when deploying models to consumer GPUs or local devices, or when reducing costs, including model pruning [10, 11, 12, 13, 14], knowledge distillation into smaller models [15, 16, 17, 18], and quantization of weights [19, 20, 21]. While quantization reduces parameter 1https://github.com/Guinan-Su/auto-merge-llm Correspondence to: shenli6@mail.sysu.edu.cn Preprint. Under review. Figure 1: Our Approach: Model Pruning through Cutting and Stitching. We achieve competitive model pruning performance by running zero-order search that tailors layers based on shared pool of finetuned variants of the original model, selecting and stitching layers if necessary. The model finetunes accentuate task-specific skills, allowing us to merge key components into smaller model, maintaining, for example, 97% of capabilities of Llama-13B, even after 25% reduction in layers. precision but requires specific hardware support, and knowledge distillation necessitates costly retraining of smaller models, structured pruning offers more flexible and hardware-agnostic approach by eliminating redundant parameters to decrease computation costs. Existing pruning methods typically focus on pruning individual models through manually designing metrics that assess the importance of specific structures or layers based on hidden state changes or gradient information [13, 22, 14]. However, these approaches inevitably cause performance degradation and require additional post-training with full parameters to recover performance. To address these limitations, we take radically different perspective and re-formulate structured pruning as the problem of pruning not individual models, but family of task-specific finetuned versions of given model. These finetuned variants are surprisingly helpful for model pruning, as each variant accentuates particular task, such as coding, math, or language understanding. Further, the variants are close enough that model merging can be employed to re-combine layers from multiple variants, if needed [23]. These observations lead us to our main question: Can we develop better compressed models by strategically combining or merging layers from different models? Motivated by this question, we propose novel structured pruning method based on zero-order optimization that supports three different operations to combine layers from different models into smaller, more efficient model: (1) Layer removal, (2) Layer selection from related candidate models, (3) Layer merging. For the optimization, we define multiple objective functions that capture different aspects of model performance across different tasks to better preserve the original models capabilities and run fully data-driven zero-order optimization, instead of relying on expert-made heuristics for pruning. We employ SMAC [24], which strategically allocates computational resources by evaluating configurations at different calibration data sizes, thereby reducing computational costs while boosting the efficiency of finding superior solutions. We rigorously validate our methods effectiveness by evaluating it on Llama-7B and Llama-13B with four state-of-the-art structural pruning methods across comprehensive benchmarks. Our experimental results demonstrate that our approach maintains excellent performance while outperforming existing pruning methods. In summary, the main contributions of this paper are: We propose novel structured pruning method that formulates pruning as zero-order optimization problem over pool of candidate models, enabling automated discovery of efficient models that leverage capabilities from multiple models. We find that this approach allows for cost-effective model pruning stage that is effective without the need for post-training to heal the pruned model. 2 We validate our methods effectiveness through extensive experiments, comparing against modern LLM pruning methods on 14 benchmark tasks. Our method maximally preserves the capabilities of the dense model: 92.2% for the 7B model and 97.3% for the 13B model. significantly outperforming previous state-of-the-art methods."
        },
        {
            "title": "2 Related Work",
            "content": "Compression of Language Models. Large language models [1, 2, 3] necessitate efficient compression methods to reduce parameters and latency. These methods include structural pruning [10, 11, 12, 13, 14], knowledge distillation [15, 16, 18], and quantization [19, 20, 21]. Our work focuses on structural pruning, which removes sub-components from neural networks for hardwarefriendly compression - instead of pruning through sparsification, which requires significant effort to materialize gains on standard hardware. Recent methods focus on pruning via expert-designed criteria, include LLMPruner [14], which removes non-critical structures using gradient information; SliceGPT [25], which replaces weight matrices with smaller ones to reduce dimensions; LaCo [26], which prunes by collapsing the weights of later layers into earlier layers based on activation similarity, and ShortGPT [22], which uses Block Influence (BI) to measure layer importance based on hidden state changes. Unlike these metric-based methods targeting individual models, our approach employs zero-order search, namely hyperparameter optimization to combine pruning and merging across model families. While LaCo also uses layer merging, it focuses only on merging similar layers for single model, whereas we focus on strategically combining or merging layers from different models, which we find to noticeably improve upon within-model merging. Additionally, our approach differs from the weight-sharing NAS-based pruning method [27], requiring costly super-network training; our approach directly optimizes across fine-tuned models, strategically combining layers from diverse variants rather than searching within single model. Model Merging. Model merging enhances capabilities without additional training data or computation. The field evolved from simple weighted parameter averaging [28] that often yielded suboptimal results to advanced techniques like Task Arithmetic [29] which computes differences between model parameters and SLERP [30] which performs interpolation along spherical paths. Later approaches leveraged neural network sparsity, with TIES-Merging [31] selecting parameters based on magnitude while addressing sign conflicts, and DARE [32] combining sparsification with parameter rescaling. Recent advances include Evolutionary model merging [33] optimizing coefficients through evolutionary search, and multi-fidelity approach [34] that enables fine-grained exploration while reducing costs. Our work also builds upon multi-fidelity optimization framework to allow for an efficient search for compressed models."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we provide detailed explanation of our approach. Unlike conventional model compression pipelines, we formulate pruning as zero-order optimization problem over the layers and merging hyperparameters of set of candidate models. We begin in Section 3.1 by outlining our problem formulation and defining the optimization pipeline for pruning with three key components: search space, target objective, and an optimizer. Section 3.2 follows with description of the search spaces. In Section 3.3, we introduce our designed target objective function. Finally, In Section 3.4, we describe our choice of optimization strategy, which efficiently navigates the defined search space to identify optimal pruning configurations. An overview of the pipeline is provided in Figure 1."
        },
        {
            "title": "3.1 Problem Setup",
            "content": "Given pre-trained base model Mbase and set of candidate models = {M1, M2, ..., MK} fine-tuned from the same base model, our goal is to find an optimal pruned model that maximizes performance while adhering to target sparsity constraint. Let denote the target sparsity factor, where [0, 1] indicates the fraction of parameters to be pruned. The pruned model is constructed through combination of layers from candidate models, employing operations such as layer-wise merge, layer selection, and layer removal. These combinations and operations are determined by set of hyperparameters ω Ω, with Ω representing the search space of all possible hyperparameter 3 configurations. Each configuration ω defines specific way to combine the layers from candidate models to form pruned model Mω. The performance of the pruned model can be evaluated using function (Mω), which measures the models effectiveness on specific datasets and tasks. This leads to our optimization problem: ω = arg min ωΩ (Mω) subject to S(Mω) (1) where S() calculates the fraction of pruned parameters in the model compared to the base model, and ω represents the optimal hyperparameter configuration that yields the performing pruned model."
        },
        {
            "title": "3.2 Search Space Design",
            "content": "The search space Ω encompasses all possible pruning configurations that can be applied to construct our pruned model. We formulate this space based on structural layer-wise pruning operations. We aim to support three operations: (1) Layer removal, (2) layer selection, and (3) Layer merging. We designed our search space as follows: Given base model with layers and candidate models fine-tuned from this base model, we design the search space through binary vector = [r1, r2, . . . , rl] where ri {0, 1} indicates whether the i-th layer is retained (ri = 0) or removed (ri = 1), satisfying (cid:80)l i=1 ri = s to achieve our target sparsity s. For each retained layer position i, we define selection vector ci = [ci,1, ci,2, . . . , ci,K] where ci,j {0, 1} indicates whether the layer from the j-th candidate model is selected. If (cid:80)K j=1 ci,j = 0, we retain the layer from the base model instead. When multiple candidate models contribute to layer position (i.e., (cid:80)K j=1 ci,j > 1), we specify merge method mi {1, 2, . . . , Z} from available merging techniques. Each merge method mi is associated with set of hyperparameters hi = [hi,1, hi,2, . . . , hi,Pi ], where Pi is the number of hyperparameters for the specific merge method. These hyperparameters govern the precise mechanism of layer combination, such as interpolation weights or mask ratio parameters. Therefore, complete configuration ω Ω is represented as ω = {r, {ciri = 0}, {miri = 0 and (cid:80)K j=1 ci,j > 1}}. The total cardinality of the search space can be calculated as: Ω = (cid:0) i:ri=0 2K (cid:81) j=1 ci,j >1 hi. which enables wide exploration of pruning j=1 ci,j > 1}, {hiri = 0 and (cid:80)K j=1 ci,j >1 (cid:81) i:ri=0,(cid:80)K i:ri=0,(cid:80)K (cid:1) (cid:81) ls strategies while maintaining the target sparsity constraint."
        },
        {
            "title": "3.3 Target Objective Function",
            "content": "To evaluate the quality of pruned model, we define multi-objective function that measures the models effectiveness across tasks. Specifically, we measure performance on calibration datasets Dcalibration, quantifying metrics such as accuracy for classification tasks or perplexity for language modeling tasks. This provides direct assessment of how well the pruned model preserves the capabilities of the original model. We define multi-task objective function that captures different aspects of model performance across range of tasks to produce comprehensive pruned model. Let = {T1, T2, . . . , Tm} be set of tasks. For pruned model Mω with configuration ω, we employ Pareto Efficient Global Optimization (ParEGO)[35] to identify Pareto-optimal solutions across different objectives. Specifically, the ParEGO algorithm transforms multi-objective optimization problems into series of single-objective problems through scalarization methods: fmulti(Mω, λ) = max i=1,...,m {λi fi(Mω)} + α (cid:88) i= λi fi(Mω) (2) where fi(Mω) is the i-th objective function, λi is the corresponding weight satisfying (cid:80)m i=1 λi = 1 and λi 0, and α is small positive constant (typically set to 0.05). The Chebyshev norm component maxi=1,...,m{λi fi(Mω)} ensures that all non-dominated solutions on the non-convex Pareto front can be identified, while the term α (cid:80)m i=1 λi fi(Mω) enhances the algorithms stability. The final output of our optimizer is Pareto front of pruning configurations, where each configuration represents different trade-off between performance on various tasks. In our experiments, we randomly selected three configurations from this Pareto front and report their results."
        },
        {
            "title": "3.4 Search Optimizer",
            "content": "To efficiently navigate the search space and find optimal pruning configurations, we employ SMAC [24], which strategically allocates computational resources by evaluating configurations at different fidelity levels. we use calibration dataset size as fidelity type, represented by budgets where bmin bmax. Each budget value corresponds to specific portion of the calibration data used for evaluation - smaller budgets (lower fidelity) use fewer samples for faster but less precise evaluations, while larger budgets (higher fidelity) use more samples for slower but more accurate assessments. We use Random Forest [36] as surrogate model to sample new configurations. Given configuration space Ω, minimum budget bmin, maximum budget bmax, reduction factor η and the maximum trials Tmax, the whole process is described in Algorithm 1. Algorithm 1 The optimization process of pruning. Require: Configuration space Ω, minimum budget bmin, maximum budget bmax, reduction factor η, maximum trials Tmax Ensure: Optimized configuration ω smax = logη for {smax, smax 1, . . . , 0} and < Tmax do , , bmax bmin (s+1) ηs, bmin ηs (smax+1) Sample Configurations(n, D, Ω) for {0, 1, . . . , s} and < Tmax do ni ηi, ri ηi for each and < Tmax do Initialization Config count & budget Sample configurations Stage parameters Evaluate yw fmulti(Mw, λ, ri), {(w, ri, yw)}, + 1 end for Sort by performance, keep the top ni/η configurations in end for end for return the best-performing configuration ω evaluated at highest budget This efficient optimization strategy enables us to handle the search space defined in Section 3.2, identifying high-performing pruned models that satisfy our multi-objective function from Section 3.3, with significantly reduced computational cost compared to exhaustive search approaches."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Benchmarks. To evaluate the pruned models capabilities, we utilized the OpenCompass evaluation framework [37]. Specifically, we conduct evaluations in five aspects: Reasoning, Language, Knowledge, Examination and Understanding. Reasoning: CMNLI (CNLI)[38], HellaSwag (HeSw)[39], PIQA [40]. Language: CHID [41], WSC [42]. Knowledge: CommonSenseQA (CSQA) [43], BoolQ [44]. Examination: MMLU [45], CMMLU (CMLU) [46]. Understanding: Race-High/Middle (H/M) [47], XSum [48], C3 [49]. For CHID and XSum, we use generative evaluation. For the WSC dataset, we use cloze log-likelihood (WSCP) and generative (WSCG) evaluation. The remaining benchmarks are evaluated using cloze log-likelihood. See detailed benchmark information in Appendix B. Baselines. To evaluate the effectiveness of our method, we compared with four state-of-the-art structured pruning methods: LLM-Pruner (LLMPru) [14], SliceGPT [25], LaCo [26], and ShortGPT [22]. In our experiments, we set the pruning ratios of our method to be equivalent to ShortGPT and LaCo, and slightly higher than others to ensure fair comparison. Furthermore, as our method is based on multiple candidate models, we check three comprehensive comparison scenarios to guarantee fairness: (1) Applying each baseline pruning method individually to all candidate models and picking the strongest one, (2) First pruning each candidate model using the baseline method and then merging them, and (3) First merging the candidate models and then applying pruning. For model merging across all baseline experiments, we employ the same task-arithmetic merging [29] technique used in our search space, with merging factors within the range [0.5, 1.0] [29]. 5 Table 1: Comparison of pruning methods on multiple natural language benchmarks. \"Single\" refers to the best performance achieved when pruning single model directly, while \"Merge\" refers to the best performance achieved through either \"pruning-then-merging\" or \"merging-then-pruning\". For 7b model: Llama-2-7B-Chat (LM), MAmmoTH-7B (Math), Llama-2-Coder-7B (Code), and Llama-2-7B (Base), for 13b model: WizardLM-13B (LM), WizardMath-13B (Math), llama-2-13bcode-alpaca (Code), and Llama-2-13B (Base). The cells highlighted in blue show three selected Pareto-optimal solutions of our method. LLM Pruner Type (ratio) Reasoning Language Knowledge Understanding Avg Avg* CNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 38.46 55.04 70.70 46.67 36.54 50.78 69.36 43.04 59.62 64.29 74.77 48.30 41.35 57.41 71.04 46.22 20.15 57.28 23.21 0.00 19.90 52.14 24.01 5.77 19.49 40.09 25.38 4.81 2.88 19.41 42.66 25.22 34.62 22.11 67.22 29.08 22.12 23.42 72.66 29.30 48.08 52.50 67.34 43.68 36.54 55.61 73.21 36.84 62.90 75.08 48.75 43.27 41.35 62.74 66.24 47.39 46.15 62.00 75.90 48.73 63.46 67.24 71.38 55.84 60.58 65.03 73.70 53.48 19.23 60.36 78.44 54.21 63.46 68.88 72.72 55.92 19.00 63.24 23.27 0.00 0.00 21.46 61.96 23.84 19.23 32.51 41.22 33.09 49.04 37.76 38.38 33.55 34.62 52.58 62.66 36.26 63.46 18.84 64.65 41.83 50.96 65.44 67.71 53.50 43.27 65.68 78.01 51.26 62.50 69.37 74.28 55.90 55.77 67.49 73.70 54.61 54.81 68.06 69.82 53.99 Base 32.98 71.34 78.18 41.56 37.50 Math 32.99 68.60 75.79 39.71 39.42 LM 31.30 71.28 75.95 36.11 63.46 Code 32.99 70.27 78.62 41.61 36.54 Single 32.99 59.57 73.34 30.32 46.15 Merge 34.71 60.57 73.50 26.62 40.38 Single 31.89 41.55 58.81 18.43 39.42 Merge 32.85 37.61 57.56 17.33 53.85 Single 32.97 55.24 69.53 31.47 36.54 Merge 31.89 56.26 71.22 27.32 39.42 Single 33.09 57.42 66.54 21.53 56.73 Merge 34.10 54.18 64.42 16.83 61.54 35.46 54.43 67.74 23.63 63.46 34.94 58.14 69.48 21.53 63.46 34.95 54.92 67.08 24.48 63.46 Base 32.99 74.77 79.71 47.35 50.96 LM 35.36 70.41 78.73 36.21 57.69 Math 32.99 68.78 77.26 44.36 36.54 Code 32.99 74.82 80.14 47.30 51.92 Single 33.49 60.28 75.57 23.68 39.42 Merge 33.86 64.11 73.50 22.18 60.58 Single 33.19 42.44 59.90 18.03 54.81 Merge 30.98 46.83 62.57 19.33 51.92 Single 32.33 60.18 70.57 32.67 34.62 Merge 33.49 62.50 74.37 35.26 63.46 Single 32.95 62.64 73.50 28.22 36.54 Merge 31.07 63.24 68.61 27.17 49.04 32.99 66.81 75.03 29.07 54.81 31.80 68.63 72.52 30.97 60.58 29.05 69.76 72.74 34.22 58. 33.36 19.55 43.84 45.47 42.30 36.42 20.88 43.45 44.25 41.70 55.22 22.45 47.56 52.63 47.24 39.69 18.79 46.25 46.73 43.79 21.52 15.19 31.07 32.68 32.74 22.98 15.51 32.49 32.64 32.60 8.78 39.56 28.98 28.64 26.88 24.72 12.78 40.22 29.78 28.67 28.27 14.68 43.51 37.14 36.45 26.81 16.11 43.62 36.52 36.21 31.69 12.40 39.45 42.24 35.97 45.89 10.12 35.73 42.40 37.62 58.64 12.99 44.16 48.55 43.73 50.56 3.46 41.53 46.01 39.96 57.45 13.20 43.01 48.54 43.56 60.17 23.47 47.51 55.11 50.48 71.66 22.44 52.00 55.30 50.97 48.82 19.51 44.66 47.93 47.05 63.72 24.45 48.38 55.86 51.30 21.45 17.13 32.00 32.58 33.21 21.59 14.98 32.11 34.14 33.17 9.99 37.75 33.37 29.74 29.87 9.95 39.67 35.13 28.55 23.05 8.79 49.21 44.51 43.84 62.53 25.97 15.93 39.51 42.16 34.71 71.38 19.12 48.60 50.49 47.43 62.67 16.94 44.05 49.66 46.38 71.03 16.80 46.74 54.33 49.22 70.13 16.19 48.11 53.69 48.97 66.71 16.60 51.01 53.29 48.65 31.88 32.16 33.93 32.20 25.16 25.30 25.02 24.68 26.16 26.00 28.31 25.61 33.86 34.11 34.13 38.74 30.85 38.12 39.26 25.23 25.62 25.75 25.22 25.80 24.87 30.73 36.88 39.71 39.29 38.36 35.53 30.36 52.52 41.25 21.56 23.07 25.59 25.21 28.53 25.19 32.53 42.94 55.35 49.17 54.03 57.98 66.12 47.74 58.03 22.36 22.16 29.45 23.53 60.38 26.10 65.52 57.38 65.52 61.92 62.32 Llama -7B Llama -13B Dense (0.0%) LLMPru (25.3%) SliceGPT (26.3%) LaCo (27.1%) ShortGPT (27.1%) Ours (27.1%) Dense (0.0%) LLMPru (21.2%) SliceGPT (23.6%) LaCo (24.6%) ShortGPT (24.6%) Ours (24.6%) Model Selection. To assess the effectiveness of the proposed method, we search for pruned versions of the popular Llama2-7B and Llama2-13B [1]. For 7B models, we use Llama-2-7B [1] as our base model, with three candidate models: Llama-2-7B-Chat [1] (LM), MAmmoTH-7B [50] (Math), and Llama-2-Coder-7B [51] (Code). For 13B models, we use Llama-2-13B [1] as the base model, with WizardLM-13B [52] (LM), WizardMath-13B [53](Math), and Llama-2-13B-Code-Alpaca [54] (Code) as candidate models. For the 7B models, we set the sparsity ratio to 9/32, removing approximately 28% of the layers. For the 13B models, we set the sparsity ratio to 10/40, removing approximately 25% of the layers. These two ratios are matching the best settings from prior work in ShortGPT and LaCo, while being slightly higher than other baseline methods, allowing for fair comparisons. For layer merging, we implement task-arithmetic [29] merging with configurable merging factor that controls the magnitude of task-specific adaptations. Calibration Data. For our calibration dataset, we selected multiple-choice datasets to ensure the models generalization ability across different capabilities. Specifically, we sampled from diverse datasets: 1000 examples from the PIQA [40] training set, 500 examples from the WSC [42] training set, 1000 examples from the CSQA [43] training set, and 1000 examples from the MMLU [45] validation set (which is distinct from the MMLU test set). This diverse collection allows us to calibrate our model across broad spectrum of linguistic and reasoning capabilities. Objective and Optimizer. Our implementation builds upon SMAC [24] for optimization. We allocate 500 search trials for both 13B and 7B experiments. To improve optimization efficiency, we use models with randomly removed middle layers as starting points, since models are relatively robust to changes in these intermediate layers [34]. We set the minimum budget bmin as 100, maximum budget bmax as the 1000, and reduction factor η as 3. This resulted in budgets of {100, 300, 1000} for PIQA, CSQA, and MMLU. For the WSC, we set budgets to {100, 200, 500} 6 Figure 2: (a) Structure of our best-performing 7B-pruned model. The model integrates layers from multiple candidates: Llama-2-7B-Chat (LM), MAmmoTH-7B (Math), Llama-2-Coder-7B (Code), and Llama-2-7B (Base). The pruning ratio is 9/32, removing 9 layers out of 32 total layers. (b) Structure of our best-performing 13B-pruned model. The model integrates layers from multiple candidates: WizardLM-13B (LM), WizardMath-13B (Math), llama-2-13b-code-alpaca (Code), and Llama-2-13B (Base). The pruning ratio is 10/40, removing 10 layers out of 40 total layers."
        },
        {
            "title": "4.2 Main Results",
            "content": "To validate the efficiency of our method, we compared it with the four baselines: LLM-Pruner (LLMPru) [14], SliceGPT [25], LaCo [26], and ShortGPT [22]. We reproduce the results from these methods and evaluate on OpenCompass [37]. As mentioned, to validate that our proposed approach of \"pruning while merging\" is optimal, we also re-run each pruning method on (1) pruning each candidate model individually and picking the best, (2) \"pruning-then-merging\": First pruning each candidate model using the baseline method and then merging them, and (3) \"merging-then-pruning\": First merging the candidate models and then applying pruning. Table 1 reports the best single model pruning and best merge results of all baselines, with full results in Appendix D. We selected three Pareto-optimal solutions from our results. Our approach achieves the best results across multiple benchmarks compared to all tested LLM pruning methods. In terms of overall performance, our method maximally preserves the capabilities of the dense model: 92.2% (48.55/52.63) for the 7B model and 97.3% (54.33/55.86) for the 13B model. To ensure our results were not biased by our calibration data, we also calculate an avg* excluding the four benchmarks from which training data was selected for calibration (MMLU, CSQA, WSC, PIQA). As shown in the avg* column, our method still outperformed all baselines, further validating our approach. Notably, our method achieved comparable or even better results than dense models on many benchmarks. We attribute these gains to: 1) Pruning might mitigate \"overthinking\" [55] effects, evident in benchmarks like CNLI and WSC where other baseline pruning methods also improved performance, and 2) our merging strategy is effectively compensating for information loss from pruning. Figure 2 illustrates our best-performing 7B-pruned model and best-performing 13B-pruned modelss structure (See Table 13 and Table 12 for architectural details). We observe that both models tend to remove middle-to-later layers, with the 13B model removing layers from layer 25 and the 7B model from layer 19. This suggests information redundancy in these layers, aligning with findings that later layers exhibit high similarity and redundancy [22, 56]. The 13B model shows simpler structure dominated by single LM model with concentrated layer removal, while the 7B model shows more complex structure utilizing mixed and specialized models with scattered layer removal. This suggests that as model size decreases, more diverse mixing strategies may be needed to maintain performance. This architectural difference, coupled with the superior preservation rate of the 13B model compared to the 7B model, demonstrates that robustness (redundancy) scales with model size."
        },
        {
            "title": "4.3 Efficiency Analysis",
            "content": "Our optimizer dynamically adjusts the budget allocation during the search process, where the budget is defined as the calibration dataset size used for search. As the allocation of search trials directly determines the overall search duration. Here, we analyze the budget distribution during the search process, as shown in Table 2. Our analysis reveals that only 22% of the search trials utilize the full budget, while over 41.4% of the evaluations were conducted with the minimum budget, which is 5-10 times smaller. This efficient allocation enables our pruning to significantly increase the chance of discovering superior configurations under the same computational budget. 7 Search Budget Percentage Trials Dataset Size Low 41. 207 Medium 36.6 183 High 22. 110 100 PIQA 100 WSC CSQA 100 MMLU 100 300 PIQA 200 WSC CSQA 300 MMLU 300 1000 PIQA 500 WSC CSQA 1000 MMLU 1000 Table 2: Budget allocation to search trials for pruning. 41% of trials require only the smallest budget size, significantly increasing efficiency. Figure 3: Performance Comparison Across Different Pruning Ratios."
        },
        {
            "title": "4.4 Which Parts of the Search Space are Critical ?",
            "content": "To determine where the benefits of our approach come from, we designed ablation experiments to evaluate the contribution of different components in our search space. As our framework supports: (1) selectively choosing layers from different candidate models, (2) layer merging, and (3) layer removal, we conducted several experiments to isolate the impact of each component. Table 3 summarizes the performance comparison across various benchmarks (full results available in the Table 9) Layer Removal Only (LR-only). We restricted the search space to only allow layer removal operations on single 7B model. We ran experiments on all 7B models and report the best performer. This ablation shows performance drop from our full approach (48.55 44.83 on average), confirming that merely pruning layers from single model is insufficient for optimal performance. The performance degradation is particularly notable on language tasks (WSCP : 63.46 49.04) and understanding benchmarks (RaceH : 55.35 42.51, RaceM : 58.64 43.04). It is worth highlighting that even our layer-removal-only for single model still outperforms the strongest baseline method, ShortGPT (44.83 vs. 42.24). This demonstrates that our approach can enhance performance even in this simplified setting. Layer Selection and Removal (LS+LR). In this setting, we enabled both layer selection from different candidate models and layer removal operations but disabled the layer merging functionality. The results show even greater performance degradation (48.55 43.20 average) compared to the layer-removal-only setting. We observe dramatic drop on WSCG (43.27 26.92), indicating that merging operations play critical role for certain grammatical reasoning tasks. The superior performance of LR-only (44.83) compared to LS+LR (43.20) demonstrates that simply combining layers from different models without proper integration through merging is suboptimal. Table 3: Comparison of different searching settings across various benchmarks. Settings: LR-only: Layer-remove only, LS+LR: Layer-selection + layer-remove, FL-merge: Folding Layers Merging, Single-obj: Single-objective, PPL-obj: PPL as search objective. Setting Reasoning Language Knowledge Understanding Avg CNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 35.46 54.43 67.74 23.63 63.46 Ours 34.96 53.80 66.70 18.58 49.04 LR-only LS+LR 32.92 55.84 65.07 17.98 63.46 FL-merge 32.99 52.90 63.66 19.28 46.15 Single-obj 32.15 56.02 67.46 19.08 39.42 33.39 23.89 52.07 14.84 45.19 PPL-obj 43.27 58.65 26.92 62.50 48.08 7.69 62.90 75.08 60.61 68.87 58.97 51.22 60.52 75.20 62.33 74.43 19.33 39.51 48.75 47.85 48.97 48.30 47.40 24.25 33.86 33.54 34.61 34.33 34.14 24.69 55.35 42.51 48.68 50.77 50.94 22.81 58.64 43.04 49.44 55.29 52.86 21. 12.99 44.16 48.55 8.05 41.42 44.83 8.33 42.41 43.20 6.39 39.40 46.26 12.35 41.97 45.62 0.06 26.36 25."
        },
        {
            "title": "4.5.1 Varying the Pruning Ratio",
            "content": "To evaluate the benefits of evolving from \"pruning single model\" to \"pruning from model variants\" under varying pruning ratios, we compared our approach with each candidate model using the layer-remove configuration, as it achieves the strongest performance among single-model pruning 8 methods, even surpassing the best-performing baseline, ShortGPT. Figure 3 visualizes the average accuracy among benchmark performances at different pruning ratios, with detailed results in Table 10. The accuracy of all models decreases as the pruning ratio increases. Our model achieves the best performance at almost all pruning ratios, especially in the low pruning ratio range of 0 - 37.5%. When pruning reaches 50%, the performance gap narrows across all models as they all experience performance collapse, including ours. We believe this is due to excessive parameter removal, after which effective model function cannot be maintained without additional post-training."
        },
        {
            "title": "4.5.2 Pruning through Layer Folding",
            "content": "LaCo [26] is another pruning approach based on merging, but it differs from our method by merging only the later layers of single model into adjacent earlier layers, based on activation similarity heuristics. To validate the effectiveness and potential of this type of within-model merge operation, we use our hyperparameter optimization framework with specially designed search space consisting of: (1) binary selection vector = [s1, s2, . . . , sk] indicating which layers to remove, and (2) An importance weight vector = [w1, w2, . . . , wk] representing each layers importance value. Retained layer performs depth-wise linear combination with itself and adjacent removed layers: = βi Li + (cid:88) jN (i) βj Lj 1sj =1 where (i) represents adjacent layers to Li, 1sj =1 indicates layer is removed, and βj are normalized weights derived from such that βi + (cid:80) jN (i) βj 1(sj = 1) = 1. This ensures retained layers incorporate information from nearby removed layers, preserving network functionality. This configuration parametrizes the options proposed in LACO and achieves better overall performance (46.26) compared to previous ablations but still falls short of our full approach. The performance gap is most pronounced on XSum (12.99 6.39) and PIQA (67.74 63.66), highlighting the importance of our optimized merging strategy for generative and reasoning tasks."
        },
        {
            "title": "4.5.3 Different Calibration Datasets and Metrics",
            "content": "In our method, we use multiple-choice datasets as calibration data with accuracy as the metric in multi-objective optimization approach. This results in pruned model with broad capabilities. To further analyze this design choice, we conducted the following experiments: Single Objective (Single-obj). We used the MMLU validation dataset for calibration with accuracy as the optimization objective. We evaluate the resulting pruned models across our benchmark suite. As shown in Table 3, while these models still perform adequately (45.62 average), the single-objective optimization led to noticeable decline from our full approach (48.55 45.62). Importantly, the single-objective models demonstrated stronger performance on MMLU-related tasks but showed performance degradation on certain other tasks due to their narrow optimization focus. This confirms our hypothesis that broad, multi-objective optimization is necessary to preserve the broad functionality of modern LLMs, rather than overfitting to single task domain. Perplexity Objective (PPL-obj). We also experiment with perplexity (PPL) on WikiText [57] as search metric, taking 1500 examples as our calibration dataset. The performance of resulting pruned models across benchmarks in Table 3 demonstrates dramatic performance gap compared to all other configurations, with an average score of only 25.38 across benchmarks. Even when compared to the single-objective MMLU optimization (which uses similarly sized dataset), the PPL-optimized models showed considerably weaker performance across most tasks. The catastrophic degradation on tasks like XSum (12.99 0.06) and reasoning benchmarks like HeSw (54.43 23.89) underscores the limitations of perplexity-guided optimization. This evidence reveals that while perplexity is common metric for language model evaluation, it fails to serve as an effective signal for preserving model capabilities during pruning, particularly for tasks requiring reasoning and knowledge application rather than just fluent text generation."
        },
        {
            "title": "4.5.4 Extending to Llama-3",
            "content": "We further extend our validation to Metas Llama 3 8B model[58]. Llama 3 training on 15 trillion tokens7 more data than Llama 2, incorporates architectural improvements including universal Grouped Query Attention (GQA), an optimized 128K vocabulary tokenizer and longer context 9 window. These modifications further boost the performance on reasoning, code generation, and multilingual tasks. Despite similar model size, Llama-3 8B achieves better performance compared to Llama-2 7B[1], which may carry different semantic densities that provide new challenges for maintaining model performance under compression. Validating on this next-generation model is crucial for establishing the practical applicability of our method in rapidly evolving LLM landscapes. we use Meta-Llama-3-8B[58] as our base model, with three candidate models: Meta-Llama-3-8BInstruct (LM)[58], Code-Llama-3-8B (Code)[59], and MathCoder2-Llama-3-8B (Math)[60, 61]. We target removing 9 of the 32 layers and use the same experimental settings as Llama2-7B. We compare our method with the best-performing baseline, ShortGPT. As results shown in Table 4(full results available in the Table 11), our method retains 84.55%(53.17/63.61) of the original performance after removing 9 layers, outperforming ShortGPTs 62.79%(39.94/63.61) retention under same compression ratios. while both show lower retention than our Llama2-7B results (92.2%) with similar model size. This decline indicates Llama3s reduced compressibility, We attribute this decline to (1) higher parameter utilization efficiency and (2) denser knowledge distribution from large-scale training, which eliminates layer redundancy. Despite this challenge, our method consistently outperforms the baseline, validating its effectiveness across model generations. Table 4: Comparison of pruning methods on multiple natural language benchmarks. \"Single\" refers to the best performance achieved when pruning single model directly, while \"Merge\" refers to the best performance achieved through either \"pruning-then-merging\" or \"merging-then-pruning\". For 8b model: Meta-Llama-3-8B-Instruct (LM), MathCoder2-Llama-3-8B (Math), Code-Llama-3-8B (Code), and Meta-Llama-3-8B (Base). The cells highlighted in blue show three selected Paretooptimal solutions of our method. LLM Pruner Type Reasoning Language Knowledge Understanding ratio/layer Dense Llama3 -8B ShortGPT Ours CMNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 32.98 74.67 80.96 73.78 56.73 36.54 73.79 69.97 64.74 50.79 63.21 70. Base 3.28 55.18 57.65 LM 33.00 71.08 80.69 65.53 55.77 69.23 76.66 78.87 65.97 53.64 76.44 81.75 17.97 63.95 63.61 32.99 71.66 77.97 57.09 37.50 58.65 68.22 69.08 62.08 45.85 64.75 69.08 Math 8.68 53.86 55.53 32.98 65.56 74.70 78.42 61.54 61.54 63.47 78.35 48.03 34.55 52.40 58.43 19.36 46.41 55.41 Code 3.68 43.51 39.94 Single 32.83 45.06 65.78 23.38 41.35 53.85 39.56 63.73 32.37 28.69 40.14 45.19 3.66 44.27 37.10 Merge 32.95 48.58 64.96 18.43 36.54 35.58 42.83 67.22 33.05 28.71 30.16 32.45 3.22 47.12 52.81 33.46 54.81 69.53 32.27 41.35 58.65 72.81 65.29 63.36 50.14 71.41 75.97 33.07 55.15 69.37 31.77 46.15 64.42 73.55 64.43 62.35 49.06 74.36 78.27 2.95 46.03 53.64 3.00 46.52 53.78 33.42 54.83 69.75 34.02 47.12 62.50 73.79 64.34 63.13 50.04 72.81 77.65 Avg"
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we presented novel LLM compression approach that strategically combines layers from fine-tuned model variants instead of pruning single models. By formulating this as zero-order optimization problem with newly designed search space that supports layer removal, selection, and merging, our method effectively preserves model capabilities while reducing size. Experiments on Llama2-7B and Llama2-13B demonstrated that our compressed models retain 92.2% and 97.3% of original performance, respectively, despite removing 25% of parameters, outperforming previous state-of-the-art methods without requiring expensive post-training. Overall, our work demonstrates that cutting and stitching layers from multiple fine-tuned variants of model is more effective approach to LLM compression than traditional single-model pruning. While the search complexity increases with the number of candidate models, this computational aspect represents an opportunity for future optimization techniques to further enhance efficiency."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank Alexander Panfilov and Niccolò Ajroldi for their insightful comments and valuable feedback on this work. JG acknowledges the support of the Hector II foundation. GS acknowledges the support of the International Max Planck Research School for Intelligent Systems (IMPRS-IS)."
        },
        {
            "title": "References",
            "content": "[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [2] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [4] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [5] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [6] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [7] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. Advances in neural information processing systems, 32, 2019. [8] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. convergence theory for deep learning via over-parameterization. In International conference on machine learning, pages 242252. PMLR, 2019. [9] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez. Train big, then compress: Rethinking model size for efficient training and inference of transformers. In International Conference on machine learning, pages 59585968. PMLR, 2020. [10] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. [11] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078, 2023. [12] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. [13] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: simple depth pruning for large language models. arXiv preprint arXiv:2402.02834, 11, 2024. [14] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. [15] Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. Disco: Distilling counterfactuals with large language models. arXiv preprint arXiv:2212.10534, 2022. 11 [16] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023. [17] Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller language models. Findings of the Association for Computational Linguistics: ACL 2023, pages 70597073, 2023. [18] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. [19] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:2716827183, 2022. [20] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael Mahoney, and Kurt Keutzer. survey of quantization methods for efficient neural network inference. In Low-power computer vision, pages 291326. Chapman and Hall/CRC, 2022. [21] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:10088 10115, 2023. [22] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. [23] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR, 2022. [24] Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. Smac3: versatile bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research, 23(54):19, 2022. [25] Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. [26] Yifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187, 2024. [27] Aaron Klein, Jacek Golebiowski, Xingchen Ma, Valerio Perrone, and Cedric Archambeau. Structural pruning of pre-trained language models via neural architecture search. arXiv preprint arXiv:2405.02267, 2024. [28] Joachim Utans. Weight averaging for neural networks and local resampling schemes. In Proc. AAAI-96 Workshop on Integrating Multiple Learned Models. AAAI Press, pages 133138. Citeseer, 1996. [29] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. [30] Tom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016. [31] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Ties-merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. [32] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. [33] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of model merging recipes. arXiv preprint arXiv:2403.13187, 2024. [34] Guinan Su and Jonas Geiping. Fine, ill merge it myself: multi-fidelity framework for automated model merging. arXiv preprint arXiv:2502.04030, 2025. [35] Joshua Knowles. Parego: hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE transactions on evolutionary computation, 10(1):5066, 2006. [36] Leo Breiman. Random forests. Machine learning, 45:532, 2001. [37] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. [38] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: chinese language understanding evaluation benchmark. arXiv preprint arXiv:2004.05986, 2020. [39] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [40] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [41] Chujie Zheng, Minlie Huang, and Aixin Sun. Chid: large-scale chinese idiom dataset for cloze test. arXiv preprint arXiv:1906.01265, 2019. [42] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. KR, 2012:13th, 2012. [43] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. [44] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [45] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [46] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. [47] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. [48] Shashi Narayan, Shay Cohen, and Mirella Lapata. Dont give me the details, just the topic-aware convolutional neural networks for extreme summarization. arXiv summary! preprint arXiv:1808.08745, 2018. [49] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155, 2020. 13 [50] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [51] Manuel Romero. llama-2-coder-7b (revision d30d193), 2023. [52] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. [53] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. [54] Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca, 2023. [55] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras. Shallow-deep networks: Understanding and mitigating network overthinking. In International conference on machine learning, pages 33013310. PMLR, 2019. [56] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. [57] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [58] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [59] Ajibawa. Code-llama-3-8b, 2023. [60] Zimu Lu, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. In The Twelfth International Conference on Learning Representations, 2024. [61] Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathcoder2: Better math reasoning from continued pretraining on modeltranslated mathematical code, 2024. [62] Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019."
        },
        {
            "title": "A Baseline",
            "content": "To ensure fair comparison, we applied various baseline pruning methods including LLMPruner(LLMPru) [14], SliceGPT [25], LaCo [26] and ShortGPT [22]: LLM-Pruner adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLMs functionality. It applies post-training to the pruned model, for fair comparison, we do not apply post training to it. SliceGPT is post-training sparsification scheme that replaces each weight matrix with smaller matrix, reducing the embedding dimension of the network. Specifically, they applied PCA to the hidden representation from shallow to deep layers, and incorporated the dimension reduction matrix into existing network parameters. LaCo is pruning method for large language models based on reducing layers. LaCo gradually merges similar layers from deep to shallow and sets threshold to avoid merging too many layers. ShortGPT introduced the Block Influence (BI) metric, which uses the similarity between layers input and output to measure the importance of each layer."
        },
        {
            "title": "B Evaluation Benchmarks",
            "content": "CMNLI (Chinese Multi-Genre Natural Language Inference) (CNLI) consists of two parts: XNLI and MNLI. It contains text from various domains, including fiction, telephone conversations, travel, and government sources. XNLI is cross-lingual extension of the MultiNLI corpus, professionally translated into multiple languages, including Chinese, providing robust framework for assessing language understanding across linguistic boundaries. Models must determine whether pairs of sentences exhibit entailment, contradiction, or neutrality. HellaSwag (HeSw) tests commonsense reasoning about physical situations. The dataset uses \"Goldilocks\" zone of complexity where examples are obviously nonsensical to humans but challenging for state-of-the-art models. Despite being trivial for humans (>95% accuracy), even advanced models struggled with this benchmark upon its release, making it effective for measuring progress in commonsense inference. PIQA (Physical Interaction Question Answering) Developed by Bisk et al. (2020), this multichoice question and answer dataset focuses on everyday scenarios, exploring models understanding of real-world physical laws through daily situations. CHID (Chinese IDiom) is an idiom cloze test focusing on the representation and selection of Chinese idioms, requiring cultural and linguistic knowledge specific to Chinese. WSC (Winograd Schema Challenge) serves as prominent benchmark for evaluating machine understanding through pronouns resolution problems that are trivial for humans but require commonsense reasoning for machines to solve correctly. The dataset consists of pairs of sentences differing in one or two words with ambiguous pronouns resolved differently in the two sentences, designed to test systems commonsense reasoning abilities. CommonSenseQA (CSQA) is multiple-choice question answering dataset containing 12,102 questions with one correct answer and four distractor answers, requiring different types of commonsense knowledge to predict the correct answers. The dataset was constructed using ConceptNet relations and crowd-sourced questions to test commonsense reasoning. BoolQ provides 15,942 yes/no questions that occur naturally in unconstrained environments, testing models binary decision-making abilities. MMLU (Massive Multitask Language Understanding) evaluates models across 57 diverse subjects covering STEM, humanities, and social sciences. The benchmark tests knowledge and problemsolving ability with content ranging from elementary to professional levels. This benchmark has become standard evaluation metric in the field, with scores prominently reported for virtually all language models, and uses multiple-choice questions that allow for simple accuracy calculations. CMMLU (Chinese Massive Multitask Language Understanding) (CMLU) Developed to address the gap in evaluating knowledge and reasoning capabilities in Chinese, CMMLU is comprehensive benchmark covering 67 subjects from elementary to advanced professional levels across natural sciences, social sciences, engineering, and humanities. The benchmark includes topics with Chinesespecific answers that may not be universally applicable in other regions or languages, making it fully Chinese-oriented evaluation tool. RACE (Reading Comprehension from Examinations) is collected from English examinations in China designed for middle and high school students, providing culturally diverse reading assessment. XSum evaluates abstract single document summarization systems, focusing on the ability to create concise one-sentence summaries capturing the essence of articles. C3 (Chinese Multiple-Choice Machine Reading Comprehension) consists of multiple-choice questions from Chinese proficiency exams and ethnic Chinese exams."
        },
        {
            "title": "C Task Arithmetic Merging",
            "content": "Task Arithmetic [29] enhances model capabilities through vector operations by leveraging weighted combinations of task-specific knowledge. Given base model with weights θpre and task-specific fine-tuned weights {θft θpre. The merged weights are t=1, task vectors are defined as τt = θft }n 15 then computed through θMerge = θpre + λ (cid:80)n adaptations. t=1 τt, where λ controls the magnitude of task-specific"
        },
        {
            "title": "D Full Baseline results",
            "content": "To validate the efficiency of our proposed method, we conducted comparative experiments against established baseline techniques. For fair comparison with other baseline methods, we selected the same pruning ratios matching those used in LaCo [26] and ShortGPT [22] while being lower than those of other approaches. In order to make fairer comparison, we reproduced all the results and evaluated them on OpenCompass [37] as in LaCo.All experiments run on NVIDIA Tesla A100 GPUs. For each baseline method, we explored three scenarios: (1) applying each baseline pruning method individually to all candidate models, (2) first pruning each candidate model using existing methods and then merging them, and (3) first merging the candidate models and then applying pruning techniques. We use the official implement of LLM-pruner and LaCo, Its worth noting that when reproducing the LaCo method, we referenced the hyperparameter settings from the original paper. Due to differences in hardware, we couldnt fully reproduce the papers results: we couldnt obtain models with pruning ratios consistent with the paper using the provided hyperparameters. We maintained consistency in all other parameters while gradually adjusting the threshold from 0.75 until achieving the desired pruning ratio. The specific parameters are detailed in the Table 5. For the reproduction of ShortGPT, we implemented the algorithm based on the original paper and similarly sampled 10,000 instances from the PG19 [62] dataset as calibration data, following the methodology described in the paper. The resulting removed layers are shown in the Table. The removed layers for the base model align with those reported in the ShortGPT paper, albeit in different sequence. We attribute this variation to slight differences in calculated layer importance scores. The specific configuration of removed layers for each model is detailed in the Table 6. For the merging process, we employed task arithmetic with weighting parameters in the range of [0.5, 1.0]. The full results of the baseline methods on the 7B model and the 13B model are presented in Table 7 and Table 8, respectively. Table 5: Hyperparameter settings for LaCo results. C: Number of layers combined in each merge; L,H: Layer range [L, H]; I: Minimum interval between two adjacent merged layers; : Threshold for representation similarity."
        },
        {
            "title": "C L H I T",
            "content": "6 1 40 2 0.7 Llama-2-13B 6 1 40 2 0.65 WizardLM-13B WizardMath-13B 6 1 40 2 0.7 llama-2-13b-code-alpaca 6 1 40 2 0.7 6 1 40 2 0.65 Merge-then-prune 6 1 40 2 0.65 Prune-then-merge Llama-2-7B Llama-2-7B-Chat MAmmoTH-7B Llama-2-Coder-7B Merge-then-prune Prune-then-merge 6 1 40 2 0.7 6 1 40 2 0.65 6 1 40 2 0.7 6 1 40 2 0.7 6 1 40 2 0.65 6 1 40 2 0.65 Llama2-13B Llama2-7B Table 6: Setup of Removed Layers for Candidate Models in ShortGPT."
        },
        {
            "title": "Removed Layers",
            "content": "Llama-2-7B Llama-2-7B-Chat MAmmoTH-7B 25, 27, 24, 28, 26, 29, 23, 22, 21 27, 25, 24, 28, 29, 26, 23, 22, 21 27, 25, 24, 28, 29, 23, 26, 22, Llama-2-Coder-7B 27, 25, 24, 28, 29, 26, 23, 21, 22 Llama-2-13B WizardLM-13B WizardMath-13B 33, 32, 31, 30, 34, 35, 29, 28, 27, 33, 32, 31, 30, 34, 35, 29, 28, 27, 36 33, 31, 32, 30, 34, 35, 29, 28, 27, 36 llama-2-13b-code-alpaca 33, 31, 32, 30, 34, 35, 29, 28, 27, 26 Table 7: The main results of baseline methods on the 7B model across multiple natural language benchmarks using candidate models: Llama-2-7B-Chat (LM), MAmmoTH-7B (MAth), Llama-2Coder-7B (Code), and Llama-2-7B (base). \"PTM\" (Pruning-then-Merging) refers to first pruning each candidate model using current pruner and then merging them. \"MTP\" (Merging-then-Pruning) refers to first merging the candidate models and then applying pruning. For LLMPruner and SliceGPT, alignment challenges exist after pruning. LLMPruner removes different model blocks, while SliceGPT calculates orthogonal transformation matrices that are highly dependent on each models specific weight distributions and activation patterns, resulting in incompatible transformation spaces. Therefore, we only implemented \"merge then prune\". Avg LLM Pruner Type Reasoning Language Knowledge Understanding (ratio/layer) Dense LLMPruner (25.32%) SliceGPT (26.33%) Llama -7B"
        },
        {
            "title": "LACO",
            "content": "ShortGPT (27.1%) 0.00 9.62 0.00 5.77 0.00 0.96 4.81 CMNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 32.98 71.34 78.18 41.56 37.50 38.46 55.04 70.70 46.67 31.88 35.53 33.36 19.55 43.84 45.47 Base Math 32.99 68.60 75.79 39.71 39.42 36.54 50.78 69.36 43.04 32.16 30.36 36.42 20.88 43.45 44.25 LM 31.30 71.28 75.95 36.11 63.46 59.62 64.29 74.77 48.30 33.93 52.52 55.22 22.45 47.56 52.63 32.99 70.27 78.62 41.61 36.54 41.35 57.41 71.04 46.22 32.20 41.25 39.69 18.79 46.25 46.73 Code 19.74 57.25 23.69 25.49 22.07 21.10 14.67 28.11 31.93 33.00 58.72 72.25 29.52 41.35 Base LM 34.94 59.25 72.85 22.28 43.27 19.41 57.61 23.77 24.51 21.78 22.42 16.32 28.66 32.62 MATH 32.99 55.74 70.84 25.82 37.50 21.15 18.84 54.31 24.77 25.20 22.87 23.89 10.91 28.00 32.35 32.99 59.57 73.34 30.32 46.15 20.15 57.28 23.21 25.16 21.56 21.52 15.19 31.07 32.68 Code 34.71 60.57 73.50 26.62 40.38 19.90 52.14 24.01 25.30 23.07 22.98 15.51 32.49 32.64 MTP 20.88 37.95 24.78 24.78 21.24 21.73 Base 6.58 37.42 27.63 31.08 42.90 61.43 19.53 36.54 21.21 38.96 25.56 25.28 21.93 22.42 13.13 38.36 28.79 LM 31.70 43.50 61.37 18.28 40.38 8.78 39.56 28.98 MATH 31.89 41.55 58.81 18.43 39.42 19.49 40.09 25.38 25.02 25.59 26.88 Code 31.81 44.02 63.17 18.48 36.54 13.46 19.74 37.92 24.71 25.22 21.41 21.66 2.59 38.19 28.49 19.41 42.66 25.22 24.68 25.21 24.72 12.78 40.22 29.78 32.85 37.61 57.56 17.33 53.85 MTP Base 9.38 42.47 32.99 20.39 62.02 26.60 25.27 24.70 23.61 32.85 53.33 68.23 31.62 36.54 LM 32.97 55.24 69.53 31.47 36.54 34.62 22.11 67.22 29.08 26.16 28.53 28.27 14.68 43.51 37.14 32.97 55.24 69.53 31.47 50.00 34.62 22.11 67.22 29.44 26.16 22.53 23.68 14.68 39.34 37.07 Math 20.56 61.99 26.31 25.43 27.10 22.70 11.14 43.07 33.15 32.28 53.68 69.15 32.22 36.54 Code MTP 8.27 44.33 36.45 32.43 57.80 71.82 28.97 41.35 16.35 27.52 71.28 30.49 26.88 25.76 27.09 PTM 31.89 56.26 71.22 27.32 39.42 22.12 23.42 72.66 29.30 26.00 25.19 26.81 16.11 43.62 36.52 52.5 67.34 43.68 28.31 32.53 31.69 12.40 39.45 42.24 Base 6.93 37.21 40.00 LM 33.85 53.93 63.82 14.59 39.42 22.12 58.48 67.95 35.85 26.60 48.03 51.18 MATH 33.97 56.69 63.38 17.78 54.81 44.23 37.26 69.82 30.68 25.26 28.24 30.29 8.26 31.67 38.02 32.74 56.69 65.07 17.78 58.65 35.58 53.24 67.52 44.82 28.92 35.62 37.53 14.32 40.66 42.08 Code MTP 34.10 54.18 64.42 16.83 61.54 36.54 55.61 73.21 36.84 25.61 42.94 45.89 10.12 35.73 42.40 PTM 34.10 54.18 64.42 16.83 61.54 36.54 55.61 73.21 36.84 25.61 42.94 45.89 10.12 35.73 42.40 33.09 57.42 66.54 21.53 56.73 48.08 2.88 4. 1.92 17 Table 8: The main results of baseline methods on the 13B model across multiple natural language benchmarks using candidate models: WizardLM-13B (LM), WizardMath-13B (Math), llama-2-13bcode-alpaca (Code), and Llama-2-13B (Base). \"PTM\" (Pruning-then-Merging) refers to first pruning each candidate model using the current pruner and then merging them. \"MTP\" (Merging-thenPruning) refers to first merging the candidate models and then applying pruning. For LLMPruner and SliceGPT, alignment challenges exist after pruning. LLMPruner removes different model blocks, while SliceGPT calculates orthogonal transformation matrices that are highly dependent on each models specific weight distributions and activation patterns, resulting in incompatible transformation spaces. Therefore, we only implemented \"merge then prune\" Avg LLM Pruner Type Reasoning Language Knowledge Understanding ratio/layer Dense LLMPruner (21.2%) SliceGPT (23.6%) Llama -13B LaCo (24.6%) ShortGPT (24.6%) 0.00 0.00 0.00 0.00 0. CMNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 Base 32.99 74.77 79.71 47.35 50.96 63.46 67.24 71.38 55.84 38.74 57.98 60.17 23.47 47.51 55.11 LM 35.36 70.41 78.73 36.21 57.69 60.58 65.03 73.70 53.48 30.85 66.12 71.66 22.44 52.00 55.30 MATH 32.99 68.78 77.26 44.36 36.54 19.23 60.36 78.44 54.21 38.12 47.74 48.82 19.51 44.66 47.93 32.99 74.82 80.14 47.30 51.92 63.46 68.88 72.72 55.92 39.26 58.03 63.72 24.45 48.38 55.86 Code 19.57 45.35 23.08 25.36 21.61 21.80 14.41 29.64 31.77 Base 33.27 63.57 75.41 34.17 37.50 19.00 63.24 23.27 25.23 22.36 21.45 17.13 32.00 32.58 LM 33.49 60.28 75.57 23.68 39.42 19.08 53.18 23.06 25.53 21.36 21.31 12.25 29.10 31.26 MATH 32.99 55.49 72.91 30.02 41.35 19.90 47.80 23.19 25.52 21.61 22.08 16.08 29.59 32.58 33.18 64.21 75.52 34.17 43.27 Code 21.46 61.96 23.84 25.62 22.16 21.59 14.98 32.11 34.14 33.86 64.11 73.50 22.18 60.58 MTP 8.78 39.56 31.60 30.39 46.69 63.22 18.78 42.31 25.96 25.23 37.83 30.43 25.14 23.47 24.65 Base 9.99 37.75 33.37 LM 33.19 42.44 59.90 18.03 54.81 19.23 32.51 41.22 33.09 25.75 29.45 29.87 MATH 32.73 36.27 59.30 17.38 42.31 1.54 40.82 28.09 21.62 37.83 30.33 25.16 23.84 24.16 8.83 40.00 31.86 30.82 46.69 63.00 19.18 42.31 27.88 24.82 37.83 31.38 25.20 23.47 24.65 Code 30.98 46.83 62.57 19.33 51.92 49.04 37.76 38.38 33.55 25.22 23.53 23.05 9.95 39.67 35.13 MTP Base 32.97 59.38 73.45 36.26 37.50 37.50 19.41 57.31 25.03 24.41 22.47 23.19 16.39 37.92 35.94 8.79 49.21 44.51 LM 32.33 60.18 70.57 32.67 34.62 34.62 52.58 62.66 36.26 25.80 60.38 62.53 33.97 56.51 72.25 33.52 44.23 44.23 21.38 64.19 25.35 24.55 21.98 21.94 12.77 37.48 36.74 Math 19.49 53.18 24.48 24.72 22.87 22.28 17.70 37.53 34.30 32.99 59.53 75.03 38.41 51.92 Code 33.49 62.50 74.37 35.26 63.46 63.46 18.84 64.65 41.83 24.87 26.10 25.97 15.93 39.51 42.16 MTP 0.40 35.12 29.31 PTM 31.85 29.80 51.31 12.74 36.54 36.54 19.57 62.08 24.37 25.19 22.10 22.77 Base 32.99 67.07 73.45 36.46 42.31 45.19 66.99 58.56 54.74 38.39 56.89 54.06 18.58 46.19 49.42 LM 32.95 62.64 73.50 28.22 36.54 50.96 65.44 67.71 53.50 30.73 65.52 71.38 19.12 48.60 50.49 7.73 43.84 42.51 59.71 70.00 52.70 36.94 43.51 44.29 MATH 32.99 59.63 70.40 31.12 40.38 Code 32.92 67.03 74.37 36.41 55.77 46.15 68.96 60.55 54.94 38.30 53.60 58.57 8.41 47.18 50.23 MTP 31.07 63.24 68.61 27.17 49.04 43.27 65.68 78.01 51.26 36.88 57.38 62.67 16.94 44.05 49.66 PTM 31.08 63.32 68.66 27.12 49.04 43.27 65.68 77.98 51.23 36.82 57.40 62.47 17.01 43.95 49.65 1.92 0.00 0.00 18 Table 9: Performance comparison of various model pruning strategies across multiple benchmark categories. The settings include LR-only (Layer Removal only), LS+LR (combined Layer Selection and Layer Removal), FL-merge (Folding Layers Merging), Single-obj (Single-objective optimization), and PPL-obj (Perplexity-based objective). For multi-objective optimization approaches, three representative Pareto-optimal solutions (numbered 1-3) are presented. setting Reasoning Language Knowledge Understanding Avg CNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 33.93 57.51 65.49 18.18 62.46 LR-only-LM-1 33.58 52.10 64.25 19.53 50.00 LR-only-LM-2 LR-only-LM-3 34.96 53.80 66.70 18.58 49.04 LR-only-Math-1 33.77 54.49 68.23 21.93 62.50 LR-only-Math-2 31.69 56.56 68.77 27.07 63.46 LR-only-Math-3 32.94 58.43 69.64 25.97 54.81 LR-only-Code-1 30.13 57.60 70.35 27.07 63.46 LR-only-Code-2 34.94 57.37 68.55 28.67 42.31 LR-only-Code-3 34.93 56.71 69.42 25.92 59.62 LR-only-Base-1 32.67 54.21 66.00 26.07 36.54 LR-only-Base-2 32.22 56.48 67.46 26.32 61.54 LR-only-Base-3 31.13 52.90 67.95 27.97 36.54 FL-merge-1 FL-merge-2 FL-merge-3 32.99 52.90 63.66 19.28 46.15 32.99 51.99 63.44 18.33 46.15 33.89 51.15 62.62 18.63 50.00 LS+LR-1 LS+LR-2 LS+LR-3 Single-obj PPL-obj 34.75 53.65 66.32 17.83 63.46 31.74 55.25 68.39 26.77 63.46 32.92 55.84 65.07 17.98 63.46 32.15 56.02 67.46 19.08 39.42 33.39 23.89 52.07 14.84 45.19 48.03 62.50 58.65 37.50 30.77 25.96 11.54 41.35 31.65 1.92 50.00 0.00 62.50 63.46 61.54 22.12 10.58 26.92 48.08 7. 58.79 62.18 63.64 41.80 60.61 68.87 27.85 57.52 36.69 62.35 29.89 62.84 50.94 65.96 54.46 63.00 52.83 62.20 49.47 64.19 41.44 66.91 54.63 64.13 60.52 75.20 61.26 74.77 60.44 75.78 59.71 70.61 58.72 66.27 58.97 51.22 62.33 74.43 19.33 39.51 45.76 48.33 47.85 37.08 39.17 33.46 42.64 42.49 43.03 44.47 40.54 43.01 48.30 48.80 48. 47.32 47.40 48.97 47.40 24.25 30.95 32.84 33.54 28.73 29.15 26.92 30.96 27.39 28.80 28.84 28.01 30.03 34.33 33.84 33.96 33.77 33.15 34.61 34.14 24. 49.54 51.03 42.51 31.42 33.39 31.39 36.39 34.88 38.51 38.99 37.94 35.56 50.77 51.11 50.74 36.62 40.02 48.68 50.94 22.81 53.36 51.46 43.04 34.05 38.65 32.10 36.77 35.31 39.07 38.86 39.35 37.05 55.29 56.34 55. 33.91 45.26 49.44 1.45 38.60 44.73 5.47 39.56 44.01 8.05 41.42 44.83 7.51 37.92 38.61 4.41 43.34 40.39 8.06 40.16 38.04 3.15 43.78 40.77 4.08 43.78 41.33 2.87 41.70 41.95 0.25 41.59 37.43 0.96 41.92 42.22 6.79 41.70 37.81 6.39 39.40 46.26 5.75 37.86 46.15 5.72 38.03 46.15 8.54 42.35 42.21 2.62 44.16 42.41 8.33 42.41 43.20 52.86 21.17 12.35 41.97 45.62 0.06 26.36 25. 19 Table 10: Model Performance Comparison Across Pruning Ratios Model Prune Ratio Reasoning Language Knowledge Understanding Avg Base Base Base Base Base Base Base 0 12.5 25 37.5 50 62.5 75 LM 0 LM 12.5 LM 25 LM 37.5 LM 50 LM 62.5 LM 75 Math 0 Math 12.5 Math 25 Math 37.5 Math 50 Math 62.5 Math Code 0 Code 12.5 Code 25 Code 37.5 Code 50 Code 62.5 Code 75 Ours Ours Ours Ours Ours Ours Ours 0 12.5 25 37.5 50 62.5 75 CNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C3 32.98 71.34 78.18 41.56 37.50 32.99 67.06 74.92 39.61 36.53 32.98 63.80 69.21 35.37 36.54 32.58 45.04 61.53 20.68 36.54 34.51 34.89 55.33 17.08 36.54 35.14 29.71 52.83 14.94 39.42 34.94 26.71 51.03 13.59 36.54 31.30 71.28 75.95 36.11 63.46 32.42 67.58 72.72 28.91 50.92 30.10 60.63 66.82 20.53 48.96 33.29 45.13 60.66 20.03 36.54 34.93 34.67 56.20 16.18 36.54 34.11 30.50 53.21 14.34 51.92 34.87 27.03 52.19 14.54 39. 32.99 68.60 75.79 39.71 39.42 32.97 64.72 73.06 37.50 23.08 34.92 46.24 61.92 19.38 36.54 32.99 55.42 62.81 23.82 38.38 32.73 35.93 55.06 16.73 39.42 34.93 31.06 54.08 13.79 58.65 34.94 27.35 52.07 14.39 43.27 32.99 70.27 78.62 41.61 36.54 32.97 65.79 75.78 39.06 36.54 32.99 63.06 72.02 35.67 36.54 33.21 44.12 62.13 20.78 36.54 34.93 34.15 54.95 16.73 36.54 34.72 29.67 52.99 14.39 40.38 34.94 26.79 50.82 13.99 38.46 36.88 73.16 78.67 39.46 64.46 33.00 66.78 75.19 34.92 64.42 32.99 57.31 68.34 22.38 63.46 35.67 51.02 63.44 20.68 62.50 33.97 41.99 58.16 21.08 38.54 33.30 28.34 51.96 18.09 46.15 34.93 30.45 49.18 20.48 39.54 38.46 1.92 0.00 2.88 11.54 1.92 8.65 59.62 60.50 42.31 11.73 8.65 2.88 0.00 36.54 23.07 56.73 4.81 39.42 4.81 2. 41.35 0.96 0.00 2.88 17.31 8.65 5.77 45.19 63.46 63.46 22.00 24.12 6.88 10.81 55.04 70.70 57.41 69.36 50.78 64.74 42.18 64.43 19.82 62.29 21.46 50.06 20.56 52.60 64.29 74.77 60.92 72.88 65.88 70.82 59.38 68.07 22.28 62.14 20.56 57.95 20.23 53.87 50.78 69.36 51.43 71.16 45.45 72.81 37.87 68.68 20.15 64.34 20.56 46.24 20.88 56.51 57.41 71.04 56.67 71.13 50.59 68.87 48.81 63.91 22.03 62.54 22.52 50.70 24.08 48. 65.37 78.43 63.98 75.87 57.58 62.17 57.99 67.52 26.52 46.03 23.88 45.81 21.98 45.29 46.67 47.15 40.80 39.87 28.72 24.55 24.23 48.30 46.69 42.09 39.18 32.01 24.58 24.45 43.04 42.91 35.07 36.46 29.94 26.70 24.25 46.22 47.09 40.50 40.29 28.46 24.78 24.08 49.75 48.79 45.92 47.09 32.32 26.41 25. 31.88 31.61 30.31 29.42 25.10 25.16 24.47 33.93 32.02 32.40 29.64 26.44 25.21 24.83 32.16 31.90 29.78 27.19 25.52 25.05 23.14 32.20 32.00 28.87 29.56 25.16 25.15 24.52 35.08 34.13 30.96 34.11 28.30 26.95 24.68 35.53 39.11 35.19 31.90 23.41 26.76 23. 52.52 51.34 48.23 39.71 25.39 23.13 21.41 30.36 32.99 31.45 28.02 26.82 26.56 24.76 41.25 44.73 36.64 36.25 24.13 27.16 22.73 58.78 53.89 52.20 44.00 28.99 28.73 26.30 33.36 38.65 35.62 29.74 26.04 25.42 22.63 55.22 54.45 50.43 42.20 25.49 23.75 22. 36.42 36.07 34.33 33.79 26.60 26.53 24.79 39.69 44.84 38.59 35.52 24.44 28.04 22.49 61.65 56.20 56.06 46.38 28.88 28.72 26.93 19.55 43.84 45.47 17.59 44.60 42.75 16.11 43.51 39.64 2.77 41.37 34.35 1.21 35.07 29.40 0.09 27.62 26.80 0.08 27.29 26.17 22.45 47.56 52.63 18.26 45.94 49.68 15.75 43.62 45.11 6.36 41.04 39.40 2.34 35.01 29.88 0.18 27.12 27.82 0.02 26.69 25.82 20.88 43.45 44.25 19.30 43.83 41.71 6.24 39.89 39.34 13.88 39.37 36.04 2.31 35.56 32.15 0.57 28.33 28.42 0.15 27.45 27. 18.79 46.25 46.73 19.21 47.29 43.86 17.59 45.64 40.51 5.35 42.14 35.82 2.03 36.62 30.00 0.12 27.78 27.50 0.13 27.29 26.03 24.50 49.33 54.34 20.21 45.37 52.59 7.12 39.67 47.11 2.96 39.34 42.00 6.30 36.11 32.23 5.09 28.47 28.48 0.46 28.38 27.47 Table 11: The main results of the Llama3-8B model across multiple natural language benchmarks using candidate models: Meta-Llama-3-8B-Instruct (LM), MathCoder2-Llama-3-8B (Math), CodeLlama-3-8B (Code), and Meta-Llama-3-8B (Base). \"PTM\" (Pruning-then-Merging) refers to first pruning each candidate model using the current pruner and then merging them. \"MTP\" (Mergingthen-Pruning) refers to first merging the candidate models and then applying pruning. LLM Pruner Type ratio/layer"
        },
        {
            "title": "Understanding",
            "content": "CMNLI HeSw PIQA CHID WSCP WSCG CSQA BoolQ MMLU CMLU RaceH RaceM XSum C"
        },
        {
            "title": "Dense",
            "content": "Llama3 -8B ShortGPT (24.6%) Base 32.98 74.67 80.96 73.78 56.73 36.54 73.79 69.97 64.74 50.79 63.21 70.54 3.28 55.18 57.65 LM 33.00 71.08 80.69 65.53 55.77 69.23 76.66 78.87 65.97 53.64 76.44 81.75 17.97 63.95 63.61 Math 32.99 71.66 77.97 57.09 37.50 58.65 68.22 69.08 62.08 45.85 64.75 69.08 8.68 53.86 55.53 Code 32.98 65.56 74.70 78.42 61.54 61.54 63.47 78.35 48.03 34.55 52.40 58.43 19.36 46.41 55.41 1.17 38.96 38.12 Base 36.00 31.36 62.84 25.77 36.54 63.46 53.97 50.61 36.05 33.83 30.73 32.38 3.68 43.51 39.94 LM 32.83 45.06 65.78 23.38 41.35 53.85 39.56 63.73 32.37 28.69 40.14 45.19 Math 32.98 42.89 63.00 17.18 36.54 36.54 45.37 46.30 33.95 29.71 28.87 30.22 1.45 40.49 34.68 3.57 39.01 34.85 Code 32.26 45.99 64.96 17.03 36.54 36.54 36.20 63.98 28.78 26.25 27.27 29.46 3.64 44.33 37.08 MTP 32.98 48.51 64.85 18.33 36.54 35.58 42.83 67.06 33.05 28.73 30.07 32.66 3.66 44.27 37.10 PTM 32.95 48.58 64.96 18.43 36.54 35.58 42.83 67.22 33.05 28.71 30.16 32.45 20 Table 12: Architecture Parameters of pruned 13B models"
        },
        {
            "title": "Layer",
            "content": "0 1 2 3 4 5 6 7 8 9 10 11 12 Model-1 Merge Factor - - - - - - - - - - - - -"
        },
        {
            "title": "Base\nLM\nLM\nLM\nLM\nCode\nBase\nLM\nLM\nLM\nLM\nLM\nLM",
            "content": "Model-2 Merge Factor - - -"
        },
        {
            "title": "Type",
            "content": "Output Scale 1.00 LM 1.00 LM+Math 0.64 1.00 LM+Code 0.60 LM 1.00 1.00 LM 1.00 LM+Math 0.59 1.00 LM 1.00 LM+Math 0.60 LM 1.00 LM 1.00 LM 1.00 1.00 LM+Code 0.66 0.91 LM+Code 0.60 - - - - Output Scale 1.00 1.00 1.05 1.00 1.00 1.00 1.00 1.00 1.00 0.84 1.02 0.77 1."
        },
        {
            "title": "1.00 LM+Math 0.60",
            "content": "1.00 - - - -"
        },
        {
            "title": "14 LM+Math 0.70\nLM\n15\nBase\n16\nLM\n17\n18\nLM\n19 LM+Code 0.70\n20 LM+Code 0.70\nLM\n21\nLM\n22\nLM\n23\n24\nLM\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39",
            "content": "- - - - REMOVED REMOVED REMOVED REMOVED REMOVED REMOVED REMOVED REMOVED REMOVED - - - - - REMOVED"
        },
        {
            "title": "LM\nBase\nLM\nLM\nLM",
            "content": "- REMOVED"
        },
        {
            "title": "Base",
            "content": "1.00 1.00 1.00 1.00 1.00 1.00 1.07 1.00 - - - REMOVED - REMOVED - REMOVED REMOVED REMOVED - REMOVED REMOVED REMOVED - - REMOVED - - - 21 LM"
        },
        {
            "title": "LM\nLM\nMath",
            "content": "1.00 1.00 1.00 1.00 0.75 Model-3 Merge Factor - - 0.60 0.60 - 0.58 - 0.60 0.59 - - 0.66 0.60 Output Scale 1.00 1.00 1.05 1.00 1.00 1.00 1.00 1.00 1.00 0.93 1.22 1.00 1.13 0.60 1.11 - 0.66 0.60 - 1.00 1.00 1.00 1."
        },
        {
            "title": "Type",
            "content": "LM Base LM+Code LM+Code LM LM+Math LM LM+Math LM+Code LM LM LM+Math LM+Code LM+Math +Code LM LM+Math LM+Math LM"
        },
        {
            "title": "REMOVED",
            "content": "LM+Code 0.60 1."
        },
        {
            "title": "REMOVED",
            "content": "Base LM+Math - 0.60 1.07 1."
        },
        {
            "title": "REMOVED",
            "content": "1."
        },
        {
            "title": "Base",
            "content": "- 1."
        },
        {
            "title": "REMOVED",
            "content": "1.04 LM - 1."
        },
        {
            "title": "REMOVED\nREMOVED\nREMOVED",
            "content": "1."
        },
        {
            "title": "Base",
            "content": "- 1."
        },
        {
            "title": "REMOVED",
            "content": "LM - 1."
        },
        {
            "title": "REMOVED",
            "content": "1.00 1.13 1.00 1.00 1."
        },
        {
            "title": "LM\nMath\nMath",
            "content": "- -"
        },
        {
            "title": "REMOVED",
            "content": "- - - 1.00 1.28 1.00 1.00 1.00 Table 13: Architecture Parameters of pruned 7B models"
        },
        {
            "title": "Layer",
            "content": "Model-"
        },
        {
            "title": "Type",
            "content": "Merge Factor - LM LM+Math+Code 0.50 0 1 - LM 2 - LM 3 - LM 4 0.59 LM+Code 5 - Code 6 - Code 7 - LM 8 - LM 9 - Base 10 0.50 LM+Math 11 - LM 12 - Math 13 0.60 LM+Math 14 - LM 15 16 0.50 LM+Math 17 LM+Math+Code 0.50 18 0.50 Math+Code REMOVED 19 20 REMOVED 21 22 23 24 25 26 27 28 29"
        },
        {
            "title": "REMOVED\nREMOVED\nREMOVED\nREMOVED\nREMOVED",
            "content": "REMOVED 0.50 REMOVED 0.50 LM+Code LM+Math LM LM - - Model-2 Merge Factor"
        },
        {
            "title": "Type",
            "content": "Output Scale 1.00 Math+Code 0.48 LM 1.00 LM+Code 1.03 Base 1.00 LM 1.04 LM+Math 1.08 Math 1.19 LM+Code 0.88 LM 1.28 LM 0.86 LM 1.00 Math 1.00 LM+Math 1.00 LM+Math 1.00 LM 1.00 Code 1.18 LM+Math 1.00 Code 1.00 Base 1.00 - 0.52 - - 0.38 - 0.50 - - - - 0.41 0.50 - - 0.50 - -"
        },
        {
            "title": "Type",
            "content": "Model-3 Merge Output Factor Scale 0.48 LM+Math 1.00 - LM 1.00 - LM 1.06 - Math 0.98 - LM 1.11 - LM 1.12 - Code 1.25 0.50 LM+Code 0.77 - LM 1.34 0.51 LM+Code 0.93 - LM 1.00 - LM 1.02 0.41 LM+Math 0.99 0.58 LM+Math 1.20 0.54 LM+Math 1.00 - Code 0.97 1.00 0.45 LM+Math 1.00 Math+Code 0.50 Base 1.00 - Output Scale 0.92 1.00 1.03 1.05 1.11 1.13 1.11 0.77 1.19 0.56 1.00 1.05 1.00 1.20 1.00 1.05 1.00 1.00 1."
        },
        {
            "title": "REMOVED\nREMOVED\nREMOVED\nREMOVED\nREMOVED",
            "content": "LM - 1."
        },
        {
            "title": "REMOVED\nREMOVED",
            "content": "LM - 1."
        },
        {
            "title": "Base\nLM\nLM",
            "content": "- - - 0.99 1.00 1.00 LM - 0."
        },
        {
            "title": "REMOVED",
            "content": "LM+Code 0.50 1."
        },
        {
            "title": "REMOVED",
            "content": "LM+Math 0.50 1.00 1.00 1.00 1. 1."
        }
    ],
    "affiliations": [
        "ELLIS Institute Tübingen",
        "Max Planck Institute for Intelligent Systems",
        "Sun Yat-sen University",
        "Tübingen AI Center",
        "University of Oxford",
        "University of Surrey",
        "University of Tübingen"
    ]
}