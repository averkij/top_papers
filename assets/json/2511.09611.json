{
    "paper_title": "MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation",
    "authors": [
        "Ye Tian",
        "Ling Yang",
        "Jiongfan Yang",
        "Anran Wang",
        "Yu Tian",
        "Jiani Zheng",
        "Haochen Wang",
        "Zhiyang Teng",
        "Zhuochen Wang",
        "Yinjie Wang",
        "Yunhai Tong",
        "Mengdi Wang",
        "Xiangtai Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 3 1 1 6 9 0 . 1 1 5 2 : r Preprint, Under Review. MMADA-PARALLEL: MULTIMODAL LARGE DIFFUSION LANGUAGE MODELS FOR THINKING-AWARE EDITING AND GENERATION Ye Tian1,2 Ling Yang3 Jiongfan Yang1 Anran Wang2 Yu Tian2 Haochen Wang 2,4 Zhiyang Teng2 Zhuochen Wang2 Yinjie Wang5 Yunhai Tong1 Mengdi Wang3 Xiangtai Li2 1Peking University 2ByteDance 3Princeton University 4CASIA 5The University of Chicago Huggingface: MMaDA-Parallel-Model Code: MMaDA-Parallel-Code Jiani Zheng"
        },
        {
            "title": "ABSTRACT",
            "content": "While thinking-aware generation aims to improve performance on complex tasks, we identify critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), novel strategy that applies semantic rewards along the trajectory to enforce crossmodal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving 6.9% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing more robust paradigm for thinking-aware image synthesis."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in multimodal generative models have achieved remarkable progress in instructionbased image generation and editing (Esser et al., 2024a; Labs, 2024; Wei et al., 2024; Liu et al., 2025b). Given diverse textual prompts, these models can produce visually coherent and semantically aligned results across wide range of tasks. However, these models often struggle with complex instructions that require reasoning over world knowledge, frequently leading to incorrect editing and generation (Wu et al., 2025c; Niu et al., 2025; Zhao et al., 2025). To mitigate this gap, recent studies have introduced intermediate reasoning steps before visual generation (Fang et al., 2025; Jiang et al., 2025a; Deng et al., 2025a). In these approaches, textual reasoning is first performed to guide subsequent image editing and generation. Such explicit reasoning has proven effective in improving the quality and consistency of image editing and generation (Deng et al., 2025a). Despite the general effectiveness of incorporating reasoning process prior to image synthesis, we observe counterintuitive and critical phenomenon. On certain benchmarks (Wu et al., 2025c), the inclusion of reasoning can in fact reduce the semantic fidelity of the generated images. For example, in Figure 1(a), thinking-aware model starts with correct reasoning but then shifts to refining minor details like background textures. This reduces attention on the primary subject and causes the final edit to misidentify it completely. The resulting image thus deviates from the users core instruction and even contradicts its own thinking prompt, leading to clear performance drop. This raises crucial question: What underlies this performance degradation? Equal Contribution. Correponding Authors 1 Preprint, Under Review. Figure 1: Sequential vs. parallel thinking-aware image synthesis. (a) Sequential generation (Bagel, GPT4o) may suffer from vague or incorrect reasoning. (b) Parallel generation aligns text and image at each denoising step, reducing hallucination and errors. (c) Quantitative comparison shows reasoning can degrade performance in certain categories. (d) Poorer categories also exhibit weaker reasoningimage alignment, highlighting the need for stronger cross-modal alignment. Based on these failure cases, we hypothesize that the degradation stems from the reasoning text itself. However, this hypothesis is difficult to verify with existing benchmarks (Wu et al., 2025c; Zhao et al., 2025). These benchmarks only evaluate the final image against the initial prompt, but cannot evaluate the intermediate reasoning step or its alignment with the final output. Therefore, we introduce ParaBench, our new benchmark designed to explicitly evaluate this output alignment between models generated reasoning and its final image. Using ParaBench to evaluate the state-of-the-art model Bagel (Deng et al., 2025a), we find strong correlation: performance degradation occurs precisely in categories where output alignment is weakest (Figure 1(d)). We attribute this to the compounding errors inherent in sequential autoregressive models, where ambiguous or incomplete reasoning provides unreliable guidance for the subsequent image generation, ultimately degrading the final output. Thus, while pre-reasoning can in principle enhance multimodal generation, its reliance on an autoregressive pipeline makes the process vulnerable to error accumulation and semantic drift. Recently, another line of work has explored discrete diffusion models for text or image generation (Nie et al., 2025; Yang et al., 2025a; Ye et al., 2025a), which remove the token-by-token constraint of autoregression and instead employ confidence-based sampling to achieve greater global consistency. Inspired by these advances, we ask: What if multimodal models could generate text and images in parallel? Such paradigm directly addresses the limitations of AR reasoning: text and images can attend to each other at every denoising step, avoiding the propagation of hallucinations and vague priors while grounding textual descriptions in visual evidence. Building on this insight, we propose purely diffusion-based framework for parallel textimage generation, where cross-modal interaction is maintained throughout the trajectory to ensure robust and semantically faithful multimodal editing and generation, as shown in Figure 1(b)). To train this framework, we first establish thinking-aware data curation pipeline. We prompt powerful VLM with data triplets (input image, edit instruction, output image) sourced from widely-adopted image editing and generation datasets. The VLM is tasked to generate reasoning trace that explains the edit process. This pipeline yields training dataset of quadruplets: input image, instruction, reasoning trace, output image, designed to elicit the models reasoning and generation capabilities. We use this dataset to perform supervised fine-tuning on MMaDA (Yang Preprint, Under Review. Figure 2: MMaDA-Parallel supports parallel, thinking-aware image editing and generation. Compared with Bagel, MMaDA-Parallel demonstrates superior reasoning quality and stronger alignment between the generated text and image outputs. et al., 2025a). This parallel version, MMaDA-Parallel, demonstrates higher output consistency compared to sequential baselines, as can be observed in Figure 2. Notably, such consistency is observed not only in the final outputs but also throughout the generation trajectory. We observe that during the parallel denoising process, the image region corresponding to specific semantic concept is often refined simultaneously with its textual counterpart. However, standard SFT and conventional reinforcement learning algorithms optimize for the final outcome only. This output-level supervision is too coarse to enforce the fine-grained, stepwise alignment we observe and cannot guarantee consistency at intermediate steps. To fully leverage this trajectory-level consistency, we draw inspiration from process-level and trajectory-level optimization methods (Li & Li, 2024; Wang et al., 2025) and introduce Parallel Reinforcement Learning (ParaRL). Instead of focusing solely on the final outcome, ParaRL incorporates stepwise semantic supervision to refine alignment along the denoising trajectory. Our experiments demonstrate that this trajectory-level optimization provides more granular and effective signal for diffusion models compared to traditional output-level supervision. Extensive quantitative and qualitative results validate the effectiveness of MMaDA-Parallel for thinking-aware image editing and generation, and further highlight the additional gains achieved through ParaRL. Our contributions can be summarized as follows: 1. In-depth Benchmarking and Analysis of Thinking-aware Image Synthesis. We propose ParaBench, which systematically evaluates thinking-aware image generation and editing, focusing on text and image quality and their alignment. 2. Parallel Multimodal Diffusion Framework. We propose purely discrete diffusionbased approach for parallel thinking-aware image editing and generation, which enables bidirectional attention between modalities at every denoising step and effectively alleviates the error accumulation of autoregressive pipelines. 3 Preprint, Under Review. 3. Parallel Reinforcement Learning. We introduce parallel reinforcement learning strategy, ParaRL, which assigns semantic rewards along the denoising trajectory, further enhancing alignment between the output modalities and the overall performance. 4. Extensive Evaluation and State-of-the-Art Alignment. Our comprehensive experiments validate the framework, establishing state-of-the-art performance among open-source models with 6.9% gain in Output Alignment over Bagel on our ParaBench benchmark, while maintaining comparable performance on single-modality metrics."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent progress in multimodal models for image understanding, generation, and editing has been rapid, yet most approaches remain constrained to single-modal generation conditioned on multiple modalities (Esser et al., 2024b; Wu et al., 2025a; Labs et al., 2025; Bai et al., 2025). To improve the accuracy and fidelity of multimodal generation, growing line of work has explored introducing textual Chain-of-Thought reasoning process before image generation or editing. We refer to this paradigm as thinking-aware image generation and editing. For instance, early efforts such as Chameleon (Team, 2024) and Mogao (Liao et al., 2025) investigated interleaved generation, enabling interleaving sequences of text and image tokens. Image-CoT (Guo et al., 2025b) and GoT (Fang et al., 2025) incorporated CoT reasoning before image synthesis, demonstrating that reasoning traces can enhance generation quality. Bagel (Deng et al., 2025a) further extended this idea by integrating chain-of-thought reasoning into both image generation and editing, enabling more flexible and semantically aligned outputs. Building on this direction, follow-up works such as OmniGen2 (Wu et al., 2025b) and IRG (Huang et al., 2025a) introduced reflective reasoning after image generation, using multi-turn textual feedback to refine visual outputs iteratively. Most existing methods, however, rely on sequential autoregressive interleaved pipeline, which could limit direct cross-modal interaction and make the model prone to error accumulation from imperfect reasoning traces. Exploring parallel generation framework that enables more interaction within output modalities is still lacking in this scenario. (More related work can be found in Appendix C)."
        },
        {
            "title": "3 MMADA-PARALLEL",
            "content": "3.1 FINDINGS AND BENCHMARKING ON THINKING-AWARE SYNTHESIS To investigate whether pre-generation reasoning genuinely enhances performance, we conduct controlled study on image editing tasks, which provides clearer instruction-grounded evaluation than naive synthesis. We sample inputs from established benchmarks (Wu et al., 2025c; Zhao et al., 2025) and generate paired outputs using Bagel (Deng et al., 2025a)an advanced, open-source, unified model supporting thinking-aware generationwith and without thinking. We report the average editing evaluation metrics in Kris-Bench (Wu et al., 2025c) in Figure 1(c) and also Table 1. Findings. While the reasoning step enhanced performance on most tasks, notable countertrend emerged: performance declined in significant subset of cases, about 23%, particularly in complex compositional edits. closer analysis reveals that these failures often stemmed from low-quality or vague reasoning text, which misguides the image generation process. This exposes critical gap in existing protocols: they evaluate the final image but ignore the quality of the intermediate reasoningthe other generated modality. Benchmarking mixed modalities. This analysis reveals fundamental limitation in current evaluation paradigms: existing benchmarks (Wu et al., 2025c; Zhao et al., 2025; Ghosh et al., 2023) only evaluate images, ignoring the quality of the reasoning itself and its consistency with the image. To address this gap, we introduce ParaBench, new benchmark specifically designed for the comprehensive evaluation of thinking-aware image synthesis. ParaBench comprises 300 challenging prompts, split into 200 for editing and 100 for generation. The editing prompts are meticulously curated to test wide spectrum of abilities, covering not only general operations (e.g., add, remove, replace) but also complex tasks requiring reasoning. The 100 generation prompts focus on openended creative synthesis of complex scenes. We evaluate models on ParaBench using the GPT-4.1 across six fine-grained aspects: for the textual output, we assess Text Quality and Text Alignment; 4 Preprint, Under Review. Table 1: Thinking may degrade the performace of visual synthesis. Bagels performance comparison on ParaBench editing tasks with and without thinking. We also report the reasoning quality (Text Qual.) and cross-modal alignment (Output Align.). Editing Category w/o Thinking w/ Thinking (w/ w/o) Text Qual. Output Align. Temporal General Causal Knowledge Spatial 72.3 68.9 70.1 74.5 69.8 75.6 71.4 67.2 76.8 65.0 +3.3 +2.5 2.9 +2.3 4. 92.6 86.2 75.3 87.8 73.2 57.3 58.1 46.2 55.5 45.2 for the visual output, we evaluate Image Quality, Image Alignment, and Image Consistency; and finally, the overall Output Alignment between them. More details are included in Appendix G. To demonstrate ParaBenchs diagnostic capabilities, we apply it to representative baseline, Bagel. While full quantitative results are presented in Sec A, Table 1 highlights crucial finding by focusing on two key metrics: Text Quality and Output Alignment. The results reveal clear correlation between the quality of the reasoning step and the final performance. Notably, the categories that exhibited performance degradation also suffered from significant drops in both reasoning quality and reasoning-image synergy. This pattern strongly suggests that poor reasoning does not merely fail to provide helpful guidance but actively misleads the generation process, validating the necessity of explicitly improving the synergy between text and image generation. Motivations on parallel multimodal diffusion. Our benchmarking results reveal critical limitation in current thinking-aware generation: the sequential generation paradigm, where reasoning precedes image synthesis, creates rigid dependency that can propagate errors and limit cross-modal synergy. When reasoning quality degrades, it directly undermines the subsequent image generation, as demonstrated by the correlated performance drops in spatial and temporal editing tasks. To address this fundamental issue, we propose parallel unified multimodal diffusion framework that enables simultaneous generation of both reasoning text and images, fostering genuine multimodal collaboration while eliminating the error propagation inherent in sequential approaches. Figure 3: Parallel Generation Architecture: During (a) training, image and text responses are masked and predicted in parallel with uniform mask predictor, optimized by the masked token likelihood objective. During (b) sampling, the model performs parallel decoding to generate both image and text responses jointly, enabling efficient multimodal response generation. 3.2 BASIC ALGORITHM AND ARCHITECTURE Discrete diffusion models have demonstrated strong performance for both image and text generation (Bai et al., 2024; Nie et al., 2025; Zhu et al., 2025). Building on the unified discrete-diffusion view, MMaDA (Yang et al., 2025a) demonstrates that single diffusion framework can jointly model multiple modalities; however, its decoding remains sequential across modalities. To overcome this limitation, we propose parallel multimodal diffusion framework that: (i) represents all modalities as discrete tokens, (ii) arranges them in an interleaved sequence with bidirectional attention, and (iii) employs single mask predictor shared across modalities, enabling synchronous denoising for both text and images. An overview of this framework is shown in Figure 3. 5 Preprint, Under Review. Interleaved discrete sequence layout. Following the MMaDA framework (Yang et al., 2025a), we process both text and images within unified discrete token space. Specifically, we tokenize text using the LLaDA tokenizer (Nie et al., 2025) and encode images into grid of discrete visual tokens using pretrained MAGVIT-v2 (Yu et al., 2023) quantizer. These tokenized modalities are then serialized into single interleaved sequence, using explicit sentinels and task tags to enable full bidirectional cross-modal attention: Input: <task><soi>[img]<eoi><bos>[text]<eos> Output: <soi>[output img]<eoi><bos>[output text]<eos> During training, we concatenate the input and output templates into single sequence, allowing the model to attend from outputs to inputs within unified context. The task token <task> is instantiated differently depending on the scenario, with <thinkgen> used for thinking-aware generation and <thinkedit> used for thinking-aware editing. This single-sequence design eliminates the ordering asymmetry and exposure bias introduced by autoregressive cross-modal pipelines. Training objective. Let x0 {1, . . . , }L denote the concatenated training sequence (input part followed by output part), where is the total number of tokens in the sequence. We keep the input part static and apply noise only to the output part. At sampled timestep {1, . . . , }, for each token in the output part we replace it with [MASK] with probability βt and keep it unchanged with probability 1 βt; tokens in the input part are left unchanged: x(i) = (cid:40) x(i) 0 x(i) 0 with prob. (1 βt), [MASK] with prob. βt if in input, if in output. (1) k=1(1 βk), and is the one-hot distribution of [MASK]. Equivalently, for positions in the output, the absorbing-state marginal after steps is q(xt x0) = αt x0 + (1 αt) where αt = (cid:81)t The parallel diffusion model pθ( xt) is formulated as unified masked-token predictor over the joint vocabulary of text and image tokens. Let 1, . . . , denote token positions in the concatenated inputoutput sequence. Since only the output segment is noised during diffusion, the model predicts ground-truth tokens x0 at the currently masked positions within this segment. To better balance the training dynamics across modalities, we make the timestep-dependent loss weight modality-specific: tokens in the output image segment and the output text segment are assigned separate weights, wimg(t) and wtext(t). For compactness, we write the objective using unified token-aware weight function w(t, i). We optimize timestep-reweighted cross-entropy: (cid:35) Lparallel(θ) = Et, x0, xt w(t, i) 1(cid:2)x(i) = [MASK](cid:3) log pθ (cid:0)x(i) 0 (cid:1) xt , (2) (cid:34) (cid:88) i=1 where 1[] is the indicator function and (cid:40)wimg(t), wtext(t), w(t, i) = if lies in the output image segment, if lies in the output text segment. We empirically find that applying timestep-dependent weighting wtext(t) = 1/t for text tokens and constant weighting wimg(t) = 1 for image tokens substantially stabilizes the training of image quality and output alignment. We illustrate this process in Figure 3(a) and include detailed additional preliminaries with ablations in Appendix D. Parallel denoising with dual schedulers. Decoding proceeds along shared diffusion time axis tT t0, as is shown in Figure 3(b). We define two modality-specific schedulers, uimg(t), utext(t) [0, 1], which specify the target proportion of unmasked tokens at step t. At each reverse step: (i) the model jointly predicts distributions for all currently masked positions; (ii) for each modality, fraction of tokens is sampled (e.g., via confidence-based sampling), while the remaining positions are retained as [MASK]. Because attention is bidirectional across the entire sequence, text and image can inform each other at every step of decoding. In our experiments, the text schedule is implemented as fully linear reveal schedule combined with semi-autoregressive confidence-based decoding Nie et al. (2025), while the image schedule follows cosine reveal schedule with global confidence-based decoding. More details can be found in Appendix E. 6 Preprint, Under Review. Figure 4: Overview of our proposed Parallel Reinforcement Learning (ParaRL). Rather than optimization only to the final denoised outputs, ParaRL introduces reward signals along the entire denoising trajectory, reinforcing semantic alignment consistently throughout the generation process. 3.3 POST TRAINING WITH PARALLEL REINFORCEMENT LEARNING Supervised Finetuning for Parallel Synthesis key challenge in our approach is that existing generation and editing datasets lack the reasoning traces required for our parallel synthesis framework. To address this, we construct suitable training dataset by first aggregating samples from various sources. For each sample comprising an input image (for editing tasks), an instruction, and the final output image, we employ multimodal LLM (Qwen-2.5-VL in our implementation) to generate corresponding reasoning trace. Further details on the dataset construction process, including the sources and categories, are provided in Appendix F. We then use this dataset to perform supervised fine-tuning on MMaDA (Yang et al., 2025a). This process adapts it into parallel variant capable of performing thinking-aware synthesis, where reasoning and generation occur concurrently. Synergy along the denoising trajectory. While analyzing generations from the finetuned model, we observe that certain semantic concepts emerge synchronously in text and image at intermediate denoising steps. As illustrated in Figure 5, when tasked to change shirt to vibrant rainbow color, the specific color words and their corresponding visual features appear at the same timestep. This observation leads to key insight: cross-modal alignment is not an endpoint phenomenon but is progressively established throughout the generation trajectory. This implies that supervision applied to these intermediate steps, not just the final output, can further improve this alignment. Figure 5: Synergy of sampling. Given the prompt: change the blue shirt to vibrant rainbow color, the specific color decoding in text and image emerges at the same step. Parallel reinforcement learning with trajectory optimization. Building on this insight, we further introduce Parallel Reinforcement Learning (ParaRL), novel training paradigm that directly leverages this intermediate cross-modal synergy. Instead of rewarding only the final output, ParaRL uses the alignment between text and image tokens at each denoising step as dense reward signal. trajectory τi Specifically, (cid:0)τi(1), . . . , τi(τi)(cid:1), where τi is the total number of denoising steps and τi(t) is the set of tokens decoded at step t. While this formulation provides step-wise reward ri,t for each intermediate response τi(t), optimizing over the entire dense trajectory is computationally prohibitive. To make training feasible, we adopt sparse optimization strategy. During each online rollout, we pre-select sampling steps and fix subset of step indices {1, . . . , τi}, = and only compute rewards ri,t and their corresponding standardized advantages Ai,t for timesteps S. We adapt diffusion GRPO objective (Gong et al., 2025) that accommodates token-level likelihood ratios with the generated response is full for given query Q, 7 Preprint, Under Review. advantages calculated at these sampled steps: Jpolicy(θ) = QDtask {τi}G i=1πold(Q) β KL(cid:2)πθ πold (cid:3), (cid:88) (cid:88) i=1 tS 1 τi(t) (cid:88) oτi(t) Cϵ (cid:18) πθ(o Q, τi(1:t 1)) πold(o Q, τi(1:t 1)) (cid:19) , Ai,t (3) where Cϵ(r, A) min(cid:0)rA, clip(r, 1 ϵ, 1 + ϵ) A(cid:1). In this objective, the summation is performed over the sparsely sampled steps S. The term ranges over all tokens within the state τi(t) at sampled step t, and τi(1:t 1) denotes the full history of tokens generated prior to step t. Finally, πold is the behavior policy for generating rollouts, and β controls the KL penalty strength. Trajectory reward design. In typical trajectory-level optimization frameworks, well-trained process reward model (PRM) (Li & Li, 2024) or value function Wang et al. (2025) is often required, since intermediate partial outputs usually lack sufficient semantic information for reliable evaluation. Surprisingly, in our parallel textimage generation setting, we find that intermediate fragments are already semantically meaningful. For instance, even partially decoded text tokens often reveal enough semantic cues to compute alignment with the simultaneously generated image content, as illustrated in Figure 4. This observation allows us to bypass the need for dedicated PRM: we directly employ semantic alignment between text and image as the reward signal. Unlike tasks with binary rewards (e.g., mathematical reasoning), our cross-modal alignment objective provides continuous reward signal. However, the naive CLIP score, which serves as our reward source, can exhibit high variance and an arbitrary scale, making it unstable for direct use in reinforcement learning. To ensure training stability, we therefore apply normalization scheme inspired by prior work in RL with continuous rewards (Liu et al., 2025a). We begin by estimating the mean µCLIP and standard deviation σCLIP of CLIP scores across the training distribution, where we compute on random 1% subset of the data. Let ci,t = RCLIP(text(τi(t)), image(τi(t))) be the raw CLIP score for the content generated at step t. We first standardize this score to obtain ˆci,t using ˆci,t = ci,tµCLIP . This standardized score is then clipped to the range [1, 1] and linearly rescaled to yield the final reward Ri,t, which is bounded within [0, 1]: σCLIP The corresponding advantages Ai,k used in Eq. 3 are then obtained by standardization over the Ri,t = 1 2 (1 + clip(ˆci,t, 1, 1)) (4) rollouts: Ai,t = Ri,tmean({Rj,t}G j=1) std({Rj,t}G j=1)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 IMPLEMENTATION DETAILS Training and datasets. Our final model, MMaDA-Parallel, is trained in two-stage process. We begin with supervised finetuning (SFT) on the MMaDA-MixCoT model, which integrates LLaDA8B text backbone with MagVIT-v2 image tokenizer. For this stage, we construct new dataset of 150K thinking-aware image editing and generation pairs, meticulously sourced and filtered from multiple existing benchmarks. In the second stage, we apply reinforcement learning with GRPObased objective. To enhance training efficiency, this RL stage focuses on the most challenging 10% of the SFT examples, optimizing the policy online to improve cross-modal semantic alignment. More details of the dataset and training details can be found in Appendix and H. Evaluation setup. We conduct our primary evaluation on the ParaBench benchmark, which was introduced in the Method section. We employ an LLM-as-a-judge framework (GPT-4.1) to assess performance across the six fine-grained metrics previously described, covering text quality, image fidelity, and cross-modal alignment. The prompts used for the LLM judge are detailed in the Appendix G. Our MMaDA-Parallel is compared against state-of-the-art thinking-aware models, including Bagel (Deng et al., 2025a), GPT-4o, and Gemini-2.5, as well as leading image-only generators like Qwen-Image (Wu et al., 2025a), Qwen-Image-Edit (Wu et al., 2025a), Flux.1-dev (Labs, 2024) and Flux.1-Kontext (Labs et al., 2025). 8 Preprint, Under Review. Figure 6: Qualitative results in comparison with Bagel. 9 Preprint, Under Review. Table 2: Main results on ParaBench. Evaluation across all editing and generation tasks. For non-thinking image editing or generation models, text evaluation and output alignment cannot be computed. Model Text Qual. Text Align. Image Cons. Image Align. Image Qual. Output Align. Overall Open-source models (Non-thinking) Flux.1-Dev Qwen-Image Flux.1-Kontext Qwen-Image-Edit Bagel (w/o think) Closed-source models GPT-4o Gemini-2.5 - - - - - 92.5 94.1 Open-source models (Thinking-aware) Bagel (w/ think) Show-o* (tuned) MMaDA-Parallel w/o Para-RL MMaDA-Parallel w/ Para-RL 82 75.2 76.5 80.4 4.2 MAIN RESULTS - - - - - 93.4 95.2 70.5 70. 70.4 71 - - 77.9 78.2 72.2 86.2 88.5 76.7 69.1 70.5 73.4 65.2 67.2 65 73.5 50. 85.7 76.2 63.4 57.5 58.2 63.2 77.5 84.2 84 84.1 80.1 88.1 90.2 81.5 78. 80.5 81.2 - - - - - 69.5 63.4 52.9 48.9 51.5 59.8 - - - - - 85.9 84.6 71.2 66.6 67.9 71.5 Table 2 reports the overall performance on our ParaBench benchmark. Our proposed method, MMaDA-Parallel, achieves the highest Output Alignment among all open-source models, confirming the effectiveness of its parallel multimodal decoding and trajectory-level optimization. In terms of general text and image quality, MMaDA-Parallel performs on par with Bagel, despite Bagel being trained on dataset nearly three orders of magnitude larger. Compared to leading closed-source models like GPT-4o and Gemini-2.5, MMaDA-Parallel substantially narrows the gap in alignment metrics while maintaining competitive text and image quality, demonstrating remarkable data efficiency. Furthermore, the results indicate that our ParaRL stage consistently improves output textimage consistency, suggesting that trajectory-level optimization effectively strengthens cross-modal grounding throughout the generation process. In addition, we provide qualitative comparison with open-source models in Figure 6, showcasing examples of both editing and generation. key observation is that MMaDA-Parallel produces more precise and descriptive reasoning traces. This enhanced reasoning leads to superior visual fidelity in the final image. For instance, our model accurately renders complex instructions like melting cake and correctly applies causal reasoning to depict withered grass. Moreover, MMaDAParallel demonstrates stronger compositional abilities, particularly in counting, correctly generating three people or two faces of clock where Bagel often fails. In contrast, Bagels reasoning in these challenging cases tends to be vague or omits crucial details, leading to inaccurate image synthesis. These results further underscore MMaDA-Parallels capability for advanced thinking-aware editing and generation, driven by better-aligned semantic information. 4.3 ANALYSIS OF KEY CONTRIBUTIONS Table 3: Parallel vs sequential decoding. Denoising Text Align. Image Align. Output Align. Sequential Parallel 70.6 70.4 56.1 58. 48.9 51.5 Table 4: Output vs trajectory-level RL. Model Text Align. Image Align. Output Align. before RL w/ Output-level RL w/ ParaRL (Ours) 70.4 70.7 71 58.2 62.3 63.2 51.5 53.6 59.8 ParaRL Text Qual. Text Align. Image Cons. Image Align. Image Qual. Output Align. Overall Table 5: Ablation on sampling steps in ParaRL. Before RL ParaRL s=2 ParaRL (s=3) (default) ParaRL (s=4) 76.5 77.9 80.4 80.5 70.4 70.3 71.0 70. 70.5 71.5 73.4 73.2 10 58.2 62.8 63.2 63.5 80.5 80.7 81.2 80.8 51.5 53.6 59.8 58.7 67.9 68.6 71.5 71. Preprint, Under Review. After presenting the overall results, we now return to the two central research questions that motivated our work: RQ1: Does parallel denoising improve generation quality compared with sequential denoising? RQ2: Does trajectory-level finetuning improve over output-level finetuning? The Benefit of Parallel Decoding (RQ1). We compare our model against sequential baseline (MMaA-Sequential) that generates text before images. During training, noise was applied to only one modality at time to align with this sequential inference process. Table 3 shows our parallel framework substantially outperforms this baseline on key alignment metrics, with comparable text and image quality. This result validates our core hypothesis: simultaneous, interactive decoding is crucial for reducing error propagation and producing coherent multimodal outputs. The Benefit of Trajectory-Level Optimization (RQ2). We compare two reinforcement learning strategies: (i) output-level RL, where rewards are computed on the final generated sample, and (ii) our proposed ParaRL with trajectory-level finetuning, where rewards are aggregated across denoising steps. As shown in Table 4, trajectory-level optimization yields gains in textimage consistency and output alignment, and Figure 7 further shows that it enables more stable training dynamics. Another key hyperparameter in this strategy is the number of sampled steps, s. We analyze its impact in Table 5 and report the training curve in Figure 8. We find that using = 3 or = 4 yields substantial improvements over = 2, as denser reward signal provides more stable guidance. We adopt = 3 in the final configuration for the best balance between performance and efficiency. Figure 7: ParaRL reward training curve between trajectory and output level optimization. Figure 8: ParaRL reward training curve across different sampling steps of the trajectory."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we investigated critical phenomenon where sequential thinking-aware models can paradoxically suffer from performance degradation on complex tasks. We conducted an in-depth analysis using our proposed ParaBench benchmark, which uniquely evaluates both output modalities, and found strong correlation between this degradation and poor alignment between the generated modalities. To resolve this, we propose parallel multimodal diffusion framework trained with supervised finetuning and further optimized by Parallel Reinforcement Learning (ParaRL)our novel method of applying rewards along the entire denoising trajectory. Experiments validate that our approach significantly improves cross-modal alignment and semantic consistency, establishing more robust paradigm for thinking-aware image synthesis. 11 Preprint, Under Review."
        },
        {
            "title": "APPENDIX CONTENTS",
            "content": "A Scaling of MMaDA-Parallel Additional Results B.1 Qualitative results . . B.2 Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . More Related Work Preliminaries D.1 Preliminaries of discrete Diffusion Models. . . . . . . . . . . . . . . . . . . . . . D.2 Group Relative Policy Optimization for Discrete Diffusion Models . . . . . . . . . Sampling Details on Text and Image Details of Training Dataset Curation Details of ParaBench More Implementation Details More Ablation Studies Limitations and Future Work Prompts for evaluation 12 12 12 13 16 17 18 19 20 21 22 22 SCALING OF MMADA-PARALLEL To further validate our MMaDA-Parallel on larger-scale training, we extend our post-training framework on Lumina-DiMOO Xin et al. (2025). Lumina-DiMOO shares similar architecture with MMaDA, but benefits from much larger-scale data training. same training settings for LuminaDiMOO, and report its corresponding quantitative and qualitative results in Table 6 and Figure 9. The results clearly show that after applying our Parallel framework and ParaRL post-training, Lumina-DiMOO surpasses Bagel and achieves new state-of-the-art performance in thinking-aware synthesis. This finding strongly validates the scalability of our method."
        },
        {
            "title": "B ADDITIONAL RESULTS",
            "content": "B.1 QUALITATIVE RESULTS We provide more qualitative results in Figure 10 and Figure 11 for thinking-aware image editing and generation. B.2 QUANTITATIVE RESULTS We also report additional image-only results of MMaDA-Parallel on RISEBench Wu et al. (2025c) and GenEval Ghosh et al. (2023). For fair comparison, we evaluate against the sequential version 12 Preprint, Under Review. Figure 9: Additional qualitative results with the same training settings. Table 6: Main results on ParaBench. Post-training with Lumina-DiMOO. Model Text Qual. Text Align. Image Cons. Image Align. Image Qual. Output Align. Overall Open-source models (Non-thinking) Flux.1-Dev Qwen-Image Flux.1-Kontext Qwen-Image-Edit Bagel (w/o think) Closed-source models GPT-4o Gemini-2.5 Open-source models (Thinking-aware) Bagel (w/ think) Show-o* (tuned) MMaDA-Parallel * w/o Para-RL MMaDA-Parallel * w/ Para-RL - - - - - 92.5 94.1 82.0 75.2 82.6 84. - - - - - 93.4 95.2 74.5 70.7 73.7 76.5 - - 77.9 78.2 72.2 86.2 88. 76.7 69.1 71.3 71.0 65.2 67.2 65 73.5 50.3 85.7 76.2 63.4 57.5 64.6 67. 77.5 84.2 84 84.1 80.1 88.1 90.2 81.5 78.5 82.6 83.6 - - - - - 89.5 83. 52.9 48.9 63.3 68.8 - - - - - 89.2 88.9 71.8 66.6 73.0 75. of MMaDA, where MMaDA-Parallel achieves consistent performance improvements, demonstrating that parallel generation leads to overall gains in image alignment. Compared with the original MMaDA, our approach further narrows the performance gap with Bagel."
        },
        {
            "title": "C MORE RELATED WORK",
            "content": "Diffusion large language models. Diffusion models have achieved remarkable progress in vision (Ho et al., 2020; Rombach et al., 2022; Esser et al., 2024b; Song et al., 2020; Peebles & Xie, 2023), motivating their extension to text. The discrete nature of textual tokens, however, makes direct adaptation non-trivial. Two main approaches have emerged: learning continuous latent representations (Chen et al., 2022; Mahabadi et al., 2023; Ye et al., 2023; Gong et al., 2022), and designing discrete diffusion models (Ou et al., 2024; Gong et al., 2024; Liu et al., 2025c; Ye et al., 13 Preprint, Under Review. Figure 10: Additional qualitative results on thinking-aware image editing. Table 7: Overall performance on RISEBench. . Models Temporal Causal Spatial Logical Overall GPT-4o-Image Gemini-2.0-Flash-exp BAGEL MMaDA(Sequential) MMaDA-Parallel 34.1% 8.2% 3.5% 3.9 % 4.2% 32.2% 37.0% 10.6% 28.9% 13.3% 15.5% 5.8% 4.4% 23.0% 9.0% 4.7% 5.9% 5.2% 5.5% 8.1% 8.3% 4.8% 5.1% 5.5% 5.75% 2025b; Zhu et al., 2025). Among the latter, Masked Diffusion Models (MDMs) stand out by leveraging bidirectional attention for global consistency and supporting parallel decoding. Systems such as Dream7B (Ye et al., 2025b) and LLaDA (Nie et al., 2025) achieve performance comparable to autoregressive LLMs. Beyond text, diffusion-based LLMs have also been extended to multimodal domains. LaViDA (Li et al., 2025) employs multi-view image encoding with masked-denoising 14 Preprint, Under Review. Figure 11: Additional qualitative results on thinking-aware image generation. Table 8: Results on GenEval.. Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall SDXL Show-o Xie et al. (2024) MMaDA (Yang et al., 2025a) Bagel (Deng et al., 2025a) MMaDA(Sequential) MMaDA-Parallel 0.98 0.95 0.99 0.98 0.99 0. 0.74 0.52 0.76 0.95 0.78 0.83 0.39 0.49 0.61 0.84 0.66 0.70 0.85 0.82 0.84 0.95 0.87 0. 0.15 0.11 0.20 0.78 0.34 0.40 0.23 0.28 0.37 0.77 0.37 0.47 0.55 0.53 0.63 0.88 0.68 0. training, LLaDA-V (You et al., 2025) integrates masked diffusion with visual instruction tuning, and MMaDA (Yang et al., 2025a) unifies reasoning across text and vision generation through chainof-thought supervision and reinforcement learning. These advances highlight the scalability and versatility of diffusion-based language models across both unimodal and multimodal settings. Nev15 Preprint, Under Review. ertheless, existing approaches have not yet explored parallel textimage co-generation, leaving cross-modal reasoning and alignment still constrained by sequential pipelines. Reinforcement learning for multimodal foundation models. Reinforcement Learning (RL) has emerged as powerful paradigm for enhancing reasoning and controllability in large models. The widely adopted GRPO (Guo et al., 2025a) applies rewards primarily on the correctness of the final answer and the adherence to predefined format. Recently, RL has been adopted in multimodal large language models (Chen et al., 2025b; Meng et al., 2025; Yang et al., 2025b; Zhang et al., 2025; Deng et al., 2025b; Huang et al., 2025b), incorporating task-specific rewards such as answer correctness, intersection-over-union (IoU) for localization (Liu et al., 2025d), and imagetext alignment scores (e.g., T2I-R1 (Jiang et al., 2025a)). Extensions such as (Jiang et al., 2025b; Hong et al., 2025) further introduce cross-modality coherence rewards. In the context of diffusion language models, similar strategies have been explored with verified rewards and carefully designed probability approximations (Yang et al., 2025a; Gong et al., 2025) . Despite these advances, most existing methods focus solely on rewards applied to the final output, while largely ignoring the generative trajectory. This overlooks the fact that intermediate steps can provide crucial signals for alignment. In contrast, our work investigates the synergy between modalities during the denoising process and introduces ParaRL, which exploits stepwise semantic alignment to optimize thinking-aware multimodal generation."
        },
        {
            "title": "D PRELIMINARIES",
            "content": "D.1 PRELIMINARIES OF DISCRETE DIFFUSION MODELS. In recent years, diffusion models have set new standards in generative modeling. While Denoising Diffusion Probabilistic Models (DDPMs) excel in continuous domains like raw pixel spaces, Discrete Denoising Diffusion Probabilistic Models (D3PMs) have proven highly effective for discrete data, such as tokenized images and text. Models like VQ-Diffusion Gu et al. (2022), MaskGIT (Chang et al., 2022), Muse (Chang et al., 2023), Show-o (Xie et al., 2024), and MMaDA Yang et al. (2025a) have demonstrated that discrete diffusion process can generate highfidelity outputs with great efficiency. Our models architecture is built upon this discrete diffusion paradigm. We now provide the formal preliminaries, beginning with the foundational forward and reverse processes and culminating in the simplified mask-and-predict training objective that our model employs. Forward and reverse processes. discrete diffusion model consists of two key processes: (1) The Forward Process (q), fixed Markov chain that gradually corrupts input data x0 over timesteps into noisy latents x1, . . . , xT ; and (2) The Reverse Process (pθ), learned neural network that reverses this corruption by progressively denoising xT to recover the original data distribution. Lets consider single token x0 {1, . . . , K} from codebook of size K. The forward process at each step is defined by stochastic transition matrix Qt RKK. key property is that the distribution of xt conditioned on the initial state x0 is tractable: q(xtx0) = Cat(xtx0Qt), where Qt = Q1Q2 Qt. The posterior probability, which is essential for training, is also tractable: q(xt1xt, x0) = q(xtxt1)q(xt1x0) q(xtx0) (cid:32) Cat xt1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) xtQ x0Qt1 x0Qtx (cid:33) , (5) (6) where denotes element-wise product. Absorbing mask state and transition matrix. The design of the transition matrix Qt dictates the nature of the corruption. highly effective approach, inspired by masked language modeling, is to introduce special absorbing [MASK] state. This expands the token vocabulary to + 1 states. Once token becomes [MASK], it remains masked for all subsequent timesteps. This explicitly signals corrupted positions to the model. The transition matrix for this Absorbing-Uniform process 16 Preprint, Under Review. is defined as: Qt = ωt + νt νt ... νt 0 νt ωt + νt ... νt 0 νt αt νt αt ... ... . . . ωt + νt αt 1 0 R(K+1)(K+1), (7) where at each step t, token has probability αt to be masked, probability βt to be replaced by random token, and probability ωt = (1 αt βt) to remain unchanged. The [MASK] token (last row) always transitions to itself. Objective as mask prediction. The training objective for diffusion models is derived by maximizing the Evidence Lower Bound (ELBO) on the data log-likelihood. The negative ELBO, which is minimized during training, can be decomposed into several terms representing different stages of the diffusion process: LELBO = Eq (cid:20) log pθ(x0x1) (cid:124) (cid:125) (cid:123)(cid:122) Reconstruction Term + (cid:88) t=2 KL(q(xt1xt, x0)pθ(xt1xt)) (cid:123)(cid:122) (cid:125) (cid:124) Denoising Matching + KL(q(xT x0)p(xT )) (cid:123)(cid:122) (cid:125) Prior Matching (cid:124) (cid:21) . (8) Here, the objective consists of three main components: (1) reconstruction term that learns to generate the final data from x1, (2) series of KL divergence terms that train the reverse process pθ to match the true posterior at each denoising step, and (3) prior matching term that aligns the final noisy latent with simple prior distribution. Following derivations in D3PMs Austin et al. (2021), this complex objective can be simplified to weighted sum of reconstruction terms: Lsimple = (cid:88) t=1 Eq(x0,xt)[ log pθ(x0xt)]. (9) When using the absorbing mask state strategy, this simplified objective becomes equivalent to Cross-Entropy loss for mask token prediction, as used in MaskGIT Chang et al. (2022). This approach is highly effective as it focuses the models capacity on reconstructing only the corrupted parts of the data. Our work leverages this powerful paradigm for both text and image token generation. D.2 GROUP RELATIVE POLICY OPTIMIZATION FOR DISCRETE DIFFUSION MODELS Group Relative Policy Optimization (GRPO) (Guo et al., 2025a) is powerful policy gradient algorithm originally designed for autoregressive models. However, its direct application to discrete diffusion models is non-trivial. The core challenge lies in computing the importance sampling ratios and sequence-level likelihoods; these are straightforward in an autoregressive chain but ill-defined in non-autoregressive, parallel decoding process. Diffusion models lack sequential history for token-level probabilities, and their policy distributions are implicitly dependent on masking patterns, making direct likelihood estimation computationally prohibitive. To bridge this gap, we adopt the efficient random masking framework from MMaDA (Yang et al., 2025a) to adapt GRPO for our diffusion-based architecture. This strategy circumvents the need for direct likelihood computation by using the models predictions on randomly masked inputs as an unbiased estimate of the policy likelihoods. First, the advantage ˆAi for each response oi in generated group {oj}G j=1 is computed in the standard group-relative manner: ˆAi = ri mean({rj}G j=1) + ϵ std({rj}G j=1) , (10) where ri is the reward for response oi. The policy gradient is then calculated using an importance sampling ratio i,t(θ) defined over randomly masked version of each response, where unique 17 Preprint, Under Review. mask ratio pi [0, 1] is sampled for each response at each training step. This allows the standard clipped GRPO objective to be adapted for diffusion models as follows: JDiff-GRPO(θ) = EqD,{oi}πold, {pi}U [0,1] (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 Mi (cid:32) (cid:88) (cid:16) min i,t(θ) ˆAi, (cid:16) clip i,t(θ), 1 ε, 1 + ε tMi (cid:33) (cid:17) (cid:17) ˆAi βDKL(π θπ (cid:35) ref) (11) , where the expectation is also taken over the random mask ratios, the inner summation is only over the masked tokens Mi, and π denotes the policy likelihoods approximated via the masking scheme. This formulation enables stable and efficient policy optimization by effectively adapting the principles of GRPO to non-autoregressive setting."
        },
        {
            "title": "E SAMPLING DETAILS ON TEXT AND IMAGE",
            "content": "Parallel sampling and denoising strategy. Our model employs parallel sampling strategy, predicting logits for all text and image tokens simultaneously in single forward pass. The denoising process for both modalities is guided by confidence-based re-masking schedule, inspired by MaskGIT (Chang et al., 2022) and LLaDA (Nie et al., 2025). Crucially, while the logits are generated jointly, we apply distinct masking schedulers and confidence metrics to the text and image tokens to account for their different statistical properties and generation requirements. i}M i=1. For each masked position i, we sample candidate token Image token denoising. For image generation, we follow the iterative decoding process from MaskGIT. At each timestep t, given the current set of masked image tokens, the model predicts logits ℓt = {ℓt from the predicted probability distribution and compute its confidence score si. mask scheduling function γ(t/T ) determines the number of tokens = γ(t/T )M that should be kept (i.e., remain unmasked). We select the tokens with the highest confidence scores to keep for the next step + 1, and the remaining tokens are re-masked. The update rule for token at position is: (cid:26)u, i, if si < sortedj(sj)[m] otherwise u(t+1) (12) = , where represents the [MASK] token and sortedj(sj)[m] is the m-th value in the sorted list of confidence scores. This iterative refinement continues until all image tokens are finalized. In our implementation, we generate 512px image, which is encoded into 1024 discrete tokens and takes 30 steps to decode. Text token denoising. For text generation, we adopt the semi-autoregressive denoising strategy from LLaDA (Nie et al., 2025), where the output sequence is generated in blocks from left to right. Within each block, however, generation is non-autoregressive and iterative. The core of this process is reverse sampling step that transforms partially masked sequence xt at step into less masked sequence xs at an earlier step < t. This transition is formally characterized by the probability: = xi = [M] = [M] qst(xsxt) = qst(xi qst(xi sxi t) pθ(xi 1 (cid:89) sxi t) = and 1, = [M], xi xi xi = [M], xi = [M], xi 0xt), xi otherwise, 1 , 1αt αsαt 1αt 0, i=0 (13) where pθ(xi 0xt) is the models prediction of the original token for the masked position and αt = 1 t. In practice, this involves an iterative refinement loop. At each step, given the current sequence xt, we first sample candidate tokens for all masked positions. Then, following the deterministic lowconfidence re-masking strategy adopted by LLaDA, we identify the tokens with the lowest prediction confidence scores and re-mask them for the next refinement iteration. In our implementation, we generate the sequence with 256 sequence length, in blocks of 64 tokens and 128 steps. At each denoising step within block, we unmask the two tokens with the lowest confidence scores. This block-based, semi-autoregressive approach is essential for generating 18 Preprint, Under Review. coherent and naturally structured sentences, as it mitigates issues like the premature generation of end-of-sequence (EOS) tokens that can arise in fully non-autoregressive setting."
        },
        {
            "title": "F DETAILS OF TRAINING DATASET CURATION",
            "content": "Figure 12: Overview of our dataset for thinking-aware editing Figure 13: Overview of our dataset for thinking-aware editing Our training dataset is carefully curated collection of 150,000 high-quality samples designed for thinking-aware image synthesis. The primary challenge was that existing public datasets for image editing and generation typically provide input-output pairs without the intermediate reasoning traces required by our method. Therefore, our curation process involved three main stages: (1) aggregating data from state-of-the-art sources, (2) generating high-quality reasoning traces to augment this data, and (3) applying rigorous filtering and enhancement pipeline. The final dataset consists of 100,000 editing pairs and 50,000 generation pairs, achieving 2:1 ratio. An overview of the dataset is shown in Figure 12 and 13 Source datasets for editing data. We constructed the 100,000 thinking-aware editing pairs by sourcing from four diverse and challenging benchmarks: HQ-Edit (Hui et al., 2024): This dataset provides high-resolution images with wide variety of detailed editing instructions, serving as source of high-quality visual content for our training. Preprint, Under Review. UltraEdit (Zhao et al., 2024): We leverage UltraEdit for its collection of complex editing instructions that require strong reasoning and compositional abilities, pushing the model beyond simple object manipulation. AnyEdit (Yu et al., 2025): Given the vast size of AnyEdit, we selectively sampled from its more challenging categories. Specifically, we focused on the implicit editing subset, which contains instructions that do not explicitly mention the target object, requiring the model to infer the users intent. EditWorld (Yang et al., 2024): This dataset is crucial for its focus on edits that require world knowledge and complex reasoning, such as causal (e.g., what if storm occurs) and temporal (e.g., Whats this man like in twenty years?) edits. To further bolster our models capabilities in these areas, we performed data augmentation on this subset, using GPT-4o to generate three times the amount of similar, complex reasoning-based instructions and corresponding edits. Source dataset for generation Data. For the 50,000 thinking-aware generation pairs, we sourced data from ShareGPT4o (Chen et al., 2025a). This dataset contains rich collection of diverse, real-world prompts and corresponding high-quality image outputs, providing strong foundation for general-purpose, knowledge-intensive image synthesis. Reasoning trace generation. core step in our curation process was to augment the source data with reasoning traces. Since the original datasets only provide triplets of (input image, instruction, output image), we utilized the powerful multimodal model Qwen2.5-VL-7B (Bai et al., 2025) to generate plausible reasoning text for each sample. The model was prompted with the input/output image pair and the instruction to produce step-by-step rationale explaining the transformation. This transformed our dataset into quadruplets: (input image, instruction, reasoning trace, output image), which is the required format for our thinking-aware training. Data filtering and quality control. Finally, to ensure the highest quality, we applied multistage filtering pipeline to the entire 150,000-sample dataset. First, we removed near-duplicates to increase data diversity. Second, we used scoring mechanism based on Qwen-VL to identify and discard samples with low-quality or visually unappealing images. For cases where the instruction was valuable but the image quality was poor, we leveraged GPT-4o to regenerate higher-fidelity candidate images. This comprehensive curation process resulted in clean, diverse, and high-quality dataset optimized for our training objectives."
        },
        {
            "title": "G DETAILS OF PARABENCH",
            "content": "ParaBench is comprehensive benchmark designed to address the limitations of existing evaluation protocols for thinking-aware image synthesis. Unlike traditional benchmarks that focus solely on the final image, ParaBench is built to assess the entire generation process, including the quality of the intermediate reasoning trace and its synergy with the visual output. It comprises total of 300 challenging prompts, curated from various sources and divided into 200 for editing and 100 for generation. Composition of editing prompts. The 200 editing prompts are meticulously curated and synthesized from various existing benchmarks to test wide spectrum of complex reasoning abilities. To provide structured analysis, we group them into five distinct categories: Spatial Reasoning (40 prompts): These are tasks requiring deep understanding of object locations, orientations, and spatial relationships. Examples include instructions like place the book to the left of the lamp or make the person in the background larger. Temporal Reasoning (40 prompts): These prompts involve reasoning about time and require the model to infer past or future states. Examples include show what this street might look like 50 years from now or revert the shattered vase to its original state. Causal Reasoning (40 prompts): This category contains instructions that require the model to infer and depict cause-and-effect relationships. Examples include show the Preprint, Under Review. ground after heavy rain or make the plants look like they havent been watered for weeks. World Knowledge (40 prompts): These are edits that require external, real-world knowledge to execute correctly. Examples include instructions like turn this car into model from the 1980s or edit the painting to be in the style of Van Gogh. General Editing (40 prompts): This category includes broad set of common, foundational editing operations that do not fit into the specialized categories above. It primarily consists of instructions for adding, removing, or replacing objects and serves as baseline for fundamental editing capabilities. Composition of generation prompts. The 100 generation prompts are sourced from the ShareGPT4o (Chen et al., 2025a) dataset. They are designed to be open-ended and cover wide range of scenarios, including the generation of creative scenes, complex compositions with multiple interacting objects, and images that require interpreting long, descriptive narratives. Evaluation axes. All 300 prompts in ParaBench are evaluated using our LLM-as-a-judge framework across six fine-grained axes to provide holistic assessment of models performance. The evaluation criteria are as follows: Text Quality: Assesses the fluency, coherence, and grammatical correctness of the generated reasoning text. Text Alignment: Measures how well the reasoning text follows the users input instruction and accurately plans the edit/generation. Image Quality: Evaluates the photorealism, aesthetic quality, and absence of visual artifacts in the generated image. Image Alignment: Measures how faithfully the generated image adheres to the users instruction. Image Consistency (for editing tasks): Assesses how well the model preserves the unedited parts of the original image, maintaining background, style, and object identity. Output Alignment: Evaluates the cross-modal consistency between the generated reasoning text and the final generated image. We provide the prompts for thinking-aware image editing in Appendix K.The prompts for image generation follow the same format, with only minor modifications in the input and representation style."
        },
        {
            "title": "H MORE IMPLEMENTATION DETAILS",
            "content": "Training details. Our model is initialized from the weights of MMaDA-MixCoT (Yang et al., 2025a), which utilizes LLaDA-8B as its text backbone and MagVIT-v2 for image tokenization. The post-training process consists of two stages. In the first stage, we perform supervised finetuning (SFT) for 30,000 steps on our curated dataset of 150,000 thinking-aware samples. In the second stage, we conduct Parallel Reinforcement Learning (ParaRL) for 10,000 steps, using challenging subset of approximately 15,000 examples (10%) drawn from the SFT dataset. Both training stages were conducted on 32 NVIDIA A100 GPUs with global batch size of 768. We utilized the AdamW optimizer with learning rate of 2e-5 and cosine learning rate schedule with warm-up of 500 steps. We drop 10% of text input and 10% of image input to support classifier-free guidance sampling. In ParaRL, we randomly sample = 3 trajectory points. The steps of these certain points are identical in the same rollout and uniformly sampled in all rollouts. We set KL constraints β = 0.0001 to keep the same with MMaDAs baseline. Inference details. During inference, our model employs parallel sampling strategy, generating the logits for all text and image tokens simultaneously in single forward pass. The images are generated with classifier-free guidance scale of 3.5, and text with scale of 0. 21 Preprint, Under Review."
        },
        {
            "title": "I MORE ABLATION STUDIES",
            "content": "Table 9: Ablation on modality reweighting. Default uses wtext(t)=1/t, wimg(t)=1. Table 10: Ablation on decoding strategy. Fully parallel is our default. Setting Text Align. Image Align. Output Align. Strategy Text Align. Image Align. Output Align. Both 1/t Both 1 wtext=1/t, wimg=1 69.5 65.7 71 58.1 61.9 63.2 56.3 57.0 59. Sequential (text image) Semi-parallel (grouped) Fully parallel (ours) 64.2 68.3 71 56.5 60.7 63.2 54.1 57.5 59.8 We further analyze three key design choices of our framework: (1) modality-aware reweighting in the training objective, and (2) the decoding strategy (parallel vs semi-parallel vs sequential). Modality reweighting. Table 9 shows that using wtext(t) = 1/t and wimg(t) = 1 stabilizes image training and yields the best overall performance. Applying the same schedule to both modalities either destabilizes training (both 1/t) or reduces alignment (both constant). Decoding strategy. Table 10 contrasts fully parallel, semi-parallel, and fully sequential decoding. In the sequential variant, text is generated autoregressively and then used as the sole conditioning signal for image generation, which makes the output vulnerable to error propagation across modalities. In the semi-parallel variant, we first generate the reasoning text for the initial half of timesteps to provide partial textual prior, and then interleave image generation with the remaining text. This strategy mitigates some sequential errors and yields improvements over the fully sequential baseline. Finally, the fully parallel variant, i.e., MMaDA-Parallel, generates text and image jointly at every denoising step. We find that fully parallel decoding achieves strong results without requiring extensive textual priors, likely because the early image steps can already establish coarse scene layouts, and excessive initial text may even bias attention toward irrelevant details."
        },
        {
            "title": "J LIMITATIONS AND FUTURE WORK",
            "content": "Although our approach achieves notable improvements, several limitations remain. First, our base model MMaDA is trained on relatively limited data, which constrains its fundamental capabilities. As result, it is difficult to consistently surpass large-scale models such as Bagel that benefit from substantially larger training corpora. Second, our current sampling and training strategies are not yet fully unified across modalities, and exploring more integrated interaction paradigms may further enhance performance. For future work, we plan to extend our paradigm to broader scenarios, such as story generation and multimodal outputs that combine text and images, which we believe will further demonstrate the potential of parallel thinking-aware generation."
        },
        {
            "title": "K PROMPTS FOR EVALUATION",
            "content": "22 Preprint, Under Review. Figure 14: Output alignment evaluation prompt 23 Preprint, Under Review. Figure 15: Text quality evaluation prompt 24 Preprint, Under Review. Figure 16: Text alignment evaluation prompt 25 Preprint, Under Review. Figure 17: Image consistency evaluation prompt 26 Preprint, Under Review. Figure 18: Image quality evaluation prompt 27 Preprint, Under Review. Figure 19: Image alignment evaluation prompt 28 Preprint, Under Review."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution textto-image synthesis. arXiv preprint arXiv:2410.08261, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025a. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025b. Accessed: 2025-02-02. Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025b. URL https://arxiv.org/abs/2503.17352. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024b. Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. 29 Preprint, Under Review. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pp. 10696 10706, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025b. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, and Rui Yan. Reinforcing multimodal understanding and generation with dual self-rewards. arXiv preprint arXiv:2506.07963, 2025. Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-toimage generation. arXiv preprint arXiv:2509.06945, 2025a. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025b. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025a. Jingjing Jiang, Chongjie Si, Jun Luo, Hanwang Zhang, and Chao Ma. Co-reinforcement learning for unified multimodal understanding and generation. arXiv preprint arXiv:2505.17534, 2025b. Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. URL https://arxiv.org/abs/2506.15742. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025. Wendi Li and Yixuan Li. arXiv:2410.11287, 2024. Process reward model with q-value rankings. arXiv preprint Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Preprint, Under Review. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Longllada: Unlocking long context capabilities in diffusion llms. arXiv preprint arXiv:2506.14429, 2025c. Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. SegarXiv preprint zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv:2503.06520, 2025d. Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew Peters, and Arman Cohan. Tess: Text-to-text self-conditioned simplex diffusion. arXiv preprint arXiv:2305.08379, 2023. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. RevolutionizarXiv preprint ing reinforcement learning framework for diffusion large language models. arXiv:2509.06949, 2025. Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In ICLR, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, Ming-Hsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025c. 31 Preprint, Under Review. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025a. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025b. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025a. URL https://hkunlp.github.io/blog/2025/dream. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025b. Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Mingxuan Wang. Dinoiser: Diffused conditional sequence learning by manipulating noises. arXiv preprint arXiv:2302.10025, 2023. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2612526135, 2025. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        }
    ],
    "affiliations": [
        "ByteDance",
        "CASIA",
        "Peking University",
        "Princeton University",
        "The University of Chicago"
    ]
}