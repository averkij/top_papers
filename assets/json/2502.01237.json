{
    "paper_title": "The Differences Between Direct Alignment Algorithms are a Blur",
    "authors": [
        "Alexey Gorbatovski",
        "Boris Shaposhnikov",
        "Viacheslav Sinii",
        "Alexey Malakhov",
        "Daniil Gavrilov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the $\\beta$ parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +$3.46$ (ORPO) and +$8.27$ (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms."
        },
        {
            "title": "Start",
            "content": "Alexey Gorbatovski 1 Boris Shaposhnikov 1 Viacheslav Sinii 1 Alexey Malakhov 1 Daniil Gavrilov 1 5 2 0 2 3 ] . [ 1 7 3 2 1 0 . 2 0 5 2 : r Abstract Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the β parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +3.46 (ORPO) and +8.27 (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms. 1. Introduction Large Language Models (LLMs) demonstrate strong text generation capabilities, yet aligning them with human values remains challenging due to underspecified objectives, limited training signals, and the complexity of human intent (Ouyang et al., 2022; Stiennon et al., 2020). Traditional alignment pipelines typically involve Supervised FineTuning (SFT), reward modeling, and reinforcement learning to shape model outputs. Recently, Direct Alignment Algorithms (DAAs) have 1T-Tech. Correspondence to: Boris Shaposhnikov <b.shaposhnikov@tbank.ru>. 1 emerged as an alternative, integrating human preferences into policy optimization without explicit reward modeling or reinforcement learning (Rafailov et al., 2023; Hong et al., 2024; Azar et al., 2023; Meng et al., 2024; Chen et al., 2024; Xiao et al., 2024; DOosterlinck et al., 2024; Wang et al., 2024). These methods differ in theoretical design (pairwise vs. pointwise), implementation details (e.g., reference policy vs. odds ratio), and whether an SFT phase is required (one-stage vs. two-stage). This diversity raises key questions about their relationships, comparative advantages, and the role of SFT. In this paper, we show that one-stage methods (e.g., ORPO, ASFT) can incorporate an explicit SFT phase, improving performance. We introduce scaling parameter β that unifies their formulation with other DAAs, revealing shared optimization dynamics between methods using either an odds ratio or reference-based reward. Through theoretical and empirical analysis, we systematically compare DAAs, emphasizing pairwise vs. pointwise preference optimization. We also show that, while SFT is beneficial, using the full dataset is not always necessary, which reduces computational costs. To structure our analysis, we address the following research questions: RQ1: Does an explicit SFT stage improve the alignment quality of ORPO and ASFT? RQ2: Does the tempering factor enhance the alignment quality of ASFT and ORPO? RQ3: What factors of DAAs affect alignment quality? RQ4: How does the final alignment quality depend on the amount of data used in the SFT stage? By answering these questions, we clarify key trade-offs in alignment strategies and provide guidance for optimizing LLM training pipelines. 2. Preliminaries 2.1. Modeling Sequences Given sequence of length y, the log-probability can be written as log p(y) = (cid:80)y i=1 log p(yi y<i), which may also be conditioned on another sequence x. In practice, optimiz1 (cid:1) ing normalized log-probability 1 log p(y) = log(cid:0)p(y) The Differences Between Direct Alignment Algorithms are Blur often improves numerical stability and leads to better training. However, once normalized, the resulting quantity is no longer strict probability measure. Throughout this paper, whenever we write p(y), we refer to this normalized 1 . Whenever method does not apply this version p(y) normalization, we indicate it explicitly. Welleck et al. (2019) introduced log-unlikelihood term that reduces the probability of certain undesirable tokens: log(cid:0)1 p(c y<i)(cid:1) for C. It can be extended to an entire sequence as log(cid:0)1 p(y)(cid:1). 2.2. Reinforcement Learning from Human Feedback Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020) is prominent approach to aligning language models. It generally has three stages: Supervised Fine-Tuning (SFT). During the SFT stage, the model πθ is trained to follow instructions by maximizing the probability of correct output given input x. For single training pair (x, y), we define the per-sample SFT loss as LSFT(πθ, x, y) = log πθ(y x). During fine-tuning, we minimize the expectation of this per-sample loss over the training dataset D: (cid:105) (cid:104) E(x,y) . LSFT(πθ, x, y) Reward Modeling (RM). reward model rψ(x, y) produces satisfaction score. It is trained on preference pairs using the Bradley-Terry model (Bradley & Terry, (cid:2)log σ(cid:0)rψ(x, yw) 1952): LRM(rψ) = E(x,yw,yl)D rψ(x, yl)(cid:1)(cid:3), where yw is the preferred response and yl is the less preferred one. responses Reward Maximization. that maximize The objective is to generate the learned reward, with KL penalty to prevent reward (cid:2)rϕ(x, y)(cid:3) hacking: (cid:2)πθ(x, y) πref(x, y)(cid:3). Reinforcement β DKL learning (RL) algorithms are commonly used to optimize this objective (Schulman et al., 2017; Ouyang et al., 2022). ExD, yπθ(yx) maxπθ 2.3. Direct Alignment Algorithms Direct alignment algorithms replace the reward modeling and RL stages (but keep the SFT phase) with single alignment step. Various preference-optimization loss functions have been proposed, employing these core components: rref θ (y, x) = log(cid:0) πθ(yx) πref (yx) 2023), which acts as an implicit reward β rref normalization is used. (cid:1) from DPO (Rafailov et al., θ . No length rodds θ (y, x) = log(cid:0) πθ(yx) 1πθ(yx) (cid:1) proposed in ORPO (Hong et al., 2024), representing the odds of generating versus not generating it. Several Direct Alignment Algorithms use these notations. Information on sequence probability normalization for these methods is presented in Appendix A.1. Direct Preference Optimization (DPO) (Rafailov θ (yw, x) θ (yl, x)(cid:1).This method does not normalize probabiliet al., 2023): LDPO = log σ(cid:0)β rref β rref ties by length.2 Identity Preference Optimization (IPO) (Azar et al., θ (yw, x) rref 2023): LIPO = (cid:0)rref θ (yl, x) 1 2β (cid:1)2 . Simple Preference Optimization (SimPO) (Meng et al., 2024): LSimPO = log σ(cid:0)β log πθ(yw, x) β log πθ(yl, x) γ(cid:1). Noise Contrastive Alignment et al., 2024): LNCA = log σ(cid:0)β rref 0.5 log σ(cid:0)β rref θ (yw, x)(cid:1) 0.5 log σ(cid:0)β rref (NCA) (Chen θ (yw, x)(cid:1) θ (yl, x)(cid:1). Direct Calibrated (Cal-DPO) log σ(cid:0)rref + (cid:0)rref Preference Optimization (Xiao et al., 2024): LCalDPO = θ (yl, x)(cid:1) + (cid:0)rref θ (yw, x) rref θ (yw, x) (cid:1)2 θ (yl, x) + 1 . 2β 1 2β (cid:1)2 Anchored Preference Optimization Zero (APOZero) (DOosterlinck et al., 2024): LAPOZero = σ(cid:0)β rref θ (yw, x)(cid:1) + σ(cid:0)β rref θ (yl, x)(cid:1). 2.4. Single-Stage Alignment Methods Single-stage alignment (as subset of DAA methods) merges SFT and direct alignment in one step by adding their (cid:2)LSFT(πθ, x, yw) + losses: LSingle(πθ) = E(x,yw,yl)D λ LAlign(πθ, x, yw, yl)(cid:3), where λ is hyperparameter, and no reference policy πref is required. In this paper, we focus on: Odds Ratio Preference Optimization (ORPO) (Hong et al., 2024): LORPO = log πθ(ywx) λ log σ(cid:0)rodds (yw, x) rodds . θ θ (yl, x)(cid:1) (cid:125) (cid:124) (cid:123)(cid:122) LORPOAlign (cid:16) et al., 2024): log σ(cid:0)rodds λ (cid:124) θ Aligned Supervised Fine-Tuning (ASFT) (Wang LASFT = log πθ(ywx) (yw, x)(cid:1) log σ(cid:0)rodds θ (cid:17) . (yl, x)(cid:1) (cid:125) (cid:123)(cid:122) LASFTAlign 2Unless otherwise noted, the expectation over (x, yw, yl) is taken. 2 The Differences Between Direct Alignment Algorithms are Blur 3. Method Many DAAs have been proposed, raising questions about their differences and significance. They can be categorized in various ways. For example, one classification separates single-stage methods, which perform alignment directly after obtaining base model (ASFT and ORPO), from twostage methods (which perform SFT before alignment), as in DPO, IPO, SimPO, etc. Under this scheme, ASFT and ORPO are single-stage methods. Another classification considers whether rref or rodds is used as an implicit reward. ASFT and ORPO also differ from other losses by using an odds ratio, whereas other methods in Section 2 use normalized policy probabilities.1 DAAs can also be distinguished by whether their loss function is optimized for pairwise or pointwise preferences. DPO, for instance, increases the policys probability of choosing preferred sequences relative to rejected ones. In contrast, ASFT simply increases or decreases probabilities for chosen or rejected sequences without comparing them directly. 3.1. Generalizing ASFT and ORPO Despite these classifications, it can still be difficult to pinpoint the essential differences among DAAs, especially when design choices limit generalization. ASFT and ORPO, for example, lack parameter β, probably because they were conceived as single-stage methods, making the distance from reference policy unnecessary. It might seem odd to introduce such parameter in single-stage methods, but we will show that for both ASFT and ORPO, the singlestage design and the absence of β are not strictly required. 3.1.1. ORPO AND ASFT CAN OPERATE WITHOUT THE SFT LOSS TERM AND AS TWO-STAGE METHODS. We begin by inspecting the ASFT objective and demonstrate that it combines both likelihood and unlikelihood terms: Theorem 3.1. LASFT is equivalent to the Binary CrossEntropy (BCE) loss, encapsulating both likelihood and unlikelihood components: LASFT = (1 + λ) log πθ(ywx) λ log (cid:0)1 πθ(ylx)(cid:1). The proof of Theorem 3.1 is provided in Appendix B. Consequently, LASFTAlign = (cid:16) log πθ(ywx) + log (cid:0)1 πθ(ylx)(cid:1)(cid:17) . Next, we derive direct relationship between LORPO and 1SimPO does not explicitly use reference policy, but can be treated similarly if uniform reference policy is assumed. LASFT, showing that the latter provides an upper bound on the former: Theorem 3.2. LORPO can be expressed as: LORPO = LASFT + λ log (cid:0)πθ(ywx)(1 πθ(ylx)) + πθ(ylx)(1 πθ(ywx))(cid:1), where the additional term is symmetric in yw and yl. The proof of Theorem 3.2 is provided in Appendix C. As for LASFTAlign, the alignment term is then LORPOAlign = log πθ(ywx) log(1 πθ(ylx)) + log (cid:0)πθ(ywx)(1 πθ(ylx)) + πθ(ylx)(1 πθ(ywx))(cid:1). Corollary 3.3. LORPO LASFT and LORPOAlign LASFTAlign. This follows from the fact that the additional term in LORPO is non-positive when πθ(ywx) and πθ(ylx) lie in [0, 1], and πθ(ywx) + πθ(ylx) 1. These findings yield two main observations: LASFT provides an upper bound on LORPO. Minimizing the former also minimizes the latter. LASFT can be viewed as minimal form of DAA loss, reflecting the structure of BCE. An essential insight from these formulations is that the SFT term in the ASFT and ORPO losses is already included in the full loss. We hypothesize that this feature may allow us to omit the SFT term in the complete loss, first performing an SFT phase and then using only the alignment terms for model alignment. From this perspective, one can experiment with these methods in both single-stage and two-stage configurations to see which approach is more effective. 3.1.2. TEMPERING ASFT AND ORPO We now consider the original single-stage methods from Section 2.4 and examine how the alignment terms LORPOAlign and LASFTAlign compare. These terms optimize preferences and, depending on the coefficient λ, can dominate or have smaller impact on the final loss. LASFTAlign and LORPOAlign strongly resemble the DAA losses discussed in Section 2.3. The single-stage analogue of rref . Inspired by this analogy, we introduce θ coefficient β to scale rodds is rodds θ : θ Lβ ASFTAlign = log σ(βrodds Lβ ORPOAlign = log σ(βrodds θ θ (yw, x)) log σ(βrodds θ (yl, x)), (yw, x) βrodds θ (yl, x)). The Differences Between Direct Alignment Algorithms are Blur ASFT and Lβ Both Lβ ORPO generalize their vanilla counterparts (recovering them when β = 1). As in DPO, β can be viewed as temperature or scaling parameter that regulates the intensity of the preference for good odds. This becomes clearer when looking at the gradients: Finally: Theorem 3.6. For (cid:8)IPO, SimPO, NCA, Cal-DPO, APO-Zero(cid:9), as β 0, the gradient of LX is collinear with the gradient of LDPO. Formally, each method (yl, x) lim β0 θ LX θ LX = θ LDPO θ LDPO . = β (cid:104) σ(βrodds θ (yl, x))θrodds θ (cid:105) , (yw, x) (yw, x))(cid:1)θrodds (cid:104)(cid:0)θrodds θ θ = β (yw, x) θrodds θ (yl, x)(cid:1) θLβ ASFTAlign + (cid:0)1 σ(βrodds θ ORPOAlign θLβ (cid:16) 1 σ(βrodds θ (yw, x) βrodds θ (yl, x)) (cid:17)(cid:105) , 1πθ(yx) (y, x) = θ log πθ(yx) where θrodds . When β 0, θ σ(β ) 1 2 , both methods aggressively improve the odds ratio (increasing for yw and decreasing for yl). As β increases, the updates become bounded by the factor σ(β ) (similar to reward threshold in DPO). Hence, once the model improves, further updates are limited, either individually for Lβ . or by pairwise ranking in Lβ ORPOAlign ASFTAlign This alignment with other DAAs allows for direct comparison of all methods in different setups, clarifying which aspects are most critical for successful performance. 3.2. On the Difference Between Direct Alignment Algorithms Different methods can be grouped by the type of reward function used in their loss. In general terms, Lβ ASFTAlign and Lβ employ an odds ratio, while DPO, IPO, SimPO, NCA, Cal-DPO, and APO-Zero use ratio between the probability of the policy and that of reference policy. ORPOAlign The following theorems make this classification clearer: Theorem 3.4. The gradient of Lβ becomes collinear with the gradient of LORPOAlign as β 0. Formally, ASFTAlign θ Lβ θ Lβ lim β0 ASFTAlign ASFTAlign θ LORPOAlign θ LORPOAlign , = indicating that both gradients point in the same direction. The proof of Theorem 3.4 is provided in Appendix D.1. related property applies to Lβ Theorem 3.5. The gradient of Lβ the gradient of LORPOAlign for any β > 0. Formally, ORPOAlign ORPOAlign : is collinear with θ Lβ θ Lβ ORPOAlign ORPOAlign θ LORPOAlign θ LORPOAlign , = β > 0. The proof of Theorem 3.6 is provided in Appendix F.1. θ These theorems suggest that for sufficiently small β, these loss functions are split into two categories with indistinguishable gradient directions. Although the magnitudes may differ and they may not be collinear for β 0, one could infer that their performance should be similar when β is small. From this perspective, two main distinctions arise among these methods: the use of an odds ratio (rodds ) and the use of the ratio to reference policy (rref θ ). Both choices might influence the final performance of these methods. Furthermore, it remains an open question whether odds-ratio-based approaches outperform reference-policy-based ones (e.g., DPO), and how these distinctions compare to the contrast between pointwise and pairwise preference formulations. From traditional learning-to-rank (Liu et al., 2009) research, pairwise methods often produce more direct and less noisy ranking signals than pointwise techniques, which could lead to superior performance in practice (Burges et al., 2005; Li, 2011; Melnikov et al., 2016). In the following sections, we present experimental results that provide further insight into which aspects most strongly influence DAA training. 4. Experimental Setup We systematically compare and evaluate DAA methods using standard training and instruction-following evaluation framework (Tunstall et al., 2023; Meng et al., 2024; Gorbatovski et al., 2024). Our main experiments use the Llama 3.1 8B model (AI@Meta, 2024), trained on the UltraChat (Ding et al., 2023) and UltraFeedback (UF) (Cui et al., 2023) datasets, and evaluated on the AlpacaEval 2 (Dubois et al., 2024; Li et al., 2023) and ArenaHard (Li et al., 2024) benchmarks. For the Reddit TL;DR (Stiennon et al., 2020) task, we employ the Llama 3.2 3B model, comparing it side by side with the golden validation split (Rafailov et al., 2023; 2024) using the prompt in Appendix I. 4.1. Base vs SFT-Initialized Models. To investigate the impact of SFT and the applicability of one-stage loss LAlign component, we use the UF dataset for SFT (avoiding additional knowledge from UltraChat), and for pairwise preference optimization. We carefully tuned the hyperparameters to optimize each methods performance. The proof of Theorem 3.5 is provided in Appendix E.1. For the Base-initialized setup, we perform grid search over 4 The Differences Between Direct Alignment Algorithms are Blur Figure 1. Impact of the β Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning β (Section 3.1.2) affects both ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2 LC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each methods best configuration at β = 1 as the baseline. See Section 5.2 for more details. learning rates {6 106, 8 106, 1 105}, inspired by values suggested in ORPO and ASFT, and explore λ {0.1, 0.2, 0.5, 1.0} for 1 and 2 training epochs keeping similar budget to compare with the SFT-initialized setup. In the SFT-initialized setup, we experiment with both LORPOAlign and LASFTAlign alone, as well as in combination with LSFT, following the original methods. We tune the learning rates {5 107, 7 107, 1 106} for one epoch, starting from an SFT model trained for 1 epoch at 6 106. 4.2. β Sensitivity. Building on the theoretical insights from Section 3.2, where DAA losses share indistinguishable gradient directions as β 0, we evaluate each method across various β values to examine quality-KL trade-offs. In classical DPO, β regulates the KL penalty from the reference policy, but setting β too small can induce training instability. Therefore, we conduct thorough sweep of at least six β values per DAA, exploring the performance limit of each method. To broaden our analysis, we consider three scenarios: 1. Llama 3.2 3B TL;DR. relatively simpler Reddit TL;DR summarization task, evaluated via GPT sideby-side comparison on 500 samples from the golden validation split (Rafailov et al., 2023; 2024). 2. Llama 3.2 3B UF. The UltraChat and UF datasets serve as more challenging alignment settings due to their coverage of diverse and complex tasks, including common sense reasoning, mathematical problemsolving, code generation, logical reasoning, creative writing, and general knowledge. 3. Llama 3.1 8B UF. larger, more capable model on the same UltraChat and UF datasets, allowing us to assess how increased model capacity influences β-sensitivity in these diverse tasks. 5 For the UF-based experiments, we measure model quality primarily using the AlpacaEval 2 Length-Controlled (LC) Win-Rate and ArenaHard (AH) WR, and then track KL divergence from reference model to construct Pareto fronts. For the TL;DR scenario, we rely on GPT-based preference judgments using gpt-4o-2024-08-06 model. Concretely, in each scenario we train models for different values β, combining them with four possible learning rates {1 106, 7 107, 5 107, 3 107}. Further implementation details, including training procedures and generation hyperparameters, are provided in Appendix A. 4.3. SFT Quality. Although in principle single-stage methods do not require separate SFT phase, in practice an SFT-trained reference model often improves the final performance of two-stage pipelines (see Section 5.1). Prior work, such as (Zhou et al., 2024), has shown that small but high-quality dataset can be sufficient for instruction tuning. However, beyond response quality, it remains unclear how the amount of SFT data influences alignment effectiveness. This raises fundamental question: how much supervised data is actually needed to produce reference model that yields high-quality results after the subsequent alignment step? To investigate this, we prepared seven SFT checkpoints by training Llama 3.1 8B Base on 1%, 3%, 5%, 10%, 25%, 50%, and 100% of the UltraChat dataset (2,079, 6,236, 10,393, 20,786, 51,966, 103,932, and 207,865 records, respectively) using our SFT-initialized procedure. We then applied each alignment method using optimal hyperparameters from our β-sensitivity experiments (Appendix Table 7) to these seven SFT checkpoints and the original base model. Finally, we evaluated all resulting aligned models on AlpacaEval 2 LC, analyzing their performance relative to the fraction of SFT data used. The Differences Between Direct Alignment Algorithms are Blur Win / Tie / Lose Rate % 35.6 / 4.8 / 59.6 91.2 / 1.0 / 7.8 91.4 / 0.4 / 8.2 91.6 / 0.2 / 8.2 90.2 / 0.6 / 9.2 92.6 / 0.6 / 6.8 91.8 / 1.0 / 7.2 91.4 / 0.4 / 8.2 87.2 / 1.0 / 11.8 Figure 2. GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best hyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT achieves 87.2%, maintaining robust summarization performance. See Section 5.2 for more details. 5. Results top of an SFT-trained model. 5.1. RQ1: Does an explicit SFT stage improve the alignment quality of ORPO and ASFT? As shown in Table 1, the performance of ORPO and ASFT methods improves significantly when the alignment loss LAlign is applied after preceding SFT stage. In particular, ORPO achieves results comparable to classical DPO in both LC Win Rate and AH WR metrics. In contrast, ASFT shows notable gains in AH WR after the SFT stage, although it still underperforms compared to ORPO or DPO. Init Method LC% (std) WR% (std) AH% (CI) Base SFT SFT Base Base SFT SFT SFT SFT ORPO ASFT ORPO ASFT ORPO ASFT DPO 6.7 (0.43) 24.1 (0.84) 16.4 (0.72) 14.8 (0.71) 14.5 (0.73) 13.4 (0.69) 11.4 (0.63) 23.4 (0.85) 4.5 (0.63) 17.8 (1.17) 11.9 (0.99) 10.3 (0.95) 10.2 (0.94) 9.3 (0.91) 7.5 (0.83) 20.0 (1.18) 3.5 (-0.7, 0.8) 15.3 (-1.6, 1.8) 10.6 (-1.2, 1.3) 8.4 (-1.3, 1.3) 7.5 (-1.1, 1.2) 7.7 (-0.9, 1.1) 7.5 (-1.1, 1.1) 17.5 (-1.8, 1.8) Table 1. Base and SFT-initialized alignment methods on the Llama 3.1 8B model with the UF dataset. SFT-initialized methods demonstrate better performance compared to their traditional formulations without LSFT. Results marked with correspond to training with LSFT, using the best hyperparameters: lr = 1106 for ORPO and lr = 7 107 for ASFT. For other setups, the lr = 5 107 for standard SFT best hyperparameters are: ORPO/ASFT, and lr = 1 105/6 106 for Base ORPO/ASFT. For single-stage methods, the use of λ = 1 provides the best results within the explored grid of λ {0.1, 0.2, 0.5, 1.0}, especially after two epochs of training. However, combining LSFT and LAlign in single-stage setup leads to suboptimal results compared to explicitly separating these phases, even when starting from an SFT-trained model. Incorporating an explicit SFT stage improves overall performance for ORPO and ASFT methods. Therefore, all further experiments focus on applying the LAlign components of ORPO and ASFT on 5.2. RQ2: Does the tempering factor enhance the alignment quality of ASFT and ORPO? Figure 1 illustrates that introducing the β parameter (as described in Section 3.1.2) improves the performance of both ASFT and ORPO LAlign in our tested scenarios. For fair comparison, we used the best-performing learning rate for each baseline LASFTAlign and LORPOAlign while fixing β = 1. In the Llama 3.2 3B TL;DR experiment, these adjustments led to an improvement of +7.0 for ORPO and +43.4 for ASFT in GPT-4 WR. In the Llama 3.1 8B UF setup, tuning β provided additional gains of +3.46 for ORPO and +8.27 for ASFT on the AlpacaEval 2 LC WR. 5.3. RQ3: What factors of DAAs affect alignment quality? ORPOAlign Based on Section 3, we perform comprehensive evaluation of alignment losses, including DPO, IPO, SimPO, NCA, Cal-DPO, and APO-Zero, as well as enhanced Lβ ASFTAlign and Lβ with the introduced parameter β. Unlike classical methods where β typically regulates KL divergence against reference policy πref, β in Lβ and Lβ directly modulates the strength of preference optimization. To explore the upper limits of each methods performance, we performed an extensive hyperparameter search, analyzing both alignment quality and KL divergence. Full implementation details, including training setups and evaluation criteria, are provided in Appendix A. ORPOAlign ASFTAlign Llama 3.2 3B TL;DR: Figure 2 presents comparison of all methods on the Reddit TL;DR validation subset, using their best hyperparameters. Most methods achieve GPT-4 Win Rate exceeding 90%, indicating robust summarization performance on this relatively straightforward task. ASFT is slightly lower at 87.2% Win Rate, but still demonstrates strong overall results. Llama 3.2 3B UF and Llama 3.1 8B UF: Table 2 summa6 The Differences Between Direct Alignment Algorithms are Blur Llama 3.2 3B UF Llama 3.1 8B UF Method AlpacaEval 2 ArenaHard AlpacaEval 2 ArenaHard LC% (std) WR% (std) WR% (CI) LC% (std) WR% (std) WR% (CI) SFT 5.02 (0.34) 3.21 (0.55) 1.4 (-0.4, 0.4) 10.27 (0.54) 5.44 (0.70) 2.6 (-0.5, 0.6) DPO IPO SimPO ORPO APO Zero NCA Cal-DPO ASFT 11.43 (0.58) 11.24 (0.60) 10.56 (0.44) 10.67 (0.50) 10.36 (0.53) 10.33 (0.53) 10.62 (0.57) 10.63 (0.55) 11.79 (0.99) 11.67 (1.01) 11.94 (0.95) 12.23 (0.97) 11.22 (0.98) 11.02 (0.97) 10.15 (0.94) 9.21 (0.88) 6.8 (-1.0, 0.9) 6.8 (-1.0, 1.1) 6.4 (-1.0, 1.1) 6.6 (-1.0, 1.1) 6.0 (-1.0, 0.9) 5.1 (-0.7, 0.8) 4.8 (-0.9, 0.9) 5.1 (-0.9, 0.9) 26.82 (0.77) 28.18 (0.83) 27.65 (0.77) 28.25 (0.71) 23.15 (0.76) 23.21 (0.80) 23.19 (0.82) 20.82 (0.79) 23.69 (1.25) 24.43 (1.26) 25.62 (1.29) 28.59 (1.33) 19.03 (1.18) 18.67 (1.17) 18.85 (1.18) 16.34 (1.13) 19.0 (-1.9, 1.8) 19.1 (-1.6, 1.5) 21.5 (-1.9, 1.9) 20.9 (-2.0, 2.0) 17.3 (-1.8, 1.8) 15.1 (-1.5, 1.6) 15.2 (-1.5, 1.6) 13.5 (-1.6, 1.5) Table 2. AlpacaEval 2 and ArenaHard Results for Llama 3.2 3B and Llama 3.1 8B UF. The SFT model was trained on the UltraChat dataset. The best hyperparameters for each method were selected according to Section 4.2. Bold values indicate the best performance for each benchmark, while underlined values represent the second-best performance. See Section 5.3 for more details. Figure 4. Pairwise vs. Pointwise Ranking Methods on Toy Example. Model capacity impacts ranking accuracy, with pairwise methods outperforming pointwise ones as capacity increases. This behavior is consistent with results observed in Llama experiments on the UF dataset. See Section 5.3 for more details. their ability to more effectively balance alignment quality and divergence. Pareto fronts for the remaining setups are included in Appendix for completeness. These observations suggest that model capacity plays significant role in amplifying the advantages of pairwise ranking, where LLMs act as rankers (similar to Liu et al. (2024)). For smaller models, such as the 3B setup, limited capacity may hinder the ability to fully exploit pairwise gradient signals. This hypothesis is supported by additional evidence from the toy example experiment (Figure 4), where pairwise methods demonstrated performance similar to pointwise methods with weaker MLPs but achieved better ranking accuracy as the model capacity increased. Full details of the toy example setup are provided in Appendix H. 5.4. RQ4: How does the final alignment quality depend on the amount of data used in the SFT stage? In Section 5.1, we show that DAAs designed to bypass the SFT phase still underperform compared to models that undergo SFT and are then aligned using similar Figure 3. Pareto front for alignment quality and KL divergence. Results for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are grouped into pairwise and pointwise categories, with pairwise achieving higher LC values while remaining within overlapping confidence intervals. See Section 5.3 for more details. rizes the results for both Llama 3.2 3B UF and Llama 3.1 8B UF setups. For the smaller 3B model, the methods perform similarly on LC WR, with slight differences emerging on AH. Although these differences align with the pairwise vs. pointwise distinction (e.g., DPO, IPO, ORPO, SimPO vs. APO-Zero, NCA, Cal-DPO, ASFT), no single approach consistently dominates across metrics. The overlap in confidence intervals further indicates that the results for these methods are statistically similar in this setup, with no clear separation. In contrast, the 8B model reveals clearer performance differentiation. Pairwise methods consistently outperformed pointwise ones on AlpacaEval 2 and ArenaHard metrics, with ORPO achieving the highest overall alignment quality. As illustrated in Figure 3, pairwise approaches dominated the KL Pareto front for the larger model, demonstrating 7 The Differences Between Direct Alignment Algorithms are Blur (a) Pairwise (b) Pointwise Figure 5. Impact of SFT Dataset Size on Alignment Quality. Performance of the pairwise (a) and pointwise (b) alignment methods on AlpacaEval 2 (LC WR metric) when the SFT policy is trained on different fractions of the UltraChat dataset. Even small fraction of SFT data (e.g., 510%) yields substantial gains over starting from the raw base model. See Section 5.4 for more details. preference-optimization loss function without the SFT term. As discussed in Section 4.3, this raises the question of how much supervised data is needed to compensate for the additional computation and achieve comparable alignment performance. To investigate this, we trained seven SFT models on progressively larger UltraChat subsets (1% to 100%) and applied each alignment algorithm to these models and the non-finetuned base model, yielding eight initializations per method. Figures 5(a) and 5(b) summarize the results for pairwise and pointwise alignment methods, respectively. As the plots show, no method starting from the raw base model can match the final quality of method trained with the entire SFT dataset. However, even modest size expansion of the SFT dataset yields substantial improvements in alignment quality: for example, moving from 3% to 5% of the data more than doubles the AlpacaEval 2 LC score for the final model. Crucially, using only 10% of UltraChat for SFT yields nearly the same quality as using the entire dataset. Adding an SFT phase requires more overall training, but it pays off significantly in the final result. Moreover, one does not need the entire supervised corpus to realize most of these gains; even 510% of the data is often enough for DAAs to reach most of their potential. 6. Conclusion This paper presents comprehensive theoretical and empirical analysis of DAAs. Theoretically, we demonstrated that within each category - odds-based (rodds) and referencepolicy-based (rref ) gradient directions of popular methods align as β 0, revealing shared optimization dynamics within these groups. We also showed that single-stage losses (e.g., ASFT, ORPO) can be extended to two-stage pipelines with an explicit SFT step and optional β-scaling, enabling greater flexibility. Experimentally, we addressed four core research questions (RQ14), exploring singlevs. two-stage training, implicit rewards, objective types, and the impact of the SFT phase. Our key findings are: Include an SFT phase. An SFT stage consistently improves alignment performance (RQ1), with ORPO achieving +9.3 LC / +6.9 AH and ASFT +1.9 LC / +3.1 AH in the setup from Section 4.1. Even 510% of the supervised dataset often suffices to achieve near-optimal results (RQ4). Pairwise methods outperform pointwise objectives. Alignment quality depends more on the choice between pairwise and pointwise objectives than on the formulation of implicit reward (e.g., rodds or rref ). Pairwise methods generally perform better (e.g., ORPO outperforming ASFT by +7.43 LC / +7.4 AH in the Llama 3.1 8B UF setup), particularly in larger models (RQ3). Among these, ORPO and SimPO also stand out as practical options for memory-constrained scenarios, as they do not rely on reference policy. Choose hyperparameters carefully. Alignment performance is highly sensitive to learning rates and the coefficient β. We provide optimal configurations for different methods based on comprehensive grid searches in our experimental setups, highlighting the added gains from tuning β in odds-based methods, where it controls the strength of preference optimization (RQ2). Limitations and Future Work. Although our study systematically compares DAAs, it has several limitations. We tested limited set of datasets (UltraChat, UltraFeedback, Reddit TL;DR) and benchmarks (AlpacaEval 2, ArenaHard), which may affect generalizability to other domains. The reliance on GPT-based evaluators can introduce biases. Moreover, we evaluated on 3B8B models, so the observed advantages of pairwise over pointwise objectives could shift at larger scales. 8 The Differences Between Direct Alignment Algorithms are Blur"
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R. general theoretical paradigm to understand learning from human preferences, 2023. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T. J., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022. URL https://api.semanticscholar. org/CorpusID:248118878. Bradley, R. A. and Terry, M. E. Rank Analysis of Inclomplete Block Design: The Method of Paired Comparisons. Biometrika, 39(3-4):324345, 12 1952. ISSN 00063444. doi: 10.1093/biomet/39.3-4.324. URL https: //doi.org/10.1093/biomet/39.3-4.324. DOosterlinck, K., Xu, W., Develder, C., Demeester, T., Singh, A., Potts, C., Kiela, D., and Mehri, S. Anchored preference optimization and contrastive revisions: Addressing underspecification in alignment, 2024. URL https://arxiv.org/abs/2408.06266. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Gorbatovski, A., Shaposhnikov, B., Malakhov, A., Surnachev, N., Aksenov, Y., Maksimov, I., Balagansky, N., and Gavrilov, D. Learn your reference model for real good alignment. arXiv preprint arXiv:2404.09656, 2024. Hong, J., Lee, N., and Thorne, J. Orpo: Monolithic preference optimization without reference model, 2024. URL https://arxiv.org/abs/2403.07691. Kingma, D. P. and Ba, Adam: method J. CoRR, abs/1412.6980, for stochastic optimization. 2014. URL https://api.semanticscholar. org/CorpusID:6628106. Li, H. short introduction to learning to rank. IEICE TRANSACTIONS on Information and Systems, 94(10): 18541862, 2011. Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and Hullender, G. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pp. 8996, 2005. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, 2024. Chen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J. Noise contrastive alignment of language models with explicit rewards, 2024. URL https://arxiv.org/ abs/2402.05369. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Ding, N., Chen, Y., Xu, B., Qin, Y., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 30293051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL https:// aclanthology.org/2023.emnlp-main.183. Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P., and Hashimoto, T. B. Alpacaeval: An automatic evaluator of instruction-following https://github.com/tatsu-lab/ models. alpaca_eval, 5 2023. Liu, T., Qin, Z., Wu, J., Shen, J., Khalman, M., Joshi, R., Zhao, Y., Saleh, M., Baumgartner, S., Liu, J., et al. Lipo: Listwise preference optimization through learningto-rank. arXiv preprint arXiv:2402.01878, 2024. Liu, T.-Y. et al. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3): 225331, 2009. Melnikov, V., Hullermeier, E., Kaimann, D., Frick, B., and Gupta, P. Pairwise versus pointwise ranking: case study. Schedae Informaticae, pp. 7383, 2016. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. 9 The Differences Between Direct Alignment Algorithms are Blur Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., 2022. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. URL https://arxiv.org/abs/ 2305.18290. Rafailov, R., Chittepu, Y., Park, R., Sikchi, H., Hejna, J., Knox, B., Finn, C., and Niekum, S. Scaling laws for reward model overoptimization in direct alignment algorithms. arXiv preprint arXiv:2406.02900, 2024. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Proximal policy optimization aland Klimov, O. URL gorithms. http://dblp.uni-trier.de/db/journals/ corr/corr1707.html#SchulmanWDRK17. CoRR, abs/1707.06347, 2017. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. Learning to summarize from human feedback. In NeurIPS, 2020. Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. Wang, R., Sun, J., Hua, S., and Fang, Q. Asft: Aligned supervised fine-tuning through absolute likelihood, 2024. URL https://arxiv.org/abs/2409.10571. Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019. Xiao, T., Yuan, Y., Zhu, H., Li, M., and Honavar, V. G. Caldpo: Calibrated direct preference optimization for language model alignment, 2024. URL https://arxiv. org/abs/2412.14516. The Differences Between Direct Alignment Algorithms are Blur A. Implementation Details A.1. Probability Normalization As discussed in Section 2.1, not all DDAs incorporate length-based probability normalization by default. In this paper, however, we consistently apply such normalization wherever probabilities are involved. This choice avoids introducing extra notation and reduces the cognitive load on the reader. Table 3 summarizes the methods that originally include length-based normalization. Method DPO (Rafailov et al., 2023) IPO (Azar et al., 2023) SimPO (Meng et al., 2024) NCA (Chen et al., 2024) Cal-DPO (Xiao et al., 2024) APO-Zero (DOosterlinck et al., 2024) ORPO (Hong et al., 2024) ASFT (Wang et al., 2024) Use normalization Table 3. Methods that include () or omit () length-based probability normalization in their original formulation. A.2. Training Details Our experiments were conducted using the Llama 3.2 3B and Llama 3.1 8B Base models (AI@Meta, 2024). The training setup, datasets, and hyperparameters were designed to ensure reproducibility and consistency. Unless otherwise noted, the hyperparameters in Table 4 were used across all experiments. Hyperparameter Value Max Tokens Length Epochs Learning Rate (SFT) Learning Rate (Base Init.) Learning Rate (Alignment) Optimizer Adam β1 Adam β2 Batch Size Learning Schedule Warm-up Ratio Max Gradient Norm Memory Optimization Attention Mechanism 1024 (TL;DR setup), 4096 (UF setup) 1 (or 2 when specified) 6.0 106 {6.0 106, 8.0 106, 1.0 105} {3.0 107, 5.0 107, 7.0 107, 1.0 106} Adam (Kingma & Ba, 2014) 0.9 0.95 128 Linear Decay 0.03 2 DeepSpeed (Rasley et al., 2020) Flash Attention 2 (Dao, 2023) Table 4. Representative training hyperparameters for Llama 3.2 3B and Llama 3.1 8B models. Training was performed on 8 NVIDIA A100 GPUs with 80GB memory each. Depending on the number of epochs, training for each configuration took between 3 to 6 hours. A.2.1. DATASETS. We used two primary datasets: Reddit TL;DR (Bai et al., 2022): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B model. 11 The Differences Between Direct Alignment Algorithms are Blur UltraChat (Ding et al., 2023): used to train the initial SFT model in β-sensitivity experiments with Llama 3.2 3B and Llama 3.1 8B models. UltraFeedback (Cui et al., 2023): used for both SFT (in the Base vs. SFT-initialized comparison, where we selected chosen subset from preference pairs) and for pairwise preference optimization in all DAA methods. The dataset sizes are summarized in Table 5. For Base vs. SFT-initialized setups, only UltraFeedback was used. For β-sensitivity experiments, the models were first trained on UltraChat for SFT and subsequently fine-tuned on UltraFeedback. The Reddit TL;DR dataset was processed to remove duplicates, retaining only uniquely preferred summaries for SFT. Dataset Training Examples Validation Examples UltraChat UltraFeedback Reddit TL;DR (SFT) Reddit TL;DR (Preference) 207,865 61,135 41,947 73,396 23,110 2,000 11,941 21,198 Table 5. Summary of dataset sizes used for training and validation. A.2.2. β-SENSITIVITY EXPERIMENTS. We conducted comprehensive analysis to evaluate the sensitivity of DAA methods to β, examining its impact on the trade-off between model quality and KL divergence. Each method was trained using six or more distinct β values to identify configuration that achieves stable and effective performance. The specific β values tested for each method are as follows: Method β Values Tested DPO IPO SimPO ORPO ASFT APO-Zero Cal-DPO NCA {0.001, 0.003, 0.005, 0.01, 0.05, 0.1} {0.0007, 0.001, 0.005, 0.01, 0.05, 0.1} {0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0} {0.05, 0.1, 0.2, 0.5, 1.0, 2.0} {0.05, 0.1, 0.2, 0.5, 1.0, 2.0} {0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2} {0.00005, 0.0001, 0.0003, 0.0005, 0.001, 0.003} {0.0001, 0.0003, 0.0005, 0.001, 0.005, 0.007, 0.01, 0.03, 0.05} Table 6. Range of β values tested for each DAA method on all scenarios. For each β, we tested four learning rates (3.0 107, 5.0 107, 7.0 107, 1.0 106), training on the UltraFeedback dataset. All runs began from an SFT-initialized model trained on UltraChat (lr = 6.0 106, 1 epoch). The best-performing learning rate for each β was selected to construct Pareto fronts, balancing quality (measured via AlpacaEval 2 LC Win-Rate) and KL divergence. For SimPO in the Llama 3.1 8B UF setup, the ratio γ β = 0.5 was kept fixed as recommended by Meng et al. (2024). Additionally, single learning rate (lr = 6.0 107) was tested across all β values for this method, as the same datasets and model scale were used. For Llama 3.2 TL;DR and UF setups, we tested four learning rates similar to other DAAs. Beyond the standard β values described in Table 6, additional values were explored for specific configurations to reach the extreme points of the Pareto front. For example: - {0.00001, 0.00003} for Cal-DPO in Llama 3.2 3B TL;DR and UF setups, - {0.00001, 0.00003, 0.00005} for NCA in Llama 3.2 3B TL;DR, - {0.0003, 0.0005} for APO-Zero in Llama 3.2 3B TL;DR, - {0.0003, 0.0005, 0.001, 0.003, 0.005} for ASFT in Llama 3.2 3B TL;DR. The hyperparameters resulting in the best performance are presented in Table 7. A.3. Generation Details We evaluated model performance on AlpacaEval 2 and ArenaHard for UltraFeedback setups, while for the Reddit TL;DR setup, we used side-by-side comparisons with GPT-4o on curated golden validation subset of 500 samples. Additionally, 12 The Differences Between Direct Alignment Algorithms are Blur Method DPO IPO SimPO ORPO ASFT APO Zero NCA Cal-DPO Llama 3.2 3B TL;DR Llama 3.2 3B UF Llama 3.1 8B UF Learning Rate 7.0 107 1.0 10 3.0 107 3.0 107 3.0 107 3.0 107 3.0 107 3.0 10 β 0.05 0.005 0.5 0.5 0. 0.001 0.0001 0.00003 Learning Rate 1.0 106 7.0 107 7.0 10 5.0 107 1.0 106 3.0 107 3.0 107 5.0 107 β 0.01 0.001 1.0 0.2 0.2 0. 0.0005 0.0003 Learning Rate 1.0 106 1.0 106 6.0 107 5.0 10 7.0 107 3.0 107 3.0 107 3.0 107 β 0. 0.001 1.0 0.5 0.1 0.003 0. 0.0003 Table 7. Best hyperparameters for each DAA method across setups. KL divergence was measured on the validation subset for all setups using the generation hyperparameters listed in Table 8. For ArenaHard, the temperature was set to 0 to adhere to the original benchmark configuration. Hyperparameter Value Temperature Top-k Top-p Max New Tokens 0.9 40 1.0 256 (TL;DR setup), 4096 (UF setup) Table 8. Generation hyperparameters for Llama 3.1 8B and Llama 3.2 3B models. B. Equivalence of ASFT Loss and Binary Cross-Entropy Loss Lemma B.1. Proof. log σ(rodds θ (y, x)) = log πθ(yx) log σ(rodds θ (y, x)) = log σ(log πθ(yx) 1 πθ(yx) ) = log 1 1 + elog(1πθ(yx))log(πθ(yx)) = log 1 1 + 1πθ(yx) πθ(yx) = log (cid:16) 1 + 1 πθ(yx) πθ(yx) (cid:17) = log πθ(yx) + 1 πθ(yx) πθ(yx) = log πθ(yx). Lemma B.2. Proof. log σ(rodds θ (y, x)) = log (cid:0)1 πθ(yx)(cid:1) log σ(rodds θ (y, x)) = log σ( log πθ(yx) 1 πθ(yx) ) = log 1 1 + elog(πθ(yx))log(1πθ(yx)) = log 1 1 + πθ(yx) 1πθ(yx) = log (cid:16) 1 + πθ(yx) 1 πθ(yx) (cid:17) = log 1 πθ(yx) + πθ(yx) 1 πθ(yx) = log(1 πθ(yx)). 13 The Differences Between Direct Alignment Algorithms are Blur Theorem B.3. LASFT is equivalent to the binary cross-entropy loss, encompassing both likelihood and unlikelihood components: LASFT = (1 + λ) log πθ(ywx) λ log (cid:0)1 πθ(ylx)(cid:1). Proof. To show that LASFT is equivalent to the BCE loss, we start with the definition: LASFT = log πθ(ywx) λ log σ(rodds θ (yw, x)) λ log σ(rodds θ (yl, x)), where rodds θ (y, x) = πθ(yx) 1πθ(y,x) . Applying Lemma B.1 and Lemma B.2 to the expression, we obtain: LASFT = log πθ(ywx) λ log πθ(ywx) λ log (cid:0)1 πθ(ylx)(cid:1) = (1 + λ) log πθ(ywx) λ log(1 πθ(ylx)). C. Relationship Between ORPO and ASFT Loss Functions Theorem C.1. LORPO can be expressed as: LORPO = LASFT + λ log (cid:0)πθ(ywx)(1 πθ(ylx)) + πθ(ylx)(1 πθ(ywx))(cid:1). Proof. We start by defining the ORPO loss: LORPO = log πθ(ywx) λ log σ log (cid:18) π(ywx) 1 π(ywx) log π(ylx) 1 π(ylx) (cid:19) . Expanding the second term using the identity log σ(x) = log(ex + 1), we get: (cid:18) log σ log πθ(ywx) 1 πθ(ywx) log (cid:19) πθ(ylx) 1 πθ(ylx) = log = log 1 πθ(ywx) πθ(ywx) 1 πθ(ywx) πθ(ywx) + log + log πθ(ylx) 1 πθ(ylx) πθ(ylx) 1 πθ(ylx) + log + log (cid:18) πθ(ywx)(1 πθ(ylx)) πθ(ylx)(1 πθ(ywx)) (cid:19) + 1 (cid:18) πθ(ywx) 2πθ(ywx)πθ(ylx) + πθ(ylx) πθ(ylx)(1 πθ(ywx)) (cid:19) = log πθ(ywx) log(1 πθ(ylx)) + log (cid:0)πθ(ywx) 2πθ(ywx)πθ(ylx) + πθ(ylx)(cid:1) (cid:123)(cid:122) (cid:125) ORPOAlign (cid:124) . Combining all terms, we obtain: LORPO = (1 + λ) log πθ(ywx) λ log(1 πθ(ylx)) + λ log (cid:0)πθ(ywx)(1 πθ(ylx)) + πθ(ylx)(1 πθ(ywx))(cid:1) = LASFT + λ log (cid:0)πθ(ywx)(1 πθ(ylx)) + πθ(ylx)(1 πθ(ywx))(cid:1) D. Proof of Theorem 3.4 Theorem D.1 (Collinearity of β-ASFT and ORPO Gradients). Let Lβ ASFTAlign = log σ(cid:0)β rodds θ (yw, x)(cid:1) log σ(cid:0)β rodds θ (yl, x)(cid:1), where rodds θ (y, x) = log (cid:16) πθ(yx) 1πθ(yx) (cid:17) . The Differences Between Direct Alignment Algorithms are Blur"
        },
        {
            "title": "Define the ORPO alignment loss as",
            "content": "LORPOAlign = log σ(cid:0)rodds θ (yw, x) rodds θ (yl, x)(cid:1). Then, lim β0 θ Lβ (cid:13) (cid:13)θ Lβ ASFTAlign ASFTAlign θ LORPOAlign (cid:13) (cid:13)θ LORPOAlign , (cid:13) (cid:13) = (cid:13) (cid:13) i.e., their gradients become collinear in the same direction as β 0. Proof. Step 1. Gradient of β-ASFT. Denote pw = πθ(yw x), pl = πθ(yl x). Then rodds θ (yw, x) = log (cid:16) pw 1pw (cid:17) , rodds θ (yl, x) = log (cid:16) pl 1pl (cid:17) . By definition, Lβ ASFTAlign = log σ(cid:0)β rodds θ (yw, x)(cid:1) log σ(cid:0)β rodds θ (yl, x)(cid:1). For small β, first-order Taylor expansion of σ(β z) around 0 yields σ(β z) = 1 1 2 and σ( β rodds β θ[rodds (yw, x)) 2 . Taking gradients and applying the chain rule gives each term approximately proportional to (yl, x)) 1 θ ()]. Concretely, 4 + O(β2). Thus, σ(β rodds 2 + β θ θ Hence, summing up, θ (cid:2) log σ(β rodds θ (yw, x))(cid:3) β 2 θ (cid:2)rodds θ (yw, x)(cid:3), θ (cid:2) log σ( β rodds θ (yl, x))(cid:3) + β 2 θ (cid:2)rodds θ (yl, x)(cid:3). θ Lβ ASFTAlign (cid:104) β 2 θrodds θ (yl, x) θrodds θ (cid:105) (yw, x) . Observe that β > 0 implies the overall scalar factor β 2 is strictly positive in front of the difference of gradients. Step 2. Gradient of ORPO alignment loss. Define rodds (yw, x) rodds (x) = rodds θ θ θ (yl, x). Then LORPOAlign = log σ(cid:0)rodds θ (x)(cid:1). Its gradient (using the chain rule) is proportional to θ LORPOAlign θ (cid:2)rodds θ (yw, x) rodds θ (yl, x)(cid:3) = θrodds θ (yl, x) θrodds θ (yw, x). Up to strictly positive logistic factor (since σ() (0, 1)), the coefficient in front of θ[rodds track the absolute scalar to see it is positive. Indeed, one can write θ ()] remains negative, but we θ (cid:0)rodds θ (x)(cid:1) = κORPO θrodds θ (yl, x) κORPO θrodds θ (yw, x), κORPO > 0. Step 3. Conclusion (positive collinearity). Comparing the two gradients: θ Lβ ASFTAlign β 2 (cid:2)θrodds θ (yl, x) θrodds θ (yw, x)(cid:3), θ LORPOAlign (cid:2)θrodds θ (yl, x) θrodds θ (yw, x)(cid:3). The ratio is thus strictly positive for small β. Consequently, θ Lβ θ Lβ lim β0 ASFTAlign ASFTAlign establishing collinearity in the same direction. = θ LORPOAlign θ LORPOAlign , 15 The Differences Between Direct Alignment Algorithms are Blur E. Proof of Theorem 3.5 Theorem E.1 (Collinearity of β-ORPO and ORPO Gradients). Let and consider rodds θ (x) = rodds θ (yw, x) rodds θ (yl, x), Lβ ORPOAlign = log σ(cid:0)β rodds θ (x)(cid:1)."
        },
        {
            "title": "Its gradient is collinear with the gradient of the standard ORPO alignment loss",
            "content": "LORPOAlign = log σ(cid:0)rodds θ (x)(cid:1) for any fixed β > 0. Formally, θ Lβ (cid:13) (cid:13)θ Lβ ORPOAlign ORPOAlign θ LORPOAlign (cid:13) (cid:13)θ LORPOAlign . (cid:13) (cid:13) = (cid:13) (cid:13) Proof. Step 1. Gradient of β-ORPO. (yw, x) rodds Let rodds (x) = rodds θ θ θ (yl, x). Then Lβ ORPOAlign = log σ(cid:0)β rodds θ (x)(cid:1). By the chain rule, θ Lβ ORPOAlign = 1 σ(β rodds θ (x)) σ(cid:0)β rodds θ (x)(cid:1) β θ (cid:2)rodds θ (x)(cid:3). Since σ(z) = σ(z) [1 σ(z)], we have Thus, 1 σ(β rodds θ (x)) σ(cid:0)β rodds θ (x)(cid:1) = β(cid:2) 1 σ(cid:0)β rodds θ (x)(cid:1)(cid:3). θ Lβ ORPOAlign (cid:104) = β 1 σ(cid:0)β rodds θ (x)(cid:1)(cid:105) θ (cid:2)rodds θ (x)(cid:3). Since β > 0 and 1 σ() > 0, the factor multiplying θ[rodds θ (x)] is strictly negative. Step 2. Gradient of standard ORPO (i.e. β = 1). For LORPOAlign = log σ(cid:0)rodds θ (x)(cid:1), the gradient is θ LORPOAlign = (cid:2) 1 σ(rodds θ (x))(cid:3) θ (cid:2)rodds θ (x)(cid:3). This also has strictly negative scalar in front of θ (cid:2)rodds θ (x)(cid:3). Step 3. Conclusion (exact positive ratio). Since θ Lβ two gradients coincide up to strictly positive factor: and θ LORPOAlign both differ from θ ORPOAlign (cid:2)rodds θ (x)(cid:3) by negative coefficient, it follows that these θ Lβ ORPOAlign = κ(β) θ LORPOAlign, κ(β) > 0. Hence θ Lβ θ Lβ ORPOAlign ORPOAlign θ LORPOAlign θ LORPOAlign , = proving the claimed collinearity (in the same direction) for every fixed β > 0. 16 The Differences Between Direct Alignment Algorithms are Blur F. Proof of Theorem 3.6 Theorem F.1 (Unified Collinearity of DPO with IPO, SimPO, NCA, Cal-DPO, and APO-Zero). Let and define the DPO loss rref θ (x) = rref θ (cid:0)yw, x(cid:1) rref θ (cid:0)yl, x(cid:1), (cid:16) LDPO = log σ(cid:0)β rref θ (x)(cid:1)(cid:17) , β > 0. For each method (cid:8)IPO, SimPO, NCA, Cal-DPO, APO-Zero(cid:9), as β 0, the gradient of LX is asymptotically collinear (i.e., it differs by positive factor) with the gradient of LDPO. Formally, lim β θ LX θ LX = θ LDPO θ LDPO . Proof of Theorem 3.6. Step 1: DPO as the baseline (tracking its sign). By definition, LDPO = log σ(cid:0)β rref θ (x)(cid:1). Since σ(u) = 1/(1 + eu), for β > 0, one computes θ LDPO = β (cid:104) 1 σ(cid:0)β rref θ (x)(cid:1)(cid:105) θ rref θ (x). Observe that β > 0 and σ() (0, 1) imply Hence the factor multiplying θ rref 1 σ(cid:0)β rref θ (x)(cid:1) > 0. θ (x) is negative. To unify directions by positive multiple, note θ (x)(cid:1)(cid:105) 1 σ(cid:0)β rref θ rref θ LDPO = β θ (x), (cid:104) which has strictly positive scalar in front. Thus, θ LDPO is collinear with θ rref positive multiple of θ rref θ . θ , and in particular its negative is Step 2: IPO. The IPO loss is Its gradient is (cid:16) LIPO = rref θ (x) 1 2β (cid:17)2 . θ LIPO = 2 (cid:16) rref θ (x) 1 2β (cid:17) θ rref θ (x). As β 0, the term 1 2β dominates rref θ (x). Hence, rref θ (x) 1 2β 1 2β , so We compare this with θ LIPO 1 β θ rref θ (x). θ LDPO = β (cid:104) 1 σ(cid:0)β rref θ (x)(cid:1)(cid:105) θ rref θ (x). Both gradients are negative multiples of θ rref θ (x). Therefore, θ LIPO = κIPO(β) θ LDPO, with κIPO(β) > 0 as β 0. Hence they are collinear in the same direction asymptotically. 17 The Differences Between Direct Alignment Algorithms are Blur Step 3: SimPO. The SimPO loss is LSimPO = log σ(cid:0)β sθ γ(cid:1), where sθ = log πθ(yw x) log πθ(yl x). Its gradient takes the form θ LSimPO = β (cid:2) 1 σ(β sθ γ)(cid:3) σ(β sθ γ) θ sθ. Again, β > 0 and 1 σ() > 0. Also, σ(β sθ γ) (0, 1). Thus the prefactor β (cid:2) 1 σ(β sθ γ)(cid:3) σ(β sθ γ) is strictly negative for each β > 0. Therefore, just like DPO, θ LSimPO is in the negative direction of θ sθ. But θ sθ is proportionally the same as θ rref for small-β expansions (both are differences of log-likelihood or reward-like terms). θ So θ LSimPO = κSimPO(β) θ LDPO, κSimPO(β) > 0 for small β. Hence they are collinear with positive factor in the low-β limit. Step 4: NCA. Define Then NCA is For small β, expand = rref rref θ (cid:0)yw, x(cid:1), = rref rref θ (cid:0)yl, x(cid:1). LNCA = log σ(cid:0)β rref (cid:1) 1 2 log σ(cid:0) β rref (cid:1) 1 2 log σ(cid:0) β rref (cid:1). σ(β z) = 1 2 + β + O(β2), so log σ(β z) = log 1 θ rref 2 + O(β2) . Collecting terms shows that, as β 0, 1 + β (cid:16) 2 + log (cid:17) . Each gradient term then yields linear-in-β combination of θ rref and θ LNCA β θ (cid:0)rref rref (cid:1) = β θ rref θ (x). Comparing this with θ LDPO = β(cid:2) 1 σ(. . . )(cid:3)θ rref form, θ (x) reveals another negative factor on the DPO side. In ratio θ LNCA = κNCA(β) θ LDPO with κNCA(β) > 0 for small β. Hence collinearity follows. Step 5: Cal-DPO. The Cal-DPO loss is LCal-DPO = log σ(cid:0)rref θ (x)(cid:1) + (cid:0)rref 1 2β (cid:1)2 + (cid:0)rref + 1 2β (cid:1)2 . For β near 0, the large constants 1 2β dominate. The gradient w.r.t. θ in these squared terms is effectively β θ rref + 1 β θ rref = 1 β θ (cid:0)rref rref (cid:1) = 1 β θ rref θ (x). Since θ LDPO has the same negative sign structure in front of θ rref θ , their ratio is again positive. Thus θ LCal-DPO = κCal-DPO(β) θ LDPO with κCal-DPO(β) > 0 as β 0. Step 6: APO-Zero. APO-Zero is given by LAPO-Zero = σ(cid:0)β rref (cid:1) + σ(cid:0)β rref (cid:1). 18 The Differences Between Direct Alignment Algorithms are Blur Its gradient involves terms θ σ(β rref yields ) and θ σ(β rref θ (x). Since θ LDPO also has negative constant factor, their ratio has positive limit. Therefore, θ LAPO-Zero β θ (cid:0)rref rref (cid:1) = β θ rref ), each proportional to β θ rref and β θ rref . Subtracting these θ LAPO-Zero = κAPO-Zero(β) θ LDPO, κAPO-Zero(β) > 0 for small β. Conclusion. In each method X, one sees that θ LX has the same negative-sign structure around θ rref ensuring positive ratio in the limit. Formally, θ (x) as does θ LDPO, Thus, θ LX = κX (β) θ LDPO, κX (β) > 0, as β 0. lim β0 θ LX θ LX = θ LDPO θ LDPO , which completes the proof of their alignment in the same direction. G. Pareto fronts for Llama 3.2 setups The results presented in this section correspond to the best hyperparameter configurations identified during the hyperparameter search described in Section 4.2, including the optimal learning rate for each method. This ensures that the Pareto fronts reflect the upper performance limits for alignment quality. (a) Llama 3.2 3B TL;DR (b) Llama 3.2 3B UF Figure 6. Pareto front for alignment quality and KL divergence. Results for Llama 3.2 3B TL;DR and UF setups on GPT-4 Win Rate vs. golden validation subset and AlpacaEval 2 LC respectively with different β values. Methods are grouped into pairwise and pointwise categories. For the summarization task (Llama 3.2 3B TL;DR), both pointwise and pairwise methods achieve strong overall results. For the UF setup, methods also perform similarly within overlapping confidence intervals, indicating no clear separation. H. Toy Example Details To analyze the differences between pairwise and pointwise ranking methods, especially with respect to the ranking nature of alignment losses in LLMs, simplified toy experiment was conducted under controlled setup. dataset of 2000 triplets (x, yw, yl) was generated, where x, yw, and yl are real-valued scalars satisfying yw > yl. The data was split into 80% for training and 20% for testing. When the model processes scalar input together with candidate y, these two numbers form vector in R2, which serves as the input of the Multi-Layer Perceptron (MLP) to predict the reward r. 19 The Differences Between Direct Alignment Algorithms are Blur single-hidden-layer MLP with ReLU activation was used in two capacity settings: lower (hidden size = 1) and higher (hidden size = 3). The model takes and candidate as input, producing reward analogous to training reward model for RLHF (Stiennon et al., 2020). Two losses were evaluated: the pairwise Bradley-Terry loss (Bradley & Terry, 1952), LPairwise = log(cid:0)σ(β(rw rl))(cid:1), and the pointwise loss, LPointwise = (cid:2)log(cid:0)σ(βrw)(cid:1) + log(cid:0)σ(βrl)(cid:1)(cid:3). Each configuration was trained over 100 runs, tuning the learning rate from {0.5, 0.3, 0.1, 0.01, 0.03, 0.05} and β from {5.0, 2.0, 1.0, 0.2, 0.1, 0.05, 0.01}. Alignment accuracy was defined as the proportion of cases with rw > rl. The results show that both methods yield comparable performance in the low-capacity regime, while pairwise ranking achieves higher accuracy as model capacity increases, mirroring the effects observed in larger-scale experiments from the Section 5.3. I. GPT-4 Side-By-Side Evaluation Prompt For our Side-By-Side evaluations with GPT-4o, we designed prompt tailored to the Reddit TL;DR dataset to assess accuracy, completeness, relevance, and conciseness. The full prompt used in our experiments is detailed below. Act as an impartial judge and evaluate the quality of the summaries provided by two AI assistants for the text displayed below. Your evaluation should consider accuracy, completeness, relevance, and conciseness. You will be given text, Assistant As summary, and Assistant Bs summary. Your job is to evaluate which assistants summary is better based on the text provided. Begin your evaluation by comparing both assistants summaries with the original text. Identify and correct any inaccuracies. Ensure the summaries are complete, capturing all essential information from the text without introducing fabricated details. Assess the relevance of the information each assistant chose to include in their summary, ensuring it reflects the core message of the text. Evaluate the conciseness of the summaries, favoring those that efficiently convey the necessary information without unnecessary verbosity. Avoid any position biases and ensure the order in which the summaries were presented does not influence your decision. Do not allow the length of the summaries to influence your evaluation, except in the context of conciseness and efficiency. Do not favor certain names of the assistants. Be as objective as possible. You should only evaluate the summaries provided by both assistants and NOT the original text itself. If both summaries are irrelevant, contain hallucinations, or are inconsistent with the original text, mark the comparison as inconclusive and choose option \"C\". After providing your explanation, output your final verdict by strictly following this format: \"\"\" The Differences Between Direct Alignment Algorithms are Blur Comparison: <One-sentence comparison> Winner: <A if assistant is better, if assistant is better, and for tie.> \"\"\""
        }
    ],
    "affiliations": [
        "T-Tech"
    ]
}