{
    "paper_title": "GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning",
    "authors": [
        "GigaBrain Team",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Hao Li",
        "Jie Li",
        "Jindi Lv",
        "Jingyu Liu",
        "Lv Feng",
        "Mingming Yu",
        "Peng Li",
        "Qiuping Deng",
        "Tianze Liu",
        "Xinyu Zhou",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yang Wang",
        "Yifan Li",
        "Yifei Nie",
        "Yilong Li",
        "Yukun Zhou",
        "Yun Ye",
        "Zhichao Liu",
        "Zheng Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}."
        },
        {
            "title": "Start",
            "content": "2026-2-13 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning GigaAI Project Page: https://gigabrain05m.github.io GigaBrain Team (alphabetical order): Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Hao Li, Jie Li, Jindi Lv, Jingyu Liu, Lv Feng, Mingming Yu, Peng Li, Qiuping Deng, Tianze Liu, Xinyu Zhou, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yifei Nie, Yilong Li, Yukun Zhou, Yun Ye, Zhichao Liu, Zheng Zhu 6 2 0 F 2 1 ] . [ 1 9 9 0 2 1 . 2 0 6 2 : r Figure 1: GigaBrain-0.5M* is world model-conditioned VLA trained via world model-based reinforcement learning. Pretrained on multimodal, robot manipulation, and web video data, it enables self-improvement through human-in-the-loop (HIL) rollout that generates diverse training data for continual training. Abstract Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them natural foundation for enhancing VLA learning. Therefore, we propose GigaBrain-0.5M*, VLA model trained via world model-based reinforcement learning. Built upon GigaBrain-0.5, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. GigaBrain-0.5M* further integrates world model-based reinforcement learning via RAMP (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that RAMP achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30% on challenging tasks including Laundry Folding, Box Packing, and Espresso Preparation. Critically, GigaBrain-0.5M* exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our project page. 1. Introduction Recent advances in vision-language-action (VLA) models (Bjorck et al., 2025; Black et al., 2024; Bu et al., 2025; Cheang et al., 2025; Intelligence et al., 2025; Jiang et al., 2025; Team et al., 2025; Zhai et al., 2025) have demonstrated compelling results in understanding instructions, perceiving environments, and executing complex manipulation. Nevertheless, fundamental limitation persists in mainstream VLA architectures: their 2026 GigaAI. All rights reserved. GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning reliance on myopic observations for long-horizon action planning, this shortcoming stems from an architectural bias toward reactive control rather than prospective planning. Conversely, foundation world models trained on massive-scale video corpora have demonstrated remarkable proficiency in forecasting plausible future states, such predictive priors offer pathway to endow VLAs with foresight. Therefore, we introduce GigaBrain-0.5M*, VLA model trained via world model-based reinforcement learning. Specifically, GigaBrain-0.5M* extends GigaBrain-0.5M (our latest VLA pre-trained on over 10K hours of realworld robotic interaction data) with RAMP (Reinforcement leArning via world Model-conditioned Policy). The RAMP pipeline operates in four iterative stages: (1) The world model is pretrained with large-scale robot manipulation data to predict value and future states. (2) The policy is fine-tuned by conditioning actions on the world models predicted value and future state. (3) Deploy the policy in real environments, producing robot rollout data with human-in-the-loop intervention. (4) Continual training world model and policy with rollout data. This iterative training paradigm enables self-improvement. Therefore, we introduce GigaBrain-0.5M*, VLA model trained via world model-based reinforcement learning. Specifically, GigaBrain-0.5M* extends GigaBrain-0.5M (our latest VLA pretrained on over 10K hours of realworld robotic interaction data) by integrating RAMP (Reinforcement leArning via world Model-conditioned Policy). The RAMP framework follows an iterative four-stage training paradigm. First, the world model is pretrained on large-scale robot manipulation data to forecast future states and associated value. Second, the policy undergoes fine-tuning by conditioning its action selection on the world models predicted futures and value estimates. Third, the conditioned policy is deployed in physical environments to collect rollout trajectories under human-in-the-loop intervention. Fourth, both the world model and policy are jointly refined using the curated rollout dataset. This iterative training paradigm enables continual learning and self-improvement. The proposed RAMP is inspired by RECAP in ùúã* (Intelligence et al., 2025), as both approaches utilize additional 0.6 information as conditions for the VLA model. However, RECAP only uses sparse advantages (0 or 1) as input, providing limited information gain. In contrast, our proposed RAMP leverages future states predicted by well-pretrained world model, yielding substantial information gain. Furthermore, we theoretically verify that RECAP is special case of RAMP. In our experiments, we first conduct comprehensive internal evaluations to assess the performance of GigaBrain0.5 against strong baselines, including ùúã0.5 (Intelligence et al., 2025) and GigaBrain-0 (Team et al., 2025). Our method achieves state-of-the-art success rates across diverse suite of manipulation tasks, with particularly pronounced advantages on challenging deformable object manipulation and long-horizon procedural tasks. Furthermore, an intermediate version of GigaBrain-0.5 secured the top position on the public RoboChallenge benchmark leaderboard (RoboChallenge Team). We additionally perform extensive ablation studies to analyze the impact of different reinforcement learning algorithms on real-robot performance. Results demonstrate that our proposed RAMP significantly outperforms alternative approaches such as AWR (Peng et al., 2019) and RECAP (Intelligence et al., 2025), yielding superior multi-task generalization and markedly improved sample efficiency during policy learning. Notably, GigaBrain-0.5M* exhibits robust long-horizon reasoning capabilities, seamlessly executing complex sequential tasks, including laundry folding, box packing and espresso preparation, without interruption over extended interaction horizons. 2. Related Works 2.1. Vision-Language-Action Models Recent progress in foundation language models has catalyzed the development of VLA models (Bjorck et al., 2025; Black et al., 2024; Cheang et al., 2025; Doshi et al., 2024; Intelligence et al., 2025; Kim et al., 2024; Li et al., 2024; Liu et al., 2024; Ni et al., 2025; ONeill et al., 2024; Pertsch et al., 2025; Qu et al., 2025; Team et al., 2025, 2024; Wang et al., 2024), which pursue enhanced cross-task and cross-embodiment generalization by jointly scaling model parameters and training corpora. Such systems commonly leverage frozen or fine-tuned 2 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning vision-language backbones (Alayrac et al., 2022; Bai et al., 2025; Beyer et al., 2024; Liu et al., 2023; Marafioti et al., 2025; Peng et al., 2023; Steiner et al., 2024) to process heterogeneous sensory inputs and produce executable motor commands, employing either autoregressive tokenization strategies or continuous action spaces formulated through flow-based generative paradigms (Lipman et al., 2022; Liu, 2022). Although contemporary VLAs incorporate extensive cross-embodiment datasets (Dasari et al., 2019; Ebert et al., 2021; Khazatsky et al., 2024; ONeill et al., 2024; Walke et al., 2023) alongside large-scale proprietary archives to bolster generalization capabilities, fundamental limitation persists in their capacity for temporally extended reasoning. Specifically, these models exhibit tendency to condition action generation predominantly on immediate observation inputs when addressing long-horizon manipulation tasks. 2.2. World Models for Policy Models Recent breakthroughs in world modeling (Agarwal et al., 2025; Alhaija et al., 2025; Assran et al., 2025; Jang et al., 2025; Jiang et al., 2025; Kong et al., 2024; Liao et al., 2025; Wang et al., 2025) have accelerated the adoption of generated data to bridge the simulation-to-reality gap in embodied AI systems (Zhu et al., 2024). In autonomous driving, world models are leveraged to generate corner cases data (Gao et al., 2023, 2024; Hu et al., 2023; Ren et al., 2025; Russell et al., 2025; Wang et al., 2024; Zhao et al., 2025) and construct traffic situations (Ni et al., 2024, 2025; Zhao et al., 2024, 2025). In embodied robotics contexts, techniques such as (Team et al., 2025) harness world-model-generated samples, spanning texture-varied scenes (Dong et al., 2025; Liu et al., 2025; Yuan et al., 2025), multi-viewpoint renderings (Xu et al., 2025), and ego-centric translations (Li et al., 2025), to enrich the training data of VLA models. distinct paradigm involves forecasting future visual trajectories via world models (e.g., DreamGen (Jang et al., 2025) and ViDAR Feng et al. (2025)), subsequently inferring executable motor commands through Inverse Dynamics Models (IDMs). The efficacy of such pipelines critically hinges on the visual fidelity and physical plausibility of generated sequences. Beyond data generation, emerging approaches investigate tighter integration between world models and policy learning. Methods like (Bi et al., 2025; Cen et al., 2025; Li et al., 2026, 2025; Pai et al., 2025; Wang et al., 2024) fuse latent representations from predictive world models with policy networks to improve sample efficiency and generalization. More ambitiously, frameworks such as (Kim et al., 2026) bypass explicit policy networks altogether, directly mapping world model predictions to action sequences. 2.3. Reinforcement Learning for Vision-Language-Action Models Imitation learning policies suffer from compounding errors due to distribution shift Ross et al. (2011), inherently limiting their performance to the quality of the demonstration data. While DAgger and its variants Jang et al. (2022); Kelly et al. (2019) mitigate this issue through online expert interventions, they still rely on continuous human supervision and lack mechanisms for autonomous policy improvement. To transcend the limitations of imitation learning, reinforcement learning has been widely adopted for robotic policy optimization. Traditional approaches employ on-policy algorithms Schulman et al. (2017) or off-policy methods Kalashnikov et al. (2018) to refine policies through environment interaction. Recent works extend these paradigms to VLA models via direct policy gradient optimization Lu et al. (2025); Tan et al. (2025) or residual policy learning on frozen backbones Guo et al. (2025). However, scaling policy gradient methods to large-scale VLAs remains challenging due to training instability and sample inefficiency. An emerging direction circumvents explicit policy gradient computation by conditioning action generation on value signals, encompassing reward-conditioned policies Kumar et al. (2019), and advantage-conditioned formulations Kuba et al. (2023); Wu et al. (2023). Recently, ùúã* Intelligence et al. (2025) introduced the RECAP framework, demonstrating that advantage0.6 conditioned reinforcement learning enables VLAs to attain high performance on downstream tasks through on-robot data collection. This motivates us to explore world model-based reinforcement learning, where world model jointly predicts value and future states to serve as rich policy conditions. 3 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning 3. GigaBrain-0.5M* Building upon our foundation VLA model GigaBrain-0.5, we introduce GigaBrain-0.5M*, an enhanced policy model that integrates world model-based RL: RAMP (Reinforcement leArning via world Model-conditioned Policy). This section first details the architecture and pre-training data composition of GigaBrain-0.5. We then present RAMP, training methodology that harnesses world model predictions to iteratively refine policy behavior through experience and corrective feedback signals. 3.1. GigaBrain-0.5 GigaBrain-0.5 inherits the end-to-end VLA architecture of GigaBrain-0 (Team et al., 2025), designed to map visual observations and language instructions to action sequences for bi-manual robots. It employs mixtureof-transformers (Liang et al., 2024) backbone, utilizing pre-trained PaliGemma-2 (Steiner et al., 2024) visionlanguage model (VLM) to encode multimodal inputs and an action Diffusion Transformer (DiT) (Peebles and Xie, 2023) with flow matching (Lipman et al., 2022) to predict action chunks. To enhance reasoning capabilities, GigaBrain-0.5 generates an Embodied Chain-of-Thought (Embodied CoT) consisting of autoregressive subgoal language, discrete action tokens (Pertsch et al., 2025), and 2D manipulation trajectories t1:10. While language and discrete tokens are decoded via the VLM head, the 2D trajectory is regressed from learnable tokens through lightweight GRU decoder. In this version, depth information and 2D trajectories are treated as optional states, allowing the model to adapt to varied sensor modalities and task requirements. All components are jointly optimized under unified objective: ‚Ñí = Eùíü,ùúè,ùúñ ùëõ1 ùëó=1 ùëÄCoT,ùëó log ùëùùúÉ (ùë•ùëó+1 ùë•1:ùëó) + ùúñ ùëéchunk ùëìùúÉ (ùëéùúè,ùúñ chunk ) 2 + ùúÜ GRU(ÀÜt1:10) t1: 2 , (1) where ùíü is the training dataset and ùëÄCoT,ùëó {0, 1} is per-token mask indicating whether position ùëó belongs to the CoT reasoning stream (subgoal language or discrete actions). For the diffusion process, ùúè [0, 1] is the flow-matching timestep, ùúñ ùí© (0, I) is Gaussian noise, and ùëéùúè,ùúñ chunk = ùúè ùëéchunk + (1 ùúè ) ùúñ denotes the noised action chunk. The terms ÀÜt1:10 and t1:10 represent predicted and ground-truth trajectory keypoints, balanced by hyperparameter ùúÜ. Notably, Knowledge Insulation (Driess et al., 2025) inherently prevents optimization interference between the language and action prediction terms. 3.2. RAMP In this section, we first formulate the proposed RAMP framework and demonstrate that RECAP (Intelligence et al., 2025) is special case within this formulation. Subsequently, we detail the implementation of RAMP, which encompasses four iterative training stages: (1) World Model Pre-training, (2) Policy Pre-training, (3) Human-in-the-Loop Rollout (HILR) Data Collection, and (4) Policy Training with Rollout Data. 3.2.1. RAMP Formulation To derive scalable training objective that leverages world model latents, we extend the KL-regularized reinforcement learning framework to our augmented state space = (o, z, ùëô), where represents the latent representation extracted by the world model. Our goal is to maximize expected returns while constraining the policy ùúã from deviating from reference policy ùúãref(S) via KL divergence. Drawing on standard results in regularized reinforcement learning, the closed-form solution for the optimal policy is given by (Intelligence et al., 2025): ÀÜùúã(ùëéS) ùúãref(ùëéS) exp . (2) ( ùê¥ùúãref (S, ùëé) ùõΩ ) To mitigate the numerical instability associated with directly estimating the exponential advantage term, we introduce binary improvement indicator ùêº and assume that the probability of observing an improvement event ùëù(ùêºùëé, S) is proportional to the exponential advantage of the action. By applying Bayes theorem, we reformulate 4 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 2: Overview of RAMP. The RAMP framework operates through four-stage pipeline. (1) World Model Pre-training establishes unified representation space for both future state prediction and value estimation. (2) Policy Training with World Model Condition initializes the GigaBrain-0.5 policy with explicit world model conditioning. (3) Human-in-the-Loop Rollout (HILR) Data Collection generates diverse and high-quality trajectories through autonomous execution followed by expert corrections. (4) Continual Training with Rollout Data updates the policy using the annotated trajectory data, incorporating both successful demonstrations and corrective signals. This tightly integrated closed-loop process facilitates continuous policy refinement and self-improvement. this intractable advantage term as ratio of conditional probabilities: exp(ùê¥ùúãref (S, ùëé)/ùõΩ) ùúãref(ùëéùêº, S)/ùúãref(ùëéS). Substituting this ratio back into the optimal policy equation re-expresses ÀÜùúã as composition of the unconditional distribution and the conditional improvement distribution. Consequently, we parameterize neural network ùúãùúÉ to simultaneously fit these distributions, resulting in the final training objective of minimizing the weighted negative log-likelihood: ‚Ñí(ùúÉ) = Eùê∑ [ log ùúãùúÉ(ùëéo, z, ùëô) ùõº log ùúãùúÉ(ùëéùêº, o, zùë°, ùëô)] , (3) where ùêº = 1[ùê¥(o, z, ùëô, ùëé) > ùúñ] serves as the improvement signal. The explicit inclusion of the latent state ùëß in this objective is not merely structural choice, but theoretical necessity. To justify this design, we examine the relationship between our approach RAMP and existing methods like RECAP (Intelligence et al., 2025) from probabilistic perspective. We establish the intrinsic connection between these two paradigms. From probabilistic modeling perspective, we theoretically establish the intrinsic connection between RAMP and RECAP, demonstrating that RECAP is essentially degenerate special case of RAMP where information about future latent states is ignored. Specifically, the policy form of RECAP, ùúã(ùëéùëú, ùêº), is mathematically equivalent to the marginal distribution of the RAMP policy ùúã(ùëéo, z, ùêº) over the latent future state z: ùúãùëÖùê∏ùê∂ùê¥ùëÉ (ùëéo, ùêº) = ùëß ùúãùëÖùê¥ùëÄ ùëÉ (ùëéo, z, ùêº)ùëù(zo, ùêº)ùëëz. (4) This implies that RECAP effectively learns an average policy that must implicitly integrate over and compromise across all possible future evolutions without specific guidance. In contrast, RAMP eliminates this uncertainty by explicitly conditioning on the world models prediction z, transforming the problem from an average guess of 5 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning the future into precise planning targeting specific physical state. Furthermore, from an information-theoretic standpoint, the introduction of the spatio-temporal latent provides significant Information Gain for action generation. While RECAP relies solely on sparse binary advantage signal (ùêº {0, 1}) for coarse credit assignment, RAMP leverages to inject dense geometric structures and physical dynamics priors, thereby significantly reducing the conditional entropy of action generation ùêª(ùëéo, z, ùêº) ùêª(ùëéo, ùêº). 3.2.2. The RAMP Implementation RAMP enables VLA models to learn from experiences by incorporating world model guidance, throughout the entire training lifecycle. Spanning from large-scale offline pre-training to multi-round iterative fine-tuning on autonomous rollout data, our approach achieves progressive policy improvement. As illustrated in Fig. 2, the pipeline is structured into four progressive stages: Stage 1: World Model Pre-training. The initial phase establishes world model ùí≤ùúë capable of jointly predicting future visual states and value estimates. Following the methodology of Intelligence et al. (2025), we derive sparse rewards from episode-level success labels such that the value function corresponds to the negative expected steps-to-completion. Specifically, the reward function is defined as: ùëüùë° = ùê∂fail 1 if ùë° = ùëá and episode succeeds, if ùë° = ùëá and episode fails, otherwise, (5) where ùëá denotes the terminal timestep of the episode and ùê∂fail is large positive constant chosen to ensure that failed episodes receive substantially lower cumulative returns than successful ones. This sparse reward formulation encourages policies to minimize execution time while prioritizing task completion over partial progress. Following the latent frame injection strategy (Kim et al., 2026), we embed the value signal as an additional latent frame that is concatenated with the visual latent state before being fed into the world model. This approach requires no architectural modifications to the underlying Diffusion Transformer. Specifically, future visual observations {oùë°+ùëñ}ùëñ{12,24,36,48} are first encoded into spatiotemporal visual latents zùë° Rùêª ùëä ùê∂ using pre-trained VAE. Concurrently, scalar and low-dimensional auxiliary signals, including the current value estimate ùë£ùë° and proprioceptive state pùë° Rùëë, are transformed via spatial tiling projection Œ®(). This projection replicates and broadcasts the low-dimensional vectors across the spatial dimensions to match the shape of the visual latents. The complete latent state is then constructed as: sùë° = [zùë° ; Œ®(ùë£ùë°) ; Œ®(pùë°)], (6) where [ ; ] denotes channel-wise concatenation. This unified representation enables the world model to jointly reason about visual dynamics, task progress (via value), and robot kinematics within single forward pass. We adopt Wan2.2 (Wang et al., 2025) as the backbone architecture for our world model ùí≤ùúë. The model is trained via flow matching (Lipman et al., 2022). By treating future visual states and value estimates as temporally extended video frames, the DiT backbone naturally leverages its spatiotemporal self-attention mechanisms to model the relationship between current observations, actions, and future task outcomes: ‚ÑíWM = Eùíü,ùúè,ùúñ [ ùí≤ùúë (sùúè,ùúñ future ) (sfuture ùúñ) 2] , (7) where sùúè,ùúñ future = ùúè sfuture + (1 ùúè )ùúñ denotes the linear interpolation between source noise ùúñ ùí© (0, I) and the ground-truth latent state sequence sfuture, with ùúè ùí∞(0, 1). The target term (sfuture ùúñ) corresponds to the constant-velocity vector field along the Optimal Transport path between noise and data distributions. We utilize 4K hours real robot manipulation data to train the world model, the data distribution is visualized in Fig. 3. 6 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Stage 2: Policy Training with World Model Conditioning. The second phase initializes the policy from the pretrained GigaBrain-0.5 checkpoint and further fine-tunes it with world model conditioning. Training details are provided in Sec. 4.2. Specifically, the policy receives two auxiliary signals predicted by the world model ùí≤ùúë: (1) future state tokens zfuture, and (2) value estimates ùë£ùë°. Future state tokens are projected via lightweight MLP to align their dimensionality with the policys visual encoder outputs. Value estimates are transformed into action advantages using ùëõ-step temporal difference estimation: ùê¥(sùë°, ùëéùë°) = ùëõ1 ùëò= ùõæùëòùëüùë°+ùëò + ùõæùëõùë£ùë°+ùëõ ùë£ùë°, (8) where ùë£ùë° and ùë£ùë°+ùëõ denote value predictions for states sùë° and sùë°+ùëõ respectively, and ùõæ is the discount factor. To simplify conditioning while preserving preference structure, advantages are discretized into binary indicator ùêº = 1(ùê¥(sùë°, ùëéùë°) > ùúñ) with threshold ùúñ. The policy is then trained to generate actions conditioned on the tuple (ùêº, z) by minimizing the supervised fine-tuning objective defined in Eq. 3. To prevent over-reliance on synthetic world model signals and ensure flexible deployment, we adopt two complementary strategies during training. First, the world model performs only single denoising step during inference to minimize computational overhead. Second, we implement stochastic attention masking that randomly suppresses world model tokens with probability ùëù = 0.2 during training. This forces the policy to maintain robust performance even when world model inputs are partially or fully unavailable, enabling an efficient inference mode that bypasses world model conditioning. Stage 3: Human-in-the-Loop Rollout Data Collection. In the third phase, we deploy the policy to collect trajectories through human-in-the-loop rollouts. The resulting dataset comprises hybrid mixture of autonomous executions and expert interventions. Autonomous rollouts exhibit significantly reduced action distribution gap compared to conventional teleoperation, since the policy generates actions in its native distribution rather than mimicking human demonstrations, thereby providing more effective supervision signals for VLA learning. However, autonomous execution inevitably encounters failure modes requiring human correction. To mitigate temporal discontinuities introduced by manual interventions, we developed human-in-the-loop rollout data collection software that automatically detects and removes transitional artifacts at intervention boundaries. This smoothing mechanism ensures temporal coherence across the entire trajectory, producing clean, continuous dataset that facilitates stable policy updates in the subsequent training stage while preserving the pedagogical value of expert corrections. Stage 4: Continual Training with Rollout Data. In this stage, we fine-tune the policy using the curated HILR dataset to master complex long-horizon behaviors emerging from the diverse mixture of autonomous executions and expert corrections. Crucially, to prevent advantages collapse toward zero (ùê¥(sùë°, ùëéùë°) 0), the world model ùí≤ùúë is jointly trained with HILR dataset and base data. For the policy training, consistent with Stage 2, we maintain stochastic attention masking with masking probability ùëù = 0.2 applied to both the advantage indicator ùêº and future latent tokens zfuture. This regularization serves dual purposes: (1) it prevents policy over-reliance on world model signals by forcing robustness to missing conditioning inputs, and (2) it ensures architectural and training consistency between pretraining and fine-tuning phases, avoiding distributional shift at inference time. The rolloutannotationtraining cycle operates iteratively, establishing self-improving closed loop: as the policy improves, its autonomous rollouts cover increasingly complex and successful behaviors, which in turn generate higher-quality training data for subsequent iterations. Inference. During deployment, we enforce an optimistic control strategy by fixing the advantage indicator to ùêº = 1. Regarding the latent condition z, the architectural decoupling facilitated by stochastic masking enables two flexible execution modes: (1) an efficient mode, where the world model is bypassed to maximize inference frequency. In this setting, the attention mask is configured to render the future latent tokens invisible to the 7 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 3: Data distribution of the pre-training stage of GigaBrain-0.5. policy, compelling it to act based solely on current observations; (2) and standard mode, where the world model actively generates to provide dense look-ahead guidance. In this setting, the attention mask permits full visibility of the predicted future states, allowing the policy to leverage the prospective context for complex, long-horizon planning. 4. Experiment In this section, we first evaluate the performance of our foundation model, GigaBrain-0.5. On internal robotic evaluation, our model demonstrates robust capability in executing long-horizon, complex procedures such as box packing and coffee preparation. On the public benchmark RoboChallenge (RoboChallenge Team), our foundation model also achieves superior performance compared to ùúã0.5 (Intelligence et al., 2025). Next, we compare our world model-based reinforcement learning approach, RAMP, against established RL baselines including AWR (Peng et al., 2019) and RECAP Intelligence et al. (2025). Experimental results confirm that RAMP exhibits significantly higher sample efficiency and stronger multi-task generalization capability. Finally, we conduct ablation studies to analyze the contribution of the value prediction module within our world model, quantitatively validating its importance for policy learning and task success. 4.1. Foundation Model Performance Pre-training Details. GigaBrain-0.5 is pre-trained on diverse dataset exceeding 10,000 hours, comprising over 6,000 hours of world model-generated data and approximately 4,000 hours of real-robot collected data. The detailed data composition is illustrated in Fig. 3. We train GigaBrain-0.5 using our training framework GigaTrain1 with batch size of 3,072 for 100,000 optimization steps. To reduce per-GPU memory consumption, we employ Fully Sharded Data Parallel (FSDP) v2, applying sharding selectively to all SiglipEncoderLayer modules and only the first 16 layers of Gemma2DecoderLayerWithExpert. Post-training Details. To evaluate the performance of GigaBrain-0.5 on physical robots, we collect task1https://github.com/open-gigaai/giga-train 8 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 4: Performance of GigaBrain-0.5 on internal evaluation. Figure 5: Deployment of GigaBrain-0.5 on PiPER arms for real-world Box Packing. Figure 6: Deployment of GigaBrain-0.5 on the G1 humanoid robot for real-world Box Moving. specific demonstration data on the target robot platform and perform post-training to adapt the model to each task. We conduct comprehensive evaluations on eight internally designed tasks and additionally post-trained the model on 30 tasks from the public benchmark RoboChallenge. Details of the RoboChallenge tasks and evaluation protocol are described in (RoboChallenge Team). Our eight internal evaluation tasks include Juice Preparation, Box Moving, Table Bussing, Paper Towel Preparation, Laundry Folding, Laundry Collection, Box Packing, and Espresso Preparation. Demonstration videos for these tasks are showcased on our project page. For each task, we performed post-training with batch size of 256 for 20,000 optimization steps. 9 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 7: Deployment of GigaBrain-0.5 on PiPER arms for real-world Espresso Preparation. Figure 8: Deployment of GigaBrain-0.5 on the G1 humanoid robot for real-world Juice Preparation. Figure 9: Deployment of GigaBrain-0.5 on the G1 humanoid robot for real-world Laundry Collection. Figure 10: Deployment of GigaBrain-0.5 on PiPER arms for real-world Laundry Folding. Internal Evaluation. In our experiments, we benchmark GigaBrain-0.5 against several strong baselines, including ùúã0 (Black et al., 2024), ùúã0.5, and GigaBrain-0 (Team et al., 2025). The results are summarized in Fig. 4. GigaBrain-0.5 achieves consistent and substantial improvements over its predecessor GigaBrain-0 (Team et al., 2025) across all evaluated tasks, attaining the highest success rate in every case, with particularly notable gains in complex multi-step procedures. For instance, in Juice Preparation, task requiring sequential ingredient handling and mixing, GigaBrain-0.5 achieves 100% success rate, surpassing GigaBrain-0s 90%. 10 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 11: Deployment of GigaBrain-0.5 on PiPER arms for real-world Paper Towel Preparation. Figure 12: Deployment of GigaBrain-0.5 on the G1 humanoid robot for real-world Table Bussing. For challenging tasks such as Box Packing and Espresso Preparation, GigaBrain-0.5 improves success rates by 10% and 20% respectively over ùúã0.5. Similarly, on highly dexterous manipulation tasks (Paper Towel Preparation, Laundry Folding, and Laundry Collection), GigaBrain-0.5 achieves success rates exceeding 80%, outperforming ùúã0.5 by 15%, 5%, and 10% respectively. Additionally, we visualize eight tasks in Fig. 5Fig. 12. RoboChallenge Evaluation. Beyond our internal task evaluations, we also conduct comprehensive assessments on the RoboChallenge (RoboChallenge Team) benchmark. RoboChallenge represents the worlds first largescale embodied AI evaluation platform featuring real-robot testing. The platform has established standardized remote evaluation protocols across cluster of 20 physical robots spanning four major platforms (UR5, Franka, ARX5, and ALOHA). It further provides the open-sourced dataset (736 GB) encompassing 30 standardized manipulation tasks. Detailed task specifications and evaluation methodologies are provided in (RoboChallenge Team). An intermediate iteration model (GigaBrain-0.1) currently ranks first on the leaderboard as of February 9, 2026, achieving an average success rate of 51.67%, an improvement of 9% over ùúã0.5 (42.67%). 4.2. RAMP Performance In this section, we address three core questions through systematic empirical evaluation: (1) Does world modelbased value prediction offer superior accuracy and efficiency compared to the VLM-based approach employed ? (2) Does world model conditioning enhance cross-task generalization capabilities of vision-languagein ùúã* 0.6 action policies? (3) How does our proposed RAMP algorithm compare against alternative reinforcement learning methods in real-robot settings? Value Prediction Performance. To evaluate the efficacy of world model-based value prediction, we conduct comparisons against VLM-based baseline (Intelligence et al., 2025) and our world model-based method. For value prediction in the VLM, we insert learnable [CLS] token at the end of the visual token sequence to aggregate global scene representations, the hidden state of this token is then projected through regression head to produce scalar value prediction in [0, 1]. The model is optimized with mean squared error. Both VLM-based and world model-based value predictors are trained on identical pretraining data and evaluated on 11 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Figure 13: Value prediction visualization from the world model. The orange bounding box highlights value drop during Laundry Folding task when green garment interferes with the folding process, the predicted value recovers after the manipulator successfully removes the obstruction. validation set comprising approximately 1 million frames across the eight manipulation tasks illustrated in Fig. 4. We assess prediction quality using four complementary metrics: MAE, MSE, RMSE (all lower is better), and Kendalls tau rank correlation coefficient (higher is better, with 1 indicating perfect rank preservation). Results averaged across all tasks are summarized in Tab. 1. Our analysis reveals three key findings. First, despite employing lightweight VLM (Intelligence et al., 2025), the VLM-based approach incurs the highest per-frame inference latency (0.32 s, on A800 GPU) due to the computational overhead of the SigLIP (Zhai et al., 2023) visual encoder. Second, the world model variant predicting value alone achieves the fastest inference (0.11 s) but suffers from degraded prediction accuracy (MAE=0.0838, Kendall=0.7288), suggesting that value-only modeling fails to fully exploit the future prediction capabilities inherent in world models. Third, our proposed joint prediction scheme, simultaneously forecasting value and future states, strikes an optimal balance. It achieves the highest Kendalls tau (0.8018) and lowest MAE (0.0621) while maintaining competitive inference speed (0.25 s). This demonstrates that leveraging future state prediction provides crucial contextual grounding for accurate value estimation. Qualitative value prediction visualizations for representative tasks are provided in Fig. 13. GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Table 1: Performance comparison of different value prediction methods. Model VLM-based Inference Time (s) MAE MSE 0.0106 0.0683 0. RMSE Kendall 0.7972 0.1029 WM-based (value only) WM-based (state+value) 0.11 0.25 0. 0.0236 0.1433 0.7288 0.0621 0.0099 0. 0.8018 World Model Conditioning for Policy Learning. To evaluate whether world model conditioning enhances multi-task generalization, we conduct controlled comparison between single-task and multi-task training regimes. We select four representative manipulation tasks for evaluation: Table Bussing, Laundry Folding, Paper Towel Preparation, and Box Packing. To ensure fair comparison and isolate the effect of world model conditioning, we train all policies exclusively on the Stage-2 dataset from RAMP without incorporating any generated rollout data. For single-task training, each policy is trained independently for 20000 steps with batch size of 256. For multi-task training, we uniformly mix data from all four tasks and train single policy for 60000 steps using the same batch size. Our experimental results, as illustrated in Figure 14, demonstrate that the world model condition approach consistently outperforms the baseline across both single-task and multi-task training scenarios. Specifically, incorporating the world model yields substantial performance gains in all evaluated tasks, with significant improvements consistently observed throughout the entire training trajectory from 5,000 to 20,000 steps. Notably, the performance enhancement is particularly pronounced in the multi-task setting, where the success rate gap between the world model approach and the baseline widens progressively during training, achieving up to 30% higher success rates in tasks like Box Packing at step 20000. This indicates that the world model condition effectively facilitates knowledge transfer across multiple tasks while maintaining robust performance gains in single-task scenarios. Figure 14: Comparison of single-task and multi-task performance with and without world model conditions. Comparison with RL Baselines. We benchmark RAMP against state-of-the-art RL baselines. GigaBrain-0.5 + AWR (Peng et al., 2019): An offline RL baseline that fine-tunes the GigaBrain-0.5 policy using weighted imitation learning, utilizing rollouts generated by the current policy. GigaBrain-0.5 + RECAP (Intelligence et al., 2025): An advantage-conditioned offline RL approach that extends the GigaBrain-0.5 backbone with an advantage input, serving as an ablated variant of our method without state prediction. GigaBrain-0.5 + RAMP (GigaBrain-0.5M*): The proposed Reinforcement learning via world Modelconditioned Policy framework, which conditions the GigaBrain-0.5 policy on both the predicted value and future state latents to optimize long-horizon task performance. 13 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning Our RAMP framework establishes top performance across all three highly challenging manipulation tasks: Box Packing, Espresso Preparation, and Laundry Folding. As quantified in Fig. 15, RAMP achieves nearperfect success rates on all evaluated tasks, significantly outperforming all baseline methods (GigaBrain-0.5, GigaBrain-0.5+AWR, and GigaBrain-0.5+RECAP). Notably, RAMP demonstrates particularly substantial gains on Box Packing and Espresso Preparation, where it surpasses the RECAP baseline by approximately 30% points. Critically, the GigaBrain-0.5M* model (i.e., GigaBrain-0.5 integrated with our RAMP framework) exhibits robust and consistent task execution capabilities, achieving reliable success in real-world deployment as empirically validated by the supplementary execution videos on our project page. This unprecedented performance across multiple complex manipulation tasks underscores the effectiveness of RAMP in solving challenging real-world robotics problems. Figure 15: Comparison of different RL methods. 5. Conclusion and Future Work In this work, we present GigaBrain-0.5 and its world model-enhanced successor GigaBrain-0.5M*, advancing the frontier of VLA learning through large-scale pretraining and model-based RL. GigaBrain-0.5, pretrained on over 10,000 hours of diverse robotic data, demonstrates state-of-the-art performance across eight internal manipulation tasks and 30 standardized tasks on the RoboChallenge benchmark, achieving 51.67% average success rate and securing the top position on the public leaderboard. Building upon this strong foundation, GigaBrain-0.5M* introduces novel world model-conditioned architecture that leverages future state prediction to overcome the limited anticipation capabilities inherent in conventional VLA models. By integrating modelbased reinforcement learning through RAMP, our approach achieves robust cross-task generalization and reliably executes complex long-horizon tasks such as sequential box packing and espresso preparation. Looking ahead, the GigaBrain series will investigate more efficient utilization of model rollout data to maximize the informational value of synthetic trajectories while minimizing computational overhead. Furthermore, we aim to explore more scalable self-evolution paradigms that enable autonomous data curation, policy refinement, and world model updating through closed-loop interaction. 14 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning References [1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 3 [3] Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. [4] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 3 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [6] Lucas Beyer, Andreas Steiner, Andr√© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 3 [7] Hongzhe Bi, Hengkai Tan, Shenghao Xie, Zeyuan Wang, Shuhe Huang, Haitian Liu, Ruowen Zhao, Yao Feng, Chendong Xiang, Yinze Rong, et al. Motus: unified latent action world model. arXiv preprint arXiv:2512.13030, 2025. 3 [8] Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 1, [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. ùúã0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 1, 2, 10 [10] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 1 [11] Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 3 [12] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. 1, 2 [13] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019. [14] Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, et al. Emma: Generalizing real-world robot manipulation via generative visual transfer. arXiv preprint arXiv:2509.22407, 2025. 3 15 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning [15] Ria Doshi, Homer Walke, Oier Mees, Sudeep Dasari, and Sergey Levine. Scaling cross-embodied learning: One policy for manipulation, navigation, locomotion and aviation. arXiv preprint arXiv:2408.11812, 2024. 2 [16] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. 4 [17] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. [18] Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, and Jun Zhu. Generalist bimanual manipulation via foundation video diffusion models. arXiv preprint arXiv:2507.12898, 2025. 3 [19] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. 3 [20] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024. 3 [21] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning. arXiv preprint, arXiv:2501.16664, 2025. 3 [22] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 3 [23] Physical Intelligence, Ali Amin, Raichelle Aniceto, Ashwin Balakrishna, Kevin Black, Ken Conley, Grace : vla that learns from experience. Connors, James Darpinian, Karan Dhabalia, Jared DiCarlo, et al. ùúã* 0.6 arXiv preprint arXiv:2511.14759, 2025. 2, 3, 4, 5, 6, 8, 11, 12, 13 [24] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. ùúã0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 1, 2, 8 [25] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In CoRL, 2022. 3 [26] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through video world models. arXiv preprint arXiv:2505.12705, 2025. 3 [27] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea open-world dataset and g0 dual-system vla model. arXiv preprint arXiv:2509.00576, 2025. 1 [28] Yuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, et al. Enerverse-ac: Envisioning embodied environments with action condition. arXiv preprint arXiv:2505.09723, 2025. 16 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning [29] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. QT-Opt: Scalable deep reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. 3 [30] Michael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel Kochenderfer. Hg-dagger: Interactive imitation learning with human experts. In ICRA, 2019. 3 [31] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [32] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2 [33] Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, et al. Cosmos policy: Fine-tuning video models for visuomotor control and planning. arXiv preprint arXiv:2601.16163, 2026. 3, 6 [34] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3 [35] Jakub Grudzien Kuba, Pieter Abbeel, and Sergey Levine. Advantage-conditioned diffusion: Offline rl via generalization. 2023. 3 [36] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. CoRR, abs/1912.13465, 2019. 3 [37] Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, et al. Mimicdreamer: Aligning human and robot demonstrations for scalable vla training. arXiv preprint arXiv:2509.22199, 2025. 3 [38] Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, and Yinghao Xu. Causal world modeling for robot control, 2026. 3 [39] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. 2 [40] Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, et al. Drivevla-w0: World models amplify data scaling law in autonomous driving. arXiv preprint arXiv:2510.12796, 2025. [41] Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. 4 [42] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025. 3 [43] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3, 4, 6 [44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. 3 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning [45] Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang, and Zhizhong Su. Robotransfer: Geometry-consistent video diffusion for robotic visual policy transfer. arXiv preprint arXiv:2505.23171, 2025. 3 [46] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. 3 [47] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 2 [48] Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint, arXiv:2505.18719, 2025. [49] Andr√©s Marafioti, Orr Zohar, Miquel Farr√©, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. 3 [50] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, et al. Recondreamer: Crafting world models for driving scene reconstruction via online restoration. arXiv preprint arXiv:2411.19548, 2024. 3 [51] Chaojun Ni, Cheng Chen, Xiaofeng Wang, Zheng Zhu, Wenzhao Zheng, Boyuan Wang, Tianrun Chen, Guosheng Zhao, Haoyun Li, Zhehao Dong, et al. Swiftvla: Unlocking spatiotemporal dynamics for lightweight vla models at minimal overhead. arXiv preprint arXiv:2512.00903, 2025. 2 [52] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, and Wenjun Mei. Recondreamer-rl: Enhancing reinforcement learning via diffusion-based scene reconstruction. arXiv preprint arXiv:2508.08170, 2025. 3 [53] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In ICRA, 2024. 2, 3 [54] Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, and Elvis Nava. mimicvideo: Video-action models for generalizable robot control beyond vlas. arXiv preprint arXiv:2512.15692, 2025. [55] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 4 [56] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019. 2, 8, 13 [57] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 3 [58] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. 2, 4 [59] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. 18 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning [60] Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, et al. Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042, 2025. 3 [61] RoboChallenge Team. URL https://robochallenge.ai/home. 2, 8, 9, 11 [62] St√©phane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, pages 627635, 2011. [63] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. 3 [64] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [65] Andreas Steiner, Andr√© Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 3, 4 [66] Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp Kr√§henb√ºhl. Interactive post-training for vision-languageaction models. arXiv preprint, arXiv:2505.17016, 2025. [67] GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, et al. Gigabrain-0: world model-powered vision-language-action model. arXiv preprint arXiv:2510.19430, 2025. 1, 2, 4, 10 [68] GigaWorld Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, et al. Gigaworld-0: World models as data engine to empower embodied ai. arXiv preprint arXiv:2511.19861, 2025. 3 [69] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. 2 [70] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In CoRL, 2023. 3 [71] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 6 [72] Lirui Wang, Xinlei Chen, Jialiang Zhao, and Kaiming He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. NeurIPS, 2024. 2 [73] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-drive world models for autonomous driving. In ECCV, 2024. 3 [74] Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. In NeurIPS, 2023. 3 [75] Yuan Xu, Jiabing Yang, Xiaofeng Wang, Yixiang Chen, Zheng Zhu, Bowen Fang, Guan Huang, Xinze Chen, Yun Ye, Qiang Zhang, et al. Egodemogen: Novel egocentric demonstration generation enables viewpoint-robust manipulation. arXiv preprint arXiv:2509.22578, 2025. 3 GigaBrain-0.5M*: VLA That Learns From World Model-Based Reinforcement Learning [76] Chengbo Yuan, Suraj Joshi, Shaoting Zhu, Hang Su, Hang Zhao, and Yang Gao. Roboengine: Plug-andplay robot data augmentation with semantic robot segmentation and background generation. arXiv preprint arXiv:2505.18738, 2025. 3 [77] Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, et al. Igniting vlms toward the embodied space. arXiv preprint arXiv:2509.11766, 2025. 1 [78] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. [79] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, et al. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. 3 [80] Guosheng Zhao, Xiaofeng Wang, Chaojun Ni, Zheng Zhu, Wenkang Qin, Guan Huang, and Xingang Wang. Recondreamer++: Harmonizing generative and reconstructive models for driving scene representation. arXiv preprint arXiv:2503.18438, 2025. 3 [81] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In AAAI, 2025. 3 [82] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024."
        }
    ],
    "affiliations": [
        "GigaAI"
    ]
}