{
    "paper_title": "BUT System for the MLC-SLM Challenge",
    "authors": [
        "Alexander Polok",
        "Jiangyu Han",
        "Dominik Klement",
        "Samuele Cornell",
        "Jan Černocký",
        "Lukáš Burget"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a two-speaker automatic speech recognition (ASR) system that combines DiCoW -- a diarization-conditioned variant of Whisper -- with DiariZen, a diarization pipeline built on top of Pyannote. We first evaluate both systems in out-of-domain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for target-speaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training data -- such as missing speech segments and incorrect silence annotations -- which can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness."
        },
        {
            "title": "Start",
            "content": "BUT System for the MLC-SLM Challenge Alexander Polok1, Jiangyu Han1, Dominik Klement1, Samuele Cornell2, Jan ˇCernocky1, Lukaˇs Burget1 1Speech@FIT, Brno University of Technology, Czechia 2Language Technologies Institute, Carnegie Mellon University, USA ipoloka@fit.vut.cz 5 2 0 2 6 ] . e [ 1 4 1 4 3 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present two-speaker automatic speech recognition (ASR) system that combines DiCoWa diarization-conditioned variant of Whisperwith DiariZen, diarization pipeline built on top of Pyannote. We first evaluate both systems in out-ofdomain (OOD) multilingual scenarios without any fine-tuning. In this scenario, DiariZen consistently outperforms the baseline Pyannote diarization model, demonstrating strong generalization. Despite being fine-tuned on English-only data for targetspeaker ASR, DiCoW retains solid multilingual performance, indicating that encoder modifications preserve Whispers multilingual capabilities. We then fine-tune both DiCoW and DiariZen on the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform the fine-tuned Pyannote baseline, while DiCoW sees further gains from domain adaptation. Our final system achieves micro-average tcpWER/CER of 16.75 % and ranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several labeling inconsistencies in the training datasuch as missing speech segments and incorrect silence annotationswhich can hinder diarization fine-tuning. We propose simple mitigation strategies to address these issues and improve system robustness. Index Terms: DiCoW, Multilingual Multi-Talker ASR, DiariZen, Whisper 1. Introduction [1, 2], Recent advances in deep learning, particularly the rise of large self-supervised models large language models (LLMs) [3, 4], and Whisper-style supervised architectures [5, 6], have led to substantial progress in automatic speech recognition (ASR), even under challenging acoustic conditions. These models achieve remarkable accuracy by leveraging massive training data [7, 8] and scaling up model parameters [9]. However, such advancements have primarily benefited singlespeaker ASR, while most real-world scenarios involve conversational speech between multiple speakers [10, 11, 12]. Several approaches have been proposed for speaker-attributed ASR in multi-speaker settings [13, 14, 15]. Some systems operate modularly, combining diarization, source separation, speaker clustering, and ASR as separate components. Others follow endto-end strategies incorporating speaker tokens [16, 17] or use multiple decoder heads to generate separate transcripts [18]. As an alternative, Target-speaker ASR offers middle ground by conditioning ASR models directly on speaker identity using embeddings or enrollment audio [19, 20, 21, 22]. While effective in controlled settings, these methods often depend on speaker-specific representations, which can be difficult to generalize, especially when training data is limited or speaker variability is low. ASR systems, our previously proposed methodDiarizationConditioned Whisper (DiCoW) [14, 23, 24]conditions the model directly on frame-level diarization masks, bypassing the need for explicit speaker identity modeling. For speaker diarization, self-supervised learning (SSL) models like WavLM [1] have shown great potential, as demonstrated by several recent studies [25, 26]. Among these, our recent work DiariZen [26] built upon Pyannote [27, 28], achieves competitive performance across diverse benchmarks by combining WavLM with the Conformer [29]. In this work, we build on DiCoW by combining it with DiariZen to form complete two-speaker ASR pipeline. We begin by evaluating the zero-shot performance of both models on multilingual datawithout any domain adaptationto test their robustness in real-world scenarios. We then fine-tune them on target-domain data to analyze the benefits of adaptation. Beyond improved performance, we uncover two key insights. First, even though DiCoW was fine-tuned exclusively on English data, it retains Whispers multilingual capabilities. This finding aligns with our prior observations [24, Table 10], where the model showed only minimal degradation on standard singlespeaker tasks, suggesting that encoder conditioning via diarization masks does not compromise Whispers [5] core strengths. Second, during fine-tuning, we observed labeling inconsistencies in the training datasuch as skipped speech segments and prolonged silences annotated as speechwhich may hinder the effective training of diarization systems. As result, the diarization model may learn to reproduce these patterns on the target data, negatively impacting downstream ASR performance, even in cases where diarization error rate (DER) on the development set suggests substantial improvements. To support continued research in this direction, we release both the DiCoW1 and DiariZen2 models to the community. 2. Method This section provides an overview of our system, which is designed similarly to the official baseline3 but with two key differences: the Pyannote diarization module is replaced by DiariZen, and unlike the baseline systemwhich processes each chunk extracted from diarization independently, either with Whisper or Whisper connected to an LLMour approach operates on the full recording and integrates DiCoW. Next, we introduce both DiariZen and DiCoW in detail. 1https://huggingface.co/BUT-FIT/DiCoW_v3_MLC 2https://huggingface.co/BUT-FIT/ diarizen-wavlm-large-s80-mlc 3https://github.com/mubingshen/ To address the limitations of traditional speaker-attributed MLC-SLM-Baseline Figure 1: Framework of local EEND module for DiariZen. Figure adapted from [26]. abilities are defined as follows: (cid:89) pt = (1 d(s, t)), pt = d(sk, t) s= (1 d(s, t)) (cid:89) s=1 s=sk = (cid:0)1 pt pt (cid:1) (sk, t) , = d(sk, t) pt pt , (1) where is number of speakers, and d(s, t) is probabilty of speaker speaking in timestamp t. Each Transformer layer in the Whisper encoder layer is then augmented with four affine transformation matricesone for each STNO classwhich are blended based on their probabilities for each frame, forming the Frame-Level Diarization-Dependent Transformations (FDDT): (cid:16) (cid:16) (cid:17) (cid:17) ˆzl = zl + bl Wl (cid:16) pt + (cid:17) zl Wl (cid:16) + bl pt (cid:17) + Wl zl + bl pt + Wl Ozl + bl pt O. (2) The FDDT parameters are initialized such that the data flow in the original model is not disrupted [23], ensuring that the behavior of the pre-trained Whisper model is preserved at the start. The entire system, including the new FDDT parameters, is then jointly fine-tuned. 3. Experimental Setup 3.1. DiariZen Our diarization system builds upon the DiariZen framework4, following the training approach described in [26]. We use WavLM Large as the backbone for frame-level classification to improve local modeling. The model is pre-trained on farfield, single-channel audio from diverse collection of public datasets, including AMI [30], AISHELL-4 [31], AliMeeting [32], NOTSOFAR-1 [33], MSDWild [34], DIHARD3 [35], RAMC [36], and VoxConverse [37]. We then apply structured pruning [38] to remove 80% of WavLMs parameters. After that, the model is fine-tuned on the official MLC-SLM challenge dataset. We use powerset loss [28] with two speakers. We extract speaker embeddings using ResNet34-LM model trained with the WeSpeaker toolkit [39] on the VoxCeleb2 dataset [40]. The embeddings are then clustered using agglomerative hierarchical clustering. 3.2. DiCoW Our training follows the DiCoW [24] training recipe5 without altering the core hyperparameters or schedule. It uses three-phase training strategy: CTC preheat, FDDT preheat, and full fine-tuning. For CTC preheating, we utilize LibriSpeech [41]. During the following two phases, we train Whisper large-v3-turbo on mix of publicly available multi-speaker datasetsAMI, NOTSOFAR-1, and Libri2Mix [42]. Afterwards, we fine-tune the model on the MLC-SLM dataset. Similar to the original data preparation, we segment the training split into chunks of at most 30 seconds, in accordance with Whispers input length limitation. To achieve this, we loop through the provided segments and iteratively concatenate consecutive ones until the total duration reaches 30 seconds. During training, we aim to be more tolerant of differences in annotation styles between the original Whisper training data and the fine-tuning (FT) data. To address this, we apply challenge text normalization procedure to the reference 4https://github.com/BUTSpeechFIT/DiariZen 5https://github.com/BUTSpeechFIT/ TS-ASR-Whisper Figure 2: Overview of the DiCoW model architecture. The model is based on the Whisper architecture, with modifications to incorporate frame-level diarization information through Frame-Level Diarization Dependent Transformations (FDDT). Figure adapted from [24]. 2.1. DiariZen DiariZen is speaker diarization pipeline built on Pyannote. Given long audio recording, DiariZen first segments the input into shorter chunks and then applies local end-to-end neural diarization (EEND) to each chunk. For each speaker identified within chunk by the local EEND, speaker embedding is extracted from their corresponding speech. These embeddings are then clustered to determine correspondence between speakers across chunks and to generate the final diarization results. As illustrated in Figure 1, the EEND module in DiariZen combines WavLM and Conformer. The outputs from each WavLM layer are aggregated using weighted sum to form the input sequence for the Conformer. classification head is then used to map the Conformer outputs to powerset states [28]. 2.2. DiCoW DiCoW extends the Whisper architecture for target-speaker ASR (TS-ASR) by conditioning the model on frame-level diarization signals, as illustrated in Figure 2. Unlike traditional TS-ASR approaches that rely on speaker embeddings or enrollment utterances, DiCoW uses probabilistic speaker activity estimates derived from diarization. Specifically, it computes Silence-Target-NonTarget-Overlap (STNO) mask that captures the likelihood of four speaking conditions at each time frame: silence, target speaker active, non-target speaker active, and overlap of target speaker with different speaker(s). These probTable 1: Diarization performance of Pyannote and DiariZenbased systems across multiple languages using the MLC-SLM development data. Fine-tuned Pyannote results are from the official baseline."
        },
        {
            "title": "OOD",
            "content": "FT 33.4 21.3 27.7 18.6 11.3 38.5 34.3 15.2 42.0 42.0 30.2 16.4 23.2 27.0 27.0 27.2 20.2 13.8 18.9 13.2 8.2 22.6 22.3 10.6 26.5 23.3 17.6 11.4 12.9 10.9 14.6 16.4 24.8 18.7 22.7 18.0 10.6 28.3 30.4 12.4 34.7 24.2 28.3 12.8 21.6 18.8 20. 21.8 15.9 10.8 12.1 10.3 6.0 17.3 16.4 8.9 17.8 16.4 14.8 10.0 10.8 10.6 12.7 12.7 transcripts by lowercasing all tokens and removing punctuation. For each training sample, we then compute the crossentropy loss twiceonce using the normalized (lowercased and punctuation-removed) tokens, and once using the original cased tokensand backpropagate the smaller of the two loss values. We apply early stopping with patience of 5, selecting the best-performing checkpoint based on tcpWER evaluated every 500 steps using greedy decoding. Training is capped at 50k steps. Final outputs on the test set are generated using beam search with beam size of 10. Joint CTC decoding [43] with weight of 0.2 is utilized for English utterances. All experiments are conducted on four NVIDIA RTX A6000 GPUs with mixedprecision training and an overall batch size of 96 samples. 4. Results In this section, we evaluate our systems performance. We begin with diarization error rates (DERs), followed by TS-ASR results using both ground truth and DiariZen-derived segmentations. We then analyze labeling inconsistencies in the training and development data and their impact, and to address these issues, we incorporate an auxiliary VAD model that jointly models speech and silence alongside our diarization system. 4.1. Diarization Improvements We compare the diarization performance of DiariZen and the baseline Pyannote model under out-of-domain (OOD) and finetuned (FT) conditions. Both models are fine-tuned on the MLCSLM training data, and DER is evaluated without forgiveness collar. Results are presented in Table 1. DiariZen consistently outperforms the Pyannote baseline in both OOD and fine-tuned settings, achieving DER of 12.7 % after fine-tuning versus 16.4 % for Pyannote. However, we caution the reader against interpreting this as evidence that diarization fine-tuning itself led to the improvement. These DER gains may be misleading, as they are likely driven by the system learning to better mimic the inconsistent Table 2: tcpWER/CER (%) on the MLC-SLM development set, broken down by language, comparing the baseline and DiCoW systems using ground-truth (GT) and real diarization before and after fine-tuning (FT). The baseline system uses Whisper largev3 with chunked inference and fine-tuned Pyannote diarization model. In contrast, our system uses fine-tuned DiariZen diarization model. Results marked with an asterisk (*) are reported using tcpCER, following the official evaluation protocol."
        },
        {
            "title": "Baseline DiCoW FT Baseline DiCoW FT",
            "content": "American En. Australian En. British En. Filipino En. Indian En. French German Italian Japanese Korean Portuguese Russian Spanish Thai Vietnamese"
        },
        {
            "title": "Overall",
            "content": "14.1 11.7 10.1 9.2 14.0 28.1 20.7 17.9 21.6 13.8 21.2 17.7 12.3 14.5 27.2 16.8 20.6 19.4 16.7 17.7 14.3 27.7 21.2 16.2 19.2 12.8 24.5 17.6 11.6 31.9 30.0 53.7 11.1 52.6 7.4 71.9 7.7 50.4 7.5 70.7 13.3 96.0 16.1 86.7 23.9 83.3 12.3 71.3 13.7 8.5 59.6 19.5 118.8 69.2 11.6 75.6 8.7 83.6 14.2 82.8 15.3 36.5 23.6 26.1 25.5 14.9 37.8 30.1 19.8 25.8 24.5 33.1 22.5 18.2 34.4 33.8 22.5 13.0 17.6 15.2 14.0 27.5 27.3 16.4 23.3 22.8 29.7 16.7 16.3 20.1 24. 22.0 12.9 76.1 28.4 20.8 annotation style present in both the training and development data, rather than genuine improvements in diarization quality. We further examine and discuss these issues in Section 4.3. 4.2. Full System Comparison: Diarization and ASR In Table 2, we compare the overall performance of our system against the Pyannote & Whisper large-v3 baseline on the MLC-SLM development set. For ground-truth (GT) segmentation (left side of the table), the baseline uses Whisper large-v3, while our system employs Whisper large-v3-turbo adapted for TS-ASR. It is important to note that training TS-ASR exclusively on English data may have limited performance on other languages. Additionally, the turbo model is reported to perform slightly worse than the v3 model on certain languages, such as Thai6. However, our systems use of long-form inference may offer compensatory benefits compared to the baseline. After fine-tuning, our model outperforms the large-v3 baseline on GT segmentation. We note that results on German might be affected by labeling inconsistencies in the dataset. On the right side of Table 2, full system results are presented for real diarization segmentations using both the baseline Pyannote & Whisper large-v3 system and our out-of-domain (OOD) and fine-tuned (FT) DiCoW system conditioned by FT DiariZen segmentation. DiCoW paired with DiariZen without domain adaptation significantly outperforms the baseline Pyannote-Whisper-v3 system. Fine-tuning DiCoW further imthese results demonproves overall performance. Overall, strate consistent gains using our diarization-conditioned ASR approach, especially after fine-tuning. 6https://github.com/openai/whisper/ discussions/2363 Table 3: Comparison of DER and micro average tcpWER/CER when utilizing out-of-domain (OOD), fine-tuned (FT), and finetuned with VAD (FT+VAD) diarization models. Results are shown on the original development data and on modified test-like development set. Diarization Model DER Miss FA Conf. tcpWER/CER OOD FT FT+VAD OOD FT FT+VAD"
        },
        {
            "title": "Original development data",
            "content": "21.8 7.5 3.8 12.7 22.8 13.4 11.5 6.7 6.9 2.8 2.2 2.5 Test-like development data 13.5 7.5 3.9 12.4 16.2 13.7 3.1 4.5 0. 2.9 4.0 2.3 26.7 20.8 23.6 19.9 22.4 17.9 4.3. Labeling Inconsistencies and Their Impact We identify labeling inconsistencies in the training and development splits of the MLC-SLM dataset that affect the performance of both our diarization system, the fine-tuned Pyannote baseline, and likely other participants systems. To mitigate this, we propose simple approach using the Silero VAD [44] model. The core issue arises from the annotation protocol used in the training and development data. In some cases, actual speech is not annotated and is therefore treated as silence, while in other cases, segments labeled as speech include long internal pauses or silences. This is not problematic in Task 1, where utterancelevel ground-truth (GT) segmentation is provided, as each labeled chunk can be processed independently. However, in Task 2, which requires joint diarization and ASR, these inconsistencies introduce mismatch: the diarization system may learn from the MLC-SLM training data to skip valid speech in order to minimize loss, leading to degraded performance on the test set. Unlike the training and development data, the test set follows cleaner protocol, where all unannotated segments are explicitly muted by the organizers. This, however, introduces signal-level mismatch relative to the training and development partitions, which can further degrade performance. To better simulate these test conditions, we create modified version of the development set in which unannotated segments with noticeable energy are muted, while originally silent regions remain unchanged. Note, however, that the training data used for fine-tuning remains unchanged and inconsistent with the test-like development data. Table 3 presents diarization performance in terms of DER and tcpWER for both OOD and FT systems on the original development data and modified test-like version. We make two key observations. First, on the original development set, large portion of the OOD models DER comes from false alarms (FA), which often correspond to real speech that is simply unlabeled. This is evident from the FA dropping from 11.5 % to 3.1 % on the test-like version. Second, the higher Miss rate of the OOD model on testlike data (7.5 % vs. 3.9 % for FT) is also misleading. Due to inconsistencies in the original annotationswhere some silence regions are labeled as speechmore accurate silence detection appears as an increased Miss rate. Moreover, the fine-tuned model struggles under test-like conditions, as it often predicts speech in silent regions, causing speaker embeddings to be extracted from silence. This negaFigure 3: Example of the ground truth diarization; our system before fine-tuning on MLC; the same system after fine-tuning; and the fine-tuned system with probabilities merged using auxiliary VAD. tively impacts clustering quality and increases speaker confusion errors. To mitigate these issues, we incorporate Silero VAD model with weight of 0.8 to improve speech/silence estimation alongside our diarization system. Additionally, to comply with the annotation protocolwhich disallows overlapping speechwe redistribute the probability mass of overlapping frames across individual speakers and select the most likely speaker per frame. This approach significantly improves tcpWER on the test-like development set from 22.4 % to 17.9 %, and on the test set from 28.6 % to 17.4 %. Figure 3 shows an example from the development set (English-American-0525 002). The difference between the spectrogram and the GT segmentation reveals skipped speech and inaccurate overlaps. On the other hand, the OOD system closely follows actual speech activity, which was confirmed by listening to the audio, while fine-tuning shifts predictions toward inconsistent GT labels. Although FT+VAD seemingly worsens DER, it effectively corrects VAD errors and provides the best performance when used with the TS-ASR system. 5. Conclusions We presented simple, non-LLM approach for multilingual long-form transcription combining diarization and targetspeaker ASR. Our system placed second in the MLC-SLM Challenge, showing strong performance across diverse languages. While effective, our method has limitations that open In particular, we did not fully exavenues for future work. plore optimal training strategies for diarization on loosely annotated data. Additionally, replacing DiCoW with diarizationconditioned speech LLM could further enhance performance and remains promising direction. 6. Acknowledgements The work was supported by Ministry of Education, Youth and Sports of the Czech Republic (MoE) through the OP JAK project Linguistics, Artificial Intelligence and Language and Speech Technologies: from Research to Applications (ID:CZ.02.01.01/00/23 020/0008518) and by Czech Ministry of Interior project No. VK01020132 112. Computing on IT4I supercomputer was supported by MoE through the e-INFRA CZ (ID:90254). 7. References [1] S. Chen et al., WavLM: Large-scale self-supervised pre-training for full stack speech processing, IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., HuBERT: How much can bad teacher benefit ASR pre-training? in 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021. [3] J. Achiam et al., GPT-4 technical report, arXiv preprint arXiv:2303.08774, 2023. [4] H. Touvron et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [5] A. Radford et al., Robust speech recognition via large-scale weak supervision, in International conference on machine learning. PMLR, 2023. [6] Y. Peng et al., Reproducing Whisper-style training using an open-source toolkit and publicly available data, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023. [7] X. Li et al., YODAS: Youtube-oriented dataset for audio and speech, in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023. [8] G. Chen et al., GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio, in Interspeech 2021, 2021. [9] W. Chen et al., OWLS: Scaling laws for multilingual speech recognition and translation models, 2025. [10] S. Cornell et al., The CHiME-7 DASR challenge: Distant meeting transcription with multiple devices in diverse scenarios, in 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2023), 2023. [11] , The CHiME-8 DASR challenge for generalizable and array agnostic distant automatic speech recognition and diarization, in 8th International Workshop on Speech Processing in Everyday Environments (CHiME 2024), 2024. [12] A. Vinnikov et al., NOTSOFAR-1 challenge: New datasets, baseline, and tasks for distant meeting transcription, in Interspeech 2024, 2024. [13] S. Niu et al., The USTC-NERCSLIP systems for the CHiME8 NOTSOFAR-1 challenge, in 8th International Workshop on Speech Processing in Everyday Environments (CHiME 2024), 2024. [14] A. Polok et al., BUT/JHU system description for CHiME8 NOTSOFAR-1 challenge, in 8th International Workshop on Speech Processing in Everyday Environments (CHiME 2024), 2024. [15] K. Huang et al., The NPU-TEA system for the CHiME-8 NOTSOFAR-1 challenge, in 8th International Workshop on Speech Processing in Everyday Environments (CHiME 2024), 2024. [16] N. Kanda et al., Serialized output training for end-to-end overlapped speech recognition, in Interspeech 2020, 2020. [17] S. Cornell et al., One model to rule them all? Towards end-to-end joint speaker diarization and speech recognition, in 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024. [18] D. Yu et al., Permutation invariant training of deep models for speaker-independent multi-talker speech separation, in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017. [19] N. Kanda et al., Auxiliary interference speaker loss for targetspeaker speech recognition, in Interspeech 2019, 2019. [20] Y. Zhang et al., Conformer-based target-speaker automatic speech recognition for single-channel audio, in 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. [21] Z. Huang et al., Adapting self-supervised models to multi-talker speech recognition using speaker embeddings, in 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. [22] L. Meng et al., Empowering Whisper as joint multi-talker and target-talker speech recognition system, in Interspeech 2024, 2024. [23] A. Polok et al., Target speaker ASR with Whisper, in 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. [24] , DiCoW: Diarization-conditioned Whisper speaker automatic speech recognition, 2024. for target [25] A. Plaquet et al., Mamba-based segmentation model for speaker diarization, in ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. [26] J. Han et al., Leveraging self-supervised learning for speaker diarization, in 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2025. [27] H. Bredin, pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe, in Interspeech 2023, 2023. [28] A. Plaquet and H. Bredin, Powerset multi-class cross entropy loss for neural speaker diarization, in Interspeech 2023, 2023. [29] A. Gulati et al., Conformer: Convolution-augmented transformer for speech recognition, in Interspeech 2020, 2020. [30] I. Mccowan et al., The AMI meeting corpus, Intl. Conf. on Methods and Techniques in Behavioral Research, 01 2005. [31] Y. Fu et al., AISHELL-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario, in Proc. Interspeech, 2021. [32] F. Yu et al., M2MeT: The ICASSP 2022 multi-channel multiIEEE, party meeting transcription challenge, in Proc. ICASSP. 2022. [33] A. Vinnikov et al., NOTSOFAR-1 challenge: New datasets, baseline, and tasks for distant meeting transcription, in Interspeech 2024, 2024. [34] T. Liu et al., MSDWild: Multi-modal speaker diarization dataset in the wild. in INTERSPEECH, 2022. [35] N. Ryant et al., The third DIHARD diarization challenge, in Interspeech 2021, 2021. [36] Z. Yang et al., Open source MagicData-RAMC: rich annotated Mandarin conversational (RAMC) speech dataset, arXiv preprint arXiv:2203.16844, 2022. [37] J. S. Chung et al., Spot the conversation: Speaker diarisation in the wild, in Interspeech 2020, 2020. [38] J. Han et al., Fine-tune before structured pruning: Towards compact and accurate self-supervised models for speaker diarization, arXiv preprint arXiv:2505.24111, 2025. [39] S. Wang et al., Advancing speaker embedding learning: Wespeaker toolkit for research and production, Speech Communication, 2024. [40] J. S. Chung et al., VoxCeleb2: Deep speaker recognition, in Interspeech 2018, 2018. [41] V. Panayotov et al., Librispeech: An ASR corpus based on public domain audio books, in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. [42] J. Cosentino et al., LibriMix: An open-source dataset for generalizable speech separation, arXiv: Audio and Speech Processing, 2020. [43] T. Hori et al., Joint CTC/attention decoding for end-to-end speech recognition, in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Association for Computational Linguistics, Jul. 2017. [44] S. Team, Silero VAD: pre-trained enterprise-grade voice activity detector (VAD), number detector and language classifier, 2024."
        }
    ],
    "affiliations": [
        "Language Technologies Institute, Carnegie Mellon University, USA",
        "Speech@FIT, Brno University of Technology, Czechia"
    ]
}