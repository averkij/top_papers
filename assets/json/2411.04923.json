{
    "paper_title": "VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
    "authors": [
        "Shehan Munasinghe",
        "Hanan Gani",
        "Wenqi Zhu",
        "Jiale Cao",
        "Eric Xing",
        "Fahad Shahbaz Khan",
        "Salman Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 3 2 9 4 0 . 1 1 4 2 : r VideoGLaMM : Large Multimodal Model for Pixel-Level Visual Grounding in Videos Shehan Munasinghe1 Hanan Gani1 Wenqi Zhu2 Jiale Cao 2 Eric Xing1,3 Fahad Shahbaz Khan1,4 Salman Khan1,5 1Mohamed bin Zayed University of AI, 2Tianjin University, 3Carnegie Mellon University, 4Linköping University, 5Australian National University shehan.munasinghe@mbzuai.ac.ae, hanan.ghani@mbzuai.ac.ae https://mbzuai-oryx.github.io/VideoGLaMM"
        },
        {
            "title": "Abstract",
            "content": "Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: Large Language Model, dual vision encoder that emphasizes both spatial and temporal details, and spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable VL and LV adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate multimodal dataset featuring detailed visually-grounded conversations using semiautomatic annotation pipeline, resulting in diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks. 1. Introduction The rise of Large Language Models (LLMs) has significantly advanced progress in language-based tasks [7, 10, 13, 34, 45]. Their success in solving language-based complex reasoning tasks has led to their adoption in visual domains, resulting in Large Multimodal Models (LMMs). To align textual and visual modalities, previous works [11, 19, 20, Figure 1. Grounded Conversation with VideoGLaMM. Our proposed multimodal video conversational model provides text responses grounded at the pixel level in the input video. The generated masks are spatio-temporally consistent across frames. The fine-grained grounded outputs from VideoGLaMM describe different levels of granularity, e.g., person, objects (bike), stuff (road), and explain object and scene attributes. Existing VideoLMMs do not offer pixel-level grounded conversational capability. 27, 55] train projection layer or cross-attention block that maps visual features into the latent space of an LLM. This straightforward adaptation has enabled advanced spatial understanding, allowing detailed conversations about image content. Recently, these models have been extended to video, aligning textual instructions with the spatio-temporal inputs, leading to the development of Video-LMMs. Existing Video-LMMs [9, 21, 23, 2931, 50], similar to image-based LMMs, tune single or multiple projection layers to align videos with the language modality using the conventional visual instruction tuning paradigm. Although this simple alignment aids in understanding the global content of videos, it poses challenges in capturing localized object-specific context. Consequently, existing works [21, 23, 31, 50] have demonstrated capabilities in video comprehension and dialogue, they lack the crucial feature of fine-grained visual grounding, which aims to associate the LMMs response to specific objects within the video input. The ability of an LMM to generate visually grounded responses ensures that the model understands fine-grained spatial and temporal details in video and can relate them with the generated text. To bridge this gap, we introduce VideoGLaMM, large video multimodal model capable of pixel-level spatiotemporal grounding. The model responds to natural language queries from the user and intertwines spatio-temporal object masks in its generated textual responses to provide detailed understanding of video content. VideoGLaMM seamlessly connects three key components: Large Language Model (LLM); dual vision encoders; and spatiotemporal pixel decoder. The dual vision encoders extract spatial and temporal features separately, which are jointly passed to the LLM to output responses rich in both spatial and temporal cues. Our spatio-temporal pixel decoder outputs the fine-grained object masks corresponding to the specific objects in the LLM output to visually ground its responses. These components are integrated via tunable Vision-to-Language (VL) and Language-to-Vision (LV) adapters that enable close vision-language alignment, unlike existing works that perform alignment with single adapter. As there currently exists no instruction-tuning dataset with fine-grained masks associated with video conversations, we present benchmark instruction tuning dataset curated through semi-automatic pipeline (Sec. 4). The dataset consists of 38k grounded video-QA triplet pairs with 83k objects and 671k fine-grained masks. The proposed benchmark dataset enables spatio-temporal modeling and significantly augments the capacity of the model to understand videos comprehensively, leading to state-of-the-art performance in grounded conversation generation, temporal grounding, and referring video segmentation tasks under zero-shot settings. In summary, our contributions are as follows: We introduce VideoGLaMM, video large multimodal model, capable of pixel-level spatio-temporal grounding, featuring an end-to-end alignment mechanism. To achieve fine-grained spatio-temporal alignment, we introduce benchmark instruction tuning dataset consisting of 38k grounded video-QA triplet pairs and 83k objects and roughly 671k fine-grained spatio-temporal masks. We assess the performance of VideoGLaMM across diverse tasks spanning grounded conversation generation, visual grounding, and referring video segmentation, where it achieves state-of-the-art performance. 2. Related work Large Multi-modal Models (LMMs). Vision-language models like [36] have made notable advancements, demonstrating impressive zero-shot capabilities using millions of noisy image-text pairs during training. These models have been effective in various applications, from detection and segmentation [6, 22] to more complex tasks such as 3D understanding and video analysis [28, 32, 42, 46]. The rise of LLMs has driven significant progress in Natural Language Processing (NLP) tasks and sparked interest in developing LMMs. Early models [2, 4] incorporate visual information into intermediate embeddings for frozen LLM using cross-attention mechanism, trained on billions of image-text pairs to align visual and linguistic modalities. Similarly, BLIP-2 [20] introduces Q-Former to better align visual features with language space. MiniGPT-4 [55] and LLAVA [27] finetune on detailed image descriptions using single projection layer to align frozen visual encoder with frozen LLM. Subsequent LLaVA series models [25] employ multi-layer perceptron and two-stage instruction tuning to refine the alignment process. While these works work on static images, our work focuses on efficiently aligning videos with linguistic cues. Video LMMs. Recent advancements in image-based multimodal models have paved the way for video LMMs, which are essential for handling spatiotemporal sequences. Models such as VideoChat [21], Video-LLaMA, VideoChatGPT [50], Video-LLAVA [23] and Video-GPT+ [30] extend the capabilities of LLMs to video domain by aligning video features with language, followed by instruction tuning on datasets annotated by either GPT models or humans. While these models have shown effectiveness in video comprehension, they still face limitations in finegrained spatio-temporal modeling and visual grounding. This restricts their ability to accurately understand or localize specific objects and detailed segments within videos, highlighting the need for further advancements in developing better multimodal models capable of visual grounding. Visual Grounding. Recently Grounded LMMs [9, 19, 35, 38, 47, 49, 52] have made significant strides in enhancing visual and language comprehension and excel in complex localization tasks. These models demonstrate proficiency in tasks such as referring expression comprehension and image segmentation, highlighting the advanced image understanding capabilities of LLMs. Approaches such as Figure 2. Working of VideoGLaMM. VideoGLaMM consists of dual spatio-temporal encoder for encoding image and video level features. The spatial features represent the local information and the temporal features represent global information. The spatial and temporal tokens are passed through V-L adapters and concatenated with the text tokens, before feeding to LLM. L-V projector is employed to align LLMs response with the visual space of pixel decoder. Finally, the aligned LLM features along with the frame features from frame encoder are passed to grounded pixel decoder, to obtain the fine-grained object masks corresponding to the LLM response. [9, 35, 47] primarily focus on creating language-based context for visual grounding. In contrast, [52] integrates visual elements with language, while [19] leverages visionlanguage embeddings to produce segmentation masks. Additionally, [38] is adept at generating natural language responses linked with object segmentation masks, facilitating detailed visual-textual interactions. However, these models are limited to image-based applications and do not extend to video understanding. Recently, Munasinghe et al. [31] incorporates audio transcripts alongside visual and textual data for more detailed video understanding. However, it combines pre-trained modules that cannot be trained endto-end. Although effective in video understanding, the lack of fine-grained spatial-temporal modeling in [31] hinders the precise understanding and location of segments in detail. To this end, we propose novel fine-grained alignment strategy to align language instruction across both spatial and temporal dimensions. Our approach facilitates more finegrained video understanding via precise visual grounding. 3. VideoGLaMM 3.1. Overview In this work, we introduce VideoGLaMM, multi-modal video LMM with spatio-temporal pixel grounding capability. The task of spatio-temporal visual grounding focuses on linking models response to user-specific text query with particular objects and regions within video, ensuring that both spatial (whats happening in each frame) and temporal (how things change over time) details are accurately reflected in the generated output  (Fig. 1)  . By grounding responses in specific objects and actions across frames, the model demonstrates an understanding of both the evolving and static elements in video, enabling it to produce responses that align closely with the visual narrative. Our proposed VideoGLaMM is designed to achieve effective spatio-temporal grounding due to its ability to process spatial and temporal features simultaneously. VideoGLaMMs architecture  (Fig. 2)  leverages dualencoder structure: one encoder focuses on extracting spatial details from images, while the other captures temporal information from video sequences, ensuring complementary representation from both modalities. The visual features from both encoders are then integrated with LLM using separate spatial and temporal adapters (VL), guided by specific textual instructions. The LLM outputs are aligned back with the visual space using an LV adapter and further processed by pixel decoder, which also takes video frames as input to produce the final grounded outputs. For end-to-end spatio-temporal alignment, we train VideoGLaMM on our proposed fine-grained benchmark dataset, consisting of 38k video-QA pairs with 83k objects and roughly 671k masks. During training, we finetune the LoRA parameters of the LLM, along with VL and LV adapters. This approach seamlessly combines spatial and temporal data through an improved alignment mechanism and precise grounding framework, enhancing the models capability for visual grounding and understanding. The proposed model can perform diverse range of tasks, including grounded conversation generation, visual grounding, and referring video segmentation. 3.2. Architecture The overall architecture of our VideoGLaMM consists of the following components: (i) Spatio-Temporal Dual Encoder, (ii) Dual Alignment V-L Adapters for image and video features, (iii) Large Language Model (LLM), (iv) Promptable Pixel Decoder. Below we provide detailed description and also explain how the input is handled in each of these components. Spatio-Temporal Dual Encoder. Our architecture consists of separate image and video encoders for extracting spatial and temporal features, thus leveraging the complementary strengths of both. This enables the model to have both local and global properties. The image encoder Fg, processes the video frames separately such that the input video RT HW C. The output of the image encoder, represented by fg, produces local spatial features that provide framelevel context. fg = Fg(V ), RT HW (1) Meanwhile, for extracting video features, we use segmentwise Sampling following [30] to obtain fine-grained temporal cues. Given an input video RT HW C, we divide it into segments, where each segment consists of = . The video encoder Fh, operates on low-resolution video segments Vk RsHW yielding global features that provide segment-wise temporal context. fh = Fh(Vk), Vk RsHW (2) Dual Alignment (VL) Adapters To align visual features with the LLM space, we use two separate VL adapters for image and video encoders. Wg represents the spatial adapter, and Wh represents the temporal adapter. These adapters project the visual features into the LLMs projection space, thus aligning the two modalities. The spatial and visual features corresponding to image and video samples after projecting from Wg and Wh are represented by Zg and Zh, respectively. Zg = Wg(fg) Zh = Wh(fh) (3) Large Language Model The tokenized spatio-temporal visual features are then concatenated with the textual tokens Ztext RLDt to obtain final feature embedding = [Zg, Zh, Ztext] which is fed into the LLM. Thus, input to the LLM contains both the spatial and temporal cues for robust video understanding. We further expand the original LLM vocabulary with new token, i.e., <SEG>, which signifies the request for the segmentation output. Thus the LLM response can be described as, = LLM(Z) = LLM([Zg, Zh, Ztext]) (4) The LLM output contains the <SEG> whenever the task requires to generate the segmentation mask. Pixel Decoder Our Pixel decoder consists of prompt encoder (H) and mask decoder D, capable of predicting masks with spatio-temporal grounding. The pixel decoder is adapted to videos and can implicitly process temporal information. The last layer embeddings from the LLM denoted as lseg corresponding to <SEG> token is extracted, which is enriched with both spatial and temporal cues. The LLM embeddings act as prompts for the mask decoder and are processed by the prompt encoder. Simultaneously, we extract visual features of the input frames using grounded frame encoder which is aligned with pixel decoder and is further equipped with the ability to produce multi-scale features during training. For aligning the output embeddings from LLM with the pixel decoder, we train an (LV) adapter layer Wp between the LLM and prompt encoder such that the output from the adapter is denoted as ep seg = Wp(lseg). The ep seg is fed to prompt encoder H, such that the encoded output H(ep seg) is used to prompt the mask decoder. The encoded prompts H(ep seg) along with the grounded visual features P(V ) are passed to mask decoder D. Subsequently, produces the output mask M. (cid:16) = P(V ), H(ep seg) (cid:17) (5) 3.3. Training Strategy We train VideoGLaMM end-to-end in single stage. As stated above, we use dual encoder consisting of separate image and video encoders for processing spatial and temporal inputs to obtain local and global features, respectively. These encoders are initialized with weights if strong encoders. During the training, we keep the encoders fixed and only train the VL adapters Wg and Wh associated with these encoders. These adapters are used to project the spatio-temporal visual features in the space of LLM and align the two modules. The spatio-temporal encoder is kept frozen and only the VL adapters are updated. The textual features from the last layer of LLM, rich in spatial and temporal cues, are projected into the space of the pixel decoder using multi-layer projection LV adapter Wp. For the LLM, we keep its weights frozen and only finetune LoRA [14] parameters during training. The frame encoder associated with the pixel decoder is instantiated with pre-trained weights and produces multi-scale features from the input video frames. We keep the frame encoder and pixel decoder frozen and only train the LV adapter layer. We optimize the output of the LLM by minimizing the cross entropy CE objective between the autoregressively obtained text output and dense grounded ground-truth caption. For the output of mask decoder, we optimize the intersection over union (IOU) between the predictions of mask decoder and groundFigure 3. Proposed Semi-automatic Annotation Pipeline. Our dataset for grounded conversation generation (GCG) is built from three video dataset types: i) Videos having masks only: Object patches are extracted from video frames using masks and processed by the Gemini model for initial object descriptions, which are then refined to produce detailed object captions. These refined captions and masks are used again with the Gemini model to create dense, grounded captions. ii) Videos having bbox annotations and captions: Frames are first processed with Video-LMM to generate comprehensive caption which is combined with the original caption and fed to GPT-4o to obtain dense grounded captions. Masks are generated using frames and ground-truth bounding boxes with the SAM model. iii) Videos having object bboxes and referring expressions: Frames, bounding boxes, and referring expressions are input to GPT-4o for dense grounded captions, while masks are generated by feeding frames and bounding boxes to the SAM model. truth masks denoted as Lmasked. The total loss is the sum of CE loss and masked loss. Ltotal = CE + Lmasked (6) The first component of Ltotal ensures that the LLM generates textual embeddings that not only align with the ground truth but also offer informative spatio-temporal cues to the mask decoder for effective grounding. The second component facilitates efficient grounding by leveraging these textual cues from the LLM. 4. Our Benchmark & Annotation Pipeline Our benchmark video dataset comes from different sources: YTVIS [16], BURST [3] ActivityNet entities [54], ReferYTVOS [43], MEVIS [12], VidSTG [51] and HCSTVG [44]. To create fine-grained grounded captions, we develop semi-automated pipeline  (Fig. 3)  that ensures high-quality and scalable annotation. Our annotation pipeline is categorized into three streams based on the availability of the ground truth annotations. We explain each stream below. a) Videos with only Mask annotations: Fig. 3(a) shows the annotation process for the videos having only masks as ground truth labels. To generate the corresponding dense i) Object Degrounded caption, we use following steps: scription Generation: For each object in the video, we begin by creating bounding box based on the ground truth mask provided in the annotation file. This bounding box allows us to crop the object from each frame, producing sequence of image patches that capture the object throughout the video. We then feed these image patches to the GeminiPro model [41] to obtain rough description of each object in the video. ii) Object Description Refinement: The bounding boxes from the previous stage are superimposed on the corresponding video frames, and the entire video is then fed into the Gemini-Pro model to obtain more accurate and detailed description of the objects. iii) Caption Generation: The bounding boxes of corresponding objects overlayed across the video frames are labeled according to their object IDs. Then, we input these frames into the GeminiPro model to obtain dense captions. This results in comprehensive description of the video. Finally, we manually review the {obj_id} in the generated video captions based iv) Detailed Dense Captions. To on the video content. enhance the detail and accuracy of the video captions, we leverage two advanced Video LMMs: Video-LLAVA [50] and LLAVA-NeXT [26]. Using the semi-automatically generated captions as reference, we integrate and refine the outputs from these models, merging their results to produce the final, comprehensive dense captions. b) Videos with Bounding Box annotations and Captions: Fig. 3(b) shows the annotation process for the videos having both captions and object bounding box (Bbox) annotations. To obtain the corresponding dense grounded caption, the video frames are first passed to an open-source VideoLMM [26] to obtain detailed caption, which is fed along with the reference ground truth caption to GPT-4o mini [33] to obtain the final dense grounded caption. The Bbox annotations are used as prompts to SAM model [18] which takes the video frames as input and provides the masks corresponding to the objects. c) Videos with Bounding Box annotations and Referring Expressions: Fig. 3(c) shows the annotation process for the videos having object bounding box (Bbox) annotations and referring expressions corresponding to different objects. The video frames along with Referring expressions and Bbox annotations are prompted to GPT-4o mini, which provides the corresponding dense grounded caption. To obtain the masks corresponding to the objects, the video frames are fed to SAM model, which is prompted with Bbox annotations of the objects. Overall, our proposed GCG dataset has 38,788 grounded video-QA triplets along with 83,877 objects and 6,71,016 fine-grained masks in total. We further curate separate test set of 308 refined video-QA triplets with 826 objects and 22762 finegrained masks for grounded conversation generation evaluation task. 5. Experimental Setup Implementation details. Our spatio-temporal dual encoders follow the design of image and video encoders from VideoGPT+ [30]. For the image encoder, we use pretrained CLIP ViT-L/14 (336 336)[36] model, and for the temporal encoder, we select the pretrained encoder of InternVideov2 (224 224) [48]. The VL projectors are initialized with the weights of MLP adapter from VideoGPT+ [30]. The LLM is instantiated with Phi3-Mini-3.8B [1] weights. Both the frame encoder and pixel decoder are initialized with SAM2 [40] encoder-decoder weights. The training (Sec. 3.3) is carried out end-to-end on 4 Nvidia A100 40GB GPUs with distributed training based on DeepSpeed [39]. Datasets. We train the model on our proposed grounded conversation (GCG) dataset containing 38k grounded video-QA triplets along with 83k objects and 671k finegrained masks. Our proposed benchmark is derived from YTVOS [16], BURST [3] ActivityNet entities [54], ReferYTVOS [43], MEVIS [12], VidSTG [51] and HCSTVG [44] datasets. During training, we also include variety of other image and video datasets with our proposed benchmark dataset for more robust alignment. Our choice of image datasets include: ADE20K [53], COCO-Stuff [8], LVIS-PACO [37], refCOCO, refCOCO+, refCLEF, refCOCOg [15], LLaVA-Instruct-150k [24], ReasonSeg [19] and GranDf [38]. For video-datasets we include train samples from: Refer-DAVIS17 [17], VideoInstruct100K [29]. Tasks. We evaluate VideoGLaMM on three challenging tasks: grounded conversation generation (GCG), visual grounding, and referring video segmentation, in zero-shot settings. For grounded conversation generation, we curate separate dataset of 308 refined video-QA triplets containing 826 objects and 22,762 fine-grained masks, following our proposed annotation pipeline. For Visual Grounding, we evaluate our model on two challenging datasets: VidSTG [51] and HC-STVG [44]. VidSTG dataset consists of 5693 videos paired with multiform sentences (both interrogative and declarative). In the case of referring video segmentation, we leverage the MeViS [12] validation dataset consisting of 140 videos and 2,236 referring sentences. All the results on MeViS dataset are obtained via official CodaLab evaluation suite. Evaluation metrics. For GCG task, we use mean Intersection over Union (mIOU) and Recall to determine the correctness of generated masks, and METEOR, CIDEr and CLAIR score for determining the goodness of conversaIn the case of visual grounding, we report tional output. mean Intersection over Union (mIOU) to quantify the performance. Finally, for referring video segmentation, We report Region Jaccard , Boundary measure F, and their mean &F. Baselines. We compare our VideoGLaMM with two challenging baselines employing LLMs capable of visual grounding: PG-Video-LLaVA [31] and GLaMM [38]. Since GlaMM is designed for pixel grounding in images, we enable temporal properties in GLaMM by augmenting its architecture with SAM2. For referring segmentation, we also compare VideoGLaMM with the recently released Video-LISA [5]. Training Recipe. VideoGLaMM follows gradual training schedule. We do not train VideoGLaMM on our GCG dataset directly from the start, rather we take gradual approach. We first train the model on image and video segmentation datasets until epoch 20 and then introduce our GCG dataset and train the model until epoch 30. This training recipe ensures that model learns both the spatial and temporal cues effectively. Model mIoU Recall METEOR CIDEr CLAIR PG-Video-LLaVA [31] GLaMM [38] + SAM2 [40] VideoGLaMM 24.03 28.60 62.34 0.093 0.117 0.375 0.10 0.097 0.103 0.01 0.15 0. 15.0 22.9 28.2 Table 1. Evaluation on grounded conversation generation (GCG): VideoGLaMM shows superior performance in generating accurate video-level captions which are tied to corresponding segmentation masks in the video frames. 5.1. Grounded Conversation Generation The Grounded Conversation Generation (GCG) task aims to provide video-level detailed captions with specific phrases directly tied to corresponding segmentation masks in the video frames. For example, <An adult> in white clothes holds <cup> in the room\", as shown in the first row of Fig. 4, features how each bracketed phrase is anchored to unique segmentation mask. This creates densely annotated caption that aligns textual descriptions with visual regions in the frames, enriching the videos contextual interpretation. To obtain GCG output, we query the model with the following sample prompt: Provide detailed description of the image. Respond with interleaved segmentation masks for the corresponding parts of the answer. The model generates detailed caption along with interleaved segmentation masks, employing the format <p>An adult woman in brown</p><SEG> is talking to another <p>adult man wearing jacket</p><SEG> shown in the third row of Fig. 4. We use special tokens, namely <p>, </p> and <SEG>, to delineate the start and end of each phrase and its corresponding region mask, respectively. Figure 4. Qualitative results of VideoGLaMM on grounded conversation generation (GCG). Given user queries, the VideoGLaMM generates textual responses and grounds objects and phrases using pixel-level masks, showing its detailed understanding of the video. Our proposed GCG test set is meticulously constructed as explained in Sec. 4, capturing fine-grained annotations. The test set contains 308 refined video-QA triplets with 826 objects and 22,762 fine-grained masks. As shown in Table 1, our proposed Video-GLaMM performs better in generating detailed captions containing references to objects in the video frames, as is evident from high METEOR, CIDEr and CLAIR scores. Regarding the quality of masks, Video-GLaMM consistently outperforms baselines in terms of mIOU and Recall scores, signifying higher overlap with ground-truth masks. Fig. 4 further shows the qualitative visualizations of VideoGLaMM on GCG samples. 5.2. Referring Video Segmentation the output should be For referring video segmentation, grounded as per the given phrase, pointing towards specific instances in the video. Given sentence or referring expression containing specific object instance, the goal is to localize the object instances present across the video frames. This task operates in an open vocabulary setting, assessing the models ability to localize objects both spatially and temporally. Given referring phrase expression Phrase, we prompt the model using the following instruction prompt to obtain the instance masks: What is {Phrase} in this video? Respond with segmentation masks\". As seen from Table 2, both the region Jaccard and boundary F-measure are high in the case of VideoGLaMM, significantly outperforming the baselines. Similarly, the mean score &F follows the same trend. Additionally, the scores corresponding to VideoLISA are reported with post-processing step. Notably, VideoLISA involves an additional post-processing step to boost performance. Therefore, we further fine-tune the VideoGLaMM on the task of referring segmentation post epoch 30 until epoch 40. Clearly, VideoGLaMM outperforms the Video-LISA (postprocessed) on both and F, including the mean &F. Model PG-Video-LLaVA [31] GLaMM [38]+ SAM2 [40] VideoLISA [5] VideoGLaMM 18.35 35.80 41.30 42.07 19.39 41.50 47.60 48.23 &F 18.87 38.66 44.40 45.15 Table 2. Performance comparison of VideoGLaMM on referring expression segmentation: VideoGLaMM shows superior performance on motion grounding and segmenting referring objects in the videos. VideoGLaMM-FT, which is further finetuned on the task of referring segmentation, shows improvements over post-processed VideoLISA. 5.3. Visual Grounding To quantitatively assess VideoGLaMMs visual grounding capability, we conduct quantitative evaluations on the benchmark datasets derived from the test set of VidSTG dataset as stated in the Tasks section above. The visual grounding task measures the adeptness of the model at correlating textual descriptions with visual elements in the video, critical aspect of contextual comprehension. This ability is crucial for applications that integrate continuous visual data with language. The output of this task is refined masks that correlate with the caption caption. To obtain the visual grounding output, we query the model with interrogative captions. For these captions, the prompt format follows {caption} Please respond with segmentation masks.\". Table 3 shows VideoGLaMMs improved visual grounding precision as it outperforms the baselines and demonstrates its fine-grained understanding. Model VidSTG (interrogative mIoU) Decoder Configuration mIoU Recall METEOR CIDEr CLAIR PG-Video-LLaVA-7B [31] PG-Video-LLaVA-13B [31] GLaMM [38] + SAM2 [40] VideoGLaMM 34.20 35.10 38.63 39.66 Table 3. Performance comparison of VideoGLaMM with other models on spatial grounding: Results on VidSTG (interrogative) benchmark highlights VideoGLaMMs superior ability in correlating textual instructions with the visual frames. 5.4. Ablation studies Effect of Spatio-Temporal Dual Encoder. We employ separate image and video encoders to process spatial and temporal information. While spatial processing induces local information, temporal processing includes global view of video in the model. Both are necessary from the perspective of grounding. To verify the effectiveness of using the dual spatio-temporal encoder, we conduct an ablation study where we measure the effectiveness of each encoder for grounded conversation generation task (see Table 4). We notice that using only an image encoder gives suboptimal results, as we notice drop in both the localization and captioning metrics. Using only video branch leads to the highest mIOU; however, relatively lower METEOR, CIDEr, and CLAIR scores. To get the best of both worlds, i.e., optimal mIOU and good conversational abilities, VideoGLaMM uses both image and video branches. Encoder Configuration mIoU Recall METEOR CIDEr CLAIR Image encoder Video encoder Dual encoder 60.06 64.62 62.34 0.395 0.375 0.375 0.081 0.097 0.103 0.371 0.568 0.590 18.9 26.5 28.2 Table 4. Effect of Spatio-Temporal Dual Encoder: We obtain low performance using only spatial (image) encoder. Using only video encoder gives the highest mIOU but lower scores on CLAIR, METEOR and CIDEr. For better trade-off, we employ dual (image and video) encoders to have accurate, grounded conversations. Spatial vs Spatio-temporal Pixel decoder. Pixel decoder in VideoGLaMM can operate in two configurations. The first configuration processes video frames individually, ignoring temporal consistency. The second configuration employs both spatial and temporal branches for spatiotemporal context. Table 5 demonstrates the impact of adding temporal branch alongside the spatial branch in our grounded conversation generation dataset. Results indicate that using only the spatial configuration reduces performance, with nearly 3% drop in mIOU scores compared to the spatio-temporal configuration. Similarly, metrics like METEOR, CIDEr, and CLAIR also show decline. Effect of number of frames for Pixel Decoder. The Pixel decoder receives the raw input frames as supervision sigSpatial decoder Spatio-temporal decoder 59.68 62.34 0.369 0.375 0.097 0.103 0.553 0. 26.7 28.2 Table 5. Spatial vs Spatio-temporal Pixel decoder: We observe that using Pixel decoder without the temporal branch gives limited performance as the model faces difficulties in temporal grounding. When using temporal branch, the performance on both the temporal grounding and grounded LLM response improves indicating the importance of temporal processing in VideoGLaMM. nals through the frame encoder. However, Pixel decoder can only take one frame as input at time, which doesnt offer temporal information. To provide more temporal supervision, we instead feed the Pixel decoder multiple frames to enhance its temporal understanding. This allows it to learn semantic information that generalizes across frames. Table 6 shows the performance when 4 and 8 frames are input to the decoder. We observe that while the mIOU with 8 frames is slightly lower compared to 4 frames, the conversational quality measured by METEOR, CIDEr and CLAIR is higher. Hence, to achieve decent mIOU with higher conversational output, we stick to 8 frames as input. Decoder Input frames mIoU Recall METEOR CIDEr CLAIR 4 frames 8 frames 63.82 62. 0.37 0.37 0.094 0.103 0.659 0.590 27.2 28.2 Table 6. Effect of number of frames for Pixel Decoder: We observe that using 4 supervision frames for pixel decoder gives better mIOU but relatively modest conversation quality measured by METEOR and CLAIR. With 8 supervision frames, mIOU slightly decreases while the conversational quality increases. 6. Limitations and Future Work Our GCG dataset plays key role in enhancing the models grounding capabilities. While we validated annotations manually, some noise may still be present. Also, each scene contains several objects and the video descriptions do not exhaustively cover all objects in the scenes. higherquality densely annotated set could further boost model performance but would require substantial annotation resources. Additionally, VideoGLaMM struggles with objects of varying granularities, likely due to limited representation in the training data. Another improvement is to extend VideoGLaMM for longer videos, as the current GCG dataset mainly focuses on short-medium duration clips. 7. Conclusion In this paper, we introduce VideoGLaMM, LMM specifically designed to address the challenge of fine-grained pixel-level grounding in videos. By integrating dual vision encoder with spatio-temporal decoder and employing tunable Vision-Language adapters, our model achieves precise instrucalignment between video content and textual tions. To facilitate this alignment, we introduce refined instruction-tuning dataset curated via semi-automatic annotation pipeline. Our experimental evaluations across Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation demonstrate that VideoGLaMM consistently outperforms existing models."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 6 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [3] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: benchmark for unifying object recognition, segmentation and tracking in video. In WACV, 2023. 5, 6 [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 2 [5] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. Language instructed reasoning segmentation in videos. arXiv preprint arXiv:2409.19603, 2024. 6, 7 One token to seg them all: [6] Hanoona Bangalath, Muhammad Maaz, Muhammad Uzair Khattak, Salman Khan, and Fahad Shahbaz Khan. Bridging the gap between object and image-level representations for open-vocabulary detection. In NeurIPS, 2022. [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proc. of NeurIPS, 2020. 1 [8] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Cocostuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. 6 modal llms referential dialogue magic. arXiv:2306.15195, 2023. 2, 3 arXiv preprint [10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 1 [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. In Proc. of NeurIPS, 2023. [12] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023. 5, 6 [13] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: general language In model pretraining with autoregressive blank infilling. Proc. of ACL, pages 320335, 2022. 1 [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 4 [15] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787798, Doha, Qatar, 2014. Association for Computational Linguistics. 6 [16] Lei Ke, Henghui Ding, Martin Danelljan, Yu-Wing Tai, ChiKeung Tang, and Fisher Yu. Video mask transfiner for highIn European Conferquality video instance segmentation. ence on Computer Vision (ECCV), 2022. 5, 6 [17] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. arxiv: 1803.08006, 2018. [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 5 [19] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint arXiv:2308.00692, 2023. 1, 2, 3, 6 [20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 2 [21] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv:2305.06355, 2023. 2 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiFeng Zhu, and Rui Zhao. [22] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70617070, 2023. 2 [23] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 6 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [26] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5 [27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2 [28] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks by interacting with chatbots beyond language. arXiv preprint arXiv:2305.05662, 2023. 2 [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv:2306.05424, 2023. 2, [30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 2, 4, 6 [31] Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large videolanguage models. arXiv preprint arXiv:2311.13435, 2023. 2, 3, 6, 7, 8 [32] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for genIn European Conference on Comeral video recognition. puter Vision, pages 118. Springer, 2022. 2 [33] OpenAI. Gpt-4v(ision) system card. https://openai.com/ research/gpt-4v-system-card, 2023. 5 [34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Proc. of NeurIPS, 2022. [35] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2, 3 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 6 [37] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts In Proceedings of the and attributes of common objects. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. 6 [38] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. arXiv preprint arXiv:2311.03356, 2023. 2, 3, 6, 7, 8 [39] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD, pages 35053506, 2020. 6 [40] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 6, 7, [41] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 5 [42] David Rozenberszki, Or Litany, and Angela Dai. Languagegrounded indoor 3d semantic segmentation in the wild. In European Conference on Computer Vision, pages 125141. Springer, 2022. 2 [43] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with In Computer VisionECCV 2020: large-scale benchmark. 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pages 208223. Springer, 2020. 5, 6 [44] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE Transactions on Circuits and Systems for Video Technology, 32(12):82388249, 2021. 5, 6 [45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, 2023. 1 [46] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: new paradigm for video action recognition. arXiv preprint arXiv:2109.08472, 2021. [47] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907, 2023. 2, 3 [48] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation modarXiv preprint els for multimodal video understanding. arXiv:2403.15377, 2024. 6 [49] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 2 [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv:2306.02858, 2023. 2, 5 [51] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng Liu, and Lianli Gao. Where does it exist: Spatio-temporal video grounding for multi-form sentences. In CVPR, 2020. 5, 6 [52] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. 2, [53] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 6 [54] Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason Corso, and Marcus Rohrbach. Grounded video description. In CVPR, 2019. 5, 6 [55] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1,"
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of AI",
        "Tianjin University",
        "Carnegie Mellon University",
        "Linköping University",
        "Australian National University"
    ]
}