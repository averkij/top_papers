{
    "paper_title": "Adaptive Layer-skipping in Pre-trained LLMs",
    "authors": [
        "Xuan Luo",
        "Weizhi Wang",
        "Xifeng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 8 9 7 3 2 . 3 0 5 2 : r Adaptive Layer-skipping in Pre-trained LLMs Xuan Luo, Weizhi Wang, Xifeng Yan Department of Computer Science, UC Santa Barbara xuan_luo@ucsb.edu, weizhiwang@ucsb.edu, xyan@cs.ucsb.edu"
        },
        {
            "title": "Abstract",
            "content": "Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-38B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and dataset documenting FlexiDepths layer allocation patterns for future exploration. Model Dataset xuan-luo/FlexiDepth-Llama-3-8B-Instruct xuan-luo/FlexiPatterns-Llama-3-8B-Instruct"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable success in various tasks, including translation [44], code generation [16], and math reasoning [6]. Currently, LLMs typically generate each token by performing full forward pass through all Transformer decoder layers. However, such uniform allocation is counter-intuitive, as simpler tasks intuitively require fewer computational resources, whereas more complex tasks demand more extensive processing. This uniform allocation not only results in computational inefficiencies but may also contribute to overfitting in LLMs. To improve the efficiency of token generation, various layer-skipping techniques have been proposed. One line of research leverages statistical information, such as the difference between layer inputs and outputs, to identify and skip less important layers [26, 13, 43]. Another collection of methods involves early-exit [37, 18, 34], where confidence measure or router dynamically determines whether to bypass all subsequent layers at an intermediate point of LLM. While these techniques have successfully reduced computational costs, they overlook fundamental question: How do computational demands vary across the generation of different tokens In this work, we propose FlexiDepth, method for adaptive layer-skipping in pre-trained large language models. At each transformer layer, our method determines whether the hidden state input shall forward pass the layer or skip it. This layer-wise approach offers the flexibility to tailor the computation path for each token, enabling processing with varying numbers of transformer layers. Figure 1: Token depth map illustrating layer-skipping patterns when applying FlexiDepth to Llama-38B-Instruct. The light-to-dark blue gradient indicates layer usage ranging from 16 to 32. Contrast to layer skipping methods requiring training LLM from scratch, i.e. mixture-of-depth (MoD) [31], FlexiDepth enables adaptive layer-skipping in pre-trained LLMs without modifying their original parameters. Specifically, FlexiDepth introduces two light-weight plug-in modules at each decoder layer: (1) router that makes binary decisions about whether to skip the layer, and (2) adapter that resolves representation mis-alignments caused by layer skipping. The router and adapter are the only trainable components within our framework. All other parameters are inherited from pre-trained LLM and remain frozen. At each layer, the input hidden states are first evaluated by the router and directed to go through or skip the layer. For hidden states that go through the layer, they are processed by the original attention and FFN modules of the LLM. Conversely, hidden states that skip the layer bypass these modules and are instead processed by the adapter. The adapters role is to transform these skipped hidden states, aligning their representation to match those undergo full processing. Finally, both the adapted hidden states and the processed hidden states are combined to produce the layer output. To ensure compatibility with autoregressive generation, FlexiDepth computes the KV cache [28] for all hidden states, including those that skip this layer. This crucial step preserves the full context required for the attention mechanism. Without maintaining the KV cache, subsequent queries would be unable to attend to the hidden states that skipped the layer, rendering those tokens permanently unavailable for future generation. This would compromise the models ability to produce coherent and contextually accurate sequences. Preparing the full KV cache offers simple method to maintain the integrity of autoregressive generation. We comprehensively evaluate FlexiDepth across diverse set of benchmarks. When applied to Llama-3-8B-Instruct model [8], FlexiDepth preserves full performance (100.7%) while skipping 8 out of 32 layers, significantly outperforming existing layer-skipping methods, particularly in continuous generation tasks. Beyond performance gain, FlexiDepth can reveal layer allocation patterns of pre-trained LLMs, addressing the question of how computational demands vary when generating different types of tokens. To demonstrate this phenomenon, we created colored map that shows the number of layers used to generate each token, and call it \"token depth map\". This token depth map visualization, as shown in Figure 1 (left), reveals that for different language generation tasks, summarization typically utilizes more layers on average than extractive question answering. Similarly, in mathematical reasoning tasks like addition (Figure 1, right), tokens on the left-hand side of equations require fewer layers to generate compared to those on the right-hand side. This type of adaptive allocation appears to resonate with human intuition. For instance, in language tasks, humans typically exert more effort in summarization to condense the entire context into concise form. In contrast, extractive question answering often relies on retrieval and extraction, making it easier to identify the answer. Likewise, in 2 Figure 2: The FlexiDepth layer. Left: Full-processing path where hidden states undergo the pretrained attention and FFN modules. Right: Skipping path where hidden states bypass the attention module and processed by lightweight adapter. The router and adaptor (in red) are the only trainable components within the FlexiDepth Block. mathematical tasks, deriving the right-hand side of an equation involves computation that consumes more steps. To inspire further research along this direction, we compile dataset documenting the layer allocation behaviors of FlexiDepth across various tasks."
        },
        {
            "title": "2 Method",
            "content": "In this section, we present the detailed implementation of FlexiDepth, which consists of multiple FlexiDepth layers. Each FlexiDepth layer enhances standard pre-trained decoder layer with plug-in router and adapter, allowing adaptive layer-skipping without modifying the original parameters. Different from Coda [21] and COLT5 [1] that adopt adapters for conditional computation in transformer encoders, our method focuses on seamless integration with decoder-only models. As illustrated in Figure 2, in each FlexiDepth layer, the router first processes normalized input hidden states to make skipping decisions. Based on the routers output, each hidden state either follows the full-processing path (Figure 2, left) or the skipping path (Figure 2, right). The outputs from both paths are then combined to produce the layer output. In the following parts, we elaborate on the core designs of FlexiDepth, including router design, attention skipping, FFN skipping, and layer-skipping loss. 2.1 Router design FlexiDepth employs router to compute gating scores for routing. Let = [x1, x2, . . . , xT ] RT denote the input hidden states, where is the sequence length and is the hidden dimension. The router computes their corresponding gating scores = [g1, g2, . . . , gT ] RT as follows: = σ(Router(Norm(X))), (1) where Norm refers to the RMSNorm [42], and σ is the sigmoid function to ensure gi (0, 1). In previous works [31, 36], the router is typically implemented as simple linear transformation, and jointly optimized with the transformer during training. However, in our setting, the original parameters of the pre-trained transformer are frozen. Merely relying on linear transformation is insufficient to capture the nuances of the hidden states for routing. To address this issue, we design parameter-efficient router based on bottlenecked MLP: Router(z) = Wr (W Norm(tanh(Wz))), (2) 3 Figure 3: Comparison of attention masks. Left: the vanilla attention mask. Middle: the attention mask without KV Cache, where hidden states x1 and x3 skip this layer. Right: the attention mask with KV Cache, where despite x1 and x3 skipping this layer, their corresponding keys and values are still computed. where Rdrd, Rddr , and Wr R1d represent the weights of down-projection, up-projection, and router head, respectively. The dr denotes the bottleneck dimension. Upon obtaining the gating scores G, we implement threshold-based routing mechanism with predefined threshold τ . For each hidden state xi, if gi > τ , it will be routed to the full-processing path (Figure 2, left), with the output multiplied by gi to maintain gradient flow to the router. Conversely, if gi τ , it will be routed to the skipping path (Figure 2, right), with the output multiplied by (1 gi). 2.2 Attention skipping In the skipping path of FlexiDepth layer (Figure 2 right), the input hidden states simply bypass the attention module via shortcut. WhileHowever, this naive approach reduces computational overhead, it risks losing critical contextual informleads to significant loss of contextual information, resulting in performance degradation. The attention mechanism depends onleverages query, key, and value vectors to enable tokens to attend to one another,each other, thereby capturing long-range dependencies essential for generation. When aFor hidden states that skips the attention module entirely, its, their corresponding key and value vectors are.. not generated generated, preventing subsequent tokens from accessing its informationttending to them. We show an example in Figure 3 to describe this. As illustrated in Figure 3 (left), in the vanilla attention mask, every token can attend to all preceding tokens. For instance, query vector q4 attends to keys k0 through k4. However, when employing FlexiDepth without maintaining the key-value cache (KV-cache), skipping hidden states such as x1 and x3 results in missing keys k1 and k3, as depicted in the attention mask labeled No KV Cache (Figure 3, middle). Consequently, future tokens like q4 lose access to the contextual information stored in these hidden states. To address this issue, we propose simple solution: we still compute the KV-cache for hidden states that skip the attention module. Specifically, we still bypass the computation of query vectors (e.g., q1 and q3) and their associated scaled dot-product operations for the skipped states x1 and x3. However, we retain the calculation of their corresponding keys k1, k3 and values v1, v3, as demonstrated in the attention mask labeled KV Cache (Figure 3, right). This ensures that future generation maintains access to the entire context, preserving the integrity of autoregressive generation. 2.3 FFN skipping We observe that directly skipping the feedforward network (FFN) via simple shortcut leads to significantly degrade model performance. Unlike the attention module, which only consists of linear q, k, transformations, the FFN module introduces nonlinearities. Consequently, hidden states transformed by the FFN might not share the same latent space as those skipping it. To mitigate this issue, we employ lightweight adapter to align their representations. This adapter follows the same structure as the FFN but features significantly reduced intermediate dimension, with reduction 4 factor of 16x. As shown in Figure 2 right, the adapter is placed at the same position as the skipped FFN. 2.4 Layer-skipping loss To balance computational efficiency and generation quality, we introduce skipping loss that is jointly optimized with the next-token prediction loss. This loss is designed to penalize the squared sum of the layers used. The squared loss imposes larger penalty on tokens that activate more layers and smaller penalty on those that use fewer. This non-uniform penalty stabilizes training by preventing the model from falling into extreme patterns, such as skipping all layers or none. The loss function is formulated as: Lskip ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) (cid:32) (cid:88) (cid:33)2 gl , t= l=1 (3) where gl Lskip with the original language modeling loss Llm, weighted by factor α: denotes the gating score for layer at time step t. The final loss integrates the skipping loss = α Lskip + Llm. (4)"
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Implementation details We implement FlexiDepth on the pre-trained Llama-3-8B-Instruct model [8], which consists of 32 transformer layers. To enable adaptive layer-skipping, we convert the latter 16 layers into FlexiDepth layers, as prior works [26, 35] and empirical studies suggest that skipping earlier layers degrades performance. For each FlexiDepth layer, the router, as defined in Equation 3, uses bottleneck dimension dr = 1 16 d, where is the hidden dimension. The routers gating function is implemented using SparseMixer [23, 24] to ensure differentiability. The adapter has the same structure as the original FFN but reduces the intermediate dimension by factor of 16. For the loss function in Equation 4, we set the coefficient α = 1 103, which results in approximately 8 layers skipped during generation. We train FlexiDepth on the Tulu-v2 dataset [17] for 3 epochs using the AdamW [25] optimizer with learning rate of 1 104, β1 = 0.9, β2 = 0.999, and ϵ = 1 108. We use warmup ratio of 0.03 and global batch size of 64. The training takes approximately 7 hours on 8 NVIDIA A100-PCIE-40GB GPUs. 3.2 Main results In this part, we present the experimental results of applying the proposed FlexiDepth to the pre-trained Llama-3-8B-Instruct model [8]. Benchmarks. We evaluate FlexiDepth across diverse set of tasks. For single-token generation benchmarks, we include: MMLU [14], HellaSwag [40], and Winogrande [33]. For multi-token generation tasks, we test on: GSM8K [5], HumanEval [4], CoQA [32]. Evaluations are conducted using the lm-evaluation-harness toolkit [10], with 5-shot settings for MMLU, HellaSwag, Winogrande, and GSM8K, and zero-shot settings for HumanEval and CoQA. The evaluation metrics are accuracy (acc) for MMLU, normalized accuracy (acc_norm) for HellaSwag, accuracy (acc) for Winogrande, exact-match (EM) for GSM8K, pass-at-1 (Pass@1) for HumanEval, and F1 score (F1) for CoQA. Baselines. We compared FlexiDepth with previous layer-skipping methods that are compatible with LLMs. All baseline methods are applied to the LLama-3-8B-Instruct model, which consists of 32 transformer layers. We define as the number of layers to skip during generation, with all methods configured to skip the same number of layers for fair comparison. The baseline methods are as follows: (1) LayerSkip [9]: This early-exit method skips the last consecutive layers during decoding and employs speculative decoding [22] to refine generation results. For comparison, we disable 5 Methods Single-Token Generation Multi-Token Generation Retain % MMLU Hellaswag Winogrande GSM8K HumanEval CoQA Vanilla 0.673 0.706 0.744 0. 0.299 0.784 100.0% LayerSkip ShortGPT LaCo MindSkip Ours LayerSkip ShortGPT LaCo MindSkip Ours 0.659 0.664 0.671 0.664 0. 0.650 0.307 0.656 0.602 0.616 0.636 0.662 0.693 0.698 0.724 0.525 0.462 0.628 0.650 0.705 Skip 4 Layers 0.676 0.700 0.724 0.722 0.756 0.004 0.536 0.581 0.378 0. Skip 8 Layers 0.640 0.597 0.695 0.646 0.735 0.0 0.001 0.065 0.039 0.662 0.0 0.092 0.031 0.189 0.390 0.0 0.0 0.006 0.024 0.341 0.350 0.145 0.778 0.720 0. 0.049 0.005 0.707 0.620 0.801 54.0% 69.1% 81.7% 84.2% 106.5% 43.9% 32.0% 65.3% 60.2% 100.7% Table 1: Performance comparison based on Llama-3-8B-Instruct, which consists of 32 layers. Retain % represents the percentage of average retained benchmark performance. speculative decoding. (2) ShortGPT [26]: This approach prunes layers deemed less important by assessing their input-output difference. (3) LaCo [39]: This method employs layer-collapse strategy to reduce the model by layers, merging subsequent layers into prior one using ReservingDifferences-while-Seeking-Common (RDSC) approach. Please read the original paper for the details. (4) MindSkip [12]: This method explores attention, FFN, and layer skipping via simple linear router, finding that skipping FFN or entire layers leads to significant performance degradation. For comparison, we employ its layer skipping setting and train it on the same tulu-v2 [17] dataset for comparison. FlexiDepth demonstrates varied number of skipped layers for different tasks. To compare fairly with baselines at consistent number of skipped layers k, we trained multiple FlexiDepth models with varying penalty weights α and evaluated their performance. For each task, we report the results of the model that achieves the target number of skipped layers k. Later, we will highlight FlexiDepths adaptability by showing its performance across these tasks with single, fixed α. Table 1 presents the comparison results. FlexiDepth consistently outperforms baseline methods, especially in multi-token generation tasks. For example, when skipping 8 layers, baseline methods retain reasonable performance on single-token generation benchmarks and the short-form questionanswering task CoQA. However, they suffer from near-zero accuracy on GSM8K and HumanEval that requires longer reasoning. In contrast, FlexiDepth excels across both single-token and multi-token generation tasks, achieving an average performance retention of 100.7%. Interestingly, FlexiDepth even slightly surpasses the original models performance on certain benchmarks, highlighting the potential benefits of adaptive depth. To further examine whether this improvement stems from training data or the mechanism itself, we compare FlexiDepth with the fully fine-tuned allenai/llama-3-tulu-2-8b model on huggingface, which is trained on the same dataset as FlexiDepth. Notably, this model achieves 0.554 on GSM8K and 0.354 on HumanEval, while FlexiDepth achieves 0.695 and 0.390 on the same taskseven when skipping 4 layers. This result suggests that the performance gains are not solely attributable to the training data. We hypothesize that its adaptive layer-skipping mechanism may implicitly act as form of regularization, by skipping less informative or noisy parameters during inference, thus enhancing generalization. Future work is needed to explore this possibility and investigate the broader potential of adaptive layer-skipping. 3.3 FlexiDepth on different language models We evaluate FlexiDepth on language models of varying sizes. These models are trained under the identical configuration and dataset described in our implementation details. All of the models are instruction-tuned models. Larger models, specifically Llama-2-13B-Instruct and Llama-3-8BInstruct, exhibit greater number of layers skipped compared to the smaller Qwen-2.5-3B-Instruct [38]. 6 Model MMLU Hellaswag Winogrande GSM8K HumanEval CoQA Retain % Llama-2-13B FlexiDepth Skipped Llama-3-8B FlexiDepth Skipped Qwen-2.5-3B FlexiDepth Skipped 0.492 0.481 7.08 0.673 0.663 4. 0.651 0.643 1.23 0.727 0.741 1.98 0.706 0.743 2.00 0.702 0.701 1.25 0.714 0.716 5.00 0.744 0.756 3. 0.639 0.679 1.15 0.236 0.253 6.65 0.679 0.657 10.42 0.576 0.583 1.71 0.102 0.096 11.61 0.299 0.323 9. 0.487 0.476 1.69 0.790 0.780 6.19 0.784 0.803 7.44 0.705 0.741 1.88 100.0% 100.2% - 100.0% 102.1% - 100.0% 101.5% - Table 2: Performance and skipping patterns of FlexiDepth on instruction-tuned models. Each model includes baseline performance (first row), FlexiDepths performance (second row), and average skipped layers (third row). The -Instruct are omitted for simplicity. Concretely, the Llama-2-13B-Instruct and Llama-3-8B-Instruct skip approximately 7 and 6 layers respectively across various tasks, whereas the Qwen-2.5-3B-Instruct skips only about 1 to 2 layers. This pattern suggests that larger models inherently possess higher redundancy, thus allowing more aggressive layer skipping without significant performance deterioration. These observations highlight the substantial potential of adaptive layer skipping methods when applied to larger language models. 3.4 Layer allocation dataset To investigate these layer-skipping patterns and their relationship to task complexity, we constructed dataset using FlexiDepth with Llama-3-8B-Instruct as the base model. This dataset captures layer usage for token generation in two key domains: language comprehension and math reasoning. By recording the number of layers utilized per token, we aim to uncover how FlexiDepth dynamically adjusts its depth to meet varying task demands. Text Generation. For text generation tasks, we constructed dataset by randomly sampling 100 paragraphs from the XSum test set [27], which is collection of news articles. We evaluated FlexiDepth on three subtasks for each paragraph: copying, summarization, and continuation: Please copy this paragraph: <paragraph>...</paragraph> Directly output the copied paragraph here: ------------------------------------------------------------------------------- Please summarize this paragraph into single sentence: <paragraph>...</paragraph> Directly output the summarized paragraph here: ------------------------------------------------------------------------------- Please continue writing this paragraph: <paragraph>...</paragraph> Directly output the continued paragraph here: We recorded the number of layers used per token across all outputs: copying averaged 21.95 layers (variance 6.05), summarization averaged 28.65 layers (variance 18.31), and continuation averaged 30.27 layers (variance 12.59). These results reveal clear pattern: tasks requiring deeper contextual understanding, such as continuation and summarization, utilize more layers compared to the simpler copying task. The higher variance in summarization (18.31) and continuation (12.59) versus copying (6.05) suggests greater variability in layer allocation, likely reflecting the diverse cognitive demands of generating concise summaries or creative continuations. Detailed examples of these patterns are presented in Appendix A. Math Reasoning. For math reasoning, we created dataset of 100 samples, each featuring randomly generated list of 510 integers between 10 and 99. This controlled setup enables us to test basic arithmetic operations systematically. FlexiDepth was prompted to process each list with three subtasks: repeating the list, calculating the sum, and the product. 7 MMLU Hellaswag Winogrande GSM8K HumanEval CoQA Retain % Vanilla FlexiDepth Linear Router No KV Cache No Adapter 0.673 0.663 0.619 0.525 0.226 0.706 0.743 0.684 0.699 0.356 0. 0.756 0.701 0.745 0.579 0.679 0.657 0.131 0.366 0.004 0.299 0.323 0.055 0.226 0.0 0. 0.803 0.716 0.775 0.047 100.0% 102.1% 68.7% 84.3% 28.1% Table 3: Ablation studies based on Llama-3-8B-Instruct. Please repeat the following list for 5 times: [...] ------------------------------------------------------------------------------- Please calculate the sum of numbers in the following list: [...] ------------------------------------------------------------------------------- Please calculate the product of numbers in the following list: [...] The layer usage pattern across these three tasks was as follows: repeating averaged 20.09 layers (variance 8.08), addition averaged 22.45 layers (variance 14.67), and multiplication averaged 23.90 layers (variance 21.61). These findings align with task complexityrepetition, straightforward memory task, uses the fewest layers, while multiplication, requiring iterative computation, demands the most. The higher variance in multiplication (21.61) compared to addition (14.67) and repetition (8.08) indicates that computational tokens exhibit more diverse layer needs, likely due to the increasing complexity of intermediate steps. Appendix provides additional results, showing that tokens tied to arithmetic operations consistently require more layers than directly copying numbers from the context. 3.5 Ablation Studies We show our ablation studies in this part. All experiments in this section utilize the same layerskipping penalty, dataset, and training parameters as the original FlexiDepth implementation on the pre-trained Llama-3-8B-Instruct. Linear Router. In FlexiDepth, we employed bottlenecked MLP as the router to facilitate nuanced routing decisions. When replacing this component with simpler linear layer followed by sigmoid activation, as previously used in mixture-of-depth (MoD) [31], we observe substantial decrease in performance, particularly on the math reasoning task. Our analysis shows that the linear router mistakenly assigns too few layers to tokens requiring complex computations, significantly reducing the GSM8K performance from 0.657 to 0.131. Overall, the linear router retains only 78.6% of the original performance, emphasizing the importance of the bottlenecked MLP for routing. No KV Cache. We conducted an ablation study in which the KV cache is not computed for tokens routed to skip layers (illustrated as \"No KV Cache\" in Figure 3). Removing the KV cache results in substantial performance decline, retaining only 87.4% of the baseline performance. This highlights the need to retain contextual information during generation. No adapter. Many layer skipping methods do not have adapters. We therefore investigate the performance of FlexiDepth without adapter. This simplification drastically reduces model performance to merely 32.2% of the original FlexiDepth results. The adapter proves critical in aligning latent representations between skipped and processed states, ensuring representation coherence across the models layers."
        },
        {
            "title": "4 Limitations",
            "content": "Although FlexiDepth reduces FLOPs by adaptively skipping transformer layers, our implementation does not lead to improved throughput on the existing GPU hardware. In FlexiDepth, samples within the same batch can take different execution paths during decoding. Consequently, each FlexiDepth layer must handle tokens that undergo full processing alongside those that skip layers, requiring simultaneous management of both computation and skip branches. This introduces overhead from 8 control-flow management and irregular memory accesses, outweighing the speedup of theoretical FLOP reductions. Future work could investigate hardware-oriented optimizations to address these bottlenecks, leveraging techniques such as token grouping [30], expert sharding [30], and load balancing [15] to better align with GPU architectures. Such hardware-tailored enhancements could further unlock the efficiency potential of FlexiDepth in practice."
        },
        {
            "title": "5 Related works",
            "content": "The existing language models can be broadly classified into three categories: encoder-only models (e.g., BERT [7]), encoder-decoder models (e.g., T5 [29]), and decoder-only models (e.g., GPT [3]). Across these architectures, various conditional computation methods have been proposed to optimize resource allocation by dynamically processing tokens. In encoder-only models, conditional token processing has been explored to prune less critical tokens during processing. For instance, PoWER-BERT [11] and LTP [19] utilize attention scores to assess token importance, dynamically eliminating those deemed less significant. Similarly, LoT [20] introduces router at each layer to determine whether tokens should be processed or skipped, tailoring computation to token relevance. These approaches leverage the bidirectional nature of encoders, allowing noncausal routing decisions based on full sequence context. For encoder-decoder models, methods such as CoDA [21] and COLT5 [1] enable conditional computation within the encoder component. By replacing standard attention or feed-forward network (FFN) modules with lightweight alternatives for certain tokens, these techniques assign varying computational resources based on token complexity. However, their noncausal design constrains their applicability to decoder. In decoder-only models, the autoregressive nature poses unique challenges for dynamic token processing, as tokens must be generated sequentially while preserving contextual dependencies. Piorneering efforts like MoD [31] and SkipLayer [41] address this by deploying router at each Transformer layer, enabling tokens to dynamically bypass it. These models are trained end-to-end from scratch, integrating the routing mechanism into the learning process. Duo-LLM [2] also employs router for conditional computation, but with distinct method. It first constructs dataset of oracle routing strategies via exhaustive enumeration and then trains the router on this data. While effective, these methods demand significant computational resources for training and dataset preparation, making them impractical for adapting pre-trained models. MindSkip [12] enables skipping in pretrained models via learnable routers. It finds that bypassing FFN modules or entire layers significantly degrades performance and instead proposes method to skip the attention module efficiently. Unlike these approaches, our method, FlexiDepth, enables adaptive layer-skipping in pre-trained models without compromising generation performance by using plug-in router and adapter architecture."
        },
        {
            "title": "6 Conclusion",
            "content": "We present FlexiDepth, an approach to dynamically skip layers in pre-trained large language models. It achieves state-of-the-art performance without altering the original model parameters. FlexiDepth offers valuable insights into the varying computational demands of token generation. We constructed dataset capturing these allocation patterns to encourage further study of computational needs of tokens, thereby advancing the development of more adaptive and resource-efficient language models."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontañón, Siddhartha Brahma, Yury Zemlyanskiy, David C. Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. Colt5: Faster long-range transformers with conditional computation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, pages 50855100, 2023. [2] Keivan Alizadeh, Iman Mirzadeh, Hooman Shahrokhi, Dmitry Belenko, Frank Sun, Minsik Cho, Mohammad Hossein Sekhavat, Moin Nabi, and Mehrdad Farajtabar. Duo-llm: framework for studying adaptive computation in large language models. CoRR, 2024. 9 [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020. [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and others. Evaluating large language models trained on code. CoRR, 2021. [5] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, 2021. [6] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, and others. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, 2025. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: NAACL-HLT 2019, pages 41714186, 2019. [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and others. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. [9] Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu. Layerskip: Enabling early exit inference and self-speculative decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024. [10] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 2024. [11] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan T. Chakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert: Accelerating BERT inference via progressive word-vector elimination. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, pages 36903699, 2020. [12] Shwai He, Tao Ge, Guoheng Sun, Bowei Tian, Xiaoyang Wang, Ang Li, and Dong Yu. Routertuning: simple and effective approach for enabling dynamic-depth in transformers. CoRR, 2024. [13] Shwai He, Guoheng Sun, Zheyu Shen, and Ang Li. What matters in transformers? not all attention is needed. CoRR, 2024. [14] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. [15] Haiyang Huang, Newsha Ardalani, Anna Y. Sun, Liu Ke, Shruti Bhosale, Hsien-Hsin S. Lee, Carole-Jean Wu, and Benjamin Lee. Toward efficient inference for mixture of experts. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, 2024. [16] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. CoRR, 2024. 10 [17] Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels in changing climate: Enhancing lm adaptation with tulu 2. CoRR, 2023. [18] Metod Jazbec, Alexander Timans, Tin Hadzi Veljkovic, Kaspar Sakmann, Dan Zhang, Christian Andersson Naesseth, and Eric T. Nalisnick. Fast yet safe: Early-exiting with risk control. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [19] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In KDD 22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022, pages 784794, 2022. [20] Yeachan Kim, Junho Kim, Jun-Hyung Park, Mingyu Lee, and SangKeun Lee. Leap-of-thought: Accelerating transformers via dynamic token routing. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1575715769, 2023. [21] Tao Lei, Junwen Bai, Siddhartha Brahma, Joshua Ainslie, Kenton Lee, Yanqi Zhou, Nan Du, Vincent Y. Zhao, Yuexin Wu, Bo Li, Yu Zhang, and Ming-Wei Chang. Conditional adapters: Parameter-efficient transfer learning with fast inference. In Advances in Neural Information Processing Systems 36: NeurIPS 2023, 2023. [22] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, Honolulu, Hawaii, USA, pages 1927419286, 2023. [23] Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, and Jianfeng Gao. Bridging discrete and backpropagation: Straight-through and beyond. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [24] Liyuan Liu, Jianfeng Gao, and Weizhu Chen. Sparse backpropagation for moe training. CoRR, 2023. [25] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, 2019. [26] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. CoRR, 2024. [27] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 17971807, 2018. [28] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. In Proceedings of the Sixth Conference on Machine Learning and Systems, MLSys 2023, Miami, FL, USA, June 4-8, 2023, 2023. [29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., pages 140:1140:67, 2020. [30] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-ofexperts inference and training to power next-generation AI scale. In International Conference on Machine Learning, ICML 2022, Baltimore, Maryland, USA, pages 1833218346, 2022. 11 [31] David Raposo, Samuel Ritter, Blake A. Richards, Timothy P. Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. CoRR, 2024. [32] Siva Reddy, Danqi Chen, and Christopher D. Manning. Coqa: conversational question answering challenge. Trans. Assoc. Comput. Linguistics, pages 249266, 2019. [33] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, New York, NY, USA, February 7-12, 2020, pages 87328740, 2020. [34] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [35] Qi Sun, Marc Pickett, Aakash Kumar Nain, and Llion Jones. Transformer layers as painters. CoRR, 2024. [36] Zhen Tan, Daize Dong, Xinyu Zhao, Jie Peng, Yu Cheng, and Tianlong Chen. DLO: dynamic layer operation for efficient vertical scaling of llms. CoRR, 2024. [37] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Investigating acceleration of llama inference by enabling intermediate layer decoding via instruction tuning with lite. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, 2024. [38] An Yang, Baosong Yang, Beichen Zhang, and others. Qwen2.5 technical report. CoRR, 2024. [39] Yifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer collapse. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 64016417, 2024. [40] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pages 47914800, 2019. [41] Dewen Zeng, Nan Du, Tao Wang, Yuanzhong Xu, Tao Lei, Zhifeng Chen, and Claire Cui. Learning to skip for language modeling. CoRR, 2023. [42] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019. [43] Yang Zhang, Yawei Li, Xinpeng Wang, Qianli Shen, Barbara Plank, Bernd Bischl, Mina Rezaei, and Kenji Kawaguchi. Finercut: Finer-grained interpretable layer pruning for large language models. CoRR, 2024. [44] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 27652781, 2024."
        },
        {
            "title": "A Layer allocation Examples",
            "content": "In this section, we present examples to illustrate the layer allocation patterns of FlexiDepth, using the pre-trained Llama-3-8B-Instruct model [8] as the base. A.1 Language Task Copy: Here is an example where FlexiDepth uses 21.84 layers in average per token: Please copy this paragraph: <paragraph>Bissouma penned professional deal with the northern club only in July 2016, four months after joining the reserves from AS Real Bamako. The 20-year-old, whose previous deal ran until June 2019, says he is very happy at Stade Pierre-Mauroy. \"In football, when you put in the work you always get rewarded,\" Bissouma told the clubs website. \"I still have lot of progress to make. want to give myself every chance of doing that and fulfilling my objectives. \"The first is to keep the club in Ligue 1, and then would also like to win trophy with Lille.\" Bissouma has scored once in 17 appearances for the club since making his debut against Toulouse in September 2016.</paragraph> Directly output the copied paragraph here: Bissouma penned professional deal with the northern club only in July 2016, four months after joining the reserves from AS Real Bamako.nThe 20yearold, whose previous deal ran until June 2019, says he is very happy at Stade PierreMauroy.n \"In football, when you put in the work you always get rewarded,\" Bissouma told the clubs website.n\"I still have lot of progress to make. want to give myself every chance of doing that and fulfilling my objectives.n\"The first is to keep the club in igue 1, and then would also like to win trophy with Lille.\"nBissouma has scored one in 17 appearances for the club since making his debut against Toulouse in September 2016.<eot_id> This task requires minimal contextual understanding, as it involves direct reproduction, which explains the relatively low layer usage. The first token requires much more layers to generate because the generation of the first token involves the prefilling stage, where the whole context is been processed. Summarization: Here is an example using 29.15 layers in average per token: Please summarize this paragraph into single sentence: <paragraph>Bissouma penned professional deal with the northern club only in July 2016, four months after joining the reserves from AS Real Bamako. The 20-year-old, whose previous deal ran until June 2019, says he is very happy at Stade Pierre-Mauroy. \"In football, when you put in the work you always get rewarded,\" Bissouma told the clubs website. \"I still have lot of progress to make. want to give myself every chance of doing that and fulfilling my objectives. \"The first is to keep the club in Ligue 1, and then would also like to win trophy with Lille.\" Bissouma has scored once in 17 appearances for the club since making his debut against Toulouse in September 2016.</paragraph> Directly output the summarized paragraph here: Bissouma, who signed professional deal with Lille in July 2016 and has scored once in 17 appearances, is happy at the club and aims to keep them in Ligue 1 and win trophy.<eot_id> Summarization involves understanding and condensing the input into concise form, requiring deeper contextual abstraction and rephrasing, which leads to higher layer usage of 29.15 compared to copying. We observe that tokens forming predictable collocations, such as \"souma\" following \"Bis\" in \"Bissouma\" or \"trophy\" after \"win a\" in \"win trophy,\" require fewer layers due to their low uncertainty after the initial tokens are generated. Continuation: Here is an example that utilizes 31.14 layers in average: Please continue writing this paragraph: <paragraph>Bissouma penned professional deal with the northern club only in July 2016, four months after joining the reserves from AS Real Bamako. The 20-year-old, whose previous deal ran until June 2019, says he is very happy at Stade Pierre-Mauroy. \"In football, when you put in the work you always get rewarded,\" Bissouma told the clubs website. \"I still have lot of progress to make. want to give myself every chance of doing that and fulfilling my objectives. \"The first is to keep the club in Ligue 1, and then would also like to win trophy 13 with Lille.\" Bissouma has scored once in 17 appearances for the club since making his debut against Toulouse in September 2016.</paragraph> Directly output the continued paragraph here: Despite his young age, Bissouma has already shown impressive maturity in his performances, with his manager, Marcel Blondeau, praising him for his ability to adapt to the teams system. \"He is very intelligent player,\" Blondeau said. \"He can play in different positions and has good understanding of the game. He is also very strong in the tackle and can win the ball back.\" With his contract running until 2021, Bissouma is expected to continue playing crucial role in Lille midfield for the coming seasons.<eot_id> Continuation demands creative generation and the maintenance of contextual coherence, necessitating long-range dependencies and novel content creation, which results in higher layer usage of 31.14. The generation process typically engages more layers, except for predictable tokens like names (\"Bissouma\" and \"Blondeau\"), which require fewer layers due to their memorized nature and low uncertainty. A.2 Math Task Repeating: Here is an example that uses 20.15 layers in average: Please repeat the following list for 5 times: [63, 15, 43, 75, 72, 61, 48, 71] Here is the list repeated 5 times [63, 15, 43, 75, 72, 61, 48, 71] [63, 15, 43, 75, 72, 61, 48, 71] [63, 15, 43, 75, 72, 61, 48, 71] [63, 15, 43, 75, 72, 61, 48, 71] [63, 15, 43, 75, 72, 61, 48, 71]<eot_id> Repeating is memory-based task requiring minimal computation, as it involves recalling and reproducing the input list, which explains the low layer usage. This example fits within the reported variance (8.08), reflecting the tasks consistent layer needs. Addition: Here is an example that uses 22.31 layers in average: Please calculate the sum of numbers in the following list: [63, 15, 43, 75, 72, 61, 48, 71] Lets calculate the sum of the numbers in the list 63 + 15 = 78 + 43 = 121 121 + 75 = 196 196 + 72 = 268 268 + 61 = 329 329 + 48 = 377 377 + 71 = The sum of the numbers in the list is 448<eot_id> Addition requires iterative computation, using more layers than repeating since FlexiDepth calculates each intermediate sum. The right-hand side of each equation (e.g., \"78\", \"121\") demands more layers due to the transformers internal computation, while the left-hand side (e.g., \"63 + 15\") generally uses fewer layers as it copies numbers from the input. We also find that the initial tokens on the left-hand side, such as \"63\" in \"63 + 15\", use slightly more layers than the rest of the left-hand side tokens. We hypothesize that this is to determine which intermediate result to copy from the context. Production: Here is an example that uses 24.51 layers in average: 14 [63, 15, 43, 75, 72, Please calculate the product of numbers in the following list: 61, 48, 71] To calculate the product of the numbers in the given list, Ill multiply each number individually Heres the stepbystep calculation 1. 63 15 = 945 2. 945 43 = 40515 3. 40515 75 = 3026125 4. 3026125 72 = 218171500 5. 218171500 61 = 13311439500 6. 13311439500 48 = 7. 637385920000 71 = 45213914128000 The final product of the numbers in the provided list is: 45213914128000<eot_id> Multiplication involves complex iterative computations, as FlexiDepth handles large intermediate results and maintains numerical precision, resulting in higher layer usage. The right-hand side of each equation (e.g., \"945\", \"40515\") consistently uses the full number of layers to compute the product, reflecting the tasks high computational demand. Similar to addition, on the left-hand side, initial tokens like \"63 15\" in the first step may use slightly more layers to identify the intermediate result to copy from the context. Subsequent tokens (e.g., \"945 43\", \"40515 75\") exhibit gradually decreasing trend in layer usage, possibly due to increasing certainty when retrieving intermediate results as the computation progresses."
        }
    ],
    "affiliations": [
        "Department of Computer Science, UC Santa Barbara"
    ]
}