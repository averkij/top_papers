{
    "paper_title": "ToolRL: Reward is All Tool Learning Needs",
    "authors": [
        "Cheng Qian",
        "Emre Can Acikgoz",
        "Qi He",
        "Hongru Wang",
        "Xiusi Chen",
        "Dilek Hakkani-Tür",
        "Gokhan Tur",
        "Heng Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research."
        },
        {
            "title": "Start",
            "content": "ToolRL: Reward is All Tool Learning Needs Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, Heng Ji University of Illinois Urbana-Champaign {chengq9, hengji}@illinois.edu 5 2 0 2 6 1 ] . [ 1 8 5 9 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving 17% improvement over base models and 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the code are released to facilitate future research."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in Large Language Models (LLMs) have showcased remarkable capabilities in complex reasoning tasks (Kumar et al., 2025). Among the techniques that have significantly contributed to this progress, Reinforcement Learning (RL) has emerged as powerful paradigm, enabling 1 Data and codes released at https://github.com/ qiancheng0/ToolRL Figure 1: SFT on distilled deep-thinking trajectories suffers from overthinking and limited generalization. LLMs to develop emergent capabilities such as selfreflection, self-correction, and long-horizon planning (Guo et al., 2025; Team et al., 2025). These capabilities have been instrumental in the success of models like o1 and R1, particularly in mathematical and logical reasoning domains (Qin et al., 2024a; Huang et al., 2024; Li et al., 2025b; Kang et al., 2025). Beyond traditional reasoning tasks, an increasingly important area is Tool-Integrated Reasoning (TIR). TIR involves LLMs interacting with external tools, such as search engines (Jin et al., 2025; Zheng et al., 2025), calculators (Chen et al., 2023b; Qin et al., 2023), or code interpreters (Gou et al., 2023; Liao et al., 2024), in multi-step, feedbackdriven loop to arrive at solutions. TIR is particularly important because it addresses core limitations of LLMs, such as outdated knowledge, calculation inaccuracy, and shallow reasoning. By integrating external tools that offer real-time access and specialized capabilities, TIR enables models to tackle complex tasks in more grounded and goal-directed way. Unlike textual reasoning, which primarily involves deduction and inference from static text, Figure 2: Main results (left) and reward trends over training steps for GRPO Cold Start across four models (right). GRPO Cold Start, equipped with our proposed reward design, consistently achieves the highest performance, with reward curves showing rapid increase during training. TIR additionally demands the models ability to select appropriate tools, interpret intermediate outputs, and adaptively refine its trajectory on the fly. These dynamic and interactive reasoning skills position TIR at the core of the emerging paradigm of LLMs-as-agents. As such, TIR enables wide range of applications, including scientific discovery (Roohani et al., 2024; Inoue et al., 2024), research automation (Baek et al., 2024; Wang et al., 2024), embodied task completion (Zhang et al., 2023; Huang et al., 2023), and everyday decisionmaking (Ye et al., 2023; Zhai et al., 2024). Training LLMs for TIR tasks has predominantly relied on Supervised Fine-Tuning (SFT), wherein existing approaches typically generate these integrated reasoning steps offline, followed by subsequent SFT on these trajectories (Chen et al., 2023a; Zeng et al., 2024; Chen et al., 2024; Acikgoz et al., 2025). While SFT is effective to some extent, it struggles with generalization, exploration, and adaptability (Chu et al., 2025; Guo et al., 2025). As illustrated in Figure 1, model trained with SFT on deep-thinking trajectories over-interprets the tool and fails to reject the inappropriate tool, merely imitating cues like but wait without engaging in genuine deep thinking. As such, SFT often fails to capture the strategic flexibility needed for optimal tool use, particularly in open-ended or multi-step settings. This motivates fundamental research question: Can RL-based training methods better equip LLMs with agentic tool-using capabilities, and if so, what is the optimal RL design for TIR? Recent efforts such as Search-R1 (Jin et al., 2025) and TORL (Li et al., 2025b) have begun to explore this direction. However, their focus is narrow: either constrained to search tools in question answering settings or code tools in math problemsolving. In contrast, our work aims to study RLbased training for general-purpose tool selection and application, across diverse and complex tool sets with different task types. For an RL algorithm to be effective, welldesigned reward is essential. Unlike math tasks with single correct answer, Tool-Integrated Reasoning (TIR) tasks introduce multiple layers of complexity: they often involve multi-step interactions where each turn may require invoking multiple tools, each with carefully specified parameters. Designing effective reward signals to guide learning through this complexity remains an open and underexplored challenge. In this paper, we focus on the problem of reward design for TIR and propose principled, generalizable framework that can be applied across various RL algorithms. While our reward design is algorithm-agnostic by nature, we empirically demonstrate its effectiveness using both Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and Proximal Policy Optimization (PPO) (Schulman et al., 2017), showcasing its versatility and impact on improving tool use performance. We begin by formalizing the TIR task, and outlining general principles for effective reward design. Building on this foundation, we show how RL algorithm can be leveraged to train LLMs for robust and context-aware tool selection and application. Empirical results demonstrate that our approach outperforms base models by 17% and SFT models by 15% across multiple tool use and QA benchmarks. Moreover, the trained model exhibits strong generalization to unseen scenarios and task objectives, along with emergent behaviors such as proactiveness and metacognitive reasoning. To identify optimal reward strategies, we next systematically explore broad spectrum of reward configurations across four key dimensions: (1) reward type (what aspect to reward), (2) reward scale (how much to reward), (3) reward granularity (how detailed the reward signal is), and (4) reward dynamics (how rewards evolve over time). Through extensive experiments, we identify reward designs that best align with agentic tool use and uncover insights into what makes reward useful for tool invoking LLMs. We summarize the core insights we derive as follows: Longer reasoning trace is not inherently better and length rewards can degrade performance. Dynamic reward scale helps models transition smoothly from simple to complex behaviors. Finegrained reward decomposition leads to more stable and effective learning. We also summarize the overall contributions of our paper as follows: We present the first systematic study on RLbased training for general-purpose tool selection and application in LLMs. We propose principled reward design framework tailored for TIR and validate its effectiveness through RL algorithms including GRPO. We conduct extensive experiments analyzing the effects of various reward strategies and distill actionable insights for future research on LLMagent training. This work pioneers the application of RL to general TIR and provides the first empirical roadmap for reward design in TIR, paving the way toward more capable and autonomous LLM agents."
        },
        {
            "title": "2 Related Work",
            "content": "Tool-Integrated Reasoning of LLMs. Toolintegrated reasoning (TIR) has emerged as promising approach to enhance the capabilities of LLMs. Early studies introduced the concept of equipping LLMs with external tools to overcome their inherent limitations (Schick et al., 2023; Qin et al., 2024b; Yao et al., 2023), such as program executors (Chen et al., 2022) and search engines (Vu et al., 2023). To systematically assess these enhanced capabilities, several benchmarks have been proposed to evaluate tool use performance across various dimensions, including API selection, argument generation, and generalization (Qin et al., 2024c; Patil et al., 2023; Qian et al., 2024a). Building on this foundation, subsequent research has focused on constructing high-quality tool use datasets (Liu et al., 2024; Qian et al., 2025), enabling models to autonomously create and invoke tools (Qian et al., 2023, 2024b), and applying these techniques to problems spanning different modalities (Shen et al., 2025) and specialized domains (Ling et al., 2023). More recently, reinforcement learning (RL) has been explored as an effective framework to further improve TIR, demonstrating success in tasks such as information retrieval (Jin et al., 2025) and math computation (Li et al., 2025b). These advances collectively highlight the growing potential of tool-augmented LLMs for general-purpose reasoning in open-domain settings. Exploration of RL in LLMs. Previous work has primarily relied on supervised fine-tuning (SFT) with carefully curated datasets to enhance LLM performance in tool use (Schick et al., 2023; Qin et al., 2024c). Recently, reinforcement learning (RL) has gained traction as more scalable and generalizable training paradigm. The development of RL methods for LLMs has evolved from reinforcement learning from human feedback (RLHF) (Kaufmann et al., 2023) and proximal policy optimization (PPO) (Schulman et al., 2017) to more advanced techniques such as direct preference optimization (DPO) (Rafailov et al., 2023), SimPO (Meng et al., 2024), and group relative policy optimization (GRPO) (Shao et al., 2024). Extensions like dynamic sampling policy optimization (DAPO) (Yu et al., 2025) and the more recent value-based augmented proximal policy optimization (VAPO) (Yuan et al., 2025) further improve training stability and efficiency. Among these, GRPO (Shao et al., 2024) is specifically designed for LLMs, replacing the traditional critic with group-based evaluation strategy. It has shown strong performance in enhancing reasoning abilities across range of tasks, including mathematical problem solving (Shao et al., 2024; Xie et al., 2025), search engine interaction (Jin et al., 2025; Song et al., 2025), and code generation (Li et al., 2025b). Beyond task variety, recent studies have analyzed the influence of dataset scale (Li et al., 2025a) and GRPOs effectiveness in smaller model settings (Dang and Ngo, 2025). GRPOs flexible reward function enables adaptation to diverse objectives, such as assigning weights to subtasks (Yu et al., 2024) or constraining tool use frequency (Li et al., 2025b). In this work, we extend GRPO to enhance general tool use capabilities, improving LLMs ability to select and interact with external tools across wide range of scenarios."
        },
        {
            "title": "3 Method",
            "content": "Supervised fine-tuning (SFT), as illustrated in Figure 1, often suffers from overfitting to certain patterns and constrains the models ability to learn optimal strategies for tool use. To address this, we introduce reinforcement learning (RL) approach for enhancing tool-integrated reasoning (TIR) in LLMs. In this section, we begin by defining the TIR task (Section 3.1), followed by our customized rollout strategy (Section 3.2) and reward design (Section 3.3). These components are then integrated into the Group Relative Policy Optimization (GRPO) framework (Shao et al., 2024) to guide model training on general TIR tasks (Section 3.4). 3.1 Task Definition Tool-Integrated Reasoning (TIR) is the process of incorporating external tools into the reasoning trajectory of an LLM to solve user task. typical TIR trajectory involves multiple tool invocations over several reasoning steps, with the final outcome determined by the cumulative success of these intermediate decisions. Formally, given tool set = {t1, t2, . . . , tn} containing available tools, and user query Q, the reasoning trajectory up to step is denoted as: sk = (r1, T1, o1) , (r2, T2, o2) , . . . , (rk, Tk, ok) , where ri denotes the models natural language reasoning at step i, Ti denotes the set of tool calls invoked at step i, and oi denotes the observation received after executing tools in Ti, possibly including both environment and user feedback. At each step + 1, the model must generate the next reasoning step rk+1, select set of tools Tk+1 , and formulate grounded tool call (i.e., parameterized invocation of each tool) to make progress toward solving Q. The models policy is defined as π : sk (rk+1, Tk+1), where the models objective at each step is to select tool set Tk+1 that maximizes the immediate reward: k+1 = arg max Tk+1T R(sk, Tk+1, ok+1), where R() represents the reward function that evaluates progress made by invoking the tools in Tk+1. While the immediate reward at each step is maximized, the models policy is implicitly optimized to maximize the cumulative reward over the entire trajectory, formulated as: max π Eπ (cid:34) (cid:88) k=1 (cid:35) R(sk, Tk+1, ok+1) , This formulation is valid because our training data includes ground truth tool calls at each step, allowing step-wise reward signals to guide multi-step success. Unlike QA tasks that focus solely on the final answer, tool selection and application tasks provide dense intermediate feedback. Moreover, we later demonstrate that our method enables the model to generalize to settings where tool calls are free-form and only the final outcome matters. Therefore, out task setting encourages the model to optimize tool use at each step while aligning with the overall task goal. 3.2 TIR Rollout To enable the model to autonomously generate reasoning traces and tool calls, we utilize system prompt as shown in Figure 4 during rollout. The Tool List placeholder denotes the tool set , which contains all tools available for invocation. We indicate in the instruction that the LLM should use special tokens <think>, <tool_call>, and <response> to indicates their thoughts, tool calls and responses in output. As illustrated in Figure 3, when the model output includes <tool_call>, we automatically parse the tool calls into individual invocations using the model-predicted parameters. The outputs from executions are then inserted into the <obs> field and appended to the dialogue history, whose format is shown in Figure 12, serving as the models interaction trajectory. Similarly, if the output contains <response>, the corresponding response is parsed and appended to the dialogue history. Figure 3: Illustration of TIR rollout and calculation of format and correctness reward. It is important to note that <tool_call> and <response> are not mutually exclusive; they may cooccur within single output. The users initial query is placed in the Initial User Input placeholder, and any subsequent user inputs are also appended to the dialogue history when present. Tool Name Matching: rname = NG NP NG NP [0, 1] where NG and NP are the sets of tool names extracted from the ground-truth and predicted tool calls, respectively. 3.3 Reward Design Parameter Name Matching: Rule-based reward mechanisms have demonstrated strong empirical performance and are commonly employed. In our training, we similarly adopt reward formulation that combines structural and correctness-based components, in line with prior works (Jin et al., 2025; Li et al., 2025b; Xie et al., 2025). Specifically, the format reward assesses whether the model output adheres to the expected structure including thoughts, tool calls, and responses, while the correctness reward evaluates the accuracy of tool invocations. Formally, the overall reward Rfinal() is decomposed into two components: Rformat + Rcorrect, each described in detail below: Format Reward. The format reward Rformat {0, 1} checks whether the model output contains all required special tokens in the correct order as specified by the ground truth: Rformat = 1, if all required fields appear and are in the correct order 0, otherwise Correctness Reward. The correctness reward Rcorrect [3, 3] evaluates predicted tool calls = {P1, ..., Pm} against ground-truth calls = {G1, ..., Gn}. It includes three components: rparam = (cid:88) Gj keys(PG) keys(PP ) keys(PG) keys(PP ) [0, G] where keys(PG) and keys(PP ) represent the parameter names of the predicted and ground-truth tool calls, respectively. Parameter Content Matching: rvalue = (cid:88) (cid:88) 1[PG[k] = PP [k]] Gj kkeys(Gj ) (cid:88) [0, keys(Gj)] Gj where PG[k]] and PP [k] represent the values of the parameters for the predicted and ground truth tool calls. Total match score for each match is: rmatch = rname + rparam + rvalue [0, Smax] where Smax = 1 + + (cid:80) denotes the maximum possible score. Gj keys(Gj) The total score is computed by finding the optimal matching between and to maximize the total match score: Rcorrect = 6 Rmax Smax 3 [3, 3] where Rmax denotes the total match score from the optimal matching. The final correctness reward System Prompt for Training You are helpful dialogue assistant capable of leveraging tool calls to solve user tasks and provide structured chat responses. Available Tools In your response, you can use the following tools: {{Tool List}} Steps for Each Turn 1. Think: Recall relevant context and analyze the current user goal. 2. Decide on Tool Usage: If tool is needed, specify the tool and its parameters. 3. Respond Appropriately: If response is needed, generate one while maintaining consistency across user queries. Output Format <think> Your thoughts and reasoning </think> <tool_call> {name: Tool name, parameters: {Parameter name: Parameter content, ... ...: ... ...}} {name: ... ..., parameters: {... ...: ... ..., ... ...: ... ...}} ... </tool_call> <response> AIs final response </response> Important Notes 1. You must always include the <think> field to outline your reasoning. Provide at least one of <tool_call> or <response>. Decide whether to use <tool_call> (possibly multiple times), <response>, or both. 2. You can invoke multiple tool calls simultaneously in the <tool_call> fields. Each tool call should be JSON object with name field and parameters field containing dictionary of parameters. If no parameters are needed, leave the parameters field an empty dictionary. 3. Refer to the previous dialogue records in the history, including the users queries, previous <tool_call>, <response>, and any tool feedback noted as <obs> (if exists). Figure 4: The system prompt used for TIRs rollout. Rcorrect is the normalized reward for the matching process. We empirically set the reward scale within the range of [3, 3], with more analysis and ablatiions of reward scale presented in Section 5. The final reward value Rfinal is finally derived as the sum of Rformat and Rcorrect: Rfinal = Rformat + Rcorrect [3, 4] Unlike prior works that often rely on binary or overly simplified reward signals, our design captures the nuanced structure of tool calls by evaluating multiple interdependent components including tool names, parameter schemas, and parameter values. This fine-grained formulation better reflects the complexity of real-world tool use, where correctness cannot be reduced to single binary criterion. We further validate the impact of this design through comprehensive analysis in Section 5. Overall, our reward design ensures balanced and interpretable evaluation signal by explicitly separating structural compliance from semantic correctness. By aligning rewards with both format adherence and fine-grained tool call accuracy, the model is guided to produce outputs that are not only syntactically valid but also semantically faithful, which is crucial for downstream tool execution and final task success. 3.4 RL Training with GRPO To tune the model with structured rewards, we employ GRPO, variant of PPO that introduces advantage normalization within grouped samples. This normalization helps stabilize training by reducing variance across samples that share common input context. Let πθ represent the current policy. Normalized Advantage Across Query Groups. For each query Q, its responses derived from the rollout form group GQ consisting of multiple responses and their corresponding reward values: GQ = {A, (s1, r1), (s2, r2), . . . , (sn, rn)} where denotes the ground-truth annotation for Q, and each reward ri is computed as the sum of the format and correctness rewards associated with response si, i.e., ri = Rformat(si, A)+Rcorrect(si, A). For each group, we calculate the mean and standard deviation of the rewards: µQ = 1 n (cid:88) i=1 ri, σQ = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) (ri µQ)2 i=1 Then, for each sample si in the group, we define the normalized advantage: Ai(siQ) = ri µQ σQ + η where η is constant to avoid division by zero. Policy Optimization Objective. The policy πθ is optimized using the standard clipped PPO objective, adapted with our group-wise normalized advantages: JGRPO(θ) = EQDEsiπθ (cid:104) min (cid:16) πθ(siQ) πold(siQ) , 1 ϵ, 1 + ϵ(cid:1)Ai(siQ) Ai(siQ), (cid:17)(cid:105) clip(cid:0) πθ(siQ) πold(siQ) Unlike the original GRPO formulations, we omit the KL penalty term against reference model. This design choice encourages the model to more freely adapt its behavior to our custom response format and structured reward signals. In practice, we observe that this leads to faster convergence and comparable performance, while also simplifying the training pipeline. Overall, this objective guides the policy to generate structurally consistent and semantically accurate tool calls, while group-wise normalization mitigates reward variance across queries, leading to more stable and sample-efficient alignment with task-specific response requirements."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Training Dataset To support robust tool learning through RL, we construct mixed dataset spanning diverse tool use scenarios: ToolACE (Liu et al., 2024): general tool use dataset where the model learns when to invoke tools versus respond directly, improving decision-making in multi-step interactions. Hammer (Masked) (Lin et al., 2024): subset of Hammer with randomized tool and parameter names, forcing the model to rely on descriptions rather than memorized labels, thus enhancing generalization and reducing overfitting to certain tools. xLAM (Zhang et al., 2024): compositional dataset requiring one or multiple tool calls per turn, encouraging the model to reason about tool dependencies and plan diverse tool calling action actively. For RL training, we sample 2K examples from ToolACE and 1K each from Hammer and xLAM, creating balanced dataset spanning diverse levels of complexity and tool use. Multi-step trajectories are decomposed into single-step instances, with prior dialogue history injected into the user prompt (as shown in Figure 12) to preserve context. This setup encourages strategic exploration and teaches the model to select and apply tools appropriately within each step. Please see Appendix for more details and justifications. 4.2 Experiment Settings Training. We conduct all RL experiments using the veRL framework (Sheng et al., 2024), adopting the GRPO algorithm detailed in the previous section. For each training step, we sample batch of 512, and generate 4 responses per query, training for 15 epochs in total (see Appendix for full configuration details). To encourage broader policy exploration, we remove KL regularization and apply generation temperature of 1.0. We initialize our models with the Qwen-2.5-Instruct (Team, 2024) and Llama-3.2-Instruct (Dubey et al., 2024) series, which are further tuned under the GRPO objective with our customized reward design. Evaluation. We evaluate our approach on the Berkeley Function Call Leaderboard (BFCL) (Patil et al., 2024), comprehensive benchmark that spans diverse set of challenges, including single-step reasoning, multi-step tool use, realtime execution, irrelevant tool rejection, simultaneous multi-tool selection, and multi-tool application2. In addition, we present results on API2https://gorilla.cs.berkeley.edu/blogs/13_ bfcl_v3_multi_turn.html Bank (Li et al., 2023), three-level evaluation framework comprising 73 diverse and complex API tools. It assesses an LLMs ability to select and apply tools through natural multi-turn dialogues, across three levels of difficulty. We also evaluate on representative QA benchmark Bamboogle (Press et al., 2022), which comprises variety of questionanswering tasks where performance is measured based on the final answer accuracy rather than the correctness of tool use. These broad coverage makes our evaluation setting effective for evaluating real-world LLM tool use proficiency. All results are reported in terms of accuracy. Baselines. We compare our approach against several baselines to better isolate the effects of GRPO training: (1) Raw Instruct Model: the original model without any additional fine-tuning or RL, evaluated using the same prompts. (2) SFT on RL Data: the instruct model fine-tuned using the same 4K / selected 400 data points as the RL training set, providing comparison point to assess whether GRPO training outperforms standard SFT. (3) GRPO on SFT Model: GRPO is applied to model that has already undergone SFT on the selected 400 data points. This setup allows us to evaluate the impact of initializing GRPO with format-aware model, in contrast to starting from the raw instruct model in cold start manner. (4) PPO: We also include the standard PPO setting as baseline to evaluate whether our reward design is effective beyond GRPO. We report results for both cold start PPO model and PPO model initialized with SFT, using the same hyperparameters as in the GRPO setup for fair comparison. Please refer to Appendix for more details and justifications. 4.3 Results Main Results. We report BFCL and API-Bank results in Table 1 and Table 2, respectively. Our GRPO method, trained from scratch on the Qwen2.5-Instruct series, generally outperforms other baselines, achieving 10% absolute gains over SFT trained on the same data volume. In contrast, LLaMA-3.2-Instruct shows less improvement, possibly due to the models lower adaptability to GRPO-style generalization. Nevertheless, it remains competitive and outperforms most baselines on API-Bank. SFT Initialization Interestingly, GRPO also improves models initialized with limited SFT, often outperforming full-scale SFT Impacts. (a) Format Reward (b) Correctness Reward Figure 5: Format (left) and correctness (right) reward trends across training steps for Qwen2.5-3B-Instruct with different model initialization strategies. (a) Format Reward (b) Correctness Reward Figure 6: Format (left) and correctness (right) reward trends across training steps for Qwen2.5-3B-Instruct with different RL strategies (GRPO v.s. PPO). trained on 10 times more data. However, this setup still underperforms compared to cold start GRPO. We hypothesize that SFT initialization leads to memorization and overfitting, which reduces the impact of GRPOs effectiveness in generalization. As shown in Figure 5, SFT-initialized models achieve higher training rewards due to distributional alignment between SFT and RL data, but empirically generalize worse on the two benchmarks. This further highlights that higher training rewards do not necessarily translate to better generalization. Reward Design on PPO. We also evaluate PPO under both cold start and SFT-initialized settings to examine the effectiveness of our reward design. The results show that while PPO with cold start can outperform SFT in some cases, it tends to be less stable across different model settings. In contrast, GRPO consistently achieves higher rewards even from cold start, suggesting that our reward design is partially effective for PPO but works best in the GRPO framework. As shown in Figure 6, GRPO not only achieves higher correctness rewards but also gains format rewards more rapidly during training. Interestingly, PPO benefits from SFT initialization, generally yielding better results than cold start, whereas GRPO performs better Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400) Qwen2.5-1.5B-Instruct (SFT4k) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (PPO Cold Start) Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400) Qwen2.5-3B-Instruct (SFT4k) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (PPO Cold Start) Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400) Qwen2.5-7B-Instruct (SFT4k) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (PPO Cold Start) Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400) Llama-3.2-3B-Instruct (SFT4k) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (PPO Cold Start) Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) 19.41% 40.21% 40.67% 42.95% 40.93% 38.32% 46.20% 33.04% 34.08% 41.97% 45.80% 46.42% 51.15% 52.98% 41.97% 34.08% 36.53% 42.02% 39.25% 46.68% 58.38% 22.09% 41.22% 44.16% 41.62% 42.54% 42.98% 44.10% 16.00% 65.12% 59.94% 77.65% 70.54% 79.40% 77.96% 42.52% 69.29% 62.85% 78.29% 76.21% 82.42% 81.58% 66.02% 69.29% 45.15% 83.90% 80.69% 79.33% 86.17% 17.44% 64.27% 65.42% 68.10% 65.15% 84.00% 74.38% 13.18% 61.11% 59.84% 69.75% 60.79% 70.11% 76.98% 40.80% 61.50% 54.73% 71.09% 68.93% 78.52% 79.43% 70.11% 66.68% 53.5% 72.62% 74.34% 78.16% 78.25% 14.57% 62.18% 67.02% 69.88% 68.98% 72.00% 75.18% 35.58% 56.69% 59.31% 55.73% 56.33% 45.24% 60.73% 53.96% 41.40% 59.17% 58.76% 64.15% 67.78% 73.78% 53.51% 41.4% 57.13% 51.84% 46.51% 63.17% 74.9% 43.85% 58.37% 63.04% 52.98% 59.40% 52.80% 56.86% 0.00% 1.00% 1.00% 1.88% 1.00% 0.87% 2.25% 1.00% 0.00% 0.75% 5.12% 1.75% 4.88% 3.75% 4.25% 0.00% 0.75% 0.25% 0.25% 0.38% 18.12% 0.00% 0.75% 1.38% 3.00% 0.88% 2.88% 1.37% 44.44% 94.44% 88.89% 100.00% 94.44% 100.00% 100.00% 64.71% 94.44% 77.78% 94.12% 88.89% 94.12% 88.24% 76.47% 94.44% 72.22% 100.00% 100.00% 88.89% 83.33% 77.78% 66.67% 77.78% 94.12% 72.22% 100.00% 94.44% Table 1: BFCL V3 Benchmark Results (Main Result) 82.49% 60.14% 71.34% 48.40% 58.63% 18.09% 56.44% 56.01% 8.11% 75.12% 54.70% 71.76% 73.87% 84.85% 62.66% 8.11% 72.32% 29.66% 14.19% 52.92% 76.68% 66.07% 71.12% 78.25% 56.29% 65.80% 31.94% 62.23% Model Overall Acc Level 1 Level 2 Level 3 Model Accuracy Avg Num Tool Call Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400) Qwen2.5-1.5B-Instruct (SFT4k) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (PPO Cold Start) Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400) Qwen2.5-3B-Instruct (SFT4k) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (PPO Cold Start) Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400) Qwen2.5-7B-Instruct (SFT4k) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (PPO Cold Start) Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400) Llama-3.2-3B-Instruct (SFT4k) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (PPO Cold Start) Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) 30.65% 53.60% 47.07% 57.12% 61.31% 40.54% 63.15% 51.59% 52.76% 50.92% 65.16% 62.48% 57.62% 67.00% 62.48% 50.59% 47.07% 63.15% 54.10% 61.64% 64.66% 40.54% 52.76% 43.89% 57.79% 56.78% 55.78% 59.13% 28.32% 35.82% 35.11% 57.14% 50.75% 44.27% 52.88% 52.24% 26.72% 60.9% 50.75% 48.85% 64.16% 58.21% 54.20% 44.61% 31.34% 32.82% 70.68% 61.19% 41.22% 59.65% 32.84% 36.64% 59.65% 50.75% 32.82% 55.64% 43.28% 40.46% 67.92% 55.22% 61.83% 68.67% 58.21% 45.80% 64.66% 59.70% 35.11% 73.43% 67.16% 47.33% 70.68% 49.25% 44.27% 55.89% 50.75% 34.35% 51.13% 34.33% 41.22% 72.43% 58.21% 37.40% 61.40% 52.24% 32.82% 68.67% 44.78% 48.85% 73.93% 61.19% 38.17% 44.86% 29.85% 32.82% 60.65% 35.82% 37.40% 53.88% 29.85% 20.61% 63.16% 47.76% 46.56% 63.60% 41.79% 43.51% 60.65% 41.79% 48.09% 65.66% 52.24% 42.75% Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400) Qwen2.5-1.5B-Instruct (SFT4k) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (PPO Cold Start) Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400) Qwen2.5-3B-Instruct (SFT4k) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (PPO Cold Start) Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400) Qwen2.5-7B-Instruct (SFT4k) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (PPO Cold Start) Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400) Llama-3.2-3B-Instruct (SFT4k) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (PPO Cold Start) Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) 20.8% 24.8% 23.2% 36.8% 38.4% 23.2% 44.0% 52.0% 54.4% 49.6% 43.2% 56.8% 40.0% 60.0% 69.6% 28.8% 30.4% 45.6% 29.6% 48.0% 72.0% 34.4% 44.0% 48.8% 39.2% 45.6% 29.6% 52.0% 0.61 0.78 1.25 1.06 0.96 2.38 1.19 1.77 0.86 0.92 1.04 0.99 1.14 1.32 1.42 3.71 1.06 3.54 3.70 1.25 1.63 1.25 0.98 0.98 1.33 1.00 1.42 0.89 Table 2: API-Bank Test Results (Main Result) Table 3: Bamboogle Test Results (Main Result) when trained from scratch. These findings highlight that while PPO can benefit from our reward design, its impact is more limited compared to the more robust and consistent improvements observed with GRPO. Generalization Studies. We evaluate the generalization ability of our trained model in two challenging settings: unfamiliar scenarios and novel task goals (both from BFCL benchmark subset). Specifically, we test the models performance in tool usage within unseen programming languages and its ability to detect irrelevant tools, neither of (a) Unfamiliar Scenario (b) Unfamiliar Goal Figure 7: Qwen2.5-3B-Instructs performance across unfamiliar programming language scenarios (left) and novel relevance detection task goals (right), evaluated under different training settings. User: would like to buy movie ticket in San Jose at 11 oclock in the night. Tools Available: 1. Movies_1_BuyMovieTickets(movie_name, number_of_tickets, ...) Buy tickets for movie at specific time and location. 2. Movies_1_FindMovies(location, ...) Search movies by location, genre, or format. 3. Movies_1_GetTimesForMovie(movie_name, location, ...) Get showtimes for movie on specific date and location. <think> The user wants to buy movie ticket in San Jose at 11 PM. The available tool, Movies_1_BuyMovieTickets, allows us to purchase movie tickets with the required details such as movie name, number of tickets, show date, location, show time, and show type. We need to find the movie name and the show date. However, the user did not specify the movie name or the show date. We can infer the show date from the current date, but we need to know the movie name to proceed with purchase. </think> <response> To assist in purchasing movie ticket, need the name of the movie and the specific date of the show. Could you please provide this information? </response> User: What is the slope of the line which is perpendicular to the line with the equation = 3x + 2? Tools Available: 1. find_critical_points(function, variable, range=[0.0, 3.4]) Find critical points of function with respect to variable, optionally within range. <think> The problem is asking for the slope of line that is perpendicular to the line with the equation = 3x + 2. The slope of the given line is 3. The slope of line that is perpendicular to another line is the negative reciprocal of the original lines slope. So, the slope of the line perpendicular to = 3x + 2 would be -1/3. The given tool find_critical_points is not relevant to finding the slope of line and cannot be used to solve this problem. </think> <response> The slope of the line that is perpendicular to the line with the equation = 3x + 2 is -1/3. </response> Table 4: Cases of Qwen2.5-3B-Instruct (GRPO Cold Start) generalizing to irrelevant tool detection goals. which were explicitly included during RL training or in the dataset. As shown in Figure 7, Qwen2.53B-Instruct, when trained from scratch with our GRPO-based reward design, consistently achieves highest performance. Additionally, Section 4.3 presents two qualitative examples where the model proactively rejects inappropriate tool usefirst by clarifying ambiguous intent, and second by opting to answer directly without tools. These behaviors reflect emergent proactivity and metacognition, enhancing efficiency, reducing hallucinations, and signaling foundational agentic intelligence. Free-form Inference Effectiveness. While our model is trained with focus on tool call format and correctness, we further evaluate its ability to handle free-form tool use in QA setting. Unlike the structured tool selection and application tasks, QA setting: (1) imposes no constraints on tool call parameters, and (2) evaluates only the final answer, making it goal-oriented rather than process-oriented task. This naturally introduces multi-step interaction scenario. Specifically, we use Bamboogle, multi-hop QA dataset, to assess this capability. The model is equipped with web search tool, and we report both the answer accuracy and the number of tool calls for all baselines and our approach. As shown in Table 3, our reward design achieves the highest (a) Response Length (b) Length Reward Figure 8: Response length (left) and its reward (right) trends across training steps for different models. performance, despite this setting not being explicitly seen during training. Notably, our cold start GRPO model surpasses others in accuracy without relying on excessive number of tool calls. This suggests that the model can flexibly invoke tools when needed, effectively leverage feedback, wisely and efficiently navigating toward the correct answer."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we conduct series of ablation studies to identify the most effective reward design for tool calling. We explore various factors including reward type, scale, granularity, and temporal dynamics. 5.1 Effect of Length Reward We first examine the role of length-based reward. Prior work has demonstrated that the R1-like models can promote deeper reasoning, often reflected in longer thinking traces. To encourage this behavior, we introduce reward term proportional to the length of the <think> field: Rlength = min (cid:18) Lthink Ltarget (cid:19) , 1 where Lthink denotes the length of the thinking segment in models output, and Ltarget denotes the target output length, which we empirically set to 512. We found that the raw model rarely generates responses longer than half this length, making 512 reasonable and effective target for encouraging longer outputs. This length-based component is added to the overall reward, which now consists of format, correctness, and reasoning length. As shown in Figure 8, both response length and the length reward generally increase throughout training, particularly for the Qwen model series. This indicates that the length reward effectively encourages longer reasoning. However, the downstream results in Table 5 reveal that adding length Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Qwen2.5-1.5B-Instruct (Original) Qwen2.5-1.5B-Instruct (w/ Length Reward) Qwen2.5-1.5B-Instruct (Dynamic) Qwen2.5-3B-Instruct (Original) Qwen2.5-3B-Instruct (w/ Length reward) Qwen2.5-3B-Instruct (Dynamic) Llama-3.2-3B-Instruct (Original) Llama-3.2-3B-Instruct (w/ Length reward) Llama-3.2-3B-Instruct (Dynamic) 46.20% 33.23% 28.51% 52.98% 48.89% 48.24% 44.10% 44.98% 43.15% 77.96% 70.58% 53.23% 81.58% 77.83% 77.60% 74.38% 78.02% 75.50% 76.98% 71.36% 48.23% 79.43% 78.61% 79.11% 75.18% 77.54% 71.64% 60.73% 35.63% 38.07% 73.78% 63.56% 63.22% 56.86% 56.55% 56.06% 2.25% 0.50% 0.00% 3.75% 4.50% 3.00% 1.37% 1.25% 1.00% 100.00% 94.44% 55.56% 88.24% 88.24% 88.89% 94.44% 100.00% 100.00% 56.44% 4.52% 25.08% 84.85% 71.22% 68.53% 62.23% 63.76% 57.82% Table 5: BFCL V3 Benchmark Results (Length) (a) Response Length (b) Length Reward Figure 9: Response length (left) and its reward (right) trends across training steps within the dynamic length reward training setting. reward does not consistently improve task performance, and in smaller-scale models, it can even cause substantial degradation. These observations suggest that while extended reasoning may appear desirable, it is not always beneficial for tool use tasks. In fact, excessive length may introduce unnecessary complexity, leading to overthinking and reduced effectiveness. Dynamic Length Reward. Since fixed-length rewards showed minimal impact and converged quickly, we explored dynamic length reward that adapts over training steps. Specifically, we define: Rdynamic = min (cid:18) Lthink Ltarget (1 + p) (cid:19) , 1 where denotes the training steps and = Scurrent [0, 1] represents the normalized training Stotal progress. This formulation gradually increases the target thinking length over time, aligning with model maturity. As shown in fig. 9, this approach yields steadier growth in thinking length, particularly for the Llama model. However, the performance results in Table 5 reveal that even scheduled rewards fail to improve performance. This further supports our hypothesis that extended reasoning may not benefit this task and can even have adverse effects. Takeaway 1: While length rewards encourage longer reasoning traces, they do not consistently improve task performance and may even harm it in smaller models, highlighting that longer reasoning is not inherently better for tool use tasks. 5.2 Effect of Reward Scale Next, we investigate the effect of reward scaling, specifically the relative weighting between correctness and format rewards. Prior work in R1-style RL commonly assigns higher weight to correctness reward than to format reward (Xie et al., 2025; Jin et al., 2025), emphasizing the importance of learning correct answer over superficial adherence to format. This strategy helps prevent reward hacking, where model might exploit formatting heuristics without learning task semantics. To test the importance of this design choice, we conduct an ablation where we equalize the maximum correctness and format rewards by setting the formers range to [1, 1], matching that of the format reward. This adjustment only affects the final normalization step of the correctness reward: Rcorrect = 2 Rmax Smax 1 [1, 1] where all variables are defined as in Section 3.3. As shown in Table 6, this equal-scaling variant, denoted as Equal Max, results in slight drop in overall accuracy across most models, with the exception of Qwen2.5-3B, which maintains performance comparable to the original setting. These results underscore the importance of assigning greater weight to correctness reward: doing so helps steer the model toward mastering the core reasoning and tool use capabilities necessary for robust generalization. Dynamic Reward Scaling. Building on the insight that correctness reward plays more critical Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Qwen2.5-1.5B-Instruct (Original) Qwen2.5-1.5B-Instruct (Equal max) Qwen2.5-1.5B-Instruct (Two stage) Qwen2.5-1.5B-Instruct (Dynamic) Qwen2.5-3B-Instruct (Original) Qwen2.5-3B-Instruct (Equal max) Qwen2.5-3B-Instruct (Two stage) Qwen2.5-3B-Instruct (Dynamic) Llama-3.2-3B-Instruct (Original) Llama-3.2-3B-Instruct (Equal max) Llama-3.2-3B-Instruct (Two stage) Llama-3.2-3B-Instruct (Dynamic) 46.20% 39.47% 38.85% 45.71% 52.98% 51.76% 50.66% 53.81% 44.10% 42.47% 41.33% 46.85% 77.96% 78.56% 77.96% 78.31% 81.58% 81.50% 80.62% 81.44% 74.38% 67.77% 65.54% 83.00% 76.98% 75.50% 76.23% 75.73% 79.43% 79.50% 78.82% 80.75% 75.18% 75.05% 72.70% 72.77% 60.73% 45.45% 44.51% 58.91% 73.78% 69.79% 67.93% 75.43% 56.86% 55.75% 55.22% 61.00% 2.25% 2.50% 2.25% 2.50% 3.75% 4.25% 3.50% 3.62% 1.37% 1.00% 0.75% 3.38% 100.00% 100.00% 100.00% 100.00% 88.24% 88.89% 88.89% 77.78% 94.44% 88.89% 88.89% 88.89% 56.44% 16.44% 10.61% 57.20% 84.85% 78.07% 76.42% 88.82% 62.23% 59.56% 57.59% 59.37% Table 6: BFCL V3 Benchmark Results (Scale) role, we are further motivated by the intuition that different reward components may benefit from being emphasized at different stages of training. This leads us to explore dynamically adjusting reward scales in accordance with training progress. Specifically, we hypothesize that in early training, the model should prioritize learning the correct output format, which entails an easier objective, before gradually shifting focus to the more challenging goal of tool use correctness. To test this hypothesis, we design two dynamic reward scaling strategies: Two stage (Coarse) Setting: We divide training into two phases. In the first training steps, we downscale the correctness reward to 1 3 of its original scale while keeping the format reward at its original scale. After step s, we restore the correctness reward to its original scale and simultaneously reduce the format reward to range [0, 0.5] ( 1 2 of its original scale). Formally the reward scales are: Scaleformat = Scalecorrect = (cid:40) if Scurrent < [0, 1] [0, 0.5] otherwise , (cid:40) if Scurrent < [1, 1] [3, 3] otherwise where Scurrent denotes the current training step. In our experiments, we empirically set the switching point to = 30 steps, as we observed that the format reward typically experiences significant increase within the first 30 steps. Therefore, it is more beneficial for later steps to shift focus toward optimizing correctness. Dynamic (Finegrained) Setting: We apply continuous interpolation between the two reward scales throughout training. Initially, both the format and correctness reward scales are set equally. Over time, the format reward scale linearly decays to its original value, while the correctness (a) Format Reward (b) Correctness Reward Figure 10: Format (left) and correctness (right) reward trends across training steps for Qwen2.5-3B-Instruct with different reward scale dynamics. reward scale gradually increases to its original value, allowing the training to shift focus from format adherence to task correctness accordingly. Formally, the dynamic scaling is then defined as: Scaleformat = [2 + p, 2 p], Scalecorrect = [2 p, 2 + p] where [0, 1] similarly represents the normalized training progress. This design ensures smooth shift of learning focus from format fidelity to correctness. We present the reward dynamics of the original and two dynamic scaling strategies in Figure 10. As shown in Table 6, the Two stage (Coarse) reward setting unexpectedly leads to drop in performance, whereas the Dynamic (Finegrained) scaling could improve models benchmarking performance. These findings suggest that abrupt shifts in reward scale may negatively impact the training dynamics. In contrast, smoother and gradual transition from simpler objectives to more nuanced ones appears to better support the models learning trajectory and generalization during GRPO training. Takeaway 2: Gradually adjusting reward scales during training, rather than abrupt changes, better supports model learning and generalization, highlighting the benefits of smoother transition from simpler objectives to more complex ones. 5.3 Effect of Reward Granularity We now perform detailed analysis of the effect of reward granularity, focusing specifically on the correctness reward. Tool calling, by nature, poses challenges for reward assignment, as it involves multiple facets beyond single definitive answer (e.g., in contrast to math reasoning tasks). Our original reward design decomposes correctness into matching the tool name, parameter names, and parameter values, offering finegrained, processoriented signal that reflects partial correctness in tool usage. To assess the impact of this granularity, we evaluate three alternative reward formulations with progressively coarser levels of aggregation: Finegrained: We apply strict exact-match constraints to both tool name and parameter name matching. Specifically, we define: rname = 1[NG = NP ] {0, 1} (cid:88) 1[keys(PG) = keys(PP )] [0, G] rparam = Gj Intermediate: We combine the parameter name and value rewards into single term that enforces an exact match on the entire parameter dictionary. Formally: rparam + rvalue = (cid:88) Gj 1[PG = PP ] [0, G] Coarse: At the coarsest level, we fully entangle tool name, parameter names, and parameter values, treating the entire tool set as unit. Reward is given only if the generated tool set exactly matches the ground truth: rname + rparam + rvalue = 1[G = ] {0, 1} All other aspects of reward computation are kept identical to those described in Section 3.3. Starting from our original design, which is the most finegrained, we progressively entangle reward components to derive increasingly coarse-grained alternatives. Figure 11: Correctness reward trends across training steps for Qwen2.5-3B-Instruct with different reward granularity. The reward dynamics across training steps, shown in Figure 11, demonstrate that as reward granularity becomes coarser, it becomes harder for the model to achieve higher reward values during RL training. This suggests that overly strict and entangled rewards may lead to sparse learning signals, potentially hindering effective credit assignment. Empirical results in Table 7 further support this insight: our original, most finegrained reward strategy performs well across models. In general, finergrained reward decomposition leads to better training outcomes and higher final task performance, indicating its advantage in promoting more stable and effective policy learning. Takeaway 3: Finegrained reward decomposition provides richer learning signals, highlighting its role in enabling more effective training compared to coarse reward formulations, which can impede progress and degrade final performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present reward design tailored for GRPO training on tool use tasks. Empirically, our model trained from scratch using GRPO consistently outperforms both SFT-based and SFTinitialized RL baselines, as well as models trained with alternative RL algorithms, across variety of held-out tool use benchmarks. Furthermore, we demonstrate that our model generalizes well to QA settings, exhibiting robust multi-turn interactions, emergent proactiveness, and metacognitive behaviors, all of which are key traits for efficient and adaptable tool use, lying at the core of foundational agent capabilities. Our in-depth analysis of reward types, scaling strategies, granularity, and temporal dynamics provides further insights into how reward shaping influences learning and behavior. We hope these findings serve as roadmap for future work Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Qwen2.5-1.5B-Instruct (Original) Qwen2.5-1.5B-Instruct (Finegrained) Qwen2.5-1.5B-Instruct (Intermediate) Qwen2.5-1.5B-Instruct (Coarse) Qwen2.5-3B-Instruct (Original) Qwen2.5-3B-Instruct (Finegrained) Qwen2.5-3B-Instruct (Intermediate) Qwen2.5-3B-Instruct (Coarse) Llama-3.2-3B-Instruct (Original) Llama-3.2-3B-Instruct (Finegrained) Llama-3.2-3B-Instruct (Intermediate) Llama-3.2-3B-Instruct (Coarse) 46.20% 40.71% 37.65% 36.72% 52.98% 52.06% 51.36% 51.40% 44.10% 39.82% 38.62% 35.95% 77.96% 78.00% 77.94% 76.44% 81.58% 81.65% 81.15% 79.48% 74.38% 64.71% 59.83% 52.00% 76.98% 75.55% 72.46% 70.86% 79.43% 79.64% 80.07% 78.54% 75.18% 70.68% 71.86% 61.43% 60.73% 48.91% 43.00% 41.27% 73.78% 69.21% 68.64% 68.73% 56.86% 52.20% 50.56% 48.96% 2.25% 2.00% 1.62% 2.12% 3.75% 5.50% 4.25% 5.62% 1.37% 0.25% 0.25% 1.12% 100.00% 100.00% 100.00% 100.00% 88.24% 83.33% 88.89% 88.89% 94.44% 100.00% 94.44% 83.33% 56.44% 24.84% 12.45% 12.24% 84.85% 78.14% 75.74% 77.80% 62.23% 56.68% 55.68% 61.92% Table 7: BFCL V3 Benchmark Results (Granularity) in applying reinforcement learning to tool use. Ultimately, we envision that reward is all tool learning needs, and that RL offers powerful path toward generalizable and creative agent behavior."
        },
        {
            "title": "References",
            "content": "Emre Can Acikgoz, Jeremiah Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil Koukoumidis, Dilek Hakkani-Tür, and Gokhan Tur. 2025. Can single model master both multi-turn conversations and tool use? coalm: unified conversational agentic language model. Preprint, arXiv:2502.08820. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023a. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Nuo Chen, Hongguang Li, Baoyuan Wang, and Jia Li. 2023b. From good to great: Improving math reasoning with tool-augmented interleaf prompting. arXiv preprint arXiv:2401.05384. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-FLAN: Designing data and methods of effective agent tuning for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 93549366, Bangkok, Thailand. Association for Computational Linguistics. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. 2025. Sft memorizes, rl generalizes: comparative study of arXiv preprint foundation model post-training. arXiv:2501.17161. Quy-Anh Dang and Chris Ngo. 2025. Reinforcement learning for reasoning in small llms: What works and what doesnt. arXiv preprint arXiv:2503.16219. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2023. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489. Yoshitaka Inoue, Tianci Song, and Tianfan Fu. 2024. Drugagent: Explainable drug repurposing agent with large language model-based reasoning. arXiv preprint arXiv:2408.13378. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Minki Kang, Jongwon Jeong, and Jaewoong Cho. 2025. T1: Tool-integrated self-verification for test-time compute scaling in small language models. arXiv preprint arXiv:2504.04718. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023. survey of reinforcement arXiv preprint learning from human feedback. arXiv:2312.14925. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350. Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Salman Khan, and Fahad Shahbaz Khan. 2025. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025a. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025b. arXiv preprint Torl: Scaling tool-integrated rl. arXiv:2503.23383. Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024. Mario: Math reasoning with code interpreter outputa reproducible pipeline. arXiv preprint arXiv:2401.08190. Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, et al. 2024. Hammer: Robust function-calling for on-device language models via function masking. arXiv preprint arXiv:2410.04587. Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. 2023. Domain specialization as the key to make large language models disruptive: comprehensive survey. arXiv preprint arXiv:2305.18703. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. 2024. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025. Smart: Self-aware agent for tool overuse mitigation. arXiv preprint arXiv:2502.11435. Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023. Creator: Tool creation for disentangling abstract and concrete reasoning of large In Findings of the Association language models. for Computational Linguistics: EMNLP 2023, pages 69226939. Cheng Qian, Peixuan Han, Qinyu Luo, Bingxiang He, Xiusi Chen, Yuji Zhang, Hongyi Du, Jiarui Yao, Xiaocheng Yang, Denghui Zhang, et al. 2024a. Escapebench: Pushing language models to think outside the box. arXiv preprint arXiv:2412.13549. Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2024b. Toolink: Linking toolkit creation and using through chain-of-solving on opensource model. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 831854. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024a. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Tool Chaojun Xiao, Chi Han, et al. 2023. learning with foundation models. arXiv preprint arXiv.2304.08354, 10. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. 2024b. Tool learning with foundation models. ACM Computing Surveys, 57(4):140. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024c. Toolllm: Facilitating large language models to master 16000+ real-world apis. In The Twelfth International Conference on Learning Representations. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:53728 53741. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, et al. 2024. Autosurvey: Large language models can automatically write surveys. Advances in Neural Information Processing Systems, 37:115119115145. Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, and Jure Leskovec. 2024. Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. arXiv preprint arXiv:2405.17631. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. 2025. Vlmr1: stable and generalizable r1-style large visionlanguage model. arXiv preprint arXiv:2504.07615. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Qwen Team. 2024. Qwen2.5: party of foundation models. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2023. Rational decision-making agent with arXiv preprint internalized utility judgment. arXiv:2308.12519. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. 2025. Dapo: An opensource llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. 2024. Steptool: step-grained reinforcement learning framework for tool learning in llms. arXiv preprint arXiv:2410.07745. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. 2025. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. AgentTuning: Enabling generalized agent abilities for LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 30533077, Bangkok, Thailand. Association for Computational Linguistics. Yuanzhao Zhai, Tingkai Yang, Kele Xu, Feng Dawei, Cheng Yang, Bo Ding, and Huaimin Wang. 2024. Enhancing decision-making for llm agents via step-level q-value models. arXiv preprint arXiv:2409.09345. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua Tenenbaum, Tianmin Shu, and Chuang Gan. 2023. Building cooperative embodied agents modularly with large language models. arXiv preprint arXiv:2307.02485. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023. Freshllms: Refreshing large language models with search engine augmentation. arXiv preprint arXiv:2310.03214. Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. 2024. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160."
        },
        {
            "title": "A User Prompt Details",
            "content": "The system instruction is shown in Figure 4. The user prompt is used to store the trajectory history, including intermediate thoughts, tool calls, environment observations, and any additional user commands. The complete user instruction is presented in Figure 12."
        },
        {
            "title": "B Experiment Details",
            "content": "Training Data Details. We empirically use 4K data points for training, as each dataset consists of samples drawn from the same distribution. Adding more data of similar nature does not increase task diversity. Moreover, we observe that increasing the dataset size beyond 4K does not yield noticeable improvements in the training convergence or final performance, suggesting diminishing returns from additional data under this setting. GRPO Setting Details. For all the tool calls in the dataset, we all use JSON format to represent tool call as its easy to parse and is the most general and structure way of performing tool call. For the GRPO training, we use 2 A100 (80G) GPUs per run with the following hyper-parameters: Category Hyperparameter Data Configuration Train Batch Size Validation Batch Size Max Prompt Length Max Response Length Optimization Learning Rate PPO Mini Batch Size KL Loss Used Rollout Configuration 512 128 2048 1024 1e-6 128 False Rollout Name GPU Memory Utilization Number of Rollouts vllm 0.6 4 Training & Logging Save Frequency (Steps) Test Frequency (Steps) Total Epochs 15 5 15 Table 8: Configuration for GRPO training. Baselines. The 400 selected data points used for SFT share the same distribution as the 4k data points used for RL training, but differ in content. For SFT, each data point includes <think> field, with thought content distilled from Deepseek-R1 trajectories. In contrast, GRPO does not require ground truth thought, as only the tool calls are used to compute rewards in the GRPO setting. We use 400 data points for SFT based on empirical observations that this amount is sufficient to help the raw model learn to follow our tool call format. This provides stronger initialization and reduces the burden of learning the format from scratch during RL training. However, we also find that relying solely on SFT can lead to overfitting, which may ultimately degrade performance. PPO Setting Details. We apply approximately the same parameter settings as GRPO for the PPO training. Similarly, we use 2 A100 (80G) GPUs per run with the following hyper-parameters: Category Hyperparameter Data Configuration Train Batch Size Validation Batch Size Max Prompt Length Max Response Length Optimization Actor Learning Rate Critic Learning Rate PPO Mini Batch Size PPO Micro Batch Size KL Coefficient Rollout Configuration 512 128 1024 512 1e-6 1e-5 128 8 0. Rollout Name GPU Memory Utilization vllm 0.3 Training & Logging Critic Warmup Steps Save Frequency (Steps) Test Frequency (Steps) Total Epochs 0 15 5 15 Table 9: Configuration for PPO training."
        },
        {
            "title": "C Additional Results",
            "content": "We present additional results on three benchmarks, applying GRPO and PPO methods to models initialized with SFT on 4K data points. This setting serves as theoretical upper bound, since the same 4K data is first used for SFT and subsequently reused for RL training. The results are shown in Table 10, Table 11, and Table 12 for BFCL, API-Bank, and Bamboogle, respectively. We compare RL training initialized with models fine-tuned on either 400 or 4K SFT data points. User Prompt for Training Dialogue History <user> {{ Initial User Input }} </user> <think> Round 1 Model Thought </think> {{ Round 1 model output <tool_call> or <response> }} <obs> Round 1 Observation </obs> ... ... <user> {{ User Input }} </user> ... ... Figure 12: The user prompt used for TIRs rollout. Model Overall Acc Non-Live AST Acc Non-Live Exec Acc Live Acc Multi Turn Acc Relevance Detection Irrelevance Detection Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (SFT4k+PPO) Qwen2.5-1.5B-Instruct (SFT4k+GRPO) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (SFT4k+PPO) Qwen2.5-3B-Instruct (SFT4k+GRPO) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (SFT4k+PPO) Qwen2.5-7B-Instruct (SFT4k+GRPO) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (SFT4k+PPO) Llama-3.2-3B-Instruct (SFT4k+GRPO) 19.41% 42.95% 40.93% 40.24% 42.63% 33.04% 45.80% 46.42% 48.22% 47.82% 41.97% 42.02% 39.25% 33.80% 35.18% 22.09% 41.62% 42.54% 45.41% 45.50% 16.00% 77.65% 70.54% 66.42% 66.60% 42.52% 78.29% 76.21% 77.75% 75.12% 66.02% 83.90% 80.69% 42.67% 43.58% 17.44% 68.10% 65.15% 73.71% 70.69% 13.18% 69.75% 60.79% 62.02% 64.77% 40.80% 71.09% 68.93% 73.18% 69.52% 70.11% 72.62% 74.34% 49.50% 50.39% 14.57% 69.88% 68.98% 68.46% 67.70% 35.58% 55.73% 56.33% 54.58% 60.15% 53.96% 58.76% 64.15% 64.27% 68.19% 53.51% 51.84% 46.51% 51.80% 55.49% 43.85% 52.98% 59.40% 62.27% 64.73% 0.00% 1.88% 1.00% 2.50% 1.38% 1.00% 5.12% 1.75% 5.25% 2.38% 4.25% 0.25% 0.25% 2.38% 0.87% 0.00% 3.00% 0.88% 2.50% 1.00% 44.44% 100.00% 94.44% 94.12% 88.89% 64.71% 94.12% 88.89% 94.12% 77.78% 76.47% 100% 100% 77.78% 77.78% 77.78% 94.12% 72.22% 82.35% 77.78% 82.49% 48.40% 58.63% 55.09% 67.98% 56.01% 54.70% 71.76% 66.41% 76.16% 62.66% 29.66% 14.19% 55.79% 67.12% 66.07% 56.29% 65.80% 68.75% 78.85% Table 10: BFCL V3 Benchmark Results (Additional Result) Model Overall Acc Level 1 Level 2 Level 3 Model Accuracy Avg Num Tool Call Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (SFT4k+PPO) Qwen2.5-1.5B-Instruct (SFT4k+GRPO) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (SFT4k+PPO) Qwen2.5-3B-Instruct (SFT4k+GRPO) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (SFT4k+PPO) Qwen2.5-7B-Instruct (SFT4k+GRPO) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (SFT4k+PPO) Llama-3.2-3B-Instruct (SFT4k+GRPO) 30.65% 57.12% 61.31% 61.31% 59.46% 51.59% 65.16% 62.48% 60.13% 60.80% 62.48% 63.15% 54.10% 59.30% 52.60% 40.54% 57.79% 56.78% 54.10% 50.92% 28.32% 35.82% 35.11% 60.9% 50.75% 48.85% 64.16% 58.21% 54.20% 64.91% 56.72% 52.67% 65.16% 53.73% 45.04% 59.65% 32.84% 36.64% 67.92% 55.22% 61.83% 68.67% 58.21% 45.80% 64.41% 44.78% 54.96% 64.41% 56.72% 51.91% 70.68% 49.25% 44.27% 72.43% 58.21% 37.4% 61.40% 52.24% 32.82% 61.40% 40.30% 61.60% 56.39% 34.33% 50.38% 44.86% 29.85% 32.82% 63.16% 47.76% 46.56% 63.60% 41.79% 43.51% 60.65% 40.30% 41.22% 59.15% 34.33% 34.35% Table 11: API-Bank Test Results (Additional Result) Qwen2.5-1.5B-Instruct (Raw) Qwen2.5-1.5B-Instruct (SFT400+PPO) Qwen2.5-1.5B-Instruct (SFT400+GRPO) Qwen2.5-1.5B-Instruct (SFT4k+PPO) Qwen2.5-1.5B-Instruct (SFT4k+GRPO) Qwen2.5-3B-Instruct (Raw) Qwen2.5-3B-Instruct (SFT400+PPO) Qwen2.5-3B-Instruct (SFT400+GRPO) Qwen2.5-3B-Instruct (SFT4k+PPO) Qwen2.5-3B-Instruct (SFT4k+GRPO) Qwen2.5-7B-Instruct (Raw) Qwen2.5-7B-Instruct (SFT400+PPO) Qwen2.5-7B-Instruct (SFT400+GRPO) Qwen2.5-7B-Instruct (SFT4k+PPO) Qwen2.5-7B-Instruct (SFT4k+GRPO) Llama-3.2-3B-Instruct (Raw) Llama-3.2-3B-Instruct (SFT400+PPO) Llama-3.2-3B-Instruct (SFT400+GRPO) Llama-3.2-3B-Instruct (SFT4k+PPO) Llama-3.2-3B-Instruct (SFT4k+GRPO) 20.8% 36.8% 38.4% 36.8% 34.4% 52.0% 43.2% 56.8% 46.4% 47.2% 69.6% 45.6% 29.6% 40.0% 32.0% 34.4% 39.2% 45.6% 49.6% 42.4% 0.61 1.06 0.96 1.06 1.02 1.77 1.04 0.99 1.01 0.98 1.42 3.54 3.70 1.25 1.25 1.25 1.33 1.00 1.02 1. Table 12: Bamboogle Test Results (Additional Result) Interestingly, our findings suggest that initializing from model finetuned on 4K data does not consistently outperform initialization from model finetuned on only 400 data points. In the BFCL benchmark, we even observe cases where performance drops below that of the raw instruct model. This counterintuitive result may stem from overfitting during the SFT phase, which could restrict the models ability to explore during RL training and lead to poorer generalization on held-out tasks."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}