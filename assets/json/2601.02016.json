{
    "paper_title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
    "authors": [
        "Matthias Bartolo",
        "Dylan Seychell",
        "Gabriel Hili",
        "Matthew Montebello",
        "Carl James Debono",
        "Saviour Formosa",
        "Konstantinos Makantasis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings."
        },
        {
            "title": "Start",
            "content": "SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 1 Enhancing Object Detection with Privileged Information: Model-Agnostic TeacherStudent Approach"
        },
        {
            "title": "Matthias Bartolo",
            "content": ", Dylan Seychell , Senior Member, IEEE, Gabriel Hili ,"
        },
        {
            "title": "Matthew Montebello",
            "content": ", Senior Member, IEEE, Carl James Debono , Senior Member, IEEE,"
        },
        {
            "title": "Saviour Formosa",
            "content": ", and Konstantinos Makantasis , Member, IEEE 6 2 0 2 5 ] . [ 1 6 1 0 2 0 . 1 0 6 2 : r AbstractThis paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce general, model-agnostic methodology for injecting privileged informationsuch as bounding box masks, saliency maps, and depth cuesinto deep learning-based object detectors through teacherstudent architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings. Index TermsComputer Vision, knowledge distillation, learning using privileged information, litter detection, object detection I. INTRODUCTION DVANCEMENTS in computing hardware, particularly GPUs, have enabled the rapid adoption of artificial intelligence and automation technologies. Within this landscape, object detection has emerged as cornerstone problem, driving applications in areas such as autonomous systems, environmental monitoring, and robotics. Over the past decade, models such as YOLO [1], Faster R-CNN [2], and RetinaNet [3] have delivered fast and accurate detection capabilities, making object detection widely deployable technology. Despite this progress, achieving consistently high detection accuracy (Corresponding author: Matthias Bartolo.) Faculty Intelligence, Technology, University are with Information matthias.bartolo@um.edu.mt; and Konstantinos Makantasis of Seychell, Gabriel Hili, Matthew MonMatthias Bartolo, Dylan the Department tebello, and Commuof Artificial of Malta, MSD 2080, Malta nications dylan.seychell@um.edu.mt; (e-mail: gabriel.hili@um.edu.mt; konstantinos.makantasis@um.edu.mt). Carl James Debono is with the Department of Communications and Computer Engineering, Faculty of Information and Communications Technology, University of Malta, MSD 2080, Malta (e-mail: carl.debono@um.edu.mt). Saviour Formosa is with the Department of Criminology, Faculty of Social Wellbeing, University of Malta, MSD 2080, Malta (e-mail: saviour.formosa@um.edu.mt). matthew.montebello@um.edu.mt; This work is part of the project Aerial Waste Identification and Geolocation System (AWIGS), financed by Xjenza Malta, through the FUSION: R&I Technology Development Programme Lite. Figure 1. Visual comparison of baseline object detection predictions and those of LUPI-trained student model, showing improved accuracy while keeping the same architecture. The figure also illustrates the LUPI training pipeline, including privileged information, teacher models, and knowledge distillation, with these boosts arising solely from the bolstered learning process. remains challenge. Many state-of-the-art detectors rely on increasingly complex architectures [4], [5], which still need to be fine-tuned for specific domain use cases using large annotated datasets [6], both of which introduce significant practical constraints. Deep models often require extensive training time and computational resources, while large-scale datasets demand costly and labour-intensive annotation to improve detection accuracy [6], [7]. However, annotated images contain highly rich information that current state-of-the-art object detection models do not fully exploit. In this study, we test the hypothesis that highly descriptive, fine-grained information can be automatically constructed and leveraged during training to improve object detector performance. Building on our preliminary results [8], we adopt the Learning Under Privileged Information (LUPI) paradigm [9], [10], [11] and tailor its components for effective use within object detection. The LUPI paradigm addresses problems where information asymmetry exists between training and testing: supplementary information is available during training but not during inference. By leveraging highly informative data streams that are inaccessible at test time, LUPI significantly reduces the requirement for large annotated datasets without sacrificing model accuracy. Privileged information can take many forms, SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 2 including depth cues, saliency maps, high-resolution imagery, or domain-specific annotations [12], [13], [14]. By incorporating such signals, models learn richer feature representations during training, improving generalization and accelerating convergence while maintaining unchanged inference requirements (see Figure 1)."
        },
        {
            "title": "Our work is novel",
            "content": "is several ways. First, we propose and develop general methodology for injecting privileged information into any deep learning-based object detector. The proposed methodology is model-agnostic and not restricted by architectural choices. Second, we investigate the impact of our methodology across five open-source state-of-the-art pretrained object detection models using multiple UAV-based litter detection datasets and the Pascal VOC benchmark. Third, we build upon and significantly extend our earlier work [8] by analyzing performance across object scales, standard COCO metrics, and different forms of privileged information including depth, saliency, and their combinationswhile also examining practical factors such as inference time and model size. Finally, through extensive experimental validation, we demonstrate the importance of privileged information for boosting model performance and provide deeper insights into the viability of our LUPI-based approach in generic object detection, assessing both the scientific and practical implications of this paradigm. II. RELATED WORK Object detection is complex problem that involves both classification and localization [7]. The field has rich history, evolving from early works using traditional feature matching [15], [16] and machine learning techniques [17], [18] to the incorporation of deep learning methods [19], which currently provide state-of-the-art performance. This section reviews related studies on deep learning-based object detection and LUPI for computer vision applications. A. Deep Learning for Object Detection Object detection is supervised learning task that encompasses several key challenges. These include detecting objects against complex backgrounds and interferences, accounting for scale variability, handling occlusion, mitigating class imbalance and dataset bias, and detecting small objects [6], [20]. All of these challenges require robust learning algorithms. Current state-of-the-art object detection models leverage various deep learning architectures that produce outputs in the form of bounding boxes accompanied by categorical labels. These networks are generally categorized into four groups: onestage, two-stage, transformer-based, and other deep learning approaches. One-stage detectors solve the localization and classification problems using single network. Popular examples include YOLO (You Only Look Once) [1] and SSD (Single Shot MultiBox Detector) [21]. Two-stage detectors use separate networks to perform localization and classification, with examples such as Faster R-CNN [2] and Mask R-CNN [22]. Transformer-based detectors, such as DETR [4] and RT-DETR [5], leverage self-attention mechanisms for object detection. Other deep learning approaches, like CenterNet [23], or SAHI [24] incorporate reinforcement learning techniques. These diverse approaches highlight the range of strategies developed to tackle the challenges of object detection. Onestage detectors prioritize speed and efficiency, while two-stage detectors excel in accuracy for complex scenes. Transformerbased models offer flexibility in modeling object relationships, demonstrating that different architectural paradigms address different aspects of the detection problem. B. Learning Using Privileged Information in Computer Vision The use of LUPI in computer vision remains relatively underexplored, particularly within object detection. Our earlier work [8] represents one of the first contributions in this area. Early efforts in the literature focused on object localization tasks. Feyereisl et al. (2014) [25] used segmentation masks and SURF features as privileged information for object localization using the Structural SVM+ algorithm on the Caltech-UCSD Birds dataset [26]. Improvements were marginal, and deep learning models were not yet widely adopted at the time. Similarly, Sun et al. (2018) [27] examined object localization with privileged information on the same dataset, achieving limited improvements. LUPI has seen broader exploration in image classification. Sharmanska et al. [28], [29] investigated semantic attributes, bounding boxes, textual descriptions, and annotator rationale as privileged information, showing measurable improvements within the SVM+ framework. Wang et al. [13] incorporated privileged data, such as high-resolution images and tags, into multi-label classification, demonstrating that leveraging such information enhances performance. Makantasis et al. [30], [31] used audio and physiological features as privileged information for developing vision-based models of affect in an attempt to bridge the gap between in-vitro and in-vivo affect modeling tasks. LUPI is closely related to knowledge distillation [32], technique that has gained increasing popularity in recent years, particularly within computer vision [33]. In this framework, high-capacity network (the teacher) transfers information to smaller network (the student), enabling the student to learn richer and more informative representations [34]. Hintons concept of generalized distillation [35] formalizes this process. Computer vision applications commonly employ methods such as feature and logit matching between student and teacher networks, while localization distillation specifically targets spatially informative regions, helping student models focus on the most relevant areas and thereby improving detection performance [33], [34], [35]. However, knowledge distillation and LUPI differ fundamentally in their objectives and information requirements. The main objective of knowledge distillation is to build compact student that performs on par with much larger teacher model, where both models use identical input information. In contrast, LUPI [9], [10], [11] aims not to compress large network but to transfer knowledge from teacher model trained using highly informative privileged information to student model that makes predictions in the absence of that privileged information. Thus, while knowledge distillation addresses model SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 3 Figure 2. Detailed architecture of the training setup. The teacher network receives both RGB images and privileged input channels, producing richer intermediate representations. The student network only processes RGB images, but is trained with additional supervision through knowledge distillation from the teacher. baseline RGB-only model is included for comparison. The student demonstrates refined predictions relative to the baseline. compression, LUPI addresses information asymmetry between training and inference. III. METHODOLOGY To test our hypothesis that highly descriptive, fine-grained information can be automatically constructed and leveraged during training to improve object detector performance, we follow the work in [11] and employ teacher-student framework. This section formalizes the problem and describes the proposed approach. A. Problem Formulation We consider supervised object detection setting where each training sample consists of standard input image X, additional privileged information available only during training, and the corresponding ground-truth label , which includes bounding box and class label per depicted object. The training dataset is therefore set of triplets Dtrain = {(xi, , yi)}N i=1, with the ground-truth label yi for image to be the set yi = {(bj, lj)}M j=1. (1) (2) In (2), stands for the number of depicted objects in the image. Our objective is to estimate function fw : parameterized by such that := w(X, , ). (3) (4) In our case, the function fw is implemented by neural network and the parameters correspond to the networks weights. Equations (3) and (4) demonstrate that the network makes predictions using only X, while its parameters are estimated using not only and , but also the additional privileged information . B. Proposed Approach We leverage privileged information by adopting the teacherstudent paradigm. The teacher network fteacher : has access to both standard and privileged inputs, allowing it to learn richer and more informative intermediate : representations. In contrast, observes only the standard inputs and has no direct access to the privileged information. During training, however, the student is encouraged to replicate the teachers latent representations at an intermediate layer l, hereby benefiting indirectly from the additional privileged context. the student network fstudent"
        },
        {
            "title": "Both fteacher and fstudent are implemented as neural networks",
            "content": "composed of layers: fteacher := (t) (5) fstudent := (s) 2 (t) 2 (s) 1 (t) 1 (s) Here, denotes function composition, and (t) represent the i-th layer of the teacher and student networks, respectively. The l-th layer of both networks is constrained to have the same number of hidden neurons, enabling direct comparison between their latent representations. (t) , (s) . , (s) (6) For each triplet (xi, , yi) Dtrain, the student is trained to align its latent features (s) (xi) with the corresponding teacher features (t) ). This alignment forms the basis of the knowledge transfer process, allowing the student to approximate the teachers richer intermediate representations while relying solely on standard inputs. (xi, l SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 4 Figure 3. saliency, depth, fusion, and bounding box mask representations. The bounding box mask yielded the highest improvement in detection accuracy. Investigation of different forms of privileged information using the RetinaNet model on the SODA 1-metre dataset. The comparison includes The student is optimised with combined loss function that balances standard detection supervision with knowledge transfer from the teacher: LS = (1 α) Ldet + α D(f (t) , (s) ), (7) where Ldet is the standard detection loss and D() measures the cosine distance between teacher and student feature vectors: D(f (t) , (s) ) = 1 (s) (t) (cid:13) (cid:13) (cid:13) (cid:13)f (s) (cid:13)f (t) (cid:13) (cid:13) (cid:13) (cid:13) (8) (cid:13) (cid:13) (cid:13) In (7), α [0, 1] controls the relative weight between supervision from ground-truth labels and guidance from the teacher. The success of the teacher-student framework is closely tied to the type and quality of privileged information used during training. We next describe how this information is constructed and integrated to bolster the learning process. C. Privileged Information for Object Detection Selecting effective forms of privileged information for object detection requires cues that meaningfully contribute to both localisation and classification. Cognitive studies suggest that humans rely on physical reasoning, such as estimating an objects center of mass, when recognising objects [36]. This observation implies that structured spatial signals can enhance detection models. In deep learning, prior work has investigated auxiliary sources such as saliency [37], [38], [39] and depth maps [40], [41], with saliency shown to correlate more strongly with detection performance [42]. However, directly adding these signals as input channels has yielded limited performance gains. Building on these findings, this study systematically evaluates multiple forms of privileged information within to enhance detection accuracy, yet the teacherstudent training framework. Both saliencyand depth-based representations were explored for their potentheir contributions tial remained modest. Among the investigated alternatives, the previously proposed bounding box mask [8] achieved the highest improvement. Figure 3 illustrates the different forms of privileged information alongside corresponding teacher model performance. The mask formulation effectively guides the networks attention toward object regions by embedding both localisation and class cues within single, structured representation. Each mask image consists of black background with bounding boxes filled using grayscale values proportional to their class labels. This compact yet informative representation provided the best balance between simplicity, interpretability, and overall detection accuracy. The mask is generated using ground-truth annotations available only during training, thereby satisfying the LUPI condition. Bounding boxes are drawn in descending size order to minimize occlusion. Although polygonal and segmentation masks [43], [22], [44] were also considered, bounding box masks were ultimately preferred for their simplicity, consistency with existing datasets, and stable performance across experiments. IV. IMPLEMENTATION Having outlined the theoretical basis of the proposed LUPIthe next section describes its based detection framework, practical implementation and training setup. A. TeacherStudent Framework for LUPI To leverage the privileged information introduced in the previous section, teacherstudent framework was implemented SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 5 (see Figure 2). The teacher network receives RGB images together with the additional privileged input, such as bounding box masks. To accommodate this extra channel, the teachers input layer is extended to four channels, with the added weights initialized using Kaiming Normal initialization [45], while the remaining layers retain pre-trained COCO weights. This adaptation enables the teacher to exploit richer feature representations without modifying the overall architecture. The student network processes only RGB images but otherwise mirrors the teacher architecture. Its training objective combines detection losses with knowledge transfer term (see (7)), computed by comparing the students and teachers feature representations at the final backbone layer using (8). weighting parameter α controls the balance between direct supervision and teacher guidance. During inference, the student operates exclusively on RGB inputs while still benefiting from the knowledge transferred from the teacher. B. Object Detection Models and Training Protocol Building on the teacherstudent framework outlined in the previous section, all models were implemented using opensource architectures from the torchvision1 library. The complete training pipeline is publicly available on GitHub2. Five object detection models were selected for evaluation: Faster R-CNN [2], SSD [21], RetinaNet [3], SSDLite [46], and FCOS [47]. These architectures cover both one-stage and two-stage detection paradigms and represent diverse range of computational complexities. The teacher networks were adapted to accept an additional privileged input channel, while the student networks retained standard RGB inputs. Following (7), knowledge transfer was performed using features from the final backbone layer, which captures semantically rich representations [48]. Specifically, the last convolutional layer before the FPN was used for Faster R-CNN, FCOS, and RetinaNet, while for SSD and SSDLite, the final convolutional layer before the auxiliary heads was selected (see Figure 2). This design ensures that performance improvements can be attributed to the integration of privileged information and knowledge transfer, rather than architectural changes. To maintain consistency across experiments, identical training, preprocessing, and postprocessing procedures were applied to all models. Models were trained for 100 epochs using the Adam optimizer with fixed learning rate of 1 103, employing early stopping and checkpointing based on validation loss. Input images, including privileged channels, were normalised using min-max scaling, resized to 800800 pixels, and standardised per channel to zero mean and unit variance. Non-maximum suppression with an IoU threshold of 0.5 was applied to final predictions to remove redundant detections. By standardizing the architectures, training setup, and preprocessing pipelines, this implementation isolates the impact of privileged information and knowledge transfer, ensuring fair and interpretable evaluation of their contribution to object detection performance. 1https://docs.pytorch.org/vision/main/models.html#object-detection 2https://github.com/mbar0075/lupi-for-object-detection V. EVALUATION STRATEGY This section presents the experimental evaluation of the proposed LUPI-based object detection framework. It outlines the datasets, metrics, and experimental procedures used to assess the robustness of the approach, followed by performance analysis across different models and conditions. A. Datasets and Metrics Having defined the LUPI framework and integrated privileged information into the teacherstudent setup, the evaluation focused on UAV-based litter detection [49], [50]a challenging and practical application due to small object sizes, complex backgrounds, and high scene variability. Publicly including SODA [49], BDW [51], and available datasets, UAVVaste [52], were selected for their high-quality annotations and real-world relevance. Subsets of SODA were used for within-dataset experiments to analyze model performance in controlled setting, while cross-dataset evaluations on BDW and UAVVaste assessed generalization to unseen environments. Additionally, the Pascal VOC 2012 dataset [53] was included to evaluate the general applicability of the proposed approach across broader range of object categories. For each model architecture, baseline RGB-only detectors were compared against their LUPI teacherstudent counterparts. Teacher networks were also evaluated separately to confirm the contribution of privileged information during training. The study further examined runtime performance, assessing whether student models improved detection accuracy without incurring additional inference cost. Ablation experiments were conducted to investigate the impact of the loss balancing parameter α, while qualitative analysis used Grad-CAM visualizations [54] to inspect model attention. Evaluation metrics included standard object detection measuresmAP, precision, recall, F1 score, and mAR along with COCO-style metrics [55] to assess detection quality across different object scales. This evaluation strategy enabled consistent and controlled analysis of how privileged information enhances accuracy, generalization, and efficiency across diverse detection models. B. Withinand Cross-Dataset Experiments Within-dataset experiments focused on UAV-based litter detection using subsets of the SODA dataset to evaluate model performance under controlled conditions. Three scenarios were explored: (i) binary litter detection at 1-metre altitude without tiling, (ii) binary detection across multiple altitudes with 33 tiling, and (iii) multi-label detection across altitudes also with 33 tiling. For each scenario, the five selected object detection architectures were trained as both teacher and student models. The parameter α was varied from 0 to 1 in steps of 0.25, following the methodology of [48], to analyze the effect of teacher supervision strength. This experimental design enabled systematic evaluation of how the LUPI framework enhances student performance. Cross-dataset experiments evaluated the generalization capacity of models trained on SODA when applied to other litter datasets. For BDW, models trained on SODA at an SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 6 Table COMPARISON OF TEACHER MODEL PERFORMANCE ACROSS ALL EXPERIMENTS USING COCO METRICS (2 DECIMAL PLACES). INCLUDES WITHIN-DATASET, CROSS-DATASET, AND PASCAL VOC 2012 EVALUATIONS. FASTER R-CNN SHOWS THE HIGHEST AVERAGE PERFORMANCE, WITH RETINANET AND FCOS PERFORMING SIMILARLY, WHILE SSD AND SSDLITE EXHIBIT LOWER RESULTS. NOTE THAT THE SODA 1-METRE SUBSET CONTAINS NO SMALL OBJECTS. FOR CROSS-DATASET EVALUATIONS, PRIVILEGED INFORMATION WAS ALSO GENERATED, AND THE TEACHER MODELS WERE EVALUATED ACCORDINGLY. Model Dataset mAP mAP mAR mAR mAP @50 @75 @Small @Medium @Large @1 @10 @100 @Small @Medium @Large RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite SODA at 1-metre SODA Tiled Binary SODA Tiled Multi-label BDW UAVVaste Pascal VOC 2012 0.94 0.96 0.96 0.78 0.61 0.90 0.89 0.96 0.49 0.18 0.88 0.91 0.95 0.36 0.11 0.46 0.49 0.48 0.55 0.23 0.40 0.42 0.44 0.13 0.01 0.77 0.80 0.77 0.42 0.49 0.98 0.98 0.99 0.96 0.73 0.95 0.94 0.99 0.62 0.23 0.92 0.95 0.99 0.49 0.13 0.96 0.96 0.97 0.95 0.37 0.78 0.71 0.84 0.24 0.03 0.86 0.88 0.91 0.56 0. 0.96 0.97 0.98 0.94 0.72 0.94 0.93 0.98 0.59 0.19 0.91 0.94 0.98 0.45 0.13 0.32 0.43 0.34 0.62 0.27 0.37 0.47 0.44 0.12 0.01 0.79 0.82 0.82 0.49 0.54 0.78 0.80 0.92 0.19 0.00 0.75 0.83 0.91 0.15 0.00 0.00 0.10 0.20 0.00 0.00 0.31 0.36 0.37 0.06 0.00 0.28 0.56 0.51 0.00 0.00 0.93 0.93 0.98 0.78 0.00 0.98 0.95 0.99 0.77 0.05 0.98 0.97 0.98 0.55 0.00 0.35 0.37 0.40 0.45 0.01 0.72 0.74 0.75 0.45 0.06 0.63 0.67 0.66 0.06 0.00 0.95 0.97 0.96 0.78 0.73 0.97 0.97 0.99 0.75 0.80 0.98 0.97 0.98 0.55 0.46 0.52 0.55 0.52 0.58 0.31 0.93 0.90 0.85 0.80 0.20 0.80 0.83 0.79 0.48 0.58 0.63 0.63 0.63 0.54 0.48 0.34 0.34 0.35 0.27 0.17 0.66 0.68 0.70 0.33 0.13 0.38 0.39 0.38 0.43 0.21 0.13 0.14 0.15 0.08 0.01 0.60 0.61 0.59 0.41 0.46 0.96 0.97 0.98 0.81 0.63 0.83 0.82 0.87 0.51 0.19 0.89 0.92 0.96 0.41 0.13 0.54 0.56 0.54 0.59 0.24 0.44 0.46 0.46 0.15 0.01 0.81 0.84 0.82 0.48 0. 0.96 0.97 0.98 0.81 0.63 0.91 0.90 0.97 0.51 0.19 0.89 0.92 0.96 0.41 0.13 0.54 0.56 0.54 0.59 0.24 0.47 0.46 0.46 0.15 0.01 0.81 0.84 0.82 0.48 0.55 0.84 0.83 0.94 0.21 0.01 0.77 0.85 0.93 0.16 0.00 0.00 0.10 0.20 0.00 0.00 0.41 0.39 0.39 0.08 0.00 0.30 0.57 0.56 0.00 0.00 0.94 0.94 0.99 0.82 0.00 0.98 0.97 0.99 0.80 0.07 0.98 0.98 0.99 0.59 0.00 0.40 0.43 0.41 0.48 0.01 0.74 0.76 0.77 0.50 0.07 0.67 0.72 0.72 0.07 0.00 0.97 0.98 0.97 0.81 0.77 0.98 0.98 0.99 0.80 0.83 0.99 0.98 0.99 0.62 0.54 0.59 0.61 0.59 0.64 0.34 0.95 0.90 0.85 0.80 0.20 0.84 0.86 0.85 0.56 0.65 altitude of 1-metre were directly tested without retraining, while UAVVaste evaluations used models trained on 33 tiled SODA images across multiple altitudes. All experiments focused on binary detection, enabling analysis of how effectively LUPI-trained students adapt to unseen environments and varying object distributions. These evaluations also examined runtime considerations, emphasizing performance gains achieved without increasing model size or inference time. C. Pascal VOC 2012 Experiment To assess the broader applicability of the proposed LUPI framework, experiments were conducted on the Pascal VOC 2012 dataset, which includes multi-label detection across 20 diverse object categories. This evaluation examined whether student models could effectively leverage teacher guidance in complex scenes containing multiple objects and varying scales, extending the analysis beyond UAV-specific litter detection. Baseline RGB-only models, LUPI student models, and teacher networks were compared using identical architectures and training protocols, ensuring controlled assessment of generalization across more heterogeneous set of classes. D. Ablation Study on TeacherStudent Balance Ablation studies investigated the effect of the balancing parameter α on student performance, which regulates the contribution of teacher supervision relative to ground-truth labels. Values of ranging from 0 to 1, in increments of 0.25 as in [48], were tested across the SODA dataset (binary and multilabel scenarios) and Pascal VOC 2012 to determine optimal weighting for different tasks. These experiments provided insight into how the degree of teacher reliance influences learning dynamics, guiding the selection of α values that maximize performance while avoiding excessive dependence on privileged information. VI. RESULTS AND DISCUSSION Based on the evaluation setups described above, results are presented collectively to enable direct comparison across experiments, covering performance, ablation, interpretability, and efficiency. A. Teacher Model Performance Having established the optimal form of privileged information (Figure 3), it is essential to assess its impact across SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 7 Figure 4. Comparative analysis of baseline and best LUPI-trained student models across all datasets for the five architectures, shown as multi-radar graph. The figure highlights notable improvements in strict mAP and F1 score, with the largest boosts observed in within-dataset evaluations, while other datasets show smaller yet meaningful improvements using identical architectures. different architectures and datasets. Table summarizes teacher model performance for all experimentswithin-dataset, crossdataset, and Pascal VOC 2012covering the five selected detection architectures. The results indicate that incorporating informative privileged input substantially improves teacher accuracy, with strict mAP and mAR values approaching 1, demonstrating high reliability. Although the improvement is less pronounced for small objects, reflecting the difficulty of this category, performance for medium and large objects remains consistently strong, suggesting that privileged signals help the model focus on more easily detectable targets. While performance decreases slightly under more challenging conditions, such as cross-dataset generalization and multilabel detection, teacher models still maintain robust accuracy. Overall, Faster R-CNN achieves the highest average mAP, followed closely by RetinaNet and FCOS, whereas SSD and SSDLite exhibit comparatively lower performance. B. Baseline vs. Student Model Comparison Following the teacher model evaluation, we assess the performance of LUPI-trained student models relative to their baseline RGB-only counterparts. Figure 4 presents these comparisons across all datasets and detection architectures. Overall, student models show consistent gains over the baselines, particularly in strict mAP and F1 score metrics. The most substantial improvements are observed in within-dataset UAV litter detection, with smaller yet meaningful gains in crossdataset evaluations. Faster R-CNN, FCOS, and RetinaNet benefit most from teacher guidance in UAV-based scenarios, whereas SSD and SSDLite exhibit clearer improvements on Pascal VOC. Although relative gains diminish in more demanding taskssuch as multi-label detection and crossdataset generalizationthe results confirm that LUPI effectively enhances student performance across architectures and domains. C. The Effect of Balancing the α Parameter While the teacher models exhibited high accuracy and the student models consistently outperformed their baselines, it remains important to analyze how varying reliance on teacher guidance affects student learning. This dependency is governed by the parameter α, which controls the weight of the teachers contribution during knowledge transfer. Experiments were conducted with α {0, 0.25, 0.5, 0.75, 1}, where α = 0 corresponds to the baseline and α = 1 to full teacher supervision. Figure 5 summarizes the results across all datasets and architectures using COCO metrics, with red downward arrows indicating top performance in strict mAP@5095. Intermediate values of α (0.25 and 0.5) generally yield the best performance, balancing learning from ground-truth labels and teacher knowledge, while α = 0.75 occasionally performs well and α = 1 tends to underperform. These trends are consistent across datasets and models. Smaller objects continue to present challenges, showing limited improvement, whereas medium and large objects benefit more significantly, reflecting the richer semantic features transferred from the teacher. It is also observed that SSD and SSDLite perform comparatively worse on certain datasets, consistent with their architectural limitations. In the case of Faster R-CNN, only the Pascal VOC baseline outperformed its student counterpart, likely because the teachers additional region proposals introduced ambiguity in supervision limiting the students gains, though this difference remains marginal. D. Interpretability Analysis To further understand the performance improvements observed in LUPI-trained student models, we performed interpretability analysis using Grad-CAM visualisations [54] on the final backbone layer. Figure 6 shows results for the SODA 1-metre dataset, comparing baseline and student models. The visualisations show that LUPI-trained student SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 8 Figure 5. Ablation study results across all datasets and experiments using COCO metrics. Baseline model corresponds to α = 0; other lines represent student models. Red downward arrows indicate top performance in the strict map@5095 metric. Best results are generally observed for α = 0.25 and 0.5, with α = 0.75 also performing well, while α = 1 shows lower average performance. models focus sharply on litter objects, producing higherconfidence detections with fewer misclassifications, whereas the baseline models attention is less concentrated and often highlights irrelevant areas in the background. This targeted attention aligns with the improvements observed in strict mAP and F1 score, the student models are not only performing better quantitatively but also learning more meaningful and task-relevant feature representations. Similar patterns were observed across the other datasets, though these results are omitted for brevity, indicating that the effect is consistent. indicating that E. Performance and Efficiency Analysis While we have consistently shown that the proposed LUPIbased object detection approach improves accuracy, one might question whether these results come at substantial computational cost. primary limitation is the increased training time, as both teacher and student model must be trained, effectively doubling the workload, as shown in Figure 7. However, inference is typically performed far more frequently than training in practical deployments. Table II shows that the baseline and LUPI-trained student models are nearly identical in size, number of parameters, GFLOPS, and FPS, with only minor variations in speed that were consistent across multiple runs. This demonstrates that while training requires more time, inference speed, model size, and efficiency remain unaffected, SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 9 Figure 6. Visual comparison of model predictions and interpretability results on the SODA 1-metre dataset experiment. (a) Baseline detection results. (b) Baseline Grad-CAM visualisation. (c) Best LUPI-trained student detection results. (d) Best Student Grad-CAM visualisation. The LUPI-trained student produces more accurate litter predictions than the baseline. For the Grad-CAM visualisations applied to the respective distillation layers, the students attention is more concentrated on litter objects, whereas the baseline exhibits more diffuse activation across the background. Table II RUNTIME COMPARISON OF BASELINE AND STUDENT MODELS ON PASCAL VOC 2012, SHOWING MODEL TYPE, SIZE, PARAMETERS, GFLOPS, AND FPS. PERFORMANCE IMPROVEMENTS FOR STUDENTS COME WITH NO ADDITIONAL INFERENCE COST. Model Type Size (MB) Parameters (M) GFLOPS FPS Baseline Student RetinaNet FCOS Faster R-CNN SSD SSDLite RetinaNet FCOS Faster R-CNN SSD SSDLite 124.22 122.48 157.92 100.27 9.42 124.22 122.48 157.92 100.27 9.42 32.56 32.11 41.40 26.29 2.47 32.56 32.11 41.40 26.29 2. 265.10 252.94 268.75 62.94 0.95 265.10 252.94 268.75 62.94 0.95 39.74 39.55 30.66 67.44 36.74 38.00 34.65 30.39 67.67 36.75 Figure 7. Comparison of training times on the Pascal VOC 2012 dataset, highlighting the increased duration for LUPI teacherstudent training. ensuring that the improved accuracy of LUPI-trained students can be fully utilised in deployment. For brevity, only Pascal VOC 2012 results are shown, though similar trends were observed across other datasets. F. Discussion Object detection is multifaceted problem, as different architectures handle localisation and classification in distinct ways. Two-stage detectors with region proposal networks and feature pyramids leverage spatial and semantic cues, while single-stage models with auxiliary layers or lightweight designs depend more on end-to-end feature extraction, affecting how they respond to additional guidance. Across our experiments, integrating privileged information through the LUPI framework consistently improved student learning, although the magnitude and nature of these improvements varied across models and datasets. Faster R-CNN, FCOS, and RetinaNet were more effective in UAV litter detection, reflecting their ability to utilise spatial context, while SSD and SSDLite performed comparatively better on Pascal VOC tasks, highlighting differences in feature aggregation and receptive fields. Ablation studies show that moderate teacher learning by balancing reliance weighting supports student on ground-truth supervision with teacher guidance, whereas excessive dependence can occasionally confuse the student in complex multi-label scenarios. Grad-CAM visualisations indicate that LUPI-trained students focus more sharply on relevant objects, producing more discriminative and semanSUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 10 tically coherent representations, while baseline models show more diffuse attention. Importantly, these improvements are achieved with minimal architectural changes, demonstrating that the framework complements the intrinsic characteristics of each model, and inference speed and efficiency remain consistent. Overall, the results illustrate nuanced interaction between architecture, dataset characteristics, and teacher guidance, emphasising that the benefits of privileged information depend on both model design and task complexity, with LUPI serving as an augmenter of the capabilities already present in the underlying model. VII. PRACTICAL APPLICATIONS The LUPI paradigm within object detection, as presented in this study, can be applied to wide range of object detection and geolocation systems and is especially well-suited for lightweight deployment. By leveraging compact models, it reduces inference costs while maintaining high accuracy, enabling efficient processing even on resource-constrained platforms. This makes the approach suitable for real-world applications that demand fast, reliable, and consistent detection, such as UAV monitoring, traffic analysis, and surveillance systems, where both speed and precision are critical. VIII. CONCLUSION This study investigated the LUPI paradigm within object detection through an extensive series of experiments across multiple architectures and datasets, evaluated using strict COCO metrics. The results consistently demonstrate that incorporating privileged information during training enhances detection accuracy, increasing model depth, parameter count, or inference time. The student models trained under this paradigm remained identical to their baseline counterparts in architecture and efficiency, yet achieved higher accuracy through the use of additional teacher guidance during training. improving accuracy without Some limitations remain. The generation of privileged information can be affected by overlapping objects of the same category, occlusions from larger bounding boxes, and limited color differentiation within mask representations. Moreover, the need to train both teacher and student model introduces longer training phase compared to conventional single-model setups. Future avenues extending this work include the integration of the approach with more recent detection architectures such as YOLOv12 [56] and RF-DETR [57], the exploration of richer and more diverse forms of privileged information such as semantic maps or attention-based cues, and the adaptation of the framework to related tasks like object segmentation. Overall, the findings of this study reaffirm that LUPI provides practical and effective strategy for enhancing object detection performance in computationally constrained environments, inference efficiency while achieving maintaining identical higher accuracy."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, 2016, cite arxiv:1506.02640. [2] S. Ren, K. He, R. B. Girshick, and J. Sun, Faster R-CNN: towards real-time object detection with region proposal networks, CoRR, vol. abs/1506.01497, 2015. [3] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Dollar, Focal loss for dense object detection, in 2017 IEEE International Conference on Computer Vision (ICCV). Venice, Italy: IEEE Computer Society, 2017, pp. 29993007. [4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, End-to-end object detection with transformers, CoRR, vol. abs/2005.12872, 2020. [5] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen, Detrs beat yolos on real-time object detection, 2024. [6] Z. Li, Y. Dong, L. Shen, Y. Liu, Y. Pei, H. Yang, L. Zheng, and J. Ma, Development and challenges of object detection: survey, Neurocomputing, vol. 598, p. 128102, 2024. [7] D. Prasad, Survey of the problem of object detection in real images, International Journal of Image Processing (IJIP), vol. 6, p. 441, 12 2012. [8] M. Bartolo, K. Makantasis, and D. Seychell, Learning using privileged information for litter detection, in Proceedings of the 2025 13th European Workshop on Visual Information Processing (EUVIP), Valletta, Malta, 2025. [9] V. Vapnik and A. Vashist, new learning paradigm: Learning using privileged information, Neural networks, vol. 22, no. 5-6, pp. 544557, 2009. [10] D. Pechyony and V. Vapnik, On the theory of learnining with privileged information, Advances in neural information processing systems, vol. 23, 2010. [11] V. Vapnik, R. Izmailov et al., Learning using privileged information: Similarity control and knowledge transfer. J. Mach. Learn. Res., vol. 16, no. 1, pp. 20232049, 2015. [12] V. Vapnik and A. Vashist, new learning paradigm: Learning using privileged information, Neural networks : the official journal of the International Neural Network Society, vol. 22, pp. 54457, 07 2009. [13] S. Wang, S. Chen, T. Chen, and X. Shi, Learning with privileged information for multi-label classification, Pattern Recognition, vol. 81, pp. 6070, 2018. [14] V. Sharmanska, N. Quadrianto, and C. H. Lampert, Learning to transfer privileged information, CoRR, vol. abs/1410.0389, 2014. [15] H. Bay, T. Tuytelaars, and L. Van Gool, SURF: Speeded Up Robust Features, Proceedings IEEE European Conference on Computer Vision, pp. 404417, 2006. [16] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, Orb: An efficient alternative to sift or surf, in 2011 International Conference on Computer Vision, 2011, pp. 25642571. [17] P. Viola and M. Jones, Rapid object detection using boosted cascade of simple features, in Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001, vol. 1. IEEE, 2001, pp. I511. [18] H. Bhatt, V. Shah, K. Shah, R. Shah, and M. Shah, State-of-the-art machine learning techniques for melanoma skin cancer detection and classification: comprehensive review, Intelligent Medicine, vol. 3, no. 3, pp. 180190, 2023. [19] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature, vol. 521, pp. 43644, 05 2015. [20] G. Chen, H. Wang, K. Chen, Z. Li, Z. Song, Y. Liu, W. Chen, and A. Knoll, survey of the four pillars for small object detection: Multiscale representation, contextual information, super-resolution, and region proposal, IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52, no. 2, pp. 936953, 2022. [21] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg, SSD: single shot multibox detector, in Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part I, ser. Lecture Notes in Computer Science, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Springer, 2016, vol. 9905, pp. 2137. [22] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, Mask R-CNN, in 2017 IEEE International Conference on Computer Vision (ICCV). Venice, Italy: IEEE Computer Society, 2017, pp. 29802988. [23] X. Zhou, D. Wang, and P. Krahenbuhl, Objects as points, CoRR, 2019. [24] F. C. Akyon, O. Altinuc, and S. Temizel, Slicing aided hyper inference and fine-tuning for small object detection, in 2022 IEEE International SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 11 Conference on Image Processing (ICIP). 2022. Shanghai, China: IEEE, [25] J. Feyereisl, S. Kwak, J. Son, and B. Han, Object localization based on structural svm using privileged information, in Advances in Neural Information Processing Systems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27. Curran Associates, Inc., 2014. [26] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, The caltech-ucsd birds-200-2011 dataset, California Institute of Technology, Tech. Rep. CNS-TR-2011-001, 2011. [27] S. Sun, C. Zhang, and Y. Tian, new method for structured learning with privileged information, in Computational Science ICCS 2018, Y. Shi, H. Fu, Y. Tian, V. V. Krzhizhanovskaya, M. H. Lees, J. Dongarra, and P. M. A. Sloot, Eds. Cham: Springer International Publishing, 2018, pp. 453461. [28] V. Sharmanska, N. Quadrianto, and C. H. Lampert, Learning to rank using privileged information, in 2013 IEEE International Conference on Computer Vision, 2013, pp. 825832. [29] , Learning to transfer privileged information, arXiv preprint arXiv:1410.0389, 2014. [Online]. Available: https://arxiv.org/abs/1410. 0389 [30] K. Makantasis, D. Melhart, A. Liapis, and G. N. Yannakakis, Privileged information for modeling affect in the wild, in 2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 2021, pp. 18. [31] K. Makantasis, K. Pinitas, A. Liapis, and G. N. Yannakakis, From the lab to the wild: Affect modeling via privileged information, IEEE Transactions on Affective Computing, vol. 15, no. 2, pp. 380392, 2023. [32] D. Lopez-Paz, L. Bottou, B. Scholkopf, and V. Vapnik, Unifying distillation and privileged information, arXiv preprint arXiv:1511.03643, 2015. [33] G. Habib, T. jan Saleem, S. M. Kaleem, T. Rouf, and B. Lall, comprehensive review of knowledge distillation in computer vision, 2024. [34] Z. Zheng, R. Ye, Q. Hou, D. Ren, P. Wang, W. Zuo, and M.-M. Cheng, Localization distillation for object detection, 2022. [35] G. E. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, CoRR, vol. abs/1503.02531, 2015. [36] T. Boger and T. Ullman, What is where: Physical reasoning informs object location, Open Mind (Cambridge), vol. 7, pp. 130140, 5 2023. [37] L. Itti, C. Koch, and E. Niebur, model of saliency-based visual attention for rapid scene analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 20, no. 11, pp. 12541259, 11 1998. [38] A. Linardos, M. Kummerer, O. Press, and M. Bethge, Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling, CoRR, vol. abs/2105.12441, 2021. [39] D. Seychell and C. J. Debono, Ranking regions of visual saliency in rgb-d content, in 2018 International Conference on 3D Immersion (IC3D), 2018. [40] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, 2024. [41] R. Ranftl, A. Bochkovskiy, and V. Koltun, Vision transformers for dense prediction, CoRR, vol. abs/2103.13413, 2021. [42] M. Bartolo and D. Seychell, Correlation of object detection performance with visual saliency and depth estimation, 2024. [43] D. Seychell, M. Kenely, M. Bartolo, C. J. Debono, M. Bugeja, and M. Sacco, Efficient automatic annotation of binary masks for enhanced training of computer vision models, in 2023 IEEE International Symposium on Multimedia (ISM), 2023, pp. 256259. [44] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollar, and R. Girshick, Segment anything, 2023. [45] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, CoRR, vol. abs/1502.01852, 2015. [46] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks, 2019. [47] Z. Tian, C. Shen, H. Chen, and T. He, FCOS: Fully Convolutional OneStage Object Detection, in 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Seoul, Korea (South): IEEE, 2019, pp. 96269635. [48] K. Makantasis, K. Pinitas, A. Liapis, and G. N. Yannakakis, From the lab to the wild: Affect modeling via privileged information, IEEE Transactions on Affective Computing, vol. 15, no. 2, pp. 380392, 2024. [49] D. Pisani, D. Seychell, C. J. Debono, and M. Schembri, Soda: dataset for small object detection in uav captured imagery, in 2024 IEEE International Conference on Image Processing (ICIP), 2024, pp. 151157. [50] M. Cordova, A. Pinto, C. C. Hellevik, S. A.-A. Alaliyat, I. A. Hameed, H. Pedrini, and R. d. S. Torres, Litter detection with deep learning: comparative study, Sensors, vol. 22, no. 2, 2022. [51] J. Wang, W. Guo, T. Pan, H. Yu, L. Duan, and W. Yang, Bottle detection in the wild using low-altitude unmanned aerial vehicles, in 2018 21st International Conference on Information Fusion (FUSION), 2018, pp. 439444. [52] M. Kraft, M. Piechocki, B. Ptak, and K. Walas, Autonomous, onboard vision-based trash and litter detection in low altitude aerial images collected by an unmanned aerial vehicle, Remote Sensing, vol. 13, no. 5, 2021. The 2012 [53] M. Everingham, L. Van Gool, C. K. J. Winn, I. Williams, PASCAL Visual Object Classes http://www.pascaland A. Zisserman, Challenge network.org/challenges/VOC/voc2012/workshop/index.html, 2012. [54] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra, Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization, CoRR, vol. abs/1610.02391, 2016. (VOC2012) Results, [55] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft COCO: common objects in context, in Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, ser. Lecture Notes in Computer Science, D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, Eds., vol. 8693. Cham, Switzerland: Springer, 2014, pp. 740755. [56] Y. Tian, Q. Ye, and D. Doermann, Yolov12: Attention-centric real-time object detectors, 2025. [57] I. Robinson, P. Robicheaux, and M. Popov. (2025) Rf-detr. https://github. com/roboflow/rf-detr. SOTA Real-Time Object Detection Model. Matthias Bartolo received the B.Sc. IT degree (Hons.) in Artificial Intelligence from the University of Malta with First Class Honours, ranking first in his cohort, and was recognised on the Deans List for outstanding academic achievement. He subsequently obtained the M.Sc. degree from the University of Malta. His research interests include computer vision, applied machine learning, and the development of AI solutions that assist and support people. He has contributed to several research projects and peer-reviewed publications. In addition, Matthias is actively involved in voluntary initiatives, reflecting strong commitment to applying his technical expertise in the service of others. Dylan Seychell (Senior Member, IEEE) received his BSc IT (Hons), M.SC. and Ph.D. degrees in computer vision from the University of Malta, Msida, Malta. He is currently Resident Academic and Senior Lecturer in the Department of Artificial Intelligence at the University of Malta. His research interests include computer vision, applied machine learning, and remote sensing, with focus on deploying AI systems in operational environments. Dr Seychell serves as the Principal Investigator for several nationally funded research projects focusing on environmental sustainability and media integrity. He is also Technical Expert with the Malta Digital Innovation Authority (MDIA). SUBMITTED TO IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. N/A, NO. N/A, JANUARY 2026 12 Gabriel Hili received his M.Sc. degree in applied computer vision in the context of news media from the University of Malta. He is currently researcher at the University of Malta, where he works on collaborative research project with the Government of Maltas Cleansing and Maintenance Division. His research interests include computer vision and language technologies, with focus on the practical application of AI in public infrastructure and media. Mr Hili also contributes to the academic curriculum by assisting senior faculty in the delivery of lectures IEEE) (Member, Konstantinos Makantasis received the Diploma, M.Sc., and Ph.D. degrees in computer engineering from the Technical University of Crete. He is Lecturer with the Department of AI, University of Malta. He received the prestigious MSCA IF Widening Fellowship, to work on tensorbased machine learning methods for affect modeling. He has more than 80 publications in international journals and conferences on computer vision, signal and image processing, and machine learning. He is mostly involved and interested in computer vision, and the development of teaching materials. machine learning/pattern recognition, and probabilistic programming. Matthew Montebello (Senior Member, IEEE) is an academic and researcher at the University of Malta with over 35 years of experience in higher education. He holds doctorates in Computer Science and Education, with research expertise in personalisation, artificial intelligence, and digital education. His work focuses on AI in higher education, generative AI, ePortfolios, and ethical, human-centred educational technologies. He has led and coordinated multiple nationally and internationally funded projects, published with leading academic publishers, and received awards for innovation and research. He is actively involved in curriculum design, doctoral supervision, and institutional AI strategy. Carl James Debono (Senior Member, IEEE) received the B.Eng. degree (Hons.) in electrical engineering from the University of Malta, Malta, in 1997, and the Ph.D. degree in electronics and computer engineering from the University of Pavia, Italy, in 2000. From 1997 to 2001, he was Research Engineer in the area of Integrated Circuit Design at the University of Malta. In 2001, he was appointed Lecturer with the Department of Communications and Computer Engineering, University of Malta, where he is currently Professor. He currently serves as the Dean of the Faculty of Information and Communication Technology, University of Malta. Prof. Debono has participated in number of local and European research projects in the area of communication systems and image/video processing. His research interests include multiview video coding, resilient multimedia transmission, and computer vision. Saviour Formosa is senior academic at the University of Malta and consultant to national and international public-sector entities. Formerly Head of the Department of Criminology (20162020), he holds PhD in spatio-temporal environmental criminology, an MSc in GIS, and BA(Hons) in Sociology. His research focuses on spatio-temporal analysis, 3D scene reconstruction, advanced scanning technologies, and safety and security. He has acquired and led major EU and national projects including InMotion, SIntegraM, and SpatialTRAIN, securing over C80 million in funding. He directs immersive and digital-twin research through the Immersion Lab at the University of Malta."
        }
    ],
    "affiliations": [
        "Department of Artificial Communications of Malta",
        "Department of Communications and Computer Engineering, Faculty of Information and Communications Technology, University of Malta",
        "Department of Criminology, Faculty of Social Wellbeing, University of Malta",
        "Faculty Intelligence, Technology, University"
    ]
}