{
    "paper_title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
    "authors": [
        "Weizhi Wang",
        "Rongmei Lin",
        "Shiyang Li",
        "Colin Lockard",
        "Ritesh Sarkhel",
        "Sanket Lokegaonkar",
        "Jingbo Shang",
        "Xifeng Yan",
        "Nasser Zalmout",
        "Xian Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development."
        },
        {
            "title": "Start",
            "content": "Weizhi Wang1,2 Rongmei Lin2 Sanket Lokegaonkar2 Shiyang Li2 Colin Lockard2 Ritesh Sarkhel2 Jingbo Shang2,3 Xifeng Yan1 Nasser Zalmout2 Xian Li2 1UC Santa Barbara 2Amazon Stores Foundational AI 3UC San Diego"
        },
        {
            "title": "Abstract",
            "content": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development. Website Code UniFilter-Model UniFilter-Train-Data OBELICS-HQ https://victorwz.github.io/UniFilter https://github.com/Victorwz/UniFilter https://huggingface.co/weizhiwang/UniFilter-Qwen2.5-1.5B https://huggingface.co/datasets/weizhiwang/unifilter_train_data https://huggingface.co/datasets/weizhiwang/OBELICS_HQ_5M_UniFilter 5 2 0 2 6 1 ] . [ 1 2 6 1 5 1 . 0 1 5 2 : r 1. Introduction Large-scale multimodal datasets significantly motivates the recent advances in Vision Language Models (VLMs) [23, 35, 42] and Multimodal Large Language Models (MLLMs) [20, 25, 31, 43]. The scaled up data allows the MLLMs to harvest the knowledge in the training corpora to the greatest extent and promotes the state-of-the-art MLLMs. The MLLMs are trained on mixture of image-text caption data and interleaved document data to enhance both zero-shot and few-shot capability. Moreover, with the limited computing resources but the overwhelming number of data mining from CommonCrawl Snapshots, the recent large-scale MLLMs are only trained on data subset for less than one epoch. Therefore, the data quality became the major bottleneck in training stronger models. In selecting high-quality image-text caption dataset, the representative model-based filter, CLIPScore filter [11, 36] has become the predominant data filtering method. However, CLIPScore can only deal with image-text caption data based on the similarity between single image and short text caption. It is completely un-explored on how to select high-quality image-text interleaved data, which contains multiple images and long text paragraphs interleaving in one document. To address this problem, we propose to train an efficient MLLM as Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). Adopting an MLLM architecture for the proposed data quality classifier effectively overcomes the limitation of CLIPScore, which can only process single image-text pairs. The proposed UniFilter can process both image-text paired and interleaved data and output float quality score to indicate the quality of this multimodal data sample. Meanwhile, it outperforms CLIPScore on curating high-quality image-text caption data for enhancing both VLM and MLLM pre-training. Simultaneously, it achieves high inference throughput of 130 samples/s by leveraging Qwen-2.5-0.5b as LLM backbone, sligtly outperforming the CLIPScore methods 128 samples/s on the same hardware. The key to train an effective data quality classifier lies in constructing accurate samplescore pairs [9, 34]. Human annotations for these pairs are costly and challenging to maintain consistency across different annotators. To address this, we propose novel semi-synthetic multimodal data generation method by leveraging the proprietary MLLMs. Given that the proprietary MLLMs excel in text generation given multimodal inputs and the raw images are readily available, we sample diverse set of original images from captioned or interleaved data. We then use proprietary MLLMs to generate the full multimodal data following quality requirements across 4 quality levels (Section 2.1), in which similar 4 level quality score is also used in FineWeb-Edu-Quality-Classifier [34]. Then the synthetic data can be easily constructed as sample-score pairs, with score labels 0, 1, 2, and 3 corresponding to the defined quality levels in the prompts. In addition to well-designed synthetic training data construction, we conduct comprehensive ablation studies on the effective and efficient multimodal model architecture of UniFilter on the held-out validation synthetic sample-score data. We experiment with 6 combinations of choices of the vision encoder, visual projector, and the LLM backbone for constructing the UniFilter architecture. The architecture designs of SigLIP-SO-400M vision encoder [48], adaptive average pooling projector and the Qwen-2.5-0.5B LLM [44] achieves the best trade-off between data quality classification performance and efficiency. We conduct comprehensive experiments on image-text caption data and interleaved document data filtering to demonstrate the effectiveness of our method over strong baselines. We firstly validate the priority of UniFilter on curating high-quality image-text caption data over strong baselines through the experiments on MLLM pre-training with curated caption data only. Our method UniFilter outperforms the state-of-the-art (SOTA) CLIP-based filter2 Figure 1 The pipeline of semi-synthetic data generation for image-text caption data and interleaved document data. ing method, Data Filtering Network (DFN) [10] and the SOTAQ MLLM-based data filtering model, MLMFilter [41] on all 5 zero-shot VQA datasets. Secondly, pre-training MLLMs on the high-quality image-text interleaved document data curated by UniFilter promotes the few-shot learning capabilities of MLLMs by +0.7 and +2.8 average scores on 4-shot and 8-shot VQA performance over the baseline data filters. Finally, instruction-tuned MLLM based on UniFilter pre-training outperforms baselines and achieves +3.1 average improvement on VQA tasks and +1.5 improvement on MMMU benchmark, demonstrating the broad benefits of our approach. The summarization of the contributions of UniFilter are as follows: 1. We introduce UniFilter, the first unified approach for filtering both image-text caption and interleaved document data. By leveraging an MLLM-based architecture, UniFilter overcomes the limitations of existing methods that can only process single image-text pairs, enabling effective quality assessment of complex multimodal data structures. 2. We propose an efficient semi-synthetic data generation method that combines original images with synthetic text across multiple quality levels. This approach addresses the challenge of obtaining diverse, labeled multimodal data for classifier training, enabling scalable and cost-effective creation of high-quality training datasets. 3. MLLMs pre-trained on high-quality caption data and interleaved document data filtered by UniFilter demonstrate significant performance improvements over models trained with baseline data filtering methods. These gains from high-quality pre-training also persist after the SFT stage, further enhancing model capabilities. 2. Synthetic Data Construction Compared with collecting training data of data quality classification tasks from web-resources or human annotators, the synthetic data can be easily generated on large scale to provide sufficient training data for models, which is more feasible solution to empowering the effective training of data quality classifier. Furthermore, by incorporating controlled 4-level quality requirements  (Table 1)  into the data generation prompts, synthetic data generation can adhere strictly to these designated quality standards, ensuring clear quality boundaries among data at different quality levels. This proposed approach guarantees data sufficiency of quality classification tasks for effective training of data quality classifier and meanwhile enhances the generalization capabilities of the classifiers on data across multiple quality levels. The synthetic data generation pipeline is shown in Figure 1. 2.1. Define Data Quality Requirements We firstly design fine-grained data quality taxonomy on the multimodal data. Instead of using binary classification of positive and negative, we establish four quality levels: easy negative, medium negative, hard negative, and positive. These four quality levels are designed to capture the spectrum of data quality typically encountered in real-world multimodal datasets. The"
        },
        {
            "title": "Easy Negative",
            "content": "a negative image caption which is completely unrelated to this image. Medium Negative"
        },
        {
            "title": "Hard Negative",
            "content": "a negative image caption which has remarkable errors in describing the image. hard negative image caption which has subtle difference with the positive caption. The negative caption contains only one property error in describing the image."
        },
        {
            "title": "Positive",
            "content": "a high-quality, comprehensive, detail-enriched caption for this image."
        },
        {
            "title": "Easy Negative",
            "content": "Medium Negative This document should involve many errors in writing and the document itself is not fluent in reading. The images and the text in the document should be completely not related. The images are inserted in inappropriate and arbitrary places in the document. This document should be knowledge limited and has no educational value to be used as textbooks in primary school or grade school teaching. This document is readable but still contains several writing errors. The images and document text are under the same topic and the text contents are still not aligned well to the images. The document is knowledge sparse and has very limited educational value to be used as textbooks in primary school or grade school teaching. Hard Negative This document should involve several errors in writing. The images and the text in the document are partially related. However, the images cannot help the understanding of the text and cannot provide any additional information. The images are inserted in reasonable places in the document. This document should contain several factual or commonsense knowledge errors which makes it inappropriate for educational purposes."
        },
        {
            "title": "Positive",
            "content": "This document is high-quality, comprehensive, detail-enriched document. The images are inserted in the appropriate places in the document to provide additional information to the statement or provide the background information. Table 1 Data quality requirements for synthetic caption data and interleaved document data generation. easy negative category represents completely irrelevant or nonsensical data, while medium negative captures data with significant but not entirely unrelated errors. Hard negative simulates subtle mismatches or minor inaccuracies that are challenging to detect, and positive represents high-quality, well-aligned multimodal data. This granular approach enables our classifier to learn discriminative features across range of quality levels, enhancing its ability to filter real-world data effectively. We develop different prompts for both caption and interleaved data to accurately describe each quality level, shown in Table 1. These prompts guide Claude-3-Sonnet [2] in generating synthetic multimodal data that follows the specified quality requirements. The full prompt giving to Claude-3-Sonnet are presented in Appendix F. 2.2. Semi-Synthetic Data Generation The synthetic data on the proposed mutlimodal data quality classification tasks should be diversified and generalized on both image and text sides to train generalized multimodal data quality classifier. We initially considered generating fully synthetic multimodal data. However, we found that the SOTA image generation models like Midjourney and Dalle-3 [4] are stuck into specific image styles, i.e. carton, due to the post-training adaptations for these diffusion models. Thus, we adopted semi-synthetic approach: sampling original images from web-crawled caption and interleaved document datasets, while using Claude-3-Sonnet [2] to 4 Figure 2 The unified model architecture of UniFilter which uses an efficient MLLM to classify the quality scores of both image-text paired data (Left) and interleaved data (Right). generate corresponding text at various quality levels. This semi-synthetic approach offers several advantages. It ensures visual diversity and realism by using real-world images while allowing for controlled text generation at various quality levels. This method is highly scalable and efficient, enabling the creation of large, diverse datasets for classifier training. For our semi-synthetic data generation, we selected two prominent datasets as our source data: DataComp for image-text captions and OBELICS for interleaved documents. To enhance the diversity and topic coverage on the image sampling process, we cluster the DataComp-small images into 10k clusters based on their image embeddings extracted by CLIP ViT-L/14. Then we select 4 images from each cluster to get the original 40k image data for the synthetic caption data generation. For the interleaved document data from OBELICS, we compute the average pooling of image embeddings of all images within single document to create representative visual embedding for each document. We then clustered the OBELICS dataset into 10k document-level clusters and sampled 40k image-text interleaved documents from these clusters. Finally, we generate 40k synthetic caption data and 40k synthetic interleaved document data across 4 designed quality levels. We assign the integer quality scores to each synthetic sample while the quality levels of easy negative, medium negative, hard negative, and positive are corresponding to 0, 1, 2, and 3. We held 5% of 80k data as the validation set for further model developments on the proposed multimodal data quality classification tasks. After collecting original synthetic data, we adopt Llama-guard-3-8B [9] to efficiently scan the synthetic text and ensure there is no safety concerns in the generated texts. We also include 4k non-synthetic high-quality image-caption data from MSCOCO [26] and Flickr [46] into the final dataset, of which these 4k non-synthetic data are all assigned with quality score of positive. We use the joint sample-score paired data of both image-text caption and interleaved data to train SINGLE unified multimodal data quality classifier, which can process both image-text caption data and interleaved document data. 3. UniFilter Architecture To achieve unified architecture to process both the image-text caption and interleaved data, we construct the UniFilter based on MLLM architecture. Figure 2 presents how MLLM-based multimodal data quality classifier can process the two major types of image-text data. For the image-text interleaved data, the images and texts are encoded separately with vision encoder and word embedding layer and then reconstructed in original interleaving order. For the 5 LLM Vision Encoder Projector #Tokens per Image Image Resolution Validation Acc Validation F1 Phi-3-3.8b Phi-3-3.8b Phi-3-3.8b Phi-3-3.8b Gemma-2-2b Qwen2.5-1.5b Qwen2.5-0.5b MLP 256 CLIP-L CLIP-L AvgPool+MLP 144 SigLIP-SO-400M AvgPool+MLP 256 SigLIP-SO-400M AvgPool+MLP 144 SigLIP-SO-400M AvgPool+MLP 144 SigLIP-SO-400M AvgPool+MLP 144 SigLIP-SO-400M AvgPool+MLP 224 336 384 384 384 384 384 90.8 88.7 91.5 91.9 88.2 95.2 94.8 87.1 84.0 87.8 88.5 83.1 94.3 93.8 Table 2 Ablation studies on the MLLM architecture of UniFilter. caption data, the image encoding and caption embeddings are concatenated and forwarded into the LLM backbone. trainable one-dimensional classification head is appended on the top of LLM backbone to output logit indicating the quality score of the input caption or interleaved data sample. Adopting MLLM-based data quality classifier can substantially improve the quality classification performance compared with CLIP-based architectures, while the introduced billion-level model parameters will bring huge inference cost to the data quality label inferences on the pre-training data scale. In order to to train the UniFilter to be both efficient and capable, we perform comprehensive ablation study on the MLLM architecture of the UniFilter. The recent advances on the architecture design of MLLMs [3, 8, 28] all deploy the modality fusion architecture with 3 major modules of vision encoder, vision-language projector, and the LLM. We inherit this architecture and perform detailed ablations on design choices on different modules. The configuration for model architecture ablations om each module is as follows: Vision Encoder: We choose two models with different input image resolutions of CLIPViT-Large-224px and CLIP-ViT-Large-336px from CLIP model family as well as SigLIP-ViTSO400m-384px [48] for the ablation studies on vision encoders. Visual Projector: We consider two type of projector architecture of the non-compressive Multi-Layer Perceptron (MLP) with 2x inner embedding size used in LLaVA [28], and the compressive two-dimensional Adaptive Average Pooling layer with MLP used in DECO [45]. LLM Backbone: We experiment with four representative small LLMsPhi-3-mini-3.8B, Gemma-2-2B, and Qwen-2.5 (1.5B and 0.5B), as the base LLM for UniFilter. Larger LLMs with more than 4B parameters exceeds efficiency requirements for generating quality scores on pre-training data scale. The pre-trained language modeling head (Embd_Size Vocab_Size) is deprecated while newly-initialized classification head (Embd_Size 1) is trained for UniFilter. Then the output scalar logit is aligned to the synthetic quality label using MeanSquare-Error (MSE) loss. Because of the quadratic time complexity of LLMs with respect to number of concatenated multimodal input tokens, the computation efficiency of MLLMs are heavily affected by the number of image tokens for representing one image. Therefore, conducting compression on image patches to fixed number of image tokens is mandatory to ensure the efficiency, especially for high-resolution vision encoders, i.e. SigLIP-so400m-384px and CLIP-Large-336px. Yao et al. [45] compares the performance of 4 popular compressive vision projectors, Q-Former [22], C-Abstractor [5], D-Abstractor [5] and AdaptiveAveragePooling (AvgPool), and the AvgPool significantly outperforms other competitors. Thus, we adopt the two-dimensional AvgPool as the compressive vision projector and compare it with the non-compressive MLP projector. We train each UniFilter variant for 10 epochs on synthetic contrastive data and the best model is selected based on validation accuracy. The UniFilter based on Qwen2.5-1.5b achieves the 6 Figure 3 The classification F1 versus inference speed of different MLLM architecture ablation configurations on the held validation data of quality classification task. Methods GQA VQA-v2 VizWiz OKVQA TextVQA Avg. DFN [10] MLMFilter-Image-Text-Matching [41] MLMFilter-Object-Detail-Fulfillment [41] MLMFilter-Caption-Text-Quality [41] MLMFilter-Semantic-Understanding [41] UniFilter 25.8 28.3 28.1 28.4 24.0 29. 39.6 42.7 39.1 40.7 40.8 43.2 21.6 21.7 20.4 20.3 18.5 22.9 26.0 26.0 27.7 27.2 23.8 28. 30.7 31.6 31.6 35.2 29.8 32.5 28.7 30.2 29.4 30.4 27.4 31.3 Table 3 Zero-shot multimodal benchmark results of different pre-trained base MLLMs which are trained on only curated caption data for 5B tokens. best quality classification performance while introducing significant computational overhead due to the additional 1b parameters compared with Qwen2.5-0.5b model. Among all group of MLLM architecture configurations, the architecture with SigLIP-SO400M vision encoder, AvgPool visual projector of 144 tokens per image, and the Qwen-2.5-0.5B LLM achieves the best trade-off between quality classification performance and efficiency in Figure 3, which is used as the final UniFilter model. Surprisingly, the final UniFilter model can achieve comparable inference speed with DFN-CLIP-Large [10]. 4. Experiments 4.1. MLLM Pre-Training on Image-Text Caption Data Only We apply each filtering method, including UniFilter, to curate high-quality image-text caption data from the DataComp-medium-128M pool [11], and subsequently pre-train separate MLLMs using the datasets curated by the respective filtering methods. The DataComp-medium-128M pool is noisy, web-crawled image-text caption dataset that employs only basic rule-based filtering, which is specifically designed to evaluate the effectiveness of model-based filtering methods in selecting high-quality caption data. Baselines We pick the following baseline methods for fair comparisons on MLLM pretraining: 1) Data-Filtering-Network (DFN), strong CLIP-based data filtering model, which continually pre-trains the OpenAI CLIP-large on high-quality caption dataset for data filtering purpose; 2) MLM-Filter [41], which fine-tunes MLLM to generate quality scores for caption data filtering with 4 different scoring metrics, Image-Text Matching (ITM), Object Detail Fulfillment (ODF), Caption Text Quality (CTQ), and Semantic Understanding (SU). Training Setup. We compare the baseline and our method using the same MLLM architecture and training settings. The model architecture we adopt in MLLM pre-training consists of 3 modules of SigLIP-so400m vision encoder, AvgPool visual projector with 144 tokens per image, and the Phi-3-mini-3.8b LLM. The vision encoder is frozen at all time while other parameters are trainable. To ensure the fair comparisons, we set fixed 30% fraction of retained high-quality subset from DataComp-medium-128M pool for each filtering method, which can be tokenized into about 6B multimodal tokens for pre-training. Then, each MLLM is trained on filtered image-text caption data by each filtering method for 5B multimodal tokens, eliminating the effects of slightly different number of tokens in training for one epoch for each filtered dataset. Other hyper-parameters and details for multimodal pre-training are presented in Appendix B. Additionally, we perform an ablation study for the filtering fraction hyperparamter in Appendix H. Evaluation Benchmarks. We evaluate the zero-shot performance of each base pre-trained MLLMs on 5 visual-question answering datasets, including GQA [15], VQA-v2 [12], VizWiz [13], TextVQA [39], and OKVQA [30]. Among the 5 VQA datasets, the VQA-v2 and OKVQA focus on the commonsense knowledge understanding in the images. GQA and VizWiz emphasizes on the scene and spatial understandings and TextVQA lies on evaluating the OCR capability. Results. The results in Table 3 demonstrate the superiority of UniFilter on curating imagetext caption data for enhancing the understanding and reasoning capabilities of base non-sft base MLLMs. Moreover, the base MLLM pre-trained UniFilter curated caption data outperforms both DFN and all MLM-Filter metrics on the average performance of 5 multimodal benchmarks, demonstrating the strong generalization and diversity of the UniFilter-curated caption data in enhancing the capabilities of MLLMs across OCR, general reasoning, knowledge-reasoning, and scene reasoning. The MLLM trained with UniFilter curated data only lags behind the MLM-CTQ metric on TextVQA task, in which TextVQA dataset requires models to read and reason about text in images. MLMFilter-CTQ metric can effctively differentiate the caption data with great text quality and might be the best performing data filtering metric for OCR or text-rendering related pre-training data. 4.2. MLLM Pre-Training on Mixed Image-Text Caption and Interleaved Data Since the experimental results on caption-data pre-training in Section 4.1 have demonstrated the effectiveness and priority of UniFilter on caption data filtering, we further investigate the effectiveness of UniFilter on filtering interleaved image-text document data to promote the incontext learning capability of MLLM during multimodal pre-training. We use the OBELICS [19] as the original image-text interleaved document data resource for these experiments. Baseline and Training Setup. Since there is no effective data filter on filtering high-quality image-text interleaved data on document level, we consider one baseline of no filtering on interleaved data and another DFN variant baseline for processing interleaved data. For DFN variant, we follow Zhu et al. [50] to compute the cosine similarity between each image and each text paragraph in the original interleaved document, and only discard the images of which they do not achieve 0.15 similarity threshold with any of the text paragraph within the same document. MLM-Filter baseline is deprecated here because it can only process the image-text paired caption data and cannot process the interleaved document data. As for our filtering method, the top-15% high-quality documents, as determined by the UniFilter quality scores, are selected as the training data for our MLLM. Given that each induced MLLM will be trained on mixture of caption and interleaved data, we fix the pre-training caption data as the UniFilter curated caption data in Table 3 from DataComp-Medium for two baselines and our method to ensure there are no effects from the high-quality caption data side. And then we mix the 5B fixed caption data tokens and 5B image-text interleaved data tokens curated by three methods for each MLLM pre-training. This data mixture ratio of 1:1 is validated in MM1 [31] to be the 8 Methods #Train Tokens MM1-3B [31] 400B MM1-7B [31] 400B BLIP-3 [43] 100B No Filtering 10B DFN [10] 10B UniFilter 10B Shots GQA VQA-v2 VizWiz OKVQA TextVQA Avg. 0 4 8 0 4 0 4 8 0 4 8 0 4 8 0 4 8 - - - - - - - 17.6 40.4 40.7 21.8 40.9 41.0 22.9 42.2 42.0 46.2 57.9 63.6 47.8 60.6 64. 43.1 66.3 66.9 22.5 58.0 58.6 36.6 60.0 61.5 37.8 59.7 60.8 15.6 38.0 46.4 15.6 37.4 45. - - - 12.2 37.9 51.5 16.7 39.2 45.9 22.4 40.6 56.3 26.1 48.6 48.4 22.6 46.6 51. 28.0 48.9 50.1 23.9 44.6 45.5 20.6 43.6 44.9 25.1 44.8 46.4 29.4 45.3 44.6 28.8 44.4 46. 34.0 54.2 55.3 29.1 38.6 41.0 30.4 43.6 43.9 33.9 43.5 45.5 - - - - - - - - - 21.1 43.9 47.4 25.2 45.5 47.4 28.4 46.2 50.2 Table 4 Results of MLLMs trained on baseline data and UniFilter curated high-quality data for 10B tokens. Each 4/8-shot accuracy value is the mean score on 5 random seeds for multimodal in-context learning evaluations. optimal data mixture ratio to enhance both the multimodal few-shot and zero-shot learning. We report the 4-shot and 8-shot multimodal in-context learning performance on each VQA dataset for the baselines and our method. We select 5 random seeds for demonstration example sampling and report the mean score on 5 random seeds as the final task performance. Results. The results of 0-shot, 4-shot and 8-shot multimodal in-context learning on 5 VQA datasets are presented in Table 4. We also provide the original results of MM1 [31] and BLIP3 [43] as references even if their training size is 10-40 times larger than ours. Our method UniFilter significantly outperforms DFN variant filter baseline on GQA, VizWiz, OKVQA and TextVQA, while slightly lags behind VQA-v2 because the VQA-v2 is constructed from MSCOCO [26], which is used as the continue training data of DFN. Finally, the UniFilter induced MLLM achieves +0.7 and +2.8 average accuracy improvements over the DFN baseline on 4-shot and 8-shot in-context learning, respectively. The 0-shot in-context learning improvements are much more remarkable than that of 4-shot and 8-shot settings, achieving +3.2 average VQA task improvements. Compared with 4-shot and 8-shot in-context learning with useful instructional information and knowledge from the demonstrations, the 0-shot setting is more challenging task which relies more on the instruction following capability of models gained from pre-training corpus. Such outstanding 0-shot improvements demonstrate the benefit of effective high-quality interleaved data filtering. 4.3. Visual Supervised Fine-Tuning To further investigate the advantage of high-quality multimodal pre-training on the instructiontuned MLLM, we perform visual supervised fine-tuning (SFT) on different pre-trained base MLLMs from Section 4.2. The multimodal SFT data is joint set of visual instruction data from LLaVA-1.5 [27] and ShareGPT4V [7]. The composition of 575k multimodal SFT data is listed in Appendix E. In addition to 5 VQA datasets, we include 4 multimodal benchmarks, POPE [24], 9 Interleaved Pretrain Data No-Pretrain DFN UniFilter GQA VQA-v2 VizWiz OKVQA TextVQA 28.5 32.3 33.0 49.7 57.3 60. 15.5 16.7 19.5 27.5 27.0 32.3 32.3 41.6 44. VQA Avg. 30.7 35.0 38.1 POPE MMMU Val MMBench Dev MMStar 81.8 82.9 83.2 40.5 39.8 42.0 74. 75.4 77.0 36.9 38.0 38.5 Table 5 Zero-shot results of different instruction-tuned MLLMs on VQA datasets and multimodal benchmarks. MMMU (Val) [47], MMBench (Dev) [29], and MMStar [6] for comprehensive evaluations towards SFTed models. Results. The evaluation results of fine-tuned MLLMs are presented in Table 5. The finetuned MLLM pre-trained on high-quality multimodal data curated by UniFilter significantly outperforms the instruction-tuned MLLMs with baseline filtering methods, surpassing the best baseline by +3.1 average VQA accuracy, +1.5 MMMU accuracy, and +1.6 MMBench accuracy. Further, the 0-shot VQA task performance comparisons between the SFT MLLMs and their corresponding base models demonstrate all MLLMs benefit from visual SFT on completing out-of-distribution VQA tasks. The No-Pretrain (SFT-only) baseline significantly lags behind all pre-trained and fine-tuned MLLMs, demonstrating the necessity and benefit of multimodal pre-training. 5. Related Work Data Filtering for LLM and MLLM Pre-training. The family of Phi LLMs [1] adopt the educational value metric as the data quality metric for filtering high-quality text data for model pre-training, and FineWebEdu-Classifier [34] is an open-source effort on training data quality classifier for assessing the educational value of web pages. The SOTA open-sourced LLMs, Llama-3 also adopt similar data quality classifier trained on the synthetic sample-score pairs generated by Llama-2-70b, while their classifier training details are not released. In additional, DCLM [21] proposes that instead of training multi-way quality classifier, simple binary fasttext [16] classifier trained on positive instruction tuning data and negative web-crawled data is effective enough to curate high-quality data for SOTA LLM pre-training. In multimodal scenarios, LAION [36] firstly adopts CLIPScore-based data filtering to select high-quality imagetext caption data, and BLIP [23] adopts the Cap-Filt data quality boosting method to generate high-quality multimodal training data. Data Quality Classifier Trained with Synthetic Data. DCLM [21] proposes to construct contrastive data for training binary text data quality classifier by selecting LLM generated instruction data as positive data and original web-crawled data as negative data. To go beyond the synthetic binary scores, FineWebEdu-Classifier adopts Llama3-70b [9] to generate multi-way quality scores following well-defined human-drafted score annotation criteria. The data quality classifier to support Llama-3 pre-training also adopts similar pipeline to instruct Llama-2-chat [40] model to generate the quality scores. In synthetic quality score generation for multimodal data, MLM-Filter [41] prompts the GPT-4V to generate the 100-way quality scores on 4 different quality metrics to train quality classifier for filtering image caption data from 4 distinct perspectives, while AITQE [14] simlifies MLM-Filter to one unified quality metric on scale of 0-10. 10 6. Conclusion We propose an efficient MLLM-based Unified Multimodal Data Quality Classifier to filter both high-quality image-text caption and interleaved data. Pre-training MLLMs on the high-quality data curated by the proposed UniFilter can significantly enhance the capability of these generalpurpose models on downstream tasks. UniFilter overcomes the limitation of being only capable of filtering caption data in CLIP-based data filters and paves way to steadily improve both zero-shot and few-shot multimodal in-context learning capability of pre-trained and fine-tuned MLLMs via unified multimodal high-quality data filtering."
        },
        {
            "title": "References",
            "content": "[1] M. Abdin et al. Phi-3 technical report: highly capable language model locally on your phone. In: arXiv preprint arXiv:2404.14219 (2024). [2] Anthropic. Introducing the next generation of Claude. https://www.anthropic.com/ news/claude-3-family. Accessed on March 4, 2024. 2024. [3] [4] [5] J. Bai et al. Qwen-vl: frontier large vision-language model with versatile abilities. In: arXiv preprint arXiv:2308.12966 (2023). J. Betker et al. Improving image generation with better captions. In: Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf 2.3 (2023). J. Cha et al. Honeybee: Locality-enhanced projector for multimodal llm. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 13817 13827. [6] L. Chen et al. Are We on the Right Way for Evaluating Large Vision-Language Models? In: arXiv preprint arXiv:2403.20330 (2024). [7] L. Chen et al. Sharegpt4v: Improving large multi-modal models with better captions. In: arXiv preprint arXiv:2311.12793 (2023). [8] Z. Chen et al. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. In: arXiv preprint arXiv:2312.14238 (2023). [9] A. Dubey et al. The llama 3 herd of models. In: arXiv preprint arXiv:2407.21783 (2024). [10] A. Fang et al. Data filtering networks. In: arXiv preprint arXiv:2309.17425 (2023). [11] S. Y. Gadre et al. DataComp: In search of the next generation of multimodal datasets. In: arXiv preprint arXiv:2304.14108 (2023). [12] Y. Goyal et al. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, pp. 69046913. [13] D. Gurari et al. Vizwiz grand challenge: Answering visual questions from blind people. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2018, pp. 36083617. [14] H. Huang et al. Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining. In: arXiv preprint arXiv:2410.16166 (2024). [15] D. A. Hudson and C. D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In: CVPR. 2019. [16] A. Joulin et al. Bag of Tricks for Efficient Text Classification. In: arXiv preprint arXiv:1607.01759 (2016). 11 [17] S. Kazemzadeh et al. Referitgame: Referring to objects in photographs of natural scenes. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014, pp. 787798. [18] R. Krishna et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. In: International journal of computer vision 123 (2017), pp. 3273. [19] H. Laurençon et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In: Advances in Neural Information Processing Systems 36 (2024). [20] H. Laurençon et al. What matters when building vision-language models? In: arXiv preprint arXiv:2405.02246 (2024). [21] [22] [23] J. Li et al. DataComp-LM: In search of the next generation of training sets for language models. In: arXiv preprint arXiv:2406.11794 (2024). J. Li et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: arXiv preprint arXiv:2301.12597 (2023). J. Li et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: International Conference on Machine Learning. PMLR. 2022, pp. 1288812900. [24] Y. Li et al. Evaluating object hallucination in large vision-language models. In: arXiv preprint arXiv:2305.10355 (2023). [25] J. Lin et al. Vila: On pre-training for visual language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 2668926699. [26] T.-Y. Lin et al. Microsoft coco: Common objects in context. In: Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13. Springer. 2014, pp. 740755. [27] H. Liu et al. Improved baselines with visual instruction tuning. In: arXiv preprint arXiv:2310.03744 (2023). [28] H. Liu et al. Visual instruction tuning. In: arXiv preprint arXiv:2304.08485 (2023). [29] Y. Liu et al. Mmbench: Is your multi-modal model an all-around player? In: European Conference on Computer Vision. Springer. 2025, pp. 216233. [30] K. Marino et al. Ok-vqa: visual question answering benchmark requiring external knowledge. In: Proceedings of the IEEE/cvf conference on computer vision and pattern recognition. 2019, pp. 31953204. [31] B. McKinzie et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. In: arXiv preprint arXiv:2403.09611 (2024). [32] A. Mishra et al. Ocr-vqa: Visual question answering by reading text in images. In: 2019 international conference on document analysis and recognition (ICDAR). IEEE. 2019, pp. 947 952. [33] O. M. Parkhi et al. Cats and dogs. In: 2012 IEEE conference on computer vision and pattern recognition. IEEE. 2012, pp. 34983505. [34] G. Penedo et al. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In: arXiv preprint arXiv:2406.17557 (2024). [35] A. Radford et al. Learning Transferable Visual Models From Natural Language Supervision. In: ICML. 2021. [36] C. Schuhmann et al. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. In: arXiv preprint arXiv:2111.02114 (2021). 12 [37] D. Schwenk et al. A-okvqa: benchmark for visual question answering using world knowledge. In: European conference on computer vision. Springer. 2022, pp. 146162. [38] O. Sidorov et al. Textcaps: dataset for image captioning with reading comprehension. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16. Springer. 2020, pp. 742758. [39] A. Singh et al. Towards vqa models that can read. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 83178326. [40] H. Touvron et al. Llama 2: Open foundation and fine-tuned chat models. In: arXiv preprint arXiv:2307.09288 (2023). [41] W. Wang et al. Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters. In: arXiv preprint arXiv:2403.02677 (2024). [42] Z. Wang et al. Simvlm: Simple visual language model pretraining with weak supervision. In: arXiv preprint arXiv:2108.10904 (2021). [43] L. Xue et al. xGen-MM (BLIP-3): Family of Open Large Multimodal Models. In: arXiv preprint arXiv:2408.08872 (2024). [44] A. Yang et al. Qwen2. 5 technical report. In: arXiv preprint arXiv:2412.15115 (2024). [45] L. Yao et al. DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models. In: arXiv preprint arXiv:2405.20985 (2024). [46] P. Young et al. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In: Transactions of the Association for Computational Linguistics 2 (2014), pp. 6778. [47] X. Yue et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 95569567. [48] X. Zhai et al. Sigmoid loss for language image pre-training. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023, pp. 1197511986. [49] X. Zhai et al. The visual task adaptation benchmark. In: (2019). [50] W. Zhu et al. Multimodal c4: An open, billion-scale corpus of images interleaved with text. In: Advances in Neural Information Processing Systems 36 (2024). A. Additional Results on DataComp-Medium The DataComp benchmark [11] is an image-text data filtering benchmark to systematically compare the performance of different data filtering methods. In DataComp, each data filtering method is required to curate high-quality subset from fixed image-text data pool and such selected subset will be used for CLIP VLM pre-training for fair comparisons. The training code and computational budget is controlled across all competing methods to facilitate fair comparison between data filtering methods. The performance of each data filtering method is measured by the evaluation on the zero-shot capabilities of the final induced CLIP model on suite of 38 classification and retrieval tasks. We select the Medium scale training setting to train ViT-B/32 CLIP models on datasets resulting from baselines and our methods. Baselines. We select 5 baselines from original DataComp release, including the No-Filtering, Text-based Filtering, Image-based Filtering, Intersection of Image-based Filtering and CLIPSCore Filtering, and CLIPScore Filtering. We also include the strongest baseline of Data Filtering Network (DFN), which is continually pre-trained based on CLIP to become stronger data Methods #Filtered Samples #Total Training Samples ImageNet -1k ImageNet dist. shifts VTAB Retrieval Avg. over 38 datasets 99.6M No Filtering 24.8M Text-based Image-based 23.7M Image-based CLIPScore-30% 11.1M 29.6M CLIPScore-30% [11] 15.0M DFN-Public [10] 15.0M DFN-15% [10] UniFilter-15% UniFilter-20% UniFilter-25% UniFilter-30% UniFilter-30% DFN-15% UniFilter-30% DFN-15% UniFilter-25% DFN-15% 15.0M 19.9M 24.9M 29.9M 13.3M 30.5M 26.9M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M 128M 16.6 23.6 23.7 25.5 25.5 26.7 34.4 29.3 31.1 30.5 29.8 33.8 29.9 31. 14.8 19.9 19.3 20.8 21.6 22.7 27.2 23.9 25.2 25.3 24.8 26.9 25.4 25.4 25.8 29.8 29.3 31.2 32.5 33.0 35.8 34.3 34.6 35.1 35.0 35.6 34.6 36. 21.0 23.2 23.1 20.0 23.1 23.0 25.9 24.6 25.8 26.0 27.9 25.7 27.7 27.6 25.7 28.8 28.7 29.7 31.5 31.3 34.5 33.0 34.0 33.9 34.3 34.2 33.9 35. Table 6 Results of UniFilter and baselines on DataComp-medium scale caption data filtering benchmark. All the baselines are reproduced for fair comparisons because we can only download 99.6M samples from the original 128M released urls. curation model. The original data pool of DataComp-128M medium scale is released in only image-urls due to copyright considerations and 22.3% of these image-urls are no longer downloadable at the time of June 2024. To ensure the fair comparisons of different data filtering methods under the same original data pool, we reproduce all baseline methods based on the 99.6M downloadable data. The best filtering fraction of retained high-quality data for CLIPScore and DFN are 30% and 15% respectively, which are validated in their original literature [10, 11]. Figure 4 The comparisons between UniFilter and DFN on each dataset in the 38 dataset evaluation suite of DataComp Meidum. The Green bars mean UniFilter beats DFN. Results. The results on DataComp-Medium image-text caption data filtering benchmark are shown in Table 6. We consider 4 different fraction of original data retained as high-quality data, 15%, 20%, 25%, and 30%. When working individually as the data filter, the UniFilter retaining top-30% of 99.6M data performs best on the average performance over 38 datasets, which is comparable with DFN. We also explore the intersection or union of the filtered high-quality data between ours and DFNs. The union of UniFilter-25% and DFN data achieves the SOTA 14 performance on 38 dataset average score and VTAB average score [49]. Meanwhile, we find that the high-quality data filtered by UniFilter can significantly boost the CLIPs performance on image-to-text and text-to-image retrieval tasks. UniFilter-30% achieves SOTA performance on retrieval task average performance and incorporating UniFilter data with DFNs data improves DFNs retrieval performance by +1.8 average scores. Such significant improvements on retrieval tasks are also observed in other MLLM-based data filters [41]. We suppose that MLLM-based data filter may favor the image captions with high text quality like linguistic fluency and readability, which contributes more to the retrieval tasks than classification tasks. We also provide the detailed comparisons between DFN and UniFilter on each dataset of the 38 dataset suite in Figure 4. UniFilter surpasses DFN on 19 datasets while does worse on ImageNet related datasets because DFN is continually fine-tuned on ImageNet-1k dataset for distribution alignment. We find that UniFilter significantly lags behind DFN on Oxford-Pet dataset [33] (37.99% versus 52.52%). The Oxford-Pet dataset includes fine-grained pet categories (e.g., Abyssinian, American Bulldog) that are rarely represented in publicly available image-text datasets. We also check the performance of other strong baselines like CLIPScore and MLMFilter on this Oxford-Pet dataset, which are 35.25% and 38.62% respectively. We hypothesize that DFNs superior performance on this dataset may be attributed to the extensive coverage of fine-grained pet class names in Apples internal HQ-357M dataset used for training DFN. B. Training Settings of MLLM Pre-Training The training details and hyperparameters for MLLM pre-training are presented in Tab. 7. We do not follow Qwen-VL [3] to perform separate stage to train the visual projector only. The MLLM pre-training only involves one single stage to update the parameters of visual projector and LLM backbone, while the vision encoder is frozen all the time. To accelerate the MLLM training and avoid too many padding tokens, we perform sequence packing to regroup image-text data at varied length into fixed context size sequences. special <endofchunk> token is added before the start of every image in an image-text interleaved document to indicate the end of text paragraph. The MLLM training on 10B mixed multimodal tokens is conducted on 4 A100-40G gpus nodes, and each node contains 8 A100-40G gpus. The training for 10B tokens takes about 640 A100-40G gpu hours."
        },
        {
            "title": "Vision Encoder\nVisual Projector\nLLM Backbone\nContext Length",
            "content": "Precision Global Batch Size # Training Steps # GPUs Peak LR # Warmup Steps Ratio LR Scheduler Weight Decay Adam ( 𝛽1, 𝛽2) MLLM Pre-Training SigLIP-so400m-384px 2d Adaptive Average Pooling Phi-3-mini-4k-instruct 4096 BF16 256 9537 32 A100 3e-5 3% Cosine LR Decay 0.01 (0.9, 0.98) Table 7 Training details for MLLM pre-training on 10B multimodal tokens. C. Statistics of Multimodal Datasets We list the statistics of the large-scale image-text caption dataset and image-text interleaved document dataset in Tab. 8 as well as their licenses. The filtered high-quality data subset by UniFilter will also inherit the original licenses of these datasets and ensure the proper usage of them. All images in two datasets are released in image-urls rather than files, leading to large-scale invalid data samples. We discard the whole document from OBELICS if any one of the image in the document is invalid. As of June 2024, only half of OBELICS interleaved documents are fully downloadable. Dataset #Samples Downloadable #Samples License DataComp-Medium 128M 141M OBELICS 99.6M 70.5M MIT CC-BY-4.0 Table 8 Pre-Training Multimodal Dataset Statistics. D. Statistics of the Curated Interleaved Document Dataset Filter Avg. #Img. Avg. Text Len. Avg. Document Len. Filtering Fraction None DFN UniFilter 1.98 1.88 3. 842.5 841.1 1627.8 1125.8 1110.4 2078.3 100% 90.5% 15% Table 9 The curated interleaved document data statistics using different filtering methods. We analyze the features and statistics of the curated interleaved document data using different filtering methods in Table 9. Compared with the No-Filter and DFN-Filter baselines, the selected high-quality document data contains more images and more text tokens in one document. Additionally, the DFN-CLIP-Filter can only remove the irrelevant images in document based on cosine similarity, leading to an uncontrollable filtering fraction. The proposed UniFilter can achieve document-level filtering and flexibly select the filtering fraction hyperparameter based on the data quality needs. E. Visual SFT Data Composition We select comprehensive and diverse task sets for constructing the visual SFT instruction dataset. Since the 5 VQA datasets are used as evaluation benchmarks, the visual instruction data constructed from these 5 VQA dataset are excluded in the joint SFT dataset. For visual instructions, we select LLaVA-Conversations [28], LLaVA-Reasoning [28], ShareGPT4V-Caption [7], OCRVQA [32], A-OKVQA [37], TextCaps [38], RefCOCO [17], and VG [18]. We use ShareGPT as the text instruction data. The final joint SFT dataset consists of 575k multimodal instructions. F. Full Synthetic Data Generation Prompts The full prompts for both caption data and interleaved data generation are listed in Tab. 11. The \"{multi-level quality requirements}\" are placeholders for integrating the defined quality requirements in Tab. 11 into the synthetic data generation prompt. 16 Data Size Response formatting prompts Visual Instructions 517k Conversation Reasoning ShareGPT4V OCRVQA 58K 77k 100k 80k Answer the question using single word or phrase. A-OKVQA 66K Answer with the options letter TextCaps 22K from the given choices directly. Provide one-sentence caption for the provided image. RefCOCO 48K Note: randomly choose between the VG 86K two formats Provide short description for this region. Provide the bounding box coordinate of the region this sentence describes. Text Instructions 40k ShareGPT 40K Total 575k Table 10 Visual and Text SFT Data Composition. G. Ablations on In-Context Learning Prompt Templates The demonstration prompt template affects the performance of multimodal in-context learning. We perform an ablation study on different prompt templates for constructing demonstration examples, shown in Tab. 12. The results present that the <endofchunk> token is significant to multimodal in-context learning capability of MLLMs. The <endofchunk> token is inserted in the end of each text paragraph of the interleaved document data during the data pre-processing. Thus, adding this token to each demonstration example template constructs the few-shot demonstrations into an interleaved document, which may help trigger the models parametric memory towards the pre-trained knowledge in the interleaved document data. H. Ablation Study on Filtering Fraction for Caption Data We further investigate the effects of different fraction of retained high-quality subset from the original pool to the performance of pre-trained MLLMs. We perform ablation studies on DFN baseline and UniFilter on two filtering fractions of 15% and 30%. The results in Table 13 demonstrates that for both methods 30% filtering fraction is better hyperparameter choice compared with retraining only 15% data. Generally, retraining less data will hurt the data diversity and distribution of curated dataset, and 30% achieves the best trade-off between highest average quality and data diversity. I. Effects of Introducing System Prompts in Multimodal In-Context Learning In addition to the demonstration prompt template, we also investigate the effects of introducing the system prompts in the in-context learning templates. The results on these ablation studies are presented in Tab. 14. We consider 3 different system prompts as follows: Phi-3 Default: <system>You are helpful assistant.<end> 17 Caption Data Generation Prompt You are helpful assistant to help users write two opposite image captions for the given image in JSON format. The JSON object must contain the following keys: - \"topic\": string, topic word of this image - \"positive_caption\": string, high-quality, comprehensive, detail-enriched caption for this image. - \"negative_caption\": string, {multi-level quality requirements} Please adhere to the following guidelines: - Both captions should be at least {num_words} words long. - Both captions should be in English. - Please avoid using complex or advanced words in the captions. Ensure that the language is suitable for high school level audience or lower. Your output must always be JSON object only, do not explain yourself or output anything else. Be creative! Interleaved Data Generation Prompt You are an assistant to help users to write document given several images. These images are extracted from paper, report, or article in which these images are inserted. <guideline> Please firstly generate xml tag for each image in order for future generation. For each image, please generate xml tag like \"<img>image description</img>\". You need to replace the image description with your generated short description of this image which is less than 5 words. For the second task, {multi-level quality requirements} Please adhere to the following guidelines when writing this document: - The paragraphs in the document should be in varied length. - The document should contain at least 500 words. - You NEED to use xml tag as the placeholder to indicate the place where an image is inserted into. - You NEED to ensure that all given images are used and considered. - You MUST NOT use the image xml tag within your sentences. You should add them between sentences and paragraphs. - You MUST use each image for ONLY ONCE in the document. Your output must always be JSON object only. The JSON object must contain the keys of \"image_tags\" and \"document\". </guideline> Now, it is your turn. Please strictly follow the above guidelines in <guideline> xml tags when writing the document. Table 11 Prompting templates for synthetic caption data and interleaved document data generation. LLaVA-1.5 Default: chat between curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans questions. Claude-3 Generated: You are tasked with answering open-ended questions based on images provided from the visual question answering dataset. Each question may require understanding the visual content of the image, interpreting natural language, and applying commonsense knowledge. Your goal is to generate the most accurate answer based on the image, considering multiple possible interpretations. 18 Prompt Templates for Demonstration Examples <user><image>n{Question}Answer the question with single word or phrase.<assistant>{Answer}<end> <bos><user><image>n{Question}Answer the question with single word or phrase.<assistant>{Answer}<end> GQA 35.79 38.46 <bos><user><image>n{Question}Answer the question with single word or phrase.<assistant>{Answer}<endofchunk> 39.64 <bos><image>n{Question}Answer the question with single word or phrase.n{Answer}<endofchunk> <bos><image>n{Question}Answer the question with single word or phrase.n{Answer}<endoftext> <bos><image>n{Question}Answer the question with single word or phrase.n{Answer}<end> <bos><image>nQuestion: {Question}Answer the question with single word or phrase.nAnswer: {Answer}<endofchunk> <bos><image>nQuestion: {Question}nAnswer: {Answer}<endofchunk> 39.53 34. 39.09 42.2 37.67 Table 12 Ablation studies on the effects of different in-context learning prompt construction templates on the 4-shot performance of GQA using the no-filtering baseline model."
        },
        {
            "title": "Methods",
            "content": "Fraction GQA VQA-v2 VizWiz OKVQA TextVQA Avg."
        },
        {
            "title": "UniFilter\nUniFilter",
            "content": "15% 30% 15% 30% 25.7 25.9 26.7 29.6 35.8 41.0 41.8 43. 21.6 21.2 20.1 22.9 24.9 24.1 24.5 28.2 30.3 29.9 28.3 32. 27.7 28.4 29.3 31.3 Table 13 Ablation studies on the fraction of retained high-quality subset from the original 128M data pool for MLLM pre-training. System Prompts Phi-3 Default LLaVA-1.5 Default GQA 40.57 40.77 Claude-3 Generated 41.60 Table 14 Ablation studies on the effects of different system prompt construction in templates on the 4-shot performance of GQA using the no-filtering baseline model. Concluding from Tab. 14, introducing the system prompt in the in-context learning templates promotes the GQA performance of induced MLLM, demonstrating the success of multimodal pre-training to train the base MLLM to follow instructions. J. Examples of Claude-3 Generated Contrastive Interleaved Documents. We provide pair of contrastive positive and hard negative synthetic documents in Figure 5 and Figure 6. The positive document is apparently knowledge-intensive and has better detailed image-text alignment compared with the hard negative document. 19 Figure 5 positive synthetic document generated by Claude-3. The image is sampled from OBELICS dataset. Figure 6 hard negative synthetic document generated by Claude-3. The image is sampled from OBELICS dataset."
        }
    ],
    "affiliations": [
        "Amazon Stores Foundational AI",
        "UC San Diego",
        "UC Santa Barbara"
    ]
}