{
    "paper_title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents",
    "authors": [
        "Han Xiao",
        "Guozhi Wang",
        "Yuxiang Chai",
        "Zimu Lu",
        "Weifeng Lin",
        "Hao He",
        "Lue Fan",
        "Liuyang Bian",
        "Rui Hu",
        "Liang Liu",
        "Shuai Ren",
        "Yafei Wen",
        "Xiaoxin Chen",
        "Aojun Zhou",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 9 4 1 2 . 5 0 5 2 : r UI-Genie: Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents Han Xiao1,2, Guozhi Wang2, Yuxiang Chai1,2, Zimu Lu1, Weifeng Lin1,2, Hao He1, Lue Fan1, Liuyang Bian2, Rui Hu2, Liang Liu2, Shuai Ren2(cid:66), Yafei Wen2, Xiaoxin Chen2, Aojun Zhou1 (cid:66), Hongsheng Li1,3 (cid:66) 1CUHK MMLab 2vivo AI Lab 3CPII under InnoHK {1155229123@link,hsli@ee,aojunzhou@link}.cuhk.edu.hk shuai.ren@vivo.com Project lead (cid:66)Corresponding author Interns at vivo"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce UI-Genie, self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by reward model and self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently processes historical context and unifies action-level and task-level rewards. To support the training of UI-Genie-RM, we develop deliberately-designed data generation strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UIGenie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory generation without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [20, 21, 1] have demonstrated remarkable potential for powering autonomous mobile GUI agents capable of completing complex tasks through natural language instructions. The emergence of Multi-modal Large Language Models (MLLMs) [12, 15, 2] represents significant advancement in this field, introducing enhanced perception and reasoning abilities crucial for interface navigation. By directly perceiving screenshots, identifying functional UI elements, and generating executable action sequences in vision-centric manner, MLLM-based GUI agents have achieved better performance compared to their text-only counterparts. While recent advancements in general MLLMs have demonstrated the effectiveness of large-scale synthetic data generation [3, 45], creating high-quality synthetic data for GUI agents remains particularly challenging: (1) Accurate Trajectory Outcome Verification. Verifying GUI agent trajectory outcomes presents unique challenges that distinguish it from other multi-modal understanding tasks. Unlike common question-answering tasks where correctness can be directly judged by checking final answer, the completion state of GUI agent tasks is heavily dependent on historical context, Preprint. Under review. Table 1: UI-Genie dataset statistics. UI-Genie-RM-517k is the first dedicated GUI agent reward dataset, while UI-Genie-Agent-16k contains synthetic trajectories without manual annotation. Reward Datasets UI-Genie-RM-517k Agent Datasets Android Control AMEX AndroidLab UI-Genie-Agent-16k Size 263k 170k 24k 59k Size 88k 34k 6k 16k Manual Annotation Data Source Manual Annotation AndroidControl AMEX AndroidLab Exploration Average Steps 5.5 12.8 8.6 7.1 Positive Num 121k 68k 14k 29k Trajectories 15283 2946 726 2208 Negative Num 142k 102k 10k 30k Task Instruct High&Low High High High&Low Figure 1: Performance comparison between UI-Genie, Qwen2.5-VL and UI-TARS on three benchmarks. making evaluation substantially more complex. Existing assessment frameworks, including those utilizing proprietary models as judges [22, 37], fail to provide accurate outcome verification and reliable step-level validation of intermediate actions. (2) Scalable High-Quality Training Data. Due to the lack of reliable verification methods, current training approaches still rely on human-annotated operation trajectories [13, 5, 38], which are inherently time-consuming, expensive, and difficult to scale. These manually-created datasets suffer from limited volume and diversity, particularly lacking high-quality demonstrations for complex multi-step tasks. To address these challenges, we introduce UI-Genie, self-improving framework that generates highquality synthetic trajectories with accurate process supervision, eliminating the need for extensive human annotation. To provide fine-grained trajectory assessment, we build UI-Genie-RM, the first reward model specifically designed for evaluating GUI agents. Our reward model features an image-text interleaved architecture that efficiently processes historical screenshots and actions as context while unifying both action-level and task-level rewards. This innovative approach enables both single-step and multi-step trajectory evaluation within the same architecture. To overcome the significant challenge of the absence of training datasets designed for GUI agent reward models, we develop deliberate data generation strategies, including rule-based verification, controlled trajectory corruption, and hard negative mining based on open-source training datasets. This comprehensive dataset construction process, combined with UI-Genie-RMs long-context processing capabilities and unified reward representation, enables accurate validation at both the action level and task level without requiring manual annotation. To generate high-quality synthetic trajectories, we leverage UI-Genie-RM for process supervision during agent exploration in dynamic environments. In these rollouts, UI-Genie-RM ranks candidate actions by reward scores and expands only the most promising trajectories. At terminal states, we use outcome verification to assess trajectory success. To address low initial success rates on complex tasks, we introduce self-improvement framework where both the agent and reward models evolve iteratively. Specifically, we expand training data with successful trajectories to boost performance on increasingly complex tasks, as well as correctly labeled steps from failed trajectories to refine the reward model. Our framework overcomes the limitations of static, manually-annotated datasets and enables continuous improvement in GUI agent performance. Through this process, we generate UI-Genie-RM-517k, the first large-scale reward dataset for GUI agents, and UI-Genie-Agent-16k, high-quality synthetic trajectory dataset. As shown in Tab. 1, our method yields substantially more data without human annotation. Experiments show that UI-Genie achieves state-of-the-art results with three self-improvement cycles, as illustrated in Fig. 1. We release our full open-source framework to support further research in GUI agents. Our contributions can be summarized as follows: 1. We develop UI-Genie-RM, specialized reward model for GUI trajectory assessment with rich interleaved image-text observations, while unifying action-level and task-level rewards. 2. We introduce UI-Genie, novel self-improving framework where both the agent model and reward model evolve iteratively. This framework progressively solves complex GUI tasks through reward-guided trajectory exploration, training data expansion and iterative model fine-tuning. 3. We create and open-source two novel datasets (UI-Genie-RM-517k and UI-Genie-Agent-16k) along with our complete framework implementation, establishing the first reward-specific dataset for GUI agents and demonstrating synthetic trajectory generation without manual annotation."
        },
        {
            "title": "2 Related Work",
            "content": "Multi-modal GUI Agents. Recent advances in Large Language Models (LLMs) [20, 21, 24, 32] have enabled the development of GUI agents, with Multi-modal LLMs (MLLMs) further enhancing UI information perception [8, 11]. Prior work [1, 30, 31, 43] leveraging commercial MLLMs has shown promising results across mobile and desktop environments. Meanwhile, open-source approaches have focused on specialized architectures and training methods. OS-Atlas [36] adopts unified action space for different UI grounding tasks. UI-TARS [23] introduces comprehensive pretraining on element grounding and reasoning. [18, 17] apply rule-based Reinforcement Learning [10] and GRPO [25] to achieve competitive performance. Despite these advances, significant challenge persists: most approaches remain dependent on human-annotated trajectories. While some methods [38, 28, 23] incorporate synthetic trajectory generation, they still rely on human effort or closed API models for verification, limiting scalability. In contrast, our work enables fully automatic trajectory synthesis and evaluation with an effective self-improvement pipeline. Reward Models. Reward models have been widely used in reasoning-intensive tasks to assist in test-time scaling [27, 14, 40], expand solution paths [9], and facilitate model fine-tuning [29, 33, 41]. Prior studies such as [14] and [29] demonstrate that, compared to outcome-supervision reward models (ORMs), process-supervision reward models (PRMs) can better perceive the correctness of intermediate steps in reasoning trajectories, thereby providing more reliable feedback. To avoid the high cost of step-level human annotation, various works [19, 33] have proposed methods to automatically obtain process supervision by estimating the potential of intermediate steps to lead to the correct final result. Similar to these studies, our work also leverages PRM trained on step-level rewards obtained through sophisticated rollout in dynamic environments. Recent works [34, 42, 35] have extended the application of PRMs from LLMs to MLLMs. Most related to our work, [28] employ GPT-4o as trajectory-level reward model to assess the quality of GUI agent trajectories. In contrast, our method builds the first reward model specifically designed for GUI agents based on open-source foundation models. Our reward model provides more fine-grained step-level rewards to filter generated trajectories and does not rely on costly proprietary models."
        },
        {
            "title": "3 Method",
            "content": "This section introduces UI-Genie, framework for solving mobile GUI agent challenges through an iterative self-improving approach. Sec. 3.1 presents the specialized reward model. Then, we detail the dataset construction strategies in Sec. 3.2. Finally, Sec. 3.3 explains how the self-improvement pipeline works to continuously enhance both components. 3.1 UI-Genie-RM: Unified Reward Model Previous approaches of trajectory verification [23, 36, 37, 22] rely on human efforts or proprietary models for validation, which are inherently costly and difficult to scale. Moreover, these methods fails to provide validation of intermediate steps, constraining their practical adaptability. To address these limitations, we develop UI-Genie-RM, GUI reward model which builds upon standard MLLM backbone, but incorporates crucial modifications to handle sequential GUI interactions and unify step-level and outcome-level rewards. Model Architecture. UI-Genie-RM determines if proposed action At advances the task goal G, given the current state. Because GUI tasks depend on long-term context, accurate action assessment requires understanding more than the current screenshot. For example, in cross-app task where user copies headline from News to Notes, the correct action in Notes (typing the headline) depends on knowing what was seen earlier in News, not just the present screenshot. To address this issue, UI-Genie-RM uses an image-text interleaved architecture that leverages past interactions to evaluate current actions. As shown in Fig. 2, the model takes in the task goal, current screenshot, and candidate actions, plus an interleaved sequence of recent screenshots and action history for context. This helps the model track progress and understand the impact of previous steps. Processing full histories is costly for complex tasks. We address this by context window using only the five most recent screenshots, while summarizing earlier actions as language descriptions. Our agent predicts these natural language summaries (e.g., Access Categories & Budget section) 3 Figure 2: Overview of UI-Genie-RM model and reward training data construction. The model processes task instruction, historical context, current screenshot, and candidate action as inputs. Outputs are supervised by both action-level and task-level rewards. The training data are constructed by rule-based verification, trajectory corruption, and hard negative mining processes. alongside action primitives (e.g., click(x,y)), providing semantic context even when screenshots are missing. This representation balances historical coverage and computational efficiency. Unified Action-Level and Task-Level Rewards. Our model architectures ability to process historical information provides necessary context for trajectory-level judgments, which enables us to unify step-level and task-level assessment within the same architecture. To achieve this goal, we formulate task completion as special action within the action space, allowing us to judge whether this terminal action is appropriate given all past GUI interactions when evaluating task-level outcomes. This unified representation allows both action-level and task-level rewards to be handled through the same optimization objective, eliminating the need for separate models across different evaluation granularities. Specifically, our reward model assigns binary rewards to both actions and tasks given the task goal G, current screenshot It, and historical context descriptions = {H0, H1, ..., Ht1}. We define the reward ground truth Rt {0, 1}, where 1 indicates correct action and 0 indicates an incorrect one. Following previous work [44], we formulate reward prediction as next-token generation task. The model is trained to maximize the likelihood of predicting y+ for correct actions and for incorrect ones. Given dataset containing positive action examples A+ and negative examples , our training objective minimizes LRM = (cid:88) log (y+G, It, H, A+ ) (cid:88) log (yG, It, H, t ), (1) (A+ ,y+)D (A ,y)D where y+ and denote the positive and negative action labels, respectively. 3.2 Data Construction for Reward Model Training To train an MLLM into an effective GUI agent reward model, we develop deliberately-designed data construction strategies that create comprehensive training corpus of approximately 517k reward data. The detailed data statistics are presented in Tab. 1. We initially leverage task instructions and corresponding ground-truth actions from open-source datasets [13, 5, 38] as sources. We then apply three complementary data generation techniques on the base data source to create positive and negative samples, resulting in 458k synthetic training samples for our initial reward model. This dataset is further enriched with an additional 59k samples acquired through trajectory exploration in dynamic environments as part of our self-improvement pipeline described in Sec. 3.3. Rule-based Verification. We first train an initial agent model on the base GUI operation trajectory datasets [13, 5, 38]. For each task instruction, we generate candidate actions by sampling prediction 4 Figure 3: Self-improvement of agent and reward models for UI-Genie. It expands training sets for both agent and reward models through reward-guided trajectory exploration and outcome verification, then finetunes both models. This process repeats iteratively to improve capabilities on increasingly complex tasks. from this initial agent given the same task instructions, then validate them against ground truth actions using three criteria: (1) Type alignmentwhether predicted and ground truth actions must share the same type; (2) Coordinate accuracyfor spatial operations like click and long press, predicted coordinates must fall within ground truth bounding boxes to ensure functional validity; (3) Semantic consistencyfor text-based operations like typing, generated content must maintain semantic equivalence with ground truth. Controlled Trajectory Corruption. To create negative trajectory-level samples, we systematically perturb successful trajectories through three mechanisms: (1) Early truncationterminating trajectories at intermediate steps to simulate incomplete executions; (2) Cross-task substitutionreplacing trajectory segments with actions from different tasks within the same application, mimicking goal misinterpretation; (3) Redundant continuationappending unnecessary steps beyond task completion to model failure in recognizing terminal states. Hard Negative Mining. We enhance the reward models robustness by identifying challenging negative examples. Leveraging UI elements annotations from [5], we convert functional descriptions to executable action descriptions using an open-source LLM [16]. After training an initial reward model on the rule-based verification data, we identify samples that this model incorrectly classified as positive and retain them as hard negatives, strengthening the models discriminative capabilities in ambiguous scenarios. These complementary strategies yield diverse, high-quality dataset that enables effective training of UI-Genie-RM across varying difficulty levels and failure modes, establishing robust foundation for accurate single-step and multi-step assessment. 3.3 Iterative Self-Improvement of Agent and Reward Models Following the training of initial UI-Genie-RM reward model, we establish self-improvement framework that dynamically generates GUI operation trajectories and progressively enhances both reward and agent models. As illustrated in Fig. 3, this framework operates within dynamic Android environment where agent actions directly modify environmental states. We first introduce our approach of reward-guided trajectory exploration and training data expansion, then present our recipe of iterative model fine-tuning with increased task complexity. Reward-Guided Trajectory Exploration. For GUI trajectory exploration, we find traditional Monte Carlo Tree Search (MCTS) mechanisms [4, 26] are suboptimal due to their computational expense. More importantly, conventional MCTS fails to effectively address the unique characteristics of GUI interaction spaces. Specifically, unlike mathematical problem solving where erroneous steps lead to incorrect answers, GUI interactions often include invalid actions that maintain the last state (e.g., clicking blank areas or non-functional elements). To address these challenges, we develop rewardguided beam search approach inspired by [7]. At each step, UI-Genie-Agent generates ten candidate actions that are evaluated by UI-Genie-RM to produce step-level rewards. We rank these candidates by cumulative rewards along each partial trajectory and retain only the top-5 paths, as indicated by the 5 green circles in the tree in Fig. 3. This exploration efficiently discovers both successful completion paths and informative partial trajectories for our training data expansion process. Training Data Expansion with Outcome Verification. Our framework iteratively expands training data for both the agent and reward models through dual labeling approach. To expand the training set of UI-Genie-Agent, we apply outcome verification using UI-Genie-RM to identify successful completions. These successful trajectories are directly added to the agents training set to enhance its generalization capabilities. To expand the training set of UI-Genie-RM, we employ potential-based labeling approach [33] for step-level reward annotation. For successful trajectories, each constituent step is annotated as correct (label y+). For unsuccessful trajectories, we determine the viability of each intermediate step through continuation rollouts. From each intermediate step, we sample five continuation paths and evaluate their outcomes. If any of these rollouts successfully completes the task, we classify the original step as correct (y+), indicating it maintains task completion potential despite being part of failed trajectory. This approach automatically generates fine-grained step-level supervision for refining the reward model. Iterative Model Fine-tuning. We implement our framework through three iterative rounds with gradually increasing task complexity: 1. Initial generation: We utilize task instructions directly derived from source training data [38, 5], establishing baseline performance. 2. Expansion generation: We introduce novel task instructions generated by prompting an advanced open-source LLM [16] with app descriptions and example tasks from the first generation. 3. Complex task generation: We combine filtered instructions of failed tasks from previous rounds with manually crafted complex scenarios requiring more than ten steps to complete. This progressive approach creates positive feedback loop between framework components: enhanced agent capabilities generate more diverse, successful trajectories for increasingly complex tasks; these trajectories provide richer supervision signals for the reward model; the refined reward model delivers more precise guidance during exploration; and this improved guidance enables the discovery of solutions to even more complex tasks. Each generation thus reinforces both components capabilities while progressively expanding the frontier of solvable GUI tasks."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate UI-Genie across diverse benchmarks designed to assess both GUI agent capabilities and reward model accuracy. We first describe our implementation details in Sec. 4.1, followed by comprehensive overview of the evaluation benchmarks and metrics in Sec. 4.2. Ablation studies are provided in Sec. 4.3 to validate the design choices of each component. 4.1 Implementation Details We implement both UI-Genie-RM and UI-Genie-Agent based on the Qwen2.5-VL family of models due to its strong multimodal understanding capabilities. For the reward model, we adopt Qwen2.5VL-7B as the backbone. For agent models, we experiment with three variants across different model sizes: 3B, 7B, and 72B, enabling analysis of scaling effects on performance. Reward Model Training. UI-Genie-RM is initialized with Qwen2.5-VL-7B and trained using binary classification objective. We introduce special tokens + and to represent positive and negative class labels, respectively, and formulate reward prediction as next-token generation task. We first conduct supervised fine-tuning on our constructed dataset of 458k samples derived from existing datasets. Subsequently, we perform iterative refinement using total of 59k process-reward samples collected during self-improvement cycles. We train the model using the AdamW optimizer with learning rate of 1e-5 and global batch size of 160. Agent Model Training. We first establish an initial agent model based on Qwen2.5-VL-7B using existing datasets including AndroidControl [13], AMEX [5], and AndroidLab [38]. This initial model performs trajectory exploration in our dynamic Android environment under UI-Genie-RMs supervision. Through three rounds of iterative improvement, we generate UI-Genie-Agent-16k, synthetic dataset containing 16k high-quality trajectories without manual annotation. For variants beyond 7B, we train UI-Genie-Agent-3B and UI-Genie-Agent-72B using UI-Genie-Agent-16k and the aforementioned open-source datasets for one epoch. All agent models are fine-tuned with learning rate 1e-5 and batch size 160. 6 Table 2: Performance comparison on AndroidControl benchmark [13]. The table presents commercial systems (Claude Computer Use and GPT-4o), open-source foundation models, and our approach. Higher values are better for all metrics: SR (Success Rate), Type (Action Type Accuracy), and Grounding (UI Element Interaction Accuracy) across both low-level and high-level task settings. Highest values are indicated as bold. Agent Model Size AndroidControl-Low AndroidControl-High Type Grounding 74.3 74.3 94.3 79.3 91.9 98.1 96.0 97.8 90.7 93.0 93.6 98.0 98.1 98.1 98. 0.0 0.0 87.7 82.6 72.3 83.8 87.3 93.2 94.7 73.4 88.0 89.3 94.9 89.9 95.4 SR 19.4 19. 67.3 90.8 80.6 89.3 92.1 93.8 74.2 75.0 91.4 80.5 85.2 90.8 94.3 84.4 93.7 91.3 94.8 Type Grounding 63.7 66. 58.0 84.7 81.2 82.7 82.5 66.2 71.6 82.9 85.2 83.7 83.5 85.2 84.9 0.0 0.0 43.2 56.2 73.8 78.4 74.4 82.5 65.6 62.9 78.5 80.5 82. 81.5 86.3 SR 12.5 20.8 10.2 46.6 63.7 67.5 68.9 71.1 72.9 44.5 51.7 59.1 60.1 61.5 71.2 72.5 74.2 66.4 67.4 74.7 77. Claude Computer Use GPT-4o Aria-UI UI-R1 GUI-R1 Qwen2.5-VL OS-Atlas UI-TARS InfiGUI-R1 UI-Genie-Agent (Ours) OS-Genesis GUI-R1 SeeClick Qwen2.5-VL Aguvis OS-Atlas UI-TARS UI-Genie-Agent (Ours) Aguvis Qwen2.5-VL UI-TARS UI-Genie-Agent (Ours) 3.9B 3B 3B 3B 4B 2B 3B 3B 7B 7B 9.6B 7B 7B 7B 7B 7B 72B 72B 72B 72B 4.2 Evaluation Benchmarks 4.2.1 Agent Model Evaluation To comprehensively evaluate UI-Genie, we conduct experiments on both static and dynamic benchmarks. Static evaluation assesses the agents ability to predict correct actions in static environments where only single frame with ground-truth action history is input, while dynamic evaluation tests the agents performance in dynamic, interactive settings in an emulator where each action directly affects task progress and completion. Static Evaluation. We utilize the AndroidControl [13] benchmark to evaluate UI-Genie-Agents planning and action-execution capabilities in static mobile environments without actually executing actions on device. This benchmark compares the agents predicted actions with ground truth actions, providing controlled and reproducible evaluation framework. Following previous work [39, 23], we evaluate our agent on two settings: (1) high-level tasks requiring the agent to autonomously plan and execute actions to complete task goal based on screenshots and action history; and (2) low-level tasks that provide step-by-step low-level instructions for the agent to execute predefined actions. For both settings, we employ three primary metrics: (1) Success Rate (SR), measuring the percentage of successfully completed actions; (2) Type Accuracy, assessing the agents ability to correctly predict action types (e.g., click, long-press, scroll); and (3) Grounding Accuracy, evaluating the agents precision in locating and interacting with the correct UI elements. Tab. 2 presents comparison of our UI-Genie-Agent against proprietary systems and open-source foundation models on the AndroidControl benchmark [13]. Our approach consistently outperforms existing methods across all model sizes. The 3B variant achieves 93.8% and 72.9% success rates on low and high-level tasks respectively, surpassing previous SOTA by 1.7% and 1.8%. The 7B and 72B variants further improve performance, with our 72B model reaching 77.0% success rate on high-level tasks, exceeding UI-TARS by 2.3%. The consistent improvements across model sizes demonstrate the robust scalability of our UI-Genie framework with the integration of high-quality trajectories during agent training. 7 Table 3: Performance comparison on AndroidLab [38]. The table presents commercial systems (top section), open-source foundation models and their fine-tuned variants (middle section), and our approach (bottom). The best performance values are highlighted in bold, while underlined values denote the sub-optimal results. Agent Gemini-1.0 Claude-3-Opus Gemini-1.5-Pro Claude-3.5-Sonnet GPT-4o AutoGLM LLaMA3.2-11B-Vision CogVLM2-ft UI-TARS-ft Qwen2.5-VL Qwen2-VL-ft Qwen2.5-VL-ft UI-Genie-Agent (Ours) UI-Genie-Agent (Ours) UI-TARS UI-TARS-ft Qwen2.5-VL Qwen2-VL-ft Qwen2.5-VL-ft UI-Genie-Agent (Ours) Model Size Sub-Goal Success Rate Reversed Redundancy Ratio Reasonable Operation Ratio Success Rate 11B 19B 7B 7B 7B 7B 3B 7B 72B 72B 72B 72B 72B 72B 12.6 15.1 18.5 32.7 35.0 13.0 16.1 18.2 18.7 22.6 24.2 35.4 46. 12.0 28.4 26.1 29.3 30.9 48.2 72.5 81.4 106.0 113.4 87.3 61.7 57.4 64.1 70.6 65.2 59.5 91.3 92.8 40.8 81.4 68.7 84.5 81.3 92.1 76.7 83.9 91.5 81.2 85.4 87.9 85.6 88.0 76.8 88.3 76.3 90.6 91. 70.0 81.6 81.4 90.2 79.3 93.3 10.9 13.0 16.7 29.0 31.2 36.2 10.1 11.6 14.0 14.9 18.1 20.6 28.8 38.7 9.6 22.1 23.9 24.6 25.0 41.2 Dynamic Evaluation. AndroidLab [38] provides controlled emulator environment where the agents actions are directly executed, and success depends on its ability to adapt to dynamic UI states resulting from its interactions. The benchmark comprises 138 tasks across nine categories of frequently used offline static applications (e.g., maps, calendars, books, music players, etc.). These tasks simulate typical daily user interactions such as managing events, editing notes, and retrieving information. We evaluate performance on AndroidLab using the metrics defined in [38], which capture task completion at different levels: (1) Sub-Goal Success Rate (Sub-SR), evaluating completion of individual task components; (2) Reversed Redundancy Ratio (RRR), assessing operational efficiency compared to human benchmarks; (3) Reasonable Operation Ratio (ROR), measuring the proportion of effective operations that change the screen state; and (4) Success Rate (SR), measuring the percentage of tasks where all sub-goals are completed. As shown in Tab. 3, our approach demonstrates superior performance compared to both commercial systems and various open-source foundation models and their fine-tuned variants. Besides, Android Agent Arena (A3) [6] presents more comprehensive and challenging online evaluation environment. It employs an actual Android device emulator with 201 tasks spanning 20 predefined online top-rated applications selected from 18 categories (news, travel, ticketing, etc.). This benchmark includes particularly demanding tasks requiring more than 15 interaction steps and multi-frame information retrieval. These features make A3 highly effective for assessing an agents ability to perform tasks in real-life scenarios and execute complex action sequences in response to evolving environments. The metrics detailed in A3 [6] are used to evaluate UI-Genies capability. This evaluation incorporates two distinct methods for assessing task completion: (i) function-based evaluation and (ii) commercial LLM-based essential states evaluation. Employing these different approaches provides comprehensive understanding of UI-Genies performance from multiple perspectives. Tab. 4 demonstrates UI-Genies superior performance compared to previous SOTA agents such as UI-TARS. Specifically, UI-Genie achieves an approximate 5% higher success rate in functionbased evaluation and 3% improvement in commercial LLM-based evaluation. Furthermore, its Essential State Achievement Rate (ESAR) surpasses that of UI-TARS by approximately 5% overall, highlighting UI-Genies enhanced capabilities in real-life scenarios. 8 Table 4: Evaluation results on the A3 benchmark [6]. Func SR: Task Success Rate by Evaluation Function; LLM SR: Task Success Rate by Commercial LLM Evaluation; EASR: Essential State Achievement Rate. Highest overall scores are indicated as bold for Func SR, underlined for LLM SR, and in italic for EASR. Op., S.Q. and M.Q. represent operation, single-frame query and multi-frame query in A3 respectively. S. Q. Overall Metric Agent M. Q. Med. Hard Easy Op. Claude-3.5-sonnet Qwen2.5-VL-7B UI-TARS-7B UI-Genie-Agent-7B (Ours) Func SR LLM SR ESAR Func SR LLM SR ESAR Func SR LLM SR ESAR Func SR LLM SR ESAR 11.7 13.8 23.6 23.4 27.7 39.8 28.7 34.0 55.8 38.3 40.4 61. 2.6 2.6 16.7 5.2 7.8 27.2 9.1 15.6 40.1 6.5 14.3 40.4 0.0 0.0 14.2 0.0 0.0 29. 0.0 0.0 41.8 0.0 0.0 43.6 8.4 9.8 22.1 17.5 19.6 36.4 21.0 23.8 51.1 28.0 30.1 57. 2.0 2.0 14.7 0.0 8.2 25.6 0.0 16.3 36.2 2.1 12.2 39.5 0.0 0.0 14.9 0.0 0.0 28. 0.0 0.0 35.7 0.0 0.0 34.9 6.5 8.8 17.6 12.9 15.9 33.2 16.9 21.9 46.5 20.4 24.4 51. Table 5: Performance comparison of reward models on step-level and outcome-level evaluation across different task complexity categories. We report F1-score (%) for classifying actions and outcomes as positive or negative. Model Step Reward Outcome Reward Overall Easy Medium Hard Overall Easy Medium Hard GPT-4o Gemini1.5-pro Gemini2.5-pro Qwen2.5-VL-7B Qwen2.5-VL-72B UI-Genie-RM (Ours) 68.1 65.1 72.9 56.6 66.2 79.6 72.5 70.5 74.1 60.2 71.5 83. 65.2 62.5 64.4 55.1 64.5 81.9 54.8 51.2 55.8 47.9 50.9 68.7 66.7 71.5 74.3 58.9 67.6 82. 70.3 72.4 78.2 64.1 68.6 84.3 58.8 67.5 73.2 48.4 65.5 81.3 56.8 54.5 64.9 41.3 46.4 70. 4.2.2 Reward Model Evaluation We conduct evaluation of UI-Genie-RM using custom benchmark, since there is no established standard benchmark for GUI agent reward models. Our benchmark derives from test sets of three opensource datasets: AndroidControl [13], AMEX [5], and AndroidLab [38]. For step-level evaluation, we sample 200 distinct ground truth actions as positive examples from each dataset, pairing each with corresponding negative action generated by the agent model and verified through rule-based methods. For outcome-level evaluation, we include 200 ground truth trajectories as positive samples, complemented by an equal number of negative trajectories created through controlled trajectory corruption for AndroidControl and AMEX. We further augment this with 100 additional trajectories (50 successful, 50 failed) generated during AndroidLab dynamic testing and validated using predefined rules. This creates comprehensive benchmark containing 1,050 paired items. We evaluate UI-Genie-RMs accuracy using F1-score for both step-level and outcome-level reward assessment, with detailed performance breakdowns across task complexity categories: easy (under 5 steps), medium (5-10 steps), and hard (over 10 steps). For comparison, we establish strong baselines using advanced proprietary MLLMs (GPT-4o, Gemini1.5-pro, Gemini2.5-pro) and open-source models (Qwen2.5-VL-7B/72B). As these models lack specific reward modeling training, we carefully craft prompts to adapt them for this purpose. Additionally, we enhance inputs with visual prompts in screenshots to compensate for these models limitations in processing spatial coordinates. Tab. 5 presents our evaluation results. UI-Genie-RM consistently outperforms all baseline models across both evaluation types and all task complexity levels. The performance gap becomes particularly pronounced in hard tasks, where UI-Genie-RM maintains robust performance (68.7% F1-score for step-level, 70.5% for outcome-level) while other models show significant degradation. This demonstrates the effectiveness of our specialized architecture and training approach, particularly for complex GUI interactions requiring extensive historical context understanding. 9 4.3 Ablation Study 4.3.1 UI-Genie-RM for Test-time Scaling. Beyond serving as an effective model for synthetic GUI operation trajectory selection, UI-Genie-RM can also enhance agent model performance through best-of-N sampling strategy. During inference, the agent model generate candidate actions, which are then ranked by UI-Genie-RM to select the highest-scoring one as the final output. We evaluate this approach on the AndroidControl benchmark using Qwen2.5-VL base models. Our experiments measure both step-level success rates (accuracy of predicted single-step actions compared to ground truth) and task-level success rates (successful completion of entire tasks, where each step must align with ground truth). Additionally, we categorize the results by task difficulty: easy (less than 5 steps), medium (5-10 steps), and difficult tasks (more than 10 steps). As shown in Tab. 6, UI-Genie-RM consistently improves performance across both metrics and all model sizes. The Qwen2.5-VL-3B model shows substantial gains, with the step-level success rate increasing from 60.3% to 63.1% and the task-level success rate improving from 7.5% to 10.2% when using best-of-10 sampling. Similarly, the Qwen2.5-VL-7B model demonstrates improvements, particularly for medium-difficulty tasks where the task-level success rate increases from 10.5% to 12.2%. Importantly, our results demonstrate scaling benefits when expanding the sampling space from N=5 to N=10. This performance improvement indicates that UI-Genie-RM effectively identifies optimal actions from larger candidate pools, showcasing its ability to provide accurate reward signals for GUI interactions across varying task complexities. 4.3.2 Effectiveness of UI-Genie-RM Architecture To validate the key design choices of our UI-Genie-RM architecture, we conduct comprehensive ablation studies focusing on two critical components: historical context processing and unified reward representation. We systematically evaluate these components through three ablation settings: (1) removing historical information entirely, processing only the current screenshot; (2) varying the temporal depth of historical context by incorporating different numbers of historical screenshots (1, 3, and 5); and (3) eliminating the unified representation by training separate models for step-level and task-level reward evaluation. Tab. 7 presents quantitative results across these configurations. The performance degradation observed when removing historical information (61.3% vs. 79.6% for step-level rewards, 60.5% vs. 82.1% for outcome rewards) validates our hypothesis that historical context is essential for accurate GUI action evaluation. This aligns with our understanding that GUI interactions are inherently sequential, with action correctness often dependent on prior observations. Furthermore, our results demonstrate clear performance scaling with increased historical context depth. Step-level reward accuracy improves progressively from 72.4% with single historical image to 79.6% with five images, with similar trends for outcome rewards (73.8% to 82.1%). These gains are particularly pronounced for hard tasks, where deeper context enables more accurate evaluation of complex action sequences. This demonstrates our models ability to effectively utilize longer historical sequences for comprehensive trajectory assessment. Finally, our results demonstrate that the unified representation approach substantially improves both the step-level and outcome-level evaluation performance. The performance gains suggest that joint learning of step-level and task-level rewards creates beneficial knowledge transfer between these complementary evaluation tasks, significantly outperforming the approach of training separate specialized models. 4.3.3 Effectiveness of Self-Improvement Approach To quantitatively evaluate our iterative self-improvement framework, we track the performance enhancement of both the agent and reward models across multiple rounds. Fig. 4 illustrates the progressive performance improvement measured through task success rate on AndroidLab (for UIGenie-Agent-7B) and step reward accuracy (for UI-Genie-RM). Our results demonstrate substantial and consistent improvements across iterations. Notably, the task success rate of UI-Genie-Agent-7B on AndroidLab increases substantially from an initial 18.1% (round 0) to 38.7% (round 3), finally surpassing the state-of-the-art AutoGLM method. Concurrently, the reward model accuracy of UIGenie-RM improves from 68.2% to 79.6%, demonstrating the effectiveness of our self-improvement 10 Table 6: Performance comparison of Qwen2.5-VL models with and without UI-Genie-RM test-time scaling. Results show step-level success rate (Step SR) and task-level success rate (Task SR). Model Overall Easy Medium Hard Step SR Task SR Step SR Task SR Step SR Task SR Step SR Task SR QwenVL2.5-3B - +UI-Genie-RM 5 +UI-Genie-RM 10 QwenVL2.5-7B - +UI-Genie-RM 5 +UI-Genie-RM 10 60.3 61.8 63.1 64.9 64.9 65.2 7.5 8.3 10.2 14.7 15.1 16. 50.4 52.2 53.5 64.4 65.8 65.5 11.6 13.5 15.5 29.7 31.1 31.7 63.5 65.7 67.0 66.7 67.6 68. 7.1 7.6 9.7 10.5 10.7 12.2 58.3 58.4 59.6 61.1 58.6 58.9 0.9 0.9 1.9 1.4 0.9 1. Table 7: Ablation study of UI-Genie-RM components. \"Hist.\" indicates whether historical information is included, \"# Imgs\" shows the number of historical screenshots incorporated, and \"Unified\" indicates whether we use the unified representation for both action and task rewards. Model Configuration Step-level Reward Outcome Reward Hist. # Imgs Unified Overall Hard Overall Hard 0 1 3 5 61.3 72.4 77.8 75.2 79.6 50.8 64.7 71.5 70.3 68. 60.5 73.8 79.5 73.4 82.1 48.2 66.1 72.3 62.1 70.5 Figure 4: Performance evolution across iterative self-improvement rounds. approach. The most dramatic improvement occurs during the first round, highlighting the strength of our reward-guided exploration for trajectory discovery. Rounds 2 and 3 show continued improvement as the framework progressively incorporates more complex tasks and refines both models. These results validate our self-improvement frameworks ability to break through the initial performance limitations on complex tasks through mutual enhancement of agent and reward models."
        },
        {
            "title": "5 Discussions",
            "content": "We presented UI-Genie, self-improving framework that addresses key challenges in training GUI agents by generating high-quality synthetic trajectories and introducing specialized reward model, UI-Genie-RM. Our framework eliminates reliance on manual annotation through iterative agentreward model co-evolution, achieving state-of-the-art performance and producing two novel datasets. The results demonstrate the potential of synthetic data generation with process supervision for scalable GUI agent development. Limitations. While effective overall, our reward model may occasionally generate suboptimal rewards signals, resulting in failed trajectories during training data expansion. Although we observe significant performance improvements when applying our synthetic data to enhance agent capabilities, the framework cannot guarantee the production of fully correct trajectories across all GUI tasks. Broader Impact. This work can help people with disabilities better use mobile devices, making the technology more accessible to broader population. However, the high computation cost of training such model results in significant carbon emission."
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile 11 abilities. ArXiv, abs/2308.12966, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143, 2012. [5] Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. [6] Yuxiang Chai, Hanhao Li, Jiayu Zhang, Liang Liu, Guangyi Liu, Guozhi Wang, Shuai Ren, Siyuan Huang, and Hongsheng Li. A3: Android agent arena for mobile gui agents. arXiv preprint arXiv:2501.01149, 2025. [7] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553, 2024. [8] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [9] Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [12] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. arXiv preprint arXiv:2307.12981, 2023. [13] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv e-prints, pages arXiv2406, 2024. [14] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [15] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. [16] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [17] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Infigui-r1: Advancing multimodal gui agents from reactive actors to Yang, and Fei Wu. deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. 12 [18] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [19] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. [20] OpenAI. Chatgpt. https://chat.openai.com, 2023. [21] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. [22] Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. [23] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [25] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [26] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [27] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [28] Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. [29] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [30] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024. [31] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. [32] Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, and Hongsheng Li. Mathcoder-VL: Bridging vision and code for enhanced multimodal mathematical reasoning. In The 63rd Annual Meeting of the Association for Computational Linguistics, 2025. [33] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. [34] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. 13 [35] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. [36] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [37] Yu Xia, Jingru Fan, Weize Chen, Siyu Yan, Xin Cong, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Agentrm: Enhancing agent generalization with reward modeling. arXiv preprint arXiv:2502.18407, 2025. [38] Yifan Xu, Xiao Liu, Xueqiao Sun, Siyi Cheng, Hao Yu, Hanyu Lai, Shudan Zhang, Dan Zhang, Jie Tang, and Yuxiao Dong. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024, 2024. [39] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [40] Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724, 2023. [41] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. [42] Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. [43] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 120, 2025. [44] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [45] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 14 UI-Genie Training A.1 Definition of Action Space To facilitate effective cross-dataset training, UI-Genie implements comprehensive unified action space across all datasets including AndroidControl [13], AMEX [5], AndroidLab [38], and our newlygenerated synthetic data. While building upon the foundation established by Qwen2.5-VL models [3], we incorporate several critical enhancements to improve operational flexibility and generalization capabilities. As detailed in Tab. 8, our action space comprises eight fundamental action types: open, click, swipe, long_press, type, system_button, wait, and terminate. For each action type, we define specific parameters that ensure precise execution and compatibility with real-world mobile environments. key advancement over the original Qwen2.5-VL implementation is UI-Genies dual-mode interaction capability. Beyond predicting absolute coordinates, our model can process Set-of-Mark (SoM) annotations, where interactive UI elements are labeled with numeric tags in the screenshot. This enables UI-Genie to predict element indices rather than exact coordinates when provided Set-ofMark visual prompts, significantly enhancing generalization across diverse interface layouts. For each action type, we define tailored parameters: the open action specifies an app name; click and long_press use either coordinates or SoM indices to identify targets; swipe includes starting position (coordinate/SoM), direction (up/down/left/right), and distance (short/medium/long); wait specifies duration in seconds; and terminate indicates task completion status (success/failure). Additionally, we incorporate semantic \"action_desc\" parameter across all action types, offering crucial semantics when incorporated into the historical record of operations. Table 8: Action space and parameter specification for UI-Genie. Action Type open click swipe long_press type system_button wait terminate Parameters text; action_desc coordinate; som; action_desc coordinate; som; direction; distance; action_desc coordinate; som; action_desc text; action_desc button; action_desc time; action_desc status; action_desc A.2 Implementation Details We implement both UI-Genie-RM and UI-Genie-Agent based on the Qwen2.5-VL model family, leveraging its strong multimodal understanding capabilities. For our experiments, we develop variants across three model sizes (3B, 7B, and 72B) to analyze performance scaling effects. All experiments are conducted on distributed training setup of 20 L40s machines with 4 GPUs each. A.2.1 Reward Model Training For UI-Genie-RM, we adopt Qwen2.5-VL-7B as the backbone and introduce special tokens + and into the vocabulary to represent positive and negative class labels, effectively formulating reward prediction as next-token generation task. We conduct initial supervised fine-tuning on our constructed dataset of 458K samples derived from existing GUI operation datasets. Then we perform iterative refinement using 59K process-reward samples collected during self-improvement cycles. For the reward model training, we use the AdamW optimizer with learning rate of 1e-5 and global batch size of 160. We fully fine-tune the model parameters while keeping the vision encoder frozen. Input images are constrained to maximum of 602,112 pixels to maintain computational efficiency. A.2.2 Agent Model Training Our agent model training follows progressive enhancement approach: Figure 5: Step-level reward evaluation prompt used for comparative baseline models. Initial model establishment: We first train 7B baseline agent using existing datasets from AndroidControl, AMEX, and AndroidLab. This model is fully fine-tuned with only the vision encoder frozen, using learning rate of 1e-5 and batch size of 160. Iterative self-improvement: Using this initial agent, we conduct three rounds of trajectory exploration in dynamic Android environments under UI-Genie-RMs supervision. This process generates UI-Genie-Agent-16K, our synthetic dataset containing 16K high-quality trajectories without manual annotation. After each round, we continue fine-tuning the 7B model on newly collected trajectories for one epoch, maintaining the same hyperparameters. Model scaling: Finally, we train UI-Genie-Agent at different parameter scales (3B and 72B) using the combined UI-Genie-Agent-16K and open-source datasets. The 3B model is fully fine-tuned with frozen vision encoder using AdamW (lr=1e-5, batch size=160) We fine-tune the 72B model using rank-stabilized LoRA (rslora) due to GPU memory constraints, with lora modules added to the vision encoder, projection layer, and LLM. We set both lora rank and lora alpha to 64 and 256 respectively, and initialize lora weights using the PiSSA method. This progressive training approach enables us to effectively leverage both existing and synthetic data while efficiently scaling our model across different parameter sizes."
        },
        {
            "title": "B Prompt for Evaluation",
            "content": "In this section, we provide the detailed prompts used to adapt our comparative baseline models for reward evaluation tasks. Since neither proprietary MLLMs (GPT-4o, Gemini1.5-pro, Gemini2.5-pro) nor open-source models (Qwen2.5-VL-7B/72B) are specifically trained for reward modeling in GUI operations, we carefully craft specialized prompts to enable fair comparison. Our step-level reward evaluation prompt and outcome-level reward evaluation prompt are presented in Fig. 5 and 16 Figure 6: Outcome-level reward evaluation prompt used for comparative baseline models. Fig. 6, respectively. These prompts were designed to provide structured guidance for evaluating both individual action correctness and overall task completion success, leveraging the models capabilities for processing visual information from UI screenshots. Example of UI-Genie Training Data C.1 Reward Data Example We present examples of our reward data used for model training. Fig. 7 illustrates action-level reward data samples, which include both positive and negative action examples. These samples are crucial for training our model to distinguish effective UI interactions from ineffective ones. Fig. 8 showcases trajectory-level reward data, comprising complete successful trajectories alongside failed ones, which serve as positive and negative examples respectively. 17 Figure 7: Examples of action-level reward data used for UI-Genie-RM training. The visual prompts displayed here are for illustration purposes only and are not used during model training. For simplicity, we omit the history image displays, though they are included in the actual training process. 18 Figure 8: Examples of trajectory-level reward data showing successful and failed task completions. We omit the history of actions in this visualization, but the complete action sequences are utilized during training. C.2 Synthetic Trajectory example We present illustrative examples of trajectories synthesized by UI-Genie, demonstrating both successful and unsuccessful attempts at completing UI tasks. Fig. 9 showcases successful trajectory generated under the guidance of UI-Genie-RM. As shown in the sequence, the reward model effectively steers the agent through series of actions, resulting in the successful completion of the assigned task. In contrast, Fig. 10 presents failure case that illustrates limitation in trajectory exploration. Although the reward model provides guidance for individual steps within the trajectory, the final outcome reward is negative, indicating overall task failure. Figure 9: successful trajectory example. Under the reward guidance of UI-Genie-RM, the agent successfully discovers and executes sequence of actions that complete the assigned task. 20 Figure 10: failure trajectory example. Despite receiving process reward guidance at each individual step, the sequence ultimately results in task failure as indicated by the negative outcome reward. UI-Genie Trajectory Example on Online Evaluation D.1 AndroidLab case We provide examples demonstrating UI-Genie-Agents capabilities on AndroidLab tasks. Fig. 11 illustrates the agent executing financial task in which it successfully adjusts an expenditure amount to 500 CNY for the specific date of May 15, 2024 in bluecoins app. This example showcases the models ability to navigate through the applications interface to locate and modify transaction data according to precise user instructions. In Fig. 12, we present another example where UIGenie-Agent-72B interacts with the calendar application. The agent demonstrates its capability to edit event details, specifically modifying the end time of an event titled work to 7:00 PM. This example highlights the models proficiency in handling time-based modifications within scheduling applications. These examples from our AndroidLab evaluation illustrate UI-Genie-Agents ability to understand contextual instructions and execute precise actions within real-world mobile applications, showcasing its versatility across different types of mobile interfaces and task requirements. Figure 11: Example of UI-Genie-Agent-72B executing an AndroidLab task. The task instruction: Adjust the expenditure on May 15, 2024, to 500 CNY. 22 Figure 12: Example of UI-Genie-Agent-72B executing an AndroidLab task. The task instruction: Edit the event with title work, change the end time to be 7:00 PM. 23 D.2 Android Arena (A3) case We further evaluate UI-Genies capabilities using the Android Arena (A3) benchmark. Fig. 13 demonstrates UI-Genie-Agent-7B executing news search task within the CNN application. The agent successfully interprets the task instruction and navigates through the applications interface to locate and utilize the search functionality, ultimately retrieving panda-related news articles. This example highlights the models ability to understand domain-specific instructions and operate effectively within media consumption applications. Fig. 14 presents another example of UI-Genie-Agent-7Bs versatility, showing the agent completing settings modification task in the Coursera application. The agent demonstrates its capacity to navigate through application menus, locate configuration options, and toggle specific settings. These examples illustrates UI-Genie-Agents robust performance across diverse applications and task types, from content searches to application configuration changes. Figure 13: Example of UI-Genie-Agent-7B executing task defined in Android Arena (A3). The task instruction: Search news about panda in CNN. 24 Figure 14: Example of UI-Genie-Agent-7B executing task defined in Android Arena (A3). The task instruction: Open settings in Coursera and switch to dark mode."
        }
    ],
    "affiliations": [
        "CPII under InnoHK",
        "CUHK MMLab",
        "vivo AI Lab"
    ]
}