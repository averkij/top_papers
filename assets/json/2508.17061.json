{
    "paper_title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework",
    "authors": [
        "Stefanos Pasios",
        "Nikos Nikolaidis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Photorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains a major challenge due to the tradeoff between visual quality and performance. In this short paper, we present a novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative Network framework (REGEN), which employs a robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into a simpler paired image-to-image translation task. This enables training with a lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training a lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of real-world images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN."
        },
        {
            "title": "Start",
            "content": "REGEN: Real-Time Photorealism Enhancement in Games via Dual-Stage Generative Network Framework Stefanos Pasios, Nikos Nikolaidis 1 5 2 0 2 3 ] . [ 1 1 6 0 7 1 . 8 0 5 2 : r AbstractPhotorealism is an important aspect of modern video games since it can shape the player experience and simultaneously impact the immersion, narrative engagement, and visual fidelity. Although recent hardware technological breakthroughs, along with state-of-the-art rendering technologies, have significantly improved the visual realism of video games, achieving true photorealism in dynamic environments at real-time frame rates still remains major challenge due to the tradeoff between visual quality and performance. In this short paper, we present novel approach for enhancing the photorealism of rendered game frames using generative adversarial networks. To this end, we propose Real-time photorealism Enhancement in Games via dual-stage gEnerative Network framework (REGEN), which employs robust unpaired image-to-image translation model to produce semantically consistent photorealistic frames that transform the problem into simpler paired image-to-image translation task. This enables training with lightweight method that can achieve real-time inference time without compromising visual quality. We demonstrate the effectiveness of our framework on Grand Theft Auto V, showing that the approach achieves visual results comparable to the ones produced by the robust unpaired Im2Im method while improving inference speed by 32.14 times. Our findings also indicate that the results outperform the photorealism-enhanced frames produced by directly training lightweight unpaired Im2Im translation method to translate the video game frames towards the visual characteristics of realworld images. Code, pre-trained models, and demos for this work are available at: https://github.com/stefanos50/REGEN. Index TermsPhotorealism Enhancement, Image-to-Image Translation, Grand Theft Auto V, Computer Vision I. INTRODUCTION"
        },
        {
            "title": "P HOTOREALISM in modern video games is a key factor",
            "content": "in shaping player experience. High-quality graphics not only enhance the immersion and realism but also support the narrative, emotional investment, and understanding of the game. In competitive markets where first impressions of the target audience are critical, photorealistic visuals alone can make or break video games success. Despite considerable advances in the computation capabilities of modern hardware and the development of sophisticated computer graphics pipelines with state-of-the-art technologies such as real-time ray tracing and lumen, achieving true photorealism in dynamic and interactive environments continues to be significant challenge. Conventional rendering methods often have to compromise on quality to achieve sufficient performance, particularly in real-time conditions where constant, high enough frame rate is of utmost importance. The are with authors Thessaloniki, University of {pstefanos,nnik}@csd.auth.gr). the"
        },
        {
            "title": "School",
            "content": "of Thessaloniki Informatics, Aristotle (email: 54124, Greece To overcome the limitations of traditional computer graphics pipelines, recent advances in Artificial Intelligence (AI), particularly in generative models [1], have opened new opportunities for bridging the gap between games and reality. Image-toImage (Im2Im) translation [2] is an approach that is frequently utilized to enhance the photorealism of rendered images based on target domain of the real world. Im2Im translation can be either paired [3], [4], where models learn from corresponding image pairs, or unpaired [5], [6], [7], where mappings are learned between domains without pixel-level alignment. Since acquiring real-world images that exhibit per-pixel correspondence to the content depicted in the respective rendered game images is hard, for photorealism enhancement, this process is performed in an unpaired manner. The distribution differences in unpaired Im2Im translation between the source (game) and the target (real-world) domains often introduce significant artifacts that undermine player experience [8]. This has resulted in research on more robust unpaired Im2Im translation methods that utilize intermediate information [8], [9], [10]. Such information is employed during image rendering and includes various properties of the virtual environment, such as depth, geometry, and materials. However, these methods have never been applied to video game due to their challenging implementation, which requires access to information that is integrated deep into the engine, and the fact that they are restricted to frame rates of 10 frames per second (FPS) or less. In this paper, we propose Real-time photorealism Enhancement in Games via dual-stage gEnerative Network framework (REGEN), new framework to perform photorealism enhancement in games that is easy to integrate and runs in real-time. In detail, we propose the utilization of robust unpaired Im2Im translation method [8] that can generate highly photorealistic and semantically consistent results in non-real-time. Considering that the generated data perfectly aligns semantically with the initial rendered frames of the game, the task is transformed into paired Im2Im translation [6], [11] problem, which is significantly easier process and thus can be performed effectively, without visual artifacts, with smaller models that can be deployed in real-time. Particularly, we attempt to enhance the photorealism of the popular game Grand Theft Auto (GTA V), which has sold more than 210 million copies. Through experimentation, we illustrate both qualitatively and quantitatively that the proposed approach maintains comparable visual performance with the robust unpaired Im2Im translation method while it reduces the inference time by factor of 32. It is also shown that the approach is more effective compared to performing unpaired Im2Im translation with lightweight model [4] using directly the images from the game and the real world. Our contributions are summarized as follows: 1) We propose novel, publicly available, two-stage framework that employs robust unpaired image-to-image translation model to generate photorealistic pairs in nonreal time, enabling the training of lightweight paired image-to-image translation model for robust real-time photorealism enhancement in games. 2) The introduced framework can be seamlessly integrated into games without requiring access to low-level engine information, which is typically required as input by the current state-of-the-art (SotA) Im2Im translation methods for computer graphics. 3) We conduct qualitative and quantitative evaluation showing that the proposed approach achieves comparable visual quality to the initial robust unpaired Im2Im translation method while providing 32X improvement in inference speed. Additionally, it is illustrated that the results outperform the photorealism-enhanced images that were produced by directly translating the game images towards real-world frames using lightweight unpaired Im2Im translation method. II. RELATED WORK Enhancing the photorealism of computer graphics rendered images has been widely studied in the literature, as it is fundamental step toward improving the photorealism of visual synthetic datasets. In detail, the employment of computergraphics-based simulators [12] to generate large-scale synthetic datasets for training deep-learning methods has given rise to the need for enhanced photorealism. GTA is game that has been extensively used in deep learning research to extract synthetic datasets [13], [14] through various tools [15] that transform GTA into deep learning data generator. Thus, several works have attempted to fill the simulationto-reality (sim2real) gap [8], [16] that exists in the rendered game frames and can affect the real-world performance of the models that are to be trained on those datasets. Traditional unpaired Im2Im translation methods [3], [4], [17] have been extensively utilized in both simulators and games such as GTA and have been proven to generate significant number of artifacts [8]. The real-world datasets [18], [19] that are typically employed as the target domain for photorealism enhancement via unpaired Im2Im translation are challenging due to the different distribution between the game images and their frames. To illustrate, having realworld dataset captured in geographic domain with trees as the most dominant object at the top of the frames [18] has been proven to result in the generation of non-realistic objects, such as trees in the sky, on the photorealism-enhanced images [8]. To overcome the issues of traditional unpaired Im2Im translation methods, recent works started to inject additional information [20], [21], namely semantic segmentation information, to guide the translation process into more robust results that preserve the semantic content of the initial rendered image. Most recent advances in Im2Im translation for computer graphics started to employ additional information generated 2 by the game engine to achieve robustness in various Im2Im translation tasks, including photorealism enhancement [8] and style transfer [9], [10] in games. This information, typically referred to as G-Buffers, is generated through deferred rendering and provides information about scene depth, geometry (e.g., normals), and material properties (e.g., metallic, base color, and roughness) of the virtual objects. As result, these approaches have illustrated results that can preserve the overall semantic structure and content of the scene in addition to the material properties, such as color, of the objects. Moreover, considering that most of the G-Buffers (e.g., base color, metallic, and specular) are view and sequence-independent, these methods can maintain temporal consistency, which is important in time-sequential data such as videos and games. Despite significant research in both paired and unpaired Im2Im translation for deep learning research and games, to date, none of the approaches has reached commercial video game. These methods are typically categorized into two categories. The ones that can achieve fast inference time but are more prone to visual errors, and the methods that can achieve robust results but cannot reach real-time performance. While in deep-learning research, achieving fast inference time to generate photorealism-enhanced synthetic datasets is not required, games must achieve both factors in order to reach point that can be applied in commercial video games. III. METHODOLOGY In this section, we present the proposed framework, which can be split into four phases as illustrated in Fig. 1: A) collection of the video game and real-world datasets, B) training of robust unpaired Im2Im translation method to generate the photorealism enhanced the version of the video game dataset, C) training of lightweight paired Im2Im translation method on the video game and the generated photorealism enhanced dataset, and D) integration of the resulting paired Im2Im translation model into the game engine. A. Dataset Collection"
        },
        {
            "title": "In order to train a model",
            "content": "to enhance the photorealism of video game frames, two types of datasets are required: the source dataset captured through the game and the realworld dataset. Considering that the most robust methods utilize information that is generated through the game engine (GBuffers), this can be challenging task for closed-source games. An approach that can be used in this case is to detect and export this information directly from the GPU video memory, where this information is natively being generated [13]. In modern game engine, such as Unreal Engine1, this information can be exposed through post-processing materials and rendered and exported through render targets. On the other hand, real-world datasets that depict scenes similar to those in the game can be found in public benchmarks employed in deep learning research or captured manually using cameras in real-world environments. Examples include urban driving datasets such as Cityscapes [18] and KITTI [19] that have been 1https://www.unrealengine.com/en-US 3 Fig. 1. Overview of the REGEN framework, divided into four main phases: a) collection of an unpaired set between video game and real-world datasets, b) training of robust unpaired Im2Im translation network to produce semantically consistent photorealism-enhanced version of the game dataset, c) training of lightweight paired Im2Im translation method between the video game and the photorealism-enhanced datasets, and d) final integration in the game. employed to enhance the photorealism in simulated automotive scenarios or urban scenes in games, such as GTA [8]. B. Robust Unpaired Image-to-Image Translation Network Having access to the appropriate in-game and real-world datasets enables the training of robust unpaired Im2Im translation method to produce photorealism-enhanced version of the game dataset. In this work, we utilize the Enhancing Photorealism Enhancement (EPE) framework proposed by Richter et al. [8], which to date has illustrated SotA performance in enhancing the photorealism of computer-graphics-based applications and is the only method that was presented as an approach specifically designed for photorealism enhancement in games. In detail, the approach employs patch-matching approach prior to training to match patches between the source (game) and the target (real-world) domain that depict similar objects. This is performed by extracting features from these patches through VGG-16 architecture and executing similarity search by utilizing the Facebook AI Similarity Search (FAISS) library [22]. Additionally, the method employs the G-Buffers generated by the game engine and processes them through G-Buffer encoder, which consists of multiple streams for each semantic class of the game as defined by the stencil. This enables the network to treat individual virtual objects differently and utilize only the G-Buffers that are informative for each semantic class. For instance, the sky in virtual environment does not contain information about the material properties, and thus, the respective G-Buffers (e.g., metallic, base color, and roughness) always contain black pixels in that region. Finally, the approach employs robust semantic segmentation network, MSEG [23], that was trained on seven different semantic segmentation datasets (including Cityscapes) with unified semantic taxonomy of 194 classes. This enables the guidance of the network, and particularly the discriminator, which tries to classify whether generated sample is real or fake without requiring annotations for the real-world dataset that are also fully compatible with the ones generated by the stencil of the game engine. C. Paired Image-to-Image Translation Network After generating robust photorealism-enhanced dataset based on non-real-time unpaired Im2Im translation that is semantically consistent with the video game dataset, paired Im2Im translation method can be employed and trained on the in-game and photorealism-enhanced pairs. Pix2PixHD [6] is such translation method that is well-suited for translating games towards realism due to its ability to generate highresolution images, which are important for games. Pix2PixHD builds upon the original Pix2Pix model [7] by adding coarse-to-fine generator network, multi-scale discriminators, and feature matching losses. These additions enable it to generate visually coherent and detailed results even at resolutions of 20481024 with an inference delay of around 20-30 ms (33.33-50 FPS) with GTX 1080Ti GPU. One benefit of this method is that it supports supervised training with ground truth pairs, allowing for the use of pixel-level losses (i.e., L1, perceptual loss, and adversarial loss) to direct the translation. This generally leads to more stable and accurate outputs than unpaired Im2Im translation techniques, particularly when fine visual details and consistency with game geometry matter. D. Game Integration The need for integrating deep-learning models has emerged into various frameworks that enable seamless deployment across platforms such as mobile devices, edge computing units, and cloud infrastructure. Among them, Open Neural Network Exchange (ONNX) has become an essential framework for deploying machine learning based models into products due to its flexibility, which is ideal for real-time inference scenarios. Combined with ONNX Runtime2, optimized inference speed can be achieved by hardware accelerators such as TensorRT3, which enables the inference in mixed precision utilizing the tensor cores that are available in modern gaming and AI GPUs. Due to these significant features of ONNX Runtime, the library was also implemented in the latest version of Unreal Engine, Unreal Engine 5 (UE5)4, and Unity5, enabling the utilization of generative models as post-process filters. Since the proposed framework removes the requirement of additional G-Buffers imposed by robust unpaired Im2Im translation methods, the approach is fully compatible with the UE5 and Unity ONNX Runtime libraries, which are built solely for inference on the final rendered frame. IV. EXPERIMENTS In this section, we present qualitatively and quantitatively the results of the proposed approach and compare them to those of the EPE method and baseline lightweight Im2Im translation approach, CUT. The latter was trained directly using video game and real-world dataset. In detail, we enhance the photorealism of GTA towards the characteristics of the real-world Cityscapes dataset. In the following subsections, we provide information regarding the employed datasets and metrics, give the setup and implementation details, and present and discuss the results. A. Datasets Playing for Data (PFD) [13] is large-scale dataset extracted through GTA under various environmental conditions, namely, different weather scenarios and time of day. It includes 25, 000 images accompanied by pixel-level semantic segmentation annotations extracted by the game engines rendering pipeline. The classes are distributed among the subset of 19 Cityscapes classes typically used when benchmarking semantic segmentation models [24]. that Cityscapes is real-world dataset is considered standard benchmark for photorealism enhancement via Im2Im translation. Cityscapes images were captured across 50 cities in Germany under good weather conditions. The dataset includes pixel-level semantic segmentation annotation for 30 unique categories. The original dataset contains 5, 000 images, but it was also extended to larger version with 20, 000 frames accompanied by coarse annotations. B. Metrics 4 to evaluate the similarity between datasets that result from generative models and images of the target domain. Both FID and KID operate with the goal of quantifying the discrepancy between the distribution of the generated and the target (real) images by comparing their feature representations. These are extracted from pre-trained deep-learning architecture, which is typically the Inception-V3 model [27]. FID makes the assumption that feature embeddings follow multivariate Gaussian distribution and computes the Frechet distance between the two distributions. Lower FID scores indicate that the generated images are more similar to the target domain in terms of both visual quality and diversity. On the contrary, KID employs the squared Maximum Mean Discrepancy (MMD) with polynomial kernel to directly compare finite samples without assuming any specific underlying distribution. This marks KID as an unbiased estimator, making it more reliable for smaller sample sizes. Thus, KID has been proven to be better metric compared to FID [26] and has become the standard in evaluating the generative models performance. C. Setup & Implementation Details In [8], the EPE output between GTA and Cityscapes is provided for subset of 19, 252 photorealism enhanced frames of the PFD dataset (Enhanced PFD). We have thus employed those frames in order to create paired dataset between PFD and enhanced PFD to train Pix2PixHD with the default parameters at resolution of 960x512. In detail, Pix2PixHD was trained for 20 epochs, which took 1 day of training time to translate the PFD images towards the characteristics of the output produced by EPE (Enhanced PDF). The results of CUT [4] when employing PFD and Cityscapes as the source and target datasets, respectively, are also included in [8]. Since CUT is an unpaired Im2Im method that achieves good balance between accuracy and inference speed, we utilize its results as baseline. Thus, we used FID and KID to evaluate the similarity of CUT results, EPE initial results, and the photorealism-enhanced images produced by the proposed framework with the 5, 000 images of the real-world Cityscapes dataset. This evaluation approach highlights the impact on the performance of the proposed approach compared to the initial photorealism enhanced dataset produced by EPE. It also illustrates its benefits over directly training lightweight unpaired Im2Im translation (CUT) between the PFD and the Cityscapes dataset. gaming system configured with an Intel I7 14700F CPU, an NVIDIA RTX 4090 with 24GB VRAM, and 64GB of DDR4 system memory was utilized in all experiments. The inference speed and VRAM requirements of the models were measured while GTA was rendered on the same GPU at resolution of 1920x1080 in the minimum graphics settings of the game without any additional optimization (e.g., TensorRT). The Frechet Inception Distance (FID) [25] and the Kernel Inception Distance (KID) [26] are commonly employed D. Results and Discussion 2https://onnxruntime.ai/ 3https://developer.nvidia.com/tensorrt 4https://github.com/microsoft/OnnxRuntime-UnrealEngine 5https://github.com/asus4/onnxruntime-unity In this subsection, we present and discuss the results of the REGEN framework compared to the initial outputs of EPE and CUT trained directly between the PFD and the Cityscapes dataset. We additionally include the similarity between the 5 Fig. 2. Visual comparison of the images generated by (b) CUT, (c) EPE, and (d) REGEN, when given as input frame (a) from the PFD dataset (GTA V) that was not seen during training. TABLE COMPARISON OF UNPAIRED IM2IM TRANSLATION METHODS IN TERMS OF VISUAL REALISM, INFERENCE SPEED, AND MEMORY EFFICIENCY COMPARED TO THE PROPOSED REGEN FRAMEWORK. KID AND FID METRICS ARE EMPLOYED TO EVALUATE VISUAL REALISM. INFERENCE SPEED AND MEMORY REQUIREMENTS ARE REPORTED FOR THE ENTIRE SYSTEM WITHOUT ANY OPTIMIZATION (E.G., TENSORRT) WHILE GTA WAS RENDERED ON THE SAME GPU AT MINIMUM GRAPHICS SETTINGS. Method KID100 FID ms/iter Memory (GB) FPS PFD [13] CUT [4] EPE [8] REGEN 7.96 4.35 3.20 3.38 74.49 43.93 38.30 39.62 47.7 1110 33.53 11.7 9.5 11.5 20.96 0.9 29.83 game (PFD) and the Cityscapes datasets for reference. As illustrated in Table I, EPE generates frames that, as shown by the significantly reduced KID and FID metrics, are much more similar to Cityscapes than the ones included in PFD. The proposed REGEN approach, which was trained on the output of EPE in paired manner, managed to maintain competitive similarity with Cityscapes in terms of FID and KID. On the contrary, the generated samples from CUT illustrate significantly higher FID and KID scores, which indicate that while the method managed to reduce these metrics compared to images in PFD, the produced frames are still subject to visual artifacts that can be limiting factor in integrating this model in games. Concerning the performance of the models, the inference time, the FPS, and the GPU memory requirements of each of the approaches are provided. All the models were inferred to produce images of resolution of 960x512, which is similar to the ones used in [8] (957x526). As presented, while EPE does not require significant GPU memory, it is incapable of running in real-time, and thus, an interactive frame rate of 0.9 FPS is observed. On the other hand, CUT significantly improves the performance at 20 FPS; however, it requires more GPU VRAM. REGEN achieved the best balance between inference speed and memory requirements and is capable of achieving real-time performance of 30 FPS, which is close to the games standard. In particular, by learning to enhance the photorealism of GTA through the output of EPE, an improvement of 32.14x times in inference speed was achieved, while maintaining comparable visual results. It is worth noting that the memory requirements for all the methods are well within the capabilities of the modern mid-range GPUs (e.g., RTX 4060 Ti, RTX 4070). Visually, Fig. 2 showcases the output of EPE, CUT, and REGEN on two original rendered frames. It shall be noted that these frames were not seen during the training of the proposed approach. It is apparent that CUT introduces significant artifacts and image distortion, including the Mercedes-Benz logo of the ego vehicle, which is result of the distribution differences between PFD and Cityscapes, with the latter having the logo depicted across all of the samples. Additionally, it is evident that the visual results of the proposed framework do not contain any significant artifacts and are comparable to those produced by EPE. Particularly, the texture of the road is significantly improved, and the glossiness of the vehicles is increased. Additionally, since the generalization is important in games, we illustrate sequence of photorealism-enhanced frames while playing GTA with the proposed approach applied as post-processing filter. As illustrated in Fig. 3, REGEN preserves the content of the initial rendered GTA image and significantly improves the general photorealism of the virtual scene, similarly to EPE. Fig. 3. Photorealism enhanced game frames produced by REGEN while playing GTA V. 6 [12] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, Carla: An open urban driving simulator, 2017. [Online]. Available: https://arxiv.org/abs/1711. [13] S. R. Richter, V. Vineet, S. Roth, and V. Koltun, Playing for data: Ground truth from computer games, in European Conference on Computer Vision (ECCV), ser. LNCS, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds., vol. 9906. Springer International Publishing, 2016, pp. 102118. [14] M. Scucchia, M. Ferrara, and D. Maltoni, From gaming to research: Gta for synthetic data generation for robotics and navigations, 2025. [Online]. Available: https://arxiv.org/abs/2502.12303 [15] B. Kiefer, D. Ott, and A. Zell, Leveraging synthetic data in object detection on unmanned aerial vehicles, 2021. [Online]. Available: https://arxiv.org/abs/2112.12252 [16] S. Pasios and N. Nikolaidis, Carla2real: tool for reducing the sim2real appearance gap in carla simulator, IEEE Transactions on Intelligent Transportation Systems, pp. 115, 2025. [17] J. Han, M. Shoeiby, L. Petersson, and M. A. Armin, Dual contrastive learning for unsupervised image-to-image translation, in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021, pp. 746755. [18] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, The cityscapes dataset for semantic urban scene understanding, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 3213 3223. [19] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, Vision meets robotics: The kitti dataset, Int. J. Rob. Res., vol. 32, no. 11, p. 12311237, Sep. 2013. [Online]. Available: https://doi.org/10.1177/0278364913491297 [20] S. Brehm, S. Scherer, and R. Lienhart, Semantically consistent image-to-image translation for unsupervised domain adaptation, 2021. [Online]. Available: https://arxiv.org/abs/2111.03522 [21] J. Kang, B. Zang, and W. Cao, Domain adaptive semantic segmentation via image translation and representation alignment, in 2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom), 2021, pp. 509516. [22] J. Johnson, M. Douze, and H. Jegou, Billion-scale similarity search with gpus, IEEE Transactions on Big Data, vol. 7, no. 3, pp. 535547, 2021. [23] J. Lambert, Z. Liu, O. Sener, J. Hays, and V. Koltun, Mseg: composite dataset for multi-domain semantic segmentation, in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 28762885. [24] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, Segformer: Simple and efficient design for semantic segmentation Information Processing Systems, with transformers, 2021. [Online]. Available: https://api.semanticscholar.org/CorpusID: in Neural [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, Gans trained by two time-scale update rule converge to local nash equilibrium, in Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS17. Red Hook, NY, USA: Curran Associates Inc., 2017, p. 66296640. [26] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, Demystifying MMD gans, in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [Online]. Available: https://openreview.net/forum?id=r1lUOzWCW [27] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 28182826. [28] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 38133824. V. CONCLUSION In this paper, novel two-stage framework for real-time photorealism enhancement in games is proposed. We developed pipeline that learns to enhance the photorealism of the popular game, GTA V, towards the real-world dataset Cityscapes, by employing robust unpaired Im2Im translation method (EPE) to transform the problem into an easier, paired Im2Im translation task that can be performed with lightweight paired Im2Im architectures such as Pix2PixHD. To evaluate the contribution of the proposed framework, we employed the photorealism-enhanced results of EPE, the results of directly performing unpaired Im2Im translation with lightweight unpaired Im2Im method (CUT), and the ones produced by the introduced framework. The results illustrated that our approach can maintain comparable visual realism with EPE while increasing the inference speed up to 32.14x times. It was also shown that it can outperform CUT in terms of visual realism. This short paper outlines promising directions for future research on paired Im2Im translation methods specifically designed and optimized to accurately replicate the characteristics of generated game frames produced by the current robust Im2Im translation approaches (e.g., NST [9]) or potentially diffusion models that preserve the semantic content such as ControlNet [28], while achieving low GPU memory consumption and high throughput."
        },
        {
            "title": "REFERENCES",
            "content": "[1] S. S. Sengar, A. B. Hasan, S. Kumar, and F. Carroll, Generative artificial intelligence: systematic review and applications, Multimedia Tools and Applications, vol. 84, no. 21, pp. 23 66123 700, June 2025. [Online]. Available: https://doi.org/10.1007/s11042-024-20016-1 [2] H. Hoyez, C. Schockaert, J. Rambach, B. Mirbach, and D. Stricker, Unsupervised image-to-image translation: review, Sensors, vol. 22, no. 21, 2022. [Online]. Available: https://www.mdpi.com/1424-8220/ 22/21/8540 [3] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, Unpaired image-to-image translation using cycle-consistent adversarial networks, in 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2242 2251. [4] T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu, Contrastive learning for unpaired image-to-image translation, in Computer Vision ECCV 2020, A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, Eds. Cham: Springer International Publishing, 2020, pp. 319345. [5] M. Safayani, B. Mirzapour, H. aghaebrahimiyan, N. Salehi, and H. Ravaee, translation with content preserving perspective: review, 2025. [Online]. Available: https: //arxiv.org/abs/2502.08667 Unpaired image-to-image [6] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro, High-resolution image synthesis and semantic manipulation with conditional gans, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. [7] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, Image-to-image translation with conditional adversarial networks, in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5967 5976. [8] S. R. Richter, H. A. Alhaija, and V. Koltun, Enhancing photorealism enhancement, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 2, pp. 17001715, 2023. [9] E. Ioannou and S. Maddock, Towards real-time g-buffer-guided style transfer in computer games, IEEE Transactions on Games, pp. 19, 2024. [10] , Neural style transfer for computer games, 2023. [Online]. Available: https://arxiv.org/abs/2311. [11] T. Rott Shaham, M. Gharbi, R. Zhang, E. Shechtman, and T. Michaeli, Spatially-adaptive pixelwise networks for fast image translation, in Computer Vision and Pattern Recognition (CVPR), 2021."
        }
    ],
    "affiliations": [
        "Aristotle University of Thessaloniki, Informatics, 54124, Thessaloniki, Greece"
    ]
}