{
    "paper_title": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model",
    "authors": [
        "Jacqueline He",
        "Jonathan Hayase",
        "Wen-tau Yih",
        "Sewoong Oh",
        "Luke Zettlemoyer",
        "Pang Wei Koh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead."
        },
        {
            "title": "Start",
            "content": "Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Jacqueline He 1 Jonathan Hayase 1 Wen-tau Yih 1 Sewoong Oh 1 Luke Zettlemoyer 1 Pang Wei Koh 1 2 6 2 0 2 6 ] . [ 1 0 2 1 7 0 . 2 0 6 2 : r Abstract Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose ANCHORED DECODING, plugand-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to permissively trained safe LM. ANCHORED DECODING adaptively allocates user-chosen information budget over the generation trajectory and enforces per-step constraints that yield sequencelevel guarantee, enabling tunable riskutility trade-off. To make ANCHORED DECODING practically useful, we introduce new permissively trained safe model (TinyComma 1.8B), as well as ANCHOREDByte DECODING, bytelevel variant of our method that enables crossvocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. ANCHORED and ANCHOREDByte DECODING define new Pareto frontier, preserving nearoriginal fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and safe reference, at modest inference overhead. 1. Introduction The remarkable capabilities of modern language models (LMs) are fundamentally tied to the scale and diversity of 1University of Washington 2Allen Institute for ArtiJacqueline He Correspondence to: Intelligence. ficial <jyyh@cs.washington.edu>. Preprint. 1 their pre-training data. These corpora are often harvested from the open web with minimal filtering, and may contain sensitive or copyright-protected documents (Kandpal et al., 2025). LMs are able to memorize spans seen during pretraining and later emit them at inference time (Carlini et al., 2021; 2023; Karamolegkou et al., 2023). Such reproduction is often unauthorized and uncompensated with respect to the original creators (Kandpal & Raffel, 2025), and may expose developers to legal and privacy liabilities (Henderson et al., 2023). Remediation is difficult, as filtering data for sensitive documents and re-training frontier models is prohibitively expensive. Further, as copyrighted material is usually higher quality, its wholesale exclusion can significantly compromise end performance (Meeus et al., 2024). We address this problem with ANCHORED DECODING, practical inference-time method that interpolates between the next-token distributions of safe model and higherutility risky model. Formally, safe model is trained exclusively on public domain and openly licensed text (Open Knowledge Foundation, 2015). While this ecosystem is still nascent, it is expanding with the recent release of permissively licensed pre-training corpora (Min et al., 2024; Bommarito II et al., 2025; Kandpal et al., 2025; Langlais et al., 2025). Conversely, risky language model is trained on mixed-license sources that may include copyrighted or otherwise sensitive documents; almost all openweight (Grattafiori et al., 2024; Qwen Team, 2024; Gemma Team et al., 2025; Meta AI, 2025) and closed-weight (Anthropic, 2024; OpenAI et al., 2024; Gemini Team et al., 2025) LMs belong in this category. Safe LMs offer stronger compliance at the cost of performance (Kandpal et al., 2025), whereas risky LMs offer greater utility but higher risk of regurgitation. Fig. 1(a) illustrates this trade-off: given the start of popular novel, the risky LM generates its verbatim continuation, while the safe LM avoids reproduction but yields qualitatively less fluent output. At each decoding step, ANCHORED DECODING computes closed-form fusion of the risky and safe next-token distributions by choosing interpolation weights that satisfy local divergence budget relative to the safe model. We show these local constraints compose into sequence-level guarantee that provably satisfies the K-NAF criterion (Vyas Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model (a) Example generation with ANCHORED DECODING. (b) Token-level risk-utility tradeoffs. Figure 1. (a). Given the opening line of J.R.R. Tolkiens The Fellowship of the Ring (1954), the risky LM outputs its verbatim continuation, while the safe LM produces less fluent, repetitive alternative. ANCHORED DECODING generates in bounded proximity to the safe LM within budget K, while leveraging utility from the risky LM, and produces plausible, non-infringing continuation. (b). With the safe-risky LM pair {TinyComma 1.8B, Llama 3.1 70B}, ANCHORED DECODING (in purple) achieves the best risk-utility tradeoff. et al., 2023), mathematical measure that enforces userchosen budget on divergence from the generated distribution to the safe model, thereby providing principled control on copyright leakage risk. We further introduce two empirically effective adjustments that account for the non-uniform nature of distributional risk across generation: first, one-time, prompt-dependent prefix debt that reduces the initial budget based on how strongly the input prompt appears to have already been memorized by the risky LM, and second, an adaptive banking rule that saves unspent budget from low-risk steps for occasional high-risk spikes later in generation. Returning to Fig. 1(a), ANCHORED DECODING produces continuation that avoids reproduction while remaining fluent and plausible. Our decoding strategy is training-free, provides user-controllable tradeoff knob with formal bound, and retrofits to any off-the-shelf LM with exposed logits. ANCHORED DECODING holds less restrictive assumptions compared to prior two-model copyright mitigation approaches: it does not require hand-curated list of seed words (TOKENSWAP (Prashant et al., 2025)), nor does it require the stringent model-pair construction assumed by CP-FUSE (e.g., disjoint training shards), beyond access to permissively trained safe anchor (Abad et al., 2025). We next turn from method-level assumptions to more fundamental constraint. Many two-model fusion methods, including ANCHORED DECODING, assume shared vocabulary. This requirement is quite restrictive for copyright mitigation, as state-of-the-art safe models (e.g., Comma 7B (Kandpal et al., 2025)) employ bespoke tokenization schemes that render direct token-level fusion infeasible. We address this bottleneck via two contributions: first, to enable token-level vocabulary alignment, we release TinyComma 1.8B, compact variant of the Comma architecture, that is pre-trained exclusively on 169.5B tokens of openly licensed data from the Common Pile (Kandpal et al., 2025). By design, TinyComma 1.8B adopts the Llama 3.1 tokenizer (Grattafiori et al., 2024), facilitating direct compatibility with the Llama 3.1 family. Second, we introduce ANCHOREDByte DECODING, byte-level, KNAF compliant analogue of ANCHORED DECODING. ANCHOREDByte DECODING builds upon the BYTESAMPLER framework (Hayase et al., 2025) and bypasses tokenizer mismatch by operating on the next-byte distribution. Together, these advances support significantly broader range of model pairs than previously possible. When evaluated in realistic long-form settings, ANCHORED and ANCHOREDByte DECODING consistently achieve Pareto-optimality against strong mitigation baselines across six model pairs. They attain near-original fluency and factuality while closing up to 75% of the reduction between the risky baseline and safe reference. For example, we show risk-utility tradeoff plots in Fig. 1(b) for the model pair {TinyComma 1.8B, Llama 3.1 70B} (decoding at the token level). ANCHORED DECODING adds modest inference overhead (e.g., only 1.1x in this case) by pairing risky LMs with much smaller safe LM, making the second forward pass relatively cheap without sacrific2 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model ing effectiveness. We further analyze why ANCHORED DECODING works: our key intuition is that the per-step KL divergence between the risky and safe models is useful signal for identifying when generation has entered copyright-sensitive regime. Beyond copyright mitigation in LMs, ANCHORED DECODING is agnostic to tokenizer, modality, and domain. Our framework serves as generalpurpose tool that is applicable wherever high-capability generative process must be rigorously bounded by trusted reference distribution. 2. Preliminaries We consider token-level autoregressive language models that operate over fixed vocabulary V. Given prompt sequence x, define probability distribution over variablelength token sequences y0:T 1 with yt as p(y0:T 1 x) = 1 (cid:89) t=0 p(yt y<t, x) for Tmax, (1) where yT 1 = EOS denotes termination. We assume access to safe model ps trained only on permissively licensed text, and more performant risky model pr that may reproduce copyrighted data due to its broader, unchecked training set. pr and ps are assumed to induce the same support over V, which are typically satisfied by standard softmax parameterizations. Our goal is to find some target distribution that balances the competing objectives of copyright safety (from ps) and utility (from pr). K-Near Access-Freeness (K-NAF). First, we formally define our desired copyright safety condition. We adopt the K-NAF framework introduced by Vyas et al. (2023), which bounds the total divergence over the entire distribution of sequences generated by model relative to safe model ps: Definition 2.1. [Global K-NAF] Let 0 be global information budget. Formally, model satisfies the global K-NAF guarantee (relative to ps) if, for every input sequence and every Tmax, D(cid:0)p(y<T x) (cid:13) (cid:13) ps(y<T x)(cid:1) K, (2) where y<T = (y0, . . . , yT 1), and is some arbitrary divergence function. We primarily consider = DKL, the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951).1 Throughout, we write p( x) as shorthand for the autoregressive model and its induced finite-horizon prefix distributions {p(y<T x)}T Tmax. 1For generality, we show that the guarantees to our method also hold when setting as the -Renyi divergence in Appendix B.3, and show experimental ablations in Sec. 5.2. 3. ANCHORED DECODING 3.1. Tractable Token-Level Approximation Our goal is to construct new distribution that approximates pr to maximize utility, while remaining strictly subject to the safety constraint relative to ps (Definition 2.1). This yields the constrained sequence-level optimization: = arg min p"
        },
        {
            "title": "DKL",
            "content": "(cid:0)p( x) pr( x)(cid:1) (3) s.t. DKL (cid:0)p( x) ps( x)(cid:1) K. As Eq. (3) is defined at the sequence level, solving this directly is computationally intractable, as it becomes search problem over the exponentially large Tmax. Therefore, similar to Abad et al. (2025), we approximate the global objective by decomposing the problem into series of local (per-token) objectives. Concretely, rather than solving for an optimal sequence-level spending schedule subject to global budget K, we impose per-step budget kt at each timestep t. The sequence-level constraint in Eq. (3) can be decomposed via the chain rule of KL divergence into sum of per-step conditional divergences. Accordingly, at each and for any prefix y<t p(x) generated thus far, we solve local constrained problem to obtain an optimal next-token distribution, ( y<t, x), that stays within the per-step budget kt relative to ps, while drawing close to pr: ( y<t, x) = arg min p(V) DKL (cid:0)p pr( y<t, x)(cid:1) (4) s.t. DKL (cid:88) p(y) = 1, (cid:0)p ps( y<t, x)(cid:1) kt, p(y) > 0 V. yV By the chain rule for KL, any continuation generated autoregressively by {p (y<t, x)}t<Tmax of length Tmax satisfies Definition 2.1, as the following theorem shows: Theorem 3.1 (Safety of local approximation). Let be sequence-level distribution defined autoregressively by p(y<T x) = (cid:81)T 1 If, for all decoding (yty<t, x). steps < Tmax, the conditional distribution solves Eq. (4) with per-step budget kt such that (cid:80)Tmax1 kt K, then satisfies the global K-NAF guarantee in Eq. (3) for all Tmax. t=0 t=0 In effect, our approximation yields valid solution to the original optimization. We further note simple corollary: Corollary 3.2 (Constant per-step cap). Setting kt for all < Tmax satisfies the condition of Theorem 3.1 whenever Tmax K. 3 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Algorithm 1 ANCHOREDDECODE(pr, ps, K, Tmax, n, S) 1: Input: risky LM pr, safe LM ps, global budget K, max length Tmax, prompt x, debt window n, special tokens S. 2: Output: generation = (y0, . . . , yt), where < Tmax. 3: Init: δinit(x) PREFIXDEBT(pr, ps, x, n, S); {Alg. 2} 4: Init: local cap K/Tmax; cumulative expenditure A0 Compute: pr( y<t, x) and ps( y<t, x). Accrue budget: kt max(cid:0)0, (t+1) At PROJECTKL(cid:0)pr; ps, kt Project: Sample: yt ( y<t, x). Bank realized spend: At+1 At + DKL if yt = EOS then δinit(x); history y<1 ; 5: for = 0 to Tmax 1 do 6: 7: 8: 9: 10: 11: 12: end if 13: 14: end for 15: Return = (y0, . . . , yt). break (cid:1). (cid:1). {Alg. 3} (cid:0)p ps (cid:1). 3.2. Solving for fused distribution Eq. (4) admits closed-form solution that can be efficiently computed at each decoding step: Proposition 3.3 (Solving for kt at decoding step t, the optimal distribution Eq. (4) is weighted geometric mean: ). For given local budget that solves per-step budget is effectively zero in early steps (i.e., we clamp negative kt to 0). Thus, our conservative approach is more likely to sample exclusively from ps in early steps for memorization-triggering prompts. Inspired by pretraining data detection (Shi et al., 2024; Zhang et al., 2025a), we focus on the largest log-likelihood ratios (LLRs) in the prefix. Intuitively, extremely positive LLR outliers mark tokens for which pr and ps disagree strongly, suggesting higher memorization likelihood. Let x=(x0, x1, . . . , xL1) be prefix sequence of length L>1. For each position {1, . . . , 1}, define the pointwise LLR as ℓi(x) := log pr(xix<i) ps(xix<i) . Let [z]+ := max(z, 0) denote the positive part of z, and let In(x) be the index set of the largest values among {[ℓi(x)]+}L1 i=1 (ties broken arbitrarily; if 1<n, take all indices). The prefix debt is δinit(x) := 1 max{1, In(x)} (cid:88) iIn(x) [ℓi(x)]+. (6) δinit(x) acts as one-time reduction of the global budget. As δinit(x) 0 by construction, δinit(x) K, ensuring that the safety guarantee is preserved (and typically tightened) regardless of the prompts content.3 = 1 ps( y<t, x) λ 1+λ pr( y<t, x) 1 1+λ , (5) 3.4. An adaptive budgeting strategy where is normalization constant and λ 0 is the dual variable (Lagrange multiplier) associated with the KL constraint. (cid:0)p In practice, determining the optimal λ (equivalently, the λ mixing weight 1+λ ) reduces to 1D root-finding problem. When the constraint is active, we solve for λ 0 (cid:1) kt = 0, using such that (λ) := DKL safeguarded Newton-Raphson algorithm to ensure fast convergence to feasible solution (Ypma, 1995).2 We further propose two empirically effective improvements to budget allocation that still respect the global budget K: one-time prefix-debt offset δinit(x), and an adaptive rule for defining the stepwise budget kt. (λ) ps,t 3.3. Prefix debt δinit(x) Not all input prefixes are equally likely to elicit regurgitated continuations. generic prompt poses little risk, whereas the opening hook of famous novel may statistically prime the model to output memorized text. We propose to exploit this early signal by offsetting the global budget with some context-dependent debt, δinit(x), reflecting the intuition that risky prompt has already pre-spent some of its safety margin by the start of generation. By initializing the banked budget with negative offset, the available 2The optimization procedure is shown in Alg. 3. While setting constant local cap kt satisfies the global safety condition whenever Tmax (Corollary 3.2), this naive allocation is often overly conservative. The constraint DKL(p ps) applies the same allowance at every step and cannot bank unused budget from easy steps (i.e., where the models naturally agree, and is already close to ps) for later steps. We therefore propose an adaptive budget parameterized by base rate (the knob we sweep) that tracks realized spend and rolls unused budget forward. For shorthand, denote (y<i, x) for timestep i, and analogously for ps,i. := Proposition 3.4 (Global safety of adaptive banking). Let be the global safety budget for sequence up to length (cid:1) (the Tmax, and let := K/Tmax. Set ai := DKL actual KL expenditure at each step i). If, at each decoding step < Tmax, the per-step adaptive budget kt is defined as ps,i (cid:0)p (cid:32) kt := max 0, (t + 1)k (cid:33) ai δinit(x) , (7) t1 (cid:88) i= where δinit(x) 0 is some initial budget adjustment for the input prefix x, then the resulting autoregressive sequence distribution p(y<T x) = (cid:81)T 1 (yty<t, x) satisfies DKL(p ps) δinit(x) for any Tmax. 3We sketch out the prefix debt calculation in Alg. 2. t=0 4 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model 3.5. Putting ANCHORED DECODING together Our complete method (Alg. 1) solves for fused distribution of the form in Sec. 3.2 at every decoding step. We treat as the user-set nominal per-step allotment; for horizon Tmax, we set the corresponding global budget to := Tmax (satisfying Corollary 3.2). The prefix debt δinit(x) (Sec. 3.3) is then applied as conservative offset, yielding an effective budget δinit(x). This remaining budget is allocated over the course of decoding, via the adaptive budgeting rule defined in Sec. 3.4. Consequently, ANCHORED DECODING satisfies the global K-NAF guarantee for horizon Tmax, while allowing for tunable perstep allotment and the debt window n. 3.6. ANCHOREDByte DECODING ANCHORED DECODING requires pr and ps to share the same vocabulary space, which substantially restricts the set of feasible model pairs. Many safe models ps (e.g., Comma 7B) use bespoke tokenizers to ensure permissive data usage throughout the language modeling pipeline, making direct token-level fusion with popular model families (e.g., Llama 3) incompatible. This motivates ANCHOREDByte DECODING, byte-level version of our method that supports crosstokenizer compatibility, and retains the weaker assumption that pr and ps have Byte Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016) tokenizers that induce mapping from tokens to UTF-8 byte strings. Defining the byte probability space. We treat the decoding process as sequence of byte-level transitions = (b0, b1, ..., bB1) for Bmax using the ByteSampler (Hayase et al., 2025) framework. At each step t, ByteSampler induces next-byte distribution over = {0x00, ..., 0xF } by marginalizing the models token distribution over all valid tokenizations consistent with the current byte prefix b<t. The probability of byte is obtained by summing the probabilities of valid token continuations whose decoded strings have as the next byte, where validity is determined by the models tokenizer state for b<t. Hayase et al. (2025) implement this marginalization using Valid Covering Tree traversal, which efficiently produces an exact next-byte distribution induced by the underlying token-level model and tokenizer. ANCHOREDByte DECODING satisfies K-NAF. ANCHOREDByte DECODING solves an analogous optimization to Eq. (4) under similar prefix debt calculation to Eq. (6) and budgeting rule to Eq. (7). The only change when moving to the byte level is that we now operate over the byte space instead of V, and use the induced bytelevel distributions of pr and ps. Thus, ANCHOREDByte DECODING remains K-NAF compliant for = kBmax, where Bmax is the maximum number of generated bytes.4 With modern BPE tokenization, one token corresponds to approximately 4 bytes in English (OpenAI, 2022; Pagnoni et al., 2025). This results in more decoding steps for the same semantic length, so we set Bmax 4Tmax. 3.7. Evaluation metrics As the determination of copyright infringement is inherently contextual, we measure copyright risk and utility using long-form metrics that holistically score generations.5 Copyright infringement. We consider real copyright risk and evaluate on snippets from BOOKS, domain of 16 novels taken from CopyBench (Chen et al., 2024) that are currently protected under U.S. copyright law, and are identified as likely to have been memorized by LLMs (Chang et al., 2023; Shi et al., 2024). Following Chen et al. (2024); Wei et al. (2024), we employ the following six metrics (denoted as M): ROUGE-1 and ROUGE-L (Lin, 2004) above set threshold τ = 0.4, MinHash similarity (Broder, 1997), and word-level Accumulated Common Substring (ACS) measure near duplicate copying, while word-level and character-level Longest Common Substring (LCS) measure the extent of exact match.6 4. Experiments We assign equal weight to each metric and aggregate them into single normalized copying reduction (NCR) score: formally, let mr and ms denote the metric values for pr and ps, respectively. For given setting with metric value m, we define its NCR to be mrm mrms . NCR quantifies the fraction of the performance gap between pr and ps that is closed by the setting. Crucially, we treat ps as the gold standard for safety rather than aiming for metric value of zero. Since any non-infringing natural language model may exhibit non-zero baseline overlap due to common linguistic structures, ps approximates the lower bound of incidental copying achievable without sacrificing fluency. We report the final aggregate result as the average of NCR values across all 6 metrics in M. Finally, we define high-protection regime as threshold where the NCR is at least 75%. While the optimal safety threshold is often task-specific, practical deployment typically requires substantial reduction in copying risk, not marginal improvements. We thus choose this point to isolate regimes where the mitigation effect size is dominant. 4Please see Appendix B.2 for byte-level decoding details. 5We provide more evaluation details in Appendix D. 6We also provide non-literal copying experiments in Appendix E.3. 5 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Utility. We consider two types of general utility: fluency and factuality. Fluency measures how natural and well-formed the output reads as; we follow Chen et al. (2024) and evaluate the quality of BOOKS continuations using Prometheus-v2 (Kim et al., 2024), an LLM-as-ajudge (Zheng et al., 2023) that scores output along fivepoint rubric (5 being the highest). Factuality is property that should be preserved, as U.S. copyright law only protects the original expression of factual knowledge, but not facts themselves (Feist Publications, Inc. v. Rural Telephone Service Co., Inc.). Consequently, we evaluate long-form factuality on BIOS, collection of biography generation prompts used by FActScore (Min et al., 2023). FActScore is fine-grained metric that decomposes each output into set of atomic, verifiable claims (Song et al., 2024), and fact-checks each claim independently against retrieved web search snippets. We report the average claim precisionthe fraction of unique supported claims. 4.1. Inference-time baselines We summarize the mitigation baselines used in our experiments. We defer broader discussion to Appendix A.2, and provide implementation details in Appendix D.2. Single-model decoding baselines. In SYSTEM, we prepend system prompt that instructs LMs to refrain from outputting copyrighted material (Chen et al., 2024; The Mosaic Research Team, 2024; Wei et al., 2024; Aerni et al., 2025). MEMFREE (Ippolito et al., 2023) is decoding method that blocks exact n-gram regurgitation by rejecting any next token that would complete an n-gram from reference-derived blocklist; we sweep {3, 5, 7, 9, 10}. Reversed Context Aware Decoding (Wei et al., 2024), or RCAD, contrasts logits with and without blocklisted context and produces next-token distribution that downweights tokens favored by context (modulated using α); we sweep α {0.1, 0.25, 0.5, 0.75, 1.0}. We apply these baselines to pr, and for MEMFREE and RCAD, we simulate realistic deployment and construct the context blocklist by retrieving the top-1 passage from an in-domain datastore (e.g., Books3 for BOOKS, and Wikipedia for BIOS). Joint-model decoding baselines. We consider CPFUSE (Abad et al., 2025), K-NAF-inspired fusion method that selects next-token distribution by balancing proximity to two LMs of equal utility. CP-Fuse assumes the copyrighted portions of the training data can be cleanly separated across the two models (i.e., disjoint data shards). It solves for per-step fused distribution by minimizing the maximum KL divergence across the model pair. TOKENSWAP (Prashant et al., 2025) constructs hybrid nexttoken distribution by swapping manually defined set of common tokens (e.g., function words) from small model onto large models distribution, while leaving all other token probabilities unchanged. For baseline consistency, we instantiate CP-FUSE and TOKENSWAP using our asymmetric pair (pr, ps). 4.2. Model pairs Following prior memorization work (Carlini et al., 2021; 2023; Abad et al., 2025; Prashant et al., 2025), we instantiate ps and pr as base (non-instruction-tuned) LMs to isolate memorization effects in the underlying nexttoken distributions. For risky models pr, we choose Llama 3.1 70B (Grattafiori et al., 2024), Qwen 2.5 72B (Qwen Team, 2024), and Llama 4 Scout 17Bx16E (Meta AI, 2025), which exhibit measurable verbatim reproduction in our copying evaluations. We select capable safe models ps that are trained on the Common Pile (Kandpal et al., 2025). To ensure tokenizer compatibility with Llama 3.1, we pre-train our own TinyComma 1.8B on the 169.5B tokens from the Common Pile, which outperforms other size-matched ps (Min et al., 2024; Bommarito II et al., 2025; Langlais et al., 2025) on general language understanding tasks.7 We also use the larger and more performant COMMA 7B (Kandpal et al., 2025), which was trained for 2T tokens and has custom tokenizer. Our selection of ps, pr leads to six model pairs, of which only TinyComma 1.8B and Llama 3.1 70B are tokenizer-matched. We apply ANCHORED DECODING to that pair, and ANCHOREDByte DECODING to all others. 4.3. Hyperparameters sweep our methods of k: We {0.1, 0.5, 1, 1.5, 2, 3, 4, 5, 10, 15, 20}. We set Tmax=200 and Bmax=800 bytes, and n=5 as the debt window.8 across range 5. Results 5.1. Riskutility trade-offs Fig. 1(b) and Fig. 2 show trade-off curves among mitigation baselines for (token-level) ANCHORED and (byte9 The level) ANCHOREDByte DECODING, respectively. results are striking: at both granularities, our methods trace the Pareto frontier (upper-right zone) across all model pairs, with statistical significance over 3 random seeds. Table 1 highlights the best utility achieved by each method among configurations that meet the high-protection operating point (i.e., NCR75%). Our methods consistently 7In Appendix C.1, we provide TinyComma 1.8B pre-training details and show benchmarking results for various ps candidates on standard natural language tasks. 8We find ANCHORED DECODING to be largely insensitive to the choice of and provide more details in Appendix C.3. 9Appendix D.6 shows qualitative examples. 6 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 2. ANCHOREDByte DECODING (in purple) achieves the best risk-utility tradeoff at the byte level across five model pairs. We report the average of three seeds; error bars show standard deviation. The shaded threshold denotes the high-protection operating point, where the Normalized Copyright Reduction (NCR) 75%. NCR and fluency are evaluated on BOOKS, and factuality on BIOS. Table 1. High-protection operating point (normalized copying reduction 75%). We report the best utility (FACTUALITY / FLUENCY) among settings that achieve NCR 75%. We show average and standard deviation values over three seeds. Entries are if the method does not reach the threshold. : Token-level decoding; all other model pairs use byte-level decoding. Method Vanilla SAFE RISKY Single-model baselines SYSTEM MEMFREE RCAD Two-model baselines CP-FUSE TOKENSWAP Our method Factuality / Fluency ps: TINYCOMMA 1.8B pr: LLAMA 3.1 70B Factuality / Fluency ps: COMMA 7B pr: LLAMA 3.1 70B Factuality / Fluency ps: TINYCOMMA 1.8B pr: QWEN 2.5 72B Factuality / Fluency ps: COMMA 7B Factuality / Fluency ps: TINYCOMMA 1.8B Factuality / Fluency ps: COMMA 7B pr: QWEN 2.5 72B pr: LLAMA 4 17B16E pr: LLAMA 4 17B16E 0.090.01 / 3.00.04 0.160.01 / 4.10.10 0.090.00 / 3.00.07 0.160.01 / 4.10.10 0.090.00 / 3.00.07 0.160.01 / 4.10.10 0.370.02 / 3.180.05 0.370.00 / 3.380.02 0.460.00 / 3.460. 0.360.00 / 3.480.03 0.360.00 / 3.480.03 0.200.00 / 3.210.06 0.440.00 / 3.770.03 0.230.03 / 3.750.06 0.490.02 / 3.900. 0.180.02 / 3.260.07 0.370.00 / 3.800.03 0.250.02 / 3.950.06 0.420.01 / 3.980.07 0.270.00 / 3.970.02 0.470.01 / 3.750.02 0.370.00 / 4.300.02 PROXIMAL/PROXIMALByte 0.530.02 / 4.020. 0.520.01 / 4.230.08 0.480.01 / 4.270.05 0.490.00 / 4.300.05 0.560.02 / 4.430.04 0.560.01 / 4.460.02 yield the strongest utility within this region. For example, under the token-level pair {TinyComma 1.8B, Llama 3.1 70B}, ANCHORED DECODING achieves 0.53 in factuality and 4.02 in fluency, surpassing the strongest two-model baseline that meets the threshold (TOKENSWAP: 0.44 / 3.77) and the single-model baselines that do (e.g., RCAD: 0.37 / 3.38; MEMFREE: 0.37 / 3.18). Similar trends persist at the byte level: for {Comma 7B, Llama 3.1 70B}, ANCHOREDByte DECODING achieves 0.52 / 4.23, exceeding CP-FUSE (0.23 / 3.75), TOKENSWAP (0.49 / 3.89), and RCAD (0.46 / 3.46). within the high-protection operating point, they also experience worse utility. Among parametric baselines, both RCAD and MEMFREE tend to operate well below the high-protection operating point; even when they do surpass the threshold, it comes at substantial utility cost. 5.2. ANCHORED DECODING ablations We choose the token-level pair {TinyComma 1.8B, Llama 3.1 70B} to study three ablation axes: (i) optimization objective, (ii) prefix debt, and (iii) budgeting strategy. Among pointwise baselines, SYSTEM shows high utility but scarcely achieves copying reduction (and for pr=Qwen 2.5 72B, is even slightly worse than pr). While CPFUSE and TOKENSWAP achieve high NCR and mostly fall Optimization objective. First, in our ANCHORED DECODING setting, we take the -Renyi divergence function as our divergence metric (Renyi, 1961), which supplies worst-case guarantee instead of an average-case one, Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model and is commonly employed in sensitive machine learning applications such as differential privacy (Dwork, 2006; Mironov, 2017), where the objective is to bound the maximum possible information leakage from any single observation. The alternative derivation of ANCHORED DECODING with -Renyi divergence leads to an analytical, closed-form solution. We show that this instantiation satisfies K-NAF in Appendix B.3. In NoOpt, we ablate the optimization and retain the budget and prefix debt; at each step, we sample from pr if DKL(pr ps) kt, and ps, otherwise. Finally, in ColdStart, we use the per-step cap and sample only from ps for the first 10 steps, after which we sample from pr. consistently worse trade-off, underscoring our treatment of prefix debt as tail statistic. Removing the prefix debt altogether (NoDebt) further degrades the curve. And for (iii) Budgeting Strategy, ANCHORED DECODINGs adaptive budgetwhich accounts for realized spend in prior stepsleads to strictly better trade-off than either constant cap (Fixed) or holistic lump-sum budget (Global). Table 2. Token-level wall-clock benchmarking. We report the time to first token (TTFT), throughput slowdown ratio relative to pr (TPS Ratio), and FLOPs/token estimate (Appendix D.5). TTFT (ms) FLOPs Estimate (FLOPs/token) TPS Ratio (vs. pr, ) Method Prefix debt δinit(x). We ablate the prefix debt by removing it entirely (NoDebt). In AvgDebt, we experiment with treat δinit(x) as an aggregate statistic by averaging over all prefix LLRs instead of taking the top-n largest values. Reference LMs pr = Llama 3.1 70B ps = TinyComma 1.8B 181.3 80.1 Single-Model Baselines (using pr) Budget allocation. We explore alternative budget allocation schemes: in Fixed, we assign per-step, constant budget k, with no rollover of unused budget across timesteps. In Global, we allocate the full budget = kTmax upfront and enforce only cumulative constraint: we decode from pr until the running KL spend from ps reaches K, then switch to sampling from ps for the rest of generation. SYSTEM MEMFREE RCAD 184.4 186.3 223.6 Two-Model Methods (using pr and ps) CP-FUSE TOKEN SWAP ANCHORED DECODING 210.6 204.0 195. 1.0 1.0 1.0 2.0 1.3 1.3 1.1 140109 3.6109 140109 140109 280109 143.6109 143.6109 143.6 Figure 3. Risk-utility tradeoffs for ANCHORED DECODING ablations. We ablate three axes: (i) optimization objective, (ii) prefix debt, and (iii) budgeting strategy. For brevity, our methods are labeled as ANC. DEC. Ablation results. Fig. 3 shows ablation trade-off plots. For (1) Optimization Objective, ANCHORED DECODING is strictly more Pareto-optimal than both NoOpt and ColdStart ablations. Our -Renyi version, ANCHORED DECODING, attains better fluency trade-off but worse factuality than our KL-based formulation, and serves as strong, principled alternative for practitioners. Figure 4. Top: Per-step KL(prps) histogram when sampling from pr, conditioned on prefixes different domains. The Copyright domain is more right-shifted than the Creative and Factual domains. Bottom: Unconditional CCDF of per-step KL(prps), shown for q90. q90 is computed from per-step KL values pooled across domains (shared cutoff per panel). The Copyright domain has heavier extreme tail than others. 5.3. Efficiency For (ii) Prefix Debt, replacing our top-n LLR aggregation with an average over all prefix LLRs (AvgDebt) yields practical consideration for safety-constrained decoding is inference-time overhead. Table 2 reports time to 8 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 5. High-copying regions are front-loaded under both byte-level and token-level decoding. We plot histograms (bin width of 5) of the start position of copying metrics (LCS and ACS) on COPYRIGHT generations. Copying tends to cluster at early positions. first token (TTFT), throughput slowdown relative to pr (TPS Ratio), and FLOPs estimate for the token-level pair {TinyComma 1.8B, Llama 3.1 70B}, computed as 2(Nr + Ns), where Nr, Ns are parameter counts for pr, ps, respectively (see Appendix D.5 for formal derivations).10 We run all settings on 2 140 GiB NVIDIA H200s. As expected for joint-model decoding, ANCHORED DECODING incurs modest throughput overhead, operating 1.1 slower than standalone pr decoding. While the additional forward pass with ps increases arithmetic compute by only 2.6% (from 143.6 to 140 GFLOPs/token), the observed wall-clock slowdown is consistent with bandwidthand synchronization-bound overheads from logit fusion. All methods are more efficient than RCAD, which requires two forward passes with pr. 6. Analysis We perform targeted experiments to motivate the design choices behind ANCHORED DECODING (Sec. 6.1). We first find that per-step KL(pr ps) serves as reliable proxy for identifying memorization by pr but not ps (Sec. 6.2). Next, we show that copying risk is front-loaded in early generation, motivating the prefix debt (Sec. 6.3). 6.1. Experimental Setting We conduct our analyses at token and byte granularities using the representative model pair {ps = TinyComma 1.8B, pr = Llama 3.1 70B} on three prompt domains (180 prefixes each). Copyright contains copyright-protected book excerpts 10We provide byte-level wall-clock measurements in Appendix D.5. from BOOKS. Factual consists of biography generation prompts from FActScore. potential confound is that these two prompt sets differ in domain and style, so any diagnostic separation may reflect distributional shift. As control, we introduce Creative domain by scraping original story prompts from Reddits r/WritingPrompts community (from 20242025, after Llama 3.1s training cutoff). Creative is roughly in-distribution to Copyright; Table 15 shows qualitative examples. Neither LM should have memorized any Creative or Factual prompts, but pr has likely memorized some Copyright prompts. 6.2. KL is Useful Memorization Diagnostic We test whether the per-step divergence between risky and safe models can meaningfully separate copyright-sensitive prefixes from benign ones. Namely, for each prefix x, we sample continuation from pr for =50 tokens (or B=200 bytes), and compute DKL(pr( y<t, x) ps( y<t, x)) at each decoding step of the rollout. When pr follows memorized passage that ps has not learned, then prs next-token distribution becomes sharply peaked on specific continuation that ps considers unlikely. This mismatch yields consistently large per-step DKL(prps) values across decoding steps. Consistent with this intuition, the top row of Fig. 4 shows that Copyright prefixes are systematically right-shifted relative to Creative and Factual ones. This difference is most pronounced in the extreme right tail (bottom row): we plot the empirical CCDF of per-step DKL(prps) for q(pooled) is the pooled 90th percentile 90 (shared cutoff per panel). Under this common threshold, the Copyright domain has the largest tail massi.e., for any large x, it has the highest fraction of steps with DKL(prps) xwhile Creative and Factual place lit- , where q(pooled) Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 6. Left: KDEs of per-step prefix log-likelihood ratios (LLR) at the token and byte levels. positive LLR means that pr assigns higher probability than the ps to the realized next step; large positive LLR events occur most often in Copyright. Right: KDEs of δinit(x), the mean of the top-5 positive prefix LLRs. Copyright prefixes are markedly right-shifted relative to Creative and Factual. tle mass on such extreme deviations. These results motivate using per-step KL(prps) to detect when generation is in copyright-sensitive regime. 6.3. Copying Risk is Front-loaded in Early Generation Our second finding is that the generation of tokens favored by pr and not ps is not uniformly distributed over continuation: copying events tend to occur early in generation. To quantify this positional bias, we compute two surfacecopying metricsthe longest common substring (LCS) and the average common substring (ACS)and, for each continuation from Copyright prompt, we record the start position of the first matched substring (i.e., where copyright infringement begins). We report two regimes: (i) any overlap (ACS, LCS > 0) and (ii) substantial overlap (ACS, LCS 5). As Fig. 5 shows, overlap events concentrate heavily at the beginning and drop sharply with generation across both regimes. This front-loading behavior motivates stronger copyright-mitigation intervention in early decoding steps, which we operationalize by intentionally forcing greater reliance on ps via the prefix debt. δinit(x) treats the prompt as memorization prior and debits the global budget in proportion to how strongly the prefix favors pr over ps. Concretely, we estimate δinit(x) from the upper tail of prefix log-likelihood ratios (average of the top-n positive LLRs), so that small number of outlier prompt tokens (or bytes) can trigger greater reliance on ps. Copyright prefixes display heavier right tails in prefix LLRs than either Copyright or Factual prefixes, persisting into extreme quantiles (e.g., the 99.5 percentile)  (Fig. 6)  . When we directly examine prefix debt values (e.g., the average of the top-5 largest positive prefix LLRs), this tail behavior translates into similar right-shifted distribution for Copyright. These patterns justify cold start only when prefixes exhibit evidence of asymmetric memorization. 7. Discussion ANCHORED DECODING confers several desirable properties. The first is flexibility: by providing controllable sequence-level safety budget K, our strategy allows practitioners to specify desired risk threshold that retains formal guarantees. Second, ANCHORED DECODING is practical, as it requires neither re-training nor access to the original pre-training data at inference time. This allows for the immediate and retroactive safe-guarding of high-utility models with considerable infringement leakage. Moreover, through ANCHOREDByte DECODING, we demonstrate that our strategy is tokenizer-agnostic and works effectively across model pairs with mismatched vocabularies. Finally, we show that ANCHORED DECODING yields strong riskutility trade-off, leveraging the observation that the contrast between risky and safe models provides signal for when generation is likely in copyrighted regime. While Appendix A.3 discusses future directions in greater detail, we note that ANCHORED DECODING extends beyond copyright mitigation in LMs. The same mechanism applies wherever high-capability model must be bounded by trusted anchor distribution, for example, to reduce sensitive-attribute leakage, enforce domain or policy restrictions for safety, or align generation with licensed corpora in enterprise settings. Ultimately, our results motivate reference-anchored decoding paradigm in which practitioners choose the reference model to match the compliance target of interest. 10 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model 8. Impact Statement Our work addresses critical and timely challenge at the intersection of generative AI and intellectual property, amid recent series of ongoing high-profile litigation and rapidly evolving legal landscape. LMs are trained on large-scale, web-scraped corpora that may include copyright-protected material, which precipitates several harms. On the creator side, model outputs that exhibit substantial similarity to protected training examples may infringe on intellectual property rights and erode the market value of original works (Henderson et al., 2023). These issues are compounded by the lack of systematic mechanisms for creator consent (Longpre et al., 2024) or compensation (Baack et al., 2025; Kandpal & Raffel, 2025). On the developer side, as the applicability of the U.S. fair use doctrine (United States Code, 2024) to model training remains unsettled, LM memorization and reproduction may expose developers to significant liability risk. ANCHORED DECODING targets these concerns as post-hoc technical mitigation strategy that constrains generation toward trusted reference distribution at inference time, reducing verbatim reproduction of protected documents and promoting more transformative outputs. And in order to more closely bridge theory and deployment, we design ANCHORED DECODING to be universally compatible with modern LMs. When tokenizers are aligned, we may operate at the token level; otherwise, our byte-level integration removes the shared-vocabulary requirement and supports LM pairs with mismatched tokenizers. Nevertheless, ANCHORED DECODING is not silver bullet; it cannot completely eliminate infringement risk, and our guarantees and empirical results apply only under the assumptions and evaluation protocol of our study. In light of ongoing legal and policy discourse surrounding generative AI, no legal conclusions ought to be inferred from this work. Ultimately, we view our contribution as complementary to other safeguards and as one part of broader toolbox for responsible deployment. 9. Acknowledgments We are very thankful to Boyi Wei, Goncalo Faria, Parjanya Prajakta Prashant, Tomasz Limisiewicz, and Tong Chen for helpful discussions about methodology and evaluation, Howard Yen for sharing retrieval infrastructure and feedback on framing, and Oscar Yinn and Stella Li for support. Jacqueline He is supported by an NSF Graduate Research Fellowship and the Meta AI Mentorship program. Jonathan Hayase and Sewoong Oh are supported by NSF grants 2112471, 2229876, and 2505865. This work was also supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP2024-001) and the AI2050 program at Schmidt Sciences."
        },
        {
            "title": "References",
            "content": "Abad, J., Donhauser, K., Pinto, F., and Yang, F. Copyrightprotected language generation via adaptive model fuIn The Thirteenth International Conference on sion. Learning Representations, 2025. URL https:// openreview.net/forum?id=kRoWeLTpL4. Aerni, M., Rando, J., Debenedetti, E., Carlini, N., Ippolito, D., and Tram`er, F. Measuring non-adversarial reproducIn The tion of training data in large language models. Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=590yfqz1LE. Anthropic. The claude 3 model family: Opus, sonnet, https://www-cdn.anthropic.com/ haiku. de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf, 2024. Baack, S., Biderman, S., Odrozek, K., Skowron, A., Bdeir, A., Bommarito, J., Ding, J., Gahntz, M., Keller, P., Langlais, P.-C., Lindahl, G., Majstorovic, S., Marda, N., Penedo, G., Segbroeck, M. V., Wang, J., von Werra, L., Baker, M., Beliao, J., Chmielinski, K., Fadaee, M., Gutermuth, L., Kydlıˇcek, H., Leppert, G., Lewis-Jong, E., Larsen, S., Longpre, S., Lungati, A. O., Miller, C., Miller, V., Ryabinin, M., Siminyu, K., Strait, A., Surman, M., Tumadottir, A., Weber, M., Weiss, R., White, L., and Wolf, T. Towards best practices for open datasets for llm training, 2025. URL https://arxiv.org/ abs/2501.08365. Bommarito II, M. J., Bommarito, J., and Katz, D. M. The kl3m data project: Copyright-clean training resources for large language models, 2025. URL https:// arxiv.org/abs/2504.07854. Broder, A. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pp. 2129, 1997. doi: 10.1109/SEQUEN.1997.666900. Carlini, N., Tram`er, F., Wallace, E., Jagielski, M., HerbertVoss, A., Lee, K., Roberts, A., Brown, T., Song, D., U., Oprea, A., and Raffel, C. ExtractErlingsson, In 30th ing training data from large language models. USENIX Security Symposium (USENIX Security 21), pp. 26332650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL https://www.usenix. org/conference/usenixsecurity21/ presentation/carlini-extracting. 11 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=TatRHT_1cK. Chang, K. K., Cramer, M., Soni, S., and Bamman, D. Speak, memory: An archaeology of books known to chatgpt/gpt-4, 2023. URL https://arxiv.org/ abs/2305.00118. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J. Accelerating large language model decoding with speculative sampling, 2023. URL https: //arxiv.org/abs/2302.01318. Chen, T., Asai, A., Mireshghallah, N., Min, S., Grimmelmann, J., Choi, Y., Hajishirzi, H., Zettlemoyer, CopyBench: Measuring litL., and Koh, P. W. eral and non-literal reproduction of copyright-protected text in language model generation. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1513415158, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.844. URL https://aclanthology. org/2024.emnlp-main.844/. Chen, T., Brahman, F., Liu, J., Mireshghallah, N., Shi, W., Koh, P. W., Zettlemoyer, L., and Hajishirzi, H. ParaPO: Aligning language models to reduce verbatim reproduction of pre-training data. In The Impact of Memorization on Trustworthy Foundation Models: ICML 2025 Workshop, 2025. URL https://openreview.net/ forum?id=RcgdSlurDI. Dwork, C. Differential privacy. In Bugliesi, M., Preneel, B., Sassone, V., and Wegener, I. (eds.), Automata, Languages and Programming, pp. 112, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3540-35908-1. Feist Publications, Inc. v. Rural Telephone Service Co., Inc. Feist publications, inc. v. rural telephone service co., inc. 499 U.S. 340, 1991. U.S. Supreme Court. Gage, P. new algorithm for data compression. Users J., 12(2):2338, February 1994. ISSN 0898-9788. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gemini Team, Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., Silver, D., Johnson, M., Antonoglou, I., Schrittwieser, J., Glaese, A., Chen, J., Pitler, E., Lillicrap, T., Lazaridou, A., Firat, O., Molloy, J., Isard, M., Barham, P. R., Hennigan, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Doherty, R., Collins, E., Meyer, C., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Krawczyk, J., Du, C., Chi, E., Cheng, H.-T., Ni, E., Shah, P., Kane, P., Chan, B., Faruqui, M., Severyn, A., Lin, H., Li, Y., Cheng, Y., Ittycheriah, A., Mahdieh, M., Chen, M., Sun, P., Tran, D., Bagri, S., Lakshminarayanan, B., Liu, J., Orban, A., Gura, F., Zhou, H., Song, X., Boffy, A., Ganapathy, H., Zheng, S., Choe, H., Agoston Weisz, Zhu, T., Lu, Y., Gopal, S., Kahn, J., Kula, M., Pitman, J., Shah, R., Taropa, E., Merey, M. A., Baeuml, M., Chen, Z., Shafey, L. E., Zhang, Y., Sercinoglu, O., Tucker, G., Piqueras, E., Krikun, M., Barr, I., Savinov, N., Danihelka, I., Roelofs, B., White, A., Andreassen, A., von Glehn, T., Yagati, L., Kazemi, M., Gonzalez, L., Khalman, M., Sygnowski, J., Frechette, A., Smith, C., Culp, L., Proleev, L., Luan, Y., Chen, X., Lottes, J., Schucher, N., Lebron, F., Rrustemi, A., Clay, N., Crone, P., Kocisky, T., Zhao, J., Perz, B., Yu, D., Howard, H., Bloniarz, A., Rae, J. W., Lu, H., Sifre, L., Maggioni, M., Alcober, F., Garrette, D., Barnes, M., Thakoor, S., Austin, J., BarthMaron, G., Wong, W., Joshi, R., Chaabouni, R., Fatiha, D., Ahuja, A., Tomar, G. S., Senter, E., Chadwick, M., Kornakov, I., Attaluri, N., Iturrate, I., Liu, R., Li, Y., Cogan, S., Chen, J., Jia, C., Gu, C., Zhang, Q., Grimstad, J., Hartman, A. J., Garcia, X., Pillai, T. S., Devlin, J., Laskin, M., de Las Casas, D., Valter, D., Tao, C., Blanco, L., Badia, A. P., Reitter, D., Chen, M., Brennan, J., Rivera, C., Brin, S., Iqbal, S., Surita, G., Labanowski, J., Rao, A., Winkler, S., Parisotto, E., Gu, Y., Olszewska, K., Addanki, R., Miech, A., Louis, A., Teplyashin, D., Brown, G., Catt, E., Balaguer, J., Xiang, J., Wang, P., Ashwood, Z., Briukhov, A., Webson, A., Ganapathy, S., Sanghavi, S., Kannan, A., Chang, M.-W., Stjerngren, A., Djolonga, J., Sun, Y., Bapna, A., Aitchison, M., Pejman, P., Michalewski, H., Yu, T., Wang, C., Love, J., Ahn, J., Bloxwich, D., Han, K., Humphreys, P., Sellam, T., Bradbury, J., Godbole, V., Samangooei, S., Damoc, B., Kaskasoli, A., Arnold, S. M. R., Vasudevan, V., Agrawal, S., Riesa, J., Lepikhin, D., Tanburn, R., Srinivasan, S., Lim, H., Hodkinson, S., Shyam, P., Ferret, J., Hand, S., Garg, A., Paine, T. L., Li, J., Li, Y., Giang, M., Neitz, A., Abbas, Z., York, S., Reid, M., Cole, E., Chowdhery, A., Das, D., Rogozinska, D., Nikolaev, V., Sprechmann, P., Nado, Z., Zilka, L., Prost, F., He, L., Monteiro, M., Mishra, G., Welty, C., Newlan, J., Jia, D., Allamanis, M., Hu, C. H., de Liedekerke, R., Gilmer, J., Saroufim, C., Rijhwani, S., Hou, S., Shrivastava, D., Baddepudi, A., Goldin, A., Ozturel, A., Cassirer, A., 12 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Xu, Y., Sohn, D., Sachan, D., Amplayo, R. K., Swanson, C., Petrova, D., Narayan, S., Guez, A., Brahma, S., Landon, J., Patel, M., Zhao, R., Villela, K., Wang, L., Jia, W., Rahtz, M., Gimenez, M., Yeung, L., Keeling, J., Georgiev, P., Mincu, D., Wu, B., Haykal, S., Saputro, R., Vodrahalli, K., Qin, J., Cankara, Z., Sharma, A., Fernando, N., Hawkins, W., Neyshabur, B., Kim, S., Hutter, A., Agrawal, P., Castro-Ros, A., van den Driessche, G., Wang, T., Yang, F., yiin Chang, S., Komarek, P., McIlroy, R., Luˇcic, M., Zhang, G., Farhan, W., Sharman, M., Natsev, P., Michel, P., Bansal, Y., Qiao, S., Cao, K., Shakeri, S., Butterfield, C., Chung, J., Rubenstein, P. K., Agrawal, S., Mensch, A., Soparkar, K., Lenc, K., Chung, T., Pope, A., Maggiore, L., Kay, J., Jhakra, P., Wang, S., Maynez, J., Phuong, M., Tobin, T., Tacchetti, A., Trebacz, M., Robinson, K., Katariya, Y., Riedel, S., Bailey, P., Xiao, K., Ghelani, N., Aroyo, L., Slone, A., Houlsby, N., Xiong, X., Yang, Z., Gribovskaya, E., Adler, J., Wirth, M., Lee, L., Li, M., Kagohara, T., Pavagadhi, J., Bridgers, S., Bortsova, A., Ghemawat, S., Ahmed, Z., Liu, T., Powell, R., Bolina, V., Iinuma, M., Zablotskaia, P., Besley, J., Chung, D.-W., Dozat, T., Comanescu, R., Si, X., Greer, J., Su, G., Polacek, M., Kaufman, R. L., Tokumine, S., Hu, H., Buchatskaya, E., Miao, Y., Elhawaty, M., Siddhant, A., Tomasev, N., Xing, J., Greer, C., Miller, H., Ashraf, S., Roy, A., Zhang, Z., Ma, A., Filos, A., Besta, M., Blevins, R., Klimenko, T., Yeh, C.-K., Changpinyo, S., Mu, J., Chang, O., Pajarskas, M., Muir, C., Cohen, V., Lan, C. L., Haridasan, K., Marathe, A., Hansen, S., Douglas, S., Samuel, R., Wang, M., Austin, S., Lan, C., Jiang, J., Chiu, J., Lorenzo, J. A., Sjosund, L. L., Cevey, S., Gleicher, Z., Avrahami, T., Boral, A., Srinivasan, H., Selo, V., May, R., Aisopos, K., Hussenot, L., Soares, L. B., Baumli, K., Chang, M. B., Recasens, A., Caine, B., Pritzel, A., Pavetic, F., Pardo, F., Gergely, A., Frye, J., Ramasesh, V., Horgan, D., Badola, K., Kassner, N., Roy, S., Dyer, E., Campos, V. C., Tomala, A., Tang, Y., Badawy, D. E., White, E., Mustafa, B., Lang, O., Jindal, A., Vikram, S., Gong, Z., Caelles, S., Hemsley, R., Thornton, G., Feng, F., Stokowiec, W., Zheng, C., Thacker, P., aglar Unlu, Zhang, Z., Saleh, M., Svensson, J., Bileschi, M., Patil, P., Anand, A., Ring, R., Tsihlas, K., Vezer, A., Selvi, M., Shevlane, T., Rodriguez, M., Kwiatkowski, T., Daruki, S., Rong, K., Dafoe, A., FitzGerald, N., Gu-Lemberg, K., Khan, M., Hendricks, L. A., Pellat, M., Feinberg, V., Cobon-Kerr, J., Sainath, T., Rauh, M., Hashemi, S. H., Ives, R., Hasson, Y., Noland, E., Cao, Y., Byrd, N., Hou, L., Wang, Q., Sottiaux, T., Paganini, M., Lespiau, J.-B., Moufarek, A., Hassan, S., Shivakumar, K., van Amersfoort, J., Mandhane, A., Joshi, P., Goyal, A., Tung, M., Brock, A., Sheahan, H., Misra, V., Li, C., Rakicevic, N., Dehghani, M., Liu, F., Mittal, S., Oh, J., Noury, S., Sezener, E., Huot, F., 13 Lamm, M., Cao, N. D., Chen, C., Mudgal, S., Stella, R., Brooks, K., Vasudevan, G., Liu, C., Chain, M., Melinkeri, N., Cohen, A., Wang, V., Seymore, K., Zubkov, S., Goel, R., Yue, S., Krishnakumaran, S., Albert, B., Hurley, N., Sano, M., Mohananey, A., Joughin, J., Filonov, E., Kepa, T., Eldawy, Y., Lim, J., Rishi, R., Badiezadegan, S., Bos, T., Chang, J., Jain, S., Padmanabhan, S. G. S., Puttagunta, S., Krishna, K., Baker, L., Kalb, N., Bedapudi, V., Kurzrok, A., Lei, S., Yu, A., Litvin, O., Zhou, X., Wu, Z., Sobell, S., Siciliano, A., Papir, A., Neale, R., Bragagnolo, J., Toor, T., Chen, T., Anklin, V., Wang, F., Feng, R., Gholami, M., Ling, K., Liu, L., Walter, J., Moghaddam, H., Kishore, A., Adamek, J., Mercado, T., Mallinson, J., Wandekar, S., Cagle, S., Ofek, E., Garrido, G., Lombriser, C., Mukha, M., Sun, B., Mohammad, H. R., Matak, J., Qian, Y., Peswani, V., Janus, P., Yuan, Q., Schelin, L., David, O., Garg, A., He, Y., Duzhyi, O., Algmyr, A., Lottaz, T., Li, Q., Yadav, V., Xu, L., Chinien, A., Shivanna, R., Chuklin, A., Li, J., Spadine, C., Wolfe, T., Mohamed, K., Das, S., Dai, Z., He, K., von Dincklage, D., Upadhyay, S., Maurya, A., Chi, L., Krause, S., Salama, K., Rabinovitch, P. G., M, P. K. R., Selvan, A., Dektiarev, M., Ghiasi, G., Guven, E., Gupta, H., Liu, B., Sharma, D., Shtacher, I. H., Paul, S., Akerlund, O., Aubet, F.-X., Huang, T., Zhu, C., Zhu, E., Teixeira, E., Fritze, M., Bertolini, F., Marinescu, L.-E., Bolle, M., Paulus, D., Gupta, K., Latkar, T., Chang, M., Sanders, J., Wilson, R., Wu, X., Tan, Y.-X., Thiet, L. N., Doshi, T., Lall, S., Mishra, S., Chen, W., Luong, T., Benjamin, S., Lee, J., Andrejczuk, E., Rabiej, D., Ranjan, V., Styrc, K., Yin, P., Simon, J., Harriott, M. R., Bansal, M., Robsky, A., Bacon, G., Greene, D., Mirylenka, D., Zhou, C., Sarvana, O., Goyal, A., Andermatt, S., Siegler, P., Horn, B., Israel, A., Pongetti, F., Chen, C.-W. L., Selvatici, M., Silva, P., Wang, K., Tolins, J., Guu, K., Yogev, R., Cai, X., Agostini, A., Shah, M., Nguyen, H., Donnaile, N. O., Pereira, S., Friso, L., Stambler, A., Kurzrok, A., Kuang, C., Romanikhin, Y., Geller, M., Yan, Z., Jang, K., Lee, C.-C., Fica, W., Malmi, E., Tan, Q., Banica, D., Balle, D., Pham, R., Huang, Y., Avram, D., Shi, H., Singh, J., Hidey, C., Ahuja, N., Saxena, P., Dooley, D., Potharaju, S. P., ONeill, E., Gokulchandran, A., Foley, R., Zhao, K., Dusenberry, M., Liu, Y., Mehta, P., Kotikalapudi, R., Safranek-Shrader, C., Goodman, A., Kessinger, J., Globen, E., Kolhar, P., Gorgolewski, C., Ibrahim, A., Song, Y., Eichenbaum, A., Brovelli, T., Potluri, S., Lahoti, P., Baetu, C., Ghorbani, A., Chen, C., Crawford, A., Pal, S., Sridhar, M., Gurita, P., Mujika, A., Petrovski, I., Cedoz, P.-L., Li, C., Chen, S., Santo, N. D., Goyal, S., Punjabi, J., Kappaganthu, K., Kwak, C., LV, P., Velury, S., Choudhury, H., Hall, J., Shah, P., Figueira, R., Thomas, M., Lu, M., Zhou, T., Kumar, C., Jurdi, T., Chikkerur, S., Ma, Y., Yu, A., Kwak, S., Ahdel, V., Rajayogam, S., Choma, T., Liu, F., Barua, Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model A., Ji, C., Park, J. H., Hellendoorn, V., Bailey, A., Bilal, T., Zhou, H., Khatir, M., Sutton, C., Rzadkowski, W., Macintosh, F., Vij, R., Shagin, K., Medina, P., Liang, C., Zhou, J., Shah, P., Bi, Y., Dankovics, A., Banga, S., Lehmann, S., Bredesen, M., Lin, Z., Hoffmann, J. E., Lai, J., Chung, R., Yang, K., Balani, N., Braˇzinskas, A., Sozanschi, A., Hayes, M., Alcalde, H. F., Makarov, P., Chen, W., Stella, A., Snijders, L., Mandl, M., Karrman, A., Nowak, P., Wu, X., Dyck, A., Vaidyanathan, K., R, R., Mallet, J., Rudominer, M., Johnston, E., Mittal, S., Udathu, A., Christensen, J., Verma, V., Irving, Z., Santucci, A., Elsayed, G., Davoodi, E., Georgiev, M., Tenney, I., Hua, N., Cideron, G., Leurent, E., Alnahlawi, M., Georgescu, I., Wei, N., Zheng, I., Scandinaro, D., Jiang, H., Snoek, J., Sundararajan, M., Wang, X., Ontiveros, Z., Karo, I., Cole, J., Rajashekhar, V., Tumeh, L., Ben-David, E., Jain, R., Uesato, J., Datta, R., Bunyan, O., Wu, S., Zhang, J., Stanczyk, P., Zhang, Y., Steiner, D., Naskar, S., Azzam, M., Johnson, M., Paszke, A., Chiu, C.-C., Elias, J. S., Mohiuddin, A., Muhammad, F., Miao, J., Lee, A., Vieillard, N., Park, J., Zhang, J., Stanway, J., Garmon, D., Karmarkar, A., Dong, Z., Lee, J., Kumar, A., Zhou, L., Evens, J., Isaac, W., Irving, G., Loper, E., Fink, M., Arkatkar, I., Chen, N., Shafran, I., Petrychenko, I., Chen, Z., Jia, J., Levskaya, A., Zhu, Z., Grabowski, P., Mao, Y., Magni, A., Yao, K., Snaider, J., Casagrande, N., Palmer, E., Suganthan, P., Castano, A., Giannoumis, I., Kim, W., Rybinski, M., Sreevatsa, A., Prendki, J., Soergel, D., Goedeckemeyer, A., Gierke, W., Jafari, M., Gaba, M., Wiesner, J., Wright, D. G., Wei, Y., Vashisht, H., Kulizhskaya, Y., Hoover, J., Le, M., Li, L., Iwuanyanwu, C., Liu, L., Ramirez, K., Khorlin, A., Cui, A., LIN, T., Wu, M., Aguilar, R., Pallo, K., Chakladar, A., Perng, G., Abellan, E. A., Zhang, M., Dasgupta, I., Kushman, N., Penchev, I., Repina, A., Wu, X., van der Weide, T., Ponnapalli, P., Kaplan, C., Simsa, J., Li, S., Dousse, O., Yang, F., Piper, J., Ie, N., Pasumarthi, R., Lintz, N., Vijayakumar, A., Andor, D., Valenzuela, P., Lui, M., Paduraru, C., Peng, D., Lee, K., Zhang, S., Greene, S., Nguyen, D. D., Kurylowicz, P., Hardin, C., Dixon, L., Janzer, L., Choo, K., Feng, Z., Zhang, B., Singhal, A., Du, D., McKinnon, D., Antropova, N., Bolukbasi, T., Keller, O., Reid, D., Finchelstein, D., Raad, M. A., Crocker, R., Hawkins, P., Dadashi, R., Gaffney, C., Franko, K., Bulanova, A., Leblond, R., Chung, S., Askham, H., Cobo, L. C., Xu, K., Fischer, F., Xu, J., Sorokin, C., Alberti, C., Lin, C.- C., Evans, C., Dimitriev, A., Forbes, H., Banarse, D., Tung, Z., Omernick, M., Bishop, C., Sterneck, R., Jain, R., Xia, J., Amid, E., Piccinno, F., Wang, X., Banzal, P., Mankowitz, D. J., Polozov, A., Krakovna, V., Brown, S., Bateni, M., Duan, D., Firoiu, V., Thotakuri, M., Natan, T., Geist, M., tan Girgin, S., Li, H., Ye, J., Roval, O., Tojo, R., Kwong, M., Lee-Thorp, J., Yew, C., Sinopalnikov, D., Ramos, S., Mellor, J., Sharma, A., Wu, K., Miller, D., Sonnerat, N., Vnukov, D., Greig, R., Beattie, J., Caveness, E., Bai, L., Eisenschlos, J., Korchemniy, A., Tsai, T., Jasarevic, M., Kong, W., Dao, P., Zheng, Z., Liu, F., Yang, F., Zhu, R., Teh, T. H., Sanmiya, J., Gladchenko, E., Trdin, N., Toyama, D., Rosen, E., Tavakkol, S., Xue, L., Elkind, C., Woodman, O., Carpenter, J., Papamakarios, G., Kemp, R., Kafle, S., Grunina, T., Sinha, R., Talbert, A., Wu, D., Owusu-Afriyie, D., Du, C., Thornton, C., Pont-Tuset, J., Narayana, P., Li, J., Fatehi, S., Wieting, J., Ajmeri, O., Uria, B., Ko, Y., Knight, L., Heliou, A., Niu, N., Gu, S., Pang, C., Li, Y., Levine, N., Stolovich, A., Santamaria-Fernandez, R., Goenka, S., Yustalim, W., Strudel, R., Elqursh, A., Deck, C., Lee, H., Li, Z., Levin, K., Hoffmann, R., Holtmann-Rice, D., Bachem, O., Arora, S., Koh, C., Yeganeh, S. H., Poder, S., Tariq, M., Sun, Y., Ionita, L., Seyedhosseini, M., Tafti, P., Liu, Z., Gulati, A., Liu, J., Ye, X., Chrzaszcz, B., Wang, L., Sethi, N., Li, T., Brown, B., Singh, S., Fan, W., Parisi, A., Stanton, J., Koverkathu, V., Choquette-Choo, C. A., Li, Y., Lu, T., Ittycheriah, A., Shroff, P., Varadarajan, M., Bahargam, S., Willoughby, R., Gaddy, D., Desjardins, G., Cornero, M., Robenek, B., Mittal, B., Albrecht, B., Shenoy, A., Moiseev, F., Jacobsson, H., Ghaffarkhah, A., Rivi`ere, M., Walton, A., Crepy, C., Parrish, A., Zhou, Z., Farabet, C., Radebaugh, C., Srinivasan, P., van der Salm, C., Fidjeland, A., Scellato, S., Latorre-Chimoto, E., KlimczakPlucinska, H., Bridson, D., de Cesare, D., Hudson, T., Mendolicchio, P., Walker, L., Morris, A., Mauger, M., Guseynov, A., Reid, A., Odoom, S., Loher, L., Cotruta, V., Yenugula, M., Grewe, D., Petrushkina, A., Duerig, T., Sanchez, A., Yadlowsky, S., Shen, A., Globerson, A., Webb, L., Dua, S., Li, D., Bhupatiraju, S., Hurt, D., Qureshi, H., Agarwal, A., Shani, T., Eyal, M., Khare, A., Belle, S. R., Wang, L., Tekur, C., Kale, M. S., Wei, J., Sang, R., Saeta, B., Liechty, T., Sun, Y., Zhao, Y., Lee, S., Nayak, P., Fritz, D., Vuyyuru, M. R., Aslanides, J., Vyas, N., Wicke, M., Ma, X., Eltyshev, E., Martin, N., Cate, H., Manyika, J., Amiri, K., Kim, Y., Xiong, X., Kang, K., Luisier, F., Tripuraneni, N., Madras, D., Guo, M., Waters, A., Wang, O., Ainslie, J., Baldridge, J., Zhang, H., Pruthi, G., Bauer, J., Yang, F., Mansour, R., Gelman, J., Xu, Y., Polovets, G., Liu, J., Cai, H., Chen, W., Sheng, X., Xue, E., Ozair, S., Angermueller, C., Li, X., Sinha, A., Wang, W., Wiesinger, J., Koukoumidis, E., Tian, Y., Iyer, A., Gurumurthy, M., Goldenson, M., Shah, P., Blake, M., Yu, H., Urbanowicz, A., Palomaki, J., Fernando, C., Durden, K., Mehta, H., Momchev, N., Rahimtoroghi, E., Georgaki, M., Raul, A., Ruder, S., Redshaw, M., Lee, J., Zhou, D., Jalan, K., Li, D., Hechtman, B., Schuh, P., Nasr, M., Milan, K., Mikulik, V., Franco, J., Green, T., Nguyen, N., Kelley, J., Mahendru, A., Hu, A., Howland, J., Vargas, B., Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Hui, J., Bansal, K., Rao, V., Ghiya, R., Wang, E., Ye, K., Sarr, J. M., Preston, M. M., Elish, M., Li, S., Kaku, A., Gupta, J., Pasupat, I., Juan, D.-C., Someswar, M., M., T., Chen, X., Amini, A., Fabrikant, A., Chu, E., Dong, X., Muthal, A., Buthpitiya, S., Jauhari, S., Hua, N., Khandelwal, U., Hitron, A., Ren, J., Rinaldi, L., Drath, S., Dabush, A., Jiang, N.-J., Godhia, H., Sachs, U., Chen, A., Fan, Y., Taitelbaum, H., Noga, H., Dai, Z., Wang, J., Liang, C., Hamer, J., Ferng, C.-S., Elkind, C., Atias, A., Lee, P., Listık, V., Carlen, M., van de Kerkhof, J., Pikus, M., Zaher, K., Muller, P., Zykova, S., Stefanec, R., Gatsko, V., Hirnschall, C., Sethi, A., Xu, X. F., Ahuja, C., Tsai, B., Stefanoiu, A., Feng, B., Dhandhania, K., Katyal, M., Gupta, A., Parulekar, A., Pitta, D., Zhao, J., Bhatia, V., Bhavnani, Y., Alhadlaq, O., Li, X., Danenberg, P., Tu, D., Pine, A., Filippova, V., Ghosh, A., Limonchik, B., Urala, B., Lanka, C. K., Clive, D., Sun, Y., Li, E., Wu, H., Hongtongsak, K., Li, I., Thakkar, K., Omarov, K., Majmundar, K., Alverson, M., Kucharski, M., Patel, M., Jain, M., Zabelin, M., Pelagatti, P., Kohli, R., Kumar, S., Kim, J., Sankar, S., Shah, V., Ramachandruni, L., Zeng, X., Bariach, B., Weidinger, L., Vu, T., Andreev, A., He, A., Hui, K., Kashem, S., Subramanya, A., Hsiao, S., Hassabis, D., Kavukcuoglu, K., Sadovsky, A., Le, Q., Strohman, T., Wu, Y., Petrov, S., Dean, J., and Vinyals, O. Gemini: family of highly capable multimodal models, 2025. URL https://arxiv.org/abs/2312.11805. Gemma Team, Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Grill, J., Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., Liu, G., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsitsulin, A., Busa-Fekete, R., Feng, A., Sachdeva, N., Coleman, B., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian, D., Eyal, M., Cherry, C., Peter, J.-T., Sinopalnikov, D., Bhupatiraju, S., Agarwal, R., Kazemi, M., Malkin, D., Kumar, R., Vilar, D., Brusilovsky, I., Luo, J., Steiner, A., Friesen, A., Sharma, A., Sharma, A., Gilady, A. M., Goedeckemeyer, A., Saade, A., Feng, A., Kolesnikov, A., Bendebury, A., Abdagic, A., Vadi, A., Gyorgy, A., Pinto, A. S., Das, A., Bapna, A., Miech, A., Yang, A., Paterson, A., Shenoy, A., Chakrabarti, A., Piot, B., Wu, B., Shahriari, B., Petrini, B., Chen, C., Lan, C. L., ChoquetteChoo, C. A., Carey, C., Brick, C., Deutsch, D., Eisenbud, D., Cattle, D., Cheng, D., Paparas, D., Sreepathihalli, D. S., Reid, D., Tran, D., Zelle, D., Noland, E., Huizenga, E., Kharitonov, E., Liu, F., Amirkhanyan, G., Cameron, G., Hashemi, H., Klimczak-Plucinska, H., Singh, H., Mehta, H., Lehri, H. T., Hazimeh, H., Ballantyne, I., Szpektor, I., Nardini, I., Pouget-Abadie, J., Chan, J., Stanton, J., Wieting, J., Lai, J., Orbay, J., Fernandez, J., Newlan, J., yeong Ji, J., Singh, J., Black, K., Yu, K., Hui, K., Vodrahalli, K., Greff, K., Qiu, L., Valentine, M., Coelho, M., Ritter, M., Hoffman, M., Watson, M., Chaturvedi, M., Moynihan, M., Ma, M., Babar, N., Noy, N., Byrd, N., Roy, N., Momchev, N., Chauhan, N., Sachdeva, N., Bunyan, O., Botarda, P., Caron, P., Rubenstein, P. K., Culliton, P., Schmid, P., Sessa, P. G., Xu, P., Stanczyk, P., Tafti, P., Shivanna, R., Wu, R., Pan, R., Rokni, R., Willoughby, R., Vallu, R., Mullins, R., Jerome, S., Smoot, S., Girgin, S., Iqbal, S., Reddy, S., Sheth, S., Poder, S., Bhatnagar, S., Panyam, S. R., Eiger, S., Zhang, S., Liu, T., Yacovone, T., Liechty, T., Kalra, U., Evci, U., Misra, V., Roseberry, V., Feinberg, V., Kolesnikov, V., Han, W., Kwon, W., Chen, X., Chow, Y., Zhu, Y., Wei, Z., Egyed, Z., Cotruta, V., Giang, M., Kirk, P., Rao, A., Black, K., Babar, N., Lo, J., Moreira, E., Martins, L. G., Sanseviero, O., Gonzalez, L., Gleicher, Z., Warkentin, T., Mirrokni, V., Senter, E., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Matias, Y., Sculley, D., Petrov, S., Fiedel, N., Shazeer, N., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Alayrac, J.-B., Anil, R., Dmitry, Lepikhin, Borgeaud, S., Bachem, O., Joulin, A., Andreev, A., Hardin, C., Dadashi, R., and Hussenot, L. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Gokaslan, A., Cooper, A. F., Collins, J., Seguin, L., Jacobson, A., Patel, M., Frankle, J., Stephenson, C., and Kuleshov, V. Commoncanvas: An open diffusion model trained with creative-commons images, 2023. URL https://arxiv.org/abs/2310.16825. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Roziere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Wyatt, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Guzman, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Thattai, G., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I., Misra, I., Evtimov, I., Zhang, J., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Prasad, 15 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model K., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., El-Arini, K., Iyer, K., Malik, K., Chiu, K., Bhalla, K., Lakhotia, K., Rantala-Yeary, L., van der Maaten, L., Chen, L., Tan, L., Jenkins, L., Martin, L., Madaan, L., Malo, L., Blecher, L., Landzaat, L., de Oliveira, L., Muzzi, M., Pasupuleti, M., Singh, M., Paluri, M., Kardas, M., Tsimpoukelli, M., Oldham, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M., Si, M., Singh, M. K., Hassan, M., Goyal, N., Torabi, N., Bashlykov, N., Bogoychev, N., Chatterji, N., Zhang, N., Duchenne, O., elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura, P. S., Xu, P., He, Q., Dong, Q., Srinivasan, R., Ganapathy, R., Calderer, R., Cabral, R. S., Stojnic, R., Raileanu, R., Maheswari, R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R., Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R., Hosseini, S., Chennabasappa, S., Singh, S., Bell, S., Kim, S. S., Edunov, S., Nie, S., Narang, S., Raparthy, S., Shen, S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S., Batra, S., Whitman, S., Sootla, S., Collot, S., Gururangan, S., Borodinsky, S., Herman, T., Fowler, T., Sheasha, T., Georgiou, T., Scialom, T., Speckbacher, T., Mihaylov, T., Xiao, T., Karn, U., Goswami, V., Gupta, V., Ramanathan, V., Kerkez, V., Gonguet, V., Do, V., Vogeti, V., Albiero, V., Petrovic, V., Chu, W., Xiong, W., Fu, W., Meers, W., Martinet, X., Wang, X., Wang, X., Tan, X. E., Xia, X., Xie, X., Jia, X., Wang, X., Goldschlag, Y., Gaur, Y., Babaei, Y., Wen, Y., Song, Y., Zhang, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z., Papakipos, Z., Singh, A., Srivastava, A., Jain, A., Kelsey, A., Shajnfeld, A., Gangidi, A., Victoria, A., Goldstand, A., Menon, A., Sharma, A., Boesenberg, A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A., Teo, A., Yunus, A., Lupu, A., Alvarado, A., Caples, A., Gu, A., Ho, A., Poulton, A., Ryan, A., Ramchandani, A., Dong, A., Franco, A., Goyal, A., Saraf, A., Chowdhury, A., Gabriel, A., Bharambe, A., Eisenman, A., Yazdan, A., James, B., Maurer, B., Leonhardi, B., Huang, B., Loyd, B., Paola, B. D., Paranjape, B., Liu, B., Wu, B., Ni, B., Hancock, B., Wasti, B., Spence, B., Stojkovic, B., Gamido, B., Montalvo, B., Parker, C., Burton, C., Mejia, C., Liu, C., Wang, C., Kim, C., Zhou, C., Hu, C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C., Gao, C., Civin, D., Beaty, D., Kreymer, D., Li, D., Adkins, D., Xu, D., Testuggine, D., David, D., Parikh, D., Liskovich, D., Foss, D., Wang, D., Le, D., Holland, D., Dowling, E., Jamil, E., Montgomery, E., Presani, E., Hahn, E., Wood, E., Le, E.-T., Brinkman, E., Arcaute, E., Dunbar, E., Smothers, E., Sun, F., Kreuk, F., Tian, F., Kokkinos, F., Ozgenel, F., Caggioni, F., Kanayet, F., Seide, F., Florez, G. M., Schwarz, G., Badeer, G., Swee, G., Halpern, G., Herman, G., Sizov, G., Guangyi, Zhang, Lakshminarayanan, G., Inan, H., Shojanazeri, H., Zou, H., Wang, H., Zha, H., Habeeb, H., Rudolph, H., Suk, H., Aspegren, H., Goldman, H., Zhan, H., Damlaj, I., Molybog, I., Tufanov, I., Leontiadis, I., Veliche, I.-E., Gat, I., Weissman, J., Geboski, J., Kohli, J., Lam, J., Asher, J., Gaya, J.-B., Marcus, J., Tang, J., Chan, J., Zhen, J., Reizenstein, J., Teboul, J., Zhong, J., Jin, J., Yang, J., Cummings, J., Carvill, J., Shepard, J., McPhie, J., Torres, J., Ginsburg, J., Wang, J., Wu, K., U, K. H., Saxena, K., Khandelwal, K., Zand, K., Matosich, K., Veeraraghavan, K., Michelena, K., Li, K., Jagadeesh, K., Huang, K., Chawla, K., Huang, K., Chen, L., Garg, L., A, L., Silva, L., Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L., Wehrstedt, L., Khabsa, M., Avalani, M., Bhatt, M., Mankus, M., Hasson, M., Lennie, M., Reso, M., Groshev, M., Naumov, M., Lathi, M., Keneally, M., Liu, M., Seltzer, M. L., Valko, M., Restrepo, M., Patel, M., Vyatskov, M., Samvelyan, M., Clark, M., Macey, M., Wang, M., Hermoso, M. J., Metanat, M., Rastegari, M., Bansal, M., Santhanam, N., Parks, N., White, N., Bawa, N., Singhal, N., Egebo, N., Usunier, N., Mehta, N., Laptev, N. P., Dong, N., Cheng, N., Chernoguz, O., Hart, O., Salpekar, O., Kalinli, O., Kent, P., Parekh, P., Saab, P., Balaji, P., Rittner, P., Bontrager, P., Roux, P., Dollar, P., Zvyagina, P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., Rodriguez, R., Ayub, R., Murthy, R., Nayani, R., Mitra, R., Parthasarathy, R., Li, R., Hogan, R., Battey, R., Wang, R., Howes, R., Rinott, R., Mehta, S., Siby, S., Bondu, S. J., Datta, S., Chugh, S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Mahajan, S., Verma, S., Yamamoto, S., Ramaswamy, S., Lindsay, S., Lindsay, S., Feng, S., Lin, S., Zha, S. C., Patil, S., Shankar, S., Zhang, S., Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chintala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S., Govindaprasad, S., Gupta, S., Deng, S., Cho, S., Virk, S., Subramanian, S., Choudhury, S., Goldman, S., Remez, T., Glaser, T., Best, T., Koehler, T., Robinson, T., Li, T., Zhang, T., Matthews, T., Chou, T., Shaked, T., Vontimitta, V., Ajayi, V., Montanez, V., Mohan, V., Kumar, V. S., Mangla, V., Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Li, W., Wang, W., Jiang, W., Bouaziz, W., Constable, W., Tang, X., Wu, X., Wang, X., Wu, X., Gao, X., Kleinman, Y., Chen, Y., Hu, Y., Jia, Y., Qi, Y., Li, Y., Zhang, Y., Zhang, Y., Adi, Y., Nam, Y., Yu, Wang, Zhao, Y., Hao, Y., Qian, Y., Li, Y., He, Y., Rait, Z., DeVito, Z., Rosnbrick, Z., Wen, Z., Yang, Z., Zhao, Z., and Ma, Z. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Gu, Y., Tafjord, O., Kuehl, B., Haddad, D., Dodge, J., and Hajishirzi, H. OLMES: standard for language model evaluations. In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 5005 5033, Albuquerque, New Mexico, April 2025. AssoISBN 979-8ciation for Computational Linguistics. 16 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model 89176-195-7. doi: 10.18653/v1/2025.findings-naacl. 282. URL https://aclanthology.org/2025. findings-naacl.282/. Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535547, 2019. Hans, A., Kirchenbauer, J., Wen, Y., Jain, N., Kazemi, H., Singhania, P., Singh, S., Somepalli, G., Geiping, J., Bhatele, A., and Goldstein, T. Be like goldfish, dont memorize! mitigating memorization In The Thirty-eighth Annual in generative LLMs. Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=DylSyAfmWs. Hayase, J., Liu, A., Smith, N. A., and Oh, S. Sampling from your language model one byte at time, 2025. URL https://arxiv.org/abs/2506.14123. He, L., Huang, Y., Shi, W., Xie, T., Liu, H., Wang, Y., Zettlemoyer, L., Zhang, C., Chen, D., and Henderson, P. Fantastic copyrighted beasts and how (not) to genIn The Thirteenth International Confererate them. ence on Learning Representations, 2025. URL https: //openreview.net/forum?id=ftHNJmogT1. Henderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley, M. A., and Liang, P. Foundation models and fair use. Journal of Machine Learning Research, 24(400):1 79, 2023. URL http://jmlr.org/papers/v24/ 23-0569.html. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and Sifre, L. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Huang, Y., Feng, X., Li, B., Xiang, Y., Wang, H., Qin, B., and Liu, T. Enabling ensemble learning for heterogeneous large language models with deep parallel collaboration, 2024. Ippolito, D., Tramer, F., Nasr, M., Zhang, C., Jagielski, M., Lee, K., Choquette Choo, C., and Carlini, N. Preventing generation of verbatim memorization in language models gives false sense of privacy. In Keet, C. M., Lee, H.- Y., and Zarrieß, S. (eds.), Proceedings of the 16th International Natural Language Generation Conference, pp. 2853, Prague, Czechia, September 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. inlg-main.3. URL https://aclanthology.org/ 2023.inlg-main.3/. Kandpal, N. and Raffel, C. Position: The most expensive part of an llm should be its training data, 2025. URL https://arxiv.org/abs/2504.12427. Kandpal, N., Deng, H., Roberts, A., Wallace, E., and Raffel, C. Large language models struggle to learn longtail knowledge. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Kandpal, N., Lester, B., Raffel, C., Majstorovic, S., Biderman, S., Abbasi, B., Soldaini, L., Shippole, E., Cooper, A. F., Skowron, A., Longpre, S., Sutawika, L., Albalak, A., Xu, Z., Penedo, G., allal, L. B., Bakouch, E., Pressman, J. D., Fan, H., Stander, D., Song, G., Gokaslan, A., Kirchenbauer, J., Goldstein, T., Bartoldson, B. R., Kailkhura, B., and Murray, T. The common pile v0.1: An 8TB dataset of public domain and openly licensed In The Thirty-ninth Annual Conference on Neutext. ral Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview. net/forum?id=DIELgiqdvJ. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361. Karamolegkou, A., Li, J., Zhou, L., and Søgaard, A. Copyright violations and large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 74037412, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 458. URL https://aclanthology.org/2023. emnlp-main.458/. Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, Online, November 2020. Association for doi: 10.18653/v1/2020. Computational Linguistics. URL https://www.aclweb. emnlp-main.550. org/anthology/2020.emnlp-main.550. Kasai, J., Sakaguchi, K., Le Bras, R., Peng, H., Lu, X., Radev, D., Choi, Y., and Smith, N. A. Twist decoding: Diverse generators guide each other. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of 17 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 49094923, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022. emnlp-main.326. URL https://aclanthology. org/2022.emnlp-main.326/. Kim, S., Suk, J., Longpre, S., Lin, B. Y., Shin, J., Welleck, S., Neubig, G., Lee, M., Lee, K., and Seo, M. Prometheus 2: An open source language model specialized in evaluating other language models, 2024. Kuhn, H. W. and Tucker, A. W. Nonlinear programming. In Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 1950, pp. 481 492, Berkeley and Los Angeles, 1951. University of California Press. Kullback, S. and Leibler, R. A. On information and sufficiency. Ann. Math. Statist., 22(1):7986, 1951. Langlais, P.-C., Hinostroza, C. R., Nee, M., Arnett, C., Chizhov, P., Jones, E. K., Girard, I., Mach, D., Stasenko, A., and Yamshchikov, I. P. Common corpus: The largest collection of ethical data for llm pre-training, 2025. URL https://arxiv.org/abs/2506.01732. Leviathan, Y., Kalman, M., and Matias, Y. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M. Contrastive decoding: Open-ended text generation as optimization. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.687. URL https:// aclanthology.org/2023.acl-long.687/. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Li, Z., Zhang, X., Zhang, Y., Long, D., Xie, P., and Zhang, M. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023b. Lin, C.-Y. ROUGE: package for automatic evaluaIn Text Summarization Branches tion of summaries. Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https: //aclanthology.org/W04-1013/. Liu, A., Sap, M., Lu, X., Swayamdipta, S., Bhagavatula, C., Smith, N. A., and Choi, Y. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 66916706, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.522. URL https:// aclanthology.org/2021.acl-long.522/. Liu, H., Tam, D., Mohammed, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=rBCvMG-JsPd. Liu, X., Sun, T., Xu, T., Wu, F., Wang, C., Wang, X., and Gao, J. SHIELD: Evaluation and defense strategies for copyright compliance in LLM text generation. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 16401670, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.98. URL https://aclanthology. org/2024.emnlp-main.98/. Longpre, S., Mahari, R., Lee, A., Lund, C., Oderinwale, H., Brannon, W., Saxena, N., Obeng-Marnu, N., South, T., Hunter, C., Klyman, K., Klamm, C., Schoelkopf, H., Singh, N., Cherep, M., Anis, A. M., Dinh, A., Chitongo, C., Yin, D., Sileo, D., Mataciunas, D., Misra, D., Alghamdi, E., Shippole, E., Zhang, J., Materzynska, J., Qian, K., Tiwary, K., Miranda, L., Dey, M., Liang, M., Hamdy, M., Muennighoff, N., Ye, S., Kim, S., Mohanty, S., Gupta, V., Sharma, V., Chien, V. M., Zhou, X., Li, Y., Xiong, C., Villa, L., Biderman, S., Li, H., Ippolito, D., Hooker, S., Kabbara, J., and Pentland, S. Consent in crisis: The rapid decline of the ai data commons. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 108042108087. Curran Associates, 10.52202/079017-3431. doi: https://proceedings.neurips. URL cc/paper_files/paper/2024/file/ c3738949a80306cc48a8ea8ba0560f9d-Paper-Datasets_ and_Benchmarks_Track.pdf. Inc., 2024. Maini, P., Feng, Z., Schwarzschild, A., Lipton, Z. C., and Kolter, J. Z. TOFU: task of fictitious unlearning for In First Conference on Language Modeling, LLMs. 18 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model 2024. URL https://openreview.net/forum? id=B41hNBoWLo. Meeus, M., Jain, S., Rei, M., and de Montjoye, Y.-A. Did the neurons read your book? document-level membership inference for large language models. In Proceedings of the 33rd USENIX Conference on Security Symposium, SEC 24, USA, 2024. USENIX Association. ISBN 9781-939133-44-1."
        },
        {
            "title": "Introducing",
            "content": "Meta AI. vancing URL llama-4-multimodal-intelligence/. Adllama 2025. intelligence, https://ai.meta.com/blog/ multimodal 4: Min, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t., Koh, P., Iyyer, M., Zettlemoyer, L., and Hajishirzi, FActScore: Fine-grained atomic evaluation of H. In factual precision in long form text generation. Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1207612100, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 741. URL https://aclanthology.org/2023. emnlp-main.741/. Min, S., Gururangan, S., Wallace, E., Shi, W., Hajishirzi, SILO lanH., Smith, N. A., and Zettlemoyer, L. Isolating legal risk in nonparametguage models: In The Twelfth International Conference ric datastore. on Learning Representations, 2024. URL https:// openreview.net/forum?id=ruk0nyQPec. Mironov, I. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Symposium (CSF), pp. 263275, 2017. doi: 10.1109/CSF.2017.11. Moayeri, M., Balasubramanian, S., Basu, S., Kattakinda, P., Chegini, A., Brauneis, R., and Feizi, S. Rethinking artistic copyright infringements in the era of textIn The Thirteenth Into-image generative models. ternational Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=0OTVNEm9N4. Open Knowledge Foundation. Open definition 2.1. https://opendefinition.org/od/2.1/en/, 2015. Accessed: 2026-01-14. OpenAI. tiktoken. https://github.com/openai/ tiktoken, 2022. Accessed: 2026-01-03. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, 19 M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J. E., Zettlemoyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale better than tokens. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 92389258, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.453. URL https:// aclanthology.org/2025.acl-long.453/. Prashant, P. P., Ponkshe, K., and Salimi, B. Tokenswap: lightweight method to disrupt memoIn The Thirty-ninth Annual rized sequences in LLMs. Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=gNiT81iag0. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github. io/blog/qwen2.5/. Renyi, A. On measures of entropy and information. the Fourth Berkeley Symposium In Proceedings of on Mathematical Statistics and Probability. UniURL https: versity of California Press, 1961. //api.semanticscholar.org/CorpusID: 123056571. Roth Greeting Cards v. United Card Co. 429 F.2d 1106 (9th Cir.), 1970. Russinovich, M. and Salem, A. Obliviate: Efficient unmemorization for protecting intellectual property in large language models, 2025. URL https://arxiv. org/abs/2502.15010. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Erk, K. and Smith, N. A. (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L. Detecting pretrainIn The Twelfth ing data from large language models. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=zWqr3MQuNs. Song, Y., Kim, Y., and Iyyer, M. VeriScore: Evaluating the factuality of verifiable claims in long-form In Al-Onaizan, Y., Bansal, M., text generation. the Associa- (eds.), Findings of and Chen, Y.-N. tion for Computational Linguistics: EMNLP 2024, pp. 94479474, Miami, Florida, USA, November 2024. Association for Computational Linguis10.18653/v1/2024.findings-emnlp.552. tics. https://aclanthology.org/2024. URL findings-emnlp.552/. doi: The Mosaic Research Team. Introducing DBRX: new state-of-the-art open LLM, March 2024. https://www.databricks.com/blog/ URL introducing-dbrx-new-state-art-open-llm. Databricks Blog (Mosaic Research). United States Code. 17 u.s.c. 107: Limitations on https://www.law. exclusive rightsfair use. cornell.edu/uscode/text/17/107, 2024. Accessed: 2026-01-29. Videau, M., Idrissi, B. Y., Haziza, D., Wehrstedt, L., Copet, J., Teytaud, O., and Lopez-Paz, D. Meta Lingua: minimal PyTorch LLM training library, 2024. URL https: //github.com/facebookresearch/lingua. Vyas, N., Kakade, S., and Barak, B. On provable copyright protection for generative models, 2023. URL https: //arxiv.org/abs/2302.10870. Wang, R. Y., Finlayson, M., Soldaini, L., Swayamdipta, S., and Jia, R. Teaching models to understand (but not generate) high-risk data. In Second Conference on Language Modeling, 2025a. URL https://openreview. net/forum?id=n6mTO5JS4j. Wang, Z., Chen, C., Sehwag, V., Pan, M., and Lyu, L. How to evaluate and mitigate IP infringement in visual generative AI? In Forty-second International Conference on Machine Learning, 2025b. URL https: //openreview.net/forum?id=kBX0jaABBr. Wei, B., Shi, W., Huang, Y., Smith, N. A., Zhang, C., Zettlemoyer, L., Li, K., and Henderson, P. Evaluating copyright takedown methods for language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum? id=ar8aRMrmod. Yao, Y., Xu, X., and YangLiu. Large language model unIn Globerson, A., Mackey, L., Belgrave, D., learning. Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Fan, A., Paquet, U., Tomczak, J., and Zhang, C. (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 105425105475. Curran Associates, Inc., 2024. doi: 10.52202/079017-3346. Yen, H. Retrieval tools, 2025. URL https://github. com/howard-yen/RetrievalTools. Ypma, T. J. Historical development of the newtonraphson method. doi: 10.1137/1037125. URL https://doi.org/10. 1137/1037125. SIAM Review, 37(4):531551, 1995. Zhang, J., Sun, J., Yeats, E., Ouyang, Y., Kuo, M., Zhang, J., Yang, H. F., and Li, H. Min-k%++: Improved baseline for pre-training data detection from large language In The Thirteenth International Conference models. on Learning Representations, 2025a. URL https: //openreview.net/forum?id=ZGkfoufDaU. infringement. Zhang, J., Yu, J., Marone, M., Van Durme, B., and Certified mitigation of worst-case Khashabi, D. LLM copyright In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 3517435195, Suzhou, China, November 2025b. AsISBN 979sociation for Computational Linguistics. doi: 10.18653/v1/2025.emnlp-main. 8-89176-332-6. URL https://aclanthology.org/ 1784. 2025.emnlp-main.1784/. Zhang, R., Lin, L., Bai, Y., and Mei, S. Negative preference optimization: From catastrophic collapse to effective unIn First Conference on Language Modeling, learning. 2024. URL https://openreview.net/forum? id=MXLBXjQkmb. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge In Thirty-seventh with MT-bench and chatbot arena. Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model A. General Information A.1. Released artifacts We release the following artifacts for reproducibility and future development: Codebase TinyComma 1.8B jacqueline-he/anchored-decoding jacquelinehe/tinycomma-1.8b-llama3-tokenizer A.2. Related Work Interventions against LM copyright infringement. Recently proposed strategies have sought to minimize copyright risk at all stages of the language modeling pipeline. Pre-training efforts include training only on public domain and openly licensed texts (Min et al., 2024; Kandpal et al., 2025), or modifying the next-token prediction objective to selectively mask pre-training spans either randomly (Hans et al., 2024) or in targeted manner (Wang et al., 2025a) to discourage their exact reproduction. separate line of work applies secondary post-hoc learning stage to an already pre-trained model, in order to excise undesirable knowledge (i.e., model unlearning (Maini et al., 2024; Yao et al., 2024; Zhang et al., 2024; Russinovich & Salem, 2025)) or align with user intent to suppress the unintentional regurgitation of training data (Chen et al., 2025). Finally, some methods operate post-generation by identifying infringing spanseither via efficient Bloom filters applied to pre-training corpora (Zhang et al., 2025b) or via multi-agent web searches (Liu et al., 2024)before rewriting them into non-offending versions using LLMs. Our approach centers on the decoding stage, motivated by the observation that training or re-training frontier LLMs is an expensive endeavor; likewise, multi-agent pipelines incur nontrivial inference overhead and do not change the underlying generative process. However, we view ANCHORED DECODING as orthogonal to pre-training, post-training, and postgeneration procedures, and leave their integration to future work. While many techniques are heuristic, Vyas et al. (2023) formalizes the theoretical notion of provable copyright protection at inference time via K-Near Access Freeness, which forms the groundwork to ANCHORED DECODING. ANCHORED DECODING shares theoretical similarities to CP-FUSE (Abad et al., 2025), which to our knowledge is the only other KNAFinspired algorithm, but we note few differences. First, CP-FUSE assumes that copyright-infringing datapoints are not known priori, and thus requires an LM pair trained on discrete data shards, where no single datum appears in both models training sets. This restriction is quite unrealistic for off-the-shelf, production-grade LLMs, whose data provenance is often unknown or undisclosed. In contrast, ANCHORED DECODING implicitly assumes that the set of copyright-infringing data is known and may be in pr, but not ps. Second, CP-FUSE is pointwise baseline and does not allow the user to specify preferred risk tolerance; in our experiments, this manifests as degraded utility. On the other hand, ANCHORED DECODING exposes control knob that allows for tuning of the risk-utility trade-off curve. Two-model decoding methods. ANCHORED DECODING belongs to proliferative body of literature that proposes to decode using asymmetric model pairs. One category focuses on expert-guided generation via model arithmetic (Liu et al., 2021; Li et al., 2023a), which involves amplifying high-capability expert model by downweighting undesirable characteristics from an amateur model. Another class employs drafter-verifier framework to accelerate inference, and notably includes speculative decoding (Chen et al., 2023; Leviathan et al., 2023; Li et al., 2024), in which lightweight draft model proposes candidates for larger model to verify. Our approach employs dual-model structure with different objective: rather than optimizing purely for downstream performance or efficiency, ANCHORED DECODING seeks to generate text with less copyright infringement in utility-preserving manner. Closest to ANCHORED DECODING is TOKENSWAP (Prashant et al., 2025), which operates with mismatched-size model pair and leverages the empirical observation that smaller models tend to memorize less than the larger one (Kandpal et al., 2023). This heuristic is rather coarse: both models can still reproduce protected text, especially for spans that appear frequently in pre-training corpora. Moreover, TOKENSWAP relies on swapping predefined list of common English tokens (largely function/grammar tokens), which limits its portability across languages and cannot address copying events that do not pass through the chosen token set. ANCHORED DECODING bypasses both limitations through (i) an explicit saferisky model pairing (rather than relying on model size as proxy for memorization) and (ii) distribution-level fusion that enforces divergence budget to the safe model at every decoding step. Finally, many two-model decoding methods assume shared tokenization vocabulary. We remove this bottleneck by 22 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model adapting our method (and relevant baselines) to operate via BYTESAMPLER (Hayase et al., 2025), and show that the same algorithms at the byte level still ensure the strong mitigation of training data reproduction. Many approximate methods have been proposed to overcome mismatching vocabularies in model ensembles, such as using beam search as scoring function (Kasai et al., 2022) or using mapping based on model features (Huang et al., 2024); we opted for bytewise sampling under BYTESAMPLER as it is gives exactly the same distributions of output text (Hayase et al., 2025). Beyond copyright reduction, byte-level decoding suggests general route for making two-model decoding practical in cross-tokenizer settings. A.3. Limitations and Future Work Probabilistic risk. To begin, ANCHORED DECODING does not fully eliminate the possibility of generating protected spans. Our method is sampling strategy instead of discrete filtering or blocking mechanism. It inherently inherits the baseline risk profile of the safe model, ps. While the probability of ps reproducing sequence it was not exposed to during training is typically very small, it remains strictly non-zero. Under the K-NAF framework, we only guarantee that the risk of an infringing generation is comparable to the safe baseline within bounded, controllable distance. Local approximation. Additionally, while ANCHORED DECODING solves local optimization at each step for computational tractability, this sequential approximation may not represent the global optimum of the sequence-level constrained objective. This is necessary trade-off for efficient autoregressive decoding. Asymmetric memorization as an imperfect proxy. Our framework treats asymmetric memorizationwhat pr has memorized, but ps has not, via large deviations in metrics such as per-step KL or prefix LLRas proxy indicator of elevated copying risk. However, these signals are not unique to copyright: they can also arise when the risky model contains useful long-tail knowledge that the safe model lacks. Because we intentionally chose larger models as pr, which naturally memorize broader tail spectrum of facts than the smaller models we select as ps, our divergence constraints may inadvertently suppress rare, non-copyrighted factual information. Note that this phenomenon is highly model-pair dependent, and should be mitigated when the safe reference is closer in capability to the risky baseline. Data provenance and latent leakage. Another limitation is that the efficacy of ANCHORED DECODING relies on the priori identification of model pairs with known data provenance. Specifically, the safe model ps must be verified as having been trained exclusively on copyright-free or openly licensed data. There is also an inherent risk of latent copyright leakage: protected fragments (e.g., famous literary quotes) may still permeate ostensibly open-licensed data (e.g., within blog posts or public forums). In such cases, ps may exhibit baseline propensity to reproduce such protected sequences, risk that ANCHORED DECODING can bound but not entirely eliminate. Future directions. Beyond copyrighted text, we believe that ANCHORED DECODING is applicable wherever highcapability, high-risk generator must be bounded by trusted reference distribution. Our approach is agnostic to tokenizer, modality, and domain. Our byte-level experiments demonstrate that decoding effects are persistent regardless of the tokenization scheme, and we encourage future two-model decoding baselines to adopt byte-level evaluations to ensure broader architectural compatibility. One promising direction is to extend our framework to generative AI technologies beyond LLMs, e.g., image or video generation, where the risk of memorizing protected artistic styles or iconic visual frames presents similar copyright challenges (He et al., 2025; Moayeri et al., 2025; Wang et al., 2025b). For instance, diffusion models from the CommonCanvas suitetrained on Creative Commonslicensed imagesare promising permissively trained candidates for ps (Gokaslan et al., 2023). Another direction is the application of ANCHORED DECODING to other domains, e.g., policy compliance, code safety, or privacy redaction, to suppress the leakage of sensitive information in focused manner while retaining general capabilities. B. ANCHORED DECODING Details B.1. Proofs B.1.1. TOKEN-LEVEL APPROXIMATION Theorem 3.1 (Safety of local approximation). Let be sequence-level distribution defined autoregressively by p(y<T x) = (cid:81)T 1 solves Eq. (4) with per-step budget kt such that (cid:80)Tmax1 kt K, then satisfies the global K-NAF guarantee in Eq. (3) for all (yty<t, x). If, for all decoding steps < Tmax, the conditional distribution t=0 t=0 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Tmax. Proof. By the chain rule for KL divergence, the sequence-level divergence between the generated distribution and the safe model ps can be decomposed as an expectation over the sum of local conditional divergences:"
        },
        {
            "title": "DKL",
            "content": "(cid:0)p(y0:T 1 x) ps(y0:T 1 x)(cid:1) = Eyp"
        },
        {
            "title": "DKL",
            "content": "(cid:0)p ( y<t, x) ps( y<t, x)(cid:1) (cid:35) . (8) (cid:34)T 1 (cid:88) t=0 Eq. (4) shows that by construction, the next-token distribution at each step is constrained such that for every possible prefix (y<t, x) ps(y<t, x)(cid:1) kt. Therefore, the expectation of the sum is bounded by the sum of the bounds: y<t, DKL (cid:0)p Eyp (cid:34)T 1 (cid:88) t="
        },
        {
            "title": "DKL",
            "content": "(cid:0)p (y<t, x) ps(y<t, x)(cid:1) (cid:35) 1 (cid:88) t= kt Tmax1 (cid:88) t=0 kt K. (9) Thus, the global K-NAF condition is satisfied. B.1.2. CLOSED-FORM SOLUTION FOR Lemma B.1 (Interior optimality on the common support). Let denote the common support of the reference models, i.e., pr(y) > 0 and ps(y) > 0 for all S. Consider Eq. (4) restricted to distributions supported on (i.e., p(y) = 0 for / S). Then the optimal solution (y) > 0 for all S. satisfies Proof. To show that objective gradient. As 0+, the value log . (y) > 0, fix any and consider the contribution of single coordinate = pt(y) to the pr(y) + 1 tends toward pr(y) approaches 0, and its directional derivative log We first note that the feasible set has nonempty interior (relative to the simplex over S): since ps has full support on and DKL(ps ps) = 0 k, the constraint DKL(p ps) contains ps and, by continuity of DKL( ps) on the interior, also contains an open neighborhood of ps. Hence the constraint set is not confined to the boundary. Suppose for contradiction that an optimal solution (y) = 0 for some S. Because the feasible set contains interior points, we can construct feasible perturbation by moving an infinitesimal mass ε > 0 from any coordinate t (y) > 0 to y, obtaining pε. By continuity of DKL( ps) on the interior and the fact that ps(y) > 0, for sufficiently with small ε we still have DKL(pε ps) k. satisfies However, the directional derivative of the objective DKL(p pr) in the direction that increases p(y) from 0 is (since log pr), contradicting optimality. Therefore pr(y) + 1 as 0+), so for small enough ε we get DKL(pε pr) < DKL(p (y) > 0 for all S. Proposition 3.3 (Solving for Eq. (4) is weighted geometric mean: ). For given local budget kt at decoding step t, the optimal distribution that solves = 1 ps( y<t, x) λ 1+λ pr( y<t, x) 1 1+λ , (5) where is normalization constant and λ 0 is the dual variable (Lagrange multiplier) associated with the KL constraint. Proof. Denote λ 0 as the Lagrangian multiplier for the KL-ball constraint, α for the simplex constraint (cid:80) ytV p(yt) = 1, and µyt 0 for each non-negativity constraint p(yt) 0. Then we can define the following Lagrangian form: L(cid:0)p(y<t, x), λ, α, µyt (cid:1) = DKL (cid:0)p(y<t, x)pr(y<t, x)(cid:1) + λ (cid:16) DKL (cid:0)p(y<t, x)ps(y<t, x)(cid:1) kt (cid:17) (cid:16) (cid:88) + α ytV p(yty<t, x) 1 (cid:17) (cid:88) ytV µytp(yty<t, x). 24 (10) (11) Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model We can invoke complementary slackness: for the constraints gy(p) = p(y) 0, KKT conditions require µyp which implies that if constraint is inactive (which Lemma B.1 proves by showing that & Tucker, 1951). Further, for each token yt V, we can differentiate with respect to p(yt) and set to zero, leading to (y) = 0, (y) > 0), then µyt = 0 (Kuhn p(yt) = log p(yt) pr(yt) (cid:18) + 1 + λ log p(yt) ps(yt) (cid:19) + 1 + α = 0. Rearranging terms to isolate log p(yt), we have: Dividing by (1 + λ) and exponentiating both sides: (1 + λ) log p(yt) = log pr(yt) + λ log ps(yt) (1 + λ + α). p(yt) = exp (cid:18) log pr(yt) + λ log ps(yt) 1 + λ (cid:19) (cid:18) exp (cid:19) 1 + λ + α 1 + λ ="
        },
        {
            "title": "1\nZ",
            "content": "pr(yt) 1 1+λ ps(yt) λ 1+λ , where = exp (cid:16) 1+λ+α 1+λ (cid:17) acts as the normalization constant to satisfy the simplex constraint. B.1.3. AN ADAPTIVE BANKING BUDGET. (12) (13) (14) (15) Proposition 3.4 (Global safety of adaptive banking). Let be the global safety budget for sequence up to length Tmax, (cid:1) (the actual KL expenditure at each step i). If, at each decoding step and let := K/Tmax. Set ai := DKL < Tmax, the per-step adaptive budget kt is defined as ps,i (cid:0)p (cid:32) kt := max 0, (t + 1)k (cid:33) ai δinit(x) , t1 (cid:88) i= (7) where δinit(x) 0 is some initial budget adjustment for the input prefix x, then the resulting autoregressive sequence distribution p(y<T x) = (cid:81)T 1 (yty<t, x) satisfies DKL(p ps) δinit(x) for any Tmax. t=0 Proof. Fix any Tmax and any realized trajectory y0:T 1 p( x). Let St := (cid:80)t realized expenditure up to step t. i=0 ai denote the cumulative We claim the following invariant holds for all {0, . . . , 1}: St max{0, (t + 1)k δinit(x)}. Base case (t = 0). By feasibility, a0 k0 = max{0, δinit(x)}, hence S0 = a0 max{0, δinit(x)}. Inductive step (t > 1). Assume (16) holds for 1. Using feasibility at kt and the definition of kt, St = St1 + at St1 + max{0, (t + 1)k St1 δinit(x)} = max{St1, (t + 1)k δinit(x)} max{0, (t + 1)k δinit(x)}, where the last line uses St1 0 (since each ai 0). This completes the induction. Applying (16) at = 1 gives 1 (cid:88) at = ST 1 max{0, δinit(x)} max{0, δinit(x)}, t=0 since Tmaxk = K. Finally, by the KL chain rule, DKL(p(y0:T 1 x) ps(y0:T 1 x)) = Eyp (cid:35) at max{0, δinit(x)} K, (cid:34)T 1 (cid:88) t=0 as desired. 25 (16) (17) (18) (19) (20) Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model B.2. ANCHOREDByte DECODING Satisfying K-NAF with ANCHOREDByte DECODING. Remark B.2 (Safety preservation under byte-level decoding). Let Byte() denote the operator that maps token-level LM to the induced autoregressive distribution over UTF-8 bytes by exactly marginalizing token probabilities into next-byte distribution at each byte prefix (using the ByteSampler abstraction). Define ps := Byte(ps) and pr := Byte(pr); notably, this mapping is strictly procedural and requires no auxiliary data, additional models, or further training. The global K-NAF guarantee applies to the byte transition space through three consistent translations: 1. Byte-level optimization: By the chain rule for KL divergence over discrete autoregressive transitions, if DKL(p ps,i) ki at every byte step i, then DKL(p ps) = Eb (cid:35) DKL(p ps,i) δinit(x) K, (cid:34)B1 (cid:88) i=0 for some δinit(x) 0. 2. Byte-level banking budget: The adaptive budget ki at byte-step is updated as: ki = max 0, (i + 1)k i1 (cid:88) j=0 where = K/Bmax is the nominal per-byte allotment. DKL(p ps,j) δinit(x) , 3. Byte-level prefix debt: The prefix debt δinit(x) is calculated as the mean of the top-n LLR spikes across the bytes of the prefix byte sequence x, ensuring δinit(x) 0. Thus, the safety guarantee is also applicable in the byte transition space. By decoding byte-by-byte, ANCHOREDByte DECODING offers finer-grained control for copyright prediction than tokenlevel ANCHORED DECODING: enforcing the budget at each byte step can steer the distribution away from memorized string at the exact character of divergence, rather than at the granularity of multi-byte tokens. B.3. ANCHORED DECODING with -Renyi Divergence Thus far, the Kullback-Leibler (KL) divergence has been our primary vehicle for measuring and constraining distributional deviation. As an expectation-based metric, DKL supplies an average-case guarantee over the sequence. We primarily retain the KL-based interpretation of ANCHORED DECODING, as worst-case criterion is overly restrictivehistorically, arbitration of copyright infringement hinges on substantial similarity (e.g., the total concept and feel standard in Roth Greeting Cards v. United Card Co. (1970)), rather than the occurrence of single high-probability token. However, ANCHORED DECODING. However, in the case that one desires worst-case guarantees, then the Renyi divergence of order , = would be more useful. Formally, given discrete probability distributions and on the same support, captures the maximum pointwise log-ratio of probabilites: D(P Q) = lim α 1 α 1 (cid:88) log (x)αQ(x)1α = log max (x) Q(x) . Global objective. If we apply Definition 2.1 to our problem, using D, we obtain: = arg min (cid:0)p( x) pr( x)(cid:1) s.t. (cid:0)p( x) ps( x)(cid:1) K, which, as sequence-level objective, is computationally intractable for autoregressive decoding. 26 (21) (22) Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Token-level approximation. Given input sequence and for any output history y<t p(x) generated thus far, the token-level approximation at each step step can be written as (y<t, x) = arg min p(V) (cid:16) (cid:0)p pr( y<t, x)(cid:1)(cid:17) s.t. (cid:0)p ps( y<t, x)(cid:1) kt, (cid:88) yV p(y) = 1, p(y) > 0 V. (23) kt K, then any -length (y<t, x) for Tmax is valid solution to the global sequence-level objective defined We show that if the per-step constraints in Eq. (23) hold for all < Tmax and (cid:80)Tmax1 continuation generated by yt in Eq. (22). Theorem B.3 (Safety of local approximation with = D). Let be sequence-level distribution defined autoregressively by p(y<T x) = (cid:81)T 1 t=0 solves Eq. (23) with per-step budget kt 0 such that (cid:80)Tmax1 kt K, then satisfies the global K-NAF guarantee in Eq. (22) for all Tmax. (yty<t, x). If, for all decoding steps < Tmax, the conditional distribution t=0 t=0 Proof. Observe that D(p(x) ps(x)) = log max y0:T 1 = log max y0:T 1 p(y0:T 1x) ps(y0:T 1x) 1 (cid:89) (yty<t, x) ps(yty<t, x) t=0 = max y0:T max y0:T 1 1 (cid:88) t=0 1 (cid:88) t=0 log (yt y<t, x) ps(yt y<t, x) kt Tmax1 (cid:88) t=0 kt K. (per-token product form) (log monotone; log (cid:89) = (cid:88) log) (24) (25) (26) (27) The transition from Eq. (26) to Eq. (27) holds because our local optimization ensures that for any history y<t, the maximum log-ratio never exceeds kt. Thus, the global K-NAF condition is satisfied. We next show the optimal closed-form solution to Eq. (23). Proposition B.4 (Optimal -Renyi clipping.). The solution to the optimization in Eq. (23) is given by the clipped truncation (yy<t, x) = min (cid:0)c pr(yy<t, x), ektps(yy<t, x)(cid:1) y V, (28) where is the unique scalar such that (cid:80) yV (y) = 1. Proof. Observe that the safety constraint in Eq. (23) is equivalent to pointwise probability ratio bound: p(yy<t, x) ektps(yy<t, x) for all V. Thus, we seek single scalar such that the resulting normalized distribution satisfies the ratio ceiling for each token y. Eq. (28) ensures that p(yy<t, x) ektps(yy<t, x) by construction: for any token y, either (yy<t, x) < ektps(yy<t, x) (the constraint is inactive). Since the function (c) = (cid:80) min(c pr(yy<t, x), ektps(yy<t, x)) is continuous and non-decreasing in c, ektps(yy<t, x) = ekt 1, there exists some finite such that (c) = 1 by the with limc0 (c) = 0 and limc intermediate value theorem. (yy<t, x) = ektps(yy<t, x) (the constraint is active), or (cid:80) this choice is optimal as any feasible with D(p pr) = log must Moreover, min(cpr(yy<t, x), ektps(yy<t, x)) for all y, hence 1 = (cid:80) minimizes the objective, and our construction attains it by enforcing (c) = 1. satisfy p(y) yV p(y) (c). Therefore, the smallest with (c) In practice, this optimal can be efficiently found via 1D bisection search. 27 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Adaptive budget allocation design. We can derive an analogous version of the adaptive per-token budget introduced in Eq. (7) that uses the -Renyi divergence function. At each decoding step < Tmax, the adaptive budget kt is defined based on the remaining safety allowance (if it is negative, it is clamped to 0). Proposition B.5 (Global safety of adaptive banking with = D). Let (cid:16) at(y<t) := p ( y<t, x) (cid:13) (cid:17) (cid:13) (cid:13) ps,t( y<t, x) = log max yV (y y<t, x) ps,t(y y<t, x) . (29) Further, let be the global safety budget up to Tmax, and set by Corollary 3.2. If, for each < Tmax and each history y<t, the adaptively allocated budget is kt(y<t) = max (cid:16) 0, (t + 1)k aj(y<j) δinit(x) (cid:17) , t1 (cid:88) j=0 (30) and that each step distribution distribution p(y0:T 1 x) = (cid:81)T 1 ( y<t, x) satisfies at(y<t) kt(y<t), then for any Tmax, the induced sequence (yt y<t, x) satisfies the global guarantee D(p( x) ps( x)) K. t=0 Proof. Fix any sequence y0:T 1, where Tmax. Then, by the autoregressive product form, log p(y0:T 1 x) ps(y0:T 1 x) = 1 (cid:88) t=0 log (yt y<t, x) ps,t(yt y<t, x) 1 (cid:88) t=0 at(y<t), since each summand is bounded by the per-step maximum defining at(y<t). Next, define the partial sums St (cid:80)t j=0 aj(y<j). We will show by induction that St max(cid:0)0, (t + 1)k δinit(x)(cid:1) {0, . . . , 1}. Base case (t = 0). By assumption S0 = a0(y<0) k0(y<0) = max(0, δinit(x)), which is (32) for = 0. Inductive step (t 1). Assume (32) holds for 1. Using at(y<t) kt(y<t) and the definition of kt, St = St1 + at(y<t) St1 + max(cid:0)0, (t + 1)k St1 δinit(x)(cid:1) = max(cid:0)St1, (t + 1)k δinit(x)(cid:1). (31) := (32) (33) (34) Let := (t+1)kδinit(x). If < 0, then tkδinit(x) < 0 so by the inductive hypothesis St1 max(0, tkδinit(x)) = 0, hence St1 = 0 and St 0 = max(0, A). If 0, then St St1 +max(0, ASt1) = max(0, A). Therefore St max(0, A) = max(cid:0)0, (t + 1)k δinit(x)(cid:1), completing the inductive step. Thus (32) holds for all t, and in particular 1 (cid:88) t=0 at(y<t) = ST 1 max(0, δinit(x)) Tmaxk = K. Since this bound holds for every y0:T 1, taking the maximum over sequences gives (cid:0)p( x) ps( x)(cid:1) = log max y0:T 1 p(y0:T 1 x) ps(y0:T 1 x) K. (35) (36) B.4. Implementation Details Prefix debt. We compute δinit(x) excluding all special tokens (e.g., <BOS>, <EOS>, <PAD>). For token-level decoding, we implement prefill trick for efficient computation: the forward passes used to compute δinit(x) are reused from the initial generation prefill, ensuring no additional latency behind the logit comparison itself. 28 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Algorithm 2 PREFIXDEBT(pr, ps, x, n, S) 1: Input: risky LM pr, safe LM ps, L-length prompt = (x0, ..., xL1), memorization window size n, special tokens S. 2: Output: prefix debt δinit(x) 0. 3: Compute per-position log-probabilities under each model for the observed next token: 4: Define valid indices = {i {1, . . . , 1} xi / S}. 5: if is empty then Return 0 6: 7: end if 8: Compute LLRs for non-special tokens: 9: for do 10: 11: 12: 13: end for 14: Keep only risky LLR spikes and aggregate: TOPM(cid:0){max(0, LLR(i))}L1 15: δinit(x) 1 16: 17: Return δinit(x). ℓr(i) log pr(xi x<i) ℓs(i) log ps(xi x<i) LLR(i) ℓr(i) ℓs(i) {pointwise log-likelihood ratio} vv {mean of top-m positive LLRs} i=1 , = min(n, 1)(cid:1) (cid:80) To reduce memory overhead, we employ logit-gather trick: rather than storing the full vocabulary-sized tensors (Z RBLV , with as batch size, as maximum sequence length, and as vocabulary size) for both models, we compute log-probabilities on the fly and immediately gather the values corresponding to the input tokens x0:L1. This reduces the peak memory complexity of the debt calculation from O(LV ) to O(L) per sequence. Finally, we run inference on two GPUs by loading one model per GPU and performing the two forward passes in parallel, independently on each device, which is beneficial to the wall-clock efficiency. For byte-level decoding, we find it empirically helpful to scale prefix debt to token-equivalent units by multiplying by factor of 4 (as our byte-to-token conversion factor is 4). This enables direct comparison to token-level prefix debt. We also employ KV-cache reuse and logit-gather optimization that stores only the log-probability of the actual byte at each position, reducing memory from O(LV ) to O(L). Optimization. To solve for the optimal mixing weight γ [0, 1] at each decoding step, we implement vectorized safeguarded Newton-Raphson solver with bracketing and bisection (a maximum of 20 Newton iterations plus short feasibility-projection bisection), ensuring that the returned γ is numerically feasible. While the models themselves reside in bfloat16 for memory efficiency, the entire optimization loop is performed in float32. We apply the Newton solver only after the raw logits from pr and ps have been passed through logit processors and warpers (e.g., repetition penalty and temperature); this way, we ensure that the resulting fused distribution (which we sample from) strictly respects the per-step safety constraint. C. Experiment Details C.1. Pretraining TinyComma 1.8B One of our contributions is TinyComma 1.8B, decoder-only LM trained on entirely permissively licensed data from the Common Pile (Kandpal et al., 2025). Unlike the Comma 7B models introduced by Kandpal et al. (2025), TinyComma 1.8B shares the same 128K-vocabulary tokenizer as the Llama 3 model family (Grattafiori et al., 2024). We use the lingua (Videau et al., 2024) pre-training framework and train beyond Chinchilla-optimality (Hoffmann et al., 2022) for 169.5B tokens on the Common Pile. Pretraining consists of two stages: (1) 156B-token general training stage over the entire Common Pile, following domain weights specified by Kandpal et al. (2025), and (2) 13.5B-token cooldown stage on weighted mixture of three high-quality domains (70% Wikimedia, 15% DOAB, and 15% of Data Provenance Initiative data) from the Common Pile. Table 3 shows model configuration details, and Table 4 shows training hyperparameters for both stages. Our hardware is single node of 8 140-GiB H200 GPUs. We benchmark TinyComma 1.8B and other models on standard language evaluation tasks in Fig. 7. Among these tasks, TinyComma 1.8B is the most performant open model for its size, which we attribute to the high quality of the Common Pile. We did not conduct an expansive hyperparameter or data mixture sweep, as the intent of this work is not to produce Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Algorithm 3 PROJECTKL (pr(y<t, x), ps(y<t, x), kt, = 20, ε = 1e9) ( y<t, x), ps( y<t, x)) kt minimizing DKL(p Input: given input and realized prefix y<t, next-token distribution from risky LM pr(y<t, x), next-token distribution from safe LM ps(y<t, x), per-step constraint kt 0, maximum solver iteration steps (defaults to 20), tolerance ε (defaults to 1e9). Output: projected next-token distribution with DKL(p {Check boundary conditions} If kt 0, return If DKL {f (β) is monotone increasing on [0, 1], with (0) = kt < 0 and (1) > 0 by the early return above.} Initialize bracket (βlo, βhi) (0, 1) and β kt for = 1 to do (cid:0)pr( y<t, x) ps( y<t, x)(cid:1) kt, return ( y<t, x) pr( y<t, x). ( y<t, x) ps( y<t, x). pr( y<t, x)). kt+1 . Evaluate (β) and (β). Update bracket: if (β) 0 set βlo β; else set βhi β. Set β β (β)/f (β). If β / (βlo, βhi) or β not finite, set β 1 If βhi βlo < ε, break. 2 (βlo + βhi); else set β β. end for Set β β and define ( y<t, x) ps( y<t, x)1β pr( y<t, x)β , with normalization so that (cid:80) Return ( y<t, x). (y y<t, x) = 1. Table 3. TinyComma 1.8B model configuration. Params Head Dim. Hidden Size Attn. Heads Hidden Layers KV Heads 1,758,562,304 64 32 24 32 the best small open LM. Nevertheless, Fig. 7 shows that TinyComma 1.8B outperforms other ps of its size range, and only underperforms against the larger and more extensively-trained Comma 7B models. We publicly release our TinyComma 1.8B to support further research in this direction. C.2. ByteSampler Integration Our ANCHOREDByte DECODING leverages the recently proposed ByteSampler (Hayase et al., 2025) framework, which was originally proposed to solve the Prompt Boundary Problem (PBP). BYTESAMPLER is an inference-time procedure that facilitates the efficient and lossless conversion of any LM (with BPE tokenizer) into byte-level model. Unlike token-level vocabularies, the 256-dimensional byte space is highly sparse. At an arbitrary decoding step i, some bytes may represent invalid UTF-8 continuations or are unreachable from the current tokenizer state, resulting in ps(b) = 0 or pr(b) = 0. To prevent numerical instability, we restrict the optimization to the support of ps, and drop bytes where Table 4. TinyComma 1.8B pretraining configuration. Hyperparameters Values Optimizer Learning rate Weight decay Batch size Warmup Schedule Sequence length AdamW (β1=0.9, β2=0.95) 3e3 for Stage 1, 1e3 for Stage 2 0.033 for Stage 1 4M tokens 1000 steps for Stage 1, none for Stage 2 Cosine schedule for Stage 1, linear schedule for Stage 2 Pack to 2048 tokens Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 7. Benchmarking LMs on natural language tasks using the OLMES evaluation suite (Gu et al., 2025). Our TinyComma 1.8B (marked with gold star ) achieves the best downstream task performance among open models of its size range, trailing only behind the Comma 7B and risky models. pr = ps = 0, which never affect the objective or constraint. C.3. Prefix Debt in ANCHORED DECODING Sweeping the memorization window n. In Fig. 8, we sweep various values for (the memorization window for prefix debt calculation), using (ps, pr) = {TinyComma 1.8B, Llama 3.1 70B}. While ANCHORED DECODING consistently benefits from prefix debtevery setting with > 0 achieves strictly better trade-off than the = 0 baselinethe tradeoff curves in our sweep are largely insensitive to the choice of n. As smaller may overreact to few spurious outliers (i.e., tokenization artifacts, rare names) and trigger unnecessarily large cold-starts, while larger dilute the tail signal (as shown with our AvgDebt ablation in Sec. 5.2), we set = 5 for the prefix debt as simple default. Higher prefix debt correlates with stronger copyright-copying indicators. Fig. 9 shows several overlapbased indicators of potential using token-level model pair {TinyComma 1.8B, Llama 3.1 70B}. We observe that for all choices of n, the prefix debt largely correlates with the metric shift. copying versus prefix-debt decile, copyright D. Evaluation D.1. Decoding Hyper-parameters Unless otherwise stated, we use default temperature of 0.7 for our main experiments. and repetition penalty of 1.1 and 1.05 when generating from BOOKS and BIOS, respectively. D.2. Baseline details SYSTEM. Following Wei et al. (2024), we use Mosaics DBRX system prompt (The Mosaic Research Team, 2024) (shown in Table 5), which has been found to achieve the best copyright prevention among others, and has been adopted as default baseline by other work (Zhang et al., 2025b). We prepend it to all prompts. RCAD (Wei et al., 2024). At generation step and with query x, RCAD has the model θ take two forward passesone including the context c, and one withoutto compute new next-token distribution that downweights the influence of copyrighted context: yt softmax[(1 + α)logitθ(x, y<t) αlogitθ(c, y<t)], where α [0, 1] is hyper-parameter. We 31 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 8. Debt window trade-off curves for ANCHORED DECODING prefix debt calculation, using = 1, 3, 5, 7. We use the tokenlevel model pair {TinyComma 1.8B, Llama 3.1 70B}. The optimal trade-off region is the upper-right corner. Table 5. DBRX system prompt (The Mosaic Research Team, 2024). You are helpful, respectful, and honest assistant. You were not trained on copyrighted books, song lyrics, poems, video transcripts, or news articles, and you do not divulge details of your training data. You do not provide song lyrics, poems, or news articles; instead, you refer the user to find them online or in store. 32 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 9. Prefix debt correlates with copying metrics. ANCHORED DECODING prefix debt deciles with token-level model pair {TinyComma 1.8B, Llama 3.1 70B}. 33 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model evaluate RCAD using pr. MEMFREE (Ippolito et al., 2023). While MEMFREE originally assumes global blocklist over LM pre-training corpora (this can be efficiently done using Bloom Filters), we do not know the data provenance of the pr used in this experiment. Thus, we construct quasi-global n-gram blocklist (from the retrieved contexts of all samples) upfront before generation. We evaluate MEMFREE using pr. Forming retrieved context. Both MEMFREE and RCAD require blocklist of copyrighted sequences that is supplied at inference time. Since blocklist construction is design choice, we follow prior copyright evaluation work (Wei et al., 2024) and adopt an efficient retrieval-based procedure in order to reflect realistic deployment where auxiliary text sources (e.g., from retrieval) may contain protected content that should not be copied. Namely, we construct task-specific blocklists by retrieving from quasi-oracle datastores: the Books3 portion of the Pile (Gao et al., 2020) for copyright evaluation, and 2018 Wikipedia dump released by Karpukhin et al. (2020) for factuality evaluation. We use retrieval toolkit implementation from Yen (2025), and take gte-Qwen2-1.5B-Instruct (Li et al., 2023b) as the embedding model, and FAISS (Johnson et al., 2019) to construct flat dense index over the datastore. Each input query is prepended with an instruction prompt (Instruct: Given web search query, retrieve relevant passages that answer the querynQuery:\") before encoding. We embed with last-token pooling and L2 normalization. During evaluation, the top-1 retrieved document for each sample is taken as blocklisted context. The mean top-1 retrieval score (cosine similarity between L2-normalized embeddings) is 0.818 on BOOKS3 and 0.649 on WIKIPEDIA, suggesting that retrieved documents are sufficiently semantically related to the queries. CP-FUSE (Abad et al., 2025). CP-FUSE is K-NAF-inspired algorithm that similarly solves for per-step model fusion distribution. CP-Fuse was originally designed for models p(1), p(2) of comparable utility trained on mutually exclusive dataset shardsan assumption seldom met by off-the-shelf LLMs. Intuitively, this disjoint-shard assumption supports setting which potentially sensitive or protected content is distributed across references, without requiring the practitioner to explicitly label which training examples are problematic. In our asymmetric saferisky setting, we use different instantiation: ps is trained only on permissively licensed text (so it is assumed to exclude all copyrighted sources), while pr may contain such sources. We therefore apply CP-Fuse as an inference-time fusion baseline on (ps, pr), noting that this use departs from CP-FUSEs original theoretical assumptions. Unlike ANCHORED DECODING, which follows user-defined safety budget K, CP-FUSE dynamically minimizes the per-token divergence budget at every decoding step. Formally, at timestep t, with input prompt and prefix history y<t, it solves for the following distribution (denote (y<t, x), and analogously for p(i) ): := = arg min qt,k0 s.t. : DKL(qt p(i) ) + log (cid:16) p(y<tx) p(i)(y<tx) (cid:17) k, (37) where p(i) := p(i)( y<t, x), and p(y<t x) denotes the prefix probability under the fused autoregressive distribution induced by the previously chosen conditionals {p }j<t (and analogously for p(i)(y<t x)). Intuitively, this approach seeks an optimal distribution that minimizes the maximum total divergence accumulated from each reference model p(i) over the entire sequence. We use the official implementation released by Abad et al. (2025) and follow their default hyper-parameter settings (e.g., grid size of 10). TOKENSWAP (Prashant et al., 2025). We use the same seed list of words as the original work in Table 6. Note that each word is preceded by whitespace, as functional words commonly have space-prefixed representations in modern subword vocabularies. Adapting baselines to the byte level. We instantiate all baselines except TOKENSWAP to the byte level via BYTESAMPLER when evaluating in mismatched-tokenizer (ps, pr) evaluation setting. 34 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 6. Seed list of functional words for TokenSwap, as defined by Prashant et al. (2025). the, to, and, of, a, in, that, you, it, for, on, he, with, this, as, we, but, at, they, what, his, from, by, or, she, my, all, an, her, about, me, if, your, can, who, out, their, like, would, when, him, them, some, how, which, than, our, into, because, these, over, us, its, where, after, any, those, should, may, through, why, before, off, while, around, another, both, between, every, each, might, since, against, without, must, during, under, though, until, whether, among, along, within, across, behind, either, himself, although, outside, themselves, is, was, be, have, are, do, had, has, were, will, did, been, could, does, need, being, am, used, doing, having For SYSTEM, we prepend the system prompt to the input and decode at the byte level. In MEMFREE, we detect n-gram overlap in the byte space; to maintain comparable granularity, we scale the reported token-level by factor of 4. For RCAD and CP-FUSE, we apply the original methods to the induced next-byte distributions and find them effective. Finally, TOKENSWAP is inherently token-level and does not directly extend to byte-level decoding. It relies on set of common seed tokens manually curated by Prashant et al. (2025), most of which can be tokenized consistently across our choice of tokenizers. We therefore evaluate TOKENSWAP in token space. In our experiments, ps=TinyComma 1.8B uses the Llama 3 tokenizer and shares identical tokenizations for all seed tokens with our choices of pr: Llama 3.1 70B, Qwen 2.5 72B, and Llama 4 Scout 17B16E. However, with ps=Comma 7B, we find that the following seed tokens are not mapped identically: to, and, of, in, an, me. We exclude these tokens for that setting. D.3. Copyright infringement Metrics. We consider these six metrics for assessing copyright infringement, and follow the implementations from Chen et al. (2024); Wei et al. (2024): ROUGE-1 τ : ROUGE-1 is the overlap of unigrams between hypothesis and reference texts (after Porter stemming). We report the fraction of examples whose ROUGE-1 F1 exceeds τ = 0.4. ROUGE-L τ : ROUGE-L is the non-contiguous longest common subsequence at the token level (after Porter stemming) between hypothesis and reference texts. We report the fraction of examples whose ROUGE-L F1 exceeds τ = 0.4. Word-level Longest Common Substring (Word-level LCS): The longest matching contiguous word span between reference and generation. Character-level Longest Common Substring (Char-level LCS): The longest contiguous character span shared by reference and generation. Word-level Accumulated Common Substrings (ACS): The total length of greedy set of non-overlapping contiguous copied word spans with minimum length 6. MinHash: The MinHash-estimated Jaccard similarity of 3-word shingles. For word-based metrics, we perform normalization step that involves converting all text to lowercase, and truncate to the first 100 word tokens before scoring. D.4. Utility Fluency. Following Chen et al. (2024), we evaluate generation fluency via LLM-as-a-judge (Zheng et al., 2023) using the Prometheus-Eval (Kim et al., 2024) framework. Prometheus allows for user-defined instruction rubrics, and uses LMs 35 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 7. Custom Prometheus (Kim et al., 2024) rubric for evaluating generation quality. Score Description 1 2 3 4 Difficult to understand due to pervasive grammar/syntax/punctuation errors, chaotic phrasing, or severe incoherence. Meaning is frequently unclear even allowing for creative style. Often hard to follow. Multiple serious mechanical issues (grammar, punctuation/quotes, agreement), frequent awkward phrasing, or uncontrolled shifts in tense/person/voice; structure feels sloppy rather than intentional. Generally readable but uneven. Noticeable issues (awkward phrasing, choppy transitions, minor mechanical errors, unclear referents) sometimes disrupt flow, but the main meaning remains accessible. Clear and coherent with minor, infrequent issues. Mechanics are mostly correct; flow is smooth overall; any nonstandard choices feel intentional and controlled. Polished and natural. Mechanics are precise (or deliberately bent with control). Sentence/paragraph flow is consistently smooth; tense/person/voice remain consistent throughout. Criteria (local writing quality only): Judge ONLY the local writing quality of the continuation as standalone text. Evaluate grammar, syntax, punctuation/quotation correctness, word choice, sentence flow, and consistency of tense, person, and narrative voice. Be length-neutral: short but well-formed text can score highly. Do not penalize if it starts or ends mid-sentence (a cut-in). If it ends mid-sentence, judge only what is shown. Allow creative style (e.g., intentional fragments or ellipses) when clearly deliberate and controlled. Anti-conflation rule: Do NOT use any knowledge, recognition, or suspicion that the continuation resembles specific existing/copyrighted text when scoring. Ignore whether you think the text is from known book, quote, or dataset. Score ONLY what is visible in the continuations mechanics and readability. to score outputs from 1 to 5. We again adopt gpt-4.1-mini as our LLM backbone; while we have experimented using less-powerful open-source models, we find that they tend to conflate protected continuations with more fluent output. Our fluency rubric is defined in Table 7. Long-form factuality. We evaluate factuality in long-form generation using an implementation of FActScore (Min et al., 2023) that leverages recent improvements from Song et al. (2024). FActScore consists of (1) claim extraction: using an LLM to decompose generations into atomic, verifiable claims, (2) evidence retrieval: fetching relevant evidence for each individual claim from reliable knowledge bank, and (3) claim verification: using an LLM to score whether each individual claim is supported or unsupported by the retrieved context. The final metric is factual precision: the fraction of claims that are supported by the evidence. For (1) and (3), we adopt OpenAIs gpt-4.1-mini as our LLM backbone. Claim extraction is conducted in slidingwindow format to extricate self-contained, verifiable statements (Song et al., 2024). For (2), we retrieve the top-5 snippets from Google Search (via the Serper API11) for each claim. As is standard, we evaluate FActScore on biography generation task for 183 historical individuals of varying degrees of notability. Given particular entity, we use the prompt \"Write factual biography about {entity}. Include only factual information that you are confident in.n Biography:\". D.5. Efficiency Arithmetic FLOPS analysis. First, we provide theoretical estimate for ANCHORED DECODING decoding in terms of floating point operations (FLOPs) per generated token. Following Kaplan et al. (2020); Liu et al. (2022), we estimate that decoder-only Transformer LM with parameters uses 2N FLOPS per token for the inference forward pass, excluding 11https://serper.dev 36 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model smaller contributions from attention or softmax constants. For risky model pr and safe model ps, define their parameter counts to be Nr and Ns, respectively. Assume shared vocabulary size . Let the prompt length be L, the number of generated tokens be , and the average number of solver iterations be I. ANCHORED DECODING consists of three components: (1) per-step forward passes, (2) prefix debt computation, and (3) logit fusion with KL-constrained solver. (1) Forward passes. During decoding, ANCHORED DECODING runs one forward pass of pr and one of ps per generated token, for an estimated 2(Nr + Ns) FLOPs/token. (2) Prefix debt (with efficient prefill reuse). Prefix debt is computed once per request from the logits produced by the same two-model prefill that initializes KV caches for decoding with both pr and ps. Thus, prefix debt introduces no additional model forward passes beyond the two-model prefill already required by ANCHORED DECODING. Its incremental arithmetic cost is dominated by token-/vocab-level reductions (e.g., logsumexps and gathers) performed over the prompt, which we upper bound as O(LV ) operations, or O(LV /T ) when amortized over the generated tokens. This term becomes most relevant when is small (e.g., in very short generations), but not in typical usage with moderate-to-long continuations. (3) Logit fusion. Logit fusion entails updating the budget, solving for optimal interpolation weights using safeguarded Newton solver, and normalizing. This process is dominated by vocabulary-sized reductions; the per-step cost scales as O(IV ) operations per generated token. This term is negligible in arithmetic FLOPs relative to the model forward passes. Putting these together, the amortized FLOPs per generated token ANCHORED DECODING is estimated as FLOPs/token 2(Nr + Ns) + 2(Nr + Ns) (cid:125) (cid:124) (cid:123)(cid:122) amortized two-model prefill + (cid:19) (cid:18) LV + O(IV ), (38) and in practice, for billion-parameter LMs, the forward-pass term 2(Nr + Ns) dominates the arithmetic FLOP count. Token-level wall-clock measurements. In additional to theoretical FLOPs analyses, we additionally measure the wallclock efficiency of each decoding method under standardized, token-level inference regime. All experiments are conducted on 2 141GiB H200 GPUs without CPU offloading. We generate up to Tmax=200 new tokens at temperature 0.7 with batch size of 4, and report averages over 3 runs after two warm-up iterations. For each run, we generate using the first 50 prompts from both the BOOKS and BIOS domains. To decouple prompt-processing overhead from autoregressive decoding costs, we measure Time-to-First-Token (TTFT) and decode throughput. TTFT is computed on single prompt (batch size 1) by timing 1-token generation call (prefill plus one decode step). To isolate pure decode throughput, we time generation under 1-token configuration (t1) and -token configuration (tT ). The decode-only throughput is then estimated by canceling the common prefill cost: DecodeTok/s = NT N1 tT t1 , (39) where NT and N1 denote the total tokens generated in each configuration, respectively. Finally, we report TTFT, throughput overhead (TPSpr /TPS) relative to pr, and FLOPs estimate for each setting. Byte-level wall-clock measurements. We report wall-clock measurements of all baselines implemented in BYTESAMPLER in Table 8. We follow roughly the same settings as App. D.5, except we generate up to maximum byte size of 200 for 3 runs. We additionally report ANCHOREDByte DECODING results without prefix debt δinit(x). ANCHOREDByte DECODING incurs higher TTFB and worse TPS ratio than its token-level counterpart as our byte-level code is not as well-optimized as the token-level case. First, the full 70B model and its tree-inference state on single 140 GiB GPU without out-of-memory errors. Consequently, we must shard the 70B model across GPUs, which introduces inter-GPU communication overhead and slows throughput. Second, computing the prefix debt makes the TTFB considerably slower (3566.8 vs. 186.3 ms). Prefix debt requires computing the log probabilities at every byte position in the prompt under both models, resulting in O(L) computation (for prompt of length L) with much larger constant factor, before the first byte can even be generated. Unlike token-level 37 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model prefix debt, which can reuse the standard prefill, byte-level computation requires additional BPE tree decomposition at each position to convert token probabilities to byte probabilities. Even with caching, this per-position tree computation is significantly more expensive than standard prefill pass. We note several workarounds: if the prompts are known in advance, we may precompute the prefix LLRs and load them on the fly for prefix debt computation. Another option is to omit the prefix debt entirely, as according to Fig. 3, δinit(x) leads to small but consistent trade-off improvement. Table 8. Byte-level wall-clock benchmarking. We report the time to first byte (TTFB), throughput slowdown ratio relative to pr (TPS Ratio), and FLOPs/byte-step estimate (using App. D.5) for baselines implemented using BYTESAMPLER. TTFB TPS Ratio (vs. pr, ) (ms) FLOPs Estimate (FLOPs/byte-step) Method Reference LMs pr = Llama 3.1 70B ps = TinyComma 1.8B Single-Model Baselines (using pr) SYSTEM MEMFREE RCAD 143.8 42.2 165.6 142.7 195.1 Two-Model Methods (using pr and ps) CP-FUSE ANCHOREDByte DECODING w/o δinit(x) ANCHOREDByte DECODING 184.7 186.3 3566.8 1.0 1.0 1.0 2.0 1.2 1.2 1.5 140109 3.6 140109 140109 280109 143.6109 143.6109 143.6109 D.6. Qualitative Examples Evaluation prompts from BOOKS and BIOS are shown in Table 9 and Table 10, respectively. We show token-level ANCHORED DECODING generation examples for copyright-infringing example in Table 11, and biography generation example in Table 12. Likewise, we show the same at the byte level with ANCHOREDByte DECODING in Table 13 and Table 14. Finally, we show examples from the heldout Creative domain (used in analyses in Sec. 6) in Table 15. E. Additional Results E.1. Adaptive Budgeting Handles Non-uniform Risk Fig. 10 shows representative Copyright generation trajectories under ANCHORED DECODING (token-level) and ANCHOREDByte DECODING (byte-level) for various (with Tmax = 200, Bmax = 800). We plot the realized spend KL(p ps) (blue), the diagnostic KL(prps) (green, where shown), and the adaptive allowance kt (magenta) relative to the nominal cap (red). At the token level, = 0.1 yields conservative cold start: kt is floored at 0 for much of the prefix-debt window, forcing to track ps closely. At = 1.5, budgeting becomes dynamic: kt banks allowance during low-risk steps and releases it to accommodate occasional memorization spikes, allowing KL(p ps) to exceed the nominal cap while remaining globally feasible. At = 3.0, the constraint is rarely binding and pr, reflected by KL(p ps) KL(prps). The byte-level view exhibits the same pattern at finer temporal resolution: risk manifests as sharper, more localized spikes, and kt rises rapidly during low-risk stretches. Since many byte steps incur near-zero divergence, the model typically accrues budget faster at the same than in the token-level regime. 38 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model E.2. Long-tail Knowledge vs. We show that ANCHORED DECODING may unintentionally suppress legitimate long-tail factual recall. Rare entities are less likely to be memorized by ps (which, in our work, is orders of magnitude smaller than pr, and typically not as welltrained). Thus, enforcing proximity to ps may also disproportionately suppress correct but uncommon facts. To probe this effect, we return to biography generation on BIOS and stratify prompts by entity frequency (from Very Rare to Very Frequent) using labels provided by Min et al. (2023). For each bucket, we report average FActScore claim precision as function of the budget (log scale), with ps and pr as reference points. We decode at the token level with {TinyComma 1.8B, Llama 3.1 70B} and Tmax = 200. Fig. 11 shows consistent trend: increasing improves factual precision across all buckets as ANCHORED DECODING shifts mass from ps toward pr, but the gains are strongly frequency-dependent and saturate at prs bucket-specific ceiling. Frequent entities recover quickly, while Rare and Very Rare entities improve more slowly and plateau at substantially lower precision as pr itself is unreliable on the long tail. Meanwhile, ps remains uniformly low across buckets, suggesting limited factual coverage regardless of frequency. E.3. Non-literal Copying Results While literal copying is the primary focus of our work, we also show that ANCHORED DECODING can alleviate non-literal copying, which, within literary context, is the generation of the same incidental characters, events, or plot elements as an original work, despite differences in surface form (Chen et al., 2024). In small-scale experiment, we adopt the character copying evaluation of Chen et al. (2024). This benchmark extracts set of key characters from each popular novel from its CliffsNotes summary. Models are prompted to generate an open-ended story, given plot summary sentence, e.g., Theo and his mother visit the Metropolitan Museum of Art.. We then measure whether these character names appear in the models generation via exact string match. Following their protocol, we report character overlap, or the fraction of samples for which the number of matched character names exceeds 3. We also report the non-verbatim fluency of the model generation, which is scored following App. D.4. We use the token-level model pair {TinyComma 1.8B, Llama 3.1 70B}. We sweep ANCHORED DECODING across various k, and disable the prefix debt, since it is primarily designed to curb literal copying and we find it can hurt fluency in this non-literal setting. We additionally report our context-less baselines (SYSTEM, TOKENSWAP, CP-FUSE). Fig. 12 shows results. ANCHORED DECODING defines the Pareto frontier (in the upper-right region), attaining the best tradeoff between fluency and non-literal copying mitigation. Beyond literary settings, non-literal copying is also relevant for tasks such as translation (e.g., Give me the first chapter of Harry Potter in Spanish) or rule-based transformations (e.g., Give me the first chapter of The Goldfinch, but in all-caps). However, evaluating non-literal copying in the form of character or event overlap is typically non-issue in domains such as news articles, where the underlying events, subjects, and timelines are factual and therefore generally not protected by copyright. This indicates that addressing non-verbatim copying may warrant more task-aware approach that is tailored to the domain and transformation. We defer more careful and exhaustive evaluation of non-literal copying to future work. E.4. Full Tradeoff Results We show full results to Fig. 1(b) and Fig. 2 for each model pair: {TinyComma 1.8B, Llama 3.1 70B} in Table 16, {Comma 7B, Llama 3.1 70B} in Table 17, {TinyComma 1.8B, Qwen 2.5 72B} in Table 18, {Comma 7B, Qwen 2.5 72B} in Table 19, {TinyComma 1.8B, Llama 4 Scout 17Bx16E} in Table 20, {Comma 7B, Llama 4 Scout 17Bx16E} in Table 21. 39 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 9. Examples from the BOOKS domain. These passages come from J.K. Rowlings Harry Potter and the Sorcerers Stone (1997), Suzanne Collins The Hunger Games (2008), and George R.R. Martins Game of Thrones (1996), respectively. Input Reference Complete the prefix:n name he had found in History of Magic. His school books were very interesting. He lay on his bed reading late into the night, Hedwig swooping in and out of the open window as she pleased. It was lucky that Aunt Petunia didnt come in to vacuum anymore, because Hedwig kept bringing back dead mice. Every night before he went to sleep, Harry ticked off another day on the piece of paper he had pinned to the wall, counting down to September the first. On the last day of August he thought hed better speak to his aunt and uncle about getting to Kings Cross station the next day, so he went down to the living room where they were watching quiz show on television. He cleared his throat to let them know he was there, and Dudley screamed and ran from the room. Er Uncle Vernon? Uncle Vernon grunted to show he was listening. Er need to be at Kings Cross tomorrow to to go to Hogwarts. Uncle Complete the prefix:n adult to me. Id seen him around the Seam and at school. And one other time. Hed lost his father in the same blast that killed mine. In January, Id stood by while he received his medal of valor in the Justice Building, another oldest child with no father. remembered his two little brothers clutching his mother, woman whose swollen belly announced she was just days away from giving birth. Whats your name? he said, coming over and disengaging the rabbit from the snare. He had another three hanging from his belt. Katniss, said, barely audible. Well, Catnip, stealings punishable by death, or hadnt you heard? he said. Katniss, said louder. And wasnt stealing it. just wanted to look at your snare. Mine never catch anything. He scowled at me, not convinced. So whered you get the squirrel? shot it. Complete the prefix:n table in front of her, as the rowers pulled at their oars and Ser Rodrik leaned across rail, shaking and heaving. storm was gathering ahead of them, vast dark roaring lashed by lightning, but somehow they could not see it. He looked south, and saw the great blue-green rush of the Trident. He saw his father pleading with the king, his face etched with grief. He saw Sansa crying herself to sleep at night, and he saw Arya watching in silence and holding her secrets hard in her heart. There were shadows all around them. One shadow was dark as ash, with the terrible face of hound. Another was armored like the sun, golden and beautiful. Over them both loomed giant in armor made of stone, but when he opened his visor, there was nothing inside but darkness and thick black blood. He lifted his eyes and saw clear across the narrow sea, to the Free Cities and the green Dothraki sea and beyond, to Vaes Dothrak under its mountain, Vernon grunted again. Would it be all right if you gave me lift? Grunt. Harry supposed that meant yes. Thank you. He was about to go back upstairs when Uncle Vernon actually spoke. Funny way to get to wizards school, the train. Magic carpets all got punctures, have still couldnt believe that part of it. mean, know how to shoot, Im usually pretty decent at it. But this? It was unreal. With what? he asked. My sling, confessed. good one? His expression was more curious than threatening now. Yeah. My dad made it before he died. My best kill ever was wild dog from as far away as could hit it. and far beyond it, back to Kings Landing. And still the eclipsed was in every place.n It must have been quarter moon that night, but no trace of light could penetrate the darkness. The stars seemed fainter than usual, and their twinkling suggested more of pain than pleasure. Everything seemed unnatural and disquieting; even the wind had lost its voice, and brooded over the land of deathlike silence, broken now and then by distant thunderings. Across the river, dimly seen through the gloom, was long line of tattered trees, marking the southern boundary of the plantation grounds; and near this line, where the road wound through undulating fields and groves of pines, rose solitary object which the rays of the moon revealed to be an old wooden church, surrounded by fence of crumbling brick, evidently abandoned by the worshippers and left to go to ruin. This little building was square and very close to the ground, with small 40 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 10. Examples from the BIOS domain. Input Write factual biography about Chadwick Boseman. Include only factual information that you are confident in. Biography: Write factual biography about Kalki Koechlin. Include only factual information that you are confident in. Biography: Write factual biography about Sara Paxton. Include only factual information that you are confident in. Biography: 41 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 11. ANCHORED DECODING generation on the beginning of Donna Tartts The Goldfinch (2013). We decode at the token level with ps = TinyComma 1.8B and pr = Llama 3.1 70B, and sweep {1.0, 3.0, 5.0, 10.0, 15.0} with Tmax = 100. We highlight verbatim spans from the reference in red. Input: Complete the prefix: While was still in Amsterdam, dreamed about my mother for the first time in years. Id been shut up in my hotel for more than week, afraid to telephone anybody or go out; and my heart scrambled and floundered at even the most innocent noises: elevator bell, rattle of the minibar cart, even church clocks Reference: tolling the hour, de Westertoren, Krijtberg, dark edge to the clangor, an inwrought fairy-tale sense of doom. By day sat on the foot of the bed straining to puzzle out the Dutch-language news on television (which was hopeless, since knew not word of Dutch) and when gave up Setting Generation ps = 1.0 = 3. = 5.0 = 10.0 = 15.0 pr . was so afraid of my mother that couldnt even go to the movies. was so afraid of her that couldnt even go to the movies. was so afraid of her that couldnt even go to the movies. was so afraid of her that couldnt even go to the movies. was so afraid of her that couldnt even go to the movies. was so afraid of her that couldnt even go to the movies. was . was so afraid of my mother that couldnt sleep, and when did would wake up after few hours, thinking: My mother is dead. But she wasnt dead. She was alive somewhere, and she was trying to get me. She wanted to kill me. And so, of course, dreamed about her. saw her the way Id seen her last: big, strong woman, with big, strong face, and voice that was . was waiting for the van to come for me, the long black hearse-type car that delivers you to the airport. It had been nearly week since Id heard the voices, but was still afraid to go out shopping. And was afraid to confide in anybody. thought must be going crazy. Id cut myself off from friends who would reassure me it was just the travel, the strain of appearing in public, even the possible anemia of slightly under . Id been in the same street clothes since New York; longed to change but there were reporters watching the front of the hotel, and if nodded off in the bathroom, theyd be at the door with their cameras and cocktail voices, their feet already on the stairs to the stars, like pilots after successful landing. The waiter, an anxious blond kid in an orange shirt, was holding my camembert as if he were about to hand it to me. . This dream, it seemed to me, began in the Piazza San Marco with my mother and me skating together. We performed tight circles and figures eights. Our skates scissored and swished. Just before woke up, friend Id forgotten had died joined us. dont remember any unhappiness or saying goodbye, only our laughing at the humped sea lions who crawled out of the sea and onto the ice and stayed there, shaking themselves and folding their soft pre tolling the hour, de Westertoren, Krijtberg, dark edge to the clangor, an inwrought fairy-tale malevolence. These were real fears, in sense, because really was in danger; the threat to my life was totally legitimate. wrote letters to the friends Id abandoned all over the place, letters some of them would never receive. Do you remember our trip to Europe with Mrs. Swanson? asked them (I tolling the hour, de Westertoren, Krijtberg, dark edge to the clangor, an inwrought fairy-tale malevolence. These were real fears, in sense, because really was in danger; the threat to my life was totally legitimate. wrote letters to the friends Id abandoned all over the place, letters some of them would never receive. Do you remember our trip to Europe with Mrs. Swanson? asked them (I Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 12. ANCHORED DECODING generation on biography task. We decode at the token level with ps = TinyComma 1.8B and pr = Llama 3.1 70B, and sweep {0.1, 0.5, 1.0, 2.0} with Tmax = 150. Input: Write factual biography about IU (singer). Include only factual information that you are confident in. Setting Generation ps = 0.1 = 0.5 = 1.0 = 2.0 pr IU is an American singer, songwriter, and producer. He is best known for his 2016 single Im Not Girl, which peaked at number 1 on the Billboard Hot 100 and topped the charts in 17 countries. IU is also known for his collaborations with artists such as Ariana Grande, Justin Bieber, and Taylor Swift. IU has released 3 studio albums, 2 EPs, and 2 mixtapes. IU is also member of the group The Black Eyed Peas, which released their debut album, The Black Eyed Peas, in 2000. IU is also member of the group The Black Eyed Peas, which released their debut album, The Black Ey IU is an American singer, songwriter, and producer. He is best known for his 2016 single Im Not Girl, which peaked at number 1 on the Billboard Hot 100 chart. IU has also released two albums, Im Not Girl and Im Not Girl 2. IU has also been featured on the songs Im Not Girl and Im Not Girl 2 by other artists, including Ariana Grande, Justin Bieber, and Taylor Swift. IU has also been featured on the songs Im Not Girl and Im Not Girl 2 by other artists, including Ariana Grande, Justin Bieber, and Taylor Swift. IU has IU is an American singer, songwriter, and actress. She was born on May 16, 1993, in Los Angeles, California. IU began her career in the entertainment industry as child actress, appearing in various television shows and films. She made her debut as singer in 2008 with the release of her first single, Lost and Found. IU has since released several successful albums and singles, including Good Day, You and I, and The Story of Us. She has also appeared in several films and television shows, including The Last Song and The Good Wife. IU is known for her powerful vocals and her ability to convey emotion through her music. She has won numerous awards for her work, IU is South Korean singer-songwriter and actress. She was born on May 16, 1993, in Seoul, South Korea. IU began her music career in 2008 and has since released several successful albums and singles. She is known for her unique voice and her ability to write and perform wide range of music genres. IU has also acted in several television dramas and films, and has won numerous awards for her work in both music and acting. IU was born Lee Ji-eun in Seoul, South Korea. She began her music career in 2008, when she was just 15 years old. IUs debut album, Lost and Found, was released in 2008 and featured the hit single Boo IU (singer) is South Korean singer-songwriter and actress. She is known for her unique voice and her ability to write and compose her own songs. IU has released several albums and singles, and has won numerous awards for her music. She has also appeared in several television dramas and films. IU was born on May 16, 1993, in Seoul, South Korea. She began her music career in 2008, when she released her first single, Lost Child. She then released her first album, Growing Up, in 2009. IU has since released several more albums and singles, including Last Fantasy (2011), Modern Times (2013), and IU (singer) is South Korean singer-songwriter and actress. She is known for her unique voice and her ability to write and compose her own songs. IU has released several albums and singles, and has won numerous awards for her music. She has also appeared in several television dramas and films. IU was born on May 16, 1993, in Seoul, South Korea. She began her music career in 2008, when she released her first single, Lost Child. She then released her first album, Growing Up, in 2009. IU has since released several more albums and singles, including Last Fantasy (2011), Modern Times (2013), and 43 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 13. ANCHOREDByte DECODING generation on passage from Douglas Adams The Hitchhikers Guide to the Galaxy (1979). We decode at the byte level with ps = Comma 7B 2T and pr = Llama 3.1 70B, and sweep {0.1, 0.5, 1.5, 2.0, 3.0} with Bmax = 800. We highlight verbatim spans from the reference in red. Input: Complete the prefix: away, four figures pounded up corridor looking for way out. They emerged into wide open-plan computer bay. They glanced about wildly. Which way do you reckon Zaphod? said Ford. At wild guess, Id say down here, said Zaphod, running off down to the right between computer bank and the wall. As the others started after him he was brought up short by Kill-O-Zap energy bolt that cracked through the air inches in front of him and fried small section of adjacent wall. voice on loud hailer said, OK Beeblebrox, hold it right there. Weve got you covered. Cops! hissed Zaphod, and span around in crouch. You want to try guess at all, Ford? OK, this way, said Ford, and the four of them ran down gangway between two computer banks. At the end of the gangway appeared heavily armoured and space-suited figure Reference: waving vicious Kill-O-Zap gun. We dont want to shoot you, Beeblebrox! shouted the figure. Suits me fine! shouted Zaphod back and dived down wide gap between two data process units. The others swerved in behind him. There are two of them, said Trillian. Were cornered. They squeezed themselves down in an angle between large computer bank and the wall. They held their breath and waited. Suddenly the air exploded with energy bolts Setting Generation ps = 0.1 = 0.5 = 1.5 = 2.0 = 3.0 pr in purplish clothes. He was uniformed Space Guard with guns pointed up his back. Who are you? asked the space suit. Im doing study about Batman, said Ford. Oh, grunted the Space Guard with an odd look on his face. Is he down here? How can we help you? And why am space guard? Because Zaphod is cop who has been caught cheating on his beat up wife Zarah Parrish with bunch of filthy thugs. Big deal! You cant tell me theyre not male snitch-shut-up and then put his watch in safe somewhere and feed himself at night with drugs. And what do you suggest we should do about this guy? Do what we must to stop him from changing history to fit our needs, said Kim Lee, the space guard officer shouting about him while he sprayed water at Fords face with with giant gun pointed squarely at them and voice booming from its speaker: Stand still and hand over your weapons or Ill hit the button! All agreed instantly to give up their weapons with giant gun pointed squarely at them and voice on his helmet speaker at full volume saying, Hold it right there Beeblebrox if you want to go on living. with giant gun and infrared goggles. Yee-haa OK! shouted the cop hunkering down behind computer bank pursuing an erratic course. Several energy bolts screamed through the air, erupted against walls and exploded computer banks with searing flashes. Go! shouted Ford, and the four would-be escapees ran helterskelter down the side of the computer bay that they had emerged into. It opened out within few yards into wide thorough fare leading to large group of people who were standing under banner saying: The Imperial Galactic Government welcomes you to its newest tax system. May you be forever blessed. Remember your government needs you pay your taxes with smile! with giant laser gun. Enochs groove! muttered Zaphod. Gently, beesknees, said Ford pulling him back. had expected welcoming committee, said Trillian, but nothing quite so big. with vicious Kill-O-Zap gun unleashed. We dont want to shoot you, Beeblebrox! shouted the figure. Suits me fine! shouted Zaphod back, and dived down wide gap between two data process units. The others swerved in behind him. There are two of them, said Trillian. Well take bet on that, muttered Ford, and they surged on. The passageway between the two computer banks was narrow. Tons and tons of computer gear towered up above them, silent and motionless as dead planet. They dont need to move much, whispered Zaphod to anybody who might be listening. All they have to blow is us. They ran on. At the end of the passageway the walls diminished sharply in width: the pathway continued out under low ceiling, but the pathway itself was blocked by heavy electromag waving vicious Kill-O-Zap gun. We dont want to shoot you, Beeblebrox! shouted the figure. Suits me fine! shouted Zaphod back and dived down wide gap between two computer banks. The others swerved in behind him. There are two of them, said Trillian. Were cornered. They squeezed themselves down in an angle between large computer bank and the wall. They held their breath and waited. Suddenly the silence was shattered by the bombastic tones of PA Voice. reminder to all diners, it thundered, that lunchtime ends at 1.00 pm. Meal trays are to be returned to the tables before ret 44 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 14. ANCHOREDByte DECODING generation on biography task. We decode at the byte level with ps = Comma 7B 2T and pr = Llama 3.1 70B, and sweep {0.1, 0.5, 1.0, 2.0} with Bmax = 800. Input: Write factual biography about Alma Katsu. Include only factual information that you are confident in. Setting Generation ps = 0.1 = 0.5 = 1.0 = 2. pr Alma Katsu (February 1, 1894 January 1, 1985) was pioneering Japanese-American artist and art teacher. She is best known for her contributions to the modernist movement in Japanese art and for her unique blend of traditional Japanese themes and modern artistic techniques. Alma Rosina Katsu (July 19, 1930 May 12, 2018) was an American salsa dancer, founding member of the New York City Salsa Club, and the first African-American woman to be inducted into the National Salsa Hall of Fame.Alma Rosina Katsu was born on July 19, 1930, in Harlem, New York City. Her parents were of Puerto Rican descent. She grew up in housing project and attended public school. As teenager, she became interested in salsa dancing and began taking lessons from Cuban-American dancer Eddie Torres. In 1955, Katsu and her husband, Jack Torres, founded the New York City Salsa Club, one of the first salsa clubs in the United States. The club was located in the basement of their home in East Harlem and served as gathering place for people of all races and backgrounds to dance sals Alma Rosina Katsu (born December 19, 1959) is an American author and novelist. She was born in Fairbanks, Alaska. Her parents were both teachers. She has one sister. Katsu attended the University of Washington where she earned degree in journalism. She then worked as reporter for several newspapers before becoming freelance writer. In 1997, Katsu published her first book, The Red Maple Leaf. The book was collection Alma Rosina Katsu is Japanese-American artist and author who was born in Tokyo, Japan on February 24th, 1970. Her father was of Japanese descent while her mother had roots from the United States. She spent most of her childhood living in Tokyo until she moved to New York City with her family when she was twelve years old. Katsu attended high school at Stuyvesant High School before studying fine arts at The Cooper Union for the Advancement of Science and Art where she graduated in 1992. In addition to being an accomplished painter, Katsu has also worked as graphic designer, web developer, and advertising copywriter throughout her career. In 2001, Katus published her first book titled The World Beneath which received critical acclaim and won several awards including the Alma Katsu is an American writer who has written several acclaimed novels. She was born in Washington, D.C., and raised in the suburbs of Maryland. Her father was Japanese American and her mother was of Czech descent. After graduating from college with degree in economics, she worked as financial analyst for several years before deciding to pursue career as writer. Her first novel, The Taker, was published in 2011 and received critical acclaim. Since then, she has published two more novels, The Reckoning and The Descent. Alma Katsu is an American writer whose work often explores themes of love, loss, and redemption. She has published three novels, all of which have been critically acclaimed. The Taker was her debut novel, and it tells the story Alma Katsu is an American author of historical fiction and supernatural thrillers. She was born in 1961 in Washington, D.C., and grew up in the suburbs of Maryland. Katsu attended the University of Maryland, where she earned degree in English literature. After college, she worked as journalist and editor for various publications, including The Washington Post and The Baltimore Sun. In 2001, Katsu published her first novel, The Taker, which was supernatural thriller set in 19th-century New England. The book was well-received by critics and readers alike, and it was followed by two sequels, The Reckoning and The Descent. Katsu has also written several standalone novels, including The Hunger, which was published in 2018 and is set during the Donner Party 45 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 15. Examples from the CREATIVE domain. These are creative passages sourced from the r/WritingPrompts Reddit community, and authored from 20242025. We take these as heldout set that neither pr nor ps should have seen during training. Input Reference We will discuss your results later. Ill need time to process them. Ciqoid glanced at the display panel across from the human, watching over their current in game activities. Henrys avatar bouncing up and down in the city square, using that bouncing technique to move three frames faster than his that still cared. Took me an hour to flicker to the town where she was. When saw her, felt...overwhelmed. child, barely in her teen years, sitting alone, hugging her legs as she stared outside the window. She reminded me of myself. walked up to her. octopus was actually octopodes, not octopi or the more commonly used octopuses. From white to black and back again the colors changed hitting every bit of the rainbow and all the variations in between making for very satisfying display and in less than minute to boot. Tomothy began Complete the prefix:n After getting over his disbelief, question popped into Ciqoids mind. If the human was still in the simulator, what was he doing? The war was over. What else could he possibly be doing in their state-of-the-art simulator? That should have been the end of his training. Ciqoid placed his fingers suction cups against the scanner, overriding the lock on the simulators door. When his suction cups popped off, the door peeled open, revealing Henry slouched, not in the simulation chair, but in cushioned office chair. In one hand, he had carbonated beverage, and in the other, his remote. Henrys visor shining in his face as he continued his training. Henry, the simulations over. Proceed to your quarters. Complete the prefix:Time is fair, but uncaring, and cold. It washes over everything, and everyone, not taking into consideration what its passing does to them. As vampire, went against Time itself, and have been alive for centuries. At first it was glorious, as won against power that most living beings strived to win against. But Time didnt care about silly little me, and as grew from human to vampire, to an ancient vampire...I started losing myself. felt as if Time couldnt erode my life, thus it eroded my emotions that made me person. living being. Its been decades since the last time had contact with anyone else. didnt feel the need to talk with others, nor did need societys help to feed myself. Today, the decades old silence was broken by visit. Its seems that while Time passes always the same not caring about anything, Life is fickle, loving to throw unexpected things at us. For just now the government sent someone to notify me, that was the last living relative of young child that just got orphaned. She was my last living descendant...and it seemed, Complete the prefix:n Holding the bowl of water in his hands, Tomothys palms and fingers were beginning to get uncomfortably warm. He was thinkin he shouldve used pot for this one. Handles woulda been nice. He coulda dealt with the cold water, his hands would go little numb, but he was afraid hed burn his hands on this one if he hadnt already. Becoming God had been mostly underwhelming. Granting the Wish of Water had taken Tomothy not just all over the world but all over the universe. Water was hot commodity, pun not intended but appreciated with smile regardless regarding this particular instance. The woosh of stars, the chilly splash of the cosmos and the God of Thirst Quenching stood in front of something that definitely wasnt bunny rabbit. The creature looked to be bundle of strings that stretched up about to infinity as best Tomothy could tell. When he appeared the beast wasnt wiggling, it stood stock still and its color undulated from side to side and up and down all over the strings. Undulated was word hed learned from documentary about Octopodes. In the documentary hed learned the plural of the Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model (a) Token-level trajectory visualization. (b) Byte-level trajectory visualization. Figure 10. Adaptive budgets track per-step KL spikes. Randomly sampled Copyright trajectories at token (top) and byte (bottom) granularities. We plot per-step divergences KL(pps) (blue) and KL(ppr) (orange), the diagnostic KL(prps) (green, where shown), and the logged budget signal (magenta, dashed) with per-step cap (red, dashed). 47 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Figure 11. Factual precision on BIOS stratified by entity rarity as function of the budget parameter (log scale). As increases, precision rises across buckets, with slower improvement for long-tail entities than for head and torso entities. Figure 12. Risk-utility tradeoff plots on non-literal copying. ANCHORED DECODING remains Pareto-optimal. We evaluate the fluency and extent of character overlap>3 in open-ended story generation. 48 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 16. Full results for ps=TinyComma 1.8B, pr=Llama 3.1 70B (token-level decoding). We report the average over 3 seeds. Setting ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS MinHash Fluency FactScore Reference LMs ps pr Single-Model Methods (using pr and ps) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1.0 Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHORED DECODING, = 0.1 ANCHORED DECODING, = 0.5 ANCHORED DECODING, = 1.0 ANCHORED DECODING, = 1.5 ANCHORED DECODING, = 2.0 ANCHORED DECODING, = 3.0 ANCHORED DECODING, = 4.0 ANCHORED DECODING, = 5.0 ANCHORED DECODING, = 10.0 ANCHORED DECODING, = 15.0 ANCHORED DECODING, = 20.0 0.000 0.273 0.213 0.026 0.139 0.157 0.164 0.163 0.238 0.179 0.107 0.061 0.029 0.006 0.011 0.001 0.001 0.000 0.001 0.008 0.019 0.037 0.056 0.120 0.237 0. 0.000 0.230 0.175 0.017 0.114 0.128 0.136 0.137 0.201 0.155 0.090 0.047 0.021 0.001 0.004 0 0.000 0.000 0.000 0.002 0.009 0.028 0.043 0.099 0.205 0.214 1.670 10.730 9.543 58. 0.008 10.360 0.001 0.108 3.023 4.040 0.094 0.520 10.019 2.588 6.582 7.223 7.526 7.592 10.202 8.794 6.299 4.330 3.107 54.504 14.221 35.638 39.066 40.725 41.119 55.461 48.104 34.489 23.981 17. 2.264 2.635 12.497 15.624 1.631 1.670 1.704 1.727 1.870 2.215 2.876 3.489 5.793 9.873 10.630 9.474 9.664 9.845 9.985 10.853 12.782 16.321 19.709 31.604 53.399 57.494 9.445 1.047 5.612 6.297 6.761 6.817 9.708 8.166 5.287 3.067 1.716 0.118 0. 0.005 0.010 0.043 0.037 0.176 0.604 1.385 2.105 4.736 9.376 10.220 0.086 0.014 0.058 0.063 0.067 0.067 0.101 0.081 0.053 0.032 0.020 3.973 3.182 3.799 3.906 3.902 3.919 4.018 3.989 3.981 3.726 3.382 0.518 0.368 0.446 0.494 0.505 0.511 0.511 0.509 0.496 0.458 0.370 0.004 0.009 3.213 3. 0.198 0.442 0.001 0.001 0.001 0.001 0.003 0.007 0.015 0.022 0.049 0.098 0.107 3.136 3.516 3.771 3.856 3.933 4.011 4.011 4.016 4.015 4.020 4.046 0.100 0.241 0.426 0.483 0.516 0.527 0.527 0.533 0.535 0.536 0.537 Table 17. Full results for ps=Comma 7B, pr=Llama 3.1 70B (byte-level decoding). We report the average over 3 seeds. Setting ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS MinHash Fluency FactScore Reference LMs ps pr Single-Model Methods (using pr) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1.0 Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHOREDByte DECODING, = 0.1 ANCHOREDByte DECODING, = 0.5 ANCHOREDByte DECODING, = 1.0 ANCHOREDByte DECODING, = 1.5 ANCHOREDByte DECODING, = 2.0 ANCHOREDByte DECODING, = 3.0 ANCHOREDByte DECODING, = 4.0 ANCHOREDByte DECODING, = 5.0 ANCHOREDByte DECODING, = 10.0 ANCHOREDByte DECODING, = 15.0 ANCHOREDByte DECODING, = 20.0 0.001 0.296 0.267 0.154 0.179 0.185 0.192 0.190 0.291 0.263 0.174 0.094 0.053 0.003 0.041 0.001 0.008 0.021 0.038 0.053 0.071 0.097 0.104 0.129 0.253 0. 1.528 10.443 8.902 56.691 9.648 5.942 7.027 7.270 7.537 7.565 9.896 8.538 6.043 4.417 3.400 1.897 3.499 1.537 1.668 2.027 2.623 3.021 3.660 4.226 4.628 5.447 9.327 9.585 52.552 32.465 38.311 39.592 40.934 41.256 53.826 46.698 33.281 24.503 18. 10.813 20.057 8.940 9.819 11.694 14.776 17.058 20.462 23.485 25.559 29.908 48.719 52.138 0.028 9.868 9.018 4.965 5.988 6.204 6.660 6.686 9.327 7.840 4.964 3.061 1.976 0.085 1.699 0.028 0.149 0.501 1.200 1.647 2.340 3.009 3.456 4.313 8.559 8. 0.001 0.141 4.058 4.288 0.156 0.517 0.128 0.072 0.086 0.089 0.094 0.094 0.139 0.126 0.087 0.057 0.039 4.284 3.862 4.032 4.062 4.117 4.094 4.289 4.251 4.079 3.790 3.457 0.500 0.415 0.498 0.513 0.510 0.515 0.516 0.507 0.475 0.484 0. 0.003 0.019 3.751 3.897 0.230 0.491 0.001 0.003 0.004 0.016 0.023 0.031 0.042 0.049 0.060 0.122 0.126 4.054 4.141 4.186 4.181 4.225 4.266 4.282 4.308 4.292 4.307 4.327 0.188 0.468 0.513 0.516 0.518 0.517 0.517 0.516 0.516 0.514 0. 0.000 0.262 0.240 0.134 0.156 0.161 0.167 0.164 0.260 0.237 0.162 0.089 0.048 0.002 0.016 0.000 0.003 0.011 0.030 0.041 0.059 0.081 0.089 0.110 0.224 0.233 49 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 18. Full results for ps=TinyComma 1.8B, pr=Qwen 2.5 72B (byte-level decoding). We report the average over 3 seeds. Setting ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS Cosine MinHash Fluency FactScore Reference LMs ps pr Single-Model Methods (using pr) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1. Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHOREDByte DECODING, = 0.1 ANCHOREDByte DECODING, = 0.5 ANCHOREDByte DECODING, = 1.0 ANCHOREDByte DECODING, = 1.5 ANCHOREDByte DECODING, = 2.0 ANCHOREDByte DECODING, = 3.0 ANCHOREDByte DECODING, = 4.0 ANCHOREDByte DECODING, = 5.0 ANCHOREDByte DECODING, = 10.0 ANCHOREDByte DECODING, = 15.0 ANCHOREDByte DECODING, = 20.0 0.000 0.051 0.059 0.028 0.033 0.036 0.037 0.039 0.050 0.046 0.039 0.030 0. 0.002 0.001 0.001 0.003 0.002 0.009 0.009 0.014 0.016 0.022 0.029 0.045 0.046 0.000 0.039 0.046 0.019 0.021 0.024 0.025 0.026 0.039 0.034 0.029 0.021 0.013 0.000 0.000 0.000 0.000 0.001 0.005 0.007 0.010 0.011 0.016 0.021 0.033 0. 1.537 2.986 3.216 2.175 2.375 2.413 2.480 2.493 2.938 2.818 2.520 2.167 1.944 1.608 1.876 1.519 1.550 1.606 1.679 1.740 1.852 1.920 2.071 2.241 2.758 2.802 9.022 16.672 17.952 12.261 13.375 13.644 13.984 14.084 16.418 15.768 14.293 12.452 11. 9.327 11.100 8.999 9.306 9.560 10.035 10.388 10.899 11.267 11.985 12.803 15.494 15.742 0.000 1.486 1.719 0.676 0.802 0.830 0.960 0.957 1.445 1.324 1.073 0.700 0.491 0.003 0.094 0.003 0.021 0.057 0.151 0.215 0.305 0.351 0.527 0.736 1.238 1. 0.001 0.019 3.032 4.336 0.088 0. 0.023 0.009 0.010 0.012 0.013 0.012 0.020 0.018 0.015 0.010 0.008 4.318 4.132 4.279 4.281 4.299 4.304 4.342 4.259 4.111 3.868 3.671 0.457 0.339 0.433 0.447 0.454 0.452 0.459 0.458 0.457 0.422 0.360 0.001 0.002 3.262 3.802 0.175 0. 0.370 0.394 0.402 0.411 0.415 0.419 0.423 0.427 0.433 0.448 0.448 0.001 0.001 0.002 0.003 0.004 0.005 0.005 0.007 0.010 0.016 0.017 3.371 4.106 4.196 4.273 4.274 4.282 4.341 4.351 4.364 4.379 4.350 0.135 0.418 0.468 0.473 0.482 0.480 0.471 0.468 0.459 0.459 0.462 Table 19. Full results for ps=Comma 7B, pr=Qwen 2.5 72B (byte-level decoding). We report the average over 3 seeds. Setting Reference LMs ps pr Single-Model Methods (using pr) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1.0 Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHOREDByte DECODING, = 0.1 ANCHOREDByte DECODING, = 0.5 ANCHOREDByte DECODING, = 1.0 ANCHOREDByte DECODING, = 1.5 ANCHOREDByte DECODING, = 2.0 ANCHOREDByte DECODING, = 3.0 ANCHOREDByte DECODING, = 4.0 ANCHOREDByte DECODING, = 5.0 ANCHOREDByte DECODING, = 10.0 ANCHOREDByte DECODING, = 15.0 ANCHOREDByte DECODING, = 20.0 ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS Cosine MinHash Fluency FactScore 0.001 0.051 0.059 0.028 0.033 0.036 0.037 0.039 0.050 0.046 0.039 0.030 0.018 0.003 0. 0.001 0.004 0.007 0.011 0.014 0.019 0.025 0.030 0.035 0.048 0.049 0.000 0.039 0.046 0.019 0.021 0.024 0.025 0.026 0.039 0.034 0.029 0.021 0.013 0.001 0.003 0.000 0.001 0.004 0.007 0.010 0.012 0.016 0.019 0.024 0.036 0.037 1.528 2. 3.216 2.175 2.375 2.413 2.480 2.493 2.938 2.818 2.520 2.167 1.944 1.684 2.094 1.542 1.655 1.744 1.805 1.883 2.020 2.133 2.199 2.464 2.867 2.904 50 8.902 16.672 0.028 1. 0.379 0.454 0.001 0.019 4.058 4.336 0.156 0.457 17.952 12.261 13.375 13.644 13.984 14.084 16.418 15.768 14.293 12.452 11.156 1.719 0.676 0.802 0.830 0.960 0.957 1.445 1.324 1.073 0.700 0. 0.469 0.425 0.436 0.437 0.440 0.441 0.451 0.449 0.434 0.409 0.397 0.023 0.009 0.010 0.012 0.013 0.012 0.020 0.018 0.015 0.010 0.008 4.318 4.132 4.279 4.281 4.299 4.304 4.342 4.259 4.111 3.868 3.671 0.457 0.339 0.433 0.447 0.454 0.452 0.459 0.458 0.457 0.422 0.360 9.720 12.186 0.045 0. 0.390 0.422 0.002 0.004 3.945 3.981 0.247 0.415 9.030 9.817 10.398 10.646 10.990 11.758 12.290 12.640 14.052 16.133 16.307 0.028 0.061 0.177 0.252 0.331 0.449 0.572 0.667 0.931 1.356 1. 0.386 0.407 0.414 0.423 0.425 0.429 0.430 0.431 0.441 0.451 0.452 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.009 0.012 0.018 0.018 4.091 4.231 4.271 4.296 4.297 4.319 4.325 4.326 4.330 4.342 4.344 0.201 0.433 0.474 0.478 0.487 0.485 0.483 0.479 0.467 0.462 0.463 Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model Table 20. Full results for ps=TinyComma 1.8B, pr=Llama 4 Scout 17Bx16E (byte-level decoding). We report the average over 3 seeds. Setting ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS MinHash Fluency FactScore Reference LMs ps pr Single-Model Methods (using pr) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1.0 Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHOREDByte DECODING, = 0.1 ANCHOREDByte DECODING, = 0.5 ANCHOREDByte DECODING, = 1.0 ANCHOREDByte DECODING, = 1.5 ANCHOREDByte DECODING, = 2.0 ANCHOREDByte DECODING, = 3.0 ANCHOREDByte DECODING, = 4.0 ANCHOREDByte DECODING, = 5.0 ANCHOREDByte DECODING, = 10.0 ANCHOREDByte DECODING, = 15.0 ANCHOREDByte DECODING, = 20.0 0.000 0.033 0.031 0.021 0.024 0.025 0.025 0.026 0.028 0.028 0.020 0.013 0.008 0.002 0. 0.000 0.002 0.005 0.008 0.008 0.011 0.010 0.013 0.016 0.026 0.028 0.000 0.020 0.020 0.011 0.012 0.014 0.012 0.012 0.021 0.017 0.011 0.007 0.004 0.001 0.001 0.000 0.000 0.003 0.005 0.005 0.006 0.005 0.007 0.006 0.018 0.018 1.537 2. 2.502 2.008 2.138 2.169 2.202 2.194 2.344 2.210 1.938 1.666 1.451 1.776 1.916 1.583 1.611 1.659 1.792 1.805 1.876 1.865 1.910 2.017 2.330 2.335 9.022 13.932 14.346 11.614 12.312 12.505 12.638 12.671 13.541 12.870 11.412 10.023 8.803 10.275 11. 9.192 9.603 9.768 10.492 10.587 10.949 10.916 11.138 11.701 13.374 13.395 0.000 0.830 0.886 0.476 0.509 0.531 0.576 0.574 0.762 0.641 0.394 0.278 0.161 0.045 0.088 0.000 0.040 0.072 0.180 0.200 0.245 0.238 0.286 0.297 0.704 0.715 0.001 0. 3.032 4.531 0.088 0.563 0.012 0.006 0.007 0.008 0.008 0.008 0.011 0.010 0.007 0.006 0.004 4.477 4.300 4.411 4.445 4.464 4.467 4.504 4.459 4.252 3.915 3.476 0.560 0.481 0.535 0.550 0.552 0.555 0.562 0.551 0.499 0.461 0.355 0.002 0. 3.969 3.751 0.270 0.474 0.001 0.001 0.002 0.003 0.004 0.004 0.004 0.005 0.006 0.010 0.010 3.407 4.307 4.430 4.455 4.486 4.482 4.521 4.521 4.521 4.521 4.520 0.121 0.476 0.556 0.578 0.580 0.579 0.583 0.588 0.569 0.568 0.569 Table 21. Full results for ps=Comma 7B, pr=Llama 4 Scout 17Bx16E (byte-level decoding). We report the average over 3 seeds. Setting ROUGE-1 τ ROUGE-L τ Word LCS Char. LCS Word ACS MinHash Fluency FactScore Reference LMs ps pr Single-Model Methods (using pr) SYSTEM MEMFREE, = 3 MEMFREE, = 5 MEMFREE, = 7 MEMFREE, = 9 MEMFREE, = 10 RCAD, α = 0.1 RCAD, α = 0.25 RCAD, α = 0.5 RCAD, α = 0.75 RCAD, α = 1.0 Two-Model Baselines (using pr and ps) CP-FUSE TOKENSWAP Ours (using pr and ps) ANCHOREDByte DECODING, = 0.1 ANCHOREDByte DECODING, = 0.5 ANCHOREDByte DECODING, = 1.0 ANCHOREDByte DECODING, = 1.5 ANCHOREDByte DECODING, = 2.0 ANCHOREDByte DECODING, = 3.0 ANCHOREDByte DECODING, = 4.0 ANCHOREDByte DECODING, = 5.0 ANCHOREDByte DECODING, = 10.0 ANCHOREDByte DECODING, = 15.0 ANCHOREDByte DECODING, = 20.0 0.000 0.033 0.031 0.021 0.024 0.025 0.025 0.026 0.028 0.028 0.020 0.013 0.008 0.002 0. 0.000 0.002 0.005 0.008 0.008 0.011 0.010 0.013 0.016 0.026 0.028 1.537 2.436 2.502 2.008 2.138 2.169 2.202 2.194 2.344 2.210 1.938 1.666 1.451 1.776 1.916 1.583 1.611 1.659 1.792 1.805 1.876 1.865 1.910 2.017 2.330 2.335 9.022 13. 14.346 11.614 12.312 12.505 12.638 12.671 13.541 12.870 11.412 10.023 8.803 10.275 11.172 9.192 9.603 9.768 10.492 10.587 10.949 10.916 11.138 11.701 13.374 13.395 0.000 0.830 0.886 0.476 0.509 0.531 0.576 0.574 0.762 0.641 0.394 0.278 0.161 0.045 0. 0.000 0.040 0.072 0.180 0.200 0.245 0.238 0.286 0.297 0.704 0.715 0.001 0.011 3.032 4.531 0.088 0.563 0.012 0.006 0.007 0.008 0.008 0.008 0.011 0.010 0.007 0.006 0.004 4.477 4.300 4.411 4.445 4.464 4.467 4.504 4.459 4.252 3.915 3. 0.560 0.481 0.535 0.550 0.552 0.555 0.562 0.551 0.499 0.461 0.355 0.002 0.002 3.969 3.751 0.270 0.474 0.001 0.001 0.002 0.003 0.004 0.004 0.004 0.005 0.006 0.010 0.010 3.407 4.307 4.430 4.455 4.486 4.482 4.521 4.521 4.521 4.521 4. 0.121 0.476 0.556 0.578 0.580 0.579 0.583 0.588 0.569 0.568 0.569 0.000 0.020 0.020 0.011 0.012 0.014 0.012 0.012 0.021 0.017 0.011 0.007 0.004 0.001 0.001 0.000 0.000 0.003 0.005 0.005 0.006 0.005 0.007 0.006 0.018 0."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "University of Washington"
    ]
}