{
    "paper_title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training",
    "authors": [
        "Yifan Wang",
        "Binbin Liu",
        "Fengze Liu",
        "Yuanfan Guo",
        "Jiyao Deng",
        "Xuecheng Wu",
        "Weidong Zhou",
        "Xiaohuan Zhou",
        "Taifeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The data mixture used in the pre-training of a language model is a cornerstone of its final performance. However, a static mixing strategy is suboptimal, as the model's learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in a computationally efficient manner remains a significant challenge. To address this, we propose TiKMiX, a method that dynamically adjusts the data mixture according to the model's evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as a search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses a regression model to predict a superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that a model's data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, a direct measure of these preferences, significantly improves performance by mitigating the underdigestion of data seen with static ratios."
        },
        {
            "title": "Start",
            "content": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training Yifan Wang, Binbin Liu, Fengze Liu, Yuanfan Guo, Jiyao Deng, Xuecheng Wu Weidong Zhou, Xiaohuan Zhou*, Taifeng Wang ByteDance yifanyfwang@gmail.com, zhouxiaohuan@bytedance.com 5 2 0 2 5 2 ] . [ 1 7 7 6 7 1 . 8 0 5 2 : r Abstract The data mixture used in the pre-training of language model is cornerstone of its final performance. However, static mixing strategy is suboptimal, as the models learning preferences for various data domains shift dynamically throughout training. Crucially, observing these evolving preferences in computationally efficient manner remains significant challenge. To address this, we propose TiKMiX, method that dynamically adjusts the data mixture according to the models evolving preferences. TiKMiX introduces Group Influence, an efficient metric for evaluating the impact of data domains on the model. This metric enables the formulation of the data mixing problem as search for an optimal, influence-maximizing distribution. We solve this via two approaches: TiKMiX-D for direct optimization, and TiKMiX-M, which uses regression model to predict superior mixture. We trained models with different numbers of parameters, on up to 1 trillion tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like REGMIX while using just 20% of the computational resources. TiKMiX-M leads to an average performance gain of 2% across 9 downstream benchmarks. Our experiments reveal that models data preferences evolve with training progress and scale, and we demonstrate that dynamically adjusting the data mixture based on Group Influence, direct measure of these preferences, significantly improves performance by mitigating the under digestion of data seen with static ratios. Introduction The availability of large-scale public datasets has been key factor in the creation of Large Language Models (LLMs). The pre-training data for LLMs is predominantly sourced from the internet (Wettig et al. 2025; Yu, Liu, and Xiong 2025), encompassing wide range of materials such as academic papers (Tirumala et al. 2023), books (Tirumala et al. 2023), and more. The mixture ratio of data from these different domains has significant impact on the capabilities of an LLM (Zhang et al. 2025b; Liu et al. 2025b; Bai et al. 2024a). For instance, the authors of GPT-3 (Floridi and Chiriatti 2020) considered Wikipedia to have very high-quality data and consequently decided to increase its proportion in the training dataset. REGMIX (Liu et al. 2024) uses results from small-scale experiments to automatically set its mixing ratios, but it does not account for dynamic changes in the state of the *Corresponding author. Figure 1: Performance Comparisons of our TiKMiX versus SOTA Data Mixing Strategies for Pre-training 1B Parameter Language Model with 1T Tokens. model (Yu, Das, and Xiong 2024; Zhang et al. 2025a). This leads to critical research question: How can we dynamically select the training data for model based on its evolving data preferences in manner that is both scalable and efficient? Prior research (Xie et al. 2023; Fan, Pagliardini, and Jaggi 2023; Team 2024; Albalak et al. 2023) has leveraged small proxy models to determine domain weights for large-scale language models. This approach is computationally expensive, as it requires training these proxy models on massive datasets, often exceeding 100 billion tokens. Some methods assume that the relative performance of data mixtures is stable across different model scales and training durations (Liu et al. 2024), they neglect the dynamic nature of models data preferences as training progresses. Approaches such as ODM (Albalak et al. 2023) attempt to address this by monitoring training dynamics to guide data allocation, but their iterative nature proves inefficient when dealing with the everincreasing scale of pre-training data (Jin et al. 2024; Wang et al. 2025). significant gap exists in current practices: leading LLMs (Yang et al. 2025; Team et al. 2025; Dubey et al. 2024) utilize multi-stage pre-training, but they lack mechanism for rapid, dynamic data re-weighting between stages that aligns with the models evolving preferences. We propose data mixing strategy that dynamically adjusts data proportions during training with minimal computational overhead. To this end, we introduce Group Influence, which efficiently evaluates each domains collective impact on validation performance at low computational cost by leveraging gradient accumulation. This allows us to quantify the models data preferences at any training stage. Building upon this, we introduce TiKMiX, method that dynamically adjusts the data mixing strategy by framing it as an optimization problem: finding the data combination that maximizes positive influence. We devise two approaches to solve this: TiKMiXD, which directly optimizes weighted sum of influences from individual domains to find the best mixing ratios; and the more sophisticated TiKMiX-M, which uses TiKMiX-Ds output as starting point, conducts perturbation experiments in its vicinity, and then uses regression model to fit the relationship between mixing ratios and performance, thereby predicting globally optimal mixture for subsequent largescale training. With the proposed TiKMiX, we can dynamically adjust the data mixture strategy throughout the models entire pretraining cycle, adapting to changes in model scale and training stage. Following previous work (Bai et al. 2024b; Kang et al. 2024; Diao et al. 2025; Tao et al. 2025), we trained models with varying parameter sizes, scaling up to 1 trillion tokens. TiKMiX-D outperforms state-of-the-art methods such as REGMIX while requiring only 20% of the computational resources. TiKMiX-M achieves an average performance improvement of 2% across 9 downstream benchmarks as shown in Fig. 1. Moreover, we further discuss the feasibility and implications of employing TiKMiX in even largerscale models. Our experiments also revealed several key phenomena:(1)A models data preferences change as training progresses;(2)Models of different scales exhibit different patterns of preference change;(3) Dynamically adjusting the data mixture promotes more thorough learning of the data by the model. In conclusion, the main contributions of this paper can be summarized as follows: We propose Group Influence, novel and efficient method for observing and quantifying the dynamic preferences of Large Language Models for different data domains during the pre-training process. We designed TiKMiX, dynamic data mixture framework that leverages the observations from Group Influence to adaptively adjust data ratios, aiming to balance the models performance across multiple tasks. Extensive experiments demonstrate that our method not only significantly enhances model performance but also provides profound insights into how models data preferences evolve with the training process and model scale, thereby validating the effectiveness of dynamically adjusting data proportions."
        },
        {
            "title": "Related Work",
            "content": "Influence Function Influence Functions offer mathematically grounded method to estimate the effect of training data on model predictions without costly retraining (Koh and Liang 2017). Their application to high-dimensional models like Large Language Models (LLMs) has been hampered by the computational challenge of inverting the Hessian matrix. Recent work has overcome this barrier through scalable approximation techniques. Notably, the work by Anthropic (Grosse et al. 2023) adapted EK-FAC (George et al. 2018), an efficient Hessian approximation, to successfully apply influence functions to 50B-parameter Transformer models. This breakthrough established influence functions as viable tool for performing data attribution at the scale of modern LLMs, enabling the identification of specific pre-training data that drives model outputs (Kou et al. 2025; Choe et al. 2024; Lin et al. 2024a). However, computation at the sample level incurs prohibitive overhead in large-scale pre-training scenarios. Therefore, we propose Group Influence, method that extends influence functions to groups of data. By leveraging gradient accumulation techniques, Group Influence can efficiently evaluate the collective impact of an entire data domain with relatively low computational cost. This allows us to quantify the models current data preferences. Data Selection and Mixing Strategic curation of training data significantly enhances model performance (Koh and Liang 2017; Albalak et al. 2023). For pre-training Large Language Models (LLMs), data curation methods are commonly categorized by granularity: Token-level Selection: The most fine-grained approach, which filters individual tokens according to specific criteria (Lin et al. 2024b). Sample-level Selection: Methods include heuristic-based approaches (Sharma et al. 2024; Soldaini et al. 2024) and learning-based techniques employing optimization algorithms (Chen et al. 2024; Shao et al. 2024). Additionally, approaches such as MATES (Yu, Das, and Xiong 2024) utilize model-derived signals (e.g., perplexity or loss) to inform selection (Marion et al. 2023; Ankner et al. 2024). Group-level Selection: This method partitions data into groups or domains and seeks optimal mixing ratios. Earlier work relied on manually defined ratios, while recent advances favor learning-based strategies. Offline methods like REGMIX (Liu et al. 2024) and DoReMi (Xie et al. 2023) use proxy models to assign static group weights, whereas dynamic methods such as Quad (Zhang et al. 2025a) and ODM (Albalak et al. 2023) iteratively adjust weights during training. Current mainstream pre-training pipelines are typically divided into multiple stages but often lack mechanism to dynamically adjust the data mixture ratio based on the models state in different stages. Our proposed method, TiKMiX, is semi-offline, group-level selection approach that dynamically adjusts the data mixture ratio across multiple training stages. Unlike fully dynamic methods that require repeated iterative updates, TiKMiX directly optimizes the mixture ratio based on the models current data preferences, enabling efficient adaptation without multiple rounds of adjustment. Figure 2: The process involves periodically measuring domain contributions via Group Influence and adjusting the data mixture to maximize learning efficiency. Methodology In this section, we introduce TiKMiX, framework for dynamically optimizing the data mixture during large language model pre-training as shown in Fig. 2. Our approach is centered on novel metric, Group Influence, designed to efficiently measure the real-time contribution of each data domain to the models learning. We formulate the dynamic data mixture problem as an optimization task aimed at maximizing this Group Influence. To solve this, we propose two distinct methods : TiKMiX-D, which directly optimizes the mixture based on influence scores, and TiKMiX-M, which leverages regression model for computationally efficient approximation. We first define the problem setup and Group Influence, then elaborate on these two optimization strategies. Group Influence Group Influence function extends the classical influence function framework from individual data points to cohesive groups of data. We first establish the theoretical motivation for this extension, then provide rigorous mathematical derivation of Group Influence, and finally, discuss its computational properties. Influence functions offer principled and computationally efficient method for estimating the effect of single training instance on models parameters or predictions (Koh and Liang 2017). By approximating the change in model parameters resulting from upweighting training point z, they provide valuable insights into model behavior without the need for retraining. However, many complex model behaviors, such as systemic bias, factual recall, or vulnerability to specific adversarial attacks, are not attributable to single, isolated training example. Instead, they often emerge from the collective effect of group of semantically related instances. linear summation of individual influence scores, i.e., (cid:80) ziS I(zi), is insufficient as it fails to capture the nontrivial interactions between data points during optimization. The collective gradient of group can shape the loss landscape in manner distinct from the sum of its constituent parts. To quantify the consolidated impact of data subset as single entity, we define the Group Influence function. Let model, parameterized by θ Rd, be trained on dataset = {z1, . . . , zN } by minimizing an empirical risk objective J(θ): θ = arg min θ J(θ) = arg min θ 1 N (cid:88) i=1 L(zi, θ), (1) where L(zi, θ) is the loss function for instance zi. To measure the influence of subset D, we introduce perturbed objective where every member of is simultaneously upweighted by an infinitesimal positive value ϵ. The new optimal parameters θ ϵ are found by minimizing this perturbed objective: θ ϵ = arg min θ 1 (cid:88) i= L(zi, θ) + ϵ (cid:88) zj L(zj, θ) . (2) This formulation models scenario where the training process is nudged to place greater emphasis on the group S. For ϵ=0 = θ. ϵ = 0, we recover the original optimal parameters, θ The influence of group on the model parameters is then defined as the rate of change of θ ϵ with respect to ϵ, evaluated at ϵ = 0. closed-form expression for this quantity can be derived using the implicit function theorem. The first-order optimality condition for any ϵ requires that the gradient of the perturbed objective at its minimum θ ϵ is zero, which can be formulated as: θJϵ(θ ϵ , S) = 1 (cid:88) i=1 θL(zi, θ ϵ )+ϵ (cid:88) zj θL(zj, θ ϵ ) = 0. (3) Figure 3: The impact of different pre-training data domains on the validation set as training progresses. Differentiating this entire equation with respect to ϵ via the chain rule yields: Evaluating this expression at ϵ = 0 (where θ dϵ [θJϵ(θ ϵ , S)] = 2 dθ ϵ dϵ ϵ + θJϵ(θ (θJϵ(θ ϵ , S) ϵ , S)) = 0. (4) ϵ=0 = θ), ϵ , S) simplifies to the Hessian of the θJ(θ). The partial derivative zj θL(zj, θ). Substituting these into θJϵ(θ the Hessian 2 original objective, Hθ 2 term becomes (cid:80) Equation 4 gives: Hθ dθ ϵ dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 (cid:88) + zj θL(zj, θ) = 0. (5) Assuming the Hessian Hθ is positive definite and thus invertible, we can solve for the influence of group on the model parameters: Iparam(S) dθ ϵ dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = 1 θ (cid:88) zj θL(zj, θ) . (6) common practical application is to measure the influence of on scalar-valued function of the parameters, (θ), such as the loss on test sample, (θ) = L(ztest, θ). By applying the chain rule, the influence of on is given by: (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = θf (θ)T dθ ϵ dϵ If (S) df (θ ϵ ) dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 (7) . Substituting Equation 6 into Equation 7 yields the final expression for the Group Influence function: If (S) = θf (θ)T 1 θ (cid:88) θL(zj, θ) . (8) zj The scalar value If (S) quantifies the extent to which upweighting the group during training would increase (If (S) > 0) or decrease (If (S) < 0) the value of the function . significant computational advantage of Equation 8 is its structure. The term (cid:80) zj θL(zj, θ) is the accumulated gradient of the group S. This allows for an efficient implementation where the gradients for all samples within the group are first computed and aggregated. Subsequently, the computationally intensive Hessian-inverse-vector product is performed only once. This structure ensures the computation of Group Influence is scalable, as its cost is not dominated by the cardinality of the group S. TiKMiX-D: Directly maximize influence Building upon the Group Influence metric, which quantifies the impact of data domains on the models state as shown in Fig. 3, our objective is to determine an optimal data mixture, represented by weight vector w, that maximizes the aggregate benefits of this influence. To this end, we introduce TiKMiX-D, method that formulates this task as multiobjective optimization problem. This approach dynamically adjusts the data mixture for the subsequent training stage to achieve balanced performance improvement, maximize overall gains, and maintain data diversity. The Group Influence scores, as computed in the previous section, are first organized into an Influence Matrix, S, where is the number of validation tasks and is the number of data domains. Each element Sij represents the influence of data domain dj on validation task vi. Given data mixture weight vector = [w1, w2, . . . , wm]T , the expected total influence on each validation task is captured by the vector = w. To facilitate fair comparison across tasks of varying scales, we normalize this influence vector. The normalized influence ˆPi for task vi can be computed as: Pi maxj Sij + ϵ ˆPi = , (9) where ϵ is small constant (e.g., 108) to prevent division by zero. The optimization objective for TiKMiX-D is formulated as unified function L(w) that integrates three distinct goals: L(w) = α std( ˆP ) β (cid:88) i=1 ˆPi γ H(w). (10) This function balances three key components. First, influence uniformity, measured by the standard deviation of the normalized influence, std( ˆP ), encourages balanced capability gains across all tasks. Second, overall influence gain, represented by the total sum of normalized influences, (cid:80) ˆPi, aims to maximize the models aggregate performance improvement; hence, its negative is minimized. Third, data diversity, measured by the information entropy of the weight vector, H(w) = (cid:80)m j=1 wj log(wj), promotes more uniform weight distribution to ensure robust generalization. The hyperparameters α, β, and γ control the trade-offs among these objectives; in our experiments, they are set to 1 to assign equal importance. The complete optimization problem is subject to several constraints to ensure valid and beneficial solution. The weights must be non-negative (wj 0) and sum to one ((cid:80) wj = 1). Furthermore, to guarantee continuous improvement, we enforce Pareto improvement constraint, ensuring that the influence generated by the new mixture is no less than that of the prior mixture wprior for any task, i.e., wprior. This leads to the final constrained non-linear optimization problem: minimize α std( ˆP ) β subject to (cid:88) wj = (cid:88) i=1 ˆPi γ H(w) (11) j=1 wj 0, wprior. {1, . . . , m} We employ the Sequential Least Squares Quadratic Programming algorithm (Gupta and Gupta 2018) to solve this problem, initializing the weights with uniform distribution. The resulting optimal vector, wbest, serves as the dynamic data mixture for the subsequent training stage. TiKMiX-M: Mix influence model While TiKMiX-D provides an efficient strategy for data mixing through direct optimization, it operates on the assumption that the influences of data domains are linearly additive. This simplification may overlook the mix of different domain, non-linear cross-domain interactions that arise when different data sources are combined. We introduce TiKMiX-M, optimize mixture proportions by modeling the interactions within domain mixtures To more accurately capture these mixture effects. To explore the models performance across diverse range of domain weightings, we generate set of candidate mixture vectors. Our approach is anchored by an empirically determined prior weight vector, worig RD, where is the number of domains. For each domain i, we define plausible sampling interval by scaling the original weight. We employ Latin Hypercube Sampling (Loh 2021) within this D-dimensional hyperrectangle to efficiently generate candidate vectors, ensuring uniform and non-collapsing coverage of the parameter space. Each candidate vector wcand produced by Latin Hypercube Sampling is subsequently normalized to satisfy the sum-toone constraint ((cid:80)D i=1 wi = 1), yielding normalized vector wnorm = wcand/ (cid:80)D j=1 wcand,j. However, this normalization can shift components outside their predefined intervals. Therefore, we implement rejection sampling scheme, where normalized vector wnorm is accepted into our final set only if it satisfies the boundary constraints for all dimensions, i.e., wnorm,i [li, hi] for all {1, . . . , D}. This iterative process is repeated until valid weight vectors that meet both the summation and boundary conditions have been collected, resulting in robust and well-distributed set of weights for subsequent analysis. For each generated candidate mixture wi, we calculate its true aggregate influence score, yi, across all validation sets using the Group Influence evaluation method. This score can be the sum of normalized influences, (cid:80) ˆPi, or another comprehensive metric. Following these steps, we obtain training set Dtrain = {(wi, yi)}N i=1. We select LightGBM (Ke et al. 2017), an efficient gradient boosting decision tree model, as our regression surrogate. This model, fLGBM, is trained to predict the aggregate influence for given data mixture w, i.e., = fLGBM(w).We leverage it to efficiently explore the mixture space without performing expensive, true influence evaluations. We design an iterative search algorithm that balances exploration and exploitation to find the optimal mixture. Algorithm 1: Iterative Search via TiKMiX-M Input: Surrogate fsur, initial mix w(0), iters , samples , exploration [αmin, αmax], top-k. Output: Optimized mixture w. wbest w(0) Generate exploration strengths {αt}T from αmax to αmin. for = 1 to do t=1 logarithmically i=1 Dirichlet(αt wbest). Sample candidates {wi}N Predict scores yi = fsur(wi) for each wi. Let Itop-k be the indices of the top scores. wbest 1 wi. iItop-k (cid:80) end for return wbest The process is detailed in Algorithm 1. We start from the ratio from TiKMiX-D, wbest-D. At each step, we sample candidate mixtures on the current best solution. The distributions concentration parameter is annealed over step, beginning with large value to encourage global exploration and gradually decreasing to promote local exploitation near the optimum. We employ the surrogate model to evaluate all sampled candidates. The center for the next iteration is then updated to be the average of the top-k candidates with the highest predicted scores. This procedure is repeated until convergence or maximum number of iterations is reached. TiKMiX-M not only accounts for non-linear cross-domain interactions but also significantly enhances search efficiency through the surrogate model, enabling it to discover superior solutions within the vast mixture space. Experiments This section presents comprehensive set of experiments designed to validate the effectiveness of our TiKMiX framework. We first outline the experimental setup, including evaluation benchmarks, datasets, and baseline methods. Subsequently, we demonstrate that: (1) the pre-training data mixture significantly impacts downstream task performance; (2) our proposed Group Influence is an effective predictor of downstream performance; and (3) the TiKMiX framework, particularly TiKMiX-D and TiKMiX-M, markedly improves model performance and surpasses existing SOTA methods. Experimental Setup Datasets and Models Optimizing the data mixture of webscale corpora is crucial and highly impactful step in pretraining performant Large Language Models (LLMs). While the diversity of web data presents unique challenges, effective mixing strategies can unlock significant performance gains. To systematically investigate this, we conduct our experiments on the RefinedWeb dataset (Penedo et al. 2023), which comprises 26 distinct data domains. Our models, ranging in size from 1B to 7B parameters, are trained on up to 1 trillion tokens. The training process is divided into two distinct stages, each consisting of 500 billion tokens, with strategic adjustment of the data mixture ratio at the transition point between stages. We compare TiKMiX against several representative data mixing strategies: Pile-CC (Gao et al. 2020): The original data mixture proposed by the authors of The Pile based on heuristics. REGMIX (Liu et al. 2024): SOTA method that uses regression model to predict and optimize validation loss for determining the mixture. DoReMi (Xie et al. 2023): classic dynamic data mixing method that relies on proxy model. QUAD (Zhang et al. 2025a): method for dynamic selection during training after clustering data We use the best-reported mixture from their paper, re-normalized to the domains available in our setup. Downstream Task Evaluation To comprehensively evaluate our proposed method, we curated diverse set of 9 widely recognized downstream benchmarks, which were strategically divided into two categories: in-domain and outof-domain. This division allows for rigorous assessment of both the models core capabilities and its generalization prowess. Our in-domain evaluation suite was designed to cover wide spectrum of reasoning and knowledge-based tasks. It includes MMLU (Massive Multitask Language Understanding) (Hendrycks et al. 2020), challenging benchFigure 4: Analysis of the Group Influence and actual performance on the benchmark. mark measuring knowledge across 57 diverse subjects; HellaSwag (Zellers et al. 2019), commonsense reasoning task that involves choosing the most plausible continuation for given context; ARC (Clark et al. 2018), which we evaluate on both the Easy (ARC-E) and the more difficult Challenge (ARC-C) sets of grade-school science questions; and TriviaQA (Joshi et al. 2017), reading comprehension benchmark requiring models to locate answers within lengthy documents. To evaluate the generalization capabilities of our method, we selected set of out-of-domain benchmarks. These include PiQA (Bisk et al. 2020), commonsense benchmark focused on physical interactions; OpenBookQA (Mihaylov et al. 2018), question-answering task requiring reasoning over given set of science facts; BoolQ (Clark et al. 2019), dataset of naturally occurring yes/no questions; and MathQA (Amini et al. 2019), mathematical reasoning benchmark with multi-step word problems."
        },
        {
            "title": "Group Influence as an Effective Predictor of\nPerformance",
            "content": "The core hypothesis of our introduced TiKMiX framework is that maximizing Group Influence can effectively enhance overall downstream task performance. To validate this hypothesis, we calculated the impact of 10 different data mixtures on various benchmarks. As validation, we trained 1B-parameter model on 500B data using the corresponding mixtures. The normalized scores are shown in Fig. 4. We can clearly observe strong positive correlation (i.e., Pearson correlation coefficient ρ = 0.789) between the total Group Influence and the average downstream scores. This indicates that mixtures generating higher total influence almost invariably lead to better downstream performance. This finding not only confirms the validity of Group Influence as an optimization target but also provides solid theoretical foundation for the design of our TiKMiX-D and TiKMiX-M. Benchmark Human DoReMi Average QUAD Pile-CC REGMiX TiKMiX-D TiKMiX-M In-Domain Benchmarks MMLU (Hendrycks et al. 2020) HellaSwag (Zellers et al. 2019) ARC Easy (Clark et al. 2018) ARC Challenge (Clark et al. 2018) Triviaqa (Joshi et al. 2017) Out-of-Domain Benchmarks PiQA (Bisk et al. 2020) OpenBookQA (Mihaylov et al. 2018) Boolq (Clark et al. 2019) MathQA (Amini et al. 2019) Estimated FLOPs Average Perf. Best On 31.3 55.5 64.4 33.7 17. 73.5 35.8 56.3 22.7 0 43.4 0/9 31.2 55.3 65.7 33.6 15.5 73.1 36.5 59.2 23.1 4.2e19 43.7 0/9 30.9 55.9 64.1 32.1 17. 71.5 34.6 58.3 23.7 0 43.2 0/9 31.7 56.5 62.8 33.5 17.6 72.4 36.6 60.5 23.9 2.3e18 43.9 0/9 31.2 55.6 63.2 32.7 16. 69.2 37.1 58.7 22.5 0 42.9 0/9 31.5 56.0 66.2 33.2 15.8 73.3 37.0 58.9 23.3 32.2 57.4 69.3 37.0 17.7 74.1 37.4 61.3 23. 31.8 56.6 70.7 38.3 17.3 74.5 37.4 62.2 24.2 3.7e18 43.9 0/9 7.2e17 45.5 4/9 3.2e18 45.9 6/9 Table 1: Comparison of 1B Parameter Models Trained on 1T Tokens Across Various Benchmarks. The best-performing model on each benchmark is highlighted in bold. TiKMiX Improves Downstream Performance Building on the preceding findings, we formally evaluate the two implementations of our TiKMiX framework: TiKMiX-D and TiKMiX-M. During 1T-token pre-training process, we dynamically adjusted the data mixture every 200B tokens using TiKMiX. As shown in Table 1, both of our methods significantly outperform all baselines. On average, across 9 benchmarks, TiKMiX-D and TiKMiX-M improved performance by 1.6% and 2.0%, respectively, over the strongest baseline, REGMIX. Notably, on challenging tasks like ARC Easy and ARC Challenge, TiKMiX-M achieved performance advantage of over 4.8%. Analysis of Computational Efficiency TiKMiX also excels in computational efficiency. Unlike methods such as MATES(Yu, Das, and Xiong 2024),GroupMATES(Yu et al. 2025) and REGMIX, which require training small proxy models, the Group Influence calculation and optimization process of TiKMiX is highly efficient. In our 1B model experiments, the total computational overhead for TiKMiX-D to determine the next-stage mixture (including influence calculation and regression model inference) was only about 20% of that required by the RegMix method, while achieving comparable or even superior performance. This high efficiency makes TiKMiX practical and powerful tool for large scale LLM training. Ablation Study We conduct series of ablation studies, with the results presented in Table 2. Our primary investigation focused on the efficacy of using group influence and TiKMiX for preference observation and data mixture adjustments. As shown in Table 2, our approach allows for the accurate observation of model preferences using only 0.1B tokens and requires no model training, leading to significant performance improvement over the loss. This highlights the superiority of our method in efficiently identifying and correcting data biases. We further discuss the effectiveness of our model on larger scale in the appendix. Benchmark In-Domain Benchmarks MMLU (Hendrycks et al. 2020) HellaSwag (Zellers et al. 2019) ARC Easy (Clark et al. 2018) ARC Challenge (Clark et al. 2018) TriviaQA (Joshi et al. 2017) Out-of-Domain Benchmarks PiQA (Bisk et al. 2020) OpenBookQA (Mihaylov et al. 2018) BoolQ (Clark et al. 2019) MathQA (Amini et al. 2019) Average Perf. Loss TiKMiX-D 5B 10B 0.1B 0.5B 31.4 56.3 67.3 34.4 16.5 73.2 36.4 59.4 23. 44.3 31.2 56.4 65.6 33.4 16.9 73.5 36.6 59.7 23.7 44.1 32.2 57.4 69.3 37.0 17.7 74.1 37.4 61.3 23. 45.5 32.1 57.6 69.1 37.1 17.9 74.2 37.3 61.5 23.6 45.6 Table 2: Ablation study of Loss and TiKMiX on different data sizes."
        },
        {
            "title": "Conclusion and Discussions",
            "content": "In this work, we address the suboptimality of static data mixing strategies in language model pre-training, demonstrating that models learning preferences for different data domains evolve dynamically with its training progress. To tackle this, we introduce TiKMiX, novel framework that dynamically adjusts the data mixture based on Group Influence, highly efficient metric to evaluate the contribution of data domains to the models performance. By framing data mixing as an influence-maximization problem, we developed two approaches: TiKMiX-D, which directly optimizes the mixture and surpasses state-of-the-art methods like REGMIX using only 20% of the computational resources, and TiKMiX-M, which uses regression model to predict superior mixtures, achieving an average performance gain of 2% across 9 downstream benchmarks. Our experiments confirm that dynamically adjusting the data mixture based on Group Influence significantly improves performance by mitigating the under-digestion of data seen with static ratios. We plan to conduct further experiments on larger-scale models and more diverse datasets to further validate the effectiveness of Group Influence and TiKMiX. References Albalak, A.; Pan, L.; Raffel, C.; and Wang, W. Y. 2023. Efficient online data mixing for language model pre-training. arXiv preprint arXiv:2312.02406. Amini, A.; Gabriel, S.; Lin, P.; Koncel-Kedziorski, R.; Choi, Y.; and Hajishirzi, H. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319. Ankner, Z.; Blakeney, C.; Sreenivasan, K.; Marion, M.; Leavitt, M. L.; and Paul, M. 2024. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541. Bai, T.; Liang, H.; Wan, B.; Xu, Y.; Li, X.; Li, S.; Yang, L.; Li, B.; Wang, Y.; Cui, B.; et al. 2024a. survey of multimodal large language model from data-centric perspective. arXiv preprint arXiv:2405.16640. Bai, T.; Yang, L.; Hao Wong, Z.; Peng, J.; Zhuang, X.; Zhang, C.; Wu, L.; Qiu, J.; Zhang, W.; Yuan, B.; et al. 2024b. Multiagent collaborative data selection for efficient llm pretraining. arXiv e-prints, arXiv2410. Bisk, Y.; Zellers, R.; Gao, J.; Choi, Y.; et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 74327439. Chen, X.; Wang, Z.; Sow, D.; Yang, J.; Chen, T.; Liang, Y.; Zhou, M.; and Wang, Z. 2024. Take the bull by the horns: Hard sample-reweighted continual training improves llm generalization. arXiv preprint arXiv:2402.14270. Choe, S. K.; Ahn, H.; Bae, J.; Zhao, K.; Kang, M.; Chung, Y.; Pratapa, A.; Neiswanger, W.; Strubell, E.; Mitamura, T.; et al. 2024. What is your data worth to gpt? llm-scale data valuation with influence functions. arXiv preprint arXiv:2405.13954. Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins, M.; and Toutanova, K. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044. Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick, C.; and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Diao, S.; Yang, Y.; Fu, Y.; Dong, X.; Su, D.; Kliegl, M.; Chen, Z.; Belcak, P.; Suhara, Y.; Yin, H.; et al. 2025. Climb: Clustering-based iterative data mixture bootstrapping for language model pre-training. arXiv preprint arXiv:2504.13161. Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.; et al. 2024. The llama 3 herd of models. arXiv e-prints, arXiv2407. Fan, S.; Pagliardini, M.; and Jaggi, M. 2023. Doge: Domain reweighting with generalization estimation. arXiv preprint arXiv:2310.15393. Floridi, L.; and Chiriatti, M. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and machines, 30(4): 681 694. Gao, L.; Biderman, S.; Black, S.; Golding, L.; Hoppe, T.; Foster, C.; Phang, J.; He, H.; Thite, A.; Nabeshima, N.; et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. George, T.; Laurent, C.; Bouthillier, X.; Ballas, N.; and Vincent, P. 2018. Fast approximate natural gradient descent in kronecker factored eigenbasis. Advances in neural information processing systems, 31. Grosse, R.; Bae, J.; Anil, C.; Elhage, N.; Tamkin, A.; Tajdini, A.; Steiner, B.; Li, D.; Durmus, E.; Perez, E.; et al. 2023. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296. Gupta, M.; and Gupta, B. 2018. An ensemble model for breast cancer prediction using sequential least squares programming method (slsqp). In 2018 eleventh international conference on contemporary computing (IC3), 13. IEEE. He, P.; Gao, J.; and Chen, W. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. arXiv preprint arXiv:2111.09543. Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2020. Measuring masarXiv preprint sive multitask language understanding. arXiv:2009.03300. Jin, X.; Zhu, H.; Li, S.; Wang, Z.; Liu, Z.; Tian, J.; Yu, C.; Qin, H.; and Li, S. Z. 2024. survey on mixup augmentations and beyond. arXiv preprint arXiv:2409.05202. Joshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Kang, F.; Sun, Y.; Wen, B.; Chen, S.; Song, D.; Mahmood, R.; and Jia, R. 2024. AutoScale: Automatic Prediction of Compute-optimal Data Compositions for Training LLMs. Ke, G.; Meng, Q.; Finley, T.; Wang, T.; Chen, W.; Ma, W.; Ye, Q.; and Liu, T.-Y. 2017. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30. Koh, P. W.; and Liang, P. 2017. Understanding black-box predictions via influence functions. In International conference on machine learning, 18851894. PMLR. Kou, S.; Tian, Q.; Xu, H.; Zeng, Z.; and Deng, Z. 2025. Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions. arXiv preprint arXiv:2505.19949. Lin, X.; Wang, W.; Li, Y.; Yang, S.; Feng, F.; Wei, Y.; and Chua, T.-S. 2024a. Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval, 365374. Lin, Z.; Gou, Z.; Gong, Y.; Liu, X.; Shen, Y.; Xu, R.; Lin, C.; Yang, Y.; Jiao, J.; Duan, N.; et al. 2024b. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965. Liu, F.; Zhou, W.; Liu, B.; Yu, Z.; Zhang, Y.; Lin, H.; Yu, Y.; Zhang, B.; Zhou, X.; Wang, T.; et al. 2025a. Quadmix: Quality-diversity balanced data selection for efficient llm pretraining. arXiv preprint arXiv:2504.16511. Xie, S. M.; Pham, H.; Dong, X.; Du, N.; Liu, H.; Lu, Y.; Liang, P. S.; Le, Q. V.; Ma, T.; and Yu, A. W. 2023. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36: 6979869818. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yu, S.; Liu, Z.; and Xiong, C. 2025. Craw4LLM: Efficient Web Crawling for LLM Pretraining. arXiv preprint arXiv:2502.13347. Yu, Z.; Das, S.; and Xiong, C. 2024. Mates: Model-aware data selection for efficient pretraining with data influence models. Advances in Neural Information Processing Systems, 37: 108735108759. Yu, Z.; Peng, F.; Lei, J.; Overwijk, A.; Yih, W.-t.; and Xiong, C. 2025. Data-efficient pretraining with group-level data influence modeling. arXiv preprint arXiv:2502.14709. Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and Choi, Y. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Zhang, C.; Zhong, H.; Zhang, K.; Chai, C.; Wang, R.; Zhuang, X.; Bai, T.; Jiantao, Q.; Cao, L.; Fan, J.; et al. 2025a. Harnessing Diversity for Important Data Selection in Pretraining Large Language Models. In The Thirteenth International Conference on Learning Representations. Zhang, X.; Wang, D.; Dou, L.; Zhu, Q.; and Che, W. 2025b. survey of table reasoning with large language models. Frontiers of Computer Science, 19(9): 199348. Liu, J.; Zhu, D.; Bai, Z.; He, Y.; Liao, H.; Que, H.; Wang, Z.; Zhang, C.; Zhang, G.; Zhang, J.; et al. 2025b. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407. Liu, Q.; Zheng, X.; Muennighoff, N.; Zeng, G.; Dou, L.; Pang, T.; Jiang, J.; and Lin, M. 2024. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492. Loh, W.-L. 2021. On Latin hypercube sampling. The annals of statistics. Marion, M.; Ustun, A.; Pozzobon, L.; Wang, A.; Fadaee, M.; and Hooker, S. 2023. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564. Mihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789. Penedo, G.; Malartic, Q.; Hesslow, D.; Cojocaru, R.; Alobeidli, H.; Cappelli, A.; Pannier, B.; Almazrouei, E.; and Launay, J. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36: 7915579172. Shao, Y.; Li, L.; Fei, Z.; Yan, H.; Lin, D.; and Qiu, X. 2024. Balanced data sampling for language model training with clustering. arXiv preprint arXiv:2402.14526. Sharma, V.; Padthe, K.; Ardalani, N.; Tirumala, K.; Howes, R.; Xu, H.; Huang, P.-Y.; Li, S.-W.; Aghajanyan, A.; Ghosh, G.; et al. 2024. Text quality-based pruning for efficient training of language models. arXiv preprint arXiv:2405.01582. Soldaini, L.; Kinney, R.; Bhagia, A.; Schwenk, D.; Atkinson, D.; Authur, R.; Bogin, B.; Chandu, K.; Dumas, J.; Elazar, Y.; et al. 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159. Tao, Z. S.; Vinken, K.; Yeh, H.-W.; Cooper, A.; and Boix, X. 2025. Merge to Mix: Mixing Datasets via Model Merging. arXiv preprint arXiv:2505.16066. Team, K.; Bai, Y.; Bao, Y.; Chen, G.; Chen, J.; Chen, N.; Chen, R.; Chen, Y.; Chen, Y.; Chen, Y.; et al. 2025. Kimi K2: Open Agentic Intelligence. arXiv preprint arXiv:2507.20534. Team, Q. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Tirumala, K.; Simig, D.; Aghajanyan, A.; and Morcos, A. 2023. D4: Improving llm pretraining via document deduplication and diversification. Advances in Neural Information Processing Systems, 36: 5398353995. Wang, K.; Zhang, G.; Zhou, Z.; Wu, J.; Yu, M.; Zhao, S.; Yin, C.; Fu, J.; Yan, Y.; Luo, H.; et al. 2025. comprehensive survey in llm (-agent) full stack safety: Data, training and deployment. arXiv preprint arXiv:2504.15585. Wettig, A.; Lo, K.; Min, S.; Hajishirzi, H.; Chen, D.; and Soldaini, L. 2025. Organize the Web: Constructing Domains Enhances Pre-Training Data Curation. arXiv preprint arXiv:2502.10341."
        },
        {
            "title": "Appendix",
            "content": "Experimental Setup Datasets and Models Web data serves as one of the core sources for pre-training large language models (LLMs), playing crucial role in enhancing model capabilities due to its broad coverage and diversity. However, precisely because web data encompasses wide range of domainsincluding news, encyclopedias, forums, and academic contentits highly diverse origins make it extremely challenging to achieve balanced mixture across different domains. We follow the same experimental setup as prior studies on web data mixture (Wettig et al. 2025; Liu et al. 2025a), utilize the RefinedWeb dataset (Penedo et al. 2023), and employ the domain classifier (He, Gao, and Chen 2023) to categorize the data into 26 distinct domains. Our models, ranging in size from 1B to 7B parameters, are trained on up to 1 trillion tokens. The training process is divided into two distinct stages, each consisting of 500 billion tokens, with strategic adjustment of the data mixture ratio at the transition point between stages. We compare TiKMiX against several representative data mixing strategies: Pile-CC (Gao et al. 2020): The original data mixture proposed by the authors of The Pile based on heuristics. REGMIX (Liu et al. 2024): SOTA method that uses regression model to predict and optimize validation loss for determining the mixture. DoReMi (Xie et al. 2023): classic dynamic data mixing method that relies on proxy model. QUAD (Zhang et al. 2025a): method for dynamic selection during training after clustering data We use the best-reported mixture from their paper, re-normalized to the domains available in our setup. Our proposed TiKMiX method achieves balance between dynamic adaptability and computational efficiency in data mixture strategies. Similar to other dynamic approaches such as DoReMi and QUAD, TiKMiX adjusts the data mixture ratios according to the current state of the model. However, unlike these methods, TiKMiX does not require multiple iterations, which significantly improves training efficiency. Furthermore, TiKMiX simplifies the data mixing process and reduces engineering complexity without sacrificing model performance. To systematically evaluate the effectiveness of different data mixing strategies, we conduct large-scale experiments on the RefinedWeb dataset. Our models range in size from 1B to 7B parameters and are trained on up to 1 trillion tokens. The training process is divided into two distinct stages, each consisting of 500 billion tokens. At the transition point between these two stages, we strategically adjust the data mixture ratios to further assess the impact of mixing strategies on model performance. Downstream Task Evaluation To conduct comprehensive and rigorous evaluation of our proposed method, we curated diverse suite of nine widelyrecognized downstream benchmarks. This evaluation matrix is strategically divided into two categories: in-domain and out-of-domain. This bifurcation allows for dual-faceted assessment of our models capabilities: on one hand, to measure its proficiency on tasks closely aligned with its training objectives, and on the other, to critically examine its ability to generalize learned skills to novel tasks and knowledge domains. The consistent performance gains observed across both categories underscore our methods ability to enhance the models foundational capabilities and foster robust generalization. In-Domain Evaluation Our in-domain evaluation suite is designed to probe the models core competencies in complex reasoning, commonsense understanding, and knowledgeintensive applications. These benchmarks are thematically aligned with our methods primary optimization goals and serve to quantify the depth of improvement in these critical areas. MMLU (Massive Multitask Language Understanding) (Hendrycks et al. 2020): highly challenging multitask benchmark that assesses knowledge across 57 disparate subjects, ranging from elementary mathematics and U.S. history to computer science and law. MMLU demands not only vast repository of knowledge but also the ability to perform precise, domain-specific reasoning, making it key indicator of models comprehensive intellectual and academic capabilities. HellaSwag (Zellers et al. 2019): commonsense reasoning benchmark that tasks the model with selecting the most plausible continuation for given context. HellaSwag is distinguished by its use of adversariallygenerated distractors, which are designed to be highly confusable for models that rely on superficial statistical cues. It therefore serves as robust test of models deeper understanding of causality and everyday situations. ARC (AI2 Reasoning Challenge) (Clark et al. 2018): This benchmark evaluates reasoning and comprehension on grade-school science questions. We assess performance on both its subsets: ARC-Easy (ARC-E), which contains questions often solvable via information retrieval, and the more difficult ARC-Challenge (ARC-C), which requires multi-step reasoning and synthesis of knowledge. Evaluating on both allows for fine-grained analysis of the models capabilities, from basic knowledge retrieval to complex scientific inference. TriviaQA (Joshi et al. 2017): large-scale reading comprehension benchmark where questions are authored by trivia enthusiasts, leading to high degree of diversity and complexity. The task requires models to locate answers within lengthy, evidence-rich documents, often amidst significant distractor information. It primarily evaluates the models proficiency in long-context processing, precise information retrieval, and fact verification. Out-of-Domain Evaluation To rigorously assess the generalization power of our method, we selected set of outof-domain benchmarks that are distinct from the in-domain tasks in terms of subject matter, format, or required reasoning skills. Performance on these benchmarks directly reflects the models ability to transfer its learned meta-skills to new and unseen challenges. PiQA (Physical Interaction QA) (Bisk et al. 2020): commonsense benchmark focused on physical reasoning. Table 3: Ablation study of REGMIX and TiKMiX on 1B and 7B models. Benchmark REGMIX TiKMiX-D REGMIX TiKMiX-D 1B Model 7B Model In-Domain Benchmarks MMLU (Hendrycks et al. 2020) HellaSwag (Zellers et al. 2019) ARC Easy (Clark et al. 2018) ARC Challenge (Clark et al. 2018) TriviaQA (Joshi et al. 2017) Out-of-Domain Benchmarks PiQA (Bisk et al. 2020) OpenBookQA (Mihaylov et al. 2018) MathQA (Amini et al. 2019) Average Perf. 31.5 56.0 66.2 32.2 15.8 73.3 37.0 23.2 43.9 32.2 57.4 69.3 37.0 17.7 74.1 37.4 23. 45.5 40.7 76.6 78.5 49.4 46.4 79.1 43.2 28.8 55.3 41.5 76.4 78.4 50.2 45.3 79.2 45.4 29. 56.0 Presented in question-answering format, it requires the model to understand the properties and affordances of everyday objects (e.g., How can you cool cup of water faster?). PiQA probes the models intuitive grasp of how the physical world operates, domain of commonsense distinct from academic knowledge, making it an excellent test of generalization. OpenBookQA (Mihaylov et al. 2018): This benchmark simulates an open-book exam, requiring the model to answer questions using given set of elementary science facts. Success demands not only reading comprehension but, more importantly, the ability to reason over and combine these facts to answer questions whose solutions are not explicitly stated. It critically evaluates the models capacity for multi-step reasoning and knowledge application within constrained context. BoolQ (Boolean Questions) (Clark et al. 2019): dataset of naturally occurring yes/no questions, sourced from real user search queries. The challenge lies in the fact that the relationship between the question and the provided evidence passage is often implicit, requiring sophisticated syntactic and semantic analysis to arrive at correct Boolean judgment. BoolQ effectively measures the models fine-grained comprehension of natural, conversational language. MathQA (Amini et al. 2019): mathematical reasoning benchmark featuring multi-step word problems. The task requires models to parse natural language descriptions, formulate correct sequence of operations, and execute them to find solution. Covering diverse range of mathematical reasoning categories, MathQA is crucial benchmark for evaluating models symbolic reasoning and logical chain-of-thought capabilities, representing significant test of higher-order cognitive skills. By systematically evaluating our method across this dualcategory, nine-benchmark matrix, we demonstrate that our approach not only enhances performance in core competency areas (as shown by MMLU and ARC-C) but also significantly improves the transfer of these abilities to novel contexts (as evidenced by PiQA and MathQA). This comprehensive improvement across both in-domain and out-of-domain tasks provides strong evidence for the effectiveness and generalizability of our method. To further investigate the impact of model scale on data utilization, we present supplementary analysis in Figures 5 to 11. Our key finding is that models of different scales (1B and 7B) exhibit significantly different learning responses and form distinct preferences, even when trained on the exact same data. This phenomenon reveals complex interplay between data utility and model scale. It provides solid theoretical foundation for understanding and optimizing the data mixture for models of varying sizes. Experiments on models of different sizes Considering computational overhead, for the 7-byte model, we adopted an experimental design similar to REGMIX(Liu et al. 2024), training with 500B tokens in the first stage and 200B tokens in the second stage. Table 3 presents the experimental results of our method on models of different scales. It can be observed that our proposed method significantly outperforms the current state-of-the-art approach, REGMIX, on both in-domain and out-of-domain benchmarks. The performance on the 7B model effectively demonstrates the scalability of our approach. Furthermore, we note that unlike the 1B model, the 7B models performance on the benchmarks consistently improves throughout the training process. This suggests that the advantage of TiKMiX could be even more pronounced with additional training data. Observation of data mixing with Group Influence To conduct rigorous analysis of inter-domain interactions during mixed training, we designed an experiment to test the principle of influence additivity. Our hypothesis was that the influence of mixed dataset on validation set could be accurately predicted by weighted sum of the influences from its individual constituent domains. To verify this, we first established baseline mixing recipe using our TiKMiXD method. We then systematically explored the local space around this recipe by generating 256 perturbed configurations, created by applying random scaling factor between 0.5 and 2.0 to each domains original proportion. After filtering out two sampling outliers, we proceeded with 254 unique data mixture configurations. For each of these 254 points, we sampled corresponding 0.1B token dataset and measured its direct influence. We then compared this empirical influence value against predicted influence, which was calculated by summing the pre-computed influences of each individual domain, weighted by their respective proportions in the mixture. As depicted in Fig 13 , this comparison revealed strong linear correlation. Specifically, the Pearson correlation coefficients on the ARC(Clark et al. 2018), Hellaswag(Zellers et al. 2019), and TriviaQA(Joshi et al. 2017) benchmarks reached 0.845, 0.848, and 0.931, respectively, all of which are statistically highly significant (p < 0.0001). This result provides compelling evidence that the outcome of data mixing is highly predictable and can be modeled as linear combination of inter-domain influences. Consequently, this finding offers solid empirical justification for the theoretical soundness of our proposed two-stage optimization framework, encompassing both TiKMiX-D and TiKMiX-M. Figure 5: The impact of domains on 1B models performance on the ARC benchmark as training progresses. Figure 6: The impact of domains on 1B models performance on the HELLASWAG benchmark as training progresses. Figure 7: The impact of domains on 1B models performance on the MMLU benchmark as training progresses. Figure 8: The impact of domains on 1B models performance on the TRIVIAQA benchmark as training progresses. Figure 9: The impact of domains on 7B models performance on the ARC benchmark as training progresses. Figure 10: The impact of domains on 7B models performance on the HELLASWAG benchmark as training progresses. Figure 11: The impact of domains on 7B models performance on the MMLU benchmark as training progresses. Figure 12: The impact of domains on 7B models performance on the TRIVIAQA benchmark as training progresses. Figure 13: Group Influence-based Analysis of Data Mixing Effects on Various Benchmarks."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}