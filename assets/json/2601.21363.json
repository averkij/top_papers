{
    "paper_title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
    "authors": [
        "Weidong Huang",
        "Zhehan Li",
        "Hangxin Liu",
        "Biao Hou",
        "Yao Su",
        "Jingwen Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 3 6 3 1 2 . 1 0 6 2 : r Published as conference paper at ICLR 2026 TOWARDS BRIDGING THE GAP BETWEEN LARGESCALE PRETRAINING AND EFFICIENT FINETUNING FOR HUMANOID CONTROL Weidong Huang1, Zhehan Li1,2, Hangxin Liu1, Biao Hou2, Yao Su1, Jingwen Zhang1 1State Key Laboratory of General Artificial Intelligence, BIGAI 2School of Artificial Intelligence, Xidian University"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and high UpdateTo-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes deterministic policy while stochastic exploration is instead confined to physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning. Code and videos: https://lift-humanoid.github.io Figure 1: Large-scale pretraIning and efficient FineTuning (LIFT) Framework. In stage (i), we implement SAC in JAX to support large-batch update and high UTD, achieving fast, robust convergence in massively parallel simulation and zero-shot deployment to real humanoid in outdoor experiments. In stage (ii), we pretrain physics-informed world model on the SAC data, combining Lagrangian dynamics with residual predictor to capture contact forces and other unmodeled effects. In stage (iii), we finetune both the policy and the world model to new environments while executing only deterministic actions in the environment. Stochastic exploration is confined to rollouts within the world model. This framework enhances both the safety and efficiency of finetuning. Corresponding Author. 1 Published as conference paper at ICLR"
        },
        {
            "title": "INTRODUCTION",
            "content": "Learning generalist physical agents is long-standing goal in AI, and humanoid robots offer broad task coverage among robotic embodiments. PPO (Schulman et al., 2017) becomes mainstream baseline for humanoid robot control due to its robustness and fast wall-clock convergence in massively parallel GPU simulation (Schwarke et al., 2025). However, when only limited real-world or cross-task data can be collected, on-policy methods are disadvantaged because they discard offpolicy experience (Haarnoja et al., 2018b; Lillicrap et al., 2015; Fujimoto et al., 2018). Moreover, even when PPO policy achieves zero-shot deployment in new environments, performance metrics can degenerate (Gu et al., 2024) (e.g., poorer velocity tracking or reduced execution precision). These limitations highlight the need for frameworks that can reuse prior experience and adapt policies efficiently in new environments. Off-policy algorithms offer an appealing backbone for this paradigm because they can leverage replayed experience for sample-efficient finetuning (Lillicrap et al., 2015). However, these algorithms tend to overfit to replayed data, which can bias exploration during fine-tuning, especially when large-scale parallel simulation with them has received limited attention. During fine-tuning, directly adapting policy to new environment often leads to unsafe actions because of stochastic exploration. This risk is especially high for humanoid robots: small support polygons, particularly during single-support phases, make the system highly sensitive to perturbations. Constraining exploration within learned world model provides safer and more sample-efficient alternative (Levy et al., 2024; Hafner et al., 2023), though synthetic rollouts may introduce model bias that destabilizes finetuning (Ha & Schmidhuber, 2018; Janner et al., 2019). physics-informed world model (Levy et al., 2024) that incorporates structural priors from physcis improves the fidelity of synthetic rollouts, reduces model bias, and enables more reliable finetuning even with limited data. However, training such model-based systems from scratch (Levy et al., 2024) remains extremely slow in wall-clock time and prone to local minima. Meanwhile, we find that off-policy SAC (Haarnoja et al., 2018b) integrates more naturally with model-based algorithms (Levy et al., 2024) than PPO, achieving robust and sample-efficient improvements. These findings further motivate our pipeline. We introduce Large-scale pretraIning and efficient FineTuning (LIFT), threestage framework as shown in the Figure 1: (i) large-scale policy pretraining, (ii) physics-informed world model pretraining, and (iii) efficient finetuning of the policy and world model. We make the following contributions: 1. We provide scalable JAX implementation of SAC that supports robust convergence in massively parallel simulation and zero-shot deployment to physical humanoid robot within one hour of wall-clock training on single NVIDIA RTX 4090 GPU. The resulting SAC policy also serves as the policy module within our model-based finetuning stage. 2. We develop fine-tuning strategy that executes deterministic actions in new environments while limiting stochastic exploration in physics-informed world model, which improves sample efficiency and stabilizes convergence. This physics-informed design is pivotal for finetuning, enabling data-efficient in-distribution adaptation and stronger outof-distribution generalization. 3. We release an open-source pipeline for humanoid control spanning pretraining, zero-shot deployment, and finetuning, providing practical baseline for the robotics community."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Large-Scale RL Pretraining in Simulation. With the rapid progress of GPU technology, several GPU-based physics simulators have emerged, such as IsaacGym (Makoviychuk et al., 2021), Mujoco Playground (Zakka et al., 2025), and Brax (Freeman et al., 2021). These frameworks enable fast parallel simulation of thousands of robot environments directly on GPUs. PPO (Schulman et al., 2017) has become the dominant baseline in this setting due to the ease of implementation and robust convergence. By leveraging domain randomization (Tobin et al., 2017), PPO policies can achieve fast wall-clock convergence and even zero-shot transfer to physical humanoid robots (Rudin et al., 2022; Gu et al., 2024). However, the low sample efficiency of on-policy methods limits their ability to adapt or continue training in new environments. 2 Published as conference paper at ICLR 2026 Off-Policy RL. When data collection is expensive or risky, improving sample efficiency becomes critical. Off-policy algorithms such as TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018b) achieve higher sample efficiency by reusing past experiences and leveraging critic gradients to update the policy. SAC has been used to train quadruped robots from scratch with limited real-world data (Haarnoja et al., 2018c; Ha et al., 2020; Haarnoja et al., 2018a; Smith et al., 2022b), and pretrained SAC policies can be finetuned on hardware with the same algorithm (Smith et al., 2022a; Ball et al., 2023). However, these methods typically rely on stochastic exploration in the environment, where injected action noise may damage actuators or induce unsafe states. This risk is amplified for humanoids, whose smaller support polygons (especially in single-support phases) make them sensitive to perturbations compared to quadrupeds. These methods also underutilize large-scale parallel simulation, resulting in high wall-clock training time. Recent large-scale off-policy efforts include Parallel Q-Learning (Li et al., 2023), which scales DDPG (Lillicrap et al., 2015) across massive simulations yet without sim-to-real validation, and FastTD3 (Seo et al., 2025), which achieves humanoid sim-to-real but its finetuning ability in new environments remains unclear. Raffin (2025) show that SAC can be made stable in massively parallel simulators through extensive parameter tuning, but their work also lacks sim-to-real evaluation and does not examine fine-tuning performance. Model-Based Techniques Model-based methods can further improve sample efficiency, such as MBPO (Janner et al., 2019) using synthetic rollouts and Dreamer (Hafner et al., 2023) showing strong data efficiency in real-world tasks (Wu et al., 2023). However, they often still require stochastic exploration in the environment. Sun et al. (2025) train world model that explicitly reconstructs the environment state and use it as policy input to enhance locomotion robustness, but the model is not used to generate synthetic data for policy fine-tuning aimed at improving adaptability. ASAP (He et al., 2025) collects real-world data using pretrained policy and learns delta-action model to correct simulator actions before fine-tuning. However, the delta network can output unbounded action corrections while finetuning, which may lead to undesired behaviors, and the approach requires substantial real-world data to sufficiently cover the gaps between the simulator and the physical robot, especially across diverse motions. These limitations highlight an open question: how can physics priors be incorporated into world models to improve predictive accuracy while reducing the amount of data required for policy fine-tuning? Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) represent one possible approach by embedding physical constraints directly into the learning architecture. For example, Greydanus et al. (2019) proposed Hamiltonian neural networks that learn energy-conserving dynamics in an unsupervised manner, while Cranmer et al. (2020) introduced Lagrangian neural networks that model dynamics without requiring canonical coordinates. Deep Lagrangian Networks (Lutter et al., 2019) further demonstrated strong performance in robot tracking and extrapolation to novel trajectories. Yet these approaches remain limited to energy-conserving systems and struggle with contact-rich dynamics. To address this, SSRL (Levy et al., 2024) proposed combining rigid-body dynamics with learned contact-force models, enabling quadruped robot to learn to walk with just three minutes of real-world data. Nevertheless, training from scratch remains unsafe for humanoids due to their fragility and instability. Motivated by this, our work adopts pretrainfinetune pipeline that extends physics-informed modeling to large-scale humanoid pretraining and safe finetuning."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Problem Formulation. RL is typically modeled as Markov Decision Process (MDP) (Altman, 1999): = (S, A, P, R, µ, γ), where and denote the state and action spaces, respectively. P(ss, a) and R(rs, a) represent the transition probability and reward function. µ() denotes the initial state distribution, and γ is the discount factor. stochastic policy πθ parameterized by θ specifies the action probability πθ(as) for given state s. In continuous control, the stochastic policy is typically Gaussian, πθ(as) = N(cid:0)µθ(s), Σθ(s)(cid:1), where the covariance can be either stateindependent or state-dependent. During pretraining, domain randomization is employed to enhance robustness by perturbing the environment transition function across episodes. At the same time, the stochastic policy is used to promote diverse exploration. For the deployment and finetuning stage, however, we adopt the deterministic policy induced by the mean action µθ(s) to collect data and evaluate performance in environments. 3 Published as conference paper at ICLR 2026 Model-based RL Problem. For the finetuning stage, we seek policy πθ Πθ that maximizes the model-predicted return ϕ (πθ), defined as follows: ϕ (πθ) = R (cid:34) (cid:88) t=0 γtrt+1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) s0 µ, at πθ(st), (st+1, rt+1) Pϕ( st, at) . (1) (cid:35) Here Pϕ(st, at) denotes world model parameterized by ϕ, which approximates the true environment dynamics. The initial state s0 is sampled from the true initial distribution µ, while subsequent states are generated by the learned model. In practice, stochastic policy πθ is used to interact with Pϕ so as to generate diverse imaginary trajectories, thereby improving exploration coverage for policy optimization. When the predictions of the world model are sufficiently accurate, maximizing the model-predicted return corresponds to maximizing the return in the real environment (Janner et al., 2019; Hafner et al., 2023). Physics-Informed World Model for Robots Incorporating physics priors into the world model improves the accuracy of its predictions (Raissi et al., 2019). We adopt physics-informed formulation of robot dynamics based on the Lagrangian equations of motion (Featherstone, 2008): (qt) qt + C(qt, qt) + G(qt) = Bτt + t + τ , (2) is the external contact force with Jacobian J, and τ Here, qt and qt are the generalized coordinates and velocities at time t. The motor torque τt is applied to the joints via the matrix B. denotes dissipative (parasitic) torques. The matrices , C, and are the mass, Coriolis/centrifugal, and gravity terms. Because B, , C, and depend only on the robots shape and inertial parameters, they are known beforehand. Thus, the main model uncertainties are the contact term t and the dissipative term τ . To model these unknown contributions, Levy et al. (2024) introduce residual network that approximates them. The network predicts the sum of the contact term and the dissipative term, as well as the uncertainty of the next-state prediction: τϕ(st, at) τ σϕ(st, at) = log σ2 F + τ (3) (4) Here, τϕ and σϕ are outputs from different heads of the neural network, with σ2 representing the predicted variance of the next state. This hybrid physics-informed world model Pϕ combines the known rigid-body dynamics with learned residuals and can be rolled out to generate more precise trajectories for policy optimization."
        },
        {
            "title": "4 LARGE-SCALE PRETRAINING AND EFFICIENT FINE-TUNING",
            "content": "To enable humanoid robot to learn quickly and with high sample efficiency in new environments, we first select the algorithm best suited for fine-tuning and then design the framework around it. Using single algorithm across both stages aligns training objectives, enables continuous policy updates that mitigate forgetting, and provides unified, easy-to-implement framework. While SSRL (Levy et al., 2024) successfully trained quadruped robot from scratch using SAC and physics-informed world model, its reliance on deterministic data collection leads to impractically slow run-times when scaled to humanoid robots. Combining this approach with PPO is natural alternative, as PPO offers extensive parallel training infrastructure (Schwarke et al., 2025). However, our preliminary study (see Appendix A.1) highlights two advantages of SAC over PPO: its off-policy nature ensures stable convergence with limited samples, and its state-dependent stochastic actor promotes exploration in the world model, enhancing the diversity and utility of synthetic rollouts. Nevertheless, in our setting, none of these methods complete the target task within 48 hours of runtime without large-scale parallel pretraining, motivating the development of full pretrainingfinetuning pipeline using SAC as the backbone algorithm. 4.1 LARGE-SCALE POLICY PRETRAINING Policy Optimization. We adopt Soft ActorCritic (SAC) (Haarnoja et al., 2018b) with an asymmetric actorcritic setup (Pinto et al., 2017): the actor πθ(atst) receives state st, while the critics Qψi(sp that include additional information. our method uses , at) are trained on privileged states sp 4 Published as conference paper at ICLR only the robots proprioceptive state for both the actor and the critics. We normalize each feature of st and sp using running mean and variance (Lee et al., 2024), ensuring balanced feature scales during training. We pretrain the SAC policy in MuJoCo Playground (Zakka et al., 2025) with thousands of vectorized environments on single GPU. Our SAC implementation is written in JAX (Bradbury et al., 2018) with fixed tensor shapes, which allows efficient operation fusion and reuse of compiled kernels. As result, large-batch updates (high UTD) incur no additional data transfer overhead. The same design seamlessly accelerates interaction with the world model during finetuning and supports multi-GPU scaling, improving wall-clock training time. Unlike FastTD3 (Seo et al., 2025) and PQL (Li et al., 2023), we do not use per-environment mixed Gaussian noise. Instead, we rely entirely on SACs stochastic policy for exploration, i.e., πθ(a s) = (µθ(s), Σθ(s)), with state-dependent variance produced by the actor and temperature α controlling entropy. We employ the Optuna framework (Akiba et al., 2019) for systematic hyperparameter tuning. On the Booster T1 locomotion task, we conducted approximately ten hours of hyperparameter search on single NVIDIA RTX 4090 GPU. The results show that, after tuning, the convergence time is reduced from about seven hours to only half an hour. The training objective and hyperparameter tuning process are detailed in Appendix B.1. Large-Batch Updates and High UTD. Raising the update-to-data (UTD) ratio reuses experiIn our T1LowDimJoystick ence more aggressively but can amplify value-estimation bias. setupSAC trained in MuJoCo Playground with 1,024 parallel environments, large-batch updates (batch size 1,024), and 106-transition replay bufferwe observe that increasing UTD from 1 to 10 improves sample efficiency without any auxiliary stabilizers or architectural modifications. Beyond this range, gains diminish while wall-clock time increases predictably. We do not claim generality: these improvements appear tied to our massively parallel, domain-randomized regime, which likely suppresses early overfitting and curbs bias accumulation at moderate UTDs. By contrast, prior works typically couple high UTD with stabilizers such as periodic resets (DOro et al., 2023; Nikishin et al., 2022) or architectural regularization (ensemble/dropout critics, batch-normalized critics) (Chen et al., 2021; Hiraoka et al., 2022; Bhatt et al., 2024). We view these techniques as complementary and worth re-evaluating in large-scale parallel training for SAC to potentially push the usable UTD higher. 4.2 WORLD MODEL PRETRAINING Offline Pretraining from Logged Transitions Unlike MBPO or Dreamer, which update the policy and world model together online, we decouple them to improve wall-clock efficiency under massive parallelism. With thousands of parallel simulators, each step generates on the order of O(103) transitions, and training the world model online would slow the training. During SAC pretraining, we log all transitions to disk, saving records of the on-device replay buffer. After the policy converges, we train the world model entirely offline from these records. From each record, we construct = (sp , nt, ht], which includes joint angles and velocities, base linear and angular velocity in the body frame, base orientation, and body height. t+t). In our setting, the privileged state is sp , at, sp = [qt, , ωb qt, vb , at World Model and Loss. We learn residual predictor that injects external torques into differentiable physics step. Residual predictor takes the concatenation of the privileged observation and (cid:3) as input and outputs external uncertainties of torques and per-feature predictive unaction (cid:2)sp certainty of the next-observation prediction:(τ , log σ2 ], we predict the next privileged state using fully differentiable pipeline. Concretely, we implement the world model based on Brax (Freeman et al., 2021) and perform the following steps: (a) map privileged state to Brax generalized coordinates and velocities, (b) convert actions to motor torques via PD controller, (c) integrate the Lagrangian dynamics with semi-implicit Euler, and (d) reconstruct the next privileged state for learning and rollout. Using Braxs differentiable rigid-body primitives (q), C(q, q), and G(q) we avoid reimplementing the dynamics and keep the rollout loss pipeline end-to-end differentiable. See Appendix B.2 for details. ), as shown in Eq. (3). Given [sp , at, τ Compared to SSRL(Levy et al., 2024), we (i) correct the mapping from privileged state to Braxs generalized state (q, q), resolving discrepancy we identified in the publicly available SSRL implementation; and (ii) align each observation/state dimension between MuJoCo Playground and Brax (frames, quaternion form, normalization) to avoid training error;(iii) we include the base height ht 5 Published as conference paper at ICLR 2026 in the privileged state. For humanoid tasks, omitting ht consistently caused unstable world model rollouts and non-convergent fine-tuning, whereas SSRLs quadruped configuration omitted height without issue. These adjustments improve training stability in humanoids setting. After obtaining the next privileged state (cid:98)sp t+t from the differentiable pipeline, we minimize the negative log-likelihood of Gaussian predictive distribution. Let denote the batch size. For prediction (cid:98)sp t+t, target sp , the loss is t+t, and elementwise log-variance log σ2 (cid:88) (cid:0) (cid:0) b,t+t sp (cid:98)sp b,t+t Lϕ ="
        },
        {
            "title": "1\nB",
            "content": "b=1 (cid:1)2 exp(cid:0) log σ2 (cid:1) + log σ2 b,t (cid:1), (5) b,t where is elementwise multiplication. Gradients backpropagate through normalization, frame transforms, the PD controller, and the Euler Step, so the residual predictor is trained end-to-end from next-state errors. Minimizing this loss encourages the world model to predict small variance for seen states and large variance for unseen states. In practice, we exploit this behavior during rollouts by sampling next states from the predicted Gaussian (cid:98)sp (cid:98)sp t+t, diag(σ2 t+t (cid:0) )(cid:1)."
        },
        {
            "title": "4.3 POLICY AND WORLD MODEL FINETUNING",
            "content": "Deployment & Data Collection. We deploy the pretrained policy in the target environment (Brax (Freeman et al., 2021) in our case) and collect trajectories using deterministic actions (i.e., the action mean, without exploration noise). If the robot enters an unsafe state, the episode is reset. Each episode has maximum length of Tep = 1000. All transitions are stored in replay buffer, which is subsequently used to fine-tune both the world model and the policy. After collecting Tep steps of data, we fine-tune the world model and policy before beginning the next iteration of data collection and training. More sophisticated pipelines, such as asynchronous data collection and learning (Luo et al., 2024), are left for future work to avoid additional complexity in this study. World-Model Fine-Tuning. We fine-tune the pretrained world model on the collected buffer. For each epoch we independently shuffle data, roll physics inside the loss, and evaluate world model on held-out testing data set for early stopping. We observe that training world model for multiple epochs on the same replay buffer, combined with auto-regressive training, enhances sample efficiency. The auto-regressive training objective is provided in the Appendix B.3. After World model trainings done, we use the policy to explore in the world model to generate data that used to train the actor critic. Once world-model training is complete, we use the stochastic SAC actor to explore within the world model and generate trajectories. t+1 = Pϕ(sp (a) Asymmetric Observation Generation. BeExploration in Physics-informed World-Model. cause data are collected with deterministic policy (action mean only), the logged trajectories have limited diversity. To drive policy improvement during finetuning, we therefore explore inside the world model using stochastic SAC policy. We keep the same asymmetric structure for actor-critic during finetuning as in pretraining: the actor consumes the proprioceptive state, while the critic uses the privileged state. Let sp Rdp denote the privileged state and st Rds the proprioceptive state, from which the action at πθ(st) is sampled. The learned physics-informed world model Pϕ predicts the next privileged state, (cid:98)sp , at) and the proprioceptive state is obtained by fixed, simulator-consistent projection Π : Rdp Rds (same deterministic transforms used (cid:1). (b) Safety Reset. For safety and numerical robustness, by the environment): (cid:98)st+1 = Π(cid:0) (cid:98)sp we apply physics-informed terminal checks and terminate world model rollout immediately upon detecting an abnormal statespecifically, when base height, world-frame linear/angular velocities, body roll/pitch, or joint positions/velocities violate prescribed bounds. Early termination stabilizes training and prevents NaNs over long horizons. We leave richer safeguardse.g., self-collision checks derived from link posesas future optimization. (c) Physics-Consistent Reward. Since (cid:98)sp provides all necessary quantities (joint states, link poses, base kinematics), rewards can be computed deterministically and remain fully consistent with the underlying physics in the world model rollout. An example is provided in the Appendix B.4. In contrast, using learned scalar predictor ˆrψ(op , at) (Hafner et al., 2023) is unstable in deterministic data collection settings. Even small state prediction errors accumulate over long horizons, degrading policy convergence. We sample batch of privileged states from the replay buffer as initial conditions and roll out trajectories of horizon Hwm = 20 using the world model and the stochastic policy. These synthetic trajectories are added t+1 Published as conference paper at ICLR 2026 to the replay buffer to train the actor-critic via SAC. The updated policy is then deployed in the environment to collect new data, thus completing one cycle of an iterative process that continues until convergence."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Our approach performs comparably to baselines in pretraining and, importantly, enables efficient fine-tuning for humanoids in new environments with limited data. We validate on two platformsBooster T1 and Unitree G1and report LIFT results averaged over 8 independent runs. All experiments are executed on single NVIDIA 4090 GPU with 32 CPU cores in cloud setting. Evaluation uses the average undiscounted return over episodes, ˆJ(π) = 1 t=0 rt, computed without network updates. In practice, we set = 1024 and Tep = 1000. To characterize efficiency end-to-end, we compare (i) wall-clock time during pretraining (training speed) and (ii) environment steps required during fine-tuning (sample efficiency). These experiments are designed to answer three key questions: (Q1) Is the LIFT pretraining module comparable to established baselines? (Q2) Does this module better support subsequent fine-tuning than baselines? (Q3) Which components of our pipeline affect fine-tuning efficiency and stability? (cid:80)Tep (cid:80)E i=1 Baseline algorithms. The baselines include: (1). FastTD3 (Seo et al., 2025) (Model-free): An efficient variant of TD3 with parallel environments and mixed exploration noise. (2). PPO (Schulman et al., 2017) (Model-free): widely used on-policy algorithm with clipped surrogate objective. We use baseline PPO in Mujoco playground. (3). SAC (Haarnoja et al., 2018b) (Model-free): An off-policy algorithm with stochastic actor and entropy regularization to encourage exploration. (4). SSRL (Levy et al., 2024) (Model-based): physics-informed world model approach for training policies from scratch. (5). MBPO (Janner et al., 2019) (Model-based): Utilizes an ensemble of neural networks as the world model to improve sample efficiency of RL. 5.1 PRETRAINING EXPERIMENTS To answer Q1, policy pretraining is performed in the MuJoCo Playground (Zakka et al., 2025) across six humanoid tasks, combining two terrain types (flat and rough) with three robot configurations: low-dimensional Booster T1 (12-DoF, legs only), full Booster T1 (23-DoF, including head, waist, arms, and legs), and the Unitree G1 (29-DoF, including waist, arms, and legs). Full details are provided in the Appendix C.2. Pretraining results. As shown in the Appendix A.2, LIFT achieves comparable or higher evaluation returns than PPO and FastTD3 while stabilizes at its peak return faster on rough terrain environments. On flat terrain, it achieves comparable peak performance with similar wall-clock runtime. These curves indicate that the LIFT pretraining module is competitive in reward returns and convergence speed. Additionally, we also apply LIFT pretraining to whole-body tracking tasks for Unitree G1, providing preliminary evidence that the same framework extends naturally to non-locomotion skills. More details and discussion of these results are provided in Appendix A.8. Sim-to-real with LIFT Pretrained Policy. To demonstrate the potential for zero-shot deployment, we deploy the pretrained policy for the low-dimensional Booster T1 directly on the physical robot. Zero-shot transfer to previously unseen surfaces (e.g., grass, uphill, downhill, mud, etc.) is shown in the Appendix A.6. This provides practical evidence that large-scale, parallel SAC pretraining can yield deployable humanoid controllers, thereby addressing Q1. And it also establishes suitable starting point for subsequent fine-tuning. 5.2 FINETUNING EXPERIMENTS Sim-to-sim Finetuning. We evaluate finetuning in Brax (Freeman et al., 2021) after large-scale policy pretraining in the MuJoCo Playground. The world model is also pretrained with data collected with pretrained policies. Because the two simulators differ in contact models and constraints, this sim-to-sim transfer provides controlled but nontrivial test of adaptation. We transfer the pretrained policy to Brax and keep the reward design same as Booster Gym Wang et al. (2025) for both pretraining and finetuning. During pretraining, target linear velocities along the x-axis were uniformly sampled in [1, 1] m/s. For fine-tuning in Brax, we specify new forward-velocity targets and the Published as conference paper at ICLR 2026 policy explores with deterministic action execution (action mean only; no stochastic sampling in the environment). We measure (i) whether policies converge within fixed sample budget and (ii) velocity-tracking accuracy relative to the specified target. Implementation details are provided in the Appendix C.3. Figure 2: Results of finetuning Booster T1 robot with varying target speeds. The black dashed line represents the target velocity for each task. Results are averaged over 8 random seeds. To address Q2, we design three scenarios and compare against PPOand FastTD3-pretrained policies: (i)In-Distribution: targets within the pretraining range; (ii) Long-Tail: rare targets poorly represented during pretraining; (iii) Out-of-Distribution: targets outside [1, 1] m/s. As shown in the Figure 2, LIFT consistently converges across all tasks, achieving stable walking that closely tracks the desired forward speed during the Booster T1 finetuning experiments. After finetuning, the policy demonstrates significantly reduced body oscillations and less deviation from the desired speed direction, with noticeable improvements in velocity. In contrast, SAC, trained without explicit exploration noise, quickly diverges and fails to recover, indicating rapid overfitting to the deterministically collected data. Although PPOs clipping mechanism stabilizes updates by keeping the new policy close to the previous one, its performance in our setting initially remains reasonable but then gradually degrades and ultimately collapses. This suggests that, under deterministic execution and limited data collection, PPO struggles to sustain stable policy improvement. FastTD3 exhibits strong oscillations and ultimately collapses without converging. SSRL shows signs of convergence at 0.6 m/s but fails to reach the target speed, and it does not converge at all on higher-speed tracking tasks (1.01.5 m/s). This highlights the difficulty of training humanoid locomotion from scratch and further motivates our pretrainfinetune design. These results demonstrate that LIFT enables robust adaptation under both in-distribution and out-of-distribution finetuning tasks, while standard baselines struggle with stability. During finetuning, policy control and data collection run at 50 Hz (one environment step = 0.02 s), so 4 104 steps correspond to approximately 800 of on-robot interaction, highlighting the potential feasibility of applying LIFT on real humanoid robots which remains as future works. Additionally, we finetuned the Unitree G1 in the Brax environment, which also improved policy behavior and reward performance. The detailed results can be found in the Appendix A.7. Real-world Finetuning. We evaluate LIFT on the Booster T1 humanoid by first pretraining policy in the MuJoCo-Playground T1LowDimJoystickFlatTerrain task with most energy-related regularizations removed, keeping only an action-rate L2 penalty. This policy transfers well from MuJoCo to Brax but fails in zero-shot sim-to-real, providing challenging starting point for real-world finetuning. Using this policy as initialization, our finetuning framework progressively improves initial unstable behavior: after collecting 80590 of data, the robot shows more upright posture, smoother gait patterns, and more stable forward velocity  (Fig. 3)  , demonstrating that LIFT can substantially strengthen weak sim-to-real policy with only several minutes of real data. However, our current real-world implementation has two main practical limitations: (1) LIFT requires base-height es8 Published as conference paper at ICLR 2026 timation for world-model training and rewards, but Booster T1 does not provide this onboard, so we rely on Vicon motion-capture system, which restricts the tracking area and requires human supervision. (2) We estimate base linear velocity by integrating IMU acceleration, which introduces drift and may limit policy tracking performance. (3) Each finetuning iteration is executed sequentiallyup to 8 of data collection at 50 Hz, world model update, then synthetic rollouts for policy updatesleading to multi-hour wall-clock time and frequent battery swaps despite using only minutes of real data. We view these as engineering rather than conceptual constraints, and expect that adopting an asynchronous pipeline similar to SERL (Luo et al., 2024), together with camera-based height and velocity estimation onboard, would make repeated real-world finetuning substantially more practical. More results are provided in Appendix A.3. Figure 3: Real-world finetuning progression on the Booster T1 humanoid. video demonstration is available on our project website. 5.3 ABLATION STUDY To answer Q3 and identify which components of LIFT affect fine-tuning efficiency and stability, ablations over three aspects are conducted: (i) pretraining stageslarge-scale SAC pretraining and world-model (WM) pretraining; (ii) world-model choiceour physics-informed model versus an MBPO-style ensemble; and (iii) key hyperparametersUTD ratio, replay-buffer size, batch size (pretraining), entropy coefficient α and autoregressive loss horizon (fine-tuning). Effect of Pretraining. As shown in the Figure 4, we ablate WM and SAC pretraining. With both pretraining stages, LIFT converges within 4 104 environment steps and successfully tracks the target speed. Removing WM pretraining still allows the method to converge eventually, but noticeably slows down training, indicating that WM pretraining improves sample efficiency. Removing both pretraining stages reduces the LIFT to SSRL Levy et al. (2024), which mostly learns to stand in place with near-zero forward velocity. Thus, largescale SAC pretraining is essential to avoid poor local minima, and WM pretraining further improves finetuning efficiency and stability. Ablation of the pretraining on Figure 4: Booster T1 (target forward speed = 1.5 m/s). Results are averaged over 8 random seeds. Physics-informed vs. Non-physics-informed World Models We pretrain MBPOs ensemble world model (ensemble size = 5, elite size = 3) on the same dataset and finetune with identical hyperparameters to LIFT; the only difference is the choice of world model. MBPO fails to converge: episode return remain near zero (training curves as shown in Figure 5). On the test set its mean squared error (MSE) of world model is substantially worse than LIFTs. During model-based rollouts, stochastic policy frequently produces actions that lie outside the distribution that has been seen in world-model training. These out-of-distribution actions induce physically implausible predictions (e.g., body height) for MBPO, which cause the critic loss to explode and inhibit policy improvement. This behavior likely stems from the purely neural networks limited ability to 9 Published as conference paper at ICLR generalize. By contrast, LIFTs physics-informed world model supplies strong inductive priors that improve generalization under limited data and produce stable, learnable rollouts, enabling successful finetuning. Figure 5: Ablation of Physics informed World Model on Booster T1 (target speed = 1.5 m/s). Results are averaged over 8 random seeds. Effect of Hyperparameters. During pretraining, we find that the UTD ratio, buffer size, and batch size strongly influence convergence speed and performance. UTD = 1 converges very slowly, while increasing it to 5 speeds up learning. Larger buffer and batch sizes further accelerate convergence, though excessively large buffers increase GPU memory usage. Beyond certain thresholds, larger batch sizes or higher UTD provide little additional benefit. In finetuning, the entropy coefficient α and autoregressive horizon length are critical. Excessively large α promotes exploration into high-uncertainty regions of the world model, destabilizing learning. Using α from pretraining works reasonably, but smaller values achieve more stable convergence. loss horizon length of 1 sometimes fails to reach the target velocity, while lengths of 2 or 4 consistently ensure stable learning, indicating that multi-step autoregressive prediction improves world-model training and policy stability. The training curves are provided in the Appendix A.4."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSION",
            "content": "In this work, we propose LIFT, pretrainingfinetuning framework for humanoid control that bridges large-scale simulation and data-efficient adaptation. By leveraging massively parallel environments with SAC, LIFT enables fast, robust pretraining and zero-shot sim-to-real transfer. The pretrained policy then guides model-based fine-tuning, while the same pretraining data bootstrap physics-informed world model to improve sample efficiency. During fine-tuning, deterministic action execution is combined with stochastic exploration inside the world model, providing stable learning under limited data. Experimental results highlight practical path toward continuous, efficient humanoid learning. Despite these results, there still exist limitations in our current work that inspire several future directions: (1) Safety management during real-world finetuning: collecting deterministic trajectories in the real world still carries inherent risks due to model errors in the actor network. We enforce strict termination conditions, aligned with those in simulation, to halt episodes when key physical quantities exceed predefined thresholds. Human operators also remotely terminate any episode exhibiting unsafe behaviors (e.g., drifting toward obstacles). Upon termination, the policy is halted and the Booster T1 is switched to damping mode to restrict further motion. The robot is then guided back to the starting area using its default walking controller, followed by standing controller to reset it to consistent initial configuration. This procedure maintains stable initial states across episodes and mitigates distribution mismatch during data collection. Future work may incorporate more automated safety mechanismssuch as robot-assisted resetting (Hu et al., 2025), uncertainty-aware exploration (An et al., 2021; Yu et al., 2020), or recovery policies with safety switches (Thananjeyan et al., 2021; Smith et al., 2022a)to further improve safety and reduce human intervention during real-world humanoid finetuning; (2) high-dimensional external sensor and visual inputs: LIFT currently operates exclusively on proprioceptive observations and does not incorporate camera or other high-dimensional sensory inputs, in contrast to vision-based frameworks such as DreamerV3 (Hafner et al., 2023). Scaling LIFT to tasks with vision-centric objectivessuch as dexterous manipulation or object-centric controlwill likely require latent world models capable of capturing dynamics beyond the robots body state, which we view as promising direction for future extensions. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This work was supported in part by the National Natural Science Foundation of China (No. 62403064, 62403063). We thank the engineering team from Booster Robotics for technical support. Reproducibility Statement. We train all LIFT agents on single NVIDIA RTX 4090 GPU. Our experiments use the MuJoCo Playground1 and Brax2 simulators. The source code and full results are available on our project website: https://lift-humanoid.github.io/. Our baseline implementations are adapted from the following open-source repositories: PPO3, FastTD34, MBPO, and SSRL5. Detailed hyperparameters, additional experiments, and environment configurations are provided in Appendix A, and C."
        },
        {
            "title": "REFERENCES",
            "content": "Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: In Proceedings of the 25th ACM next-generation hyperparameter optimization framework. SIGKDD international conference on knowledge discovery & data mining, pp. 26232631, 2019. Eitan Altman. Constrained Markov decision processes: stochastic modeling. Routledge, 1999. Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:74367447, 2021. Philip Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In International Conference on Machine Learning, pp. 15771594. PMLR, 2023. Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, and Jan Peters. Crossq: Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity. In International Conference on Learning Representations (ICLR), 2024. URL https://openreview.net/forum?id=PczQtTsTIX. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao JAX: composable transformations of Python+NumPy programs, 2018. URL http: Zhang. //github.com/jax-ml/jax. Xinyue Chen, Che Wang, Zijian Zhou, and Keith W. Ross. Randomized ensembled double qlearning: Learning fast without model. In International Conference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=AY8zfZm0tDd. Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel, and Shirley Ho. Lagrangian neural networks. arXiv preprint arXiv:2003.04630, 2020. Pierluca DOro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G. Bellemare, and Aaron Courville. Sample-efficient reinforcement learning by breaking the replay ratio barIn International Conference on Learning Representations (ICLR), 2023. URL https: rier. //openreview.net/forum?id=OpC-9aBBVJe. Roy Featherstone. Rigid body dynamics algorithms. Springer, 2008. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem. arXiv preprint Braxa differentiable physics engine for large scale rigid body simulation. arXiv:2106.13281, 2021. 1https://github.com/google-deepmind/mujoco_playground 2https://github.com/google/brax 3https://github.com/google/brax 4https://github.com/younggyoseo/FastTD3 5https://github.com/CLeARoboticsLab/ssrl 11 Published as conference paper at ICLR 2026 Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. CoRR, abs/1802.09477, 2018. URL http://arxiv.org/abs/1802. 09477. Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. Advances in neural information processing systems, 32, 2019. Xinyang Gu, Yen-Jen Wang, and Jianyu Chen. Humanoid-gym: Reinforcement learning for humanoid robot with zero-shot sim2real transfer. arXiv preprint arXiv:2404.05695, 2024. David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Sehoon Ha, Peng Xu, Zhenyu Tan, Sergey Levine, and Jie Tan. Learning to walk in the real world with minimal human effort. arXiv preprint arXiv:2002.08550, 2020. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. arXiv preprint arXiv:1812.11103, 2018a. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018b. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018c. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Nicklas Hansen, Jyothir SV, Vlad Sobal, Yann LeCun, Xiaolong Wang, and Hao Su. Hierarchical world models as visual whole-body humanoid controllers. arXiv preprint arXiv:2405.18418, 2024. Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025. Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. In International Conference on Learning Representations (ICLR), 2022. URL https://openreview.net/forum?id= xCVJMsPv3RT. Kaizhe Hu, Haochen Shi, Yao He, Weizhuo Wang, Karen Liu, and Shuran Song. Robot trains arXiv preprint robot: Automatic real-world policy adaptation and learning for humanoids. arXiv:2508.12252, 2025. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. Advances in neural information processing systems, 32, 2019. Hojoon Lee, Dongyoon Hwang, Donghu Kim, Hyunseung Kim, Jun Jet Tai, Kaushik Subramanian, Peter Wurman, Jaegul Choo, Peter Stone, and Takuma Seno. Simba: Simplicity bias for scaling up parameters in deep reinforcement learning. arXiv preprint arXiv:2410.09754, 2024. Jacob Levy, Tyler Westenbroek, and David Fridovich-Keil. Learning to walk from three minutes of real-world data with semi-structured dynamics models. arXiv preprint arXiv:2410.09163, 2024. Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, and Pulkit Agrawal. Parallel q-learning: Scaling off-policy reinforcement learning under massively parallel simulation. In International Conference on Machine Learning, pp. 1944019459. PMLR, 2023. Qiayuan Liao, Takara Truong, Xiaoyu Huang, Guy Tevet, Koushil Sreenath, and Karen Liu. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. Published as conference paper at ICLR 2026 Timothy Lillicrap, Jonathan Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, and Sergey Levine. Serl: software suite for sample-efficient robotic reinforcement learning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1696116969. IEEE, 2024. Michael Lutter, Christian Ritter, and Jan Peters. Deep lagrangian networks: Using physics as model prior for deep learning. arXiv preprint arXiv:1907.04490, 2019. Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac gym: High performance gpu-based physics simulation for robot learning. arXiv preprint arXiv:2108.10470, 2021. Evgenii Nikishin, Max Schwarzer, Pierluca DOro, Pierre-Luc Bacon, and Aaron Courville. The In Proceedings of the 39th International Conprimacy bias in deep reinforcement learning. ference on Machine Learning (ICML), volume 162 of Proceedings of Machine Learning Research, pp. 1682816847. PMLR, 2022. URL https://proceedings.mlr.press/ v162/nikishin22a.html. Lerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asymmetric actor critic for image-based robot learning. arXiv preprint, arXiv:1710.06542, 2017. URL https://arxiv.org/abs/1710.06542. Antonin Raffin. Getting sac to work on massive parallel simulator: An rl journey with off-policy algorithms. araffin.github.io, Feb 2025. URL https://araffin.github.io/post/ sac-massive-sim/. Maziar Raissi, Paris Perdikaris, and George Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics, 378:686707, 2019. Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning. In Conference on Robot Learning, pp. 91100. PMLR, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Clemens Schwarke, Mayank Mittal, Nikita Rudin, David Hoeller, and Marco Hutter. Rsl-rl: learning library for robotics research. arXiv preprint arXiv:2509.10771, 2025. Younggyo Seo, Carmelo Sferrazza, Haoran Geng, Michal Nauman, Zhao-Heng Yin, and Pieter Abbeel. Fasttd3: Simple, fast, and capable reinforcement learning for humanoid control. arXiv preprint arXiv:2505.22642, 2025. Laura Smith, Chase Kew, Xue Bin Peng, Sehoon Ha, Jie Tan, and Sergey Levine. Legged robots that keep on learning: Fine-tuning locomotion policies in the real world. In 2022 international conference on robotics and automation (ICRA), pp. 15931599. IEEE, 2022a. Laura Smith, Ilya Kostrikov, and Sergey Levine. walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. arXiv preprint arXiv:2208.07860, 2022b. Wandong Sun, Long Chen, Yongbo Su, Baoshi Cao, Yang Liu, and Zongwu Xie. Learning humanoid locomotion with world model reconstruction. arXiv preprint arXiv:2502.16230, 2025. Brijen Thananjeyan, Ashwin Balakrishna, Suraj Nair, Michael Luo, Krishnan Srinivasan, Minho Hwang, Joseph Gonzalez, Julian Ibarz, Chelsea Finn, and Ken Goldberg. Recovery rl: Safe reinforcement learning with learned recovery zones. IEEE Robotics and Automation Letters, 6 (3):49154922, 2021. 13 Published as conference paper at ICLR Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pp. 2330. IEEE, 2017. Yushi Wang, Penghui Chen, Xinyu Han, Feng Wu, and Mingguo Zhao. Booster gym: An end-to-end reinforcement learning framework for humanoid robot locomotion. arXiv preprint arXiv:2506.15132, 2025. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: In Conference on robot learning, pp. 22262240. World models for physical robot learning. PMLR, 2023. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:1412914142, 2020. Kevin Zakka, Baruch Tabanpour, Qiayuan Liao, Mustafa Haiderbhai, Samuel Holt, Jing Yuan Luo, Arthur Allshire, Erik Frey, Koushil Sreenath, Lueder Kahrs, et al. Mujoco playground. arXiv preprint arXiv:2502.08844, 2025. 14 Published as conference paper at ICLR"
        },
        {
            "title": "A ADDITIONAL EXPERIMENTS",
            "content": "A.1 PRELIMINARY STUDY We consider the practical case where the pretrained policy entirely fails in the deployment stage, finetuning must effectively learn new policy from scratch, with data collection constrained to deterministic policy. To study this setting, we design simple experiment using the BoosterT1 robot, which has 12 lower-body degrees of freedom. The reward is composed of three terms commonly used in humanoid locomotion: swing-leg reference, body-height tracking, and linear velocity tracking. The policy is trained to achieve target forward velocity of 0.2 m/s. The implementation details can be found in Appendix C.1. As shown in Figure 6, we compare PPO and SAC within the SSRL (Levy et al., 2024) framework. The default setting uses SAC, which leverages state-dependent stochastic actor to explore in the world model and improve policy performance. For comparison, we replace SAC with PPO (implemented in RSL-RL (Schwarke et al., 2025)) while keeping all other components fixed, tuning only the hyperparameters. We observe that the most critical factor for PPO performance is the initial action standard deviation (std), which strongly influences the distribution of states explored in the world model. larger std makes training difficult to converge and tends to drive the policy into regions where the world model is poorly trained, while smaller std leads to under-exploration and unstable convergence, with some seeds failing to learn at all. To mitigate this, we replace the PPO actor with the SAC actor that outputs both mean and std of the action distribution. This modification greatly improves stability, with all 8 seeds converging, but still requires two to three times more samples than SAC. Meanwhile, we run these experiments for 48 hours on single Brax (Freeman et al., 2021) simulation. None of the methods converge to the target velocity within this time, highlighting the need for large-scale pretraining to reduce wall-clock time and avoid convergence to local minima. Figure 6: Comparison of PPO and SAC in preliminary fine-tuning experiment with the BoosterT1 robot, run for 48 hours on single Brax simulation. The left figure shows the evaluation episode return, and the right figure shows the bodys linear velocity along the x-axis. Curves represent the average over 8 random seeds, with evaluation performed in 128 parallel environments. PPO performance is highly sensitive to the initial action standard deviation (std). Replacing the PPO actor with SAC-style actor that outputs both the mean and standard deviation improves stability, although SAC still requires fewer samples to achieve the same performance. 15 Published as conference paper at ICLR A.2 PRETRAINING EXPERIMENTS Figure 7: Pretraining performance comparison of LIFT (red), PPO (orange), and FastTD3 (blue) across six humanoid tasks (top row: rough terrain for the three robot configurations; bottom row: flat terrain for the three robot configurations). Results show the mean over 8 random seeds. A.3 FINETUNING EXPERIMENTS IN THE REAL-WORLD Figure 8: Finetuning performance of LIFT in the Real-World. Across 3 random seeds, LIFT consistently improves episode return, forward velocity tracking, and angular-velocity tracking, while reducing the action-rate penalty. Note that the velocity tracking error (target at 0.6 m/s) might come from the noisy base-velocity estimation from the onboard IMU accelerator. 16 Published as conference paper at ICLR A.4 THE EFFECT OF HYPERPARAMETER Figure 9: Effect of pretraining hyperparameters on Low dimensional Booster T1. From left to right: UTD ratio, buffer size, and batch size. UTD values compared: 1, 5, 10, 15, 20; buffer sizes: 10,000, 100,000, 1,000,000; batch sizes: 256, 512, 1,024, 4,096, 8,192. Larger UTD and batch sizes accelerate convergence; increasing buffer size improves learning speed but at the cost of much higher GPU memory usage. Figure 10: Impact of the entropy coefficient α and autoregressive loss horizon on finetuning performance (Booster T1, target speed = 1.5 m/s). Curves show evaluation episode return and body forward velocity. Larger α values degrade performance, leading to less stable convergence and lower final forward velocity. Using the pretraining α gives reasonable results, while smaller α yields more stable learning. Horizon = 1 sometimes fails to reach the target velocity, while horizons = 2 and = 4 ensure stable convergence, indicating that multi-step autoregressive prediction improves worldmodel training and stabilizes policy finetuning. 17 Published as conference paper at ICLR 2026 A.5 PRETRAINING EXPERIMENTS Figure 11: Gaits of LIFT across different tasks in the MuJoCo Playground. 18 Published as conference paper at ICLR 2026 A.6 REAL WORLD EXPERIMENTS Figure 12: Sim-to-real reinforcement learning with LIFT. Figure 13: Sim-to-real gait comparison in indoor and outdoor environments. 19 Published as conference paper at ICLR 2026 A.7 FINETUNING: G1 EXPERIMENTS Figure 14: Sim-to-sim transfer and fine-tuning results for the Unitree G1 in Brax with target velocity of 1.5 m/s. Top: Policy before fine-tuning, exhibiting instability and shuffling gait. Bottom: Policy after fine-tuning, demonstrating stable, human-like walking gait with reduced torso pitch. Figure 15: Finetuning results on the Unitree G1 in Brax. LIFT consistently improves policy behavior and reward performance. At target speed of 0.6 m/s, body oscillations are significantly reduced, while at 1.5 m/s, the policy initially exhibits unstable motion in sim2sim but, after finetuning, successfully stands and walks. Published as conference paper at ICLR 2026 A.8 NON-LOCOMOTION TASKS Our framework is built around learning world model of the robots state dynamics, so any task whose reward depends only on the robot proprioceptive state can, in principle, be handled directly. This includes different locomotion gaits, whole-body motion tracking, and balance under disturbances, since as long as the reward can be computed from the state, the policy can be finetuned entirely through world-model rollouts. Whole body motion tracking Tasks. To better demonstraste this potential, we extended our pretraining pipeline from velocity-tracking locomotion to BeyondMimic-style whole-body tracking (Liao et al., 2025). We reimplemented the observation and reward structure of BeyondMimic in JAX within MuJoCo Playground and used the Unitree motion dataset (LAFAN1) to pretrain whole-body tracking policy for the Unitree G1 humanoid. Video demonstrations are available on our project website. Due to the engineering effort required to reproduce consistent observation/reward definitions and contact handling across MuJoCo and Brax, and to integrate these components into our world-model finetuning pipeline, we were able to complete only the pretraining stage within the current timeframe. We therefore present these whole-body tracking results as preliminary evidence of LIFTs broader applicability. This limitation is practical rather than conceptual: once the corresponding Brax environment is finalized or real Unitree G1 hardware is available, the same stage-(iii) finetuning procedure can be directly applied. Figure 16: Gaits of LIFT across different whole body motion tracking tasks in the MuJoCo Playground. Object-centric Tasks. For tasks involving external objects (e.g., kicking ball), the current framework would need to be extended to also model object dynamics, for example by augmenting the state with object pose and using simple physical priors such as conservation of momentum. This is feasible but requires additional engineering and environment design, so we explicitly leave it as future work. Likewise, obstacle avoidance and more complex navigation are more naturally handled at higher level: once reliable low-level tracking controller is available (as in our setup), high-level policy can be trained to output velocity or pose targets, potentially with its own high-level world model as explored in hierarchical model-based RL (Hansen et al., 2024). Extending LIFT to hierarchical and object-centric tasks is promising direction, but beyond the scope of this paper. 21 Published as conference paper at ICLR"
        },
        {
            "title": "B TRAINING DETAILS",
            "content": "Algorithm 1 The Finetuning phase of LIFT Require: Pretrained policy π; pretrained world model wm; world-model replay buffer Dwm; batch size B; SAC replay buffer Dsac; maximum episode length Tep; number of training iterations numtrain; world-model rollout horizon Hwm. (cid:83){st, at, st+1, rt+1} (cid:83){st, at, st+1, rt+1} for 1 to Tep do at π(st) st+1, rt+1 = env.step(st, at) Dwm Dwm Dsac Dsac 1: while π not converged do s0 = env.reset() 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: 18: end while 19: return π end for Sample data from and finetune the world model using equation 19. for 1 to numtrain do // numtrain is linearly increased from 10 to 1000 Sample batch of (Batch size B) initial state s0 from the the Dwm. for 1 to Hwm do // Hwm is linearly increased from 1 to 20 at π(st) st+1, rt+1 = wm.step(st, at) // predict the next state and reward via world model. Dsac Dsac (cid:83){st, at, st+1, rt+1} end for Sample the transition from the Dsac, and update the SAC actor and critic. B.1 PRETRAINING DETAILS OF SAC Training Objective We pretrain the SAC policy in MuJoCo Playground. The critics minimize the entropy-regularized Bellman residual: E(sp t+1, at+1)α log πθ(at+1st+1)))2(cid:3), (cid:2)Qψ(sp ψi(sp t+1)D, at+1πθ(st+1) ,at,rt+1,sp , at)(rt+1+γ( min i=1,2 (6) where ψi denotes the target critics. Transitions are sampled from the replay buffer D. The actor is updated by maximizing the entropy-augmented Q-value: Est,sp D, atπθ(st)[α log πθ(atst) min i=1,2 Qψi(sp , at)]. (7) The entropy coefficient α is adjusted to match target entropy via EstD, atπθ(s)[α(log πθ(atst) + H)]. (8) Finally, the target critics are updated by Polyak averaging: ψ τ ψ + (1 τ ) ψ, where τ (0, 1) is the update rate. Hyperparameter-Tuning We employ the Optuna framework (Akiba et al., 2019) for systematic hyperparameter tuning, leveraging the CmaEsSampler for parameter exploration in combination with PatientPruner (built on top of SuccessiveHalvingPruner) to automatically terminate trials with poor early performance. The tuned hyperparameters include the number of gradient updates per step (UTD), replay buffer size, entropy coefficient α, discount factor γ, learning rates of the actor, critic, and temperature parameter α, batch size, reward scaling, number of parallel environments, and Polyak coefficient τ . The optimization objective is the episode reward obtained during policy evaluation. On the T1LowDimJoystick tasks, we conducted approximately ten hours of hyperparameter search on single NVIDIA RTX 4090 GPU. The results indicate that tuning reduces the convergence time from about seven hours to only half an hour. On the G1Joystick and T1Joystick tasks, we carried out similar ten-hour search using eight NVIDIA RTX 4090 GPUs, which led to improved reward performance and more robust convergence. Table 1 lists the tuned hyperparameters. 22 Published as conference paper at ICLR 2026 Table 1: LIFT Hyperparameter Configurations for MuJoCo Playground Environments Hyperparameter T1LowDimJoystick* G1/T1 Joystick* T1JoystickFlatTerrain Parallel environments Batch size Gradient updates/step Discount factor (γ) Reward scaling Soft update (τ ) Actor learning rate Critic learning rate Alpha learning rate Target entropy coefficient Initial log α Policy hidden layers Q-network hidden layers Activation Max replay size (106) Min replay size Observation normalization Action repeat 1,000 1,024 0.987 1.0 0.024 1.03e-4 1.00e-4 9.97e-3 0.50 -3.35 4,096 16,384 19 0.982 16.0 0.002 1.70e-4 1.88e-4 6.44e0.40 -3.00 4,096 16,384 16 0.982 32.0 0.007 2.05e-4 1.37e-4 6.41e-4 0.17 -6.05 (512, 256, 128) (1024, 512, 256) swish (512, 256, 128) (1024, 512, 256) swish (512, 256, 128) (1024, 512, 256) swish 1.0 8,192 True 1 1.0 8,192 True 1 1.0 8,192 True 1 B.2 TRAINING DETAILS OF WORLD MODEL (a) From Privileged State to Brax Generalized State. We build the generalized coordinates/velocities of Brax using the privileged state and denote them simply by (qt, qt) for the rest of the section: (cid:3), qt = (cid:2)pt, nt, qj,t qt = (cid:2)vw (cid:3), , ωb qj,t , where R(n) SO(3) be the rotation matrix from the base quaternion (bodyworld), with R(q) its inverse. Brax exposes differentiable rigid-body dynamics primitivesmass matrix (q), Coriolis/centrifugal terms C(q, q), and gravity G(q)thereby avoiding any reimplementation of dynamics and keeping the entire rolloutloss pipeline end-to-end differentiable. pt = (0, 0, ht); = R(nt) vb vw , (10) (9) via PD controller: τ (b) From Privileged State and Action to Torque (repeat substeps). The policy action is converted to motor torques τ qj,t)+Kd (0 qj,t), where Kp and Kd are stiffness and damping; qdefault come from the network τϕ(sp , at). We assume the control rate fctrl is lower than the simulator/collection rate fsim. Hence each control interval contains = fsim simulator substeps of size tsub, so that fctrl the control interval length is = tsub. is default/standing joint position. The external torque τ = Kp (at+qdefault (c) One Semi-implicit Euler Step (repeat substeps). For substep = 0, . . . , 1 with step size tsub: + τ C(qt, qt) G(qt)], qt = 1(qt)[τ qt+tsub = qt + tsub qt, qj,t+tsub = qj,t + qt+tsub pt+tsub = pt + tsub vw (cid:16) nt+tsub = normalize t+tsub , nt quat(cid:0) (cid:92) ωb t+tsub , ωb t+tsub tsub (cid:1)(cid:17) . (11) (12) (13) (14) (15) Here, (cid:98)ω = ω/ω is the unit rotation axis; quat((cid:98)ω, θ) = [cos( θ 2 )] is the axisangle increment as unit quaternion; denotes quaternion multiplication; normalize() re-unitizes the quaternion to avoid numerical drift. After substeps, we obtain: qt+t = (cid:2)pt+t, nt+t, qj,t+t qt+t = (cid:2)vw t+t, ωb qj,t+t 2 ), (cid:98)ω sin( θ t+t, (cid:3), (cid:3). (17) (16) 23 Published as conference paper at ICLR 2026 (d) From (qt+t, qt+t) to the Next Privileged State.: Joints Position/Velocity: (qj,t+t, qj,t+t). (cid:98)sp t+t : Base Linear Velocity: vb Base Angular Velocity: ωb t+t = R(nt+t)vw t+t. Quaternion: nt+t. t+t. Body Height: ht+t = pt+t,z. (18) B.3 TRAINING DETAILS OF WORLD MODEL IN FINETUNING STAGE , Concretely, we sample length-H+1 sequences DH = b=1 where is the mini-batch size and is the loss horizon (e.g., H=4). Let (cid:98)sp b,t+1 be the models onestep prediction (obtained by differentiable physics given (sp b,t+1 the target, and log σ2 b,t the models elementwise log-variance. We minimize an auto-regressive diagonal-Gaussian negative log-likelihood over an H-step unroll: b,t, ab,t)), sp t+H ) t+1, at+1, . . . , sp , at, sp (sp (cid:110) (cid:111)B Lϕ = 1 (cid:88) H1 (cid:88) b=1 t=0 (cid:104)(cid:0) (cid:98)sp b,t+1 sp b,t+1 (cid:1)2 exp(cid:0) log σ2 b,t (cid:1) + log σ b,t (cid:105) , (19) where denotes elementwise multiplication, (cid:98)sp (sp b,t+1 is the target, and log σ2 gate through the entire H-step unroll. b,t, ab,t), sp b,t+1 is the models next-step prediction given b,t is the elementwise log-variance. Gradients backpropaB.4 TRAINING DETAILS OF POLICY IN FINETUNING STAGE key advantage of our physics-informed design is that (cid:98)sp exposes kinematic and dynamic signals that enable exact, simulator-consistent reward computation without training reward model. We thus define per-step reward: rt+1 = (cid:88) (cid:0) , at, (cid:98)sp (cid:98)sp wk rk t+ (cid:1), (20) with each rk computed analytically. For example, foot orientation stability follows directly from each foots roll angle and is penalized by rfeet-roll = (cid:88) (cid:0)φf (cid:1) , {Left foot,Right foot} (21) where φf is the roll of foot extracted from the link quaternion in (cid:98)sp hyperparameters in finetuning. . Table 1 lists the LIFT Published as conference paper at ICLR 2026 Table 2: Finetune Hyperparameter of LIFT Hyperparameter Policy hidden layers Q-network hidden layers World Model hidden size Activation Learning rate Discount factor (γ) Batch size Soft update (τ ) Initial log α Gradient updates/step Real data ratio Init exploration steps Replay buffer size Reward scaling Model trains/epoch Env steps/training Horizon of exploration in world model Gradient step per update Convergence criteria Loss horizon Actor-Critic World Model [512, 256, 128] [1024, 512, 256] swish [400,400,400,400] swish 2e-4 0.99 256 0.001 -7.13 20 0.06 1,000 400k 1.0 1 1,000 120 (epochs 010) 101000 (epochs 04) 1e-3 200 60k 0.01 (6 epochs)"
        },
        {
            "title": "C ENVIRONMENT SETUP",
            "content": "C.1 PRELIMINARY STUDY - BOOSTER ENVIRONMENT Task summary. The agent controls 12-DoF T1 humanoid robot with simplified lower-body kinematics. At each step, the agent outputs 12D continuous action that perturbs motor target positions relative to default pose. State (39D). The state consists of torso quaternion (4), joint angles (12), base linear velocity in body frame (3), base angular velocity in body frame (3), joint velocities (12), gait phase represented by cosine/sine (2), gait progression (1), gait frequency (1), and base height (1). Privileged state (39D). The privileged state uses the same 39-dimensional observation space as the state. Action Space. The action space is continuous 12-dimensional vector, where policy outputs are constrained to the range [1, 1]. Motor targets are computed using PD control: = Kp(a as + qdefault q) + Kd( qdes q) (22) where as = 0.25 is the action scale factor. The proportional and derivative gains are defined as: Kp = [200, 200, 200, 200, 50, 50, 200, 200, 200, 200, 50, 50] Kd = [5, 5, 5, 5, 1, 1, 5, 5, 5, 5, 1, 1] The gains are symmetric for both legs, with higher gains for hip joints (200 for Kp, 5 for Kd) and lower gains for knee joints (50 for Kp, 1 for Kd). Reward Function Design. Uses multi-objective weighted reward function: rt = (cid:88) siri(st, at) Control Architecture. Uses two-level control strategy: 1. High-level Policy: Outputs joint position increments, control frequency 100 Hz Published as conference paper at ICLR 2026 Table 3: Reward Function Components and Weights Reward Term Description Default Weight base height tracking lin vel Linear velocity tracking reward ref"
        },
        {
            "title": "Base height reward",
            "content": "0.2 1.2 3.0 Gait Generation. Phase-based gait controller: ϕ = mod(t fg, 1.0) Left Leg Swing Phase = ϕ 0.25 < 0.5 Tswing Right Leg Swing Phase = ϕ 0.75 < 0.5 Tswing where Tswing = 0.2s is the swing phase duration. Termination Conditions. Episode terminates under the following conditions: Base height / [0.3, 0.8] Base linear velocity > 10.0 m/s or angular velocity > 10.0 rad/s Torso roll angle ϕ > π/4 or pitch angle θ > π/4 Joint position or velocity exceeds mechanical limits Experimental Setup. In the Preliminary Study (Section A.1): Target forward velocity: 0.2 m/s Main reward terms: swing leg reference, base height tracking, linear velocity tracking 26 Published as conference paper at ICLR 2026 C.2 PRETRAINING ENVIRONMENTS C.2.1 T1LOWDIMJOYSTICK (T1LowDimJoystick as an example to illustrate the pretraining environment T1Joystick and G1Joystick are omitted for brevity.) setup, Task summary. The agent controls 12-DoF T1 humanoid robot with simplified lower-body kinematics. At each step, the agent outputs 12D continuous action that perturbs motor target positions relative to default pose. The environment returns noisy states and scalar reward. Episodes terminate upon falls or numerical errors (NaNs). State (47D). The state consists of gravity vector in the body frame (3), gyroscope readings (3), joystick command (vx, vy, ω) (3), gait phase represented by cosine/sine (2), joint angles relative to the default pose (12), joint velocities scaled by 0.1 (12), and previous action (12). Privileged state (110D). The privileged state includes all elements of the 47D state, together with additional raw sensor signals (gyroscope, accelerometer, gravity vector, linear velocity, global angular velocity), joint differences, root height, actuator forces, contact booleans, foot velocities, and foot air time. Action space. The action is continuous 12-D vector; policy outputs lie in [1, 1]. Motor targets are computed using PD control law: = Kp(a + qdefault q) + Kd(0 q) (23) where is the output motor torque, Kp and Kd are the proportional and derivative gains, respectively, is the action output from the policy qdefault is nominal joint position, is the current joint position, and is the current joint velocity. Reward Instantaneous reward: rt = (cid:80) 0.02s. , at), where si are term weights and = si ri(sp Table 4: Default reward terms (Note: [a, b] denotes uniform distribution over [a, b]) Term Description Survival bonus per step. survival tracking lin vel Track commanded vx velocity. tracking lin vel Track commanded vy velocity. tracking ang vel feet swing base height orientation torques torque tiredness power lin vel ang vel xy dof vel dof acc root acc action rate dof pos limits collision feet slip feet roll feet yaw diff feet yaw mean feet distance Track commanded yaw rate. Reward proper foot swing phase. Penalize deviation from target height (0.68m). Penalize torso tilt from vertical. Penalize large torques. Penalize torque near limits. Penalize positive mechanical power. Penalize vertical base velocity. Penalize torso roll/pitch rates. Penalize joint velocity. Penalize joint acceleration. Penalize root link acceleration. Penalize rapid action changes. Penalize joint limit violations. Penalize self-collisions. Penalize slipping contacts. Penalize foot roll angles. Penalize difference in foot yaw angles. Penalize deviation from base yaw. Penalize feet being too close. Scale 0.25 1.0 1.0 2.0 3.0 -20.0 -10.0 -1.0e-4 -5.0e-3 -1.0e-3 -2.0 -0.2 -1.0e-4 -1.0e-7 -1.0e-4 -0.5 -1.0 -10.0 -0.1 -1.0 -1.0 -1.0 -10.0 Implementation details 27 Published as conference paper at ICLR 2026 Control period: ctrl dt = 0.02 s; simulation step: sim dt = 0.002 s. External pushes enabled with interval 5.0-10.0s and magnitude 0.1-1.0N. Enhanced tracking rewards with separate x/y linear velocity tracking. Sophisticated foot kinematics penalties including roll, yaw, and swing phase rewards. Table 5: Domain randomization parameters (Note: [a, b] denotes uniform distribution over [a, b], (c) denotes uniform distribution over [c, c]) Parameter Range Description [0.2, 0.6] [0.9, 1.1] [1.0, 1.05] [0.98, 1.02] +U [1.0, 1.0] kg Additional torso mass"
        },
        {
            "title": "Ground friction coefficient\nJoint friction scaling\nJoint armature scaling\nBody mass scaling",
            "content": "Floor friction Joint friction loss Joint armature Link masses Torso mass Initial joint positions [0.05] rad [0.7, 1.3] Joint stiffness [0.7, 1.3] Joint damping [0.5, 2.0] Ankle damping"
        },
        {
            "title": "Default pose randomization\nActuator gain scaling\nDamping coefficient scaling\nHigher range for ankle joints",
            "content": "C.3 FINETUNING ENVIRONMENTS C.3.1 T1LOWDIMJOYSTICK (FINETUNE) (T1LowDimJoystick as an example to illustrate the finetuning environment G1LowDimJoystick is omitted for brevity.) setup, Task summary. This is simplified version of the T1LowDimJoystick environment designed specifically for sim-to-sim transfer from MuJoCo Playground to Brax and finetuning in Brax. The environment ensures that policies trained in MuJoCo Playground can be successfully transferred to Brax with similar reward scales and performance characteristics. The reward settings are adopted from Booster Gym (Wang et al., 2025). Key modifications for finetuning: Normalized observations using predefined limits for consistent scaling across simulators Simplified reward structure with many terms zeroed out to focus on essential behaviors Continuous gait phase representation using gait process and frequency State (47D, normalized). The normalized state consists of gravity vector (3), gyroscope readings (3), joystick command (vx, vy, ω) (3), gait phase cosine/sine (2), joint angles (12), joint velocities (12), and previous action (12). All observations are normalized to [1, 1] using predefined limits. Privileged state (87D, normalized). Includes base state plus raw gyroscope, gravity vector, orientation quaternion (4), base velocity, joint positions/velocities, root height, gait process, and gait frequency. Observation normalization: Observations are normalized using: obs normalized = 2 (obs min) max min 1 with predefined limits for each observation dimension. Action space. Motor targets are computed using PD control law: = Kp(a + qdefault q) + Kd(0 q) (24) where is the output motor torque, Kp and Kd are the proportional and derivative gains, respectively, is the action output from the policy qdefault is nominal joint position, is the current joint position, and is the current joint velocity. 28 Published as conference paper at ICLR 2026 Table 6: Finetune reward terms (Note: many terms disabled for sim-to-sim transfer) Term Description Survival bonus per step. survival tracking lin vel Track commanded vx velocity. tracking lin vel Track commanded vy velocity. tracking ang vel base height orientation feet swing feet slip feet yaw diff feet yaw mean feet roll feet distance Track commanded yaw rate. Reward proximity to target height (0.65m). Penalize torso tilt from vertical. Reward proper foot swing phase. Penalize slipping contacts. Penalize difference in foot yaw angles. Penalize deviation from base yaw. Penalize foot roll angles. Penalize feet being too close. Scale 0.25 1.0 1.0 2.0 0.2 -5.0 3.0 -0.1 -1.0 -1.0 -1.0 -10.0 Reward terms (simplified for transfer): Disabled reward terms: torques, torque tiredness, power, lin vel z, ang vel xy, dof vel, dof acc, root acc, action rate, dof pos limits, collision, feet vel (set to 0.0 scale). Implementation details External pushes enabled (interval: 5.0-10.0s, magnitude: 0.1-1.0N) Normalized observations ensure consistent scaling across simulators Table 7: Domain randomization parameters (Note: [a, b] denotes uniform distribution over [a, b], (c) denotes uniform distribution over [c, c]) Parameter Range Description [0.2, 0.6] [0.9, 1.1] [1.0, 1.05] [0.98, 1.02] +U [1.0, 1.0] kg Additional torso mass Ground friction coefficient Joint friction scaling Joint armature scaling Body mass scaling Floor friction Joint friction loss Joint armature Link masses Torso mass Initial joint positions [0.05] rad [0.7, 1.3] Joint stiffness [0.7, 1.3] Joint damping [0.5, 2.0] Ankle damping Default pose randomization Actuator gain scaling Damping coefficient scaling Higher range for ankle joints"
        },
        {
            "title": "D LARGE LANGUAGE MODEL USAGE",
            "content": "We gratefully acknowledge the use of ChatGPT and DeepSeek to assist in polishing, refining, and condensing the text of this paper."
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, Xidian University",
        "State Key Laboratory of General Artificial Intelligence, BIGAI"
    ]
}