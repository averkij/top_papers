{
    "paper_title": "PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion",
    "authors": [
        "Sophia Tang",
        "Yinuo Zhang",
        "Pranam Chatterjee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Peptide therapeutics, a major class of medicines, have achieved remarkable success across diseases such as diabetes and cancer, with landmark examples such as GLP-1 receptor agonists revolutionizing the treatment of type-2 diabetes and obesity. Despite their success, designing peptides that satisfy multiple conflicting objectives, such as target binding affinity, solubility, and membrane permeability, remains a major challenge. Classical drug development and structure-based design are ineffective for such tasks, as they fail to optimize global functional properties critical for therapeutic efficacy. Existing generative frameworks are largely limited to continuous spaces, unconditioned outputs, or single-objective guidance, making them unsuitable for discrete sequence optimization across multiple properties. To address this, we present PepTune, a multi-objective discrete diffusion model for the simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with state-dependent masking schedules and penalty-based objectives. To guide the diffusion process, we propose a Monte Carlo Tree Search (MCTS)-based strategy that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTS integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity inherent to discrete spaces. Using PepTune, we generate diverse, chemically-modified peptides optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling characteristics on various disease-relevant targets. In total, our results demonstrate that MCTS-guided discrete diffusion is a powerful and modular approach for multi-objective sequence design in discrete state spaces."
        },
        {
            "title": "Start",
            "content": "PepTune: De Novo Generation of Therapeutic Peptides with Multi-Objective-Guided Discrete Diffusion Sophia Tang,1,2, Yinuo Zhang,1,3, Pranam Chatterjee1,4,5, 1Department of Biomedical Engineering, Duke University 2Management and Technology Program, University of Pennsylvania 3Center of Computational Biology, Duke-NUS Medical School 4Department of Computer Science, Duke University 5Department of Biostatistics and Bioinformatics, Duke University These authors contributed equally Corresponding author: pranam.chatterjee@duke.edu"
        },
        {
            "title": "Abstract",
            "content": "Peptide therapeutics, major class of medicines, have achieved remarkable success across diseases such as diabetes and cancer, with landmark examples such as GLP1 receptor agonists revolutionizing the treatment of type-2 diabetes and obesity. Despite their success, designing peptides that satisfy multiple conflicting objectives, such as target binding affinity, solubility, and membrane permeability, remains major challenge. Classical drug development and target structure-based design are ineffective for such tasks, as they fail to optimize global functional properties critical for therapeutic efficacy. Existing generative frameworks are largely limited to continuous spaces, unconditioned outputs, or single-objective guidance, making them unsuitable for discrete sequence optimization across multiple properties. To address this, we present PepTune, multi-objective discrete diffusion model for the simultaneous generation and optimization of therapeutic peptide SMILES. Built on the Masked Discrete Language Model (MDLM) framework, PepTune ensures valid peptide structures with state-dependent masking schedules and penalty-based objectives. To guide the diffusion process, we propose Monte Carlo Tree Search (MCTS)-based strategy that balances exploration and exploitation to iteratively refine Pareto-optimal sequences. MCTS integrates classifier-based rewards with search-tree expansion, overcoming gradient estimation challenges and data sparsity inherent to discrete spaces. Using PepTune, we generate diverse, chemicallymodified peptides optimized for multiple therapeutic properties, including target binding affinity, membrane permeability, solubility, hemolysis, and non-fouling characteristics on various disease-relevant targets. In total, our results demonstrate that MCTS-guided discrete diffusion is powerful and modular approach for multi-objective sequence design in discrete state spaces."
        },
        {
            "title": "Introduction",
            "content": "Peptides possess unique advantages as therapeutic modality, including their ability to bind to diverse set of binding motifs without requiring stable binding pockets, making them ideal for targeting structurally diverse protein surfaces [1, 2]. Moreover, their relatively large size and flexible backbone enable them to disrupt protein-protein interactions (PPIs) that are central to disease processes, particularly those requiring interactions with larger surface areas [3]. These attributes have driven surge in interest, with over 33 FDA-approved therapeutic peptides introduced since 2000 and 4 2 0 2 3 2 ] . - [ 1 0 8 7 7 1 . 2 1 4 2 : r more than 170 in clinical development for diseases ranging from diabetes to cancer [2]. Specifically, peptides are behind one of the most landmark breakthroughs in recent clinical history: GLP-1 receptor agonists like semaglutide and liraglutide, which have transformed the treatment landscape for type2 diabetes and weight loss [4, 5]. These engineered peptides have achieved remarkable efficacy, following years of meticulous structural and functional optimization [4, 5]. Their success highlights the potential of therapeutic peptides to address complex diseases where more traditional approaches, such as small molecules, often fall short [2]. As evidenced by the journey to generate semaglutide and liraglutide [5], peptides containing only the 20 wild-type amino acids have limitations including susceptibility to enzymatic degradation and low membrane permeability [2]. To overcome these limitations, non-natural amino acids (nAAs) containing diverse chemical modifications to the peptide backbone and side chains have been integrated into peptides to enhance their therapeutic properties. For example, selespressin, which contains nAAs at its proteolytic site, has been shown to have longer plasma half-life than its natural analog [6] and GLP-1 analogs containing the nAA e-Nheptanoyl-l-lysine has demonstrated stronger binding affinity to their target [7, 5]. Furthermore, chemical modifications are commonly used to generate cyclic peptides, with over 99.6% of cyclic peptides containing nAAs [8]. Due to their stable conformation, cyclic peptides have stronger binding affinity and specificity, enhanced membrane permeability, and low degradation [9]. Despite this progress in peptide drug development, designing peptides that effectively engage new therapeutic targets remains major limitation, with traditional methods involving screening large phage libraries of up to trillions of random amino acid permutations [10, 11]. This motivates the development of generative deep learning models that can effectively learn the space of clinically relevant peptides and sample de novo peptides conditioned with various therapeutic properties, including binding affinity, solubility, and membrane permeability. Generative diffusion models are considered state-of-the-art for de novo binder design, with new models such as RFpeptides even enabling the design of high-resolution macrocyclic peptides given target structure [12]. However, structure-based models [1214], or those that require an intermediate structure prediction step [15], rely on stable tertiary conformations of target proteins, precluding the design of peptide binders to disordered and dynamic targets, which drive sizable portion of diseases [16]. Generative peptide design language models relying only on the target sequence, such as PepPrCLIP [17] and PepMLM [18], have demonstrated robust success on disordered and structurallydiverse targets, but their utilization of only the 20 wild-type amino acids limits these models from sampling from the space of chemically-modified or cyclic peptides, precluding exploitation of the full therapeutic potential of synthetic peptides. Recently, discrete generative models have shown incredible promise in areas such as text generation [1925], image synthesis [19, 21], executable code generation [23], DNA sequence optimization [26, 27, 25], and even protein design [28, 27, 24, 29], but they still face significant limitations in multi-objective-guided generation and optimization. In our case, the challenge lies in simultaneously optimizing for conflicting therapeutic properties, critical requirement for generating clinically viable peptides [2, 30, 31]. Classifier-based and classifier-free guidance strategies have been explored to steer discrete diffusion objectives toward specific properties [29, 24, 26, 27], yet these approaches often struggle with gradient estimation and the sparsity of quality data in discrete spaces. This motivates the development of methods that can balance diverse objectives while addressing the unique challenges of discrete peptide generation. In this work, we introduce PepTune, multi-objective-guided discrete diffusion model for de novo peptide design. PepTune incorporates Monte Carlo Tree Search (MCTS)-based guidance framework to condition Masked Discrete Language Model (MDLM) [22], pre-trained on large dataset of chemically-modified and cyclic peptides represented as Simplified Molecular Input Line Entry System (SMILES) strings [32]. However, because the vast majority of SMILES strings are neither chemically valid or nor represent synthesizable peptides, de novo sampling from the space of therapeutically relevant peptide SMILES is virtually impossible. By conditioning on synthesizability, PepTune specifically enables the generation of valid peptide SMILES containing both non-natural amino acids (nAAs) and cyclic structures while achieving Pareto-optimal solutions across multiple therapeutic properties. Our results highlight PepTunes unique capability to balance diverse objectives, setting new standard for property-conditioned sequence generation. Our contributions are as follows: 2 1. Masked Diffusion Language Model for Discrete Sampling of Peptide SMILES. We introduce the first Masked Diffusion Language Model (MDLM) with RoFormer backbone for de novo generation of peptide SMILES sequences that is capable of generating valid and diverse chemically-modified and cyclic peptides that cannot be encoded with canonical amino acid sequences. 2. State-Dependent Masking Schedule. Since the presence of peptide bonds is fundamental component of all valid peptides, we introduce novel state-dependent masking schedule that masks peptide bond tokens at slower rate at earlier time steps in the forward diffusion process than non-peptide bonds. We demonstrate that our masking schedule increases the weight of the negative log-loss associated with peptide bond tokens in time-dependent manner and results in earlier unmasking of peptide bond tokens in the reverse diffusion process. 3. Global Sequence Invalidity Loss. Since the vast majority of SMILES sequences do not translate into valid peptides, we introduce an invalid loss that penalizes predicted token probabilities where the argmax token sequence is not valid peptide SMILES. Since the argmax function is not differentiable, we scale the penalty score by the softmax probability of the sampled token. This objective smoothly propagates the penalty from single discrete sequence to the smooth token probability distributions through gradient updates without the need for complex techniques like straight-through gradient estimators. 4. MCTS for Robust Classifier-Based Multi-Objective Guidance. We propose novel classifierbased multi-objective guidance strategy for discrete diffusion based on the Monte-Carlo Tree Search (MCTS) algorithm that selects sequence of optimal unmasking steps based on preceding iterations, samples new set of unmasked sequences to compute their objective scores, and finally back-propagates the scores in the path and updates set of global Pareto-optimal sequences. This strategy balances exploration of the peptide space through the pre-trained MDLM and multi-objective guidance through the MCTS guidance to enable the discovery of set of valid Pareto-optimal peptide sequences. We also provide case study for time-dependent multi-objective guidance strategy to enable prioritization of properties. 5. Peptide SMILES Property Prediction Toolkit. To supplement our multi-objective guidance strategy, we train suite of classification and regression models for property-prediction given peptide SMILES input. We train regression models to predict binding affinity to protein target using cross-multi-head attention and membrane permeability (log Pexp) using boosted trees. In addition, we train boosted trees for logistic regression of solubility, hemolysis, and non-fouling."
        },
        {
            "title": "2 Results",
            "content": "2.1 Unconditional Generator of Peptide SMILES with Masked Discrete Diffusion Given the immense success of masked language models (MLMs) in learning bidirectional relationships of sequential data [33], including peptide-protein interaction [18], we explore the recent Masked Diffusion Language Model (MDLM) formulation (Figure 1) [22]. MDLM is discrete diffusion architecture that leverages the MLM objective to effectively learn the clean distribution of sequences p(x) by reconstructing clean sequences from sequences corrupted with [MASK] tokens. To train the MDLM, we randomly sample values of Uniform(0, 1) such that each batch is masked for times ranging evenly between 0 (fully unmasked) and 1 (fully masked). For the forward masking at time t, the token at each position has αt probability remaining unchanged and (1 αt) probability of transitioning to [MASK] token. Therefore, the probability distribution of single sample assigned time in the forward diffusion process is given by q(ztx0) = Cat (zt; αtx0 + (1 αt)m) (1) where is one-hot encoding vector with 1 at the index of the [MASK] token. We specifically select the MDLM framework as our generative model to explicitly enforce SUBS parameterization such that once token is unmasked at time t, it remains unmasked at the same state for the remainder of the unmasking process for 0. Since slight modifications to the peptide sequence can result in significant alterations to its properties [34], SUBS parametrization allows us to backpropagate rewards from unmasked sequences to earlier unmasking steps that accurately reflect the favorability of the particular unmasking step in generating optimal peptide SMILES sequences. 3 Figure 1: PepMDLM. PepMDLM is discrete masked diffusion model for unconditional de novo generation of peptide SMILES representations. The backbone model used to generate the predicted probabilities xθ(zt, t) of transitioning from masked state to any token is predicted by backbone RoFormer architecture (See Appendix C.1). RoFormer leverages rotary positional embeddings (RoPE) to capture long-range dependencies between tokens [35] which effectively captures the relative inter-token interactions in peptide SMILES, especially for cyclic peptides. After empirical training, we find that even after convergence to training loss of 0.59 and validation loss of 1.41, the majority of generated SMILES were invalid peptides due to slight inaccuracies when generating the fundamental peptide backbone formed by peptide bonds between amino acid side chains. Motivated by these findings, we hypothesized that decreasing the masking rate for tokens forming peptide bonds at earlier time steps and masking them at later time steps in the training process would encourage the model to accurately unmask the peptide bonds first during generation before proceeding to fill in side-chain tokens. To this end, we conceptualized novel state-dependent masking schedule αt(x0) such that peptide bond tokens follow polynomial masking schedule that increases slower for small and rapidly approaches for close to 1. αt(x0) = (cid:26)1 tw x0 = x0 = 1 (2) With our state-dependent masking scheme, we derive the parameterized reverse diffusion distribution pθ(zszt) that defines the transition from the partially masked sequence zt at time to slightly unmasked sequence zs at time = 1 (Appendix B.2). pθ(zszt) = (cid:40)(cid:0)(cid:0) sw tw (cid:1) + ts 1(cid:1) zt xθ(zt, t)zs + (cid:0)(cid:0) sw tw t (cid:1) + 1(cid:1) xθ(zt, t)m zt = zt = (3) Furthermore, we derive continuous-time state-dependent NELBO loss function NELBO (Methods 4.3, Appendix B.3) that scales the loss of incorrect predictions for peptide-bond tokens by the exponent w, encouraging the model to predict peptide bond tokens at high confidence. In addition to enforcing peptide bond structure, we find that small syntactical errors during the sampling can result in completely invalid SMILES. Therefore, we introduce an additional invalid loss term Linvalid for when the sequence sampled from taking the argmax of the predicted probabilities for each training step corresponds to an invalid peptide SMILES based on our peptide SMILES validity filter (Methods 4.5). We scale the loss with the softmax of each of the sampled token probabilities to bypass the vanishing gradient when taking the argmax. With these enhancements, the loss converged after only two epochs of training on the dynamicallybatched set of 11 million peptide SMILES (Methods 4.1), with per-token training loss of 0.832 and validation loss of 0.880, but we trained for total of 8 epochs  (Table 1)  . Our optimized unconditional MDLM trained on 11 million peptide SMILES achieved high valid peptide generation rate of 45% with token length of 100 (15 amino acids) and 36% with token length of 200 (30 amino acids) when passed through our peptide SMILES validity filter that evaluates the presence of peptide bonds and canonical and non-canonical side-chains (Methods 4.5). Table 1: Training and validation loss after convergence on 11 million peptide SMILES with state-dependent masking and invalid loss. Model Train Loss () Train PPL () Val Loss () Val PPL ()"
        },
        {
            "title": "PepMDLM",
            "content": "0.832 2.460 0.880 2.277 2.2 PepMDLM Generates Diverse Chemically-Modified and Cyclic Peptides To evaluate the unconditional generation quality as benchmark for our guided generation model, we leverage the Moses metrics [36], including validity, uniqueness, diversity, and similarity to nearest neighbor (SNN) (Methods 4.5), to compare our model with an autoregressive generator of macrocyclic peptides, HELM-GPT [37]. In contrast to HELM-GPT, we use our peptide SMILES validity filter that checks not only that the SMILES string is valid molecule but also that contains peptide bonds and valid canonical and non-canonical side-chains. Overall, our model shows increased uniqueness and diversity while lower SNN, showing PepMDLMs capability of generating diverse peptides. Despite our lower validity, our model is unique to peptide SMILES, which has much higher granularity than HELM notation  (Table 2)  . Table 2: Benchmark of PepMDLM unconditional model against HELM-GPT. Model Validity () Uniqueness () Diversity () SNN () HELM-GPT PepMDLM 0.839 0. 0.913 1.000 0.595 0.705 0.975 0.513 1 The best scores are bolded. HELM-GPT was trained on HELM notation, where each token is monomer encoding natural and modified residues. Since there are no existing peptide SMILES generative models, we chose HELM-GPT as the closest comparison. The validity is assessed differently since we use our in-house peptide SMILES filter. Since the advantage of SMILES-based representation of peptides lies in its ability to represent chemically-modified and non-natural amino acids (nAAs), we compared the frequency of nAAs in the peptides generated by PepMDLM with the top 100 peptides with the highest log(Pexp) scores measuring lipophilicity in the CycPeptMPDB database [38] that contains total of 7334 labeled peptide SMILES with permeability scores between -8.0 (1.0 108cm/s) and -4.0 (1.0 104cm/s) generated from the parallel artificial membrane permeability (PAMPA) assay (Figure 2). We also compared to the dataset of experimentally tested canonical and non-canonical binding peptides from the PepLand dataset [39]. We found that the peptides present in both datasets had an average nAA frequency greater than two per peptide, indicating the significance of nAAs in defining various peptide properties. We show that PepMDLM generates nAAs from collection of over 200 nAAs from SwissSidechain [40] by passing them through our peptide sequence identification function, SMILES2PEPTIDE (Algorithm 7). This demonstrates PepMDLMs unique ability to design de novo peptides with 5 cyclic and nAA modifications, expanding the search space of therapeutic peptides well beyond any generative model trained on canonical amino acid representations. Permeability Data Binding Data PepMDLM Mean nAAs Per Peptide Cyclic Peptides (%) 2.215 0.467 2.150 0. 2.940 0.100 Figure 2: PepMDLM generates cyclic and modified peptides. (Above) Distribution comparison of nonnatural amino acid frequency for 100 unconditionally-generated peptide SMILES with the peptide SMILES dataset of experimentally-validated peptides for membrane permeability (PAMPA) and binding affinity (Methods 4.1). (Bottom) Per peptide frequency of non-natural amino acids (nAAs) and percentage of cyclic peptides in PepMDLM-generated sequences and experimentally-validated membrane-permeable peptides. 2.3 Multi-Objective Guidance on Therapeutic Properties for Discrete Diffusion To generate peptides with high clinical potential, they must achieve high binding affinity with protein target while optimizing for an array of therapeutic properties like membrane permeability to reach intracellular targets, solubility to improve drug-loading, and non-fouling and non-hemolysis to mitigate negative side-effects. Guiding the discrete diffusion objective is challenging due to the lack of data on property-specific peptides to train large generative models and the lack of gradients of the discrete sequence space. Although existing strategies have been explored such as gradient estimation and converting to continuous latent space [23, 26, 27], there has yet to be robust strategy that operates directly in the discrete space and can scale for various distinct properties without sacrificing performance in any one property. To address this gap, we introduce classifier-based multi-objective guidance strategy to generate set of non-dominated peptide SMILES using Monte Carlo Tree Search (MCTS) (Figure 3). sequence is non-dominated when no other sequence has strictly larger score in one or more objectives while maintaining equal scores for the remaining objectives. We leverage the unique capability of our unconditional MDLM model to sample from the diverse space of peptide SMILES and our trained property classifiers to search the constrained space of peptides with therapeutically optimal properties through an iterative selection, expansion, rollout, and backpropagation loop (Methods 4.4). The PepTune multi-objective guidance framework is defined as follows. We start each iteration from fully masked sequence zT , defining the root node of the MCTS tree. During the selection step, we traverse an optimal path through the MCTS tree defined as series of partial unmasking steps that have been previously traversed based on cumulative reward vector U(zs) (defined in Equation 21), indicating the likelihood that the given step generates Pareto non-dominated sequence. 6 Figure 3: PepTune. PepTune is multi-objective discrete diffusion model guided with Monte-Carlo Tree Search (MCTS). The full algorithm is detailed in Algorithm 2. Upon reaching terminal leaf node zt defined as partially masked sequence at time > 0 that has yet to be further unmasked, we expand the leaf node to explore different possible unmasking steps by batched Gumbel unmasking (Equation 23), which applies independently sampled Gumbel noise vectors to the token probabilities predicted by the MDLM backbone for sampling each child node zs,i for = 1 . . . , balancing diversity across unmasking schemes while following the predicted token distribution. Each expanded node is unmasked with greedy Gumbel-max sampling to obtain fully unmasked sequence xs,i. For all i, we compute K-dimensional score vector for objectives and compare it with the scores of the current pool of Pareto non-dominated sequences to generate reward vector r(xs,i) RK, where the entry rk(xs,i) at index is value in [0, 1] indicating the Pareto-optimality of the sequence for the kth objective compared to the current pool of Pareto non-dominated . rk(xs,i) = 1 P (cid:88) n=1 I(cid:2)sk(xs,i) sk(xn)(cid:3) (4) After computing the reward vector for all valid sequences generated during rollout, we take the sum of the reward vectors subtracted by penalty score proportional to the fraction of sampled SMILES that are not peptides and add the resulting K-dimensional vector to the cumulative reward vectors W(zt) of all successor nodes leading to the root node zT , which determines the selection steps of proceeding iterations. Even though the selection process favors high-reward unmasking steps, we show that the resulting pool of generated peptides retains similar uniqueness and diversity scores to the unconditional MDLM and the training dataset distribution  (Table 3)  . In addition, after only 20 iterations of the MCTS search algorithm, the fraction of valid peptides reaches 100%. Table 3: Evaluation metrics for generative quality of peptide SMILES sequences of max token length set to 200.1 Model SNN () Randomness () KL-Divergence () Validity () Uniqueness () Diversity () Data PepMDLM PepTune 1.000 0.450 1.000 1 The best scores are bolded. 1.000 1.000 1.000 0.885 0.705 0.677 1.000 0.513 0.486 4.55 4.11 4. 0 (Reference) 0.174 0.173 While MCTS has been integrated into autoregressive [41, 42] and inpainting [43] generative models, it has yet to be effectively applied to diffusion-based models. Here, we have provided robust 7 framework for MCTS-guided discrete diffusion that can extend beyond de novo peptide generation to other multi-objective generative tasks. 2.4 Therapeutic Property Prediction for Peptide SMILES While several classifiers exist for predicting properties of small-molecule SMILES sequences and amino-acid representations of peptides, there exists gap in high-quality property models trained specifically on peptide SMILES data. To fill this gap, we train regression models for target-binding affinity and permeability and binary classification models for solubility, hemolysis, and non-fouling specifically on peptide SMILES data  (Table 4)  . Table 4: Benchmarks of solubility, hemolysis, and non-fouling prediction for PeptideCLM and PeptideBERT embeddings. Each was trained using XGBoost for classification. Solubility Hemolysis Non-fouling"
        },
        {
            "title": "PeptideBERT",
            "content": "F1 Accuracy 0.660 0.661 0.597 0.651 0.846 0.846 0.483 0.823 0.768 0. 0.699 0.873 1 The optimal thresholds for the positive class were determined to be 0.500 for solubility, 0.800 for hemolysis (non-hemolysis), and 0.450 for non-fouling. To guide the generation of peptides with high binding affinity to given protein target, we trained Transformer-based model with cross multi-head attention layers that learn the joint latent space of ESM-2-650M [44] embeddings of the protein amino acid sequence and PeptideCLM [45] embeddings of the peptide SMILES sequence (Figure 4. See full architectural details in Appendix C.2). Given peptide SMILES sequence and protein amino acid sequence, the model was trained on non-canonical and canonical peptide SMILES Kd/Ki/IC50 binding affinity to predict score that indicates weak binding (< 6.0), medium binding (6.0-7.5), and high binding ( 7.5). Our regression model achieved strong Spearman correlation coefficient of 0.869 on the training data and 0.633 on the held-out validation data. Figure 4: Architecture of binding affinity regression model. Embeddings for the target protein sequence are generated with ESM-2 and embeddings for the peptide SMILES are generated using PeptideCLM. Cross multi-head attention layers combine the embeddings and predict binding affinity score. We also trained cell membrane permeability regression model by training an XGBoost [46] boosted tree model on PeptideCLM [47] embeddings which outputs PAMPA score (logP) given peptide SMILES sequence, where values 6.0 indicate strong permeability and values < 6.0 indicate weak permeability. We trained our model on 34,853 experimentally validated peptide SMILES (See Methods 4.1) and achieved strong Spearman correlation coefficient of 0.998 on the training dataset and 0.943 on the test dataset (Figure 5, Table 5). 8 Figure 5: Correlation plots for binding affinity and membrane permeability classifiers. Plot of true permeability (logP) on the x-axis and predicted permeability on the y-axis for the (A) validation set and (B) training set. Plot of true binding affinity (log-scale) on the x-axis and predicted permeability on the y-axis for the (C) validation set and (D) training set. Table 5: Held-Out Validation Performance of Binding Affinity and Membrane Permeability Regression Models Trained on Peptide SMILES. Metric Binding Affinity Membrane Permeability Spearman Rank Correlation MSE 0.633 0.566 1 Spearman rank correlation and MSE were calculated on the 20 percent held-out 0.943 0.088 validation set 2.5 Case Studies for Multi-Objective Generation of Peptide Binders With our trained property classifiers, we first conduct experiments for five diverse, therapeutically relevant protein targets to evaluate our multi-objective MCTS guidance strategy. To demonstrate generalizability, we include targets with known peptide binders such as GLP-1R and TfR and proteins with no known binders, including GFAP, GLAST, and AMHR2. These targets include both receptor proteins involved with active transport pathways as well as intracellular targets where cell membrane permeability is crucial to achieving therapeutic effects. For each target, we condition the generation on the binding affinity score given the target protein sequence along with solubility, hemolysis, nonfouling, and cell membrane permeability for intracellular targets. For external testing and validation, we use Autodock Vina [48] to compute in silico binding affinities of our generated binders (See Methods 4.5). 9 Figure 6: PepTune-generated peptide binders to TfR. (A) Density plot depicting the frequency of predicted binding affinity scores from our trained regression model for the sequences in the data used to train the regression model, the generated peptides from our unconditional PepMDLM model, and our PepTune model conditioned on TfR binding affinity, solubility, hemolysis, and non-fouling. (B) Plots depicting the mean scores for each property over the number of iterations or traversals of the MCTS algorithm for 128 iterations and maximum token length of 200. The shaded region represents the standard deviation. (C) Two-dimensional visualization of generated binders with token length 100, their corresponding docking scores () computed using Vina docking, and predicted scores () from the trained classifiers. (D) Visualizations of generated binders with token length 200, their docking scores, and predicted classifier scores. Targeting Receptors on the Blood-Brain Barrier. The Transferrin receptor (TfR) is receptor protein abundant on the selectively permeable blood-brain barrier (BBB) that is responsible for transporting iron-binding transferrin proteins into the brain parenchyma [49]. Given its selective expression on brain endothelial cells and glioma cells and its ability to recycle back to the luminal surface after facilitating the internalization of cargo through the BBB [50], TfR has been extensively studied as target for the intravenous delivery of various therapeutics and therapeutic nanocarriers through the BBB [51, 52]. To generate relevant binders for TfR, we condition PepTune on binding affinity with the TfR sequence, in addition to solubility, hemolysis, and non-fouling. At each iteration, we measured the mean of the properties scores across all rolled-out sequences from the selected node to evaluate the effectiveness of the optimization strategy. We show that all properties, except solubility, exhibited an upward trend over iterations, with the average score of the binding affinity classifier exhibiting significant increase in score to over 9.0 (Figure 6B). After plotting the distribution of 100 peptides generated from single run of PepTune with the minimum number of sequences set to 100, we confirm that our multi-objective MCTS algorithm shifted the distribution to higher predicted binding affinity than the unconditionally generated peptides (PepMDLM) and the data used to train the binding regression model (Figure 6A). Despite being conditioned on four distinct properties, PepTune is capable of generating higher affinity binders than the unconditional model, supporting the effectiveness of our multi-objective guidance strategy. Encouraged by these results, we sampled the Pareto-optimal sequences from the generated peptides and used Vina docking to compute their optimized docking score. Notably, we observed that all of the generated binders that were selected for docking produced affinity scores below -6.0 kcal/mol, with our top-performing binder achieving -8.4 kcal/mol binding affinity (Figure 6C). From the docking scores, we took the two binders with the best docking scores and visualized their binding conformation with TfR, showing that they bind to distinct motifs on the protein surface (Figure 6B, F, G). 10 Figure 7: Comparison of PepTune-generated peptides and established T7-peptide to TfR. Two-dimensional chemical structure of (A) PepTune-generated binder 1, (C) established T7 peptide, and (E) PepTune-generated TfR binder 2 and their Vina docking scores to TfR (). Zoomed-in visualization of the docked binding positions of (A) binder 1, (B) T7, and (C) binder 2 with TfR. Polar contacts within 3.5 Å are annotated, and shared contacts between T7 and binder 1 (purple) and between T7 and binder 2 (blue) are highlighted. (C) Overlay of peptide binders on full TfR protein To further confirm binding affinity to TfR, we compared our peptides to the well-established 7-amino acid peptide T7 (sequence: HAIYPRH) that selectively binds to an alternative site as compared to endogenous Tf on TfR [53]. T7 has been extensively explored for targeted delivery of nanoparticles to the brain [5460], and has demonstrated 7.89-fold enhanced brain penetration in in vivo mice models [61]. After docking T7 with TfR, we obtained docking score of -8.4 kcal/mol. Notably, our peptides optimized on all four therapeutic properties including TfR binding affinity show competitive docking scores to T7 (Figure 7A, C, E), suggesting that PepTune is capable of generating promising candidates for future in vivo delivery across the BBB. Furthermore, after annotating polar contacts within 3.5 Å we determine that both of the generated peptides with the best binding affinity scores have shared residue contacts when binding with TfR as T7 (Figure 7B, D, F), indicating that our generated peptides have superior binding properties to T7, enabling it to bind more strongly with the target despite shared binding site. Furthermore, our generated binders have diverse structural features, such as cycles in binder 1 and side-chain modifications in binder 2. Since T7 is known to bind to an alternative site than endogenous Tf [53], we show that PepTune can generate viable candidates for non-competitive binding to TfR for enhanced targeting. Targeting GLP-1R. Given the significant development of glucagon-like peptide-1 (GLP-1R) peptide agonists for the treatment of type-2 diabetes and obesity [4], we compared GLP-1R binding affinityconditioned peptides generated using PepTune with recent blockbuster GLP-1R agonists: semaglutide and liraglutide. Both semaglutide and liraglutide are over 30 amino acids in length and act by mimicking the binding of natural GLP-1 by binding to the activation pocket of GLP-1R with high precision (Figure 8) [62, 63]. Shorter agonists or antagonists to GLP-1R would serve several benefits to the treatment of insulinrelated disorders, including reduced cost and complexity of synthesis, lower immunogenicity, and faster tissue penetration. Therefore, we sought to generate shorter-chain peptides that are capable of binding to GLP-1R with comparable affinity to the existing agonists. We first generated pool of peptide binders conditioned on binding affinity with the GLP-1R sequence, solubility, hemolysis, and non-fouling. After selecting the peptides with the highest affinity scores from the Pareto-non11 Figure 8: Comparison of docked PepTune-generated peptides to existing GLP-1R agonists. (A, B) Docking images of semaglutide (score: -5.7 kcal/mol) and liraglutide (score: -5.1 kcal/mol) binding to GLP-1R. (C) Full view of the positive control GLP-1R agonists and the PepTune-generated binders on GLP-1R. (D, E) Docking images of binder 1 (score: -7.4 kcal/mol) and 2 (score: -7.0 kcal/mol) were generated using PepTune conditioned on predicted affinity to GLP-1R, solubility, hemolysis, and non-fouling. Shared polar contacts between binder 1 and either controls are highlighted in pink, shared polar contacts between binder 2 and either controls are highlighted in green, and the shared contacts across both binders are highlighted in purple. dominant set, we performed docking and determined affinities of -7.4 kcal/mol and -7.0 kcal/mol for the two best candidates. We note that our peptides have strong affinity to GLP-1R and while also interacting at motifs that strongly overlap with the binding sites of semaglutide and liraglutide derived from its natural hormone ligand, GLP-1 (Figure 8). These results suggest that our PepTune-derived peptides can serve as potent agonists or antagonists of GLP-1R signaling. Targeting Intracellular Proteins. Glial fibrillary acidic protein (GFAP) is an intracellular protein differentially expressed in astrocytes, family of glial cells in the brain [64]. Dysregulation of GFAP expression has been found to cause Rosenthal fibers, astrocytic cytoplasmic inclusions that are responsible for Alexander disease, fatal neurodegenerative disease affecting infants [65, 66]. Discovering potent binders that inhibit or degrade GFAP proteins can have significant therapeutic implications. However, no established peptide binders exist to GFAP, which motivates their de novo design. In addition to achieving high binding affinity with GFAP, we posit that an optimal peptide binder must also cross the astrocyte cell membrane into the cytosol to access GFAP. Therefore, we condition the generation of GFAP binders on five properties: binding affinity to GFAP, solubility, hemolysis, non-fouling, and cell membrane permeability using our permeability regression model, demonstrating optimization across all of these properties (Figure 9). To confirm GFAP engagement, our docking results demonstrate strong affinities of our designed peptides, motivating downstream experimental validation in astrocyte cultures (Figure 9B and D). Targeting Extracellular Proteins Without Existing Binders. To test the ability of our model to generate binders to challenging extracellular targets without existing binders, we evaluate PepTunegenerated peptides for NCAM1 and AMHR2, two therapeutically-relevant receptor proteins. Neural cell adhesion molecule 1 (NCAM1), is transmembrane protein expressed on the surface of neurons and glial cells [67]. Beyond its roles in neuronal migration and synaptogenesis, NCAM1 is also crucial for memory formation, highlighting its significance in brain development [68]. As NCAM1 is an extracellular protein, we generated library of peptides with PepTune optimized NCAM1 binding affinity, solubility, hemolysis, and non-fouling (Figure 10F, G). All properties exhibited an upward trend across optimization iterations. We selected two binders with the highest Vina docking scores for visualization (Figure 10A-E). Notably, in silico docking analysis revealed that binder 1 exhibits markedly high affinity binding (-8.6 kcal/mol) while binder 2 wraps around the NCAM1 structure via numerous polar contacts, suggesting extensive and specific interactions (Figure 10B and D). 12 Figure 9: PepTune-generated peptide binders to intracellular protein GFAP. (A, C) Two-dimensional structures of GFAP binder 1 and 2 with predicted property scores, including cell membrane permeability. (B, D) GFAP binders 1 and 2 docked to GFAP with score of -8.5 kcal/mol and -7.1 kcal/mol respectively. (E) Full GFAP protein structure with docked binders 1 and 2. (F) The distribution of PAMPA membrane permeability scores from 34,853 experimentally-validated peptides compared to 100 peptides generated using our unconditional PepMDLM model, and 100 peptides generated with PepTune conditioned on both cell membrane permeability and affinity to GFAP. The permeability curve shifted towards higher permeability with mean of -6.295. (G) Simultaneously, the distribution of predicted binding affinity scores to GFAP for the PepTune-generated peptides is shifted to higher scores with mean of 8.053 compared to set of experimentally-tested peptides and unconditional PepMDLM-generated peptides. Figure 10: PepTune-generated peptide binders to NCAM1. Two-dimensional structures of (A) binder 1 and (C) binder 2 genered with PepTune. Docking positions of (B) binder 1 and (C) binder 2 on NCAM1 with annotated polar contacts within 3.5 Å(G) Full NCAM1 protein structure with docked peptide binders 1 and 2. (H) (Top) Density plot of NCAM1 binding affinity scores for PepTune (mean: 6.708), PepMDLM (mean: 5.298), and peptides from control set of experimentally-tested peptide SMILES (mean: 5.360). (Bottom) Plots depicting the average predicted score for NCAM1 binding affinity, solubility, hemolysis, and non-fouling over iterations of MCTS. Anti-Müllerian hormone type-2 receptor (AMHR2) is transmembrane receptor involved in sex differentiation. Mutations in the AMHR2 gene are leading cause of Persistent Müllerian duct syndrome (PMDS) in males, resulting in the retention of female gonads alongside male reproductive structures [69]. In females, polymorphisms of AMHR2 have been associated with infertility [70, 71]. Most interestingly, antagonism of AMHR2 with therapeutic peptides can potentially serve as specific therapy for polycystic ovarian syndrome (PCOS), which affects an estimated 4% to 10% of women globally [72], as AMHR2 signaling has been implicated in follicular arrest and dysregulated ovarian function [73]. Following similar computational set-ups as described previously, we generated in silico binders with high Vina predicted binding affinities (<-6 kcal/mol), despite observing decrease in the predicted solubility along iterations (Figure 11). However, our observation of reduced solubility upon binder 13 Figure 11: PepTune-generated peptides to AMHR2. Two-dimensional structures of (A) binder 1 and (B) binder 2 generated with PepTune. Docking positions of (A) binder 1 and (B) binder 2 on NCAM1 with annotated polar contacts. (G) Full AMHR2 protein structure with docked peptide binders 1 and 2. (H) (Top) Density plot of AMHR2 binding affinity scores for PepTune (mean: 8.212), PepMDLM (mean: 6.832), and peptides from control set of experimentally-tested peptide SMILES (mean: 6.740). (Bottom) Plots depicting the average predicted score for AMHR2 binding affinity, solubility, hemolysis, and non-fouling over iterations of MCTS. docking can be attributed to the presence of hydrophobic patches within the AMHR2 extracellular domain, particularly near the binding site to its ligand AMH [74]. This phenomenon highlights the importance of balancing solubility and binding affinity in binder development. With further optimization of their therapeutic properties, we hope to demonstrate the potential of these binders for applications in fertility treatment in the future. The examples above demonstrate the versatility of our method, which can be effectively applied to discover peptide binders for single target proteins lacking known ligands, thereby unlocking their therapeutic potential. 2.6 PepTune Generates Multi-Target Specific Peptides Multi-target drug discovery is of significant interest in various fields including cancer therapeutics and drug delivery for neurological disorders given their ability to perform multiple different functions such as binding to biological barriers like the blood-brain barrier, penetrating target cells, and inhibiting protein-protein interactions [75], [76]. The design of dual-target drugs remains challenging across small-molecule, peptide, and protein domains due to the often contradictory structures and properties required for high affinity and specificity to multiple protein targets [75]. Traditional techniques involve performing subsequent rounds of phage display to discover candidates that bind to both targets, which does not explore the full space of potential candidates and often results in peptides that bind to one target but fail to bind to the other. Guided diffusion presents promising solution to de novo design of multi-target binding peptides; however, multi-target conditioning in the discrete sequence space remains under-explored. PepTune is uniquely positioned to tackle the multi-target optimization task since it can explore several different unmasking pathways while maintaining set of Pareto-optimal peptide sequences with non-dominated binding affinity scores with each of the protein targets. Our strategy enables conditioning on multiple target proteins to design binders with high affinity to both targets without sacrificing the discovery of peptides that bind strongly to only one of the targets since the model will return all non-dominant peptides. Targeting TfR and GLAST for Drug Delivery to Astrocytes. To evaluate PepTunes capabilities in multi-target guidance, we use PepTune to generate bi-specific peptide binders to TfR and glutamateaspartate transporter (GLAST) protein abundant on the surface of astrocytes, type of glial cell in the brain. Successfully generating these peptides can facilitate BBB-crossing via TfR binding and uptake in astrocytes via GLAST binding for intravenous delivery of therapeutics for multitude of neurological disorders where astrocytes are involved, including Alexander disease [77], Alzheimers 14 Figure 12: Property Scores Over Iteration for Dual-Target Conditioning on TfR and GLAST. (A) Plot of average predicted binding affinity score to GLAST over iterations. (B) Plot of average predicted binding affinity score to TfR over iterations. (C, D, E) Plot of average predicted solubility, hemolysis, and non-fouling scores over iterations. disease [78], Parkinsons disease [79], Huntingtons disease [80], multiple sclerosis [81], and several psychiatric disorders [82]. We generated pool of 100 peptide binders conditioned on five properties: predicted binding affinity to TfR, predicted binding affinity to GLAST, solubility, hemolysis, and non-fouling. Notably, we observed an increase across all properties over iterations, with the final solubility, hemolysis, and non-fouling scores surpassing the binders conditioned only on TfR binding affinity (Figure 12). This suggests that multi-target guidance does not result in significant trade-offs in property scores. To confirm that our generated binders indeed bind to both TfR and GLAST, we selected seven binders and conducted docking against TfR and GLAST separately for each binder. Incredibly, the docking scores across all seven binders were less than or equal to -7.5 kcal/mol for both targets, with the best-scoring binder simultaneously achieving score of -10.5 kcal/mol for TfR and score of -9.2 kcal/mol for GLAST  (Table 6)  . In addition, the top-performing binders have diverse secondary structures (Figure 13) and have positive solubility, and hemolysis probabilities  (Table 6)  . The binding positions and polar interactions vary greatly across the top-performing candidates, enabling the selection of binders that fit specific location constraints. This indicates that PepTune can discover wide subspace of optimal peptides with strong binding affinity to both TfR and GLAST that is not dependent on specific binding motif. Our next steps consist of validating the dual-binding affinity of our top candidates in an in vitro BBB-transwell model and observing whether both BBB-crossing and uptake into basolateral astrocytes are enhanced for dual-target compared to single-target conditioned and control peptides. Binder ID TfR Docking Score (kcal/mol) () GLAST Docking Score (kcal/mol) () Solubility () Hemolysis () Non-fouling () Table 6: PepTune-generated dual-target binders to TfR and GLAST. Binder 1 Binder 2 Binder 3 Binder 4 Binder 5 Binder 6 Binder 7 -8.8 (8.800) -8.0 (7.599) -8.3 (7.537) -7.6 (7.748) -10.5 (8.714) -8.4 (8.197) -9.3 (8.321) -8.9 (7.775) -7.9 (6.751) -8.2 (6.662) -7.5 (6.946) -8.5 (7.398) -7.5 (7.076) -9.2 (7.190) 0.975 0.938 0.972 0.959 0.811 0.971 0.881 0.743 0.835 0.914 0.902 0.748 0.855 0.860 0.118 0.309 0.214 0.290 0.202 0.165 0. 1 The predicted binding affinity scores by our trained classifier are placed in brackets beside the docking score. Larger scores indicate stronger binding for our classifier. Dual-Targeting of GFAP and an E3 Ubiquitin Ligase for Target Protein Degradation. As another dual-target case study, we used PepTune to generate peptides with high binding affinity to GFAP 15 Figure 13: PepTune-generated bi-specific peptides to TfR and GLAST. Full protein binding location and close-up binding position for (A) dual binder 1, (B) dual binder 6, and (C) dual binder 8 with TfR (left) and GLAST (right). Polar contacts within 3.5 Å are highlighted. Binder ID GFAP Docking Score (kcal/mol) () E3 Docking Score (kcal/mol) () Solubility () Hemolysis () Non-fouling () Table 7: PepTune-generated dual-target binders to GFAP and RBX1. Binder 1 Binder 2 Binder 3 Binder 4 -8.4 (7.468) -9.3 (7.089) -8.7 (7.158) -8.7 (7.000) 1 The predicted binding affinity scores by our trained classifier are placed in brackets beside the docking score. Larger scores indicate stronger binding for -8.0 (8.384) -8.3 (7.395) -7.3 (7.925) -8.8 (7.144) 0.730 0.972 0.935 0. 0.111 0.134 0.143 0.158 0.894 0.869 0.812 0.807 our classifier. protein and an E3 ubiquitin ligase protein RBX1, protein in the Skp1/Cullin-1/F-box (SCF) E3 ubiquitin ligase complex that recruits E2 to catalyze ubiquitination [83]. peptide generated for this task would be capable of binding to GFAP proteins overexpressed in Alexander disease and mediating their proteasomal degradation, which could alleviate the production of disease-causing Rosenthal fibers in astrocytes [84]. After conditioning PepTune generation on binding affinity to GFAP, binding affinity to RBX1, solubility, hemolysis, and non-fouling  (Table 7)  , we selected three non-dominated binders with predicted affinities greater than 7.0 for docking experiments. For these PepTune-generated peptides, we indeed observed strong binding affinities for both GFAP and RBX1 post-docking, indicating their unique potential for multi-target interaction (Figure 14). As note, GFAP is an intermediate filament protein [85], and thus forms unique rod-like structure with head domain and tail domain. The docking positions of all three candidates were along the rod domain, binding in the gap between adjacent rods in the filament. Contrarily, docked candidates to RBX1 consistently bound close to its interaction site of Cullin, rather than at the Skp2 F-box adaptor site (Figure 14), indicating that further motif conditioning, as done with recent peptide design language models [86], would benefit PepTunes clinical potential."
        },
        {
            "title": "3 Discussion",
            "content": "In this work, we introduce PepTune, generative framework that achieves multi-objective optimization directly in discrete sequence space. By leveraging MCTS for guidance, PepTune identifies Pareto-optimal peptide SMILES sequences conditioned on diverse therapeutic properties such as binding affinity, solubility, membrane permeability, and hemolysis. Unlike previous guidance methods, which struggle with gradient estimation [27] or rely on projections between continuous and discrete spaces, PepTune operates natively in the discrete latent space. Our approach combines exploration through batched unmasking and reward-based exploitation of classifier predictions, ensuring valid peptide structures with state-dependent masking schedule and straight-through estimator-based penalty. Most importantly, unlike recent binder design methods [12, 14, 15, 87], PepTune requires no obligate target three-dimensional structures or predicted structures (only the target sequence), 16 Figure 14: PepTune-generated peptides with dual GFAP and RBX1 affinity. Full protein binding location and close-up binding position for (A) dual binder 3, (B) dual binder 2, and (C) dual binder 4 with GFAP (left) and RBX1 (right). Polar contacts within 3.5 Å are annotated and shared polar contacts between binders are highlighted. enabling peptide design to conformationally diverse proteins, optimized for properties beyond local geometric interactions. Despite its strengths, PepTune relies on synthetic peptide data (e.g., CycloPs [88]) and rare nonnatural amino acids (nAAs) [40], which may increase synthesis complexity and costs. While we address this through feature-rich embeddings from pre-trained chemical language model [45, 89], improving high-quality labeled datasets remains critical for enhancing property prediction accuracy. Furthermore, while we evaluate binding using an external, state-of-the-art docking strategy via AutoDock Vina [48], there is lack of biophysical models for other properties optimized via PepTune, including peptide solubility, hemolysis, and membrane permeability, outside of existing predictor algorithms. As such, we are currently conducting in vitro assays to confirm these properties of our generated peptides, and will update the manuscript as these results are obtained. Our next steps are to leverage PepTune for clinically-relevant peptide generation. As concrete example in this manuscript, we have generated peptides to targets with functional relevance for Alexander disease [66, 65]. Building on our work developing peptide-guided degraders [17, 18, 90, 91], we will extend PepTune to generate bi-specific peptides that both bind to dysregulated GFAP and recruit various other E3 ubiquitin ligases, especially those that are differentially expressed in astrocytes. Such system would likely be superior to modalities like proteolysis-targeting chimeras (PROTACs), which are limited to only minimal, general set of E3 ubiquitin ligases and require either putative or cryptic binding pockets, which do not exist on large majority of disease-driving targets [92, 93]. Further, by optimizing sequences for BBB permeability (via TfR binding) and astrocytespecific uptake (via GLAST binding) as we have done here, PepTune may enable the design of complete, specific therapies for this challenging disease. Overall, this work establishes and advances new paradigm for peptide-based precision medicine, where multi-objective discrete optimization enables therapeutic peptide design with unprecedented control over functional properties."
        },
        {
            "title": "4 Methods",
            "content": "4.1 Data Curation and Tokenization. MDLM Training Data. To train the unconditional masked diffusion language model generator, we collected 11 million peptide SMILES consisting of 7451 sequences from the CycPeptMPDB database [94], 825,632 unique peptides from SmProt [95], and approximately 10 million modified peptides generated from CycloPs [88, 47], which consists of 90% canonical amino acids, 10% unnatural amino acids from SwissSidechain [40], 10% dextro-chiral alpha carbons, 20% N-methylated amine backbone atoms, and 10% PEGylated peptides. All possible cyclization conformations were attempted on the 17 peptides generated with CycloPs. We used SELFIES [96] to check the integrity of the SMILES sequences. We split our data by k-means clustering into 1000 groups of sequences with similar chemical properties based on their Morgan fingerprint [97], which is bit-vector representation of the full peptide sequence where each bit encodes feature relating to the SMILES atom types, connectivity, and bonding environment. The final dataset was 0.8 to 0.2 split based on the clusters, maintaining similar diversities of the SMILES strings. Since the degree of masking is evenly spread between = 0 to = 1 within each training batch, grouping similar SMILES in the same batch ensures the model learns to reconstruct diverse set of peptide SMILES from various degrees of masking. Dynamic Batching. We applied dynamic batching to handle variable-length token sequences and increase computational efficiency. Inspired by ESM-2s dynamic batching technique [44], input SMILES are sorted by length to maximize the utility of GPU memory. The maximum token size is 16k per GPU. SMILES Tokenization To enable the novel generation of non-natural amino acids containing cyclizations and diverse backbone and side-chain modifications, we trained our generative diffusion model on Simplified Molecular-Input Line-Entry System (SMILES) [32] representations of peptides. We experimented with several tokenization schemes that capture common motifs in the training data to enhance the generation of valid peptide SMILES. We find that the SMILES Pair Encoding (SPE) tokenization scheme [95] with the PeptideCLM [45] vocabulary of 581 SMILES tokens and 5 special tokens with an average length of 4 characters per token, demonstrated superior performance, generating precise but valid peptides (Appendix D.2). Classifier Data and Training. We trained our membrane permeability XGBoost regression model using 34,853 experimentally-validated peptide SMILES, with 22,040 SMILES sequences obtained from the ChEMBL database [98] and 7451 sequences from the CycPeptMPDB database [94]. We collected binding affinity, solubility, hemolysis, and non-fouling data for the classifier training [39, 30]. The binding affinity data was split based on binding affinity distributions with 0.8/0.2 ratio. The classifier was trained with cross-attention between ESM-2 protein embeddings and PeptideCLM SMILES representations [47]. The hyperparameters were chosen with 50 trials of OPTUNA search [99]. For other classifiers, data were randomly shuffled and split into 0.8/0.1/0.1 ratio for train, validation, and test. XGBoost classifiers [100, 101] were applied on PeptideCLM embeddings with 50 trials of OPTUNA search for the optimal tree hyperparameters. 4.2 Unconditional Masked Discrete Diffusion Model Notation. Let x0 {0, 1}V represent the one-hot vector of token in sequence in the training data and xθ(zt, t) be the vector of predicted token probabilities across the vocabulary given the current state zt at time t. In most contexts, x0 will be used to denote single token, but when discussing the full sequence, x(ℓ) is used to denote the token at position ℓ in the sequence. Let 0 denote the total number of time steps in the discrete forward and reverse diffusion processes and t(n) [0, 1] denote time step in the forward and backward diffusion process and let s(n) = t(n) 1 denote the previous time step with [1 . . . 1]. Then, let zt(n) and zs(n) denote the state of specific token at time t(n) and s(n) in the diffusion process, respectively. In some cases, t(n) is simplified to t. Let αt(x0) : RV denotes function that takes token x0 and outputs value in [0,1] representing the probability of remaining unmasked at time in the forward diffusion process. Let RV denote vector with ones at indices of peptide bond tokens. State-Dependent Masking Schedule. Since all peptides follow distinct SMILES structure consisting of un-modified or modified peptide bonds before and after each central carbon atom with an amino acid side chain, we hypothesized that applying state-dependent masking and unmasking schedules would allow the reverse diffusion process to learn to unmask the crucial structural components of peptide SMILES that are common across all peptides before filling in the segments between with diverse amino acid side-chains. Therefore, we devised masking schedule where the probability of masking token within peptide bond increases at slower rate in earlier times in the masking process compared to non-peptide bond tokens. To achieve this, we define the discrete-time log-linear masking schedule σ(t) = log(1 t) for non-peptide bond tokens and the log-polynomial masking schedule σ(t) = log(1 tw) for 18 peptide-bond tokens. We show in Appendix B.1 that the continuous-time probability of remaining unmasked at time in the forward diffusion process is given by the function αt(x0) : RV that takes the vector encoding the token x0 and returns probability. (cid:26)1 tw x0 = x0 = αt(x0) = 1 (5) where represents the set of one-hot encoding vectors for tokens that make up all types of peptide bonds, including all modified peptide-bond tokens. Since the probability of transitioning to [MASK] token at time is given by 1 αt(x0), there is lower probability tw for [0, 1] of masking peptide bond token than the probability of masking non-peptide bond token, especially in earlier time steps for smaller (Fig. 15A). As 1, αt(x0) 0 for both peptide and non-peptide bond tokens, ensuring that the model can learn to reconstruct the full token sequence during the reverse diffusion process. Therefore, we define the state-dependent forward transition matrix as q(ztx0) = Cat(zt; αt(x0)x0 + (1 αt(x0))m) (6) The reverse transition from state given the state-dependent forward masking schedule is derived in Appendix B.3 as q(zszt, x0) = (cid:40)(cid:0)(cid:0) sw tw (cid:1) + ts 1(cid:1) zt x0x0 + (cid:0)(cid:0) sw tw (cid:1) + 1(cid:1) x0m zt = zt = (7) To estimate the reverse posterior, we define parameterized model xθ(zt, t) : [0, 1] that takes the masked sample at time and predicts vector of token probabilities for the clean sample. Substituting = xθ(zt, t) gives us the following reverse diffusion transition distribution: pθ(zszt) = (cid:40)(cid:0)(cid:0) sw tw (cid:1) + ts 1(cid:1) zt xθ(zt, t)zs + (cid:0)(cid:0) sw tw (cid:1) + 1(cid:1) x0m zt = zt = (8) For larger w, peptide bonds are masked at later timesteps, encouraging earlier unmasking in the reverse diffusion process. However, setting too large can result in the model over-fitting to the dataset [102]. Empirically, we found that = 3 increased peptide validity while maintaining diversity across generated samples. SUBS Parameterization. Following Sahoo et al. [22], we parameterize the reverse diffusion model using SUBS parameterization, which enforces zero-masking probability and carry-over unmasking. This strategy enforces the constraints applied in the forward diffusion process and has been shown to minimize test perplexity [103]. 1. Zero Masking Probability. The forward process operates under the assumption that token can only be masked once across times = 0 1. It follows that the probability of sequence being masked in the reverse diffusion process is 0, we set the probability of the sequence being masked in the reverse diffusion model to be negative infinity such that if token is unmasked at time t, it remains unmasked for all timesteps from 0. xθ(zt, t), = 0 (9) 2. Carry-Over Unmasking. For each transition in the forward pass, all tokens either remain unchanged or are masked. Therefore, in the reverse process, when token is unmasked at time t, the unmasked token is carried over all time steps from 0. xθ(zt, t) = zt (10) 4.3 Loss Functions State-Dependent Continuous-Time Diffusion Loss. To optimize the parameters θ of the reverse diffusion model, we maximize the evidence lower bound (ELBO) of the distribution log p(x0), which 19 is the log-probability distribution of generating the peptide sequences x0 present in the dataset. Therefore, we define our loss function as the negative ELBO (NELBO) given by LNELBO = Eq(zt(1)x0) (cid:124) (cid:20) log pθ(x0zt(1)) (cid:123)(cid:122) reconstruction loss (cid:21) (cid:125) + Eq(zt(T ),zs(T )x0) (cid:124) (cid:20) log (cid:123)(cid:122) prior loss pθ(zt(T )) q(zt(T )zs(T )) (cid:21) (cid:125) + 1 (cid:88) n=1 (cid:124) Eq(zs(n),zt(n),zt(n+1)x0) (cid:20) log pθ(zs(n)zt(n)) q(zt(n)zs(n)) (cid:123)(cid:122) diffusion loss (cid:21) (cid:125) (11) Training on samples masked for continuous values of U(0, 1) yields tighter lower bound compared to discrete values of t. When the predicted probability distribution xθ(zℓ t, t) is exactly the one-hot encoding vector xℓ 0 for each token at position ℓ in the true sequence, the loss reduces to 0, which supports our objective. With the state-dependent masking schedule, we separate the summation into the sum of the negative log-losses (NLLs) for all non-peptide bond tokens that follow log-linear masking schedule and the sum of the NLLs for all peptide bond tokens that follow log-polynomial schedule. By our derivation in Appendix A.2, we derive the continuous-time state-dependent NELBO as NELBO = EtU (0,1]Eq(xtx0) (cid:20) (cid:88) ℓ:x(ℓ) 0 =b logx0, xθ(zt, t) (cid:21) logx0, xθ(zt, t) (cid:88) ℓ:x(ℓ) 0 =b 1 (12) Since the NLL term is minimized when the predicted probability of the ground truth token is close to 1, we show that applying the log-polynomial masking schedule for an exponent > 1 scales the diffusion loss NELBO by factor of from the log-linear schedule. However, for earlier timesteps as 0, both NLL weights increase to , ensuring high precision in the final unmasking steps. Given that peptide bonds form the fundamental backbone structure of peptide, our state-dependent masking strategy for peptide bonds acts as peptide bond loss that introduces higher penalty when the token predictions at positions of peptide bonds are inconsistent from the ground truth tokens during training, forcing the model to learn the fundamental structure of peptide SMILES strings in vast space of non-peptide SMILES strings. Invalid Peptide Loss. To further discourage the generation from predicting token logits that produce invalid peptide SMILES, we incorporate loss to penalize sampling of invalid peptide SMILES during training by taking the argmax of the predicted logits and passing the sampled SMILES string into filter that classifies it as valid or invalid peptide. However, since the argmax function is not differentiable, we use the softmax probability of the sampled token to scale the penalty score which acts as scalar multiplier in the loss function. Given sequence of one-hot encoding vectors x(ℓ) {0, 1}K of the tokens with the highest probability from the predicted logits x(ℓ) θ (zt, t), we aim to minimize the penalty generated by validity function that returns 0 when the argmax sampled sequence is valid peptide SMILES string and 1 when the sequence does not translate to peptide. To ensure differentiability, we define the loss function as the penalty multiplied by the softmax probability sampling the given token in an invalid sequence. Linvalid = = (cid:88) ℓ=1 (cid:88) ℓ=1 x(ℓ) 0 SM(cid:0)x(ℓ) θ (zt, t)(cid:1) 1[x0 is Invalid] exp(x(ℓ) θ,k) j=1 exp(x(ℓ) θ,j) (cid:80)K 1[x0 is Invalid] (13) (14) where 1[x0 is Invalid] denotes an indicator function that returns 1 when the sampled sequence x0 (cid:0)zt, t)(cid:1) is the token with the highest predicted is invalid and 0 if valid and = arg maxj(x(ℓ) θ probability at position ℓ of the sequence. 20 Differentiating the invalid loss with respect to the probability vector x(ℓ) θ (zt, t) for position ℓ, we derive the gradient with respect to the predicted probability of the sampled token = and all other tokens in the vocabulary = in Appendix B.1 as (cid:16) Linvalid = (cid:40) SM(x(ℓ) θ,k) SM(x(ℓ) (cid:17) 1 SM(x(ℓ) θ.k) θ,j)SM(x(ℓ) θ,k) = j = (15) Minimizing this objective function updates the parameters to lower the predicted probabilities for tokens that resulted in invalid peptide SMILES and increase the probabilities of the remaining tokens while maintaining consistent relative probability distribution. Training. To train the MDLM to accurately approximate the true reverse transition distribution q(zszt, x0) of training sample x0 for all continuous timesteps = 1 0, we train parameterized model xθ(zt, t) that takes the current sequence zt and returns V-dimensional vector of predicted probabilities of transitioning to each token at time < (Algorithm 1). For each training batch B, we randomly sample values Uniform(0, 1) and off-set each time by to get = (t + δ) mod 1 to ensure the model learns to regenerate δ = the clean sample zt for wide range of time steps. After applying state-dependent masking to each training sequence and obtaining the predicted probabilities xθ(zt, t) and the discrete sequence x0 from greedy sampling, we minimize the total loss function given by , . . . , B1 , (cid:105) , 1 0, 1 (cid:104) = NELBO + Linvalid 1 (cid:88) (cid:18) 1 (cid:88) = ztB ℓ:xℓ=b logxθ(zℓ t), xℓ + (cid:88) ℓ:xℓ=b logxθ(zℓ t), xℓ (16) (17) x(ℓ) 0 SM(cid:0)x(ℓ) (cid:19) θ (zt, t)(cid:1) 1[x0 is Invalid] + (cid:88) ℓ=1 By increasing batch size and applying dynamic batching as described in (Methods 4.1, we obtain tighter ELBO of the true distribution log p(x0). The model used to generate the validation results in this manuscript is trained on our in-house 8A6000 Nvidia GPUs (50G memory) for 1600 GPU hours using the AdamW optimizer with learning rate of 0.0003 and weight decay of 0.075. Sampling. To sample from the unconditional PepMDLM model, we start with sequence of length of only [MASK] tokens. We first compute the diffusion time steps { 1 , . . . , 1} where = 128. From the predicted token probabilities xθ(zt, t) generated by feeding zt through the trained RoFormer backbone, we compute the reverse transition token distribution pθ(zszt) following Equation (8) and perform Gumbel-max sampling to get the next token zs. , 2 zs arg max (cid:18) log pθ(zszt) (cid:19) (G RV) Gi = log( log(ui + ϵ) + ϵ) (ui Uniform(0, 1)) where Gi is the Gumbel noise applied to token and ϵ = 1e 10. Then, we return the newly sampled tokens only when zt = m, while keeping all unmasked tokens unchanged. After timesteps, we obtain fully unmasked sequence x. 4.4 Multi-Objective Guidance for Discrete Diffusion In this section, we will define the multi-objective discrete diffusion problem. Then, we will introduce the concept of Pareto optimal sequence and Monte Carlo Tree Search (MCTS). Finally, we will break down our framework of integrating MCTS to iteratively find Pareto optimal solutions using our uniform discrete diffusion model. Pareto Optimization. When optimizing sequences for multiple objectives (e.g. affinity to multiple protein targets, stability, expression yield, etc.), there is likely no single best sequence that achieves the highest score across all objectives. Optimizing one objective often leads to sacrificing performance on another objective. Therefore, we focus on finding set of Pareto optimal sequences that minimize the trade-offs between objectives to achieve overall optimal performance across all objectives. Formally, Pareto-optimal 21 sequences (or non-dominated solutions) cannot be further optimized in any single objective without sacrificing performance in another objective. Let s(x) = [s1(x), . . . , sK(x)] RK be vector of scores that measures the performance of sequence in different objectives, with higher scores indicating better performance. sequence is said to dominate another sequence (denoted as x) if and only if it satisfies the following property. For all objectives [1, K], the score for the kth objective for is greater than or equal to the score for the kth objective for and for at least one objective k, the score for is strictly greater than the score for z. s(x) s(x) (cid:125) (cid:123)(cid:122) (cid:124) dominates iff [1, K] sk(x) sk(x) (cid:125) (cid:123)(cid:122) (cid:124) is no worse than in any objective [1, K] sk (x) sk (x) (cid:124) (cid:125) (cid:123)(cid:122) is strictly better than in at least one objective (18) Pareto-optimal sequence is sequence where there does not exist another sequence in the current Pareto-optimal set that dominates it. Since there are trade-offs between objectives, this does not mean that is dominant to all other sequences. s.t. s(x) s(x) (19) We define the Pareto frontier (PF) as the set K-dimensional objective score vectors corresponding to each Pareto-optimal sequence. = (cid:8)(cid:0)x, s(x)(cid:1) L s(x) s(x)(cid:9) (20) Since infinitely many trade-offs can exist between the objectives, there can be an infinite number of Pareto-optimal sequences. Therefore, multi-objective optimization aims to approximate finite set of Pareto-optimal sequences with reasonable number of iterations. Notation. Here, we let zt denote the partially unmasked sequence at time t. zt also corresponds to node in the MCTS tree with set of children nodes denoted as children(zt) = {zs,1, . . . , zs,M } indexed from [1 . . . ]. Each child node is itself partially unmasked sequence at time derived from sampling the MDLM reverse posterior pθ(zszt). The children nodes at each iteration of MCTS are rolled out into set of clean sequences denoted as {xs,1, . . . , xs,M }, for each of which we compute score vector s(xs,i) RK and rewards vector r(xs,i) RK, where is the number of objectives guiding the MCTS search. Let = {xn} be the set of Pareto-optimal sequences indexed [1 . . . ], which is updated at each iteration. At each node zt, we store cumulative rewards vector W(zt) and counter for the number of times the node has been visited across all iterations Nvisit(zt). Finally, we denote the iteration index as [1 . . . Niter], where Niter is the total number of search iterations. Initialization. We initialize sequence of length with only [MASK] tokens as the root node of the MCTS tree at = 0 and an empty set that will maintain clean sequences with Paretooptimal score vectors. We initialize vector of scoring functions : RK that takes clean sequence xs,i generated from the partially masked sequence zs,i and outputs vector of real values s(xs,i) RK that measures its performance in each of the objectives. We also set the hyperparameters, including the number of iterations Niter, the number of children , and the length of the token sequence L. At each iteration, four steps are performed to update the set of Pareto optimal solutions: traversing the tree by selecting the best child node until reaching leaf node (selection), expanding the leaf node into distinct partially unmasked sequences (expansion), fully unmasking each child node into clean sequence and computing multi-objective scores and rewards (rollout), and finally backpropagating the rewards to the parent nodes to guide the selection process at the next iteration (backpropagation). Selection. At each iteration, we traverse the tree starting at the root node (fully masked sequence) zT and selecting child node based on an edge score U(zt, zs,i) that balances child nodes that generate high reward sequences from previous iterations and unexplored unmasking actions that could lead to larger pool of diverse sequences. From parent node zt, we compute the selection score vector RK, where is the number of objectives, for each child node zs,i by normalizing the cumulative rewards vector by the number of times the node has been visited. This ensures that we select an optimal unmasking step based on previous rollout rewards without biasing towards highly visited nodes. U(zs,i) = W(zs,i) Nvisit(zs,i) 22 (21) Then, we select uniformly at random from the pool of children nodes zs,i select whose reward vectors are non-dominant, such that there doesnt exist another child zs,j where the rewards across each of the K-objectives are equal to the reward of zs,i and there exists reward strictly greater than zs,i. select = {zs,i zs,j children(zt) s.t. r(zs,j) r(zs,i)} If the selected node is non-leaf node, the loop repeats with the selected node zs,i as the new parent node. Once leaf node is reached, the loop ends and the next step executes. (22) Expansion. At the iteration at time t, we sample sequences from the reverse posterior pθ(zszt) defined in Equation (8) to get set of partially masked sequences which form the set of children nodes of zt: children(zt) = {zs,1, . . . , zs,M }. All the children nodes are added to the tree. To ensure that the expansion step results in distinct unmasking steps, we experimented with two different batched unmasking techniques from the single partially masked sequence at parent node. For the first method, we repeated the array corresponding to the parent node tokens over dimensions and added independently sampled Gumbel noise values Gs,i,j, where denotes the sequence in the batch, denotes the position in the sequence, and denotes the token index. (cid:1) + Gs (cid:0)zszt) = log pθ (cid:0)zszt (23) pθ Gs = log ( log(us,i,j + ϵ) + ϵ) us,i,j Uniform[0, 1) (24) where pθ denotes the modified probability distribution after applying Gumbel noise and Gs RLV is matrix of independently sampled noise values. Then, for each position in the sequence, we take the arg max index of the token with the highest probability at each position and each sequence in the batch and use it to partially unmask the tokens in zt. The second method involves taking the softmax (denoted as SM) across the top probabilities after applying Gumbel noise and drawing random samples from the re-normalized softmax distribution over only the top most probable tokens. (cid:18) (cid:0)zszt) = SM pθ topk(cid:8) log(pθ (cid:0)zszt)(cid:1) + Gs (cid:9) (cid:19) (25) Rollout. From each child node generated at time s, we completely unmask the sequence by greedily sampling the arg max tokens from the predicted Gumbel-max distribution pθ(zszt) at all remaining time steps from 0 to get set of clean sequences {xs,1, . . . , xs,M } of SMILES tokens. For each clean sequence xs,i, we feed it as input to the scoring functions for each of the objectives to generate the score vector s(xs,i) = (cid:2)s1(xs,i), . . . , sK(xs,i)(cid:3) RK. Then, we use the score vector to compute vector of rewards r(xs,i) = (cid:2)r1(xs,i), . . . , rK(xs,i)(cid:3) RK. The reward of child node sequence for the kth objective is the fraction of the peptides in the current set of Pareto-optimal sequences where the child node has higher classifier score for the given objective. Specifically, the reward for the ith child node zs,i and the resulting unmasked sequence xs,i for the kth objective is computed as rk(xs,i) = 1 P (cid:88) n= I(cid:2)sk(xs,i) sk(xn)(cid:3) (26) is an indicator function that returns 1 if the score for the kth objective of the child node is greater than or equal to the score of the nth sequence in the set of Pareto-optimal sequences. In parallel to computing the reward, we add all non-dominated children sequences to the set of Pareto optimal sequences and remove all dominated sequences. = (cid:8)(zs,i, s(xs,i)) s(xs,i) s(x)(cid:9) = (cid:8)x xs,i children(zt) s.t. s(xs,i) s(x)(cid:9) In Appendix D.1, we show proof-of-concept for time-dependent multi-objective guidance strategy where the update to the Pareto-optimal set depends on the rewards for only subset of objectives (27) (28) 23 that changes depending on the current iteration, enabling the prioritization of properties with larger influence on peptide structure and function in earlier iterations and fine-tuning on additional properties in later iterations. Back-propagation. At each child node zs,i, the reward vector r(xs,i) is used to initialize the cumulative reward vector W(zs,i), and the number of visits Nvisits(zs,i) is initialized to 1. W(zs,i) rk(xs,i) Nvisit(zs,i) 1 (29) (30) Then, we backtrack through the parent nodes of zs,i up to the root node zT , adding the child reward vector to the cumulative reward vector and incrementing the number of visits for each node in the path. zs,i parent(zs,i) = zt W(zt) W(zt) + (cid:88) i=1 r(xs,i) Nvisit(zt) (zt) + 1 (31) (32) (33) These updated scores are used to guide the selection process in the next iteration such that the unmasking paths that result in the highest reward sequences have greater chance of being selected and explored further. Penalizing Invalid Peptides. To discourage the selection process from choosing unmasking steps that result in invalid or unsynthesizable peptide SMILES, we subtract penalty score calculated as the fraction of invalid SMILES sequences rolled out from the expanded children of parent node determined based on our peptide SMILES validity filter described in Section 4.1. Since properties are irrelevant if the peptide SMILES is invalid, we subtract the penalty scaled by constant from all dimensions of the cumulative reward vectors. This penalty-adjusted reward vector is backpropagated to all parent nodes to avoid paths leading to high invalid rates in the next iteration. After empirical experimentation, we determined that setting the constant = 0.5 leads to an increase in the fraction of valid peptides over iterations up to 100% validity. W(zt) W(zt) + (cid:88) i=1 r(zs,i) (cid:19) (cid:18) Ninvalid (34) Output. The output after Niter iterations is the set of Pareto-optimal sequences across the objectives. Our strategy simultaneously guides the de-noising process towards multiple objectives while exploring the space of peptide sequences using the pre-trained unconditional MDLM without the need to approximate conditional probability distribution or perform gradient iterations in the continuous space. Furthermore, we generate an entire set of Pareto-optimal sequences from single pass through the model which is optimal across total of Niter total sequences sampled across all iterations. 4.5 Evaluation Peptide Validity Filter. Among the sequential representations of peptides including amino acid sequences, HELM [104], and SMILES [32], SMILES is the most intricate representation of peptide sequences. Although this enables the representation of non-natural amino acids, diverse side-chain, and backbone modifications, and cyclic peptides, it also means that the vast majority of SMILES strings are not synthesizable peptides. Therefore, we devised an algorithm that determines whether SMILES string is valid peptide, characterized by peptide bonds and central carbon atoms. The filter first checks if the SMILES sequence is valid molecule using RDKit [105]. Then we check whether the generated SMILES is peptide or not by its peptide bonds and N-methylated peptide bonds and determine whether the peptides are cyclic. We use regular expressions to detect bond patterns for peptide bonds, N-methylated peptide bonds, reversed peptide bonds, and ester bonds, along the sequence to split the sequence into several segments with bond before and after each segment. The filter then checks each segment for chemical modifications based on their bond type, including N-methylation (N-Me) and O-linked glycosylation. The remaining segment is matched to the corresponding amino acid side chains for either natural or non-natural sources. The tool is freely available on HuggingFace: https: //huggingface.co/spaces/ChatterjeeLab/SMILES2PEPTIDE. Generation Quality Metrics. To evaluate the generation quality of our unconditional MDLM, PepMDLM, and our MCTS-guided MDLM, PepTune, we leverage the Moses metrics [36]. Validity is the percentage of the generated SMILES resulting in valid peptide SMILES based on our validity filter that first checks if the string is valid molecule SMILES, then checks for the presence of unmodified and modified peptide bonds within the sequence. Uniqueness is the fraction of distinct peptide SMILES sequences among the valid peptide SMILES. Diversity is calculated as one minus the Tanimoto similarity between the Morgan fingerprints for every pair of generated sequences. Similarity to nearest neighbor (SNN) takes the maximum Tanimoto similarity score for each generated sequence with any other sequence and averages across all generated sequences to measure the similarity on average that sequence to its closest neighbor. Due to the limit of memory and CPU time required to load all the training dataset of 11 million peptide SMILES, we chose to sample subset of 1000 batches randomly (100k sequences) for diversity and SNN calculation. To assess the novelty of generated sequences, we employed Shannon entropy [106] to quantify the SMILES token randomness between 100 conditionally and 100 unconditionally generated sequences and the same randomly sampled 1000 subsets from the training set. Then Kullback-Leibler (KL) divergence was also used to evaluate divergence across token distributions. The equations for all metrics are provided in Appendix C.5. Docking. For generated SMILES that were identified as peptides by our filter and fulfilled the targeted properties, we used Autodock Vina [48] (v 1.1.2) for in silico docking experiment between the target proteins and the ligands. Targets were preprocessed with MGITools [107] (v 1.5.7) and the conformations of the SMILES were optimized by ETKDG from RDKit [48, 108]. The final results were visualized in PyMol [109] (v 3.1) with polar contacts within 3.5 Å being labeled."
        },
        {
            "title": "5 Declarations",
            "content": "Acknowledgments. We thank the Duke Compute Cluster, Pratt School of Engineering IT department, and Mark III Systems, for providing database and hardware support that have contributed to the research reported within this manuscript. We thank Alexander Tong for reviewing the theoretical formulations of PepTune. We also thank Sophia Vincoff and Lauren Hong for assistance with figure generation. Author Contributions. S.T. devised and developed PepTune architecture and theoretical formulations, and trained and benchmarked generation, prediction, and sampling models. Y.Z. advised on model design and theoretical framework, trained classifier models, and performed molecular docking. S.T. drafted the manuscript and S.T. and Y.Z. designed the figures. P.C. conceived, designed, supervised, and directed the study, and reviewed and finalized the manuscript. Data and Materials Availability. Our peptide filtering, analysis, and visualization tool, SMILES2PEPTIDE, is freely available on HuggingFace: https://huggingface.co/spaces/ ChatterjeeLab/SMILES2PEPTIDE. The PepTune codebase is freely accessible to the academic community via non-commercial license at https://huggingface.co/ChatterjeeLab/ PepTune. Funding Statement. This research was supported by NIH grant R35GM155282 as well as gift from the EndAxD Foundation to the lab of P.C. Competing Interests. P.C. is co-founder of Gameto, Inc. and UbiquiTx, Inc. and advises companies involved in peptide therapeutics development. P.C., S.T., and Y.Z. have and are currently filing patent applications related to this work. P.C.s interests are reviewed and managed by Duke University in accordance with their conflict-of-interest policies."
        },
        {
            "title": "References",
            "content": "[1] C. V. Dang, E. P. Reddy, K. M. Shokat, and L. Soucek, Drugging the undruggable cancer targets, Nature Reviews Cancer, vol. 17, p. 502508, June 2017. 25 [2] L. Wang, N. Wang, W. Zhang, X. Cheng, Z. Yan, G. Shao, X. Wang, R. Wang, and C. Fu, Therapeutic peptides: current applications and future directions, Signal Transduction and Targeted Therapy, vol. 7, Feb. 2022. [3] I. Petta, S. Lievens, C. Libert, J. Tavernier, and K. De Bosscher, Modulation of proteinprotein interactions for the development of novel therapeutics, Molecular Therapy, vol. 24, p. 707718, Apr. 2016. [4] N. Alfaris, S. Waldrop, V. Johnson, B. Boaventura, K. Kendrick, and F. C. Stanford, Glp-1 single, dual, and triple receptor agonists for treating type 2 diabetes and obesity: narrative review, eClinicalMedicine, vol. 75, p. 102782, Sept. 2024. [5] J. M. Friedman, The discovery and development of glp-1 based drugs that have revolutionized the treatment of obesity, Proceedings of the National Academy of Sciences, vol. 121, Sept. 2024. [6] P.-F. Laterre, S. M. Berry, A. Blemings, J. E. Carlsen, B. François, T. Graves, K. Jacobsen, R. J. Lewis, S. M. Opal, A. Perner, P. Pickkers, J. A. Russell, N. A. Windeløv, D. M. Yealy, P. Asfar, M. H. Bestle, G. Muller, C. Bruel, N. Brulé, J. Decruyenaere, A.-M. Dive, T. Dugernier, K. Krell, J.-Y. Lefrant, B. Megarbane, E. Mercier, J.-P. Mira, J.-P. Quenot, B. S. Rasmussen, H.-C. Thorsen-Meyer, M. Vander Laenen, M. L. Vang, P. Vignon, I. Vinatier, S. Wichmann, X. Wittebole, A. L. Kjølbye, and D. C. Angus, Effect of selepressin vs placebo on ventilatorand vasopressor-free days in patients with septic shock: The sepsis-act randomized clinical trial, JAMA, vol. 322, p. 1476, Oct. 2019. [7] C. Fu, Q. Chen, F. Zheng, L. Yang, H. Li, Q. Zhao, X. Wang, L. Wang, and Q. Wang, Genetically encoding lipidated amino acid for extension of protein half-life in vivo, Angewandte Chemie International Edition, vol. 58, p. 13921396, Dec. 2018. [8] J. Li, K. Yanagisawa, and Y. Akiyama, Cycpeptmp: enhancing membrane permeability prediction of cyclic peptides with multi-level molecular features and data augmentation, Briefings in Bioinformatics, vol. 25, July 2024. [9] X. Ji, A. L. Nielsen, and C. Heinis, Cyclic peptides for drug development, Angewandte Chemie, vol. 136, Oct. 2023. [10] M. Muttenthaler, G. F. King, D. J. Adams, and P. F. Alewood, Trends in peptide drug discovery, Nature Reviews Drug Discovery, vol. 20, p. 309325, Feb. 2021. [11] A. A. Vinogradov, Y. Yin, and H. Suga, Macrocyclic peptides as drug candidates: Recent progress and remaining challenges, Journal of the American Chemical Society, vol. 141, p. 41674181, Feb. 2019. [12] S. A. Rettie, D. Juergens, V. Adebomi, Y. F. Bueso, Q. Zhao, A. N. Leveille, A. Liu, A. K. Bera, J. A. Wilms, A. Üffing, A. Kang, E. Brackenbrough, M. Lamb, S. R. Gerben, A. Murray, P. M. Levine, M. Schneider, V. Vasireddy, S. Ovchinnikov, O. H. Weiergräber, D. Willbold, J. A. Kritzer, J. D. Mougous, D. Baker, F. DiMaio, and G. Bhardwaj, Accuratede novodesign of high-affinity protein binding macrocycles using deep learning, bioRxiv, Nov. 2024. [13] P. Bryant and A. Elofsson, Peptide binder design with inverse folding and protein structure prediction, Communications Chemistry, vol. 6, Oct. 2023. [14] M. Pacesa, L. Nickel, C. Schellhaas, J. Schmidt, E. Pyatova, L. Kissling, P. Barendse, J. Choudhury, S. Kapoor, A. Alcaraz-Serna, Y. Cho, K. H. Ghamary, L. Vinué, B. J. Yachnin, A. M. Wollacott, S. Buckley, A. H. Westphal, S. Lindhoud, S. Georgeon, C. A. Goverde, G. N. Hatzopoulos, P. Gönczy, Y. D. Muller, G. Schwank, D. C. Swarts, A. J. Vecchio, B. L. Schneider, S. Ovchinnikov, and B. E. Correia, Bindcraft: one-shot design of functional protein binders, Cold Spring Harbor Laboratory, Oct. 2024. [15] Q. Li, E. N. Vlachos, and P. Bryant, Design of linear and cyclic peptide binders of different lengths from protein sequence information, bioRxiv, June 2024. 26 [16] V. N. Uversky, C. J. Oldfield, and A. K. Dunker, Intrinsically disordered proteins in human diseases: Introducing the d2concept, Annual Review of Biophysics, vol. 37, p. 215246, June 2008. [17] S. Bhat, K. Palepu, L. Hong, J. Mao, T. Ye, R. Iyer, L. Zhao, T. Chen, S. Vincoff, R. Watson, T. Wang, D. Srijay, V. S. Kavirayuni, K. Kholina, S. Goel, P. Vure, A. J. Desphande, S. H. Soderling, M. P. DeLisa, and P. Chatterjee, De novodesign of peptide binders to conformationally diverse targets with contrastive language modeling, Cold Spring Harbor Laboratory, June 2023. [18] T. Chen, M. Dumas, R. Watson, S. Vincoff, C. Peng, L. Zhao, L. Hong, S. Pertsemlidis, M. Shaepers-Cheu, T. Z. Wang, D. Srijay, C. Monticello, P. Vure, R. Pulugurta, K. Kholina, S. Goel, M. P. DeLisa, R. Truant, H. C. Aguilar, and P. Chatterjee, Pepmlm: Target sequenceconditioned generation of therapeutic peptide binders via span masked language modeling, arXiv, 2023. [19] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. v. d. Berg, Structured denoising diffusion models in discrete state-spaces, Advances in Neural Information Processing Systems, 2021. [20] A. Lou, C. Meng, and S. Ermon, Discrete diffusion modeling by estimating the ratios of the data distribution, International Conference on Machine Learning, 2024. [21] J. Shi, K. Han, Z. Wang, A. Doucet, and M. K. Titsias, Simplified and generalized masked diffusion for discrete data, arXiv, 2024. [22] S. S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. T. Chiu, A. Rush, and V. Kuleshov, Simple and effective masked diffusion language models, Advances in Neural Information Processing Systems, 2024. [23] I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Q. Chen, G. Synnaeve, Y. Adi, and Y. Lipman, Discrete flow matching, Advances in Neural Information Processing Systems, 2024. [24] J. Rector-Brooks, M. Hasan, Z. Peng, Z. Quinn, C. Liu, S. Mittal, N. Dziri, M. Bronstein, Y. Bengio, P. Chatterjee, A. Tong, and A. J. Bose, Steering masked discrete diffusion models via discrete denoising posterior prediction, arXiv, 2024. [25] O. Davis, S. Kessler, M. Petrache, I. I. Ceylan, M. Bronstein, and A. J. Bose, Fisher flow matching for generative modeling over discrete data, Advances in Neural Information Processing Systems, 2024. [26] H. Stark, B. Jing, C. Wang, G. Corso, B. Berger, R. Barzilay, and T. Jaakkola, Dirichlet flow matching with applications to dna sequence design, Proceedings of the 41st International Conference on Machine Learning, 2024. [27] H. Nisonoff, J. Xiong, S. Allenspach, and J. Listgarten, Unlocking guidance for discrete state-space diffusion and flow models, arXiv, 2024. [28] A. Campbell, J. Yim, R. Barzilay, T. Rainforth, and T. Jaakkola, Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design, arXiv, 2024. [29] X. Wang, Z. Zheng, F. Ye, D. Xue, S. Huang, and Q. Gu, Diffusion language models are versatile protein learners, arXiv, 2024. [30] C. Guntuboina, A. Das, P. Mollaei, S. Kim, and A. Barati Farimani, Peptidebert: language model based on transformers for peptide property prediction, The Journal of Physical Chemistry Letters, vol. 14, p. 1042710434, Nov. 2023. [31] W.-F. Zeng, X.-X. Zhou, S. Willems, C. Ammar, M. Wahle, I. Bludau, E. Voytik, M. T. Strauss, and M. Mann, Alphapeptdeep: modular deep learning framework to predict peptide properties for proteomics, Nature Communications, vol. 13, Nov. 2022. [32] D. Weininger, Smiles, chemical language and information system. 1. introduction to methodology and encoding rules, Journal of Chemical Information and Computer Sciences, vol. 28, p. 3136, Feb. 1988. 27 [33] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv, 2018. [34] W. M. Hewitt, S. S. F. Leung, C. R. Pye, A. R. Ponkey, M. Bednarek, M. P. Jacobson, and R. S. Lokey, Cell-permeable cyclic peptides from synthetic libraries inspired by natural products, Journal of the American Chemical Society, vol. 137, p. 715721, Jan. 2015. [35] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, Roformer: Enhanced transformer with rotary position embedding, arXiv, 2021. [36] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov, A. Kadurin, S. Johansson, H. Chen, S. Nikolenko, A. Aspuru-Guzik, and A. Zhavoronkov, Molecular sets (moses): benchmarking platform for molecular generation models, Frontiers in Pharmacology, vol. 11, Dec. 2020. [37] X. Xu, C. Xu, W. He, L. Wei, H. Li, J. Zhou, R. Zhang, Y. Wang, Y. Xiong, and X. Gao, Helm-gpt: de novo macrocyclic peptide design using generative pre-trained transformer, Bioinformatics, vol. 40, June 2024. [38] J. Li, K. Yanagisawa, M. Sugita, T. Fujie, M. Ohue, and Y. Akiyama, CycPeptMPDB: comprehensive database of membrane permeability of cyclic peptides, J. Chem. Inf. Model., vol. 63, pp. 22402250, Apr. 2023. [39] R. Zhang, H. Wu, Y. Xiu, K. Li, N. Chen, Y. Wang, Y. Wang, X. Gao, and F. Zhou, Pepland: large-scale pre-trained peptide representation model for comprehensive landscape of both canonical and non-canonical amino acids, arXiv, 2023. [40] D. Gfeller, O. Michielin, and V. Zoete, Swisssidechain: molecular and structural database of non-natural sidechains, Nucleic Acids Research, vol. 41, p. D327D332, Oct. 2012. [41] Y. Yang, G. Chen, J. Li, J. Li, O. Zhang, X. Zhang, L. Li, J. Hao, E. Wang, and P.-A. Heng, Enabling target-aware molecule generation to follow multi objectives with pareto mcts, Communications Biology, vol. 7, Sept. 2024. [42] H. Qian, C. Lin, D. Zhao, S. Tu, and L. Xu, Alphadrug: protein target specific de novo molecular generation, PNAS Nexus, vol. 1, Sept. 2022. [43] M. Wang, S. Li, J. Wang, O. Zhang, H. Du, D. Jiang, Z. Wu, Y. Deng, Y. Kang, P. Pan, D. Li, X. Wang, X. Yao, T. Hou, and C.-Y. Hsieh, Clickgen: Directed exploration of synthesizable chemical space via modular reactions and reinforcement learning, Nature Communications, vol. 15, Nov. 2024. [44] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin, R. Verkuil, O. Kabeli, Y. Shmueli, A. Dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives, Evolutionaryscale prediction of atomic-level protein structure with language model, Science, vol. 379, pp. 11231130, Mar. 2023. [45] A. L. Feller and C. O. Wilke, Peptide-aware chemical language model successfully predicts membrane diffusion of cyclic peptides, Cold Spring Harbor Laboratory, Aug. 2024. [46] T. Chen and C. Guestrin, Xgboost: scalable tree boosting system, 2016. [47] A. L. Feller and C. O. Wilke, Peptide-aware chemical language model successfully predicts membrane diffusion of cyclic peptides, bioRxiv, 2024. [48] J. Eberhardt, D. Santos-Martins, A. F. Tillack, and S. Forli, Autodock vina 1.2. 0: New docking methods, expanded force field, and python bindings, Journal of chemical information and modeling, vol. 61, no. 8, pp. 38913898, 2021. [49] K. B. Johnsen, A. Burkhart, F. Melander, P. J. Kempen, J. B. Vejlebo, P. Siupka, M. S. Nielsen, T. L. Andresen, and T. Moos, Targeting transferrin receptors at the blood-brain barrier improves the uptake of immunoliposomes and subsequent cargo transport into the brain parenchyma, Scientific Reports, vol. 7, Sept. 2017. [50] W. A. Jefferies, M. R. Brandon, S. V. Hunt, A. F. Williams, K. C. Gatter, and D. Y. Mason, Transferrin receptor on endothelium of brain capillaries, Nature, vol. 312, p. 162163, Nov. 1984. [51] S. Gosk, C. Vermehren, G. Storm, and T. Moos, Targeting antitransferrin receptor antibody (ox26) and ox26-conjugated liposomes to brain capillary endothelial cells using in situ perfusion, Journal of Cerebral Blood Flow &amp; Metabolism, vol. 24, p. 11931204, Nov. 2004. [52] D. Zhang, J. Duque-Jimenez, F. Facchinetti, G. Brixi, K. Rhee, W. W. Feng, P. A. Jänne, and X. Zhou, Transferrin receptor targeting chimeras for membrane protein degradation, Nature, Sept. 2024. [53] J. H. Lee, J. A. Engler, J. F. Collawn, and B. A. Moore, Receptor mediated uptake of peptides that bind the human transferrin receptor, European Journal of Biochemistry, vol. 268, p. 20042012, Apr. 2001. [54] Y. Kuang, S. An, Y. Guo, S. Huang, K. Shao, Y. Liu, J. Li, H. Ma, and C. Jiang, T7 peptidefunctionalized nanoparticles utilizing rna interference for glioma dual targeting, International Journal of Pharmaceutics, vol. 454, p. 1120, Sept. 2013. [55] G. Kim, M. Kim, Y. Lee, J. W. Byun, D. W. Hwang, and M. Lee, Systemic delivery of microrna-21 antisense oligonucleotides to the brain using t7-peptide decorated exosomes, Journal of Controlled Release, vol. 317, p. 273281, Jan. 2020. [56] Y. Bi, L. Liu, Y. Lu, T. Sun, C. Shen, X. Chen, Q. Chen, S. An, X. He, C. Ruan, Y. Wu, Y. Zhang, Q. Guo, Z. Zheng, Y. Liu, M. Lou, S. Zhao, and C. Jiang, T7 peptide-functionalized peg-plga micelles loaded with carmustine for targeting therapy of glioma, ACS Applied Materials &amp; Interfaces, vol. 8, p. 2746527473, Oct. 2016. [57] L. Cai, C. Yang, W. Jia, Y. Liu, R. Xie, T. Lei, Z. Yang, X. He, R. Tong, and H. Gao, Endo/lysosome-escapable delivery depot for improving bbb transcytosis and neuron targeted therapy of alzheimers disease, Advanced Functional Materials, vol. 30, May 2020. [58] Z. Wang, Y. Zhao, Y. Jiang, W. Lv, L. Wu, B. Wang, L. Lv, Q. Xu, and H. Xin, Enhanced anti-ischemic stroke of zl006 by t7-conjugated pegylated liposomes drug delivery system, Scientific Reports, vol. 5, July 2015. [59] Y. Zhao, Y. Jiang, W. Lv, Z. Wang, L. Lv, B. Wang, X. Liu, Y. Liu, Q. Hu, W. Sun, Q. Xu, H. Xin, and Z. Gu, Dual targeted nanocarrier for brain ischemic stroke treatment, Journal of Controlled Release, vol. 233, p. 6471, July 2016. [60] M. Liang, C. Gao, Y. Wang, W. Gong, S. Fu, L. Cui, Z. Zhou, X. Chu, Y. Zhang, Q. Liu, X. Zhao, B. Zhao, M. Yang, Z. Li, C. Yang, X. Xie, Y. Yang, and C. Gao, Enhanced bloodbrain barrier penetration and glioma therapy mediated by t7 peptide-modified lowdensity lipoprotein particles, Drug Delivery, vol. 25, p. 16521663, Jan. 2018. [61] M. Yu, D. Su, Y. Yang, L. Qin, C. Hu, R. Liu, Y. Zhou, C. Yang, X. Yang, G. Wang, and H. Gao, D-t7 peptide-modified pegylated bilirubin nanoparticles loaded with cediranib and paclitaxel for antiangiogenesis and chemotherapy of glioma, ACS Applied Materials &amp; Interfaces, vol. 11, p. 176186, Dec. 2018. [62] M. K. Mahapatra, M. Karuppasamy, and B. M. Sahoo, Semaglutide, glucagon like peptide-1 receptor agonist with cardiovascular benefits for management of type 2 diabetes, Reviews in Endocrine and Metabolic Disorders, vol. 23, p. 521539, Jan. 2022. [63] M. A. Nauck and D. A. DAlessio, Tirzepatide, dual gip/glp-1 receptor co-agonist for the treatment of type 2 diabetes with unmatched effectiveness regrading glycaemic control and body weight reduction, Cardiovascular Diabetology, vol. 21, Sept. 2022. [64] E. M. Hol and M. Pekny, Glial fibrillary acidic protein (gfap) and the astrocyte intermediate filament system in diseases of the central nervous system, Current Opinion in Cell Biology, vol. 32, p. 121130, Feb. 2015. 29 [65] M. Brenner, A. B. Johnson, O. Boespflug-Tanguy, D. Rodriguez, J. E. Goldman, and A. Messing, Mutations in gfap, encoding glial fibrillary acidic protein, are associated with alexander disease, Nature Genetics, vol. 27, p. 117120, Jan. 2001. [66] A. Grossi, F. Rosamilia, S. Carestiato, E. Salsano, I. Ceccherini, and T. Bachetti, systematic review and meta-analysis of gfap gene variants in alexander disease, Scientific Reports, vol. 14, Oct. 2024. [67] G. Paratcha, F. Ledda, and C. F. Ibáñez, The neural cell adhesion molecule ncam is an alternative signaling receptor for gdnf family ligands, Cell, vol. 113, p. 867879, June 2003. [68] V. Vukojevic, P. Mastrandreas, A. Arnold, F. Peter, I.-T. Kolassa, S. Wilker, T. Elbert, D. J.-F. de Quervain, A. Papassotiropoulos, and A. Stetak, Evolutionary conserved role of neural cell adhesion molecule-1 in memory, Translational psychiatry, vol. 10, no. 1, p. 217, 2020. [69] S. Imbeaud, C. Belville, L. Messika-Zeitoun, R. Rey, N. di Clemente, N. Josso, and J.-Y. Picard, 27 base-pair deletion of the anti-müllerian type ii receptor gene is the most common cause of the persistent müllerian duct syndrome, Human molecular genetics, vol. 5, no. 9, pp. 12691277, 1996. [70] C. Rigon, A. Andrisani, M. Forzan, D. DAntona, A. Bruson, E. Cosmi, G. Ambrosini, G. M. Tiboni, and M. Clementi, Association study of amh and amhrii polymorphisms with unexplained infertility, Fertility and sterility, vol. 94, no. 4, pp. 12441248, 2010. [71] L. Lazaros, A. Fotaki, C. Pamporaki, E. Hatzi, C. Kitsou, A. Zikopoulos, C. Virgiliou, I. Kosmas, I. Bouba, T. Stefos, et al., The ovarian response to standard gonadotropin stimulation is influenced by amhrii genotypes, Gynecological Endocrinology, vol. 32, no. 8, pp. 641645, 2016. [72] S. Singh, N. Pal, S. Shubham, D. K. Sarma, V. Verma, F. Marotta, and M. Kumar, Polycystic ovary syndrome: Etiology, current management, and future therapeutics, Journal of Clinical Medicine, vol. 12, p. 1454, Feb. 2023. [73] N. di Clemente, C. Racine, and R. A. Rey, Anti-müllerian hormone and polycystic ovary syndrome in women and its male equivalent, Biomedicines, vol. 10, p. 2506, Oct. 2022. [74] K. N. Hart, W. A. Stocker, N. G. Nagykery, K. L. Walton, C. A. Harrison, P. K. Donahoe, D. Pépin, and T. B. Thompson, Structure of amh bound to amhr2 provides insight into unique signaling pair in the tgf-β family, Proceedings of the National Academy of Sciences, vol. 118, June 2021. [75] X. Li, X. Pu, X. Wang, J. Wang, X. Liao, Z. Huang, and G. Yin, dual-targeting peptide for glioblastoma screened by phage display peptide library biopanning combined with affinityadaptability analysis, International Journal of Pharmaceutics, vol. 644, p. 123306, Sept. 2023. [76] L. Y. Chan, D. J. Craik, and N. L. Daly, Dual-targeting anti-angiogenic cyclic peptides as potential drug leads for cancer therapy, Scientific Reports, vol. 6, Oct. 2016. [77] L. Li, E. Tian, X. Chen, J. Chao, J. Klein, Q. Qu, G. Sun, G. Sun, Y. Huang, C. D. Warden, P. Ye, L. Feng, X. Li, Q. Cui, A. Sultan, P. Douvaras, V. Fossati, N. E. Sanjana, A. D. Riggs, and Y. Shi, Gfap mutations in astrocytes impair oligodendrocyte progenitor proliferation and myelination in an hipsc model of alexander disease, Cell Stem Cell, vol. 23, pp. 239251.e6, Aug. 2018. [78] N. Habib, C. McCabe, S. Medina, M. Varshavsky, D. Kitsberg, R. Dvir-Szternfeld, G. Green, D. Dionne, L. Nguyen, J. L. Marshall, F. Chen, F. Zhang, T. Kaplan, A. Regev, and M. Schwartz, Disease-associated astrocytes in alzheimers disease and aging, Nature Neuroscience, vol. 23, p. 701706, Apr. 2020. [79] S. P. Yun, T.-I. Kam, N. Panicker, S. Kim, Y. Oh, J.-S. Park, S.-H. Kwon, Y. J. Park, S. S. Karuppagounder, H. Park, S. Kim, N. Oh, N. A. Kim, S. Lee, S. Brahmachari, X. Mao, J. H. Lee, M. Kumar, D. An, S.-U. Kang, Y. Lee, K. C. Lee, D. H. Na, D. Kim, S. H. Lee, V. V. 30 Roschke, S. A. Liddelow, Z. Mari, B. A. Barres, V. L. Dawson, S. Lee, T. M. Dawson, and H. S. Ko, Block of a1 astrocyte conversion by microglia is neuroprotective in models of parkinsons disease, Nature Medicine, vol. 24, p. 931938, June 2018. [80] B. S. Khakh, V. Beaumont, R. Cachope, I. Munoz-Sanjuan, S. A. Goldman, and R. Grantyn, Unravelling and exploiting astrocyte dysfunction in huntingtons disease, Trends in Neurosciences, vol. 40, p. 422437, July 2017. [81] M. A. Wheeler, I. C. Clark, E. C. Tjon, Z. Li, S. E. J. Zandee, C. P. Couturier, B. R. Watson, G. Scalisi, S. Alkwai, V. Rothhammer, A. Rotem, J. A. Heyman, S. Thaploo, L. M. Sanmarco, J. Ragoussis, D. A. Weitz, K. Petrecca, J. R. Moffitt, B. Becher, J. P. Antel, A. Prat, and F. J. Quintana, Mafg-driven astrocytes promote cns inflammation, Nature, vol. 578, p. 593599, Feb. 2020. [82] M. Martin-Fernandez, S. Jamison, L. M. Robin, Z. Zhao, E. D. Martin, J. Aguilar, M. A. Benneyworth, G. Marsicano, and A. Araque, Synapse-specific astrocyte gating of amygdalarelated behavior, Nature Neuroscience, vol. 20, p. 15401548, Sept. 2017. [83] Q. Yang, J. Zhao, D. Chen, and Y. Wang, E3 ubiquitin ligases: styles, structures and functions, Molecular Biomedicine, vol. 2, July 2021. [84] A. A. Sosunov, G. M. McKhann, and J. E. Goldman, The origin of rosenthal fibers and their contributions to astrocyte pathology in alexander disease, Acta Neuropathologica Communications, vol. 5, Mar. 2017. [85] L. F. Eng, Glial fibrillary acidic protein (gfap): the major protein of glial intermediate filaments in differentiated astrocytes, Journal of Neuroimmunology, vol. 8, p. 203214, 1985. [86] T. Chen, Y. Zhang, and P. Chatterjee, moppit:de novogeneration of motif-specific binders with protein language models, bioRxiv, Aug. 2024. [87] J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, N. Hanikel, S. J. Pellock, A. Courbet, W. Sheffler, J. Wang, P. Venkatesh, I. Sappington, S. V. Torres, A. Lauko, V. De Bortoli, E. Mathieu, S. Ovchinnikov, R. Barzilay, T. S. Jaakkola, F. DiMaio, M. Baek, and D. Baker, De novo design of protein structure and function with rfdiffusion, Nature, vol. 620, p. 10891100, July 2023. [88] F. J. Duffy, M. Verniere, M. Devocelle, E. Bernard, D. C. Shields, and A. J. Chubb, Cyclops: Generating virtual libraries of cyclized and constrained peptides including nonnatural amino acids, Journal of Chemical Information and Modeling, vol. 51, p. 829836, Mar. 2011. [89] L. Wang, R. Pulugurta, P. Vure, Y. Zhang, A. Pal, and P. Chatterjee, Pepdora: unified peptide language model via weight-decomposed low-rank adaptation, arXiv, 2024. [90] G. Brixi, T. Ye, L. Hong, T. Wang, C. Monticello, N. Lopez-Barbosa, S. Vincoff, V. Yudistyra, L. Zhao, E. Haarer, T. Chen, S. Pertsemlidis, K. Palepu, S. Bhat, J. Christopher, X. Li, T. Liu, S. Zhang, L. Petersen, M. P. DeLisa, and P. Chatterjee, Salt&peppr is an interface-predicting language model for designing peptide-guided protein degraders, Communications Biology, vol. 6, Oct. 2023. [91] T. Ye, A. Alamgir, C. M. Robertus, D. Colina, C. Monticello, T. C. Donahue, L. Hong, S. Vincoff, S. Goel, P. Fekkes, L. M. Camargo, K. Lam, J. Heyes, D. Putnam, C. A. Alabi, P. Chatterjee, and M. P. DeLisa, Programmable protein degraders enable selective knockdown of pathogenic β-catenin subpopulationsin vitroandin vivo, bioRxiv, Nov. 2024. [92] M. Békés, D. R. Langley, and C. M. Crews, Protac targeted protein degraders: the past is prologue, Nature Reviews Drug Discovery, vol. 21, p. 181200, Jan. 2022. [93] C. Pu, S. Wang, L. Liu, Z. Feng, H. Zhang, Q. Gong, Y. Sun, Y. Guo, and R. Li, Current strategies for improving limitations of proteolysis targeting chimeras, Chinese Chemical Letters, vol. 34, p. 107927, June 2023. 31 [94] J.-N. Li, G. Yang, P.-C. Zhao, X.-X. Wei, and J.-Y. Shi, Cpromg: controllable protein-oriented molecule generation with desired binding affinity and drug-like properties, Bioinformatics, vol. 39, p. i326i336, June 2023. [95] Y. Li, H. Zhou, X. Chen, Y. Zheng, Q. Kang, D. Hao, L. Zhang, T. Song, H. Luo, Y. Hao, R. Chen, P. Zhang, and S. He, Smprot: reliable repository with comprehensive annotation of small proteins identified from ribosome profiling, Genomics, Proteomics &amp; Bioinformatics, vol. 19, p. 602610, Aug. 2021. [96] M. Krenn, F. Häse, A. Nigam, P. Friederich, and A. Aspuru-Guzik, Self-referencing embedded strings (selfies): 100% robust molecular string representation, Machine Learning: Science and Technology, vol. 1, no. 4, p. 045024, 2020. [97] D. Rogers and M. Hahn, Extended-connectivity fingerprints, Journal of Chemical Information and Modeling, vol. 50, p. 742754, Apr. 2010. [98] D. Mendez, A. Gaulton, A. P. Bento, J. Chambers, M. De Veij, E. Félix, M. Magariños, J. Mosquera, P. Mutowo, M. Nowotka, M. Gordillo-Marañón, F. Hunter, L. Junco, G. Mugumbate, M. Rodriguez-Lopez, F. Atkinson, N. Bosc, C. Radoux, A. Segura-Cabrera, A. Hersey, and A. Leach, Chembl: towards direct deposition of bioassay data, Nucleic Acids Research, vol. 47, p. D930D940, Nov. 2018. [99] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama, Optuna: next-generation hyperparameter optimization framework, in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 26232631, 2019. [100] T. Chen and C. Guestrin, Xgboost: scalable tree boosting system, in Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp. 785794, 2016. [101] L. Grinsztajn, E. Oyallon, and G. Varoquaux, Why do tree-based models still outperform deep learning on tabular data?, 2022. [102] J. Shi, K. Han, Z. Wang, A. Doucet, and M. K. Titsias, Simplified and generalized masked diffusion for discrete data, arXiv, 2024. [103] S. S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. T. Chiu, A. Rush, and V. Kuleshov, Simple and effective masked diffusion language models, arXiv preprint arXiv:2406.07524, 2024. [104] T. Zhang, H. Li, H. Xi, R. V. Stanton, and S. H. Rotstein, Helm: hierarchical notation language for complex biomolecule structure representation, Journal of Chemical Information and Modeling, vol. 52, p. 27962806, Sept. 2012. [105] RDKit: Open-source cheminformatics, [106] J. Lin, Divergence measures based on the shannon entropy, IEEE Transactions on Information theory, vol. 37, no. 1, pp. 145151, 1991. [107] G. M. Morris, R. Huey, W. Lindstrom, M. F. Sanner, R. K. Belew, D. S. Goodsell, and A. J. Olson, Autodock4 and autodocktools4: Automated docking with selective receptor flexibility, Journal of computational chemistry, vol. 30, no. 16, pp. 27852791, 2009. [108] S. Wang, J. Witek, G. A. Landrum, and S. Riniker, Improving conformer generation for small rings and macrocycles based on distance geometry and experimental torsional-angle preferences, Journal of chemical information and modeling, vol. 60, no. 4, pp. 20442058, 2020. [109] Schrödinger, LLC, The PyMOL molecular graphics system, version 1.8. November 2015. [110] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. v. d. Berg, Structured denoising diffusion models in discrete state-spaces, arXiv, 2021. [111] N. Gruver, S. Stanton, N. C. Frey, T. G. J. Rudner, I. Hotzel, J. Lafrance-Vanasse, A. Rajpal, K. Cho, and A. G. Wilson, Protein design with guided discrete diffusion, arXiv, 2023. 32 [112] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, Digress: Discrete denoising diffusion for graph generation, 2022. [113] A. Lou, C. Meng, and S. Ermon, Discrete diffusion modeling by estimating the ratios of the data distribution, arXiv, 2023."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Results"
        },
        {
            "title": "2.2 PepMDLM Generates Diverse Chemically-Modified and Cyclic Peptides",
            "content": ". . . . ."
        },
        {
            "title": "2.5 Case Studies for Multi-Objective Generation of Peptide Binders",
            "content": ". . . . . . . . . ."
        },
        {
            "title": "2.6 PepTune Generates Multi-Target Specific Peptides . . . . . . . . . . . . . . . . . .",
            "content": "3 Discussion 4 Methods"
        },
        {
            "title": "4.1 Data Curation and Tokenization.",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.2 Unconditional Masked Discrete Diffusion Model",
            "content": ". . . . . . . . . . . . . . . . . . 4.3 Loss Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 5 6 8 9 14 17 17 18 19 4.4 Multi-Objective Guidance for Discrete Diffusion . . . . . . . . . . . . . . . . . . . 21 4.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Declarations Extended Background A.1 Continuous-Time Discrete Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Continuous-Time Negative Evidence Lower Bound (NELBO) . . . . . . . . . . . A.3 Guided Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Details B.1 State-Dependent Masking Schedule . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Derivation of State-Dependent Reverse Posterior . . . . . . . . . . . . . . . . . . B.3 Derivation of State-Dependent NELBO Loss . . . . . . . . . . . . . . . . . . . . . B.4 Gradient Flow of Invalid Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Architectures and Training Details C.1 RoFormer Architecture Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Target-Binding Prediction Model . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Boosted Trees for Peptide SMILES Property Prediction . . . . . . . . . . . . . . . C.4 PDB Docking Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Further Experiments D.1 Case Study for Time-Dependent Multi-Objective Guidance . . . . . . . . . . . . . D.2 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Algorithms 24 25 35 36 38 39 39 43 45 46 46 47 48 48 49 49"
        },
        {
            "title": "A Extended Background",
            "content": "A.1 Continuous-Time Discrete Diffusion Discrete diffusion models [110] are subset of diffusion models where the forward corruption and reverse denoising processes operate in the discrete latent space via categorical probability distributions for discrete variables. We denote token in sequence from the dataset as one-hot vector x(ℓ) 0 {0, 1}V. The discretetime forward diffusion process involves applying discrete categorical noise to x0 at varying degrees based on noise schedule σ(t) for total of time steps ranging from no noise at = 0 to maximum noise at = 1. To clearly distinguish each step, we denote the nth transition in the forward pass as the transition from s(n) to t(n), where s(n) = n1 . The marginal noise that transforms the sequence zs(n) at time s(n) to progressively noisier sequence zt(n) the next time step t(n) = s(n) + 1 is given by V transition matrix Qts = σ(t)IV + (1 σ(t))1π, where the (i, j)th entry denotes the probability of transitioning from token to token at each position in the sequence. and t(n) = Therefore, the marginal categorical distribution of zt(n) in the discrete-time forward-pass diffusion process can be derived as q(zt(n)zs(n)) = Cat(zt(n); tszs(n)) = Cat(zt(n); σ(t(n))zs(n) + (1 σ(t(n)))π) (35) where σ(t(n)) the marginal probability of single position in the sequence remaining the same token during the transition s(n) t(n) and (cid:0)1 σ(t(n))) is the marginal probability of transitioning to different token based on the token probability distribution π V. For simplicity, we denote σ(t(n)) = σ(n). The cumulative transition from time 0 to time t(t) is denoted as the product of the marginals Qt = (cid:81)t n=0 Qts, which is the product of marginal transitions s(n) t(n) for ranging from 0 to applied to the clean input sequence x0. (cid:32) (cid:89) Qt = (cid:33) (cid:32) (1 σ(n)) + 1 (cid:89) (cid:33) (1 σ(n)) 1π n=0 n=0 (36) For the continuous-time forward pass diffusion process where and 1 limit as to derive an expression for the continous-time transition probability, αt. (cid:32) (cid:33) 0, we can take the lim (1 σ(n)) = lim exp log (1 σ(n)) (cid:89) n=0 (cid:89) n=0 = lim exp lim exp (cid:32) (cid:88) n= (cid:32) (cid:88) n=0 (cid:33) log(1 σ(n)) (cid:33) σ(n) (cid:18) = exp (cid:90) (cid:19) σ(n)dn (cid:32) = exp n=0 (cid:90) t(t) s= (cid:33) σ(s)ds (log(1 x) for small x) (37) (cid:16) Now, we can define the continuous-time marginal transition probability from as αt = exp and write the continuous-time cumulative transition matrix at the limit where and define the continuous-time distribution q(ztx0) as follows (cid:17) s=0 σ(s)ds (cid:82) t(t) Qt = αtI + αt1π q(ztx0) = Cat(zt; Qtx0) = Cat(zt; αtx0 + (1 αt)π) (38) (39) 35 It follows that the marginal transition Qst is the cumulative transition Qt divided by all previous transition probabilities, denoted as Qs = (cid:81)s Following Austin et al. [19] and substituting the marginal and cumulative probability distributions, we derive the true reverse distribution from conditioned on clean sequence x0 as r=0 Qsr, so αst = αt αs . q(zszt, x0) = q(ztzs, x0)q(zsx0) q(ztx0) (cid:32) = Cat zs; (cid:32) = Cat zs; (cid:33) Qtszt Q x0 x0 [αtszt + (1 αts)1πz] [αszt + (1 αs)zs] z x0 (cid:33) (40) The numerator of the discrete reverse posterior is the element-wise product of V-dimensional vectors representing the marginal probability distribution of sampling zt given zs and the total probability distribution of sampling zs from the original clean sequence x0. The denominator is scalar probability of the specific token zt being drawn from the noisy probability distribution at time t. A.2 Continuous-Time Negative Evidence Lower Bound (NELBO) The objective of denoising diffusion probabilistic models (DDPMs) [64] is to iteratively sample slightly less noisy intermediate sequences zt until obtaining clean sequence that has high probability of being drawn from the distribution p(x0) of all possible valid sequences. Since p(x) is unknown, we compute an Evidence Lower Bound (ELBO) of log p(x0) such that by maximizing the ELBO, we are maximizing log p(x0) that measures how well the model can generate true samples x0 from its learned distribution pθ(x0zt). To convert this into loss minimization objective, we define the loss function as the Negative ELBO (NELBO). First, we can compute p(x0) by integrating over the joint probability of all possible paths of intermediate states from the noisy state xT at = to the clean sample x0 at = 0. log p(x0) = log = log = log (cid:90) (cid:90) (cid:18) p(x0:T )dx1:T q(x1:T x0) (cid:21) (cid:20) p(x0:T ) q(x1:T x0) dx1:T Eq(x1:T x0) (cid:21)(cid:19) (cid:20) p(x0:T ) q(x1:T x0) (41) Using Jensons inequality, we move the logarithm inside the expectation to get the ELBO for log p(x0). Further, we split the terms associated with the forward noising process q(x1:T x0) and the reverse denoising model p(x0:T ) into reconstruction term, the intermediate diffusion terms, and the 36 prior term. LNELBO = Eq(z1:T x0) (cid:20) log (cid:34) = Eq(z1:T x0) log (cid:34) (cid:21) p(z0:T ) q(z1:T x0) p(x0zt(1))p(zt(T )) (cid:81)T 1 q(xT xT 1) (cid:81)T 1 n=1 p(zs(n)zt(n)) (cid:35) n=1 q(zt(n)zs(n)) = Eq(z1:T x0) log p(x0x1) log p(xT ) q(zt(T )zs(T )) log (cid:81)T 1 t=1 p(zs(n)zt(n)) n=1 q(zt(n)zs(n)) (cid:81)T 1 (cid:35) (cid:35) (cid:34) (cid:20) = Eq(z1:T x0) = Eq(z1:T x0) log p(x0zt(1)) log p(zt(T )) q(zt(T )zs(T )) 1 (cid:88) n=1 log log p(x0zt(1)) (cid:21) + Eq(z1:T x0) (cid:20) log p(zs(n)zs(n)) q(zt(n)zs(n)) (cid:21) p(zt(T )) q(zt(T )zs(T )) + 1 (cid:88) n=1 Eq(z1:T x0) (cid:20) log (cid:21) p(zs(n)zt(n)) q(zt(n)zs(n)) (cid:20) = Eq(zt(1)x0) log p(x0zt(1)) (cid:21) + Eq(zt(T ),zs(T )x0) (cid:20) log (cid:123)(cid:122) prior loss p(zt(T )) q(zt(T )zs(T )) (cid:21) (cid:125) (cid:124) + 1 (cid:88) n=1 (cid:124) (cid:123)(cid:122) reconstruction loss Eq(zs(n),zt(n),zt(n+1)x0) (cid:124) (cid:125) (cid:20) log (cid:123)(cid:122) diffusion loss pθ(zs(n)zt(n)) q(zt(n)zs(n)) (cid:21) (cid:125) (42) Now, we can take the limit for each of the loss terms as to derive the general form of the continuous-time MDLM objective. Reconstruction Loss Lreconst. The reconstruction loss evaluates the final step of the reverse diffusion process that denoises the sequence at time t(1) to the clean sequence at time = 0. Since t(0) = 1 , the distribution that the sequence zt(1) is drawn from in the forward pass diffusion is given by p(zt(1)x0) = Cat(zt(1); α t(1)x0x0 + (1 α t(1)x0)m) Since we have αt(x0) = 1 tw for x0 = and αt(x0) = 1 for x0 = b, we can write αt(x0)x0 + (1 αt(x0))m = (cid:26)(cid:0)1 1 (cid:0)1 (cid:1) x0 + 1 x0 = (cid:1) x0 + 1 x0 = T (43) (44) In the limit as , both cases converge to x0, so we have zt(1) Cat(zt(1); x0) and zt(1) = x0. Since q(zt(1)x0) = x0 in the forward pass, by parameterizing the reverse posterior to copy-over unmasked tokens, we get pθ(x0zt(0)) = x0. Therefore, the reconstruction loss reduces to 0. (cid:20) Eq(zt(1)x0) log pθ(x0zt(1)) (cid:21) (cid:20) = Eq(zt(1)x0) log pθ(x0x0) (cid:21) = 0 Prior Loss Lprior. The prior loss measures the first reverse transition from the fully masked sequence zt(T ) to slightly unmasked sequence zt(T 1). Eq(zt(T ),zs(T )x0) (cid:20) log (cid:21) p(zt(T )) q(zt(T )zs(T )) = Eq(zs(T )x0) Eq(zt(T )zs(T )) (cid:20) log p(zt(T )) q(zt(T )zs(T )) (cid:124) (cid:20) (cid:18) (cid:123)(cid:122) KL Divergence = Eq(zs(T )x0) KL q(zt(T )zs(T ))pθ(zt(T )) (45) (cid:21) (cid:125) (cid:19)(cid:21) Since t(T ) = 1 and t(T 1) = , we can derive q(zt(T )x0) = Cat(cid:0)zt(T ) ; αt(x0)x0 + (1 αt(x0))m(cid:1) = Cat(cid:0)zt(T ); m(cid:1) 37 (46) It follows that the prior distribution pθ(zt(T )) = Cat(cid:0)zt(T ); m(cid:1) and the KL divergence reduces to 0. Diffusion Loss LT . The diffusion loss measures the error of each reverse diffusion step for time steps. That is, it measures the consistency of the learned reverse transitions with the forward pass diffusion. 1 (cid:88) n=1 Eq(zs(n),zt(n),zt(n+1)x0) (cid:20) log (cid:21) pθ(zs(n)zt(n)) q(zt(n)zs(n)) = = 1 (cid:88) n=1 1 (cid:88) n=1 Eq(zs(n),zt(n+1)x0) Eq(zt(n)zs(n)) (cid:20) log pθ(zs(n)zt(n)) q(zt(n)zs(n)) (cid:124) (cid:20) (cid:18) (cid:123)(cid:122) KL divergence Eq(zs(n),zt(n+1)x0) KL q(zt(n)zs(n))pθ(zs(n)zt(n)) (47) (cid:21) (cid:125) (cid:19)(cid:21) Since the objective is to accurate predict zs(n) given zt(n), we cannot condition on the term zs. Instead, we can condition q(zt(n)zs(n)) on x0 and use Bayes theorem to derive q(zt(n)zs(n), x0) = q(zs(n)zt(n), x0)q(zt(n)x0) q(zs(n)x0) Rearranging the terms we get an expression for the true reverse transition q(zs(n)zt(n), x0) conditioned on the clean data x0. By minimizing the KL divergence between the learned reverse transition pθ(zs(n)zt(n)) and q(zs(n)zt(n), x0), we can rewrite the diffusion loss as LT = 1 (cid:88) n=1 Eq(zt(n)x0) (cid:20) (cid:18) (cid:19)(cid:21) KL q(zs(n)zt(n), x0)pθ(zs(n)zt(n)) (48) In Appendix B.3, we derive the state-dependent continuous-time NELBO loss from its general form above. A.3 Guided Diffusion Models Guided diffusion aims to sample from the noisy data distribution conditioned on some property y, p(x0zT , y), such that p(yx0) is maximized. Therefore, the marginal reverse transition aims to sample from distribution q(zszt, x0, y) conditioned on the target unmasked sequence x0 and property y. Since we aim to reconstruct the sequence x0 in the reverse process, the learned distribution is conditioned on learned unmasked sequence xθ(zt, t). Using Bayes theorem, we can decompose the guided conditional distribution as: q(zszt, y) = q(yzs, zt)q(zszt) q(yzt) (49) There are two strategies to generate samples from this conditional distribution: classifier-free and classifier-based guidance. Classifier-Free Guidance. Classifier-free guidance strategies aim to model the conditional distribution q(zszt, y) by directly training the diffusion model on directly on subset of the unconditional data with property y, such that after training, the model samples from learned distribution pθ(zszt, y). However, classifier-free guidance fails at tasks where quality annotated data is scarce, including peptide generation. Furthermore, this strategy cannot scale to multiple objectives which would require data conditioned on more than one property. Classifier-Based Guidance. Classifier-based guidance trains an unconditional diffusion model pθ(zszt) and classifier model pϕ(yzs) with learned parameters ϕ that generates score measuring the probability that the intermediate sequence zs has property y. By Bayes theorem, we can model the conditional distribution as pθ,ϕ(zszt, y) = pϕ(yzs)pθ(zszt) p(yzt) (50) 38 Since the model parameters implicitly learn the normalized distribution, we can drop the p(yzt) term. Then, at each iteration, we update the parameters θ, ϕ in the direction of the gradient of log pθ,ϕ(zszt, y) obtained by combining the unconditional distribution log pθ(zszt) and classifier probability pϕ(yzs) with respect to the sampled sequence zs. zs log pθ,ϕ(zszt, y) = zs log pϕ(yzs) + zs log pθ(zszt) (51) After joint training with the classifier and unconditional data distribution, we can sample from the learned conditional distribution pθ,ϕ(zszt, y). Unlike classifier-free guidance, classifier-based guidance does not require training generative model from conditioned dataset. However, the gradient-based strategy for classifier-based guidance is not directly applicable to discrete state spaces due to the lack of defined gradient. To mimic gradientbased updates to each sampling step, Gruver et al. [111] leveraged iterative guidance steps on hidden latent embeddings ht for each token before projecting back to discrete categorical distribution pθ(zsht) at each time step. However, projecting to and from the continuous and discrete spaces results in inconsistencies in the guided diffusion process, where optimized hidden embeddings do not always map to optimal tokens. Guidance in the Discrete State Space. To directly guide the diffusion objective in the discrete space, we must compute the optimality of single discrete reverse transition zs against all other possible transitions to maximize the conditional probability p(yzs, zt). That is, we need to compute Equation (49) with the denominator expanded to represent all possible transitions from zt. pθ,ϕ(zszt, y) = pϕ(yzs)pθ(zszt) (cid:80) pϕ(yz s)pθ(z szt) (52) However, computing pϕ(yz s) for all the possible transitions from state zt is infeasible. Previous work has bypassed this limitation by approximation. For continuous and differentiable classifier functions p(yx) : RLV R, we can approximate the denominator using the first-order Taylor expansion given by log pϕ(yzs, zt) log pϕ(yzt) + (xs xt)z log p(yz)z=zt (53) which approximates the likelihood of observing property at the slightly denoised state zs = zt 1 given the known log-probability of observing for state xt. This eliminates the need to explicitly sample xs for all possible state transitions and compute log pϕ(yzs, zt). This approximation strategy has been used by Digress [112] for guided discrete diffusion on categorical graph generation. Furthermore, Nisonoff et al. [27] uses the Taylor-approximated conditional distribution log pϕ(yzs) to adjust the unconditional transition rates Rt(zt, zsy) given the unconditional rates Rt(zt, zs) for predictor-guidance of Continuous-Time Markov Chains (CTMCs) in the discrete state space. Rt(zt, zsy) = Rt(zt, zs) log pϕ(yzs, zt) log pϕ(yzt) (54) where Rt(zt, zsy) is the predictor-adjusted rate of transitioning from state zt to state zt This strategy fails to scale to multiple objective guidance since it would require computing the joint probability over objectives pϕ(y1, y2, . . . , yKzs, zt) for some > 1. If all properties are mutually independent, we can factorize the distribution and compute the estimated probability of each objective and take their product pϕ(y1, y2, . . . , yKzs, zt) = (cid:81)K k=1 pϕ(ykzs, zt). For the majority of multi-objective tasks including therapeutic peptide generation, independence across properties is not reasonable assumption, and computing the joint distribution is required. Moreover, for objectives that guide toward contradictory optimal rates or transitions, training model conditioned on these objectives could prevent the model from generating optimal sequences for either objective. Given these limitations, there remains gap for efficient classifier-based conditioning for discrete diffusion that is robust to multi-objective tasks, which we address in this work."
        },
        {
            "title": "B Theoretical Details",
            "content": "B.1 State-Dependent Masking Schedule From Equation (37), we define the continuous-time forward masking probability 1 αt at time with αt = exp(σ(t)) , where σ(t) : [0, 1] R+. Following Lou et al. [113], we apply log-linear 39 Figure 15: Plots of state-dependent masking schedules. (A) The probability of remaining unmasked during the continuous-time forward diffusion process over time given different values of as the exponent of the masking schedule αt = 1 tw. We use = 1 for non-peptide bond tokens and = 3 for peptide bond tokens, enabling slower masking of peptide bond tokens. (B) The weight of the negative log-loss for different exponents in the log-polynomial masking schedule. The weight of the loss is higher for larger in earlier time steps, which results in higher penalty for wrong predictions of peptide bond tokens compared to other tokens. masking schedule σ(t) = log(1 t) for the forward diffusion process which has shown to result in the lowest variance in the negative likelihood bound (NELBO) loss [103]. Therefore, the continuoustime probability of remaining unmasked at time is equal to αt = exp ( log(1 t)) = 1 and the weight that scales the negative log loss (NLL) is given by 1 by our derivation in Appendix B.3. For peptide-bond tokens, we alter the masking schedule such that peptide-bonds are masked at slower rate at earlier time steps by defining log-polynomial masking schedule σ(t) = log(1 tw), for some constant exponent 1. Note that when = 1, the log-polynomial schedule reduces to the log-linear schedule. Therefore, the probability of remaining unmasked becomes αt = exp (cid:0) log(1 tw)(cid:1) = 1 tw and the weight that scales the negative log loss (NLL) is given by by our derivation in Appendix B.3. Since (0, 1], the probability that peptide-bond token remains unmasked at time is equal to αt = 1 tw which is larger than the log-linear schedule for > 1. Conversely, the probability that peptide-bond token is masked before is 1 αt = tw which is smaller than the log-linear schedule for > 1. As 1, αt 0 for both the log-linear and log-polynomial time schedules, which means that both peptide-bond and non-peptide bond tokens will have high probability of being masked in later times in the forward pass diffusion process. This means that the NLL of the peptide-bond tokens is weighted more heavily than non-peptide bond tokens for close to 1. As 0, the NLL weight approaches for all tokens. This biases the reverse diffusion process to unmask peptide bond tokens earlier since it was trained to minimize the loss associated with each unmasking step. As 0, the large NLL weight ensures that the final unmasking steps during the reverse diffusion process result in an unmasked sequence that lies within the space of valid peptide SMILES. B.2 Derivation of State-Dependent Reverse Posterior For single token, the state-dependent forward diffusion process is defined by the probability distribution q(ztx0) which transforms the clean inputs to sequences with varying degrees of masking based on probability distribution given by the function αt(x0) : RV R, which takes the token encoding x0 and outputs the probability of remaining unmasked at time depending on whether x0 encodes peptide bond token. q(ztx0) = Cat(zt; (αt(x0))x0 + (1 αt(x0))m) Then, the marginal forward transition from time s(n) t(n) is defined as (cid:18) (cid:18) (cid:19) q(zt(n)zs(n)) = Cat zt(n); x0 + 1 (cid:18) αt(x0) αs(x0) αt(x0) αs(x0) (55) (56) (cid:19) (cid:19) 40 In this work, we classify each token into one of two states: peptide-bond token, and all remaining tokens representing peptide side-chains and modifications. We define function that generates mask with values of 1 indicating tokens containing or contained within peptide bond, and 0 otherwise (Appendix 6). Letting RV denote vector with ones at indices of peptide bond tokens, we have bx(ℓ) 0 when token at position ℓ is peptide bond token. Note that is defined differently depending on the context of token in the full sequence which is handled by the BONDMASK function, and we use for derivation purposes only. Then, we have αt = (cid:0)1 bx0 (cid:1)(1 t) + bx0(1 tw) or equivalently we can write αt(x0) = (cid:26)1 tw x0 = x0 = 1 (57) By Bayes rule, the general state-independent form of the true reverse posterior is given by q(zszt, x0) = = q(ztzs)q(zsx0) q(ztx0) (cid:17) (cid:16) zt + (cid:104)(cid:16) αt αs (cid:17) (cid:105) (cid:2)αsx mzt 1 αt αs (cid:2)αtx 0 zt + (1 αt) mzt (cid:3) 0 zt + (1 αs) mzt (cid:3) (58) With state-dependent masking, the value of αt(x0) and αs(x0) depend on the state of x0, so the state-dependent reverse posterior becomes (cid:104)(cid:16) αt(zs) αs(zs) 0 zt + (1 αs(x0)) mzt (cid:105) (cid:2)αs(x0)x zt + mzt (cid:16) (cid:17) (cid:17) (cid:3) q(zszt, x0) = 1 αt(zs) αs(zs) (cid:2)αt(x0)x 0 zt + (1 αt(x0)) mzt (cid:3) (59) When zt = x0, the true reverse posterior simplifies to (cid:104)(cid:16) αt(zs) 1 αt(zs) αs(zs) αs(zs) (cid:2)αt(x0)x q(zszt = x0, x0) = x0 + (cid:17) (cid:16) (cid:17) (cid:104)(cid:16) αt(zs) αs(zs) = (cid:105) x0 αt(x0) [αs(x0)] (cid:17) mx (cid:105) (cid:2)αs(x0)x 0 x0 + (1 αs(x0)) mx0 (cid:3) 0 x0 + (1 αt(x0)) mx0 (cid:3) (60) When zs = x0, x0 = 0 so q(zs = x0zt = x0, x0) = 0. When zs = x0, we have q(zs = x0zt = x0, x0) = (cid:104)(cid:16) αt(x0) αs(x0) (cid:17) = (cid:18) αt(x0) αs(x0) [αs(x0)] (cid:105) 0 x0 αt(x0) (cid:19) (cid:18) αs(x0) αt(x0) (cid:19) = 1 which means that zt remains unchanged after unmasking. This supports the carry-over unmasking scheme which explicitly sets the probability of changing an unmasked token equal to . In the forward diffusion process, token either remains unchanged or is masked, so the only other case we need to consider is zt = m. Since the masking schedule differs only when the ground truth token is peptide bond token, or x0 = b, we can consider two cases: first, when x0 = and second, when x0 = b. Case 1. Consider the case when x0 = or the ground truth token x0 is peptide-bond token. From our modified masking schedule, we have αt(b) = 1 tw. Therefore, we can write the probability distribution for unmasking peptide-bond token as q(zszt = m, x0 = b) = (cid:104)(cid:16) αt(zs) αs(zs) (cid:17) m + (cid:16) 1 αt(zs) αs(zs) (cid:17) mm (cid:105) (cid:2)αs(b)bzs + (1 αs(b)) mzs (cid:3) (cid:104)(cid:16) αt(zs) αs(zs) (cid:17) m + = [αt(b)bm + (1 αt(b)) mm] (cid:16) 1 αt(zs) αs(zs) (cid:17)(cid:105) (cid:2)αs(b)bzs + (1 αs(b)) mzs (1 αt(b)) (cid:3) (61) 41 The probability of transitioning from masked state to peptide-bond token is simplified to q(zs = bzt = m, x0 = b) = (cid:104)(cid:16) αt(b) αs(b) (cid:17) bm + (cid:16) 1 αt(b) αs(b) (cid:17)(cid:105) (cid:2)αs(b)bb + (1 αs(b)) mb(cid:3) (1 αt(b)) (cid:16) 1 1tw 1sw (cid:17) (1 sw) (1 (1 tw)) (cid:16) 1sw1+tw 1sw (cid:17) (1 sw) = = = tw sw tw sw tw = 1 tw (62)"
        },
        {
            "title": "The probability of remaining in a masked state is",
            "content": "q(zs = mzt = m, x0 = b) = = = = = (cid:104)(cid:16) αt(m) αs(m) (cid:17) mm + (cid:16) 1 αt(m) αs(m) (cid:17)(cid:105) (cid:2)αs(b)bm + (1 αs(b)) mm(cid:3) (cid:104)(cid:16) αt(m) αs(m) (cid:17) + (cid:16) 1 αt(m) αs(m) (cid:17)(cid:105) (1 αt(b)) (1 αt(b)) (1 αs(b)) 1 αs(b) 1 αt(m) 1 (1 sw) 1 (1 tw) sw tw (63) which aligns with the constraint that zt {m, x0} in the forward diffusion process. Case 2: Consider the case when x0 = or the ground truth token x0 is not peptide-bond token. From the baseline log-linear masking schedule, we have α x0 = 1 t. Therefore, we can write the probability distribution for unmasking peptide-bond token as (cid:104)(cid:16) αt(zs) αs(zs) m + 0 + (1 αs(x0)) mm(cid:3) (cid:16) (cid:17) (cid:17) q(zszt = m, x0 = b) = (cid:104)(cid:16) αt(zs) αs(zs) (cid:17) m + (cid:16) = mm 1 αt(zs) αs(zs) (cid:2)αt(x0)x 1 αt(zs) αs(zs) (cid:105) (cid:2)αs(x0)x 0 + (1 αt(x0)) mm(cid:3) (cid:17)(cid:105) (cid:2)αs(x0)x 0 + (1 αt(x0))(cid:3) 0 + (1 αs(x0))(cid:3) (cid:2)αt(x0)x With similar steps to Case 1, the probability of transitioning from masked state to non-peptide-bond token is given by (64) q(zs = x0zt = m, x0 = b) = (cid:16) (cid:16) (cid:17) 1 αt(x0) αs(x0) (1 αt(x0)) (1 αs(x0)) (cid:17) (1 (1 s)) 1 1t 1s (1 (1 t)) = = t = 1 It follows that the probability of remaining in masked state in the reverse process is q(zs = mzt = m, x0 = b) = 42 (65) (66) Combining Equations (63) and (66) we get the following distribution for the case when zt = and zs = q(zs = mzt = m, x0) = (cid:19) x0 (cid:18) sw tw (cid:19) bx0 + = = (cid:18) sw tw (cid:18)(cid:18) sw tw t 1 + (cid:19) + (cid:19) 1 x0 (67) Similarly, combining (62) and (65) we get the following distribution for the case when zt = and zs = or equivalently zs = x0. q(zs = x0zt = m, x0) = = = (cid:18) (cid:18) (cid:18)(cid:18) (cid:19) sw tw bx0 + (cid:16) 1 (cid:17) (cid:19) 1 x0 (cid:19) 1 x0 sw tw + 1 (cid:19) sw tw + (68) (69) Now, we can write the true reverse posterior as (cid:40)(cid:0)(cid:0) 1(cid:1) (cid:1) + ts sw tw q(zszt, x0) = zt x0x0 + (cid:0)(cid:0) sw tw (cid:1) + 1(cid:1) x0m zt = zt = Therefore, we get the following expression for the parameterized reverse posterior pθ(zszt) = (cid:40)(cid:0)(cid:0) sw tw (cid:1) + ts 1(cid:1) zt xθ(zt, t)zs + (cid:0)(cid:0) sw tw (cid:1) + 1(cid:1) xθ(zt, t)m zt = zt = (70) B.3 Derivation of State-Dependent NELBO Loss The diffusion objective in its general form is given by Eq(xtx0) (cid:20) (cid:18) q(zszt, x0)(cid:12) (cid:12) KL (cid:19)(cid:21) (cid:12) (cid:12)pθ(zszt) 1 (cid:88) t=1 (71) First, we will derive an expression for the state-dependent KL-divergence, which measures the difference between the learned reverse posterior q(cid:0)zszt, xθ(zt, t)(cid:1) and the true reverse posterior q(zszt, x0) conditioned on the training distribution x0. KL(q(zszt, x0)pθ(zszt)) = = (cid:88) zs=ek q(zszt = m, x0) log q(zszt = m, x0) pθ(zszt = m) (cid:88) zs{x0,m} q(zszt = m, x0) log q(zszt = m, x0) pθ(zszt = m) = q(zs = x0zt = m, x0) log + q(zs = mzt = m, x0) log q(zs = x0zt = m, x0) pθ(zs = x0zt = m) q(zs = mzt = m, x0) pθ(zs = mzt = m) sw (cid:0)(cid:0) x0 log = (cid:18)(cid:18) (cid:19) sw tw + t 1 (cid:19) + (cid:18)(cid:18) sw tw (cid:19) + (cid:19) x0 log 1(cid:1) x0 (cid:0)(cid:0) (cid:0)(cid:0) sw (cid:1) + ts 1(cid:1) tw (cid:1) + ts sw tw 1(cid:1) (cid:1) + tw 1(cid:1) (cid:1) + (cid:0)(cid:0) sw tw xθ(zt, t) xθ(zt, t) (72) 43 In the case where the true token x0 = b, we can simplify to (cid:19) KL(q(zszt, x0)pθ(zszt)) = + (cid:0) sw tw + (cid:1) (cid:1) 0 x0 0 xθ(zt, t) log (cid:0) sw tw 0 xθ(zt, t)(cid:1) log (cid:0)x (cid:18) (cid:18) sw tw + 1 (cid:19) sw tw (cid:18) tw sw tw 1 (cid:19) = = logx0, xθ(zt, t) (73) Substituting = 1 , we can simplify sw to (cid:19)w (cid:18) sw ="
        },
        {
            "title": "1\nT",
            "content": "(cid:18) (cid:20) = 1 (cid:18) = tw 1 (cid:18) = tw 1 (cid:19)(cid:21)w (cid:19)w ] + 1 tT 1 tT tT wtw1 = tw + two (cid:19)(cid:19) (cid:18) 1 2 (cid:18) 1 2 (cid:19) ((1 + x)w = 1 + wx + o(x2)) (74) (75) Now, we can write KL(q(zszt, x0)pθ(zszt)) = (cid:16) tw tw wtw + two (cid:0) 1 tw 2 (cid:1)(cid:17) logx0, xθ(zt, t) (cid:32) wtw = = (cid:18) tT 2 two (cid:0) 1 tw (cid:18) 1 2 (cid:19)(cid:19) (cid:33) (cid:1) logx0, xθ(zt, t) logx0, xθ(zt, t) In the case where the true token x0 = b, we can simplify to KL(q(zszt, x0)pθ(zszt)) = (cid:16) 1 (cid:17) log (cid:0)1 (cid:1) 0 x0 0 xθ(zt, t) (cid:0)1 log (cid:0)x (cid:1) 0 xθ(zt, t)(cid:1) = = (cid:17) (cid:19) (cid:16) 1 (cid:18) logx0, xθ(zt, t) (76) Similarly, substituting = 1 , we have KL(q(zszt, x0)pθ(zszt)) = (cid:33) (cid:1) (cid:32) (cid:0)t 1 logx0, xθ(zt, t) 1 tT Now, we can combine the two cases using the indicator functions 1[x0 = b] that evaluates to 1 when x0 = and 0 otherwise and 1[x0 = b] that evaluates to 1 when x0 = and 0 otherwise. Since this definition of KL divergence is only applicable when zt = m, we have logx0, xθ(zt, t) = (77) KL(q(zszt, x0)pθ(zszt)) (cid:20) = 1[x0 = b] (cid:18) tT (cid:19)(cid:19) (cid:18) 1 2 logx0, xθ(zt, t) 1[x0 = b] (cid:21) logx0, xθ(zt, t) (78) 1 tT 44 Substituting this back into the equation for the discrete-time diffusion loss, we get (cid:20) (cid:18) KL Eq(xtx0) (cid:20) (cid:19)(cid:21) (cid:12) (cid:12)pθ(zszt) q(zszt, x0)(cid:12) (cid:12) (cid:18) (cid:18) 1 2 tT (cid:19)(cid:19) logx0, xθ(zt, t) LNELBO = t{ 1 , ,...,1} = t{ 1 , 2 ,...,1} Eq(xtx0)T 1[x0 = b] 1[x0 = b] 1 tT logx0, xθ(zt, t) (cid:21) = t{ 1 , 2 ,...,1} (cid:20) Eq(xtx0) 1[x0 = b] 1[x0 = b]"
        },
        {
            "title": "T\ntT",
            "content": "logx0, xθ(zt, t) (cid:19)(cid:19) (cid:18) 1 2 (cid:18) wT tT (cid:21) logx0, xθ(zt, t) = t{ 1 , 2 ,...,1} (cid:20) Eq(xtx0) 1[x0 = b] (cid:18) o (cid:19)(cid:19) (cid:18) 1 logx0, xθ(zt, t) 1[x0 = b] 1 logx0, xθ(zt, t) (cid:21) Finally, taking the limit as , the higher-order term limT (cid:0) 2 (cid:1) = 0 and we get NELBO = lim"
        },
        {
            "title": "LNELBO",
            "content": "= lim t{ 1 , 2 ,...,1} (cid:20) Eq(xtx0) 1[x0 = b] (cid:18) o (cid:19)(cid:19) (cid:18) 1 logx0, xθ(zt, t) 1[x0 = b] (cid:21) logx0, xθ(zt, t) 1 (cid:20) = EtU (0,1]Eq(xtx0) 1[x0 = b] logx0, xθ(zt, t) 1[x0 = b] (cid:21) logx0, xθ(zt, t) 1 (79) (80) which is the continuous-time NELBO loss for single token. Therefore, the loss across sequence of tokens denoted as x(ℓ) 0 , we have logx0, xθ(zt, t) (cid:88) ℓ:x (ℓ) 0 =b 1 logx0, xθ(zt, t) (81) (cid:21) NELBO = EtU (0,1]Eq(xtx0) (cid:20) (cid:88) ℓ:x which proves the loss defined in (12). (ℓ) 0 =b B.4 Gradient Flow of Invalid Loss In this section, we show that the penalty for invalid token samples through the arg max function on predicted logits can be effectively back-propagated through the model parameters via our softmax scaling strategy that bypasses the undifferentiable arg max function. Here, we will denote the θ (zt, t)(cid:1) with the highest probability as x(ℓ) predicted probability for the token = arg maxj and all remaining token probabilities as x(ℓ) (cid:0)x(ℓ) θ,j for = [1 . . . V]. θ,k First, we define the softmax function as SM(cid:0)x(ℓ) θ,k (cid:1) = The partial derivative of the softmax probability xj x(ℓ) θ,j (cid:32) exp(x(ℓ) θ,k) (cid:80)V j=1 exp(x(ℓ) θ,j) (cid:18) x(ℓ) θ,j (cid:33) = exp(x(ℓ) θ,k) (cid:80)V exp(x(ℓ) θ,k) j=1 exp(x(ℓ) θ,j) θ for every token is given by equation (cid:19) (cid:16)(cid:80)V (cid:17) j=1 exp(x(ℓ) θ,j) (cid:16)(cid:80)V x(ℓ) θ,j (cid:80)V (cid:17) (cid:18) j=1 exp(x(ℓ) θ,j) (82) j=1 exp(x(ℓ) θ,j) (cid:19) (cid:16) (cid:17) exp(x(ℓ) θ,k) 45 (83) Therefore, we have two cases for the derivative: first, the derivative with respect to x(ℓ) θ,k which denotes the predicted probability for the token that was sampled, and second, the derivative with respect to x(ℓ) θ,j for = which denotes the predicted probabilities for all remaining tokens. For the first case when = k, the partial derivative simplifies to x(ℓ) θ,k (cid:32) exp(x(ℓ) θ,k) (cid:80)V j=1 exp(x(ℓ) θ,j) (cid:33) = exp(x(ℓ) θ,k) (cid:80)V θ,j) exp(x(ℓ) θ,k) exp(x(ℓ) θ,k) j=1 exp(x(ℓ) (cid:16)(cid:80)V (cid:17)2 j=1 exp(x(ℓ) θ,j) (cid:33) (cid:32) (cid:80)V j=1 exp(x(ℓ) (cid:80)V θ,j) exp(x(ℓ) θ,k) j=1 exp(x(ℓ) θ,j) (cid:32) exp(x(ℓ) θ,k) = (cid:80)V j=1 exp(x(ℓ) θ,j) (cid:16) (cid:17) = SM(x(ℓ) 1 SM(x(ℓ) θ,k) θ,k) (cid:33) (84) For all = k, the derivative simplifies to x(ℓ) θ,j (cid:32) exp(x(ℓ) θ,k) (cid:80)K j=1 exp(x(ℓ) θ,j) (cid:33) = 0 exp(x(ℓ) (cid:16)(cid:80)K θ,j) exp(x(ℓ) θ,k) (cid:17)2 j=1 exp(x(ℓ) θ,j) (cid:32) exp(x(ℓ) θ,j) = (cid:80)V j=1 exp(x(ℓ) θ,j) θ,j)SM(x(ℓ) θ,k) = SM(x(ℓ) (cid:33) (cid:32) exp(x(ℓ) θ,k) (cid:80)V j=1 exp(x(ℓ) θ,j) (cid:33) (85) The parameters θ are updated such that the predicted probability of sampling the token ℓ with arg max probability x(ℓ) θ,k which resulted in an invalid peptide SMILES sample is reduced. The gradient update is minimized for predicted probabilities near 0 and 1, suggesting that the loss function pushes the model towards higher confidence predictions from uncertain predictions to minimize invalid sampling. θ,k x(ℓ) x(ℓ) θ,k η SM(x(ℓ) θ,k) (cid:17) (cid:16) 1 SM(x(ℓ) θ,k) (86) where η is the learning rate. Conversely, the parameters of the remaining tokens x(ℓ) θ,j are updated such that the predicted probability of sampling the other tokens increases in proportion to their original softmax probabilities. This prevents extreme changes in the predicted probabilities of the remaining tokens and ensures that the token distribution remains relatively consistent with the previous iteration. θ,j x(ℓ) x(ℓ) θ,j + η SM(x(ℓ) θ,j)SM(x(ℓ) θ,k) (87) Here, we show the theoretical basis of the gradient update that effectively updates parameters to reduce the position-specific token probabilities that result in invalid sequence samplings and push the model predictions toward other high-likelihood tokens."
        },
        {
            "title": "C Model Architectures and Training Details",
            "content": "C.1 RoFormer Architecture Details To predict the token probabilities at each reverse step xθ(zt, t), we trained RoFormer model [35] that leverages rotary positional embeddings (RoPE) robust to varying input lengths and long-range dependencies between tokens. The specific hyperparameters of our model are given below. 46 Table 8: Roformer Architecture Hyperparameters Hyperparameter"
        },
        {
            "title": "Input Dimension\nHidden Dimension\nIntermediate Dimension\nNumber of Layers\nAttention Heads\nMax Positional Embeddings\nHidden and Attention Dropout Probability",
            "content": "PepTune 581 (vocab size) 768 3072 8 8 1035 0.1 C.2 Target-Binding Prediction Model We trained multi-head cross-attention network with ESM-2 [44] protein sequence embeddings and PeptideCLM [47] peptide SMILES embeddings for the target binding affinity model. We trained on 1806 sequences from the PepLand [39] canonical and non-canonical binding datasets containing the protein-target sequence, peptide SMILES sequence, and the experimentally-validated Kd/Ki/IC50 binding affinity score. After training for 50 epochs, the regression model achieved strong Spearman correlation coefficient of 0.949 on the test dataset and 0.633 on the validation dataset. Table 9: Target-Binding Affinity Predictor Protein Dimension Peptide Dimension Layers Embedding Module Linear Layer Layer Norm Cross-Attention 3 Multi-Head Attention (h = 8) Linear Layer ReLU Dropout Linear Layer Shared Prediction Head Linear Layer ReLU Dropout Regression Head Classification Head 1280 512 512 2048 2048 2048 512 768 512 512 512 2048 2048 2048 512 1024 1024 1024 1 3 C.3 Boosted Trees for Peptide SMILES Property Prediction Here, we present the details on the training and hyperparameters of our trained XGBoost boosted tree regression model for membrane permeability prediction and the boosted tree binary classification model for solubility, hemolysis, and non-fouling. 47 Table 10: XGBoost Hyperparameters for Classification and Regression Classification Hyperparameters Regression Hyperparameters Hyperparameter Value/Range Hyperparameter Value/Range"
        },
        {
            "title": "Objective\nLambda\nAlpha\nColsample by Tree\nSubsample\nLearning Rate\nMax Depth\nMin Child Weight\nTree Method",
            "content": "binary:logistic Objective [1e8, 10.0] [1e8, 10.0] [0.1, 1.0] [0.1, 1.0] [0.01, 0.3] [2, 30] [1, 20] hist"
        },
        {
            "title": "Lambda\nAlpha\nGamma\nColsample by Tree\nSubsample\nLearning Rate\nMax Depth\nMin Child Weight\nTree Method\nScale Pos Weight",
            "content": "reg:squarederror [0.1, 10.0] (log scale) [0.1, 10.0] (log scale) [0, 5] [0.5, 1.0] [0.6, 0.9] [1e5, 0.1] [2, 30] [1, 20] hist [0.5, 10.0] (log scale) C.4 PDB Docking Structures Table 11: PDB structures used for docking. Protein PDB 6A9P GFAP TfR 3KAS GLP-1R 3C5T 7L0J AMHR2 5LM4 GLAST 2HAZ NCAM1 1LDJ RBX C.5 Evaluation Metrics Validity is defined as the fraction of peptide SMILES that pass our SMILES2PEPTIDE filter, indicating that it translates to synthesizable peptide. Uniqueness is defined as the fraction of mutually distinct peptide SMILES. Diversity is defined as one minus the average Tanimoto similarity between the Morgan fingerprints of every pair of generated sequences, which measures the similarity in structure across generated peptides. Diversity = 1 1 (cid:0)Ngenerated (cid:1) (cid:88) i,j (xi) (xj) (xi) + (xj) (xi) (xj) (88) where (xi) and (xj) are the 2048-dimensional Morgan fingerprint with radius 3 for pair of generated sequences xi and xj. Similarity to Nearest Neighbor (SNN) is defined as the maximum Tanimoto similarity between generated sequence xi with sequence in the dataset xj. SNN = max jD (cid:18) (xi) (xj) (xi) + (xj) (xi) (xj) (cid:19) Randomness is defined as the Shannon Entropy [106] on tokenized sequences given as: = (cid:88) pi log2(pi) 48 (89) (90) where pi is the probability of i-th unique token divided by the total number of tokens in the sequence. KL-Divergence is defined as the divergence between the token distribution in the generated peptide SMILES pi and the token distribution in the training data. KL(P (cid:12) (cid:12) (cid:12) (cid:12)Q) = (cid:40) pi log2( pi ) qi pi log2( pi 109 (cid:88) iV if qi > 0 if qi = 0 ) (91) where pi is the probability of token in the training data, and qi is the probability of token in the generated data."
        },
        {
            "title": "D Further Experiments",
            "content": "D.1 Case Study for Time-Dependent Multi-Objective Guidance Some properties of peptides require more intense guidance towards specific structural or sub-structural features, while others may only require small changes in the sidechain composition or non-natural modifications. To enable the prioritization of properties during guidance, we introduce timedependent multi-objective guidance strategy that guides the generation based on only subset of properties depending on the current iteration number of the MCTS search (Algorithm 2). To achieve this, we define K-dimensional vector = [i1, i2, . . . , iK] where each ik is the iteration number to begin guidance for the kth objective. Properties where ik = 0 are used to guide all iterations, whereas properties where ik > 1 are used to guide only the iterations from ik Niter. Our time-dependent guidance operates as follows. During the expansion and rollout steps on iteration i, the rolled-out child sequences xs,i that are non-dominated across the sub-vector of property scores si = [sk ik Niter] dependent on the iteration are added to the Pareto-optimal set . Therefore, xs does not need to be non-dominant in the properties where ik > i. Similarly, only the sequences that are dominated in the subset of properties represented in si(x) are removed from . = (cid:8)(zs, s(xs)) si(xs) si(x)(cid:9) = (cid:8)x xs children(zt) s.t. si(xs) si(x)(cid:9) (92) (93) Then, during selection, we only consider the cumulative rewards Ui = [uk ik Niter] for the properties where ik s.t. ik Niter to form the set of Pareto-optimal children nodes that will be chosen from uniformly at random. select (cid:8)zs Ui(xs) Ui(x)(cid:9) select = = (cid:8)x xs children(zt) s.t. Ui(xs) Ui(x)(cid:9) (94) (95) select similarly to the time-independent case. Then, we select the next node zs To test this strategy, we generated 100 peptides conditioned only on membrane permeability for the first 50 iterations since we found it as the most challenging property to optimize. Then we conditioned all properties including membrane permeability, binding affinity to GFAP, solubility, hemolysis, and non-fouling. We show that during the first 50 iterations, all properties except membrane permeability show relatively constant average scores, whereas the permeability score increased (Fig ??). Then, after the 50 iteration mark, GFAP binding affinity and solubility curves increased significantly while the hemolysis and non-fouling curves increased slightly for the remainder of the iterations (Fig ??). Although all the results in this paper leverage peptides without time-dependent guidance, this serves as proof of concept for future experiments varying the start times across properties and testing the generated peptides to refine additional properties at later time steps where the generated sequences are already constrained to certain predefined substructures. D.2 Ablation Studies In this section, we report the performance of our multi-objective MCTS-guided discrete diffusion model over different hyperparameter settings. Specifically, we discuss the effects of varying the 49 Figure 16: Time-Dependent Multi-Objective Guidance. (A) Plot of average membrane permeability score for 50 sampled sequences in the expansion and rollout step over iterations where the MCTS search is conditioned on permeability for all iterations. (B) Plot of average predicted binding affinity score to GFAP over iterations when conditioned starting from epoch 50. (C, D, E) Plot of average predicted solubility, hemolysis, and non-fouling scores over iterations when conditioned starting from epoch 50. Pink dotted lines denote the iteration where the MCTS search began conditioning on the property number of iterations, the number of expanded children nodes, the weight of invalid penalty, the weight of binding affinity reward, and the batched sampling method. Number of Children. The number of children is the hyperparameter that determines the batch size during the expansion step of MCTS. small number of expanded nodes would limit the degree of exploration and number of generated sequences for evaluation at each iteration. If the initial iterations resulted in sub-optimal unmasking steps for all children, this could prevent the algorithm from discovering local or global optimum across objectives. Suppose the number of children is too large. In that case, this can result in lack of diversity if several sequences from the same expansion step are added to the Pareto-optimal set given their sequence similarity, leading to similar property scores. large also slows down runtime significantly. To determine value for within the two extremes, we evaluated the performance of the MCTS search for = 10, 50, 70, 100. Overall, we found that = 50 yields consistently increasing scores across all properties, which we use for the remainder of the study. Number of Iterations. The number of iterations Niter determines the number of selection, expansion, rollout, backpropagation loops are executed in single MCTS run. In addition, Niter is the maximum value of or the number of unmasking steps that can be executed before the rollout begins, which corresponds to the maximum tree depth. We found that equating the number of diffusion steps to the number of MCTS iterations Niter results in convergence on the property prediction scores as the selection process becomes biased towards single unmasking scheme. As shown in Figure 6, all property scores converge for Niter = = 128, which we use for the remainder of the study. Tokenization Scheme. To evaluate the effect of different tokenization methods on the generation quality, we experimented with three different tokenization schemes: pre-trained PeptideCLM SMILES Pair Encoding (SPE) tokenization, and Atom Pair Encoding (APE) tokenization trained on the 30K fine-tuning dataset of valid peptide SMILES. Table 12: Effect of Tokenization on Sequence Length, Training, and Validation Loss after Convergence Tokenization Scheme Vocab Size Avg Sequence Length in Data Train Loss () Val Loss () Val PPL () SMILES SPE Tokenizer SMILES APE Tokenizer SELFIES APE Tokenizer 581 605 605 0.65 1.33 1.56 0.75 2.32 2.50 2.12 10.18 12.12 84 19"
        },
        {
            "title": "E Algorithms",
            "content": "Algorithm 1 outlines the forward pass diffusion process (training) for PepMDLM, our unconditional peptide SMILES generator. Algorithms 2, 3, 4, and 5 describe PepTune, our MCTS-guided peptide SMILES generator. Algorithms 6 and 7 describe the bond mask function and peptide sequence decoder which can also act as validity filter. Algorithm 1 PepMDLM Training Inputs: Batched training examples x0 Output: Pre-trained unconditional MDLM for peptide SMILES generation xθ(zt) procedure TRAIN Sample Uniform(0, 1) state-dependent masking schedule αt(x0) (cid:0)1BONDMASK(x0)(cid:1)(1 t)+BONDMASK(x0)(1 tw) mask each sequence in batch at varying degrees Sample zt Cat(zt; (αt(x0))x0 + (1 αt(x0)m) xθ(zt, t) RoFormerθ(zt, t) if z(ℓ) = then x(ℓ) θ (zt, t) zt else if z(ℓ) = then x(ℓ) θ (zt, t) x(ℓ) θ (zt, t) sample continuous times carry-over unmasking zero-masking probability predict token logits with RoFormer backbone (cid:18) 1 (cid:80) ℓ:xℓ=b logxθ(zℓ t), xℓ + (cid:80) ℓ:xℓ=b logxθ(zℓ t), xℓ (cid:19) SM(cid:0)x(ℓ) (cid:19) θ (zt, t)(cid:1) 1[x0 is Invalid] total loss update backbone parameters to minimize loss (cid:80) ztB end if NELBO 1 B 0 arg max x(ℓ) x(ℓ) Linvalid 1 L θ θ ηθL ztB NELBO + Linvalid (cid:80) θ (zt, t) (cid:18) (cid:80)L ℓ=1 x(ℓ) end procedure 51 Algorithm 2 PepTune: Multi-Objective Guided Discrete Diffusion with Monte-Carlo Tree Search (MCTS) Inputs: Pre-trained MDLM denoiser pθ(zszt), score function s(x) : RK containing classifiers for objectives s1, s2, . . . , sK, number of time steps , number of iterations Niter Output: Set of Pareto-optimal sequences for the objectives and their K-dimensional classifier score vectors = {(x )} procedure SAMPLEPEPTUNE , initialize fully masked sequence initialize Pareto-front select expandable leaf node unmasked to time t(n) initialize vector that will store the sum of all rewards from expanded children zT [MASK]L {} for = 1, . . . , Niter do zt(n), t(n) SELECT(zT ) 0 children(zt) BATCHEDREVERSESTEP(zt) num_rollout_steps t(n) [ , . . . 1 ] dt 1 for = 1, . . . , do , n1 zt zs,i for in range(num_rollout_steps) do pθ(zszt) REVERSEDIFFUSIONSTEP(zt) zs pθ(zszt) rollout to fully unmasked sequence end for xs,i arg max (cid:0)xθ(zt, 1)(cid:1) s(xs,i) s(xs,i) r(zs,i), UPDATEPARETOFRONT(cid:0)P , (zs,i, s(zs,i))(cid:1) add sequence if nondominant children(zt).append(cid:0)zs,i, s(zs,i)(cid:1) get clean sequence compute score vector add child node + r(zs,i) end for while not None do U(z) U(z) + Nvisits(z) Nvisits(z) + 1 parent(z) end while end for return end procedure add child reward to total reward for node zt backpropagate scores Return Pareto-optimal sequences 52 Algorithm 3 Batched Reverse Step Inputs: Partially unmasked sequence zt at time (representing the selected node in MCTS search), value of for top sampling (k = 0 for batched Gumbel-max sampling), total time steps Output: Set of slightly unmasked sequences children(zt) = {zs,1, . . . zs,M } at time that become the child nodes of zt procedure BATCHEDREVERSESTEP children(zt) {} xθ(zt, t) RoFormerθ(zt, t) 1 if zt = then pθ(zszt) (cid:0)(cid:0) pθ(zs = mzt) 0 sw tw (cid:1) + ts 1(cid:1) xθ(zt, t)zs + (cid:0)(cid:0) sw tw t 1(cid:1) (cid:1) + zero-masking probability xθ(zt, t)m else if zt = then zs zt carry-over unmasking end if us Uniform(0, 1)LV Gs log ( log(us + ϵ) + ϵ) for = 1 . . . do if = 0 then pθ,i (cid:0)zszt else if > 0 then (cid:0)zszt) log pθ (cid:18) (cid:0)zszt) SM pθ,i end if define slightly different distribution for each sample in batch (cid:1) + Gs,i batched Gumbel-max distributions topk(cid:8) log(pθ (cid:0)zszt)(cid:1) + Gs,i (cid:19) (cid:9) batched top sampling end for for = 1 . . . do zs,i pθ,i children(zt).append(zs,i) end for return children(zt) end procedure 53 Algorithm 4 Selection Inputs: Masked root node zt(T ) Output: Expandable leaf node zt procedure SELECT while True do if zt is non-leaf node and = 0 then select {} for zs,i in children(zt) do initialize Pareto front of select scores if zs,i is non-leaf or expandable leaf node then select UPDATEPARETOFRONT(P compute normalized cumulative reward vector select, (zs,i, U(zs,i))) update Pareto fronU(zs,i) W(zs,i) Nvisit(zs,i) tier end if end for set parent node for next iteration as child node selected uniformly at random from Pareto-optimal set zt select SELECT(zt) else if = 0 then SELECT(zT ) else return zt end if end while end procedure recursively call select until leaf node is reached node is already fully unmasked restart selection process from root node return leaf node for expansion 54 Algorithm 5 Update Pareto Front Inputs: Current Pareto-front sequences and score vectors = {(x sequence and score vector (zs, s(xs)) Output: Reward vector r(zs) and updated Pareto-optimal set procedure UPDATEPARETOFRONT , )}, newly sampled if is empty then .append((zs, s(xs))) r(zs) 1K else set reward vector to ones vector of boolean flags indicating which sequences are nondominant to nondominateFlag new bool[P ] toDelete {} r(zs) 0K for (x ) in do , set reward vector to zeroes define vector with 1 where xs is non-dominated in the property [nk = 1 if sk(xs) k,i] define vector with 1 where xs,i is dominant in the property [dk = 1 if sk(xs) k] r(zs,i) r(zs) + if (nk s.t. nk = 1) (dk s.t. dk = 1) then update reward vector dominates is not dominated by x dominates toDelete.append(x) nondominateFlag[i] True else if nk s.t. nk = 1 then nondominateFlag[i] True else nondominateFlag[i] False end if end for if xs is either dominant or non-dominated by all in Pareto-optimal set , then add to if nondominateFlag[i] = True then .append(zs, s(xs)) end if for in toDelete do .delete(x, s) end for end if return r(zs), end procedure return reward vector and updated Pareto-optimal set Algorithm 6 BondMask Inputs: List of peptide SMILES strings smiles_list Output: Position-wise bond mask for each sequence with 1 in positions of peptide bonds and 0 otherwise mask procedure BONDMASK bond_patterns [(rOC(=O), ester), (rN(C)C(=O), n_methyl), (rC(=O)N(C), n_methyl), (rN[12]C(=O), peptide), (rC(=O)N[12]?, peptide) ] for batch_idx,smiles in enumerate(smiles_list) do positions empty_list() used empty_set() for pattern,bond_type in bond_patterns do for match in re.finditer(pattern,smiles) do list to store bond positions set to track used positions identify bonds using patterns if not any(p range(match.start(), match.end()) for in used) then positions.append(cid:0){start: match.start(), end: used.update(cid:0)range(match.start(), match.end())(cid:1) type: bond_type, pattern: match.end(), match.group()}(cid:1) end if end for end for for pos in positions do update the mask for the current SMILES mask[batch_idx, pos[start]:pos[end]] 1 end for end for return mask end procedure 56 Algorithm 7 SMILES2PEPTIDE Inputs: SMILES String Output: Batch of sequences at time s. procedure ANALYZER if is correct SMILES format then, if contains peptide bond [NC(=O)] or N-methylated peptide bond [N(C)C(=O)] then, IS_PEPTIDE TRUE positions empty_list() for pattern,bond_type in bond_patterns do for match in re.finditer(pattern,smiles) do positions BONDMASK segments empty_list() positions.sort() if positions[0][start] > 0 then, first segment segments.append(cid:0) content: smiles[0:positions[0][start]], bond_after: positions[0][pattern] ) end if for in len(positions)-1 do current = positions[i] next_pos = positions[i+1] segments.append(cid:0) content: smiles[current[end]:next_pos[start]], bond_before: current[pattern], bond_after: next_pos[pattern] ) other segments end for if positions[-1][end] < len(smiles) then, segments.append(cid:0) content: smiles[positions[-1][end]:], last segment bond_after: positions[-1][pattern] ) end if end for end for for residue in segments do residue Regex pattern end for end if end if end procedure Empirical Amino Acid Regex Pattern"
        }
    ],
    "affiliations": [
        "Center of Computational Biology, Duke-NUS Medical School",
        "Department of Biomedical Engineering, Duke University",
        "Department of Biostatistics and Bioinformatics, Duke University",
        "Department of Computer Science, Duke University",
        "Management and Technology Program, University of Pennsylvania"
    ]
}