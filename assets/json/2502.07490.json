{
    "paper_title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
    "authors": [
        "Xialie Zhuang",
        "Zhikai Jia",
        "Jianjin Li",
        "Zhenyu Zhang",
        "Li Shen",
        "Zheng Cao",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models."
        },
        {
            "title": "Start",
            "content": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Xialie Zhuang * 1 2 Zhikai Jia * 2 Jianjin Li * 3 Zhenyu Zhang 4 Li Shen 5 Zheng Cao 2 Shiwei Liu"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 1 1 ] . [ 1 0 9 4 7 0 . 2 0 5 2 : r Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latters in-context retrieval capabilities. Specifically, MEAP first randomly masks small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoderdecoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lostin-the-middle scenarios, outperforming NTP by 11.77% percentage points. Our analysis indicates that MEAPs effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on reduced set of non-masked tokens. This mechanism improves the models focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as promising training paradigm for large language models.Code is provided in the link. *Equal contribution 1School of Artificial Intelligence, University of Chinese Academy of Sciences, China 2SCITIX (SGP) TECH PTE. LTD., Singapore 3South China Normal University, China 4University of Texas at Austin, USA 5Sun YatSen University, China 6University of Oxford, UK. Correspondence to: Zheng Cao <zcao@scitix.ai>, Shiwei Liu <shiwei.liu@maths.ox.ac.uk>. The entire development process relies on the Siflow platform (https://scitix.ai/), provided by SCITIX (SGP) TECH PTE. LTD. 1 Next-token prediction (NTP) (Radford, 2018) is the foundational training objective for many large language models (LLMs), including OpenAIs GPT series (Brown, 2020). NTP trains models to predict the next word (or token) in sequence, given all preceding tokens. Its scaling efficiency and exceptional performance in text generation have established it as the dominant paradigm for state-of-the-art LLMs such as GPT-4 (Achiam et al., 2023), LLaMa3 (Dubey et al., 2024), Gemini 1.5 Pro (Team et al., 2024), and DeepSeekV3 (Liu et al., 2024a). However, recent studies highlight the limitations of NTP-based LLMs in accurately retrieving key information from context (Liu et al., 2024b; Kamradt, 2023). In contrast, masked language modeling (MLM), used in BERT (Devlin, 2018), adopts denoising objective that reconstructs masked inputs using bidirectional attention. This cloze-type nature makes MLM particularly effective for tasks requiring precise information retrieval and sentencelevel understanding. However, MLMs inherent focus on reconstructing masked tokens reduces its effectiveness in tasks requiring coherent and long-form text generation (Wang & Cho, 2019; Dong et al., 2019). While intuitively appealing, combining NTP and MLM to leverage their respective strengths remains non-trivial challenge. MLM typically operates best within two-stack encoder-decoder architectures, and performance degrades significantly when applied to decoder-only Transformers (Tay et al., 2022). Efforts to integrate the two often rely on unified pre-training pipelines where multiple objectives are alternated during the pretraining process (Dong et al., 2019; Tay et al., 2022). However, this multi-objective approach introduces substantial complexity to the training pipeline, making it cumbersome to scale, especially for models with billions or trillions of parameters. To this end, we propose Mask-Enhanced Autoregressive Prediction (MEAP), simple yet effective LLM training paradigm that seamlessly integrates masked tokens into next-token prediction. Specifically, we first randomly mask small fraction of the input tokens and then directly perform standard next-token prediction using decoder-only Transformer in an autoregressive manner. This straightforward modification eliminates the need for bidirectional attention Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More respectively. In Section 5, we further analyze the underlying reasons for MEAPs effectiveness. Section 6 provides an ablation study, and we conclude the paper in Section 7. 2. Related Work Masked Language Modeling. Pre-training is one of the most important pillars of LLMs. BERT first trained bidirectional, encoder-only Transformer with masked language modeling (MLM), where the model is trained to predict masked input tokens. XLNet (Yang, 2019) introduced the Permutation-based Language Modeling to account for dependencies between masked tokens during training. RoBERTa(Liu, 2019) further improves the pre-training of BERT by training the model longer, over more data, with longer sequences, etc. MLM was further advanced by T5 (Roberts et al., 2019). Specifically, T5 frames every text processing task as text-to-text problem, leveraging increased lengths of corrupted tokens to achieve improved performance on classification tasks, which has contributed to its growing popularity. However, these models have shown limited performance in open-text generation and in-context learning, limiting their usage in modern LLMs. Next Token Prediction. In parallel vein, Radford et al. (2019) proposed next-token prediction (NTP) where decoder-only Transformer is trained to predict the next token from left to right using unidirectional attention ensured by casual mask. By predicting the next token based on previously generated tokens and the given input context, NTP maintains coherence and logical flow in the generated text, well-suited for text generation. Moreover, NTP eliminates the need for an encoder, significantly improving the scalability of language models. Due to the above advantages, NTP serves as the most popular pre-training objective of modern LLMs (Brown, 2020; Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2023; Yang et al., 2024; Liu et al., 2024a). Unified Training Paradigms. There are works that propose unified training paradigms aiming to train one Transformer with multiple objective functions. For instance, UniLM (Dong et al., 2019) trains bidirectional encoder on unidirectional language modeling (LM), bidirectional LM, and Sequence-to-Sequence LM. UL2 (Tay et al., 2022) proposes unified pre-training paradigm with Mixture-of-Denoisers (MoD) to combine diverse pre-training paradigms together, improving the performance over T5 and GPT. While effective, the preference for encoder-decoder architectures and the complicated switches among different training objectives hinder their applications in practice. In contrast, our approach seamlessly integrates masked tokens into NTP without incurring any additional pre-training or inference costs, while preserving the ultra-efficiency of Figure 1. Overview of next token prediction, masked language modeling, and our MEAP. or an expensive encoder-decoder architecture, thereby incurring no additional computational overhead during training. During inference, our resulting LLMs can work as simply as LLMs that are trained with NTP with no extra engineering effort. The simplicity of MEAP enables us to enhance LLMs performance of key information retrieval and longcontext reasoning, while retaining the impressive scaling efficiency of decoder-only LLMs. Figure 1 shows the illustrations of different training paradigms. As general pre-training paradigm, MEAP works effectively for scenarios of pre-training and fine-tuning. For the pre-training setting, we conduct control experiments by pre-training 1.1B LLaMa-style LLMs (Zhang et al., 2024) with NTP and MEAP, where the training tokens scale from 40B to 200B. Our results demonstrate that MEAP substantially improves the performance in key information retrieval tasks such as Needle in Haystack (Kamradt, 2023) by up to 33% average score and Multi-Document Question Answering (MDQA) (Liu et al., 2024b) by up to 27.2 percentage points, while preserving general knowledge learned during pre-training. It is noteworthy that MEAP achieves 85.8% accuracy with 60B training tokens on the Needle in Haystack, while NTP requires 200B for similar performance, highlighting MEAPs superior data efficiency in key information retrieval. In addition, compared to the original NTP, MEAP also suffers less from hallucination. In addition, the promise of MEAP also holds for LLM fine-tuning. Our MEAP framework demonstrates consistent improvements across multiple commonsense reasoning tasks, achieving an average gain of 1.12 scores over the NTP baseline. On Multi-Document Question Answering, MEAP achieves an average improvement of 11.77% across all positions. Our analysis suggests that MEAPs effectiveness stems from its ability to enhance attention distinguishability by focusing on reduced set of non-masked tokens. This mechanism sharpens the models attention to task-relevant signals while reducing the impact of peripheral context. In essence, MEAP learns more by attending to fewer tokens. The structure of this paper is as follows. Section 3 details the MEAP algorithm. The evaluation of MEAP on LLM pretraining and fine-tuning is presented in Sections 4.1 and 4.2, 2 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More We carefully selected the masking ratio, = 15% for pretraining, to ensure that the model receives an adequate level of training difficulty and learning signals, without excessively disrupting the pre-training process. The relatively moderate number of masked tokens allows this approach to be seamlessly integrated into existing NTP frameworks, without significantly increasing pre-training overhead or altering the original training procedure. LLM fine-tuning. MEAP can also be extended to finetuning scenarios. In this approach, we duplicate the training samples and apply the same random masking strategy to the copied sequences during fine-tuning. The original sequences and their masked counterparts are then combined into single input sequence to feed into the model. Formally, the next-token prediction with causal attention mechanism models the following objective: Figure 2. Training frameworks of MEAP: Left (Pre-training): certain portion of input tokens is randomly masked, followed by standard next-token prediction (NTP). Right (Fine-tuning): Training samples are duplicated, and the random masking strategy is applied to the copied sequences. Standard NTP is then performed on the modified input for fine-tuning. NTP. More importantly, MEAP is more suitable for modern LLMs, as our method does not alter the core mechanism of NTP, the resulting models remain fully compatible with existing pipelines, platforms, and hardware optimized for modern LLMs. pθ(X ) = (cid:89) t=1 pθ(xt x1, . . . , xt1; x1, [mask], . . . , xt1) 3. Mask-Enhanced Autoregressive Prediction In this section, we introduce Mask-Enhanced Autoregressive Prediction (MEAP). LLM pre-training. To enhance the performance of LLM in handling and understanding long texts, particularly in key tasks such as key information retrieval and long context, we designed and implemented simple yet efficient random masking strategy. The core idea of this method is to selectively mask portions of the input during the pretraining phase. Specifically, we employed fixed proportion masking mechanism, where tokens in the input sequence are randomly masked according to predefined percentage . In this way, the model is forced to learn in the absence of some contextual information, which helps improve its deep understanding and reasoning capabilities. Formally, given decoder-only Transformer θ and sequence of input = (x1, x2, ...xn1, xn), we first randomly mask fraction of tokens having = (x1, [mask], ..., xt1, xt). We then perform the standard next-token prediction using the masked input in left-toright manner: pθ(X ) = (cid:89) t=1 pθ(xt x1, [mask], . . . , xt1) This design addresses critical concern: input sequences in supervised fine-tuning often contain key information essential for downstream tasks. Directly masking the original sequence risks removing crucial information, potentially compromising the models performance on the target tasks. Masking the duplicated sequence incorporates MLM to NTP while avoiding this concern. We choose = 10% for finetuning in our experiment. We only perform MEAP for the QA pair whose answer length exceeds 50, otherwise, we perform the standard NTP for the pair. Notably, while MEAP doubles the sequence length during fine-tuning, Figure 5 shows that it achieves superior performance to NTP with only half the training time, essentially attaining stronger results with even fewer training tokens. We believe that the effectiveness of MEAP stems from its ability to promote more distinguishable attention by focusing on fewer tokens during LLM training, as masked tokens typically receive negligible attention. This modification helps the model focus on task-relevant signals while reducing the impact of peripheral context, as verified in Section 5. 4. Experimental Results To evaluate the effectiveness of MEAP in training LLMs, we conduct controlled experiments comparing LLMs pretrained/fine-tuned by MEAP with those trained by NTP. Same as NTP, when the model is tasked with predicting masked token, it employs causal masked attention, using only the preceding tokens to predict the masked token. 4.1. Pre-training Evaluation Setup. We follow the Llama architecture (Touvron et al., 2023) as our base model. Specifically, we train 1.1B 3 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Table 1. Pre-training Evaluation. Zero-shot performance of MEAP and NTP on various commonsense reasoning tasks. Results are measured directly after pre-training on 200B tokens with no fine-tuning. ARC-c ARC-e BoolQ PIQA HellaSwag WinoGrande OBQA Average"
        },
        {
            "title": "NTP\nMEAP",
            "content": "22.9 25.4 55.7 56.4 53.3 59.5 73.6 72.3 44.1 43.4 55.0 55. 23.2 22.6 46.2 47.8 decoder-only Transformers (Vaswani, 2017) following the setting of Zhang et al. (2024). Our model has 24 layers with 32 attention heads, hidden size of 2,048, an intermediate hidden size of 5,632, 32 attention heads, and context length of 4096. We follow the common configurations of LLM components, e.g., Rotary Positional Embedding (RoPE) (Su et al., 2024), Pre-Norm (Ba, 2016) with RMSNorm (Zhang & Sennrich, 2019), SwiGLU (Shazeer, 2020), and Grouped-query Attention (Ainslie et al., 2023). To assess the scalability of MEAP, we increase the training token size from 40B to 60B, and further to 200B. For all experiments, we implement learning rate warm-up during the first 10% of the training steps, followed by cosine annealing schedule, which decays the learning rate to 10% of its initial value. We use the AdamW optimizer with the following settings: β1 = 0.9, β2 = 0.95. The maximum learning rate is set to 4 104, the minimum learning rate is 4 105, and the weight decay is 5 102. 4.1.1. LANGUAGE MODELING EVALUATION While the primary goal of MEAP is to enhance LLM performance in key information retrieval, it is essential to ensure that integrating MLM into NTP does not compromise the models fundamental language modeling capability. To evaluate this, we employ the LM Eval Harness benchmark (Gao et al., 2024), assessing models in zero-shot setting. The results, presented in Table 1, show that MEAP performs comparably to, or even outperforms, NTP, achieving 1.6% improvement in the overall average score. This finding provides strong evidence that incorporating random masking into NTP does not degrade the models language modeling capacity. In the following evaluations, we will examine whether MEAP further improves performance in key information retrieval and long-context modeling. 4.1.2. NEEDLE-IN-A-HAYSTACK RETRIEVAL Table 2. Single needle accuracy (%) with 32K context. Token 40B 60B 200B NTP MEAP 65.9 80. 52.8 85.8 87.1 98.2 For key information retrieval, we choose the wellestablished Needle-in-a-Haystack evaluation (Liu et al., 2024b), where the model is asked to retrieve the random fact or statement (the needle) in the middle of long context window (the haystack). This approach provides quantitative metrics for assessing precise information extraction from extended contexts, particularly relevant for document analysis applications. As this evaluation involves long-context modeling capacity, we follow the setting of Ye et al. (2024) and conduct length extension to 64K. In particular, we continue training our model for additional 4B tokens from SlimPajama (Soboleva et al.) using the approach proposed in (Fu et al., 2024). The implementation utilizes modified Rotary Position Embeddings with θbase = 640, 000. To demonstrate MEAPs scalability, we increase the training token size to 40B, 80B, and 200B, reporting the results of needle retrieval in Table 2. The results show that MEAP consistently outperforms NTP across different training scales. At 40B tokens, MEAP achieves 80.2% accuracy, significantly surpassing the baselines 65.9%. The performance gap peaks at 60B tokens, with MEAP maintaining steady improvement and reaching 85.8% accuracy. At 200B tokens, MEAP approaches optimal performance, attaining 98.2% accuracy, while the NTP baseline still falls short of 90% accuracy. It is noteworthy that MEAP achieves 85.8% accuracy using just 60B training tokens, whereas NTP requires approximately three times as many (200B tokens) to reach similar level. This demonstrates MEAPs superior data efficiency over NTP in key information retrieval. We further illustrate the retrieval performance of our 200Btoken model with 32K context length in Figure 3. The accuracy is reported across varying answer needle depths (y-axis) and context lengths (x-axis). The results show that MEAP generally maintains perfect accuracy across different context lengths and depths, with errors limited to only two grid cells. In contrast, NTP begins to exhibit accuracy degradation at context length of 24K, affecting wide range of depths from 50% to 100%. 4.1.3. MULTI-DOCUMENT QUESTION ANSWERING We report the accuracy improvement of MEAP over NTP in Table 3. MEAP again consistently outperforms NTP 4 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More (a) NTP Pre-training (b) MEAP Pre-training Figure 3. Training dynamics comparison between standard pretraining and MEAP framework. Scores are computed using ROUGE-1, measuring unigram overlap between model responses and expected answers. Table 3. Pre-training Evaluation. Relative accuracy (%) improvement of MEAP over NTP on multi-document QA. Answer Position 1 5 10 documents 20 documents +7.6 +12.4 +7.0 +4.0 +30.6 +5.1 15 +3. 20 +27.2 by good margins across all configurations, with significant gains at later positions (+30.6% at position 3 in 10-doc, +27.2% at position 5 in 20-doc). These results indicate that MEAP enhances the models ability to retrieve relevant information from long contexts, maintain performance across different context lengths and positions, and handle complex scenarios with multiple distractors. The improvements highlight the effectiveness of the masking strategy in enhancing the models overall capability for long-context information retrieval tasks. 4.1.4. LONG-CONTEXT REASONING EVALUATION We evaluate long-context reasoning capabilities using the Multi-Needle Reasoning Task (M-RS) (Li et al., 2024a), which requires models to retrieve and extract multiple pieces of information from long texts and using them to logically answer questions that demand an integrated understanding and reasoning of various text segments. This forces the model to distribute attention across contextually relevant tokens rather than focusing solely on local patterns. We leverage the OpenCompass evaluation framework (Contributors, 2023) and report the results in Figure 4. MEAP consistently outperforms NTP across context lengths with 6.6 percentage point average improvement. demonstrates MEAPs enhanced capacity to maintain attention coherence Figure 4. Long-context reasoning performance comparison between MEAP and NTP on the Multi-Needle Reasoning Task (MRS) across different context lengths. over extended sequences. 4.1.5. CONTEXTUAL HALLUCINATION EVALUATION Table 4. Accuracy (i.e., free of hallucinations) on text summarization datasets. Task XSum MultiNews WikiSum NTP MEAP 0.09 0.13 0.17 0.19 0.24 0.33 Since MEAP improves more accurate key information retrieval, we expect it to suffer less from contextual hallucination. To verify, we evaluate MEAP in reducing contextual hallucinations on three summarization datasets: XSum (Narayan et al., 2018), WikiSum (Cohen et al., 2021), and MultiNews (Fabbri et al., 2019), following Ye et al. (2024). For this setting, we fine-tune the pre-trained models with Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Alpaca and evaluate them. We compare model-generated and reference summaries using Deepseek-V3 (Liu et al., 2024a) as the hallucination detector across 100 random samples per dataset. As shown in Table 4, our masking strategy achieves consistent reduction in hallucination rates across all datasets. 4.2. Fine-tuning Evaluation Setup. We fine-tune the Llama-3-8B (Dubey et al., 2024) on the Alpaca instruction dataset (Taori et al., 2023a). The training configuration uses global batch size of 512. The model is optimized with AdamW (β1 = 0.9, β2 = 0.95), learning rate of 2 105 (with 10% warmup and cosine decay), and weight decay set to 0.01. We retain key architectural components from Llama-3, such as RoPE embeddings (Su et al., 2024), RMSNorm (Zhang & Sennrich, 2019), and grouped-query attention (Ainslie et al., 2023). During fine-tuning, we randomly mask portion of tokens in the assistants response, while keeping the source context intact. Only the masked tokens are predicted during finetuning. The training process uses bfloat16 precision with DeepSpeed Zero Stage 2 (Ren et al., 2021), and the Llama-3 tokenizer (Dubey et al., 2024) with maximum sequence length of 4096 tokens. 4.2.1. LANGUAGE MODELING EVALUATION Similar to the pre-training evaluation, we first assess MEAPs effectiveness in language modeling. Table 5 presents the evaluation results. Our MEAP framework demonstrates consistent improvements across multiple tasks, achieving an average gain of 1.12 scores over the NTP baseline. The performance improvements are particularly notable on ARC-c and WinoGrande, indicating enhanced reasoning capabilities. The results highlight MEAPs effectiveness in fine-tuning complex reasoning tasks. 4.2.2. MULTI-DOCUMENT QUESTION ANSWERING We evaluate MEAPs context-aware reasoning using the multi-document QA task with distractor suppression (Liu et al., 2024b). To ensure fair comparison, we train MEAP for 2 epochs and NTP for 4 epochs, such that both approaches process similar number of tokens. Table 6 quantifies the exact match (EM) improvements across critical document positions in the 20-document setting. MEAP consistently achieves notable gains across all positions, further demonstrating its superiority over NTP. Two key patterns emerge from the experimental results: Figure 5. Comparison of fine-tuning efficiency between MEAP and NTP. MEAP-n refers to MEAP training for epoch. Mid-Context Advantage: The maximum improvement at position 20 (+15.22%) demonstrates enhanced longrange dependency modeling, crucial for connecting concepts across scientific documents. These findings validate MEAPs effectiveness in preserving signal integrity across long contexts while highlighting opportunities for temporal reasoning enhancement and cross-document entity disambiguation. 4.3. Training Efficiency Analysis MEAP introduces no additional overhead for pre-training or inference compared to standard NTP, as the only difference lies in the masking operation. During fine-tuning, MEAP requires duplicating the input sequence and training with doubled sequence length, resulting in increased training overhead. This overhead, however, is effectively amortized by MEAPs higher data utilization efficiency. Specifically, compared to NTP, MEAP requires only 50% of the epochs with similar number of tokens being processed, while outperforming the latter significantly. To verify, we report the results on the multi-document QA retrieval from 20 documents (Liu et al., 2024b), where retrieval performance is assessed by computing the average retrieval values across 5 positions. As shown in Figure 5, single epoch of MEAP training significantly outperforms two epochs of NTP training by large margin while also reducing total training time. This highlights MEAPs data efficiency, achieving similar or better results while reducing computational resources. Consistent Improvement: MEAP achieves substantial gains across all positions with an average improvement of 11.77%, showing robust performance throughout the document range. In summary, MEAP delivers significant training time reductions with improved or comparable performance on the retrieval task, highlighting its efficiency and effectiveness in large-scale training scenarios. Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Table 5. Fine-tuning Evaluation. Performance of MEAP and NTP on various commonsense reasoning tasks. Results are measured by fine-tuning with Llama-3-8B. ARC-c ARC-e BoolQ PIQA HellaSwag WinoGrande OBQA Average"
        },
        {
            "title": "NTP\nMEAP",
            "content": "53.58 55.12 81.10 83.21 83.98 83.82 79.27 81.01 62.74 63.31 72.06 74. 39.40 38.20 67.30 68.42 Table 6. Fine-tuning Evaluation. Accuracy (%) of MEAP and NTP on multi-document QA with 20 documents. Position 1 24.29 33.22 22.82 34.16 10 24.11 36.01 15 25.46 36. 20 31.11 46.33 +8.93 +11.34 +11.90 +11. +15."
        },
        {
            "title": "NTP\nMEAP",
            "content": "5. Why Does MEAP Work? In this section, we attempt to interpret the underlying reasons for the effectiveness of MEAP. We conjecture that MEAPs effectiveness stems from its ability to promote more distinguishable attention by focusing on fewer tokens during LLM training, as masked tokens [MASK] are expected to receive marginal attention scores. While effective, attention mechanisms in LLMs often struggle with long-context understanding, where redundant and non-informative attention is assigned to tokens (Liu et al., 2024b; Li et al., 2024b). plausible explanation is that the attention module relies on the Softmax function to normalize attention scores within (0, 1), which tends to minimize differences among tokens, especially when training on sequences of thousands of tokens. Furthermore, LLMs exhibit phenomenon known as attention sinks, where the initial few tokens receive disproportionately high attention scores compared to the rest (Xiao et al., 2023). Collectively, these factors lead to small and nearly indistinguishable attention scores across tokens, which is generally undesirable. When LLMs fail to properly differentiate between tokens, they are more likely to generate incorrect outputs. By randomly replacing tokens with masks, MEAP implicitly penalizes the attention scores at masked positions, thereby amplifying the attention differences among non-masked tokens. This masking mechanism encourages the model to generate more distinguishable attention scores, allowing it to focus on task-relevant texts while mitigating the influence of peripheral context. We validate this hypothesis through the following experiments. 5.1. Masking Leads to More distinguishable Attention To elucidate the mechanistic impact of our masking strategy on model behavior, we conducted detailed analysis of 7 attention distribution patterns. Our experimental protocol involved sampling 500 sequences. The original unmodified samples refer to the input sequence of NTP XN , and their masked counterparts (same samples with 15% masks), XM , designated as the input for MEAP. These sequence pairs were then processed through our 1.1B models pre-trained with NTP and MEAP, respectively. We compare two values: (1) Attention Score Decay: the percentage decrease in the averaged attention score at masked positions, computed as: Att(XN [mask = 1]) Att(XM [mask = 1]) Att(XN [mask = 1]) (2) Attention Variance Increase: the attention variance increase at non-mask positions, computed as: ar(Att(XM [mask = 0]))V ar(Att(XN [mask = 0])) Expectations. We anticipate that the average attention score at masked positions will undergo significant decline in the MEAP-trained model, indicating that masked tokens receive minimal attention in MEAP. Consequently, this reduction is expected to increase the attention variance at non-masked positions, making the attention distribution in MEAP more distinguishable compared to NTP. Results. Table 8 confirms our expectations. MEAP assigns 53.34% less attention to masked tokens, resulting in 7.80% increase in attention variance. This finding validates that MEAP learns more distinguishable attention scores compared to NTP. 5.2. MEAP Focus More on Task-Relevant Tokens To verify if MEAP learns more effective attention, we measure the average attention scores that the model assigns to different input segments. We structured our input sequences into distinct segments: context-before, answer, context-after, query, and EOS token. The complete input sequence was formed by concatenating these segments sequentially, followed by an EOS token. This structured format enabled precise tracking of attention allocation across different functional components. Expectation. Our expectation is that MEAP tends to amplify attention to answer spans and meanwhile reduce the attention to less relevant tokens. Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More (a) NTP (b) MEAP Figure 6. Attention distribution comparison between NTP and MEAP during inference. The input sequence consists of: context-before (In the heart of Paris, the Eiffel Tower stands tall, symbolizing both the city and the entire country.), answer (Designed by Gustave Eiffel), context-after (, it was completed in 1889 for the Worlds Fair. Originally criticized for its unusual design, it has since become one of the most recognizable landmarks in the world. Tourists from all over the globe visit it every year, making it one of the most photographed monuments.), and query (question: Who designed the Eiffel Tower?). MEAP allocates much higher attention score to answer-relevant tokens (0.345) compared to NTP (0.094). Table 7. Performance comparison of different mask ratios in pre-training and fine-tuning. The best results are highlighted in bold. Pre-training Fine-tuning Mask Ratio NTP 0.05 0.10 0.15 0.20 NTP 0. 0.10 0.15 Accuracy 0.52 0.54 0. 0.58 0.56 0.72 0.77 0.81 0. Results. The attention distributions during inference for both models are visualized in Figure 6. Notably, MEAP exhibits substantial improvement in answer-relevant attention (34.5% vs. 9.4%) while reducing the dominance of context-before attention from 73.1% to 49.1%. Both models maintain similar attention levels for peripheral components, including context-after, query sections, and EOS tokens (all approximately 5%6%). These results demonstrate that the MEAP framework enhances attention allocation during inference, prioritizing key information more effectively. Table 8. Attention score comparison between NTP and MEAP. Input Length Mask Ratio Score Decay Var. Increase 1,024 4,096 0.15 0. 34.08% 53.34% 12.66% 7.80% 6. Ablation Study We conduct ablation studies on the mask ratio for both pre-training and fine-tuning settings. Table 7 summarizes the results. For pre-training, we evaluate our pre-trained model in Section 4.1 on the Multi-Document QA task using the nq-open-oracle dataset (Liu et al., 2024b). For fine-tuning, we train MEAP on the Alpaca dataset (Taori et al., 2023b) for 3 epochs with different mask ratios against 8 standard NTP baselines with 6 epochs for fair comparison. The results show that mask ratio of 0.15 achieves the best performance in pre-training, while mask ratio of 0.10 yields the highest accuracy in fine-tuning. MEAP consistently outperforms standard NTP in pre-training and fine-tuning, demonstrating its effectiveness in leveraging masked tokens for improved performance. 7. Conclusion This work addresses challenges in information processing through straightforward approach that masks 10%15% of input while maintaining traditional prediction methods. Our results show significant improvements in comprehension across longer contexts, achieved without additional computational costs. This approach demonstrates remarkable efficiency, matching performance metrics with just 60B training examples that typically require 200B examples with conventional methods. The results indicate that this strategy leads to more effective processing of key information through improved focus on relevant content. Since it requires no structural changes, this method can be readily integrated into existing systems without disrupting workflows. Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More"
        },
        {
            "title": "Impact Statement",
            "content": "This work proposes modified pre-training paradigm that may influence how both industry and academia approach language model training. MEAP integrates seamlessly with existing LLM frameworks without requiring additional engineering effort or computational resources. While the improvement in information retrieval and reasoning capabilities could have broad implications for downstream applications, the methods computational efficiency and architectural compatibility mean it can be readily adopted within current training infrastructures. We anticipate this work will contribute to more efficient model development while maintaining established training pipelines and computational requirements."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fabbri, A. R., Li, I., She, T., Li, S., and Radev, D. R. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749, 2019. Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y., and Peng, H. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Kamradt, G. Needle in haystack-pressure testing llms. Ba, J. L. Layer normalization. arXiv preprint Github Repository, pp. 28, 2023. arXiv:1607.06450, 2016. Brown, T. B. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Cohen, N., Kalinsky, O., Ziser, Y., and Moschitti, A. Wikisum: Coherent summarization dataset for efficient human-evaluation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 212 219, 2021. Contributors, O. Opencompass: universal evaluation platform for foundation models. https://github. com/open-compass/opencompass, 2023. Devlin, J. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H.-W. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32, 2019. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, Li, M., Zhang, S., Liu, Y., and Chen, K. Needlebench: Can llms do retrieval and reasoning in 1 million context window?, 2024a. URL https://arxiv.org/abs/ 2407.11963. Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Longcontext llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024b. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024b. Liu, Y. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019. Narayan, S., Cohen, S. B., and Lapata, M. Dont give me the details, just the summary. Topic-Aware Convolutional Neural Networks for Extreme Summarization ArXiv, abs, 2018. 9 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Radford, A. Improving language understanding by generaVaswani, A. Attention is all you need. Advances in Neural tive pre-training. 2018. Information Processing Systems, 2017. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Wang, A. and Cho, K. Bert has mouth, and it must speak: Bert as markov random field language model. arXiv preprint arXiv:1902.04094, 2019. Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551564, 2021. Roberts, A., Raffel, C., Lee, K., Matena, M., Shazeer, N., Liu, P. J., Narang, S., Li, W., and Zhou, Y. Exploring the limits of transfer learning with unified text-to-text transformer. Google, Tech. Rep., 2019. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Soboleva, D., Al-Khateeb, F., Myers, R., Steeves, J. R., Hestness, J., and Dey, N. Slimpajama: 627b token cleaned and deduplicated version of redpajama, 2023. URL https://huggingface. co/datasets/cerebras/SlimPajama627B. Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, Z. Xlnet: Generalized autoregressive pretrainarXiv preprint language understanding. ing for arXiv:1906.08237, 2019. Ye, T., Dong, L., Xia, Y., Sun, Y., Zhu, Y., Huang, G., and Wei, F. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Zhang, P., Zeng, G., Wang, T., and Lu, W. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. Taori, R., Gulrajani, strong, paca: model. stanford_alpaca, 2023b. I., Zhang, T., et al. Alreplicable instruction-following https://github.com/tatsu-lab/ Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Shakeri, S., Bahri, D., Schuster, T., et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022. Team, G., Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 10 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More A. Experimental Details of Pre-training A.1. Architecture and Hyperparameters This section outlines the pre-training hyperparameters of the MEAP model, designed to ensure efficient training and optimal performance. The sequence length is fixed at 4096 tokens, enabling the model to handle long-range dependencies while maintaining computational efficiency. The learning rate schedule includes an initial warm-up phase for the first 10% of training steps, followed by cosine decay to 10% of the initial value, allowing gradual and precise parameter adjustments. The AdamW optimizer is used with standard hyperparameters β1 = 0.9 and β2 = 0.95 to stabilize the optimization process. Learning rate bounds are set between 4 104 and 4 105 to ensure effective learning throughout training, while weight decay of 5 102 helps prevent overfitting and promote generalization by penalizing excessively large weights. Complete training hyperparameters are documented in Table 9. The model sizes and corresponding hyperparameters are shown in Table 10. Table 9. Hyperparameters of training"
        },
        {
            "title": "Hyperparameter",
            "content": "optimizer lr schedule clip max learning rate min learning rate weight decay sequence length batch size epoch AdamW cosine 1.0 4 104 4 105 5 102 4096 256 1 Table 10. Hyperparameters of pretrained MEAP models. Data amount are specified in tokens. Params Hidden Intermediate Heads Layers Steps Data amount 100M 500M 1.1 768 1024 2048 2048 4864 5632 12 16 24 12 24 32 2K 10K 190K 2 10 200 A.2. Pre-training Loss of Difference Model Sizes The loss curves of the MEAP model at various sizes, as shown in Figure 7, provide detailed visualization of the models performance across different scales. A.3. Language Modeling Evaluation Of All Size Models for Pre-training As shown in Table 11, we present the evaluation results of models of different scales implemented using our method. To comprehensively assess the language modeling performance of these models, we conducted detailed analysis for each model, with particular focus on their performance at varying scales. A.4. Pretrained Model Evaluation Under Different Masking Rates As shown in Table 12, we present the evaluation results of models implemented with our approach, where different mask rates are applied during training. comprehensive and detailed analysis of the language modeling performance is conducted for each mask rate, with focus on how varying levels of masking influence key performance metrics. This analysis elucidates the effects of mask rates on the models ability to handle diverse linguistic tasks, highlighting any changes in accuracy as the mask rate is adjusted. 11 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Figure 7. Overview of loss for all size pretrained-models Table 11. Results of all size MEAP pretrained models Benchmark 100M 500M 1.1B ARC-Challenge ARC-Easy BoolQ HellaSwag OpenBookQA PIQA WinoGrande Avg 17.32 31.99 45.14 26.82 11.41 58.49 52.09 34.75 18.4 42.0 55.63 30.77 16.40 66.81 49.57 39.94 25.4 56.4 59.5 43.4 22.6 72.3 55.3 47.85 A.5. Details Of Contextual Hallucination Evaluation Here are the prompt for summarization generation, where doc is the original text to be summarized. Summarize the following article: doc We use the following prompts to let the Deepseek v3 model perform binary classification to determine whether there is hallucination in the model output compared to the human summary. The model output is the output of the model, and the predicted label is the manually annotated label. Please compare the model output with the predicted label. By comparing the two, check if the model output is similar. If it is similar, return 1; otherwise, return 0. An explanation of the output is required. Here is the output format provide. Please follow it strictly!! Score: xx 12 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Table 12. Results of the 1.1b MEAP model under different masking rates"
        },
        {
            "title": "ARC Challenge\nARC Easy\nBoolQ\nHellaSwag\nOpenBookQA\nPIQA\nWinogrande\nAvg",
            "content": "Mask Ratio Mask Ratio Mask Ratio 0.05 0.15 0.1 25.4 56.4 59.5 43.4 22.6 72.3 55.3 47.84 26.11 56.1 56.5 43.69 22.0 72.63 56.4 47.63 24.3 54.3 53.4 43.85 21.8 72.91 56.91 46."
        },
        {
            "title": "Area",
            "content": "context before answer context after query eos Table 13. Attention change of example"
        },
        {
            "title": "MEAP attention NTP attention",
            "content": "The Great Wall of China,stretching over 13,000 miles, is one of the most impressive feats of ancient engineering. Built to protect Chinese states from invasions the wall took several dynasties over Its im2,000 years to complete. mense length and historical significance make it popular tourist attraction today. The walls construction involved countless workers, many of whom faced difficult conditions. question:What was the purpose of the Great Wall of China? <s> 0. 0.731 0.329 0.078 0.108 0.80 0. 0.069 0.070 0.071 A.6. Details of Attention Distribution of MEAP and NTP To validate the generality of attention changes, we conducted corresponding tests on additional examples and observed that the attention changes in these examples are consistent with the results presented in the main text. The specific changes are detailed in Table 13, Table 14, and Table 15."
        },
        {
            "title": "Area",
            "content": "context before answer context after query eos Area context before answer context after query eos Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More Table 14. Attention change of example"
        },
        {
            "title": "MEAP attention NTP attention",
            "content": "In the early 20th century, Albert Einstein introduced his theory of relativity, which changed the way we understand space, time, and gravity. His famous equation, E=mc² shows the relationship between energy and mass. Einsteins ideas revolutionized physics, and his work led to the development of technologies like GPS and nuclear energy. Despite facing initial skepticism, his theories were eventually proven through experiments and observations, earning him Nobel Prize in Physics in 1921. question:What famous equation did Albert Einstein create? <s> 0. 0.694 0.386 0.066 0.115 0.074 0. 0.055 0.060 0.057 Table 15. Attention change of example 3 Content MEAP attention NTP attention At the center of Rome, the Colosseum rises as magnificent testament to ancient Roman architecture, symbolizing the grandeur of the Roman Empire. Constructed between 70 and 80 AD under the emperors Vespasian and Titus, it was used for gladiatorial contests and public spectacles. Once symbol of Roman power, the Colosseum has weathered centuries of change but remains one of the most iconic structures in the world. Tourists flock to see it every year, making it one of the most photographed monuments in history. question:Who built the Colosseum? <s> 0.579 0.748 0.219 0.065 0.071 0. 0.063 0.068 0.050 0.069 Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More B. Details of Fine-tuning Experiments B.1. Architecture and Hyperparameters This section details the MEAP fine-tuning hyperparameters for the Llama3 model. The maximum sequence length is 4096 tokens, optimizing long-range dependencies and efficiency. The batch size is 512, and the learning rate schedule includes warm-up for the first 10% of training steps. The AdamW optimizer is used with β1 = 0.9 and β2 = 0.95, and the learning rate is set to 2 105. Table 16. MEAP fine-tuning hyperparameters of Llama3 model Name Hyperparameter optimizer lr schedule clip learning rate weight decay maximum sequence length batch size AdamW cosine 1.0 2 105 5 102 4096"
        }
    ],
    "affiliations": [
        "SCITIX (SGP) TECH PTE. LTD., Singapore",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences, China",
        "South China Normal University, China",
        "Sun YatSen University, China",
        "University of Oxford, UK",
        "University of Texas at Austin, USA"
    ]
}