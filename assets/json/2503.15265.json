{
    "paper_title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning",
    "authors": [
        "Ruowen Zhao",
        "Junliang Ye",
        "Zhengyi Wang",
        "Guangce Liu",
        "Yiwen Chen",
        "Yikai Wang",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Triangle meshes play a crucial role in 3D applications for efficient manipulation and rendering. While auto-regressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, a framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating a novel tokenization algorithm, along with improvements in data curation and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design a scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality. Project page: https://zhaorw02.github.io/DeepMesh/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 5 6 2 5 1 . 3 0 5 2 : r DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning Ruowen Zhao1,3* Junliang Ye1,3* Zhengyi Wang1,3* Guangce Liu3 Yiwen Chen2 Yikai Wang1 Jun Zhu1,3 Tsinghua University1 Nanyang Technological University2 ShengShu 3 https://zhaorw02.github.io/DeepMesh/ Figure 1. Gallery of DeepMeshs generation results. DeepMesh efficiently generates aesthetic, artist-like meshes conditioned on the given point cloud. tion and processing, and (2) the introduction of Reinforcement Learning (RL) into 3D mesh generation to achieve human preference alignment via Direct Preference Optimization (DPO). We design scoring standard that combines human evaluation with 3D metrics to collect preference pairs for DPO, ensuring both visual appeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh generates meshes with intricate details and precise topology, outperforming state-of-the-art methods in both precision and quality."
        },
        {
            "title": "Abstract",
            "content": "Triangle meshes play crucial role in 3D applications for efficient manipulation and rendering. While autoregressive methods generate structured meshes by predicting discrete vertex tokens, they are often constrained by limited face counts and mesh incompleteness. To address these challenges, we propose DeepMesh, framework that optimizes mesh generation through two key innovations: (1) an efficient pre-training strategy incorporating novel tokenization algorithm, along with improvements in data cura- * Equal contribution. Corresponding authors. 1 1. Introduction Triangle meshes are fundamental representation for 3D assets and are widely used across various industries, including virtual reality, gaming, and animation. These meshes can be either manually created by artists or automatically generated by applying Marching Cubes [33] to volumetric fields, such as Neural Radiance Fields (NeRF) [36] or Signed Distance Fields (SDF) [40]. Artist-crafted meshes typically exhibit well-optimized topology, which facilitates editing, deformation, and texture mapping. In contrast, meshes generated by Marching Cubes [33] prioritize geometric accuracy but often lack optimal topology, resulting in overly dense and irregular structures. Recently, several approaches [46, 14, 48, 53, 67] have emerged to generate artist-like topology from given geometry. By taking point clouds extracted from the geometry as input, these methods learn to auto-regressively predict mesh vertices and faces, effectively preserving the structured and artistically optimized topology. Auto-regressive mesh generation methods face two significant challenges: (1) Pre-training involves several difficulties. Tokenizing 3D meshes for transformers often leads to excessively long sequences, which increase computational costs. Moreover, training stability is further compromised by low-quality meshes with poor geometry, resulting in spikes in loss. (2) Existing methods lack mechanisms to align outputs with human preferences, limiting their ability to produce artistically refined meshes. Additionally, generated meshes often exhibit geometric defects, such as holes, missing parts, and redundant structures. In this paper, we aim to propose more refined and effective pre-training framework for auto-regressive mesh generation. To enhance training efficiency, we introduce an improved mesh tokenization algorithm that reduces the sequence length by 72% without losing geometry details, which greatly reduce the training computation cost. In addition, we propose specially designed data packaging strategy that accelerates data loading and ensures better load balancing during training. Additionally, to ensure the quality of training data, we develop data curation strategy that filters out meshes with poor geometry and chaotic structures. This approach effectively mitigates loss spiking and enhances training stability. With these improvements, we successfully pre-train series of large-scale transformer models for topology generation, scaling from 500 million to 1 billion parameters. To further enhance the ability of pre-trained topology generation model, we pioneer to adapt Direct Preference Optimization (DPO) [43] for 3D auto-regressive models, aligning the model outputs with human preference. First, we generate pairwise training data using the pre-trained model and annotate them with human evaluations and 3D geometry metrics. We subsequently employ reinforcement learning (RL) to fine-tune the model with these preferencelabeled samples. These improvements enable our framework to generate diverse, high-quality artist-like meshes with up to 30k faces at quantization resolution of 512. In summary, our contributions are as follows: 1. We propose more refine pre-training framework including an efficient tokenization algorithm for highresolution meshes along with some pre-train strategies for the auto-regressive model to facilitate efficient training. 2. We poineer to adpat DPO to enhance our artist-mesh generative auto-regressive model with human feedback. 2. Related Work 2.1. 3D Mesh Generation Early 3D generation methods utilize SDS-based optimization [2, 8, 23, 25, 34, 41, 44, 50, 51, 56, 60, 62, 76] due to the limited 3D data. To tackle the Janus problem, [42, 47, 57, 75] strengthen the semantics of different views when generating multi-view images. To minimize generation time, some approaches [9, 17, 2729, 32, 46, 55, 65, 68, 73, 83] predict multi-view images and use reconstruction algorithms to produce 3D models. The Large Reconstruction Model (LRM) [15] proposes transformerbased reconstruction model to predict NeRF representation [36] from single image within seconds. Subsequent research [21, 49, 52, 58, 64, 71, 72, 79, 80, 87, 88] further improve LRMs generation quality by incorporating multi-view images or other 3D representations [19]. Additionally, analogous to 2D diffusion models, some early approaches [13, 18, 30, 38] rely on uncompressed 3D representations, such as point clouds, to develop 3D-native diffusion models. However, these methods are often limited by small-scale datasets and struggle with generalization. More recent approaches [7, 16, 24, 59, 69, 70, 74, 78, 81, 84] have focused on adapting latent diffusion models, which train VAE to compress 3D representations. 2.2. Artist-like Mesh Generation However, All of the aforementioned works first generate 3D assets and subsequently convert them into dense meshes through mesh extraction such as Marching Cubes [33]. Consequently, they fail to model the mesh topology, leading to inefficient topology such as poorly structured or tangled wireframe. Recently, approaches using auto-regressive models to generate meshes have gained attention. pioneer work, MeshGPT [48] introduces combination of VQ-VAE [54] and an auto-regressive transformer architecture. Subsequent works [4, 5, 14, 63, 66] explore different model architectures and extend this approach to conditional generation. However, due to the low quality of VQ-VAE, researchers propose to develop mesh quantization methods to serialize meshes. For example, LLaMA2 Figure 2. An overview of our method. DeepMesh is an auto-regressive transformer composed of both self-attention and cross-attention layers. The model is pre-trained on discrete mesh tokens generated by our improved tokenization algorithm. To further enhance the quality of results, we propose scoring standard that combines 3D metrics with human evaluation. With this standard, we annotate 5,000 preference pairs and then post-train the model with DPO to align its outputs with human preferences. Mesh [63] enables LLMs to generate 3D meshes from text prompts. MeshAnythingv2 [6] employs Adjacent Mesh Tokenization, EdgeRunner [53] utilizes an algorithm derived from EdgeBreaker, and BPT [67] introduces its patchified and blocked strategy. Despite these advancements, these tokenization techniques face challenges in balancing compression ratio and vocabulary size, limiting their scalability to generate high-resolution meshes. 2.3. RLHF with Direct Preference Optimization The above methods often typically adopt auto-regressive model architectures from existing large language models. With the rapid advancement of LLMs, aligning policy models with human preferences has become increasingly critical. Reinforcement Learning from Human Feedback (RLHF) is one of the most widely used post-training methods on large language models to better reflect user intentions [77]. RLHF contains reward model, which is trained on win-lose pairs annotated by humans, and aligns the policy model with reinforcement learning algorithms [35, 39]. However, the two-stage training pipeline often suffers from instability and imposes high computational demands. Therefore, Direct Preference Optimization (DPO) [43] has emerged as reward model-free approach that can be easily performed. Despite DPO-based methods being extensively tested on LLMs [31, 45] and VLLMs [22, 85, 86] across text and image modalities, their application to LLMs in the 3D mesh modality remains largely unexplored. 3. Method In this section, we detail our design of DeepMeshs framework. In section 3.1, we explain our improved mesh tokenization algorithm, which efficiently discretizes meshes at high resolution and achieves an approximate 72% com3 pression ratio without losing geometric details. Section 3.2 outlines the details of our pre-training process, including data curation, packaging and truncated training strategy. Furthermore, to enhance generation quality and align outputs with human preferences, we construct dataset of preference pairs and post-train the model with Direct Preference Optimization (DPO) [43], as illustrated in Section 3.3. 3.1. Tokenization Algorithm Analogous to text, meshes must be converted into discrete tokens to be processed by an auto-regressive model. In existing mesh tokenization scheme, continuous vertex coordinates are quantized into bins with spatial resolution of and then classified into categories. After quantization, triangular mesh is then treated as sequence of faces, each with three discretized 3D vertex coordinates. However, this vanilla representation causes each vertex to appear as many times as the number of its connected faces, leading to considerable redundancy. Although prior works [6, 53] have introduced tokenization methods to compress mesh sequences, they still suffer from relatively long token sequences, leading to increased computational costs. Recently, BPT [67] proposed compressive mesh representation with state-of-the-art compression ratio of around 74% at 128 resolution by local-aware face traversal and block-index coordinates encoding. However, BPT only works effectively for low-resolution meshes due to its dramatic vocabulary increase at higher resolution, resulting in training difficulty and costs. To address these limitations, we improve its block-wise indexing to better handle highresolution meshes. Similar to BPT, we first traverse mesh faces by dividing them into local patches according to their connectivity to minimize redundancy in the vanilla representation. This localized traversal ensures each face only relies on short context of previous faces, thereby avoiding long-range dependency between face tokens and mitigating the difficulty of learning. Then we sort and quantize the coordinates of each vertex in faces, and flatten them in XY order to form complete token sequence. To further shorten the sequence length, we partition the whole coordinate system into three hierarchical levels of blocks and index the quantized coordinates as offsets within each block. As the quantized coordinates are sorted, neighbor vertices often share the same offset index. Therefore, we merge the indexes with the identical values to save more length. We provide more details in the supplementary material. With these designs, our enhanced algorithm reaches approximately 72% compression, significantly reducing sequence length and making it easier to train on high-poly datasets. Moreover, we also achieve much smaller vocabulary size for model to learn, which improves training Figure 3. Distribution of face count in training dataset. We present the distribution of face counts in our training dataset. Our dataset size is approximately 500k, with an average face count of 8k. efficiency lot (details are seen in Sec 4.4.1). 3.2. Pre-training of DeepMesh 3.2.1. Data Curation The quality of training data fundamentally governs model performance. However, existing 3D datasets exhibit high variability in quality, with many samples containing irregular topology, excessive fragmentation, or extreme geometric complexity. To mitigate this issue, we propose data curation strategy that filters out poor-quality meshes based on their geometric structure and visual fidelity (more details are in supplementary material). As shown in Figure 3, the face count distribution of our curated dataset highlights high-quality mesh collection. 3.2.2. Truncated Training and Data Packaging As illustrated in Figure 3, high-poly meshes are prevalent in our dataset, resulting in long token sequences that significantly increase computational costs during training. To address this, we adopt truncated training from [14] to enhance efficiency. Specifically, the input token sequence is first partitioned into fixed-size context windows, with padding applied to insufficient-length segment. Then, we utilize sliding window mechanism to shift the window step by step and train each windowed segment accordingly. To reduce unnecessary sliding in the truncated training caused by discrepancies in sequence lengths within each batch, we categorize training meshes based on face count and allocate meshes with similar face counts to each batch on each GPU. This strategy can ensure better load balancing and reduce redundant computation during training. 4 Figure 4. Some examples of the collected preference pairs. We annotate the preferred meshes based on their geometry completeness, surface details and wireframe structure. 3.2.3. Model Architecture The core structure of our DeepMesh is an auto-regressive transformer, where each layer contains cross-attention layer and self-attention layer with feed-forward network. For point cloud-conditioned generation, we employ jointly-trained perceiver encoder based on Michelangelo [84]. Then the conditioned point cloud features are integrated through cross-attention. To accelerate training, we adopt the Hourglass Transformer from [14, 37], which can save 50% memory while maintaining the performance. 3.3. Performance Enhancement by DPO Although our pre-trained model is capable of generating high-quality meshes, it occasionally suffers from inaesthetic appearance and incomplete geometry. To further enhance the results, we employ Direct Preference Optimization (DPO) [43] to align the outputs with human preferences. Moreover, we develop comprehensive annotation pipeline to curate preference dataset, enhancing the overall quality of the results. 3.3.1. Score Standard Mesh quality is primarily influenced by two factors: geometric integrity and visual appeal. Therefore, we propose scoring standard for artist-like mesh generation, which comprehensively accounts for these two aspects. Geometric integrity focuses on the completeness and accuracy of the generated mesh. We employ 3D metrics such as the Chamfer Distance to measure the similarity between the generated mesh and its corresponding ground truth. lower Chamfer Distance indicates higher fidelity and more complete geometric representation. On the other hand, visual appeal evaluates the aesthetic qualities of the mesh, including regular wireframes and surface details. Since there exists no score model to assess the visual quality of meshes, we recruit volunteers to compare different meshes and decide which is more visually attractive based on their subjective preferences. This method of gathering human feedback captures aesthetic judgments that conventional metrics might overlook. 3.3.2. Preference Pair Construction We employ our proposed scoring standard to construct dataset of preference pairs. For each input point cloud, our model generates two distinct meshes and preference pair is selected. Specifically, we first apply the Chamfer Distance metric to assess the geometric completeness of the generated meshes. If both meshes exhibit Chamfer Distance above predefined threshold, they are discarded. In cases where one mesh showcases high geometric fidelity while the other suffers from deficiencies, the superior mesh is designated as the preferred choice. Finally, if both meshes meet the geometric criteria, volunteers are engaged to express their aesthetic preferences. Their judgments help determine the chosen response, ensuring that it aligns with human-like preferences. Figure 4 presents some examples of our collected data pairs, each distinguished by geometry and appearance appeal. We totally collect 5,000 preference pairs to support the post-training of DPO. 3.3.3. Direct Preference Optimization DPO [43] is used to align generative models with human preferences. By training on pairs of generated samples with positive (y+) and negative labels (y), the model learns to generate positive samples with higher probability. The ob5 Figure 5. Qualitative comparison on point cloud conditioned generation between DeepMesh and baselines. DeepMesh outperforms baselines in both generated geometry and preservation of fine-grained details. The meshes generated by ours have much more faces than others. jective function of DPO is formulated as: LDPO (πθ; πref ) = E(c,y+,y)D log σ (cid:34) (cid:16) β log πθ (y+ c) πref (y+ c) β log πθ (y c) πref (y c) (cid:35) (cid:17) . (1) 6 where β is coefficient that balance preferred and dispreferred terms. We post-train our model using the constructed preference pairs dataset with the above loss function to align its outputs with both geometric fidelity and aesthetic appeal. Additionally, to maintain training efficiency, we also adopt the same truncated training strategy used in the pre-training stage to handle long token sequences, which are generated by the high-poly meshes in dataset. preference with DPO. Moreover, while the baselines generate only simple meshes with few faces, our approach is capable of producing high-quality meshes with much more faces, which benefits from our adopted truncated training strategy. 4.2.2. Image Conditioned For image-conditioned generation, we first utilize TRELLIS [70] for image-to-3D generation. Then we sample point clouds from the meshes to conduct point-cloud conditioned generation. Figure 6 demonstrates our high-quality generated outputs. Figure 6. Image-conditioned generation results of our method. Our method can generate high-fidelity meshes aligned with the input images. 4. Experiments 4.1. Implementation Details Our model is trained on the mixture of ShapeNetV2 [1], ABO [10], HSSD [20], Objaverse [12], Objaverse-XL [11] and licensed data. To yield better generalization ability, we randomly rotate the meshes with with degrees from (0, 90, 180, 270). For each mesh, we sample 20k points and randomly select 16, 384 points as the condition. Our model is pre-trained on 128 NVIDIA A800 GPUs for 4 days, with cosine learning rate scheduler from 1e 4 to 1e 5. For the post-training stage with DPO, we fine-tune our model with learning rate of 1e 5 for 10 epoch. The models truncated context length is set to 9k tokens. During mesh generation, we use probabilistic sampling with temperature of 0.5 for stability. More implementation details can be seen in supplementary material. 4.2. Qualitative Results 4.2.1. Point-cloud Conditioned For point cloud conditioned generation, we compare our results with the latest open-source artist-like mesh generation in the point cloud conditioned generation, such as MeshAnythingv2 [6] and BPT [67]. As illustrated in Fig 5, the baselines fails to capture fine topological details, suffering from surface holes and missing components. In contrast, our method generates aesthetically appealing artistlike meshes that preserve the geometric details. This is because we improve the tokenization algorithm for highresolution meshes and further align the model with human Figure 7. Diversity of generations. DeepMesh can generate meshes with diverse appearance given the same point cloud. 4.2.3. Diversity We evaluate the diversity of generated meshes by providing the same point clouds repeatedly and observe the variations in the meshes. Figure 7 shows our model generates variety of distinct meshes that are consistent with the input point cloud, highlighting the ability to produce creative high-fidelity outputs with diverse appearance. This diversity is crucial for applications that require multiple design options and variations. 4.3. Quantitative Results We compared our point cloud-conditioned results with MeshAnythingv2 and BPT on test dataset of 100 meshes generated from [70]. Similar to [61, 67], we uniformly sample 1,024 point clouds from the surfaces of ground truth and generated mesh and compute the Chamfer Distance (C.Dist.) and Hausdorff Distance (H.Dist.) between them. As shown in Table 1, our method outperforms all of the baselines in geometry similarity. In addition, we also conduct user study to assess the subjective visual appeal of the generated meshes. Volunteers are asked to compare our results with those produced by baselines. It is also can be 7 Figure 8. Ablation study on the effectiveness of DPO. We can observe that while both approaches yield excellent geometry, the results generated using DPO are more visually appealing. Metrics C.Dist. H.Dist. User Study MeshAnythingv2 [6] BPT [67] Ours w/o DPO Ours DPO 0.1249 0.1425 0.1001 0. 0.2991 0.2796 0.1861 0.1708 10% 19% 34% 37% Table 1. Quantitative comparison with other baselines. Our method outperforms other baselines in generated geometry and visual quality. Metrics AMT EdgeRunner BPT Comp Ratio Vocal Size Time (s) 0.46 512 816 0.47 512 - 0.26 40960 540 Ours 0.28 4736 Table 2. Quantitative comparison with other tokenization algorithms. Our improved tokenization algorithm achieves low compression ratio, small vocabulary size, and the highest computational efficiency, making it both compact and highly efficient for mesh processing. training. Larger vocabularies indicate greater memory storage and training difficulty. Moreover, we evaluate the training time of different methods on test dataset comprising 80 meshes, each with face count of 20k. As shown in Table 2, our tokenization method achieves balanced trade-off between compression ratio and vocabulary size while outperforming all baselines in computational efficiency. 4.4.2. DPO Post-training We collected human preference pairs and fine-tuned our pre-trained model using DPO to enhance its capability to generate meshes with superior geometry and aesthetics. To validate the effect of DPO, we compared the outputs of the post-trained model with those of the pre-trained model. Figure 8 indicates that the post-trained model exhibits clear advantage over the pre-trained one. This suggests the importance of learning from preference pairs, which reduces the likelihood of generating suboptimal outputs. Additionally, quantitative evaluations presented in Table 1 demonstrate that DPO-enhanced results have the most similarity with the ground truth and are most preferred by users. found that the generation results of ours are most preferred. 5. Conclusion 4.4. Ablation Study 4.4.1. Tokenization Algorithm We compare our tokenization algorithm for 512-resolution meshes with Adjacent Mesh Tokenization (AMT) [6], EdgeRunner [53], and BPT [67]. First, we assess the compression ratio, defined as the reduction in sequence length relative to the vanilla representation (which corresponds to nine times the number of faces). lower compression ratio indicates more compact representation, which improves storage and computational efficiency. Second, we calculate the vocabulary size, which impacts the complexity of model We introduce DeepMesh, novel approach that generates artist-like meshes with reinforcement learning. By improving the tokenization algorithm for high-resolution meshes, we preserve intricate details of high-poly meshes while achieving significant sequence compression. We also introduce several pre-training strategies including data curaIn tion and data packaging to boost training efficiency. addtion, by aligning results with human preferences using DPO [43], we refine both topology and visual quality of the generated meshes. The extensive experiments demonstrate that DeepMesh outperforms existing methods across various metrics, enabling the creation of meshes with geometric complexity and details."
        },
        {
            "title": "References",
            "content": "[1] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 7 [2] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for arXiv preprint high-quality text-to-3d content creation. arXiv:2303.13873, 2023. 2 [3] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. arXiv preprint arXiv:2412.17808, 2024. 15 [4] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. Meshxl: Neural coordinate field for generative 3d foundation models. Advances in Neural Information Processing Systems, 37:9714197166, 2025. 2 [5] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. arXiv preprint arXiv:2406.10163, 2024. 2 [6] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. arXiv preprint arXiv:2408.02555, 2024. 2, 3, 4, 7, 8, [7] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. arXiv preprint arXiv:2409.12957, 2024. 2 [8] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. In Proceedings of Text-to-3d using gaussian splatting. the IEEE/CVF conference on computer vision and pattern recognition, pages 2140121412, 2024. 2 [9] Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion models are effective 3d generators. arXiv preprint arXiv:2403.06738, 2024. 2 [10] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object unIn Proceedings of the IEEE/CVF conference derstanding. on computer vision and pattern recognition, pages 21126 21136, 2022. 7 [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 7 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. [13] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. Advances In Neural Information Processing Systems, 35:3184131854, 2022. 2 [14] Zekun Hao, David Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale. arXiv preprint arXiv:2412.09548, 2024. 2, 4, 5, 14, 15 [15] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2 [16] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. 2 [17] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. Advances in Neural Information Processing Systems, 37:125879125906, 2025. 2 [18] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. Shap-e: GeneratarXiv preprint [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [20] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1638416393, 2024. 7 [21] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 2 [22] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 3 [23] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arxiv:2310.02596, 2023. [24] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. 2 [25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, 9 Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution In IEEE Conference on Comtext-to-3d content creation. puter Vision and Pattern Recognition (CVPR), 2023. 2 [26] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024. [27] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36:2222622246, 2023. 2 [28] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [29] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [30] Zhen Liu, Yao Feng, Michael Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133, 2023. 2 [31] Zixuan Liu, Xiaolin Sun, and Zizhan Zheng. Enhancing llm safety via constrained direct preference optimization. arXiv preprint arXiv:2403.02475, 2024. 3 [32] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. 2 [33] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 2 [34] Zhiyuan Ma, Yuxiang Wei, Yabin Zhang, Xiangyu Zhu, Zhen Lei, and Lei Zhang. Scaledreamer: Scalable text-to3d synthesis with asynchronous score distillation. In European Conference on Computer Vision, pages 119. Springer, 2024. [35] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022. 3 [36] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [37] Piotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Łukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. arXiv preprint arXiv:2110.13711, 2021. 5, 14 [38] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2 [39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. 3 [40] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. 2 [41] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. [42] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to3d. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99149925, 2024. 2 [43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 2, 3, 4, 5, 8 [44] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23492359, 2023. 2 [45] Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo: Advancing multilingual reasoning through multilingual alignment-aspreference optimization. arXiv preprint arXiv:2401.06838, 2024. 3 [46] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 2 [47] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [48] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1961519625, 2024. 2 [49] Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and 10 David Novotny. Meta 3d assetgen: Text-to-mesh generation with high-quality geometry, texture, and pbr materials. In Advances in Neural Information Processing Systems, pages 95329564. Curran Associates, Inc., 2024. 2 [50] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior. arXiv preprint arXiv:2310.16818, 2023. 2 [51] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [52] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. 2 [53] Jiaxiang Tang, Zhaoshuo Li, Zekun Hao, Xian Liu, Gang Zeng, Ming-Yu Liu, and Qinsheng Zhang. Edgerunner: Auto-regressive auto-encoder for artistic mesh generation. arXiv preprint arXiv:2409.18114, 2024. 2, 3, 4, 8 [54] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [55] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. 2 [56] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. arXiv preprint arXiv:2212.00774, 2022. 2 [57] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 2 Imagedream: Image-prompt arXiv preprint [58] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Pf-lrm: Pose-free large reconstruction model Zhang. arXiv preprint for arXiv:2311.12024, 2023. 2 joint pose and shape prediction. [59] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45634573, 2023. 2 [60] Xinzhou Wang, Yikai Wang, Junliang Ye, Fuchun Sun, Zhengyi Wang, Ling Wang, Pengkun Liu, Kai Sun, Xintong Wang, Wende Xie, Fangfu Liu, and Bin He. Animatabledreamer: Text-guided non-rigid 3d model generation and reconstruction with canonical score distillation. In Computer Vision ECCV 2024, pages 321339, Cham, 2025. Springer Nature Switzerland. [61] Yuxuan Wang, Xuanyu Yi, Haohan Weng, Qingshan Xu, Xiaokang Wei, Xianghui Yang, Chunchao Guo, Long Chen, and Hanwang Zhang. Nautilus: Locality-aware au11 toencoder for scalable mesh generation. arXiv:2501.14317, 2025. 7 arXiv preprint [62] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2 [63] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. 2, [64] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh arXiv preprint with convolutional reconstruction model. arXiv:2403.05034, 2024. 2 [65] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023. 2 [66] Haohan Weng, Yikai Wang, Tong Zhang, CL Chen, and Jun Zhu. Pivotmesh: Generic 3d mesh generation via pivot vertices guidance. arXiv preprint arXiv:2405.16890, 2024. 2 [67] Haohan Weng, Zibo Zhao, Biwen Lei, Xianghui Yang, Jian Liu, Zeqiang Lai, Zhuo Chen, Yuhong Liu, Jie Jiang, Chunchao Guo, et al. Scaling mesh generation via compressive tokenization. arXiv preprint arXiv:2411.07025, 2024. 2, 3, 4, 7, 8, 13, 15 [68] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [69] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. [70] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. 2, 7, 14 [71] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217, 2023. 2 [72] Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein. Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation. In European Conference on Computer Vision, pages 120. Springer, 2024. 2 [73] Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, and Tao Mei. Hi3d: Pursuing highresolution image-to-3d generation with video diffusion models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 68706879, 2024. 2 [74] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, [87] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Longsequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. 2 [88] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and Song-Hai Zhang. Triplane meets gaussian splatting: Fast and generalizable single-view In Proceedings of 3d reconstruction with transformers. the IEEE/CVF conference on computer vision and pattern recognition, pages 1032410335, 2024. 2 Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. [75] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. DreamIn reward: Text-to-3d generation with human preference. European Conference on Computer Vision, pages 259276. Springer, 2024. 2 [76] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67966807, 2024. 2 [77] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. Advances in Neural Information Processing Systems, 36:1093510950, 2023. 3 [78] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 2 [79] Chubin Zhang, Hongliang Song, Yi Wei, Yu Chen, Jiwen Lu, and Yansong Tang. Geolrm: Geometry-aware large reconstruction model for high-quality 3d gaussian generation. arXiv preprint arXiv:2406.15333, 2024. 2 [80] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. In European Conference on Computer Vision, pages 119. Springer, 2024. 2 [81] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2 [82] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. [83] Ruowen Zhao, Zhengyi Wang, Yikai Wang, Zihan Zhou, and Jun Zhu. Flexidreamer: single image-to-3d generation with flexicubes. arXiv preprint arXiv:2404.00987, 2024. 2 [84] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in neural information processing systems, 36:7396973982, 2023. 2, 5 [85] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 3 [86] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. 3 12 Figure 9. Details of our tokenization algorithm. We first traverse mesh faces by dividing them into patches according to their connectivity and quantize each vertex of faces into bins (in our setting = 512).Then we partition the whole coordinate system into three hierarchical levels of blocks and index the quantized coordinates as offsets within each block. We merge the index of neighbor vertices if they have the identical values. A. Details of Tokenization Algorithm In this section, we detail our improved tokenization algorithm. As illustrated in Figure 9, we first traverse mesh faces to reduce redundancy in the vanilla mesh representation. Specifically, we divide mesh faces into multiple local patches according to their connectivity similar to [67]. Each local patch is formed by grouping central vertex with its adjacent vertices P1:n, which are organized based on their connectivity order: length, we partition the whole coordinate system into three hierarchical levels of blocks and index the quantized coordinates as offsets within each block, as shown in Figure 9. The volume of each block is A, and respectively. In our setting, = 4, = 8 and = 16. We scale quantized Cartesian coordinate (x, y, x) of each vertex into (i, j, k) by: = (x C) A2 + (y C) + (z C) =(x%B C) B2 + (y%B C) (3) LO = (O, P1, P2, , Pn) (2) + (z%B C) This organization helps maintain local mesh connectivity by explicitly encoding edge-sharing relationships between adjacent faces. To find each center vertex, we begin by sorting all the unvisited faces. Next, we select the first unvisited face and choose the vertex connected to the most unvisited faces as the center. Then, we iteratively traverse the neighboring vertices within the centers unvisited faces, expanding the local patch by adding adjacent vertices that connect to the current patch. Once the patch is complete, we mark all its faces as visited. We repeat the process above until every face is visited. After the local-wise face traversal, we normalize and quantize each vertex in faces and flatten them in XY order. With resolution of r, coordinates of each vertex are quantized into [0, 1] (in our setting, = 512). The coordinates of all vertices are then concatenated to form complete sequence of tokens. To further reduce the sequence = (x%C) 2 + (y%C) + z%C As the coordinates are sorted, it is common for neighbor vertices to share the same offset in block. Therefore, we merge the adjacent (i, j, k) if they have the identical values to save more length. Specifically, for vertices vi, v2, , vn, the sequence of their coordinate representation can be simplified as follows: (v1, v2, , vn) = (i1, j1, k1, i1, j1, k2, , i1, j2, ks+1) = (i1, j1, k1, k2, , ks, j2, ks+1, , kn) (4) To distinguish different patches, we extend the vocabulary size of and for each center vertex in patches. This design eliminates the need for special tokens to separate adjacent local patches, thereby avoiding unnecessary increases in mesh sequence length."
        },
        {
            "title": "500 M\n9\n21\n10\n1280\n5120\n1e − 4\nCosine\n0.1\n1.0",
            "content": "1.1B 5 20 14 1792 7168 1e 4 Cosine 0.1 1.0 Table 3. Deepmeshs architectural and training details. age face count of 8k. B.2. Preference Pair Constructed Pipeline The point clouds in our preference pair dataset come from the training dataset and manually selected high-quality test dataset. This ensures diverse dataset for learning human preference. Since high-poly mesh generation is extremely time-consumingfor example, our full-scale model requires at least 10 minutes to generate single mesh with over 30K facesit is crucial to pre-select candidate list when constructing the DPO dataset to improve efficiency and feasibility. To ensure that the preference data is representative, we filter out overly simple and complex meshes. Specifically, we follow similar data-filtering approach in Section B.1, removing samples with fewer than 5,000 faces as well as those with extremely high or low test loss. Using the remaining curated data, we then construct our preference pair dataset for post-training. For mesh generation, we use temperature of 0.5 and generate 2 meshes for each point cloud. B.3. More Training Details We respectively train small-scale model and large-scale model for DeepMesh, with architecture details provided in the Table 3. We train both of the models for 100k iteration steps to ensure convergence. Moreover, we employ FlashAttention and Zero2 to reduce GPU memory usage. B.4. Hourglass Transformers Inspired by [14, 37], we adopt the Hourglass Transformers architecture for efficient training. For hyperparameters, we maintain the settings from [14]. Specifically, the shortening factor is set to 3, while both the downsampling and upsampling layers are used with the Linear layers. Figure 10. Comparison with other tokenization algorithms in training effciency. We integrate all tokenization algorithms into our model architecture and train them on dataset of 80 meshes for each face count category (10K, 20K, 30K, 40K). Our method achieves the fastest training time across all face count categories, demonstrating superior training efficiency. B. More Implementation Details B.1. Training Data Filtering Pipeline The data in training dataset varies significantly in quality, which may lead to three primary challenges: 1. Unstructured topology that fails to meet the artist-mesh standard. 2. Fragmented data that cannot assemble into complete surfaces. 3. Overly complex structures, such as characters with tangled or messy hair geometry. To efficiently filter out low-quality data, we propose an data-filtering pipeline comprising the following four stages: (1) First of all, We remove meshes with mesh.area metric below 1 to filter out the fragmented data. (2) Then, We construct high-quality subset of the training data and perform low-cost pretraining on it to build baseline model. (3) Subsequently, the pretrained model is then used to evaluate the remaining data, recording their test losses. Meshes exceeding predefined loss threshold are flagged and placed on candidate deletion list. (4) Finally, The candidate meshes are rendered into images and scored using pretrained aesthetic assessment model [70]. We retain the top 20% of the highest-scoring meshes to ensure that high-quality but complex meshes are not mistakenly removed. After filtering out the poor-quality data, our dataset size decreases from 800k to approximately 500k, with an aver14 C. More Ablation Study C.1. Efficiency of Tokenization We evaluate the computational efficiency of our mesh tokenization algorithm compared to other baselines [6, 14, 67]. To ensure fair comparison, we integrate each methods compressed mesh representation into our model while keeping all other parameters unchanged, as detailed in Table 3. For training, we use single GPU and dynamically adjust the batch size to fully utilize available memory. We test on dataset of 80 meshes for each face count category: 10K, 20K, 30K, and 40K faces. As shown in Figure 10, our method consistently exhibits lowest training time across all face count categories, and achieves the best training efficiency. C.2. Data Curation During the initial stages of training, we observe frequent spikes in the loss curve, as illustrated in Figure 11. This suggests that certain training samples lead to irregular loss values, potentially disrupting the learning process. To address this issue, we apply the data filtering strategy outlined in Section B.1, removing low-quality samples to ensure stable training. This filtering process can mitigate the inconsistencies caused by poor mesh structures. The impact of this curation is reflected in the improved training loss curve, also shown in Figure 11. D. Limitations and Future Work Although DeepMesh demonstrates impressive mesh generation capabilities, there are several limitations to address in future work. First, Tthe generation quality of DeepMesh is constrained by the low-level features of point cloud conditioning. As result, it struggles to recover fine-grained details present in the original meshes. To address this, future improvements could focus on enhancing the point cloud encoder or integrating salient point sampling techniques, such as those proposed in [3]. Also, DeepMesh is trained on limited number of 3D data. We believe incorporating more datasets could further enrich the generated results. Additionally, we use only 1B model due to limited computational resources. We believe that using larger scale model would further improve the generation quality. E. More Results We provide more visualization results respectively in Figure 12 and Figure 13. Additionally, we select specific cases and present their high-resolution renderings in Figure 14,15 and 16 to see their finer details. (a) Before data curation (b) After data curation Figure 11. Training loss before and after data curation. Before data curation, we observe frequent loss spikes. After data curation, pre-training becomes significantly more stable. Figure 12. More results of DeepMesh. We present more high-fidelity results generated by our method. 16 Figure 13. More results of DeepMesh. We present more high-fidelity results generated by our method. 17 Figure 14. High resolution results of our generated meshes. 18 Figure 15. High resolution results of our generated meshes. 19 Figure 16. High resolution results of our generated meshes."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "ShengShu",
        "Tsinghua University"
    ]
}