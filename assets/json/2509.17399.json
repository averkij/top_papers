{
    "paper_title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context",
    "authors": [
        "Pramit Sahoo",
        "Maharaj Brahma",
        "Maunendra Sankar Desarkar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are widely used in various tasks and applications. However, despite their wide capabilities, they are shown to lack cultural alignment \\citep{ryan-etal-2024-unintended, alkhamissi-etal-2024-investigating} and produce biased generations \\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence. Evaluation of LLMs for cultural awareness and alignment is particularly challenging due to the lack of proper evaluation metrics and unavailability of culturally grounded datasets representing the vast complexity of cultures at the regional and sub-regional levels. Existing datasets for culture specific items (CSIs) focus primarily on concepts at the regional level and may contain false positives. To address this issue, we introduce a novel CSI dataset for Indian culture, belonging to 17 cultural facets. The dataset comprises $\\sim$8k cultural concepts from 36 sub-regions. To measure the cultural competence of LLMs on a cultural text adaptation task, we evaluate the adaptations using the CSIs created, LLM as Judge, and human evaluations from diverse socio-demographic region. Furthermore, we perform quantitative analysis demonstrating selective sub-regional coverage and surface-level adaptations across all considered LLMs. Our dataset is available here: \\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI}, project webpage\\footnote{\\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}}, and our codebase with model outputs can be found here: \\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}."
        },
        {
            "title": "Start",
            "content": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for"
        },
        {
            "title": "India\nDataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context",
            "content": "Pramit Sahoo* Maharaj Brahma* Maunendra Sankar Desarkar Natural Language and Information Processing Lab (NLIP) Indian Institute of Technology Hyderabad Hyderabad, India pramitsahoo.gnipst@gmail.com, cs23resch01004@iith.ac.in, maunendra@cse.iith.ac.in"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 2 2 ] . [ 1 9 9 3 7 1 . 9 0 5 2 : r Large language models (LLMs) are widely used in various tasks and applications. However, despite their wide capabilities, they are shown to lack cultural alignment (Ryan et al., 2024; AlKhamissi et al., 2024) and produce biased generations (Naous et al., 2024) due to lack of cultural knowledge and competence. Evaluation of LLMs for cultural awareness and alignment is particularly challenging due to the lack of proper evaluation metrics and unavailability of culturally grounded datasets representing the vast complexity of cultures at the regional and sub-regional levels. Existing datasets for culture specific items (CSIs) focus primarily on concepts at the regional level and may contain false positives. To address this issue, we introduce novel CSI dataset for Indian culture, belonging to 17 cultural facets. The dataset comprises 8k cultural concepts from 36 sub-regions. To measure the cultural competence of LLMs on cultural text adaptation task, we evaluate the adaptations using the CSIs created, LLM as Judge, and human evaluations from diverse socio-demographic region. Furthermore, we perform quantitative analysis demonstrating selective sub-regional coverage and surface-level adaptations across all considered LLMs. Our dataset is available here: https://huggingface.co/datasets/nlip/DIWALI, project webpage1, codebase with model outputs can be found here: https://github.com/pramitsahoo/cultureevaluation. and our"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs), despite having vast knowledge and being trained on extensive large data, are shown to lack cultural knowledge and competence (Naous et al., 2024; Wang et al., *Equal contributions 1https://nlip-lab.github.io/nlip/publications/diwali/ Figure 1: Comparison of the number of cultural concepts between CANDLE (Nguyen et al., 2023) and Ours (DIWALI), highlighting differences in coverage of cultural concepts for five facets (clothing, drink, cuisine, rituals, and traditions) available in CANDLE (Domain = Country, Subject = India). 2024; Masoud et al., 2025). This is primarily attributed to highly biased pre-training data towards Western culture (Li et al., 2024; Naous and Xu, 2025; Li et al., 2025). Cultural text adaptation refers to modifying text from general or specific culture to align with particular target cultural groups linguistic, social, and conceptual norms. Cultural adaptation has wide range of applications, ranging from education (Burstein et al., 2007), dialogue (Singh et al., 2024), movie subtitling (Ferreira, 2024; El-Farahaty and Alwazna, 2024), hospitality (Radojevic et al., 2024), advertisement (Barthwal and Gupta, 2011; Li, 2024), cross-cultural communication (Wilczewski et al., 2019), and storytelling (Bhatt and Diaz, 2024; Hamna et al., 2025). However, ensuring effective and emotionally resonating cultural adaptation is challenging, as it requires deep understanding and knowledge of social norms, subjective historical references, resonating events, and cultural nuances. With its vast linguistic and cultural diversity, India presents unique challenge for culture-driven NLP systems. In particular, India houses 28 states and eight union territories2, each with multiple languages, dialects, traditions, and socio-cultural norms. Furthermore, cultural diversity exists at the state level and within sub-regions. For example, the state of Assam has communities like Bodo, Rabha, Karbi, and so on each with distinct cultural norms, artifacts, and language3. Despite recent advances, existing LLMs struggle with cultural adaptation. Studies (Naous et al., 2024) have shown that LLMs trained on English-centric corpora often fail to generate culturally appropriate content, especially for cultures from different parts of the world. In this work, we systematically evaluate multiple open-weight LLMs for cultural text adaptation on closed-text generation. In particular, we utilize existing text corpora from the education domain belonging to American culture. We rely on cultural concepts to assess the cultural competence of various LLMs for cultural text adaptation tasks. Culture Specific Items (CSIs) represent particular cultures concepts, objects, items, and customs. For example, Mekhela Chador (clothing), Pongal (food), and Darjeeling Tea (drink) are cultural concepts from India. Prior work, CANDLE (Nguyen et al., 2023), has made foundational efforts to automatically extract culturally specific concepts from large web text for various cultures. In another work, DOSA (Seth et al., 2024) builds community-driven cultural artifacts for sub-regional culture belonging to India. However, these datasets have limited representation of cultural concepts from India. Figure 1 shows the comparison of the created dataset with respect to existing concepts from the CANDLE4 framework. To address this gap, we built novel, large-scale dataset of CSIs for the Indian subregion. In particular, our dataset contains 8k cultural concepts from 17 facets. We evaluate using automatic CSI-based and human evaluation metrics to systematically assess the adaptation quality of various LLMs. Our dataset provides valuable resource to the community for further research in the comprehensive and standardized assessment of cultural text adaptation. We define culture at the regional or country level, focusing on India. In summary, our contributions are as follows: We present DIWALI, high-quality Culture Specific Items (CSIs) dataset of India, cov2https://knowindia.india.gov.in/states-uts/ 3https://assam.gov.in/about-us/391 4https://candle.mpi-inf.mpg.de/ ering total of 36 subregions5. Our dataset contains total of 8,817 concepts from 17 cultural facets. We assess 7 open-weight models across 3 LLM families using both automatic and human evaluation metrics for cultural text adaptation. We conduct cultural sub-regional evaluation and analysis of LLMs in the context of cultural text adaptation."
        },
        {
            "title": "2 Culture & Culture Specific Items",
            "content": "Culture is multifaceted and complex construct. It means different things to different groups of people. It encompasses knowledge, beliefs, morals, values, and customs. Broadly, we can divide culture into material and non-material components. Material components include tangible elements such as food, dress, houses, and ornaments. In contrast, non-material components refer to symbols, ideas, beliefs, norms, values, morality, and attitudes that guide social behavior. For example, the practice of fasting during Navratris6. Culture is complex concept and hard to define. As result, the community has turned to using demographic and semantic proxies to approximately define culture (Adilazuarda et al., 2024). Demographic proxies typically include geographical region, language, gender, race, religion, education, and ethnicity. In contrast, people associate semantic proxies with emotional expressions, food habits, social and political relations, actions, and naming conventions. In this work, we consider culture as regional demographic proxy with the geographical country India as region and its states and union territories as sub-regions. Newmark (2003) defined culture as ... the way of life and its manifestations that are peculiar to community that uses particular languages as means of expression. This definition views culture as collection of objects, customs, and traditions that hold meaning within specific community. Newmark (2003) categorizes cultural manifestations into five collections or facets: (a) ecology, (b) material culture, (c) social culture, (d) organizations, customs, activities, procedures, concepts, and (e) gestures and habits. Culture Specific Items represent concepts, objects, items, and customs that hold significant cultural importance and relevance 5Represents states and union territories of India 6a religious observance common in Northern India. to particular culture. It may vary across different subregions. Existing CSI datasets, such as CANDLE and DOSA, are limited in scope, covering only subset of cultural items. CANDLE groups cultural common sense knowledge as facet and concept. Each facet contains multiple concepts that are culturally relevant to domain and subject. DOSA (Seth et al., 2024) builds an India-specific cultural dataset by gamifying the process of data collection from 19 socio-diverse geographical locations, with total of 615 social artifacts. Despite these foundational efforts, through an initial manual inspection, we observe that cultural concepts extracted by CANDLE are incorrectly mapped to facet, for example: Light, Africa, Nepal under Rituals facet (Details in Appendix A)."
        },
        {
            "title": "3 DIWALI Dataset",
            "content": "DIWALI comprises 17 systematically curated cultural facets that capture the rich diversity of India across 36 sub-regions. The sub-regional cultural items are defined at the level of states and union territories of the country. Sub-sub-regional distinctions are grouped under their broader sub-region to maintain consistency. For example, cultural items from communities such as the Bodo and Rabha of Assam are grouped in the Assam sub-region without further subgrouping. In total, the dataset contains 8,817 culture specific items, with the highest concepts from the food facet (1,419), followed by dance forms (1,105). Figure 2 shows the distribution of concepts across the 17 facets, which collectively cover significant aspects of Indian culture."
        },
        {
            "title": "3.1 Choice of facets",
            "content": "CANDLE considers only limited set of facets that may not fully represent Indian culture, we extend this scope by curating 17 well-known cultural facets of India. To define these facets, we (authors) initially discussed various cultural concepts that are available across the sub-regions through brainstorming session and grouped them into common cultural constructs: material, social practices, geographical locations & languages, and names."
        },
        {
            "title": "3.1.1 Material Culture\nUnder material culture, we consider facets that have\ntangible cultural items: Clothing, Textiles, Jew-\nellery, Food, and Drinks. It covers clothing choices,\nregional weaving traditions, traditional ornaments,\nfood, and traditional drinks.",
            "content": "Figure 2: Distribution of concepts across 17 cultural facets present in DIWALI."
        },
        {
            "title": "3.2 Data Construction",
            "content": "We initially start by prompting GPT-4o using the simple prompt (Presented in the Appendix section J), which produced concepts sourced from Wikipedia. Wikipedia provides limited coverage of Indias cultural diversity, resulting in incomplete and potentially biased datasets. Moreover, the prompt-based method for curating datasets is limited by the scope of the knowledge learned by the LLM. To address these limitations, we extend our approach by combining GPT-4o-plus with web searches. To enhance coverage and authenticity, we broaden our data collection scope by searching for official cultural and tourism websites for each sub-region7. These sources provide more authentic and verified documentation, ensuring comprehensive and representative dataset. The detailed prompt formulated for Dance facet is provided in the Appendix section under Curating Indiaspecific CSIs."
        },
        {
            "title": "3.3 Quality Check",
            "content": "To ensure the reliability and cultural validity of the DIWALI dataset, we conduct two-stage verification process. The dataset comprises 8,817 culture specific items, each of which is manually inspected by one of the authors."
        },
        {
            "title": "4.1 Problem Definition",
            "content": "We define cultural text adaptation as the task of transforming text x, originally grounded in source cultural context Cs, into an adapted text that aligns with the target cultural context Ct. 7Here we consider sub-region as the States and the Union territories of India. 8https://asi.nic.in/ The objective is to preserve the original semantic intent of while ensuring is culturally relevant and emotionally resonant for an audience with Ct. In this work, we focus on American (source) to Indian (target) cultural contexts and evaluate LLMs ability for culturally relevant adaptations."
        },
        {
            "title": "4.2 Evaluation Dataset",
            "content": "We use GSM8k (Cobbe et al., 2021) and its multilingual variant MGSM (Shi et al., 2022) as our evaluation datasets. These benchmarks consist of grade school-level math word problems to evaluate the reasoning capabilities of LLMs. We select these datasets for two primary reasons. First, the educational domain is cognitively engaging and often reflects socio-cultural values, making it wellsuited for evaluating cultural adaptation. Second, the problems usually contain culturally grounded entities such as names, food items, locations, and units, which can vary in relevance across cultures. For our experiments, we use the GSM8k (1,319 samples) and MGSM (250 samples) test sets, both originally situated in an American cultural context. To assess robustness beyond math word problems, we additionally evaluate on two distinct domains: dialogue and story. Specifically, we sample 100 instances from DailyDialog (Li et al., 2017) and 100 instances from ROCStories (Mostafazadeh et al., 2016)."
        },
        {
            "title": "4.3 Models",
            "content": "We evaluate models from three families of large language models, with parameter sizes ranging from 1B to 9B: (a) Llama 2 7B Chat (Touvron et al., 2023), (b) Llama 3.1 8B Instruct (Grattafiori et al., 2024), (c) Llama 3.2 1B Instruct (Grattafiori et al., 2024), (d) Llama 3.2 3B Instruct (Grattafiori et al., 2024), (e) Mistral 7B Instruct (Jiang et al., 2023), (f) Gemma 2 2B Instruct (Team et al., 2024), and (g) Gemma 2 9B Instruct (Team et al., 2024). To evaluate cultural text adaptation, we design an assistant-style instruction prompt in both English and Bengali. The prompt explicitly instructs the model to adapt given text with respect to (i) cultural relevance, (ii) tone and intent, and (iii) cultural sensitivity. Given the sensitivity of LLMs to system instructions and prompts, we provide clear, structured guidelines to ensure consistent behavior. For easy processing, we ensure that the adapted text, along with replaced concepts, is in structured format. Full prompts are provided in the Appendix (Prompt title: Cultural Text Adaptation), and details on decoding strategies and inference setup are given in Appendix C."
        },
        {
            "title": "4.4.1 CSI Adaptation Score",
            "content": "To quantify the extent of cultural adaptation in generated text, we introduce an Adaptation Score, which measures the correctness of cultural concept replacements. Given an input text with set of replaced cultural concepts: = {w1, w2, . . . , wN }, the score is defined as the proportion of replacements that match pre-specified set of target cultural concepts. Formally, for each replaced concept R, we define an indicator function I(w) as: I(w) = if is successfully matched, (cid:40) 1, 0, otherwise, We follow two matching strategies: 1. Exact Match: We first normalize the concept (For example, by lowercasing and removing punctuation) and check for an exact match in the target cultural concept set C. That is, if normalize(w) C, then I(w) = 1. 2. Fuzzy Match: If an exact match is not found, we perform fuzzy matching using tokenbased similarity measure9. Let be the best fuzzy match from for such that similarity(normalize(w), c) τ. Here τ is predetermined threshold (For example, τ = 80 in our experiments). If this condition is met, we set I(w) = 1. Based on the above matching strategy, the Adaptation Score for adapted text is given by: ADAPTATION SCORE = 1 wR I(w) (cid:80) For example: Suppose the LLM adapts three non-Indian concepts: {Muffins, Christmas, Beer} into { Paratha , Diwali , Ginseng }. Assuming two of these replacements are valid matches according to our dataset (DIWALI). The adaptation score is computed as follows: 1+1+ 3 = 2 3 ."
        },
        {
            "title": "4.4.2 LLM as Judge",
            "content": "Motivated by recent progress in utilizing large language models to evaluate text generations (Zheng et al., 2023). We conduct our evaluation using two state-of-the-art LLMs: Llama-3.1-8B-Instruct and Mistral 7B Instruct-v0.3. Both models are employed to assess the quality of culturally adapted 9We use token_sort_ratio metric from the FuzzyWuzzy library with default settings. text based on detailed prompt and scoring criteria. In particular, we prompt LLMs to score adapted text in three dimensions: Cultural Relevance, Language Fluency, and Mathematical Integrity. We follow Likert scale of 0 (very poor) to 5 (perfect)."
        },
        {
            "title": "4.4.3 Human Evaluation",
            "content": "For human evaluation, we randomly sample 50 adapted generations for each of 7 models, totaling 350 instances. Unlike LLM-based automatic evaluation, performing human evaluation captures subjective cultural nuances that LLMs may overlook. We assess the cultural adaptation of various models along Cultural Relevance dimension, defined in Table 14 under Appendix E. Each annotator is asked to rate the cultural relevance on 6-point Likert scale (0 = very poor, 5 = perfect) while assuming Country: India as culture proxy. To ensure consistency, we provide annotators with predefined score descriptions as evaluation instructions (Details mentioned in 16 under Appendix F). We consider five annotators from diverse socio-demographic backgrounds, specifically from Chhattisgarh, West Bengal, Maharashtra, Delhi, and Kerala in India. All evaluators are native Indian residents with 20+ years of residency in their respective sub-regions. The educational qualification and demographic details of the evaluators are presented under Appendix in Table 17. Due to constraints, we perform human evaluation only for adaptation with an English prompt on GSM8k set."
        },
        {
            "title": "5 Results and Discussions",
            "content": "This section describes results and presents discussions for the considered LLMs on CSIs based on average adaptation score, LLM as Judge, and Human Evaluation."
        },
        {
            "title": "Scores",
            "content": "Table 1 presents detailed comparison of the average adaptation score (AAS) obtained using three different sets of ground-truth Culture Specific Items (CSIs): CANDLE, DOSA, and DIWALI for both fuzzy and exact matching strategies10. To prepare CSIs11 for use with Bengali prompts, we per10Fuzzy match is computed only after removing exact matches, so it may be lower or higher than an exact match. 11Apart from DIWALI, we also translate CSIs of CANDLE and DOSA for MGSM comparison. Dataset: GSM Model Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match AAS (CANDLE) Prompt: English AAS (DOSA) AAS (DIWALI) AAS (CANDLE) Prompt: Bengali AAS (DOSA) AAS (DIWALI) Llama-2-7b-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct-v0.3 Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.050 0.040 0.083 0.051 0.106 0.067 0.060 0.028 0.040 0.058 0.028 0.056 0.049 0.039 0.053 0.048 0.052 0.037 0.087 0.551 0.202 0.005 0.007 0.006 0.006 0.005 0.007 0. 0.615 0.400 0.489 0.253 0.445 0.393 0.642 0.855 0.605 0.933 0.672 0.563 0.635 0.479 Dataset: MGSM 0.000 0.175 0.000 0.210 0.286 0.248 0.114 0.000 0.087 0.000 0.068 0.143 0.155 0.084 0.000 0.084 0.000 0.049 0.070 0.075 0. 0.000 0.018 0.000 0.044 0.014 0.008 0.007 0.000 0.403 0.000 0.337 0.531 0.456 0.365 0.000 0.131 0.000 0.195 0.102 0.075 0.127 Model Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match Fuzzy Match Exact Match AAS (CANDLE) Prompt: English AAS (DOSA) AAS (DIWALI) AAS (CANDLE) Prompt: Bengali AAS (DOSA) AAS (DIWALI) Llama-2-7b-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct-v0.3 Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.000 0.040 0.016 0.124 0.065 0.052 0. 0.000 0.016 0.040 0.077 0.024 0.013 0.036 0.000 0.045 0.032 0.062 0.024 0.013 0.324 0.000 0.000 0.000 0.000 0.008 0.000 0.000 1.000 0.160 0.200 0.316 0.192 0.169 0.428 1.000 0.494 0.488 0.285 0.608 0.416 0.364 0.051 0.115 0.000 0.094 0.254 0.118 0. 0.122 0.038 0.000 0.073 0.217 0.118 0.040 0.041 0.019 0.000 0.031 0.119 0.024 0.028 0.265 0.020 0.000 0.005 0.004 0.019 0.008 0.153 0.231 0.000 0.204 0.430 0.222 0.291 0.612 0.230 0.000 0.058 0.094 0.033 0.085 Table 1: Comparison of adaptation scores across different models. Here, AAS represents the Average Adaptation score, and GSM represents the GSM8k dataset."
        },
        {
            "title": "DOSA",
            "content": "E F"
        },
        {
            "title": "DIWALI\nF\nE",
            "content": "Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.157 0.104 0.169 0.107 0.057 0.078 0.166 0.326 0.021 0.868 0.071 0.057 0.098 0.083 0.044 0.000 0.000 0.048 0.014 0.039 0.000 0.258 0.083 0.358 0.059 0.028 0.078 0.166 0.842 0.416 0.547 0.345 0.171 0.098 0. 0.573 0.416 0.547 0.535 0.486 0.313 0.437 Table 2: Average Adaptation scores reported on DailyDialog. Here, and represent Exact and Fuzzy match, respectively."
        },
        {
            "title": "DOSA",
            "content": "E F"
        },
        {
            "title": "DIWALI\nF\nE",
            "content": "Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.020 0.067 0.304 0.055 0.100 0.000 0.088 0.020 0.067 0.130 0.109 0.086 0.037 0.022 0.000 0.000 0.000 0.000 0.014 0.000 0.022 0.020 0.067 0.174 0.055 0.071 0.074 0.444 0.969 0.467 0.826 0.472 0.371 0.667 0. 0.061 0.233 0.652 0.472 0.600 0.296 0.511 Table 3: Average Adaptation scores on ROCStories. Here, and represent Exact and Fuzzy match, respectively. form zero-shot translation12 from English to Bengali. Specifically, for each CSI from DIWALI we prompt: Translate this {concept} into Bengali without providing additional context or domain knowledge. The translated CSIs were then manually verified by one of the native authors13. For the GSM with an English prompt, DIWALI significantly outperforms CANDLE. For instance, the Llama-2-7b-chat-hf model achieves fuzzy match score of 0.615 and an exact match score of 0.855 with our dataset, compared to very low scores of 0.050 and 0.028, respectively, with CANDLE. This performance gap is consistently observed across models, indicating that DIWALI is more sensitive in capturing their adaptation capabilities. 12We used OpenAIs GPT-o3 for translation 13Belonging to West Bengal state of India"
        },
        {
            "title": "For",
            "content": "score the GSM with Bengali prompt, the differences between the two CANDLE and DIWALI remain evident. While some such as Llama-2-7b-chat-hf and models, Llama-3.2-1B-Instruct, zero for both fuzzy and exact match, while others instance, Llama-3.1-8B-Instruct achieves fuzzy match score of 0.175 and an exact match score of 0.087 using CANDLE, while DIWALI yields higher scores of 0.403 and 0.131, respectively. Similar observations are made for Mistral-7B-Instruct-v0.3 and Gemma-2-9B-Instruct. show clear gains. near"
        },
        {
            "title": "For",
            "content": "On MGSM, DIWALI scores much higher than CANDLE or DOSA, especially with the English prompt. For example, Llama-3.1-8B-Instruct exact match scores 0.494 with DIWALI but 0.016 with CANDLE and DOSA. We observe similar trend for the Bengali prompt. For instance, DIWALI scores Llama-2-7B-chat-hf 0.612 for exact match versus 0.122 and 0.265 for CANDLE and DOSA, respectively. Small parameter model, Llama-3.2-1B-Instruct stays near zero across all methods. Additional domains: On DailyDialog, all models achieve substantially higher AAS with DIWALI than with CANDLE or DOSA, where scores are nearzero for many models. Notably, Llama-2-7B-chat-hf leads with high AAS, achieving scores of 0.842 and 0.573 in exact and fuzzy match. Similarly, on ROCStories, we observe the same trend. For instance, Llama-2-7B-chat-hf achieves score of 0.969 with DIWALI, as compared to 0.020 for CANDLE. The comprehensive AAS results for both dialogue and story-based adaptation are reported in Table 2 and 3, respectively."
        },
        {
            "title": "5.2 LLM as Judge Results",
            "content": "by Instruct-v0.3), two LLM judges: Table 4a reports the average Likert scores assigned Llama (Llama-3.1-8B-Instruct) and Mistral (Mistral 7B for Cultural Relevance (CR), Language Fluency (LF), and Mathematical Integrity (MI). The prompt used for LLM as Judge is presented in the Appendix under Section under the prompt title LLM as Judge. Overall, the scores indicate that the evaluated models perform consistently well across these dimensions. For example, Gemma-2-9B-Instruct and Mistral-7B-Instruct-v0.3 achieve high scores consistently for both LLM judges."
        },
        {
            "title": "5.3 Human Evaluation Results",
            "content": "Table 4b presents the average Cultural Relevance scores as rated by five human evaluators for each model on 6-point Likert scale. Llama-3.1-8B-Instruct achieves the highest average cultural relevance score of 2.68, while Llama-3.2-1B-Instruct has the lowest average score of 0.75. We observe that LLM as Judge assigns higher cultural relevance scores than human evaluators. Suggesting that LLM as Judge might not be an appropriate method for evaluation of the cultural text adaptation task (Details are presented in Appendix H). Inter-annotator agreement: To ensure consistency among annotators and the reliability of the scores. We measure the inter-annotator reliability using Cohens κ (Landis and Koch, 1977) on the pair of human raters for each model. As shown in Table 5, there is acceptable agreement for models under the Llama and Gemma families. At the same time, evaluation scores for Mistral-7B-Instruct have low inter-annotator agreement."
        },
        {
            "title": "6 Analyses",
            "content": "In this section, we perform detailed analysis to understand the sub-regional coverage and shallow cultural adaptation of cultural concepts across 17 cultural facets. Sub-regional coverage: Despite their strong incontext capabilities and training on large corpus, LLMs lack pluralistic (Sorensen et al., 2024) alignment. This leads to the propagation of cultural ideas and views from their training data, causing dominating effect while underrepresenting other cultures. To investigate this, we pose the following question: Do LLMs, when prompted with the task of regional-level text adaptation, fairly represent all the sub-regional concepts? To analyze this, we map each replaced concept with sub-region (State or Union Territory) and plot geographical heatmap for each facet. The heatmap for the food facet is shown in Figure 3, which reveals significant bias in how concepts are adapted to represent sub-region. In particular, food-related concepts adapted by Gemma-2-2B-Instruct primarily originate from Uttar Pradesh, Madhya Pradesh, Maharashtra, and Punjab, with complete absence of adaptations from the Northeastern states. Similar sub-regional skews are observed for the rest of the facets. The heatmaps for the remaining 16 facets are presented in Appendix I. Surface vs. Deep level: Effective cultural adaptation must be faithful to the source text while emotionally resonating with the target audience by incorporating deep cultural nuances. (Hershcovich et al., 2022) define culture along three axes: Aboutness, Objectives & Values, and Common ground. In particular, Aboutness refers to the contextual relevance of concepts across cultures. For instance, the discussion of cricket match might be less meaningful in cultures where cricket is less prevalent. Deeper adaptation connects culturally relevant events and scenarios. Therefore, cultural adaptation requires adapting from one culture to another by connecting relevant events and scenarios. For example, in the festival of Durga puja (event), people from Bengal might relate more to pandal hopping (scenario), thus impacting the aboutness of the text in context with culture. Such adaptations are at deeper level and are more emotionally relatable. To understand the adaptation level surface or deep: We hypothesize that LLMs struggle with deeper level of cultural adaptation, particularly in connecting events to meaningful scenarios. To test this, we manually analyse model generations and found that LLMs often fail to connect events and scenarios together, thus leading to failure to establish aboutness. In Table 6, we show the detailed analysis of the Llama-3.1-8B-Instruct, adapted Mistral-7B-Instruct-v0.1, and Gemma-2-7B-Instruct. In particular, we observe Llama-3.1-8B-Instruct adapts the event Tuesday Diwali (festival). However, the associated scenario, i.e., sells CDs, remains unchanged. The scenario of selling CDs lacks any meaningful or emotionally resonating connection text for"
        },
        {
            "title": "Llama\nLF MI",
            "content": "CR"
        },
        {
            "title": "Mistral",
            "content": "CR"
        },
        {
            "title": "Model",
            "content": "A E Avg. Llama-2-7b-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 3.070 3.194 4.468 3.182 3.390 4.579 2.707 2.844 4.136 3.150 3.314 4.624 3.561 3.780 4.680 3.398 3.626 4.626 3.236 3.405 4.689 3.291 3.310 4.467 3.246 3.289 4.529 3.207 3.422 4.576 3.204 3.323 4.483 3.351 3.357 4.528 3.167 3.256 4.398 3.270 3.340 4.501 Llama-2-7b-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 2.02 2.42 0.46 1.76 1.86 1.90 2.38 2.18 2.46 0.72 2.08 1.98 1.82 2. 2.72 3.10 1.26 1.26 2.86 2.24 2.68 2.58 2.90 0.46 2.48 2.68 2.18 2.82 2.12 2.50 0.84 2.08 1.84 1.76 2.42 2.32 2.68 0.75 1.93 2.24 1.98 2.53 (a) LLM-as-Judge scores. Here, CR represents Cultural Relevance, LF represents Language Fluency, and MI represents Mathematical Integrity. (b) Human evaluation scores (Culture Relevance). Here AE are human evaluators from diverse socio-demographic sub-regions of India. Table 4: Evaluation scores of LLM-as-Judge and Human evaluation for cultural text adaptation. Figure 3: Comparison of heatmaps for Food facet across different subregions."
        },
        {
            "title": "Model",
            "content": "Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct"
        },
        {
            "title": "IAA",
            "content": "0.451 0.354 0.213 0.415 0.230 0.589 0.443 Table 5: Inter-annotator agreement (Cohens κ) on Cultural Relevance (CR). to the festival of Diwali. This demonstrates that current LLMs often perform surface-level concept replacement but fail to make deeper adaptations necessary for cultural resonance."
        },
        {
            "title": "7 Related Work",
            "content": "tifies candidate sentences using techniques such as string matching, named entity recognition, and set of handcrafted lexico-syntactic rules. These candidates are then classified into cultural facets such as geography, religion, or occupation using zero-shot classification methods based on natural language inference. Finally, CANDLE clusters sentences containing similar facts using sentence embeddings and hierarchical agglomerative clustering, allowing it to produce coherent, representative summaries for each cluster. While this approach achieves broad coverage, its application to India is limited, consisting of only 650 concepts, and these entries focus on five facets: clothing, drinks, cuisines, rituals, and traditions. In this section, we describe the various works related to building specific cultural artifacts with focus on Indian culture. CANDLE (Nguyen et al., 2023) develops large CSI collection by extracting cultural commonsense knowledge from webscale corpora using an end-to-end method. It idenIn contrast, DOSA (Seth et al., 2024) adopts participatory research methodology by engaging 260 participants across 19 Indian states to compile dataset of 615 artifacts. This method captures broader range of cultural dimensions, including regional rituals, local customs, and specific art forms,"
        },
        {
            "title": "Adapted Text",
            "content": "Llama-3.1-8B-Instruct Rohan sells CDs. He has 8 customers on Diwali. His first 3 customers buy one CD each. His next 2 customers buy 2 CDs each. His last 3 customers dont buy any CDs. How many CDs did Rohan sell on Diwali? Mistral-7B-Instruct Ram sells DVDs. He has 8 customers on Tuesdays (Til-lakshadhi). His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. His last 3 customers dont buy any DVDs. How many DVDs did Ram sell on Tuesdays (Til-lakshadhi)? Gemma-2-9B-Instruct Babu sells DVDs. He has 8 customers on Tuesday. His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. His last 3 customers dont buy any DVDs. How many DVDs did Babu sell on Tuesday? Llama-3.1-8b-Instruct Mistral-7B-Instruct Gemma-2-9B-Instruct Rohan and Aaradhya are having contest to see who can eat the most fruit jam-filled parathas, so they unroll as many as they can find. Unfortunately, someone makes mistake and Aaradhyas was two parathas wide and 24 parathas long while Rohans was 3 parathas wide and 14 parathas long. If they both ate their entire amount, how many did they eat on average? Ramesh and Meera are participating in competition to determine who can consume the most jalebis, so they unroll as many as they can find. Unfortunately, an error occurs, and Meeras jalebi stack is twice as wide and 24 stacks high, while Rameshs is 3 stacks wide and 14 stacks high. If they both consume their entire amount, how many did they eat on average? Anika and Rohan are having contest to see who can eat the most laddoos, so they unwrap as many as they can find. Unfortunately, someone makes mistake and Anikas was two laddoos wide and 24 laddoos long while Rohans was 3 laddoos wide and 14 laddoos long. If they both ate their entire amount, how many did they eat on average?"
        },
        {
            "title": "Observation",
            "content": "proper noun event scenario aboutness proper noun event scenario aboutness proper noun event scenario aboutness proper noun event scenario aboutness proper noun event scenario aboutness proper noun event scenario aboutness Table 6: Surface level adaptation shown for original text: (1) Billy sells DVDs. He has 8 customers on Tuesday. His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. His last 3 customers dont buy any DVDs. How many DVDs did Billy sell on Tuesday? (2) Marcell and Beatrice are having contest to see who can eat the most fruit roll-ups, so they unroll as many as they can find. Unfortunately, someone makes mistake and Beatrices was two roll-ups wide and 24 rolls up long while Marcells was 3 roll-ups wide and 14 roll-ups long. If they both ate their entire amount, how many did they eat on average? providing richer insights into Indias sub-regional diversity. However, while providing depth, such participatory methods are difficult to scale. Our work addresses these gaps by constructing comprehensive resource of cultural specific items, covering 17 facets across 36 sub-regions of India. cultural adaptation: recent explorations in open text generation, such as writing story (Bhatt and Diaz, 2024) and dialog-based (Singh et al., 2024), have highlighted the need for cultural adaptation. CSIs can enable better and holistic adaptation."
        },
        {
            "title": "9 Conclusion",
            "content": "Culture specific items (CSIs) are part of cultural knowledge bases and can potentially help elicit cultural knowledge in various applications, such as (a) cultural competence evaluation: existing methods on cultural competence evaluations of LLMs like Culture Bias Score (Naous et al., 2024) and Edit level analysis (Singh et al., 2024) rely on CSIs to measure the cultural biases of LLMs. Thus, having large-scale, sub-regional CSIs for India would enable more robust and nuanced evaluation of model competence across various downstream tasks, (b) cross-culture translation: while most machine translation systems are culture agnostic, recent work (Conia et al., 2024; Yao et al., 2024; Zhang et al., 2024) has focused on culturally-aware translation. Having comprehensive CSI collection is essential for this task, as it allows models to identify and appropriately translate culture-specific entities for better cross-cultural translation, and (c) In this paper, we introduce DIWALI, novel Cultural Specific Items (CSIs) dataset for Indian culture covering 36 subregions of India. The dataset is composed of carefully selected facets and manually verified, and contains total of 8,817 concepts across 17 facets. Leveraging our datasets improved cultural coverage over existing resources, we evaluate LLMs on the task of cultural text adaptation. We show the coverage of DIWALI by performing cultural adaptation for different domains such as education, daily conversations, and stories. Our human evaluation results on cultural text suggest that LLMs fail to perform this task adequately. Furthermore, our human evaluation analysis of surface adaptation demonstrates that LLMs mostly perform shallow-level adaptations. These findings highlight the need for future research to develop models and training methods that can grasp the nuanced context behind cultural concepts."
        },
        {
            "title": "References",
            "content": "In this work, we introduce novel culture specific item dataset, particularly belonging to Indian culture, covering the sub-regional level granularity. However, our study has certain limitations. First, our evaluations are restricted to widely used large language models that are trained on multilingual and diverse global datasets, rather than models that are specifically pre-trained or finetuned on countryspecific linguistic and cultural contexts. We intend to expand our study to country-specific LLMs. Second, our analysis on the surface or deeper level of generations is limited to few examples. The study of aboutness in cultural contexts necessitates analysis by large cultural population. Third, we performed human evaluation by recruiting five annotators from five diverse socio-demographic regions. However, multifaceted concept such as cultures requires more diverse evaluation sourced across all possible socio-demographic regions to facilitate more diverse and fair evaluation."
        },
        {
            "title": "Acknowledgements",
            "content": "We express our heartfelt thanks to all human evaluators for assessing the adaptations by the models. Furthermore, we thank anonymous reviewers and the meta-reviewer for their valuable suggestions and feedback, which significantly improved the quality of the paper."
        },
        {
            "title": "Ethical Considerations",
            "content": "Culture-specific items are sourced from publicly available sources, including Wikipedia, government, and tourism websites. We do not store any personal information in the DIWALI dataset. The dataset is intended for academic research and cultural analysis with careful consideration to avoid misrepresentation or community-specific practices. Furthermore, we do not collect any personal information from human evaluators. The dataset is available on Huggingface under MIT license. All annotators mentioned in Section 4.4.3 are research students, all with at least bachelors or masters degree. The annotation was done as part of their research activity, for which they were hired. The remuneration was covered under their monthly research assistantship. We used generative AI (ChatGPT-o4-mini) for language editing purposes only such as paraphrasing, spell-checking, and refining polishing the authors original content. Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Shivdutt Singh, Alham Fikri Aji, Jacki ONeill, Ashutosh Modi, and Monojit Choudhury. 2024. Towards measuring and modeling culture in LLMs: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1576315784, Miami, Florida, USA. Association for Computational Linguistics. Badr AlKhamissi, Muhammad ElNokrashy, Mai Alkhamissi, and Mona Diab. 2024. Investigating cultural alignment of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1240412422, Bangkok, Thailand. Association for Computational Linguistics. Sunil Barthwal and N. L. Gupta. 2011. Cultural values as advertisement themes in international and Indian Journal of Marketing, indian advertising. 42(11):1621. Yoav Benjamini and Yosef Hochberg. 2018. Controlling the false discovery rate: practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series (Methodological), 57(1):289300. Shaily Bhatt and Fernando Diaz. 2024. Extrinsic evaluation of cultural competence in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1605516074, Miami, Florida, USA. Association for Computational Linguistics. Jill Burstein, Jane Shore, John Sabatini, Yong-Won Lee, and Matthew Ventura. 2007. The automated In Proceedings of Human text adaptation tool. Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 34, Rochester, New York, USA. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Simone Conia, Daniel Lee, Min Li, Umar Farooq Minhas, Saloni Potdar, and Yunyao Li. 2024. Towards cross-cultural machine translation with retrievalaugmented generation from multilingual knowledge graphs. In EMNLP. Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhikers guide to testing statistical significance in natural language processing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13831392, Melbourne, Australia. Association for Computational Linguistics. Bradley Efron. 1979. Bootstrap methods: Another look at the jackknife. Annals of Statistics, 7:126. Hanem El-Farahaty and Rafat Y. Alwazna. 2024. Translating cultural references and sensitive elements between arabic and english: Netflix subtitles as case study. SAGE Open, 14(2):21582440241258293. Ana Luisa Ferreira. 2024. Cultural adaptation in film subtitles: study of brazilian films for hispanic audiences. Journal of Linguistics and Communication Studies, 3(4):4853. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Hamna, Deepthi Sudharsan, Agrima Seth, Ritvik Budhiraja, Deepika Khullar, Vyshak Jain, Kalika Bali, Aditya Vashistha, and Sameer Segal. 2025. Kahani: Culturally-nuanced visual storytelling tool for nonwestern cultures. In Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies, COMPASS 25, page 379400, New York, NY, USA. Association for Computing Machinery. Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders Søgaard. 2022. Challenges and strategies in crossIn Proceedings of the 60th Annual cultural NLP. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 69977013, Dublin, Ireland. Association for Computational Linguistics. Huihan Li, Arnav Goel, Keyu He, and Xiang Ren. 2025. Attributing culture-conditioned generations to preIn The Thirteenth International training corpora. Conference on Learning Representations. Xianggui Li. 2024. Cross cultural adaptation of advertising in multinational enterprises - case study of the nike inc. Perspectives in Social Sciences and Arts, 3(1):139. Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986995, Taipei, Taiwan. Asian Federation of Natural Language Processing. Reem Masoud, Ziquan Liu, Martin Ferianc, Philip C. Treleaven, and Miguel Rodrigues Rodrigues. 2025. Cultural alignment in large language models: An explanatory analysis based on hofstedes cultural dimensions. In Proceedings of the 31st International Conference on Computational Linguistics, pages 84748503, Abu Dhabi, UAE. Association for Computational Linguistics. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839849, San Diego, California. Association for Computational Linguistics. Tarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. 2024. Having beer after prayer? measuring cultural bias in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1636616393, Bangkok, Thailand. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Tarek Naous and Wei Xu. 2025. On the origin of cultural biases in language models: From pre-training In Proceedings of data to linguistic phenomena. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 64236443, Albuquerque, New Mexico. Association for Computational Linguistics. J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159174. Cheng Li, Mengzhuo Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. 2024. Culturellm: Incorporating cultural differences into large language models. In Advances in Neural Information Processing Systems, volume 37, pages 8479984838. Curran Associates, Inc. Peter Newmark. 2003. textbook of translation. Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. 2023. Extracting cultural comIn Proceedings of monsense knowledge at scale. the ACM Web Conference 2023, WWW 23, page 19071917, New York, NY, USA. Association for Computing Machinery. Tijana Radojevic, Nemanja Stanišic, and Nenad Stanic. 2024. How culture shapes the restaurant experience: study of hofstedes dimensions and service quality. Hotel and Tourism Management, 12(1). Michael Ryan, William Held, and Diyi Yang. 2024. Unintended impacts of LLM alignment on global representation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1612116140, Bangkok, Thailand. Association for Computational Linguistics. Agrima Seth, Sanchit Ahuja, Kalika Bali, and Sunayana Sitaram. 2024. DOSA: dataset of social artifacts from different Indian geographical subcultures. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 53235337, Torino, Italia. ELRA and ICCL. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 63496384, Bangkok, Thailand. Association for Computational Linguistics. Michał Wilczewski, Anne-Marie Søderberg, and Arkadiusz Gut. 2019. Storytelling and cultural learningan expatriate managers narratives of collaboration challenges in multicultural business setting. Learning, Culture and Social Interaction, 21:362 377. Binwei Yao, Ming Jiang, Tara Bobinac, Diyi Yang, and Junjie Hu. 2024. Benchmarking machine translation with cultural awareness. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1307813096, Miami, Florida, USA. Association for Computational Linguistics. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022. Language models are multilingual chain-of-thought reasoners. Preprint, arXiv:2210.03057. Zhonghe Zhang, Xiaoyu He, Vivek Iyer, and Alexandra Birch. 2024. Cultural adaptation of menus: finegrained approach. In Proceedings of the Ninth Conference on Machine Translation, pages 12581271, Miami, Florida, USA. Association for Computational Linguistics. Pushpdeep Singh, Mayur Patidar, and Lovekesh Vig. 2024. Translating across cultures: LLMs for inIn Proceedings of tralingual cultural adaptation. the 28th Conference on Computational Natural Language Learning, pages 400418, Miami, FL, USA. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Taylor Sorensen, Jillian Fisher, Jared Moore, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. 2024. Position: roadmap to pluralistic alignment. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, and 179 others. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael Lyu. 2024. Not all countries celebrate thanksgiving: On the cultural dominance in large language models. In"
        },
        {
            "title": "A CANDLE False Positives",
            "content": "Cultural concepts extracted by CANDLE are limited and contain false positives for Indian culture. To filter such false positives, we further utilize prompt-based method to check for cultural relevance with respect to the facets in CANDLE. We design simple prompt template shown in Prompt - CSIs Relevancy. For example: Is rishikesh cultural ritual concept of India? Answer in Yes or No. Each filtration is then manually inspected by one of the authors. We remove total of 393 concepts. The detailed statistics of the concepts removed and kept are shown in Table 7."
        },
        {
            "title": "Clothing",
            "content": ""
        },
        {
            "title": "Food",
            "content": ""
        },
        {
            "title": "Rituals\nTraditions",
            "content": "183 191 107 117 78"
        },
        {
            "title": "98 Western, Pakistan, Seersucker, Bohemian, Kimono, Japanese Fireman, Chinese",
            "content": "40 98 Dragon Robe, Poncho China, Gewurztraminer, Japan, Thailand, Pakistan, Sri Lanka, Bangladesh, England, Arabica, Robusta, Ginseng, Espresso Chinese, Continental, Italian, Southeast, American British, Arabica, Robusta, Gewurztraminer, Pakistan, Philippines, Burmese, Mauritius, Chinese Food, Sri Lankan, Thai, Singapore, Malay, Pakistani, Nepalese Light, Africa, Nepal 133 150 American, British, Christmas, Africa, Nepal, Jew, Buddhism Table 7: False positive cultural concepts from CANDLE framework (Domain = Country, Subject = India) ."
        },
        {
            "title": "B DIWALI Details",
            "content": "The facets per sub-region distributions are listed in Table 8. DIWALI contains facet, concept, description, subregion, and source link. Some of the samples from DIWALI are depicted in Table 9. Table 10 lists out the facets considered and their description, along with examples. Sub-region"
        },
        {
            "title": "AN\nAP\nAR\nAS\nBH\nCH\nCT\nDNH\nDL\nGA\nGJ\nHR\nHP\nJK\nJH\nKA\nKL\nLA\nLD\nMP\nMH\nMN\nML\nMZ\nNL\nOD\nPY\nPB\nRJ\nSK\nTN\nTG\nTR\nUP\nUK\nWB",
            "content": "3 8 10 10 7 5 5 1 5 4 8 7 10 10 7 8 10 5 3 8 10 8 8 7 7 8 4 9 8 3 18 9 7 6 9 10 20 43 20 80 30 13 40 13 15 30 38 19 43 27 30 41 46 17 13 34 27 24 27 12 24 28 5 45 38 22 49 45 20 43 38 46 15 20 24 15 15 10 15 15 10 15 15 13 20 27 19 20 15 10 10 15 15 19 10 15 10 15 10 15 20 10 20 10 15 15 20 10 31 37 20 67 65 10 29 16 15 84 20 24 20 10 23 16 10 12 6 10 9 31 25 19 10 60 10 10 10 8 12 11 11 47 10 65 30 37 20 42 35 50 31 27 42 44 36 41 31 40 40 39 46 45 38 34 45 47 30 37 30 47 37 50 41 40 86 36 36 43 36 30 4 20 10 20 10 2 12 0 10 10 11 20 20 11 10 15 12 5 0 11 20 10 8 10 9 20 8 20 10 5 20 10 15 10 10 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 10 31 15 17 14 2 6 7 15 30 15 13 8 5 6 13 20 12 5 9 10 10 12 20 20 20 10 10 12 10 15 8 10 48 20 15 20 15 24 15 10 8 15 9 7 10 12 13 13 10 12 19 37 8 4 10 17 15 13 15 20 29 7 10 15 12 21 7 10 10 15 15 6 10 8 8 8 5 8 6 5 11 10 9 7 10 9 10 10 4 7 10 10 7 10 5 7 10 5 9 10 10 6 10 6 7 5 10 5 12 20 13 15 10 30 4 25 14 20 16 20 11 25 15 17 10 10 10 20 9 13 5 10 11 10 28 13 29 19 40 7 10 19 17 7 6 9 7 6 7 9 6 8 6 9 7 6 6 6 7 8 6 6 7 8 5 6 7 7 7 6 6 6 7 6 6 4 6 6 9 10 10 14 19 10 19 10 10 14 10 10 10 10 10 16 20 10 10 10 12 14 10 10 10 10 10 10 10 10 15 10 9 10 10 10 6 28 58 19 20 10 33 9 8 17 16 10 15 10 10 11 10 10 5 8 10 10 10 9 8 10 5 10 10 10 8 8 16 10 8 9 10 15 10 19 10 10 6 10 10 15 13 10 10 10 10 10 20 10 5 10 10 10 8 10 10 10 10 10 10 10 10 10 10 10 10 10 6 10 8 9 7 7 6 11 10 11 10 10 7 9 5 10 10 6 6 10 10 10 10 7 5 10 5 10 20 6 10 10 12 12 13 10 8 36 11 16 10 10 17 8 20 13 19 10 7 20 14 25 28 15 13 18 27 46 14 10 16 17 19 29 12 15 12 16 10 27 10 10 Table 8: Distribution of 17 cultural facets (columns) across 36 States/UTs (rows), using abbreviations: AN = Andaman and Nicobar Islands, AP = Andhra Pradesh, AR = Arunachal Pradesh, AS = Assam, BH = Bihar, CH = Chandigarh, CT = Chhattisgarh, DNH = Dadra and Nagar Haveli and Daman and Diu, DL = Delhi, GA = Goa, GJ = Gujarat, HR = Haryana, HP = Himachal Pradesh, JK = Jammu and Kashmir, JH = Jharkhand, KA = Karnataka, KL = Kerala, LA = Ladakh, LD = Lakshadweep, MP = Madhya Pradesh, MH = Maharashtra, MN = Manipur, ML = Meghalaya, MZ = Mizoram, NL = Nagaland, OD = Odisha, PY = Puducherry, PB = Punjab, RJ = Rajasthan, SK = Sikkim, TN = Tamil Nadu, TG = Telangana, TR = Tripura, UP = Uttar Pradesh, UK = Uttarakhand, WB = West Bengal. https://www.indiatravel. app/traditionaldressf-arunachal-pradesh/ https://diversityassam.c om/culture/apong-a-tradi tionalricebeerof- ass am/ https://www.deccanchroni cle.com/lifestyle/food-a nd-recipes/270323/famous -food-of-andhra-pradesh.h tml https://sombitdeyphotogr aphy.com/blog/bengali-mar riage-rituals https://aratigoa.wordpre ss.com/2024/06/20/unders tanding-goas-ecology-thr ough-rituals-and-festiva ls/ https://en.wikipedia.org /wiki/Bangalore https://en.wikipedia.org /wiki/Folk_dance_forms_o f_Odisha https://en.wikipedia.org /wiki/Ross_Island_(Andama n) https://www.holidify.com /collections/festivals-i n-himachal-pradesh https://www.lehladakhtou rism.com/about-ladakh/la dakh-religion.html https://en.wikipedia.org /wiki/Maram_language https://www.memeraki.com /blogs/posts/the-beautif ul-arts-of-bihar-manjush a-tikuli-madhubani"
        },
        {
            "title": "Kupaan",
            "content": "Simple cotton wrap worn by Nyishi men, often with bamboo hats."
        },
        {
            "title": "Apong",
            "content": "Rice beer brewed by the Mising tribe."
        },
        {
            "title": "Pulihora",
            "content": "Tamarind rice with spices, often prepared during festivals and special occasions."
        },
        {
            "title": "Saptapadi",
            "content": "Bengali wedding ritual where couple takes seven steps on betel leaves."
        },
        {
            "title": "Zoti",
            "content": "Traditional practice of offering cucumbers during monsoon."
        },
        {
            "title": "Jhumair",
            "content": "Indias tech hub blending cosmopolitan vibe with Dravidian roots. Folk dance during harvest season, prevalent in North and Western Odisha."
        },
        {
            "title": "Ross Island",
            "content": "Former British administrative headquarters with ruins."
        },
        {
            "title": "Andaman and Nicobar Islands",
            "content": "Tibetan New Year celebrated in Lahaul and Spiti with traditional rituals."
        },
        {
            "title": "Vaddanam",
            "content": "Ancient animistic faith influencing local traditions. Maram is Sino-Tibetan language spoken by the Maram Naga tribe in the Senapati district. The language is integral to the tribes cultural identity, with rich oral traditions and customary practices. Tikuli is traditional art form from Bihar that involves the creation of intricate designs on glass, adorned with gold and silver foils. Historically, it was used to make bindis (forehead decorations) for women. Today, Tikuli art has evolved to include decorative items and paintings, reflecting the rich cultural heritage of the region. Situated in Jaipur, the Hawa Mahal, or Palace of Winds, was built in 1799 by Maharaja Sawai Pratap Singh. This fivestory structure features 953 small windows, called jharokhas, designed to allow royal ladies to observe street festivities without being seen. Players stand at specific distance with mark between them. Holding bamboo pole under their right armpits, they grasp it firmly and try to push each other over the mark. Himroo is traditional fabric from Aurangabad, blending silk and cotton to create luxurious texture. The weaving technique produces intricate patterns, often inspired by Persian designs, reflecting the regions historical connections. Gold waist belt; heavy ornament; worn during weddings and festivals."
        },
        {
            "title": "Devya",
            "content": "Means Gods gift."
        },
        {
            "title": "Jammu and Kashmir",
            "content": "Table 9: Samples from DIWALI dataset"
        },
        {
            "title": "Rajasthan",
            "content": "https://en.wikipedia.org /wiki/Hawa_Mahal"
        },
        {
            "title": "Tripura",
            "content": "https://en.wikipedia.org /wiki/Tripuri_games_and_ sports"
        },
        {
            "title": "Telangana",
            "content": "https://textilevaluechai n.in/in-depth-analysis/ar ticles/traditional-texti les/traditionaltextile s-of-maharashtra https://sribhavanijewels .home.blog/2020/07/02/te langana-traditional-jewel lery/ https://www.behindthenam e.com/submit/names/usage /dogri"
        },
        {
            "title": "C Decoding Strategies and Inference Setup",
            "content": "Decoding Settings: For all experiments, we set the sampling temperature to 0. This is used to ensure deterministic output generation, which is crucial for reproducibility and consistent evaluation. Moreover, by not specifying any top-p (nucleus sampling) or top-k parameters, we disable stochastic sampling methods in favour of greedy decoding. This further guarantees that the outputs are stable across different runs. We obtained generations using this approach in single run for the entire dataset. Inference Setup: Inference is configured to generate maximum of 2048 new tokens per prompt, ensuring"
        },
        {
            "title": "Description",
            "content": "Traditional and regional attire worn. For example, Mekhela Chador. Beverages with cultural and historical significance, including regional specialties and traditional drinks. For example, Darjeeling Tea. Dishes and food items. For example, Dosa. Customs followed in religious, social, and life-cycle events. For example, Chathurthi Vrat. Cultural practices, values, and beliefs. For example, Gaye holud. Administrative divisions of India. For example, Gujarat. Traditional and prominent dance styles. For example, Kuchipudi. Common geographical and locations. For example, Dal lake. Common festivals and celebrations. For example, Pongal. Prominent Religious activities. For example, Buddhism. Cuisine (Food) Rituals Traditions States & Capitals Dance forms Places Festivals Religion Languages & Dialects Spoken languages and dialects. For example, Konda. Arts Architectures Traditional games Textiles Jewellery Names Painting, Sculpture, and other creative expressions. For example, Terracotta. Architectural styles. For example, Kesariya Stupa. Indigenous games, and recreational activities. For example, Gilli Danda. Regional Weaving Traditions. For example, Gamsa. Traditional ornaments, and adornments. For example, Tora. Common and significant names. For example, Arjun. Table 10: Facets considered that the models have sufficient capacity to produce complete and coherent responses. Our experiments were conducted on NVIDIA A100 GPUs with 40GB of memory. Depending on the specific experiment, we used either single GPU or multiple GPUs to accelerate inference."
        },
        {
            "title": "D Token Lengths and Significance Tests",
            "content": "D.1 Average Input/Output Lengths"
        },
        {
            "title": "Model",
            "content": "Avg. characters tokens Input (per sample) 3,771 Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct-v0.3 Gemma-2-2B-Instruct Gemma-2-9B-Instruct"
        },
        {
            "title": "Overall mean output",
            "content": "482 370 526 392 437 596 356 451 942 120 92 132 98 109 149 89 113 Table 11: Average input and output lengths shown for GSM8k dataset on cultural text adaptation task. We also show rough token count using simple rule of thumb: about 4 characters per token. D.2 Significance Testing For every dataset and model, we report the mean exact-match accuracy and 95% confidence interval. We use non-parametric bootstrap (Efron, 1979) with 10,000 resamples. Each resample picks examples with replacement from the test set, we recompute the mean, and we take the 2.597.5 percentile range as the 95% CI. When comparing two systems on the same items, we use paired approximate randomisation test (10,000 sign flips; (Dror et al., 2018)). We flip the sign of each per-item difference at random and recompute the mean difference. The two-sided p-value is the fraction of permutations with an absolute difference at least as large as the observed one (with the usual +1 correction). We also report the mean gap in percentage points (pp). How we correct for multiple comparisons. Within each family (all modelmodel pairs on given dataset/prompt, and all DIWALICANDLE deltas per model on that dataset/prompt), we apply BenjaminiHochberg (BH) (Benjamini and Hochberg, 2018). We call result significant if < 0.05."
        },
        {
            "title": "Model",
            "content": "Exact-match 95% CI"
        },
        {
            "title": "Model",
            "content": "Exact-match 95% CI Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.0285 0.0859 0.0474 0.0296 0.0554 0.0438 0.0401 [0.018, 0.040] [0.070, 0.102] [0.034, 0.061] [0.020, 0.040] [0.043, 0.069] [0.031, 0.058] [0.029, 0.052] Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.7808 0.5401 0.8095 0.5718 0.5047 0.5574 0.4488 [0.754, 0.806] [0.511, 0.568] [0.784, 0.834] [0.543, 0.601] [0.477, 0.532] [0.524, 0.591] [0.420, 0.478] (a) CANDLE baseline scores (b) DIWALI scores"
        },
        {
            "title": "Model",
            "content": "(DIWALI - CANDLE) Significant? Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct 0.7523 0.4542 0.7621 0.5422 0.4494 0.5136 0. 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1 104 1"
        },
        {
            "title": "TRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE",
            "content": "Table 13: DIWALI vs. CANDLE improvements. is the mean exact-match difference in percentage points. from the paired approximate-randomisation test (10k permutations). is BH-adjusted within the DIWALI CANDLE family. Takeaways: All seven models are significantly better on DIWALI than on CANDLE. On DIWALI, every pairwise model comparison is significant after BH correction, with the biggest gap being +0.37 pp (Llama-3.2-1B-Instruct over Gemma-2-9B-Instruct). Thus, DIWALI is more comprehensive and also separates models more clearly."
        },
        {
            "title": "E Likert Scale",
            "content": "The description of the Likert scale for cultural relevance is listed in Table 14, and Language and Mathematical fluency are listed in Table 15."
        },
        {
            "title": "F Instructions for Human Evaluation",
            "content": "In this section, we describe the detailed instructions provided to the human evaluators for annotating cultural adaptation scores for various LLMs. The instruction set was designed after multiple brainstorming sessions with authors and annotators all belonging to different sub-regions. Details are provided in Table 16."
        },
        {
            "title": "G Human Annotators",
            "content": "In this section, we describe the socio-demographic location of our human evaluators. Details are provided in Table 17. Human evaluation vs. LLM as Judge: Comparison on Cultural Relevance This section compares Cultural Relevance (CR) scores from human annotators (Table 4b) against CR scores from the two LLM judges (Table 4a). We observe that across all evaluated models, the LLM judges consistently assign higher CR scores than human evaluators with score inflation of approximately +0.5 to +2.5 (Details are presented in Table 18). We hypothesize that this discrepancy arises because LLM judges tend to overvalue surface-level entity changes such as simple proper noun swaps. This finding suggests that the LLM as Judge method may not be reliable for evaluating the deeper nuances of cultural text adaptation. Sub-regional Adaptation Coverage The heatmaps for various facets are shown in Figures 5, 6, 7, 9, and 10."
        },
        {
            "title": "Description",
            "content": "0 1 2 3 4 5 Very Poor No adaptation at all, or only direct transliteration / literal translation with no cultural tailoring. Poor Fair Moderate Good Perfect Adaptations are non-sensical or illogical, introducing absurd elements that misrepresent culture. Only simple replacement of proper nouns; broader context remains unchanged. Proper nouns replaced and surrounding context altered to fit typical Indian usage. Multiple entities adapted, showing coherent and deeper integration with Indian culture. Cultural references are deeply integrated and fully resonant; no further adaptation is possible. Table 14: Descriptions for the 05 Likert scale used in evaluating cultural relevance."
        },
        {
            "title": "Rubric",
            "content": "Description (LF) Description (MI) 0 1 2 3"
        },
        {
            "title": "Perfect",
            "content": "Uses completely Western terminology and expressions; no Indian linguistic elements appear. Minimal attempt at adaptation; Indian terms used incorrectly or out of place. Contains few Indian terms, but they are often mis-used or feel forced. Some Indian terms used correctly, though overall phrasing lacks natural flow. Effectively incorporates Indian English and terminology with smooth, natural flow. Seamlessly blends Indian terminology, expressions, and natural language patterns. Mathematics is flawlessly integrated with cultural elements while reMathematics is unsound or illogical. Basic arithmetic is correct but poorly integrated with cultural context. Mathematically sound, but cultural context could be better aligned. Well-structured problems with clear cultural context. Problem is mathematically nonsensical after adaptation. Table 15: Likert-scale rubric (05) for evaluating Language Fluency (LF) and Mathematical Integrity (MI). maining precise and clear."
        },
        {
            "title": "Original Text",
            "content": "0 No adaptation or complete transliterations/translations. 1 Non-sensical adaptations (breaks logic or value conversion). 2 Simple replacement of proper nouns only. 3 Proper nouns with minor contextual change. Incorrect Adaptation Ramesh ka 3 sprints 3 times week 60 meter par 3 baar run karta hai. Isse kya total meter run karta hai? James decides to run 3 sprints 3 times week. He runs 60 meters each sprint. How many total meters does he run week? Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns are there total? Ramesh plants 3 gulab jamun plants. Each gulab jamun plant has 25 gulab jamuns. Each gulab jamun has 8 thorns. How many thorns are there total? has sheep twice Toulouse as many as Charleston. Charleston has 4 times as many sheep Seattle. as How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep? Darrell and Allens ages are in the ratio of 7:11. If their total age now is 162, calculate Allens age 10 years from now. has sheep twice Toulouse as many as Charleston. Charleston has 4 times as many sheep as Seattle. Seattle has 20 sheep. Toulouse has 2 4 20 sheep. How many sheep do Toulouse, Charleston, and Seattle have together? Ramesh and Rohans ages are in the ratio of 7:11. If their total age now is 16,200, calculate Rohans age 10 years from now. Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins does Gretchen have? Ramesh has 110 coins. There are 30 more gold coins than silver coins. How many gold coins does Ramesh have? Cody eats three times as many cookies as Amir eats. If Amir eats 5 cookies, how many cookies do both of them eat together? Rohan eats three times as many laddoos as Priya eats. If Priya eats 5 laddoos, how many laddoos do both of them eat together? traditional Indian silk saree requires 2 bolts of blue silk and half as much white cotton. How many bolts of fabric in total are needed? Rohan slept 9 hours last night. His friend Vijay slept only 2/3 of what Rohan slept. How many more hours did Rohan sleep than Vijay? 4 Multiple entities changed with deeper connection to Indian culture. robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? Fully culturally resonant adaptation (deemed necessary). Harry slept 9 hours last night. His friend James slept only 2/3 of what Harry slept. How many more hours did Harry sleep than James? Table 16: Instruction for Human evaluation Evaluator Location (Sub-Sub-Region/Sub-Region/Country) YoR Educational Qualification"
        },
        {
            "title": "A\nB\nC\nD\nE",
            "content": "Hoogly / West Bengal / India Raipur / Chhattisgarh / India Mumbai / Maharashtra / India Karol Bagh / Delhi / India Ernakulam / Kerala / India 22 28 25 24 25 Graduate Post-Graduate Graduate Post-Graduate Graduate Table 17: Demographic information of human evaluators. Here, YoR represents years of residence in the sub-region."
        },
        {
            "title": "Model",
            "content": "Llama-2-7B-chat-hf Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Mistral-7B-Instruct Gemma-2-2B-Instruct Gemma-2-9B-Instruct Human CR Avg. Llama Judge (CR) Mistral Judge (CR) Score"
        },
        {
            "title": "Score",
            "content": "2.32 2.68 0.75 1.93 2.24 1.98 2.53 3.070 3.182 2.707 3.150 3.561 3.398 3.236 +0.75 +0.50 +1.96 +1.22 +1.32 +1.42 +0.71 3.291 3.246 3.207 3.204 3.351 3.167 3. +0.97 +0.57 +2.46 +1.27 +1.11 +1.19 +0.74 Table 18: Cultural Relevance (CR): Human vs. LLM-as-Judge. denotes (LLM Judge CR Human CR). LLM judges consistently score higher than human annotators. (a) Architecture (b) Arts Figure 4: Comparison of heatmaps for Architecture and Arts. (a) Clothing (b) Dance Forms Figure 5: Comparison of heatmaps for Clothing, and Dance Forms. (a) Festivals (b) Jewellery Figure 6: Comparison of heatmaps for Festivals, Jewellery. (a) Language and Dialects Figure 7: Comparison of heatmaps for Language & Dialects, and Places. (b) Places (a) Religion (b) Rituals (a) States and Capitals (b) Textiles Figure 9: Comparison of heatmaps for States & Capitals, and Textiles. (a) Traditional Games (b) Names Figure 10: Comparison of heatmaps for Traditional games, and Names. (a) Festivals Figure 11: Comparison of heatmaps for Festivals"
        },
        {
            "title": "Simple Prompt",
            "content": "Template: Please provide comprehensive, deduplicated list of 500 culture-specific {facet} items associated with Indian culture. Ensure that each item is unique and pertains specifically to Indian traditions and practices. Present the output in CSV format. Curating India-specific CSIs SYS INST.: Generate detailed and comprehensive list of traditional and modern dance forms specific to State/UT, India, covering various cultural, functional, and social contexts. Present the information in CSV format with the following columns: Dance Form Name: The name of the dance form. Description: brief description (maximum 20 words) of the dance form, highlighting its cultural, functional, or symbolic significance. Reference Link: reliable source link for further reading or verification. Ensure the list includes: Dance forms performed in different contexts, including festivals, weddings, rituals, community gatherings, and stage performances. Representation of dance forms for men, women, and children across diverse social, cultural, and economic settings. Detailed coverage of both traditional folk dances and contemporary adaptations. Include dance forms that are specific to various communities, tribes, and cultural groups within State/UT. Representative of regional uniqueness and cultural diversity."
        },
        {
            "title": "Cultural Text Adaptation",
            "content": "SYS INST.: You are an AI assistant tasked with adapting text to suit specific cultural context in particular language while maintaining the original meaning and intent. Your goal is to ensure the text feels natural and appropriate for the target audience, considering cultural nuances, values, and sensitivities. When making these adaptations, follow these key steps: 1. Cultural Relevance: Adjust any idioms, metaphors, or cultural references that may not resonate with the target audience. Replace them with culturally appropriate alternatives. 2. Tone and Intent: Preserve the original emotional tone and message, even when making cultural adjustments. 3. Cultural Sensitivity: Be mindful of topics, words, or phrases that might be sensitive or inappropriate in the target culture. Specific guidelines: Replace foreign names with common Indian names (female for female, male for male). Use diverse set of names. Use Indian locations in place of foreign locations. Convert all foreign currencies to Indian Rupees (INR) using clear symbolic or approximate rate (for example, $1 = 83). Remove any remaining references to foreign currency (USD, etc.). Incorporate Indian traditions, festivals, and cultural practices only if it is contextually appropriate and does not distort the original meaning. Use regional-specific terminology and expressions without changing the logical sense of the text. Replace foreign food items with Indian equivalents only if it makes sense. For example, muffins can become parathas, but do not replace food items that are already commonly used in India or are essential to the texts logic (e.g., do not replace eggs if its about chickens laying eggs). Maintain original mathematical operations and numerical values. Do not show any calculations or provide step-by-step solutions. Do not transliterate. Do not solve the problem or provide the answer. Do not hallucinate or introduce factual errors. Ensure the adapted text is coherent and flows naturally in its new cultural context. Provide your response as single-line JSON array without any line breaks or extra whitespace. The response must be valid JSON that can be parsed directly. For the replaced_concepts dictionary: ONLY include terms that were actually changed to different terms (e.g., \"John\":\"Ramesh\"). DO NOT include terms that remained the same (e.g., do not include \"eggs\":\"eggs\" if it was not changed). Use the symbol directly in the values, not Unicode escape sequences. Include ONLY the meaningful substitutions you made (e.g., \"John\":\"Ramesh\",\"muffins\":\"parathas\", \"$10\":\"830\"). Example of correct replaced_concepts: Good: {\"John\":\"Ramesh\", \"muffins\":\"parathas\", \"$10\":\"830\"} Bad: {\"eggs\":\"eggs\", \"John\":\"Ramesh\", \"muffins\":\"parathas\", \"$10\":\"u20b9830\"} Provide only the adapted text and replaced words of the problem statement in valid JSON format, like this example: [{\"cultural_adapted_text\":\"Ramesh bought 5 parathas for 830 and gave 2 to his friend. How much money did he spend per paratha?\", \"replaced_concepts\":{\"John\":\"Ramesh\",\"muffins\":\"parathas\", \"$10\":\"830\"}}] USER PROMPT: Adapt the following text to Indian culture: {input text} Figure 12: Bengali Prompt for Cultural Text Adaptation LLM-as-Judge Consider yourself as an AI expert trained to evaluate the cultural adaptation of given text. Original Text: {original_text} Adapted Text: {adapted_text} Target Culture: Indian Rate each criterion on 05 scale and give concise justification for each score: 1. Cultural Relevance (05): 0 No adaptation: retains Western concepts with no cultural shift 1 Non-sensical adaptation: culturally inappropriate or absurd elements (e.g., paratha growing on plants) 2 Simple proper-noun swap: only names changed to Indian equivalents 3 Names plus limited context change: some culturally relevant objects or settings 4 Deep contextual change: multiple entities adapted with clear Indian cultural grounding 5 Fully resonant: authentic, natural Indian context with no further improvements possible 2. Language Fluency (05): 0 No Indian linguistic elements 1 Very poor: minimal, incorrect Indian terms 2 Poor: few Indian terms, often mis-used 3 Moderate: correct terms but slightly forced flow 4 Good: smooth Indian English with natural terminology 5 Perfect: seamless blend of Indian expressions and natural language patterns 3. Mathematical Integrity (05): 0 Mathematically nonsensical 1 Incorrect mathematics 2 Basic math correct but weak cultural tie-in 3 Sound math; cultural link could be clearer 4 Well-structured math with clear cultural context 5 Mathematics flawlessly integrated with cultural elements Format your response exactly like this (replace the numbers with your scores): Cultural Relevance: 5 Explanation: The adapted text is deeply integrated with Indian culture, accurately reflecting significant traditions and practices. Language Fluency: 5 Explanation: The text uses natural Indian expressions and terminology, making it easy to read and culturally coherent. Mathematical Integrity: 5 Explanation: The mathematical problem is presented correctly and is well-integrated into the cultural context. Do not output any extra text or placeholders such as [score]. Figure 13: Prompt for LLM as Judge"
        }
    ],
    "affiliations": [
        "Natural Language and Information Processing Lab (NLIP) Indian Institute of Technology Hyderabad"
    ]
}