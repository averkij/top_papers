{
    "paper_title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "authors": [
        "Qintong Zhang",
        "Xinjie Lv",
        "Jialong Wu",
        "Baixuan Li",
        "Zhengwei Tao",
        "Guochen Yan",
        "Huanyao Zhang",
        "Bin Wang",
        "Jiahao Xu",
        "Haitao Mi",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data."
        },
        {
            "title": "Start",
            "content": "DocDancer: Towards Agentic Document-Grounded Information Seeking Qintong Zhang*, Xinjie Lv, Jialong Wu, Baixuan Li, Zhengwei Tao, Guochen Yan, Huanyao Zhang, Bin Wang, Jiahao Xu, Haitao Mi, Wentao Zhang Peking University, Shanghai AI Lab, Tencent AI Lab wujialongml@gmail.com, wentao.zhang@pku.edu.cn 6 2 0 2 8 ] . [ 1 3 6 1 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-thenSynthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBenchDoc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data."
        },
        {
            "title": "Introduction",
            "content": "Understanding and answering questions over long, multi-modal documents is critical capability for real-world intelligent systems (Tkaczyk et al., 2015; Liu et al., 2025b). Document Question Answering (DocQA) lies at the core of documentcentric intelligence, enabling models to access, reason over, and synthesize information from complex and heterogeneous document sources. Existing DocQA methods can be broadly categorized into three paradigms. The first paradigm relies on optical character recognition (OCR) to convert documents into plain text, which is then processed by downstream language models (Xu et al., 2020). The second paradigm adopts embeddingbased retrieval mechanisms, most commonly instantiated through retrieval-augmented generation *Equal Contributions. Jialong Wu is the project leader. Corresponding Author. 1 Figure 1: The overall of DocDancer for documentgrounded information seeking, where search and read tools for effective document retrieval and comprehension over processed documents. (RAG), to identify and incorporate relevant document segments during inference (Saad-Falcon et al., 2024). More recently, agent-based paradigms have gained increasing attention, as they better support complex scenarios that require iterative exploration, tool invocation, and multi-step reasoning over long and structured documents (Sun et al., 2025a; Zhu et al., 2025). Recent advances in large language models (LLMs) (Team, 2025; Liu et al., 2025a) enable such agents to dynamically decompose queries, interact with documents, and adapt to intermediate observations, alleviating the limitations of OCRand RAG-based approaches. Despite their promise, existing DocQA agents are typically implemented as prompt-based pipelines, with limited learning of autonomous agentic behaviors. In contrast, we aim to train the first end-to-end DocQA agent model that is explicitly grounded in information-seeking principles, moving beyond prompt-based agent designs. We first formulate DocQA as an agentic information-seeking problem and design tool-centric agent framework that decomposes document understanding into two complementary capabilities. Specifically, we introduce efficient search tools for global information acquisition and fine-grained read tools for localized comprehension. This design enables the agent to actively explore long documents, iteratively refine its hypotheses, and dynamically adapt its strategy based on intermediate observations. Notably, when instantiated with proprietary LLM, our framework achieves state-of-the-art performance and exceeds reported human-level performance. Furthermore, key bottleneck in training such agent models is the scarcity of high-quality DocQA pairs (Huang et al., 2025), as most publicly available datasets provide only test splits and lack sufficiently annotated training data. To address this challenge, we propose an Exploration-thenSynthesis DocQA generation pipeline that progressively enhances QA pairs from easy to hard. Specifically, we first explore source document through intent-guided, tool-augmented interactions to collect grounded evidence (the Exploration stage), and then synthesizes high-quality document-grounded QA pairs via multi-observation reasoning (the Synthesis stage). We then train our DocQA agent, DocDancer, on the synthesized dataset, instantiating it with two open-source backbones, Qwen3-4BThinking-2507 and Qwen3-30B-A3B-Thinking2507 (Team, 2025). Despite being trained with only 5,000 instances, both variants achieve competitive performance, with the 30B-A3B model attaining state-of-the-art results in several settings. Extensive experiments are conducted on two long-context document understanding benchmarks, MMLongBench-Doc (Ma et al., 2024) and DocBench (Zou et al., 2025). The results demonstrate the effectiveness of the proposed DocDancer. Further analyses provide insights into document parsing strategies, tool design, and the role of synthetic data in agent learning. In summary, our contributions are three-fold: Effective Agentic DocQA Framework: We propose tool-driven DocQA agent framework grounded in information-seeking principles, which achieves SOTA performance when paired with proprietary LLM. Autonomous Data Synthesis Pipeline: We introduce an Exploration-then-Refine data synthesis pipeline that generates high-quality training data for learning agentic behaviors. Empirical Performance: Our method achieves state-of-the-art results and provides practical insights into effective and efficient agentic system design."
        },
        {
            "title": "2 Related Work",
            "content": "Document Question Answering Methods. Traditional DocQA methods rely on OCR-based pipelines (Ding et al., 2022) or end-to-end visionlanguage models (Sukh, 2025; Hu et al., 2025), but both are constrained by limited input length and struggle with long documents (Ma et al., 2024; Zou et al., 2025; Dong et al., 2025a). Retrieval-augmented generation (Zhang et al., 2024; Dong et al., 2025a,b) improves scalability, yet most approaches decouple retrieval and reasoning in single-shot manner, making them brittle to retrieval errors and ineffective for complex, multi-step queries (Zhang et al., 2025). Recent agent-based DocQA systems (Wu et al., 2025c; Sun et al., 2025a; Dong et al., 2025c) address these issues through iterative document navigation and reading, but they predominantly depend on promptengineered, closed-source LLMs. In this work, we aim to train an open-source document agent with learnable behaviors for robust and scalable DocQA. Synthetic Data for Agent Training. High-quality training data is critical for training agents. Due to its scalability, rapid iteration, and inherent trainability, synthetic data offers significant advantages over manually annotated data, serving as highly effective alternative to human-labeled datasets for agent learning (Liu et al., 2025a; Team et al., 2025b). Prior work has demonstrated that large-scale agentsynthesized data can be effectively generated for search agents (Wu et al., 2025a; Li et al., 2025b; Tao et al., 2025), code agents (Yang et al., 2025), GUI agents (Sun et al., 2025b; Guo et al., 2025a) and general-purpose agents (Fang et al., 2025; Prabhakar et al., 2025). In contrast, this work focuses on the DocQA agent setting. Existing DocQA datasets are primarily constructed through semiautomated (Van Landeghem et al., 2023; Dong et al., 2025b) or expert-annotated (Hendrycks et al., 2021; Deng et al., 2025) processes, both of which require substantial human involvement or result in questions that lack sufficient depth. Inspired by advances in search agents, we formulate DocQA as an agentic information-seeking problem, with the goal of synthesizing high-quality training data tailored for DocQA agents."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Agent Setup Framework. We adopt the vanilla ReAct (Yao et al., 2022) as the agents framework, which synergizes reasoning and acting. In this paradigm, the agent generates both reasoning trace (thought), τ , and subsequent action, a, in an interleaved manner. This process forms trajectory, HT , which is sequence of thought-action-observation triplets: HT = (τ0, a0, o0, . . . , τi, ai, oi, . . . , τT , aT ), (1) where aT represents the final answer to the given task. At any given step , the agents policy, π, generates the current thought τt and action at based on the history of all previous interactions, Ht1: τt, at π(Ht1). (2) Inspired by The Bitter Lesson (Sutton, 2019), we employ single-agent setup with carefully selected, highly effective tools, rather than relying on multiagent designs or test-time scaling. Document Processing. Prior works (Sun et al., 2025a) show that an XML-based hierarchical representation for document outlines that organizes parsed content into nested trees, using sections as partitioning units and elements such as text, images, and tables as nodes. While this structure enables efficient positioning and search, it suffers from structural and content inaccuracies and does not incorporate retrieval-aware visual information, which limits its applicability to agent-based processing of long, visually rich documents. To address these issues, we substantially enhance the document outline. For content accuracy, we leverage MinerU2.5 (Niu et al., 2025) for high-precision layout analysis and extraction, defining 17 element types and enriching outline nodes with layout and semantic attributes while removing structurally irrelevant elements such as headers and footers. For structural accuracy, title elements are visually cropped and clustered to infer hierarchical levels, enabling fine-grained section segmentation and reducing information loss in long documents. To improve visual retrieval, we generate captions for images and charts using an multimodal model Mm and incorporate them as auxiliary information, allowing the outline to better align and retrieve visual content. Tool Design. We point out that DocQA can be naturally formulated as an agentic information-seeking task in which the external information source is restricted to the given documents. Accordingly, our tool design aims to enable agents to efficiently and effectively locate and extract relevant information from documents, while keeping the overall toolkit complexity low to ensure ease of use for agent models. Specifically, we design the following two tools for DocDancer: Search. Conducts keyword-based full-text search over the given documents, returning the section IDs, page numbers, and surrounding text snippets for each match. visible window is used to constrain the snippet length for efficient localization. This tool provides the agent with global textual signals for guiding subsequent information access. Read. Given goal and set of section IDs, the tool performs fine-grained reading to extract goal-relevant information from the specified sections. This includes (i) local textual information, consisting of all text within the section; (ii) local visual information, consisting of images and tables within the section, together with page-level screenshot that captures the full layout of the page containing the section. Subsequently, multimodal summarization model Mm is used as an auxiliary reader to jointly integrate textual and visual inputs and return consolidated goal-relevant content. This design deliberately integrates textual and visual signals, capturing both localized evidence and global layout cues, while keeping the toolkit limited to two tools to facilitate efficient utilization."
        },
        {
            "title": "3.2 Data Synthesis",
            "content": "It is crucial to curate complex and diverse Document DocQA pairs that are capable of eliciting multi-step reasoning, goal decomposition, and rich interaction trajectories. To this end, we first construct broad and heterogeneous collection of PDF documents to serve as the grounding corpus for question answering. We then synthesize QA pairs based on these documents, ensuring coverage of diverse reasoning patterns and document structures. Sources. To construct robust and diverse dataset for document-based question answering, we select four representative datasets, LongDocURL (Deng et al., 2025), MMDocRAG (Dong et al., 2025b), CUAD (Hendrycks et al., 2021) and DUDE (Van Landeghem et al., 2023), that cover long-context understanding, multimodal retrieval, 3 Figure 2: Overall of the Exploration-then-Synthesis framework. (i) Exploration stage iteratively interacts with the source document through Action(u)Observation(y)Intent(i) steps. (ii) Synthesis stage aggregates the collected evidence to generate the final question and answer. We present concrete case illustrating the whole generation process in Appendix A. legal expertise, and complex layout analysis. These sources provide the foundational PDF documents used for our automated QA generation pipeline. The distribution of the collected PDF documents is illustrated in Figure 3. Figure 3: Distribution of document used to synthesise. Exploration-then-Synthesis Framework. We propose two-stage framework for DocQA generation, consisting of an Exploration Stage and Synthesis Stage as shown in Figure 2. The overall objective is to transform source document into diverse and high-quality set of grounded QA pairs through iterative interaction and reasoning. Exploration Stage. Given source document D, utilze an LLM Me to iteratively interact with and collect information relevant to potential QA pairs. Conditioned on the interaction history ht and the document D, we employ model Ms jointly generates an intent-action pair (it, at): (it, ut) πMe(i, ht, D), (3) where it denotes the exploration intent and ut corresponds to invoking document-grounded tool such as Search or Read, which is the same as the 4 agents tool action. The construction of question implicitly induces the strategy required to resolve it. The explicit modeling of intent helps prevent uninformative exploration, guiding the agent toward more concrete, goal-directed trajectories (Pahuja et al., 2025). Executing action at yields an observation: yt = (at, D), (4) where denotes the document interaction interface. The interaction history is then updated as: ht+1 = ht {(it, ut, yt)}, (5) and the intent it+1 may be revised based on the newly acquired information. This process is repeated for multiple steps, enabling the agent to progressively refine its understanding of the document and uncover diverse and informative content. The explicit modeling of intent allows for flexible and open-ended exploration, permitting additional interactions when necessary. The output of the exploration stage is trajectory ξ = {(it, ut, yt)}T t=1, (6) which serves as structured evidence for downstream QA generation. In the exploration stage, each exploration step can be viewed as random walk over the knowledge graph implicitly embedded in the entire document. When the number of such walks is sufficiently large, this process can, in principle, reconstruct the underlying document-level knowledge graph in reverse manner. This idea is conceptually aligned with prior work on QA generation based on knowledge graphs in web search agent (Li et al., 2025b,a). We do not explicitly construct document-level knowledge graph in advance, as such an approach would incur substantial engineering complexity and overhead. Instead, our method adopts more lightweight design that is nevertheless capable of generating challenging DocQA pairs, achieving better trade-off between efficiency and effectiveness. Synthesis Stage. Given the exploration trajectory ξ, the agent enters the synthesis stage to generate document-grounded QA pairs. synthesis model Ms performs reasoning over the accumulated observations and generates QA pair: (q, a) Ms(ξ, D), (7) This stage emphasizes (i) reasoning over multiple observations collected during exploration, (ii) grounding both questions and answers in the source document, and (iii) producing semantically coherent and well-formed outputs. The final output is set of K, document-grounded QA pairs: QA = {(qk, ak)}K k=1, (8) which can be used for training an agent. We employ strong open-source model Mt to perform rejection sampling over these QA pairs, QA, thereby obtaining high-quality training trajectories."
        },
        {
            "title": "3.3 Agent Training",
            "content": "Following the empirical findings of (Chen et al., 2023), twe mask loss contributions from observation tokens to mitigate interference from external feedback during training, which has been shown to improve both performance and robustness. Given the task context tc and the complete execution trajectory = (x0, ..., xn1, xn), where each xi {τ, α, o}, the loss is computed as follows: = (cid:80)H i="
        },
        {
            "title": "1\nI[xi ̸= o]",
            "content": "H (cid:88) i=1 I[xi = o] (9) log πθ(xi tc, x<i) Here, I[xi = o] filters out tokens corresponding to external feedback, ensuring the loss is computed only over the agents decision steps."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we aim to answer the following research questions (RQs): RQ1: How effective is the proposed informationseeking agent framework for DocQA? RQ2: How effective is the proposed synthetic data pipeline for training open-source DocQA agents? RQ3: Which components of the agent framework contribute most to performance? RQ4: How does the proposed DocDancer in qualitative evaluations? 4.1 Experimental Setup We fine-tune Qwen3-30B-A3B-Thinking-2507 and Qwen3-4B-Thinking-2507 on our dataset, resulting in DocDancer. Our detailed implementation is provided in Appendix B, trained with only 5,000 agent trajectories. Benchmarks. We evaluate the proposed DocAgent on two multimodal long-context document question answering benchmarks: MMLongBenchDoc (Ma et al., 2024) and DocBench (Zou et al., 2025). MMLongBenchDoc comprises 135 documents with an average length of 47.5 pages, featuring rich layouts and multimodal components across seven diverse domains. The dataset includes 1,091 questions derived from multiple sources, such as text, tables, charts, and images, with 33% involving cross-page reasoning. DocBench consists of 229 real-world documents and 1,082 questions, covering five domains and four major question types. Metrics. For MMLongBench-doc, we follow the official evaluation protocol. Answers are extracted using GPT-4.1 and evaluated with rule-based scoring to compute F1 (F1) and Accuracy (acc). To mitigate extraction errors and improve robustness to diverse response formats, we additionally employ an LLM-as-Judge (LasJ) setting, where gpt-4o assigns binary scores using carefully designed prompts. For DocBench, we likewise adhere to the official evaluation procedure, using the provided instructions to guide GPT-4.1 for assessment. Baselines. We compare our approach with the following three categories of baselines: (1) VLM-based methods: Following the setting of MMLongBench-Doc, PDF pages are scanned at 144 DPI and used as input to the VLM. (2) OCRbased methods: Text is extracted from documents using an OCR tool, and the parsed plain text is provided to LLM for answering. Text beyond the models context length is truncated. (3) RAGbased methods: In this category, we compare existing RAG frameworks for DocQA, including VisRAG (Yu et al., 2024), Colpali (Faysse et al., 2024), M3DocRAG (Cho et al., 2025), MMGR (Wan and Yu, 2025), and RAGAnything (Guo et al., 5 Method VLM Baseline Model MMLongBench-Doc DocBench LasJ acc LasJ F1 Naive VL (Ma et al., 2024) Naive VL (Zhu et al., 2025) GPT-4o Gemini-2.5-Pro 42.8 44.9 58.1 OCR-based Baseline fitz1 Tesseract (Smith, 2007) Tesseract (Smith, 2007) RAG-based Baseline GPT-4 GPT-4o Gemini-2.0-Flash VisRAG (Yu et al., 2024) Colpali (Faysse et al., 2024) M3DocRAG w/ ColPali (Cho et al., 2025) RAGAnything (Guo et al., 2025b) GPT-4o GPT-4o Qwen2-VL-7B GPT-4o-mini Prompt-based Agent Doc-React (Wu et al., 2025c) MDocAgent (Han et al., 2025) MACT (Yu et al., 2025) SimpleDoc (Jain et al., 2025) SimpleDoc (Jain et al., 2025) DocLens (Zhu et al., 2025) DocLens (Zhu et al., 2025) DocAgent (Sun et al., 2025a) DocAgent (Sun et al., 2025a) Ours DocDancer GPT-4o GPT-4o MiMo-VL-7B Claude-4-Sonnet Gemini-2.5-Pro Claude-4-Sonnet Gemini-2.5-Pro GPT-4o Claude-3.5-Sonnet GPT-4o Gemini-2.5-Pro GPT-5.2 Qwen3-4B (ft) Qwen3-30B-A3B (ft) Human Baseline 30.1 39. 29.0 32.2 31.4 42.8 38.1 42.0 47.4 51.8 57.3 52.3 56.3 57.0 48.4 54.4 65.8 30.5 37.2 27.8 30.8 36.5 38.3 49.1 54.1 50.8 55.3 56.8 49.2 53.9 66.0 58.6 56.6 63.3 67.6 59.2 65.9 67.6 59.4 65.3 63.1 67.9 63.4 79.9 73.5 79.9 85.5 79.8 81.2 81.2 Table 1: Performance comparison across two long-context understanding benchmarks. The best results among all methods are bolded and the second-best results are underlined. 2025b). (4) Agent-based methods: We include several recent and well-performing training-free agentic frameworks, namely Doc-React (Wu et al., 2025c), MDocAgent (Han et al., 2025), MACT (Yu et al., 2025), SimpleDoc (Jain et al., 2025), DocLens (Zhu et al., 2025), and DocAgent (Sun et al., 2025a). The detailed introduction of the baseline is provided in Appendix C."
        },
        {
            "title": "4.2 Overall Performance (RQ1)",
            "content": "We evaluate our agent framework against OCRbased, RAG-based, and prompt-based baselines on long-document DocQA benchmarks. Based on the experimental results in Table 1, we draw the following observations. First, agent-based approaches substantially outperform VLM-based methods, OCR-based baselines, and RAG-based baselines across evaluated benchmarks, highlighting the advantage of explicit tool use and iterative reasoning for long-context document understanding. Second, under the same backbone, our single-agent framework matches or surpasses multiagent systems. In particular, on MMLongBenchDoc, DocDancer with GPT-5.2 attains 56.8 F1 / 67.6 LasJ, outperforming all prior methods, and on DocBench, it reaches 85.5, exceeding the human baseline by 4 points. Third, models trained on our synthetic DocQA dataset demonstrate strong generalization and data efficiency. Even with relatively small model sizes, such as 30B-A3B and 4B, the resulting agents achieve performance competitive with closed-source models. These results indicate that training agentic capabilities on smaller-scale models is both feasible and highly valuable, substantially lowering the barrier to building effective document-understanding agents."
        },
        {
            "title": "4.3 Effectiveness of Synthetic Data (RQ2)",
            "content": "Overall Performance. We investigate whether the Exploration-then-Synthesis data generation 6 Figure 4: Ablation study on document parsing and tools. Figure 5: Performance comparison between models trained on our synthesized QA data and open-source QA data. pipeline provides effective supervision for learning agentic behaviors, and whether models trained solely on the synthesized data achieve strong performance compared to existing open-source QA pairs. In Figure 5, we use the same PDF sources (Section 3.2) and construct two training sets of equal size (5,000 instances): one from our synthesized QA data and the other from human-annotated QA data provided with the PDFs (OS-QA). Both models are trained on Qwen3-30B-A3B-Thinking-2507. Overall, DocDancer consistently outperforms OS-QA across all metrics and benchmarks, demonstrating the effectiveness of our data synthesis strategy. Detailed Results on Domains. Figure 6 reports domain-level results on MMLongBench-Doc. DocDancer consistently outperforms the QA baseline across all document domains, including Academic, Financial, Industry, and Report. The gains are more pronounced in structurally complex domains that require iterative information seeking and finegrained reasoning. Overall, the results indicate that DocDancer generalizes well across diverse document types and is robust to domain variation. Figure 6: Detailed domain-wise performance comparison on MMLongBench-Doc between DocDancer and the model trained on OS-QA. 4.4 Influence of Agentic Tools (RQ3) We conduct ablation studies on document processing for outline construction and tool usage in Figure 4. The baseline is the Actor Agent from DocAgent (Sun et al., 2025a). For outline construction, DocAgent relies on Adobe PDF Extract as well as DocXChain (Yao, 2023) and PyMuPDF. In contrast, our enhanced method employs MinerU2.5 (Niu et al., 2025) for outline generation. The results demonstrate that, when combined with the same tools, our processing approach consistently outperforms the baseline, confirming that MinerU2.5 produces higher-quality document outlines. Regarding tool usage, DocAgent utilizes five tools: search, get_section_content, get_image, get_page_images, and get_table_image. In comparison, we only use two tools, Search and Read, following the principle of simplicity. Despite this reduced tool set, our approach achieves better performance when combined with either our own outline or the outline generated by DocAgent. The best results are obtained by combining our outline construction with our tool design, demonstrating their complementary effects. Furthermore, we conduct an ablation study on the external model used 7 Figure 7: case study demonstrating that our proposed DocDancer successfully performs multi-round information gathering to reach the correct answer, as illustrated in Table 3 in detail, whereas OS-QA produces an incorrect result. line model, which is trained on OS-QA relies on keyword-based retrieval and retrieves passages related to marketing and revenues. Due to insufficient grounding, it incorrectly uses marketing expense figure as proxy for advertising expense, yielding an erroneous ratio of 0.122. This failure illustrates the limitation of single-pass retrieval and shallow aggregation when fine-grained financial concepts are required. In contrast, DocDancer performs multi-round, question-driven inIt first retrieves and reads formation gathering. the section explicitly reporting advertising expense for FY 2015 ($714.3M), and then independently extracts the total revenue from separate tabular section ($6,779.5M). By grounding each value to its corresponding evidence and verifying semantic relevance, the system computes the correct ratio of 714.3/6,779.5 0.105. It demonstrates that accurate document-level financial question answering benefits from our synthetic data, which enables the construction of domain-specific expert-level supervision beyond ordinary human annotations."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose DocDancer, an end-to-end trained agentic model for document question answering that formulates DocQA as an information-seeking process. By introducing tool-centric framework with complementary search and read operations, DocDancer enables effective exploration and comprehension of long, structured documents. To mitigate the lack of high-quality supervision, we further Figure 8: Results on DocBench across various domains using different models used by Read tool. We report the generalized accuracy of five types of document domains, including Academia (Aca.), Finance (Fin.), Government (Gov), Law, and News. by the Read tool. Our default configuration, Mm employs Qwen3-VL-235B-A22B-Instruct. Replacing it with Gemini-3-Pro yields modest overall improvement of 0.2 accuracy points on DocBench (Figure 8), with gains in Government, Law, and News domains. These results indicate that our tool design is robust and does not depend on an exceptionally strong external model."
        },
        {
            "title": "4.5 Qualitative Analysis (RQ4)",
            "content": "We present case study of financial task on 73-page document from MMLongBench-Doc, as illustrated in Figure 7. Answering this question requires locating advertising expense and revenue figures from different sections of the document and performing numerical computation. The base8 design an Exploration-then-Synthesis data pipeline that generates compact yet effective training data for learning agentic behaviors. Experiments on MMLongBench-Doc and DocBench demonstrate that DocDancer achieves strong and competitive performance, validating the effectiveness of agentic information-seeking for document understanding."
        },
        {
            "title": "Limitations",
            "content": "This work still has several limitations. First, our experiments are conducted only on Qwen3-30B-A3BThinking-2507 and Qwen3-4B-Thinking-2507; we do not evaluate the proposed method on largerscale models or models from other families. Second, we focus exclusively on supervised finetuning (SFT) and do not explore agentic reinforcement learning (RL). Third, we do not further scale the training data, and thus do not investigate how the proposed method performs under larger or more diverse data."
        },
        {
            "title": "Ethical Considerations",
            "content": "This work studies agentic document-grounded question answering using publicly available benchmarks and documents released for research purposes. The proposed Exploration-then-Synthesis pipeline generates synthetic questionanswer pairs that are explicitly grounded in source documents and does not introduce new proprietary data or attempt to reproduce large portions of copyrighted text verbatim. While the method itself does not collect personal information, document-grounded agents may be applied to sensitive or private documents in downstream use; such applications require appropriate authorization and privacy safeguards. The synthesized data and trained models may inherit biases present in the underlying document sources, including domain and content imbalances. Finally, although improved document exploration capabilities could be misused if deployed irresponsibly, the strong grounding in retrieved evidence and our commitment to releasing code and data aim to support transparency, reproducibility, and responsible research use."
        },
        {
            "title": "References",
            "content": "Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, and Mohit Bansal. 2025. M3docvqa: Multi-modal multi-page multi-document understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 61786188. Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, ZhongZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, and 1 others. 2025. Longdocurl: comprehensive multimodal long document benchmark integrating understanding, reasoning, and locating. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11351159. Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru Chen, Yuzhong Ma, Hyunsuk Chung, and Soyeon Caren Han. 2022. V-doc: Visual quesIn Proceedings of tions answers with documents. the IEEE/CVF conference on computer vision and pattern recognition, pages 2149221498. Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, and Yong Liu. 2025a. Mmdocir: Benchmarking multi-modal retrieval for long documents. arXiv preprint arXiv:2501.08828. Kuicai Dong, Yujing Chang, Shijie Huang, Yasheng Wang, Ruiming Tang, and Yong Liu. 2025b. Benchmarking retrieval-augmented multimomal generation for document question answering. arXiv preprint arXiv:2505.16470. Kuicai Dong, Shurui Huang, Fangda Ye, Wei Han, Zhi Zhang, Dexun Li, Wenjun Li, Qu Yang, Gang Wang, Yichao Wang, and 1 others. 2025c. Docresearcher: unified system for multimodal document parsing and deep research. arXiv preprint arXiv:2510.21603. Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, and 1 others. 2025. Towards general agentic intelligence via environment scaling. arXiv preprint arXiv:2509.13311. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449. Xiangwu Guo, Difei Gao, and Mike Zheng Shou. 2025a. Auto-explorer: Automated data collection for gui agent. arXiv preprint arXiv:2511.06417. Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, and Chao Huang. 2025b. Rag-anything: All-in-one rag framework. arXiv preprint arXiv:2510.12323. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915. Siwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li, Hongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: multi-modal multi-agent framework for document understanding. arXiv preprint arXiv:2503.13964. 9 D. Hendrycks, C. Burns, A. Chen, and S. Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review. arXiv preprint arXiv:2103.06268. Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2025. mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 58175834. Tiancheng Huang, Ruisheng Cao, Yuxin Zhang, Zhangyi Kang, Zijian Wang, Chenrun Wang, Yijie Luo, Hang Zheng, Lirong Qian, Lu Chen, and 1 others. 2025. Airqa: comprehensive qa dataset for ai research with instance-level evaluation. arXiv preprint arXiv:2509.16952. Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, Zhenwen Shao, Qingyun Wu, Huazheng Wang, and 1 others. 2025. Simpledoc: Multi-modal document understanding with dual-cue page retrieval and iterative refinement. arXiv preprint arXiv:2506.14035. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Yida Zhao, Liwen Zhang, Litu Ou, Dingchu Zhang, Xixi Wu, Jialong Wu, and 1 others. 2025a. Websailorv2: Bridging the chasm to proprietary agents via synthetic data and scalable reinforcement learning. arXiv preprint arXiv:2509.13305. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, and 1 others. 2025b. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, and 1 others. 2025b. comprehensive survey on arXiv preprint long context language modeling. arXiv:2503.17407. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, and 1 others. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. Advances in Neural Information Processing Systems, 37:9596396010. Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, and 1 others. 2025. Mineru2. 5: decoupled vision-language model for efficient high-resolution document parsing. arXiv preprint arXiv:2509.22186. Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, Explorer: Scaling and Ahmed Hassan. 2025. exploration-driven web trajectory synthesis for multimodal web agents. In Findings of the Association for Computational Linguistics: ACL 2025, pages 6300 6323. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, and 1 others. 2025. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agenthuman interplay. arXiv preprint arXiv:2504.03601. Jon Saad-Falcon, Joe Barrow, Alexa Siu, Ani Nenkova, Seunghyun Yoon, Ryan A. Rossi, and Franck Dernoncourt. 2024. PDFTriage: Question answering over long, structured documents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 153 169, Miami, Florida, US. Association for Computational Linguistics. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053. Ray Smith. 2007. An overview of the tesseract ocr engine. In Ninth international conference on document analysis and recognition (ICDAR 2007), volume 2, pages 629633. IEEE. Andriy Sukh. 2025. Ocr-free document understanding using vision-language models. Li Sun, Liu He, Shuyue Jia, Yangfan He, and Chenyu You. 2025a. DocAgent: An agentic framework for multi-modal long-context document understanding. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1771217727, Suzhou, China. Association for Computational Linguistics. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, and 1 others. 2025b. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 55555579. Richard Sutton. 2019. The bitter lesson. Incomplete Ideas (blog), 13(1):38. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, and 1 others. 2025. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061. Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou. 2020. Layoutlm: Pretraining of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 11921200. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, and 1 others. arXiv preprint 2025a. Kimi-vl technical report. arXiv:2504.07491. John Yang, Kilian Lieret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. 2025. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, and 1 others. 2025b. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Dominika Tkaczyk, Paweł Szostek, Mateusz Fedoryszak, Piotr Jan Dendek, and Łukasz Bolikowski. 2015. Cermine: automatic extraction of structured metadata from scientific literature. International Journal on Document Analysis and Recognition (IJDAR), 18(4):317335. Jordy Van Landeghem, Rubén Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, and 1 others. 2023. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1952819540. Xueyao Wan and Hang Yu. 2025. Mmgraphrag: Bridging vision and language with interpretable multimodal knowledge graphs. arXiv preprint arXiv:2507.20804. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, and 1 others. 2025a. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. 2025b. WebWalker: Benchmarking LLMs in web traversal. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1029010305, Vienna, Austria. Association for Computational Linguistics. Junda Wu, Yu Xia, Tong Yu, Xiang Chen, Sai Sree Harsha, Akash Maharaj, Ruiyi Zhang, Victor Bursztyn, Sungchul Kim, Ryan Rossi, and 1 others. 2025c. Doc-react: Multi-page heterogeneous document question-answering. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6778. Cong Yao. 2023. Docxchain: powerful open-source toolchain for document parsing and beyond. arXiv preprint arXiv:2310.12430. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and 1 others. 2024. Visrag: Vision-based retrieval-augmented generation on multi-modality documents. arXiv preprint arXiv:2410.10594. Xinlei Yu, Chengming Xu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Cheng Yang, Jiangning Zhang, Shuicheng Yan, and Xiaobin Hu. 2025. Visual document understanding and reasoning: multi-agent collaboration framework with agent-wise adaptive test-time scaling. arXiv preprint arXiv:2508.03404. Jinxu Zhang, Yongqi Yu, and Yu Zhang. 2024. Cream: coarse-to-fine retrieval and multi-modal efficient tuning for document vqa. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 925934. Junyuan Zhang, Qintong Zhang, Bin Wang, Linke Ouyang, Zichen Wen, Ying Li, Ka-Ho Chow, Conghui He, and Wentao Zhang. 2025. Ocr hinders rag: Evaluating the cascading impact of ocr on retrievalIn Proceedings of the augmented generation. IEEE/CVF International Conference on Computer Vision, pages 1744317453. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. 2024. Swift:a scalable lightweight infrastructure for fine-tuning. Preprint, arXiv:2408.05517. Dawei Zhu, Rui Meng, Jiefeng Chen, Sujian Li, Tomas Pfister, and Jinsung Yoon. 2025. Doclens: tool-augmented multi-agent framework for long visual document understanding. arXiv preprint arXiv:2511.11552. Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. 2025. Docbench: benchmark for 11 evaluating llm-based document reading systems. In Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing, pages 359373."
        },
        {
            "title": "A Case Study of Synthetic Data",
            "content": "Figure 9 demonstrates how the Exploration-thenSynthesis framework iteratively navigates 73page document, aggregating heterogeneous evidence, text (in Sec. 2.43), charts (in Figure 1), and tables (in Table 1), scattered across disjoint pages (pp. 40, 41, 49) to synthesize high-quality question that requires complex reasoning. In the Exploration Stage, the agent generates exploartion trajectory ξ via iterative (it, ut) steps, effectively performing random walk over the documents implicit knowledge graph. It aggregates heterogeneous evidence by bridging disjoint pageslinking visual trends in chart (p. 40) with precise values in text (p. 49) and table (p. 41). In the Synthesis Stage, the model Ms reasons over this accumulated trajectory to construct complex multi-hop numerical question (Wu et al., 2025b). The final QA pair requires arithmetic calculation (29.92% 15% = 14.92%) rather than simple retrieval, ensuring deep document grounding and preventing shortcut learning."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Details on Prompts The prompts for the DocDancer are shown in Figure 10. B.2 Tool Schema This section details the tool schemas provided to the agent. We designed two primary tools: search for keyword-based retrieval and read for extracting content from specific document sections. The specific JSON structures defining these functions are shown in Figure 11. B.3 Training Details We fine-tune Qwen3-30B-A3B-Think2 and Qwen3-4B-Think3 the Megatron-LM using framework (Zhao et al., 2024; Shoeybi et al., 2019). Both models are trained with context length of 128k to support long-document processing tasks. We employ the AdamW optimizer with precisionaware configuration and cosine decay learning rate scheduler, featuring peak learning rate of 1.0 105, minimum of 1.0 106, and 5% warmup phase. The global batch size is configured 2https://huggingface.co/Qwen/Qwen3-30B-A3BThinking-2507 3https://huggingface.co/Qwen/Qwen3-4B-Thinking2507 to 16 for the Qwen3-30B-A3B-Think and to 40 for Qwen3-4B-Think. For Qwen3-30B-A3B-Think, we apply an auxiliary loss coefficient of 103 to ensure balanced expert routing. We train both models for 10 epochs and selected the checkpoint with best performance. B.4 Inference Details vLLM framework (Kwon et al., 2023) is used for inference; we employ temperature of 0.6, topp value of 0.95, and presence penalty of 1.1. B.5 Hyperparameter By default, Mm is Qwen3-VL-235B-A22BInstruct, and we analyze the effects of replacing it in Section 4.4. For Mt, we use the open-source and relatively strong model gpt-oss-120b to perform rejection sampling. Further analysis is provided in Table 2. First, our method substantially outperforms the base model without fine-tuning, demonstrating the effectiveness of the proposed training strategy. Second, our approach also surpasses the model trained with reject sampling, validating the quality of the synthesized questionanswer data and showing that it can effectively elicit and enhance the models performance. For Ms, we employ gpt-oss-120b in Exploration-then-Synthesis framework to synthesis data. B.6 Details on Prompts for Data Synthesis The prompts utilized for Exploration and Synthetic within the Exploration-then-Refine framework are presented in Figure 12 and Figure 13, respectively. Regarding the exploration configuration, we adjust the maximum exploration depth based on the complexity of the document sources. Specifically, we set the maximum sampling depth to 20 for LongDocURL and MMdocRAG, while for DUDE and CUAD, this limit is set to 15."
        },
        {
            "title": "C Baselines",
            "content": "We compare DocDancer against comprehensive set of baselines categorized into four groups: Naive VLM Baselines. These methods evaluate the native long-context understanding capabilities of advanced VLMs. We directly feed PDF pages converted to images (144 DPI) into the models without external parsing or retrieval. Following the settings in MMLongBench-Doc (Ma et al., 2024), 13 Figure 9: case study of the Exploration-then-Synthesis framework generating multi-hop, cross-document, cross-modal numerical reasoning QA pair. Method Model MMLongBench-Doc DocBench LasJ acc LasJ F1 DocDancer DocDancer DocDancer Qwen3-A3B-30B-Thinking GPT-oss-120B Qwen3-30B-A3B-Thinking (ft) 39.2 52.3 54.4 36.4 53.0 53. 46.9 59.8 65.3 74.1 80.8 81.2 Table 2: Performance comparison across two long-context understanding benchmarks."
        },
        {
            "title": "Prompt",
            "content": "research assistant You are an expert tasked with answering questions based on document content. You will be provided with an XML outline of the document. If you need more comprehensive, detailed, or accurate information from the document to fully address the users query, you need to use the provided tool. Ive uploaded document, and below is the outline in XML format: {document_outline}. Answer the following question based on the content of the document: {question}. we report GPT-4o 4 and Gemini-2.5-Pro 5. OCR-based Baselines. These baselines treat the task as text-only QA by first extracting content using OCR engines. We pair Tesseract (Smith, 2007) and PyMuPDF (fitz) 6 with LLMs including GPT-4, GPT-4o, and Gemini-2.0-Flash. RAG-based Baselines. We consider both visual and hybrid retrieval strategies: Visual Retrieval: VisRAG (Yu et al., 2024) and ColPali (Faysse et al., 2024) retrieve relevant page or patch-level visual evidence based on vision-centric embeddings, utilizing GPT4o for response generation. Hybrid Retrieval: M3DocRAG (Cho et al., 2025) performs joint retrieval using mul4https://platform.openai.com/docs/models/ gpt-4o 5https://ai.google.dev/gemini-api/docs/models? Figure 10: System prompt for DocDancer. #gemini-2.5-pro 6https://pymupdf.readthedocs.io/ timodal retriever with Qwen2-VL-7B. RAGAnything (Guo et al., 2025b) structures multimodal content as knowledge entities for cross-modal retrieval, using GPT-4o-mini as the backbone. Prompt-based Agentic Baselines. We include stateof-the-art agent frameworks designed for document understanding: Doc-React (Wu et al., 2025c) employs an iterative decision-making process to balance information gain and uncertainty reduction (GPT-4o). MDocAgent (Han et al., 2025) utilizes multi-agent system with five specialized roles for context retrieval (GPT-4o). MACT (Yu et al., 2025) introduces multiagent collaboration framework featuring adaptive test-time scaling (MiMo-VL-7B (Team et al., 2025a)). SimpleDoc (Jain et al., 2025) retrieves pages via ColQwen2.5, followed by LLM-based evidence selection (Claude-4-Sonnet, Gemini2.5-Pro). DocLens (Zhu et al., 2025) operates as tool-augmented multi-agent framework for focused reading (Claude-4-Sonnet, Gemini-2.5Pro). DocAgent (Sun et al., 2025a) leverages treestructured document outline combined with retrieval tools (GPT-4o, Claude-3.5-Sonnet). 15 Tool Schemas Search { \"type\": \"function\", \"function\": { \"name\": \"search\", \"description\": \"Find and extract all paragraphs and sections where any of the provided search terms appear\", \"parameters\": { \"type\": \"object\", \"properties\": { \"keywords\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"description\": \"A list of query keywords for searching\" } }, \"required\": [\"keywords\"] } } }"
        },
        {
            "title": "Read",
            "content": "{ \"type\": \"function\", \"function\": { \"name\": \"read\", \"description\": \"Read multiple sections by section IDs and extract useful information from all content contained in those sections, including both visual elements and textual elements.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"section_ids\": { \"type\": \"array\", \"items\": { \"type\": \"string\" }, \"description\": \"A list of section IDs to read from the document\" }, \"goal\": { \"type\": \"string\", \"description\": \"The user goal that guides what useful information should be extracted from the selected sections\" } }, \"required\": [\"section_ids\", \"goal\"] } } } Figure 11: Tool schema: Search and Read. 16 Exploration in Exploration-then-Refine Framework. You are exploring parsed PDF paper/report (outline + paragraphs + images + table snapshots + per-page screenshots). Your objective is to collect HIGH-QUALITY, GROUNDED evidence bundles that can later support HARD, multi-hop, visually grounded document Q&A synthesis. Final QA Constraints You Must Enable (every eventual QA must satisfy ALL): Multi-page: Combining evidence from at least THREE different pages/sections, where the pieces of evidence are related. Multi-element: Contains at least two evidence source types (text paragraphs/charts/graphics/table screenshots and/or full-page layouts). Multi-hop: require at least TWO reasoning points (e.g. cross-reference + computation, footnote rule + chart reading, layout count + comparison, multiple related searches + readings). Important: final questions should NOT rely on explicit document locations. Do NOT plan to use page numbers, section titles/IDs, or explicit figure/table numbers (e.g., Figure <number>, Table <number>) in the question. Instead, you must collect CONTENT-BASED CLUES that can uniquely identify the needed evidence: Caption keywords (short quote fragments), axis labels and units, legend item names, panel labels (a)/(b), distinctive row/column headers, and footnote phrases (restated, excluding, unaudited, unit changes). Exploration strategy using only search and read: Use search to find visuals, tables, footnotes, and their nearby discussion text. Start with keywords like: Figure, Fig., Chart, Image, Graph, legend, axis, panel, Table, Note, footnote, restated, excluding, unaudited. For each promising hit, immediately read the covering section(s) with goal that extracts: The text content of the section in question. Caption text, axis labels/units, legend items, and visual markers. The exact table header path, target cell(s), and footnote rules. The narrative claim/explanation that references the visual. Use the read function as much as possible, deliberately chain across pages. For conditional layout questions: identify page by unique visual cue, then use read to count visible tables/figures. Avoid: Broad whole-document counts unless you turn them into comparative, multi-hop questions. Word-frequency counting. Repeating identical tool calls. Statistical analysis of the number of elements. Every action during sampling should contribute to forming future HARD, multi-page, multi-element, multi-hop document QA. Figure 12: Prompt for exploration stage in Exploration-then-Refine framework. Synthesis in Exploration-then-Refine Framework. You must synthesize document Q&A training data based ONLY on the trajectory. Hard Requirements (Strict): The output must be JSON object containing only two fields: question and answer (no additional fields are allowed), and must be in English only. The question must be natural and unambiguous, containing only one question and corresponding to single, unique answer. The question must not be common-knowledge question; it must be impossible to answer based on the question alone and must be highly dependent on the document. Do not mention tools, sections, pages, section IDs, searching/reading actions, trajectories, or observations. The answer length should be limited to single sentence, ideally short phrase, entity, number, or list, and avoid simply using yes/no answers. The answer must be directly supported by evidence from the provided text and cannot be guessed randomly. Mandatory Difficulty Constraints (every QA pair must satisfy all of the following): 1. Multi-page: The question requires evidence from at least two different pages/sections to answer, and the evidence must be logically related. 2. Multiple Evidence Modalities: The question must involve at least two types of evidence, such as text, charts, figures, tables, screenshots, and/or full-page layout cues, with preference for covering visual elements. 3. Multi-step Reasoning: The question must require at least two reasoning steps (e.g., calculation + cross-validation, footnote rule application + chart reading, layout counting + comparison). No Explicit Location References in the Question: Do not mention page numbers, section IDs, titles/IDs, or explicit figure/table numbers (e.g., Figure <number>, Table <number>). Instead, provide 13 content-based clues to help locate the evidence, such as: short title phrases, axis labels/units, legend item names, unique row names, footnote keywords, or distinctive layout hints (e.g., the only multi-panel figure labeled (a) and (b)). When describing visual elements, do not directly copy long unique numbers or OCR-extracted long text strings from images (e.g., an image showing the number 7,584,322,338). Use specific entity names or semantic descriptions instead (e.g., Apples 2018 total sales table, an image showing adjusted outstanding balances, or the largest segment in the pie chart). Preferred Question Templates (all templates must be cross-page + visual + multi-step): Cross-page conditional layout: Identify pages via unique visual cues and compare the number of visible objects across pages. Textual claim + chart verification: narrative statement about change/target that is verified using chart and light calculation. Table + chart consistency: Compute ratio/difference from table and verify it against data point in chart on another page. Footnote-constrained table + chart mapping: Apply footnote/restatement/exclusion rules, then map the correct year/value to chart on another page. Table/Chart comprehension questions: Locate tables and charts via text, then derive conclusions from table structure or chart visuals. Unanswerable questions: Questions that seem reasonable but are actually impossible to answer (e.g., questions about terms/entities that do not exist in the document). For these, the answer must be Unanswerable. Counting questions: Count the occurrences of key local terms or entities in the document. Such questions should only be generated when there is sufficient and conclusive evidence. Fallback Rule: If the current trajectory cannot support question that satisfies all constraints, choose different question. After generating question, perform second-pass check and regenerate if the question falls into any of the following categories: Contains more than one question. Includes non-English languages or characters. Questions that can be answered based on an independent page/section. Common-sense questions unrelated to the document. Counting tasks spanning the entire document with broad scope. Counting tasks involving Charts/Figures/Images/Tables. For unanswerable questions, confirm that they are truly unanswerable. For counting questions, confirm completeness and answer accuracy. Do not guess or fabricate answers under any circumstances. Figure 13: Prompts for Q&A Synthesis stage in Exploration-then-Refine framework. 18 Details of Case Study Question What is advertising expense to sales ratio of Netflix in FY 2015? Round your answer to three decimal places. Answer 0. <think> Step 1 We need to answer: what is advertising expense to sales ratio of Netflix in FY 2015? Round your answer to three decimal places. We need to locate advertising expense and sales (revenues) for FY 2015. From outline we have various sections. First, find advertising expense. The term advertising appears in sections. Search for advertising. </think> <tool_call> { \"arguments\": \"{ \"keywords\": [\"advertising\"] }\", \"name\": \"search\" } </tool_call> <tool_response> Document search for advertising found 6 results: <Item type=\"Paragraph\" section_id=\"3.19\" page_num=\"5.0\">... adjust pricing or service offerings ... </Item> ... <Item type=\"Paragraph\" section_id=\"8.81\" page_num=\"47.0\"> Marketing expenses consist primarily of advertising expenses... Advertising expenses were $714.3 million, $533.1 million and $404.0 million for the years ended December 31, 2015, 2014 and 2013, respectively. </Item> </tool_response> <think> Step 2 One of the results is in section_id 8.81 paragraph page_num 47.0. It says: Advertising expenses were $714.3 million, $533.1 million and $404.0 million for the years ended December 31, 2015, 2014 and 2013, respectively. Thus advertising expense FY 2015 = $714.3 million. Now sales (revenues) for FY 2015: In consolidated statements of operations (section 8.60) we have revenues $6,779,511 (in thousands). Thats $6,779,511 thousand = $6,779.511 million. Alternatively, there may be line item Revenues in Table 5 etc. Lets locate Revenues. </think> <tool_call> { \"arguments\": \"{ \"keywords\": [\"Revenues\"] }\", \"name\": \"search\" } </tool_call> <tool_response> Document search for Revenues found 39 results: <Item type=\"Paragraph\" section_id=\"3.9\" page_num=\"3.0\"> ... <Item type=\"HTML_Table\" table_id=\"5\" section_id=\"8.20\" page_num=\"19.0\"> ... Revenues (2015): $ 6,779,511 ... </Item> ... </tool_response> Continued on next page... 19 Details of Case Study (Continued) <think> Continued from previous page Step 3 We have advertising expense: $714.3 million (2015). We need to compute ratio = advertising expense / revenue. Revenue for 2015: $6,779,511 thousand = $6,779.511 million. Thus ratio = 714.3/6, 779.511 =? ... First, we should cite sources: advertising expense paragraph (8.81) and revenue table. Lets extract the advertising expense paragraph text to be sure. </think> <tool_call> { \"arguments\": \"{ \"section_ids\": [\"8.81\"], \"goal\": \"Extract the advertising expense amount for 2015\" }\", \"name\": \"read\" } </tool_call> <tool_response> The useful information from the document section (section_id=8.81) for user goal Extract the advertising expense amount for 2015 is as follows: Evidence in document: Marketing expenses consist primarily of advertising expenses... Advertising expenses were $714.3 million, $533.1 million and $404.0 million for the years ended December 31, 2015, 2014 and 2013, respectively. Summary: The advertising expense amount for 2015 is $714.3 million... </tool_response> <think> Step 4 have successfully extracted and verified the advertising expense ($714.3 million) from section 8.81. Now, need to formally verify the total revenue for 2015 to serve as the denominator. The search results in Step 2 indicated that section 8.60 contains the Consolidated Statements of Operations, which is the authoritative source for revenue figures. will read section 8.60 to extract the exact revenue amount for the year ended December 31, 2015. </think> <tool_call> { \"arguments\": \"{ \"section_ids\": [\"8.60\"], \"goal\": \"Extract the revenue amount for 2015\" }\", \"name\": \"read\" } </tool_call> <tool_response> The useful information from the document section (section_id=8.60) for user goal Extract the revenue amount for 2015 is as follows: Evidence in document: Heading: NETFLIX, INC. Paragraph: CONSOLIDATED STATEMENTS OF OPERATIONS <HTML_Table> <table> ... <tr> <td>Revenues</td> <td>$ 6,779,511</td> <td>$ 5,504,656</td> ... </tr> ... </table> </HTML_Table> Caption: (in thousands, except per share data) Summary: Continued on next page... 20 Details of Case Study (Continued) Continued from previous page The revenue amount for 2015 is $6,779,511, as explicitly stated in the Revenues row of the CONSOLIDATED STATEMENTS OF OPERATIONS table for the year ended December 31, 2015. This value is presented in thousands, as indicated by the caption. </tool_response> Final Response Netflixs advertising expense for fiscal 2015 was $714.3 million [8.81], and its total revenue for the same year was $6,779,511 thousand (i.e., $6,779.511 million) [8.60]. Advertising-to-sales ratio = 714.3 Rounded to three decimal places, the ratio is 0.105 (or 10.5%). 6,779.511 0.10536. Table 3: Case study of financial task."
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai AI Lab",
        "Tencent AI Lab"
    ]
}