{
    "paper_title": "DDT: Decoupled Diffusion Transformer",
    "authors": [
        "Shuai Wang",
        "Zhi Tian",
        "Weilin Huang",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion \\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet $256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous diffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies."
        },
        {
            "title": "Start",
            "content": "DDT: Decoupled Diffusion Transformer Shuai Wang1 Zhi Tian2 1Nanjing University https://github.com/MCG-NJU/DDT Weilin Huang2 2ByteDance Seed Vision Limin Wang 1, # 5 2 0 2 9 ] . [ 2 1 4 7 5 0 . 4 0 5 2 : r Figure 1. Our deoupled diffusion transformer (DDT-XL/2) achieves SoTA 1.31 FID under 256 epochs. Our decoupled diffusion transformer models incorporate condition encoder to extract semantic self-conditions and velocity decoder to decode velocity."
        },
        {
            "title": "Abstract",
            "content": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations In each denoising step, and numerous inference steps. diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding lowfrequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose new Decoupled Diffusion Transformer (DDT), with decoupled design of dedicated condition encoder for semantic extraction alongside specialized velocity decoder. Our experiments reveal that more substantial encoder yields performance improvements as model size increases. For ImageNet 256 256, Our DDT-XL/2 achieves new state-of-the-art performance of 1.31 FID (nearly 4 faster training convergence compared to previous diffusion transformers). For ImageNet 512 512, Our DDTXL/2 achieves new state-of-the-art FID of 1.28. Additionally, as beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing selfcondition between adjacent denoising steps. To minimize performance degradation, we propose novel statistical dynamic programming approach to identify optimal sharing strategies. 1. Introduction Image generation is fundamental task in computer vision research, which aims at capturing the inherent data distribution of original image datasets and generating high-quality synthetic images through distribution sampling. Diffusion models [19, 21, 29, 30, 41] have recently emerged as highly promising solutions to learn the underlying data distribution in image generation, outperforming the GAN-based models [3, 40] and Auto-Regressive models [5, 43, 51]. The diffusion forward process gradually adds Gaussian noise to the pristine data following an SDE forward schedule [19, 21, 41]. The denoising process learns the score estimation from this corruption process. Once the score function is accurately learned, data samples can be synthesized by numerically solving the reverse SDE [21, 29, 30, 41]. # : Corresponding author (lmwang@nju.edu.cn). 1 Diffusion Transformers [32, 36] introduce the transformer architecture into diffusion models to replace the traditionally dominant UNet-based model [2, 10]. Empirical evidence suggests that, given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections [36]. Nevertheless, their slow convergence rate still poses great challenge for developing new models due to the high cost. In this paper, we want to tackle the aforementioned major disadvantages from model design perspective. Classic computer vision algorithms [4, 17, 23] strategically employ encoder-decoder architectures, prioritizing large encoders for rich feature extraction and lightweight decoders for efficient inference, while contemporary diffusion models predominantly rely on conventional decoder-only structures. We systematically investigate the underexplored potential of decoupled encoder-decoder designs in diffusion transformers, by answering the question of can decoupled encoder-decoder transformer unlock the capability of accelerated convergence and enhanced sample quality? Through investigation experiments, we conclude that the plain diffusion transformer has an optimization dilemma between abstract structure information extraction and detailed appearance information recovery. Further, the diffusion transformer is limited in extracting semantic representation due to the raw pixel supervision [28, 52, 53]. To address this issue, we propose new architecture to explicitly decouple low-frequency semantic encoding and high-frequency detailed decoding through customized encoder-decoder design. We call this encoder-decoder diffusion transformer model as DDT (Decoupled Diffusion Transformer). DDT incorporates condition encoder to extract semantic selfcondition features. The extracted self-condition is fed into velocity decoder along with the noisy latent to regress the velocity field. To maintain the local consistency of selfcondition features of adjacent steps, we employ direct supervision of representation alignment and indirect supervision from the velocity regression loss of the decoder. In the ImageNet256 256 dataset, using the traditional off-shelf VAE [38], our decoupled diffusion transformer (DDT-XL/2) model achieves the state-of-the-art performance of 1.31 FID with interval guidance under only 256 epochs, approximately 4 training acceleration compared to REPA [52]. In the ImageNet512 512 dataset, our DDT-XL/2 model achieves 1.28 FID within 500K finetuning steps. Furthermore, our DDT achieves strong local consistency on its self-condition feature from the encoder. This property can significantly boost the inference speed by sharing the self-condition between adjacent steps. We formulate the optimal encoder sharing strategy solving as classic minimal sum path problem by minimizing the performance drop of sharing self-condition among adjacent steps. We propose statistic dynamic programming approach to find the optimal encoder sharing strategy with negligible second-level time cost. Compared with the naive uniform sharing, our dynamic programming delivers minimal FID drop. Our contributions are summarized as follows. We propose new decoupled diffusion transformer model, which consists of condition encoder and velocity decoder. We propose statistic dynamic programming to find the optimal self-condition sharing strategy to boost inference speed while keeping minimal performance downgradation. In the ImageNet256256 dataset, using tradition SDf8d4 VAE, our decoupled diffusion transformer (DDT-XL/2) model achieves the SoTA 1.31 FID with interval guidance under only 256 epochs, approximately 4 training acceleration compared to REPA [52]. In the ImageNet512 512 dataset, our DDT-XL/2 model achieves the SoTA 1.28 FID, outperforming all previous methods with significant margin. 2. Related Work Diffusion Transformers. The pioneering work of DiT [36] introduced transformers into diffusion models to replace the traditionally dominant UNet architecture [2, 10]. Empirical evidence demonstrates that given sufficient training iterations, diffusion transformers outperform conventional approaches even without relying on long residual connections. SiT [32] further validated the transformer architecture with linear flow diffusion. Following the simplicity and scalability of the diffusion transformer [32, 36], SD3 [12], Lumina [54], and PixArt [6, 7] introduced the diffusion transformer to more advanced text-to-image areas. Moreover, recently, diffusion transformers have dominated the text-to-video area with substantiated visual and motion quality [1, 20, 24]. Our decoupled diffusion transformer (DDT) presents new variant within the diffusion transformer family. It achieves faster convergence by decoupling the low-frequency encoding and the high-frequency decoding. Fast Diffusion Training. To accelerate the training efficiency of diffusion transformers, recent advances have pursued multi-faceted optimizations. Operator-centric approaches [13, 45, 48, 49] leverage efficient attention mechlinear-attention variants [13, 45, 49] reduced anisms: quadratic complexity to speed up training, while sparseattention architectures [48] prioritized sparsely relevant token interactions. Resampling approaches [12, 16] proposed lognorm sampling [12] or loss reweighting [16] techniques to stabilize training dynamics. Representation learning enhancement approaches integrate external inductive biases: 2 Figure 2. Selected 256 256 and 512 512 resolution samples. Generated from DDT-XL/2 trained on ImageNet 256 256 resolution and ImageNet 512 512 resolution with CFG = 4.0. Figure 3. The reverse-SDE process (generation) of SiT-XL/2 in space. There is clear generation process from low frequency to high frequency. Most of the time is spent on generating highfrequency details (from = 0.4 to = 1.0). REPA [52], RCG [27] and DoD [53] borrowed visionspecific priors into diffusion training, while masked modeling techniques [14, 15] strengthened spatial reasoning by enforcing structured feature completion during denoising. Collectively, these strategies address computational, sampling, and representational bottlenecks. 3. Preliminary Analysis Linear-based flow matching [29, 30, 32] represents specialized family of diffusion models that we focus on as our primary analytical subject due to its simplicity and efficiency. For the convenience of discussion, in certain situations, diffusion and flow-matching will be used interchange3 Figure 4. The FID50K metric of SiT-XL/2 for different timeshift values. We employ 2-nd order Adams-like solver to collect the performance. Allocating more computation at noisy steps significantly improves the performance. ably. In this framework, = 0 corresponds to the pure noise timestep. As illustrated in Fig. 3, diffusion models perform autoregressive refinement on spectral components [11, 37]. The diffusion transformer encodes the noisy latent to capture lower-frequency semantics before decoding higherfrequency details. However, this semantics encoding process inevitably attenuates high-frequency information, creating an optimization dilemma. This observation motivates our proposal to decouple the conventional decode-only diffusion transformer into an explicit encoder-decoder architecture. Lemma 1. For linear flow-matching noise scheduler at timestep t, let us denote Kf req as the maximum frequency of the clean data xdata. The maximum retained frequency in the noisy latent satisfies: fmax(t) > min (cid:32)(cid:18) (cid:19)2 1 (cid:33) we designate our model as DDT (Decoupled Diffusion Transformer). , Kf req . (1) 4.1. Condition Encoder Lemma 1 is directly borrowed from [11, 37], we place the proof of Lemma 1 in Appendix. According to Lemma 1, as increases to less noisy timesteps, semantic encoding becomes easier (due to noise reduction) while decoding complexity increases (as residual frequencies grow). Consider the worst-case scenario at denoising step t, the diffusion transformer encodes frequencies up to fmax(t), to progress to step s, it must decode residual frequency of at least fmax(s) fmax(t). Failure to decode these residual frequencies at step creates critical bottleneck for progression to subsequent steps. From this perspective, if allocating more of the calculations to more noisy timesteps can lead to an improvement, it means that diffusion transformers struggle with encoding lower frequency to provide semantics. Otherwise, if allocating more of the calculations to less noisy timesteps can lead to an improvement, it means that flow-matching transformers struggle with decoding higher frequency to provide fine details. To figure out the bottom-necks of current diffusion models, we conducted targeted experiment using SiT-XL/2 with second-order Adams-like linear multistep solver. As shown in Fig. 4, by varying the time-shift values, we demonstrate that allocating more computation to early timesteps improves final performance compared to uniform scheduling. This reveals that diffusion models face challenges in more noisy steps. This leads to key conclusion: Current diffusion transformers are fundamentally constrained by their low-frequency semantic encoding capacity. This insight motivates the exploration of encoderdecoder architectures with strategic encoder parameter allocation. Prior researches further support this perspective. While lightweight diffusion MLP heads demonstrate limited decoding capacity, MAR [28] overcomes this limitation through semantic latents produced by its masked backbones, enabling high-quality image generation. Similarly, REPA [52] enhances low-frequency encoding through alignment with pre-trained vision foundations [35]. 4. Method Our decoupled diffusion transformer architecture comprises condition encoder and velocity decoder. The condition encoder extracted the low-frequency component from noisy input, class label, and timestep to serve as selfcondition for the velocity decoder; the velocity decoder processed the noisy latent with the self-condition to regress the high-frequency velocity. We train this model using the established linear flow diffusion framework. For brevity, 4 The condition encoder mirrors the architectural design and input structure of DiT/SiT with improved micro-design. It is built with interleaved Attention and FFN blocks, without long residual connections. The encoder processes three inputs, the noisy latent xt, timestep t, and class label y, to extract the self-condition feature zt through series of stacked Attention and FFN blocks: zt = Encoder (xt, t, y). (2) Specifically, the noisy latent xt are patchfied into continuous tokens and then fed to extract the self-condition zt with aforementioned encoder blocks. The timestep and class label serve as external-conditioning information projected into embedding. These external-condition embeddings are progressively injected into the encoded features of xt using AdaLN-Zero[36] within each encoder block. To maintain local consistency of zt across adjacent timesteps, we adopt the representation alignment technique from REPA [52]. Shown in Eq. (3), this method aligns the intermediate feature hi from the i-th layer in the selfmapping encoder with the DINOv2 representation r. Consistent to REPA [52], the hϕ is the learnable projection MLP: Lenc = 1 cos(r, hϕ(hi)). (3) This simple regularization accelerates training convergence, as shown in REPA [52], and facilitates local consistency of zt between adjacent steps. It allows sharing the selfcondition zt produced by the encoder between adjacent steps. Our experiments demonstrate that this encodersharing strategy significantly enhances inference efficiency with only negligible performance degradation. Additionally, the encoder also receives indirect supervision from the decoder, which we elaborate on later. 4.2. Velocity Decoder The velocity decoder adopts the same architectural design as the condition encoder and consists of several stacked interleaved Attention and FFN blocks, akin to DiT/SiT. It takes the noisy latent xt, timestep t, and self-conditioning zt as inputs to estimate the velocity vt. Unlike the encoder, we assume that class label information is already embedded within zt. Thus, only the external-condition timestep and self-condition feature zt are used as condition inputs for the decoder blocks: vt = Decoder (xt, t, zt). (4) As demonstrated previously, to further improve consistency of self-condition zt between adjacent steps, we employ AdaLN-Zero [36] to inject zt into the decoder feature. The decoder is trained with the flow matching loss as shown in Eq. (5): (cid:90) 1 Ldec = E[ 0 (xdata ϵ) vt2dt]. (5) 4.3. Sampling acceleration By incorporating explicit representation alignment into the encoder and implicit self-conditioning injection into the decoder, we achieve local consistency of zt across adjacent steps during training (shown in Fig. 5). This enables us to share zt within suitable local range, reducing the computational burden on the self-mapping encoder. Formally, given total inference steps and encoder computation bugets K, thus the sharing ratio is 1 , we define Φ with Φ = as the set of timesteps where the self-condition is recalculated, as shown in Equation 6. If the current timestep is not in Φ, we reuse the previously computed ztt as zt. Otherwise, we recompute zt using the encoder and the current noisy latent xt: (cid:40) zt = ztt, Encoder (xt, t, y), if / Φ if Φ (6) Uniform Encoder Sharing. This naive approach recaluculate self-condition zt every steps. Previous work, such as DeepCache [33], uses this naive handcrafted uniform Φ set to accelerate UNet models. However, UNet models, trained solely with denoising loss and lacking robust representation alignment, exhibit less regularized local consistency in deeper features across adjacent steps compared to our DDT model. Also, we will propose simple and elegant statistic dynamic programming algorithm to construct Φ. Our statistic dynamic programming can exploit the optimal Φ set optimally compared to the naive approaches [33]. Statistic Dynamic Programming. We construct the statistic similarity matrix of zt among different steps RN using cosine distance. The optimal Φ set would guarantee the total similarity cost (cid:80)K S[Φk, i] achieves global minimal. This question is well-formed classic minimal sum path problem, it can be solved by dynamic programming. As shown in Eq. (8), we donate Ck as cost and Pk as traced path when Φk = i. the state transition function from Ck1 (cid:80)Φk+1 i=Φk to Ck follows: Ck Pk = min j=0 = argmini {Ck1 Σi l=jS[j, l]}. j=0{Ck1 Σi l=jS[j, l]}. (7) (8) After obtaining the cost matrix and tracked path P, the optimal Φ can be solved by backtracking from PK . 5. Experiment We conduct experiments on 256x256 ImageNet datasets. The total training batch size is set to 256. Consistent with methodological approaches such as SiT [32], DiT [36], and REPA [52], we employed the Adam optimizer with constant learning rate of 0.0001 throughout the entire training process. To ensure fair comparative analysis, we did not use gradient clipping and learning rate warm-up techniques. Our default training infrastructure consisted of 16 or 8 A100 GPUs. For sampling, we take the Euler solver with 250 steps as the default choice. As for the VAE, we take the off-shelf VAE-ft-EMA with downsample factor of 8 from Huggingface1. We report FID [18], sFID [34], IS [39], Precision and Recall [25]. 5.1. Improved baselines Recent architectural improvements such as SwiGLU [46, 47], RoPE [42], and RMSNorm [46, 47] have been extensively validated in the research community [8, 31, 50]. Additionally, lognorm sampling [12] has demonstrated significant benefits for training convergence. Consequently, we developed improved baseline models by incorporating these advanced techniques, drawing inspiration from recent works in the field. The performance of these improved baselines is comprehensively provided in Tab. 2. To validate the reliability of our implementation, we also reproduced the results for REPA-B/2, achieving metrics that marginally exceed those originally reported in the REPA[52]. These reproduction results provide additional confidence in the robustness of our approach. The improved baselines in our Tab. 2 consistently outperform their predecessors without REPA. However, upon implementing REPA, performance rapidly approaches saturation point. This is particularly evident in the XL model size, where incremental technique improvements yield diminishingly small gains. 5.2. Metric comparison with baselines We present the performances of different-size models at 400K training steps in Tab. 2. Our diffusion encoderdecoder transformer(DDT) family demonstrates consistent and significant improvements across various model sizes. Our DDT-B/2(8En4De) model exceeds Improved-REPAB/2 by 2.8 FID gains. Our DDT-XL/2(22En6De) exceeds REPA-XL/2 by 1.3 FID gains. While the decoder-only diffusion transformers approach performance saturation with REPA[52], our DDT models continue to deliver superior results. The incremental technique improvements show diminishing gains, particularly in larger model sizes. However, our DDT models maintain significant performance advantage, underscoring the effectiveness of our approach. 1https://huggingface.co/stabilityai/sdvaeft- ema 5 Params Epochs FID IS Pre. Rec. FID IS Pre. Rec. 256256, w/o CFG 256256, w/ CFG MAR-B [28] CausalFusion [9] LDM-4 [38] DDT-L (Ours) MAR-L [28] VAVAE [50] CausalFusion [9] ADM [10] DiT-XL [36] SiT-XL [32] ViT-XL [16] U-ViT-H/2 [2] MaskDiT [14] FlowDCN [48] RDM [44] REPA [52] DDT-XL (Ours) DDT-XL (Ours) DDT-XL (Ours) 208M 368M 400M 458M 479M 675M 676M 554M 675M 675M 451M 501M 675M 618M 553M 675M 675M 675M 675M 800 800 170 80 800 800 800 400 1400 1400 400 400 1600 400 / 800 80 256 400 3.48 5.12 10.56 7.98 2.6 2.17 3.61 10.94 9.62 8.3 8.10 6.58 5.69 8.36 5.27 5.9 6.62 6.30 6.27 192.4 166.1 103.5 128.1 221.4 205.6 180.9 - 121.5 - - - 178.0 122.5 153.4 157.8 135.2 146.7 154.7 0.78 0.73 0.71 0.68 0.79 0.77 0.75 0.69 0.67 - - - 0.74 0.69 0.75 0.70 0.69 0.68 0.68 0.58 0.66 0.62 0.67 0.60 0.65 0.66 0.63 0.67 - - - 0.60 0.65 0.62 0.69 0.67 0.68 0. 2.31 1.94 3.6 1.64 1.78 1.35 1.77 4.59 2.27 2.06 2.06 2.29 2.28 2.00 1.99 1.42 1.52 1.31 1.26 281.7 264.4 247.7 310.5 296.0 295.3 282.3 186.7 278.2 270.3 - 263.9 276.6 263.1 260.4 305.7 263.7 308.1 310.6 0.82 0.82 0.87 0.81 0.81 0.79 0.82 0.82 0.83 0.82 - 0.82 0.80 0.82 0.81 0.80 0.78 0.78 0.79 0.57 0.59 0.48 0.61 0.60 0.65 0.61 0.52 0.57 0.59 - 0.57 0.61 0.58 0.58 0.64 0.63 0.62 0.65 Table 1. System performance comparison on ImageNet 256 256 class-conditioned generation. Gray blocks mean the algorithm uses VAE trained or fine-tuned on ImageNet instead of the off-shelf SD-VAE-f8d4-ft-ema. Model FID sFID IS Prec. Rec. 5.3. System level comparision 33.0 SiT-B/2 [32] REPA-B/2 [52] 24.4 REPA-B/2(Reproduced) 22.2 DDT-B/2 (8En4De) 21.1 6.46 6.40 7.50 7.81 43.7 59.9 69.1 73.0 0.53 0.59 0.59 0. Improved-SiT-B/2 Improved-REPA-B/2 DDT-B/2 (8En4De) 25.1 19.1 16.32 6.63 6.54 0.57 58.8 6.88 76.49 0.60 0.62 86.0 SiT-L/2 [32] REPA-L/2 [52] Improved-SiT-L/2 Improved-REPA-L/2 DDT-L/2 (20En4De) SiT-XL/2 [32] REPA-XL/2 [52] Improved-SiT-XL/2 Improved-REPA-XL/2 DDT-XL/2 (22En6De) 18.8 10.0 12.7 9.3 7. 17.2 7.9 10.9 8.14 6.62 0.64 72.0 5.29 5.20 109.2 0.69 5.48 0.65 95.7 5.44 116.6 0.67 5.50 128.1 0.68 5.07 76.52 0.65 5.06 122.6 0.70 5.3 103.4 0.66 5.34 124.9 0.68 4.86 135.1 0.69 0.63 0.65 0.65 0.65 0.64 0.66 0.66 0.64 0.65 0.65 0.66 0. 0.63 0.65 0.65 0.67 0.67 Table 2. Metrics of 400K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. By default, our DDT models are built on improved baselines. DDT means model built on naive baseline without architecture improvement and lognorm sampling, consistent to REPA. Our DDT models consistently outperformed their counterparts. ImageNet 256 256. We report the final metrics of DDTXL/2 (22En6De) and DDT-L/2 (20En4De) at Tab. 1. Our DDT models demonstrate exceptional efficiency, achieving convergence in approximately 1 4 of the total epochs compared to REPA [52] and other diffusion transformer models. In order to maintain methodological consistency with REPA, we employed the classifier-free guidance with 2.0 in the interval [0.3, 1], Our models delivered impressive results: DDT-L/2 achieved 1.64 FID, and DDT-XL/2 got 1.52 FID within just 80 epochs. By extending training to 256 epochsstill significantly more efficient than traditional 800-epoch approachesour DDT-XL/2 established new state-of-the-art benchmark of 1.31 FID on ImageNet 256256, decisively outperforming previous diffusion transformer methodologies. To extend training to 400 epochs, our DDT-XL/2(22En6De) achieves 1.26 FID, nearly reaching the upper limit of SD-VAE-ft-EMA-f8d4, which has 1.20 rFID on ImageNet256. ImageNet 512512 We provide the final metrics of DDTXL/2 at Tab. 3. To validate the superiority of our DDT model, we take our DDT-XL/2 trained on ImageNet 256 256 under 256 epochs as the initialization, fine-tune out DDT-XL/2 on ImageNet 512 512 for 100K steps. We adopt the aforementioned interval guidance [26] and we achieved remarkable state-of-the-art performance of 1.90 FID, decisively outperforming REPA by significant 0.28 6 ImageNet 512 SharRatio Acc Φ FID sFID IS Prec. Rec. Model FID sFID IS Pre. Rec. BigGAN-deep [3] StyleGAN-XL [40] 8.43 2.41 7.72 ADM-G [10] 3.85 ADM-G, ADM-U DiT-XL/2 [36] 3.04 2.62 SiT-XL/2 [32] REPA-XL/2 [52] 2.08 FlowDCN-XL/2 [48] 2.44 1.28 DDT-XL/2 (500K) 8.13 4.06 6.57 5.86 5.02 4.18 4.19 4.53 4. 177.90 0.88 267.75 0.77 172.71 0.87 221.72 0.84 240.82 0.84 252.21 0.84 0.83 274.6 0.84 252.8 305.1 0.80 0.29 0.52 0.42 0.53 0.54 0.57 0.58 0.54 0.63 Table 3. Benchmarking class-conditional image generation on ImageNet 512512. Our DDT-XL/2(512 512) is fine-tuned from the same model trained on 256 256 resolution setting of 1.28M steps. We adopt the interval guidance with interval [0.3, 1] and CFG of 3.0 performance margin. In Tab. 3, some metrics exhibit subtle degradation, we attribute this to potentially insufficient fine-tuning. When allocating more training iterations to DDT-XL/2, it achieves 1.28 FID at 500K steps with CFG3.0 within the time interval [0.3, 1.0]. 5.4. Acceleration by Encoder sharing As illustrated in Fig. 5, there is strong local consistency of the self-condition in our condition encoder. Even zt=0 has strong similarity above 0.8 with zt=1. This consistency provides an opportunity to speed up inference by sharing the encoder between adjacent steps. We employed the simple uniform encoder sharing strategy and the new novel statistics dynamic programming strategy. Specifically, for the uniform strategy, we only recalculate the self-condition zt every steps. For statistics dynamic programming, we solve the aforementioned minimal sum path on the similarity matrix by dynamic programming and recalculate zt according to the solved strategy. As shown in Fig. 6, there is significant inference speedup nearly without visual quality loss when is smaller than 6. As shown in Tab. 4, the metrics loss is still marginal, while the inference speedup is significant. The novel statistics dynamic programming slightly outperformed the naive uniform strategy with less FID drop. 5.5. Ablations We conduct ablation studies on ImageNet 256 256 with DDT-B/2 and DDT-L/2. For sampling, we take the Euler solver with 250 steps as the default choice without classifier-free guidance. For training, we train each model with 80 epochs(400k steps), and the batch size is set to 256. 0.00 0.50 0.66 0. 0.80 1.0 Uniform 1.31 1.6 Uniform 1.31 1.9 Uniform 1.32 2.3 Uniform 1.34 4.62 308.1 0.78 4.48 300.5 0.78 4.46 301.2 0.78 4.43 302.7 0.78 2.6 Uniform 1.36 StatisticDP 1.33 4.40 303.3 0.78 4.37 301.7 0. 0.83 2.7 Uniform 1.37 StatisticDP 1.36 4.41 302.8 0.78 4.35 300.3 0.78 0.87 3.0 Uniform 1.42 StatisticDP 1.40 4.43 302.8 0.78 4.35 302.4 0.78 0.66 0.65 0.65 0.65 0.64 0.64 0.64 0.64 0.64 0. Table 4. Metrics of 400K training steps with different model sizes. All results are reported without classifier-free guidance. gray means metrics are copied from the original paper, otherwise it is produced by our codebase. Our DDT models consistently outperformed its counterparts Figure 5. The cosine similarity of self-condition feature zt from encoder between different timesteps. There is strong correlation between adjacent steps, indicating the redundancy. Figure 6. Sharing the self-condition zt in adjacent steps significant speedup the inference.We tried various sharing frequency configurations. There is marginal visual quality down-gradation when the sharing frequency is reasonable. Encoder-Decoder Ratio we systematically explored ratios ranging from 2 : 1 to 5 : 1 across different model sizes. in Fig. 7 and Fig. 8. Our notation mEnnDe represents models with encoder layers and decoder layers. The invesFigure 7. The DDT-B/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-B/2(8En4De) achieves much faster convergence speed and better performance. Figure 8. The DDT-L/2 built upon Improved-baselines under various Encoder and Decoder layer ratio. DDT-L/2 prefers an unexpected aggressive encoder-deocder ratio DDT-L/2(20En4De) achieves much faster convergence speed and better performance. tigation experiments in Fig. 7 and Fig. 8 revealed critical insights into architectural optimization. We observed that larger encoder is beneficial for further improving the performance as the model size increases. For the Base model in Fig. 7, the optimal configuration emerged as 8 encoder layers and 4 decoder layers, delivering superior performance and convergence speed. Notably, the Large model in Fig. 8 exhibited distinct preference, achieving peak performance with 20 encoder layers and 4 decoder layers, an unexpectedly aggressive encoder-decoder ratio. This unexpected discovery motivates us to scale the layer ratio in DDT-XL/2 to 22 encoder layers and 6 decoders to explore the performance upper limits of diffusion transformers. Decoder Block types. In our investigation of decoder block types and their impact on high-frequency decoding performance, we systematically evaluated multiple architectural configurations. Our comprehensive assessment included alternative approaches such as simple 33 convolution blocks and naive MLP blocks. As shown in Tab. 5, the default (Attention with the MLP) setting achieves better results. Thanks to the encoder-decoder design, naive Conv blocks even achieve comparable results. 6. Conclusion In this paper, we have introduced novel Decoupled Diffusion Transformer, which rethinks the optimization dilemma DecoderBlock FID sFID IS Prec. Rec. Conv+MLP MLP+MLP Attn+MLP 16.96 24.13 16.32 7.33 7.89 6.63 85.1 65.0 86.0 0.62 0.57 0. 0.65 0.65 0.66 Table 5. Metrics of 400K training steps on DDT-B/2(8En4De) with different decoder blocks. All results are reported without classifier-free guidance. The Default Attention + MLP configuration achieves best performance. of the traditional diffusion transformer. By decoupling the low-frequency encoding and high-frequency decoding into dedicated components, we effectively resolved the optimization dilemma that has constrained diffusion transformer. Furthermore, we discovered that increasing the encoder capacity relative to the decoder yields increasingly beneficial results as the overall model scale grows. This insight provides valuable guidance for future model scaling efforts. Our experiments demonstrate that our DDT-XL/2 (22En6De) with an unexpected aggressive encoder-decoder layer ratio achieves great performance while requiring only 256 training epochs. This significant improvement in efficiency addresses one of the primary limitations of diffusion models: their lengthy training requirements. The decoupled architecture also presents opportunities for inference optimization through our proposed encoder result sharing mechanism. Our statistical dynamic programming approach for determining optimal sharing strategies enables faster inference while minimizing quality 8 degradation, demonstrating that architectural innovations can yield benefits beyond their primary design objectives."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2 [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2266922679, 2023. 2, 6 [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 1, 7 [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 2 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 1 [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [8] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama interface for vision tasks. arXiv preprint arXiv:2403.00522, 2024. 5 [9] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. 6, 12 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2, 6, 7, 11 [11] Sander Dieleman. Diffusion is spectral autoregression, 2024. 3, 4 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. 2, 5 [13] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkvarXiv preprint like architectures for diffusion models. arXiv:2404.04478, 2024. 2 [14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2316423173, 2023. 3, 6 [15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173, 2023. 3 [16] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 74417451, 2023. 2, 6 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [20] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [21] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. 1 [22] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 11 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2 [24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [25] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 5 [26] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 6 9 [27] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. Advances in Neural Information Processing Systems, 37:125441125468, 2024. 3 [28] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2025. 2, 4, 6 [29] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 3 [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, 3 [31] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, XiFit: Flexible arXiv preprint hui Liu, Wanli Ouyang, and Lei Bai. vision transformer for diffusion model. arXiv:2402.12376, 2024. 5 [32] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 2, 3, 5, 6, 7 [33] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF conference on computer vision and pattern recognition, pages 1576215772, 2024. 5 [34] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. arXiv preprint arXiv:2103.03841, 2021. 5 [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 2, 4, 5, 6, 7 [37] Severi Rissanen, Markus Heinonen, and Arno Solin. Generative modelling with inverse heat dissipation. arXiv preprint arXiv:2206.13397, 2022. 3, 4 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 6 [39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 5 [40] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 1, 7 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based 10 generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [43] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1 [44] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 6 [45] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. [46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 5 [47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 5 [48] Shuai Wang, Zexian Li, Tianhui Song, Xubin Li, Tiezheng Ge, Bo Zheng, and Limin Wang. Flowdcn: Exploring dcnlike architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024. 2, 6, 7 [49] Jing Nathan Yan, Jiatao Gu, and Alexander Rush. arXiv preprint Diffusion models without attention. arXiv:2311.18257, 2023. 2 [50] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. 5, 6 [51] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [52] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, 3, 4, 5, 6, 7, 12 [53] Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, and Luping Zhou. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. 2, 3 [54] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 2 (17) (18) (19) (20) (21) A. Model Specs Let us start by expanding the reverse ode first. Config #Layers Hidden dim #Heads B/2 L/2 XL/2 12 24 28 768 1024 1152 12 16 16 (t)xt 1 =f (t)(α(t)xdata + σ(t)ϵ) g(t)2[ g(t)2xt log p(xt) 1 2 ϵ σ(t) ] =f (t)α(t)xdata + (f (t)σ(t) + 1 2 g(t)2 σ(t) )ϵ B. Hyper-parameters To prove Eq. (16), we needs to demonstrate that: VAE VAE donwsample latent channel SD-VAE-f8d4-ft-ema 8 4 optimizer base learning rate weight decay batch size learning rate schedule augmentation diffusion sampler diffusion steps evaluation suite AdamW [22] 1e-4 0.0 256 constant center crop Euler-ODE 250 ADM [10] C. Linear flow and Diffusion Given the SDE forward and reverse process: dxt = (t)xtdt + g(t)dw (9) dxt = [f (t)xt g(t)2x log p(xt)]dt + g(t)dw (10) corresponding deterministic process exists with trajectories sharing the same marginal probability densities of reverse SDE. dxt = [f (t)xt 1 2 g(t)2xt log p(xt)]dt (11) Given xt = αtxdata + σϵ. The traditional diffusion model learns: xt log p(xt) = (12) ϵ σ(t) The flow-matching framework actually learns the following: vt = αx + σϵ = ϵ (13) (14) Here we will demonstrate in flow-matching, the vt prediction is actually as same as the reverse ode: αx + σϵ =f (t)xt 1 2 g(t)2xt log p(xt) (15) (16) α(t) = ftα(t) σ(t) = ftσ(t) + 1 2 g2 σ(t) . Here, let us derive the relation between ft and α(t), α(t). We donate xdata(t) = α(t)xdata is the remain component of xdata in xt, it is easy to find that: dxdata(t) = ftxdata(t)dt d(α(t)xdata) = ftα(t)xdatadt dα(t) = ftα(t)dt (22) (23) (24) So, Eq. (20) is right. Based on the above equation, we will demonstrate the relation of gt, ft with σ(t). Note that Gaussian noise has nice additive properties. aϵ1 + bϵ2 (0, (cid:112) a2 + b2) (25) Let us start with the gaussian noise component ϵ(t) calculation, reaching at t, every noise addition at [0, t] while been decayed by factor of α(t) α(s) . Thus, the mixed Gaussian noise will have std variance σ(t) of: (cid:115) (cid:90) ( [( σ(t) = 0 (cid:115) σ(t) = α(t) α(t) α(s) )2g2 ]ds) (cid:90) ( [( 0 gs α(s) )2]ds) (26) (27) After obtaining the relation of ft, gt and α(t), σ(t), we derive α(t) and σ(t) with above conditions: (cid:90) α(t) = ft exp[ fsds] 0 α(t) = ftα(t) (28) (29) As for σ(t), it is quit complex but not hard: E. Linear multisteps method (cid:115) σ(t) = α(t) (cid:90) ( [( gt α(s) )2]ds) + α(t) (cid:113) g2 α(t) 1 2 ((cid:82) 0 [( gt α(s) )2g2 ]ds) (30) (cid:115) σ(t) = (ftα(t)) (cid:90) ( [( 0 gt α(s) )2]ds) + α(t) g2 α2(t) 1 (cid:113) ((cid:82) 0 [( gt α(s) )2]ds) (31) σ(t) = ftα(t) (cid:115) ( (cid:90) 0 [( gt α(s) )2]ds) + α(t) (cid:113) ((cid:82) 1 2 g2 0 [( gt α(s) )2]ds) (32) σ(t) = ftσ(t) + 1 2 gt σ(t) So, Eq. (21) is right. (33) D. Proof of Spectrum Autoregressive Given the noise scheduler{αt, σt}, the clean data xdata and Gaussian noise ϵ. Denote Kf req as the maximum frequency of the clean data xdata The noisy latent xt at timestep has been defined as: xt = αtxdata + σtϵ (34) The spectrum magnitude ciof xt on DCT basics ui follows: ci = Eϵ[uT ci = Eϵ[uT xt]2 (αtxdata + σtϵ)] Recall that the spectrum magnitude of Gaussian noise ϵ is uniformly distributed. We conduct targeted experiment on SiT-XL/2 with AdamsBashforth like linear multistep solver; To clarify, we did not employ this powerful solver for our DDT models in all tables across the main paper. The reverse ode of the diffusion models tackles the following integral: xi+1 = xi + (cid:90) ti+1 ti vθ(xt, t)dt (37) The classic Euler method employs vθ(xi, ti) as an estimate of vθ(xt, t) throughout the interval [ti, ti+1] xi+1 = xi + (ti+1 ti)vθ(xi, ti). (38) The most classic multi-step solver AdamsBashforth method (deemed as Adams for brevity) incorporates the Lagrange polynomial to improve the estimation accuracy with previous predictions. vθ(xt, t) = (cid:88) ( (cid:89) j= k=0,k=j tk tj tk )vθ(xj, tj) xi+1 xi + (cid:90) ti+1 (cid:88) ( (cid:89) j=0 k=0,k=j tk tj tk )vθ(xj, tj)dt xi+1 xi + ti (cid:88) j=0 vθ(xj, tj) (cid:90) ti+1 ti (cid:89) ( k=0,k=j tk tj tk )dt ti k=0,k=j ((cid:81)i Note that (cid:82) ti+1 ttk )dt of the Lagrange polytj tk nomial can be pre-integrated into constant coefficient, resulting in only naive summation being required for ODE solving. xdatauT ϵ] + σ2 Eϵ[uT ϵ] F. Classifier free guidance. xdata]2 + 2αtσtEϵ[uT Eϵ[uT ϵ]2 xdata]2 + σ2 xdata]2 + σ2 λ ci = [αtuT ci = [αtuT [uT ci = α2 λ has bigger value than [αtuT if σ2 xdata]2, the spectrum magnitude ci on DCT basics ui will be canceled, thus the maximal remaining frequency fmax(t) of original data in xt follows: fmax(t) > min (cid:32)(cid:18) αtuT xdata σtλ (cid:19)2 (cid:33) , Kf req (35) 2 Though αtuT xdata σtλ depends on the dataset. Here, we directly suppose it as constant 1. And replace α = and σ = 1 in above equation: fmax(t) > min (cid:32)(cid:18) (cid:19)2 1 (cid:33) , Kf req (36) As classifier-free guidance significantly impacts the performance of diffusion models. Traditional classifier-free guidance improves performance at the cost of decreased diversity. Interval guidance is recently been adopted by REPA[52] and Causalfusion[9], It applies classifier-free guidance only to the high-frequency generation phase to preserve the diversity. We sweep different classifier-free guidance strength with selected intervals. Our DDT-XL/2 achieves the best performance with interval [0.3, 1] with classifer-free guidance of 2. Recall that we donate = 0 as the pure noise timestep while REPA[52] use = 1, thus this exactly correspond to the [0, 0.7] interval in REPA[52] Figure 9. FID10K of DDT-XL/2 with different Classifer free guidance strength and guidance intervals. We sweep different classifier-free guidance strength with selected intervals. Our DDTXL/2 achieves the best performance with interval [0.3, 1] with classifer-free guidance of 2."
        }
    ],
    "affiliations": [
        "ByteDance Seed Vision",
        "Nanjing University"
    ]
}