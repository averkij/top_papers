{
    "paper_title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
    "authors": [
        "Shufan Li",
        "Konstantinos Kallidromitis",
        "Hritik Bansal",
        "Akash Gokul",
        "Yusuke Kato",
        "Kazuki Kozuka",
        "Jason Kuen",
        "Zhe Lin",
        "Kai-Wei Chang",
        "Aditya Grover"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 3 8 6 1 . 5 0 5 2 : r LaViDa: Large Diffusion Language Model for Multimodal Understanding Shufan Li1, Konstantinos Kallidromitis2, Hritik Bansal1, Akash Gokul4, Yusuke Kato2 Kazuki Kozuka2, Jason Kuen3, Zhe Lin3, Kai-Wei Chang1, Aditya Grover1 1UCLA 2Panasonic AI Research 3Adobe Research 4Salesforce Research * Equal Contribution"
        },
        {
            "title": "Abstract",
            "content": "Modern Vision-Language Models (VLMs) can solve wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs potential for multimodal tasks is underexplored. We introduce LaViDa, family of VLMs built on DMs. We build LaViDa by equipping DMs with vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVaNext-Llama3-8B by +4.1 CIDEr with 1.92 speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as strong alternative to AR VLMs. Code and models is available at https://github.com/jacklishufan/LaViDa Figure 1: We propose LaViDa, the first family of diffusionbased discrete VLM models. LaViDa models achieve competitive performance against AR (LLaVa-1.6, Openbaselines across multiple LLaVa-Next) visual understanding tasks including MMMU (world knowledge), MathVista (reasoning), ChartQA (OCR), and ScienceQA (science). Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) have shown remarkable utility across diverse domains, from enduser applications like virtual assistants [36], to research tasks such as scientific image captioning and document understanding [45, 16, 5, 1, 47]. In industrial settings, VLMs support automated product tagging, content moderation, and quality control in manufacturing [68, 2]. Their ability to jointly process visual and textual information makes them indispensable for practical applications and cutting-edge research. Currently, nearly all popular VLMssuch as Qwen-VL [5, 74], Intern-VL [17, 84], and GPT-4 [57] are built on top of large language models (LLMs) that generate text in an autoregressive (AR) manner; that is, they produce tokens one by one in left-to-right sequence. While these models have demonstrated strong performance on many tasks, they suffer from several key limitations. First, their sequential generation process is inherently hard to parallelize, resulting in slow inference speed [8]. More critically, their left-to-right generation makes it difficult to handle tasks that benefit from bidirectional context or structural constraintssuch as text infilling [22]. For example, generating poem where each line starts with specific syllable, or extracting structured information from an image in predefined JSON format, often requires the model to fill in or coordinate content across the sequence. Autoregressive models still struggle to consistently satisfy such constraints, even with carefully crafted prompts and examples. Recently, discrete diffusion models (DMs) have emerged as promising alternative to AR LLMs. Most notably, LLaDA [56] and Dream [78] achieved comparable results to AR LLMs across diverse language tasks. Unlike AR LLMs, DMs treat text generation as diffusion process over discrete tokens. forward process gradually corrupts sequence of discrete text tokens into sequence of mask tokens. At inference, we start with sequence of mask tokens and gradually transform them into sequence of meaningful text tokens through learned reverse process. Compared to AR LLMs, diffusion models offer several theoretical advantages that directly address the limitations of autoregressive generation. While AR LLMs have fixed throughputgenerating one token at timeDMs allow flexible control over the speed-quality trade-off by adjusting the number of diffusion steps [78, 63, 48, 56]. Moreover, their ability to model bidirectional context makes them well-suited for tasks like text infilling, enabling more effective constrained generation and structured output formattingcapabilities especially valuable in vision-language settings where outputs may need to follow specific schemas. In this work, we propose LaViDa (Large Vision-Language Diffusion Model with Masking), the first family of VLMs based on diffusion. LaViDa enables pretrained DMs to perceive visual inputs by integrating vision features into the diffusion backbone via vision encoderanalogous to how LLaVA [45, 43] augments large language models (LLMs) with visual inputs. Specifically, we adopt two-stage training pipeline with diffusion objective: pretraining followed by supervised fine-tuning. Adapting DMs for vision-language tasks presents several practical challenges. First, standard DM training is data-inefficient: the model learns only from the subset of corrupted tokens at each timestep. For example, in question-answering task where the target answer is The answer is dog, the corruption process may mask The [mask] is dog, leaving the key token dog unmasked and thus excluded from the loss. This is especially problematic for multimodal tasks, where answer tokens often carry crucial semantic content grounded in the image [60]. To address this, we introduce complementary masking scheme that ensures every token in the output sequence contributes to learning, improving data efficiency. Second, inference algorithms used by existing DMs are slow in practice due to the lack of KV cache supportan inherent limitation of bidirectional context modeling [3]. This leads to repeated recomputation over the full prompt at every decoding step. While tolerable for short-text settings, it becomes significant bottleneck in vision-language tasks, where multimodal prompts may include hundreds of visual tokens. To address this, we propose Prefix-DLM decoding, simple yet effective technique that enables caching of multimodal prompts (i.e., image and question tokens), significantly accelerating inference. Lastly, DMs offer unique advantage over autoregressive models: the ability to trade off speed and quality by varying the number of diffusion steps. However, the widely used linear masking schedulewhich unmasks fixed number of tokens per stepperforms poorly at low step counts. Motivated by text-to-image diffusion models like SD3 [21], we adopt timestep shifting strategy that 2 adaptively adjusts how many tokens are unmasked per step. This leads to better sample quality under aggressive step reduction, allowing faster generation without large degradation in quality. We conducted extensive evaluations of LaViDa across wide range of vision-language tasks. Results show that LaViDa achieves competitive performance on most benchmarks, including MMMU [79], MathVista [50], ChartQA [52] and ScienceQA [51], when compared with AR VLMs like LLaVa1.6-7B [44, 42] and Open-LLaVa-Next-Llama3-8B [15]. We highlight these results in Figure ??. On constrained generation tasks, LaViDa greatly outperforms AR baselines (+59% on Poem Completion). It also supports flexible speed-quality tradeoff, achieving higher quality on COCO image captioning (+ 4.1 CIDEr) and 1.92 speedup [41]. In summary, our contributions are: We introduce LaViDa, the first family of VLMs based on diffusion models. Our models achieve competitive performance across wide range of tasks compared to AR VLMs, while offering the unique benefits of DMs. We introduce several novel training and inference techniques for DMs, including complementary masking, Prefix-DLM, and timestep shifting that improve the training efficiency, inference speed, and sample quality of LaViDa. We conduct systematic study of various design choices for adapting DMs to visionlanguage tasks (e.g. image resolution), offering insights for future work in this direction."
        },
        {
            "title": "2 Background and Related Works",
            "content": "2.1 Vision-Language Models Vision-Language Models (VLM) extend the capability of Large Language Models to visual understanding tasks [35, 45, 74, 5, 9, 84, 57]. The common recipe to build VLM is to start with strong large language model and combine it with vision encoder [45, 74]. These VLMs typically undergo multiple stages of training, which can be generally categorized into pretraining and finetuning stages. The pretraining data usually consists of text-image pairs for vision-language alignment, while the finetuning data consists of wide range of instruction-following tasks. There are several dedicated lines of work focusing on different components of this overarching framework, such as the design of vision encoders [26, 76] and training techniques [40, 71]. To this date, most vision-language models such as LLaVa [45, 35] and Qwen-VL [5, 74] series employ an autoregressive training objective. 2.2 Diffusion Language Models Diffusion models first emerged as powerful alternative to GANs for generating continuous data such as images [61, 58, 21]. Early explorations in diffusion language models directly built continuous diffusion models for latent text embeddings [38, 49], with limited success. More recently, discrete diffusion models [4, 63, 48] have proven to be better candidates for language modeling, achieving performance comparable to AR models while offering unique advantages, such as speed-quality tradeoffs and controllability. Most notably, LLaDa-8B and Dream-8B [56, 78] demonstrated that DLMs can achieve competitive performance against AR LLMs at scale. Formally, given text sequence of tokens X0 = [X 1 0 ], the forward process q(XtXs) 1 ...X gradually converts it to sequence full of mask tokens \"[M]\", denoted by X1 = [X 1 1 ], through the continuous time-interval [0, 1], with 1 0. neural network pθ is used to model the reverse process p(XsXt). The diffusion language modeling objective can be defined as: 0 ...X 1 , 0 , 2 LDLM = Et,X0,Xt (cid:21) log pθ(X0Xt) (cid:20) 1 (1) where log pθ(X0Xt) is assumed to be factorized into (cid:81)L 0Xt). At each training step, we sample uniformly from the interval [0, 1] and sample X0 from some data distribution D. Xt is then sampled from the forward process q(XtX0). The loss is only computed over the masked tokens in Xt, since pθ(X = [M ]. We offer additional background on the details of these formulations in Appendix A.1. We also incorporate addition backgrounds on other relevant literature, such as masked generative models [10, 11] and multimodal diffusion models [67, 37], in Appendix C. 0Xt) has closed form representation that does not depend on θ when i=1 pθ(X 3 Figure 2: Overall design of LaViDa. LaViDas architecture consists of vision encoder, diffusion language model, and an MLP vision projector. The bottom half of the figure illustrates the image encoding process, while the top half depicts the diffusion language modeling process. These two pipelines are described in detail in Sec. 3.1."
        },
        {
            "title": "3 Method",
            "content": "3.1 Model Architecture LaViDas model architecture follows similar design to common AR VLMs like LLaVa [35]. It consists of vision encoder and diffusion language model. These two parts are connected by MLP projection network. The overall design is illustrated in Figure 2. Vision Encoder. Given an input image and text prompt , we first resize the image to 7682 and divide it into four non-overlapping views of 3842, denoted I1:4. We also resize the original image to 3842 to obtain fifth view, I5, following the design of prior works [35, 44]. These five views are independently encoded by the vision encoder (SigLIP-400M [81]), each producing 272 embeddings, denoted V1:5. In total, this yields 3645 embeddings per image. To reduce sequence length for efficient training, we apply 2 2 average pooling on each view, reducing embeddings to 142 per view, or 980 total. The embeddings of five views are then flattened and concatenated into 1D sequence before being processed by the projection network to obtain the final visual context of the diffusion language model. This process mirrors the vision encoding process in AR LLMs [26] and is illustrated in the bottom part of Figure 2. Diffusion Language Model. The diffusion language model is multi-layer Transformer [70] whose architecture resembles that of LLMs. The only major difference is that its attention mask is noncausal, and it uses the diffusion language modeling objective described in Section 2.2 instead of the next-token prediction used in AR models. The input to the diffusion language model consists of the projected vision embeddings, the prompt , and partially masked response Xt. The outputs of the last transformer block are passed through final linear layer to obtain token-wise logits pθ(X 0I, P, Xt) for the unmasked response X0. In our experiments, we explored LLaDA-8B (default) and Dream-7B as our diffusion language model. This process is illustrated in the upper half of Figure 2. 3.2 Training Algorithms Our training objective is based on the diffusion language modeling objective described in Section 2.2. Each training sample consists of an image I, text prompt and clean text answer X0 from the training data. For multi-round conversation, we sample one round as the \"answer\" and treat the history as \"prompt\". We first sample timestep [0, 1] and partially masked answer Xt using the forward process described in Section 2.2. LaViDa then implements the conditioned reverse process pθ(X0I, P, Xt). The canonical diffusion vision-language modeling objective is formulated as: LD-VLM = Et,I,P,X0,Xt (cid:21) log pθ(X0I, P, Xt) (cid:20) 1 (2) 4 (a) Complementary Masking (b) Attention Mask of Prefix-DLM Figure 3: Technical Details of LaViDa. (a) We propose Complementary Masking to ensure loss is calculated over all tokens in the data for training efficiency. (b) We propose Prefix-DLM attention mask that enables KV caching. We visualize the attention mask of image tokens (Img1-2), prompt tokens (Pmt1-2), and text and mask tokens in the noise answer Xt (Ans1, [M], Ans3). Rows represent queries, while columns are keys. Colored squares indicate that queries and keys can interact. where pθ(X0I, P, Xt) factorizes into (cid:81)L 2.2. Notably, the loss is only computed over mask tokens where is not dependent on θ when i=1 pθ(X = [M ]. 0I, P, Xt) following the formulation in Section 0I, P, Xt) = [M ], because pθ(X Complementary Masking. Prior diffusion language models (e.g., LLaDa, Dream) apply stochastic estimator to Equation 2, masking tokens independently across samples in batch. However, for visionlanguage tasks, this leads to inefficiencies: (1) only 50% of tokens contribute to the loss on average, and (2) critical answer tokensoften short and sparse in vision tasks like VQAmay not be masked, resulting in misaligned gradients for the vision encoder. For example, in \"The answer is dog.\" the key token \"dog\" might be unmasked in Xt and thus ignored in loss computation. To address this, we introduce complementary masking: for each sample, we generate two masked versions Xt and with disjoint corrupted spans (e.g., one masks \"The [M] [M] dog .\", the other \"[M] answer is [M] [M]\", ensuring all tokens are eventually used in training and improving sample efficiency and gradient flow. When computing the loss over Xt and , we copy the encoded vision embeddings to further boost training efficiency. This process is illustrated in Figure 3a. 3.3 Inference Algorithms At inference time, we first create sequence of mask tokens as X1, where is the response generation length. Then we gradually unmask them through discrete timestamps t1..tK, where t1 = 1 and tK = 0, until we reach clean, mask-free sequence X0. At each timestamp ti, we sample fully unmasked sequence through pθ(X0Xti) and re-mask ti+1 tokens to obtain Xti+1 . Both and are hyperparameters for inference. Additionally, we define as \"fraction of the number of functional evaluations (NFE)\" to measure sample efficiency. For example, when NFE = 100%, the diffusion model generates one token per forward pass; at NFE = 50%, it generates an average of two tokens per forward pass. Overall, the inference process of LaViDa is similar to prior DMs such as LLaDa, with two key exceptions: Prefix-DLM. While DLMs theoretically offer superior speedquality tradeoffs at inference, they are often slower than AR models in practice because they cannot leverage KV caching [56]. This issue is particularly evident for multimodal prompts containing many visual tokens. To avoid recomputing keys and values for the visual embeddings and text prompts, we propose novel Prefix-DLM scheme inspired by the autoregressive prefix-LM. Prefix-DLM adopts specialized attention mask in which visual and prompt tokens can only attend to other visual and prompt tokens, while answer tokens can attend to all tokens. Figure 3b illustrates this setup. With this design, we can cache the keys and values of the visual and prompt tokens. Empirically, this leads to speedup of up to 3.9 on COCO captioning tasks. Further details are provided in Section 4.5. Schedule Shift. Diffusion language models (DLMs) allow trading speed for quality via the number of discretization steps K. Prior models like LLaDa and Dream use linear schedule, unmasking tokens uniformly over [0, 1]. However, we find this leads to performance degradation at low sampling steps. Inspired by SD3 [21], we adopt shifting schedule: = sα(ti) = αti 1 + (α 1)ti (3) = 1. When α < 1 (we use α = 1 Here, sα(t) is monotonic map with t0 = 3 ), the schedule is convexleading to more tokens being unmasked earlier. We found that this setup outperforms alternatives. Notably, this conclusion differs from that of continuous diffusion models like SD3 and previous masked diffusion models for image generation [11], which showed concave schedules (α > 1) are more preferable. We ensure at least one token is unmasked per step. Further details are provided in Section 4 and Appendix A.2. 0 = 0, tK = t"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup At high level, LaViDa employs two-stage training process. In the pretraining phase (stage-1), only the projector is updated to align the visual embeddings with the latent space of the DLM. In the finetuning phase (stage-2), we jointly train all components end-to-end for instruction-following. Additionally, we further finetune the stage-2 model for additional steps and obtain two specialized models for reasoning and text-infilling tasks (LaViDa-Reason and LaViDa-FIM). We provide more details of these specialized models in Section 4.3 and 4.4. We use 558K image-text pairs as our stage-1 data, and 1M visual instruction-following examples as our stage-2 data. Further details on the dataset and training setup are provided in Appendix B. We evaluate LaViDa on wide range of vision-language tasks. Unless otherwise stated, we report results obtained using the stage-2 model with LLaDa-8B as the language backbone. We use lmms-eval package [82] for evaluation and set the sequence length to be the maximum generation length used for evaluating AR models. We set = L, or NFE=100% by default. Results under differet NFE are explored in Section 4.5 and Appendix B.3 and B.4. 4.2 Main Results Table 1 reports the results of LaViDa using LLaDA-8B (LaViDa-L) and Dream-7B (LaViDa-D) as the language backbones on vision-understanding tasks. We compare with several open-source, open-data models with similar data sizes and parameter counts: LLaVa-1.6-7B [44, 42] and OpenLLaVa-Next-Llama3-8B [15]. We also include comparisons with frontier open-sourced models of similar size that are trained on larger datasets, namely LLaVa-OneVision-7B [35], Qwen2.5-VL-7B [5], and InternVL-38B [84]. LaViDa demonstrates competitive performance across wide range of tasks spanning General, Reasoning, OCR, and Science categories. In general vision-language understanding, LaViDa-L achieves the highest score on MMMU [79] (43.3), outperforming all comparable models. LaViDaD also ranks second on several benchmarks in this category. For reasoning tasks, both models surpass similarly scaled baselines on math-heavy and spatially grounded benchmarks. In Science, LaViDa achieves the best and second-best scores on ScienceQA [51] (81.4 and 80.2, respectively) while performing on par with Open-Llava-Next on AI2D [31], complex diagram-based benchmark. Finally, in OCR LaViDa shows competitive performance but lags behind some of the latest AR models. This gap is primarily due to our use of average pooling for vision token compression, which leads to the loss of fine-grained spatial information. While this was necessary trade-off given our limited compute budget, it poses challenges for tasks requiring precise text recognition and layout understanding. These results highlight the strength of LaViDa, demonstrating that diffusion-based approaches can scale competitively with AR models while achieving robust performance across wide range of vision-language tasks. 6 Table 1: LaViDas Performance on Visual Understanding Tasks. We report results on General, Reasoning, OCR, and Science Benchmarks. Dashes () denote results not reported. Open-Lnxt: Open-LLaVA-Next-Llama-3-8B; L-OV: LLaVA-OneVision-7B; Qwen2.5: Qwen2.5-VL-7B; Intern3: Intern-VL3-8B. LaViDa-L LaViDa-D LLaVa-1.6 Open-Lnxt L-OV Qwen2.5 Intern3 #Params #Images (Pretrain) #Images (SFT) MME-P [23] VQAv2 [24] MMBench [46] MMMU [79] MME-C [23] MathVista [50] MathVerse [83] MathVision [73] ScienceQA [51] AI2D [31] TextVQA [64] DocVQA [54] ChartQA [52] InfoVQA [53] 8B 0.6M 1.0M 1365.6 72.2 70.5 43. 341.1 44.8 27.2 20.4 80.2 70.0 56.3 59.0 64.6 34.2 7B 0.6M 1.0M 1463.5 75.2 73.8 42.6 378.9 42.1 24.1 19.4 81.4 69.0 57.1 56.1 61.0 36.2 7B 0.6M 0.7M General 1519.3 80.1 54.6 35.1 Reasoning 322.5 34.4 14.3 12.8 Science 73.2 66.6 OCR 64.9 74.4 54.8 37.1 8B 7B 7B 0.6M 0.6M >7M 8B - 1.0M 7.2M 2M 21.7M 1610.9 71.9 74.4 37.4 336.8 41.8 14.6 14.1 78.3 70.2 61.7 69.9 69.7 36. 1580.0 - 80.8 48.8 418.0 63.2 - - 96.0 81.4 - 87.5 80.0 68.8 - - 83.5 58.6 - 68.2 - - - 83.9 84.9 95.7 87.3 82.6 - - 83.4 65.6 - 75.2 - - - 85.2 80.2 92.7 86.6 76. Table 2: Performance of Specialized stage-3 Models. We report (a) performance of LaViDa-Reason on math reasoning tasks and (b) performance of LaViDa-FIM on constrained poem completion task. (a) Math benchmark results after long CoT distillation. (b) Sentence and sample-level constraint satisfaction for poem completion tasks. M.Vista M.Verse M.Vision LaViDa LaViDa -Reason Rel. 44. 45.2 +1% 27.2 29.3 +8% 20. 24.0 +18% Sentence Sample LaViDa LaViDa-FIM LLaVa-1.6-7B Qwen2.5-VL-7B 1.00 1.00 0.41 0. 1.00 1.00 0.37 0.16 4.3 Reasoning Distillation Prior work has distilled LLMs [69] and VLMs [18, 72] using long chain-of-thought (CoT) data to elicit strong reasoning capabilities [25]. In the same spirit, we study the reasoning abilities of LaViDa by conducting additional stage-3 training using 19.2K CoT examples distilled from VLRethinker-7B, strong reasoning model. We refer to the finetuned model as LaViDa-Reason. We evaluate it on MathVista [50], MathVerse [83], and MathVision [73] with CoT generation, comparing against the stage-2 results without CoT. We set the maximum generation length = 1024 for these tasks. We report these results in Table 2a. We find that LaViDa-Reason outperforms LaViDa across all benchmarks, with the most significant gains observed on the most challenging MathVision reasoning dataset (+18% relative improvement). Further details are provided in Appendix B.3. 7 (a) Text-Infilling (b) Speed vs. Quality Tradeoff Figure 4: We showcase the advantages of LaViDa over AR VLMS in terms of controllability and speed. (a) Qualitative comparison on constrained poem generation between LaViDa /LaViDa-FIM and AR models. LaViDa variants successfully satisfy line-level constraints and adapt token length per line, unlike AR baselines. (b) Speedquality tradeoff for image captioning on COCO 2017. By adjusting the number of discretization steps (K), LaViDa offers tunable balance between latency and output quality (CIDEr score). 4.4 Text Infilling LaViDa offers strong controllability for text generation, particularly in text infilling. Given draft of tokens containing LM masks, we jump to timestep = LM and run standard inference to reach = 0. This directly replaces LM masks with LM tokens. However, in practice, the intended completion may require fewer tokense.g., \"There is [M][M][M][M] in the image\" might become either \"dog\" or \"traffic light\". To allow variable-length completions, we conduct an additional stage-3 training using 20% subset of stage-2 data and refer to this model as LaViDa-FIM. During training, we insert random-length [S]...[S][FIM] sequences mid-text. At inference, we append [FIM] to masked segments (e.g., [M][M][M][M][FIM]) to signal flexible termination. The model can then generate completions like [dog][S][S][S][FIM] or [traffic][light][S][S][FIM]. While FIM objectives are often discussed in the context of language tasks (e.g., code completion) [62, 7], they are equally relevant to multimodal applications. Figure 4a shows qualitative results on constrained poem generation, where the models generate poem describing an image, with each line starting with specific syllables. Both LaViDa and LaViDa-FIM complete the task successfully, unlike AR models. Notably, LaViDa-FIM adapts token counts per line. Table 2b shows quantitative results over 100 samples: both LaViDa variants achieve 100% constraint satisfaction, while AR baselines remain below 50%. Additional results on other text-infilling use cases are provided in Appendix B.2. 4.5 Speed vs. Quality Trade Off LaViDa offers convenient way to achieve speed-quality tradeoffs by controlling the number of discretization steps K. We compare the performance on image captioning with 500 images from the COCO 2017 val dataset [41] with varying K. We set the maximum generation length to 32, and experimented with {32, 24, 16, 8}, or equivalently, NFE {100%, 75%, 50%, 25%}. We report the average latency per image measured on single A5000 GPU and the CIDEr score in Figure 4b. At NFE=100%, LaViDa achieves higher CIDEr score than AR baselines but is slightly slower. At NFE=75% and NFE=50%, LaViDa is faster than the AR baselines and achieves better quality. At NFE=25%, it is significantly faster but trails in performance. This indicates that LaViDa can flexibly adjust its inference speed based on application needsallowing users to trade off generation latency and output quality depending on their specific requirements. Effect of KV Cache. The speed of LaViDa relies on our proposed Prefix-DLM setup, which allows us to cache the keys and values of visual and prompt tokens [3]. In Table 3a, we compare the speed and sample quality between the proposed Prefix-DLM step and the uncached full-attention-mask step used in prior works like LLaDa. We find that Prefix-DLM significantly reduces latency and achieves maximum speedup of 3.9, with marginal performance cost. These experiments are performed using our stage-2 model, which is trained on full-attention mask. We discuss training with customized 8 masks and customized kernels in Appendix B.4. In short, we found that these alternatives lead to considerable training overhead while offering little benefit. Table 3: Speed-Quality Tradeoff. (a) We compare COCO image captioning performance with and without Prefix-DLM caching. Latency is measured in seconds/sample. (b) We compare the performance of different schedules at different NFEs. (a) Effect of KV Cache (b) Effect of timestep shifting"
        },
        {
            "title": "NFE",
            "content": "CIDEr Latency Full-DLM 100% Prefix-DLM 100% 50% Full-DLM 50% Prefix-DLM Open-Lnxt-8B 121.0 117.3 118.6 114.8 111.8 7.65 1.93 4.13 1.23 1."
        },
        {
            "title": "NFE",
            "content": "cosine linear α=3 α=31 CIDEr 25% 50% 75% 100% 87.7 84.9 48.7 101.1 102.2 105.2 74.7 114.8 110.8 108.6 92.4 117.3 117.3 117.3 117.3 117. Noise Schedule. To study the effect of the proposed time-step shifting schedule, we compare the performance of different schedules with NFE {100%, 75%, 50%, 25%}. We compare the proposed time-step shifting with α = 3, and α = 31, as well as linear and cosine schedule baselines. We report results on COCO image captioning [41] in Table 3b. The convex schedule with α = 31 works the best. We also observe similar behaviors when conducting CoT inference using LaViDa-Reason on MathVision [73] dataset. At NFE=50%, α = 31 achieves an accuracy of 21.05, which is 30% higher than 16.12 achieved by linear schedule. We provide results on MathVision in Appendix B.3. 4.6 Ablation Studies We conducted thorough ablation of various design choices. In the main paper, we discuss the effect of complementary masking and input image resolution. We provide further discussion of other design choices in the Appendix B.5. We conducted these experiments using 200k subset of our (stage-2) training data. We report results in Tables 4a and 4b respectively. Table 4a shows that our proposed complementary masking scheme leads to considerable improvements across all benchmarks, most notably, complementary masking leads to relative improvement of 67% on ScienceQA [51] with affordable compute overhead during the training (8% slowdown). Table 4b shows that high-resolution input improves overall performance, with the gain on OCR tasks being more pronounced than generic vision tasks (e.g., VQAv2). We did not use average pooling for the low-resolution setup. We provide additional details in the Appendix B. Table 4: Ablation Studies. We study the effect of (a) complementary masking and (b) image resolution. For (a), we also report the wall clock time of 1000 training steps with batch size of 128. (a) Effect of complementary masking. (b) Effect of image resolution. w/o Comp.M. w/ Comp.M. MME MathVista ScienceQA MMMU Runtime 260.00 28.40 48.74 38.56 8.2 hr 297.00 33.40 81.49 41.78 8.9 hr TextVQA DocVQA ChartQA InfoVQA VQAv2 7682 48.40 43.22 42.20 26.48 65.92 55.65 58.72 57.70 36.23 66."
        },
        {
            "title": "5 Conclusion",
            "content": "In conclusion, we propose LaViDa, the first family of vision-language models based on DMs. To address various challenges, we introduce several novel training and inference techniques, including complementary masking, Prefix-DLM cache, and timestep shifting. Through extensive experiments, we show that these techniques significantly outperform naive adaptation of DMs for visual tasks. 9 Using comprehensive evaluation suite, we demonstrate that LaViDa achieves competitive performance compared to AR models trained under similar settings, while offering unique advantages such as speedquality tradeoffs and controllability via text infilling. Our work proves that LaViDa can be powerful alternative to exitsing AR VLMs, extending the prior success of DMs in the language domain to the vision space."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [2] Anthropic. Claude 3.5 family. https://www.anthropic.com/claude/sonnet, 2024. Accessed: 2025-05-15. [3] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [4] Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Tran, and Mehran Kazemi. Smaller, weaker, yet better: Training llm reasoners via compute-optimal sampling. arXiv preprint arXiv:2408.16737, 2024. [7] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022. [8] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. [9] Daniele Rege Cambrin, Gabriele Scaffidi Militone, Luca Colomba, Giovanni Malnati, Daniele Apiletti, and Paolo Garza. Level up your tutorials: Vlms for game tutorials quality assessment. arXiv preprint arXiv:2408.08396, 2024. [10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [11] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [12] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language model, 2024. [13] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 10 [15] Lin Chen and Long Xing. Open-llava-next: An open-source implementation of llava-next series for facilitating the large multi-modal model community. https://github.com/ xiaoachen98/Open-LLaVA-NeXT, 2024. [16] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [17] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [18] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [20] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [22] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022. [23] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [24] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [26] Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and highresolution images. In European Conference on Computer Vision, pages 390406. Springer, 2024. [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [28] Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, and Ponnuthurai Suganthan. Unified discrete diffusion for simultaneous vision-language generation. arXiv, 2022. [29] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 11 [30] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [31] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [32] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [34] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [35] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [36] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. [37] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. arXiv preprint arXiv:2412.01169, 2024. [38] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. [39] Xiaobo Liang, Zecheng Tang, Juntao Li, and Min Zhang. Open-ended long text generation via masked language modeling. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 223241, 2023. [40] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [42] Haotian Liu. Llava v1.6 vicuna-7b. https://huggingface.co/liuhaotian/llava-v1. 6-vicuna-7b, 2023. Accessed: 2025-05-14. [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [44] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 12 [46] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [47] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [48] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. [49] Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, and Kilian Weinberger. Latent diffusion for language generation. Advances in Neural Information Processing Systems, 36:5699857025, 2023. [50] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [51] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [52] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. [53] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [54] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [55] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. [56] Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. [57] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [58] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [60] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 13 [62] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [63] Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. [64] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. [65] Alexander Swerdlow, Mihir Prabhudesai, Siddharth Gandhi, Deepak Pathak, and Katerina Fragkiadaki. Unified multimodal discrete diffusion. arXiv preprint arXiv:2503.20853, 2025. [66] Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, and Kiyoshi Tanaka. Improved artgan for conditional synthesis of natural image and artwork. IEEE Transactions on Image Processing, 28(1):394409, 2019. [67] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36:1608316099, 2023. [68] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [69] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [71] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Q-vlm: Post-training quantization for large vision-language models. arXiv preprint arXiv:2410.08119, 2024. [72] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [73] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [74] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. [76] Chenyu Yang, Xuan Dong, Xizhou Zhu, Weijie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, and Jifeng Dai. Pvc: Progressive visual token compression for unified image and video processing in large vision-language models. arXiv preprint arXiv:2412.09613, 2024. [77] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. [78] Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. 14 [79] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. [80] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [82] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. [83] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. [84] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [85] Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using single non-autoregressive transformer. arXiv preprint arXiv:2401.04577, 2024."
        },
        {
            "title": "A Additional Technical Details",
            "content": "A.1 Formulation of Discrete Diffusion Models (DMs) In this section, we provide detailed review of the formulation of Discrete Diffusion Models (DMs) briefly described in 2.2. Given text sequence of tokens X0 = [X 1 0 , the forward process q(XtXs) gradually coverts it to sequence of mask tokens [M ], denoted by X1 = [X 1 1 , ...X 1 , 2 1 ] over the continuous time-interval [0, 1], with 1 0. Formally, this process is defined as 0 , ...X 0 , 2 q(X X s) = which has the marginal (cid:26)Cat(X Cat(X ; M), ; 1t 1s Xi + ts 1s M), if if = = q(X X 0) = Cat(X t ; (1 t)Xi 0 + tM) (4) (5) where Cat(.) denotes categorical distribution and M, Xi are probability vectors. MDLM [63] showed that the posterior of the reversal process p(XsXt, X0) can be simplified into the following 0, Xi p(X sX , 0) = (cid:26)Cat(X Cat(X s; Xi t), ; ts Xi 0 + M), if if = = (6) In practice, we use the categorical distribution induced by the neural networks prediction pθ(X in place of Xi 0 the sample from the reverse process, which gives the following parametrization 0Xt) pθ(X sXt) = (cid:26)Cat(X Cat(X s; Xi t), ; ts pθ(X 0Xt) + t M), if if = = (7) Inference Algorithm. Given target length and discretization steps t0, t1...tK where t0 = 0 and tK = 1, we first initialize 1:L 1 = , then use Equation 7 to repetitively sample tK pθ(Xtk1 Xtk ) until we reach t0 = 0. In this process, we also assume pθ(Xtk1Xtk ) factorize into (cid:81)L = 1:L Xtk ) following previous works [56, 63, 48]. i=1 pθ(X tk1 Training Objective. Recall that the training objective of DMs introduced in Section 2.2 is formulated as LDLM = Et,X0,Xt (cid:21) log pθ(X0Xt) (cid:20) 1 (8) where pθ(X0Xt) factorizes into (cid:81)L 0Xt) has closed form solution that depends only on Xt when = . Intuitively, this comes from the fact that in the reverse process, once token Xi changes from mask token to clean text token, it stays the same thereafter. Based on this observation, we can remove the terms that does not depend on the neural network θ from the learning objective, giving us the following loss 0Xt). However, Equation 6 shows that pθ(X i=1 pθ(X LDLM = Et,X0,Xt 1 (cid:88) t =M log pθ(X 0Xt) (9) Hence, the loss is only computed over the masked indices in Xt. A.2 Timestep shifting Schedule Given sequence of tokens, and timestep [0, 1], in expectation Xt contains tL masked tokens. In practice, we implement Equation 7 such that there are exactly tL masked tokens and tL clean text tokens at timestep during the sampling process. 16 (a) Discretized Schedules at Different K. (b) Different Choice of Continuous Schedules . Figure 5: Visualization of Schedules (a) We visualize discretization results of the same continuous schedule (dashed line) under different numbers of sampling steps {16, 24, 32} at = 32. (b) We visualize various choices of continuous schedules: Shift(α = 31), Shift(α = 3), Cosine, and Linear. Given fixed number of sampling step K, we define the canonical discretization as ti = for = 0, 1..K, with t0 = 0 and t1 = 1. This forms uniformed sampling schedule, where roughly fixed mount of tokens is unmakes at each sampling step. Any other schedule can be defined as = (ti) where (.) is monotonic function such that (0) = 0 and (1) = 1. When (.) is convex, the slope will be steeper when get closer to 1 , indicating that more tokens are decoded in earlier sampling steps. By contrast, when (.) is concave, the slope will be steeper when get closer to 0, indicating that more tokens are decoded in later sampling steps. Choice of Schedule. We explored wide range of choices for the continuous schedule (.). The timestep shifting schedule is family of schedule defined as (t) = sα(t) = αt 1 + (α 1)t (10) where α is hyperparameter. When α < 1, the schedule is convex. When α > 1, the schedule is concave. The cosine schedule is defined as (t) = 1 1 2 (1 + cos(πt)) (11) The linear schedule is just the identity function (t) = t. We visualize these choices in Figure 5b. Rounding in Discretization. In principle, we can pick any (.). However, given particular choice of and K, if (tk)L and (tk1)L yields the same integer, then no tokens are unmasked when we compute pθ(Xtk1 Xtk ). Hence, the actual number of function calls to the model θ may be less than, depending on the choice of (.). To address make the sampling compute cost more predictable and allow for fair comparison across different schedulers, we augment all choices of (.) to K(.) such that K(tk1)L < K(tk)L (i.e. at least one token is decoded at each step). Note that in the sampling process, the exact real value of K(tk) does not matter as long as it does not change K(tk)L. Hence, we can parameterize the sampling process in an alternative manner using sequence of integers 1 = L. Formally, set by solving the following optimization objective = K(tk)L, with 0 = 0 and 17 min 0:K }{0,1..L} {F K (cid:88) k=0 (cid:13) (cid:13)F (tk)L(cid:13) 2 (cid:13) subject to Fk1 < Fk, for = 1, 2, . . . , K"
        },
        {
            "title": "F K\nF K",
            "content": "0 = 0 1 = We visualize such examples in Figure 5a. We set = 32 and {16, 24, 32}. When = 32, is effectively linear schedule, since the only schedule with 32 steps that satisfy the constraint Fk1 < Fk is uniform schedule where we unmask exactly one token per step. As reduces to 24 and 16, we see the discretized schedule becomes closer to the continuous scheduler (visualized in dashed line). A.3 Padding We follow the design of LLaDa [56] and apply the loss function to both standard text tokens and padding tokens. AR models typically do not compute the loss over padding tokens. However, when sampling from DMs, we have specified generation length L. In the generation process, we unmask mask tokens to non-mask tokens. Since the length of the desired answer may not be exactly tokens, the model will generate padding tokens. To achieve this capability, we pad the sequences during the training, and apply the loss on the padding tokens following LLaDa."
        },
        {
            "title": "B Additional Experiment Details and Results",
            "content": "B.1 Setup In this section, we document the detailed training setup, including data, hyperparameters and compute used for the main experiments. Additional details about Prefix-DLM Cache can be found in B.4. Additional details about stage-3 math reasoning experiments can be found in B.3. Training Data Composition. For the pretraining phase, we use LCS-558K [45] consists of 558K image-text pairs. For the finetuning phase, we mostly use the dataset released by Open-LLaVa-Next [15]. We made some small adjustments to the weight of each data source and increased the weight of some QA dataset. This is used to compensate the fact that our model only learns from randomly chosen round at each training step for multi-round QA data. We document the precise dataset composition of our stage-2 training in Table 5. Table 5: Compositio of Stage-2 Training Data. We report the data sources and sample sizes used to compose the Stage-2 finetuning data. Data Source COCO[41] ALLaVA-VFLAN[12] Visual Genome[34] OCR VQA[55] GeoQA+ [13] Share-TextVQA [14] Size 349,860 202,966 86,417 80,000 72,318 Data Source GQA[29] Synthdog-En[32] TextVQA[55] ChartQA [52] AI2D[31] Web-Celebrity[15] Size 72,140 29,765 21,953 18,317 12,413 Data Source DocVQA[54] DVQA[30] SA-1B[33] LLaVA-150K [45] WikiArt[66] 500 Web-Landmark[15] Size 10,211 10,000 8,997 2,988 500 500 Training Hyperparameters. We use AdamW optimizer with learning rate of 5e-3 with cosine decay schedule for all experiments. For pretraining (Stage 1), we adopted global batch size of 256 and trained for 1 epoch. For finetuning (Stage 2), we adopted global batch size of 512 and trained for two epochs. Compute Used. We used mixture of A100s and A6000s for training experiments and A5000 for evaluations and inference speed benchmarks. Because of the memory constraint, we set the per GPU 18 batch size to 8 on A100s and 4 on A6000s. We adjust the gradient accumulation steps accordingly so that the global batch size is always 256 for the pretraining and 128 for the finetuning stage. Evaluation Setup. We implement our evlaution using LMMS-Eval [82] library. We use the default prompt provided by the library for all benchmarks. We report the split used and the generation length in Table 6. Table 6: Evaluation Setup. We report evaluation split and generation length used to produce results of Table 1 in the main paper. *We use generation length of 100 for LaViDa and generation length of 1024 for LaViDa-Reason."
        },
        {
            "title": "Split",
            "content": "MME-P VQAv2 MMBench MMMU MME-C TextVQA DocVQA test val dev val test val test testmini_format testmini_vision_dominant"
        },
        {
            "title": "100 MathVista*\n16 MathVerse*\n100 MathVision* mathvision_testmini\n16\n16\n16\n32",
            "content": "scienceqa-full test test test ScienceQA AI2D ChartQA InfoVQA 100 100 100 16 16 16 32 B.2 Text-Infilling In this section, we provide additional qualitative results of the text-infilling capability of LaViDa. We visualize the results in Figure 6. In the first example, we ask the model to extract multiple attributes from the image in JSON format. In the second example, we ask the model to edit sentence based on the image. This is achieved by deleting the original sentence and inserting mask tokens. In the final example, we ask the model to complete movie script based on the image prompt. LaViDa was able to successfully complete these tasks. While it may be possible to achieve similar results using an AR model, they require careful prompting. By contrast, as shown in Figure 6, using diffusion model for text-infilling is more straightfoward. B.3 Math Reasoning In this section, we provide additional training setup for LaViDa-Reason and provide additional experiment results on the speed-quality tradeoff on math reasoning. Data and Training Setup. In 4.3, we train LaViDaon long chain-of-thought (CoT) data to get LaViDa-Reason. Specifically, we choose strong 7B reasoning model, VLRethinker-7B [72] as teacher model to generate the long reasoning traces. Further, we choose their own ViRL-39K data that contains (image, question, final answer). Subsequently, we generate the CoT and predicted final answer from the teacher model and filter the ones that lead to the correct final answer [80, 6]. This led to the creation of the final dataset of size 19.2K.1 In particular, we finetune LaViDaon this data for 5 epochs using the identical training setup as stage-2 (e.g., batch size, learning rates) and chose the checkpoint that achieves the best performance on MathVision (testmini). We observe that the same checkpoint achieved good performance on the MathVerse and MathVista dataset too. During inference, we set the generation length to 1024 since LaViDa-Reason synthesizes long chain-of-thoughts for problem-solving. Speed-Quality Tradeoff. In the main paper, we reported the speed-quality tradeoff results on COCO image captioning and discovered that the convex schedule works the best. We conducted similar study on LaViDa-Reason for CoT inference on MathVision dataset. We report these results in Table 7. Overall, the conclusion on CoT math reasoning task is similar to that on image captioning task, with the convex schedule performing the best across different choices of sampling steps. 1We will release this data in the camera-ready version. 19 Figure 6: Additional Qualitative Results for Text Infilling. We showcase several useful applications of text-infilling capabilities. 20 Table 7: Effect of different schedule on MathVision dataset. We study the effect of different schedules on MathVision dataset using Chain-of-Though inference with LaViDa-Reason."
        },
        {
            "title": "NFE",
            "content": "25% (256 steps) MathVision Acc 50% (512 steps) 100% (1024 steps) cosine linear α=3 α=31 8.55 10.86 5.59 12.5 13.49 16.12 8.88 21. 24.02 24.02 24.02 24.02 Figure 7: Visualization of Different Choices of Attention Mask at Inference Time. represents the image embeddings, represents the prompt tokens, and Xt represents the partially masked answer tokens. Each row represents query and each column represents key. Colored region indicts tokens queries and keys can interact with each other. B.4 Prefix-DLM In this section, we discuss several alternatives to our prefix-DLM setup that we explored and document the experiment results. Inference Algorithm. Autoregressive models employ causal attention mask. Because of this, they can leverage KV cache for effective inference. By contrast, discrete diffusion models (DMs) used full attention mask. While DMs can decode multiple tokens in parallel, it cannot leverage attention mask for fast inference. Prefix-DLM combine the best of both worlds by introducing prefix attention mask such that the queries of image embeddings and text prompts can only interact with keys and values of image embeddings and text prompts, but not the keys and values of answer tokens. Through this mechanism, we can leverage the KV cache for the image embeddings and text prompts. In vision-language applications with long context (900+ vision tokens per image), this saves lot of compute at inference time, while preserving the full bidirectional context. alternative to our prefix-DLM was the recently proposed semi-autoregressive Block-Diffusion [3], which uses block-wise causal attention mask. In this setup, the input sequece are chunked into sequence of fixed length blocks, each containing LB tokens. token in Block can see all tokens in the past and current Block with i, but cannot see all future blocks Block with > i. While this design allows it to leverage block-wise KV cache, it limits the bi-directional context to at most LB tokens in the future, which is undesirable for tasks like text-infilling. Additionally, because of its semi-autoregressive nature, when we are generating Block i, we must see all mask-free past Blocks with < i. Hence, the naive training algorithm can mask at most LB tokens in each training sample (i.e. the last block), which is inefficient. To address this issue, customized attention kernel was developed to allow for parallel training. However, this leads to considerable training overhead. By contrast, Prefix-DLM can leverage the KV cache while having the full bidirectional context. It also does not require any specialized training algorithms, since we adopt it as pure inference technique of DMs. We visualize the different choices of attention masks in Figure 7. We also compare the properties of different choices in 8. Training Algorithm. We adopt Prefix-DLM as pure inference algorithm. Our training process is identical to that of standard DM with full attention mask. We made this choice mostly because of 21 Table 8: Comparison of Different Choices of Attention Mask. We compare properties of different choices of attention masks. The desired properties are highlighted."
        },
        {
            "title": "Method\nAR\nDM",
            "content": "Attn. Mask Causal Full Block-Diffusion Block-Causal Prefix-DLM"
        },
        {
            "title": "LaViDa",
            "content": "Bi-Direction Context KV Cache Training Overhead Yes No Yes Yes None Full Seq. Within Block Full Seq."
        },
        {
            "title": "No\nNo\nYes\nNo",
            "content": "efficiency reasons. We also explored adopting the prefix-DLM attention mask during the training (Prefix-DLM-FT) with four different but equivalent implementations. Generally, these four implementations are categorized into two classes: Prefix-DLM-FT1 and Prefix-DLM-FT2. We visualize these setups in Figure 8. Recall from Section 3.2 that we adopted complementary masking scheme, wherein given each triplet of image I, prompt and answer X, we create two versions of partially masked answer Xt and with complementary masking in order to utilize all tokens in the clean answer X. Empirically, this is achieved by copying the prompt embedding and concatenate Xt and to different copies respectively (Left Column of Figure 8). By default, we use the full attention for both copies. We can adopt the Prefix-DLM attention mask during the training for each copy individually. We call this setup Prefix-DLM-FT1 (Middle Column of Figure 8). Alternatively, we can concatenate I, P, Xt, into single long sequence and manipulate the attention mask such that queries of I, can see keys of I, , queries of Xt can see keys of I, P, Xt and queries of . We call this setup Prefix-DLM-FT2 (Right Column of Figure 8). can see keys of I, P, For each of the two variant Prefix-DLM-FT1 and Prefix-DLM-FT2, we designed two concrete implementations. The first set of implementations (Prefix-DLM-FT1-Mask,Prefix-DLM-FT2-Mask) simply construct 2D attention mask of size for each sample, where is the sequence length, and pass it to the torch SDPA kernel. The second variant (Prefix-DLM-FT1-Flex,Prefix-DLM-FT2Flex) merely constructs an integer tensor of size 3 per sample, containing the length of I, P, X. We then use customized flex_attention module [20] to implement the attention mask implicitly based on the length of I, P, X. Notably, these four variants of Prefix-DLM-FT generates identical loss value and will produce exactly the same training dynamics (disregarding small numerical differences between implementations). The only difference is the efficiency. Hence, we only benchmarked the performance and performed the full training using the most efficient version. We report the speed benchmark in Table 7. Overall, even the fastest finetuning version is 62% slower than the full attention mask baseline, suggesting high overhead caused by batch-dependent masking strategies during training. We also report the model performance after 1,500 training steps (roughly 200k samples from the training data) in Table 10. Overall, Prefix-DLM and Prefix-DLM-FT has mostly identical performance, with Prefix-DLM having small lead over many tasks. Because of this, we consider the 62% overhead as unacceptable and opt for training procedure using the full attention mask. This also gives user an additional dimension for speed-quality tradeoff: they can disable Prefix-DLM cache and use the full attention mask at inference to achieve slightly better performance (Results shown in Table 3a in main paper). Table 9: Training Speed of Different Variants of Prefix-DLM. We report the average training speed of different setup with batch size of 128 on 8 A6000 GPUs. Method Prefix-DLM Prefix-DLM-FT1-Mask Prefix-DLM-FT2-Mask Prefix-DLM-FT1-Kernel Prefix-DLM-FT2-Kernel Speed(s/training step) 37.2 67.3 82.6 60.2 74.4 22 Figure 8: Visualization of Training Strategies for Prefix-DLM. Given each triplet of image I, prompt and answer X, we create two versions of partially masked answer Xt and with complementary masking.(Left) By default, we construct two sequence and apply full attention mask. (Middle) Prefix-DLM-FT1 applies prefix attention mask to each copy independently. (Right) Prefix-DLM-FT2 combines I, P, Xt, into single sequence and manipulate the attention mask to achieve an equivalent effect to Prefix-DLM-FT1. Table 10: Performance of Prefix-DLM and Prefix-DLM-FT across benchmarks. We compare the performance of two variants after 1,500 steps of training. MMMU VQAv2 MME M.Vista M.Verse ScienceQA AI2D Prefix-DLM Prefix-DLM-FT 40.56 40.10 63.26 61. 286.79 290.71 33.60 35.00 19.67 18.15 80.36 80.15 64.31 64.15 B.5 Ablation Studies We conduct additional ablation studies over the choice of vision encoders. We experimented with SigLip[81], CLIP [59], and MetaCLIP[75]. We report the model performance after 1,500 training steps (roughly 200k samples from the training data) in Table 11. Overall, SigLip achieves the strongest overall performance, with notably gains in VQAv2, ScienceQA, and AI2D."
        },
        {
            "title": "C Additional Backgrounds",
            "content": "In this section, we provide additional discussions with relevant works not covered in the main paper. Masked Generative Models. Masked generative modeling has long history before the recent advancements of DMs. Earliest works such as BERT[19] and MAE[27] adopt the masked generative modeling objective as pretraining objective to learn rich text and vision features. They mainly concern the utility of learnt features to downstream perception and understanding tasks, instead of the generation capability of the model. series of subsequent works use mask modeling to build generative models for images [11, 10], texts [39] and audio [85]. Compared with these early works relying on ad-hoc sampler designs, recent works on DMs [48, 63] provided principled way for training and sampling from masked generative models. Multi-Modal Diffusion Models. Several works have explored to build multi-modal diffsion models with vision-language capabilities. CoDi [67] and OmniFlow [37] build continuous diffusion model over latent text embedding space and use autoregressive decoder to decode the generated latent mebeddings to actual texts. UniD3 [28] and UniDisc [65] build discrete diffusion models for 23 Table 11: Ablation Studies on the Choice of Vision Encoders. We compare the performance of models with different vision encoders after 1,500 steps of training."
        },
        {
            "title": "SigLip\nCLIP\nMetaCLIP",
            "content": "MMMU VQAv2 MME M.Vista M.Verse 286.79 288.21 280.36 40.56 40.44 40.33 63.26 59.58 55.96 19.67 20.05 17.26 33.60 30.90 33.80 ScienceQA AI2D 64.31 59.78 62. 80.36 76.56 78.66 simultaneous text and image generation. Overall, these models have limited language generation capabilities. Their experiments on text generations are limited in scale and mostly focusing on captioning. They cannot perform more complex instruction-following and understanding tasks (e.g. reasoning) like many modern autoregressive VLMs."
        },
        {
            "title": "D Limitations",
            "content": "In this section, we discuss the limitations of LaViDa. There are mainly two limitations. Scale. While LaViDaachieves competitive results when compared against similar-sized AR VLMs trained on similar scale of data, there remain considerable gap between LaViDas performance and that of state-of-the-art open sourced VLMs such as LLaVa-OneVision and Qwen2.5-VL. These models either comes with more training data or larger model sizes. Future work should study if DMs scale well with larger model and more training data. OCR. LaViDas performance on OCR tasks are slightly worse than the baselines, this can be mainly attributed to the average pooling operation which we introduced to reduce the sequence length by compressing the visual information. Concretely, LLaVa-1.6-7B and Open-LLaVa-Next-Llama38B baseline do not adopt average pooling, and uses 2880 tokens (24 24 5 Views) to represent each image. In our setup, removing the average pooling would lead to total of 3645 tokens (24 24 5 Views) per image. Average pooling is necessary because the base model LLaDa and Dream has context length of 4096 and 2048 respectively. Without average pooling, LLaDa will not have enough context length to fit longer training samples, while Dream will not have enough context length to even fit one image. We also tried to extend the context length of these models via techniques such as rope rescaling, but achieved limited success. We experimented with extending Dream to 4096 context length and evaluate the model using the needle-in-a-haystack task at 4096 context length. We found that Dream-7B (72.4 Acc) performs worse than LLaDa-8B (91.6 Acc) and Llama-3-8B (95.4 Acc). We hope future works on DMs with longer context will provide stronger base models to finetune on vision-language applications."
        },
        {
            "title": "E Reproducibility Statement",
            "content": "We will release the model checkpoints, training and inference code, as well as the training data to reproduce our results."
        },
        {
            "title": "F Boarder Impacts and Safeguards",
            "content": "Our model may inherit the biases embedded in the base model LLaDa-8B and Dream-7B, as well as biases incorporated in the training data. Our model may also suffer from the same hallucination problem as the base model. Just as any LLMs and VLMs, there is also the risk that our model is used to generate harmful or offensive content. Overall, our model is intended to be used by researchers to build strong diffusion model for vision-language applications. We do not recommend it be used for any other purposes."
        },
        {
            "title": "G LLM Usage",
            "content": "LLM is used to generate the math reasoning date for stage-3 training. See details in B.3."
        },
        {
            "title": "H Concurrent Work",
            "content": "Concurrent to this work, MMaDa[77] unified understanding and generation modeling using DM formulation. However, they do not leverage many of the novel techniques that we propose, leading to inferior performance and slow inference speed. We list brief comparison in Table 12. Table 12: Comparison with MMaDa. We report scores on MME, MMMU, and MMB benchmarks, along with average latency for image captioning. Model MME MMMU MMB Latency (s/image) LaViDa-Dream 1463.5 1365.6 LaViDa-LLaDa 1410.7 MMaDa 42.6 43.3 30. 73.8 70.5 68.5 1.13 1.32 7."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Panasonic AI Research",
        "Salesforce Research",
        "UCLA"
    ]
}