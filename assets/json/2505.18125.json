{
    "paper_title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations",
    "authors": [
        "Alan Arazi",
        "Eilam Shapira",
        "Roi Reichart"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 2 1 8 1 . 5 0 5 2 : r TabSTAR: Foundation Tabular Model With Semantically Target-Aware Representations Alan Arazi Eilam Shapira Roi Reichart {alanarazi7, eilam.shapira, roireichart}@gmail.com Technion - IIT"
        },
        {
            "title": "Abstract",
            "content": "While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees (GBDTs). However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: Foundation Tabular Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both mediumand large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering pathway for further performance improvements."
        },
        {
            "title": "Introduction",
            "content": "In recent years, deep learning has profoundly reshaped research and practice in computer vision [48, 63, 32, 16] and natural language processing [53, 5, 76, 15, 12]. This transformation was notably accelerated by the rise of foundation models [7, 84, 4], capable of cross-modal understanding and generalization from massive pretraining across heterogeneous data sources. Importantly, they enabled an end-to-end approach that outperformed previous modular alternatives [68, 1]. Moreover, deep learning models excel at transfer learning [87], generalizing from their pretraining data to new tasks. Their strength, combined with techniques like In-Context Learning (ICL) [12] and Parameter-Efficient Fine-Tuning (PEFT) [40], has enabled rapid adaptation to new tasks with only limited labeled data. Despite this progress, deep learning has historically lagged behind gradient-boosted decision trees (GBDTs) on tabular data [11, 14, 57], in both classification and regression tasks [62, 10, 30, 52]. The heterogeneity of tabular data, which lacks the spatial locality of images or the sequential order of text, makes it more challenging for deep models to learn. Consequently, GBDTs have remained the de facto standard for tabular learning, offering strong out-of-the-box performance, computational efficiency, and built-in inductive biases (e.g., robustness to skewed feature distributions and automatic feature selection) that make them especially well-suited to heterogeneous datasets [30]. Nonetheless, GBDTs cannot be pretrained to reuse strong representations for downstream tasks. This limitation becomes critical in low-data settings like those often found in healthcare applications [50]. Crucially, they must rely on external embedding models to process unstructured data types like text and images, yielding fixed feature representations that cannot be finetuned for specific prediction task. Preprint. Under review. Table 1: binary classification toy dataset for hospital patient release outcomes. Decision is the target variable. Age (numerical), Department (high-cardinality), and Report (textual) are the features. Age Department Report Decision 45 62"
        },
        {
            "title": "Cardiology\nNeurology\nOncology\nNeurology",
            "content": "Mild chest discomfort. Complaints of headache and occasional dizziness. Completed treatment cycle without adverse reactions. Released Reports episodes of vertigo and memory lapses."
        },
        {
            "title": "Hospitalized",
            "content": "The emerging field of Tabular Foundation Models (TFMs) has begun addressing these shortcomings, introducing powerful cross-dataset learning strategies [80, 45, 37]. However, the flagship model TabPFN-v2 [37] still handles text inputs no more flexibly than conventional GBDTs. This design choice is not incidental; historically, tabular benchmarks have prioritized numerical datasets without free-text features, largely for ease of modeling and evaluation. recent study [46] of mainstream tabular datasets benchmarks [24, 22, 83, 52] found that half of these datasets are more than 20 years old, being poor representation of modern real-world data. Real-world tabular datasets often include high-cardinality1 and free-text features [13], illustrated by toy example in Table 1. In such datasets, free-text features (e.g., Report) carry rich semantic information critical for tasks like predicting whether patient will be discharged from the hospital or require continued care. Yet, most models encode them in target-agnostic manner, delegating to generic embedding that fails to capture task-specific nuances for predicting Decision. Crucially, that same embedding would have been used for different target variable (e.g., Treatment Cost). Similarly, categorical features with dozens of unique values (e.g., Department) are difficult to encode efficiently without external knowledge, making naive approaches brittle and limiting generalization. Importantly, the column names, which could guide the model toward more effective representations, are typically ignored. Addressing these limitations is crucial for developing tabular models that leverage semantic information, transfer knowledge from many datasets, and generalize across domains. In this paper, we introduce TabSTAR: novel Tabular Foundation Model with Semantically TargetAware Representations,2 designed explicitly for end-to-end handling of purely textual features. By integrating an unfrozen text encoder at its core, TabSTAR can optimize free-text feature representations, demonstrating their clear superiority over alternative frozen embedding approaches. Additionally, it introduces novel approach of target-aware tokens, which inject semantic information about the target variable as part of the input, allowing for efficient parameter sharing and resulting in an architecture with no dataset-specific parameters (see Figure 1). TabSTARs training is highly efficient3 and its performance steadily improves with more pretraining data. Empirically, TabSTAR achieves state-of-the-art (SOTA) performance on classification benchmarks containing substantial textual content, demonstrating significant advances over GBDTs and leading TFMs."
        },
        {
            "title": "2 Related Work",
            "content": "This section reviews prior work in five areas relevant to our approach. We begin with deep learning methods tailored for tabular data, which were applied to single dataset. We then discuss cross-dataset transfer learning techniques that improve generalization by leveraging related datasets. Next, we cover the field of TFMs, which aim to generalize across diverse tasks and datasets through large-scale pretraining. Finally, we review recent work on applying large language models (LLMs) to tabular data and elaborate on existing AutoML [33] multimodal solutions. Deep Learning on Single Tabular Dataset Several architectures have been proposed to enhance deep learning for tabular data [67, 44, 78, 81]. TabNet [3] and TabTransformer [42] introduced attention mechanisms into tabular deep learning, while FT-Transformer [25] and its improvement [26] jointly integrated numerical and categorical features into transformer [76]. Other novel approaches leveraged inter-example information at inference time, with SAINT [66] proposing row1High-cardinality features are categorical columns with large number of unique values. 2Code is available at https://github.com/alanarazi7/TabSTAR. 3Pretraining within 48 hours on single A40 GPU. Finetuning with PEFT for low memory footprint. 2 level attention between examples, Non-Parametric Transformers [47] processing the entire dataset, including labels, in single forward pass, and TabR [27] combining k-nearest-neighbor mechanism with traditional Multi-Layer Perceptron (MLP) architecture. Recent works such as TabM [28] and RealMLP [38] focused on refining MLPs without an attention component. Despite these innovations, single-dataset deep learning models have not yet convincingly outperformed GBDTs [62, 30, 61]. Furthermore, none of them addressed the challenge of modeling tabular datasets with rich textual features. Cross-Dataset Transfer Learning Deep learning was proven to shine when performing transfer learning in many machine learning domains [87]. Motivated by this success, [50, 85] proved that cross-dataset learning can boost single-dataset performance, but were limited to strict requirements such as partial overlap of feature names. To address this limitation, TransTab [79] integrated semantic understanding into feature tokenization, and XTab [86] pretrained transformer backbone with datasetspecific parameters, proving that pretraining contributes to stronger initialization for downstream task. Despite their small scale, these studies demonstrated cross-dataset transfer learnings potential, laying essential groundwork for the rise of TFMs. Tabular Foundation Models TFMs represent an emerging paradigm in tabular learning. While the definition is still evolving, we adopt the framing proposed by [74], which identifies key desired characteristics of TFMs: large-scale pretraining with adaptability to downstream tasks, mixed-type column support, cross-domain generalization, use of textual metadata,4 and column-order invariance. TabPFN [35] is recognized as the first TFM, and its successor TabPFN-v2 [37] currently sets the SOTA in tabular learning, becoming popular approach for TFMs [58, 20]. TabPFN-v2 was the first model to consistently outperform GBDTs on medium-sized datasets, by pretraining Bayesian Prior-Data Fitted Networks (PFNs) [55] on 130 million synthetic datasets. Using ICL at inference time, it accepts up to 10,000 examples as input and predicts without updating its weights. Nonetheless, similarly to GBDTs, TabPFN-v2 uses off-the-shelf embeddings for text features, limiting its effectiveness. CM2 [85], CARTE [45] and TP-BERTa [80] represent shift toward semantic tabular modeling, leveraging textual signals and external knowledge at greater scale. Unlike prior methods, these models transfer knowledge via language representations. CM2 pretrained on over 2,000 datasets, but did not focus on free-text features and used static word embeddings without further finetuning them. CARTE encodes tables as star-shaped graphs, jointly representing features by their names and values, and applies attention over the graph to capture contextual relations. While effective for high-cardinality features, it lacks exposure to longer free-text fields during pretraining and was proven useful mainly for small datasets. TP-BERTa adapts RoBERTa [51] with intra-feature attention and tokenization scheme that maps numerical values into discrete relative-magnitude bins, to address the weakness of language models when tokenizing numbers [72]. Although it performs well, its use of dataset-specific output layers limits scalability and complicates multi-task learning. Consequently, they trained two separate models,5 wasting potential for better cross-dataset learning. Notably, none of these approaches finetune semantic representations during downstream task training. In our work, we demonstrate that this is critical to align textual and tabular features. Large Language Models for Tabular Data The remarkable success of LLMs is unprecedented [12, 56]. During the past years, several research attempts have tried to combine LLMs and tabular data. One line of work is on using LLMs directly for tabular prediction, by converting tabular data into serialized text. TabLLM [34] assessed LLMs under few-shot scenarios, while Tabula-8b [23] finetuned the Llama 3-8B model extensively on tabular data. Although useful for few-shot learning, these models are computationally expensive,6 suboptimal for numerical features [72, 74], and potentially compromised on widely-used benchmarks due to prior exposure during training [8]. While current generations of LLMs werent adopted for tabular learning, their emergent knowledge from their pretraining could be crucial when textual features are present [74, 19]. Additionally, LLMs can be used in multiple aspects of tabular learning, as they seem to be promising synthetic data generators [9, 65], useful data cleaners [6] and clever feature engineers [36]. 4Contextual information such as the dataset description, column names and category names. 5One for classification and one for regression. joint model for both tasks performed significantly worse. 6Llama 3-8b has orders of magnitude more parameters than TP-BERTa, which has roughly 110M parameters. Figure 1: The TabSTAR architecture illustrated with our toy dataset. The model processes numerical features, textual features, and all possible target values for classification. Multimodal AutoML Historically, textual tabular datasets have been largely overlooked in classical tabular benchmarks. However, the AutoML [33] community has made significant progress in developing multimodal solutions. In particular, AutoGluon [18] introduced the AutoML Multimodal Benchmark [60], initially focusing on text features and later evolving into AutoGluon-Multimodal [70, 69], which incorporates images as well. This powerful AutoML framework can fuse text and image foundation models with tabular models and ensemble multiple models through meta-learning approach [21], making it one of the few systems able to refine static textual representations via joint learning. Nevertheless, this line of work should not be seen as single model but rather as highly optimized, production-ready system. According to the authors, it is \"a collection of tricks that significantly enhance performance\" [70], establishing itself as robust approach for multimodal, multi-model tabular learning. However, this line of work remains somewhat orthogonal to the development of novel TFMs."
        },
        {
            "title": "3 TabSTAR",
            "content": "In this section, we introduce TabSTAR: Tabular Foundation Model with Semantically TargetAware Representations. Our training framework consists of two stages: (1) Pretraining, where it is pretrained over corpus of tabular datasets7 in multi-task regime, mixing classification with regression tasks, then (2) Finetuning, where the pretrained model is further trained with LoRA [41] on single downstream task. TabSTAR is designed to enable effective cross-dataset learning by applying supervised learning on the target variable in both stages. At its core, is uses an unfrozen encoder-only language model, which can invoke world knowledge acquired during the language model pretraining.8 The encoder is combined with tabular-specific architecture tailored to structured data, mitigating the known limitations of language models in tabular settings [72, 74]. TabSTARs architecture comprises five core modules: (1) Verbalization, mapping every feature into textual representation composed of both the column name and value, with special treatment to numerical features for full numerical precision; (2) Encoding, transforming semantic and numerical data into meaningful embeddings of the same dimension; (3) Fusion, integrating textual and numerical representations; (4) Interaction, modeling dependencies and relationships between different elements through self-attention and cross-element interactions; and (5) Prediction, where Interactions outputs are projected into real value for regression or probability distribution for classification. Figure 1 illustrates the architecture while Appendix elaborates, and Appendix discusses the training. key innovation of TabSTAR is the introduction of target-aware tokens, novel approach that integrates the target variables identity as an input to the model. Unlike existing TFMs [37, 45, 80, 82, 86, 79], which treat the target value as mere label, TabSTAR fuses target-awareness from the very beginning. For classification tasks, each target value is verbalized and encoded like any other feature. Then, features and target tokens interact with each other, building representations that are then used for prediction. Crucially, this target-awareness allows parameter sharing between all target tokens, which can later use shared prediction head that maps tokens to probabilities regardless of the number of classes and their identity. By doing so, TabSTAR eliminates the need for dataset-specific components commonly found in prior work [25, 86, 80]. TabSTARs flexible architecture effortlessly scales9 to any dataset size, and handles any number of classes in multiclass classification tasks. 7Ranging from metadata-rich, text-heavy datasets to numeric-only tables lacking column names. 8Note that the language-model pretraining occurs before TabSTARs pretraining. Unless specified differently, the term pretraining refers to TabSTARs pretraining, which assumes the use of pretrained language model. 9Except when the number of features becomes very large, where memory limitations may arise. 4 Table 2: An illustrative verbalization of the first patient of Table 1. Each semantic feature is verbalized with its name and value. The numerical Age value 45 is standardized (mapped into z-scores, e.g., 0.27) and binned (providing range to the verbalization, e.g., 40-50, and its quantile). The target variable Decision is mapped into its two possible elements, regardless of its original true value. Name Value Semantic Numerical"
        },
        {
            "title": "Age\nDepartment Cardiology\nReport\nDecision\nDecision",
            "content": "Mild chest discomfort. Hospitalized Released Age: 4050 (Quantile 5060%) Department: Cardiology Report: Mild chest discomfort. Target. Decision: Hospitalized Target. Decision: Released 0.27 - - - - Verbalization All the features and each of the target values are processed into sequence of elements. Numerical features are processed into two inputs: numerical one and semantic one. The numerical input is standardized using z-scores, with outlier clipping at 3 standard deviations. In addition, they are verbalized using quantile-based binning into 10 bins, novel approach to mitigate the precision loss inherent in language models [72]. Appendix A.1 shows precise example and 6 discusses different verbalization strategies. In contrast, semantic features are directly verbalized by concatenating the feature name and textual value, without any numerical representation. The target variable is also included as part of the input: In classification tasks, each of the possible values is represented by an element, constant for every example, while the true value remains hidden. For regression tasks, single element is verbalized, carrying only the target name. Table 1 shows toy dataset of patient records and outcomes and Table 2 shows the verbalization for the first patient. Encoding We employ pretrained e5-small-v2 [77] embedding model for semantic encoding, chosen for its strong performance on the MTEB benchmark [54] with relatively modest parameter count. By unfreezing half of its layers, the representations are optimized for predicting the target variable, which leads to significant impact on TabSTAR performance (see 6). Each verbalization element is encoded independently into semantic representation, with attention applied between tokens within each sequence element. In parallel, we encode standardized numerical values by projecting them into the same dimension using small MLP. For the patient in Table 2, this results in numerical embedding for Age alongside semantic representations for each of the five verbalizations. Fusion To obtain unified representation for each sequence element, we apply fusion block consisting of single encoder-only transformer layer. For each numerical feature, the block attends over its numerical and semantic embeddings, producing fused representation. In our running example, the representation of Age now jointly captures both its semantic context (the fact that the value represents age) as well as its numerical value (the patients age, 45, or 0.27 after standardization). Interaction The fused, semantically-rich and numerically-grounded representations of all elements interact via 6-layer Transformer encoder [76]. Each input element is now token, with feature tokens and target tokens all attending to each other. Unlike standard language models which integrate positional encoding, the Interaction modules inputs are order-invariant, desiredatum for TFMs, as defined by [74]. The encoder produces contextualized representations for each target value. In our example, this yields dedicated embeddings for the Release and Hospitalization target values. The role of these representations is to carry information about the likelihood of each value to be the true value. Prediction TabSTAR is designed for cross-dataset learning, with shared regression and classification heads used during both pretraining and finetuning. For classification, each of the target tokens is processed independently through the same classification head, which projects them to scores. We then apply softmax over all the possible values to yield probability distribution. Crucially, the fact that target tokens for every class in every dataset share the same classification head allows efficient parameter sharing, flexibly supports any number of output classes, and removes any need for dataset-specific parameters. This is not only efficient during pretraining, but also provides better initialization for finetuning. In our example, both the Released and Hospitalized tokens go through the same classification head, which maps them from representations to logits. Applying softmax yields predicted probabilities. For regression tasks, single target token is projected into real value."
        },
        {
            "title": "4 Experiments",
            "content": "The TabSTAR Pretraining Corpus While TabSTAR could be pretrained on massive scale, for this work we limit ourselves to modest pretraining corpus focusing on classification, as we believe that TabSTARs inductive biases are best suited to shine in this task. We manually curate pretraining corpus of 350 high-quality tabular datasets (253 classification, 97 regression), in tedious process in which we uncover numerous duplications in the most popular tabular repositories, OpenML [75] and Kaggle,10 as elaborated by [73]. Our datasets are sourced from popular benchmarks [24, 45, 22, 21, 30, 60, 52, 31], which have almost non-existing representation of textual tabular datasets. Thus, we furthermore increase our corpus, focusing on classification datasets with rich semantic content. See Appendix for more details. Benchmark Tabular datasets with free-text have seen little prior research, and accordingly, benchmarks are incredibly rare. Therefore, we consider all the possible datasets from AutoML Multimodal Benchmark [60], from the analysis about free-text and high-cardinality features by [31] and from the CARTE paper [45]. After deduplication process we end up with 50 datasets. However, there are two important limitations: first, the benchmark is heavily biased towards regression, with 36 datasets in total. Secondly, 29 out of these 36 datasets were solely contributed by the CARTE benchmark, which focuses more heavily on high-cardinality features as it was pretrained over knowledge graphs. While our main motivation is classification tasks with textual features, we decide nevertheless to evaluate on the full set of 50 datasets although it heavily biases towards regression problems and high-cardinality features, rather than classification and free-text (see Appendix D). Baselines We compare TabSTAR against diverse set of baselines. For tree-based methods, we evaluate CatBoost [57], XGBoost (XGB) [14], and Random Forest (RF) [11] with the default configuration proposed by [25]. For CatBoost and XGBoost we consider tuned version, where hyperparameters are optimized separately for each task using random search with 5-fold crossvalidation under 4-hour budget on 8 CPU cores. Among TFMs, we evaluate TabPFN-v2 [37] and CARTE [45]. For CARTE, we tune only the learning rate for each task, following the original paper. Since the public TabPFN-v2 model does not support text, we use their closed-sourced API client.11 For models lacking native support for textual features, we embed text using e5-small-v2 [77], allowing fair comparison. For more details about the hyperparameters for each baseline as well as exclusion of models such as TP-BERTa due to potential leakage concerns, see Appendix E. Experimental Setup Each of the 50 datasets in the benchmarks is evaluated with 10 random train-test splits (90% training, 10% testing), resulting in 500 runs per model. While 30 of the datasets have more than 10,000 examples, the evaluated TFMs have strict limitations. TabPFN-v2 employs ICL and thus receives as input at most 10,000 examples. Although CARTE imposes no size cap,12 it suffers from inefficiency and no preset configuration, requiring six learning-rate trials and totalling 6,000 slow GPU runs. Because of these important limitations, we consider two experiment conditions: (1) 10K: Each model is trained13 over at most 10,000 training examples, and (2) Unlimited: We add TabSTAR-Unlimit, CatBoost-Tuned-Unlimit, and XGBoost-Tuned-Unlimit and evaluate them on the full version of the 30 datasets,14 while retaining 10K baselines as weaker reference points. We exclude the untuned GBDTs and keep the same number of models as in the 10K condition. The TabSTAR Training To maximize the value of cross-dataset learning, instead of pretraining TabSTAR once, we create five dataset splits. Each variant is pretrained on the 350 pretraining datasets and 40 of the benchmark datasets, while the other 10 serve exclusively as its test set. Crucially, the whole collection was carefully curated to prevent any data leakage from duplicate or overly similar datasets. For finetuning, while dataset-specific hyperparameter tuning can boost performance, we believe that robust out-of-the-box defaults are essential for TFMs and their evaluation, following Tab-PFN-v2s approach. Therefore, we use default hyperparameters configuration that was found robust over disjoint set of tabular datasets, as detailed in Appendix B.2. 10https://www.kaggle.com/datasets 11https://github.com/PriorLabs/tabpfn-client 12In their own paper, CARTE was evaluated only over up to 2,048 examples, without scaling guarantees. 13While TabPFN-v2 isnt technically trained, we adopt this term for conciseness. 14We technically cap the amount of examples to 100,000 for computational efficiency."
        },
        {
            "title": "5 Results",
            "content": "We evaluate each model using AUROC (classification) and R2 (regression) as metrics. Following [37], we normalize scores per dataset split to the [0, 1] range, using the best and worst model performance as anchors.15 The normalized scores are averaged across all runs, with 95% CIs. Performance for all models on both conditions are shown in Figure 2 (classification) and Figure 3 (regression). Besides their example limit, TabPFN-v2 cannot process 4 datasets (more than 10 target classes or more than 500,000 cells) and CARTE cannot handle 15 (a bug in their PowerTransformer implementation). Reported averages for these models are computed only over the datasets where evaluation is feasible. Appendix expands on dataset level performance, head-to-head comparisons and running times. Figure 2: Comparison of normalized scores with 95% CIs between TabSTAR and baseline models in classification tasks, evaluated on up to 10,000 examples (left) and above 10,000 (right). In classification problems, TabSTAR consistently achieves SOTA performance. This is evident both when restricting the dataset size to 10,000 examples and when using larger datasets in the unlimited condition. For the 10K condition, TabSTAR achieves 0.809 score, performing better than TabPFN-v2 (0.783) and significantly better than GBDTs (0.756 CatBoost-Tuned, 0.744 XGB-Tuned). When analyzing head-to-head comparisons (Appendix F.2), TabSTAR outperforms TabPFN-v2 (7/11 datasets), XGB-Tuned (10/14) and CatBoost-Tuned (11/14). For the Unlimited condition, TabSTARUnlimit achieves 0.874 score, significantly above the second-best CatBoost-Tuned-Unlimit with 0.734. Importantly, all Unlimit variants surpass the 10K ones, emphasizing the importance of scaling. Figure 3: Comparison of normalized scores with 95% CIs between TabSTAR and baseline models in regression tasks, evaluated on up to 10,000 examples (left) and above 10,000 (right). Although regression is not our main focus, TabSTAR achieves competitive results in the 10K condition, but clearly does not set the SOTA. Surprisingly, while TabPFN-v2 is superior, it significantly underperforms compared to GBDTs which dominate this category. This emphasizes the need for better modeling of textual tabular learning, especially since TabPFN-v2 has shown remarkable performance in non-textual tabular datasets, and CARTE set the SOTA for small datasets. When analyzing the Unlimited variants, TabSTAR scales well, and surpasses other TFMs which cannot scale, but the gap from GBDTs remains significant. 7 discusses this limitation and suggests promising directions for future generations of TabSTAR to achieve SOTA in regression as well. 15For single run, the best model gets 1, the worst gets 0 and the rest are linearly scaled accordingly."
        },
        {
            "title": "6 Analysis",
            "content": "We analyze the factors contributing to TabSTARs strong performance by addressing three key research questions: Q1: How important is the encoder language model unfreezing? Q2: Does the number of datasets during pretraining contribute to the downstream task performance? and Q3: How do different verbalization methods of numerical features impact performance? To answer these questions, we pretrain several variants of TabSTAR for each analysis, limiting ourselves to subset of the tabular datasets used for the main experiment (see 4). Specifically, each variant is pretrained over 256 datasets16 including 30 datasets from our benchmark, and evaluated over the remaining 20 datasets (12 regression, 8 classification). This reduced setup allows leveraging transfer learning and exploiting our corpus, without the burden of training multiple folds per variant. Results are reported with the same normalized metric used in 5, scaling the performance to the [0, 1] range. Appendix G.1 lists the 20 datasets used for evaluation along with per-dataset results. Figure 4: Performance as function of the number of encoder layers unfrozen: Validation loss during TabSTARs pretraining (left) and normalized scores with 95% CIs on the downstream tasks (right). Unfreezing even single encoder layer significantly improves the performance of TabSTAR. Q1: The Role of the Encoder Unfreezing To investigate whether unfreezing layers of the textual encoder impacts performance, we conduct experiments where we unfreeze varying numbers of the 12 encoder layers during both TabSTARs pretraining and finetuning stages.17 Figure 4 shows the validation loss during TabSTAR pretraining (left) and the normalized score on the downstream tasks (right) as function of the number of unfrozen encoder layers. Notably, unfreezing even single encoder layer significantly outperforms using static embeddings. Further substantial improvements are observed as more layers are tuned, with the best results achieved when unfreezing 6 layers. While unfreezing 9 layers shows lower performance, it is plausible that adding more datasets to the pretraining phase will affect this finding. See Appendix G.2 for more details. Table 3: Normalized score with 95% CIs by the number of datasets used during TabSTAR pretraining. Pretraining Datasets 0 16 64 256 Classification Regression 0.352 0.086 0.338 0. 0.450 0.084 0.395 0.068 0.558 0.086 0.642 0.066 0.786 0.076 0.811 0.055 Q2: The Effect of Pretraining To evaluate the impact of pretraining on TabSTARs downstream performance, we compare pretrained version of TabSTAR with version that was finetuned from scratch.18 In line with previous work [86, 82], the pretrained model performs significantly better, highlighting the critical role of transfer learning for TabSTARs success. To further investigate the effect of the number of pretraining datasets on downstream task performance, we train two additional versions: one pretrained on 16 datasets and another on 64 datasets. As shown in Table 3, increasing the number of pretraining datasets consistently improved performance in both classification and regression tasks. Notably, the substantial gain in regression tasks suggests that TabSTARs downstream performance on 5 could improve with more pretraining data (see Appendix G.3). 16Except for variants of Q2, which analyze the effect of number of datasets on pretraining. 17For each variant, the number of unfrozen layers remains the same in both pretraining and finetuning. 18Since LoRA underperforms on random weights, we finetune the entire non-pretrained model. Q3: Numerical Verbalization key challenge in integrating language models with numerical data is determining how to best represent numerical values within linguistic framework. While some semantic tabular methods omit numerical features from the verbalization [79, 82], TP-BERTa [80] introduced Relative Magnitude Tokenization [80], which encode numerical information through non-semantic special bin tokens. In constrast, TabSTAR injects semantic numerical information into the verbalization of numerical features, as illustrated in Table 2. To quantify the effect of our novel verbalization, we explore two thinner variants: (1) Name + Bin, which excludes the quantile information, and (2) Name, which omits numeric information entirely and verbalizes the feature name only. Appendix G.4 shows an illustrative example for each variant and presents the full results. As demonstrated in Table 4, our findings reveal that incorporating numerical information significantly enhances performance, highlighting the importance of balancing numerical precision with representation format that aligns with the language models parametric knowledge. Table 4: Normalized score with 95% CIs by the numerical verbalization method."
        },
        {
            "title": "Name",
            "content": "Name + Bin"
        },
        {
            "title": "Classification\nRegression",
            "content": "0.386 0.095 0.386 0.081 0.544 0.093 0.584 0.076 0.593 0.097 0.596 0."
        },
        {
            "title": "7 Discussion and Conclusion",
            "content": "We introduce TabSTAR, Tabular Foundation Model with Semantically Target-Aware Representations, which integrates textual features through an unfrozen pretrained encoder. In addition, its novel target-aware tokens enable efficient cross-dataset generalization without dataset-specific parameters. Despite limited pretraining data and relatively small text encoder [77], TabSTAR sets the SOTA in tabular classification with textual features, significantly surpassing GBDTs and leading TFMs. Since scaling laws in data and model size have proven themselves for LLMs [43] and TabSTAR improves with the number of pretraining datasets (see 6), future work should scale TabSTAR across both model and data dimensions. For model scaling, we envision family of model sizes, common for LLMs [29, 71, 49], that will allow trade-off between quality and costs. Data scaling might leverage self-supervised learning [59] over large-scale table corpora [17], or realistic synthetic tabular data generators [9], which have proven successful [37, 2]. At scale, it could potentially unlock few-shot learning capabilities and develop automatic feature-engineering skills [36]. Beyond scaling, TabSTARs semantic approach has tremendous potential to explicitly include world knowledge, by leveraging LLMs which to date have had limited impact on tabular learning. As few motivating examples, LLMs could improve TabSTARs numerical verbalization binning approach by providing semantically informed thresholds, or provide explicit, contextual world knowledge that could be injected as strong prior in small data scenarios. While these directions seem like plausible research paths, they come with risk of data leakage due to the memorization properties of LLMs [8]. Evaluating TFMs fairly while keeping benchmarks uncontaminated would be an important enabler for tabular research. As step in this direction, we are releasing several TabSTAR variants, each with different dataset withheld during pretraining, ensuring that for every dataset there is TabSTAR model that has never seen it. We urge fellow researchers to adopt this approach in their own work. While TabSTAR sets new bar in classification, its regression results lag behind GBDTs, which outperform other TFMs as well. This gap could be narrowed through additional scaling, and also by exploring regression-via-classification techniques like [37, 2]. Furthermore, TabSTAR has not been extensively evaluated in few-shot scenarios and in purely numerical datasets.19 In addition, it demands more compute compared to GBDTs, and it may struggle with memory constraints on datasets containing hundreds of features. Despite these limitations, TabSTAR offers promising pathway toward improving performance on tabular datasets with textual fields, common in industries with high social impact (e.g., healthcare, education), or with significant economic value (e.g., banking, manufacturing). We believe TabSTAR paves the way for new generation of semantically enriched tabular models, and we welcome the research communitys innovations built on this foundation. 19Partly because of the computational burden of tuning baselines, and the lack of objective leaderboards [73]."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We thank Omri Feldman for brainstorming since the very beginning; Elad Hoffer and Ofir Lindenbaum for consulting and feedback; David Holzmüller and Myung Kim for supporting evaluations; and Noah Hollmann, Léo Grinsztajn, and the Prior Labs team for providing extensive access to TabPFN-v2."
        },
        {
            "title": "References",
            "content": "[1] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, and et al. Deep Speech 2 In Proceedings of The 33rd : End-to-End Speech Recognition in English and Mandarin. International Conference on Machine Learning, pages 173182. PMLR, June 2016. URL https://proceedings.mlr.press/v48/amodei16.html. ISSN: 1938-7228. [2] Abdul Fatir Ansari, Lorenzo Stella, Ali Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Bernie Wang. Chronos: Learning the Language of Time Series. Transactions on Machine Learning Research, May 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=gerNCVqqtR. [3] Sercan Ö Arik and Tomas Pfister. TabNet: Attentive Interpretable Tabular Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):66796687, May 2021. ISSN 23743468. doi: 10.1609/aaai.v35i8.16826. URL https://ojs.aaai.org/index.php/AAAI/ article/view/16826. Number: 8. [4] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundation ModIEEE Transactions on Patels Defining New Era in Vision: Survey and Outlook. tern Analysis and Machine Intelligence, 47(4):22452264, April 2025. ISSN 1939-3539. doi: 10.1109/TPAMI.2024.3506283. URL https://ieeexplore.ieee.org/abstract/ document/10834497. [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate, May 2016. URL http://arxiv.org/abs/1409.0473. arXiv:1409.0473 [cs]. [6] Tommaso Bendinelli, Artur Dox, and Christian Holz. Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets. March 2025. URL https://openreview.net/forum? id=RXnQPYSoun. [7] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, and et al. On the Opportunities and Risks of Foundation Models, August 2021. URL https://arxiv.org/abs/2108.07258v3. [8] Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, and Rich Caruana. Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models. First Conference on Language Modeling, 2024. [9] Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language Models are Realistic Tabular Data Generators. September 2022. URL https: //openreview.net/forum?id=cEygmQNOeI. [10] Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep Neural Networks and Tabular Data: Survey. IEEE Transactions on Neural Networks and Learning Systems, 35(6):74997519, June 2024. ISSN 2162-2388. doi: 10.1109/TNNLS.2022.3229161. URL https://ieeexplore.ieee.org/abstract/ document/9998482. [11] Leo Breiman. Random Forests. Machine Learning, 45(1):532, October 2001. ISSN 1573-0565. doi: 10.1023/A:1010933404324. URL https://doi.org/10.1023/A:1010933404324. 10 [12] Tom Brown, Benjamin Mann, Nick Ryder, and et al. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. [13] Patricio Cerda and Gaël Varoquaux. Encoding High-Cardinality String Categorical Variables. IEEE Transactions on Knowledge and Data Engineering, 34(3):11641176, March 2022. ISSN 1558-2191. doi: 10.1109/TKDE.2020.2992529. URL https://ieeexplore.ieee.org/ abstract/document/9086128. [14] Tianqi Chen and Carlos Guestrin. XGBoost: Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, pages 785794, New York, NY, USA, August 2016. Association for Computing Machinery. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939785. URL https://dl. acm.org/doi/10.1145/2939672.2939785. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:// aclanthology.org/N19-1423/. [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, June 2021. URL http://arxiv.org/abs/2010.11929. arXiv:2010.11929 [cs]. [17] Gus Eggert, Kevin Huo, Mike Biven, and Justin Waugh. TabLib: Dataset of 627M Tables with Context, October 2023. URL http://arxiv.org/abs/2310.07875. arXiv:2310.07875 [cs]. [18] Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data, March 2020. URL http://arxiv.org/abs/2003.06505. arXiv:2003.06505 [stat]. [19] Xi Fang, Weijie Xu, Fiona Anting Tan, Ziqing Hu, Jiani Zhang, Yanjun Qi, Srinivasan H. Sengamedu, and Christos Faloutsos. Large Language Models (LLMs) on Tabular Data: Prediction, Generation, and Understanding - Survey. Transactions on Machine Learning Research, March 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=IZnrCGF9WI. [20] Benjamin Feuer, Robin T. Schirrmeister, Valeriia Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, and Colin White. TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks. Advances in Neural Information Processing Systems, 37:83430 83464, December 2024. URL https://proceedings.neurips.cc/paper_files/paper/ 2024/hash/97dc07f1253ab33ee514f395a82fa7cc-Abstract-Conference.html. [21] Matthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer, and Frank Hutter. Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning. Journal of Machine Learning Research, 23(261):161, 2022. ISSN 1533-7928. URL http://jmlr.org/papers/v23/ 21-0992.html. [22] Sebastian Felix Fischer, Matthias Feurer, and Bernd Bischl. OpenML-CTR23 curated tabular regression benchmarking suite. August 2023. URL https://openreview.net/ forum?id=HebAOoMm94. [23] Josh Gardner, Juan C. Perdomo, and Ludwig Schmidt. Large Scale Transfer Learning for Tabular Data via Language Modeling. Advances in Neural Information Processing Systems, 37:45155 45205, December 2024. URL https://proceedings.neurips.cc/paper_files/paper/ 2024/hash/4fd5cfd2e31bebbccfa5ffa354c04bdc-Abstract-Conference.html. 11 [24] Pieter Gijsbers, Marcos L. P. Bueno, Stefan Coors, Erin LeDell, Sébastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. AMLB: an AutoML Benchmark. Journal of Machine Learning Research, 25(101):165, 2024. ISSN 1533-7928. URL http://jmlr.org/ papers/v25/22-0493.html. [25] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, pages 1893218943, Red Hook, NY, USA, December 2021. Curran Associates Inc. ISBN 978-1-7138-4539-3. [26] Yury Gorishniy, Ivan Rubachev, and Artem Babenko. On embeddings for numerical features in tabular deep learning. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, pages 2499125004, Red Hook, NY, USA, November 2022. Curran Associates Inc. ISBN 978-1-7138-7108-8. [27] Yury Gorishniy, Ivan Rubachev, Nikolay Kartashev, Daniil Shlenskii, Akim Kotelnikov, and Artem Babenko. TabR: Tabular Deep Learning Meets Nearest Neighbors. The Twelfth International Conference on Learning Representations, October 2023. URL https://openreview. net/forum?id=rhgIgTSSxW. [28] Yury Gorishniy, Akim Kotelnikov, and Artem Babenko. TabM: Advancing Tabular Deep Learning with Parameter-Efficient Ensembling, February 2025. URL http://arxiv.org/ abs/2410.24210. arXiv:2410.24210 [cs]. [29] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, and et al. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/2407.21783. arXiv:2407.21783 [cs]. [30] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? Advances in Neural Information Processing Systems, 35:507520, December 2022. URL https://proceedings.neurips.cc/paper_files/ paper/2022/hash/0378c7692da36807bdec87ab043cdadc-Abstract-Datasets_and_ Benchmarks.html. [31] Léo Grinsztajn, Edouard Oyallon, Myung Jun Kim, and Gaël Varoquaux. Vectorizing string entries for data processing on tables: when are larger language models better?, December 2023. URL http://arxiv.org/abs/2312.09634. arXiv:2312.09634 [stat]. [32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition, December 2015. URL http://arxiv.org/abs/1512.03385. arXiv:1512.03385 [cs]. [33] Xin He, Kaiyong Zhao, and Xiaowen Chu. AutoML: survey of the state-of-the-art. Knowledge-Based Systems, 212:106622, January 2021. ISSN 0950-7051. doi: 10.1016/j. knosys.2020.106622. URL https://www.sciencedirect.com/science/article/pii/ S0950705120307516. [34] Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. TabLLM: Few-shot Classification of Tabular Data with Large Language Models. In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, pages 55495581. PMLR, April 2023. URL https://proceedings.mlr.press/v206/ hegselmann23a.html. ISSN: 2640-3498. [35] Noah Hollmann, Samuel Müller, Katharina Eggensperger, and Frank Hutter. TabPFN: Transformer That Solves Small Tabular Classification Problems in Second. The Eleventh International Conference on Learning Representations, September 2022. URL https:// openreview.net/forum?id=cp5PvcI6w8_. [36] Noah Hollmann, Samuel Müller, and Frank Hutter. Large Language Models for AutoIntroducing CAAFE for Context-Aware Automated Feature Engimated Data Science: neering. Advances in Neural Information Processing Systems, 36:4475344775, December 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/ 8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html. 12 [37] Noah Hollmann, Samuel Müller, Lennart Purucker, Arjun Krishnakumar, Max Körfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with tabular foundation model. Nature, 637(8045):319326, January 2025. ISSN 14764687. doi: 10.1038/s41586-024-08328-6. URL https://www.nature.com/articles/ s41586-024-08328-6. Publisher: Nature Publishing Group. [38] David Holzmüller, Léo Grinsztajn, and Ingo Steinwart. Better by default: Strong pre-tuned MLPs and boosted trees on tabular data. Advances in Neural Information Processing Systems, 37: 2657726658, December 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/hash/2ee1c87245956e3eaa71aaba5f5753eb-Abstract-Conference. html. [39] Shi Bin Hoo, Samuel Müller, David Salinas, and Frank Hutter. The Tabular Foundation Model TabPFN Outperforms Specialized Time Series Forecasting Models Based on Simple Features. October 2024. URL https://openreview.net/forum?id=H02X7RO3OC#discussion. [40] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, pages 27902799. PMLR, May 2019. URL https://proceedings.mlr.press/v97/ houlsby19a.html. ISSN: 2640-3498. [41] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. October 2021. URL https://openreview.net/forum?id=nZeVKeeFYf9. [42] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. TabTransformer: Tabular Data Modeling Using Contextual Embeddings, December 2020. URL http://arxiv.org/abs/ 2012.06678. arXiv:2012.06678 [cs]. [43] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language Models, January 2020. URL http://arxiv.org/abs/2001.08361. arXiv:2001.08361 [cs]. [44] Liran Katzir, Gal Elidan, and Ran El-Yaniv. Net-DNF: Effective Deep Modeling of Tabular Data. October 2020. URL https://openreview.net/forum?id=73WTGs96kho. [45] Myung Jun Kim, Léo Grinsztajn, and Gaël Varoquaux. CARTE: pretraining and transfer for tabular learning. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of ICML24, pages 2384323866, Vienna, Austria, July 2024. JMLR.org. [46] Ravin Kohli, Matthias Feurer, Katharina Eggensperger, Bernd Bischl, and Frank Hutter. Towards Quantifying the Effect of Datasets for Benchmarking: Look at Tabular Machine Learning. [47] Jannik Kossen, Neil Band, Clare Lyle, Aidan Gomez, Thomas Rainforth, and Yarin Gal. Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. In Advances in Neural Information Processing Systems, volume 34, pages 28742 28756. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/ 2021/hash/f1507aba9fc82ffa7cc7373c58f8a613-Abstract.html. [48] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. ImageNet Classification with Deep In Advances in Neural Information Processing Systems, Convolutional Neural Networks. volume 25. Curran Associates, Inc., 2012. URL https://papers.nips.cc/paper_files/ paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html. [49] Barak Lenz, Opher Lieber, Alan Arazi, and et al. Jamba: Hybrid Transformer-Mamba Language Models. October 2024. URL https://openreview.net/forum?id=JFPaD7lpBD. [50] Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer Learning with Deep Tabular Models. The Eleventh International Conference on Learning Representations, September 2022. URL https://openreview.net/forum?id=b0RuGUYo8pA. 13 [51] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: Robustly Optimized BERT Pretraining Approach, July 2019. URL http://arxiv.org/abs/1907.11692. arXiv:1907.11692 [cs]. [52] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When Do Neural Nets Outperform Boosted Trees on Tabular Data? Advances in Neural Information Processing Systems, 36:7633676369, December 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/hash/ f06d5ebd4ff40b40dd97e30cee632123-Abstract-Datasets_and_Benchmarks.html. [53] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space, September 2013. URL http://arxiv.org/abs/1301. 3781. arXiv:1301.3781 [cs]. [54] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive Text Embedding Benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/ 2023.eacl-main.148/. [55] Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers Can Do Bayesian Inference. October 2021. URL https://openreview.net/ forum?id=KSugKcbNf9. [56] OpenAI. GPT-4 Technical Report, March 2024. URL http://arxiv.org/abs/2303.08774. arXiv:2303.08774 [cs]. [57] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorounbiased boosting with categorical features. Information Processing Systems, volume 31. Curran AssoURL https://proceedings.neurips.cc/paper/2018/hash/ gush, and Andrey Gulin. In Advances in Neural ciates, 14491b756b3a51daac41c24863285549-Abstract.html. Inc., 2018. CatBoost: [58] Jingang Qu, David Holzmüller, Gaël Varoquaux, and Marine Le Morvan. TabICL: Tabular Foundation Model for In-Context Learning on Large Data, February 2025. URL http:// arxiv.org/abs/2502.05564. arXiv:2502.05564 [cs]. [59] Ivan Rubachev, Artem Alekberov, Yury Gorishniy, and Artem Babenko. Revisiting Pretraining Objectives for Tabular Deep Learning, July 2022. URL http://arxiv.org/abs/2207. 03208. arXiv:2207.03208 [cs]. [60] Xingjian Shi, Jonas Mueller, Nick Erickson, Mu Li, and Alex Smola. Benchmarking Multimodal AutoML for Tabular Data with Text Fields. August 2021. URL https://openreview.net/ forum?id=Q0zOIaec8HF. [61] Assaf Shmuel, Oren Glickman, and Teddy Lazebnik. Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets, August 2024. URL http://arxiv.org/ abs/2408.14817. arXiv:2408.14817 [cs]. [62] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion, 81:8490, May 2022. ISSN 1566-2535. doi: 10.1016/j.inffus.2021.11.011. URL https://www.sciencedirect.com/science/article/pii/S1566253521002360. [63] Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition, April 2015. URL http://arxiv.org/abs/1409.1556. arXiv:1409.1556 [cs]. [64] Leslie N. Smith and Nicholay Topin. Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates, May 2018. URL http://arxiv.org/abs/1708. 07120. arXiv:1708.07120 [cs]. [65] Aivin V. Solatorio and Olivier Dupriez. REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers, February 2023. URL http://arxiv.org/abs/2302. 02041. arXiv:2302.02041 [cs]. [66] Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, and Tom Goldstein. SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive PreTraining, June 2021. URL http://arxiv.org/abs/2106.01342. arXiv:2106.01342 [cs]. [67] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 19, pages 11611170, New York, NY, USA, November 2019. Association for Computing Machinery. ISBN 978-1-4503-6976-3. doi: 10.1145/3357384.3357925. URL https://dl.acm.org/doi/10.1145/3357384.3357925. [68] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, volume 2 of NIPS14, pages 31043112, Cambridge, MA, USA, December 2014. MIT Press. [69] Zhiqiang Tang, Haoyang Fang, Su Zhou, Taojiannan Yang, Zihan Zhong, Cuixiong Hu, Katrin Kirchhoff, and George Karypis. AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models. AutoML Conference 2024 (ABCD Track), April 2024. URL https://openreview.net/forum?id=irStSm9waW. [70] Zhiqiang Tang, Zihan Zhong, Tong He, and Gerald Friedland. Bag of Tricks for Multimodal AutoML with Image, Text, and Tabular Data, December 2024. URL http://arxiv.org/ abs/2412.16243. arXiv:2412.16243 [cs]. [71] Gemini Team. Gemini: Family of Highly Capable Multimodal Models, May 2025. URL http://arxiv.org/abs/2312.11805. arXiv:2312.11805 [cs]. [72] Avijit Thawani, Jay Pujara, Pedro A. Szekely, and Filip Ilievski. Representing Numbers in NLP: Survey and Vision, March 2021. URL http://arxiv.org/abs/2103.13136. arXiv:2103.13136 [cs]. [73] Andrej Tschalzev, Lennart Purucker, Stefan Lüdtke, Frank Hutter, Christian Bartelt, and Heiner Stuckenschmidt. Unreflected Use of Tabular Data Repositories Can Undermine Research Quality, March 2025. URL http://arxiv.org/abs/2503.09159. arXiv:2503.09159 [cs]. [74] Boris Van Breugel and Mihaela Van Der Schaar. Position: why tabular foundation models should be research priority. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of ICML24, pages 4897648993, Vienna, Austria, July 2024. JMLR.org. [75] Joaquin Vanschoren, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. OpenML: networked science in machine learning. SIGKDD Explor. Newsl., 15(2):4960, June 2014. ISSN 1931-0145. doi: 10.1145/2641190.2641198. URL https://doi.org/10.1145/2641190.2641198. [76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [77] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text Embeddings by Weakly-Supervised Contrastive Pre-training, February 2024. URL http://arxiv.org/abs/2212.03533. arXiv:2212.03533 [cs]. [78] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In Proceedings of the Web Conference 2021, WWW 21, pages 17851797, New 15 York, NY, USA, June 2021. Association for Computing Machinery. ISBN 978-1-4503-83127. doi: 10.1145/3442381.3450078. URL https://dl.acm.org/doi/10.1145/3442381. 3450078. [79] Zifeng Wang and Jimeng Sun. TransTab: Learning Transferable Tabular Transformers Across Tables. October 2022. URL https://openreview.net/forum?id=A1yGs_SWiIi. [80] Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen. Making Pre-trained Language Models Great on Tabular Prediction. The Twelfth International Conference on Learning Representations, October 2023. URL https: //openreview.net/forum?id=anzIzGZuLi. [81] Junchen Yang, Ofir Lindenbaum, and Yuval Kluger. Locally Sparse Neural Networks for Tabular Biomedical Data. In Proceedings of the 39th International Conference on Machine Learning, pages 2512325153. PMLR, June 2022. URL https://proceedings.mlr.press/v162/ yang22i.html. ISSN: 2640-3498. [82] Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Towards Cross-Table Masked Pretraining for Web Data Mining. May 2024. URL https: //openreview.net/forum?id=9jj7cMOXQo. [83] Han-Jia Ye, Si-Yang Liu, Hao-Run Cai, Qi-Le Zhou, and De-Chuan Zhan. Closer Look at Deep Learning on Tabular Data. CoRR, January 2024. URL https://openreview.net/ forum?id=eu2cABIHge. [84] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu, Pengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu, and Lichao Sun. Comprehensive Survey on Pretrained Foundation Models: History from BERT to ChatGPT, May 2023. URL http://arxiv.org/abs/2302.09419. arXiv:2302.09419 [cs]. [85] Qile Zhou, Han-Jia Ye, Leye Wang, and De-Chuan Zhan. Unlocking the Transferability of Tokens in Deep Models for Tabular Data. October 2023. URL https://openreview.net/ forum?id=u2OVQ2Xvq1. [86] Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. XTab: Cross-table Pretraining for Tabular Transformers. In Proceedings of the 40th International Conference on Machine Learning, pages 4318143204. PMLR, July 2023. URL https: //proceedings.mlr.press/v202/zhu23k.html. ISSN: 2640-3498. [87] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. Comprehensive Survey on Transfer Learning. Proceedings of the IEEE, 109(1):4376, January 2021. ISSN 1558-2256. doi: 10.1109/JPROC.2020.3004555. URL https://ieeexplore.ieee.org/abstract/document/9134370."
        },
        {
            "title": "A Architecture",
            "content": "This appendix provides additional technical details for the architecture introduced in 3. First, we discuss the verbalization module; next, we formally describe the architecture step-by-step; and finally, we present selected experiments on the TabSTAR architecture. A.1 The Verbalization Module TabSTARs verbalization module standardizes heterogeneous tabular inputs by converting each column, whether predictive feature or target variable, into templated text blocks. We first describe the detection of column types, and then detail the processing steps for each type. Feature Detection We classify each column as either numerical, referring to quantitative values, or semantic, referring to textual values including categorical and boolean fields encoded as text. We rely on heuristics involving both the primitive data type (e.g., string, float) and human annotation (e.g., OpenML metadata). However, real-world datasets pose challenges, as numerical features can often be stored as strings (e.g., 35 years, unknown age) or may lack inherent order (e.g., country calling codes). Leveraging LLMs for contextualized data cleaning can be promising direction [6]. special case is the handling of timestamp and date columns. Similarly to [39], we rely on skrubs DatetimeEncoder20 to detect datetime columns and decompose each one of them into set of new features. Each extracted feature then undergoes its own processing: For example, the weekday is treated as semantic, while the total seconds since the Unix epoch is treated as numerical. Integrating date features more holistically remains an open research question. Table 5: Illustrative verbalization of numerical feature (Age) with 10 bins. Examples outside the range and missing values are considered as special bins. Bin Range Example Value Illustrative Verbalization Lower than 18 1 2 3 4 5 6 7 8 9 10 1823 2327 2731 3135 3540 4045 4551 5158 5867 6787 Higher than 87 Unknown 17 20 25 29 33 38 42 48 55 63 83 93 Age: Lower than 18 (Quantile 0%) Age: 1823 (Quantile 010%) Age: 2327 (Quantile 1020%) Age: 2731 (Quantile 2030%) Age: 3135 (Quantile 3040%) Age: 3540 (Quantile 4050%) Age: 4045 (Quantile 5060%) Age: 4551 (Quantile 6070%) Age: 5158 (Quantile 7080%) Age: 5867 (Quantile 8090%) Age: 6787 (Quantile 90100%) Age: Higher than 87 (Quantile 100%) Age: Unknown Value Numerical Features Numerical features are represented by both numerical and semantic representation. For the numerical representation, given value x, we compute the clipped z-score = clip(cid:0)(x µ)/σ, 3, 3(cid:1) where µ, σ are the training set mean and the standard deviation, and missing values are set to 0. For the semantic representation, we build = 10 quantile bins over the training distribution to map the value accordingly. Table 5 shows an illustrative example for the feature Age from our running example in Table 2. Semantic Features Semantic features are sanitized (e.g., normalizing whitespaces) and verbalized using the template presented in Table 6. Missing values are mapped to Unknown Value, just like for numerical features. If text exceeds the models context window (512 tokens for e5-small-v2), it is naively truncated to fit it. This limitation is far more pronounced for methods that serialize the entire example into single textual sequence [80], thereby dramatically reducing the effective context size. 20https://skrub-data.org/ Target Variables The verbalization templates for the target values are prepended to every example. For classification tasks, each possible label is verbalized, while for regression we verbalize single element consisting solely of the feature name. Employing binning strategy to treat regression as classification task is future work direction, as discussed in 7. For regression tasks, target values go through the same standardization with outlier clipping as numerical features, being used solely as the ground truth without going through the input. Table 6: Verbalization templates for semantic features and target values. Element Type"
        },
        {
            "title": "Classification target",
            "content": "Verbalization Template \"Predictive Feature: {feature_name}\" \"Feature Value: {feature_value}\" \"Target Feature: {target_name}\" \"Feature Value: {target_value}\""
        },
        {
            "title": "Regression target",
            "content": "\"Numerical Target Feature: {target_name}\" A.2 The Annotated TabSTAR Table 7 describes the number of parameters per component in the TabSTAR architecture, when using e5-small-v2 [77] as the text encoder. It has approximately 47.26M parameters, most of which come from the text encoder. When unfreezing 6 layers of the text encoder, about 24.70M parameters are tuned, with the remaining 11.92M embedding parameters and 10.65M layer ones being kept frozen. Table 7: Parameter counts for TabSTAR components Module # Parameters Encoding: Semantic Encoding: Numerical Fusion Interaction Prediction 33,360,000 296,832 1,774,464 10,646,784 1,185,794 To describe the architecture more precisely, we start by defining the dataset formally. Let = {(xi, yi)}n i=1 denote tabular dataset with examples. Each example xi = [xi1, . . . , xim] has features. The target variable yi is either continuous (regression) or discrete (classification) taking one of classes. For simplicity, we describe the architecture at the example level, though all computations are actually carried out on mini-batches of size B. The batches are always drawn from single dataset in both pretraining and finetuning, removing any need for padding. Verbalization We denote by the number of target entries where = for classification and = 1 for regression. We then form raw sequence of length = + by listing the target values, followed by the feature entries. Each element in this sequence is then verbalized into semantic string sj and numerical value nj, set to be the clipped z-score for numerical non-missing features, and zero otherwise. The example is thus represented by parallel sequences (s, n) of length e. Encoding Each semantic string sj and numerical value nj are projected into d-dimensional vector. Semantic strings are encoded with an encoder-only language model (e5-small-v2 [77]). Each string is tokenized, passed through the model, and pooled to produce its final embedding. This process is independent between elements, i.e. the attention is at the token level within single element. In parallel, each numeric value is fed through two-layer MLP that first projects from 1 to 2d dimensions, applies ReLU and dropout, and then projects back down to d. This produces matching d-dimensional embeddings for each of the elements, ready to be fused. Fusion To unify semantic and numerical embeddings into single representation, we apply single-layer Transformer Encoder21 over each elements pair of vectors. Concretely, for each element 21With 2 attention heads, feed-forward hidden size of 4d, dropout 0.1, and ReLU activation. 18 we stack its d-dimensional text and numeric embeddings and feed them through the encoder layer. For every element, the attention is applied between its two representations, and we average the two outputs to produce one fused d-dimensional embedding. This yields final sequence of length and dimension d, which will serve as tokens for the Interaction block. Interaction The fused sequence of tokens is processed by standard Transformer Encoder with model dimension = 384, = 6 layers, 6 attention heads per layer, feed-forward size 4d, dropout 0.1, ReLU activation and using pre-norm configuration. Unlike in language modeling, feature ordering is irrelevant, so no positional encodings are used. The encoder produces contextualized embeddings for every position, and we retain the target embeddings for the prediction. Prediction We initialize two identical MLP heads, one for regression and one for classification. Each of them consists of hidden layer of size 4d (with ReLU activation) followed by linear projection to single output. For each dataset, we choose the relevant head and process the target token embeddings. For classification, we independently feed each one of the = target tokens to the classification head to obtain score (logit) per class. Notably, the same head is shared across classes and datasets. We apply softmax over these scores, yielding probability distribution regardless of the number of classes. For regression, the single target token is projected into real value. Note that the heads is shared between datasets, as regression outputs are always clipped z-scores. A.3 Architecture Experiments In this section we explore the effect of different design choices for TabSTARs architecture. For each experiment, we only vary the parameter of interest, keeping everything else fixed. We follow the same pretraining regime as in Appendix B.1, except that for computational efficiency we train only 25 epochs (instead of 50) with 128 pretraining datasets (instead of 390). We evaluate each variant relying solely on pretraining performance, as an approximation for downstream task performance. We acknowledge that our conclusions might depend on this limited scale, hence we discuss subset of the experiments briefly to reflect the depth of our work and inspire future research. The Fusion Blocks Mechanism For the fusion block, we consider two simpler alternatives to the attention mechanism, both of then underperforming: (1) Concatenation, by concatenating the semantic and numerical d-dimensional vectors into 2d-dimensional vector, and projecting them back via an MLP, and (2) Multiplication, by multiplying the semantic representation directly with the numerical value22 in straightforward, parameter-free manner as in [79, 82]. The Number of Interaction Layers We experiment with the number of encoder layers, and observe that 3 yields anecdotally worse performance than 6, with lower parameter count. Nevertheless, we prioritize deeper network as for datasets with very complex relationships we believe that this might be beneficial. Additionally, we try 9-layer variant which performs significantly worse, while also increasing the parameter count. Row-Level Attention We experiment with adopting the architecture proposed by SAINT [66], which adds row-level attention to each encoder layer. Similar concepts are also employed by models that get the input the whole dataset, labels included [37, 47]. We run experiments with 2, 4 and 6 layers as they are roughly equivalent in parameter count to 3, 6, and 9 layers without row-attention. We observe no substantial gain, and thus we prioritize the simpler solution, as row-level attention is sensitive to the batch size and adds complexity to inference time."
        },
        {
            "title": "B Training",
            "content": "In this section we elaborate on the two stages of TabSTARs training: pretraining and finetuning, presented in 3. As in Appendix A.3, we summarize key pretraining experiments. 22After rescaling it to be centered around 1 rather than 0, using learned scaling factor. 19 B.1 Pretraining TabSTAR is pretrained employing supervised learning in multi-task regime, jointly learning regression, binary and multiclass classification tasks. The parameters of the architecture are fullyshared, without any need for dataset-specific parameters. Every example during the pretraining updates all the models weights, with the sole exception of the prediction head, for which every example uses its respective head depending on the nature of the task (classification or regression). Sampling For computational efficiency, each dataset is subsampled once before the pretraining. At the example level, we sample up to 300,000 examples from each dataset, stratified by the target variable for classification tasks. Since we only use fraction of each dataset for each pretraining epoch, this decision has negligible influence. In addition, we randomly sample up to 200 features per dataset. While straightforward, this decision is suboptimal as feature importance isnt taken into consideration. As this work does not focus on wide-feature datasets, we consider this trade-off acceptable. Importantly, this setup is enabled during finetuning as the TabSTAR architecture is agnostic to the number of features. We split each dataset into train-validation splits (95%-5%),23 without any need for test splits, and cap the validation set at maximum of 1,000 examples used for evaluating pretraining performance. Batching Every epoch, we randomly sample up to 2,048 examples from each dataset in minibatches of 32, and shuffle all the batches. We conduct gradient accumulation and update the model every 4 steps to reduce the chances of single update being dominated by single dataset, so the global batch size is effectively 128. Appendix B.3.1 elaborates on the effect of batch size. Metrics Our loss function is cross-entropy for classification, and MSE for regression. With standardized targets, R2 1 MSE, although this equivalence is degraded by clipping targets in preprocessing. We train with mixed-precision and apply gradient clipping to stabilize training without task-specific weights, with Appendix B.3.2 discussing the limitations of this approach. We use as metrics AUROC for classification and R2, so for each task the optimal metric value is 1. We average performance across all datasets into single metric that reflects the pretraining performance. Training We pretrain for 50 epochs with the OneCycleLR [64] optimizer, with warmup during the first 5 epochs (10%) and cosine annealing. Early stopping is conducted after 3 epochs without improvement on the pretraining metric. The weight decay is set to 0.001, and max learning rate of lr = 5 105 is applied uniformly across all layers. Appendix B.3.3 discusses experiments with differential learning rate. Pretraining running time varies depending on the number of epochs and the included datasets. The full models (390 datasets) reported in 5 train for less than 48 hours on single NVIDIA A40 GPU (48GB memory), and we believe that this could be optimized much further. B.2 Finetuning We finetune downstream tasks using LoRAs implementation of the peft package24. We use rank of = 32, set α = 2r = 64 and dropout = 0.1. We employ the same scheduler as in the pretraining phase, with the only difference being that we set lr = 0.001, and increase the patience parameter for early stopping to 5. We apply train-test split of 90%-10% and sample validation set of 10%. As opposed to the pretraining, all batches are drawn from the same dataset. Therefore, we observe no effect from changing the mini-batch size when keeping the global batch size fixed to 128. We tune 1,597,440 out of TabSTARs 47,263,874 parameters (3.4%). Finetuning hyperparameters are selected by pretraining TabSTAR over 256 datasets and performing grid-search over held-out set of 25 downstream tasks disjoint from the 50 datasets in the benchmark evaluated in 4. The search space is presented in Table 8, and we observe that average performance is relatively robust across this space. An interesting observation is that decreasing the number of parameters by setting = 16 mildly hurts performance, but it has upsides on memory and latency aspects, allowing future trade-off exploration. As final note, we argue that providing strong default configuration for TFMs is crucial for evaluating them, but for real-world applications, it is still recommendable to find the best hyperparameters tailored to the downstream task. 23We choose only 5% for efficiency, as we use hundreds of pretraining datasets. 24https://github.com/huggingface/peft 20 Table 8: LoRA hyperparameter tuning grid search for TabSTARs finetuning. Search Space Hyper-parameter LoRA rank (r) Learning Rate Dropout 16, 32, 64 0.0005, 0.001, 0.002, 0.005, 0.01 0, 0. The only experiment in this paper where we employ full finetuning instead of LoRA is for the non-pretrained variant discussed in the analysis in 6. For this variant we fully finetune the pretrained model on each downstream task. Compared to the pretraining, we use lr = 2.5 105 and increase the patience to 5. These hyperparameters are lightly tuned using the same procedure as for LoRA, and we observe that fully finetuning the model achieves comparable performance, except for small datasets, where training is more prone to overfitting. B.3 Pretraining Experiments In this section, we briefly elaborate on some experiments performed over TabSTARs pretraining protocol. As in Appendix A.3, we highlight only subset of them in high-level manner. B.3.1 Batch Size During pretraining, we use mini-batch size of 32, each of them drawn from single dataset. Since we train with gradient accumulation and global batch size of 128, varying the batch size affects the diversity of single model update: lower batch sizes are likely to be exposed to more datasets. We decrease the batch size to 16 and 8 and observe an improvement at the cost of slower training. An interesting direction for future work is moving to mixed-datasets batches, which require more complex implementation but might benefit from more regularized learning. Such approach, however, goes against row-level attention methods and ICL, as discussed in Appendix A.3. B.3.2 Loss Weights Pretraining the model over hundreds of datasets in multi-task regime presents key challenge: the loss scale of each dataset can vary substantially, depending on task difficulty or task type. For example, multiclass classification task with dozens of classes will naturally yield higher average loss than binary task. These dynamics can also shift during training. Our default approach naively averages the loss across all datasets, which risks over-weighting tasks for potentially arbitrary reasons. To address this, we explore two alternative weighting strategies: (1) Assigning constant weight per task type, accounting for the number of classes in classification tasks, and (2) Normalizing each datasets contribution by the best loss achieved by CatBoost [57] when being fitted to that dataset. While these strategies better reflect task-specific characteristics, they hardly impact performance and introduce additional complexity. Notably, adjusting loss weights across tasks impacts metric interpretability, as each weighting scheme implicitly optimizes different objective. We do not explore more sophisticated methods such as learning per-dataset weights, as these often require mixed-dataset batches and introduce additional learnable parameters. We believe, however, that multi-task pretraining over tabular datasets remains an open and important research question. B.3.3 Differential Learning Rate TabSTARs weights initialization is not balanced: the textual encoder is pretrained embedding model while the rest of the architecture parameters are randomly initialized. To counteract this imbalance, we experiment with using differential learning rates for the textual encoder layers, and experiment with scaling it by factor of 0.5 and of 0.75. To our surprise, this decision hurts performance, so we stick to uniform learning rate across all layers."
        },
        {
            "title": "C Training Datasets",
            "content": "In this appendix we briefly expand on the pretraining corpus elaborated in 4. It is composed of 350 datasets, spanning all the datasets appearing in AMLB [24], OpenML-CTR23 [22], TabZilla [52] and the ones presented by Grinsztajn [30]. After deduplication,25 this results in 152 datasets (94 classification, 58 regression). Interestingly, only 6 of these 152 datasets have free-text or highcardinality features. We manually add datasets from OpenML [75] and Kaggle, as well as from the AutoML-Benchmark-Train [21] corpus, and achieve total of 350 datasets, with 49 textual datasets. Table 9 details the 253 classification datasets and Table 10 the 97 regression ones. We elaborate the Dataset name, the number of examples n, the number of features m, and the number of classes for classification. In addition, we mark datasets that belong to one of the benchmarks, and the ones that have text features. Importantly, the textual flag is quite permissive, as it includes features with relatively short texts or potentially low predictive power (e.g., people names or addresses). Table 9: The 253 Classification Datasets of the Pretraining Corpus, with their examples, features, classes, presence in benchmark (B) and whether they are textual (T )."
        },
        {
            "title": "Dataset",
            "content": "KDDCup99 mimic_extract_los_3 Online-P2P-Lending sf-police-incidents physionet_sepsis poker-hand Higgs BAF_base Credit_Card_Fraud_ Harry-Potter-fanfiction-data porto-seguro covertype AVIDa-hIL6 airlines HolisticBias albert DBPedia hcdr_main Mental_Health_Dataset Kuzushiji-49 spoken-arabic-digit cdc_diabetes skin-segmentation LT-Vehicle-Loan-Default-Prediction Churn_Telco_Europa ldpa Give-Me-Some-Credit walking-activity social_bias_frames Wikipedia_Talk_Labels Municipal-Debt-Risk-Analysis MiniBooNE nba-shot-logs college_scorecard drug-directory TVS_Loan_Default 4,898,422 4,155,270 2,875,146 2,215,023 1,552,210 1,025,009 1,000,000 1,000,000 1,000,000 648,493 595,212 581,012 573,891 539,383 472,991 425,240 342,781 307,511 292,364 270,912 263,256 253,680 245,057 233,154 190,776 164,860 150,000 149,332 144,649 140,379 138,509 130,064 128,069 124,699 120,215 119,528 40 17 16 8 42 10 28 30 7 13 57 54 3 7 14 78 3 120 16 784 14 21 3 38 17 6 10 4 16 12 13 50 15 117 16 29 20 68 5 2 2 10 2 2 2 4 2 7 2 2 4 2 219 2 5 49 10 2 2 2 2 11 2 22 3 15 2 2 2 2 7 2 Continued on next page 25And the exclusion of the fifa dataset, which is included in the benchmark. 22 Table 9: The 253 Classification Datasets of the Pretraining Corpus."
        },
        {
            "title": "Dataset",
            "content": "n road-safety Diabetes130US fars Credit_Score_Classification numerai28.6 Run_or_walk_information jannis KDD98 APSFailure kick human-choice-prediction Traffic_violations Fashion-MNIST Cardiovascular-Disease-dataset mnist_784 connect-4 mobile_churn helena LICD CIFAR_10 REASONER volkert shuttle GTSRB-HueHist okcupid-stem KDDCup09-Upselling KDDCup09_appetency adult League-of-Legends-Diamond tamilnadu-electricity bank-marketing meta_stream_intervals jungle_chess Dynamically-Generated-Hate-Speech-Dataset Breast-cancer-prediction Click_prediction_small Hotel-Reviews electricity nomao Employee-Turnover-at-TECHCO Amazon_employee_access Credit-Risk-Dataset Default-of-Credit-Card-Clients-Dataset funpedia credit_risk_china Insurance guillermo riccardo insurance_dataset letter game-of-thrones-script-all-seasons NewspaperChurn mozilla4 pol 111,762 101,766 100,968 100,000 96,320 88,588 83,733 82,318 76,000 72,983 71,579 70,340 70,000 70,000 70,000 67,557 66,469 65,196 63,634 60,000 58,497 58,310 58,000 51,839 50,789 50,000 50,000 48,842 48,651 45,781 45,211 45,164 44,819 41,144 39,998 39,948 38,932 38,474 34,465 34,452 32,769 32,581 30,000 29,819 27,522 23,548 20,000 20,000 20,000 20,000 16,825 15,855 15,545 15,000 32 46 29 26 21 6 54 477 169 32 20 20 784 11 719 42 63 27 413 3,072 34 147 9 256 19 13,419 207 14 14 2 16 74 6 8 11 11 3 8 118 9 9 11 23 3 27 10 4,281 4,283 26 16 5 16 5 2 3 8 3 2 2 4 2 2 2 2 3 10 2 10 3 2 100 2 10 2 10 7 43 3 2 2 2 2 20 2 11 3 2 2 2 2 2 2 2 2 2 2 3 5 2 2 2 4 26 43 2 2 11 Continued on next page 23 Table 9: The 253 Classification Datasets of the Pretraining Corpus."
        },
        {
            "title": "Dataset",
            "content": "eeg-eye-state MagicTelescope nursery online-shoppers-intention Disaster-Tweets mammography PhishingWebsites Binary-Dataset-of-Phishing-and-Legitimate-URLs pendigits WBCAtt artificial-characters internet_usage robert dilbert shrutime JapaneseVowels GesturePhaseSegmentationProcessed FICO-HELOC-cleaned IBRD_Loans_Classification Indian_pines SpeedDating fabert mushroom isolet eye_movements twonorm blastchar musk first-order-theorem-proving HMEQ_Data philippine optdigits BachChoralHarmony page-blocks wall-robot-navigation christine phoneme Is_fraud sylvine Satellite Multiclass_Classification_for_Corporate_Credit Personal-Loan-Modeling churn waveform-5000 air-quality-and-pollution-assessment Heart_Failure_Prediction compas-two-years wine-quality-white wilt spambase StackOverflow-polarity hiva_agnostic Fraud-Detection-Updated ada 14,980 13,376 12,958 12,330 11,370 11,183 11,055 11,000 10,992 10,298 10,218 10,108 10,000 10,000 10,000 9,961 9,873 9,871 9,215 9,144 8,378 8,237 8,124 7,797 7,608 7,400 7,043 6,598 6,118 5,960 5,832 5,620 5,586 5,473 5,456 5,418 5,404 5,227 5,124 5,100 5,000 5,000 5,000 5,000 5,000 5,000 4,966 4,898 4,839 4,601 4,423 4,229 4,156 4,147 14 10 8 17 4 6 30 14 16 11 7 71 7,200 2,000 10 14 32 23 6 220 120 795 21 617 23 20 19 167 51 12 308 62 15 10 24 1,611 5 19 20 36 7 12 20 40 9 12 11 11 5 57 1 1,617 27 46 2 2 4 2 2 2 2 2 10 5 10 46 10 5 2 9 5 2 10 8 2 7 2 26 2 2 2 2 6 2 2 10 68 5 4 2 2 2 2 2 10 2 2 3 4 2 2 7 2 2 3 2 2 2 Continued on next page 24 Table 9: The 253 Classification Datasets of the Pretraining Corpus."
        },
        {
            "title": "Dataset",
            "content": "analcatdata_supreme hypothyroid Bioresponse Internet-Advertisements led24 kr-vs-kp splice dna gina madeline jasmine cjs madelon ozone-level-8hr segment cardiotocography Estimation_of_Obesity_Levels kc1 Corporate-Credit-Rating mfeat-factors South_Asian_Churn_dataset mfeat-zernike mfeat-fourier pbcseq steel-plates-fault car GAMETES_Heterogeneity one-hundred-plants-texture audit-data OVA_Breast amazon-commerce-reviews yeast cmc ibm-employee-attrition pc4 Data_Science_Nigeria_Telecoms_Churn hepatitis_c_virus_hcv_for_egyptian_patients Bank-Note-Authentication-UCI baseball Titanic mental-health-in-tech-survey hill-valley Heart-Disease-Dataset-(Comprehensive) volcanoes-e1 Airlines-Tweets-Sentiments MiceProtein cnae-9 solar_flare qsar-biodeg SOCC rmftsa_sleepdata autoUniv-au1-1000 collins credit-g 4,052 3,770 3,751 3,279 3,200 3,196 3,190 3,186 3,153 3,140 2,984 2,796 2,600 2,534 2,310 2,126 2,111 2,109 2,026 2,000 2,000 2,000 2,000 1,945 1,941 1,728 1,600 1,599 1,552 1,545 1,500 1,484 1,473 1,470 1,458 1,400 1,385 1,372 1,340 1,309 1,259 1,212 1,190 1,183 1,097 1,080 1,080 1,058 1,055 1,043 1,024 1,000 1,000 1,000 B 7 27 1,776 1,558 24 36 60 180 970 259 144 29 500 72 16 23 16 21 30 216 13 47 76 18 27 6 20 64 35 10,935 10,000 8 9 31 37 14 28 4 16 13 26 100 11 3 1 77 856 9 41 13 2 20 19 20 10 3 2 2 10 2 3 3 2 2 2 6 2 2 7 10 7 2 8 10 2 10 10 3 7 4 2 100 2 2 50 10 3 2 2 2 4 2 3 2 2 2 2 5 3 8 9 5 2 4 4 2 30 2 Continued on next page 25 Table 9: The 253 Classification Datasets of the Pretraining Corpus."
        },
        {
            "title": "Dataset",
            "content": "vowel The-Estonia-Disaster-Passenger-List xd6 tokyo1 tic-tac-toe Tour-and-Travels-Customer-Churn-Prediction acp-breast-cancer oil_spill anneal Cervical_Cancer_Risk_Factors vehicle analcatdata_authorship glioma_grading_clinical_and_mutation_features analcatdata_dmft regensburg_pediatric_appendicitis QSAR_Bioconcentration_classification Diabetes_Dataset blood-transfusion-service-center eucalyptus breast-w Australian soybean profb Student_Performance balance-scale Loan-Predication monks-problems-2 synthetic_control ilpd micro-mass wdbc arsenic-male-lung cylinder-bands climate-model-simulation-crashes water-treatment Early-Stage-Diabetes-Risk-Prediction-Dataset dresses-sales irish arrhythmia wholesale-customers vote cars chronic-kidney-disease differentiated_thyroid_cancer_recurrence colic breast-cancer qualitative-bankruptcy us-2020-presidential-election-speeches audiology bone_marrow_transplant_children darwin tae EgyptianSkulls lymph 990 989 973 959 958 954 949 937 898 858 846 841 839 797 780 779 768 748 736 699 690 683 672 666 625 614 601 600 583 571 569 559 540 540 527 520 500 500 443 440 435 406 400 383 368 286 250 245 192 187 174 151 150 148 12 6 9 42 9 6 1 48 18 30 18 70 23 4 55 12 8 4 19 9 14 35 8 11 4 11 6 60 10 1,082 30 4 34 18 36 16 12 5 262 7 16 7 25 16 26 9 6 5 57 36 450 5 4 18 11 2 2 2 2 2 4 2 5 5 4 4 2 6 3 3 2 2 5 2 2 19 2 4 3 2 2 6 2 20 2 2 2 2 2 2 2 2 10 2 2 3 2 2 2 2 2 7 8 2 2 3 5 3 Continued on next page 26 Table 9: The 253 Classification Datasets of the Pretraining Corpus."
        },
        {
            "title": "Dataset",
            "content": "arcene 100 9,920 2 Table 10: The 93 Regression Datasets of the Pretraining Corpus, with their examples, features, presence in benchmark (B) and whether they are textual (T )."
        },
        {
            "title": "Dataset",
            "content": "delays_zurich_transport New-York-Citi-Bike-Trip USA-Airport-Dataset New-York-Taxi-Trip Buzzinsocialmedia_Twitter nyc-taxi-green-dec-2016 515K-Hotel-Reviews-Data-in-Europe dionis Yolanda Allstate_Claims_Severity Football_players_Fifa_stats black_friday medical_charges football-manager-data wave_energy video_transcoding dating_profile diamonds sarcos physiochemical_protein fried 2dplanes mv Perth-House-Prices cps88wages fps_benchmark news_popularity2 house_16H health_insurance house_sales superconductivity california_housing avocado-sales Bike_Sharing_Demand elevators FIFA20-Players miami_housing naval_propulsion_plant Brazilian_houses German-House-Prices sulfur climate_change_impact grid_stability Credit-Card-Dataset-for-Clustering topo_2_1 yprop_4_1 5,465,575 4,500,000 3,606,803 2,083,778 583,250 581,835 515,738 416,188 400,000 188,318 183,142 166,821 163,065 159,541 72,000 68,784 59,946 53,940 48,933 45,730 40,768 40,768 40,768 33,656 28,155 24,624 24,007 22,784 22,272 21,613 21,263 20,640 18,249 17,379 16,599 14,999 13,932 11,934 10,692 10,552 10,081 10,000 10,000 8,949 8,885 8,885 14 7 14 21 77 18 16 54 100 130 37 9 3 87 32 18 30 9 21 9 10 10 10 17 6 39 4 16 11 21 81 8 11 12 18 72 15 14 11 24 5 14 12 16 261 212 Continued on next page 27 Table 10: The 93 Regression Datasets of the Pretraining Corpus. T 13 32 8 21 32 36 44 14 1,024 1,024 10 4,735 114 364 8 4 6 4 3 7 126 5 80 79 43 6 14 5 116 116 8 9 9 6 17 8 8 30 6 11 21 12 10 13 7 10 14 22 14 13"
        },
        {
            "title": "Dataset",
            "content": "seoul_bike_sharing_demand_cat pumadyn32nh kin8nm cpu_activity bank32nh Pollen-Luxembourg-1992-2018 colleges wind QSAR-TID-10980 QSAR-TID-11 Myanmar-Air-Quality Santander_transaction_value SAT11-HAND-runtime-regression Mercedes_Benz_Greener_Manufacturing abalone pollen space_ga scotch-whiskey-reviews-update-2020 quake auction_verification us_crime airfoil_self_noise house_prices house_prices_nominal NBA-PLAYERS2016-2019 Insurance-Premium-Data Moneyball socmob MIP-2016-regression geographical_origin_of_music concrete_compressive_strength Household-monthly-electricity-bill stock QSAR_fish_toxicity cars energy_efficiency kdd_el_nino-small student_performance_por strikes sensory meta forest_fires rmftsa_ladata boston no2 Diabetes(scikit-learn) NBA-2k20-player-dataset baseball-hitter bodyfat Lisbon-House-Prices tecator 8,760 8,192 8,192 8,192 8,192 7,784 7,063 6,574 5,766 5,742 5,122 4,459 4,440 4,209 4,177 3,848 3,107 2,247 2,178 2,043 1,994 1,503 1,460 1,460 1,408 1,338 1,232 1,156 1,090 1,059 1,030 1,000 950 908 804 768 709 649 625 576 528 517 508 506 500 442 439 263 252 246"
        },
        {
            "title": "D Benchmark Datasets",
            "content": "This appendix elaborates on the benchmark presented in 4. We consider all datasets proposed by AutoML Multimodal Benchmark (SHI) [60], Vectorizing (VEC) [31], and CARTE-Benchmark (CRT) [45], resulting in final set of 50 datasets. We deduplicate datasets that appear as-is in more than one benchmark. In addition, since CARTE explores the concept of multi-table learning, they introduce highly-overlapping datasets for which we remove one variant (see 4.3 and B.2 in their paper). Table 11 presents the classification datasets and Table 12 the regression ones. Each table includes an internal ID used for reference, the Dataset name, the number of examples and of features m, and the number of classes for classification.26 Finally, we also indicate the benchmark sources where each dataset appears. In addition, Table 13 presents the full benchmark with short description per dataset, and Table 14 details the datasets removed during the deduplication process. Most of the excluded datasets are regression datasets from the CARTE-Benchmark, because of its high-overlapping nature. Table 11: The 14 classification datasets of the benchmark, with their examples, features, classes, and presence in the SHI, VEC and CRT benchmarks. ID"
        },
        {
            "title": "Dataset",
            "content": "n C01 women_clothing_review us-accidents C02 data_scientist_salary C03 imdb_genre_prediction C04 product_sentiment_machine_hack C05 C06 google_qa_question_type_reason C07 michelin-guide-restaurants-2021 fake_job_postings2 C08 jigsaw_unintended_bias100K C09 yelp-reviews-dataset C10 C11 news_channel C12 wine_reviews C13 C14 melbourne_airbnb kick_starter_funding 18,788 7,728,394 15,841 800 5,091 4,863 17,735 12,725 100,000 10,000 20,284 84,123 86,502 18,316 10 42 6 11 2 39 11 5 40 5 17 5 9 5 4 6 2 4 5 5 2 2 5 6 30 2 10 SHI VEC CRT"
        },
        {
            "title": "E Baselines",
            "content": "In this appendix we first discuss models excluded from the evaluation due to data leakage concerns, and then cover implementation details for the baselines used in our main experiments 4. E.1 Excluded Baselines As opposed to GDBTs or single-dataset deep learning methods, evaluating pretrained tabular models introduces additional complexity. Indeed, leakage can come in multiple forms. When LLMs are involved, there is risk of memorization [8], and models trained on synthetic datasets [37] which try to mimic real-world distributions, can be unintentionally biased towards popular benchmarks. While these two forms of leakage are subtle and hard to detect, more direct form must be strictly avoided: When the same dataset (or variant of it) is used during pretraining, and then it is evaluated as downstream task. In such scenario there is inevitable severe data leakage, especially when running with multiple random test splits. The rest of the section explains how both TP-BERTa [80] and CM2 [82] suffer from such contamination with respect to our benchmark. As we briefly mention in 7, we advocate for improving TFM research by encouraging models that are practical to evaluate, by releasing several versions of each model, and providing default hyperparameters. TP-BERTa We exclude TP-BERTa from our evaluation for two key reasons. First, their implementation assumes that every example is treated as serialized single sequence, which allows for 26We treat ranking problems with up to 10 discrete values as multiclass problems. 29 Table 12: The 36 regression datasets of the benchmark, with their examples, features, and presence in the SHI, VEC and CRT benchmarks. ID"
        },
        {
            "title": "Dataset",
            "content": "n filmtv-movies-dataset free-7-million-company-dataset used-car-prices-in-pakistan bookprice_prediction ae_price_prediction used-cars-dataset-cardekho second-hand-mercedes-benz animeplanet-recommendation employee_salaries spotify-tracks-dataset california_house_price fifa coffee-scrap-coffeereview R01 R02 R03 R04 ML/DS-Salaries R05 Babies-R-Us R06 R07 R08 R09 R10 R11 BikeWale R12 R13 R14 R15 Employee-remuneration R16 R17 R18 museums R19 R20 wikiliq-dataset R21 R22 R23 R24 R25 R26 R27 Goodreads R28 Rotten-Tomatoes R29 R30 R31 R32 R33 mercari_price_suggestion100K 100,000 R34 wine-price-on-polish-market R35 37,814 16,392 14,391 119,628 5,085 9,228 114,000 37,951 19,178 2,440 9,003 72,655 4,989 22,662 44,574 41,399 7,173,426 22,290 8,650 12,569 3,197 1,647 16,598 41,665 45,460 1,669 3,967 7,158 8,035 4,105 31,136 1,795 beer-profile-and-ratings korean-drama videogamesales zomato-bangalore-restaurants the-movies-dataset nba-draft-basketball saudi-arabia-used-cars-dataset top-ramen-ratings-2022 Journal-Score-SJR chocolate-bar-ratings clear-corpus jc_penney_products 2,247 4,724 10,860 vivino-wine-data 112 7 14 9 12 11 18 39 28 17 6 9 8 12 5 17 7 21 6 12 24 9 5 15 20 22 14 15 12 4 21 8 9 18"
        },
        {
            "title": "SHI VEC CRT",
            "content": "maximum length of 512 tokens, as elaborated in Appendix A.1. While this decision is efficient for datasets with low amount of features and no free-text presence, around half of the datasets in our benchmark are too long for that limitation, as they either contain too many features or long free-texts. Second, TP-BERTas pretraining uses datasets that appear in our evaluation set, as listed in Table 6 of their paper [80]. It is evident that several datasets overlap directly with datasets in our benchmark 4 (e.g., 1510_fifa, 1368_IMDb-Ratings, 1639_Melbourne), disqualifying them for our purposes. Furthermore, we observe concerning overlap between their pretraining and downstream task datasets (e.g., airlines, sf police and diabetes). We believe that this questions the validity of their evaluation, and that such contamination poses serious challenge for the TFM community which could be substantially addressed by better tabular data repositories [73]. CM2 CM2 was pretrained over OpenTabs, compilation of more than 2,000 datasets drawn from public tabular data repositories, including OpenML and Kaggle. While this collection is valuable, pretraining model over these datasets compromises further evaluation of any of them. Naturally, the overlap with our benchmark here is extremely high, making it infeasible to use as 30 ID"
        },
        {
            "title": "Description",
            "content": "Table 13: Benchmark Datasets Description FIFA 2022 Players Wages Salaries of ML/DS Professionals Worldwide Prices Prediction for baby product from Babies Us website C01 Women Clothing E-Commerce Reviews C02 US Accidents between 2016 and 2023 Indian Data Scientist Salary Prediction C03 IMDB Movies Genre Prediction C04 C05 Product Sentiment Analysis C06 Google QA Question Type Reason Explanation C07 Michelin Guide Restaurants Awards C08 Fake Job Posting Detection C09 Online Social Media Comments Toxicity C10 YELP Dataset Reviews C11 News Channel Prediction C12 Wine Reviews for Variety Prediction C13 Kickstarter Funding Prediction C14 Melbourne AirBnB Listings R01 User cars and listing price in the website Cardekho R02 Second-hand cars Mercedes Benz price Italy R03 Anime-Planet Recommendation Database 2020 R04 R05 R06 Employee Salary in Montgomery County, MD R07 Spotify Tracks Popularity R08 California Houses 2020 Prices R09 R10 Coffee Review Rating R11 Bike and scooters from bikewale website in India R12 Used car prices in Pakistan 2021 R13 Book Price Prediction R14 American Eagle Retailer Price Prediction R15 Employee Remuneration and Expenses - Vancouver R16 R17 Company size prediction R18 General information on the US museums R19 Vivino Spanish Wine Data R20 WikiliQ - Alcohol dataset (May, 2022) R21 Tasting profiles and consumer reviews for beers R22 Korean Dramas R23 Video Games Sales R24 Zomato Restaurants in Bengaluru R25 Metadata of movies released until 2017 for box-office revenues R26 NBA Draft Basketball Player Data 1989-2021 R27 Books ratings R28 Rotten Tomatoes Movie Ratings R29 R30 Ramen Ratings R31 Academic impact for Scientific Journals R32 Chocolate Bar expert ratings R33 Mercari Online Marketplace Product Prices R34 R35 Readability scores for text passages spanning various genres and time periods R36 Saudi Arabia Used Cars Price from Syarah Website Information about wines on the polish market JC Penney Product Prices in Retailer Website FilmTV movies ataset rating 31 Table 14: Excluded datasets, with their benchmark origin and reason for removal: (1) Duplicate dataset, (2) Unavailable, for datasets with inconvenient or unavailable hosting outside tabular repositories, and (3) Pretraining, for two (regression) datasets mistakenly used for the pretraining. Dataset Benchmark Reason Duplicate google_qa_answer_type news_popularity2 US Presidential Journal Influence Buy Buy Baby Bikedekho Journal Score JCR Japanese Anime Mydramalist Prescription Drugs Roger Ebert US Presidential Used Cars 24 Whisky Wine.com WineEnthusiasts"
        },
        {
            "title": "Duplicate\nPretraining\nUnavailable\nDuplicate\nDuplicate\nDuplicate\nDuplicate\nDuplicate\nDuplicate\nUnavailable\nUnavailable\nUnavailable\nDuplicate\nPretraining\nDuplicate\nDuplicate",
            "content": "google_qa_question_type Journal-Score-SJR Babies-R-Us BikeWale Journal-Score-SJR animeplanet-recommendation korean-drama used-car-prices-in-pakistan wine_reviews wine_reviews baseline. Interestingly, their repository27 lists TP-BERTa as method trained on subset of OpenTabs, reinforcing that the leakage is shared between the models. E.2 Baselines Implementation and Hyperparameters This section outlines the implementation and hyperparameter tuning strategy used for the baselines reported in 4. While each baseline has its own model-specific preprocessing pipeline, we apply two shared preprocessing steps to both TabSTAR (as detailed in Appendix A.1) and all the baselines: (1) We perform date preprocessing by using skrubs DatetimeEncoder, and (2) Apply clipped z-score transformation for target variables in regression datasets. Textual Feature Handling CARTE natively supports textual inputs, and the TabPFN-v2 API client28 does as well, although its implementation details remain undisclosed. On the other hand, GBDTs do not natively support free-text features,29, so we preprocess these features into fixed-size embeddings using skrubs TextEncoder, which internally applies frozen e5-small-v2 encoder to each semantic column. This aligns with the encoder used in TabSTAR, enabling fair comparison across models. There are, however, two key differences in how TabSTAR handles these embeddings: First, the embeddings are specifically finetuned for the task, contributing significantly to its strong performance as shown in 5 and further analyzed in G.2. The second detail is that skrub applies dimensionality reduction to 30 dimensions, as proposed by [31]. This compressed representation performs comparably to the full embedding space, while offering improved inference efficiency. E.2.1 TabPFN-v2 We run TabPFN-v2 using their API client which supports text features.30 While the intrinsic details of their textual handling remain undocumented, its reasonable to assume that it resembles the processing we apply to GBDTs, as their model leverages ICL and their architecture has no textual encoder. 27https://github.com/Chao-Ye/CM2 28https://github.com/PriorLabs/tabpfn-client 29CatBoost includes built-in text module, but it underperforms compared to dense text embeddings. 30We use v2.0.8, the latest version available at the time of running the experiments. E.2.2 CARTE We run CARTE using its package,31 which inherently performs k-fold cross-validation. After consulting with the authors, we set = 5 for efficiency instead of their default, 10. We do grid search over their recommended learning rates,32 and we take the best-performing variant per dataset split. E.2.3 CatBoost and CatBoost-Tuned We run CatBoost using the catboost package33 and run the default configuration suggested by [25] by setting early_stopping_rounds = 50, od_pval = 0.001, iterations = 2000. For the tuned version, we use the Optuna package34 with random search, with budget of 4 hours for every run and parallelizing trials on 8 CPU cores. We use 5-fold cross-validation and take the best configuration selected based on this mean score. We use it then to retrain the model on the full training data. For hyperparameter space, we follow the hyperparameter search suggested by [37] as detailed in Table 15. Table 15: CatBoost-Tuned Hyperparameters Search Space Hyperparameter Search Space log U(e5, 1) U{1, 2, . . . , 20} log U(1, 10) U(0.0, 1.0) lr random_strength l2_leaf_reg bagging_temperature leaf_estimation_iterations U{1, 2, . . . , 20} iterations U{100, 101, . . . , 4000} E.2.4 XGBoost and XGBoost-Tuned We run XGBoost using the xgboost package35 and follow the same procedure as for CatBoost, except additional preprocessing (e.g., transforming categorical variables into one-hot encodings). For the default configuration, we follow the suggestion of [25] and use: booster = gbtree, early_stopping_rounds = 50, n_estimators = 2000. For the tuned variant, we follow the hyperparameter search space suggested by [37], as shown in Table 16. Table 16: XGBoost-Tuned Hyperparameters Search Space Hyperparameter Search Space log U(e7, 1) learning_rate U{1, 2, . . . , 10} max_depth U(0.2, 1) subsample U(0.2, 1) colsample_bytree colsample_bylevel U(0.2, 1) min_child_weight alpha reg_lambda gamma n_estimators log U(e16, e5) log U(e16, e2) log U(e16, e2) log U(e16, e2) U{100, 101, . . . , 4000} E.2.5 Random Forest We treat Random Forest as weak baseline to establish lower-bound reference for each dataset split. We run it with the sklearn package 36 and use its default configuration with n_estimators = 100. 31https://github.com/soda-inria/carte 32{2.5 104, 5 104, 7.5 104, 2.5 103, 5 103, 7.5 103} 33https://pypi.org/project/catboost/ 34https://pypi.org/project/optuna/ 35https://pypi.org/project/xgboost/ 36https://scikit-learn.org/ 33 Table 17: Classification performance per dataset (up to 10K). The top performance score is bolded first, and then all scores are rounded. We report average AUROC with 95% CIs. ID C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 CARTE CatB CatB-T RF TabPFN TabSTAR XGB XGB-T 88.40. 82.50.3 88.50.9 80.91.5 90.10.4 90.81.6 78.40.5 96.00.2 70.90.8 90.20.3 97.30.5 82.00.3 84.51.9 90.61.0 81.31.3 90.30.3 93.00.9 99.80.1 86.70.2 79.70.5 97.50.1 73.71.1 83.50.3 90.30.4 97.40.4 81.20.6 85.41.8 90.90.5 82.61.1 90.60.3 93.21.0 99.80.1 87.00.2 80.70.4 97.70.1 74.11.0 84.00.5 88.80.3 96.30.5 77.30.3 82.43.0 88.01.1 73.61.2 85.10.7 90.21.3 99.60.2 84.60.2 76.20.6 95.30.2 71.11.1 80.10. 90.30.3 82.40.3 88.31.5 91.20.7 87.70.5 89.80.4 91.31.1 99.80.1 87.60.4 81.40.4 72.30.9 90.80.3 97.90.5 83.00.3 83.72.3 91.30.8 87.00.6 91.50.3 93.51.5 99.30.1 89.10.2 79.20.5 98.30.1 75.00.7 84.00.3 89.30.4 97.20.3 80.50.3 80.72.9 88.11.1 81.51.0 87.20.6 91.91.1 99.70.1 85.40.3 78.20.6 96.60.2 70.20.8 81.40.4 90.20.4 97.60.3 82.10.3 85.42.0 90.30.6 83.70.9 89.30.5 94.40.7 99.80.1 86.80.2 80.20.5 97.30.1 74.11.1 83.20."
        },
        {
            "title": "F Extended Main Results",
            "content": "In this appendix we provide the main results for the experiment as reported in 5. As elaborated in 4, each model is evaluated on each dataset across 10 splits. Since performance scales vary between datasets, we follow the normalization approach proposed by [37], rescaling all scores to the [0, 1] range. The final reported performance for each model is the average over these normalized runs, and we compute 95% confidence intervals using the standard normal approximation: ˆµ 1.96 ˆσ . In this section, we often use abbreviated model names for conciseness: We refer to CatBoost variants as CatB and CatB-T, with the latter being the Tuned version. Similarly, we use XGB and XGB-T for XGBoost. We maintain the abbreviation of Random Forest (RF) and shorten TabPFN-v2 to simply TabPFN. For models that run on an unlimited number of training examples, we add -U suffix, i.e. TabSTAR-U, CatB-T-U, and XGB-T-U. F.1 Dataset Level Performance We report AUROC for classification and R2 for regression, with 95% CIs computed over the 10 runs for each dataset. Tables 17 and 18 summarize classification performance on datasets with up to 10K and over 10K examples, respectively. Tables 19 and 20 to the same for regression tasks. For conciseness, datasets are referred by their ID from Appendix D. As discussed in 5, TabPFN-v2 is unable to run over 4 datasets: C12, because it is multiclass problem with more than 10 classes, and C02, C14 and R01 because they support inference for up to 500,000 cells. Attempts to run the model over subset of the examples led to significantly worse performance, and thus we decide not to report them to allow fair comparison. Furthermore, CARTE is unable to run over 15 of the datasets in the benchmark due known bug37 in their implementation for the PowerTransformation, which struggles in the presence of features with too less unique values. F.2 Head-to-head comparisons We compare the performance of TabSTAR against each of the models in head-to-head comparisons. We report win rate, which can be seen as private case of the normalized metric with only two models. We exclude failed runs when comparing to CARTE and TabPFN-v2. Table 21 shows the performance of TabSTAR against all models competing up to 10K examples, for both regression and classification, with 95% CIs over the win rate. Table 22 does the same for TabSTAR-Unlimit. 37https://github.com/soda-inria/carte/issues/23 Table 18: Classification performance per dataset (above 10K). The top performance score is bolded first, and then all scores are rounded. We report average AUROC with 95% CIs. TabSTAR TabSTAR-U XGB-T CatB-T-U TabPFN XGB-T-U CatB-T ID C01 C02 C03 C07 C08 C09 C11 C12 C13 90.30.4 97.40.4 81.20.6 90.60.3 93.21.0 99.80.1 80.70.4 97.70.1 74.11.0 84.00.5 90.50.4 98.30.3 81.50.8 91.00.4 93.11.2 99.80.1 81.80.4 98.50.1 76.70.8 84.80.4 90.30.3 82.40.3 89.80.4 91.31.1 99.80.1 81.40.4 72.30.9 90.80.3 97.90.5 83.00.3 91.50.3 93.51.5 99.30.1 79.20.5 98.30.1 75.00.7 84.00. 91.20.3 98.40.2 83.80.5 91.90.3 95.10.9 99.50.1 81.00.5 99.10.1 77.91.0 85.10.3 90.20.4 97.60.3 82.10.3 89.30.5 94.40.7 99.80.1 80.20.5 97.30.1 74.11.1 83.20.4 90.40.3 98.20.3 82.30.3 89.90.5 94.40.9 99.80.1 81.40.4 98.40.1 76.90.9 84.20.4 Table 19: Regression performance per dataset. The top performance score is bolded first, and then all scores are rounded. We report average R2 with 95% CIs. ID R01 R02 R03 R04 R05 R06 R07 R08 R09 R10 R11 R12 R13 R14 R15 R16 R17 R18 R19 R20 R21 R22 R23 R24 R25 R26 R27 R28 R29 R30 R31 R32 R33 R34 R35 CARTE CatB CatB-T RF TabPFN TabSTAR XGB XGB-T 1000.0 71.80.5 93.20.8 97.70.8 71.61.3 93.20.8 89.20.5 99.50.2 52.82.1 96.60.2 98.70.0 95.32.0 98.10. 96.40.5 92.31.2 85.01.8 86.00.6 94.30.6 99.80.1 81.91.6 52.52.6 23.84.2 92.10.3 46.61.5 85.90.6 96.30.9 1000.0 98.31.0 73.80.4 86.27.4 89.51.3 98.10.5 66.90.8 93.00.7 89.20.6 99.00.6 94.20.9 98.50.3 57.62.7 97.30.1 79.50.9 98.70.0 94.82.4 98.20.4 85.50.8 96.00.6 92.51.1 45.25.5 85.41.2 84.00.8 95.40.5 99.40.1 85.21.3 53.42.7 94.40.7 22.74.1 92.10.4 28.86.3 47.41.7 90.92.5 84.40.5 95.20. 1000.0 98.30.9 74.10.3 85.17.1 90.01.2 98.20.5 67.80.8 93.20.7 89.30.4 98.90.6 94.00.9 98.50.2 58.22.6 97.30.1 79.91.1 98.70.0 95.22.0 98.20.4 85.80.9 96.20.5 92.61.1 45.25.9 85.51.1 85.80.7 95.30.5 99.40.1 85.31.4 53.32.8 94.40.7 24.63.9 92.00.4 28.25.8 47.81.7 91.42.1 84.50.5 95.50.9 1000.0 98.01.0 68.40.6 85.16.8 86.71.3 97.21.0 61.51.2 92.20.7 88.60.7 98.10.3 92.51.3 98.00.3 51.83.5 96.80.1 77.31.2 97.70.1 94.22.5 97.70.4 85.30.9 95.70.6 91.81.1 39.16.4 81.81.5 79.71.0 94.80.5 99.00.2 84.81.3 46.02.7 93.01.0 23.53.5 89.90.5 28.65.9 40.21.6 88.73.1 81.20.4 94.31.0 35 98.40.9 74.90.4 86.15.2 92.70.9 98.50.6 62.21.3 93.90.7 89.80.5 99.10.3 94.50.9 98.50.2 53.82.7 96.20.2 79.41.0 98.80.0 95.12.1 93.11.3 84.01.1 93.91.0 93.31.1 43.85.5 84.81.6 70.31.6 86.71.0 99.80.1 82.21.6 61.72.1 95.70.6 20.94.8 93.20.3 26.25.4 44.91.6 92.31.8 85.20.7 91.21.1 1000.0 98.01.1 70.90.8 81.58.7 93.21.2 98.10.3 71.11.8 92.80.6 88.80.5 99.50.1 93.91.1 98.30.2 53.43.0 96.40.1 78.91.5 98.70.0 94.72.3 97.40.6 83.81.1 95.60.9 92.31.0 39.75.5 83.01.6 81.81.8 94.80.5 99.60.1 82.01.6 51.52.9 94.30.9 15.75.0 91.70.4 19.45.6 46.01.8 89.12.9 85.70.8 95.70.8 99.90.1 97.91.1 69.10.5 85.87.5 87.11.4 97.10.7 61.91.5 92.30.7 88.20.7 98.60.3 93.31.1 98.20.3 49.03.3 97.10.1 76.81.2 97.70.1 94.52.6 97.50.8 83.91.0 95.30.9 91.61.0 36.17.1 83.31.2 82.90.7 94.60.6 99.10.2 83.21.2 45.32.3 93.40.8 17.54.2 90.30.5 22.77.3 40.32.1 88.93.6 81.60.8 94.40. 1000.0 98.30.9 73.70.5 87.53.7 90.11.3 97.90.7 68.80.8 93.10.7 89.30.5 99.40.2 94.20.8 98.50.2 57.52.7 97.10.1 80.31.0 97.80.1 95.41.9 98.30.4 82.42.0 96.20.5 92.61.0 46.85.8 85.11.2 85.90.9 95.40.5 99.50.1 85.51.2 53.52.7 94.50.8 25.53.7 92.00.4 31.66.2 47.91.6 91.62.1 84.40.3 95.50.9 Table 20: Regression performance per dataset (above 10K). The top performance score is bolded first, and then all scores are rounded. We report average R2 with 95% CIs. CatB-T-U TabPFN TabSTAR TabSTAR-U XGB-T XGB-T-U CatB-T ID R01 R02 R03 R04 R07 R08 R09 R12 R14 R15 R16 R17 R18 R20 R23 R24 R25 R31 R33 R36 1000.0 98.30.9 74.10.3 85.17.1 67.80.8 93.20.7 89.30.4 98.50.2 97.30.1 79.91.1 98.70.0 95.22.0 98.20.4 96.20.5 85.51.1 85.80.7 95.30.5 92.00.4 47.81.7 95.50.9 1000.0 98.80.9 75.10.4 91.30.8 85.20.3 93.80.6 89.50.4 98.90.1 97.80.1 86.50.7 98.80.0 97.60.6 99.00.2 96.30.4 86.30.7 97.10.3 95.90.4 93.00.3 55.91.1 95.50.9 98.40.9 74.90.4 86.15.2 62.21.3 93.90.7 89.80.5 98.50.2 96.20.2 79.41.0 98.80.0 95.12.1 93.11.3 93.91.0 84.81.6 70.31.6 86.71.0 93.20.3 44.91.6 91.21.1 1000.0 98.01.1 70.90.8 81.58.7 71.11.8 92.80.6 88.80.5 98.30.2 96.40.1 78.91.5 98.70.0 94.72.3 97.40.6 95.60.9 83.01.6 81.81.8 94.80.5 91.70.4 46.01.8 95.70.8 1000.0 98.41.0 72.30.6 91.00.7 79.91.8 93.30.6 89.20.6 98.40.2 96.80.2 83.91.4 98.80.0 97.60.6 97.70.6 95.31.0 85.21.7 95.50.7 95.10.5 92.70.4 57.21.3 95.50. 1000.0 98.30.9 73.70.5 87.53.7 68.80.8 93.10.7 89.30.5 98.50.2 97.10.1 80.31.0 97.80.1 95.41.9 98.30.4 96.20.5 85.11.2 85.90.9 95.40.5 92.00.4 47.91.6 95.50.9 1000.0 98.80.9 74.60.6 91.30.6 86.10.7 93.90.5 89.50.4 98.90.2 97.50.1 87.00.7 98.00.1 97.70.6 98.90.3 96.30.5 86.10.8 97.30.3 95.80.5 92.90.3 55.71.3 95.50.9 Table 21: Win rates of TabSTAR (up to 10K) against baselines. Win rate with 95% CI. XGB-T TabPFN CARTE CatB-T XGB CatB RF Classification Regression 93.35.2 35.75.9 73.67.3 33.64. 67.97.8 32.24.8 88.65.3 66.94.9 57.39.3 40.95.2 89.35.1 69.44.8 71.47.5 30.34.8 F.3 Running Times and Compute Information In this section we present the running times of the different models (see Table 23). We focus on the condition with up to 10,000 examples, excluding CatBoost-Tuned and XGBoost-Tuned as they run with 4 hours budget using 8 CPUs from AMD EPYC 7742 64-Core Processor . Compute Information TabSTAR runs on NVIDIA RTX A40 with 48GB memory. TabPFN-v2 runs directly on their API client. CatBoost, XGboost and Random Forest run using Intel(R) Core(TM) i9-10920X CPU @ 3.50GHz CPU cores. CARTE use inferior GPUs of type NVIDIA GeForce GTX 1080 Ti with 11GB memory, to allow for higher parallelization of their incredibly slow runs. Running Times TabSTARs average running time for downstream task ranges from 30 seconds (C04) to 7,692 seconds (R24). We observe that these running times are significantly lower than 14,400 seconds (4 hours), used for CatBoost-Tuned and XGBoost-Tuned, although single run of any tree model without 5-fold cross-validation is faster. The running times of TabPFN-v2 are considerably better. However, since they use ICL, they are limited to run only up to 10,000 examples and their inference time is just as costly as their fitting time. For CARTE, while their runs use inferior hardware and limited parallelization, the reported times are for single run. In practice, they run 6 times per dataset split because of their lack of default hyperparameters, making it poor choice for scale. Table 22: Win rates of TabSTAR-U (above 10K) against baselines. Win rate with 95% CI. CARTE CatB-T CatB-T-U TabPFN TabSTAR XGB-T XGB-T-U Classification Regression 98.62.8 63.57.5 86.06.8 53.06.9 67.09.3 22.05.8 75.710.1 61.17.0 94.04.7 82.05.3 85.07.0 59.56. 71.08.9 28.56.3 36 Table 23: Average model training running time in seconds, per dataset, for up to 10K examples. ID C01 C02 C03 C04 C05 C06 C07 C08 C09 C10 C11 C12 C13 C14 R01 R02 R03 R04 R05 R06 R07 R08 R09 R10 R11 R12 R13 R14 R15 R16 R17 R18 R19 R20 R21 R22 R23 R24 R25 R26 R27 R28 R29 R30 R31 R32 R33 R34 R35 R36 CARTE CatBoost RF TabPFN-v2 TabSTAR XGBoost 302 1,145 324 30 68 1,126 685 681 544 519 252 765 929 5,254 1,987 188 553 115 158 307 352 1,136 198 168 99 199 639 772 223 798 270 341 101 637 177 149 167 7,692 786 71 304 389 138 45 451 32 539 61 396 949 31 491 27 9 9 63 52 49 31 23 18 39 37 308 94 6 24 7 15 13 28 58 5 21 8 9 21 15 22 35 143 32 6 20 14 15 10 106 54 8 22 46 5 7 39 7 30 12 28 23 13, 7,297 1,227 3,068 6,559 4,631 2,428 4,411 1,774 9,439 2,174 10,639 3,185 4,637 4,252 8,239 1, 2,299 6,396 13,380 3,247 9,663 6,173 2,215 14,401 25,903 2,404 636 2,615 12,265 2,477 5,102 4, 4,816 15,389 44 420 38 11 10 68 61 40 20 22 19 169 31 500 139 9 30 10 28 22 22 57 7 28 11 13 24 28 25 34 94 34 11 22 14 16 15 90 50 10 31 45 11 9 41 8 28 26 30 18 19 346 19 6 8 31 34 41 21 22 12 16 27 140 446 16 103 10 26 43 109 366 22 37 18 22 54 30 51 118 478 631 14 144 33 21 51 170 943 11 59 163 10 18 187 11 100 24 55 81 42 61 9 14 112 83 49 134 30 33 54 357 84 62 70 112 396 55 33 36 78 38 167 66 156 204 309 66 154 25 17 58 314 261 13 40 128 32 24 240 13 136 18 48"
        },
        {
            "title": "G Extended Analysis",
            "content": "In this section we expand on the analysis results discussed in 6. G.1 Evaluation Datasets for Analysis All the experiments are evaluated over 20 datasets from the benchmark in Appendix D. Each experiment reports the performance with AUROC for classification and R2 for regression. For conciseness, each tables reports both regression and classification tasks, distinguishable by their ID. G.2 The Role of the Encoder Unfreezing (Q1) Table 24 shows the results for each variant of the experiment presented in 6. It is evident that unfreezing the textual encoder yields significant performance across datasets. Furthermore, while finetuning single layer gives significant boost, it underperforms compared to 6 unfrozen layers. G.3 The Effect of Pretraining (Q2) We pretrain three TabSTAR variants on nested dataset subsets of size 16, 64, and 256. The 64-dataset variant contains the original 16 plus 48 new datasets, and the 256-dataset variant builds on those 64 by adding another 192. This cumulative design minimizes variance between variants so that performance differences reflect only the effect of increasing data volume. While LoRA [41] is very efficient technique, its performance was much worse for randomly initialized model. Therefore, we perform full finetuning of the non-pretrained model, as explained in Appendix B.2. Table 25 shows the dataset level results. It is evident that for most of the datasets, improvement is observed when scaling, with the 256 datasets variant winning for almost all datasets. G.4 Numerical Verbalization (Q3) We show the full results for the experiment in 6, with Table 26 illustrating the verbalizations in each variant. Note that we do not include an exact value verbalization, since it would increase the number of unique text inputs and place extra memory demands. The two variants which integrate numerical information into the verbalization dominate the experiment, although the improvement seems to be marginal for some datasets. Interestingly, some datasets significantly underperform, with the R27 dataset completely failing the task. The addition of the quantile information on top of the bin seems to have limited impact, although marginally winning on the average performance. 38 Table 24: Downstream performance for Q1: The Role of the Encoder Unfreezing. Results for 20 datasets with 95% CI, for varying number of unfrozen layers. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and R2 for regression. ID 0 6 1 9 C01 C02 C03 C05 C07 C11 C12 C13 R02 R03 R05 R09 R12 R13 R18 R23 R27 R30 R33 87.80.3 94.91.2 77.60.9 87.00.7 82.11.4 77.40.5 94.40.2 66.50.9 97.01.7 67.20.8 80.82.6 88.10.7 97.00.6 37.63.6 96.31.1 79.32.4 81.31.6 14.24.6 36.02.0 84.32.8 90.40.4 97.80.3 82.40.2 90.20.9 89.10.9 78.40.6 98.30.1 71.71.1 97.91.2 69.80.8 91.81.5 88.70.7 98.20.3 45.72.1 97.10.6 81.61.6 81.31.6 15.65.6 43.61.5 87.42.9 90.80.4 97.80.3 82.80.3 90.80.6 90.60.5 78.70.7 98.30.1 72.90.7 98.01.1 70.90.9 93.20.5 89.00.5 98.20.2 51.63.2 97.40.6 82.81.6 81.51.6 19.23.1 44.31.4 88.43.4 91.00.4 98.10.3 83.00.5 91.50.9 91.30.4 78.80.5 98.30.1 74.10.7 97.91.2 71.20.8 93.10.7 89.00.6 98.30.3 51.42.9 97.30.6 83.22.2 81.31.7 19.14.0 46.01.8 87.93.5 90.80.4 97.70.3 83.00.4 89.40.8 91.00.4 78.30.6 98.20.1 73.00.8 98.01.1 70.80.6 92.90.8 88.90.8 98.20.3 52.11.8 97.00.6 82.11.9 81.41.6 17.34.5 43.52.2 88.72.7 Table 25: Downstream performance for Q2: The Effect of Pretraining. Results for 20 datasets with 95% CI, for varying number of pretraining datasets. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and R2 for regression. 256 ID 64 0 90.70.3 97.80.3 83.20.3 90.60.5 90.90.3 78.00.4 98.20.1 73.71.0 97.91.2 71.20.7 92.21.0 88.80.5 98.10.2 48.03.0 96.70.6 81.82.3 81.51.6 18.53.9 45.01.5 88.03.4 91.00.4 98.10.3 83.00.5 91.50.9 91.30.4 78.80.5 98.30.1 74.10.7 97.91.2 71.20.8 93.10.7 89.00.6 98.30.3 51.42.9 97.30.6 83.22.2 81.31.7 19.14.0 46.01.8 87.93. C01 C02 C03 C05 C07 C11 C12 C13 R02 R03 R05 R09 R12 R13 R18 R23 R27 R30 R33 R34 90.80.4 98.00.4 69.28.9 90.70.8 87.90.5 78.20.6 98.30.1 74.00.5 97.21.6 67.31.9 88.02.8 88.11.0 97.70.3 49.42.7 95.02.2 82.21.6 80.91.7 13.14.1 45.42.2 83.84.3 90.70.4 97.40.8 83.10.4 90.31.1 87.60.8 77.40.9 98.20.1 73.50.8 97.61.3 68.51.0 90.62.2 88.40.7 97.90.3 49.13.6 94.91.2 81.22.3 81.31.6 18.54.9 42.83.3 86.03.3 39 Table 26: Illustrative verbalization of numerical feature (Age) for the Q3: Numerical Verbalization experiment. Value Name Name + Bin TabSTAR 17 20 25 29 33 38 42 48 55 63 83 93 Age: Numeric Age: Lower than 18 Age: Numeric Age: 1823 Age: Numeric Age: 2327 Age: Numeric Age: 2731 Age: Numeric Age: 3135 Age: Numeric Age: 3540 Age: Numeric Age: 4045 Age: Numeric Age: 4551 Age: Numeric Age: 5158 Age: Numeric Age: 5867 Age: Numeric Age: 67-87 Age: Higher than 87 Age: Numeric Age: Unknown Value Age: Unknown Value Age: Unknown Value Age: Lower than 18 (Quantile 0%) Age: 1823 (Quantile 010%) Age: 2327 (Quantile 1020%) Age: 2731 (Quantile 2030%) Age: 3135 (Quantile 3040%) Age: 3540 (Quantile 4050%) Age: 4045 (Quantile 5060%) Age: 4551 (Quantile 6070%) Age: 5158 (Quantile 7080%) Age: 5867 (Quantile 8090%) Age: 6787 (Quantile 90100%) Age: Higher than 87 (Quantile 100%) Table 27: Downstream performance for Q3: Numerical Verbalization. Results for 20 datasets with 95% CI, for different verbalizations. The top performance score is bolded first, and then all scores are rounded. We report AUROC for classification and R2 for regression. Name + Bin TabSTAR Name ID C01 C02 C03 C05 C07 C11 C12 C13 R02 R03 R05 R09 R12 R13 R18 R23 R27 R30 R33 R34 91.00.4 98.10.2 83.30.4 90.80.9 91.20.2 78.20.5 98.20.1 71.60.7 98.01.1 67.40.7 93.11.1 88.90.5 98.20.2 50.13.3 97.10.6 81.92.4 16.76.6 16.44.4 45.61.1 88.03.6 91.20.4 97.90.3 83.30.3 91.11.2 91.40.4 78.20.6 98.40.1 73.80.9 98.01.1 71.30.6 93.20.8 88.90.6 98.30.2 49.73.4 97.10.7 82.71.9 82.01.5 17.75.3 46.22.2 88.62.5 91.00.4 98.10.3 83.00.5 91.50.9 91.30.4 78.80.5 98.30.1 74.10.7 97.91.2 71.20.8 93.10.7 89.00.6 98.30.3 51.42.9 97.30.6 83.22.2 81.31.7 19.14.0 46.01.8 87.93."
        }
    ],
    "affiliations": [
        "Technion - IIT"
    ]
}