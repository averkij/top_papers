{
    "paper_title": "Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective",
    "authors": [
        "Matthieu Zimmer",
        "Xiaotong Ji",
        "Tu Nguyen",
        "Haitham Bou Ammar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a novel approach to large language model (LLM) distillation by formulating it as a constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose a principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below a specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing a modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides a theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 2 9 2 2 . 9 0 5 2 : r RETHINKING LARGE LANGUAGE MODEL DISTILLATION: CONSTRAINED MARKOV DECISION PROCESS PERSPECTIVE Matthieu Zimmer Huawei Noahs Ark Lab matthieu.zimmer@huawei.com Xiaotong Ji Huawei Noahs Ark Lab xiaotong.ji1@h-partners.com Tu Nguyen Huawei R&D Munich tu.nguyen@huawei.com Haitham Bou Ammar Huawei Noahs Ark Lab UCL Centre for Artificial Intelligence haitham.ammar@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce novel approach to large language model (LLM) distillation by formulating it as constrained reinforcement learning problem. While recent work has begun exploring the integration of task-specific rewards into distillation processes, existing methods typically rely on ad-hoc reward weighting. We propose principled optimization framework that maximizes task-specific rewards while constraining the divergence from the teacher model to remain below specified threshold. Our approach adapts constrained state augmented reinforcement learning to the distillation setting, introducing modified reward function that maintains theoretical guarantees of constraint satisfaction without requiring state augmentation or teacher model access during deployment and without the computational overhead of the dual Lagrangian methods. Through extensive experiments on mathematical reasoning tasks, we demonstrate that our method achieves better constraint satisfaction rates and better reasoning compared to the soft Lagrangian relaxation baselines while maintaining competitive task performance. Our framework provides theoretically grounded and practically efficient solution for reward-aware distillation in resource-constrained settings."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have achieved remarkable success in wide range of natural language processing tasks (Vaswani et al., 2017; Trinh et al., 2024; Chervonyi et al., 2025; Guo et al., 2025; Christianos et al., 2023), but their size and complexity make them impractical for deployment in resource-constrained environments. Distillation (Hinton et al., 2015; Czarnecki et al., 2019), technique where smaller student model learns from larger teacher model, has been widely used to transfer knowledge while reducing computational costs. Conventional distillation methods (Sanh et al., 2020; Gu et al., 2024; Ko et al., 2024) typically focus on minimizing the divergence between the student and teacher models, often using metrics such as Kullback-Leibler (KL) divergence. However, these methods do not fully leverage additional reward signals that can provide valuable guidance, particularly in tasks requiring complex reasoning. Focusing solely on the KL divergence can lead to suboptimal learning, as it may force students to mimic complex reasoning paths that exceed their capacity rather than discovering simpler, equally effective reasoning paths. In contrast, method that purely optimizes for reward cannot guarantee that the reasoning leading to the solution is correct. When reward signals are considered together with KL, Agarwal et al. (2024) propose to focus on penalty method where hyperparameter λ is introduced to balance the preference between reward and KL. Equal contribution. 1 In this paper, we propose novel approach to LLM distillation by formulating it as constrained reinforcement learning (RL) problem. Specifically, we aim to maximize the task reward while ensuring that the divergence between the student and teacher models stays below predefined threshold. Although choosing the threshold likewise balances the rewardteacher divergence trade-off as does tuning the hyperparameter λ, it is far simpler, since it is specified directly in terms of KL scale rather than requiring delicate balance between values that may vary greatly in scale across different stages of training when adjusting λ. Finally, when the student is deemed to be close enough to the teacher, i.e. when the constraint is satisfied, the objective conveniently reduces to reward maximization, as the KL term can be safely omitted. 45 inches 1 foot 12 inches = 3.75 feet 12 Question: Leilani saw 300-foot whale with 4 45-inch remoras attached to it. What percentage of the whales body length is the combined length of the remoras? Ours: To determine what percentage of the whales body length the combined length of the remoras is, we need to follow these steps: 1. Convert the length of the remoras from inches to feet: So, each remora is 3.75 feet long. 2. Calculate the total length of the remoras: 4 remoras3.75 feet per remora = 15 3. Determine the total length of the whale: 300 feet 4. Calculate the percentage of the whales body length that the combined length of the remoras represents: (cid:18) 15 feet (cid:19) 300 feet 100% GRPO λ = 0: Lets break down the problem step by step to find the percentage of the whales body length that the combined length of the remoras represents. First, we need to convert all units to the same unit. Lets use feet for this calculation. The length of each remora is given as 45 inches. Since there are 12 inches in foot, the length of each remora in feet is: 45 = 3.75 feet Since there are 4 remoras, the total length of the remoras is: 4 3.75 = 15 feet Next, we add the length of the whale to the combined length of the remoras to get the total length of the whale: 300 + 15 = 315 feet Now, we need to find what percentage the combined length of the remoras is of the whales body length. This can be calculated using the formula for percentage: (cid:19) (cid:18) 100 5. Simplify the fraction: 315 15 300 = 0. 6. Convert the decimal to percentage: 0.05 100% = 5% Therefore, the combined length of the remoras is 5 percent of the whales body length. Simplifying the fraction inside the percentage formula: 315 = 1 21 100 21 100 = 4.7619 (cid:18) 1 (cid:19) 21 Rounding to the nearest whole number, we get: Figure 1: Example illustrating that checking the final answer alone is insufficient for evaluating reasoning. GRPO (right) makes mistakes and reaches wrong answer (4.76) but takes an extra rounding step to the correct one (5), likely as learned strategy through training. Figure 2: Comparison of our method against baselines across with Qwen2.5-1.5B-Math (top) and Llama-3.2-3B (bottom) averaged across three evaluation datasets. To ensure bigger surface means better results, the reasoning loss rate and the KL divergence were inverted. Solving our new constrained RL problem follows standard methods for constraint optimization in which we write dual Lagrangian optimization problem (Achiam et al., 2017; Boyd & Vandenberghe, 2004; Altman, 1999), but it would be impractical to solve with LLM because of the huge computational cost of solving max-min problem with large teacher models. Instead, we adopt state augmentation method known as Saute (Sootla et al., 2022b;a; Ji et al., 2025). It relaxes the constrained optimization problem by formulating new state-augmented Markov Decision Process (MDP) with reformulated reward function. This approach not only changes the reward but also introduces new state space that helps in maintaining the theoretical guarantees of the original constraints without the need for explicit Lagrangian multipliers. However, Saute assumes that it can compute the constraint in every state. For distillation, it would result in the need to have access to the teacher model at test time, which fundamentally defeats the purpose of distillation. To address this issue, we modify the Saute method by removing the state augmentation step using the assumption that the policy is history-conditioned, which is the case for LLM. This modification allows us 2 to maintain the theoretical guarantees of Saute while ensuring that the student model can operate independently of the teacher at test time. By reformulating the reward function alone, we achieve more efficient and practical solution for distillation. Through extensive experiments, we demonstrate that our proposed method effectively minimizes the KL divergence while achieving superior performance in terms of reasoning quality and comparable final answer correctness (see Figure 2). We show that reward maximization alone, as proposed in Guo et al. (2025), cannot guarantee correct reasoning steps by itself and that the teacher signal is useful for LLM to better reason (see Figure 1). Our contributions are summarized as follows: We formulate LLM distillation as constrained RL problem, integrating task-specific reward signals to guide the distillation process. We adapt the Saute method by removing the state augmentation step, ensuring the student model operates independently of the teacher at test time while maintaining the theoretical guarantees and enhancing exploration on constraint-violating trajectories. We conduct extensive experiments on mathematical reasoning tasks to demonstrate that our method identifies notable point on the Pareto front balancing divergence minimization, reward maximization, and reasoning quality. This work bridges the gap between distillation and constrained RL, offering promising direction for improving the efficiency and effectiveness of knowledge transfer in LLMs."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 DISTILLATION Knowledge distillation has emerged as critical technique for transferring knowledge from large, complex teacher models to smaller, more efficient student models (Hinton et al., 2015). Standard distillation methods primarily focus on minimizing the divergence, often Kullback-Leibler (KL) divergence, between the student and teacher models (Ba & Caruana, 2014; Gou et al., 2021), treating the distillation as supervised imitation at the token or representation level. While effective for general language understanding tasks, these methods struggle on complex reasoning tasks: minimizing solely the divergence while ignoring task-specific reward signals can fail to capture the solution paths with better performance. For instance, in mathematical reasoning tasks, the teacher model may rely on complex reasoning paths that are difficult for smaller student model to replicate due to its limited capacity, while alternative, simpler reasoning strategies that achieve the same correct outcome might be more suitable for the student to learn and memorize (Zhang et al., 2025). Recent advances incorporate reward signals into distillation (Agarwal et al., 2024), recasting it as policy-optimization problem in which the student policy π is trained to maximize expected task reward while being regularized by divergence D(π, µ) to teacher policy µ: max π Eπ [R λD(π, µ)] , (1) where λ controls the trade-off between the task performance and teacher fidelity. However, the optimal λ is difficult to anticipate and requires extensive retraining on specific tasks, making this approach unstable and computationally expensive for large sequential models. This challenge motivates viewing distillation instead as constrained learning problem that can directly maximize the task reward subject to divergence budget. This perspective eliminates ad hoc hyperparameter tuning while providing interpretable fidelity guarantees and principled foundation for reasoningoriented distillation. 2.2 CONSTRAINED REINFORCEMENT LEARNING Constrained reinforcement learning (CRL) addresses the problem of optimizing primary objective while satisfying constraint requirements (e.g., safety) (Achiam et al., 2017). In LLM distillation, we can constrain the divergence between the teacher and student policy, following the constrained MDP formulation Md = S, A, P, R, C, γ, d, where st is the current prompt with partial response, the action at is the next token generated by the student model, is the transition kernel, is the 3 task-specific reward (e.g., correctness in mathematical reasoning), Cπ(st) := Df (π(st)µ(st)) is the per-state -divergence between student π and teacher µ, γ (0, 1) is the discount factor and is predefined budget. The goal is to find policy π that maximizes the task reward while keeping the expected divergence lower than the threshold d: max π Eπ (cid:34) (cid:88) (cid:35) γtR(st, at) s.t. Eπ t=0 (cid:35) Cπ(st) d. (cid:34) (cid:88) t=0 (2) This kind of constrained problem can be solved with direct optimization: Sootla et al. (2022b) introduced state augmentation method that reformulates the constrained MDP as an augmented = S, A, P, Rn, γ, by adding auxiliary state variable zt that tracks the remaining MDP (cid:102)Mn budget at every time step t, zt+1 = zt Cπ(st), z0 = d, transforming the problem into: max π Eπ (cid:34) (cid:88) (cid:35) γt Rn(st, zt, at) , t= Rn(sT , zT , aT ) = (cid:26)R(sT , aT ) if zT 0, if zT < 0, (3) Here = is the augmented state space, : S [0, 1] is the transition kernel, and Rn is constrained reward function with large positive Rmax for penalization when the budget is exhausted. As , any optimal policy of the augmented MDP is feasible for the constraint and attains the constrained optimum under standard assumptions. This method avoids the computational overhead of Lagrange multipliers formulation (cf. equation 1), which can be written as maxπ minλ0 Eπ [(cid:80) t=0 Cπ(st) d)] for the same formulation, and typically requires tuning dual variable and running dual ascent. t=0 γtR(st, at) λ ((cid:80) However, directly applying this formulation to distillation would require maintaining the augmented state zT online during distillation, which would necessitate access to the teacher model at test time to compute Cπ at every timestep. This is impractical for distillation, where the goal is to create student model that operates independently of the teacher. In the following section, we address this challenge by proposing new formulation for LLM distillation to eliminate the need for state augmentation while preserving the theoretical guarantees."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 CONSTRAINED RL FOR LLM DISTILLATION We introduce constrained MDP formulation for distillation that removes state augmentation while retaining the hard-constraint semantics, therefore enabling constrained RL without accessing the teacher policy at every single step. In LLM distillation, we model the state as the full interaction history, so the induced control process is fully observable. Therefore, removing the augmented state zT in equation 3 from the state does not induce partial observability. At any time , we can recompute the remaining budget from the full observed history encoded in sT , hence the augmented state zT is deterministic function of the state with zT = (cid:80)T 1 t=0 Cπ(st). We propose constrained MDP formulation for LLM distillation without state augmentation (cid:99)Mn = S, A, P, ˆRn, γ, d, where ˆRn is the constrained reward that combines the task-specific reward with feasibility signal for constraint satisfaction. The goal is to find policy π that maximizes the task-specific reward while keeping the divergence lower than the threshold d: max π Eπ (cid:34) (cid:88) (cid:35) γt ˆRπ,n(st, at) , t=0 ˆRπ,n(sT , aT ) = (cid:40) R(sT , aT ) (n+ϕπ(sT)) if (cid:80)T 1 otherwise. t=0 Cπ(st) 0, (4) The constrained reward without the augmented state zT preserves the feasibility signal for the constraint satisfaction, such that the student model receives the positive task-specific reward only when the constraint with budget is satisfied, while any trajectory that violates the constraint incurs large hard penalty. This penalty is fixed value in the previous setting equation 3 for all infeasible trajectories, we refine this penalty by adding policy-dependent discrepancy term ϕπ(sT ) to differentiate the trajectories within the infeasible region: trajectories that deviate more from the teacher policy receive stronger penalty, whereas marginally deviating ones are penalized less. We define ϕπ(sT ) as any -divergence, including KL and JensenShannon divergence, between the student and teacher at sT , which is nonnegative and equals zero iff π( sT ) = µ( sT ). Therefore, the penalty (n + ϕπ(sT )) is strictly negative, while in feasible region we maintain the original task specific reward to guide exploration. This formulation preserves the augmentedMDP penalty semantics and increases sample efficiency by delivering informative negative feedback among violating trajectories, without altering feasibility decisions or the limiting optimum."
        },
        {
            "title": "3.2 POLICY GRADIENT OPTIMIZATION",
            "content": "We detail policy gradient optimization for the unaugmented objective in equation 4, and derive the policy gradient decomposition with an explicit-dependence term. Our method directly maximizes the expected discounted return with standard policy gradient, thereby avoiding the instabilities from infeasible gradient vector fields in on-policy distillation observed by Czarnecki et al. (2019). We parameterize the student policy as πθ and maximize the expected discounted return: Jn(θ) = Eπθ (cid:104) (cid:88) (cid:105) γt ˆRπθ,n(st, at) t=0 Because Jn(θ) depends on θ both through the trajectory distribution induced by πθ and inside the reward via the discrepancy ϕπθ , its gradient decomposes into (I) the likelihood-ratio term and (II) the explicit dependence term of ˆRπθ,n on θ: (cid:34) θJn(θ) = Eπθ (cid:88) t0 (cid:124) θ log πθ(at st) (cid:16) (cid:88) (cid:17) γut ˆRπθ,n(su, au) ut (cid:123)(cid:122) (I) likelihood-ratio term (cid:35) (cid:125) (cid:104) (cid:88) + Eπθ (cid:105) γt θ ˆRπθ,n(st, at) t0 (cid:124) (cid:123)(cid:122) (II) explicit-dependence term (cid:125) (5) We compute θJn(θ) following the policy gradient theorem Sutton et al. (1999) under the following minimal assumptions: A1. For each state sT , ϕπθ (sT ) is finite and differentiable in θ, and its gradient is measurable and t0 γtθϕπθ (st)(cid:3) < ; integrable along trajectories Eπθ (cid:2) (cid:80) A2. There exists an optimal policy π (cid:16) θ with finite value such that d (cid:80)T 1 t=0 Cπ θ (st) > 0 (cid:17) = 1. In practice, we take ϕ = KL with small probability floor, ensuring finiteness and differentiability. A2 ensures the existence of an optimal feasible policy, i.e., the budget is satisfied almost surely at the optimum. Under A1 and A2, we can characterize the explicit-dependence term (II) in unified way (see Appendix for the full derivation across feasible, infeasible, and boundary cases) by including the gradient and limiting sub-gradient of ˆRπθ,n with small tolerance ε 0 round the feasibility boundary. Our final gradient for optimization is (cid:34) θJn(θ) = Eπθ (cid:88) t0 θ log πθ(at st) (cid:16) (cid:88) (cid:17) γut ˆRπθ,n(su, au) (cid:35) ut (cid:34) Eπθ (cid:88) (cid:40) γt 1 t1 (cid:88) u=0 (cid:41) (cid:35) Cπθ (su) ε θϕπθ (st) 3.3 THEORETICAL GUARANTEE FOR CONSTRAINT SATISFACTION In this section, we show that our reformulation of the constrained MDP preserves the constraint satisfaction guarantee while enabling deployment without teacher access. In particular: (i) the optimal policy and value functions are equivalent between our un-augmented objective in equation 4 and the augmented objective in equation 3; (ii) Bellman optimality holds under standard assumptions; and (iii) as , every optimal policy with finite value satisfies the constraint almost surely. In LLM distillation, the student policy π is frozen within each episode, so the induced control process is time-homogeneous. We adopt this per-episode stationary view; all statements are uniform over fixed π on the reachable set. We further formalize an equivalent contextual MDP view, in which each episode carries fixed context (e.g., policy checkpoint), and prove its optimalityequivalence to the standard MDP in Appendix C. Theorem 3.1 (Optimal equivalence). For every feasible state sT , the optimal value functions of the unaugmented MDP (cid:99)Mn in equation 3 are equivalent: in equation 4 and the augmented MDP (cid:102)Mn ˆV (sT ) = (sT , zT ). This theorem justifies that removing the budget variable zt does not change the control problem we are solving. This equivalence holds because the augmented state zT is reconstructable from the observed history under any fixed student π and teacher µ via zT = d(cid:80)T 1 t=0 Cπ(st), so augmented states (sT , zT ) and un-augmented states sT induce identical trajectories and stepwise rewards along any feasible paths. We give the precise construction and full proof details in Appendix B. We adopt the following standard assumptions Hernandez-Lerma & Munoz de Ozak (1992); Sootla et al. (2022b) for the discrete token setting in distillation: B1. The reward function ˆRn(sT , aT ) is bounded, measurable, and upper semicontinuous on A; B2. The transition kernel is weakly continuous on A; B3. The action space is compact. Theorem 3.2 (Bellman optimality and value convergence). Consider the unaugmented MDP (cid:99)Md, satisfying assumption B1-B3 with the associated equation 4, then: a) the Bellman equation is satisfied in (cid:99)Md; b) the optimal value function ˆV Theorem 3.3 (Almost surely constraint satisfaction). If there exists an optimal policy π solving (cid:99)M with finite value, then π is also an optimal policy for the original constrained MDP Md and satisfies the constraint almost surely. converges monotonically to ˆV for (cid:99)M . for (cid:99)Mn These results show that our modified approach maintains the guarantees of the original constrained problem while eliminating state augmentation (see Appendix for proofs and discussion). At test time, the student operates without teacher access: the cumulative reward is computed from the students own output distribution and environment feedback. This makes our approach practical for LLM distillation while retaining guarantees of feasibility."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP We conduct experiments on two distinct distillation settings to evaluate our proposed method. For the first setting, we distill Qwen2.5-1.5B-Math student model from Qwen2.5-7B-Math-Instruct teacher model using the GSM8K training dataset. For the second setting, we distilled Llama-3.23B student model from Llama-3.2-11B-Instruct teacher model using the MATH training dataset. In both setting, we evaluated the resulting checkpoints after 20 epochs on the Apple/GSM-Symbolic (main) (Mirzadeh et al., 2025), the test set of GSM8K (Cobbe et al., 2021) and the whole test set of MATH (Hendrycks et al., 2021) (from which MATH500 is selected). Baselines. Our proposed constrained optimization method is built upon the GRPO policy gradient algorithm (Shao et al., 2024). To assess its effectiveness, we benchmark against several strong distillation baselines, each re-implemented under the same GRPO framework to ensure fairness and consistency. More precisely, for every method, the batch size and its composition is the same (64 answers, 8 questions, 8 answers per question). The learning rate (1e5) and the optimizer (AdamW) are also the same. We consider the following baselines: GRPO: The base algorithm in our experiments. GRPO optimizes purely for the task-specific reward using robust, value-function-free policy gradient with group-average reward baseline (Shao et al., 2024). 6 GKD: distillation-only baseline whose objective is to minimize the reverse KL divergence DKL(πθ µ), treating the negative per-step KL as an intrinsic reward. We use GRPO rather than the REINFORCE-style update of Agarwal et al. (2024) for consistency. GKD-GRPO: baseline that jointly optimizes for both the task-specific reward and the GKD objective. This corresponds to the standard Lagrangian relaxation of our constrained problem in Eq. (1), with λ as the balancing hyperparameter (Agarwal et al., 2024). Mini-LLM: On-policy reverse KL divergence minimization (Gu et al., 2024), accounting for the long-term effects of actions on KL (Tang & Munos, 2025). As in GKD, task reward is ignored. For consistency, we sample trajectories exclusively with the student policy and substitute PPO with GRPO-based update. Together, these baselines span the main approaches to RL-based distillation: optimizing task rewards, relying solely on KL supervision, and hybrid formulations that combine both. To approximate the Pareto frontier of the Lagrangian relaxation baseline (GKD-GRPO), we perform grid search over the multiplier λ across several orders of magnitude, reporting results for λ {0.001, 0.01, 0.1, 1.0, 10}. Note that when λ = 0, it equals to the pure GRPO baseline. The constraint threshold = 0.35 was selected based on preliminary experiments that seek to minimize only the KL (mini-LLM and GKD). Metrics. We evaluate models using four key metrics: Final Answer Correctness (FAC): It verifies that the final answer inside boxed{} is correct. It is used to define the reward function in our MDPs. Reasoning Quality: To assess the logical validity of the reasoning path beyond the final answer, we use an LLM-as-a-Judge setting (Zheng et al., 2023). Specifically, we use DeepSeek-R1Distill-Qwen-32B (DeepSeek-AI, 2025) to perform pairwise comparisons between generated solutions. The judge is provided with the correct final answer to isolate its evaluation to the reasoning process itself. This yields the Reasoning Win Rate (RWR) and Reasoning Loss Rate (RLR), reported as percentages (Zhou et al., 2025). Constraint Satisfaction: The percentage of test samples where the KL divergence between the student and teacher policies is below predefined threshold d. KL Divergence: The average student-teacher policy divergence cross the entire test set. 4.2 EXPERIMENT RESULTS We organize our set of experiments to answer the following questions: A. What is the best method in general? B. Is our method able to achieve higher constraints satisfaction? C. Can external reward help achieve better distillation? D. Does the distillation signal help to better reason? A. What is the best method in general? Figure 2 presents comprehensive comparison of our constrained RL approach against baseline methods across five key metrics. The results demonstrate that our method achieves the most balanced performance profile, excelling particularly in reasoning quality and constraint satisfaction while maintaining competitive final answer correctness. The radar plot reveals that pure reward optimization (GRPO λ=0.0) achieves the highest final answer correctness but at the cost of poor reasoning quality and severe constraint violations. Conversely, methods that focus solely on KL minimization (GKD, Mini-LLM) maintain good constraint satisfaction but suffer from lower final answer correctness. Our constrained RL formulation successfully navigates this trade-off, achieving strong performance across all dimensions. B. Is our method able to achieve higher constraints satisfaction? Figure 3 illustrates the Pareto frontier between final answer correctness and constraint satisfaction across different methods and hyperparameter settings. Our approach consistently achieves superior constraint satisfaction rates while maintaining competitive final answer correctness, occupying unique region of the Pareto front. This demonstrates the effectiveness of our constrained formulation in achieving the desired balance between task performance and teacher fidelity. Note that without introducing ϕ, our method would have great difficulty satisfying strict constraint due to the lack of signal: all trajectories would receive the same penalty and the training would diverge. 7 Figure 3: Pareto frontier analysis showing the trade-off between final answer correctness and constraint satisfaction across different methods and hyperparameter settings for Qwen2.5-1.5B-Math. Each point represents different method configuration. The points in red belong to the Pareto front. C. Can external reward help achieve better distillation? Comparing reward-based methods (GRPO, GKD-GRPO variants, and ours) against purely KL-based methods (GKD, Mini-LLM) reveals the crucial role of external rewards in distillation. Pure KL minimization methods always achieve lower final answer correctness rates in every dataset for each model (Figure 2 and Appendix E). Beyond the improvement over final answer correctness, we also observe that our method achieves higher reasoning win rates which can also be attributed to the use of the reward function. This substantial improvement demonstrates that incorporating task-specific rewards enables the student model to learn more effective reasoning strategies rather than merely mimicking the teachers surface-level outputs. D. Does the distillation signal help to better reason? Figure 4 presents comprehensive pairwise comparison matrix averaged across all three evaluation datasets with Qwen. The comparison between pure reward optimization (GRPO λ=0.0) and our constrained approach provides strong evidence for the value of teacher guidance in reasoning tasks. While GRPO achieves the highest raw final answer correctness (75-80%), it exhibits poor reasoning quality with win rates of only 12-19% and correspondingly high loss rates of 39-55%. Our constrained formulation dramatically improves reasoning quality while maintaining competitive success rates. It demonstrates that constraining the student to stay close to the teacher distribution helps preserve and transfer the teachers reasoning capabilities. The equivalent figure for Llama3.2-3B is provided in the Appendix F. Qualitative Analysis: In Figure 1, we present test set example in which both our method and the GRPO baseline yield the correct final answer. However, only our method produces logically valid reasoning steps, while GRPOs reasoning is flawed. More examples are provided in the Appendix F. Figure 4: Pairwise comparison heatmap showing the relative performance of our method against baselines, averaged across all three evaluation datasets (Apple/GSM-Symbolic, GSM8K, and MATH) with Qwen2.5-3B-Math. Darker colors indicate superior performance in row-to-column comparisons. Averaging over columns gives the reasoning win rate (RWR) and over rows the reasoning loss rate (RLR). These results collectively demonstrate that our constrained RL approach successfully addresses the core challenges of reward-aware distillation: it maintains high constraint satisfaction rates, leverages external rewards for improved task performance, and preserves the teachers reasoning capabilities in the student model. The method achieves superior balance across all evaluation dimensions 8 compared to existing approaches that typically excel in only one aspect of the distillation objective. We provide more detailed results in the Appendix E."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Task-specific Distillation. The prevailing paradigm in LLM distillation is to pass knowledge from powerful teacher to compact student by aligning their output distributions, typically through the reverse KL divergence (Hinton et al., 2015; Sanh et al., 2020; Gu et al., 2024; Agarwal et al., 2024; Zimmer et al., 2025). However, this objective does not explicitly guarantee the preservation of the teachers underlying reasoning abilities on complex tasks (Gudibande et al., 2024), motivating shift towards more sophisticated, task-aware techniques. The problem is now increasingly framed through the lens of RL, where adherence to the teacher is elegantly re-conceptualized as dense, token-level reward derived from the KL divergence. This forms the basis of general-purpose distillation methods (Agarwal et al., 2024; Ko et al., 2024; 2025), which uses REINFORCE-style update, and Mini-LLM (Gu et al., 2024), which decomposes the policy gradient to separate the high-variance, long-term reward from more stable, single-step objective. This RL framework can then be extended by composing the KL-based reward with an external task reward, Rtask (Agarwal et al., 2024). Task-aware Extensions. Beyond these RL formulations, significant body of work integrates richer, task-specific signals into the distillation process to provide denser supervision. One prominent strategy, process-aware distillation, supervises the student to replicate the teachers intermediate reasoning steps, thereby transferring the underlying causal logic rather than just the final output (Hsieh et al., 2023; Adarsh et al., 2024; Chen et al., 2025). Other approaches include logit-aware distillation, which intelligently modifies the KL divergence loss to emphasize pivotal, task-relevant tokens identified via attention or Bayesian principles (Li et al., 2025; 2024; Saadi & Wang, 2025), and knowledge-augmented methods that use retrieval to transfer teachers ability to synthesize external information (Kang et al., 2023; Tian et al., 2025). While these sophisticated strategies significantly improve signal density, they often introduce new complexities, such as the need for finegrained annotations, complex weighting heuristics, or the overhead of external knowledge bases. Constrained RL for LLM Distillation. The application of RL to task-specific LLM distillation remains relatively under-explored (Zhang et al., 2025). In standard alignment settings like RLHF (Ouyang et al., 2022), the KL penalty against reference model is primarily regularization tool to prevent catastrophic forgetting and maintain stylistic diversity (Yang et al., 2024; Stiennon et al., 2022). However, in the distillation setting, this KL term takes on the dual role of constraint, intended to preserve the teachers reasoning capabilities. Most methods still use fixed penalty, which is simple but can be brittle, as static weight may not prevent the student from exploiting task rewards via shallow or degenerate reasoning (Gudibande et al., 2024). To our knowledge, the principled distillation of task-specific, constrained RL policies from LLMs is still scarce, with most related work only examining it briefly (Agarwal et al., 2024). more robust alternative is to treat the KL divergence as an explicit trust-region constraint and solve the resulting constrained-RL problem; classic trust-region and constrained-RL methods provide standard toolkit for this (Schulman et al., 2015; Achiam et al., 2017). Dual Lagrangian solvers can then adapt the KL penalty to restore an interpretable fidelityperformance point, but at LLM scale, this is practically challenging: teacher forward passes, cached-logit strategies, and inner-loop/dual updates add significant compute, memory, and variance costs (Dasgupta et al., 2023; Achiam et al., 2017). In this work, we address these challenges by reformulating the dual Lagrange problem within state-augmented MDP framework (Calvo-Fullana et al., 2024; Sootla et al., 2022a;b), for which we provide principled and efficient optimization solution that remains practical at the LLM scale."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we moved beyond the conventional paradigm of regularized distillation and introduced principled framework based on constrained reinforcement learning. By adapting principles from the safe RL literature, we developed solution that maintains theoretical guarantees of constraint 9 satisfaction without requiring the impractical state augmentation typical of classic methods. This approach successfully navigates the trade-off between task-specific performance and teacher fidelity, eliminating the need for brittle, ad-hoc reward weighting and the prohibitive costs of traditional dual max-min optimization. Our experiments on mathematical reasoning demonstrate that it is possible to enforce strict KL divergence constraint with high fidelity while maintaining competitive task rewards. This method provides theoretically grounded and practically efficient pathway for creating smaller, reliable, and specialized models that operate reliably within defined trust region of their teachera crucial step towards more controllable and deployable LLMs."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Bingning Huang, Shyam Sundhar Ramesh and Xuexing Zhao for the fruitful discussions and insights."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, pp. 2231. JMLR.org, 2017. Shivam Adarsh, Kumar Shridhar, Caglar Gulcehre, Nicholas Monath, and Mrinmaya Sachan. Siked: Self-guided iterative knowledge distillation for mathematical reasoning, 2024. URL https: //arxiv.org/abs/2410.18574. Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from selfgenerated mistakes. In The twelfth international conference on learning representations, 2024. Eitan Altman. Constrained Markov Decision Processes. Stochastic Modeling Series. CRC Press, 1999. Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? Advances in neural information processing systems, 27, 2014. Fahiem Bacchus, Craig Boutilier, and Adam Grove. Rewarding behaviors. In Proceedings of the National Conference on Artificial Intelligence, pp. 11601167, 1996. Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, Cambridge, 2004. Miguel Calvo-Fullana, Santiago Paternain, Luiz F. O. Chamon, and Alejandro Ribeiro. State augmented constrained reinforcement learning: Overcoming the limitations of learning with rewards. IEEE Transactions on Automatic Control, 69(7):42754290, 2024. doi: 10.1109/TAC.2023. 3319070. Jack Chen, Fazhong Liu, Naruto Liu, Yuhan Luo, Erqu Qin, Harry Zheng, Tian Dong, Haojin Zhu, Yan Meng, and Xiao Wang. Step-wise adaptive integration of supervised fine-tuning and reinforcement learning for task-specific llms. arXiv preprint arXiv:2505.13026, 2025. Yuri Chervonyi, Trieu H. Trinh, Miroslav Olˇsak, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc V. Le, and Thang Luong. Gold-medalist performance in solving olympiad geometry with alphageometry2, 2025. URL https://arxiv.org/abs/ 2502.03544. Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. Pangu-agent: fine-tunable generalist agent with structured reasoning. arXiv preprint arXiv:2312.14878, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. Wojciech Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, and Max Jaderberg. Distilling policy distillation. In The 22nd international conference on artificial intelligence and statistics, pp. 13311340. PMLR, 2019. Sayantan Dasgupta, Trevor Cohn, and Timothy Baldwin. Cost-effective distillation of large lanIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of guage models. the Association for Computational Linguistics: ACL 2023, pp. 73467354, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.463. URL https://aclanthology.org/2023.findings-acl.463/. 11 DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International journal of computer vision, 129(6):17891819, 2021. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models, 2024. URL https://arxiv.org/abs/2306.08543. Arnav Gudibande, Eric Wallace, Charlie Victor Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=Kz3yckpCN5. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Onesimo Hernandez-Lerma and Myriam Munoz de Ozak. Discrete-time markov control processes with discounted unbounded costs: optimality criteria. Kybernetika, 28(3):191212, 1992. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https://arxiv.org/abs/1503.02531. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, 2023. URL https://arxiv.org/abs/2305.02301. Xiaotong Ji, Shyam Sundhar Ramesh, Matthieu Zimmer, Ilija Bogunovic, Jun Wang, and Haitham Bou Ammar. Almost surely safe alignment of large language models at inference-time. arXiv preprint arXiv:2502.01208, 2025. Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. Knowledgeaugmented reasoning distillation for small language models in knowledge-intensive tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. Distillm: Towards streamlined distillation for large language models, 2024. URL https://arxiv.org/abs/2402.03898. Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, and SeYoung Yun. Distillm-2: contrastive approach boosts the distillation of llms, 2025. URL https://arxiv.org/abs/2503.07067. Navdeep Kumar, Esther Derman, Matthieu Geist, Kfir Levy, and Shie Mannor. Policy gradient for rectangular robust markov decision processes. Advances in Neural Information Processing Systems, 36:5947759501, 2023. Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Feng Tao, Yicheng Li, Zulong Chen, and Yin Zhang. Mixed distillation helps smaller language models reason better. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (Findings), pp. 112, 2024. URL https://aclanthology.org/2024.findings-emnlp.91. Wei Li, Lujun Li, Mark Lee, Shengjie Sun, Lei Zhang, Wei Xue, and Yike Guo. Bayeskd: Bayesian knowledge distillation for compact llms in constrained fine-tuning scenarios. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 138152, 2025. Seyed Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=AjXkRZIvjB. 12 Boris Sholimovich Mordukhovich. Variational analysis and applications. Springer, 2018. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Khouloud Saadi and Di Wang. TASKD-LLM: Task-aware selective knowledge distillation for LLMs. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, 2025. URL https://openreview.net/forum?id=QQBfoVJWY2. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, 2020. URL https://arxiv.org/abs/1910. 01108. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 18891897, Lille, France, 0709 Jul 2015. PMLR. URL https://proceedings.mlr. press/v37/schulman15.html. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Aivar Sootla, Alexander Cowen-Rivers, Jun Wang, and Haitham Bou Ammar. Enhancing safe exploration using safety state augmentation. Advances in Neural Information Processing Systems, 35:3446434477, 2022a. Aivar Sootla, Alexander Cowen-Rivers, Taher Jafferjee, Ziyan Wang, David Mguni, Jun Wang, and Haitham Ammar. Saute rl: Almost surely safe reinforcement learning using state augmentation. In International Conference on Machine Learning, pp. 2042320443. PMLR, 2022b. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022. URL https://arxiv.org/abs/2009.01325. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Yunhao Tang and Remi Munos. On few pitfalls in kl divergence gradient estimation for rl. arXiv preprint arXiv:2506.09477, 2025. Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and Nitesh V. Chawla. Beyond answers: Transferring reasoning capabilities to smaller llms using multi-teacher knowledge distillation. In Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining, WSDM 25, pp. 251260, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400713293. doi: 10.1145/3701551.3703577. URL https://doi.org/10.1145/ 3701551.3703577. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In Advances in Neural InforŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. mation Processing Systems, volume 30, 2017. Yue Wang and Shaofeng Zou. Policy gradient method for robust reinforcement learning. In International conference on machine learning, pp. 2348423526. PMLR, 2022. 13 Joy Qiping Yang, Salman Salamatian, Ziteng Sun, Ananda Theertha Suresh, and Ahmad Beirami. Asymptotics of language model alignment, 2024. URL https://arxiv.org/abs/2404. 01730. Chen Zhang, Qiuchi Li, Dawei Song, Zheyu Ye, Yan Gao, and Yao Hu. Towards the law of capacity gap in distilling language models. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2250422528, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/ 2025.acl-long.1097. URL https://aclanthology.org/2025.acl-long.1097/. Kaiqing Zhang, Bin Hu, and Tamer Basar. On the stability and convergence of robust adversarial reinforcement learning: case study on linear quadratic systems. Advances in Neural Information Processing Systems, 33:2205622068, 2020. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Thirty-seventh Conference on Judging LLM-as-a-judge with MT-bench and chatbot arena. Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Zhaoyi Zhou, Yuda Song, and Andrea Zanette. Accelerating unbiased LLM evaluation via synthetic feedback. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=9ppSGIfpzq. Matthieu Zimmer, Milan Gritta, Gerasimos Lampouras, Haitham Bou Ammar, and Jun Wang. Mixture of attentions for speculative decoding. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "A DERIVATION OF POLICY GRADIENT",
            "content": "We compute the gradient of Jn(θ) w.r.t θ following the policy gradient theorem Sutton et al. (1999) under the following minimal assumptions: A1. For each state s, ϕπθ (s) is finite and differentiable in θ, and its gradient is measurable and t0 γtθϕπθ (st)(cid:3) < ; integrable along trajectories: Eπθ (cid:2) (cid:80) A2. There exists an optimal policy π (cid:16) θ with finite value such that (cid:80)T t=0 Cπ θ (st) > 0 (cid:17) = 1. Assumption A1 ensures that the discrepancy function ϕπθ and its gradient are well-behaved so that the explicit-dependence term (II) in equation 5 is finite and integrable to guarantee that the policygradient estimator has bounded variance. This assumption can be satisfied by many discrepancy functions, in our implementation, we choose ϕ as the KL divergence ϕπθ (s) = KL(πθ( s)µ( (cid:104) s)), whose gradient admits the standard score-function θϕπθ (s) = Eaπθ(s) θ log πθ(a s) (cid:0)1 + log πθ(a s) log µ(a s)(cid:1)(cid:105) . By enforcing overlapping support between πθ and µ in implementation (e.g., using probability floor), we guarantee that ϕπθ remains finite and that θϕπθ is bounded across all states, thereby satisfying assumption A1. Assumption A2 requires that the optimal policy π θ exists inside the feasible set, which implies that the budget constraint is almost surely satisfied and no probability mass is concentrated on the boundary. This assumption is mild in practice, since by choosing sufficiently large penalty parameter we can always discourage boundary-violating policies and guarantee the existence of feasible optimum. Under assumptions A1A2, we can characterize the explicit-dependence term (II) in unified way: 1) On strictly feasible trajectories, i.e., when (cid:80)t1 is locally constant in neighborhood of π vanishes. u=0 Cπθ (su) > 0, the feasibility indicator θ , so θ ˆRπθ,n(st, at) = 0 at every step and term (II) 2) When trajectory has already violated the budget, the reward switches to the penalized branch, therefore in the infeasible region term (II) reduces to θ ˆRπθ,n(st, at) = θϕπθ (st). 3) At the boundary, where the cumulative constraint exactly equals d, the reward becomes nondifferentiable. We replace the derivative with generalized subgradient, following prior RL works with non-smooth objectives Zhang et al. (2020); Wang & Zou (2022); Kumar et al. (2023). We adopt the Mordukhovich subgradient following the definition from Mordukhovich (2018), and the term (II) reduces to 1(cid:8)d (cid:80)t1 u=0 Cπθ (su) ε(cid:9) θϕπθ (st) by taking the limiting subgradient from the infeasible side with small tolerance ε 0 during training. We note that in practice, the probability of hitting the boundary exactly is small in the continuous setting of the constraint value, and term (II), through its explicit single-step decomposition, also contributes to variance reduction during training, as observed in prior works Czarnecki et al. (2019); Gu et al. (2024). As result, term (II) disappears on feasible trajectories near the optimum, while continuing to provide informative signals both for trajectories that violate the constraint and for those approaching the boundary. Therefore, our final gradient for optimization is (cid:34) θJn(θ) = Eπθ (cid:88) t0 θ log πθ(at st) (cid:16) (cid:88) (cid:17) γut ˆRπθ,n(su, au) (cid:35) ut (cid:34) Eπθ (cid:88) t0 (cid:40) γt t1 (cid:88) u=0 (cid:41) (cid:35) Cπθ (su) ε θϕπθ (st)"
        },
        {
            "title": "B PROOFS OF CONSTRAINT SATISFACTION GUARANTEE",
            "content": "Theorem B.1 (Optimal equivalence). For every feasible state sT , the optimal value functions of the unaugmented MDP (cid:99)Mn in equation 3 are equivalent: in equation 4 and the augmented MDP (cid:102)Mn ˆV (sT ) = (sT , zT ). Proof. Given the budget recursion zT = (cid:80)T 1 t=0 Cπ(st) and the fact that sT encodes the whole past and Cπ is deterministic in given fixed teacher policy µ and student policy π, zT is deterministic function of any reachable sT for any predefined budget d. Therefore, the step-wise , Rn(sT , zT , aT ) = ˆRπ,n(sT , aT ) for rewards in the feasible set are equivalent in (cid:102)Mn every reachable time along any feasible trajectories by the definitions in equation 3 and equation 4. and (cid:99)Mn The s-marginal transition kernel is identical in both formulations sT +1 PS ( sT , aT ), and the budget update is deterministic zT +1 = zT Cπ(sT ) in the augmented model (cid:102)Mn . Define the projected policy on the reachable set by π(a s) := π(a s, z(s)), where z(s) denotes the reconstructed budget associated with s. Then the action distribution under π at equals that under π at (s, z(s)). Therefore, the induced (s, a)-trajectory laws coincide, and together with the step-wise reward equality we obtain the policy-wise identity ˆV π Conversely, for any un-augmented policy π(a s) define the lifted policy π(a s, z) := π(a s). This yields π (s) on the reachable set, so the suprema over the two policy classes agree there; hence ˆV (s, z) = ˆV π (sT ) = π (sT , zT ). (sT ) = (sT , zT ). We adopt the following standard assumptions Hernandez-Lerma & Munoz de Ozak (1992); Sootla et al. (2022b) for the discrete token setting in distillation: B1. The reward function ˆRn(sT , aT ) is bounded, measurable, and upper semicontinuous on A; B2. The transition kernel is weakly continuous on A; B3. The action space is compact. Theorem B.2 (Bellman optimality and value convergence). Consider the unaugmented MDP (cid:99)Md, satisfying assumption B1-B3 with the associated equation 4, then: a) the Bellman equation is satisfied in (cid:99)Md; b) the optimal value function ˆV for (cid:99)Mn converges monotonically to ˆV for (cid:99)M . Proof. For B1, the task reward in our setting is bounded and measurable on feasible steps, 0 R(s, a) Rmax, and the discrepancy on infeasible steps is also bounded and measurable, 0 ϕπ(s) Φmax. On the discrete token stateaction space (S A), every real-valued function is continuous and hence also upper semicontinuous. Since each point is isolated, any sequence ˆRn(s, a) = ˆRn(s, a), which es- (sk, ak) (s, a) is eventually constant, so lim sup(s,a)(s,a) tablishes B1. For B2, note that for any bounded function : R, the map (s, a) (cid:55) (cid:80) P(s s, a) g(s) is continuous since the domain is discrete, which implies the usual weak continuity condition holds in this setting. For B3, the action set is finite token space, hence compact. a) Under B1B3, standard dynamic programming results ensure the existence of an optimal value function satisfying the Bellman equation for (cid:99)Mn by using Theorem 4.2 in Hernandez-Lerma & Munoz de Ozak (1992), applied here to the discrete setting. b) The penalty on infeasible steps becomes harsher with while using the same discrepancy function ϕπ. Let > n, then on infeasible steps ˆRm ˆRn. Hence ˆV π (s) for any policy π and state s, and taking supπ yields ˆV converge monotonically to ˆV (s). Therefore, the optimal values ˆV m(s) ˆV π m(s) ˆV as . 16 Theorem B.3 (Almost surely constraint satisfaction). If there exists an optimal policy π solving (cid:99)M with finite value, then π is also an optimal policy for the original constrained MDP Md and satisfies the constraint almost surely. Proof. In (cid:99)M value under π implies Pπ ((cid:80) the feasible set, where the budget is never violated, the step-wise rewards in (cid:99)M so the objectives coincide. Since π maximizes the objective in (cid:99)M it also maximizes the objective in Md and satisfies the constraint almost surely. , any trajectory that ever violates the budget receives return; therefore finite t=0 Cπ (st) d) = 1, i.e., the constraint holds almost surely. On and Md coincide, and is feasible almost surely,"
        },
        {
            "title": "C A PERSPECTIVE OF LLM DISTILLATION AS CONTEXTUAL MDPS",
            "content": "We formalized LLM distillation as standard MDP in this work, given that the student πθ is frozen within each episode and the teacher µ is fixed during distillation, so the induced control process is time-homogeneous. This is the standard formulation used in prior RL for LLM distillation works (Gu et al., 2024; Czarnecki et al., 2019) and supports standard convergence/optimality analysis. Here we note an equivalent viewpoint that treats each episode under fixed context (e.g., policy checkpoint), giving Contextual MDP that is optimality equivalent to the standard MDP formulation. Definition C.1 (Contextual MDP for LLM Distillation). The contextual MDP Mctx (C, S, A, P, Rctx contextual reward Rctx is tuple , γ), where is the context space, with fixed during an episode, the : is Rctx (s, a; c) = R(s, a), (cid:0)n + ϕ(s, c)(cid:1), 1 (cid:88) if t=0 otherwise. C(st, c) 0, with C(, c) the per-step constraint at context and ϕ(s, c) any f-divergence (e.g., ϕ(s, c) = KL(cid:0)πc( s)µ( s)(cid:1)). contextual policy is Markov kernel π( s, c) on A. For any fixed c, the slice of Mctx (cid:99)Md, with per-context reward ˆRπc,n(s, a) := Rctx s, c). Proposition C.2. For every contextual policy π( s, c), there is corresponding per-context policy πc( s) = π( s, c) such that at that context induces the per-episode stationary problem used in (s, a; c) and per-context policy πc( s) := π( π(s, c) = ˆV πc (s). Conversely, for every per-context policy πc( s) there is contextual policy π( s, c) = πc( s) with the same return. Consequently, sup π π(s, c) = sup πc ˆV πc(s), and optimal contextual policies and optimal per-context policies coincide on the reachable set. Proof sketch. This contextualization with fixed is an annotated MDP in the sense of (Bacchus et al., 1996, Def. 4.1), with extended states (s, c) and stepwise rewards Rctx (s, a; c). For any π( s, c), the (s, a)-trajectory law under Mctx coincides with that under the per-context policy πc( s) in (s, a; c) = ˆRπc,n(s, a) at the fixed (cid:99)Md; moreover the stepwise rewards agree by construction Rctx context. Hence π(s, c) = ˆV πc (s) on the reachable set. The projection/lifting correspondence for annotated expansions (cf. (Bacchus et al., 1996, Prop. 4.3 and Cor. 4.4)) then yields equality of suprema and optimal policies on the reachable set. This formulation keeps as an explicit input to the reward while remaining per-episode stationary because is fixed within an episode. It is thus notationally different but also equivalent way to present the same optimization problem as in the standard MDP."
        },
        {
            "title": "D ALGORITHM AND IMPLEMENTATION",
            "content": "D.1 REWARD FUNCTION DESIGN For mathematical reasoning tasks, we use binary rewards based on final answer correctness: R(sT , aT ) = (cid:26)1.0 0.0 if final answer is correct if final answer is incorrect (6) The reward is only assigned at the final step of each trajectory when the complete solution is generated. This sparse reward structure is typical for mathematical reasoning tasks where intermediate steps cannot be easily evaluated without domain expertise. D.2 KL DIVERGENCE COMPUTATION The KL divergence between student and teacher policies is computed at each time step as: KL(πθ(st)µ(st)) = (cid:88) aV πθ(ast) log πθ(ast) µ(ast) (7) where is the LLM vocabulary. D.3 HYPERPARAMETER SETTINGS We used the following hyperparameters for all the method: Batch size: 64 responses (8 questions 8 responses per question) Learning rate: 1e5 Optimizer: AdamW Discount factor γ = 1 Constraint threshold = 0.35. The constraint threshold was selected based on preliminary experiments that seek to minimize only the KL (mini-LLM and GKD). Number of training epochs: 20 Penalty n: 20 The training of Llama3.2-3B with GRPO was unstable due to its very poor initial performance; therefore, to bootstrap all methods, we apply KL distillation alone for the first 3 epochs (even with GRPO λ = 0). D.4 TRAINING TIME The training takes less than 2 days on single accelerator for each method. Overall, all the methods need the same amount of training time. GRPO is only bit faster because the teacher is not used, but backward phases and generation time dominate the overall training time."
        },
        {
            "title": "E MORE EXPERIMENTS RESULTS",
            "content": "18 Table 1: Distillation results of Qwen2.5-1B on GSM8K. Higher final answer correctness (FAC), reasoning win rate (RWR) and constraint satisfaction (CS) are better, while lower KL divergence and lower reasoning loose rate (RLR) are better. Method Ours Apple/GSM-Symbolic GSM8K MATH FAC RWR RLR KL CS FAC RWR RLR KL CS FAC RWR RLR KL CS 70.80 60. 10.58 0.16 ( 0.17) 96.1 76.11 58.72 7.86 0.15 (0.19) 94.99 25. 41.65 14.44 0.15 ( 0.17) 96.2 GRPO λ = 0.0 GKD-GRPO λ = 0.001 GKD-GRPO λ = 0.01 GKD-GRPO λ = 0.1 GKD-GRPO λ = 1.0 GKD-GRPO λ = 10 GKD Mini-LLM Student model Teacher model 75.94 14.89 10.64 75.50 34.87 72.52 22.34 69.88 29.12 67.47 30.01 67.8 28.1 68.34 27.65 68.02 0 88. 53.58 0.41 ( 0.28) 41.74 57.88 0.29 (0.23) 76.6 25.27 0.18 (0.25) 94.2 36.04 0.16 (0.23) 94.92 17.74 0.17 (0.29) 94.34 17.59 0.16 (0.25) 94.66 19.24 0.16 (0.25) 94.88 20.24 0.16 (0.28) 94.2 2.08 (1.89) 0.14 80.06 80.51 75.89 75.13 73.69 74.07 74.37 74.22 0.22 92.27 12.15 10.94 34.52 20.82 29.01 28.94 27.03 26.20 54.67 0.41 ( 0.29) 42.83 55.71 0.28 (0.17) 78.01 23.76 0.18 ( 0.23) 92.11 35.36 0.14 ( 0.20) 94.61 16.37 0.16 ( 0.32) 94.08 16.73 0.15 ( 0.23) 93.1 18.18 0.15 ( 0.23) 94.08 19.68 0.15 ( 0.26) 93. 1.96 (1.82) 0.45 29.04 28.60 29.04 27.68 27.16 26.6 26.42 27.06 0.54 34.46 19.49 18.5 26.55 20.86 24.04 24.01 23.12 22.01 39.62 0.27 (0.19) 78.68 38.73 0.23 (0.17) 87.40 24.23 0.15 (0.14) 94.94 30.23 0.14 (0.15) 95.46 17.37 0.15 (0.21) 95.02 17.82 0.15 (0.18) 95.12 18.07 0.15 (0.17) 94.98 19.71 0.15 (0.21) 95.56 2.47 (2.09) 3.4 Table 2: Distillation results of Llama3.2-3B on MATH. Higher success rates (SR) and constraint satisfaction (CS) are better, while lower KL divergence is better. Method Ours Apple/GSM-Symbolic GSM8K MATH FAC RWR RLR KL CS FAC RWR RLR KL CS FAC RWR RLR KL CS 36.78 42. 21.44 0.22 (0.07) 94.64 38.36 51.76 19.78 0.21 (0.07) 99.60 17.10 34. 23.58 0.15 (0.06) 99.48 GRPO λ = 0.0 GKD-GRPO λ = 0.001 GKD-GRPO λ = 0.01 GKD-GRPO λ = 0.1 GKD-GRPO λ = 1.0 GKD-GRPO λ = 10 GKD Mini-LLM Student model Teacher model 42.48 40.20 40.22 42.28 38.02 37.92 36.88 37.34 0 51.86 33.82 38.42 23.81 27.21 24.31 26.16 26.87 26. 39.12 0.71 (0.15) 0.16 32.37 0.49 (0.12) 14.56 33.77 0.29 (0.09) 72.86 27.65 0.23 (0.08) 90.56 30.63 0.21 (0.08) 94.18 28.10 0.21 (0.08) 94.5 27.71 0.21 (0.08) 94.7 28.66 0.21 (0.08) 94.38 0.73 (0.88) 49.73 53.44 52.53 53.37 42.45 38.66 38.36 39.25 2.75 54.58 21.30 37.18 43.60 32.74 31.99 30.08 41.74 26.12 57.14 0.73 (0.15) 0.3 0.5 (0.13) 12.81 34.36 28.22 0.28 (0.09) 80.89 35.67 0.23 (0.08) 92.57 35.24 0.21 (0.07) 95.98 36.90 0.20 (0.07) 95.60 28.42 0.20 (0.07) 95.98 40.73 0.21 (0.07) 95. 1.45 (1.57) 18.90 18.52 17.62 17.48 17.80 18.42 17.80 15.88 0.08 32.08 25.44 33.87 29.21 30.07 27.38 30.17 29.80 30.27 0.64 (0.2) 47.96 8.08 34.65 0.39 (0.14) 38.98 29.32 0.21 (0.08) 93.52 25.38 0.16 (0.07) 98.20 29.90 0.14 (0.06) 99.22 28.18 0.14 (0.06) 99.46 25.66 0.14 (0.06) 99.24 26.22 0.14 (0.06) 99.20 1.02 (0.9) Figure 5: Pairwise comparison heatmap showing the relative performance of our method against baselines, averaged across all three evaluation datasets (Apple/GSM-Symbolic, GSM8K, and MATH) on Llama3.2-3B. Averaging over columns gives the reasoning win rate (RWR) and over rows the reasoning loss rate (RLR)."
        },
        {
            "title": "F GENERATED ANSWERS",
            "content": "Question: Leilani saw 300-foot whale with 4 45-inch remoras attached to it. What percentage of the whales body length is the combined length of the remoras? Ours: GRPO λ = 0: To determine what percentage of the whales body length the combined length of the remoras is, we need to follow these steps: 1. Convert the length of the remoras from inches to feet: 45 inches 1 foot 12 inches = 3.75 feet So, each remora is 3.75 feet long. 2. Calculate the total length of the remoras: 4 remoras 3.75 feet per remora = 15 feet 3. Determine the total length of the whale: 300 feet 4. Calculate the percentage of the whales body length that the combined length of the remoras represents: (cid:19) (cid:18) 15 feet 300 feet 100% 5. Simplify the fraction: 15 300 = 0. 6. Convert the decimal to percentage: 0.05 100% = 5% Therefore, the combined length of the remoras is 5 percent of the whales body length. Lets break down the problem step by step to find the percentage of the whales body length that the combined length of the remoras represents. First, we need to convert all units to the same unit. Lets use feet for this calculation. The length of each remora is given as 45 inches. Since there are 12 inches in foot, the length of each remora in feet is: 45 12 = 3.75 feet Since there are 4 remoras, the total length of the remoras is: 4 3.75 = 15 feet Next, we add the length of the whale to the combined length of the remoras to get the total length of the whale: 300 + 15 = 315 feet Now, we need to find what percentage the combined length of the remoras is of the whales body length. This can be calculated using the formula for percentage: (cid:19) (cid:18) 15 315 100 Simplifying the fraction inside the percentage formula: 15 315 = 1 (cid:19) (cid:18) 1 21 100 = 100 21 4.7619 Rounding to the nearest whole number, we get: 5 Judge: Response correctly calculates the percentage by comparing the remoras total length to the whales length without adding them, ensuring accuracy. Response incorrectly adds the remoras length to the whales, leading to flawed calculation. Verdict: wins Figure 6: Example of generated answer with Qwen2.5-1.5B-Math after distillation. 20 Question: Wei saw 200-foot whale with 2 12-inch remoras attached to it. What percentage of the whales body length is the combined length of the remoras? Ours: GRPO λ = 1: To determine what percentage of the whales body length the combined length of the remoras is, we need to follow these steps: 1. Convert the length of the remoras from inches to feet: - Each remora is 12 inches long. - There are 12 inches in foot. - Therefore, the length of each remora in feet is: 12 inches 12 inches per foot = 1 foot - Since there are 2 remoras, the total length of the remoras in feet is: 2 remoras 1 foot per remora = 2 feet 2. Calculate the percentage of the whales body length: - The whales body length is 200 feet. - The combined length of the remoras is 2 feet. - To find the percentage, we use the formula: Percentage = (cid:18) Length of remoras Whales body length (cid:19) 100 - Substituting the values, we get: Percentage = (cid:19) (cid:18) 2 feet 200 feet 100 - Simplify the fraction: To determine what percentage of the whales body length the combined length of the remoras is, we need to follow these steps: 1. Convert the length of the remoras from inches to feet. 2. Add the length of the remoras to the length of the whale. 3. Calculate the percentage of the whales body length that the combined length of the remoras represents. First, we convert the length of the remoras from inches to feet. Since there are 12 inches in foot, we divide 12 by 12 to get 1 foot per inch. Therefore, the length of each remora is: 12 inches 12 = 1 foot Since there are 2 remoras, the total length of the remoras is: 2 remoras 1 foot/remora = 2 feet Next, we add the length of the remoras to the length of the whale: 200 feet + 2 feet = 202 feet Now, we calculate the percentage of the whales body length that the combined length of the remoras represents. We do this by dividing the total length of the remoras by the length of the whale and then multiplying by 100: (cid:19) (cid:18) 2 feet 200 feet 100 = 1% 2 200 = 0.01 Therefore, the combined length of the remoras is 1 % of the whales body length. - Multiply by 100 to convert to percentage: 0.01 100 = 1% Therefore, the combined length of the remoras is 1 % of the whales body length. Judge: Response is more clear and logically structured, breaking down each step with proper mathematical notation and avoiding unnecessary complexity. Response incorrectly adds the remoras length to the whales length, which is irrelevant to the problem and introduces confusion. Verdict: wins Figure 7: Example of generated answer with Qwen2.5-1.5B-Math after distillation. Question: There are 154 fourth-graders at Maple Grove School. 80 of them are girls. On Thursday, 3 fourth-grade girls and 6 fourth-grade boys were absent. How many fourth grade boys were at Maple Grove School on Thursday? Ours: GDK: To determine how many fourth-grade boys were at Maple Grove School on Thursday, we need to follow these steps: 1. Calculate the number of fourth-grade boys: - The total number of fourth-graders is 154. - The number of fourth-grade girls is 80. - Therefore, the number of fourth-grade boys is: 154 80 = 74 2. Determine the number of fourth-grade boys absent: - On Thursday, 6 fourth-grade boys were absent. 3. Calculate the number of fourth-grade boys present: - The total number of fourth-grade boys is 74. - The number of fourth-grade boys absent is 6. - Therefore, the number of fourth-grade boys present is: 74 6 = 68 Thus, the number of fourth-grade boys at Maple Grove School on Thursday is 68 . To determine how many fourth-grade boys were at Maple Grove School on Thursday, we need to follow these steps: 1. Calculate the number of fourth-grade girls present: - Total number of fourth-graders: 154 - Number of fourth-grade girls: 80 - Number of fourth-grade girls present: 154 3 = 151 2. Calculate the number of fourth-grade boys present: - Total number of fourth-graders: 154 - Number of fourth-grade boys: 154 80 = 74 - Number of fourth-grade boys present: 74 6 = 68 Thus, the number of fourth-grade boys present on Thursday is 68 . Judge: Response provides clearer and more accurate approach by first determining the total number of boys and then subtracting the absent ones. Response incorrectly calculates the number of girls present, which could lead to confusion. Verdict: wins Figure 8: Example of generated answer with Qwen2.5-1.5B-Math after distillation."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Huawei R&D Munich",
        "UCL Centre for Artificial Intelligence"
    ]
}