{
    "paper_title": "REOrdering Patches Improves Vision Models",
    "authors": [
        "Declan Kutscher",
        "David M. Chan",
        "Yutong Bai",
        "Trevor Darrell",
        "Ritwik Gupta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sequence models such as transformers require inputs to be represented as one-dimensional sequences. In vision, this typically involves flattening images using a fixed row-major (raster-scan) order. While full self-attention is permutation-equivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, a two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn a policy over permutations by optimizing a Plackett-Luce policy using REINFORCE. This approach enables efficient learning in a combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 5 7 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "REOrdering Patches Improves Vision Models",
            "content": "Declan Kutscher1 David M. Chan2 Yutong Bai2 Trevor Darrell2 Ritwik Gupta2 1University of Pittsburgh 2University of California, Berkeley"
        },
        {
            "title": "Abstract",
            "content": "Sequence models such as transformers require inputs to be represented as onedimensional sequences. In vision, this typically involves flattening images using fixed row-major (raster-scan) order. While full self-attention is permutationequivariant, modern long-sequence transformers increasingly rely on architectural approximations that break this invariance and introduce sensitivity to patch ordering. We show that patch order significantly affects model performance in such settings, with simple alternatives like column-major or Hilbert curves yielding notable accuracy shifts. Motivated by this, we propose REOrder, two-stage framework for discovering task-optimal patch orderings. First, we derive an information-theoretic prior by evaluating the compressibility of various patch sequences. Then, we learn policy over permutations by optimizing Plackett-Luce policy using REINFORCE. This approach enables efficient learning in combinatorial permutation space. REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive sequence models have become the backbone of leading systems in both language and vision. These models operate on images by first converting their 2-D grid structure into 1-D sequence of patches. Conventionally, row-major order is used for this linearization under the assumption that full self-attention is utilized by the model. As self-attention, augmented with positional embeddings, is permutation-equivariant, the exact patch order has been treated as inconsequential. However, this assumption does not hold in the context of modern, long-sequence models which introduce strong inductive biases such as locality [1], recurrence [2], or input-dependent state dynamics [3] that are sensitive to input ordering. Through mechanisms such as sparse attention via masking summarizing early parts of long-sequence into latent representations, these methods are able to model long-sequences in computationally tractable fashion. However, we assert that these design choices introduce strong dependency on patch ordering, something that has previously been overlooked. In this work, we demonstrate that merely swapping the row-major scan for one of six studied alternatives, such as column-major or Hilbert curves, yields measurable accuracy gains across multiple long-sequence backbones. Further, we introduce REOrder, method to learn an optimal patch ordering. REOrder first approximates which patch ordering may lead to best performance and then learns to rank patches in order of importance with reinforcement learning. Specifically, to convert the 2-D image into 1-D sequence, we first quantify the compressibility resulting from each of six different patch orderings. Then, Plackett-Luce ranking model is initialized with the least compressible ordering and further trained to minimize classification loss with REINFORCE. REOrder improves performance on ImageNet-1K by up to 3.01% (0.23%) and Functional Map of the World by 13.35% (0.21%). Code and animations are available on the project page. Figure 1: Visualizations of alternate patch sequence orderings. Six different patch ordersrow-major, column-major, Hilbert curve, spiral, diagonal, and snakeare shown as trajectories over 14 14 grid of patches. Each trajectory begins at the red dot and progresses to the black dot, illustrating the 1-D ordering imposed on the 2-D patch grid."
        },
        {
            "title": "2 Related Works",
            "content": "Transformers as sequence models for vision. The concept of treating visual data as linearized sequences for generative modeling and other tasks has been explored for some time, with early work using models like LSTMs on sequences of visual frames [4]. Building on the success of transformers in image-sequence modeling, the Image Transformer [5] adapted this architecture to single images by dividing each image into patches and processing these patches as 1-D sequence. To manage the computational cost for images, the original Image Transformer employed self-attention mechanisms constrained to local neighborhoods of patches. The Vision Transformer (ViT) [6] expanded this attention approach, applying global self-attention over the entire sequence of image patches. However, this global attention mechanism, while effective, is an O(n2) operation in computation and space w.r.t. the number of patches, becoming computationally prohibitive for long-sequences. To overcome this challenge and enable the application of transformers to larger images, subsequent research has developed various strategies, including hierarchical tokenization and more efficient attention mechanisms. Hierarchical tokenization, for instance, involves processing images at multiple scales to reduce the sequence length at higher levels. Gupta et al. [7] show how such hierarchical tokenization can be used for tractably modeling the resulting long-sequences. Motivated by this history of linearizing 2-D images into 1-D sequences for transformer processing and the methods developed to handle the resulting sequence length and computational complexity, in this work, we explore whether the specific order in which 2-D image is converted to 1-D sequence matters for model performance. Long-sequence vision models. As established, the quadratic computational cost of full selfattention with respect to sequence length makes processing long-sequences of patches very expensive for standard Vision Transformers. To address this, significant research has focused on developing efficient transformer architectures and long-sequence models that reduce this complexity, such as Sparse Attention [8], Longformer [1], Transformer-XL [2], and Mamba [3, 9]. While making modeling tractable, these methods often introduce specific inductive biases in how they process sequences, potentially making them sensitive to the order of the input tokens (which we discuss further in Section 3.3). In this work, we study the effect of patch ordering on Longformer, TransformerXL, and Mamba, demonstrating that the inductive biases inherent in these different long-sequence modeling approaches lead to substantial accuracy variance across different patch orders. Patch order sensitivity. Qin et al. [10] provided an in-depth study of the self-attention mechanism, establishing its property of permutation equivariance. However, comparable analysis for longsequence models that process vision tokens remains largely unexplored. This gap is significant, especially when considering findings from NLP, where studies like [11] revealed that long context models tend to neglect tokens in the middle of sequence. Motivated by these observations and the underexplored nature of token ordering in long visual sequences, our work aims to investigate these effects. While some initial work in vision, such as ARM [9], has touched upon scan order for Mamba models (settling on small, row-wise clusters), this exploration did not comprehensively cover the wider space of possible orderings. Our research addresses this limitation by more rigorously examining the search space through experiments with six different orderings across multiple sequence models. Furthermore, to move beyond predefined arrangements, our method, REOrder, employs reinforcement learning to explore the space of possible permutations for the patch sequence. 2 Learning to rank with reinforcement learning. The foundation of learning to rank from pairwise comparisons was established by Bradley and Terry [12] who proposed probabilistic model to estimate item scores via maximum likelihood. More recent work has extended learning to rank with reinforcement learning frameworks, particularly in information retrieval [13] and recommendation systems [14]. Wu et. al. [15] assign importance scores to image tokens based on their impact on CLIPs predictions then trains supervised predictor to replicate these scores for efficient token pruning. Learning to rank patches can be reformulated to learning permutation resulting from the ordering of patches by rank. Büchler et. al. learns to select effective permutations of patches or frames through reinforcement policy that maximizes the improvement in self-supervised permutation classification accuracy in discrete action space. In contrast, REOrder models stochastic policy over permutations using Plackett-Luce distribution [16, 17] and optimizes it with REINFORCE [18] and Gumbel Top-k sampling which allows for more flexible orderings."
        },
        {
            "title": "3 Preliminaries",
            "content": "We first establish the properties of the self-attention mechanism, specifically its equivariance under conjugation by permutation matrices and the matrix multiplication that lends it O(n2) complexity. We then examine how Transformer-XL, Longformer, and Mamba model relationships between longsequences and the design choices they make to side-step the quadratic complexity of self-attention. This establishes the sensitivity of long-sequence models to patch ordering. 3.1 Self-attention and permutation equivariance Self-attention is the primary mechanism used in the Vision Transformer. Let the image patches form the matrix = [x1; . . . ; xn] Rnd. Self-attention can then be computed as Attn(X) = softmax (cid:16) (XWq)(XWk) (cid:17) XWv, Wq, Wk, Wv Rdd (1) where Wq, Wk, and Wv are learnable query, key, and value matrices, respectively. The similarity matrix inside the softmax is O(n2) which is undesirable for large X. However, full self-attention has useful symmetry. Proposition 3.1 (Permutation equivariance of self-attention). For every permutation matrix {0, 1}nn, Attn(P X) = Attn(X). (2) Equation (2) states that self-attention is equivariant to arbitrary permutations of the patch ordering. Hence, the Vision Transformer, composed entirely of self-attention, is also permutation-equivariant. proof for Proposition (3.1) is in Appendix B; thorough analysis is conducted by Xu et. al. [19]. 3.2 Position embeddings The permutation equivariance of full self-attention is undesirable for image processing tasks. The spatial arrangement of patches in an image carries critical semantic information that must be preserved or made accessible to the model. Positional embeddings were introduced to explicitly provide the model with information about the original 2-D location of each patch within the image grid by summation with the image tokens. This allows the model to understand the spatial relationships between patches, regardless of their position in the input 1-D sequence. We use learned absolute position embeddings across our models. In Transformer-XL, these are incorporated in conjunction with its native relative positional encoding. When specific base patch ordering is adopted for an experiment, the positional embeddings are learned to align with that particular fixed sequence. Crucially, the positional embedding for the [CLS] token is consistently learned for the initial sequence position and is not reordered with the patch tokens, thereby maintaining stable reference for classification tasks. 3 3.3 Self-attention approximations and sensitivity to patch order Recent work reduces the O(n2) cost of full attention by sparsifying the attention pattern, factoring the softmax kernel, or replacing it with learned recurrence. These changes, however, also break the full permutation-equivariance guarantee of Eq. (2). Below we inspect three representative models. Their attention patterns are visualized in Appendix G. Transformer-XL. Transformer-XL [2] adds segment-level recurrence with memory. After segment of length is processed, each layer caches its hidden states as memory Rmd. At the next step the layer concatenates that memory with the current segments hidden states RLd: = concat(cid:2)SG(M), H(cid:3) R(m+L)d, where SG() is stop-gradient. Following Eq. (1), we define single set of projection matrices as = Wk, = Wv = HWq, Let permute the patches of only the current segment (but not the memory). Memory corresponds to previous segments, so its rows are fixed under permutation. = [SG(M); H] Because the second and third logit terms of MTXL(i, j) depend on the relative index j, conjugating by does not commute with the soft-max: ATXL(P H) = ATXL(H). Thus, Transformer-XL is permutation-sensitive even though its content-content term alone would be equivariant. Longformer. Longformer [1] uses sliding-window pattern of width plus global tokens with their own projections (g) . Because the local mask local fixes which (i, j) pairs are valid, applying to the rows/columns re-labels many entries as illegal () and others as legal, so softmax(P localP) = softmax(M local) P. , (g) , (g) The same holds for the global mask unless preserves the hand-picked global positions. Therefore, Longformer is sensitive to patch ordering by design. It converts O(n2) complexity to O(nw) under rigid spatial prior. Mamba and ARM. Mamba [3] does not implement quadratic attention and instead introduces content-dependent state-space update. ht = At ht1 + Bt xt, yt = Ct ht, with inputs xt produced by scanning the patch sequence left-to-right. permutation re-orders the stream, changing (Bt, Ct, t) and the sequence of matrix multiplications, so the recurrence yield is different. Therefore Mamba is also permutation sensitive. Its O(n) complexity comes at the price of fixed processing order. In this work, we use ARM as introduced by Ren et. al. [9]. In ARM, every Mamba layer runs four causal scans in different directions and sums their outputs before the channel-mixing MLP. For each {, , , } let x(d) in direction d. The direction-specific recurrence is t1 + B(d) = A(d) h(d) and the layer output combines the four scans via be the patch encountered at time step when traversing the image = (d) y(d) h(d) x(d) h(d) , , t yt = (cid:88) y(d) . d{,,,} Similar to Mamba, because each scan is individually directional, an arbitrary permutation of the patch order both re-orders the input streams {x(d) } and alters the learned parameter sequences (cid:9). Hence the mapping still violates permutation equivariance y(P X) = y(X). (cid:8)A(d) , B(d) , (d) 4 Figure 2: Patch order affects the performance of long-sequence models. This figure compares the top-1 accuracy of Vision Transformer (ViT), Longformer, Mamba, and Transformer-XL (T-XL) on ImageNet-1K and Functional Map of the World when using alternate patch orderings, relative to their standard row-major performance. As expected, ViT remains equivariant to patch sequence permutations. In contrast, long-sequence models exhibit substantial performance variability depending on the patch ordering. No single ordering consistently outperforms others across models or datasets, necessitating dynamic patch ordering strategies."
        },
        {
            "title": "4 Does Patch Order Matter?",
            "content": "Conventional autoregressive vision models default to raster-scan (row-major) order for flattening 2-D images into 1-D sequences. We investigate the question of whether the sequence in which image patches are presented to autoregressive vision models has an impact on their performance. We begin by outlining the datasets and models used in our empirical evaluation, followed by details of our training methodology. We then present results demonstrating that variations in patch ordering, even simple ones like column-major scans, can lead to differences in model outcomes, thereby motivating search for more optimal, potentially learned, orderings. We define and explore six fixed patch orders: row-major, column-major, Hilbert curve, spiral, diagonal, and snake. These are visualized in Figure 1 and formally defined in Appendix F. Datasets. Images captured in different contexts demonstrate varying structural biases. To study whether such datasets are susceptible to patch ordering effects to different degrees, we run experiments on two datasets: ImageNet-1K [20] (natural images) and Functional Map of the World [21] (satellite). We train on their respective training sets and report results on the validation sets. Models. We experiment with the Vision Transformer (ViT) [6], Transformer-XL (TXL) [2], Longformer [1], and Mamba [3, 9] models. We utilized the timm implementation for the Vision Transformer (ViT) and the HuggingFace implementation for Longformer. Both were adapted with minor modifications to accommodate varying patch permutations. TXL is based on the official implementation and includes newly introduced, learned absolute position embedding to account for changing patch orders across batches. We use ARM [9] as our vision Mamba model of choice due to its training stability. All four models prepend learnable class [CLS] token as fixed-length representation for image classification. The [CLS] token is always retained as the first token in the sequence. All models use their respective Base configurations. Complete details about the model configurations are in Appendix C. Training. Experiments are conducted on machines equipped with either 880GB A100 GPUs or 440GB A100 GPUs. All models are trained for 100 epochs the AdamW optimizer using β1 = 0.9, β2 = 0.999, weight decay of 0.03, and base learning rate of α = 1.0 104. Batch sizes are held constant for all runs across all model-dataset pairs (details in Appendix D). We apply cosine learning rate decay with linear warmup over 5 epochs. For the reinforcement learning experiments introduced in Section 6, we use the same optimizer configuration but with reduced base learning rate of α = 1.0 105 and no decay. 4.1 Performance Variation Across Orderings We evaluated top-1 accuracy on validation sets, estimating the Standard Error of the Mean (SEM) using non-parametric bootstrap method with 2,000 resamples. Our analysis focused on how model performance varied with different patch orderings across datasets. 5 Figure 3: Compression of 1-D sequences can serve as weak prior for optimal patch ordering. Top-1 accuracy is compared to percentage reduction for different patch orderings across four models for both ImageNet1K and FMoW. As anticipated, the Vision Transformer (ViT) demonstrated permutation equivariance, achieving consistent top-1 accuracy (approx. 37.5% on ImageNet-1K and 46.5% on FMoW) irrespective of patch order (Figure 2). In contrast, long-sequence models like Longformer, Mamba, and Transformer-XL (T-XL) showed performance variations dependent on patch order. T-XL and Mamba were particularly sensitive; on ImageNet-1K, T-XLs accuracy increased by 1.92%(0.21%) with column-major ordering and decreased by 6.43%(0.21%) with spiral. Longformer also benefited from alternative orderings (column-major, Hilbert, snake) on ImageNet-1K, improving by up to 1.83%(0.22%) over row-major. Dataset characteristics altered these trends. On FMoW, Longformers optimal ordering shifted (e.g., diagonal increased accuracy by 1.3%(0.22%) while column-major was detrimental). T-XL still favored column-major but also benefited from diagonal ordering on FMoW. Overall, FMoW exhibited less sensitivity to patch ordering than ImageNet, likely due to the greater homogeneity of satellite imagery compared to diverse natural images. Mamba consistently performed worse with non-row/column major orderings, likely because its fixed causal scan directions (d {, , , }) conflict with other patch sequences. Adapting Mambas scan order to the patch ordering could potentially mitigate this performance drop. These results highlight that there is no one ordering that works best for all models or datasets. Given that in many cases an alternate ordering outperforms the standard row-major order, this raises an important question: can we discover an optimal ordering tailored to specific model and dataset? Moreover, are there useful priors we can identify to guide the search for such orderings?"
        },
        {
            "title": "5 Learning an Optimal Patch Ordering with REOrder",
            "content": "The observation that patch order influences model performance suggests the existence of an optimal ordering for each model-dataset pair. To find such orderings, we introduce REOrder, unified framework that combines unsupervised prior discovery with task-specific learning. We begin with an information-theoretic analysis, examining the link between sequence compressibility and downstream performance. REOrder automates this process to derive prior over effective patch orderings. Building on this, it employs reinforcement learning approach to directly learn task-specific orderings, leveraging the prior as guidance. Information-Theoretic Initialization The order in which image patches are arranged affects the compressibility of the resulting sequence. For example, in conventional raster-scan order, adjacent patches often contain similar content, making the sequence more compressible. This local redundancy might make the prediction task more trivial, as the model could focus on learning simple local correlations rather than capturing more complex, long-range dependencies. We first explore whether 6 π π PERMUTE(x(b), π) (cid:1) Algorithm 1 REOrder with Plackett-Luce policy Require: mini-batch {(x(b), y(b))}B 1: Gumbel(0, 1)n 2: π argsortdesc(z + τ g) 3: x(b) 4: ˆy(b) fθ (cid:0)x(b) 1 6: LCE 7: βb + (1 β)r 8: 9: Lpolicy log (π z) 10: Ltotal LCE + Lpolicy 11: Back-propagate θ,zLtotal and update with Adam 5: LCE log ˆy(b) y(b) (cid:88) b=1 b=1, backbone fθ, logits z, Gumbel temperature τ , baseline b, momentum β Gumbel-top-k permutation Reward Advantage compression metrics could serve as proxy for evaluating different patch orderings. Specifically, we discretize images using VQ-VAE based model [22] and encode the resulting token sequence codes using both unigram and bigram tokenization. For each configuration, we measure the compression ratio achieved by LZMA, which provides quantitative measure of local redundancy in the sequence. In Figure 3, we observe that different patch orderings indeed lead to varying compression ratios. The row-major order, which is commonly used in vision transformers, achieves higher compression ratios, suggesting strong local redundancy. Interestingly, the Hilbert curve ordering, which aims to preserve spatial locality, shows similar compression characteristics to row-major order. In contrast, the column-major/spiral orderings exhibit lower compression ratios, indicating less local redundancy. While this perspective offers an interesting lens through which to view patch ordering, we find that compression alone is not sufficient to predict optimal orderings for downstream tasks. For instance, while column-major ordering in Figure 3 shows lower compression ratios, it does not consistently lead to better model performance across all architectures. This motivates our subsequent exploration of learning-based approaches to discover effective patch sequences."
        },
        {
            "title": "6 Learning to Order Patches",
            "content": "Our findings in Section 4 reveal that patch ordering can significantly affect the performance of longsequence vision models. This suggests the potential value of discovering dataand model-specific ordering to improve task performance. Unfortunately, learning discrete permutation poses unique challenge. For an image with patches, there are ! possible orderings. For = 196, there are more than 10365 options. Searching this combinatorial space exhaustively is infeasible. naïve approach would require evaluating each permutations classification loss on every training example, which is computationally intractable and incompatible with gradient-based learning. We instead treat the selection of patch orderings as stochastic policy learning problem, where the policy outputs distribution over permutations. This allows us to sample permutations during training and optimize the policy against the downstream classification loss with reinforcement learning. 6.1 Learning the Permutation Policy via REINFORCE Because permutations are discrete and non-differentiable, we adopt the REINFORCE algorithm [18, 23], score-function estimator, to search over the space of all possible permutations and optimize permutation policy. REOrder is outlined in Algorithm 1. This formulation treats the permutation sampler as stochastic policy, whose parameters are updated based on the classification reward. REINFORCE is unbiased but can have high variance. To mitigate this, we subtract running baseline bt from the reward, giving At = rt bt, where bt+1 = β bt +(1β) rt, with β (= 0.99) controlling the baselines momentum. The vision transformers parameters θ depend only on the cross-entropy loss LCE, while the policy model receives gradients from LCE and the REINFORCE loss. 7 Figure 4: The logits of the Plackett-Luce model, and therefore the permutation order, changes over the course of training. Longformer is initialized with column and row-major patch ordering and optimized with REOrder. The image is of the class keyboard. We track two patches over the course of the policy curriculum: keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As the policy learns to order patches, we see the patches related to the target class move to the end of the ordering. 6.2 The Plackett-Luce Policy for Patch Orderings To parameterize the permutation distribution, we use the Plackett-Luce (PL) model [16, 17]. This model defines distribution over permutations based on learned logit vector Rn associated with the image patches. permutation is sampled sequentially: at each step, yet-unplaced patch is selected according to softmax over the remaining logits: (π z) = (cid:89) i=1 exp(cid:0)zπi (cid:1) k=i exp(cid:0)zπk (cid:80)n (cid:1) . (3) Sampling from this distribution naively requires drawing one patch at time in sequential loop, which is inherently slow and difficult to parallelize across batch. To make training efficient, we use the Gumbel-top-k trick [24] which generates samples from the Plackett-Luce distribution by perturbing logits with Gumbel noise and sorting. policy π is sampled as π = argsort(z + τ g) where gi Gumbel(0, 1) and τ > 0 is temperature parameter that trades off exploration and exploitation. The Gumbel-top-k sampler is fully parallelizable and faster than iterative sampling, allowing permutation sampling in O(n log n) time. The log-probability of sampled permutation is computed in closed form: log (π z) = (cid:88) i=1 (cid:2)zπi log (cid:88) k=i exp(zπk )(cid:3), (4) which we implement efficiently using cumulative logsumexp in reverse. The logit vector is initialized as linear ramp from 0 to 1, then permuted according to the information-theoretic prior described in Section 5. This gives the model sensible starting point that reflects structural cues discovered during unsupervised analysis, while still allowing gradients to adapt the ordering throughout training. During training, single set of Gumbel samples is drawn per batch so that every image shares the same permutation. At test time, we compute deterministic maximum-likelihood permutation ˆπ = argsort(z). To ensure stability, we permute only the positional embeddings of image tokens with π, keeping the [CLS] token fixed. 6.3 Curriculum for Policy Learning We adopt three-stage curriculum that cleanly separates representation learning from policy learning. For the first epochs the classifier is trained with the canonical row-major patch order. This gives the vision transformer stable starting point before any permutation noise is introduced. Beginning 8 Figure 5: REOrder finds improvements over the best patch ordering prior in almost all cases. Across all models, REOrder can find better patch ordering than static prior and improve accuracy across both ImageNet-1K and Functional Map of the World. at epoch the Plackett-Luce policy is activated and trained with REINFORCE for epochs. Sampling uses the Gumbel-top-k trick with temperature schedule τt that climbs to peak value before decaying to zero. While τt > 0 the permutation remains stochastic, encouraging exploration and allowing the policy to discover beneficial orderings. Once the temperature hits zero at epoch + , sampling collapses to the single maximum-likelihood permutation ˆπ = argsort(z). The policy gradients vanish, freezing the permutation, and the remaining epochs let the backbone finish optimizing with its now-determinate input order. The evolution of the policy is visualized in Figure 4, demonstrating how salient patches for the target class of keyboard move to the end of the sequence to maximize classification accuracy. 6.4 Effectiveness of Learned Patch Ordering The Plackett-Luce policy introduced by REOrder is simple drop-in addition to model training, and as visualized in Figure 5, in almost all cases, improves performance over their respective base patch order runs. Mamba observes gains of 2.20% (0.22%) on average across different patch orderings for IN1K, and 9.32% (0.22%) on FMoW. For ImageNet, the Hilbert curve ordering for Mamba is improved by 3.01% (0.23%) while for Functional Map of the World, the diagonal ordering is improved by 13.35% (0.21%). Transformer-XL demonstrates modest gains with REOrder. On IN1K, Transformer-XL accuracy improves by an average of 0.70% (0.22%) but sees significant gains with the Hilbert curve (1.50% (0.21%)) and spiral (1.09% (0.21%)) patch orderings. On FMoW, REOrder improves the best-performing column-major order by an additional 1.10% (0.21%), demonstrating that simply using basic patch ordering alone may not be sufficient to get best performance. Longformer is unable to improve its accuracy on either dataset. However, since Longformer was the model that was initially also the least susceptible to changes in patch ordering due to its near-full approximation of self-attention, it is unsurprising that the use of REOrder does not achieve any noticeable performance gains. In summary, REOrders approach of learning patch ordering proves to be broadly effective strategy for boosting classification performance with long-sequence models, consistently elevating results even beyond the strongest baseline patch orderings for nearly all tested models and datasets."
        },
        {
            "title": "7 Conclusion",
            "content": "This work establishes that contemporary vision models are sensitive to patch order and that row-major patch ordering, an extremely common way of converting 2-D images to 1-D sequences, can be suboptimal in many cases. Architectural modifications in models like Transformer-XL, Mamba, and Longformer, while enabling the processing of long-sequences, break permutation equivariance, leading to significant accuracy variations with different scan orders. We introduce REOrder, method to learn optimal patch orderings by first deriving an information-theoretic prior and then learning patch ranking by optimizing Plackett-Luce policy with REINFORCE. This learned policy, implemented as simple drop-in addition to model training, demonstrably improves classification accuracy for multiple long-sequence models on datasets such as ImageNet-1 and Functional Map of the World by up to by up to 3.01% and 13.35%, respectively, surpassing the best fixed orderings."
        },
        {
            "title": "Acknowledgements",
            "content": "Thank you to Jiaxin Ge and Lisa Dunlap for generously reviewing this paper for clarity and content. Stephanie Fu lent her keen design instinct and helped make the figures look pretty. Anand Siththaranjan and Sanjeev Raja were always game for late night discussions about how to search over combinatorial space and the pitfalls of reinforcement learning. As part of their affiliation with UC Berkeley, the authors were supported in part by the National Science Foundation, the U.S. Department of Defense, and/or the Berkeley Artificial Intelligence Research (BAIR) Industrial Alliance program. The views, opinions, and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of any supporting entity, including the Department of Defense or the U.S. Government. This work utilized the infrastructure at the DoDs High Performance Computing Modernization Program (HPCMP)."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. [2] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context, 2019. [3] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. [4] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. Sequence to Sequence - Video to Text. In Proceedings of the IEEE International Conference on Computer Vision, pages 45344542, 2015. [5] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam M. Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning, 2018. [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, October 2020. [7] Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell, and Karttikeya Mangalam. In Proceedings of the 41st xT: Nested Tokenization for Larger Context in Large Images. International Conference on Machine Learning, pages 1706017071. PMLR, July 2024. [8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. [9] Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan Yuille, and Cihang Xie. Autoregressive Pretraining with Mamba in Vision, June 2024. [10] Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshminarayanan, Alex Beutel, and Xuezhi Wang. Understanding and improving robustness of vision transformers through patch-based negative augmentation, 2023. [11] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. [12] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [13] Tie-Yan Liu et al. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225331, 2009. 10 [14] Alexandros Karatzoglou, Linas Baltrunas, and Yue Shi. Learning to rank for recommender systems. Proceedings of the 7th ACM conference on Recommender systems, 2013. [15] Cheng-En Wu, Jinhong Lin, Yu Hen Hu, and Pedro Morgado. Patch ranking: Efficient clip by learning to rank local patches, 2024. [16] R. Duncan Luce. Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford, England, 1959. [17] R. L. Plackett. The Analysis of Permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2):193202, June 1975. [18] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992. [19] Hengyuan Xu, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. Permutation Equivariance of Transformers and Its Applications, January 2024. [20] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, June 2009. [21] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional Map of the World. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61726180, Salt Lake City, UT, USA, June 2018. IEEE. [22] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [23] Junzi Zhang, Jongho Kim, Brendan ODonoghue, and Stephen Boyd. Sample Efficient Reinforcement Learning with REINFORCE. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):1088710895, May 2021. [24] Wouter Kool, Herke van Hoof, and Max Welling. Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement, May 2019."
        },
        {
            "title": "A Discussion and Limitations",
            "content": "There are many details and nuances to our experiments that are worth discussing further. Particularly, there are areas for improvements that we would like to introduce in future versions of this work. Random baseline. random baseline for learned permutation baseline involves sampling random ordering of patches for every batch during training. We set up this baseline with TransformerXL on the ImageNet-1K dataset. This model achieved maximum top-1 accuracy of 39.07% which is 15.25% worse than the worst patch ordering tested for T-XL on IN1K (spiral). While random baseline should be run for every model and dataset combination, the drastically lower performance of the random baseline gives us confidence in the veracity of our results. Images with = 196 patches have more than 10365 Under-explored and under-tuned policy. permutations. Searching this space exhaustively is computationally infeasible (it is estimated that there are between 1078-1082 atoms in the observable universeour quantity is slightly larger than that). The Plackett-Luce policy exploration runs for vanishingly small amount of time. The curriculum, as tested in our experiments, is only active for 30 epochs. The majority of these epochs is spent warming up and cooling down the Gumbel noise temperature, with the peak noise (and therefore exploration) occurring for only one epoch. We were not able to tune these parameters due to computational constraints. Therefore, much is left on the table with respect to improving results with REOrder. Proof of Proposition 3.1 Theorem 1 (Permutation equivariance of self-attention). Let Rnd and let {0, 1}nn be any permutation matrix. With Attn() defined in Eq. (1), we have Attn(P X) = Attn(X). Proof. Let S(X) = softmax (cid:18) (XWq)(XWk) (cid:19) Rnn. Conventionally, softmax for Attn is applied row-wise over (XWq)(XWk) so that the ensuing multiplication by the value matrix serves as normalized weighted sum over value columns. Since permutation merely re-orders rows and columns, it satisfies the conjugation property softmax(P P) = softmax(M ) (5) for any square matrix and any permutation matrix . Then, Attn(P X) = softmax XWv (cid:17) (cid:16) (P XWq)(P XWk) (cid:16) (XWq)(XWk)P = softmax (cid:17) XWv (PP = I) (5)= S(X) XWv = S(X) XWv (1)= Attn(X). Hence self-attention is permutation equivariant. 12 Table 1: Patch Order Models"
        },
        {
            "title": "Size",
            "content": "# of Parameters Width Depth ViT Base Transformer-XL Base Base Longformer Base Mamba ViT Large Transformer-XL Large Large Longformer Large Mamba 86 570 728 93 764 584 107 683 048 85 036 264 304 330 216 329 601 512 379 702 760 297 041 352 768 768 768 1024 1024 1024 1024 12 12 12 12 24 24"
        },
        {
            "title": "C Model Details",
            "content": "We attempt to parameter match each model we experiment with in an effort to remove one axis of variability from our results. We experiment with the Base variant of each model in our experiments. Despite the intention for the Base variants to be roughly equivalent to each other, Longformer-Base and Mamba-Base vary by 22M parameters. To contextualize how width and depth affect the number of parameters, we provide details for Base and Large variants in Table 1."
        },
        {
            "title": "D Additional Model Training Details",
            "content": "Each training run required 100 epochs for completion. ImageNet experiments were run on 8 80GB A100 GPUs, while Functional Map of the World experiments were run on 4 40GB A100 GPUs. Transformer-XL, ViT and Mamba, and the Plackett-Luce policy were compiled with TorchDynamo using the Inductor backend. The Longformer encoder provided by HuggingFace was unable to be compiled. For ImageNet runs, Vision Transformer runs took 8.5 hours to run, Transformer-XL runs took 10 hours, Longformer runs took 12 hours, and Mamba runs took 19 hours. For Functional Map of the World, Vision Transformer runs took 17.5 hours, Transformer-XL runs took 21 hours, Longformer runs took 26 hours, and Mamba runs took 31 hours. All runs were executed on dedicated datacenters accessed remotely. All runs nearly maximized the available VRAM on their respective GPUs. The breakdown of models and batch sizes is provided below: Table 2: Batch sizes for every model-dataset pairing. Size Dataset Batch Size Model ViT ViT Base Base Transformer-XL Base Transformer-XL Base Longformer Longformer Mamba Mamba Base Base Base Base IN1K FMOW IN1K FMOW IN1K FMOW IN1K FMOW 896 448 640 640 320 640 320 In sum, the total set of experiments run required total of 9, 336 A100 GPU hours. Models were trained with minimal set of augmentations, namely: resize to 256 256 pixels, center crop to 224 224 pixels, and random horizontal flip with = 0.50. D.1 Results Without Flips We additionally run experiments where the only data processing steps applied are resize to 256256 pixels and center crop to 224 224 pixels with no flips at all. The results presented in Table 13 Table 3: REOrder consistently improves performance across all patch orders with no augmentations. Top-1 validation accuracy on IN1K using different patch orders with and without our REOrder using minimal data processing (resize and center crop only)."
        },
        {
            "title": "Dataset",
            "content": "Patch Order RL Top-1 Accuracy Difference Transformer-XL IN1K Transformer-XL IN1K"
        },
        {
            "title": "Row\nRow",
            "content": "Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K Transformer-XL IN1K"
        },
        {
            "title": "Snake\nSnake",
            "content": "57.95 59.53 57.33 59.01 52.79 55.20 50.29 53.22 49.93 52.40 52.20 54. +1.58% +1.67% +2.41% +2.94% +2.47% +1.82% show the effects observed in the main experiments are maintained albeit with different accuracy magnitudes. With no augmentations, the effect of REOrder is even greater. All tested patch orders exhibit performance gains when optimized by REOrder. The performance boost ranges from 1.58% with row-major to 2.94% with the Hilbert curve ordering. Diagonal and spiral orderings get moderate gains of 2.41% and 2.47% respectively, while column-major and snake orderings show more modest improvements of 1.67% and 1.82% respectively. These results confirm that REOrder consistently enhance task accuracy across the patch orderings even without extensive data augmentation."
        },
        {
            "title": "E Dataset Licenses and References",
            "content": "In this work, we use ImageNet-1K as provided by the 2012 ImageNet Large-Scale Visual Recognition Challenge [20] and the RGB version of Functional Map of the World [21]. ImageNet-1K is obtained from the official download portal and Functional Map of the World is obtained from their official AWS S3 bucket. ImageNet-1K is licensed non-commercial research and educational purposes only as described on their homepage. Functional Map of the World is licensed under custom version of the Creative Commons license available on their GitHub."
        },
        {
            "title": "F Rasterization Orders",
            "content": "Let an image be composed of = patches, arranged in grid of height (number of rows) and width (number of columns). Each patch is identified by its zero-indexed 2D coordinates (r, c), where 0 < and 0 < . rasterization order (or scan order) π is bijection that maps 1D sequence index {0, 1, . . . , 1} to the 2D coordinates of the k-th patch in the sequence. We denote this mapping as π(k) = (rk, ck). In all cases besides rowand column-major, direct formula for (rk, ck) from is not provided; the order is algorithmically defined by their respective procedures. F.1 Row-Major Order Row-major order, also known as raster scan, is the most common default. Patches are scanned row by row from top to bottom. Within each row, patches are scanned from left to right. The coordinates (rk, ck) for the k-th patch are given by: rk = (cid:23) (cid:22) 14 F.2 Column-Major Order ck = (mod ) In column-major order, patches are scanned column by column from left to right. Within each column, patches are scanned from top to bottom. The coordinates (rk, ck) for the k-th patch are given by: ck = (cid:23) (cid:22) rk = (mod H) F.3 Hilbert Curve Order The Hilbert curve is continuous fractal space-filling curve. Ordering patches according to Hilbert curve aims to preserve locality, meaning that patches close in the 1D sequence are often (but not always perfectly) close in the 2D grid. The coordinates (rk, ck) for the k-th patch are given by: π(k) = (rk, ck) = HH,W (k) where HH,W (k) denotes the coordinates of the k-th point generated by Hilbert curve algorithm adapted for an grid. F.4 Spiral Order In spiral scan, patches are ordered in an outward spiral path, typically starting from corner patch, e.g., (0, 0). The path moves along the perimeter of increasingly larger (or remaining) rectangular sections of the grid. The coordinates (rk, ck) for the k-th patch are: π(k) = (rk, ck) These are the k-th unique coordinates visited by path that starts at (0, 0) and spirals outwards. The path typically moves along segments of decreasing lengths (or until boundary or previously visited cell is encountered) in sequence of cardinal directions (e.g., right, down, left, up, then right again with shorter segment, etc.), effectively tracing successive perimeters of nested rectangles. F.5 Diagonal Order (Anti-diagonal Scan) In diagonal scan, patches are ordered along anti-diagonals, which are lines where the sum of the row and column indices (r + c) is constant. These anti-diagonals are typically scanned in increasing order of this sum, starting from + = 0. Within each anti-diagonal, patches are typically ordered by increasing row index (or, alternatively, by increasing column index c). The coordinates (rk, ck) for the k-th patch are: π(k) = (rk, ck) such that (rk, ck) is the k-th patch when all patches (r, c) are ordered lexicographically according to the tuple (s, r), where = + is the anti-diagonal index and = is the row index. Patches with smaller anti-diagonal sums come first. For patches on the same anti-diagonal (i.e., with the same s), those with smaller row index come first. F.6 Snake Order This scan traverses patches along anti-diagonals (where the sum of row and column indices, = + c, is constant). The anti-diagonals are processed in increasing order of s. The direction of traversal along each anti-diagonal alternates. For example, for even s, patches are visited by increasing column index c, and for odd s, by decreasing column index c. Let sk = rk + ck be the anti-diagonal index for the k-th patch π(k) = (rk, ck). The sequence of patches π(k) is generated by iterating from 0 to + 2. For each s: 1. Define the set of coordinates on the anti-diagonal: Ss = {(r, c) + = s, 0 < H, 0 < }. 15 2. Order the coordinates in Ss. For instance, by increasing column index c: Ps = [(r0, c0), (r1, c1), . . . , (rm, cm)] where c0 < c1 < < cm. 3. If is odd (or based on an alternating flag), reverse the order of Ps. 4. Append the (potentially reversed) Ps to the overall sequence."
        },
        {
            "title": "G Attention Patterns for Tested Models",
            "content": "Figure 6: Token-level attention coverage across different model architectures. Each grid cell represents patch in 224224 input image split into 49 non-overlapping 3232 patches, plus [CLS] token. Numbers indicate how many tokens attend to each patch. ViT and Mamba exhibit full attention to all patches. In contrast, Transformer-XLs causal attention and Longformers local attention restrict the number of tokens that can attend to each patch, leading to strong asymmetry and localized attention, respectively. To produce Figure 6, we visualized how many tokens attend to each patch in 224224 image divided into 49 patches (plus [CLS] token), using the following methods for each model. ViT and Transformer-XL: We extracted the raw attention weights across all layers and heads during forward pass. After computing the element-wise maximum across layers and heads, we obtained binary matrix indicating whether token attends to token j. Summing over rows yields how many tokens attend to each patch. Longformer: Due to its local attention structure, we reconstructed dense attention matrix by extracting local and (if applicable) global attention indices. We then counted how many tokens had access to each patch through these sparse connections. Mamba: Since Mamba does not use attention, we used gradient-based saliency method. We computed the gradient of the L2 norm of each output token with respect to the input embeddings. This yielded sensitivity matrix indicating the influence of each input token on each output. Thresholding non-zero entries allows us to analogously count how many tokens attend to each patch. These results reveal how the structural design of each model affects its ability to aggregate spatial information. ViT and Mamba attend to all patches uniformly, while Transformer-XLs causal structure and Longformers locality lead to uneven and limited attention coverage. These patterns explain their respective sensitivities to input token ordering."
        },
        {
            "title": "H Policy Evolution During Training",
            "content": "The REOrder policy learned for each model converged on characteristic arrangement of patches based on the underlying structure of the dataset. Across Longformer, Mamba and Transformer XL, the learned REOrder policies consistently drive keyboard patches toward the terminal sequence positions. From an initial ordering, the permutation logits gradually bias the ordering so that the salient key and its neighboring patches accumulate at the end, while irrelevant tokens remain earlier in the sequence. 16 Figure 7: Mamba observes patches most related to the class label move to the end of the sequence. Mamba is trained with column- (top) and row-major (bottom) patch orderings and optimized with REOrder. The image is of the class keyboard. We track two patches over the course of the policy curriculum: keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As training progresses, the keyboard-related patches shift into the final indices of the sequence. Figure 8: Transformer-XL observes patches most related to the class label move to the end of the sequence. Transformer-XL is initialized with column- (top) and row-major (bottom) patch orderings and optimized with REOrder. The image is of the class keyboard. We track two patches over the course of the policy curriculum: keyboard key (light red arrow) and an irrelevant orange beak (dark red arrow). As training progresses, the keyboard-related patches shift into the final indices of the sequence."
        }
    ],
    "affiliations": [
        "University of California, Berkeley",
        "University of Pittsburgh"
    ]
}