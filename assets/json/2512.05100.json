{
    "paper_title": "Structured Document Translation via Format Reinforcement Learning",
    "authors": [
        "Haiyue Song",
        "Johannes Eschbach-Dymanus",
        "Hour Kaing",
        "Sumire Honda",
        "Hideki Tanaka",
        "Bianka Buschbeck",
        "Masao Utiyama"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 0 0 1 5 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Structured Document Translation via\nFormat Reinforcement Learning",
            "content": "Haiyue Song1, Johannes Eschbach-Dymanus2, Hour Kaing1, Sumire Honda2, Hideki Tanaka1, Bianka Buschbeck2, Masao Utiyama1 1National Institute of Information and Communications Technology, Japan 2SAP, Germany {haiyue.song,hour_kaing,hideki.tanaka,mutiyama}@nict.go.jp {johannes.eschbach-dymanus,sumire.honda,bianka.buschbeck}@sap.com"
        },
        {
            "title": "Abstract",
            "content": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose Format Reinforcement Learning (FORMATRL), which employs Group Relative Policy Optimization on top of supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: structured document translation example (EnglishJapanese), with markup highlighted in color. The lower part shows the translation of XML tree structure with node ϕ(), tag(), and text() mappings."
        },
        {
            "title": "Until",
            "content": "Translating structured documents such as software manuals is essential for product localization. As shown in Figure 1, they carry markup that defines layout and interactive elements, making structural fidelity as important as content translation quality. the advent of large language models (LLMs), the most prevalent approach for translation with markup was the detag-and-project pipeline (Joanis et al., 2013; Müller, 2017; Zenkel et al., 2021). This pipeline usually leverages machine translation (MT) system to translate plain text (with tags removed) and separate word aligner to reinsert the tags into the translated text. Although straightforward, it is prone to error propagation from individual MT and alignment modules. LLMs have emerged as promising end-to-end solution for markup translation (Dabre et al., 2023, 2024). Few-shot prompting is convenient way to enable LLMs to learn markup transfer patterns with only few examples (Brown et al., 2020; Lewis et al., 2020; Dabre et al., 2023), and fine-tuning provides more robust domain adaptation capabilities thus better performance (Dabre et al., 2024). However, the training objective of supervised finetuning is to optimize token-level likelihood, leaving markup accuracy largely unaddressed. Therefore, it is difficult for them to handle complex structured documents such as the one shown in Figure 1. In this study, we address these limitations by proposing Format Reinforcement Learning (FORMATRL), which moves from the token-level likelihood optimization to directly optimizing structure-aware objectives. It first fine-tunes an LLM for basic document translation capability, then applies Group Relative Policy Optimization (GRPO) with two novel structure-aware rewards: TreeSim for measuring XML tree structural similarity via edit distance, and Node-chrF for node-level translation quality assessment. The main contributions of this paper are summarized below: We propose Format Reinforcement Learning (FORMATRL) for structured document translation. It utilizes Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to optimize structural fidelity through novel structure-aware rewards TreeSim and NodechrF. We also investigate range of additional rewards to reinforce structural fidelity and translation quality. We use new metric, Structure-Aware Area Under Curve (StrucAUC), which distinguishes between minor errors and major failures, then robustly combines both translation and structural quality into single score. Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions, with FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 ContentBLEU, and 0.93 StrucAUC scores compared to strong supervised fine-tuning baseline."
        },
        {
            "title": "2 Related Work",
            "content": "This study focuses on 2.1 structured text translation and 2.2 reinforcement learning."
        },
        {
            "title": "2.1 Structured Text Translation",
            "content": "The traditional detag-and-project approaches rely on separate modules for translation and markup handling (Du et al., 2010; Joanis et al., 2013; Müller, 2017; Hanneman and Dinu, 2020; Zenkel et al., 2021; Ryu et al., 2022; Steffen and van Genabith, 2021; Zenkel et al., 2021). However, these methods suffer from error propagation across modules, and the MT system translates at the sentence level without using document-level context. Recent end-to-end approaches for structured text translation have become possible with LLMs such as BLOOM (Le Scao et al., 2022), ChatGPT (Brown et al., 2020; OpenAI, 2023), and Llama 3 (Dubey et al., 2024), owing to their strong in-context learning and generalization capabilities. Previous studies using few-shot prompting (Dabre et al., 2023) or fine-tuning on small dataset (Dabre et al., 2024) work well for sentence translation with markup. However, they struggle to handle complex structures such as those found in XML documents."
        },
        {
            "title": "2.2 Reinforcement Learning",
            "content": "From the perspective of RL, generation is sequence of actions to maximize the reward. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is the RL algorithm used in ChatGPT (OpenAI, 2023), whereas GRPO (Shao et al., 2024) used in DeepSeek-R1 (DeepSeek-AI, 2025) further simplifies PPO by removing the separate value network. RL has been applied to tasks such as code generation (Dou et al., 2024), JSON generation (Lu et al., 2025) and format instruction following (Yao et al., 2024). To our knowledge, we are the first to apply RL algorithms to the structured document translation task, whose challenge lies in designing rewards to guide generation of exactly the same structure as that in the source document while maintaining the high translation quality."
        },
        {
            "title": "3 Method",
            "content": "Our pipeline is shown in Figure 2. We first define the task in 3.1, then describe the supervised finetuning (SFT) phase in 3.2, and finally present the core reinforcement learning phase in 3.3."
        },
        {
            "title": "3.1 Task Definition",
            "content": "This work addresses the task of translating structured document Ds in the source language into its counterpart Dt in the target language. structured document can be viewed as an XML tree = (VD, ED), where VD denotes the set of nodes and ED the set of parentchild edges. Each node is associated with tag symbol tag(v) (e.g., <p>) and may contain textual segments text(v). The translation model πθ is conditional probability distribution defined as follows: πθ : Ds Dt [0, 1] R, πθ(Dt Ds) where πθ(Dt Ds) denotes the probability of generating the target document Dt given the source document Ds, and Ds and Dt are the spaces of all possible structured documents in the source and target languages. The predicted translation ˆDt is typically obtained by maximizing this probability: ˆDt = arg max DtDt πθ(Dt Ds) We assume that the predicted document ˆDt satisfies the following two conditions we target: 3.2 3.3 Figure 2: Our FORMATRL pipeline consists of two stages. First, we fine-tune pre-trained LLM (e.g., Llama-3.18B-Instruct) using real and synthetic structured document pairs. Second, we reinforce the format handling ability through GRPO with our proposed format reward functions. 1. Structural Identity: ˆDt is isomorphic to the source tree Ds. Formally, there exists bijection ϕ : VDs ˆDt such that: For any edge (u, v) EDs, we have . (ϕ(u), ϕ(v)) ˆDt For any internal node VDs, the corresponding target node shares the same tag symbol: tag(ϕ(v)) = tag(v). 2. Translation Correspondence: For each source node VDs and its corresponding target node ϕ(v) their textual contents text(v) and text(ϕ(v)) are mutual translations. To examine the extent to which both conditions are satisfied, in practice, we measure the translation quality between the predicted tree ˆDt and reference document using well-established metrics such as BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020, 2022)."
        },
        {
            "title": "3.2 Phase I: Supervised Fine-Tuning",
            "content": "We fine-tune pre-trained LLM on parallel structured documents. To address the data scarcity problem, we synthesize training data by injecting XML markup into parallel plain-text documents. t)}N Data Synthesis. Given parallel corpus of plain documents {(di s, di i=1, we use GPT-4o to generate structured documents {(Di t)}M i=1 in which: and Di s, Di have the same structure; The original parallel texts are preserved. Both Di We ensure structural identity through validation: for each generated pair (Ds, Dt), we verify their XML trees are isomorphic. Invalid pairs are regenerated until success or hitting the retry limit."
        },
        {
            "title": "3.3 Phase II: Format Reinforcement",
            "content": "Initialized from the SFT checkpoint, we use our designed rewards to optimize the translation model (policy model as termed in GRPO) to generate structurally correct and high-quality translations."
        },
        {
            "title": "3.3.1 Reward Functions",
            "content": "The policy model learns from good samples generated by itself during training, where the reward function defines what is good. During GRPO training, reward function r( ˆDt,i, ) compares each sampled output ˆDt,i πθ(Ds) with the reference document , and indicates how good each output is. To reinforce structure-aware similarity, we propose two rewards: TreeSim and Node-chrF. TreeSim measures structural similarity between the predicted and reference XML trees. It first parses both documents as XML fragments wrapped in dummy root. The similarity is computed using the Zhang-Shasha tree edit distance (Zhang and Shasha, 1989), which counts the minimum number of node insertions, deletions, or relabelings needed to transform one tree into another. To obtain normalized similarity score, we use: TreeSim( ˆDt,i, ) = 1 EditDist( ˆDt,i, ) max( ˆDt,i, ) where EditDist is the tree edit distance and denotes the number of nodes in tree excluding the dummy root. This normalization ensures that the score remains in [0, 1], with 1 indicating identical structures and 0 maximum dissimilarity. Specifically, we assign penalty score of 0.1 for invalid XML that cannot be parsed. Node-chrF measures translation quality at the level of individual XML nodes. The algorithm performs parallel depth-first traversal of the predicted and reference XML trees, pairing nodes at corresponding positions. When the sizes differ, the algorithm extends the shorter traversal list to match the longer one by adding empty placeholders. For each node pair (vpred, vref), the metric computes: score of 0 if the nodes have mismatched tags (e.g., <p> vs <h1>) or are unpaired (e.g., one subtree has more nodes than the other) The chrF score (Popovic, 2015) of their textual content (excluding child nodes) if tags match Skip node pairs that contain only whitespace The final score is the average of all node pairs: Node-chrF = (cid:88) 1 (vpred,vref)P 1match(vpred, vref) chrF(vpred, vref) , where is the set of all node pairs in the traversal and 1 is the indicator function for tag matching. For aligned trees, this metric focuses on translation quality. If the translation contains structural mistakes, however, nodes become misaligned, and the reward degrades substantially. In practice, we scale each reward to [0, 10] for numerical stability. We also investigate the use of other metrics (6.5) as rewards and explore combining two rewards (by summing the scores)."
        },
        {
            "title": "3.3.2 Optimization",
            "content": "After calculating reward scores for group of samples, we encourage the model to generate similar high-scoring outputs. In GRPO, we calculate the relative performance comparisons within the group, called advantages, which is then used to update the document translation policy model πθ. Formally, the optimization process works as follows: for each source document Ds, we generate candidate translations { ˆDt,i}K i=1 from the current policy πθ. Instead of requiring absolute quality assessments, GRPO computes advantages by comparing each generations reward against the group mean, effectively learning which translations are better than average within the same context. Since we perform single gradient update per exploration stage when computing gradients, we can remove the min and clip operation. This yields the following objective: LGRPO = (cid:34) DsD,{ ˆDt,i}K i=1πθ(Ds)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) i=1 ˆAi log πθ( ˆDt,iDs) + β DKL(πθπSFT) (cid:35) (1a) (1b) The first term (1a) encourages the model to increase the likelihood of generations with positive advantages and to decrease the likelihood of those with negative advantages, with ˆAi computed as: ˆAi = = σr = r( ˆDt,i, t ) σr"
        },
        {
            "title": "1\nK",
            "content": "(cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 r( ˆDt,j, )"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) (r( ˆDt,j, ) r)2 j=1 The second term (1b) is Kullback-Leibler divergence regularizer that prevents the optimized policy πθ from deviating too far from the supervised finetuned model πSFT with β controlling its strength, thereby avoiding catastrophic forgetting."
        },
        {
            "title": "4 Evaluation Metrics: StrucAUC",
            "content": "Previous studies on structured data translation apply the XML-BLEU metric (Hashimoto et al., 2019) as combined score for both translation quality and structural fidelity. However, it results in zero score on the document-level even with minor structural mismatch. To provide more finegrained evaluation, we propose StrucAUC that distinguishes between minor errors and major structural failures. In detail, it provides translation quality evaluation by interpolating between two scores, Node-chrF and Optimal Node-chrF, to measure quality with different levels of error tolerance. Optimal Node-chrF provides way to measure Node-chrF for two documents with slightly different structures by node alignment. It represents each node by its entire subtree (including tags and descendants) and computes cost matrix C, where Cij is the chrF distance between the i-th nodes subtree in ˆDt and the j-th nodes subtree in the . Using the Hungarian algorithm (Kuhn, 1955), it solves the linear sum assignment problem to find the optimal one-to-one mapping that minimizes the total distance: = arg min (cid:88) (vpred,vref)M Cvpred,vref The final score evaluates chrF on node-level textual content (excluding children) under this optimal alignment: Nodes that cannot be matched are scored 0, ensuring all nodes are accounted for. StrucAUC then integrates structural tolerance through tree edit distance (Zhang and Shasha, 1989) into Optimal Node-chrF. For each document, we calculate the minimum number of edits required to transform the predicted tree into its optimally aligned version (as determined by M), with tag mismatches counting as 0.5 edits. The metric then computes curve at the corpus level: at each edit threshold {0, 0.5, 1, ..., K}, documents requiring at most edits contribute their Optimal NodechrF score, while others contribute their regular Node-chrF score. The area under this curve from 0 to yields StrucAUC@K, providing smooth degradation from perfect structural alignment to increasing structural deviations. This makes StrucAUC robust for document-level evaluation: minor structural errors (e.g., misplaced formatting tag) result in proportional score reductions rather than complete failure, while still rewarding structural fidelity. In our experiments, we report StrucAUC@5, allowing up to 5 structural edits before considering document structurally misaligned. We provide the pseudo code in Appendix H."
        },
        {
            "title": "5 Experimental Settings",
            "content": "This section describes our dataset, evaluation metrics, and implementation details of our method."
        },
        {
            "title": "5.1 Dataset",
            "content": "We use the SAP software documentation dataset (Buschbeck and Exel, 2020) that contains parallel structured documents for language pairs including JapaneseEnglish and ChineseEnglish translated by professional translators. Each language pair consists of 190 document pairs for testing, and an additional 195 document pairs, of which we use 100 for training and 95 for development. Each source target document pair contains the same number of lines with one-to-one, linear alignment, reflecting the property of this task that the page layout in different languages should be identical. and an average of 14.62 text segments per document. Overall, it covers 58 unique XML tags. Inline Markup Setting The dataset also provides simplified version with only sentenceinternal markup, as shown in Figure 3. We the inline markup setup. name it Different from the structured setup which preserves non-translatable nodes (e.g., <source>In-App Help</source> and metadata), the inline setup keeps only translatable spans with inline tags. Consequently the reference texts also differ between the two setups. We construct the data for both setups with the official SAP XSLT preprocessing scripts. Figure 3: Inline markup version of the example in Fig.1."
        },
        {
            "title": "5.2 Evaluation",
            "content": "We apply six evaluation metrics, including four from previous studies: Content-BLEU, XMLValidity, XML-Match, XML-BLEU, and two proposed metrics: Content-COMET and StrucAUC. We classify them into three categories: 1) Translation: these mainly measure translation quality, 2) Structure: the ones measure structure fidelity, and 3) Combined: the ones measure both. Translation Content-BLEU is the BLEU for document with all XML markup removed. We employ the SacreBLEU tool (Post, 2018) with language-specific tokenizers.1 Content-COMET is based on the neural MT metric COMET-22 (Rei et al., 2022). The metric is applied to the document texts without XML markup, as COMET-22 was not trained on structured documents and therefore cannot be directly applied to such data. Structure XML-Validity returns binary score of one or zero whether the output Dt passed XML parsing. XML-Match is also binary indicating whether the XML trees of output Dt and reference are exactly the same. Statistics Documents in this dataset exhibit substantial structural variety. After converting documents into XML trees, each tree has an average depth of 7.11 1.51 and contains 27.36 25.28 nodes, with median of 18 nodes per document, Combined The XML-BLEU metric (Hashimoto et al., 2019) is combined score for both translation quality and structural fidelity. First, both 1e.g., signature for Japanese: \"nrefs:1case:lceff:notok:jamecab-0.996-IPAsmooth:expversion:2.5.1\" translated and reference documents are split into text segments at XML tag boundaries. If translations XML-Match is true, the segments are paired for BLEU computation. Otherwise, the references segments are paired with empty strings, thereby penalizing structural errors. The metric is then computed on the corpus level across all segments of all documents. StrucAUC is also combined metric as described in 4. Additionally, we report empirical p-values from statistical significance testing using bootstrap resampling with 1, 000 trials. 5."
        },
        {
            "title": "Implementations",
            "content": "We report implementation details and hyperparameters selected based on our preliminary experiments."
        },
        {
            "title": "5.3.2 Supervised Fine-Tuning\nThis section describes the LLM fine-tuning method\nas in Dabre et al. (2024) on our document-level\ndata. We use Llama-3.1-8B-Instruct (Dubey et al.,\n2024) as the base model in our experiments.",
            "content": "Synthetic Data We use GPT-4o2 to synthesize markup using the Asian Language Treebank (ALT) corpus (Thu et al., 2016; Riza et al., 2016), generating 900 structured document pairs per language. ALT contains high-quality general domain parallel document units and aligns with our target language pairs. The prompt includes random example from the development set of SAP dataset and sample of five tags from the target tag vocabulary. Without domain-specific guidance, LLM defaults to generic tags (e.g., <person>), causing traintest mismatches. See full prompt in Appendix A. Hyperparameters The SFT model πSFT is then trained on both 100 real and 0 to 400 synthetic structured document pairs using standard crossentropy loss. We fine-tune for 20 epochs with batch θ 2gpt-4o-2024-08-06 size of 8, learning rate of 3 107 and cosine learning rate scheduling with warmup ratio of 0.1. We use the AdamW optimizer (Loshchilov and Hutter, 2017). Early stopping is triggered after 10 evaluations without improvement, with evaluation performed every 10 steps."
        },
        {
            "title": "5.3.3 Format Reinforcement",
            "content": "We now describe the hyperparameter configuration for the reinforcement learning phase (3.3), chosen based on our preliminary experiments. Training Configuration. We report results using TreeSim reward in 6.1, and results for Node-chrF and other rewards in 6.5. We use small learning rate of 106 and train for 5 epochs with early stopping based on validation loss. Early stopping is triggered after 3 evaluation steps without improvement, with evaluation and checkpointing performed every 3 training steps. We set the maximum sequence length to 2, 000 tokens for both prompts and completions. The KL penalty coefficient β is set to the default value of 0.01. We select the checkpoint for testing based on the development set performance. Batch and Generation Settings. We use 8 generations per document (K = 8) with per-device batch size of 8 and gradient accumulation steps of 1, resulting in an effective batch size of 64 across 8 H200 GPUs. For generation, we use sampling with default temperature of 1.0."
        },
        {
            "title": "5.3.4 Computational Efficiency",
            "content": "During training, we leverage DeepSpeed ZeRO3 optimization and mixed precision training with bfloat16 for memory and computational efficiency. Each SFT model takes about 2.1 hours and GRPO model takes about 1.3 hours of training. We employ vLLM (Kwon et al., 2023) for efficient inference where it takes 2 minutes on test set."
        },
        {
            "title": "6.1 Main Results",
            "content": "Table 1 presents our main results on the structured document translation task across four language pairs. FORMATRL using TreeSim reward consistently outperforms both the prompting and SFT baselines across nearly all evaluation metrics. Results of other rewards are shown in Appendix C. Structural Fidelity Improvements. FORMATRL with TreeSim shows significant gains in structural preservation. XML-Match scores improve by an SrcTgt Method EnZh ZhEn EnJa JaEn Prompt SFT FORMATRL Prompt SFT FORMATRL Prompt SFT FORMATRL Prompt SFT FORMATRL Translation Structure Combined Content-BLEU Content-COMET XML-Validity XML-Match XML-BLEU StrucAUC 49.88 49.66 49.88 48.82 56.41 56. 36.60 39.11 39.30 44.14 52.19 52.79 86.16 86.47 86.48 85.25 85.34 85.25 87.17 88.22 88.20 85.93 85.96 86. 91.05 94.21 95.26 93.16 94.74 95.26 87.89 95.26 95.79 90.53 95.26 94.74 76.84 85.26 87.37 82.11 83.68 86. 67.37 84.21 88.42 80.00 82.11 87.37 27.50 36.38 38.07 26.34 27.58 29.14 14.49 27.47 30.32 22.38 24.15 26. 57.75 63.57 64.12 71.39 71.66 72.84 48.60 60.40 60.48 65.25 67.92 69.82 Avg. 64.86 69.26 70. 67.84 69.90 70.94 57.02 65.78 67.09 64.70 67.93 69.57 Table 1: Results of FORMATRL and two baselines on structured documents. Bold indicates the best performance. Background colors indicate statistical significance < 0.05 compared to SFT. SrcTgt Method EnZh ZhEn EnJa JaEn Prompt SFT FORMATRL Prompt SFT FORMATRL Prompt SFT FORMATRL Prompt SFT FORMATRL Translation Structure Combined Content-BLEU Content-COMET XML-Validity XML-Match XML-BLEU StrucAUC 54.79 57.95 57.70 39.92 32.24 34.76 40.13 44.42 45.60 35.61 34.74 37.02 85.87 86.19 86.22 83.31 83.06 82. 87.90 88.40 88.60 84.86 84.66 84.96 96.32 98.42 98.42 95.26 95.26 95.79 96.32 97.37 98.42 98.95 97.89 98. 84.74 89.47 90.00 83.68 81.05 84.74 79.47 84.21 86.84 81.58 82.63 86.84 43.33 47.51 47.63 33.83 33.01 34. 26.78 32.27 35.44 27.58 26.72 30.13 56.62 63.14 64.29 67.06 65.77 66.28 45.07 54.93 55.04 64.65 63.60 65. Avg. 70.28 73.78 74.04 67.18 65.07 66.52 62.61 66.93 68.32 65.54 65.04 67.20 Table 2: Results of FORMATRL and two baselines on inline markup dataset. Bold indicates the best performance. Background colors indicate statistical significance < 0.05 compared to SFT. average of 3.69 over SFT, with the largest improvement of 5.26 points observed for JaEn. This indicates that FORMATRL effectively learns to maintain document structure beyond what SFT achieves. Translation Quality Gains. Importantly, FORMATRL maintains or slightly improves translation quality while enhancing structural fidelity. ContentBLEU scores increase by an average of 0.22 points over SFT. Content-COMET scores remain stable, suggesting that our structural improvements do not come at the cost of translation quality. Combined Performance. We show the combined improvement through XML-BLEU, which is widely used in previous work on structured data translation (Hashimoto et al., 2019; Dabre et al., 2024). It improves by 2.16 points on average, and our proposed StrucAUC metric shows gains of 0.93 points, confirming that improvements are robust across different structural error tolerances. Human Evaluation. We performed small-scale human evaluation on 60 rendered EnJa pages comparing FORMATRL and prompting methods. For each page, an annotator compared the outputs of two methods against the reference, where the order of two outputs are randomly shuffled each time. Results show FORMATRL won 29, prompting won 13, and 18 were ties. Qualitatively, outputs with (i) correct structure and (ii) correct embedded UI were preferred, suggesting that structural fidelity may be important in user experience."
        },
        {
            "title": "6.2 Results on Documents with Inline Markup",
            "content": "Table 2 presents results on the inline markup dataset, where structural complexity is reduced to inline markup. We found that although FORMATRL with TreeSim still shows improvements in all metrics, the performance gap between the baseline method and FORMATRL narrows considerably compared to structured documents. For exFigure 4: Comparison with GPT-4.1-nano (2025-04-14), GPT-4o-mini (2024-07-18), and GPT-4o (2024-08-06). ample, the XML-Match gap between Prompt and FORMATRL narrows from 10.92 to 4.74. This suggests that LLMs handle simpler inline structures effectively through in-context learning, but struggle with more structured documents. We show results of different rewards in Appendix D."
        },
        {
            "title": "6.3 Comparison with GPT-4 Models",
            "content": "We compare our approach to three GPT models which serve as reference. We show results of EnJa in Figure 4 and all directions in Appendix F. We found FORMATRL shows comparable performance on most metrics with GPT-4o and outperforms GPT-4.1-nano and GPT-4o-mini. Although with similar scores in automatic evaluation, after analyzing 60 outputs of GPT-4o and our model, we found FORMATRL outputs match the style (e.g. word choice is more formal) in source documents better than prompting with GPT-4o."
        },
        {
            "title": "6.4 Comparison with Parse-and-Assemble",
            "content": "We implemented two parse-and-assemble baselines, where we first extract translatable text blocks, then apply an LLM-based sentence-level translator, and finally assemble the texts to form the output document. SFT-Sent trains Llama 3.1 8B on parallel sentences whereas SFT-Sent w/ Content extents this by providing the whole document as context. Figure 5 shows that for EnJa, translation quality is comparable but FORMATRL achieves higher XML-Match. Although parse-and-assemble ensures correct document structure, it struggles with in-line tags whose positions vary across target language syntax. Furthermore, providing full documents for every sentence makes training 4.2 slower and inference 5.7 slower than standard SFT, showing that the end-to-end paradigm offers more natural and efficient solution. Figure 5: Comparison with parse-and-assemble baselines, in which the LLM acts as sentence-level MT model with or without document context. Figure 6: Improvement of FORMATRL over SFT using various single rewards, and combinations of two rewards. Points represent mean improvement and ellipses visualize the local covariance directional structure between two metrics improvements."
        },
        {
            "title": "6.5 Analysis: Reward Choice",
            "content": "Figure 6 shows the effect of different reward functions during GRPO training, including: 1) proposed TreeSim and Node-chrF, 2) metrics used in evaluation as rewards,3 and 3) combination of two rewards. Estimates are constructed from 8 runs of RL results. Refer to Appendix for results featuring COMET instead of BLEU. First, we found all rewards except XML-Validity to improve translation quality measured by ContentBLEU. Even pure structure-aware rewards, such as TreeSim and XML-Match, can improve translation. The combined reward Node-chrf improves both in good balance. However, not aligning to the reference (XML-Validity) is bad, hurting both translation and structure quality. Second, the best way to optimize specific metric is using it as reward. Reinforcement learning with Content-BLEU as reward achieves the highest gain in Content-BLEU, and similarly, the XML-Match reward achieves the best XML-Match performance. Finally, we observe 3Content-BLEU and XML-BLEU here are document-level. SrcTgt"
        },
        {
            "title": "Method",
            "content": "TreeSim Node-chrF"
        },
        {
            "title": "Prompt\nSFT",
            "content": "EnZh"
        },
        {
            "title": "FORMATRL",
            "content": "w/ TreeSim w/ Node-chrF"
        },
        {
            "title": "Prompt\nSFT",
            "content": "ZhEn"
        },
        {
            "title": "FORMATRL",
            "content": "w/ TreeSim w/ Node-chrF"
        },
        {
            "title": "Prompt\nSFT",
            "content": "EnJa"
        },
        {
            "title": "FORMATRL",
            "content": "w/ TreeSim w/ Node-chrF"
        },
        {
            "title": "Prompt\nSFT",
            "content": "JaEn"
        },
        {
            "title": "FORMATRL",
            "content": "w/ TreeSim w/ Node-chrF 95.97 97.86 98.19 97.70 97.24 97.54 98.12 97.97 94.40 97. 97.98 97.23 96.69 97.82 98.28 98.02 56.33 62.67 63.24 64.41 69.97 70. 71.77 73.00 47.64 59.77 59.77 59.26 63.93 66.47 68.51 68.93 Table 3: Performance comparison when optimizing TreeSim and Node-chrF rewards. Bold indicates the best performance. reward combination yields averaging effects, e.g., combining TreeSim with XML-BLEU shows better Content-BLEU improvement than TreeSim alone."
        },
        {
            "title": "6.6 Analysis: Reward-Metric Alignment",
            "content": "Table 3 compares the direct optimization effects of our two proposed rewards. We observe clear reward-metric alignment: using TreeSim as reward achieves the highest TreeSim scores, while NodechrF reward yields the best Node-chrF scores in most directions. This confirms that reinforcement learning can effectively improve the specific structural properties defined by the reward."
        },
        {
            "title": "6.7 Analysis: Synthetic Data Strategies",
            "content": "We explore the effect of using different synthetic data strategies to train the SFT model. As shown in Figure 7, although the translation quality comes close to training on real data, using synthetic data alone can lead to catastrophic structure failure, with XML-Match scores dropping below 20%. We suppose that the domain shift in textual content likely has adverse interaction effects with the structural performance. Because it is unlikely the model completely independently learns structural transfer and translation. This phenomenon highlights the crucial role of real target-domain XML Figure 7: SFT Model performance comparison for English to Chinese translation by composition of training data. syn-ALT refers to fine-tuning using the raw ALT document pairs. syn-0-shot refers to data synthesized in zero-shot manner. For syn-1-shot, the synthesizing LLM was provided with one example. The *-tag setups additionally guided the LLM with example XML tags from the development set. The Xreal+Ysyn setups are mixture of real data and synthetic data generated with the syn-1-shot-tag approach. markup. The importance of such markup is further underscored by observations that models trained on synthetic data generated without explicit guidance from in-domain examples and markup tags were more prone to structural errors. When combined with real data using the syn-1-shot-tag synthetic data, moderate amounts of synthetic data (e.g., 100real+100syn) can improve performance, whereas excessive amounts (e.g., 100real+400syn) can degrade it. For translation quality, this is not surprising: the Asian Language Treebank (Thu et al., 2016) used for data generation differs substantially in domain from software documentation. Results of all language pairs are shown in Figure 10."
        },
        {
            "title": "7 Conclusion",
            "content": "To address the challenge of translating documents with complex structures, we propose FORMATRL, novel reinforcement learning approach with proposed structure-aware rewards: TreeSim and Node-chrF. We further propose StrucAUC as fine-grained evaluation metric. Experimental results show FORMATRL improves the structural fidelity of translated documents without compromising translation quality across both simple inline markup and complex structured documents."
        },
        {
            "title": "8 Limitations",
            "content": "Limited Tag Set. We restricted the tag set used during synthetic data generation to those present in the development set. While this approach provides consistency, it raises questions about the downstream translation models ability to extrapolate to documents containing previously unseen tags. We did not evaluate this extrapolation capability due to budget constraints, as such an experiment would require generating substantially larger quantities of synthetic data with diverse markup using GPT. We did not explore tag abstraction using placeholder tags (e.g., <t1>) as which may help generalization but in the same time introduces pre-/post-editing and ignore semantics in human-interpretable tags. Applying Sentence-level Metrics to Documents. While we applied BLEU and COMET-22 to XMLstripped documents, these metrics, however, have known shortcomings when applied at the documentlevel as they are not designed/trained for such data (Jiang et al., 2022; Vernikos et al., 2022). Lack of Rigorous Human Evaluation. We performed simple human evaluation in the result section. However, we are aware that rigorous evaluation would require multiple annotators together with well-defined annotation instructions such as error taxonomies tailored to structured documents (e.g., MQM (Freitag et al., 2021) or ESA (Kocmi et al., 2024) style annotation) that explicitly capture tag mismatch, nesting errors, and their severities. We leave this to future work."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank the reviewers for their insightful comments and suggestions. This work was supported by JSPS KAKENHI Grant-in-Aid for Early-Career Scientists 25K21290."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Bianka Buschbeck and Miriam Exel. 2020. parallel evaluation data set of software documentation with document structure annotation. In Proceedings of the 7th Workshop on Asian Translation, pages 160169. Raj Dabre, Bianka Buschbeck, Miriam Exel, and Hideki Tanaka. 2023. study on the effectiveness of large language models for translation with markup. In Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track, pages 148159, Macau SAR, China. Asia-Pacific Association for Machine Translation. Raj Dabre, Haiyue Song, Miriam Exel, Bianka Buschbeck, Johannes Eschbach-Dymanus, and Hideki Tanaka. 2024. How effective is synthetic data and instruction fine-tuning for translation with markup using LLMs? In Proceedings of the 16th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 7387, Chicago, USA. Association for Machine Translation in the Americas. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv. Shihan Dou, Yan Liu, Haoxiang Jia, Enyu Zhou, Limao Xiong, Junjie Shan, Caishuang Huang, Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. StepCoder: Improving code generation with reinforcement learning from compiler feedback. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 45714585, Bangkok, Thailand. Association for Computational Linguistics. Jinhua Du, Johann Roturier, and Andy Way. 2010. TMX markup: challenge when adapting SMT to the localisation environment. In Proceedings of the 14th Annual Conference of the European Association for Machine Translation, Saint Raphaël, France. European Association for Machine Translation. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, and 1 others. 2024. The llama 3 herd of models. arXiv. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:14601474. Greg Hanneman and Georgiana Dinu. 2020. How should markup tags be translated? In Proceedings of the Fifth Conference on Machine Translation, pages 11601173, Online. Association for Computational Linguistics. Kazuma Hashimoto, Raffaella Buschiazzo, James Bradbury, Teresa Marshall, Richard Socher, and Caiming Xiong. 2019. high-quality multilingual dataset for In Proceedstructured documentation translation. ings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 116127, Florence, Italy. Association for Computational Linguistics. Yuchen Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, and Ming Zhou. 2022. BlonDe: An automatic evaluation metric for document-level machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 15501565, Seattle, United States. Association for Computational Linguistics. Eric Joanis, Darlene Stewart, Samuel Larkin, and Roland Kuhn. 2013. Transferring markup tags in statistical machine translation: two-stream approach. In Proceedings of the 2nd Workshop on Post-editing Technology and Practice, Nice, France. Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popovic, Mrinmaya Sachan, and Mariya Shmatova. 2024. Error span annotation: balanced approach for human evaluation of machine translation. In Proceedings of the Ninth Conference on Machine Translation, pages 14401453, Miami, Florida, USA. Association for Computational Linguistics. Harold Kuhn. 1955. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):8397. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, and 1 others. 2022. BLOOM: 176b-parameter open-access multilingual language model. arXiv. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, and 1 others. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. Yaxi Lu, Haolun Li, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Zhiyuan Liu, Fangming Liu, and Maosong Sun. 2025. Learning to generate structured output with schema reinforcement learning. arXiv. Mathias Müller. 2017. Treatment of markup in statistical machine translation. In Proceedings of the Third Workshop on Discourse in Machine Translation, pages 3646, Copenhagen, Denmark. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. arXiv. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Belgium, Brussels. Association for Computational Linguistics. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702. Hammam Riza, Michael Purwoadi, Gunarso, Teduh Uliniansyah, Aw Ai Ti, Sharifah Mahani Aljunied, Luong Chi Mai, Vu Tat Thang, Nguyen Phuong Thai, Vichet Chea, Rapid Sun, Sethserey Sam, Sopheap Seng, Khin Mar Soe, Khin Thandar Nwet, Masao Utiyama, and Chenchen Ding. 2016. Introduction of the asian language treebank. In 2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (OCOCOSDA), pages 16. Yonghyun Ryu, Yoonjung Choi, and Sangha Kim. 2022. Data augmentation for inline tag-aware neural machine translation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 886 894, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv. Jörg Steffen and Josef van Genabith. 2021. TransIns: Document translation with markup reinsertion. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 2834, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch, and Eiichiro Sumita. 2016. Introducing the In Proceedings Asian language treebank (ALT). of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 1574 1578, Portorož, Slovenia. European Language Resources Association (ELRA). Giorgos Vernikos, Brian Thompson, Prashant Mathur, and Marcello Federico. 2022. Embarrassingly easy document-level MT metrics: How to convert any pretrained metric into document-level metric. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 118128, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Jiashu Yao, Heyan Huang, Zeming Liu, Haoyu Wen, Wei Su, Boao Qian, and Yuhang Guo. 2024. Reff: Reinforcing format faithfulness in language models across varied tasks. arXiv. Thomas Zenkel, Joern Wuebker, and John DeNero. 2021. Automatic bilingual markup transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 35243533, Punta Cana, Dominican Republic. Association for Computational Linguistics. Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18:12451262."
        },
        {
            "title": "A Synthetic Data Generation",
            "content": "We show the prompt template used for synthetic data generation in Figure 8. It instructs GPT-4o to augment existing translation pairs without markup by inserting hierarchical XML markup elements into both source and target documents while maintaining alignment between the structures. In all k-shot settings we include full sourcetarget document pairs as exemplars. For k=5, their combined length is about 9,800 characters (5,070 Llama 3.1 tokens), which will differ for different language pairs. Example: Metrics Calculation We illustrate how the metrics work using toy example in Figure 9, including XML-Validity, XMLMatch, XML-BLEU, and the proposed StrucAUC (including the calculation of Node-chrF and Optimal Node-chrF)."
        },
        {
            "title": "Setting",
            "content": "Table 4 shows the detailed results of each reward function on the structured document setting across four language pairs (En Zh, Zh En, En Ja, Ja En). We found most of the rewards except XML-Validity improves across most metrics compared to supervised fine-tuning. Among the single reward functions, TreeSim usually achieves the best on structure score XML-Match, while NodechrF shows the highest combined scores in three directions."
        },
        {
            "title": "D Full Results of Inline Markup Setting",
            "content": "Table 5 shows the detailed results of each reward function on the inline markup setting across four language pairs (En Zh, Zh En, En Ja, Ja En). We have similar observations that most of the rewards except XML-Validity improves across most metrics compared to supervised fine-tuning especially on structure and combined scores. Synthetic Data Performance: All"
        },
        {
            "title": "Translation Directions",
            "content": "We show the effect of using different data when training the SFT model for all translation directions in Figure 10. The trends are similar: involving real data usually surpass pure synthetic data by large margin (which is also expected). And the ratio of synthetic data and real data did not affect the performance that much. Usually 1:1 is good balance, where too much synthetic data such as 4:1 slightly hurts the performance of the SFT model trained on such data."
        },
        {
            "title": "F Comparison with GPT",
            "content": "We compare our approach to three GPT variants of different sizes in Figure 11 which contains full results across all translation directions. FORMATRL is usually better than GPT-4.1-nano, comparable to GPT-4o-mini, and not as good as GPT-4o. We found GPT-4o is especially strong at preserving XML markup, achieving the highest scores on structural and combined metrics. Ablation: Reward Choice on Content-COMET and XML-Match Similar to Figure 6, Figure 12 shows the effect of different reward functions during GRPO training using the improvement of Content-COMET instead of that of Content-BLEU. We found this time using Content-BLEU as reward function did not achieve the best improvement on ContentCOMET, indicating the effect of reward overfitting: achieving the best on given metric while it is promised to achieve the best on similar metric (that measures the similar dimension). Nevertheless, TreeSim achieved the highest XML-Match improvement among single rewards except using XML-Match itself, and Node-chrF achieved the highest Content-COMET improvement, indicating the effect of the proposed rewards."
        },
        {
            "title": "H Details in StrucAUC",
            "content": "We show the StrucAUC algorithm in Algorithm 1. It is very fast algorithm, the Hungarian matching is O(n3) in the number of text nodes per document, where in our dataset n<20, so matching costs are negligible versus model inference. Tree edit operations are linear in tree size under our restricted XML grammar."
        },
        {
            "title": "I Discussion",
            "content": "We discuss some interesting aspects we think of our implementation for people who are interested in these details. About the dataset, we wanted to try FORMATRL on multiple datasets, after searching extensively for structured document translation datasets, we only found the SAP software document dataset. In the future we plan to curate some by ourselves."
        },
        {
            "title": "Synthetic Data Generation Prompt",
            "content": "Your task is to synthesize training data for machine translation of structured XML documents. Given provided translation pair, insert well-aligned hierarchical XML markup into both source and target document. Do not translate the markup elements or include language codes. Here is an example of well-aligned document pair: SOURCE: <!DOCTYPE concept PUBLIC \"-//SAP//DTD SAP DITA Composite//EN\" \"sap-ditabase.dtd\"> <concept id=\"loio16f62395d57f487e9937a092e4caefe9\" xml:lang=\"en-US\"> <title> Download </title> <shortdesc> Downloads G/L account mappings into .CSV file. ... TARGET: <!DOCTYPE concept PUBLIC \"-//SAP//DTD SAP DITA Composite//EN\" \"sap-ditabase.dtd\"> <concept id=\"loio16f62395d57f487e9937a092e4caefe9\" xml:lang=\"en-US\"> <title> ダウンロード </title> <shortdesc> G/L 勘定マッピングを.CSV ファイルにダウンロードします file. ... Now, insert markup into the following document pair. Output only the augmented source and target documents. Here are some example markup tags: <source></source> <uicontrol></uicontrol> <li></li> <p></p> <prolog></prolog> SOURCE: Every year around November 5th, people in Great Britain and some parts of the Commonwealth celebrate Guy Fawkes ... TARGET: 毎年11月5日前後にグレートブリテンと連邦の一部地域の人々は１６０５年１１月５日に国会議事堂 を爆破することができなかったヨーク... Figure 8: Prompt template of syn-1-shot-tag used for data synthesis for the structured markup translation task. This prompt features an example document pair from the development set as well as example tags sampled from development data to guide the LLM in data synthesis. For the syn-1-shot setup, the example tags are withhold. For the syn-0-shot-tag, the one-shot example is withhold. syn-0-shot features only the initial prompt and the data to synthesize from. For the inline setup, the initial prompt is altered to Your task is to synthesize training data for machine translation of documents containing XML markup. Given provided translation pair, insert well-aligned XML markup into both source and target document. Here is an example of well-aligned document pair. About hyper-parameters, we found GRPO does not require much training signal is the base SFT model has the basic structured document translation ability. In this case, the learning rate is crucial parameter, we have tried learning rate from 1e-5 to 1e-7 and found 1e-6 is good balance. Additionally, we save the checkpoint and evaluate it every 3 steps to capture the best one. Due to its efficiency, each training takes no more than 1.5 hours and we in total spend less than 800 GPU hours (100 hours in 8 H200 GPUs) for all GRPO experiments. For the memory efficiency, we found setting = 8, BatchSize = 8, and max generation token of 800 fits one H200 GPU with 141GB memory. In our analysis, we used Content-BLEU and XML-Match as reward, which may sounds like overfitting the metrics. However, the KL regularizer is added in the loss which prevents degenerate solutions that optimize only single metric. Moreover, our proposed novel metrics TreeSim and Node-chrF do not overfit any metrics used in evaluation. Figure 9: Toy-example of model translation and reference with markings for purely structural errors. XML-Validity: The translation can be successfully parsed into an XML and therefore achieves score of 1. XML-Match: The translation does not match the exact structure of the reference and therefore scores 0. XML-BLEU: Since the translation XML tree does not match the one of the reference the node contents of the reference will be paired with empty translations - e.g. (\" \", \"In-App-Help\") - for corpus BLEU computation. StrucAUC: The score is computed as corpus level area under curve based on the respective Node-chrF and Optimal Node-chrF. Node-chrF: The structural errors of the translation will lead to misalignment in the parallel depth-first traversal. For instance, we will see pairing of [...,(<conbody>,<prolog>), (<prolog>, <source>), (<source>, <conbody>)...] which overall results in low node-level chrF score of 16.89. Optimal Node-chrF: With 3.5 edit operations (note that changing the label of the <concept> is considered half an edit), the nodes of the translation can be realigned to the reference, resulting in Optimal Node-chrF of 52.92."
        },
        {
            "title": "J License",
            "content": "We use the SAP software documentation dataset which is under The Creative Commons license Attribution-Non Commercial 4.0 International (CC BY-NC 4.0), Asian Language Treebank (ALT) corpus under The Creative Commons Attribution 4.0 International (CC BY 4.0) License, and pre-trained models such as Llama-3.1-8B-Instruct under The Llama 3.1 Community License for research, which is consistent with their intended use. We have verified that the datasets do not contain personal information or offensive content. We plan to release our code upon acceptance under The Creative Commons license AttributionNon Commercial 4.0 International (CC BY-NC 4.0). The code is intended for research purposes only and may not be used for commercial applications without explicit permission."
        },
        {
            "title": "K The Use of AI Assistants",
            "content": "We used AI assistants for grammar and spelling checks. We sometimes also turn our incoherent listings of thoughts into coherent paragraph which has always undergone further manual revisions. Algorithm 1: StrucAUC Metric Input: Hypotheses { ˆDt,i}n Output: StrucAUC score i=1, References {D,i }n i=1, Maximum operations 1 Initialize Sk {} for {0, 0.5, 1, . . . , K}; 2 for = 1 to do 3 4 5 6 7 to XML trees; Parse ˆDt,i and D,i if D,i invalid then continue; if ˆDt,i invalid then Add 0 to all Sk and continue; sunaligned Node-chrFparallel( ˆDt,i, D,i ); 9 OptimalAlignment( ˆDt,i, D,i TreeEditDistance( ˆDt,i, D,i soptimal Node-chrFoptimal(M); S0 S0 {sunaligned}; for {0.5, 1, . . . , K} do 11 12 13 , M); ) // Hungarian algorithm 15 16 17 if then Sk Sk {soptimal}; else Sk Sk {sunaligned}; 18 Compute AUC via trapezoidal integration over {(k/K, mean(Sk))}; 19 return AUC 100; SrcTgt"
        },
        {
            "title": "Combined",
            "content": "Content-BLEU Content-COMET XML-Validity XML-Match XML-BLEU StrucAUC EnZh ZhEn EnJa JaEn Prompt SFT 49.88 49.66 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU Prompt SFT 49.88 50.07 49.70 49.78 49.31 49.75 49.38 48.82 56. FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU Prompt SFT 56.28 57.34 56.98 57.71 55.94 56.39 57.35 36.60 39.11 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU Prompt SFT 39.30 39.56 39.38 39.51 39.12 39.39 39.71 44.14 52.19 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU 52.79 53.67 53.20 53.12 52.43 53.12 53.53 86.16 86.47 86.48 86.54 86.47 86.32 86.21 86.37 86.41 85.25 85.34 85.25 85.36 85.32 85.41 85.16 85.16 85.49 87.17 88. 88.20 88.07 88.19 88.09 88.07 88.15 88.19 85.93 85.96 86.01 86.19 86.03 86.05 85.94 85.98 86.07 91.05 94.21 95.26 95.26 94.74 94.74 95.26 95.79 95.26 93.16 94. 95.26 95.79 95.26 95.26 94.74 94.74 94.74 87.89 95.26 95.79 95.79 94.21 95.26 95.26 95.26 94.21 90.53 95.26 94.74 95.26 94.74 95.26 95.26 95.26 94.21 76.84 85. 87.37 86.32 85.26 85.79 85.26 85.26 84.74 82.11 83.68 86.84 87.89 86.84 85.26 86.32 87.89 87.37 67.37 84.21 88.42 81.58 83.16 82.63 83.68 87.37 86.32 80.00 82. 87.37 84.21 84.74 82.63 82.63 86.32 85.26 27.50 36.38 38.07 38.31 36.17 36.64 36.17 35.97 35.56 26.34 27.58 29.14 31.22 30.52 30.34 28.73 30.31 30.17 14.49 27. 30.32 26.12 26.46 25.96 26.05 29.56 28.22 22.38 24.15 26.67 26.29 25.96 25.60 24.37 26.45 26.67 57.75 63.57 64.12 65.39 63.06 64.01 63.65 64.20 63.29 71.39 71. 72.84 74.12 72.76 72.75 71.42 72.33 73.56 48.60 60.40 60.48 60.29 59.52 60.07 59.63 60.36 60.15 65.25 67.92 69.82 70.58 69.70 69.48 69.05 69.26 69.79 Table 4: Full evaluation on structured documents, contrasting Prompt, SFT, and FORMATRL with diverse reward functions. SrcTgt"
        },
        {
            "title": "Combined",
            "content": "Content-BLEU Content-COMET XML-Validity XML-Match XML-BLEU StrucAUC EnZh ZhEn EnJa JaEn Prompt SFT 54.79 57.95 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match Prompt SFT 57.70 57.80 56.95 58.08 57.15 57.86 39.92 32. FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU Prompt SFT 34.76 28.39 29.71 36.78 28.87 35.87 37.76 40.13 44.42 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU Prompt SFT 45.60 45.04 44.11 45.72 44.44 44.14 45.53 35.61 34.74 FORMATRL w/ Reward of: TreeSim Node-chrF Node-chrF (opt.) Content-BLEU XML-Validity XML-Match XML-BLEU 37.02 35.13 32.61 35.76 33.37 33.50 33.52 85.87 86.19 86.22 86.39 86.10 86.23 86.22 86.26 83.31 83.06 82.83 82.48 82.60 83.44 82.76 82.65 83.44 87.90 88. 88.60 88.50 87.87 88.62 88.45 88.40 88.58 84.86 84.66 84.96 84.89 84.91 85.02 84.54 84.72 84.83 96.32 98.42 98.42 97.89 98.42 97.89 98.42 98.42 95.26 95. 95.79 94.21 94.21 96.32 96.84 94.74 96.32 96.32 97.37 98.42 97.37 97.89 98.42 98.42 97.37 98.42 98.95 97.89 98.42 98.42 98.95 97.37 96.32 97.89 97.37 84.74 89. 90.00 89.47 88.42 88.95 88.95 88.42 83.68 81.05 84.74 81.58 80.53 82.11 85.26 80.53 82.63 79.47 84.21 86.84 86.84 85.26 85.79 86.32 86.32 87.37 81.58 82. 86.84 82.11 83.16 86.84 78.42 82.63 88.42 43.33 47.51 47.63 47.64 45.28 47.00 46.50 46.80 33.83 33.01 34.74 32.85 32.50 33.78 30.81 32.03 33.78 26.78 32. 35.44 34.90 34.40 34.86 34.26 34.22 36.97 27.58 26.72 30.13 28.61 28.98 31.17 26.59 27.66 30.75 56.62 63.14 64.29 64.46 63.13 63.01 63.93 65.45 67.06 65. 66.28 65.54 65.11 66.01 67.27 65.94 66.89 45.07 54.93 55.04 54.89 53.12 56.08 54.12 54.11 54.29 64.65 63.60 65.82 64.41 64.46 66.05 61.87 64.09 64.95 Table 5: Full evaluation on inline markup documents. Each language pair lists Prompt, SFT, and FORMATRL with different reward functions. (a) enja (b) jaen (c) enzh (d) zhen Figure 10: Comparison of performance of different synthetic generation methods used in supervised fine-tuning, across four language pairs. Figure 11: Comparison with GPT models across four language pairs. Figure 12: Improvement of FORMATRL over SFT using various single rewards, and combinations of two rewards. Points represent mean improvement and ellipses visualize the local covariance directional structure between two metrics improvements. Estimates are constructed from RL results based on 8 SFT checkpoints each."
        }
    ],
    "affiliations": [
        "National Institute of Information and Communications Technology, Japan",
        "SAP, Germany"
    ]
}