{
    "paper_title": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation",
    "authors": [
        "Dongwon Jo",
        "Jiwon Song",
        "Yulhwa Kim",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV."
        },
        {
            "title": "Start",
            "content": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Dongwon Jo * 1 Jiwon Song * 1 Yulhwa Kim 2 Jae-Joon Kim 1 5 2 0 2 3 ] . [ 1 8 6 0 1 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00 and 1.40 improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/ dongwonjo/FastKV. 1. Introduction Recent advancements in large language models (LLMs) have enabled the processing of long-context sequences, such as those comprising 128k tokens (Achiam et al., 2023; Gem- *Equal contribution 1Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea 2Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea. Correspondence to: Yulhwa Kim <yulhwakim@skku.edu>, Jae-Joon Kim <kimjaejoon@snu.ac.kr>. Figure 1. Comparison of accuracy, TTFT, throughput across different KV cache compression methods on LLaMA-3.1-8B-Instruct ini Team et al., 2023; Anthropic). This capability significantly broadens the range of applications for LLMs (Yi et al., 2024; Laban et al., 2023; Gu, 2023). However, as the length of the input sequence increases, the size of these keyvalue (KV) caches also increases, making them significant bottleneck in the serving of long-context LLMs. Therefore, compression of KV caches is essential for optimizing the operation of LLMs. There has been active research on compressing KV cache to alleviate its burden in long-context handling (Zhang et al., 2023; Xiao et al., 2023; 2024; Oren et al., 2024; Chen et al., 2024). Some techniques have demonstrated the ability to preserve accuracy comparable to full-context processing after the compression (Li et al., 2024; Feng et al., 2024; Fu et al., 2024), while there exists compression technique that has achieved inference speedup in both the prefill and generation stages (Shi et al., 2024). However, no KV cache compression techniques currently exist that can simultaneously preserve accuracy and achieve inference speedup, particularly during the prefill stage. In this paper, we propose FastKV, an innovative approach designed to expedite long-context handling with LLMs while maintaining their accuracy. FastKV stems from our finding that the properties of attention maps differ between the early and later layers of LLMs. Through detailed analysis of LLMs, we discover that in the later layers, attention maps concentrate on limited set of significant tokens, which remain consistent across layers. In contrast, the early layers engage with broader array of tokens and exhibit diverse attention patterns as they process the context of the input prmopt. Based on these findings, FastKV adopts novel Token-Selective Propagation (TSP) method that applies difFastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 2. Comparison of the number of tokens processed in each layer/head of LLMs during prefill computation and KV caching across different KV cache compression techniques. As each token produces its corresponding KV, the KV cache size is directly proportional to the number of tokens processed. The blue background box indicates the set of sharing selected token indices. ferent strategies of KV cache compression at the early and later layers respectively As shown in Figure 1, this dualstrategy approach enables FastKV to improve both time-tofirst-token (TTFT) and throughput of long-context processing while effectively preserves the accuracy of LLMs. Our experimental results show that FastKV can achieve 2.00 speedup for TTFT and 1.40 improvement for throughput compared to HeadKV, while maintaining similar level of accuracy with less than 1% accuracy gap. These results demonstrate that FastKV promises practical solution for long context scenarios, particularly in real-time applications that require efficient KV cache management and low latency for the prefill and generation stages. 2. Background 2.1. Long-Context Processing with LLMs LLMs have significantly enhanced the natural language processing (NLP) abilities of AI systems (Brown et al., 2020; Radford, 2018; Kamalloo et al., 2023; Jiang et al., 2021) through the use of attention mechanisms (Vaswani, 2017). These attention mechanisms carefully evaluate the relationships between tokens within sentences to construct context vectors by aggregating token data according to the attention scores to extract context-specific information (Brauwers & Frasincar, 2021). Distinct from earlier NLP models such as RNNs (Sherstinsky, 2020) and LSTMs (Hochreiter, 1997), attention mechanisms consider the entire token data from input sequences, thereby avoiding issues of prompt forgetting. Benefiting from these advancements, recent researches indicates that LLMs are capable of processing long-context sequences. However, the very nature of LLMs, which retain all token data to facilitate attention mechanisms, leads to significant memory overhead. Typically, token data for attention mechanisms are stored in the form of KV cache, and the size of KV cache escalates with sequence length, becoming primary source of inefficiency in LLMs when processing long-context scenarios. For instance, in the LLaMA-3.18B model (Dubey et al., 2024), managing sequence of 128k tokens requires 17.12GB of memory for KV cache, whereas the model itself only requires 16.06GB. Therefore, the excessive KV cache demands form critical bottleneck in LLM inference during long-context processing, and efficient compression of KV cache is crucial for streamlined LLM operation. 2.2. KV Cache Compression KV cache compression techniques for long-context processing are primarily designed to prune tokens, thereby reducing the associated KV caches and effectively managing memory usage. common approach leverages the attention scores generated by each attention head to selectively prune tokens in an attention-head-wise manner. This method operates on the premise that token importance varies across attention heads. notable example is SnapKV (Li et al., 2024) in Figure 2(a). It discovers that each attention head in the model consistently focuses on specific prompt token during generation, and this robust pattern can be obtained from an observation window located at the end of the input prompt. Thus, SnapKV evaluates token importance score for each 2 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Algorithm 1 FastKV algorithm for the KV cache compression during the prefill stage Input: input sequence {I}, #layers {L}, SP layer, X, Attl, KX , VX layerl(X) K, KV Compress(KX , VX , Attl, BKV ) if == SP layer then if SP layer then SP length {BT SP }, KV budget {BKV } Output: generated token {O}, KV Cache {C} 1: Embedding(I) 2: for = 0 to 1 do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: LM Head(x) 16: return O, end if update(K, ) end if else x, Attl, Kx, Vx = layerl(x) K, KV Compress(Kx, Vx, Attl, BKV ) HiddenCompress(X, Attl, BT SP ) attention head by summing the attention scores assigned to each token as follows: Sl,h = 1 2wp + 1 wp (cid:88) Nobs(cid:88) m=wp n=0 Attl[h, NI n, + m] (1) Here, Sl,h is the importance score of i-th token in h-th attention head of the l-th layer. Attl denotes the attention score matrix of l-th layer, while NI and Nobs indicate the number of tokens in the input prompt and the observation window, respectively. SnapKV applies average pooling on the scores to prevent sparse token selection. Here, 2wp + 1 indicates the size of pooling window. SnapKV selects tokens from the observation window first and then selects tokens with the highest importance scores for KV caching. Since SnapKV relies on the attention scores produced by each attention head, it requires propagationg of the full input prompt throughout the entire model, during the prefill stage. As result, SnapKV cannot improve computational efficiency or reduce the latency of the prefill stage, which directly impacts TTFT, key factor in optimizing user experience for LLM serving. AdaKV (Feng et al., 2024) in Figure 2(b) expands SnapKV approach to grouped-query-attention (GQA) mechanism (Ainslie et al., 2023) by introducing dynamic KV budget allocation across attention groups and selecting important tokens in attention-group-wise manner. To enhance accuracy preservation, HeadKV (Fu et al., 2024) in Figure 2(c) builds upon SnapKV by introducing dynamic KV budget allocation across attention heads. This feature has 3 shown effective in preserving accuracy under aggressive KV cache compression scenarios, such as when the average KV budget is as low as 128. However, when applied to more realistic KV budget targets, such as 512, dynamic KV budget allocation exhibits marginal accuracy improvements or even slight accuracy degradation compared to SnapKV, while they introduces notable latency and throughput overhead. The primary reason of the overhead is that the processing speed of the attention mechanism is limited by the attention head with the longest KV cache. As the size of KV budget increases, the imbalance in KV cache sizes across attention heads becomes more pronounced and it amplifies latency overhead. This overhead makes these methods less practical for real-world applications, where achieving low latency is critical. In contrast, GemFilter (Shi et al., 2024) in Figure 2(d) adopts fundamentally different approach that enhances the computational efficiency of the prefill stage. GemFilter is based on the observation that LLMs can identify relevant tokens in intermediate layers, referred to as the filter layer. The filter layer selects the indices of relevant tokens at the layer level rather then head level, and GemFilter uses these indices to compress the input prompt. Then, the prefill stage is recomputed with compressed input prompt. By processing fullcontext with part of the LLM, GemFilter effectively reduces TTFT. The GemFilter paper reports successful accuracy preservation when using KV cache budget of 2k, where the input prompt is compressed with 2k tokens. Moreover, as GemFilter focuses on the selected tokens more intensively than full-context model by only processing selected tokens, with sufficient KV budget, there are some cases where it achieves even better accuracy than the full-context model. However, as the removal of tokens from the input prompt results in the complete loss of any information embedded in the discarded tokens, GemFilter struggles to maintain accuracy with lower KV budgets, such as 512, and even with 2k KV budgets when handling complex tasks. Thus, there are currently no KV cache compression techniques that can adequately preserve accuracy while simultaneously improving TTFT/throughput. This underscores the need for new methods that effectively balance these objectives. 3. Proposed FastKV 3.1. Overview of FastKV The overview of FastKV is illustrated in Figure 2(e) and Algorithm 1. FastKV implements distinct KV cache compression strategies for the early and later layers of LLMs. To speedup the prefill stage, FastKV employs the TSP approach in the later layers. This approach is based on the insight that after the early layers have analyzed contextual information, the later layers tend to focus consistently on the same crucial tokens. The TSP approach, therefore, involves FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation selectively propagating only limited set of tokens (e.g. 2k tokens) identified by the TSP layer, which is strategically positioned in the middle of the LLM architecture. The TSP layer utilizes its attention map to pinpoint these critical tokens. Unlike the early layers, which propagate the entire set of tokens, the TSP approach in the later layers transmits only the selected tokens to subsequent layers. This targeted propagation means that the later layers process far fewer tokens, thus it can accelerate the processing speed in these layers. While the TSP approach results in smaller set of KV data for the selected tokens, FastKV still applies the same KV cache compression techniques used in the early layers to optimize the compression ratio. In the early layers, FastKV evaluates the attention scores generated by each attention head to determine the importance of KV data. GQA compatible KV cache compression of FastKV selectively removes the KV data of tokens that receive low attention scores in an attention-group-wise manner. It is important to note that KV data compression occurs solely during the saving of KV data to establish KV caches for input prompt. Consequently, FastKV ensures that the entire set of KV data, including those of the removed tokens, is propagated to the subsequent layer until the TSP layer. This approach guarantees full context propagation to preserve essential contextual information from the input prompt. This dual strategy enables FastKV to achieve significant speed improvements without compromising accuracy, as it properly preserves the contextual information of input sequences. Figure 3. Rate of sum of attention scores for the top 2k important tokens selected to the total attention score at each layer. Here, denotes the number of attention head in each layer. From the TSP layer onward, TSP propagates only selected important tokens through the later layers. Unlike GemFilter, which reverts to the first layer of the LLM with selected tokens and recomputes the prefill stage, TSP directly propagates the selected tokens to subsequent layers without returning to the first layer. Meanwhile, although previous approaches to KV cache compression aimed at high accuracy by propagating the entire context throughout the prefill stage, the proposed TSP method discards the unimportant tokens after the TSP layer. Therefore, the implementation of TSP must be carefully designed based on detailed analysis of the properties of token importance and its impact on the LLM output. 3.2. Proposed TSP 3.3. Impact of Important Tokens TSP aims to enhance the efficiency of the prefill stage by propagating only selected set of important tokens, similar to the approach used by GemFilter. However, in response to the insights obtained from the previous sections, TSP is designed to retain the necessary scope of token propagation to ensure that critical contextual information is preserved. The operation of TSP is governed by three key factors: the layer for token selection (TSP layer), identification of important tokens, and the number of tokens selected for propagation (TSP length). TSP begins by identifying TSP length important tokens from the full tokens at the TSP layer. The method for identifying important tokens in TSP builds upon Equation 1. The propagation of selected tokens across LLM layers involves transmitting the hidden states associated with those selected tokens. Consequently, TSP necessitates the assessment of token importance on layer-by-layer basis. To facilitate this, TSP includes an additional step that aggregates the attention scores from all attention heads within the TSP layer as follows:"
        },
        {
            "title": "ST SP layer\ni",
            "content": "="
        },
        {
            "title": "1\nH",
            "content": "H1 (cid:88) h=0 ST SP layer,h (2) 4 In the LLM, the attention mechanism is the critical component for extracting contextual information, while other parts of the model process input in more token-isolated manner. Within the attention mechanism, each token generates new embedding by integrating the embeddings of all preceding tokens, with this integration weighted by the attention scores. Consequently, tokens that receive low attention scores have only marginal impact on the processing of contextual information. To ensure the removal of the unimportant tokens without significant impact on the LLM output, the important tokens must receive significantly higher attention scores. We evaluate the actual impact of these important tokens by examining their attention scores relative to those of the remaining tokens, as shown in Figure 3. In this analysis, we calculate the sum of the attention scores assigned to 2k important tokens selected in each layer and compare this sum to the attention scores allocated to all 128k tokens. Despite important tokens comprising only 1.56% of the total number of tokens, they receive majority of the attention scores. This is particularly notable in the later layers, where the proportion of attention scores attributed to these tokens consistently exceeds 50% and often approaches 80%. This result indicates that they are central to the LLM processing FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 4. Minimum match rate between indices of 2k important tokens selected at each layer and its subsequent layers. Figure 5. Visualization of output logits by t-SNE for (left) LLaMA3.1-8B-Instruct and (right) Mistral-Nemo-12B-Instruct. and the remaining tokens could be removed with marginal impact on the LLM output. 3.4. Dynamics of Token Importance Given that tokens discarded after the TSP layer are not propagated in subsequent layers, there must be consistency in identifying important tokens across layers to facilitate TSP. This section examines the dynamics of token importance across layers of LLMs. Specifically, we investigate whether tokens deemed important based on their high attention scores remain consistent across LLM layers. significant overlap in the set of important tokens would suggest the potential for pre-selecting these tokens before propagating them to subsequent layers. To evaluate the consistency of important tokens across layers, we analyze the important tokens in each layer of the LLM and calculate the minimum match rate of the indices of these tokens with those in subsequent layers. The minimum match rate serves as an indicator of an information bottleneck. Figure 4 presents our findings from this analysis on various LLMs, where each layer extracts 2k important tokens from 128k tokens. The minimum match rate in the first layer is below 10%, indicating low correlation between important tokens in the first and later layers. However, as we progress to subsequent layers, this match rate increases, surpassing 5 Figure 6. LongBench results of LLaMA-3.1-8B-insturct with propagation of tokens selected at each layer. 25% by the middle layer. This indicates that among the 2k tokens, at least 512 consistently appear as important across layers. Therefore, if the KV cache budget per layer is set to 512, it suffices to examine the 2k important tokens selected from the middle layer. The observed consistency in important token selection in later layers likely stems from the increasing robustness of contextual information extraction. Each layer in an LLM uses attention scores to integrate token information, progressively enhancing contextual understanding. As the model advances through its layers, contextual analysis becomes more refined, enabling more precise identification of relevant tokens. Thus, later layers of LLMs exhibit consistent recognition of important tokens, underscoring their refined understanding of token importance in the context of the overall sequence. 3.5. Overall Impact of TSP on LLM Output Building on the insights from Section 3.3 and 3.4, we can expect that the removal of unimportant tokens identified from the middle layer will not significantly impact the performance of the LLM for two main reasons. First, only small set of important tokens plays an important role in the attention mechanism and there is the consistency in the identification of important tokens in the middle layer and subsequent layers. We analyze real changes in the output logits of an LLM after selecting 2k tokens from each layer and discarding the unselected tokens before propagating them to subsequent layers. For this analysis, we use t-SNE (t-distributed Stochastic Neighbor Embedding) to project the final output logits of LLMs into 2D space. Figure 5 demonstrates the distances between output logits obtained with full-context propagation and those obtained through propagation focusing exclusively on important tokens. This visual representation shows that removing tokens in the early layers causes significant deviations in output logits compared to full-context propagation. However, as expected, removing tokens from later layers preserves high degree of similarity to the results from full-context FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation models. This contributes to stable accuracy results on the LongBench benchmark, as illustrated in Figure 6. In light of this observation, we designate the middle layer of each LLM as the TSP layer for the remainder of this paper. Moreover, in Figure 6, we compared the accuracy obtained with TSP and GemFilter-like propagation. TSP consistently outperforms GemFilter in terms of accuracy. This superior performance can primarily be attributed to how TSP manages the variability in token importance across layers. Unlike GemFilter, which reverts to the first layer of the LLM with selected tokens and recomputes the prefill stage, TSP directly propagates the selected tokens to subsequent layers without reverting to the initial layer. This direct propagation allows TSP to better accommodate the variability in token importance. Furthermore, the large accuracy gap between TSP and GemFilter when tokens are selected in the initial layers highlights the importance of leveraging token information that has been processed with the full context before making important token selections. 3.6. TSP Length and KV Budget The match rate of the important tokens is around 25% (Figure 4). This rate is sufficiently high to justify the use of TSP approach with an adequate TSP length (e.g. 2k). However, short TSP length (e.g. 512) may lead to significant information loss. Therefore, TSP length, which defines the number of tokens selected for propagation, is strategically designed to operate independently of the KV cache budget. For instance, if the KV cache budget is set to 512 while the TSP length is 2048, TSP propagates 2048 tokens during its operation, but only the final 512 tokens are stored in the KV cache. This separation allows TSP to process significantly larger set of tokens during propagation, enabling it to capture more contextual information while maintaining the KV cache within its predefined budget. As shown in Figure 7, TSP length of 2048 has been found sufficient to preserve accuracy of the model under specific KV budget while maintaining efficiency in terms of TTFT. 3.7. GQA Compatible KV Cache Compression While FastKV incorporates TSP for the latter half of the LLM, it takes different approach to KV cache compression by propagating full-context data in the early layers and evaluating the importance of KV data using attention scores generated by each layer. As the state-of-the-art LLMs (Yang et al., 2024; Abdin et al., 2024) increasingly adopt GQA (Ainslie et al., 2023), which groups multiple attention heads together and shares KV data within each group, FastKV incorporates GQA compatible KV cache compression technique presented in AdaKV to the early layers. This technique adapts SnapKV approach (Equation 1) to GQA by aggregating attention scores across all heads within the group for Figure 7. LongBench results (line) and TTFT (bar) of LLaMA-3.18B-insturct across various KV budgets and TSP lengths. each token as follows: Sl,g ="
        },
        {
            "title": "1\nHG",
            "content": "hg+HG (cid:88) h=hg Sj,h (3) Here, HG denotes the number of attention heads in each group. and hg denotes the group index and the index of the first head in the group. This aggregation evaluates the importance of tokens at the group level. FastKV then selects and stores KV cache entries corresponding to the top-KV budget important tokens in an attention-group-wise manner. 4. Experiments 4.1. Setup Models & Datasets. We evaluate two open-source LLMs of different sizes: LLaMA-3.1-8B-Instruct (Dubey et al., 2024) and Mistral-Nemo-12B-Instruct (Mistral AI Team, 2024). These models have 32 and 40 decoder layers, respectively, and employ GQA (Ainslie et al., 2023) with context window size of 128k tokens. To assess long-context understanding and retrieval capabilities, we use two benchmark datasets: LongBench (Bai et al., 2023) and Needle-in-a-Haystack (Kamradt, 2023). Implementation Details. We integrated our proposed FastKV method upon self-attention implementation of HuggingFace Transformers library, which utilizes FlashAttention-2 (Dao, 2023) kernel. We select layer 15 and 19 as the TSP layer for LLaMA-3.1-8B-Instruct, and Mistral-Nemo-12B-Instruct, respectively. The TSP length is set to 2048. We fix the observation window size (Nobs) to 8 and the kernel size of average pooling (2wp + 1) to 7. Baselines. We compare our approach with four baseline methods for KV cache compression: SnapKV (Li et al., 2024), AdaKV (Feng et al., 2024), HeadKV (Fu et al., 2024), and GemFilter (Shi et al., 2024). SnapKV, AdaKV, and HeadKV use the same local window size, pooling method, and pooling kernel size as FastKV. The indices of GemFilter filter layers for LLaMA-3.1-8B-Instruct and Mistral-NemoFastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Single-Doc QA Table 1. LongBench results comparison between baseline methods and FastKV. Few-shot Learning Summarization Multi-Doc QA Coding Method"
        },
        {
            "title": "Q asper",
            "content": "M F-en"
        },
        {
            "title": "Q A",
            "content": "2 ikiM"
        },
        {
            "title": "M Su m",
            "content": "Q"
        },
        {
            "title": "S A",
            "content": "Avg."
        },
        {
            "title": "L C C",
            "content": "R B-P Full KV 30.21 45.53 55.01 56. SnapKV AdaKV HeadKV GemFilter FastKV SnapKV AdaKV HeadKV GemFilter FastKV 30.09 29.07 30.17 18.52 30.38 31.31 30.64 30.53 23.64 30.31 41.62 40.16 44.03 22.15 41.12 44.96 45.23 44.99 41.17 45. 53.89 52.44 54.29 37.73 55.71 55.15 55.46 54.98 51.39 54.79 54.77 53.90 54.71 46.88 54.35 55.48 55.55 55.47 53.97 55.11 Full KV 26. 43.64 58.11 49.34 SnapKV AdaKV HeadKV GemFilter FastKV SnapKV AdaKV HeadKV GemFilter FastKV 23.58 24.46 25.88 25.45 25. 24.35 26.02 26.05 26.42 26.61 40.12 41.52 40.57 37.64 41.48 42.95 43.08 42.15 42.40 43.37 55.17 57.34 57.12 53.91 56.76 56.92 57.43 57.46 56.98 57.12 47.91 47.89 48.11 52.83 48. 49.08 48.76 48.49 57.64 49.41 25.28 46.65 31.28 30.78 31.14 31.42 29.24 30.46 31.22 31.10 31.58 24.52 30. 24.63 24.89 25.05 20.50 24.82 24.16 24.39 24.46 17.75 23.82 45.29 45.26 46.08 45.32 46.57 45.12 43.05 46.10 32.56 46.69 LLaMA-3.1-8B-Inustruct, KV Budget = Full 27.25 35.13 LLaMA-3.1-8B-Instruct, KV Budget = 512 24.86 27.37 24.85 25.75 26.66 31.14 23.25 27.05 24.37 26.78 LLaMA-3.1-8B-Instruct, KV Budget = 2048 26.97 31.92 26.72 31.28 27.20 32.41 26.91 32.01 27.02 31.25 Mistral-Nemo-12B-Instruct, KV Budget = Full 26.08 31.31 Mistral-Nemo-12B-Instruct, KV Budget = 512 24.08 23.77 45.69 23.88 23.58 45.83 25.25 26.19 46.11 23.87 26.23 51.26 45.43 24.02 23.42 Mistral-Nemo-12B-Instruct, KV Budget = 2048 26.07 28.27 46.43 25.93 27.80 46.53 26.17 30.27 45.85 25.96 30.51 53.92 25.81 28.40 47.22 22.48 22.11 22.92 19.42 22. 23.68 23.44 24.22 21.60 23.22 25.17 25.57 25.26 33.02 26.53 26.26 26.12 25.86 34.33 26.27 26.26 45.85 24. 73.00 91.64 43.80 63.38 56.64 48.63 71.00 69.00 73.00 60.50 72.50 71.50 72.50 72.50 70.00 73. 91.90 92.34 91.72 89.49 92.04 91.48 91.64 91.57 91.59 91.48 42.43 42.05 42.17 40.16 42.64 43.38 42.75 42.84 42.59 43.66 61.53 52.49 46.60 63.43 55.32 46.20 62.89 55.71 47.76 27.35 31.02 35.64 62.60 52.74 46.90 63.14 56.04 48.00 64.88 59.25 48.37 63.45 56.75 48.23 47.35 38.91 43.90 63.17 55.82 48. 75.00 89.66 44.32 68.58 68.11 48.33 74.00 74.00 74.50 65.50 74.00 75.00 75.00 75.00 72.00 75. 89.44 89.52 89.74 84.16 89.42 89.82 89.66 89.66 89.65 89.72 43.09 43.20 41.81 40.05 43.22 44.20 44.49 43.74 44.48 43.04 68.31 62.15 46.07 68.21 64.15 46.52 68.79 64.76 46.93 38.03 41.13 42.32 68.61 63.57 46.63 68.89 68.00 47.85 68.53 67.57 47.88 68.64 67.29 47.92 48.34 48.06 46.59 68.67 67.61 48. 12B-Instruct are 13 and 19, respectively (Shi et al., 2024). 4.2. Accuracy Evaluation LongBench. The accuracy evaluation results on LongBench are summarized in Table 1. The full breakdown of results is provided in Appendix A.1. Previous works, such as SnapKV, AdaKV, and HeadKV, which use attention-head-wise or attention-group-wise token removal, successfully maintain accuracy after KV cache compression, with an average accuracy drop of less than 2.5% and 0.51% compared to FullKV for 512 and 2048 KV budgets, respectively. GemFilter, which discards unselected tokens from the input prompt and recomputes the prefill stage to achieve both memory and computational efficiency, results in significantly larger accuracy drop, with reductions of up to 13.0% and 4.7% compared to FullKV for 512 and 2048 KV budgets, respectively. On the other hand, while the proposed FastKV is designed to achieve memory and computational efficiency comparable to GemFilter, it successfully preserves accuracy on par with SnapKV, AdaKV, and HeadKV. It presents the effectiveness of FastKVs distinct KV cache compression strategies for the early and later layers of LLMs. Needle-in-a-Haystack. Figure 8 presents the Needle-in-aHaystack evaluations for LLaMA-3.1-8B-Instruct with 512 KV budget. Additional analysis results with various LLM models and KV budgets can be found in Appendix A.2. Consistent with the results observed in LongBench, GemFilter shows the worst performance, while FastKV achieves the best performance with slight improvement over HeadKV. 4.3. Latency and Throughput Evaluation We evaluate the latency of the prefill stage (time-to-firsttoken (TTFT)) and throughput of token generation with LLaMA-3.1-8B-Instruct model and single NVIDIA A100 GPU. Figure 9 reports TTFT results with 64k/128k input with 512 KV budget, and throughput results with 128k input with 512/2048 KV budget. More detailed evaluation results can be found in Appendix A.3. TTFT. As shown in Figure 9(a), both GemFilter and FastKV achieve significant TTFT improvements over other baselines by reducing the computational complexity of the prefill stage through the concept of propagating only selected tokens during this stage. As result, FastKV achieves TTFT that is 1.60 shorter and 1.97 shorter than Full KV, with 64k and 128k input, respecitvely. While both methods enhance efficiency, GemFilter incurs additional overhead due to the need for prefill recomputation, whereas FastKV introduces extra processes for compressing generated KV caches. These two factors offset each other. Consequently, GemFilters slight advantage in TTFT is primarily attributed to its earlier filter layer (13) compared to the TSP layer (15) for LLaMA-3.1-8B-Instruct. In contrast, SnapKV, AdaKV, 7 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 8. Needle-in-a-Haystack results of LLaMA-3.1-8B-Instruct with 512 KV budget. allocates varying KV budgets across attention heads, exhibits the lowest throughput among KV cache compression techniques. Managing different sequence lengths for keys and values across attention heads increases the complexity of the attention mechanism. This effect gets stronger when the KV cache size is large, thereby HeadKV experiences steep decline in throughput as KV budget increases, whereas other methods throughput remain relatively consistent. AdaKV also adopts fine-grained KV budget allocation similar to HeadKV except that its granularity is attentiongroup. AdaKV mitigates the influence of fragmented KV budget to some extent thanks to GQA-aware compression, but it does not achieve the same level of throughput with FastKV or GemFilter. 5. Conclusion In this paper, we introduce FastKV, novel KV cache compression method designed to improve the efficiency of long-context LLM inference while preserving accuracy. Unlike previous KV cache compression techniques, FastKV enhances computational efficiency by introducing TokenSelective Propagation (TSP). This method strategically retains full-context information in early layers and propagates compressed subset of tokens in later layers, reducing the computational cost of the prefill stage. Furthermore, our approach is optimized by integrating GQA compatible KV cache compression, which enables more efficient memory and computation management by leveraging group-wise token selection. Our experimental results demonstrate that FastKV achieves higher throughput and lower TTFT than baseline methods while maintaining high accuracy on longcontext benchmarks. Figure 9. (a) TTFT and (b) throughput results across different methods on LLaMA-3.1-8B-Instruct. (dashed line: Full KV) and HeadKV exhibit even longer TTFT than Full KV. This is because they process the full-context information during the prefill stage, similar to Full KV, while also incurring additional overhead from indexing and selecting crucial tokens. Throughput. We measure the throughput for generating total of 128 tokens. As shown in Figure 9(b), all KV compression techniques result in throughput improvements compared to Full KV. Both FastKV and GemFilter achieve significant level of throughput improvement. FastKVs throughput reaches 5.07 of Full KV and 1.40 of HeadKV with 512 KV budget. In contrast, SnapKV, which stores KV caches of different tokens set for each attention head, shows slightly lower throughput. Additionally, HeadKV, which FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation"
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Anthropic. The claude 3 model family: Opus, sonnet, haiku. URL https://www-cdn.anthropic.com. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Brauwers, G. and Frasincar, F. general survey on attention mechanisms in deep learning. IEEE Transactions on Knowledge and Data Engineering, 35(4):32793298, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:18771901, 2020. Chen, Y., Wang, G., Shang, J., Cui, S., Zhang, Z., Liu, T., Wang, S., Sun, Y., Yu, D., and Wu, H. Nacl: general and effective kv cache eviction framework for llms at inference time. arXiv preprint arXiv:2408.03675, 2024. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. International Conference on Learning Representations (ICLR), 2023. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Feng, Y., Lv, J., Cao, Y., Xie, X., and Zhou, S. K. Adakv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. Gemini Team, Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gu, Q. Llm-based code generation method for golang compiler testing. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 2201 2203, 2023. Hochreiter, S. Long short-term memory. Neural Computation MIT-Press, 1997. Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962 977, 2021. Kamalloo, E., Dziri, N., Clarke, C. L., and Rafiei, D. Evaluating open-domain question answering in the era of large language models. arXiv preprint arXiv:2305.06984, 2023. Kamradt, G. Needle in haystack - pressure testhttps://github.com/gkamradt/ ing llms. LLMTest_NeedleInAHaystack, 2023. Laban, P., Kryscinski, W., Agarwal, D., Fabbri, A. R., Xiong, C., Joty, S., and Wu, C.-S. Summedits: measuring llm ability at factual reasoning through the lens of summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 96629676, 2023. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems (NeurIPS), 2024. Mistral AI Team. Mistral-nemo. https://mistral. ai/news/mistral-nemo, 2024. Oren, M., Hassid, M., Yarden, N., Adi, Y., and Schwartz, R. Transformers are multi-state rnns. arXiv preprint arXiv:2401.06104, 2024. Radford, A. Improving language understanding by generative pre-training. 2018. Sherstinsky, A. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. Physica D: Nonlinear Phenomena, 404:132306, 2020. Fu, Y., Cai, Z., Asi, A., Xiong, W., Dong, Y., and Xiao, W. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. Shi, Z., Ming, Y., Nguyen, X.-P., Liang, Y., and Joty, S. Discovering the gems in early layers: Accelerating longcontext llms with 1000x input token reduction. arXiv preprint arXiv:2409.17422, 2024. 9 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yi, Z., Ouyang, J., Liu, Y., Liao, T., Xu, Z., and Shen, Y. survey on recent advances in llm-based multi-turn dialogue systems. arXiv preprint arXiv:2402.18013, 2024. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems (NeurIPS), 2023. 10 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation A. More Experimental Results A.1. LongBench We provide full breakdown of the LongBench evaluation results for LLaMA-3.1-8B-Instruct  (Table 2)  , LLaMA-3.2-3BInstruct  (Table 3)  , and Mistral-Nemo-12B-Instruct  (Table 4)  . For LLaMA-3.2-3B-Instruct, which has 28 decoder layers, we set the TSP layer and GemFilter filter layer to 13 both. Table 2. LongBench results of LLaMA-3.1-8B-Instruct. Single-Doc QA Multi-Doc QA"
        },
        {
            "title": "Summarization",
            "content": "Few-shot Learning"
        },
        {
            "title": "Q asper",
            "content": "M F-en"
        },
        {
            "title": "M Su m",
            "content": "Q"
        },
        {
            "title": "T R E C",
            "content": "Avg."
        },
        {
            "title": "L C C",
            "content": "R B-P"
        },
        {
            "title": "Full KV",
            "content": "30.21 45.53 55.01 56.01 46.65 31.28 35. 25.28 27.25 73.00 91.64 43.80 63.38 56.64 48. LLaMA-3.1-8B-Instruct, KV Cache Size = Full"
        },
        {
            "title": "27.39\nSnapKV\n24.90\nAdaKV\nHeadKV\n29.74\nGemFilter 12.50\n27.63\nFastKV",
            "content": "30.40 50.35 24.41 49.95 40.34 53.51 11.00 16.52 27.84 52.32 30.09 SnapKV 29.07 AdaKV HeadKV 30.17 GemFilter 18.52 30.38 FastKV 41.62 53.89 40.16 52.44 44.03 54.29 22.15 37.73 41.12 55.71 31.23 SnapKV 29.23 AdaKV HeadKV 30.55 GemFilter 19.34 30.22 FastKV 42.52 53.96 44.09 53.82 44.66 54.69 34.76 46.58 44.01 55.04 SnapKV 31.31 AdaKV 30.64 30.53 HeadKV GemFilter 23.64 30.31 FastKV 44.96 55.15 45.23 55.46 44.99 54.98 41.17 51.39 45.41 54.79 53.04 53.15 54.83 28.84 52.00 54.77 53.90 54.71 46.88 54.35 55.48 54.80 55.47 46.82 54.76 55.48 55.55 55.47 53.97 55.11 LLaMA-3.1-8B-Inustruct, KV Budget = 43.49 41.73 45.35 18.82 45.27 28.89 28.55 30.62 13.53 28.35 22.87 20.54 26.40 19.66 22.00 23.00 23.21 24.23 17.67 22.81 21.74 20.28 24.58 14.34 20.64 LLaMA-3.1-8B-Instruct, KV Budget = 45.12 43.05 46.10 32.56 46.69 31.22 31.10 31.58 24.52 30.89 27.37 25.75 31.14 27.05 26.78 24.16 24.39 24.46 17.75 23.82 24.86 24.85 26.66 23.25 24.37 LLaMA-3.1-8B-Instruct, KV Budget = 45.43 44.01 46.20 42.83 46.76 31.50 31.40 31.63 27.51 30.59 29.54 28.86 32.88 29.89 29.43 24.78 24.73 25.14 18.96 24.66 26.17 26.04 27.10 25.68 26.19 LLaMA-3.1-8B-Instruct, KV Budget = 45.29 45.26 46.08 45.32 46.57 30.78 31.14 31.42 29.24 30.46 31.92 31.28 32.41 32.01 31.25 24.63 24.89 25.05 20.50 24.82 26.97 26.72 27.20 26.91 27.02 62.00 50.50 71.50 58.17 69. 71.00 69.00 73.00 60.50 72.50 70.50 72.50 73.00 63.00 73.50 71.50 72.50 72.50 70.00 73.50 91.19 89.49 88.51 78.51 91.28 91.90 92.34 91.72 89.49 92.04 91.73 91.72 91.57 90.70 91. 91.48 91.64 91.57 91.59 91.48 40.31 40.71 40.14 32.99 40.17 42.43 42.05 42.17 40.16 42.64 42.52 42.56 42.88 42.50 42.42 43.38 42.75 42.84 42.59 43.66 58.43 49.33 43.03 58.74 52.40 41.33 60.19 52.92 45.92 19.92 21.35 25.99 59.08 48.38 42. 61.53 52.49 46.60 63.43 55.32 46.20 62.89 55.71 47.76 27.35 31.02 35.64 62.60 52.74 46.90 62.25 54.72 47.31 63.22 56.33 47.38 63.76 56.06 48.26 38.09 35.14 40.13 62.54 55.02 47.64 63.14 56.04 48.00 64.88 59.25 48.37 63.45 56.75 48.23 47.35 38.91 43.90 63.17 55.82 48.10 11 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Table 3. LongBench results of LLaMA-3.2-3B-Instruct. Single-Doc QA Multi-Doc QA"
        },
        {
            "title": "Summarization",
            "content": "Few-shot Learning"
        },
        {
            "title": "Q asper",
            "content": "M F-en"
        },
        {
            "title": "M Su m",
            "content": "Q"
        },
        {
            "title": "T R E C",
            "content": "Avg."
        },
        {
            "title": "L C C",
            "content": "R B-P Full KV 23.41 40.60 49.79 48.71 38. 20.54 34.07 23.65 26.67 71.50 88. 43.20 52.13 54.16 44.02 LLaMA-3.2-3B-Instruct, KV Budget = Full 17.92 SnapKV 19.94 AdaKV HeadKV 21.59 GemFilter 10.36 19.95 FastKV 23.18 44.59 23.15 44.50 32.15 48.00 10.70 26.36 23.09 44.34 21.03 SnapKV 21.30 AdaKV HeadKV 22.79 GemFilter 20.88 21.73 FastKV 32.79 49.01 32.88 49.92 36.45 50.57 31.37 44.31 31.55 49.88 21.71 SnapKV 21.69 AdaKV 23.44 HeadKV GemFilter 21.34 23.13 FastKV 36.22 49.04 36.91 49.76 38.06 50.37 33.72 48.52 35.27 49.48 22.89 SnapKV 23.43 AdaKV HeadKV 22.56 GemFilter 17.48 24.02 FastKV 38.70 50.91 39.28 50.26 39.84 50.40 38.31 49.93 37.99 50.06 45.35 45.17 48.79 35.84 45. 48.68 47.65 48.85 47.60 47.31 48.78 48.10 49.14 50.13 47.93 48.38 48.61 49.15 50.57 47.58 LLaMA-3.2-3B-Instruct, KV Budget= 128 35.08 36.51 38.28 27.75 34.49 17.07 18.68 18.51 11.63 19. 21.96 21.86 25.30 19.23 21.34 21.05 21.46 21.94 17.00 20.63 20.38 20.57 23.86 13.59 20.02 LLaMA-3.2-3B-Instruct, KV Budget = 512 37.59 37.20 39.16 34.87 36.54 20.07 19.72 20.44 23.59 22. 25.91 25.96 29.79 25.89 25.47 22.76 22.65 23.24 19.82 22.38 24.22 24.31 26.10 24.01 24.05 LLaMA-3.2-3B-Instruct, KV Budget = 1024 38.14 39.27 39.51 33.75 38.74 21.21 21.25 20.33 20.82 21. 28.32 28.01 31.48 28.90 28.09 23.25 22.78 23.26 20.73 22.66 25.62 25.66 26.59 24.73 25.86 LLaMA-3.2-3B-Instruct, KV Budget = 2048 39.29 39.60 38.95 40.29 38.76 20.55 20.82 20.25 19.02 21. 30.79 30.39 32.98 30.95 30.45 23.17 22.94 23.73 21.50 23.24 26.46 26.65 26.71 26.28 26.29 66.00 67.00 69.50 55.50 66.00 69.00 69.50 70.50 55.00 71.50 69.00 70.00 71.00 61.50 71. 70.50 71.00 71.00 64.50 72.50 87.55 87.82 88.45 71.58 88.05 88.75 89.03 89.66 85.73 88.99 89.03 88.96 89.53 89.17 89.46 88.89 88.89 89.28 90.07 89.22 38.39 39.26 38.34 28.40 38. 41.78 41.55 40.80 35.29 40.91 41.68 42.62 41.41 38.44 41.46 42.25 42.57 42.74 40.59 41.73 48.31 45.82 38.05 49.39 47.61 38.78 52.11 50.25 41.22 19.13 27.46 26.75 48.91 47.13 38.39 51.68 51.25 41.75 51.63 51.87 41.80 53.72 53.83 43.28 26.68 32.72 36.27 52.46 52.17 41.93 53.01 53.42 42.75 53.00 53.35 42.96 53.84 54.14 43.72 29.89 34.05 38.26 53.09 54.37 43. 52.16 54.03 43.50 52.33 54.60 43.67 53.56 55.30 44.03 37.97 38.88 40.45 52.71 55.52 43.71 12 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Table 4. LongBench results of Mistral-Nemo-12B-Instruct. Single-Doc QA Multi-Doc QA"
        },
        {
            "title": "Summarization",
            "content": "Few-shot Learning"
        },
        {
            "title": "Q asper",
            "content": "M F-en"
        },
        {
            "title": "M Su m",
            "content": "Q"
        },
        {
            "title": "T R E C",
            "content": "Avg."
        },
        {
            "title": "L C C",
            "content": "R B-P Full KV 26.27 43.64 58.11 49.34 45. 26.26 31.31 24.15 26.08 75.00 89. 44.32 68.58 68.11 48.33 Mistral-Nemo-12B-Instruct, Budget = Full 21.29 SnapKV 22.29 AdaKV HeadKV 22.62 GemFilter 15.74 22.26 FastKV 34.24 51.53 36.52 52.50 38.76 52.54 18.30 42.98 35.09 53.33 23.58 SnapKV 24.46 AdaKV HeadKV 25.88 GemFilter 25.45 25.51 FastKV 40.12 55.17 41.52 57.34 40.57 57.12 37.64 53.91 41.48 56.76 23.27 SnapKV 25.33 AdaKV 25.94 HeadKV GemFilter 26.94 26.26 FastKV 41.42 56.01 41.82 57.52 41.38 56.45 40.67 53.63 42.37 57.71 24.35 SnapKV 26.02 AdaKV HeadKV 26.05 GemFilter 26.42 26.61 FastKV 42.95 56.92 43.08 57.43 42.15 57.46 42.40 56.98 43.37 57.12 48.07 47.28 46.49 41.86 49. 47.91 47.89 48.11 52.83 48.73 47.85 48.02 48.22 55.82 49.40 49.08 48.76 48.49 57.64 49.41 Mistral-Nemo-12B-Instruct, KV Budget = 128 44.55 44.73 45.97 34.20 45.63 23.99 21.84 21.80 18.44 23. 19.93 19.15 20.74 21.72 19.15 20.73 20.61 21.37 16.56 20.80 20.69 20.74 22.10 18.81 20.47 Mistral-Nemo-12B-Instruct, KV Budget = 512 45.69 45.83 46.11 51.26 45.43 25.17 25.57 25.26 33.02 26. 23.77 23.58 26.19 26.23 23.42 22.48 22.11 22.92 19.42 22.15 24.08 23.88 25.25 23.87 24.02 Mistral-Nemo-12B-Instruct, KV Budget = 1024 46.09 46.48 45.90 56.90 46.88 25.83 26.34 25.05 38.23 25. 25.97 25.68 28.67 28.68 26.17 23.70 23.12 23.72 20.76 23.08 25.37 25.26 25.72 25.34 25.29 Mistral-Nemo-12B-Instruct, KV Budget = 2048 46.43 46.53 45.85 53.92 47.22 26.26 26.12 25.86 34.33 26. 28.27 27.80 30.27 30.51 28.40 23.68 23.44 24.22 21.60 23.22 26.07 25.93 26.17 25.96 25.81 69.50 65.50 73.00 57.00 65.00 74.00 74.00 74.50 65.50 74.00 74.00 74.50 75.00 69.00 75. 75.00 75.00 75.00 72.00 75.00 89.52 88.64 89.42 63.02 90.06 89.44 89.52 89.74 84.16 89.42 89.82 89.82 89.84 87.32 89.72 89.82 89.66 89.66 89.65 89.72 39.92 39.25 39.16 31.78 39. 43.09 43.20 41.81 40.05 43.22 43.48 44.65 42.64 42.49 43.56 44.20 44.49 43.74 44.48 43.04 62.85 53.08 42.85 63.79 54.07 42.64 64.92 56.15 43.93 25.74 30.62 31.20 63.65 55.68 43.13 68.31 62.15 46.07 68.21 64.15 46.52 68.79 64.76 46.93 38.03 41.13 42.32 68.61 63.57 46.63 68.46 65.84 46.94 68.49 66.21 47.37 68.66 66.60 47.41 44.33 45.41 45.39 68.53 65.79 47. 68.89 68.00 47.85 68.53 67.57 47.88 68.64 67.29 47.92 48.34 48.06 46.59 68.67 67.61 48.11 13 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation A.2. Needle-in-a-Haystack We performed further Needle-in-a-Haystack evaluations with varying KV budgets for LLaMA-3.1-8B-Instruct (Figure 10), LLaMA-3.2-3B-Instruct (Figure 11), and Mistral-Nemo-12B-Instruct (Figure 12). For LLaMA-3.1-8B-Instruct, the overall retrieval trend remains consistent. FastKV continues to achieve strong performance across multiple budgets, often matching or surpassing baseline methods, including HeadKV. By contrast, LLaMA-3.2-3B-Instruct produces notably lower scores overall, even when using Full KV. Its smaller parameter count appears to limit retrieval capabilities at long contexts, making it more difficult for fine-grained KV allocation techniques to improve performance. Although HeadKV excelled with LLaMA-3.1-8B-Instruct, it does not show similar gains on LLaMA-3.2-3B-Instruct, suggesting that certain highly flexible optimizations may not translate effectively to smaller models. Nonetheless, FastKV outperforms SnapKV, HeadKV, and GemFilter, and achieves an average score on par with AdaKV, indicating that its core compression strategy retains efficacy when applied to compact model. Mistral-Nemo-12B-Instruct exhibits shorter effective context window than the nominal 128k, frequently struggling to retrieve needles located at extreme depths while handling nearer contexts reasonably well. Surprisingly, GemFilter significantly outperform Full KV, and FastKV also records above-Full-KV scores. GemFilter discards subset of tokens and restarts its prefill stage with the reduced sequence, and FastKV continues prefilling the remaining layers with reduced hidden states after TSP. This may lead temporary enhancement of retrieval by focusing attention on fewer tokens. However, GemFilters poor results in LongBench confirm that these gains do not generalize to complex tasks and models. 14 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 10. Needle-in-a-Haystack results of LLaMA-3.1-8B-Instruct. 15 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 11. Needle-in-a-Haystack results of LLaMA-3.2-3B-Instruct. 16 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation Figure 12. Needle-in-a-Haystack results of Mistral-Nemo-12B-Instruct. 17 FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation A.3. Latency and Throughput Evaluation We present the TTFT and the throughput results of the baselines and FastKV in Table 5 and Table 7. The throughput results of Full KV is depicted in Table 6 for effective comparison. The throughput results of KV cache compression methods are evaluated with input context length 128k. As the input context length increases, the relative TTFT gain of FastKV compared to SnapKV, AdaKV and HeadKV increases. For Mistral-Nemo-12B-instruct, FastKV achieves TTFT similar to GemFilter because the TSP layer index is same with the GemFilter filter layer. The token throughput gain of the baselines and FastKV using fixed KV budget grows rapidly as the throughput of Full KV declines due to the increasing size of KV cache. FastKV consistently achieves the highest throughput across all scenarios. Table 5. TTFT (sec) comparison of baselines and FastKV."
        },
        {
            "title": "Context Length",
            "content": "8k 16k 32k 64k 128k LLaMA-3.1-8B-Instruct 0.64 0.67 0.75 0.70 0.44 0.41 1.41 1.45 1.53 1.48 0.78 0.80 3.40 3.49 3.57 3.51 1.66 1. Mistral-Nemo-12B-Instruct 0.92 0.95 1.05 0.99 0.69 0.59 2.04 2.08 2.18 2.12 1.25 1.16 4. 4.90 5.01 4.95 2.65 2.57 9.05 9.22 9.31 9.25 4.15 4.66 12.71 12.80 12.92 12.85 6.59 6.53 27. 28.27 28.36 28.30 12.51 14.15 37.94 38.11 38.23 38.18 19.22 19.22 Table 6. Throughput (Tokens/sec) with full KV budget at varying context length."
        },
        {
            "title": "Context Length",
            "content": "LLaMA-3.1-8B-Instruct Mistral-Nemo-12B-Instruct 8k 51.55 36.10 16k 43.07 30.88 32k 32.29 23.67 64k 21.24 16.07 128k 12.60 9.68 Table 7. Throughput (Tokens/sec) comparison of baselines and FastKV."
        },
        {
            "title": "SnapKV\nAdaKV\nHeadKV\nGemFilter\nFastKV",
            "content": ""
        },
        {
            "title": "KV Budget",
            "content": "1024 LLaMA-3.1-8B-Instruct 48.71 50.57 45.64 63.40 63.84 48.62 49.53 39.48 62.32 62.97 Mistral-Nemo-12B-Instruct 38.04 28.53 27.20 41.94 42. 38.03 28.34 26.97 41.69 41.85 2048 48.29 49.47 30.78 60.37 60.74 35.44 27.82 24.92 40.17 41."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea",
        "Department of Semiconductor Systems Engineering, Sungkyunkwan University, Suwon, South Korea"
    ]
}