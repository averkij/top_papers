{
    "paper_title": "Spatial Mental Modeling from Limited Views",
    "authors": [
        "Baiqiao Yin",
        "Qineng Wang",
        "Pingyue Zhang",
        "Jianshu Zhang",
        "Kangrui Wang",
        "Zihan Wang",
        "Jieyu Zhang",
        "Keshigeyan Chandrasegaran",
        "Han Liu",
        "Ranjay Krishna",
        "Saining Xie",
        "Manling Li",
        "Jiajun Wu",
        "Li Fei-Fei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Can Vision Language Models (VLMs) imagine the full scene from just a few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new MindCube benchmark with 21,154 questions across 3,268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using MindCube, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for \"what-if\" movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from a synergistic approach, \"map-then-reason\", that jointly trains the model to first generate a cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 8 5 4 1 2 . 6 0 5 2 : r M C E"
        },
        {
            "title": "Spatial Mental Modeling from Limited Views",
            "content": "Baiqiao Yin1,3, Qineng Wang1, Pingyue Zhang1, Jianshu Zhang1, Kangrui Wang1, Zihan Wang1, Jieyu Zhang4, Keshigeyan Chandrasegaran2, Han Liu1, Ranjay Krishna4, Saining Xie2, Manling Li1, Jiajun Wu2, Li Fei-Fei2 Equal Contribution; Project Lead; Equal Advising 1Northwestern University 2Stanford University 3New York University 4University of Washington Can Vision Language Models (VLMs) imagine the full scene from just few views, like humans do? Humans form spatial mental models, internal representations of unseen space, to reason about layout, perspective, and motion. Our new D E benchmark with 21, 154 questions across 3, 268 images exposes this critical gap, where existing VLMs exhibit near-random performance. Using D E, we systematically evaluate how well VLMs build robust spatial mental models through representing positions (cognitive mapping), orientations (perspective-taking), and dynamics (mental simulation for what-if movements). We then explore three approaches to help VLMs approximate spatial mental models, including unseen intermediate views, natural language reasoning chains, and cognitive maps. The significant improvement comes from synergistic approach, map-then-reason, that jointly train the model to first generate cognitive map and then reason upon it. By training models to reason over these internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding reinforcement learning pushed performance even further to 70.7% (+32.9%). Our key insight is that such scaffolding of spatial mental models, actively constructing and utilizing internal structured spatial representations with flexible reasoning processes, significantly improves understanding of unobservable space. Website: https://mll-lab-nu.github.io/mind-cube Code: https://github.com/mll-lab-nu/MindCube Dataset: https://huggingface.co/datasets/MLL-Lab/MindCube Checkpoints: https://huggingface.co/MLL-Lab/models Corresponding: {qinengw, manling.li}@u.northwestern.edu 1. Introduction For Vision-Language Models (VLMs) [1, 2, 3, 4] to move beyond passive perception [5, 6, 7] to interact with partially observable environments [8, 9, 10], it is fundamental to reason about unseen spatial relationships from limited views. Consider how effortlessly human can infer the layout of room or the hidden objects behind furniture, all by integrating information from several egocentric observations. For example, given the second viewpoint in Figure 1, human can easily infer the unseen objects behind the plant are the tissue box and the hand sanitizer, including their position, pose, and their relationship with objects that are not 1 Figure 1 Top: VLMs cannot maintain coherent mental model when evaluating on the D E benchmark. Bottom: We study how we can help VLMs imagine space through external (scaling of views, cognitive map input) and internal strategies (fine-tuning, cognitive map elicitation). We find joint cognitive map and reasoning setting yields the highest gain (+32.86%). : Best within the same elicitation method. : Best performance combination. simultaneously visible. We humans build and update mental model of our surroundings, even when objects are out of sight. This is enabled by core cognitive function referred as spatial mental model [11, 12]: an internal representation of the environment that allows for consistent understanding and inference about space, independent of the current viewpoint. VLMs, despite their impressive progress, struggle to synthesize spatial information from limited views, maintain spatial consistency across views, and reason about objects not directly visible [13, 14, 15, 16]. This gap calls for specialized evaluation settings, which must include: (a) reasoning with partial observations where objects are occluded or out of view (such as hand sanitizerin the second viewpoint in Figure 1), (b) maintaining cross-view consistency across shifting viewpoints (such as through anchor objects plant), and (c) mental simulation to infer hidden spatial relationships (such as what if turning left and moving forward). To fill this gap, we introduce D E, featuring 21, 154 questions and 3, 268 images, organized into 976 multi-view groups through various types of viewpoint transformations (i.e., A N, N G, U in Figure 2). We annotate questions with focus on objects that are not visible in the current query view. 2 As shown in Figure 2, we systematically design question types requiring what-if mental simulations from the given view (such as what if turning to left), perspective taking (such as what if taking the sofas perspective), complex relation reasoning queries (referencing either the agent or other objects). Our extensive evaluations of 17 state-of-the-art VLMs on D E reveal that both open-weight and closed-source models perform only marginally better than random guessing. This poor performance motivates central question: How can we help VLMs reason from partial observations? Inspired by spatial cognition [17, 18, 19] operating through visual imagery, linguistic reasoning, or explicit cognitive maps, to build consistent spatial awareness across different views, we investigate whether intermediate representations can help VLMs approximate mental models through three approaches. View Interpolation generates intermediate views between given observations using recorded video, which unexpectedly is not helpful, highlighting the importance of reasoning directly from limited views. Free-form Natural Language Reasoning verbalizes the mental simulation process, achieving performance gains (+2.7%). Structured Cognitive Map simulates global spatial memory from an allocentric (birds-eye) perspective with orientation and view augmentation. Interestingly, providing ground truth cognitive maps directly to answer questions will not yield strong improvements (5.81%), only actively engaging reasoning with map achieves strong improvements (+3.62%). Despite the effectiveness of reasoning over maps, VLMs exhibit significant bottleneck in their intrinsic ability for accurate mental modeling, evidenced by low Isomorphic Rates (< 10%) with ground truth maps during generation. Recognizing this limitation, we train VLMs by constructing 10, 000 reasoning chains and 10, 000 ground truth cognitive maps, investigating how to effectively guide their thinking process through the injection of these training signals. Self-supervised Finetuning (SFT) on cognitive maps significantly boosts isomorphic similarity to 46.0% from 0.1% for the augmented setting and 73.8% from 7.4% for the plain setting. While SFT on free-form reasoning chains proved more effective with gain of +1.2%, guiding models to first build cognitive maps and then perform free-form reasoning over them achieved significantly better performance, resulting in total gain of +8.5%, proving scaffolding spatial mental models via actively constructing and utilizing internal structured spatial representations with flexible reasoning processes is highly effective. We use Reinforcement Learning (RL) to further boost post-SFT performance, guiding models to think in terms of building and reasoning over cognitive maps by injecting structured thinking before RL training, using our SFT model. This approach leads to significant improvement, raising task accuracy from baseline of 37.8% to 70.7%. Our empirical evidence substantiates critical finding: VLMs exhibit superior performance in spatial reasoning tasks when autonomously generating and leveraging internal mental representations, as compared to conventional approaches such as view interpolation or externally-supplied maps. 2. D E Benchmark and Evaluation 2.1. D E Benchmark Overview. We introduce D E, benchmark for evaluating VLMs spatial reasoning under partial observations and dynamic viewpoints. D E features multi-view image groups paired with spatial reasoning questions, enabling fine-grained analysis of spatial modeling performance. It targets key challenges such as maintaining object consistency across views and reasoning about occluded or invisible elements. Table 1 (left) summarizes the benchmarks 3 Figure 2 D E taxonomy and examples. Left: Three camera movement patterns (R T N, U D, N G) with corresponding spatial QA examples. Right: Fourdimensional taxonomy categorizing D E questions types. overall data distribution. Details on benchmark design, taxonomy, and curation are provided in the Appendix and B. Taxonomy. For fine-grained analysis of VLM spatial reasoning abilities, we introduce taxonomy that systematically categorizes the challenges in D E (visualized in Figure 2). This taxonomy spans five key dimensions: 1) Camera Movement: We mainly collect three types of camera movement: A N (Stays in place but rotates to look around), U (Moves around evaluated objects in circular path), and N (Moves among evaluated objects in circular path). 2) Visual Patterns: This describes the objects spatial configurations, including spatial linear or non-linear arrangements. 3) What-if Dynamics: The hypothetical transformations applied to the agents viewpoint, such as translation, rotation, or their combination (meanwhile and sequence). 4) Relation Query: The type of spatial relation being queried, including agentobject, agentagent, or objectobject. 5) Perspective Taking: Whether the spatial reasoning is grounded in the perceivers own viewpoint (self ) or involves adopting the viewpoint of another entity (other). The detailed definition can be found in the Appendix A.2.2. Dataset Curation. The D E dataset was created through pipeline: We first selected multi-view image groups matching our taxonomys movement patterns (Figure 2) and spatial criteria. These were then annotated with key spatial information. Finally, we algorithmically generated taxonomy-aligned questions with targeted distractors. Details are included in the Appendix A.1. 2.2. Evaluation on D E We evaluate VLMs spatial reasoning on D E using diverse set of models (Table 1, right; setup details in the Appendix B). Results reveal striking performance gap: the best 4 Table 1 Left: D E data statistics. The number next to the setting (R T N, N G, U D) means the total QA pairs. Numbers next to each dataset (e.g., Arkitscenes) mean QA pairs/image groups. For example, 865/53 for Arkitscenes in A N means we have 865 QA pairs and 53 image groups from it. Right: Performance of VLMs on D E. Dark blue indicates the best result among all models and light blue indicates the best result among open-source models. Method Baseline Random (chance) Random (frequency) Open-Weight Multi Image Models LLaVA-Onevision-7B [20] LLaVA-Video-Qwen-7B [21] LongVA-7B [22] mPLUG-Owl3-7B-241101 [23] InternVL2.5-8B [4] Qwen2.5-VL-7B-Instruct [3] Qwen2.5-VL-3B-Instruct [3] Idefics-8B-Llama3 [24] DeepSeek-VL2-Small [25] Gemma-3-12B-it [26] Mantis-8B (SigLip) [27] Proprietary Models GPT-4o [1] Claude-4-Sonnet-20250514 [2] Spatial Models RoboBrain [28] SpaceMantis [29] Spatial-MLLM [30] Space-Qwen [29] Overall Rotation Among Around 32.35 33.02 47.43 41.96 29.46 44.85 18.68 29.26 33.21 35.86 47.62 46.67 41.05 38.81 44.75 37.38 22.81 32.06 33. 36.36 38.30 36.45 35.71 35.89 37.84 36.45 38.76 37.37 35.15 37.00 38.39 37.65 32.65 48.42 35.80 37.65 38.39 38.02 32.29 32.66 48.42 43.55 29.55 47.11 18.20 29.50 33.26 35.94 50.38 48.38 40. 40.17 44.21 38.28 21.26 20.92 33.71 30.66 35.79 44.09 30.12 24.88 26.91 13.11 21.35 30.34 35.49 26.91 34.63 50.99 29.16 47.62 29.53 29.32 32.82 26. model, DeepSeek-VL2-Small, achieves only 47.62% accuracy, well above chance but far from human-level B.3. While some models show strength in specific areasnotably Claude-4-Sonnet in A N (48.42%) and Mantis-8B (SigLip) in U (50.99%)no single model excels across all categories. DeepSeek-VL2-Small performs best on N tasks (50.38%). Interestingly, large proprietary models show mixed results; Claude-4-Sonnet is highly competitive, yet others lag behind leading open-weight models. Spatial fine-tuning also yielded varied outcomes without consistently reaching top performance. Overall, neither multi-image input nor spatial fine-tuning reliably improves spatial reasoning, raising key question: How can we help VLMs develop or approximate these crucial spatial reasoning capabilities? 3. Which Scaffolds Best Guide Spatial Thinking in Unchanged VLMs? To address the identified gap, we first evaluate whether structured data forms can scaffold spatial reasoning in frozen VLMs by approximating spatial mental models under limited views. 3.1. Data Structures as Cognitive Scaffolds for Spatial Mental Models We investigate whether certain data structures can act as cognitive scaffolds that help VLMs form spatial mental models from limited visual observations. In cognitive science, spatial mental models are internal representations encoding the relative configuration of objects and viewpoints. Rather than metric-precise maps, they are schematic, manipulable constructs that support Figure 3 Grounded examples of our three data structures that approximate spatial mental models. reasoning across fragmented observations and unseen perspectives [12, 31, 32, 33]. For instance, humans can mentally simulate turning or infer what lies behind them, suggesting that such representations are flexible, incomplete, yet functionally effective. Drawing on this literature, we define three data structures below (detailed introduction can be found in Appendix C.1), each targeting distinct cognitive properties (integration, transformation, inference) of spatial mental models, with grounded examples in Figure 3: 1. View Interpolation. Interpolating between sparse camera views introduces perceptual continuity, echoing the process of mental animation [34] and supporting internal transformation such as imagined rotation. This structure scaffolds the dynamic updating capability of spatial mental models. Figure 3 shows one-frame inserting example that replaces the original question images. 2. Augmented Cognitive Map. cognitive map is 2D schematic representation of object layouts in space. Such maps resemble Tverskys cognitive collages [31], and they capture locally coherent but fragmented structures. Recent studies [8, 35] on VLM-based spatial intelligence typically adopt plain form that only encodes object positions in top-down view. We propose an augmented variant that incorporates discrete views, with both objects and views annotated by position and orientation, thereby approaching the relational consistency of spatial mental models. 3. Free Form Reasoning. Open-ended, step-by-step natural language reasoning offers procedural approximation of how spatial models are constructed and queried. While less rigid than map-like structures, such reasoning reflects the inferential function of spatial mental models, especially under ambiguous or incomplete observations [32]. 3.2. Experiment Setup We conduct controlled experiments with fixed input formats to test whether structured scaffolds can help without retraining. Each condition introduces different structure to support internal modeling under limited views. Model and Evaluation Data We conduct all experiments using Qwen2.5-VL-3B-Instruct [3]. Our evaluation is performed on D E - Y, diagnostic subset sampled from D E, containing 1,050 questions in total. Detailed statistics are: 600 from the N G, 250 from U D, and 200 from A N. Configurations Each experiment is defined by two orthogonal axes: Input Structure (what spatial evidence VLMs receive) and Output Format (the required response type). As the experimental 6 Table 3 Left: QA accuracy (%) of Qwen2.5-VL-3B-Instruct on the D E - Y benchmark under different configs for frozen VLMs. Right: Graph metrics for two cog map output settings. Config. Raw-QA VI-1 VI-2 Aug-CGMap-In FF-Rsn Aug-CGMap-FFR-Out Plain-CGMap-FFR-Out CGMap-In-FFR-Out Overall Rotation Among Around 37. 37.90 37.81 32.00 40.48 40.57 41.33 41.43 34.00 35.50 35.50 35.00 32.00 21.00 25.00 37.00 36. 37.33 36.50 30.50 36.00 43.00 39.67 41.67 45.20 41.20 42.80 33.20 58.00 50.40 58.40 44.40 foundation of this paper, we begin with the ten possible configurations listed in Table 2, from which we investigate representative subset. Specifically, our grounded cognitive maps are generated using the object arrangements annotation described in Section 2.1, and examples for all configurations are provided in the Appendix C.3. In the frozen VLMs evaluation setup, we exclude the Aug-CGMap-Out and Plain-CGMap-Out settings, as VLMs tend to conflate map generation with reasoning, even when instructed otherwise. Name Table 2 Abbreviations for the ten input-output configurations across all experiments in this work. VI = View Interpolation, CGMap = Cognitive Map, Aug = Augmented (objects + camera included), FF-Rsn and FFR = Free-Form Reasoning. QA = Raw views + question. Evaluation Metrics We evaluate task performance using QA accuracy. For generated cognitive maps, we introduce set of well-defined graph met- (1) Valid Cognirics: tive Map Rate, indicating whether the output conforms to the expected schema; (2) Overall Similarity, weighted score combining directional and facing consistency; and (3) Isomorphic Rate, measuring whether all pairwise object relations match the ground truth under optimal alignment. Full definitions are provided in the Appendix C.2. Raw-QA Raw views + question VI-1 Raw + 1 interp. view VI-2 Raw + 2 interp. views FF-Rsn Raw views + question Aug-CGMap-In Aug. cog. map + QA Aug-CGMap-Out Raw views + question Plain-CGMap-Out Raw views + question Aug-CGMap-FFR-Out Raw views + question Plain-CGMap-FFR-Out Raw views + question CGMap-In-FFR-Out Aug. cog. map + QA Direct answer Direct answer Direct answer Reasoning answer Direct answer Aug. map ans Plain map ans Aug. map + rsn ans Plain map + rsn ans Reasoning answer Input Structure Output Format 3.3. Do Scaffolds Improve Spatial Reasoning Without Training? We evaluate how well the seven input configurations defined in Table 2 support spatial reasoning in VLMs under limited views, without any model updates. Results are shown in Table 3 (left). How far can structure alone go? We begin with the baseline: raw input views and direct answering (Raw-QA), which achieves 37.81% accuracy. Adding interpolated views, which we hope to simulate smoother perceptual transitions, leads to no meaningful gain ( 0.09%). Similarly, providing pre-computed augmented cognitive map as direct input (Aug-CGMap-In) severely degrades performance to 32.00%. In contrast, enabling free-form reasoning (FF-Rsn) alone or com7 bined with other settings (Aug-CGMap-FFR-Out, Plain-CGMap-FFR-Out, CGMap-In-FFR-Out) provides substantial boost to 41.33%. These results suggest: structure alone, whether visual or spatial, is not enough. Without engaging reasoning, VLMs struggle to leverage even well-formed spatial cues. Can we prompt the model to think spatially? The answer appears to be yes. Prompting the model to generate cognitive map (Aug-CGMap-FFR-Out, Plain-CGMap-FFR-Out) before answering leads to further improvements over free-form reasoning alone (FF-Rsn) from 40.48% to 41.43%. This suggests that generating map may encourage the model to first form global understanding of the scene, which in turn supports more structured reasoning. Both map forms have great format-following ability, yet fail to generate accurate maps. Overall, augmented maps perform worse. In Table 3 (Right), despite generating syntactically valid maps for both formats, similarity to grounded maps is low (< 50%), reflecting limited mapping ability. Notably, both augmented and plain maps have low isomorphism rates (0.10%, 7.43%). The reason that the isomorphic rate for augmented map setting is nearly zero is likely because the added view-level details increase generation errors. Detailed case examples can be found in the Appendix D. (cid:17) Key Takeaways: Scaffolding Spatial Reasoning in Frozen VLMs Explicit reasoning is crucial for improving performance. Cognitive maps can help guide the reasoning process. Passive structures (like maps as input) alone and visual continuity offer little benefit. Table 4 QA accuracy (%) and cognitive map generation quality of Qwen2.5-VL-3B-Instruct under SFT configurations on D E - Y. Both FF-Rsn and FFR refer to free-form reasoning. SFT Config. Raw-QA FF-Rsn Aug-CGMap-Out Plain-CGMap-Out Aug-CGMap-FFR-Out Plain-CGMap-FFR-Out D E - Y QA Accuracy (%) Generated Cognitive Map (%) Overall Rotation Among Around Valid Rate Overall Sim. Isom. Rate 52.28 53.52 54.19 54.38 55.24 60.76 34. 36.00 35.50 35.50 49.50 47.50 52.50 54.67 53.17 53.50 52.50 62.33 66. 64.80 71.60 71.60 66.40 67.60 100.00 100.00 100.00 100.00 74.30 91.73 75.27 88.79 43.24 89.05 46.00 73.81 4. Can We Teach VLMs to Build and Leverage Spatial Representations? So far, prompting frozen VLMs with external scaffolds, such as interpolated views or cognitive maps, has yielded limited gains. These techniques fail to tackle the core limitation: VLMs do not form internal spatial representations or reason through space effectively. To go further, we want to know: Can supervised fine-tuning (SFT) teach VLMs to build and leverage spatial models from within? 4.1. Designing Robust Experimental Framework To ensure consistency and comparability, we inherit experimental configurations detailed in Sections 3.1 and 3.2. Specifically, we retain: (1) the two effective data structuresCognitive Maps (Object-only / Object + Camera) and Free-Form Reasoning, (2) the base model Qwen2.5-VL-3BInstruct, (3) the evaluation benchmark D E - Y, and (4) all established evaluation metrics. View interpolation is excluded from our fine-tuning experiments due to its limited per8 formance gains in earlier validations. Primary modifications in this SFT phase include adjusted training hyperparameters (detailed in the Appendix E.2) and the input-output configurations. SFT Task Configurations Drawing on insights from Section 3.3, we use selected configurations from Table 2 to evaluate the incremental impact of cognitive map generation and free-form reasoning in SFT. These include baseline QA without explicit reasoning (Raw-QA), reasoning guided by generated maps only (Plain-CGMap-Out, Aug-CGMap-Out), reasoning-augmented prompts (FF-Rsn), and fully integrated setup that asks VLMs to generate both maps and reasoning (Aug-CGMap-FFR-Out and Plain-CGMap-FFR-Out). Grounded Cognitive Maps Generation Grounded cognitive maps are not only used as the input in the Aug-CGMap-In and CGMap-In-FFR-Out setting for the frozen VLMs in the Section 3.2, but also as the training and comparison data in SFT. We curate such grounded cognitive maps through template-based method, where we always select the front image in our annotation as the up direction. Detailed annotation algorithm can be found in the Appendix E.1.1. Grounded Free-Form Reasoning Chain Generation We design grounded reasoning chains using detailed image annotations and structured question templates. Chains are manually constructed via template-based method, ensuring logical coherence and clear grounding in observable spatial relations (see an example in Figure 3). This yields precise, interpretable supervision signals that help VLMs learn robust spatial reasoning representations. The detailed grounded reasoning data generation pipeline is shown in the Appendix E.1.2. (a) QA accuracy (%) (b) Isomorphic Rate (%) (c) Overall Similarity (%) Figure 4 SFT per 5 step training performance on task accuracy and graph metrics. 4.2. Do VLMs Truly Benefit from Explicit Training in Spatial Reasoning? We explore several SFT configurations (results shown in Table 4), guided by series of core questions. Fine-tuning directly on raw QA pairs, without spatial supervision, raises accuracy from 37.81% to 52.28%. This suggests VLMs can absorb some spatial cues from QA data alone. We use this setup as the baseline for evaluating methods that explicitly incorporate spatial structures. Can structured approximations of mental models alone meaningfully improve performance? As shown in Table 2, supervised fine-tuning on explicit cognitive maps, either Augmented or Plain, leads to substantial improvements in graph structure quality, with more than 30% gains in both overall similarity and Isomorphic rate. However, the effect on end-task accuracy remains limited. Both augmented maps (54.19%) and Plain maps (54.38%) offer only modest gains over the fine-tuned Raw-QA (52.28%). Similarly, directly FF-Rsn also yields marginal gain (51.09%). This means that scaffold alone is not sufficient to automatically translate into performance gains. Generating both cognitive maps and free-form reasoning is the most effective approximation. 9 Among all configurations, the combination of generating plain map and then reasoning (Plain-CGMap-FFR-Out) yields standout performance gain (60.76%, 8.48% compared to Raw QA-SFT), surpassing models that rely on only map generation or reasoning alone. This suggests strong synergy between structured spatial modeling and natural language inference. Why does this combination work so well? First, improvements in task accuracy are accompanied by highquality spatial representations. For instance, the top-performing Plain-CGMap-FFR-Out model achieves excellent map quality (88.79% similarity and 73.81% isomorphism), demonstrating strong link between its enhanced accuracy and its internal spatial model. Furthermore, the training dynamics reveal crucial trade-off that explains this synergy. As shown in Figure 4 (b, c), models trained solely on map generation (Plain-CGMap-Out) learn the target structure very rapidly, quickly reaching near-perfect similarity and isomorphism. However, their QA accuracy soon plateaus (Figure 4a), suggesting the model learns the structure without fully grasping its functional utility. In sharp contrast, the top-performing Plain-CGMap-FFR-Out model learns the map structure more slowly and never reaches the same level of structural perfection. Yet, its QA accuracy continues to climb, ultimately surpassing all other configurations. This suggests that the joint pressure of the reasoning task forces the model not just to replicate structure, but to build functionally effective spatial representation, one that is optimized for downstream inference. (cid:17) Key Takeaways: Teaching VLMs to Reason Spatially Joint cogmap and reasoning setting yields optimal performance through synergistic effects. Reasoning shapes spatial representations for functional utility, not just structural perfection. Neither map generation nor reasoning alone largely outperforms the SFT QA baseline. 5. Can Reinforcement Learning Further Refine Spatial Thought Processes? While SFT establishes strong baseline for spatial reasoning, emerging evidence from models like DeepSeek R1 [36, 37] suggests reinforcement learning (RL) can offer additional gains by optimizing behavior through outcome-driven feedback. We ask: Can reward-guided refinement help VLMs build sharper spatial models and reason more effectively? 5.1. Experimental Setup We employ the VAGEN framework [38] for VLM policy optimization, using Group Relative Policy Optimization (GRPO) [39] as our core algorithm. To manage compute cost, we train each configuration for only 0.5 epoch. For fair comparison, the RL setup retains all key components from the SFT stage, including the base model, spatial input formats, benchmark dataset (M C - Y), and evaluation metrics, as detailed in Sections 3.1 and 3.2. Additional details appear in the Appendix F.1. Task Configurations and Reward Design. We evaluate three RL variants: (1) RL-FF-Rsn (from scratch), which trains Qwen2.5-VL-3B-Instruct to produce free-form reasoning chains; (2) RL-Aug-CGMap-FFR-Out (from scratch), which trains the model to jointly generate cognitive maps and reasoning; and (3) RL-Aug-CGMap-FFR-Out (from SFT), which initializes from the strongest SFT checkpoint. The reward function is sparse but targeted: +1 for structurally valid outputs, and +5 for correct answers. 10 Table 5 QA accuracy (%) and cognitive map generation quality of Qwen2.5-VL-3B-Instruct under various RL configurations on D E - Y. RL Config. N B - Y QA Accuracy (%) Generated Cognitive Map (%) Overall Rotation Among Around Valid Rate Overall Sim. Isom. Rate RL-FF-Rsn (from scratch) RL-Aug-CGMap-FFR-Out (from scratch) RL-Plain-CGMap-FFR-Out (from scratch) RL-Aug-CGMap-FFR-Out (from SFT) RL-Plain-CGMap-FFR-Out (from SFT) 50.57 52.19 53.71 70.67 70.67 36.50 32.00 33. 53.00 48.00 49.33 52.00 53.66 76.83 79.17 64.80 68.80 70. 70.00 68.40 99.90 100.00 100.00 100.00 57.03 47. 85.53 85.79 0.00 10.29 58.86 71.52 5.2. Can Reinforcement Learning Unleash the Power of Approximating Spatial Mentaling? Reinforcement learning (RL) lets model feel the consequences of its spatial thoughts through reward, but does that feedback alone forge genuine mental map, or must we first teach the model what map looks like? Table 5 summarizes three key settings and answers the question in two parts. RL in vacuum is not enough. Training from scratch with sparse rewards provides insufficient guidance for building robust spatial representations. When asked to produce free-form reasoning (RL-FF-Rsn (from scratch)), the model achieves only 50.57% overall accuracy. This result, while an improvement over initial baselines, confirms that task-level rewards alone are too unstructured to effectively teach spatial abstraction. Structured outputs provide modest benefits when learned from scratch. Introducing cognitive map structure for the policy to generate offers scaffold for its reasoning. When starting from scratch, the simpler RL-Plain-CGMap-FFR-Out configuration (53.71%) slightly outperforms its augmented counterpart (52.19%) in QA accuracy. However, in both cases, the model fails to learn meaningful geometry, with low similarity scores and near-zero isomorphism rates. This suggests that without prior concept of \"good\" map, RL struggles to exploit the provided structural format, even if it can learn to fill it out validly. RL shines when it stands on an SFT-built scaffold. The most dramatic improvements occur when warm-starting RL from an optimal SFT checkpoint. Both RL-Plain-CGMap-FFR-Out (from SFT) and its augmented version reach an identical, impressive 70.67% overall QA accuracy. This represents significant 9.91% absolute gain over the best SFT model and 16.96% gain over the best RL-from-scratch approach. Crucially, while both models achieve the same peak accuracy, their underlying spatial representations differ. The Plain-CGMap variant produces geometrically superior maps, with higher overall similarity (85.79%) and much higher isomorphism rate (71.52% vs. 58.86%). This suggests that while RL fine-tuning can guide different initial models to the same reasoning proficiency, starting with cleaner, simpler SFT scaffold (Plain) allows RL to better preserve and polish geometrically sound internal map. These results strongly indicate that RLs primary role here is (1) polishing and refining the strong priors learned during SFT, and (2) raising the performance ceiling of SFT, enabling the model to break through previous plateaus to achieve near-oracle-level performance. (cid:17) Key Takeaways: Reinforcement Learning for Spatial Reasoning Combining cognitive maps with reasoning consistently improves all learning outcomes. Starting from scratch, RL provides only marginal gains for spatial reasoning; its true power is unlocked when building upon strong SFT foundation. 11 6. Related Works Spatial Cognition. Spatial cognition encompasses skills like mental rotation, spatial visualization and object assembly, essential for perceiving and manipulating spatial relationships in both 2D and 3D environments [40, 19, 41]. At the core of these abilities are Spatial Mental Models (SMMs) [11, 12], which are internal representations that allow for consistent understanding about space. Recently, much effort has been dedicated to evaluating spatial cognition in VLMs [42, 13, 18, 43]. Moreover, some methods are proposed to enhance spatial understanding such as coordinate-aware prompting [44], CoT reasoning [10, 45], explicit spatial representation alignment [46, 29], and RL-based approach [47]. However, existing benchmarks [18, 42, 48, 49, 43, 13, 17, 50, 51, 8, 52] and approaches often neglect the mental-level spatial reasoning that underpins human cognition, leaving gap between machine and human capabilities. To bridge this gap, new approach is needed that trains VLMs to reason about space not only through visual data but also through mental-level spatial reasoning, aligning more closely with human spatial cognition. Multi Views understanding. Multiview spatial understanding leverages multiple viewpoints to reconstruct 3D structures and overcome single-view limitations. Efficient techniques optimize view processing, while reconstruction methods[53, 54, 55, 56], view synthesis methods[57, 58, 59] and multiview equivariant learning[60] enhance geometric consistency. Topological representations like [61] encode object relations for holistic reasoning, while frameworks such as [62] advance open-vocabulary concept learning from multiview data via neural fields and visionlanguage fusion. LMMs augmented with multiview inputs[63, 30, 64, 65, 18, 66, 67] demonstrate marked improvements in spatial tasks like geometric understanding and perspective taking, yet they still struggle with multiview consistency understanding due to fragmented reasoning and 2D-to-3D projection ambiguities, leaving key gap for robust spatial AI. 7. Conclusion and Future Impact We introduced D E to study how VLMs can approximate spatial mental models from limited views, core cognitive ability for reasoning in partially observable environments. Moving beyond benchmarking, we explored how internal representations can be scaffolded through structured data and reasoning. Our key finding is that constructing and reasoning over self-generated cognitive maps, rather than relying on view interpolation or externally provided maps, yields the most effective approximation of spatial mental models across all elicitation methods (input-output configurations, supervised fine-tuning, and reinforcement learning). Initializing RL from well-trained SFT checkpoint further optimizes the process, pushing spatial reasoning performance to new limits. Future Impact. Our work establishes that combining cognitive map generation with reasoning to model spatial information is the most effective. We believe that once high-quality SFT datasets for cogmap generation and reasoning are established, RL can be leveraged to further push the performance boundaries. We anticipate the exploration of novel training paradigms designed to unlock even greater synergistic effects and thus achieving \"1+1 > 2\" impact on spatial intelligence."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Hello gpt-4o. Blog, 05 2024. Accessed: November 22, 2024. [2] Anthropic. Claude 4 sonnet system card, May 2025. Version 20250514, accessed 2025-06-23. 12 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [5] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. [6] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning, 2022. [7] Renjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang. Image textualization: An automatic framework for creating accurate and detailed image descriptions. arXiv preprint arXiv:2406.07502, 2024. [8] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces, 2024. [9] Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities, 2025. [10] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jieneng Chen, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning, 2025. [11] Philip Johnson-Laird. Mental models in cognitive science. Cognitive science, 4(1):71115, 1980. [12] Philip Nicholas Johnson-Laird. Mental models: Towards cognitive science of language, inference, and consciousness. Number 6. Harvard University Press, 1983. [13] Wufei Ma, Haoyu Chen, Guofeng Zhang, Yu-Cheng Chou, Celso de Melo, and Alan Yuille. 3dsrbench: comprehensive 3d spatial reasoning benchmark, 2025. [14] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. From flatland to space: Teaching vision-language models to perceive and reason in 3d, 2025. [15] Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso de Melo, Jieneng Chen, and Alan Yuille. Spatial457: diagnostic benchmark for 6d spatial reasoning of large multimodal models, 2025. [16] Yiqi Zhu, Ziyue Wang, Can Zhang, Peng Li, and Yang Liu. Cospace: Benchmarking continuous space perception ability for vision-language models, 2025. [17] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models?, 2025. [18] Phillip Y. Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. Perspective-aware reasoning in vision-language models via mental imagery simulation, 2025. [19] Jirong Zha, Yuxuan Fan, Xiao Yang, Chen Gao, and Xinlei Chen. How to enable llm with 3d capacity? survey of spatial reasoning in llm, 2025. 13 [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [22] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [23] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. [24] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. In Workshop on Responsibly Building the Next Generation of Multimodal Foundational Models, 2024. [25] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. [26] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [27] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. Transactions on Machine Learning Research, 2024, 2024. [28] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. [29] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. arXiv preprint arXiv:2401.12168, 2024. [30] Diankun Wu, Fangfu Liu, Yi-Hsin Hung, and Yueqi Duan. Spatial-mllm: Boosting mllm capabilities in visual-based spatial intelligence. arXiv preprint arXiv:2505.23747, 2025. [31] Barbara Tversky. Cognitive maps, cognitive collages, and spatial mental models. In European conference on spatial information theory, pages 1424. Springer, 1993. [32] Barbara Tversky, Nancy Franklin, Holly Taylor, and David Bryant. Spatial mental models from descriptions. Journal of the American society for information science, 45(9):656668, 1994. [33] Barbara Tversky. Structures of mental spaces: How people think about space. Environment and behavior, 35(1):6680, 2003. [34] Mary Hegarty. Mental animation: Inferring motion from static displays of mechanical systems. Journal of experimental psychology: learning, memory, and cognition, 18(5):1084, 1992. [35] Chun-Hsiao Yeh, Chenyu Wang, Shengbang Tong, Ta-Ying Cheng, Rouyu Wang, Tianzhe Chu, Yuexiang Zhai, Yubei Chen, Shenghua Gao, and Yi Ma. Seeing from another perspective: Evaluating multi-view understanding in mllms. arXiv preprint arXiv:2504.15280, 2025. [36] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 14 [37] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. [38] Kangrui Wang*, Pingyue Zhang*, Zihan Wang*, Yaning Gao*, Linjie Li*, Qineng Wang, Hanyang Chen, Chi Wan, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, and Manling Li. Reinforcing visual state reasoning for multi-turn vlm agents, 2025. [39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [40] Wenrui Xu, Dalin Lyu, Weihang Wang, Jie Feng, Chen Gao, and Yong Li. Defining and evaluating visual language models basic spatial abilities: perspective from psychometrics, 2025. [41] Wenqi Wang, Reuben Tan, Pengyue Zhu, Jianwei Yang, Zhengyuan Yang, Lijuan Wang, Andrey Kolobov, Jianfeng Gao, and Boqing Gong. Site: towards spatial intelligence thorough evaluation, 2025. [42] Weichen Zhan, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, and Xiao-Ping Zhang. Open3dvqa: benchmark for comprehensive spatial reasoning with multimodal large language model in open space, 2025. [43] Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Allison Koenecke, Boyang Li, and Lu Wang. Sphere: Unveiling spatial blind spots in vision-language models through hierarchical evaluation, 2025. [44] Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. arXiv preprint arXiv:2406.13642, 2024. [45] Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Helong Huang, Guangjian Tian, Weichao Qiu, Xingyue Quan, Jianye Hao, and Yuzheng Zhuang. Spatialcot: Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning, 2025. [46] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language models, 2024. [47] Zhenyu Pan and Han Liu. Metaspatial: Reinforcing 3d spatial reasoning in vlms for the metaverse. arXiv preprint arXiv:2503.18470, 2025. [48] Shiqi Chen, Tongyao Zhu, Ruochen Zhou, Jinghan Zhang, Siyang Gao, Juan Carlos Niebles, Mor Geva, Junxian He, Jiajun Wu, and Manling Li. Why is spatial reasoning hard for vlms? an attention mechanism perspective on focus areas, 2025. [49] Jianing Qi, Jiawei Liu, Hao Tang, and Zhigang Zhu. Beyond semantics: Rediscovering spatial awareness in vision-language models, 2025. [50] Yihong Tang, Ao Qu, Zhaokai Wang, Dingyi Zhuang, Zhaofeng Wu, Wei Ma, Shenhao Wang, Yunhan Zheng, Zhan Zhao, and Jinhua Zhao. Sparkle: Mastering basic spatial capabilities in vision language models elicits generalization to spatial reasoning, 2025. [51] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. [52] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. In Thirty-Eighth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 15 [53] Jianyuan Wang et al. Vggt: Visual geometry grounded transformer for universal 3d reconstruction. In CVPR, 2025. [54] Deku Liu, Yihan Zhang, Zhe Chen, et al. Citygaussianv2: Efficient and geometrically accurate reconstruction for large-scale scenes. In ICLR, 2025. [55] Chuanyu Fu, Guanying Chen, et al. Maskgaussian: Differentiable mask pruning for efficient 3d gaussian rendering. In CVPR, 2025. [56] Yansong Qu, Jie Wang, et al. Drag your gaussian: Effective drag-based editing with score distillation for 3d gaussian splatting. In SIGGRAPH Asia, 2025. [57] Shao-Hua Sun, Minyoung Huh, Yuan-Hong Liao, Ning Zhang, and Joseph Lim. Multi-view to novel view: Synthesizing novel views with self-learned confidence. In ECCV, 2018. [58] Yuxuan Zhang, Yifan Yang, Jing Zhang, Yifang Wang, Yijun Zhang, and Ming-Hsuan Yang. In ECCV, Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. 2024. [59] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. Zeronvs: Zero-shot novel view synthesis from single real image. arXiv:2310.17994, 2023. [60] Yang You, Yixin Li, Congyue Deng, Yue Wang, and Leonidas Guibas. Multiview equivariance improves 3d correspondence understanding with minimal feature finetuning, 2024. [61] Juexiao Zhang, Gao Zhu, Sihang Li, Xinhao Liu, Haorui Song, Xinran Tang, and Chen Feng. Multiview scene graph, 2024. [62] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images, 2023. [63] Erik Daxberger, Nina Wenzel, David Griffiths, Haiming Gang, Justin Lazarow, Gefen Kohavi, Kai Kang, Marcin Eichner, Yinfei Yang, Afshin Dehghan, and Peter Grasch. Mm-spatial: Exploring 3d spatial understanding in multimodal llms, 2025. [64] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Tianlong Chen, Jiachen Li, Zhengzhong Tu, Zhangyang Wang, and Rakesh Ranjan. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction, 2025. [65] Duo Zheng, Shijia Huang, Yanyang Li, and Liwei Wang. Learning from videos for 3d world: Enhancing mllms with 3d vision geometry priors, 2025. [66] Baining Zhao, Ziyou Wang, Jianjie Fang, Chen Gao, Fanhang Man, Jinqiang Cui, Xin Wang, Xinlei Chen, Yong Li, and Wenwu Zhu. Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning, 2025. [67] Runsen Xu, Weiyao Wang, Hao Tang, Xingyu Chen, Xiaodong Wang, Fu-Jen Chu, Dahua Lin, Matt Feiszli, and Kevin J. Liang. Multi-spatialmllm: Multi-frame spatial understanding with multi-modal large language models, 2025. [68] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [69] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision, 2023. [70] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2237822389, 2024. [71] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [72] Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. Rethinking the bounds of llm reasoning: Are multi-agent discussions the key? arXiv preprint arXiv:2402.18272, 2024. [73] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. [74] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, pages 84698488, 2023. [75] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: arXiv preprint Composable 3d value maps for robotic manipulation with language models. arXiv:2307.05973, 2023. [76] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. [77] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37:100428100534, 2024. [78] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, et al. Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [79] Yihe Tang, Wenlong Huang, Yingke Wang, Chengshu Li, Roy Yuan, Ruohan Zhang, Jiajun Wu, and Li Fei-Fei. Uad: Unsupervised affordance distillation for generalization in robotic manipulation. arXiv preprint arXiv:2506.09284, 2025. [80] Jensen Jinghao Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025. [81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022."
        },
        {
            "title": "Table of Contents",
            "content": "A D E Benchmark 19 A.1 Details for Data Collection and Annotation . . . . . . . . . . . . . . . . . . . . . 19 A.2 Details of ourM C Benchmark . . . . . . . . . . . . . . . . . . . . . . . 21 A.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evaluation on D E 25 B.1 Prompt Templates for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.2 Details in text only evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 B.3 Human Evaluation . B.4 Evaluation Setup . . B.5 Analysis in settings . B.6 Failure case analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Data Structures as Cognitive Scaffolds, Evaluation Metrics, and Input-Output Configurations 32 C.1 Data Structures as Cognitive Scaffolds . . . . . . . . . . . . . . . . . . . . . . . . 32 C.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 C.3 Prompts for All Input-Output Configurations . . . . . . . . . . . . . . . . . . . . 39 . . Which Scaffolds Best Guide Spatial Thinking in Unchanged VLMs? 50 D.1 VLM Response Examples for Configurations in Section C.3 . . . . . . . . . . . . 50 D.2 Additional Graph Metrics for Generated Graphs . . . . . . . . . . . . . . . . . . 52 Can We Teach VLMs to Build and Leverage Spatial Representations? 52 E.1 Supervised Fine-Tuning Data Curation . . . . . . . . . . . . . . . . . . . . . . . . E.2 Detailed Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 E.3 VLM Response Examples After SFT for Configurations in Section C.3 . . . . . . 59 E.4 Detailed Graph Metric Results for SFT Graph-Related Experiments . . . . . . . 62 E.5 Which Part of VLM is the Bottleneck for Spatial Understanding? . . . . . . . . . 63 E.6 Branching from Raw-QA SFT Checkpoint . . . . . . . . . . . . . . . 64 . . . . . . . Can Reinforcement Learning Further Refine Spatial Thought Processes? 65 F.1 Detailed Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 F.2 VLM Response Examples After RL for Configurations in Section C.3 . . . . . . 66 18 A. D E Benchmark A.1. Details for Data Collection and Annotation Image Collection and Selection. Our D E benchmark comprises 3, 268 images (2,302 indoor/outdoor images from publicly released dataset and 400 self-collected images), where we implement comprehensive image selection methodology encompassing four distinct view dynamics, incorporating various data sources and processing procedures, as shown in Fig.2. For rotation view dynamics, we implement three-stage filtering strategy to extract meaningful camera trajectories and key frames from ArkitScenes [68] dataset. In the first stage, we analyze the top-down view of camera poses within each scene to identify two types of trajectories: linear paths and small rotational arcs. linear trajectory is characterized by consistently oriented cameras exhibiting significant displacement perpendicular to their viewing direction. rotational arc trajectory is identified when three to four camera positions demonstrate approximately 90-degree relative orientation changes while being distributed along an approximate circular arc. The second stage focuses on selecting two critical frames from the (a) camera pose (b) camera pose 2 Figure 1 Examples of camera poses in ArkitScenes previously identified translation segments. The selection criteria mandate that: (1) the camera movement direction must be parallel to the object arrangement direction, (2) this movement should be aligned with the horizontal axis, (3) the first frame should only capture objects and B, while the second frame should only capture objects and C, and (4) both frames must be free from motion blur and exhibit clear object visibility. The third stage processes the rotation segments to extract three or four key frames. These frames must satisfy several conditions: (1) the camera positions should appear to originate from stationary rotating camera, even if slight circular movement exists, (2) the camera orientations should align with standard cardinal directions (approximately 90 degrees apart), and (3) each frame should contain no more than three semantically distinct primary objects that occupy over 50% of the frame area relative to the background. For among view dynamics, image groups are manually selected from DL3DV-10K[69] and WildRGB-D[70] datasets. We employ single-stage selection process to identify four key frames representing cardinal viewpoints (front, left, right, and back) from 360-degree scene captures. The selection criteria are: (1) camera orientations must align with standard directions, ensuring that the central object, its background objects, and the cameras line of sight are collinear and parallel or perpendicular to standard scene elements such as tables or walls, (2) we reject sets where three or more frames share identical semantic background information, and (3) we discard sets where three or more frames have severely occluded background objects that cannot be reconstructed from information in the other frames. For around view dynamics, image groups are manually curated from the DL3DV-10K[69] dataset and assigned sequential identifiers. The front view (designated as view 1) must provide clear visibility of all relevant information. This view is established as the reference point for subsequent views in the sequence. This structured approach to image selection and processing yields rich dataset that supports subsequent model training and testing procedures. The methodology ensures comprehensive coverage of spatial relationships, occlusion states, and view-dependent object characteristics across multiple viewing scenarios. Figure 2 D E Bench construction pipeline. Data Annotation. After collecting and filtering the images, we follow two-phase paradigm for annotation: We establish systematic image annotation protocol to ensure data consistency and accuracy. The annotation framework encompasses four key dimensions: spatial relationship identification, object grouping rules, semantic orientation determination, and occlusion level assessment. We provide pdf of the annotation interface in the supplementary material. Regarding spatial relationship identification, annotators are required to identify primary object entities within images and determine their spatial relationships. These relationships are primarily categorized into two types: front-back relationships typically involving two primary objects, with priority given to objects directly behind as key entities; and left-right relationships encompassing two to four primary objects, where adjacent objects with front-back relationships can be considered as unified entity. To enhance annotation efficiency and semantic completeness, this study introduces object grouping rules. Multiple objects can be annotated as unified entity when they collectively form clear spatial relationships with other primary objects. Each object may include attribute descriptors (e.g., color, material) to enhance semantic expression. Combined object entities must maintain distinct spatial relationships with other primary objects. 20 For objects with definitive semantic fronts, the following information must be recorded: the objects inherent semantic front, the objects orientation relative to the current viewpoint (aligned, reversed, leftward, rightward, etc.), and the objects actual projected direction within the scene. Occlusion levels are evaluated using four-tier classification system: complete occlusion where the object is entirely invisible from the current viewpoint; major occlusion where primary object features are difficult to identify; minor occlusion where primary object features remain identifiable; and no occlusion where the object is fully visible. For cases of complete occlusion, the annotation system provides multi-view scene images, ensuring object visibility in at least one viewpoint to support subsequent cross-view question-answering system training. This annotation protocol provides structured semantic foundation for subsequent automated question-answer pair generation while ensuring data quality and consistency. Through this standardized annotation process, we effectively capture key information including spatial relationships, compositional features, semantic orientations, and occlusion states of objects within scenes. Examples for automatic QA generation pipeline. Our automatic QA generation pipeline Figure 3 Example of different question-related label combinations to generate QA pairs. generates different types of questions using combinations of labels. Each questions label combination is encoded in its ID (e.g., \"among_group001_q1_1_1\"), while the original object and label information is preserved in the meta_info field to track the context of question generation. A.2. Details of ourM C Benchmark A.2.1. Three kinds of invisibility settings Rotation. In this setting, our camera remains stationary while rotating in place, capturing 2 to 4 orthogonal views. In each view, central object remains visible in the foreground, while all views maintain equal importance in the spatial representation. We evaluate models understanding of spatial invisibility by asking questions such as When 21 positioned at particular viewpoint, what should be to your left or right (given that each view only reveals whats directly ahead)? or After rotating quarter or half turn, what objects would be in front of you, to your left, behind you, or to your right? We expect models to construct comprehensive spatial understanding by leveraging the sequential nature of the views and consistent spatial cues across images (such as lighting direction), thereby demonstrating their ability to reason about the complete environment despite only having access to partial visual information from each viewpoint. Around. In this setting, we leverage occlusion phenomena to force MLLMs beyond simple 2D spatial recognition. When viewing objects from different angles, some objects become partially or fully hidden, requiring models to: Maintain object permanence despite partial visibility Transform lateral relationships (left-right) from frontal views into depth relationships (front-back) for side views Integrate spatial information across multiple viewpoints to form coherent 3D understanding This approach prevents models from relying solely on direct visual cues and instead necessitates true 3D spatial reasoning by combining information from multiple perspectives. Among. In this setting, the camera rotates around central object, positioned between this central object and several surrounding objects. Four orthogonal views are captured, with each view showing the central object positioned in front of one of the surrounding objects. This setup creates interesting visibility constraints across different perspectives. For instance, surrounding object visible in one view may be invisible in another view because of the constraints imposed by the cameras field of view. Through establishing consistency relationships between these views, we can infer the relative positions of objects not directly visible from certain perspectives. When an object is not visible from particular viewpoint, consistency and spatial reasoning can determine its position relative to the central object. All views hold equal status in this framework, allowing for bidirectional establishment of invisibility relationships. This creates coherent spatial reasoning system where information from each perspective contributes to complete understanding of the three-dimensional arrangement, even when direct visual confirmation is unavailable from certain angles. A.2.2. Label taxonomy We use image related labels for better analysis and question related labels for automatic QA generation with different label combinations. Visual Patterns. In our taxonomy of spatial configurations, we classify visual patternss into distinct categories based on their geometric relationships. Linear arrangements refer to configurations where objects are positioned along single axis, forming collinear pattern. Non-linear arrangements, conversely, are characterized by objects positioned such that the connecting lines between adjacent pairs form 90-degree angles, creating rectilinear patterns. This binary classification serves as fundamental attribute in our spatial relationship labeling scheme, enabling precise description and analysis of scene compositions across various domains. 22 What if Dynamics. What if Dynamics refers to the models capability to comprehend and reason about dynamic perspective changes occurring within images or posed questions. We conceptualize viewpoint transitions as combinations of translation and rotation operations, resulting in four distinct categories: Pure Translation: Cases where the viewpoint undergoes only translational movement without rotational change. Pure Rotation: Scenarios involving rotational transformation of the viewpoint while maintaining its positional coordinates. Simultaneous Translation-Rotation(Meanwhile): Instances where both translational and rotational operations occur concurrently. Sequential Translation-Rotation(Sequence): Cases where translation and rotation occur in sequence rather than simultaneously. Notably, in our dataset, this category is uniquely represented through textual descriptions in the questions rather than through explicit visual transformations. The first three categories of What if dynamics are visually demonstrated through changes in view representation, while the sequential category requires models to interpret text-based descriptions of perspective changes. This taxonomy provides systematic framework for evaluating spatial reasoning capabilities across diverse viewpoint transformation scenarios. Relation Query. We define three distinct categories of relation queries that capture the fundamental nature of spatial reasoning tasks: Agent-Agent: This pattern involves self-referential spatial positioning, where the observer must evaluate and potentially adjust their own position in space. It requires egocentric spatial reasoning and self-awareness of ones location relative to environmental constraints. Agent-Object: This pattern focuses on determining the orientation of an observed object relative to the observers position. Unlike the P-P pattern, the emphasis here is on object perception rather than self-positioning, requiring the observer to make judgments about external entities while maintaining awareness of their own reference frame. Object-Object: This pattern involves reasoning about the spatial relationship between two discrete objects in the environment, independent of the observers position. This allocentric spatial reasoning requires understanding relative positioning, distance, and orientation between entities without necessarily using oneself as reference point. These categorizations provide structured approach to analyzing the cognitive demands of different spatial reasoning tasks and can inform both the design of spatial question answering systems and the evaluation of human spatial cognition abilities. Perspective Taking. We propose label called \"Perspective Taking\" that categorizes the complexity of viewpoint projection. This label distinguishes between three increasingly sophisticated levels of perspective reasoning: Self Perspective: Reasoning based on the current camera view or the observers own viewpoint. This represents the baseline where no perspective shift is required. 23 Others Perspective Taking-1: The ability to determine visibility relationships from another agents viewpoint. This involves understanding what objects are visible or occluded from different viewpoint (e.g., determining whether specific object is within the field of view of another camera). The another agents viewpoint is usually determined by an object with clear orientation in the image. Others Perspective Taking-2: The ability to understand how spatial relationships transform when viewed from another agents perspective. This more advanced capability requires mental rotation and spatial transformation to reason about relative positions (e.g., determining whether, from another viewpoint, object appears to be positioned behind object Y). This classification aligns with developmental psychology research on perspective-taking abilities, where Level-1 perspective taking typically develops earlier than the more cognitively demanding Level-2 perspective taking. We provide performance across different categories and labels in Table1 and 2. Upon detailed analysis of model performance across various capabilities, certain trends emerge. The O-O (Object-Object) task within Relation Pattern also demonstrates generally lower scores across the board, suggesting it is less tractable problem for current models. Notably, InternVL2-8B struggles with the sequence task, exhibiting the lowest score among all evaluated models in that category. Regarding model stability, Mantis(SigLip) demonstrates robust performance in both Object Arrangement and Relation Pattern sections, indicating consistent capability in these spatial reasoning tasks. Similarly, Qwen2.5-VL-7B-Instruct maintains relatively stable performance within Viewpoint Dynamics. In contrast, InternVL2-8B shows broader instability, with consistently lower overall scores and considerable performance fluctuations across different sub-categories, highlighting areas for further improvement in its generalizability and robustness. Table 1 Performance of VLMs on D E across categories.(Part 1) Model Overall Object Arrangement Perspective Taking Linear Perp. Self Level1 Level LLaVA-Video-7B-Qwen2 Mantis(SigLip) GPT-4o Qwen2.5-VL-3B-Instruct LongVA-7B Qwen2.5-VL-7B-Instruct deepseek-vl2-small Robobrain Claude-sonnet-4 Space-Mantis InternVL2-8B Space-Qwen LLaVA-Onevision-7B Spatial-MLLM mPLUG-Owl3-7B 41.96 41.04 38.81 33.21 29.46 29.26 47.62 37.38 44.75 22.82 18.68 33.28 47.43 32.06 44.85 A.3. Examples 30.12 50.99 29.16 30.34 24.88 21.35 26.91 29.53 47.62 29.32 13.11 26.32 44.09 20.92 26.91 24 43.11 40.08 39.75 33.49 29.91 30.02 49.63 38.14 44.48 22.19 19.22 33.95 47.75 33.13 46. 42.19 41.20 39.07 32.96 28.81 28.77 48.32 37.56 45.32 22.15 17.89 33.06 48.04 31.79 45.15 60.76 54.43 46.20 46.84 51.90 46.84 56.33 55.06 49.38 45.57 64.56 46.84 51.27 46.84 60.13 33.80 35.41 31.86 36.28 39.83 36.81 31.11 30.57 31.74 33.48 27.99 35.63 33.48 35.20 35.74 Table 2 Performance of VLMs on D E across categories.(Part 2) Model Relation Pattern Viewpoint Dynamics A-A A-O O-O Rotation Meanwhile Sequence LLaVA-Video-7B-Qwen2 Mantis(SigLip) GPT-4o Qwen2.5-VL-3B-Instruct LongVA-7B Qwen2.5-VL-7B-Instruct deepseek-vl2-small Robobrain Claude-sonnet-4 Space-Mantis InternVL2-8B Space-Qwen LLaVA-Onevision-7B Spatial-MLLM mPLUG-Owl3-7B 36.22 23.78 49.30 37.85 19.72 31.41 43.98 30.94 41.78 28.18 15.67 31.59 42.28 27.72 47.80 57.61 64.16 48.38 37.51 35.49 34.67 68.27 49.18 67.25 17.03 12.47 38.14 65.87 37.75 62.29 26.67 25.24 16.70 20.65 25.58 15.63 25.33 27.37 15.85 20.89 24.58 26.13 29.79 25.80 18. 35.71 37.65 32.65 37.37 35.89 38.76 37.00 35.80 48.42 37.65 36.45 38.02 36.45 38.39 37.84 30.12 24.99 31.09 27.88 24.67 22.87 32.97 28.79 34.76 24.98 21.78 28.51 33.80 26.84 31.02 73.45 82.74 59.73 46.05 40.50 43.76 87.13 59.66 69.53 14.46 7.36 44.58 84.38 44.19 81.55 B. Evaluation on D E B.1. Prompt Templates for Evaluation Evaluation Prompt Prefix Based on these images, answer the question based on this rule: You only need to provide *ONE* correct answer selecting from the options listed below. For example, if you think the correct answer is A. above from A. above B. under C. front D. behind., your response should only be A. above. The Question is: B.2. Details in text only evaluation In the text-only evaluation, we replace the original image input with corresponding textual descriptions and assess the performance of models based on these descriptions. The purpose of this evaluation is to highlight how much information may be lost or distorted when the visual input is substituted with text-based representations, and to demonstrate the crucial role of visual data in the models performance. We used two types of captions: brief and dense. The brief captions provide concise overview of the image, while the dense captions offer more detailed description with focus on the spatial relationships between objects. Additionally, the models are evaluated using textual descriptions (text-only evaluation) based on these captions, with no access to the actual images. Prompt for Brief Captioning Describe this image briefly. 25 Figure 4 Example of among setting. Prompt for Dense Captioning Describe this image in detail, specifially focusing on the spatial relationship between objects. Text-only evaluation Prompt Prefix You need to gather information about each image based on the descriptions provide below, and answer the given questions using those textual descriptions, without directly viewing the images. Image 1: <Caption 1> ... Image N: <Caption N> As shown in the Table 3, all three models exhibit noticeable performance decline when replacing the original image input with its corresponding text-based description. Specifically, the brief captions cause the most significant performance drop. For instance, RoboBrain-8B experiences 7.83% decrease with the brief captions, and LLaVA-OneVision-7B drops by 12.91% in the same condition. Even when using dense captions, which offer more detail, there is still 26 Figure 5 Example-1 of around setting. performance reduction, although the decrease is slightly less pronounced compared to brief captions. In conclusion, while textual descriptions can convey some information, they fail to capture the richness and intricacies of visual data, leading to marked reduction in performance across all models. Table 3 Text-only (T) evaluation vs. original evaluation with image inputs (I). The results highlight significant performance drop when the original image input is replaced with the corresponding text-based caption, particularly with the brief captions. In all cases, model performance decreases notably, underscoring that our benchmark is vision-centric. Model Brief (T) Dense (T) Original (I) RoboBrain-8B LLaVA-OneVision-7B Qwen2.5-VL-7B-Instruct 33.92% 7.83% 35.58% 6.17% 34.17% 12.91% 35.92% 11.16% 28.75% 3.58% 27.00% 5.33% 41.75% 47.08% 32.33% B.3. Human Evaluation We use our Tiny Benchmark encompassing all task categories for evaluation by 5 human annotators, each of whom independently answers every question. Here is the results4. Table 4 Comparison of Human and GPT-4 Performance (%) Model/Annotator GPT4-o Human-max Human-min Human-avg 94.77 Accuracy 36.54 94.20 94.55 This observation demonstrates the disparity in spatial reasoning capabilities between huFigure 6 Example-2 of around setting. mans and state-of-the-art multimodal large language models, where humans exhibit superior performance in solving spatial problems that remain challenging for advanced AI systems. B.4. Evaluation Setup To comprehensively evaluate model performance, we conducted experiments on diverse suite of models. This suite includes models with native multi-image reasoning capabilities (e.g., LLaVA-Onevision [20], LLaVA-Video [21], mPLUG-Owl3 [23], InternVL2.5 [4], QwenVL2.5 [3], LongVA [22], IDEFICS [24], DeepSeek-VL2 [25]), Gemma3 [26], models fine-tuned on interleaved image-text data (e.g., Mantis [27]), leading proprietary APIs (e.g., GPT-4o, Claude-4-Sonnet), and models specifically fine-tuned for spatial reasoning tasks (e.g., RoboBrain [28], Space-Mantis [29], Space-Qwen [29], and Spatial-MLLM [30]). 28 B.5. Analysis in settings B.5.1. Around First, we examine the relationship between occlusion degree and response accuracy across four visibility levels (fully visible, mostly visible, mostly occluded, fully occluded) to determine whether performance degrades proportionally with increasing occlusion. Second, we investigate the impact of camera height variation within the same lateral viewpoint, as different vertical perspectives yield distinct occlusion patterns that may challenge the models ability to maintain spatial coherence. These paradigms evaluate whether models perform consistently when transferring spatial relationships across viewpoints, particularly in scenarios with significant object size discrepancies where smaller objects may be completely occluded from one angle but visible from another. This multifaceted analysis approach enables more nuanced understanding of MLLMs genuine 3D spatial reasoning capabilities beyond simple pattern recognition of 2D visual cues. We mainly evaluated GPT-4o and Qwen2.5-VL. Occlusion Degree Analysis. Our analysis reveals notable correlation between occlusion deFigure 7 Example of complex spatial arrangement in around setting. gree and model performance. Accuracy rates declined progressively with increasing occlusion, with an average decrease of 50.7% between fully visible and fully occluded conditions (p <0.01). Interestingly, the performance degradation was non-linear, with precipitous drop occurring between the mostly visible and mostly occluded categories (28.7% decrease), suggesting potential threshold effect in the models spatial reasoning capabilities. Error analysis in Figure8 further revealed that models frequently defaulted to proximity-based guessing when confronted with heavily occluded objects, rather than leveraging cross-view information to reason about hidden spatial relationships. Camera Height Impact Analysis. Varying camera heights significantly affected model performance through different occlusion patterns. High-angle perspectives yielded 24.8% higher accuracy than eye-level views by revealing tops of partially occluded objects and providing better scene context. This advantage was most pronounced in dense arrangements where topdown angles exposed spatial gaps between objects otherwise invisible from eye-level. Models clearly benefited from the holistic understanding afforded by elevated viewpoints, where global spatial relationships became more apparent. In contrast, eye-level perspectives with more severe occlusions led to poorer spatial reasoning, suggesting limited ability to mentally reconstruct hidden scene elements from partial visual information. The integration of findings across all three dimensions indicates that current MLLMs possess partial but inconsistent 3D spatial reasoning capabilities. The models performance appears heavily influenced by the visibility of key reference points across multiple viewpoints, suggesting reliance on visual correspondence matching rather than true 3D mental modeling. The observed asymmetries in spatial transformation paradigms further support this hypothesis, as robust 3D reasoning system would demonstrate consistent performance regardless of the specific transformation required. B.5.2. Among While the correctness of one single question indicates how often model answers an isolated question correctly, it does not capture whether the model remains consistent when presented with objects with different visual attributes and different number of central objects while retaining the original peripheral scenes. To investigate this, we also propose two different tests: Attribute Invariance Test. We modify only the visual attributes (e.g., color, category) of the central object while keeping the spatial configuration of all objects unchanged, as shown in Figure10. robust spatial reasoning system should maintain consistent answers, as spatial relationships remain invariant despite superficial attribute changes. Quantity Sensitivity Test. We increase the number of central objects (e.g., from one to three) while retaining the original peripheral objects, as shown in Figure11. This modification is hypothesized to enhance reasoning performance, as additional central objects provide more reference points for establishing cross-view correspondences and consistency. We also propose to look into the proportions of paired questions in tests where the answers are inconsistent with one another. First, we classify each paired instance into three scenarios: 1) CC(Both Correct) when the model answers both the primary and paired question correctly, 2) WW (Both Wrong) when it fails both versions, and 3) IC (Inconsistent) when the model answers one version correctly but fails the other. As shown in Figure 9, we report the proportions of IC (in consistent) outcomes across 4 MLLMs in two tests two open-source (Intern2.5-VL, LLaVA-Onevision), spatial model (SpaceQwen) and closed-source GPT-4o. We have several observations: 1) SpaceQwen exhibits notably high inconsistency score IC (around 40%) on both tasks, 2) LLaVA-Onevision remain fairly balanced inconsistency and high performance across tests, while InternVL vary 30 Figure 8 Example of different camera heights impact in around setting. 31 Figure 9 Paired question-answers inconsistency in two tests. We report the proportions of IC, CC and WW. Notably, SpaceQwen has highest inconsistency(around 40%). GPT-4o and LLaVA-Onevision exhibit more balanced performance. significantly across tests. Our systematic evaluation demonstrates MLLMs can achieve attribute-invariant spatial reasoning but struggle to utilize additional reference objects effectively. This highlights the need for: (1) enhanced geometric reasoning architectures, and (2) comprehensive benchmarks evaluating both attribute invariance and quantity sensitivity in 3D spatial understanding. B.6. Failure case analysis The observed pattern of errors indicates that models primarily rely on local relationship matching rather than inferring global spatial configurations, which represents critical gap compared to human-like spatial reasoning abilities. Future architectural improvements should therefore focus on enhancing transitive spatial inference mechanisms and view-invariant scene representation to support more robust reasoning across multiple perspectives. C. Data Structures as Cognitive Scaffolds, Evaluation Metrics, and Input-"
        },
        {
            "title": "Output Configurations",
            "content": "In this section, we provide detailed descriptions of the three data structures employed as cognitive scaffolds to approximate spatial mental models in VLMs, followed by formal definitions of the evaluation metrics employed across all experiments. Furthermore, we show the prompts for all the input-output configurations that were used across the following experiments. C.1. Data Structures as Cognitive Scaffolds The human ability to navigate and reason about space, especially with incomplete information, is largely attributed to the formation of internal spatial mental models. These models, as extensively studied in cognitive science, are not necessarily veridical, metric-perfect replicas of 32 Figure 10 Examples in Attribute Invariance Test. the environment. Instead, they are often schematic and even distorted, yet functionally effective representations. These models can be especially useful for understanding the environment spatial layouts for agentic settings [71, 72], such as embodied scenarios [73, 74, 75, 76, 77, 78, 79]. Pioneering work by Barbara Tversky suggests that these internal constructs are more akin to \"cognitive collages\" flexible assemblies of spatial information gleaned from various perspectives and experiences, rather than rigid, map-like blueprints [31]. These \"cognitive collages\" allow for the integration of fragmented observations and support reasoning across unseen perspectives. Johnson-Laird [12] posits that mental models, including those for space, serve as \"structural analogs of the world,\" enabling individuals to simulate and infer spatial relationships, such as determining the relative positions of objects from sequential descriptions (e.g., \"A is to the left of B; is in front of C\"). Research by Tversky [32] has also demonstrated that individuals can construct rich, multi-dimensional mental representations even from linear, descriptive texts, and subsequently query these models from various psychological viewpoints. Inspired by these cognitive theories, we explore three distinct data structures designed to act as cognitive scaffolds for VLMs. When VLMs are presented with limited visual input, these structures aim to approximate different facets of human spatial mental modeling: dynamic updating, integrated spatial layout representation, and inferential reasoning. C.1.1. View Interpolation for Dynamic Updating Human spatial mental models are not static; they are continuously updated with new sensory information and through mental simulation, such as imagining change in viewpoint. To approximate this dynamic updating and mental animation capability [34], we employ view interpolation. This technique aims to bridge perceptual gaps between discrete, sparsely sampled views by generating intermediate visual frames. Our Setting: In our experiments, view interpolation is implemented by inserting synthetic frames between consecutive views provided to the model. For instance, if \"1 interpolated frame\" is specified, one new frame is generated and inserted between an initial view 𝑉𝑛 and the subsequent view 𝑉𝑛+1 (e.g., between View 1 and View 2). Similarly, \"2 interpolated frames\" would mean two synthetic frames are inserted in sequence between 𝑉𝑛 and 𝑉𝑛+1. For the interpolated frames, 33 Figure 11 Examples in Quantity Sensitivity Test. we either define heuristic function to choose from the original datasets [68, 70] where we sampled our data, or we use Stable Virtual Camera [80] to generate intermediate frames for those image groups without. This approach is intended to provide smoother perceptual experience, potentially aiding the VLM in tracking object relations and maintaining spatial consistency across viewpoint shifts. (Refer to Figure 3 in the main paper for conceptual illustration) C.1.2. Cognitive Maps for Integrated Spatial Layouts core aspect of spatial cognition is the ability to form an allocentric (world-centered) or surveylike understanding of an environment, capturing the relative locations of objects. Tversky [31, 33] highlights that such representations often involve different frames of reference and hierarchical structures. Cognitive maps in our context are 2D schematic representations that attempt to embody this integrated spatial layout. Our Setting: We investigate two variants of cognitive maps, both represented as structured data (e.g., JSON-like objects), to capture the spatial layout: We provide 2D grid map of the scene that is related to the question to be answered. The map uses 1010 grid, where [0, 0] is the top-left corner and [9, 9] is the bottomright corner (i.e., birds-eye view). Directions are defined as follows: up = towards the top of the grid (decreasing y-value) right = towards the right of the grid (increasing x-value) down = towards the bottom of the grid (increasing y-value) 34 Figure 12 Failure case analysis. We show GPT4-os reasoning process. In case 1, the model is unable to establish the spatial location corresponding to each view; In case 2, the model confuses the subject of the behind relationship. left = towards the left of the grid (decreasing x-value) inner = into the 2D map (perpendicular to the grid, pointing away from you) outer = out of the 2D map (perpendicular to the grid, pointing toward you) The map contains: objects list of all important items in the scene with their position facing indicating the direction an object is oriented (when applicable) views representing different camera viewpoints in the scene Augmented Cognitive Map: This version explicitly integrates the observers perspective by encoding the positions and orientations (facing directions) of the camera viewpoints within the map, alongside the objects and their locations. For instance, as depicted in our data examples (refer to Figure 3, Cognitive Map - Augmented panel), an augmented map might define list of objects with their name and position (e.g., \"Tissue box\": { \"position\": [5, 5] }), and separate list of views detailing each cameras name (e.g., \"View 1\"), position (e.g., [3, 5]), and facing direction (e.g., \"up\"). Plain Cognitive Map (Object Only): This is more simplified, object-centric representation. It primarily focuses on the spatial locations of objects and, for some objects, their intrinsic orientation (facing direction) from top-down survey perspective, without explicitly embedding camera view information within its structure. For example (refer to Figure 3, Cognitive Map - Plain panel), plain map might list objects like \"Potted plant\" with its position (e.g., [5, 6]) and facing direction (e.g., \"down\"), and another object like \"Sofa\" with only its position (e.g., [4, 5]). This type of map still allows for reasoning about object-to-object relationships and, where specified, object orientations, but abstracts away the explicit camera viewpoints that generated the scene understanding. 35 In both map types, coordinates represent positions on 2D grid, and facing directions can be categorical (e.g., \"up\", \"down\", \"left\", \"right\", \"outer\", \"inner\"). These structures aim to provide the VLM with an explicit, albeit potentially imperfect, schematic of the environment that it can then learn to generate and utilize for spatial reasoning tasks. C.1.3. Free Form Reasoning Spatial mental models are not just static representations; they are actively used for inference and problem-solving [32]. To approximate this procedural and inferential aspect, we utilize free-form reasoning, implemented as natural language Chain-of-Thought (CoT) [81] process. This encourages the VLM to externalize its step-by-step reasoning process when deducing an answer to spatial query. Our Setting: The VLM is prompted to generate textual reasoning chain before outputting the final answer. This process is guided by three-step principle, exemplified by the reasoning chain shown in Figure 3, the reasoning chain panel. For the steps shown in that example, they are: (1) Initial Observation and Grounding: The model first processes each available view, identifying key objects and their immediate spatial relationships within that specific viewpoint. For instance, the example chain begins with: \"In View 1, see potted plant, tissue box, and hand sanitizer from left to right, with sofa behind.\" This step grounds the reasoning in direct visual evidence from individual perspectives. (2) Cross-View Integration and Environment Consolidation:Next, the model attempts to identify consistent objects or environmental cues across the different views to recognize that they depict the same underlying 3D scene. The example reasoning continues: \"In View 2, see the same potted plant, so both views are from the same environment.\" This step is crucial for building unified understanding of the space from discrete observations. (3) Question-Guided Spatial Inference: Finally, based on the specific question posed and the integrated understanding from the previous steps, the model performs step-by-step logical and spatial inferences to arrive at the answer. In the example, this involves relating the object positions across views relative to the observers position in View 2: \"Since the hand sanitizer is rightmost in View 1, its spatially furthest behind the potted plant when looking in View 2. In View 2, the potted plant is closest to me, so the hand sanitizer is the furthest from me.\" C.2. Evaluation Metrics To quantitatively assess how these data structures affect the performance of VLMs in the spatial mental modeling presented in D E, and to evaluate the quality of the generated cognitive maps, we employed the following metrics: (1) QA Accuracy, and (2) Graph Metrics for Generated Cognitive Maps. C.2.1. QA Accuracy QA Accuracy serves as the core metric for evaluating task performance. It quantifies the proportion of questions that the vision-language model (VLM) answers correctly out of the total number of questions. higher QA Accuracy indicates better alignment between the models responses and the ground truth. The metric is formally defined as: 36 QA Accuracy = 𝑁correct 𝑁 total 100% where 𝑁correct denotes the number of correctly answered questions, and 𝑁 total is the total number of questions evaluated. C.2.2. Graph Metrics for Cognitive Maps To quantitatively evaluate the quality of generated cognitive map, we use set of structured graph-based metrics. The overall process consists of several key steps: 1. Validity Check. First, we ensure that the generated map is syntactically and semantically validi.e., it has correct JSON format, contains interpretable object positions, and includes at least one valid object. 2. Rotation Normalization. Since we do not enforce fixed orientation for generated maps (to allow for flexible generation from vision-language models), we evaluate the similarity between the generated map and the ground truth across set of 3D rotations. We always choose the best-aligned rotation to compute our similarity scores. 3. Structural Matching. We define relation graph between object pairs in each map, capturing directional and proximity-based relationships. core part of the evaluation is determining whether these relationships in the ground truth are preserved in the generated map. 4. Similarity Metrics. We compute coverage (how many ground-truth objects are present), directional similarity (relative spatial relations), and facing similarity (object orientation). These are aggregated into an overall similarity score. 5. Rotation-Invariant Isomorphism. We also evaluate whether generated map is graphisomorphic to the ground truth under any allowed 3D rotation, providing strict measure of structural correctness. Below, we provide precise mathematical definitions for each of these components. Notation. cognitive map is finite set of objects = {𝑜1, . . . , 𝑜𝑛} where each object 𝑜𝑖 is associated with (i) 2-D position vector 𝑝𝑖 = (𝑥𝑖, 𝑦𝑖) R2 and (ii) an optional facing label 𝑓𝑖 {up, right, down, left, inner, outer} {}. For two maps, we distinguish (1) the ground-truth map (O, 𝑝, 𝑓 ) and (2) generated map (O𝑔, 𝑝𝑔, 𝑓 𝑔). The set of objects that appear in both maps is O𝑐 = O𝑔. Extended directional relation. We define directional or proximity-based relationship between any ordered object pair (𝑜𝑖, 𝑜 𝑗) based on their spatial arrangement and optional facing annotations. This relation is captured via the function: 37 dir(𝑜𝑖, 𝑜 𝑗) = 𝑥 𝑗 𝑥𝑖 > 𝑦 𝑗 𝑦𝑖 and 𝑥 𝑗 > 𝑥𝑖, right 𝑥 𝑗 𝑥𝑖 > 𝑦 𝑗 𝑦𝑖 and 𝑥 𝑗 < 𝑥𝑖, left down 𝑦 𝑗 𝑦𝑖 𝑥 𝑗 𝑥𝑖 and 𝑦 𝑗 > 𝑦𝑖, 𝑦 𝑗 𝑦𝑖 𝑥 𝑗 𝑥𝑖 and 𝑦 𝑗 < 𝑦𝑖, up 𝑝 𝑗 𝑝𝑖 2 < 𝛿 and ( 𝑓𝑖 = inner 𝑓 𝑗 = outer), inner 𝑝 𝑗 𝑝𝑖 2 < 𝛿 and ( 𝑓𝑖 = outer 𝑓 𝑗 = inner), outer otherwise, with threshold 𝛿 = 0.5 as in the implementation. These relations form relation matrix: 𝑅(𝑜𝑖, 𝑜 𝑗) = dir(𝑜𝑖, 𝑜 𝑗). Coverage. Coverage measures how many ground-truth objects are successfully retrieved in the generated map: Cov = [0, 1]. O𝑐 Directional similarity. We now evaluate how well the generated map preserves the directional relationships among object pairs from the ground truth. Define: = {(𝑜𝑖, 𝑜 𝑗) O𝑐 O𝑐 𝑖 𝑗, 𝑅(𝑜𝑖, 𝑜 𝑗) }. Then the directional similarity score is given by: 𝑆 dir = (cid:12){(𝑜𝑖, 𝑜 𝑗) 𝑅𝑔 (𝑜𝑖, 𝑜 𝑗) = 𝑅(𝑜𝑖, 𝑜 𝑗)}(cid:12) (cid:12) (cid:12) [0, 1], which corresponds to the proportion of directional relations in the ground truth that are correctly matched in the generated map. Facing similarity. For objects with defined facing directions, we compare their orientation across the two maps: = {𝑜𝑖 O𝑐 𝑓 𝑖 }. Then: 𝑆 face = (cid:12) (cid:12){𝑜𝑖 𝑓 𝑔 𝑖 }(cid:12) 𝑖 = 𝑓 (cid:12) [0, 1]. Overall similarity. To aggregate the directional and facing similarities, we use weighted combination: overall = 𝛼 𝑆 𝑆 where 𝛼 = 0.7 places greater emphasis on spatial layout than orientation. dir + (1 𝛼) 𝑆 face [0, 1], 38 Rotation-invariant isomorphism. To ensure fair comparison regardless of orientation, we define set of 3D rotations: = {𝑅1, . . . , 𝑅𝑚}, including all 90 turns about the 𝑧-axis, and one 90 turn about each of the 𝑥and 𝑦-axes. We say the maps are rotation-invariant isomorphic if there exists rotation such that their relation matrices match completely: 𝑘 {1, . . . , 𝑚} 𝑜𝑖, 𝑜 𝑗 : 𝑅(𝑜𝑖, 𝑜 𝑗) = 𝑅𝑔 (𝑘) (𝑜𝑖, 𝑜 𝑗), where 𝑅𝑔 (𝑘) is the relation matrix computed after applying 𝑅𝑘 to the generated map. Graph validity. Finally, generated map is deemed valid if: (1) It is well-formed JSON, (2) All fields conform to expected formats and constraints, and (3) At least one object has valid position. Together, the tuple (cid:0)Cov, 𝑆 (cid:1) provides comprehensive, rotation-aware evaluation of how closely generated cognitive map matches ground truth structure and orientation. overall, Isorot face, 𝑆 dir, 𝑆 C.3. Prompts for All Input-Output Configurations Below, we provide all prompts for the input-output configurations we investigate in our work. C.3.1. Example for Raw-QA Example Prompt for Raw-QA [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. [Answer Instruction] You only need to provide *ONE* correct answer selecting from the options listed below. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should **only** be <answer>A. Above</answer>. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard 39 C.3.2. Example for FF-Rsn Example Prompt for FF-Rsn: Free-Form Reasoning [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. [Answer Instruction] Please do step by step reasoning first, then give your final answer. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should be this format: <think>(replace with your reasoning here)</think><answer>A. Above</answer>. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.3. Example for VI-1 and VI-2 Prompt for VI-1: View Interpolation with 1 Frame [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. [Answer Instruction] 40 You only need to provide *ONE* correct answer selecting from the options listed below. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should **only** be <answer>A. Above</answer>. [Question] Based on these 8 images showing the white jar from different viewpoints (from front (image 1) to left (image 3), from left (image 3) to back (image 5), from back (image 5) to right (image 7), from right (image 7) back to front (image 1)), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 7, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard Prompt for VI-2: View Interpolation with 2 Frames [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. [Answer Instruction] You only need to provide *ONE* correct answer selecting from the options listed below. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should **only** be <answer>A. Above</answer>. [Question] Based on these 12 images showing the white jar from different viewpoints (from front (image 1) to left (image 4), from left (image 4) to back (image 7), from back (image 7) to right (image 10), from right (image 10) back to front (image 1)), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 10, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.4. Example for Aug-CGMap-In Prompt for Aug-CGMap-In: Grounded Augmented Cognitive Map as Input [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. Also, we provide you cognitive map that shows the general layout for the scene. Please use the cognitive map to reason and answer the question. [Answer Instruction] You only need to provide *ONE* correct answer selecting from the options listed below. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should **only** be <answer>A. Above</answer>. [Cognitive Map Format] We provide you 2D grid map of the scene that is related to the question you should answer. Below is the description of the map: - The map uses 10x10 grid where [0,0] is at the top-left corner and [9,9] is at the bottomright corner - The map is shown in the birds view - Directions are defined as: * up = towards the top of the grid (decreasing y-value) * right = towards the right of the grid (increasing x-value) * down = towards the bottom of the grid (increasing y-value) * left = towards the left of the grid (decreasing x-value) * inner = straight into the 2D map (perpendicular to the grid, pointing away from you) * outer = straight out of the 2D map (perpendicular to the grid, pointing towards you) - \"objects\" lists all important items in the scene with their positions - \"facing\" indicates which direction an object is oriented towards (when applicable) - \"views\" represents the different camera viewpoints in the scene Below is the cognitive map of the scene related to the question. Please use it to reason and answer the question. json { \"objects\": [ {\"name\": \"white jar\", \"position\": [5, 5]}, {\"name\": \"bed sheet with floral pattern\", \"position\": [5, 8]}, {\"name\": \"white headboard\", \"position\": [2, 5]}, {\"name\": \"clothes rack\", \"position\": [5, 2]}, {\"name\": \"table with cups on it\", \"position\": [8, 5]} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [5, 6], \"facing\": \"up\"}, 42 {\"name\": \"Image 2\", \"position\": [4, 5], \"facing\": \"right\"}, {\"name\": \"Image 3\", \"position\": [5, 4], \"facing\": \"down\"}, {\"name\": \"Image 4\", \"position\": [6, 5], \"facing\": \"left\"} ] } [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.5. Example for Aug-CGMap-Out Prompt for Aug-CGMap-Out: Ask VLM to Output Augmented Cognitive Map and Direct Answer [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. You will then create detailed cognitive map representing the scene using 10x10 grid coordinate system. [Rules] 1. Focus ONLY on these categories of objects in the scene: {white jar, bed sheet with floral pattern, white headboard, clothes rack, table with cups on it} 2. Create cognitive map with the following structure in the birds view: - 10x10 grid where [0,0] is at the top-left corner and [9,9] is at the bottom-right corner - up = towards the top of the grid (decreasing y) - right = towards the right of the grid (increasing x) - down = towards the bottom of the grid (increasing y) - left = towards the left of the grid (decreasing x) - inner = straight into the 2D map (perpendicular to the grid, pointing away from you) - outer = straight out of the 2D map (perpendicular to the grid, pointing towards you) - Include positions of all objects from the specified categories - Estimate the center location (coordinates [x, y]) of each instance within provided categories - If category contains multiple instances, include all of them - Each objects estimated location should accurately reflect its real position in the scene, preserving the relative spatial relationships among all objects 43 - Combine and merge information from the images since they are pointing to the same scene, calibrating the object locations accordingly - Include camera positions and directions for each view 3. Carefully integrate information from all views to create single coherent spatial representation. [Answer Instruction] 1. Given the provided views and main objects mentioned in the above rules, you **MUST** present your cognitive map in the following JSON format **before your answer**: json { \"objects\": [ {\"name\": \"object_name\", \"position\": [x, y], \"facing\": \"direction\"}, {\"name\": \"object_without_orientation\", \"position\": [x, y]} ], \"views\": [ {\"name\": \"View/Image 1\", \"position\": [x, y], \"facing\": \"direction\"}, {\"name\": \"View/Image 2\", \"position\": [x, y], \"facing\": \"direction\"} ] } 2. Next, provide *ONE* correct answer selecting from the options. Your answer field must be in the format like \"A. Above\". 3. In general, your responses format should be like \"Based on my observation, the answer is: <cogmap>(Replace with your cogmap here)</cogmap><answer>(Replace with your answer here)</answer>\". Your option must be from the available options. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.6. Example for Plain-CGMap-Out Prompt for Plain-CGMap-Out: Ask VLM to Output Plain Cognitive Map and Direct Answer 44 [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. You will then create detailed cognitive map representing the scene using 10x10 grid coordinate system. [Rules] 1. Focus ONLY on these categories of objects in the scene: {white jar, bed sheet with floral pattern, white headboard, clothes rack, table with cups on it} 2. Create cognitive map with the following structure in the birds view: - 10x10 grid where [0, 0] is at the top-left corner and [9, 9] is at the bottom-right corner - up = towards the top of the grid (decreasing y) - right = towards the right of the grid (increasing x) - down = towards the bottom of the grid (increasing y) - left = towards the left of the grid (decreasing x) - Include positions of all objects from the specified categories - Estimate the center location (coordinates [x, y]) of each instance within provided categories - If category contains multiple instances, include all of them - Object positions must maintain accurate relative spatial relationships - Combine and merge information from the images since they are pointing to the same scene, calibrating the object locations with grid coordinates accordingly 3. Carefully integrate information from all views to create single coherent spatial representation. [Answer Instruction] 1. Given the provided views and main objects mentioned in the above rules, you **MUST** present your cognitive map in the following JSON format **before your reasoning**: json { \"object_category_1\": {\"position\": [x, y]}, \"object_category_2\": {\"position\": [x, y], \"facing\": \"direction\"}, # if the object is asked for orientation ... } 2. Next, provide *ONE* correct answer selecting from the options. Your answer field must be in the format like \"A. Above\" 3. In general, your responses format should be like \"Based on my observation, the answer is: <cogmap>(Replace with your cogmap here)</cogmap><answer>(Replace with your answer here)</answer>\". Your option must be from the available options. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard 45 C.3.7. Example for Plain-CGMap-FFR-Out Prompt for Plain-CGMap-FFR-Out: Ask VLM to Output Plain Cognitive Map and Free-Form Reasoning [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. You will then create detailed cognitive map representing the scene using 10x10 grid coordinate system. [Rules] 1. Focus ONLY on these categories of objects in the scene: {white jar, bed sheet with floral pattern, white headboard, clothes rack, table with cups on it} 2. Create cognitive map with the following structure in the birds view: - 10x10 grid where [0, 0] is at the top-left corner and [9, 9] is at the bottom-right corner - up = towards the top of the grid (decreasing y) - right = towards the right of the grid (increasing x) - down = towards the bottom of the grid (increasing y) - left = towards the left of the grid (decreasing x) - Include positions of all objects from the specified categories - Estimate the center location (coordinates [x, y]) of each instance within provided categories - If category contains multiple instances, include all of them - Object positions must maintain accurate relative spatial relationships - Combine and merge information from the images since they are pointing to the same scene, calibrating the object locations with grid coordinates accordingly 3. Carefully integrate information from all views to create single coherent spatial representation. [Answer Instruction] 1. Given the provided views and main objects mentioned in the above rules, you **MUST** present your cognitive map in the following JSON format **before your reasoning**: json { \"object_category_1\": {\"position\": [x, y]}, \"object_category_2\": {\"position\": [x, y], \"facing\": \"direction\"}, # if the object is asked for orientation ... } 2. Next, please also provide your reasons step by step in details, then provide *ONE* correct answer selecting from the options. Your answer field must be in the format like \"A. Above\" 3. In general, your responses format should be like \"Based on my observation, the answer is: <cogmap>(Replace with your cogmap here)</cogmap><think>(Replace with your reasoning here)</think><answer>(Replace with your answer here)</answer>\". Your option must be from the available options. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.8. Example for Aug-CGMap-FFR-Out Prompt for Aut-CGMap-FFR-Out: Ask VLM to Output Augmented Cognitive Map and Free-Form Reasoning [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. You will then create detailed cognitive map representing the scene using 10x10 grid coordinate system. [Rules] 1. Focus ONLY on these categories of objects in the scene: {white jar, bed sheet with floral pattern, white headboard, clothes rack, table with cups on it} 2. Create cognitive map with the following structure in the birds view: - 10x10 grid where [0,0] is at the top-left corner and [9,9] is at the bottom-right corner - up = towards the top of the grid (decreasing y) - right = towards the right of the grid (increasing x) - down = towards the bottom of the grid (increasing y) - left = towards the left of the grid (decreasing x) - inner = straight into the 2D map (perpendicular to the grid, pointing away from you) - outer = straight out of the 2D map (perpendicular to the grid, pointing towards you) - Include positions of all objects from the specified categories - Estimate the center location (coordinates [x, y]) of each instance within provided categories - If category contains multiple instances, include all of them - Each objects estimated location should accurately reflect its real position in the scene, preserving the relative spatial relationships among all objects - Combine and merge information from the images since they are pointing to the same scene, calibrating the object locations accordingly - Include camera positions and directions for each view 47 3. Carefully integrate information from all views to create single coherent spatial representation. [Answer Instruction] 1. Given the provided views and main objects mentioned in the above rules, you **MUST** present your cognitive map in the following JSON format **before your reasoning**: json { \"objects\": [ {\"name\": \"object_name\", \"position\": [x, y], \"facing\": \"direction\"}, {\"name\": \"object_without_orientation\", \"position\": [x, y]} ], \"views\": [ {\"name\": \"View/Image 1\", \"position\": [x, y], \"facing\": \"direction\"}, {\"name\": \"View/Image 2\", \"position\": [x, y], \"facing\": \"direction\"} ] } 2. Next, please also provide your reasons step by step in details, then provide *ONE* correct answer selecting from the options. Your answer field must be in the format like \"A. Above\" 3. In general, your responses format should be like \"Based on my observation, the answer is: <cogmap>(Replace with your cogmap here)</cogmap><think>(Replace with your reasoning here)</think><answer>(Replace with your answer here)</answer>\". Your option must be from the available options. [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard C.3.9. Example for CGMap-In-FFR-Out Prompt for CGMap-In-FFR-Out: Input VLM with Grounded Cognitive Map and Output with Free-Form Reasoning 48 [Task] Your task is to analyze the spatial arrangement of objects in the scene by examining the provided images, which show the scene from different viewpoints. Also, we provide you cognitive map that shows the general layout for the scene. Please use the cognitive map to reason and answer the question. [Answer Instruction] Please do step by step reasoning first, then give your final answer. For example, if you think the correct answer is A. Above from A. Above B. Under C. Front D. Behind, your response should be this format: <think>(replace with your reasoning here)</think><answer>A. Above</answer>. [Cognitive Map Format] We provide you 2D grid map of the scene that is related to the question you should answer. Below is the description of the map: - The map uses 10x10 grid where [0,0] is at the top-left corner and [9,9] is at the bottomright corner - The map is shown in the birds view - Directions are defined as: * up = towards the top of the grid (decreasing y-value) * right = towards the right of the grid (increasing x-value) * down = towards the bottom of the grid (increasing y-value) * left = towards the left of the grid (decreasing x-value) * inner = straight into the 2D map (perpendicular to the grid, pointing away from you) * outer = straight out of the 2D map (perpendicular to the grid, pointing towards you) - \"objects\" lists all important items in the scene with their positions - \"facing\" indicates which direction an object is oriented towards (when applicable) - \"views\" represents the different camera viewpoints in the scene Below is the cognitive map of the scene related to the question. Please use it to reason and answer the question. json { \"objects\": [ {\"name\": \"white jar\", \"position\": [5, 5]}, {\"name\": \"bed sheet with floral pattern\", \"position\": [5, 8]}, {\"name\": \"white headboard\", \"position\": [2, 5]}, {\"name\": \"clothes rack\", \"position\": [5, 2]}, {\"name\": \"table with cups on it\", \"position\": [8, 5]} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [5, 6], \"facing\": \"up\"}, {\"name\": \"Image 2\", \"position\": [4, 5], \"facing\": \"right\"}, {\"name\": \"Image 3\", \"position\": [5, 4], \"facing\": \"down\"}, {\"name\": \"Image 4\", \"position\": [6, 5], \"facing\": \"left\"} ] } [Question] Based on these four images (image 1, 2, 3, and 4) showing the white jar from different viewpoints (front, left, back, and right), with each camera aligned with room walls and 49 partially capturing the surroundings: From the viewpoint presented in image 4, what is to the left of the white jar? A. Table with cups on it B. Clothes rack C. Bed sheet with floral pattern D. White headboard D. Which Scaffolds Best Guide Spatial Thinking in Unchanged VLMs? The primary experimental setup for evaluating how different structural scaffolds can guide spatial reasoning in unchanged VLMs, has been comprehensively described in Section 3 of the main paper. This current section is dedicated to presenting qualitative examples of model outputs, serving as case studies for the different scaffolding configurations tested. For precise details on the input prompt structures and specific instructions provided to the Qwen2.5-VL-3B-Instruct for each experimental condition (e.g., Raw-QA, FF-Rsn, Plain-CGMap-FFR-Out), please refer to Appendix C.3. D.1. VLM Response Examples for Configurations in Section C.3 Output for Raw-QA: Input Prompt Refer to C.3.1 C. Bed sheet with floral pattern Output for VI: Input Prompt Refer to C.3.3 B. Clothes rack Output for Aug-CGMap-In: Input Prompt Refer to C.3.4 C. Bed sheet with floral pattern Output for FF-Rsn: Input Prompt Refer to C.3.2 <think>To determine what is to the left of the white jar from the viewpoint presented in image 4, lets analyze the scene: - In image 4, the white jar is placed on table covered with pink tablecloth featuring heart patterns. - The table has glass top and is positioned against wall. - To the left of the jar, there is piece of furniture that appears to be bed or bed frame. Given this information, the object to the left of the white jar is most likely bed or bed frame. </think> <answer>C. Bed sheet with floral pattern</answer> 50 Output for Aug-CGMap-FFR-Out: Input Prompt Refer to C.3. json { \"objects\": [ {\"name\": \"white jar\", \"position\": [150, 446], \"facing\": \"left\"}, {\"name\": \"bed sheet with floral pattern\", \"position\": [0, 317], \"facing\": \"left\"}, {\"name\": \"clothes rack\", \"position\": [0, 108], \"facing\": \"left\"}, {\"name\": \"table with cups on it\", \"position\": [188, 200], \"facing\": \"left\"}, {\"name\": \"white headboard\", \"position\": [188, 317], \"facing\": \"left\"} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [0, 0], \"facing\": \"left\"}, {\"name\": \"Image 2\", \"position\": [0, 0], \"facing\": \"left\"}, {\"name\": \"Image 3\", \"position\": [0, 0], \"facing\": \"left\"}, {\"name\": \"Image 4\", \"position\": [0, 0], \"facing\": \"left\"} ] } <CogMap> <Your cognitive map> <Reasoning> From the viewpoint presented in image 4, we can see that the white jar is positioned in the foreground, facing left. The bed sheet with floral pattern is located directly behind the white jar, covering the lower part of the image. The clothes rack is visible on the left side of the image, slightly above the bed sheet. The table with cups on it is situated further back, to the right of the white jar. The white headboard is also visible, but it is not as prominent as the other objects due to its position behind the bed sheet. Therefore, my answer is A. Table with cups on it. </Answer> Therefore, my answer is A. Table with cups on it. Output for Plain-CGMap-FFR-Out: Input Prompt Refer to C.3.7 json { \"white_jar\": {\"position\": [150, 440], \"facing\": \"left\"}, \"table_with_cups_on_it\": {\"position\": [187, 260], \"facing\": \"right\"} } <CogMap> 51 <Your cognitive map> <Reasoning> From the viewpoint presented in image 4, we can see that the white jar is positioned on the bed sheet with floral pattern. The table with cups on it is located to the right of the white jar, as indicated by its position in the image. The clothes rack is not visible in this particular view, and the white headboard is not directly related to the white jar or the table with cups on it. Therefore, the correct answer is A. Table with cups on it. <Answer> Therefore, my answer is A. Table with cups on it. Output for CGMap-In-FFR-Out: Input Prompt Refer to C.3.9 C. Bed sheet with floral pattern D.2. Additional Graph Metrics for Generated Graphs To quantitatively assess the quality of the cognitive maps produced by our models, we employ suite of graph-based metrics detailed in Section C.2.2. Table 5 presents comparison of the cognitive maps generated by two of our methods: Aug-CGMap-FFR-Out and Plain-CGMap-FFR-Out. The results highlight several key findings: High Validity: Both methods demonstrate strong ability to generate syntactically and semantically valid maps, achieving high Valid Rates (94.95% for Plain-CGMap-FFR-Out and 92.57% for Aug-CGMap-FFR-Out). This indicates that the models successfully learned the required JSON structure for the cognitive maps. Structural Complexity: Achieving perfect structural replication of the ground truth remains challenging, as shown by the modest Isomorphism Rates. The Plain-CGMap-FFR-Out method performs significantly better, with 7.43% of its maps being structurally identical (isomorphic) to the ground truth, compared to mere 0.10% for the augmented map method. Superior Similarity Performance: clear performance difference in semantic similarity is evident. The Aug-CGMap-FFR-Out method, which explicitly includes camera views, achieves substantially higher Overall Similarity (51.12%) and is superior in representing both the relative directional relationships (Avg. Dir. Sim. of 43.57%) and the correct orientation of individual objects (Avg. Facing Sim. of 68.75%). In contrast, while Plain-CGMap-FFR-Out maintains higher validity and isomorphism, it lags behind in all three similarity metrics. E. Can We Teach VLMs to Build and Leverage Spatial Representations? In the main paper, we demonstrated that prompting frozen VLMs with external scaffolds offers limited improvements. This highlighted core limitation: the models themselves arent effectively forming internal spatial representations or reasoning through space. To address this, we investigated whether supervised fine-tuning (SFT) could teach VLMs to build and leverage these spatial models internally. This section of the appendix provides further details on our SFT methodology, starting with the crucial step of data curation. 52 Table 5 Comparison of graph metrics for cognitive maps generated by different methods. The metrics evaluate the quality of the generated maps against the ground truth. Valid Rate: percentage of syntactically and semantically valid maps. Isomorphism Rate: percentage of maps that are structurally identical (isomorphic) to the ground truth, accounting for rotation. Overall Sim. (Similarity): weighted score combining directional and facing similarity (𝑆 overall = 𝛼 𝑆 face). Avg. Dir. Sim. (Average Directional Similarity): correctness of relative spatial relations between objects. Avg. Facing Sim. (Average Facing Similarity): correctness of object orientations. All values are percentages (%). dir + (1 𝛼) 𝑆 Method Valid Rate Isomorphism Rate Overall Sim. Avg. Dir. Sim. Avg. Facing Sim. Aug-CGMap-FFR-Out Plain-CGMap-FFR-Out 92.57 94.95 0.10 7.43 51.12 37.44 43.57 28. 68.75 58.78 E.1. Supervised Fine-Tuning Data Curation Effective SFT heavily relies on the quality and nature of the training data. To teach our VLMs the desired spatial reasoning capabilities, we meticulously curated two primary types of data: cognitive maps and free-form reasoning chains. These were designed to provide strong supervisory signals for the model to learn how to represent and reason about space. E.1.1. Cognitive Map Generation As discussed in Section C.1, cognitive maps serve as 2D schematic representations of object layouts. For the SFT phase, we needed to generate ground truth cognitive maps that the VLM could learn to produce. Our approach to generating these maps was grounded in the object arrangement annotations described in Section A.1. We aimed for representations that were not only accurate but also in format that the VLM could feasibly learn to generate. The generation process was automated via script that processes input JSONL files, where each line item contains scene details including images and, crucially, meta_info describing the objects, their potential orientations, and the camera viewpoint setup. For every item, the script first identifies its specific spatial arrangement \"setting\" (e.g., \"around,\" \"among,\" \"translation,\" or \"rotation\") by parsing the items unique ID. Based on this setting, dedicated functions apply set of predefined rules and heuristics to determine the 2D coordinates (on 10x10 grid) and facing directions for both the objects and the camera views. For instance, in the \"around\" setting, objects (typically 2-4) are placed in predetermined linear arrangement near the grids center (e.g., at coordinates like [4,5], [5,5]), and camera views are positioned at cardinal directions relative to these objects, based on the specific camera angles pertinent to the question. In the \"rotation\" setting, the camera is fixed at the center ([5,5]), and its facing direction changes across views, while object positions are defined relative to the cameras current orientation. Similar rule-based placements are implemented for \"among\" (objects in cross or T-shape with views from specific angles) and \"translation\" (objects arranged linearly to depict relationships like \"on\" or \"down to\") settings. Object orientations, if applicable, are also assigned based on the input meta_info. Finally, the generated layout of objects and views is formatted into structured JSON string, representing the cognitive map. This JSON cogmap, along with templated instructional prompts (cogmap_input for VLM input format guidance and cogmap_output for VLM output task description), is added to the original data item. The overall generation logic is summarized in 53 Algorithm 1. Algorithm 1 Cognitive Map Generation Require: Dataset 𝐷 containing items with spatial arrangement annotations Ensure: Updated dataset with cognitive maps in JSON format 1: for all 𝑖𝑡𝑒𝑚 𝐷 do 2: 𝑠𝑒𝑡𝑡𝑖𝑛𝑔 Extract setting type from 𝑖𝑡𝑒𝑚.𝑖𝑑 Initialize empty cognitive map 𝑐𝑜𝑔𝑚𝑎𝑝 Position objects and views based on setting type if 𝑠𝑒𝑡𝑡𝑖𝑛𝑔 = around then Position 2-4 objects in line with coordinates like [4,5], [5,5], etc. Place views at cardinal positions based on camera angles else if 𝑠𝑒𝑡𝑡𝑖𝑛𝑔 = among then Place center object at [5,5] and surrounding objects at [5,8], [2,5], [5,2], [8,5] Position views based on specified camera angles else if 𝑠𝑒𝑡𝑡𝑖𝑛𝑔 = translation then Position objects according to their spatial relationships (e.g., on, down) Place views to highlight these spatial relationships else if 𝑠𝑒𝑡𝑡𝑖𝑛𝑔 = rotation then Arrange objects based on rotation type (clockwise, counterclockwise, etc.) Fix camera at [5,5] with varying facing directions end if for all 𝑜𝑏 𝑗𝑒𝑐𝑡 𝑐𝑜𝑔𝑚𝑎𝑝.𝑜𝑏 𝑗𝑒𝑐𝑡𝑠 do if 𝑜𝑏 𝑗𝑒𝑐𝑡 has orientation then Add orientation information where applicable Add facing direction (up, down, left, right) end if 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end for Format 𝑐𝑜𝑔𝑚𝑎𝑝 as structured JSON Add formatted cognitive map to 𝑖𝑡𝑒𝑚 23: 24: end for 25: return Updated dataset 𝐷 E.1.2. Free-Form Reasoning Generation While cognitive maps provide structured, global understanding of the scene, effective spatial reasoning also involves procedural, step-by-step thought process. To instill this capability in our VLMs, we generated dataset of grounded free-form reasoning chains. These chains were designed to verbalize the mental simulation process required to answer the spatial questions in D E. The generation of these reasoning chains was closely tied to the question-answer (QA) templates developed in Section 2. For each specific setting (e.g., rotation, among, around), we manually constructed reasoning chains following consistent set of principles to ensure logical coherence and clear grounding in the provided visual information and the question asked. The core principles guiding the generation of these reasoning chains were: 1. Initial Scene Understanding. The reasoning begins by processing each input image individually. This involves identifying key objects visible in that view and noting their explicit spatial relationships with other objects within that same view. This step emulates the initial perceptual intake human might perform. 2. Cross-View Consistency and Environment Integration. After individual view analysis, the reasoning emphasizes that although different images are provided, they all depict the same underlying spatial environment. This is often achieved by identifying and highlighting an anchor object or consistent set of objects that appear across multiple views, thereby helping to establish unified mental model of the scene. 3. Question-Driven Inference. With foundational understanding of the scene established from the views, the subsequent steps in the reasoning chain are directly guided by the specifics of the question. This involves: (1) Mental Simulation: If the question involves hypothetical change in viewpoint or \"what-if\" scenario (e.g., \"what if you turn left?\"), the reasoning chain explicitly verbalizes this mental transformation. (2) Perspective Taking: If the question requires adopting different perspective (e.g., \"from the sofas perspective\"), the reasoning chain articulates this shift. (3) Spatial Relationship Deduction: The chain logically deduces the queried spatial relationship by integrating information from the relevant views, applying spatial concepts (like left-of, behind, further from), and referencing the established mental model of the scene. This structured approach to generating reasoning chains aimed to provide clear, step-bystep examples of spatial thought processes for the VLM to learn from. Figure 13, 14 and 15 show template example combined with the filled case for A N, N G, U D, respectively. E.2. Detailed Experimental Setup In this section, we provide more granular view of the experimental parameters employed during the Supervised Fine-Tuning (SFT) phase of our research. As stated in the main text, these experiments were designed to teach Vision-Language Models (VLMs) to build and leverage internal spatial representations. The base model for these SFT experiments was Qwen2.5-VL-3B-Instruct. We utilized consistent training script for all SFT experiments, ensuring comparability across different configurations. The primary variation across these runs was the specific dataset used (datasets variable in the script), corresponding to the different SFT task configurations discussed in Section 4.1, such as Aug-CGMap-Out. Other hyperparameters were kept constant to isolate the effects of the different training signals. The core training hyperparameters are Table 6 Training hyperparameters for SFT experiments with Qwen2.5-VL-3B-Instruct. Parameter Value 10,000 QA pairs 3 1e-5 Cosine Full-parameter 256 2 NVIDIA Dataset size Epochs Learning rate Scheduler Fine-tuning type Batch Size GPUs used Max image resolution 90,000 pixels Min image resolution Model Max Length Weight Decay Warmup Ratio Max Grad Norm Precision Optimizer 784 pixels 8192 tokens 0 0.03 1 BF16 AdamW summarized in Table 6 and further detailed by the provided training script. The training was conducted using distributed setup managed by torchrun and leveraged DeepSpeed with ZeRO Stage 3 optimization strategy for efficient full-parameter fine-tuning. 55 Figure 13 Example reasoning chain template for A N 56 Figure 14 Example reasoning chain template for N 57 Figure 15 Example reasoning chain template for U 58 Specifically, we set NPROC_PER_NODE to 2, utilizing two NVIDIA H100 GPUs, though the script template showed CUDA_VISIBLE_DEVICES=0,1,2,3 and NPROC_PER_NODE defaulting to 4, our table and resource claims point to 2 GPUs being used for these runs. The per_device_train_batch_size was set to 4, and with gradient_accumulation_steps at 32, this resulted in an effective batch size of 256. The learning rate was 1 105 with cosine learning rate scheduler and warmup ratio of 0.03 over 3 training epochs. We enabled full fine-tuning of the vision encoder, MM-MLP, and the LLM components (tune_mm_vision True tune_mm_mlp True tune_mm_llm True). The maximum sequence length for the model was set to 8192 tokens (model_max_length 8192), and the maximum image input pixels were capped at 90,000 (max_pixels 90000), with minimum of 784. Gradient checkpointing was enabled to manage memory usage (gradient_checkpointing True). Checkpoints were saved every 5 steps (save_steps 5), keeping total of 12 checkpoints (save_total_limit 12). This consistent and robust experimental framework, detailed in Sections 3.1, 3.2, and herein, allowed for fair comparison of the different SFT strategies aimed at improving the spatial reasoning capabilities of Qwen2.5-VL-3B-Instruct. E.3. VLM Response Examples After SFT for Configurations in Section C.3 Output for Raw-QA: Input Prompt Refer to C.3.1 <answer>D. White headboard</answer> Output for Aug-CGMap-Out: Input Prompt Refer to C.3. Based on my observation, the answer is: <cogmap> json { \"objects\": [ {\"name\": \"white jar\", \"position\": [5, 5]}, {\"name\": \"bed sheet with floral pattern\", \"position\": [5, 8]}, {\"name\": \"white headboard\", \"position\": [2, 5]}, {\"name\": \"clothes rack\", \"position\": [5, 2]}, {\"name\": \"table with cups on it\", \"position\": [8, 5]} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [5, 6], \"facing\": \"up\"}, {\"name\": \"Image 2\", \"position\": [4, 5], \"facing\": \"right\"}, {\"name\": \"Image 3\", \"position\": [5, 4], \"facing\": \"down\"}, {\"name\": \"Image 4\", \"position\": [6, 5], \"facing\": \"left\"} ] } </cogmap> <answer>C. Bed sheet with floral pattern</answer> 59 Output for Plain-CGMap-Out: Input Prompt Refer to C.3.6 Based on my observation, the answer is: <cogmap> json { \"white jar\": {\"position\": [5, 5]}, \"bed sheet with floral pattern\": {\"position\": [5, 8]}, \"white headboard\": {\"position\": [2, 5]}, \"clothes rack\": {\"position\": [5, 2]}, \"table with cups on it\": {\"position\": [8, 5]} } </cogmap><answer> B. Clothes rack </answer> Output for FF-Rsn: Input Prompt Refer to C.3.2 <think>In this scene, observe four images showing different perspectives. All images feature the white jar as the main object. In image 1, can see white jar in front of the table with cups on it. In image 2, can see white jar in front of the bed sheet with floral pattern. In image 3, can see white jar in front of the clothes rack. In image 4, can see white jar in front of the white headboard. To identify the position change across views, focus on the main objects angle variation. Then, analyze the angles and relative positions of other objects on the platform to back up this observation. understand that: Image 1 is the initial view. Image 2 is captured after 90-degree clockwise rotation from image 1. Image 3 is after another 90-degree clockwise rotation (180 degrees from image 1). Image 4 is after further 90-degree clockwise rotation (270 degrees from image 1). Through analyzing these perspective changes, can construct complete spatial understanding: when view bed sheet with floral pattern behind white jar in the second view, it implies that in the first view, bed sheet with floral pattern is on the right side of white jar. Similarly, when see white headboard behind white jar in the fourth view, it indicates that in the first view, white headboard is on the left side of white jar. However, am still uncertain about what lies behind me in the first view. Then, recognize that can examine the opposite view to find out. The opposite view of the fist view is the third view. As clothes rack is observed behind white jar in the third view, it means that in the first view, clothes rack is positioned behind me. This way, can fully comprehend the spatial relationships of all objects in the entire scene. So, from the perspective of image 4: table with cups on it is to the right of white jar, bed sheet with floral pattern is to my behind, clothes rack is to the left of white jar. So the answer is B. Clothes rack</think><answer>B. Clothes rack</answer> Output for Aug-CGMap-FFR-Out: Input Prompt Refer to C.3.8 Based on my observation, the answer is: <cogmap> 60 json { \"objects\": [ {\"name\": \"white jar\", \"position\": [5, 5]}, {\"name\": \"bed sheet with floral pattern\", \"position\": [5, 8]}, {\"name\": \"white headboard\", \"position\": [2, 5]}, {\"name\": \"clothes rack\", \"position\": [5, 2]}, {\"name\": \"table with cups on it\", \"position\": [8, 5]} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [5, 6], \"facing\": \"up\"}, {\"name\": \"Image 2\", \"position\": [4, 5], \"facing\": \"right\"}, {\"name\": \"Image 3\", \"position\": [5, 4], \"facing\": \"down\"}, {\"name\": \"Image 4\", \"position\": [6, 5], \"facing\": \"left\"} ] } </cogmap> <think>In this scene, observe four images showing different perspectives. All images feature the white jar as the main object. In image 1, can see white jar in front of the table with cups on it. In image 2, can see white jar in front of the clothes rack. In image 3, can see white jar in front of the bed sheet with floral pattern. In image 4, can see white jar in front of the white headboard. To identify the position change across views, focus on the main objects angle variation. Then, analyze the angles and relative positions of other objects on the platform to back up this observation. understand that: Image 1 is the initial view. Image 2 is captured after 90-degree clockwise rotation from image 1. Image 3 is after another 90-degree clockwise rotation (180 degrees from image 1). Image 4 is after further 90-degree clockwise rotation (270 degrees from image 1). Through analyzing these perspective changes, can construct complete spatial understanding: when view clothes rack behind white jar in the second view, it implies that in the first view, clothes rack is on the right side of white jar. Similarly, when see white headboard behind white jar in the fourth view, it indicates that in the first view, white headboard is on the left side of white jar. However, am still uncertain about what lies behind me in the first view. Then, recognize that can examine the opposite view to find out. The opposite view of the fist view is the third view. As bed sheet with floral pattern is observed behind white jar in the third view, it means that in the first view, bed sheet with floral pattern is positioned behind me. This way, can fully comprehend the spatial relationships of all objects in the entire scene. So, from the perspective of image 4: clothes rack is to the right of white jar, bed sheet with floral pattern is to my behind, table with cups on it is to the left of white jar. So the answer is A. Table with cups on it.</think><answer>A. Table with cups on it</answer> Output for Plain-CGMap-FFR-Out: Input Prompt Refer to C.3. Based on my observation, the answer is: <cogmap> 61 json { \"white jar\": {\"position\": [5, 5]}, \"bed sheet with floral pattern\": {\"position\": [5, 8]}, \"white headboard\": {\"position\": [2, 5]}, \"clothes rack\": {\"position\": [5, 2]}, \"table with cups on it\": {\"position\": [8, 5]} } </cogmap> <think>In this scene, observe four images showing different perspectives. All images feature the white jar as the main object. In image 1, can see white jar in front of the table with cups on it. In image 2, can see white jar in front of the clothes rack. In image 3, can see white jar in front of the bed sheet with floral pattern. In image 4, can see white jar in front of the white headboard. To identify the position change across views, focus on the main objects angle variation. Then, analyze the angles and relative positions of other objects on the platform to back up this observation. understand that: Image 1 is the initial view. Image 2 is captured after 90-degree clockwise rotation from image 1. Image 3 is after another 90-degree clockwise rotation (180 degrees from image 1). Image 4 is after further 90-degree clockwise rotation (270 degrees from image 1). Through analyzing these perspective changes, can construct complete spatial understanding: when view clothes rack behind white jar in the second view, it implies that in the first view, clothes rack is on the right side of white jar. Similarly, when see white headboard behind white jar in the fourth view, it indicates that in the first view, white headboard is on the left side of white jar. However, am still uncertain about what lies behind me in the first view. Then, recognize that can examine the opposite view to find out. The opposite view of the fist view is the third view. As bed sheet with floral pattern is observed behind white jar in the third view, it means that in the first view, bed sheet with floral pattern is positioned behind me. This way, can fully comprehend the spatial relationships of all objects in the entire scene. So, from the perspective of image 4: clothes rack is to the right of white jar, bed sheet with floral pattern is to my behind, table with cups on it is to the left of white jar. So the answer is A. Table with cups on it.</think><answer>A. Table with cups on it</answer> E.4. Detailed Graph Metric Results for SFT Graph-Related Experiments This section provides detailed look at the Supervised Fine-Tuning (SFT) training dynamics to support the main papers conclusions. The figures below plot key metrics over training steps for four map-generation settings. comparative analysis highlights that jointly training map generation and reasoning is the most effective strategy. When training on map generation alone, as in the Plain-CGMap-Out and Aug-CGMap-Out settings, the graph quality metrics show rapid convergence. However, the final QA accuracy is limited, reaching 54.38% for Plain-CGMap-Out and 54.19% for Aug-CGMap-Out. In contrast, the joint training approaches (Plain-CGMap-FFR-Out and Aug-CGMap-FFR-Out), despite slower initial convergence on graph quality metrics, ultimately achieve far superior performance in task accuracy. The Plain-CGMap-FFR-Out setting proves to be the most effective, reaching QA Accuracy of 60.00%. The Aug-CGMap-FFR-Out setting also yields strong results, with QA accuracy climbing to about 65%. This demonstrates the superiority of joint training for achieving high performance in both the final task accuracy and the quality of the generated spatial representations. Figure 16 Training dynamics for the Aug-CGMap-Out setting. Figure 17 Training dynamics for the Plain-CGMap-Out setting. Figure 18 Training dynamics for the Aug-CGMap-FFR-Out setting. Figure 19 Training dynamics for the Plain-CGMap-FFR-Out setting, showing superior final performance. E.5. Which Part of VLM is the Bottleneck for Spatial Understanding? To develop more efficient fine-tuning strategies, it is crucial to understand which component of Vision-Language Model (VLM)the vision encoder responsible for perception or the Large Language Model (LLM) responsible for reasoningpresents the primary bottleneck for spatial understanding. To investigate this, we conduct bottleneck analysis by selectively fine-tuning different parts of the VLM and observing the impact on performance. 63 We evaluate four distinct training configurations on the Raw-QA task, with results captured at an early stage of training (step 57) to assess the initial learning dynamics. The configurations are: (1) the baseline performance of the pre-trained model without any fine-tuning; (2) fine-tuning only the vision encoder while keeping the LLM frozen; (3) fine-tuning only the LLM while keeping the vision encoder frozen; and (4) the standard approach of fine-tuning all parts of the model. Table 7 VLM Training Bottleneck Analysis (Step=57, in %). Performance is measured on the D E - Y benchmark under the Raw-QA setting. Training Method Overall Rotation Among Around Raw-QA (no fine-tuning) Freeze LLM (Vision Encoder Only) Freeze Vision Encoder (LLM Only) Tune All Parts 37.81 37.81 51.43 52.28 34.00 30.50 34.00 34.50 36.00 37.00 50.00 52.50 45.20 45.60 68.80 66.00 The results, presented in Table 7, offer several key insights. First, there is dramatic performance leap from the no-fine-tuning baseline (37.81% overall), but only when the language model is trained. Methods involving LLM fine-tuning achieve over 51% accuracy, underscoring the necessity of adapting the models reasoning capabilities. Most strikingly, the performance bottleneck is almost exclusively concentrated in the LLM. Tuning only the LLM (Freeze Vision Encoder) yields an overall accuracy of 51.43%, capturing nearly the full performance gain of end-to-end fine-tuning (52.28%). In stark contrast, tuning only the vision encoder (Freeze LLM) provides no improvement whatsoever over the baseline (37.81%). This indicates that the bottleneck is not shared between modules. For this spatial task, adapting the models language-based reasoning is critical, while adapting its visual perception is surprisingly ineffective. Intriguingly, the fact that fine-tuning only the vision encoder fails to improve performance is in itself significant finding. possible explanation is that the pre-trained visual features are already sufficient to extract the necessary objects and their properties. The core challenge of the task seems to lie not in what is seen, but in how to reason about the spatial relationships across series of viewsa task primarily handled by the LLM. In conclusion, our analysis suggests that the most significant gains come from adapting the reasoning module. For efficient tuning, freezing the vision encoder and focusing solely on the LLM proves to be highly effective strategy, achieving nearly top-tier performance at fraction of the computational cost. E.6. Branching from Raw-QA SFT Checkpoint In our main experiments, we fine-tuned the model for each specific task format starting from the base pre-trained VLM. natural question arises: can curriculum-based SFT approach further improve performance? Specifically, we investigate whether first fine-tuning the model on the simplest task formatRaw-QA, which only requires outputting the final answercan establish better foundation for learning to leverage more complex reasoning formats. To test this hypothesis, we conducted set of branching experiments. We took the checkpoint from the model fully fine-tuned on the Raw-QA task. Then, we used this specialized checkpoint as the initial weights for further fine-tuning on other scaffolding tasks, namely Aug-CGMap-In, FF Rsn, and Aug-CGMap-FFR-Out. It is important to note that during this second stage of fine-tuning, the models output for all tasks was still constrained to be only the final answer option. This setup allows us to isolate the effect of the cognitive scaffolds on the models internal reasoning process, rather than its ability to generate complex text. The results, presented in Table 8, show consistent and notable improvement across all branched tasks compared to their counterparts trained from scratch. For example, both Aug-CGMap-In and Aug-CGMap-FFR-Out reach an impressive overall accuracy of 49.00%. Even the FF Rsn method benefits from this two-stage approach, with its overall accuracy rising to 46.82%. These findings suggest that two-stage SFT strategy is highly effective. By first grounding the model in the fundamental objective of the task (i.e., finding the correct answer) and then teaching it to process and leverage more complex cognitive scaffolds, we can achieve superior spatial reasoning performance. This indicates that the model, once primed for the core task, becomes more adept at utilizing the provided spatial context, even if it does not explicitly generate the reasoning chain or cognitive map. Table 8 Performance of various methods after being fine-tuned from Raw-QA SFT checkpoint. This two-stage training approach led to performance gains across all methods. All accuracies are reported as percentages (%). Method Overall Rotation Among Around Raw-QA Aug-CGMap-In FF Rsn Aug-CGMap-FFR-Out 46.36 49.00 46.82 49.00 33.50 35.50 37.00 37.00 51.20 53.20 50.60 53.20 46.75 50.50 47.00 49.75 F. Can Reinforcement Learning Further Refine Spatial Thought Processes? As discussed in the main paper, while Supervised Fine-Tuning (SFT) establishes strong foundation for spatial reasoning, reinforcement learning (RL) presents an avenue for further optimizing spatial thought processes through outcome-driven feedback. The core inquiry is whether guiding VLMs with rewards can lead to the development of more precise spatial mental models and enhanced reasoning capabilities. This section of the appendix provides more detailed exposition of the experimental setup employed for the RL phase of our research. Additionally, we present case studies to offer qualitative insights into how RL refines the models spatial representations and reasoning chains. F.1. Detailed Experimental Setup For the reinforcement learning (RL) phase of our research, we employed the VAGEN framework. The core policy optimization algorithm used was Group Relative Policy Optimization (GRPO). To ensure consistency and allow for direct comparison with earlier stages of our work, key components from the Supervised Fine-Tuning (SFT) experiments were retained. Specifically, the base Vision-Language Model (VLM) for all RL configurations was Qwen2.5-VL-3B-Instruct, and evaluations were performed on the MINDCUBE-TINY benchmark. All previously established evaluation metrics were also retained. In consideration of computational costs, each distinct RL configuration was trained for duration of 0.5 epoch. The primary hyperparameters governing the RL training process were 65 set as follows: Training Batch Size: Maximum Prompt Length: 1024 tokens Maximum Response Length: 512 tokens Actor Learning Rate: 1 106 Critic Learning Rate: 1 105 Number of Trajectories per Rollout: 8 Maximum Turns per Trajectory: 1 As detailed in Section 5.1 of the main paper, we investigated three RL task configurations: 1. RL-FF-Rsn (from scratch): The Qwen2.5-VL-3B-Instruct model was trained to generate free-form reasoning chains without prior SFT for this specific task format. 2. RL-Aug-CGMap-FFR-Out (from scratch): The model was trained to jointly produce augmented cognitive maps and associated free-form reasoning, also starting from the base pre-trained VLM. 3. RL-Aug-CGMap-FFR-Out (from SFT): For this configuration, the RL training was initialized using the weights from the strongest performing SFT checkpoint, specifically the Aug-CGMapFFR-Out SFT model. The reward function was designed to be sparse yet directly indicative of desired behaviors. reward of +1 was assigned if the models output was structurally valid (e.g., the generated cognitive map adhered to the predefined schema). more significant reward of +5 was given if the model provided the correct final answer to the spatial reasoning question. This dualcomponent reward was intended to steer the learning process towards generating both wellformed intermediate spatial representations and achieving high end-task accuracy. F.2. VLM Response Examples After RL for Configurations in Section C.3 Output for FF-Rsn: Input Prompt Refer to C.3.2 <think>From image 4, the white jar is positioned on surface with floral-patterned bed sheet. To the left of the jar, there is table with pink tablecloth featuring heart shapes and Hello Kitty design. There are also cups on the table. Therefore, the object to the left of the white jar is the table with the cups.</think><answer>A. Table with cups on it</answer> Output for RL-Aug-CGMap-FFR-Out (from scratch): Input Prompt Refer to C.3.8 <cogmap> { \"objects\": [ {\"name\": \"white jar\", \"position\": [265, 436], \"facing\": \"left\"} ], \"views\": [ {\"name\": \"View/Image 1\", \"position\": [265, 436], \"facing\": \"left\"}, 66 {\"name\": \"View/Image 2\", \"position\": [265, 436], \"facing\": \"left\"}, {\"name\": \"View/Image 3\", \"position\": [265, 436], \"facing\": \"left\"}, {\"name\": \"View/Image 4\", \"position\": [265, 436], \"facing\": \"left\"} ] } <Your cognitive map> </cogmap> <think> The white jar is located in the center of the 10x10 grid, facing to the left. The clothes rack is to the left of the white jar, as seen in the image 2 view. The bed sheet with floral pattern is located to the right of the white jar, as seen in the image 1 view. The white headboard is not visible in any of the provided images. Based on the provided images, the white jar is located in the center of the 10x10 grid, facing to the left. The clothes rack is to the left of the white jar, as seen in the image 2 view. The bed sheet with floral pattern is located to the right of the white jar, as seen in the image 1 view. The white headboard is not visible in any of the provided images.</think><answer>A. Table with cups on it.</answer> Output for RL-Aug-CGMap-FFR-Out (from SFT): Input Prompt Refer to C.3.8 Based on my observation, the answer is: <cogmap> json { \"objects\": [ {\"name\": \"white jar\", \"position\": [5, 5]}, {\"name\": \"bed sheet with floral pattern\", \"position\": [5, 8]}, {\"name\": \"white headboard\", \"position\": [2, 5]}, {\"name\": \"clothes rack\", \"position\": [5, 2]}, {\"name\": \"table with cups on it\", \"position\": [8, 5]} ], \"views\": [ {\"name\": \"Image 1\", \"position\": [5, 6], \"facing\": \"up\"}, {\"name\": \"Image 2\", \"position\": [4, 5], \"facing\": \"right\"}, {\"name\": \"Image 3\", \"position\": [5, 4], \"facing\": \"down\"}, {\"name\": \"Image 4\", \"position\": [6, 5], \"facing\": \"left\"} ] } </cogmap> <think> In this scene, observe four images showing different perspectives. All images feature the white jar as the main object. In image 1, can see white jar in front of the clothes rack. In image 2, can see white jar in front of the table with cups on it. In image 3, can see white jar in front of the bed sheet with floral pattern. In image 4, can see white jar in front of the white headboard. To identify the position change across views, focus on the main objects angle variation. Then, analyze the 67 angles and relative positions of other objects on the platform to back up this observation. understand that: Image 1 is the initial view. Image 2 is captured after 90-degree clockwise rotation from image 1. Image 3 is after another 90-degree clockwise rotation (180 degrees from image 1). Image 4 is after further 90-degree clockwise rotation (270 degrees from image 1). Through analyzing these perspective changes, can construct complete spatial understanding: when view table with cups on it behind white jar in the second view, it implies that in the first view, table with cups on it is on the right side of white jar. Similarly, when see white headboard behind white jar in the fourth view, it indicates that in the first view, white headboard is on the left side of white jar. However, am still uncertain about what lies behind me in the first view. Then, recognize that can examine the opposite view to find out. The opposite view of the fist view is the third view. As bed sheet with floral pattern is observed behind white jar in the third view, it means that in the first view, bed sheet with floral pattern is positioned behind me. This way, can fully comprehend the spatial relationships of all objects in the entire scene. So, from the perspective of image 4: clothes rack is to the right of white jar, table with cups on it is to my behind, bed sheet with floral pattern is to the left of white jar. So the answer is C. Bed sheet with floral pattern.</think> <answer>C. Bed sheet with floral pattern</answer>"
        }
    ],
    "affiliations": [
        "New York University",
        "Northwestern University",
        "Stanford University",
        "University of Washington"
    ]
}