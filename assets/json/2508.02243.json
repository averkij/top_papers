{
    "paper_title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking",
    "authors": [
        "Ziyan Liu",
        "Junwen Li",
        "Kaiwen Li",
        "Tong Ruan",
        "Chao Wang",
        "Xinyan He",
        "Zongyu Wang",
        "Xuezhi Cao",
        "Jingping Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal entity linking plays a crucial role in a wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on a one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose a novel LLM-based framework for the multimodal entity linking task, called Intra- and Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intra- and inter-modality evaluations, it employs a multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at https://github.com/ziyan-xiaoyu/I2CR/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 3 4 2 2 0 . 8 0 5 2 : r I2CR: Intraand Inter-modal Collaborative Reflections for Multimodal Entity Linking Ziyan Liu East China University of Science and Technology Shanghai, China y30241069@mail.ecust.edu.cn Junwen Li East China University of Science and Technology Shanghai, China 18602126280@163.com Kaiwen Li South China University of Technology Guangzhou, China 202164030293@mail.scut.edu.cn Tong Ruan East China University of Science and Technology Shanghai, China ruantong@ecust.edu.cn Chao Wang Shanghai University Shanghai, China cwang@shu.edu.cn Xinyan He Meituan Shanghai, China hexinyan03@meituan.com Zongyu Wang Meituan Shanghai, China wangzongyu02@meituan.com Xuezhi Cao Meituan Shanghai, China caoxuezhi@meituan.com Jingping Liu East China University of Science and Technology Shanghai, China jingpingliu@ecust.edu.cn Abstract Multimodal entity linking plays crucial role in wide range of applications. Recent advances in large language model-based methods have become the dominant paradigm for this task, effectively leveraging both textual and visual modalities to enhance performance. Despite their success, these methods still face two challenges, including unnecessary incorporation of image data in certain scenarios and the reliance only on one-time extraction of visual features, which can undermine their effectiveness and accuracy. To address these challenges, we propose novel LLM-based framework for the multimodal entity linking task, called Intraand Inter-modal Collaborative Reflections. This framework prioritizes leveraging text information to address the task. When text alone is insufficient to link the correct entity through intraand inter-modality evaluations, it employs multi-round iterative strategy that integrates key visual clues from various aspects of the image to support reasoning and enhance matching accuracy. Extensive experiments on three widely used public datasets demonstrate that our framework consistently outperforms current state-of-the-art methods in the task, achieving improvements of 3.2%, 5.1%, and 1.6%, respectively. Our code is available at https://github.com/ziyan-xiaoyu/I2CR/. Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. MM 25, Dublin, Ireland 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-2035-2/2025/10 https://doi.org/10.1145/3746027. CCS Concepts Information systems Retrieval models and ranking. Keywords Multimodal Entity Linking, Large Language Model, Collaborative Reflection ACM Reference Format: Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, and Jingping Liu. 2025. I2CR: Intraand Intermodal Collaborative Reflections for Multimodal Entity Linking. In Proceedings of the 33rd ACM International Conference on Multimedia (MM 25), October 2731, 2025, Dublin, Ireland. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3746027."
        },
        {
            "title": "1 Introduction\nEntity linking[4, 29, 30], which maps ambiguous mentions in text to\nthe standard entities in a knowledge graph (KG)[5], e.g., Wikidata,\nis essential for applications such as question answering [19, 28],\nand recommendation systems [7]. As multimodal contexts (images\nand text) become more common in real-world scenarios, recent\nstudies [16, 39, 45] suggest incorporating images to enhance entity\ndisambiguation, leading to the emergence of Multimodal Entity\nLinking (MEL)[11].",
            "content": "In the early stages, numerous deep learning (DL)-based[15] methods have been developed for the MEL task. These methods generally extract unimodal features for the text and image separately, followed by feature fusion [20, 34, 35], or directly extract multimodal features from the text and image through cross-attention mechanisms [8, 47]. These extracted features are then compared with entity features to identify the target that best matches the given mention. However, such methods face two challenges: 1) The models lack sufficient prior knowledge, making it difficult to handle MM 25, October 2731, 2025, Dublin, Ireland Ziyan Liu et al. Reflections (denoted as I2CR) for the MEL task. It primarily relies on text information to solve problems, then verifies the results using intraand inter-modal strategies, and finally introduces visual clues through multi-round iterative process when text alone is insufficient. Specifically, we first utilize the lexical relevance matching method and LLM fine-tuned on MEL data to select candidate entity from the KG for the given mention. To ensure the selection is faithful to the mention context, we then design intra-modal consistency reflection, which evaluates the semantic granularity between the mention and the selected entity. If the alignment is weak, this step triggers entity re-selection. Finally, we propose an innovative utilization of images through modality alignment reflection strategy. This includes inter-modal alignment check and visual iterative feedback to decide whether visual information is necessary for refining the selection. When image input is required, we perform multiple rounds of iteration using various image-to-text models, leveraging visual clues from different perspectives while avoiding information overload. Unlike methods that directly fuse text and image modalities, our strategy simplifies the task and reduces noise introduced to the LLM. Contributions. The contributions of this study are summarized as follows: We propose novel I2CR framework that primarily relies on text information and integrates visual clues through multiround iterative process only when text alone is insufficient, reducing images noise introduced to the LLM. We propose novel approach for image utilization that leverages the visual modality to verify text-based results and transforms image information into diverse textual visual cues, improving the entity matching performance of LLMs. Experimental results demonstrate that our model achieves state-of-the-art (SoTA) performance on three widely used datasets, with top-1 accuracy of 92.2% (+3.2%) on WikiMEL, 91.6% (+5.1%) on WikiDiverse, and 86.8% (+1.6%) on RichMEL."
        },
        {
            "title": "2 Related Work\nRelated work in this study can be divided into three categories:\nentity linking, multimodal entity linking, and LLM-based reflection.\nEntity linking. Most previous methods for entity linking using\nonly text follow two stages: retrieval[6] followed by re-ranking[3,\n33]. In the retrieval stage, the system searches a large-scale KG to\nfind entities related to the mention. In the re-ranking stage, it scores\nand ranks these candidates to choose the best one. For instance,\nBLINK [41] uses a bi-encoder[46] to represent the context of the\nmention and the descriptions of entities to retrieve candidates. It\nthen uses a cross-encoder to rank them. Similarly, Xu et al. [43]\nuse dual encoders for retrieval and a LUKE-based cross-encoder\nfor ranking. EPGEL [14] builds an entity profile and uses a seq2seq\nmodel[10, 36] to rank candidates. However, when the text is brief or\nvague, itâ€™s difficult to link entities accurately using only text[1]. In\nthese cases, adding image information can help improve the results.\nMultimodal entity linking. Existing approaches for MEL gen-\nerally fall into two categories: traditional DL- and LLM-based meth-\nods. In the first group, traditional DL-based methods usually encode\nthe text and image of a mention separately, then combine these\nfeatures to align them with candidate entities. For instance, MIMIC",
            "content": "Figure 1: Two limitations of current LLM-based methods for the MEL task: (a) unnecessary incorporation of image data, and (b) reliance on one-time extraction of visual features. samples requiring deeper reasoning. For instance, given the mention Presidential State Car, DL-based methods might identify the car in the mention image as Cadillac limousine. However, without knowing that this type of car is used by the U.S. president, it may not link it to the correct entity Presidential state car (United States). 2) These models also dont generalize well because theyre trained on one specific task with limited data. They often learn patterns that only work for the training examples. So, if the input changes even slightly, the model might get confused. Instead of learning how to deal with new situations, it mostly just memorizes the training data. With the rise of large language models (LLMs), LLM-based methods have become the mainstream paradigm for solving MEL tasks. These methods typically feed both text and images (or its visual features extracted by an image encoder) into the multimodal LLM (MLLM), enabling the model to select the most relevant entity from the KG for given mention[17, 31]. Since LLMs are trained on vast corpora, they possess extensive knowledge and robust generalization capabilities, which allow them to overcome the issues of insufficient prior knowledge and limited generalization faced by traditional DL-based MEL methods. However, two main challenges remain: 1) In the MEL task, not all scenarios require additional image information. For instance, in Figure 1(a), the model can correctly infer that the mention MySpace refers to website based on the textual context alone. However, when the image is introduced (showing person holding gun), it may mislead the model into selecting person or viral video instead of the correct entity. 2) Existing methods typically rely on single-pass approach to extract visual features from an image or generate its text description. However, this one-time processing often fails to capture the full range of relevant information, leading to incomplete or inaccurate understanding of the given mention image. As shown in Figure 1(b), if the model relies solely on the description generated by the LLM, it may incorrectly identify the mention Thorne as an actress. However, if the OCR result from the image, which shows the word MUSIC, is also considered, the model can more accurately infer that Thorne is not only an actress but also singer. To address the above problems, we propose an innovative framework known as LLM-based Intraand Inter-modal Collaborative I2CR: Intraand Inter-modal Collaborative Reflections for Multimodal Entity Linking MM 25, October 2731, 2025, Dublin, Ireland [20] extracts both global and local features from text and images using separate encoders, then uses three interaction units to calculate matching score for each mention-entity pair. Similarly, DWE [35] and DWE+ [34] enhance visual features by adding fine-grained image details (e.g., scene feature) and applying cross-modal enhancers to better align text and image modalities. Other approaches directly extract multimodal features from text and images, such as DZMNED employing modality attention [24], GHMFC using gated fusion and contrastive training [38], and OT-MEL modeling fusion as an optimal transport problem [47]. In the second group, GEMEL [31] introduces generative MEL method that directly generates the target entity name for given mention by training feature mapper to enable cross-modal interactions. UniMEL [17] leverages LLMs to enhance the representation of mentions and entities by incorporating both textual and visual information. It then uses LLMs to select the target entity from the candidates with only minimal fine-tuned. In this study, we adopt the second paradigm. However, as mentioned earlier, current LLM-based methods unnecessarily incorporate image data in some cases and rely only on one-time extraction of visual features, which may limit their effectiveness. LLM-based reflection. LLMs have been widely adopted for various tasks due to their impressive capabilities. However, they still face challenges like hallucinationsproducing inaccurate or fabricated contentand unfaithful reasoning. promising solution is to incorporate reflection steps [25], allowing models to self-refine their own outputs. Reflection feedback can be divided into two main types: 1) Self-provided feedback from the LLM itself [32], and 2) Externally provided feedback [26]. The first type uses the LLM for both evaluation and refinement. Techniques like SELFCHECK [23] and SELF-REFINE [21] follow an iterative process where the LLM continuously evaluates and refines its output until certain predefined criteria are met. The second type uses external toolssuch as separately trained models [2], additional domainspecific knowledge [26], or other resources [40]to assess and enhance the LLMs responses. External feedback adds flexibility by providing information beyond the LLMs own knowledge, helping to identify errors or inconsistencies the model might miss. It is especially useful when the LLMs self-evaluation is insufficient. In our framework, both intra-modal consistency reflection and inter-modal alignment verification belong to the second category. separately trained model assesses whether the LLM needs to reselect candidate entities. The latter also extracts visual clues using image-to-text models and fed back into the LLM."
        },
        {
            "title": "3.2 Framework\nAs shown in Figure 2, our solution for MEL can be roughly divided\ninto the following four steps:",
            "content": "(1) Target Entity Selection (TES). We utilize lexical relevance matching approach, followed by fine-tuned LLM, to initially select the Top-1 relevant entity from the KG for given mention. (2) Intra-modal Consistency Reflection (ICR). We utilize an advanced embedding model to calculate the similarity between the entity description and the mentions textboth within the text modalityto assess the semantic consistency between the candidate entity selected in Step 1 and the given mention. If the consistency is low, the process returns to Step 1 to select new candidate entity. (3) Inter-modal Alignment Verification (IAV). multimodal pre-trained model is introduced to evaluate the alignment between the entity description and the given mention imageinformation from different modalities. If the matching degree exceeds the predefined threshold, the entity is selected as the final output. If not, we move to the next step. (4) Visual Iterative Feedback (VIF). Visual clues are extracted from the mention image and used as the input for step 1 in the subsequent iteration. Note that in each iteration, we use different extraction strategies to capture diverse semantic aspects of the mention image for reasoning."
        },
        {
            "title": "4.1 Target Entity Selection\nThe purpose of this step is to select the Top-1 entity from the KG\nthat is most relevant to the given mention. This is achieved in two\nstages. In the first stage, given a mention ğ‘š and its surrounding\ntextual context ğ‘‡ğ‘š, we utilize a fuzzy string matching method1 to\nretrieve the Top-ğ‘˜ lexically relevant candidate entities from the KG,\nalong with their descriptions [38, 39]. This reduces the candidate\nspace and controls the input length for the LLM used in the next\nstage. In the second stage, we fine-tune a LLM using an instruction\ndataset constructed from the training set, where each instance in-\ncludes: (1) an instruction prompt, (2) a dictionary-style input that\nincludes the mention, its context, and candidate entities, and (3)\nthe ground-truth entity label. An example of this format is shown\nin Figure 3. The fine-tuned model is then used to select the most\nappropriate entity ğ‘’, with its description ğ·ğ‘’ , from the Top-ğ‘˜ candi-\ndates produced in the first stage. Two important considerations are\naddressed in this process: (1) The given mention may not have a\ncorresponding entity in the KG, representing an out-of-KG linking\nscenario [31]. To handle this, we explicitly instruct the LLM that the\noutput may be empty (i.e., â€œnilâ€) when no suitable entity is found.\n2) In the first iteration, the LLM relies only on the mention and its\ntextual context. In the second and subsequent iterations, the input\nis augmented with visual clues extracted in Step 4 of the previous\niteration, enabling the model to make more reasonable selection.",
            "content": "1https://github.com/seatgeek/fuzzywuzzy MM 25, October 2731, 2025, Dublin, Ireland Ziyan Liu et al. Figure 2: Our I2CR framework for the MEL task. Similarly, we construct textual representation for the entity by concatenating the entity name ğ‘’ with its associated description ğ·ğ‘’ , selected during Step 1, i.e., ğ¶ğ‘’ = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ (ğ‘’, ğ·ğ‘’ ). Then, we employ an advanced embedding model to encode both ğ¶ğ‘š and ğ¶ğ‘’ into semantic vector representations. The degree of semantic alignment is measured by computing the similarity between the two embeddings, implemented as normalized dot product: (2) ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘–ğ‘ğ‘Ÿ = ğ‘ğ‘œğ‘Ÿğ‘š (ğ¸ğ‘šğ‘ğ‘’ğ‘‘ (ğ¶ğ‘š) ğ¸ğ‘šğ‘ğ‘’ğ‘‘ (ğ¶ğ‘’ )) , where ğ‘ğ‘œğ‘Ÿğ‘š() represents the normalization operation. If ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’_ğ‘–ğ‘ğ‘Ÿ > ğ›¼, where ğ›¼ is predefined threshold, the candidate entity ğ‘’ is considered semantically consistent with the mention ğ‘š. Otherwise, the process returns to the step 1, removes ğ‘’ from the candidate set, and proceeds to reselect more appropriate entity. This process continues until the selected entity is semantically consistent with the given mention or the predefined iteration limit is reached. If the limit is reached without finding reasonable entity, the last selected entity is used as the target."
        },
        {
            "title": "4.3 Inter-Modal Alignment Verification\nThe objective of this step is to verify whether the entity selected in\nthe previous step remains semantically consistent with the given\nmention across different modalities. Specifically, given the textual\ndescription of the candidate entity ğ·ğ‘’ and the mention image ğ¼ğ‘š,\nwe utilize a multimodal pre-trained model (e.g., CLIP [27]) to project\nboth inputs into a shared embedding space. The modelâ€™s text en-\ncoder ğ¸ğ‘›ğ‘ğ‘‡ processes the entity description ğ·ğ‘’ , while the image en-\ncoder ğ¸ğ‘›ğ‘ğ¼ encodes the mention image ğ¼ğ‘š. The dot product between\nthe resulting embeddings yields a similarity score that reflects the\ncross-modal alignment between the entity and the mention. This\ncan be formally defined as:",
            "content": "ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘–ğ‘ğ‘£ = ğ¸ğ‘›ğ‘ğ‘‡ (ğ·ğ‘’ ) ğ¸ğ‘›ğ‘ğ¼ (ğ¼ğ‘š), (3) where ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ğ‘–ğ‘ğ‘£ denotes the inter-modal alignment score. If this score exceeds predefined threshold ğ›½, the candidate entity ğ‘’ is taken as the final result. Otherwise, the framework proceeds to Figure 3: Instruction example for target entity selection."
        },
        {
            "title": "4.2 Intra-modal Consistency Reflection\nThis step aims to evaluate the semantic consistency between the\ngiven mention and the candidate entity within the textual modality.\nTo construct a comprehensive textual representation of the mention,\nwe concatenate the mention ğ‘š with its textual context ğ‘‡ğ‘š to form\nğ¶ğ‘š. If the current iteration is at least 2, we additionally incorporate\nthe mentionâ€™s image-derived text ğ¼ â€²ğ‘š (see Section 4.4 for details),\nresulting in a richer and more informative input. Formally, this\nprocess is defined as:\n(cid:40)",
            "content": "ğ¶ğ‘š = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ (ğ‘š,ğ‘‡ğ‘š), ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ (ğ‘š,ğ‘‡ğ‘š, ğ¼ ğ‘š), ğ‘Ÿ = 1 ğ‘Ÿ 2 (1) I2CR: Intraand Inter-modal Collaborative Reflections for Multimodal Entity Linking MM 25, October 2731, 2025, Dublin, Ireland Table 1: Statistics of three MEL datasets. WikiMEL Wikidiverse RichMEL # samples # mentions # entities # mention images # text length 22,136 25,846 17,890 22,136 8.2 7,824 16,327 78,556 12,909 10. 17,806 18,752 72,085 15,853 13.6 the next step to explore alternative candidates using additional multi-modal information."
        },
        {
            "title": "5.1 Experimental Setup\nDatasets. We employ three widely used datasets for evaluation:\nWikiMEL [38], WikiDiverse [39], and Richpedia-MEL (denoted as\nRichMEL) [38]. WikiMEL collects data from Wikipediaâ€™s entity\npages, containing more than 22k multimodal sentences, with its\nprimary entity type being Person. It uses Wikidata as its target\nKG. WikiDiverse is built by Wikinews, covering 7 types of entities\n(i.e., Person, Organization, Location, Country, Event, Works, and\nMisc). It utilizes Wikipedia as its target KG. RichMEL compiles the\nWikidata information for entities in Richpedia [37] and gathers\nmultimodal data from Wikipedia. We adhere to the original dataset\nsplits: WikiDiverse follows an 8:1:1 partition for training, validation,\nand test sets, respectively, while WikiMEL and RichMEL utilize a\n7:1:2 split. Dataset statistics are summarized in Table 1.",
            "content": "Baselines. We compare our proposed framework with various SoTA methods, which are divided into two groups: Text-only methods: BERT [13] is used to encode the mention and entity to extract features for relevance scoring. RoBERTa [18] is BERT variant. BLINK [41] is two-stage method: retrieval and re-ranking. Visual-text fusion methods: CLIP [27] uses Transformerbased encoders for visual-text feature extraction. DZMNED [24] combines visual, textual, and character features with modality attention. JMEL [1] maps visual and textual features to joint space. GHMFC [38] employs gated fusion and contrastive training for entity linking. MIMIC [20] fuses features via interaction units with contrastive learning. DRIN [42] uses dynamic GCN for mention-entity alignment. GEMEL [31] leverages LLMs for generative linking with in-context learning. DWE [35] integrates text and visual features, refining with Wikipedia. DWE+[34] is vriant of DWE. OT-MEL [47] formulates the correlation assignment as an optimal transport problem. Unimel [17] use LLMs to select the targe entity. Metrics. Following previous works [34, 47], we employ the Topğ¾ accuracy metric for evaluation: ğ‘‡ğ‘œğ‘-ğ¾ = 1 ğ‘ ğ‘ ğ¼ (ğ‘”ğ‘– ğ‘ƒğ¾ ğ‘– ), (4) ğ‘–=1 where ğ‘ denotes the total number of samples, ğ‘”ğ‘– is the ground truth entity, ğ‘ƒğ¾ represents the set of the top-ğ¾ predicted entities, and ğ‘– ğ¼ () is the indicator function. The process of selecting the Top-K (ğ¾=3 or 5) results in our I2CR framework is as follows: In the first step (TES), we iteratively perform the process until the number of entities in the candidate set reaches ğ¾. In the second (ICR) and third (IAV) steps, we rank the entities that do not meet the judgment criteria at the end of the set. If none of the entities in the set meet the criteria, we iteratively reselect Top-ğ¾ candidate entities. Implementation details. During training, we fine-tune Llama38B [9] using LoRA [12] with the AdamW optimizer, learning rate of 2e-5, batch size 1, gradient accumulation steps of 16, and 100 warm-up steps. To test the robustness and generalization of our framework, we train it only on WikiDiverse and test it on three datasets. Since the entity descriptions in WikiDiverse are longer and more complex, using them directly as input may exceed the LLMs input limits and cause the model to truncate the text. This would affect the models ability to extract and understand the information accurately. To solve this, we follow prior work [17] and use Llama3-8B with prompt to summarize the descriptions, shortening them to length similar to those in WikiMEL and RichMEL. For inference, the temperature is set to 0.9, with other parameters at defaults. In Section 4.1, we set ğ‘˜=10. In Section 4.2, we consider SFR-Embedding-Mistral [22] as our embedding model. We set ğ›¼=0.5 for WikiMEL, 0.8 for WikiDiverse, and 0.75 for RichMEL. In Section 4.3, we use CLIP2 as the multimodal pre-trained model and set ğ›½=31 for all three datasets. ğ›¼ and ğ›½ are determined through the validation set. In Section 4.4, we employ four latest models (i.e., OCR, Image Captioning, Dense Captioning, and Image Tagging) from Azure Cognitive Services APIs3 as the image-to-text models. Our experiments are conducted on workstation running Ubuntu 20.04.6 LTS, with two NVIDIA Tesla A100 GPUs, and 80GB of memory."
        },
        {
            "title": "5.2 Main Results\nWe evaluate our framework against several strong baselines on\nthree datasets: WIKIMEL, WikiDiverse, and RichMEL. The top-1,",
            "content": "2CLIP-ViT-bigG-14-laion2B-39B-b160k 3https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/ CognitiveServicesHub//ComputerVision MM 25, October 2731, 2025, Dublin, Ireland Ziyan Liu et al. Table 2: Comparison results of different methods on three datasets. Bold indicates the best result, and underline indicates the second best. The numbers in parentheses represent the standard deviation from three rounds of results. Modality Model WikiMEL Wikidiverse RichMEL Top-1 Top-3 TopTop-1 Top-3 Top-5 Top-1 Top-3 TopText-only Visual-text BERT BLINK RoBERTa CLIP DZMNED JMEL GHMFC MIMIC DRIN DWE DWE+ GEMEL OT-MEL UniMEL 31.7 30.8 73.8 83.2 78.8 64.7 76.6 88.0 65.5 44.5 44.9 82.6 89.0 88. 40.4 38.9 85.9 92.1 90.0 80.0 88.4 95.1 - 59.7 60.4 - 95.6 95.2 48.8 44.6 89.8 94.5 92.6 84.3 92.0 96.4 91.3 66.3 67.2 - 97.0 96.9 22.2 70.9 59.5 61.2 56.9 37.4 60.3 63.5 51.1 46.6 47.1 86.3 66.1 86. 37.9 72.8 78.5 79.6 75.3 54.2 79.4 81.0 77.9 74.5 75.6 - 82.8 89.8 53.8 74.2 85.1 85.2 81.4 61.0 84.7 86.4 89.3 80.9 81.3 - 87.4 90.4 31.6 30.8 61.3 67.8 68.2 48.8 72.9 81.0 - 64.6 67.0 - 83.3 85. 36.1 35.7 81.6 85.2 82.9 66.8 86.9 91.8 - 87.4 88.3 - 92.4 92.6 42.0 38.8 87.2 90.0 87.3 74.0 90.6 94.4 - 96.6 97.1 - 94.8 96.8 Visual-text I2CR(ours) 92.2(0.24) 96.1(0.17) 97.5(0.05) 91.6(0.26) 94.7(0.22) 95.6(0.08) 86.8(0.20) 92.9(0.16) 97.2(0.05) top-3, and top-5 accuracies for all models on these datasets are presented in Table 2. From the results, we observe that our I2CR framework outperforms existing SoTA methods on all three datasets, demonstrating its effectiveness. Specifically, our framework achieves top-1 accuracies of 92.2%, 91.6%, and 86.8% on the three datasets, respectively, which are 3.2%, 5.1%, and 1.6% higher than the previous best methods. Notably, our framework is trained only on WikiDiverse, not on WikiMEL or RichMEL, yet it still achieves SoTA performance on these two datasets. This demonstrates the robustness and strong generalization ability of our framework. In addition, the performance on RichMEL is relatively lower compared to WikiMEL and WikiDiverse. From our analysis of the error cases, we find that RichMEL contains samples that require more strong prior knowledge and complex reasoning. For instance, mention refers to an athlete, but the textual context describes this individuals movie experience, and the accompanying image is an ID photo. This type of samples requires the model to possess prior knowledge that the athlete has also been involved in movies, which poses greater challenge."
        },
        {
            "title": "5.3 Ablation Study\nAblation of modules in the overall framework. To evaluate the\neffectiveness of each module in our framework, we retain the TES\nmodule and systematically delete combinations of the remaining\nthree modules (i.e., ICR, IAV, and VIF). The results are in Table 3.\nFrom the results, we notice the following: 1) Deleting any module\nleads to a decrease in performance, with a drop of at least 1% or\nmore, validating the contribution of each module in our framework.\n2) Deleting VIF alone leads to the largest decrease in performance\ncompared to deleting ICR or IAV, highlighting the crucial role of\nvisual information for our framework. This finding is further con-\nfirmed by experiments where two modules are deleted simulta-\nneously. 3) Even when all three modules (ICR, IAV, and VIF) are\ndeleted, leaving only the TES module with textual input, the model",
            "content": "Table 3: Ablation study of overall framework. b, c, and correspond to the module labels in the framework diagram, representing the ICR, IAV, and VIF modules, respectively. Model I2CR w/o w/o w/o w/o bc w/o bd w/o cd w/o bcd Top-1 Accuracy (%) WikiMEL Wikidiverse RichMEL 92.2 91.2 90.1 88. 89.3 87.3 86.4 86.0 91.6 90.1 90.7 89.4 89.3 89.1 88.9 88. 86.8 85.7 85.6 84.7 85.0 84.4 83.7 83.3 still achieves relatively good performance. This suggests that the TES module is effective in extracting the key clues from text alone. Ablation of sub-modules in VIF. To evaluate the effectiveness of different faceted visual clues in VIF, we systematically test all possible combinations of the four image-to-text models. The results are shown in Table 4. From the results, we conclude that: 1) All four sub-modules in VIF contribute positively to the overall performance of the framework, demonstrating that each faceted visual clue is valuable. 2) The contribution of each sub-module varies depending on the dataset. For instance, on WikiMEL and WikiDiverse, the image captioning sub-module provides the most significant improvement, while on RichMEL, OCR proves to be more important. In this study, we integrate all four sub-modules into the framework."
        },
        {
            "title": "5.4 Detailed Analysis\nVersatility of our I2CR framework across different LLMs. To\nfurther assess the versatility of our framework, we apply it to vari-\nous LLMs, including both open-source models (Qwen 2.5-7B [44],",
            "content": "I2CR: Intraand Inter-modal Collaborative Reflections for Multimodal Entity Linking MM 25, October 2731, 2025, Dublin, Ireland Table 4: Ablation study of VIF module. ocr, cap, den, tag indicate OCR, Image Captioning, Dense Captioning, and Image Tagging models, respectively. Table 5: Results of adapting our framework to different LLMs. Each LLM has two rows: The first row represents the output of the base LLM, while the second row represents the output of the LLM with our I2CR framework. Model I2CR w/o ocr w/o cap w/o den w/o tag w/o ocr, cap w/o ocr, den w/o ocr, tag w/o cap, den w/o cap, tag w/o den, tag w/o ocr, cap, den w/o ocr, cap, tag w/o ocr, den, tag w/o cap, den, tag w/o all Top-1 Accuracy (%) WikiMEL Wikidiverse RichMEL Model 92.2 91.7 91.3 91.9 91.8 90.4 91.4 91.1 91.3 90.8 91. 89.3 88.7 90.2 89.8 88.0 91.6 91.1 90.7 91.3 91.3 90.0 90.5 90.7 90.2 90.6 90.9 89.6 89.7 90.1 90. 89.4 86.8 86.0 86.3 86.5 86.6 85.4 85.7 85.9 86.0 86.1 86.3 85.0 85.2 85.4 85.7 84. Qwen 2.5-7B Qwen 2.5-7B++ Vicuna1.5-7B Vicuna1.5-7B++ Llama3-8B Llama3-8B++ Llama3-13B Llama3-13B++ GPT-3.5-turbo GPT-3.5-turbo++ GPT-4o GPT-4o++ Top-1 Accuracy (%) WikiMEL Wikidiverse RichMEL 88.4 90.6 51.3 66.7 86.0 92.2 87.7 92.6 89.0 91.0 93.0 97.0 87.5 89.1 72.0 74.7 88.6 91.6 88.7 91.3 62.0 74.0 68.0 81.0 79.8 82.1 62.9 69.0 83.3 86.8 82.5 85.9 84.0 86.0 85.0 88. Figure 4: Results across different iteration rounds and single iteration using all visual clues. Vicuna1.5-7B4, Llama3-8B, Llama3-13B [9]) and closed-source models (GPT-3.5-turbo and GPT-4o). For open-source models, there are two options: fine-tuning the LLM on the training set directly, or fine-tuning it using our framework. For closed-source models, the options are: using the LLM directly for inference, or using the LLM for inference with our I2CR framework. Since using closed-source models incurs API costs, we randomly select 100 samples from each of the three datasets for experiments. The results are shown in Table 5. Based on the results, we conclude that our framework improves performance for both open-source and closed-source LLMs, even for the larger 13B LLMs. Impact of visual clues on model performance. To assess the effectiveness of visual clues throughout the iteration process, we report the top-1 accuracy of the model at the different iteration round (round 1, round 2, ..., round 5). Additionally, we compare the effect of using single iteration, where all visual clues are integrated and fed back into the first step. The experimental results 4https://huggingface.co/lmsys/vicuna-7b-v1.5 Figure 5: Results of using visual clues in different orders. are presented in Figure 4. From these results, we observe: 1) As the number of iteration rounds increases, the models top-1 accuracy on the three datasets gradually improves, indicating that the different visual clues are beneficial to the model. 2) Feeding all visual clues at once shows slight improvement over the 1st round, but performs much worse than using single visual clue per iteration. This suggests that the integration of multiple clues may lead to information overload, making it harder for the model to accurately capture the key information. Impact of visual clue usage order on model performance. In the VIF module, we select one of four image-to-text models per iteration to extract visual clues from the given mention image, without repetition across iterations. In this experiment, we analyze the impact of the order in which visual clues are utilized on model performance. The experimental results are shown in Figure 5. We observe that the order of clue usage has minimal impact on model performance across the three datasets. The maximum performance difference between different orders is no more than 0.3%. Based on this observation, we empirically choose the ocr-cap-den-tag order for our experiments in the study. Average response time of different LLM-based methods. To evaluate the efficiency of our framework, we introduce the average response time for selecting the Top-1 entity across each input MM 25, October 2731, 2025, Dublin, Ireland Ziyan Liu et al. Figure 6: Case study. Blue for incorrect output, orange for correct output. Not Reached indicates that the corresponding sample already has final answer prior to this round. Table 6: Average response time (s) for returning the Top-1 result for all samples using different methods on each dataset. The values in the brackets at the lower right corner represent the accuracy improvement of I2CR compared to other LLMbased MEL methods (UniMEL and GEMEL). through (e) respectively validate the positive contributions of four different visual cuesOCR, Image Captioning, Dense Captioning, and Image Taggingwithin the iterative process. In conclusion, each iteration round effectively contributes to identifying the correct entity for the given mention."
        },
        {
            "title": "WikiMEL WikiDiverse RichMEL",
            "content": "I2CR 8.51 6.63 7.84 Avg. 7."
        },
        {
            "title": "UniMEL\nGEMEL",
            "content": "10.97 (3.5%) 4.15 (9.6%) 11.20 (5.1%) 4.53 (5.3%) 10.61 (1.6%) - 10.93 (3.4%) 4.34 (5.7%) mention in all samples as an additional metric, referred to as AvgTime. The comparison results with other LLM-based MEL methods (including UniMEL and GEMEL) are presented in Table 6. From the results, we observe the following: (1) Our proposed I2CR framework not only achieves 3.4% higher average accuracy compared to UniMEL, but also responds 3.27 seconds faster on average. This is because, for each sample, UniMEL requires at least two calls to the LLM and one to the MLLM. (2) Although our I2CR is slower than GEMEL in the Avg-Time metric, it achieves notable 5.7% higher average accuracy, significantly outperforming the GEMEL method. Case study. To more intuitively understand the importance of visual clues in each iteration, we provide case where the result in the ğ‘–-th (ğ‘– = 1, ..., 5) round is correct, while all prior rounds (1 to ğ‘–1) are incorrect. When ğ‘– = 1, the output of the (ğ‘–1)-th round refers to the result from the first step (the TES module) in the first iteration. The cases are shown in Figure 6. Case (a) highlights the positive role of intra-modal and inter-modal collaborative verification. Cases (b)"
        },
        {
            "title": "6 Conclusion and Limitations\nIn this study, we propose a novel LLM-based intra- and inter-modal\ncollaborative reflection framework for the MEL task. The frame-\nwork initially uses a fine-tuned LLM to select a candidate entity for\nthe given mention based on mention text. If the entity selected from\nthe text alone is determined to be incorrect through intra-modal\nconsistency reflection and inter-modal alignment verification, we\nintroduce a visual iterative feedback module. This module lever-\nages visual clues generated by multiple image-to-text models for\nthe mention image, assisting LLMs in refining its selection across\nseveral iterations to improve matching accuracy. Experimental re-\nsults on WikiMEL, WikiDiverse, and RichMEL demonstrate that our\nproposed framework achieves SoTA performance, with additional\ndetailed analyses validating the effectiveness of each component.\nAlthough the proposed I2CR framework significantly improves\nmodel performance for the MEL task, it has two main limitations:\n1) I2CR excels in handling common or general questions, but its\neffectiveness may be limited for long-tail questions (e.g., particularly\nrare mentions or entities). 2) I2CR is specifically designed for the\ntextual and visual modalities in MEL, without considering other\nforms of data, such as speech or video, which may also contribute\nvaluable information. We believe that with further optimization,\nour framework will evolve into a more comprehensive solution in\nthe future.",
            "content": "I2CR: Intraand Inter-modal Collaborative Reflections for Multimodal Entity Linking MM 25, October 2731, 2025, Dublin, Ireland Acknowledgments This paper was supported by the National Natural Science Foundation of China (No. 62306112), Shanghai Sailing Program (No. 23YF1409400), and Shanghai Pilot Program for Basic Research (No. 22TQ1400100-20). References [1] Omar Adjali, Romaric BesanÃ§on, Olivier Ferret, HervÃ© Le Borgne, and Brigitte Grau. 2020. Multimodal entity linking for tweets. In European Conference on Information Retrieval. Springer, 463478. [2] Afra Feyza AkyÃ¼rek, Ekin AkyÃ¼rek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. 2023. RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs. arXiv preprint arXiv:2305.08844 (2023). [3] Edoardo Barba, Luigi Procopio, and Roberto Navigli. 2022. ExtEnD: Extractive entity disambiguation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 24782488. [4] Lihan Chen, Tinghui Zhu, Jingping Liu, Jiaqing Liang, and Yanghua Xiao. 2023. End-to-end entity linking with hierarchical reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 41734181. [5] Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, et al. 2024. Knowledge graphs meet multi-modal learning: comprehensive survey. arXiv preprint arXiv:2402.05391 (2024). [6] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904 (2020). [7] Yashar Deldjoo, Markus Schedl, Paolo Cremonesi, and Gabriella Pasi. 2020. Recommender systems leveraging multimedia content. ACM Computing Surveys (CSUR) 53, 5 (2020), 138. [8] Zhang Dongjie and Longtao Huang. 2022. Multimodal Knowledge Learning for Named Entity Disambiguation. In Findings of the Association for Computational Linguistics: EMNLP 2022. 31603169. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [10] Elozino Egonmwan and Yllias Chali. 2019. Transformer and seq2seq model for paraphrase generation. In Proceedings of the 3rd Workshop on Neural Generation and Translation. 249255. [11] Jingru Gan, Jinchang Luo, Haiwei Wang, Shuhui Wang, Wei He, and Qingming Huang. 2021. Multimodal entity linking: new dataset and baseline. In Proceedings of the 29th ACM international conference on multimedia. 9931001. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [13] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, Vol. 1. Minneapolis, Minnesota. [14] Tuan Manh Lai, Heng Ji, and ChengXiang Zhai. 2022. Improving candidate retrieval with entity profile generation for wikidata entity linking. arXiv preprint arXiv:2202.13404 (2022). [15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436444. [16] Jingping Liu, Mingchuan Zhang, Weichen Li, Chao Wang, Shuang Li, Haiyun Jiang, Sihang Jiang, Yanghua Xiao, and Yunwen Chen. 2024. Beyond entities: large-scale multi-modal knowledge graph with triplet fact grounding. In Proceedings of the AAAI conference on artificial intelligence, Vol. 38. 1865318661. [17] Qi Liu, Yongyi He, Tong Xu, Defu Lian, Che Liu, Zhi Zheng, and Enhong Chen. 2024. Unimel: unified framework for multimodal entity linking with large language models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. 19091919. [18] Yinhan Liu. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 364 (2019). [19] Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. arXiv preprint arXiv:2109.05052 (2021). [20] Pengfei Luo, Tong Xu, Shiwei Wu, Chen Zhu, Linli Xu, and Enhong Chen. 2023. Multi-grained multimodal interaction network for entity linking. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 15831594. [21] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651 (2023). [22] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfrembedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog 3 (2024). [23] Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436 (2023). [24] Seungwhan Moon, Leonardo Neves, and Vitor Carvalho. 2018. Multimodal named entity disambiguation for noisy social media posts. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 20002008. [25] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. 2023. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188 (2023). [26] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 (2023). [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [28] Sanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual question answering. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 88768884. [29] Wei Shen, Yuhan Li, Yinan Liu, Jiawei Han, Jianyong Wang, and Xiaojie Yuan. 2021. Entity linking meets deep learning: Techniques and solutions. IEEE Transactions on Knowledge and Data Engineering 35, 3 (2021), 25562578. [30] Wei Shen, Jianyong Wang, and Jiawei Han. 2014. Entity linking with knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering 27, 2 (2014), 443460. [31] Senbao Shi, Zhenran Xu, Baotian Hu, and Min Zhang. 2024. Generative Multimodal Entity Linking. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 76547665. [32] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems. [33] Avirup Sil and Alexander Yates. 2013. Re-ranking for joint named-entity recognition and linking. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management. 23692374. [34] Shezheng Song, Shasha Li, Shan Zhao, Xiaopeng Li, Chengyu Wang, Jie Yu, Jun Ma, Tianwei Yan, Bin Ji, and Xiaoguang Mao. 2024. DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking. arXiv preprint arXiv:2404.04818 (2024). [35] Shezheng Song, Shan Zhao, Chengyu Wang, Tianwei Yan, Shasha Li, Xiaoguang Mao, and Meng Wang. 2024. dual-way enhanced framework from text matching point of view for multimodal entity linking. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1900819016. [36] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. 2017. Cold fusion: Training seq2seq models together with language models. arXiv preprint arXiv:1708.06426 (2017). [37] Meng Wang, Haofen Wang, Guilin Qi, and Qiushuo Zheng. 2020. Richpedia: large-scale, comprehensive multi-modal knowledge graph. Big Data Research 22 (2020), 100159. [38] Peng Wang, Jiangheng Wu, and Xiaohang Chen. 2022. Multimodal entity linking with gated hierarchical fusion and contrastive training. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 938948. [39] Xuwu Wang, Junfeng Tian, Min Gui, Zhixu Li, Rui Wang, Ming Yan, Lihan Chen, and Yanghua Xiao. 2022. WikiDiverse: Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 47854797. [40] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053 (2022). [41] Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable Zero-shot Entity Linking with Dense Entity Retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 63976407. [42] Shangyu Xing, Fei Zhao, Zhen Wu, Chunhui Li, Jianbing Zhang, and Xinyu Dai. 2023. DRIN: Dynamic Relation Interactive Network for Multimodal Entity Linking. In Proceedings of the 31st ACM International Conference on Multimedia. 35993608. [43] Zhenran Xu, Yulin Chen, Senbao Shi, and Baotian Hu. 2022. Enhancing entity linking with contextualized entity embeddings. In CCF International Conference on Natural Language Processing and Chinese Computing. Springer, 228239. [44] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. MM 25, October 2731, 2025, Dublin, Ireland Ziyan Liu et al. technical report. arXiv preprint arXiv:2412.15115 (2024). arXiv:2208.14565 (2022). [45] Barry Menglong Yao, Yu Chen, Qifan Wang, Sijia Wang, Minqian Liu, Zhiyang Xu, Licheng Yu, and Lifu Huang. 2023. AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes. arXiv preprint arXiv:2305.14725 (2023). [46] Sheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung Poon. 2022. Optimizing bi-encoder for named entity recognition via contrastive learning. arXiv preprint [47] Zefeng Zhang, Jiawei Sheng, Chuang Zhang, Yunzhi Liang, Wenyuan Zhang, Siqi Wang, and Tingwen Liu. 2024. Optimal Transport Guided Correlation Assignment for Multimodal Entity Linking. arXiv preprint arXiv:2406.01934 (2024)."
        }
    ],
    "affiliations": [
        "East China University of Science and Technology, Shanghai, China",
        "Meituan, Shanghai, China",
        "Shanghai University, Shanghai, China",
        "South China University of Technology, Guangzhou, China"
    ]
}