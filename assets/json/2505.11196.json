{
    "paper_title": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling",
    "authors": [
        "Yuang Ai",
        "Qihang Fan",
        "Xuefeng Hu",
        "Zhenheng Yang",
        "Ran He",
        "Huaibo Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns-highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53 at 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256x256-without any additional supervision during training. Code: https://github.com/shallowdream204/DiCo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 6 9 1 1 1 . 5 0 5 2 : r DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling Yuang Ai1,2 Qihang Fan1,2 Xuefeng Hu3 Zhenheng Yang3 Ran He1,2 Huaibo Huang1,2(cid:66) 1CASIA 2UCAS 3ByteDance shallowdream555@gmail.com, huaibo.huang@cripac.ia.ac.cn Code and models: https://github.com/shallowdream204/DiCo Figure 1: Diffusion ConvNet achieves state-of-the-art image quality with high efficiency. We show samples from two of our DiCo-XL models trained on ImageNet at 512512 and 256256 resolution, respectively."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformer (DiT), promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global selfattention is often redundant, predominantly capturing local patternshighlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with Preprint. Under review. significant efficiency gains. On class-conditional ImageNet benchmarks, DiCo outperforms previous diffusion models in both image quality and generation speed. Notably, DiCo-XL achieves an FID of 2.05 at 256256 resolution and 2.53 at 512512, with 2.7 and 3.1 speedup over DiT-XL/2, respectively. Furthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID of 1.90 on ImageNet 256256without any additional supervision during training."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [68, 70, 27, 69, 71] have sparked transformative advancement in generative learning, demonstrating remarkable capabilities in synthesizing highly photorealistic visual content. Their versatility and effectiveness have led to widespread adoption across broad spectrum of realworld applications, including text-to-image generation [60, 63, 61], image editing [53, 40, 7], image restoration [39, 1, 2], video generation [30, 82, 4], and 3D content creation [58, 81, 78]. Early diffusion models (e.g., ADM [11] and Stable Diffusion [61]) primarily employed hybrid U-Net [62] architectures that integrate convolutional layers with self-attention. More recently, Transformers [77] have emerged as more powerful and scalable backbone [56, 3], prompting shift toward fully Transformer-based designs. As result, Diffusion Transformers (DiTs) are gradually supplanting traditional U-Nets, as seen in leading diffusion models such as Stable Diffusion 3 [16], FLUX [44], and Sora [6]. However, the quadratic computational complexity of self-attention presents substantial challenges, especially for high-resolution image synthesis. Recent efforts [90, 73, 19, 57, 84] have explored more efficient alternatives, focusing on linear-complexity RNN-like architectures, such as Mamba [20] and Gated Linear Attention [85]. While these models improve efficiency, their causal design inherently conflicts with the bidirectional nature of visual generation [23, 49], limiting their effectiveness. Furthermore, as illustrated in Fig. 3, even with highly optimized CUDA implementations, their runtime advantage over conventional DiTs remains modest in high-resolution settings. This leads us to key question: Is it possible to design hardware-efficient diffusion backbone that also preserves strong generative capabilities like DiTs? To approach this question, we begin by examining the characteristics that underlie the generative power of DiTs. In visual recognition tasks, the success of Vision Transformers [15] is often credited to the self-attentions ability to capture long-range dependencies [36, 18, 24]. However, in generative tasks, we observe different dynamic. As depicted in Fig. 4, for both pre-trained class-conditional (DiT-XL/2 [56]) and text-to-image (PixArt-α [9] and FLUX [44]) DiT models, when queried with an anchor token, attention predominantly concentrates on nearby spatial tokens, largely disregarding distant ones. This finding suggests that computing global attention may be redundant for generation, underscoring the significance of local spatial modeling. Unlike recognition tasks, where long-range interactions are critical for global semantic reasoning, generative tasks appear to emphasize finegrained texture and local structural fidelity. These observations reveal the inherently localized nature of attention in DiTs and motivate the pursuit of more efficient architectures. In this work, we revisit convolutional neural networks (ConvNets) and propose Diffusion ConvNet (DiCo), simple yet highly efficient convolutional backbone tailored for diffusion models. Compared to self-attention, convolutional operations are more hardware-friendly, offering significant advantages for large-scale and resource-constrained deployment. While substituting self-attention with convolution substantially improves efficiency, it typically results in degraded performance. As illustrated in Fig. 5, this naive replacement introduces pronounced channel redundancy, with many channels remaining inactive during generation. We hypothesize that this stems from the inherently stronger representational capacity of self-attention compared to convolution. To address this, we introduce compact channel attention (CCA) mechanism, which dynamically activates informative channels with lightweight linear projections. As channel-wise global modeling approach, CCA enhances the models representational capacity and feature diversity while maintaining low computational overhead. Unlike modern recognition ConvNets that rely on large, costly kernels [12, 21], DiCo adopts streamlined design based entirely on efficient 11 pointwise convolutions and 33 depthwise convolutions. Despite its architectural simplicity, DiCo delivers strong generative performance. As shown in Fig. 2 and Fig. 3, DiCo models outperform state-of-the-art diffusion models on both the ImageNet 256256 and 512512 benchmarks. Notably, our DiCo-XL models achieve impressive FID scores of 2.05 and 2.53 at 256256 and 512512 resolution, respectively. In addition to Figure 2: Comparison of performance and efficiency with state-of-the-art diffusion models (DiT [56], DiC [75], and DiG [90]) on ImageNet 256256. Our proposed DiCo achieves the best performance while maintaining high efficiency. Compared to DiG-XL/2 with CUDA-optimized Flash Linear Attention [85], DiCoXL runs 2.9 faster and achieves 1.6 improvement in FID. Figure 3: (a) Runtime comparison between DiT [56] (with Attention [77]), DiS [19] (with Mamba [20]), DiG [90] (with Gated Linear Attention [85]), and our DiCo at 512512 resolution. DiCo is 3.3 faster than DiS at the small model scale and 6.8 faster at the XL scale. (b) FID vs. runtime of various methods on ImageNet 512512. DiCo-XL achieves state-of-the-art FID of 2.53 while maintaining high efficiency. performance gains, DiCo models exhibit considerable efficiency advantages over attention-based [77], Mamba-based [20], and linear attention-based [38] diffusion models. Specifically, at 256256 resolution, DiCo-XL achieves 26.4% reduction in Gflops and is 2.7 faster than DiT-XL/2 [56]. At 512512 resolution, DiCo-XL operates 7.8 and 6.7 faster than the Mamba-based DiM-H [73] and DiS-H/2 [19] models, respectively. Our largest model, DiCo-H with 1 billion parameters, further reduces the FID on ImageNet 256256 to 1.90. These results collectively highlight the strong potential of DiCo in diffusion-based generative modeling. Overall, the main contributions of this work can be summarized as follows: We analyze pre-trained DiT models and reveal significant redundancy and locality within their global attention mechanisms. These findings may inspire researchers to develop more efficient strategies for constructing high-performing diffusion models. We propose DiCo, simple, efficient, and powerful ConvNet backbone for diffusion models. By incorporating compact channel attention, DiCo significantly improves representational capacity and feature diversity without sacrificing efficiency. We conduct extensive experiments on the ImageNet 256256 and 512512 benchmarks. DiCo outperforms existing diffusion models in both generation quality and speed. To the best of our knowledge, this is the first work to demonstrate that well-designed fully convolutional backbones can achieve SOTA performance in diffusion-based generative modeling. 3 (a) Attention maps from DiT blocks of DiT-XL/2-512 [56]. (b) Attention maps from DiT blocks of PixArt-α-512 [9]. (c) Attention maps from MM-DiT [16] blocks of FLUX.1-dev [44]. Figure 4: Visualization of attention maps from well-known DiT models, including DiT-XL/2, PixArt-α, and FLUX.1-dev. The intensity of the blue color indicates the magnitude of attention scores. For self-attention across different layers in these models, only few neighboring tokens contribute significantly to the attention distribution of given anchor token (red box), resulting in highly redundant and localized representations. (a) DiT [56] (b) DiCo w/o channel activation (c) DiCo (ours) Figure 5: Comparison of channel activation scores across different diffusion models. Channel activation scores are computed using ReLU followed by global average pooling on the final layers self-attention or convolution outputs [91]. Directly replacing self-attention in DiT [56] with convolution introduces significant channel redundancy, as most channel activation scores remain at relatively low levels."
        },
        {
            "title": "2 Related Work",
            "content": "Architecture of Diffusion Models. Early diffusion models commonly employ U-Net [62] as the foundational architecture [11, 28, 61]. More recently, growing body of research has explored Vision Transformers (ViTs) [15] as alternative backbones for diffusion models, yielding remarkable results [56, 3, 52, 88, 59, 48]. Notably, DiT [56] has demonstrated the excellent performance of transformer-based architectures, achieving SOTA performance on ImageNet generation. However, the quadratic computational complexity inherent in ViTs presents significant challenges in terms of efficiency for long sequence modeling. To mitigate this, recent studies have explored the use of RNN-like architectures with linear complexity, such as Mamba [20] and linear attention [38], as backbones for diffusion models [19, 90, 73, 84, 57]. DiS [19] and DiM [73] employ Mamba to reduce computational overhead, while DiG [90] leverages Gated Linear Attention [85] to achieve competitive performance with improved efficiency. In this work, we revisit ConvNets as backbones for diffusion models. We show that, with proper design, pure convolutional architectures can achieve SOTA generative performance, providing an efficient and powerful alternative to DiTs. ConvNet Designs. Over the past decade, convolutional neural networks (ConvNets) have achieved remarkable success in computer vision [42, 67, 25, 35, 83]. Numerous lightweight ConvNets have been developed for real-world deployment [33, 65, 32, 13]. Although Transformers have gradually become the dominant architecture across wide range of tasks, their substantial computational overhead remains significant challenge. Many modern ConvNet designs achieve competitive performance while maintaining high efficiency. ConvNeXt [51] explores the modernization of standard ConvNets 4 and achieves superior results compared to transformer-based models. RepLKNet [12] investigates the use of large-kernel convolutions, expanding kernel sizes up to 3131. UniRepLKNet [14] further generalizes large-kernel ConvNets to domains such as audio, point clouds, and time-series forecasting. In this work, we explore the potential of pure ConvNets for diffusion-based image generation, and show that simple, efficient ConvNet designs can also deliver excellent performance."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Diffusion formulation. We first revisit essential concepts underpinning diffusion models [27, 71]. Diffusion models are characterized by forward noising procedure that progressively injects noise into data sample x0. Specifically, this forward process can be expressed as: q(x1:T x0) = (cid:89) t=1 q(xtxt1), q(xtx0) = (xt; αtx0, (1 αt)I), (1) where αt are predefined hyperparameters. The objective of diffusion model is to learn the reverse process: pθ(xt1xt) = (µθ(xt), Σθ(xt)), where neural networks parameterize the mean and covariance of the process. The training involves optimizing variational lower bound on the loglikelihood of x0, which simplifies to: L(θ) = p(x0x1) + (cid:88) DKL(q(xt1xt, x0)pθ(xt1xt)). (2) To simplify training, the models predicted mean µθ can be reparameterized as noise predictor ϵθ. The objective then reduces to straightforward mean-squared error between the predicted noise and the true noise ϵt: Lsimple(θ) = ϵθ(xt) ϵt2 2. Following DiT [56], we train the noise predictor ϵθ using the simplified loss Lsimple, while the covariance Σθ is optimized using the full loss L. Classifier-free guidance. Classifier-free guidance (CFG) [29] is an effective method to enhance sample quality in conditional diffusion models. It achieves such enhancement by guiding the sampling process toward outputs strongly associated with given condition c. Specifically, it modifies the predicted noise to obtain high p(xc) as: ˆϵθ(xt, c) = ϵθ(xt, ) + log p(xc) ϵθ(xt, ) + (ϵθ(xt, c) ϵθ(xt, )). where 1 controls the guidance strength, and ϵθ(xt, ) is an unconditional prediction obtained by randomly omitting the conditioning information during training. Following prior works [56, 90], we adopt this technique to enhance the quality of generated samples."
        },
        {
            "title": "3.2 Network Architecture",
            "content": "Currently, diffusion models are primarily categorized into three architectural types: (1) Isotropic architectures without any downsampling layers, as seen in DiT [56]; (2) Isotropic architectures with long skip connections, exemplified by U-ViT [3]; and (3) U-shaped architectures, such as U-DiT [76]. Motivated by the crucial role of multi-scale features in image denoising [89, 80], we adopt U-shaped design to construct hierarchical model. We also conduct an extensive ablation study to systematically compare the performance of these different architectural choices in Table 3. As illustrated in Fig. 6 (a), DiCo employs three-stage U-shaped architecture composed of stacked DiCo blocks. The model takes the spatial representation generated by the VAE encoder as input. For an image of size 256 256 3, the corresponding has dimensions 32 32 4. To process this input, DiCo applies 3 3 convolution that transforms into an initial feature map z0 with channels. For conditional informationspecifically, the timestep and class label ywe employ multi-layer perceptron (MLP) and an embedding layer, serving as the timestep and label embedders, respectively. At each block within DiCo, the feature map zl1 is passed through the l-th DiCo block to produce the output zl. Within each stage, skip connections between the encoder and decoder facilitate efficient information flow across intermediate features. After concatenation, 1 1 convolution is applied to reduce the channel dimensionality. To enable multi-scale processing across stages, we utilize pixel-unshuffle operations for downsampling and pixel-shuffle operations for upsampling. Finally, the output feature zL is normalized and passed through 3 3 convolutional head to predict both noise and covariance. 5 Figure 6: The overall architecture of the proposed DiCo model, which consists of (b) DiCo Block, (c) Conv Module, and (d) Compact Channel Attention (CCA). DConv denotes depthwise convolution."
        },
        {
            "title": "3.3 DiCo Block",
            "content": "Motivation. As shown in Fig. 4, the self-attention computation in DiT modelswhether for class-conditional or text-to-image generationexhibits distinctly local structure and significant redundancy. This observation motivates us to replace the global self-attention in DiT with more hardware-efficient operations. natural alternative is convolution, which is well-known for its ability to efficiently model local patterns. We first attempt to substitute self-attention with combination of 1 1 pointwise convolutions and 3 3 depthwise convolutions. However, the direct replacement leads to degradation in generation performance. As shown in Fig. 5, compared to DiT, many channels in the modified model remain inactive, indicating substantial channel redundancy. We hypothesize that this performance drop stems from the fact that self-attention, being dynamic and content-dependent, provides greater representational power than convolution, which relies on static weights. To address this limitation, we introduce compact channel attention mechanism to dynamically activate informative channels. We describe the full design in detail below. Block designs. The core design of DiCo is centered around the Conv Module, as shown in Fig. 6 (c). We first apply 1 1 convolution to aggregate pixel-wise cross-channel information, followed by 3 3 depthwise convolution to capture channel-wise spatial context. GELU activation is employed for non-linear transformation. To further address channel redundancy, we introduce compact channel attention (CCA) mechanism to activate more informative channels. As illustrated in Fig. 6 (d), CCA first aggregates features via global average pooling (GAP) across the spatial dimensions, then applies learnable 1 1 convolution followed by sigmoid activation to generate channel-wise attention weights. Generally, the whole process of Conv Module can be described as: = Wp2CCA(GELU(WdWp1X)), CCA(X) = Sigmoid(WpGAP(X)), (3) where Wp() is the 1 1 point-wise conv, Wd is the depthwise conv, and denotes the channel-wise multiplication. As shown in Fig. 5 (c), this simple and efficient design effectively reduces feature redundancy and enhances the representational capacity of the model. To incorporate conditional information from the timestep and label, we follow DiT by adding the input timestep embedding and label embedding y, and using them to predict the scale parameters α, γ and the shift parameter β."
        },
        {
            "title": "3.4 Architecture Variants",
            "content": "We establish four model variantsDiCo-S, DiCo-B, DiCo-L, and DiCo-XLwhose parameter counts are aligned with those of DiT-S/2, DiT-B/2, DiT-L/2, and DiT-XL/2, respectively. Compared to their DiT counterparts, our DiCo models achieve significant reduction in computational cost, with Gflops ranging from only 70.1% to 74.6% of those of DiT. Furthermore, to explore the potential of our design, we scale up DiCo to 1 billion parameters, resulting in DiCo-H. The architectural configurations of these models are detailed in Appendix Table 5. 6 Table 1: Comparison with state-of-the-art diffusion models on ImageNet 256256. We use the same training hyperparameters as DiT, DiC, and DiG. The performance at 400K training steps is reported without CFG. We mark the best results for each model scale in bold. Throughput is measured on A100 with batch size 64 at fp32 precision. DiT and DiG are optimized with FlashAttention-2 and Flash Linear Attention, respectively. Gflops Throughput (it/s) FID IS Precision Recall Model ADM-U [11] LDM-4 [61] U-ViT-H/2 [3] 742 - 133.25 Mamba-based diffusion models. DiM-H [73] DiS-H/2 [19] DiffuSSM-XL [84] DiMSUM-L/2 [57] Baselines and Ours. DiT-S/2 (400K) [56] DiC-S (400K) [75] DiG-S/2 (400K) [90] DiCo-S (400K) DiT-B/2 (400K) DiC-B (400K) DiG-B/2 (400K) DiCo-B (400K) DiT-L/2 (400K) DiG-L/2 (400K) DiCo-L (400K) DiT-XL/2 (400K) DiC-XL (400K) DiG-XL/2 (400K) DiCo-XL (400K) DiT-XL/2 (w/ CFG) DiG-XL/2 (w/ CFG) DiCo-XL (w/ CFG) DiC-H (w/ CFG) DiCo-H (w/ CFG) 210 - 280.3 84.49 6.06 5.9 4.30 4.25 23.02 23.5 17.07 16.88 80.73 61.66 60. 118.66 116.1 89.40 87.30 118.66 89.40 87.30 204.4 194."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "- - 73.45 25.06 33.95 - 59.13 1234.01 - 961.24 1695.73 380.11 - 345.89 822.97 114.63 109.01 288.32 76.90 - 71.74 208. 76.90 71.74 208.47 - 117.57 3.94 3.95 2.29 2.21 2.10 2.28 2.11 68.40 58.68 62.06 49.97 43.47 32.33 39.50 27. 23.33 22.90 13.66 19.47 13.11 18.53 11.67 2.27 2.07 2.05 2.25 1.90 215.84 178.22 263.88 - 271.32 259.13 - - 25.82 22.81 31.38 - 48.72 37.21 56.52 - 59.87 91.37 - 100.15 68.53 100.42 278.24 278.95 282.17 - 284. 0.83 0.81 0.82 - 0.82 0.86 - - - 0.39 0.48 - - 0.51 0.60 - 0.60 0.69 - - 0.63 0. 0.83 0.82 0.83 - 0.83 0.53 0.55 0.57 - 0.58 0.56 0.59 - - 0.56 0.58 - - 0.63 0. - 0.64 0.61 - - 0.64 0.61 0.57 0.60 0.59 - 0.61 Datasets and Metrics. Following previous works [56, 90, 75], we conduct experiments on classconditional ImageNet-1K [10] generation benchmark at 256256 and 512512 resolutions. We use the Fréchet Inception Distance (FID) [26] as the primary metric to evaluate model performance. In addition, we report the Inception Score (IS) [64], Precision, and Recall [43] as secondary metrics. All these metrics are computed using OpenAIs TensorFlow evaluation toolkit [11]. Implementation Details. For DiCo-S/B/L/XL, we adopt exactly the same experimental settings as used for DiT. Specifically, we employ constant learning rate of 1 104, no weight decay, and batch size of 256. The only data augmentation applied is random horizontal flipping. We maintain an exponential moving average (EMA) of the DiCo weights during training, with decay rate of 0.9999. The pre-trained VAE [61] is used to extract latent features. For our largest model, DiCo-H, we follow the training settings of U-ViT [3], increasing the learning rate to 2 104 and scaling the batch size to 1024 to accelerate training. Additional details are provided in Appendix Sec. B."
        },
        {
            "title": "4.2 Main Results",
            "content": "Comparison under the DiT Setting. In addition to DiT [56], we also select recent state-of-the-art diffusion models, DiG [90] and DiC [75], as baselines, since they similarly follow the experimental 7 Table 2: Comparison with state-of-the-art diffusion models on ImageNet 512512. We mark the best performance with CFG in bold. The performance at 1.3M/3M training steps is reported without using CFG. Gflops Throughput (it/s) FID IS Precision Recall Model ADM-U [11] U-ViT-L/4 [3] U-ViT-H/4 [3] 2813 76.52 133.27 - 128.49 73. Mamba-based diffusion models. DiM-H [73] DiS-H/2 [19] DiffuSSM-XL [84] Baselines and Ours. DiT-XL/2 (1.3M) [56] DiCo-XL (1.3M) DiT-XL/2 (3M) DiCo-XL (3M) DiT-XL/2 (w/ CFG) DiCo-XL (w/ CFG) 708 - 1066. 524.70 349.78 524.70 349.78 524.70 349.78 7.39 8.59 - 18.58 57.45 18.58 57.45 18.58 57.45 3.85 4.67 4. 3.78 2.88 3.41 13.78 8.10 12.03 7.48 3.04 2.53 221.72 213.28 263.79 - 272.33 255.06 - 132.85 105.25 146. 240.82 275.74 0.84 0.87 0.84 - 0.84 0.85 - 0.78 0.75 0.78 0.84 0.83 0.53 0.45 0. - 0.56 0.49 - 0.62 0.64 0.63 0.54 0.56 setup of DiT. Table 1 presents the comparison results on ImageNet 256256. Across different model scales trained for 400K iterations, our DiCo consistently achieves the best or second-best performance across all metrics. Furthermore, when using classifier-free guidance (CFG), our DiCo-XL achieves state-of-the-art FID of 2.05 and an IS of 282.17. Beyond performance improvements, DiCo also demonstrates significant efficiency gains compared to both the baselines and Mamba-based models. Table 2 presents the results on ImageNet 512512. At higher resolutions, our model demonstrates greater improvements in both performance and efficiency. Specifically, DiCo-XL achieves stateof-the-art FID of 2.53 and an IS of 275.74, while reducing Gflops by 33.3% and achieving 3.1 speedup compared to DiT-XL/2. These results highlight that our convolutional architecture remains highly efficient and effective for high-resolution image generation. Scaling the model up. To further explore the potential of our model, we scale it up to 1 billion parameters. As shown in Table. 1, compared to DiCo-XL, the larger DiCo-H model achieves further improvements in FID (1.90 vs. 2.05), demonstrating the great scalability of our architecture. We provide additional scalability analysis and comparison results in Appendix Sec. and Sec. D. More generated samples can be found in Appendix Sec. and the submitted Supplementary Material. Figure 7: CCA effectively reduces channel redundancy and enhances feature diversity. Left: Features from the first stage of DiCo without CCA. Right: Features from the first stage of DiCo."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "For the ablation study, we use the small-scale model and evaluate performance on the ImageNet 256256 benchmark. All models are trained for 400K iterations. In this section, self-attention in DiT is not accelerated using FlashAttention-2 to ensure fair comparison with other efficient attention mechanisms. We analyze both the overall architecture and the contributions of individual components within DiCo to better understand their impact on model performance. 8 Table 3: Ablation study on architectural design. We compare different architectural variants, including isotropic, isotropic with skip connections, and U-shape. For all designs, our DiCo consistently outperforms DiT. Model Skip Hierarchy #Params Gflops Throughput DiT (iso.) [56] DiCo (iso.) DiT (iso.&skip) DiCo (iso.&skip) DiT (U-shape) DiCo (U-shape) 32.9M 33.7M 34.3M 35.1M 33.0M 33.1M 6.06 5.67 6.44 6.04 4.23 4.25 1086.81 1901.39 1037.55 1807. 1140.93 1695.73 FID 67.16 60.58 62.63 56.95 54.00 49.97 IS Pre Rec 20.41 25.44 22.08 26.84 28.52 31.38 0.37 0.44 0.39 0. 0.42 0.48 0.57 0.55 0.56 0.56 0.59 0.58 Table 4: Ablation study on the components of DiCo. indicates that the model depth and width are adjusted for fair comparison. We analyze the effects of the activation function, depthwise convolution (DWC) kernel size, compact channel attention (CCA), and the conv module (CM). For the CM, we compare it against several advanced efficient attention mechanisms to validate its effectiveness and efficiency. Model DiCo-S GELUReLU 3355 DWC 3377 DWC Compact Channel Attn (CCA) w/o CCA CCASE module [34] CCAChannel Self-Attn [89] Conv Module (CM) CMSelf-Attn [77] CMWindow Attn [50] CMFocused Linear Attn [22] CMAgent Attn [59] #Params Gflops Throughput 33.1M 33.1M 33.2M 33.4M 33.1M 33.0M 32.9M 33.2M 33.1M 33.0M 33.0M 32.9M 33.3M 4.25 4.25 4.29 4.34 4.25 4.24 4.25 4.26 4.25 4.23 4.34 4.33 4.24 1695.73 1695.68 1628.59 1469. 1695.73 1731.13 1657.10 1569.51 1695.73 1140.93 1165.22 971.49 1160.89 FID 49.97 51.26 48.03 47.49 49.97 54.78 50.89 50.24 49.97 54.00 53.23 50.60 52. IS Pre Rec 31.38 30.23 32.51 32.93 31.38 28.40 30.49 30.85 31.38 28.52 28.33 30.85 28.82 0.48 0.47 0.49 0. 0.48 0.48 0.48 0.45 0.48 0.42 0.43 0.46 0.43 0.58 0.57 0.58 0.59 0.58 0.57 0.57 0.59 0.58 0.59 0.59 0.60 0.60 Architecture Ablation. We evaluate the performance of DiCo under various architectural designs and conduct fair comparison with DiT. As shown in Table 3, DiCo consistently outperforms DiT across all structures while also delivering significant efficiency gains. These results highlight the potential of DiCo as strong and efficient alternative to DiT. Component Ablation. We conduct component-wise analysis of DiCo, examining the effects of the activation function, convolutional kernel size, compact channel attention (CCA), and the conv module (CM). The overall ablation results are summarized in Table 4. Increasing the convolutional kernel size leads to further performance gains but at the expense of reduced efficiency, highlighting trade-off between performance and computational cost. The introduction of CCA results in 4.81point improvement in FID. As illustrated by the feature visualizations in Fig. 7, CCA significantly enhances feature diversity, demonstrating its effectiveness in improving the models representational capacity. We also compare CCA with SE module [34] and Channel-wise Self-Attention [89]; despite its simplicity, CCA achieves superior performance and higher efficiency. For the Conv Module, we benchmark it against several advanced efficient attention mechanisms (Window Attention [50], Focused Linear Attention [22], Agent Attention [59]). The results show that our CM offers both better performance and computational efficiency."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose new backbone for diffusion models, Diffusion ConvNet (DiCo), as compelling alternative to the Diffusion Transformer (DiT). DiCo replaces self-attention with combination of 1 1 pointwise convolutions and 3 3 depthwise convolutions, and incorporates compact channel attention mechanism to reduce channel redundancy and enhance feature diversity. As fully convolutional network, DiCo surpasses state-of-the-art diffusion models on the ImageNet 256256 and 512512 benchmarks, while achieving significant efficiency gains. We look forward to further scaling up DiCo and extending it to broader generative tasks, such as text-to-image generation."
        },
        {
            "title": "References",
            "content": "[1] Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, and Ran He. Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for all-in-one image restoration. In CVPR, 2024. [2] Yuang Ai, Xiaoqiang Zhou, Huaibo Huang, Xiaotian Han, Zhengyu Chen, Quanzeng You, and Hongxia Yang. Dreamclear: High-capacity real-world image restoration with privacy-safe dataset curation. Advances in Neural Information Processing Systems, 37:5544355469, 2024. [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. [7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, 2023. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang Ding. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR, 2022. [13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In CVPR, 2021. [14] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying Shan. Unireplknet: universal perception large-kernel convnet for audio video point cloud time-series and image recognition. In CVPR, 2024. [15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. [18] Qihang Fan, Huaibo Huang, and Ran He. Breaking the low-rank dilemma of linear attention. In CVPR, 2025. [19] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [20] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [21] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethinking convolutional attention design for semantic segmentation. In NeurIPS, 2022. [22] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In ICCV, 2023. [23] Dongchen Han, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, and Gao Huang. Demystify mamba in vision: linear attention perspective. In NeurIPS, 2024. [24] Dongchen Han, Tianzhu Ye, Yizeng Han, Zhuofan Xia, Siyuan Pan, Pengfei Wan, Shiji Song, and Gao Huang. Agent attention: On the integration of softmax and linear attention. In ECCV, 2024. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [28] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. [29] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [30] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. 10 [31] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. [32] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019. [33] Andrew Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. [34] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018. [35] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In CVPR, 2017. [36] Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, and Tieniu Tan. Vision transformer with super token sampling. In CVPR, 2023. [37] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, 2023. [38] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. [39] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models. In NeurIPS, 2022. [40] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. [41] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. In NeurIPS, 2023. [42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. In NeurIPS, 2012. [43] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In NeurIPS, 2019. [44] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. Accessed: 2024-11-12. [45] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. [46] Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. In NeurIPS, 2024. [47] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [48] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models and time-dependent layer normalization. In NeurIPS, 2024. [49] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. [50] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. [51] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, 2022. [52] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [53] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [54] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. [55] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. [56] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [57] Hao Phung, Quan Dao, Trung Tuan Dao, Hoang Phan, Dimitris N. Metaxas, and Anh Tuan Tran. DiMSUM: Diffusion mamba - scalable and unified spatial-frequency method for image generation. In NeurIPS, 2024. [58] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [59] Yifan Pu, Zhuofan Xia, Jiayi Guo, Dongchen Han, Qixiu Li, Duo Li, Yuhui Yuan, Ji Li, Yizeng Han, Shiji Song, et al. Efficient diffusion transformer with step-wise dynamic attention mediators. In ECCV, 2024. [60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [62] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 11 [63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. In NeurIPS, 2022. [64] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. [65] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. [66] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In SIGGRAPH, 2022. [67] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [68] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. [69] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [70] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. [71] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [72] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [73] Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. [74] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. [75] Yuchuan Tian, Jing Han, Chengcheng Wang, Yuchen Liang, Chao Xu, and Hanting Chen. Dic: Rethinking conv3x3 designs in diffusion models. In CVPR, 2025. [76] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. In NeurIPS, 2024. [77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [78] Hongbo Wang, Jie Cao, Jin Liu, Xiaoqiang Zhou, Huaibo Huang, and Ran He. Hallo3d: Multi-modal hallucination detection and mitigation for consistent 3d content generation. In NeurIPS, 2024. [79] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. In CVPR, 2025. [80] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: general u-shaped transformer for image restoration. In CVPR, 2022. [81] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2023. [82] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, 2023. [83] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017. [84] Jing Nathan Yan, Jiatao Gu, and Alexander Rush. Diffusion models without attention. In CVPR, 2024. [85] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In ICML, 2024. [86] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [87] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In NeurIPS, 2024. [88] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [89] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. [90] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear attention. In CVPR, 2025. [91] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In NeurIPS, 2018."
        },
        {
            "title": "Appendix",
            "content": "We provide the following supplementary information in the Appendix: Sec. A. detailed configurations of DiCo model variants. Sec. B. additional implementation details of DiCo. Sec. C. scalability analysis of DiCo models. Sec. D. additional comparison results with generative model family. Sec. E. additional samples generated by DiCo-XL models. Sec. F. discussion of limitations of this work. Sec. G. discussion of broader impacts of this work. Note that the generated samples in Sec. have been compressed to avoid exceeding the paper size limit. We include the uncompressed generated samples in the submitted Supplementary Material."
        },
        {
            "title": "A DiCo Model Variants",
            "content": "We introduce several variants of DiCo, each scaled to different model sizes. Specifically, we present five variants: Small (33M parameters), Base (130M), Large (460M), XLarge (700M), and Huge (1B). These variants are created by adjusting the hidden size D, the number of layers (L1, L2, L3, L4, L5), and the FFN ratio. They span wide range of parameter counts and FLOPs, from 33M to 1B parameters and from 4.25 Gflops to 194.15 Gflops, offering comprehensive foundation for evaluating scalability and efficiency. Notably, when compared to their corresponding DiT counterparts, our DiCo models require only 70.1% to 74.6% of the FLOPs, demonstrating the computational efficiency of our design. The detailed configurations for these variants are provided in Table 5. Table 5: Architecture variants of DiCo. Gflops are measured with an input size of 32 32 4. Compared to DiT, our DiCo models are more computationally efficient."
        },
        {
            "title": "Model",
            "content": "#Params (M) Gflops DiCo-S DiCo-B DiCo-L DiCo-XL DiCo-H 33.1 130.0 463.9 701.2 1037.4 4.25 16.88 60.24 87.30 194.15 GflopsDiCo GflopsDiT 70.1% 73.3% 74.6% 73.6% -"
        },
        {
            "title": "Hidden Size D",
            "content": "#Layers"
        },
        {
            "title": "FFN Ratio",
            "content": "128 256 352 416 416 [5, 4, 4, 4, 4] [5, 4, 4, 4, 4] [9, 8, 9, 8, 9] [9, 9, 10, 9, 9] [14, 12, 10, 12, 14] 2 2 2 2 4 Table 6: Implementation details of DiCo variants. For DiCo-S/B/L/XL, we follow the same experimental settings as DiT [3]. For the largest variant DiCo-H, we adopt the training hyperparameters from U-ViT [3], increasing the batch size and learning rate to accelerate training. Model Resolution DiCo-S 256 DiCo-B 256 256 DiCo-L 256 256 DiCo-XL 256 256 DiCo-XL 512 512 DiCo-H 256 256 Optimizer Betas Weight decay Peak learning rate Learning rate schedule Warmup steps Global batch size Numerical precision Training steps Computational resources Training Time Data Augmentation VAE Sampler Sampling steps AdamW (0.9, 0.999) 0 1 104 constant 0 256 fp32 400K 8 A100 11 hours random flip sd-ft-ema iDDPM 250 AdamW (0.9, 0.999) 0 1 104 constant 0 256 fp32 400K 8 A100 16 hours random flip sd-ft-ema iDDPM 250 AdamW (0.9, 0.999) 0 1 104 constant 0 256 fp32 400K 16 A100 29 hours random flip sd-ft-ema iDDPM 250 AdamW (0.9, 0.999) 0 1 104 constant 0 256 fp32 3750K 32 A100 266 hours random flip sd-ft-ema iDDPM 250 AdamW (0.9, 0.999) 0 1 104 constant 0 256 fp32 3000K 64 A100 256 hours random flip sd-ft-ema iDDPM 250 AdamW (0.99, 0.99) 0 2 104 constant 5K 1024 fp32 1000K 64 A100 113 hours random flip sd-ft-ema iDDPM 250 13 Figure 8: Scaling the DiCo models consistently improves performance throughout training. We report FID-50K and Inception Score across training iterations for four DiCo variants. Figure 9: Training loss curves for DiCo models. We also highlight early training dynamics during the first 100K iterations. Larger DiCo variants exhibit lower training losses, reflecting improved optimization with scale."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "For DiCo-S/B/L/XL models, we adopt the same experimental settings as those used for DiT. For DiCo-H, the largest variant, we use the hyperparameters from U-ViT, increasing the batch size and learning rate to expedite training. All experiments are conducted on NVIDIA A100 (80G) GPUs. During inference, all models use the iDDPM [54] sampler with 250 sampling steps. The whole implementation details are summarized in Table 6."
        },
        {
            "title": "C Scalability Analysis",
            "content": "Impact of scaling on metrics. Table. 7 and Fig. 8 illustrate the impact of DiCo model scaling across various evaluation metrics. Our results show that scaling DiCo consistently enhances performance across all metrics throughout training, highlighting its potential as strong candidate for large-scale foundational diffusion model. 14 Table 7: Performance of DiCo models without CFG at different training steps on ImageNet 256256. Scaling the ConvNet backbone consistently leads to improved generative performance."
        },
        {
            "title": "Training Steps",
            "content": "FID sFID IS Precision Recall DiCo-S 4. DiCo-B 16.88 DiCo-L 60.24 DiCo-XL 87. 50K 100K 150K 200K 250K 300K 350K 400K 50K 100K 150K 200K 250K 300K 350K 400K 50K 100K 150K 200K 250K 300K 350K 400K 50K 100K 150K 200K 250K 300K 350K 400K 109.50 79.79 68.93 62.61 57.95 54.58 51.77 49.97 84.78 53.66 43.09 37.24 33.31 30.68 28.72 27. 68.11 35.42 25.69 20.81 18.19 16.11 14.92 13.66 66.53 31.78 22.61 17.95 15.60 13.70 12.59 11.67 18.28 15.60 13.98 13.18 12.47 11.99 11.69 11.41 14.24 9.13 8.21 8.03 7.72 7.59 7.55 7.43 11.50 6.52 6.04 5.80 5.70 5.60 5.58 5.50 11.35 6.39 5.94 5.67 5.58 5.52 5.48 5. 12.00 16.39 19.93 22.87 25.58 27.74 29.68 31.38 14.85 25.01 33.02 39.10 44.89 49.34 52.95 56.52 18.62 38.12 53.91 65.78 74.77 81.47 86.01 91.37 19.41 42.81 60.49 73.21 82.96 89.44 95.49 100.42 0.283 0.382 0.411 0.433 0.450 0.467 0.479 0.481 0.392 0.494 0.536 0.561 0.577 0.584 0.594 0. 0.480 0.607 0.643 0.665 0.676 0.685 0.689 0.694 0.501 0.637 0.672 0.693 0.697 0.707 0.710 0.711 0.322 0.440 0.498 0.531 0.546 0.565 0.576 0.582 0.423 0.544 0.571 0.596 0.598 0.597 0.599 0.617 0.465 0.561 0.579 0.589 0.588 0.594 0.602 0.604 0.472 0.563 0.572 0.582 0.591 0.599 0.605 0. Impact of scaling on training loss. We further analyze the effect of model scale on training loss. As shown in Fig. 9, larger DiCo models consistently achieve lower training losses, indicating more effective optimization with increased scale."
        },
        {
            "title": "D Additional Comparison Results",
            "content": "In this section, we report results from state-of-the-art generative models spanning various categories, including GAN-based, masked prediction-based, autoregressive, visual autoregressive, and diffusionbased approaches. It is important to emphasize that the goal of this comparison is not to surpass all existing models, but to demonstrate that our diffusion-based DiCo model achieves competitive performance within the broader generative modeling landscape. ImageNet 256256. Table 8 presents comparison across generative model family on ImageNet 256256. Among diffusion models, our DiCo-XL achieves strong FID of 2.05 with only 87.3 Gflops. Our largest variant, DiCo-H, attains state-of-the-art FID of 1.90. When compared to other generative model types, DiCo also demonstrates competitive performance. Notably, DiCo-H, with just 1B parameters, outperforms VAR-d30which has 2B parametersin terms of FID. ImageNet 512512. Table 9 presents the results on ImageNet 512512. Among diffusion models, our DiCo-XL achieves state-of-the-art FID of 2.53 with only 349.8 GFLOPs. Compared to other 15 Table 8: Comparison with generative model family on ImageNet 256 256. We report the performance of state-of-the-art generative models across different paradigms, including GAN-based, masked prediction (Mask.)-based, autoregressive (AR), visual-autoregressive (VAR), and diffusion (Diff.)-based models."
        },
        {
            "title": "Type Model",
            "content": "#Params Gflops FID IS Precision Recall"
        },
        {
            "title": "GAN\nGAN\nGAN",
            "content": "BigGAN-deep [5] GigaGAN [37] StyleGAN-XL [66] Mask. MaskGIT [8] Mask. Mask. RCG [46] TiTok-S-128 [87]"
        },
        {
            "title": "VAR\nVAR\nVAR\nVAR",
            "content": "Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. VQ-GAN-re [17] ViTVQ-re [86] RQTran.-re [45] LLamaGen-3B [72] MAR-H [47] PAR-3B [79] RandAR-XXL [55] VAR-d16 [74] VAR-d20 VAR-d24 VAR-d30 ADM-U [11] U-ViT-L/2 [3] U-ViT-H/2 [3] Simple Diffusion [31] VDM++ [41] DiT-XL/2 [56] SiT-XL [52] DiM-H [73] DiS-H/2 [19] DiffuSSM-XL [84] DiMSUM-L/2 [57] DiG-XL/2 [90] DiC-H [75] DiCo-XL DiCo-H 160M 569M 166M 227M 502M 287M 1.4B 1.7B 3.8B 3.1B 943M 3.1B 1.4B 310M 600M 1.0B 2.0B 608M 287M 501M 2.0B 2.0B 675M 675M 860M 901M 673M 460M 676M 1.0B 701M 1.0B - - - - - - - - - - - - - - - - - 742 77 133.3 - - 118.7 118.7 210 - 280.3 84.49 89.40 204.4 87.30 194.15 6.95 3.45 2.30 6.18 3.49 1.97 5.20 3.04 3.80 2.18 1.55 2.29 2.15 3.30 2.57 2.09 1. 3.94 3.40 2.29 2.77 2.12 2.27 2.06 2.21 2.10 2.28 2.11 2.07 2.25 2.05 1.90 171.4 225.5 265.1 182.1 215.5 281.8 280.3 227.4 323.7 263.3 303.7 255.5 322.0 274.4 302.6 312.9 323.1 215.8 219.9 263.9 211.8 267.7 278.2 277.5 - 271.3 259.1 - 279.0 - 282.2 284. 0.87 0.84 0.78 0.80 - - - - - 0.81 0.81 0.82 0.79 0.84 0.83 0.82 0.82 0.83 0.83 0.82 - - 0.83 0.83 - 0.82 0.86 - 0.82 - 0.83 0.83 0.28 0.61 0. 0.51 - - - - - 0.58 0.62 0.58 0.62 0.51 0.56 0.59 0.59 0.53 0.52 0.57 - - 0.57 0.59 - 0.58 0.56 0.59 0.60 - 0.59 0.61 generative models, DiCo continues to demonstrate strong performance. Specifically, DiCo-XL, with only 701M parameters, outperforms VAR-d36-s, which has 2.3B parameters, achieving superior FID performance with significantly fewer parameters."
        },
        {
            "title": "E Model Samples",
            "content": "We present samples generated by the DiCo-XL models at resolutions of 256256 and 512512. Fig. 10 through 29 display uncurated samples across various classifier-free guidance scales and input class labels. As illustrated, our DiCo models demonstrate the ability to generate high-quality, detail-rich images."
        },
        {
            "title": "F Limitations",
            "content": "While this work demonstrates the strong performance and scalability of DiCo through extensive experiments, there are some limitations to note. Due to limited computational resources, our experiments focus exclusively on class-conditional image generation on ImageNet and do not explore text-to-image generation tasks. Moreover, our model is scaled to 1B parameters, while some advanced 16 Table 9: Comparison with generative model family on ImageNet 512 512. We report the performance of state-of-the-art generative models across different paradigms, including GAN-based, masked prediction (Mask.)-based, autoregressive (AR), visual-autoregressive (VAR), and diffusion (Diff.)-based models."
        },
        {
            "title": "Type Model",
            "content": "#Params Gflops FID IS Precision Recall"
        },
        {
            "title": "GAN\nGAN",
            "content": "BigGAN-deep [5] StyleGAN-XL [66] Mask. MaskGIT [8]"
        },
        {
            "title": "AR\nVAR",
            "content": "Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. Diff. VQ-GAN [17] VAR-d36-s [74] ADM-U [11] U-ViT-L/4 [3] U-ViT-H/4 [3] Simple Diffusion [31] VDM++ [41] DiT-XL/2 [56] SiT-XL [52] DiM-H [73] DiS-H/2 [19] DiffuSSM-XL [84] DiCo-XL 160M 166M 227M 227M 2.3B 731M 287M 501M 2.0B 2.0B 675M 675M 860M 901M 673M 701M - - - - - 2813 76.5 133.3 - - 524.7 524.7 708 - 1066.2 349.8 8.43 2. 7.32 26.52 2.63 3.85 4.67 4.05 4.53 2.65 3.04 2.62 3.78 2.88 3.41 2.53 177.9 267.8 156.0 66.8 303. 221.7 213.3 263.8 205.3 278.1 240.8 252.2 - 272.3 255.1 275.7 0.88 0.77 0.78 0.73 - 0.84 0.87 0.84 - - 0.84 0.84 - 0.84 0.85 0.83 0.29 0. 0.50 0.31 - 0.53 0.45 0.48 - - 0.54 0.57 - 0.56 0.49 0.56 generative models have been scaled to even larger sizes. We aim to investigate the broader generative potential of DiCo in future research."
        },
        {
            "title": "G Broader Impacts",
            "content": "Our work on the generative model DiCo contributes to advances in controllable image generation. This has potential positive applications in data augmentation, scientific visualization, and accessibility technologies. However, such models may also be misused to generate misleading or harmful content, especially in contexts involving deepfakes or biased representations of specific classes. To mitigate these risks, we encourage responsible usage aligned with ethical AI guidelines and emphasize the importance of transparency when deploying generative models in real-world applications. 17 DiCo-XL 512 512 samples, classifier-free guidance scale = 4.0 Figure 10: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = arctic wolf (270) Figure 11: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = volcano (980) DiCo-XL 512 512 samples, classifier-free guidance scale = 4.0 Figure 12: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = husky (250) Figure 13: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = ulphur-crested cockatoo (89) 19 DiCo-XL 512 512 samples, classifier-free guidance scale = 4.0 Figure 14: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = cliff drop-off (972) Figure 15: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = balloon (417) 20 DiCo-XL 512 512 samples, classifier-free guidance scale = 4.0 Figure 16: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = lion (291) Figure 17: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = otter (360) DiCo-XL 512 512 samples, classifier-free guidance scale = 2.0 Figure 18: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 2.0 Class label = red panda (387) Figure 19: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 2.0 Class label = panda (388) 22 DiCo-XL 512 512 samples, classifier-free guidance scale = 1.5 Figure 20: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 1.5 Class label = coral reef (973) Figure 21: Uncurated 512512 DiCo-XL samples. Classifier-free guidance scale = 1.5 Class label = macaw (88) 23 DiCo-XL 256 256 samples, classifier-free guidance scale = 4.0 Figure 22: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = macaw (88) Figure 23: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = dog sled (537) DiCo-XL 256 256 samples, classifier-free guidance scale = 4.0 Figure 24: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = arctic fox (279) Figure 25: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 4.0 Class label = loggerhead sea turtle (33) 25 DiCo-XL 256 256 samples, classifier-free guidance scale = 2.0 Figure 26: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 2.0 Class label = golden retriever (207) Figure 27: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 2.0 Class label = lake shore (975) 26 DiCo-XL 256 256 samples, classifier-free guidance scale = 1.5 Figure 28: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 1.5 Class label = space shuttle (812) Figure 29: Uncurated 256256 DiCo-XL samples. Classifier-free guidance scale = 1.5 Class label = ice cream (928)"
        }
    ],
    "affiliations": [
        "ByteDance",
        "CASIA",
        "UCAS"
    ]
}