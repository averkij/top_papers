{
    "paper_title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training",
    "authors": [
        "Ahmed M. Adly",
        "Mostafa Samy",
        "Amr Fawzy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Gazal-R1, a 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Built upon Qwen3 32B, our model demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains. We developed a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset of 107,033 synthetic medical reasoning examples that teaches structured clinical thinking, enhanced by advanced parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation (DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system that refines accuracy, format adherence, and reasoning quality. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12x larger. Beyond its strong empirical results, this work provides detailed insights into the challenges of training reasoning-capable models in specialized domains, including issues with reward hacking, training instability, and the fundamental tension between factual recall and detailed reasoning. Our methodology offers a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability."
        },
        {
            "title": "Start",
            "content": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training Preprint Ahmed M. Adly Research Engineer TachyHealth Riyadh 13316, Saudi Arabia amostafa@tachyhealth.com Mostafa Samy Data Science Product Manager TachyHealth Riyadh 13316, Saudi Arabia msamy@tachyhealth.com Amr Fawzy Chief Medical Officer TachyHealth Riyadh 13316, Saudi Arabia amr@tachyhealth.com Technical Report"
        },
        {
            "title": "Abstract",
            "content": "We present Gazal-R1, 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Built upon Qwen3 32B, our model demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains. We developed novel two-stage training pipeline: first, supervised finetuning on carefully curated dataset of 107,033 synthetic medical reasoning examples that teaches structured clinical thinking, enhanced by advanced parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation (DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using Group Relative Policy Optimization (GRPO) with sophisticated multi-component reward system that refines accuracy, format adherence, and reasoning quality. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12 larger. Beyond its strong empirical results, this work provides detailed insights into the challenges of training reasoning-capable models in specialized domains, including issues with reward hacking, training instability, and the fundamental tension between factual recall and detailed reasoning. Our methodology offers reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous domains, yet medical problem-solving remains distinct frontier. Effective medical AI requires more than correct answers; it necessitates transparent chain of reasoning that aligns with established clinical evidence and pathophysiological principles. This demand for explainability and faithfulness is paramount for building trust and ensuring safety in clinical applications. To address this challenge, we developed Gazal-R1, 32-billion-parameter LLM specifically fine-tuned to excel in the intricacies of medical reasoning. Starting with the powerful Qwen 3 32B [1] as our base model, we employed carefully structured two-stage training pipeline designed to embed deep clinical knowledge and cultivate systematic problem-solving framework. The latest checkpoints of Gazal-R1 are publicly available at Dr. Fawzys profound expertise as medical practitioner played pivotal role in the comprehensive medical validation and stringent verification process of the generated responses, affirming their scientific integrity and practical utility. 5 2 0 2 8 ] . [ 1 4 9 5 1 2 . 6 0 5 2 : r Technical Report Preprint https://huggingface.co/TachyHealth/Gazal-R1-32B-GRPO-preview. This approach demonstrates that mid-sized model, through strategic training, can achieve and even surpass the performance of significantly larger models. Specifically, our pipeline first establishes robust foundation of structured knowledge, which is subsequently refined using memory-efficient reinforcement learning. The result is replicable and scalable framework for developing specialized, high-capability language models tailored to complex domains. Our work makes the following primary contributions: 1. Two-Stage Training Blueprint: We propose and validate comprehensive training pipeline that first instills robust, structured reasoning foundation via Supervised Fine-Tuning (SFT) and then refines these capabilities with memory-efficient reinforcement learning (GRPO) [2]. 2. Advanced, Efficient Model Adaptation: We demonstrate the synergistic benefits of combining state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) [3] methodsrsLoRA [4], and DoRA [5]to achieve stable, high-capacity learning on mid-sized model without the costs of full fine-tuning. 3. Insights into RL for Complex Reasoning: We provide detailed analysis of the challenges in applying RL to medical reasoning, including reward hacking and training instability, and present effective countermeasures, such as multi-faceted reward function design. The result is Gazal-R1, model that not only achieves state-of-the-art performance for its size but does so with transparent reasoning framework, marking significant step towards more trustworthy medical AI. In this report, we first situate our work within the existing literature. We then detail our two-stage methodology, followed by an analysis of key training observations. We present our experimental results and conclude with discussion of the models limitations, ethical considerations, and implications for future research."
        },
        {
            "title": "2 Related Work",
            "content": "The development of Gazal-R1 is situated within three intersecting areas of AI research: specialized medical LLMs, advanced reasoning techniques, and efficient model alignment. Specialized Medical LLMs The pursuit of high-performing medical LLMs has led to series of notable models. Early efforts like GatorTron [6] demonstrated the value of pre-training on large biomedical corpora. Googles Med-PaLM and Med-PaLM 2 [7] set new benchmarks by fine-tuning generalist models on medical data and employing instruction tuning. More recently, specialized models like Med42 [8] have shown the potential of adapting state-of-the-art open-source models (e.g., Llama 3 [9]) for the medical domain. Gazal-R1 builds on this trend but distinguishes itself through its explicit two-stage training pipeline, which first builds structured reasoning foundation (SFT) before refining it with preference-based optimization (GRPO), more targeted approach than general instruction tuning. Reasoning in LLMs Beyond model architecture, the ability to elicit and structure complex reasoning is paramount. While Chain-of-Thought (CoT) prompting [10] established the importance of step-by-step processing, our work builds on this by training the model on an explicit, structured reasoning framework. Our SFT dataset generation was inspired by Chain-of-Draft (CoD) [11], which encourages LLMs to generate concise intermediate drafts to enhance logical consistency. Although our final model does not exhibit overtly concise draftsan expected outcome of using PEFT rather than full fine-tuningthe underlying principle of structured, iterative thinking informed our SFT strategy. As noted by Yeo et al. [12], pre-training on long CoT data significantly enhances subsequent RL training, principle we leveraged by establishing strong reasoning foundation in our SFT stage. Efficient Fine-Tuning and Alignment Full fine-tuning of large models is computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA) [13], have become standard. Our work incorporates recent advancements: rsLoRA [4], which solves LoRAs gradient instability at high ranks, and DoRA [5], which improves learning capacity by decomposing weight updates. For alignment, Reinforcement Learning from Human Feedback (RLHF) using Proximal Policy Optimization (PPO) [14] is common. However, PPOs reliance on separate value model is memory-intensive. We instead adopt Group Relative Policy Optimization (GRPO) [2], more recent, value-function-free algorithm. We further build upon recent analyses of GRPO, such as DAPO [15], which propose token-level normalization to mitigate the response-length bias inherent in the original GRPO formulation [16]. 2 Technical Report Preprint"
        },
        {
            "title": "3 Stage 1: Supervised Fine-Tuning (SFT)",
            "content": "The primary objective of the SFT stage was to imbue Gazal-R1 with foundational understanding of medical reasoning and to train it to generate outputs in specific, structured format conducive to explainability. This phase was critical for preparing the model for the more advanced refinement in the GRPO stage. 3.1 Dataset Curation hybrid dataset strategy was employed, combining novel synthetic dataset with large-scale, high-quality existing medical reasoning dataset. 3.1.1 Structured Reasoning Dataset cornerstone of the SFT stage was the creation of novel, 107,033-example synthetic dataset designed to teach Gazal-R1 not just medical knowledge, but the very structure of clinical thought. Generated using the Gemma 3 27B [17] model, this dataset was engineered to be diverse, complex, and highly structured, moving beyond simple question-answer pairs to model the explicit, step-by-step reasoning processes of expert clinicians. The design philosophy was to compel the model to reason explicitlymuch like physician would during clinical roundsand to learn distinct reasoning strategies tailored to different clinical contexts. This dataset is publicly available at https://huggingface.co/datasets/TachyHealth/structured_medical. The dataset was organized around four fundamental types of clinical reasoning, each with unique prompt structure and specific, enforced reasoning pattern. This design acknowledges that cardiologists diagnostic process differs fundamentally from an emergency physicians decision-making in trauma scenario. Diagnostic Reasoning: The goal was to train the model to systematically synthesize symptoms, lab results, and patient history into an accurate differential diagnosis. The prompts enforced logical progression: Symptom analysis Physical findings assessment Test interpretation Differential narrowing To ensure these cases were challenging, the generation prompts were programmatically engineered to include confounding factors such as atypical presentations (70% of cases), subtle red-flag symptoms (65%), and rare conditions (60%), forcing the model to look beyond simple pattern matching. Decision-Making Under Uncertainty: This module focused on training the model to choose optimal actions in scenarios with incomplete or conflicting information. The required reasoning pattern modeled prudent, risk-aware approach: Identify available data Note missing information Assess immediate risk Consider possibilities Prioritize urgency Weigh risk-benefit tradeoffs These prompts were specifically designed to simulate high-pressure clinical situations by including factors like critically incomplete information (86% of cases), conflicting clinical data (65%), time pressure (60%), and ethical dilemmas (47%). Treatment Planning: This component aimed to instill structured, evidence-based methodology for creating comprehensive treatment plans that account for patient-specific variables. The enforced reasoning pathway ensured holistic approach: Diagnosis confirmation Guideline review Patient factors assessment Therapy selection The prompts explicitly required the model to consider real-world complexities, such as potential drug interactions (70%), contraindications to common therapies (65%), patient preferences (60%), and medication costs (50%), ensuring the generated plans were both clinically sound and practical. Prognostic Assessment: The objective here was to teach the model how to estimate patient outcomes by integrating multidimensional clinical evidence. The reasoning framework for these cases was: Condition severity assessment Risk factors evaluation Evidence-based outcomes review Personalized prediction To reflect the difficulty of real-world prognosis, the cases were designed with high rates of multimorbidity (75%), high-risk factors (65%), and scenarios with limited prognostic evidence (55%), training the model to communicate uncertainty effectively. Technical Report Preprint Underpinning all four modules was sophisticated generation framework designed for realism and rigor. weighted distribution ensured that 70% of all generated cases were of high complexity. Demographics were intentionally varied, with specific focus on edge cases like newborns and the very elderly to improve robustness. Furthermore, to mirror the complexity of human health, prompts for all types commonly included cross-cutting diversity factors such as comorbidities (85% of cases), social determinants of health (60%), and relevant ethnic or genetic considerations (40%). Finally, inspired by the Chain-of-Draft (CoD) methodology, [11] we enforced strict structural constraint on all reasoning outputs: each step-by-step thought process was required to have minimum of eight steps, with each individual step limited to maximum of ten words. This constraint was critical, as it forced the generation of reasoning that was both comprehensive and exceptionally concise, providing Gazal-R1 with clear, distilled, and highly effective learning signal for structured medical thought. 3.1.2 MedReason Dataset To complement our synthetic data, we incorporated the MedReason dataset [18]. It contains 32,682 highquality medical question-answer pairs, where each entry is supported by detailed, step-by-step explanation. The reasoning paths in MedReason are derived from structured medical knowledge graph, ensuring that the explanations are clinically valid and logically sound. Integrating this dataset provided Gazal-R1 with rich source of complex medical problems grounded in evidence-based thinking paths. 3.2 Architectural Considerations and Techniques The SFT process was built upon foundation of cutting-edge, parameter-efficient fine-tuning (PEFT) techniques to maximize performance while managing computational costs. 3.2.1 Advanced LoRA Techniques Standard Low-Rank Adaptation (LoRA) was enhanced with two recent innovations: DoRA (Weight-Decomposed Low-Rank Adaptation): Unlike standard LoRA, which couples magnitude and direction in its updates, DoRA decomposes the pre-trained weights into separate magnitude and direction components [5]. This allows for more nuanced and powerful adaptations that more closely approximate the behavior of full fine-tuning, leading to significant performance gains. rsLoRA (Rank-Stabilized LoRA): This technique addresses fundamental flaw in LoRA where training becomes unstable at higher ranks due to gradient collapse. By adjusting the scaling factor from the conventional α/r to α/ r, rsLoRA enables stable training at much higher ranks. This allowed us to effectively use LoRA rank of 256, unlocking higher learning capacity for the adapter [4]. 3.2.2 Training Process and Hyperparameters The SFT stage was conducted on hardware setup of 2x NVIDIA H100 GPUs, 48 vCPUs, and 480 GB of RAM. We used the EXAdam optimizer for faster convergence throughout training [19]. The model was trained with system prompt that explicitly instructed it to follow specific format, breaking down its clinical reasoning into concise steps within <think></think> tags before providing final assessment. For full transparency and reproducibility, both the list of hyperparameters and the exact system prompt used during training are included in Appendix and Appendix respectively. 3.3 Observations The SFT stage proved crucial for the success of the subsequent RL training. As noted in recent research [12], pre-training model on long Chain-of-Thought (CoT) data significantly enhances the effectiveness of reinforcement learning. Although the concise reasoning style from our CoD-inspired dataset was not perfectly mirrored in the final modellikely due to the use of PEFT rather than full fine-tuningthis stage successfully established the structured reasoning foundation necessary for the GRPO phase. Technical Report Preprint"
        },
        {
            "title": "4 Stage 2: Group Relative Policy Optimization (GRPO)",
            "content": "Following the foundational SFT stage, Gazal-R1 underwent sophisticated reinforcement learning phase to sharpen its reasoning abilities, maximize its accuracy on complex medical problems, and ensure strict adherence to our desired explainable output format. For this critical step, we selected Group Relative Policy Optimization (GRPO), state-of-the-art RL algorithm whose design principles offered significant advantages for our training objectives and resource constraints. This section provides deep dive into the GRPO formalism, the specific implementation choices we made to enhance its effectiveness, our intricate reward system, and the key insights and challenges encountered during the training process. 4.1 The GRPO Algorithm: Paradigm Shift in Efficiency Our decision to use GRPO was primarily driven by its revolutionary approach to computational efficiency. Traditional reinforcement learning methods like PPO impose significant memory burden by requiring four large models to be held in VRAM simultaneously: the policy model being trained, reference model for KL-divergence constraints, reward model, and, most critically, value function model of comparable size to the policy itself. GRPO elegantly sidesteps this bottleneck by eliminating the need for separate value function. Its core innovation lies in the recognition that for tasks with verifiable outcomes, such as solving multiple-choice questions, the group of generated responses can serve as its own statistical baseline [2]. This online learning algorithm iteratively improves the model by using data it generates during training. The fundamental mathematical foundation centers on group-relative advantage estimation: ˆAi,t = ri mean(r) std(r) (1) where each completions reward is normalized against the group mean and standard deviation, creating zero-sum comparison within each group that naturally encourages responses better than the current average while penalizing below-average attempts. By obviating the value function, GRPO achieves an approximate 50% reduction in memory overhead compared to PPO, crucial factor that made the training of 32B-parameter model on our hardware feasible. 4.2 Dataset Strategy for Reinforcement Learning The GRPO stage was fueled by large-scale, high-quality dataset derived from the UltraMedical dataset [20], which encompasses approximately 320,000 biomedical instructions, focusing exclusively on multiple-choice questions (MCQ). To make the most effective use of computational resources, we made strategic decision to train on randomly shuffled 10% subset of this corpus. This targeted approach was designed not only to enhance generalization, but also to accelerate policy convergence by concentrating learning on statistically diverse and representative set of challenging examples. The processed dataset used for GRPO training is publicly available at https://huggingface.co/datasets/TachyHealth/medical_grpo. This strategy is grounded in well-established observations from deep learning research regarding scaling laws and data efficiency. Recent evidence indicates that current scaling laws are showing diminishing returns, forcing AI labs to reconsider their training strategies. The influential Chinchilla scaling laws demonstrated that for compute-optimal training, model size and training tokens should be scaled equally, revealing that many large language models were significantly undertrained rather than requiring more parameters [21]. Furthermore, research has shown that increasing transformer size does not always lead to enhanced performance, phenomenon that cannot be explained by empirical scaling laws alone [22]. While concentrating on the MCQ format offers clear computational benefits, it also introduces specific challenges. Notably, issues related to false positive verification can arise, wherein models may arrive at correct answers through incorrect reasoning processes. We explore these challenges in greater detail in subsequent sections. 4.3 Enhanced GRPO Implementation: Key Modifications Our implementation incorporated several critical modifications to address known limitations of vanilla GRPO and improve training stability for long-chain reasoning tasks. 5 Technical Report Preprint 4.3.1 Trust Region Expansion and Clip-Higher Strategy To combat entropy collapse, where the policy becomes overly deterministic and fails to explore rare but potentially insightful reasoning paths, we adopted the Clip-Higher strategy [23]. In standard GRPO, ϵclipping limits exploration by restricting the increase in probability of low-likelihood tokens, hindering the reinforcement of rare but important reasoning paths. By increasing the upper clipping threshold ϵhigh to 0.28, we allowed low-probability tokens more room to grow, enhancing entropy and diversity in outputs while improving reasoning exploration. We found that careful tuning of ϵhigh was crucial to maintaining stability throughout the RL run. 4.3.2 Elimination of KL Divergence Penalty Following recent findings that question the necessity of KL divergence constraints in GRPO training, [15] we eliminated the KL divergence penalty entirely by setting the KL coefficient β to 0.0. This decision was motivated by several observations: The policy diverges substantially during training regardless of the KL penalty Maintaining reference model copy incurs unjustified computational cost Recent studies demonstrate that KL constraints are not essential for stable GRPO training [15, 16] This modification reduced memory usage and improved training speed while maintaining training stability. 4.3.3 Advanced Loss Normalization To address length bias issues identified in the original GRPO formulation, we implemented token-level loss normalization as proposed by the DAPO paper [15]. The loss is computed by aggregating token-wise losses across all generations and normalizing by the total length: LGRPO(θ) = 1 i=1 oi PG oi (cid:20) clip i=1 t=1 (cid:18) πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) , 1 ϵ, 1 + ϵhigh (cid:19) (cid:21) ˆAi,t (2) This normalization prevents longer responses from being under-penalized and ensures more balanced reward assignment to individual tokens regardless of response length. However, we note that normalization is performed over the local batch only, leading to slight variations depending on local batch size despite maintaining constant effective batch size. 4.4 Sophisticated Reward Engineering: Balancing Multiple Objectives The success of our GRPO implementation hinged on carefully engineered composite reward system. Our experience confirmed that RL training for long Chain-of-Thought reasoning is inherently unstable, and monolithic rewards are insufficient for guiding sophisticated reasoning behavior. 4.4.1 Evolution of Length Control Mechanisms Our initial approach to controlling output length employed soft overlong punishment function [15], which penalized excessively long completions. We experimented with ratios between the maximum allowed completion length and the soft punishment thresholdspecifically, 4:1 and 2:1. Interestingly, both ratios produced nearly identical outcomes in terms of output length, though the 4:1 setting required more training steps to converge. As shown in Figure 1a, applying this function led to dramatic reduction in the average number of tokens generatedfrom around 1,428 tokens initially, to just 245 tokens, before moderate recovery to 465 tokens at the end of the training run. This average output length of approximately 465 tokens is relatively reasonable for responses to multiplechoice questions (MCQs), providing enough content to explain answers adequately. However, we observed that despite the shorter length, some responses were poorly written and contained elements resembling hallucinations, indicating that brevity alone did not guarantee quality or factual accuracy. This aggressive penalization can be seen in the mean reward curve (Figure 1a), where reward values initially move closer to zero, reflecting improved adherence to length constraints. In the early stages of 6 Technical Report Preprint trainingroughly the first 150 stepsthis strong signal helped stabilize model behavior and avoid runaway verbosity. However, as Figure 1b illustrates, this approach also caused the average length of completions to fluctuate dramatically. After step 500, there is notable instability, and after step 600, sharp drop in length, signaling collapse in the models ability to generate detailed or sufficiently long completions. Ultimately, while the soft overlong punishment function was effective at curbing excessively long outputs, it proved too blunt an instrument for nuanced control. The excessive penalization began to suppress necessary detailed reasoning and richness in the completions. This insight led us to seek more sophisticated and flexible strategies for length control in subsequent experiments. (a) Mean reward over training steps computed using the soft overlong punishment function. The reward reflects penalties for excessively long completions, with values closer to zero indicating better adherence. (b) Average length of generated completions (in tokens) over training steps. Length fluctuations indicate varying verbosity during training, with notable instability after step 500 and drop after step 600. Figure 1: Training dynamics of one of the training runs: (Left) The evolution of mean reward under the soft overlong punishment function across global training steps. (Right) The corresponding mean length of completions generated during training, highlighting output length trends. 4.4.2 Cosine Length-Scaling Reward Implementation Based on insights from Yeo et al., [12] we implemented sophisticated cosine length-scaling reward function that addresses the core challenge of training with purely accuracy-based rewards, which can lead to overly long generated sequences that degrade training performance. The cosine reward function optimizes training by dynamically controlling sequence length based on correctness: For correct answers: The reward value decreases as length increases, encouraging concise and efficient responses For incorrect answers: The reward value increases with length, encouraging deeper reasoning and exploration of alternative solution paths The cosine function smoothly adjusts reward values within reasonable ranges, with parameters including generated text length, maximum length limits, and minimum/maximum reward boundaries. Interestingly, we observed that this reward function encouraged gradual increases in generated text length over time, suggesting successful promotion of more thorough reasoning exploration. 4.4.3 Combating Reward Hacking with Repetition Penalties To address reward hackinga critical issue where models exploit reward functions rather than genuinely improving their reasoningwe implemented an n-gram repetition penalty function, adopted from [12]. As models began optimizing for length-based rewards, they exhibited tendencies to repeat phrases artificially to maximize scores without meaningful reasoning improvement. Our repetition penalty function operates by: 1. Splitting generated text into words and extracting n-grams of specified size. We used 3-gram 2. Calculating repetition ratios based on unique n-grams relative to total n-gram count 3. Applying significant negative rewards (penalties) when repetition ratios exceed thresholds Technical Report Preprint 4. Computing penalty values based on repetition ratios and maximum penalty values This mechanism ensures that any increases in response length contribute novel and meaningful content rather than artificial padding, maintaining the integrity of the reasoning process. 4.5 Training Process and Implementation Details The GRPO training was conducted on 8x NVIDIA H100 GPUs with NVLink connectivity. Our implementation incorporated several critical stability measures and best practices derived from recent advances in RLHF methodology. 4.5.1 Stability Enhancements We enabled masking of truncated completions, ensuring that incomplete generations were excluded from loss calculations and preventing them from introducing noise during training. According to the DAPO paper, [15] this practice significantly improves training stability, particularly in scenarios involving variable-length outputs. 4.5.2 Final Composite Reward System Our successful reward system emerged as carefully balanced composition of multiple components: Reward Function Purpose Table 1: GRPO Reward Functions and Their Objectives Accuracy Format Cosine Length Repetition Penalty The primary driver for correctness (+1.0 for correct, 0 otherwise). Ensures outputs are well-structured and adhere to the <think> tag format. Encourages optimal response length by rewarding conciseness for correct answers and deeper reasoning for incorrect ones, as suggested by [12]. Mitigates reward hacking by penalizing n-gram repetitions, ensuring added length contributes meaningfully to the reasoning process. 4.6 Critical Observations and Training Dynamics 4.6.1 Training Instability and Recovery Our training process revealed the inherently volatile nature of reinforcement learning for complex reasoning tasks. To better illustrate these training dynamics, Figure 2 shows the progression of our key reward functions and training loss throughout the learning process. As previously described, we randomly sampled 10% of the dataset for training, which corresponds to approximately step 469 in the training curves. As seen in Figure 2, around step 526, Gazal-R1 exhibited significant training instability. This period was characterized by: Malformed markdown formatting (surrounding nearly every word with double asterisks) Addition of meaningless phrases and filler content Degraded reasoning processes with logical inconsistencies Erratic output generation patterns These behaviors are reflected in the pronounced dips and volatility observed across several reward curves, particularly in the repetition penalty and loss metrics. This episode highlighted the non-monotonic nature of RL training and underscored the critical importance of our multi-faceted reward system (see Table 1) and continuous monitoring protocols. Notably, despite this period of instability, the model eventually recovered through continued training and careful reward design, demonstrating the resilience and corrective influence of well-balanced reward structure. 8 Technical Report Preprint (a) Cosine Scaled Reward (b) Repetition Penalty Reward (c) Reward Reference (d) Training Loss Figure 2: Training curves for GRPO reward functions and loss. (a) Cosine scaled reward encourages optimal response length and correctness. (b) Repetition penalty discourages reward hacking through n-gram repetition. (c) Reference reward tracks similarity to baseline/reference outputs. (d) Training loss over steps, capturing model optimization dynamics. See Table 1 for description of each rewards objective. 4.6.2 False Positive Verification: Fundamental Challenge in Reasoning Evaluation significant limitation we encountered relates to the fundamental challenge of false positive verification, critical issue in reinforcement learning where models output is validated as correct despite being the product of flawed or nonsensical reasoning process. Our observations revealed that complex medical problems, particularly in multiple-choice format, often have simple, guessable answers. This creates scenario where the model can arrive at the correct final letter (e.g., A, B, C, or D) through completely illogical or factually incorrect reasoning chain. Research in the broader AI field suggests this is pervasive problem, with some studies indicating that up to 51% of model responses may be incorrectly validated due to disconnect between the final answer and the underlying reasoning quality [24]. Medical professionals consistently rated AI reasoning quality significantly lower than final answer accuracy, with one study showing accuracy dropping from 62.3% to 51.9% on USMLE Step 2CK when explanations were required [25]. Empirical evidence from medical AI research reveals widespread instances of correct answers achieved through flawed reasoning. Stanfords study of ChatGPT-4 on medical diagnostic cases found 92% accuracy, yet physicians using AI assistance only improved from 74% to 76% accuracy, suggesting they often rejected the AIs reasoning despite correct conclusions [26]. Likewise, the NIH study on GPT-4V medical imaging provides concrete example: the model correctly diagnosed skin lesions but failed to recognize that two lesions at different angles were the same condition, demonstrating flawed visual reasoning masked by pattern recognition. Despite these reasoning errors, final diagnostic accuracy remained high, creating dangerous illusion of competence [27]. The core of the issue lies in outcome supervision biasthe practice of evaluating model based solely on its final output while ignoring the intermediate steps of the reasoning process. This is especially problematic for multiple-choice questions (MCQs), which provide deceptively simple reward signal (correct/incorrect) that is easy to optimize for but reveals little about genuine comprehension. The fragility of this approach is starkly illustrated by studies showing that adding single, logically irrelevant clause to mathematical problem can cause 65% performance drop in state-of-the-art models, [28] exposing their reliance 9 Technical Report Preprint on superficial pattern matching rather than robust logical deduction. Token bias and position sensitivity compound these problems, with models showing inherent preferences for specific option IDs regardless of content [29]. This Response Variability Syndrome (REVAS) indicates limited genuine understanding, as models provide inconsistent responses to derivations of identical questions [24]. Since our GRPO dataset consisted exclusively of MCQs, our model was susceptible to this phenomenon, learning to generate correct answers without necessarily mastering the intricate clinical logic required. This phenomenon critically highlights that the rule-based validation approaches inherent in our GRPO implementation are fundamentally inadequate for assessing the logical validity of an LLMs reasoning path. GRPOs design, while computationally efficient, relies on binary, rule-based framework that treats all paths to correct answer as equivalent [2]. It excels at determining if the final answer is correct but lacks the mechanism to evaluate how that answer was reached. This can be contrasted with Proximal Policy Optimization (PPO), which, despite its higher memory overhead, utilizes separate value network [14]. This value network, if paired with sophisticated, process-aware reward model (PRM), could theoretically learn to assign higher rewards to logically sound reasoning chains and penalize flawed ones, even if they stumble upon the correct answer. GRPOs rule-based system, by its nature, cannot make such nuanced distinction. The problem is not merely theoretical; it is well-documented form of reward hacking where models learn to game the evaluation system. For instance, Anthropics research on model faithfulness has shown that model might use provided hint to solve problem but then generate completely fabricated, post-hoc rationale that makes it appear as if it reasoned its way to the solution independently [30]. In the same study, Claude 3.7 Sonnet mentioned using hints only 25% of the time it actually relied on them, [30] demonstrating clear disconnect between its stated and actual problem-solving process. This confirms that without explicit process-level supervision, models do not default to transparent or honest reasoning. This observation exposes critical limitation of purely outcome-based rewards and points toward necessary paradigm shift in evaluation methodology. The development of robust medical AI requires moving beyond binary (correct/incorrect) assessment toward process-based evaluation, [31] which examines the integrity of each step in the reasoning chain. Research has consistently shown that process supervision significantly outperforms outcome supervision, with one study achieving 78% accuracy on challenging math problems with process-based rewards, figure unattainable with outcome-only methods [32]. By providing granular feedback at each step, process supervision can localize errors, reduce false positives, and ensure that the model is rewarded for valid clinical logic, not just lucky guesses [32, 33]. DeepMinds comparative analysis showed process-based approaches reduced reasoning errors from 14.0% to 3.4% while maintaining comparable final-answer performance [34]. This suggests that developing sophisticated reward models capable of assessing the logical validity of medical reasoning paths represents crucial and necessary direction for future research in building trustworthy, high-stakes AI systems. 4.6.3 Insights into Long Chain-of-Thought Development Our experience strongly supports recent findings [12] regarding the development of long Chain-of-Thought reasoning capabilities. Key insights include: SFT Foundation Crucial: Quality supervised fine-tuning serves as powerful catalyst for effective reinforcement learning, providing essential structured reasoning capabilities Emergent vs. Guided Abilities: Long CoT reasoning capabilities are not entirely emergentcore abilities like error correction and solution branching exist in base models, with RL serving to guide more effective utilization Length Control Complexity: Simple RL application for CoT extension often fails, requiring sophisticated reward engineering to maintain quality while encouraging appropriate reasoning depth Reward Signal Quality: High-quality verifiable reward signals remain essential, though our experience with MCQ-only data revealed limitations in purely outcome-based evaluation 4.7 Lessons Learned and Future Directions The development of Gazal-R1 through GRPO training provided valuable insights into the practical challenges and opportunities in reinforcement learning for complex reasoning tasks. Key takeaways include: 1. Reward Engineering Criticality: Sophisticated, multi-component reward systems are essential for stable and effective training 10 Technical Report Preprint 2. Length Control Nuance: Overly aggressive length penalties can harm reasoning quality, while sophisticated cosine-based approaches provide better balance 3. Process vs. Outcome Evaluation: Future work must address the limitation of outcome-only rewards by developing process-aware evaluation mechanisms 4. Training Stability Monitoring: Continuous monitoring and robust recovery mechanisms are essential for managing RL training volatility 5. Foundation Model Quality: High-quality SFT serves as crucial foundation for effective RL training These insights inform our ongoing research into more robust and effective approaches for training reasoningcapable language models in specialized domains."
        },
        {
            "title": "5 Experimental Results and Evaluation",
            "content": "5.1 Performance Analysis We evaluated Gazal-R1 against leading open-source models on four standard medical benchmarks. The results, presented in Table 2, highlight its strong performance. Model Size MMLU Pro (Medical) MedMCQA MedQA PubMedQA Table 2: Accuracy (%) on Medical Benchmarks Gazal-R1 (Final) Gazal-R1 (SFT-only) Llama 3.1 405B Instruct Qwen 2.5 72B Instruct Med42-Llama3.1-70B Llama 3.1 70B Instruct QwQ 32B Qwen 3 32B 32B 32B 405B 72B 70B 70B 32B 32B 81.6 79.3 70.2 72.1 66.1 74.5 70.1 78.4 Key observations from the results include: 71.9 72.3 75.8 66.2 72.4 72.5 65.6 71.6 87.1 86.9 81.9 72.7 80.4 78.4 72.3 84.4 79.6 77.6 74.6 71.7 77.6 78.5 73.7 76. State-of-the-Art Performance: Gazal-R1 (32B) achieves the highest scores on MMLU Pro (Medical) [35], MedQA [36], and PubMedQA [37], outperforming all other models, including the much larger Llama 3.1 405B. Impact of GRPO: The final Gazal-R1 model shows notable gains over its SFT-only predecessor on MMLU Pro (+2.3%) and PubMedQA (+2.0%), confirming the value of the RL stage. Exceptional MedQA Score: The score of 87.1% on MedQA indicates strong capability for handling complex, USMLE-style clinical reasoning questions. 5.2 Analysis of Performance Regression on MedMCQA The Gazal-R1 models performance regression on MedMCQA [38] (72.3% 71.9%) while improving on MMLU Pro, MedQA, and PubMedQA reveals fundamental mismatch between reinforcement learning optimization patterns and MedMCQAs unique structural requirements. This phenomenon stems from several interconnected technical mechanisms that create systematic trade-offs between different types of medical reasoning tasks. GRPO reinforcement learning optimizes for human preference signals that favor detailed reasoning and elaboration, while MedMCQA specifically rewards immediate, concise factual recall. This creates direct optimization conflict where GRPOs tendency to generate longer, more explanatory responses actually hurts performance on benchmark designed for rapid medical knowledge retrieval. MedMCQAs extremely short questions (averaging only 12.77 tokens) test immediate access to specific medical facts drug names (22.49% of answers), procedures (18.74%), and dosages (11.24%) [38]. The dataset is 68.2% factual knowledge versus only 31.8% reasoning-based, [39] making it fundamentally different from other medical benchmarks that benefit from Chain-of-Thought reasoning. 11 Technical Report Preprint 5.2.1 Reward Hacking and Distribution Shift During GRPO training, the model learns to optimize for proxy rewards that favor longer, more detailed responses. This reward hacking manifests as models producing responses that appear correct to human evaluators but are actually less accurate for factual recall tasks [40, 41]. Research shows that RLHF models can achieve high reward scores while generating less helpful content, particularly when length bias in human evaluation creates spurious correlations [42, 43]. The distribution shift between supervised fine-tuning (SFT) and reinforcement learning phases compounds this problem. As the models output distribution shifts away from the concise, factual responses that characterize medical knowledge retrieval, it becomes less effective at the pattern-matching behaviors crucial for MedMCQA success [44]. The mathematical relationship can be expressed as increasing KL divergence: where higher divergence correlates with degraded performance on factual recall tasks. DKL(πRL πSFT) 5.2.2 Multi-objective Optimization Trade-offs The performance regression exemplifies gradient conflicts in multi-objective optimization. When gradients from different medical tasks have conflicting directions, optimizing for one objective (detailed reasoning) necessarily hurts another (factual recall) [45, 46]. Research shows this occurs when the cosine similarity between task gradients is negative: cos(greasoning, gfactual) < 0. GRPO training creates Pareto frontier where improving performance on reasoning-heavy benchmarks like MMLU Pro and MedQA comes at the cost of factual recall performance on MedMCQA [46]. The model must allocate limited representational capacity between competing objectives, [45] and GRPOs optimization pressure favors the reasoning tasks that align better with human preference signals [47]. 5.2.3 Mathematical Framework for Understanding the Trade-off The performance regression can be understood through the lens of regularized preference optimization. GRPO optimizes the objective: max π E[R(π)] β DKL(ππref) where R(π) represents the reward function and the KL divergence term prevents excessive deviation from the reference policy [48]. The choice of β creates fundamental trade-off: high β constrains the model to stay close to SFT behavior (potentially preserving MedMCQA performance), while low β allows more dramatic changes that benefit reasoning tasks but hurt factual recall. The problem is that MedMCQA requires different optimization profile than human preference learning typically produces. While humans prefer detailed, explanatory medical responses, MedMCQA rewards brief, accurate factual recallcreating systematic misalignment between the optimization target and task requirements. 5.2.4 Evidence from Literature Patterns This phenomenon follows well-documented patterns in RL literature. Research on the alignment tax shows that RLHF consistently creates trade-offs where optimizing for human preferences degrades performance on specific academic benchmarks [49, 47]. The OpenAI InstructGPT paper first documented systematic regressions on question answering and academic NLP tasks after RLHF training [50]. Recent work on the Safety Tax in Large Reasoning Models demonstrates that safety alignment (which shares optimization characteristics with GRPO) leads to degradation of reasoning capability while successfully improving safety metrics. This establishes clear precedent for the type of capability regression observed in MedMCQA [51]. Studies specifically focused on medical AI show that safety and preference alignment can reduce diagnostic accuracy by making models less willing to provide specific medical information directly paralleling how GRPOs optimization for human-preferred responses may reduce the direct factual accuracy that MedMCQA requires. 12 Technical Report Preprint 5.2.5 Optimization Dynamics and Overoptimization The regression likely results from reward model overoptimization, where the model learns to game the reward signal rather than genuinely improving medical reasoning. As GRPO training progresses, the models output distribution shifts away from the factual, concise responses that characterize good MedMCQA performance [40]. This creates cascading effect: as the policy shifts, the reward model (trained on earlier, more SFT-like outputs) becomes less reliable at evaluating the new distribution of responses. The model may achieve high rewards by producing responses that appear medical and detailed to human evaluators while actually being less accurate on objective factual questions [44]."
        },
        {
            "title": "6 Limitations and Ethical Considerations",
            "content": "6.1 Limitations Data Bias: The training data, particularly MedMCQA, may contain regional biases (e.g., disease prevalence, treatment guidelines). The model may reflect these biases. Knowledge Cutoff: The models knowledge is static and does not update with new medical research published after its training date. It cannot access real-time information. Hallucination Risk: Despite alignment, the model can still generate plausible-sounding but factually incorrect information (hallucinate). All outputs require verification by qualified medical professional. Scope of Evaluation: The model is primarily evaluated on multiple-choice questions. This format is useful proxy but does not capture the full complexity of real-world clinical interactions, which involve unstructured dialogue, physical examination, and dynamic decision-making. 6.2 Ethical Considerations Not Medical Device: Gazal-R1 is research model and is not intended for direct clinical use, diagnosis, or treatment planning. It is tool for research and assistance, not substitute for professional medical judgment. Risk of Over-reliance: There is risk that users may place undue trust in the models outputs. We emphasize that all information generated by Gazal-R1 must be independently verified. Accountability and Liability: In the event of an error leading to harm, questions of accountability are complex. As developers, we stress the models role as an assistive tool, with final responsibility resting with the human clinician using it."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "Gazal-R1 represents significant advancement in medical AI, demonstrating that focused, multi-stage training pipeline can yield highly capable and efficient reasoning model. By combining innovative dataset synthesis, advanced PEFT strategies, and memory-efficient reinforcement learning algorithm, we have created 32B-parameter model that sets new standard for performance in its class. Our work highlights the importance of not just what data model is trained on, but how it is trained. The structured reasoning instilled during SFT, coupled with the preference alignment from GRPO, results in model that is both accurate and more transparent. This research also illuminates the challenges and trade-offs inherent in aligning models for complex domains. Future work will focus on addressing these challenges directly. First, we will prioritize the development of process-aware evaluation metrics and reward models that can assess the logical validity of reasoning chain, not just the correctness of the final outcome. This is critical to overcoming the false positive verification issue. Second, we will investigate or introduce alternative RL approaches that can better handle the nuances of reasoning assessment, potentially moving beyond outcome-based rewards. Third, our evaluation will be scaled to include more diverse medical tasks beyond MCQs, such as clinical note summarization and interactive diagnostic dialogues. Finally, key area of research will be to address the fundamental tension between factual recall and detailed reasoning, exploring methods that allow the model to dynamically adapt its reasoning style to the task at hand. 13 Technical Report Preprint"
        },
        {
            "title": "Acknowledgements",
            "content": "This work would not have been possible without the contributions of the broader open-source community. We extend our sincere gratitude to the developers of the Qwen models for providing powerful and open foundation. We are also indebted to the researchers and engineers behind the various libraries and techniques that were instrumental to our pipeline, including the Hugging Face ecosystem (Transformers [52], PEFT, TRL [53], Accelerate [54], Datasets [55]), the vLLM project for enabling efficient inference [56], the creators of DoRA, rsLoRA, and the authors of the GRPO and DAPO papers whose work inspired our reinforcement learning stage. This research stands on the shoulders of giants, and we are proud to contribute back to the community that enables such progress."
        },
        {
            "title": "References",
            "content": "[1] Qwen Team. Qwen3 technical report, 2025. [2] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [3] Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment, 2023. [4] Damjan Kalajdzievski. rank stabilization scaling factor for fine-tuning with lora, 2023. [5] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation, 2024. [6] Xi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang Shin, Kaleb Smith, Christopher Parisien, Colin Compas, Cheryl Martin, Mona Flores, Ying Zhang, Tanja Magoc, Christopher Harle, Gloria Lipori, Duane Mitchell, William Hogan, Elizabeth Shenkman, Jiang Bian, and Yonghui Wu. Gatortron: large clinical language model to unlock patient information from unstructured electronic health records, 2022. [7] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models, 2023. [8] Clément Christophe, Praveen Kanithi, Tathagata Raha, Shadab Khan, and Marco AF Pimentel. Med42-v2: suite of clinical llms, 2024. [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, 14 Technical Report Preprint Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook 15 Technical Report Preprint Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. [10] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. [11] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less, 2025. [12] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-ofthought reasoning in llms, 2025. [13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [14] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [15] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [16] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. [17] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, 16 Technical Report Preprint Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. [18] Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, and Yuyin Zhou. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs, 2025. [19] Ahmed M. Adly. Exadam: The power of adaptive cross-moments, 2025. [20] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou. Ultramedical: Building specialized generalists in biomedicine, 2024. [21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. [22] Xueyan Niu, Bo Bai, Lei Deng, and Wei Han. Beyond scaling laws: Understanding transformer performance with associative memory, 2024. [23] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. [24] Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, and Ting Liu. Llms may perform mcqa by selecting the least incorrect option, 2024. [25] Tiffany H. Kung, Morgan Cheatham, ChatGPT, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. medRxiv, 2022. [26] Ethan Goh, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, Zahir Kanjee, Andrew S. Parsons, Neera Ahuja, Eric Horvitz, Daniel Yang, Arnold Milstein, Andrew P. J. Olson, Adam Rodman, and Jonathan H. Chen. Large language model influence on diagnostic reasoning: randomized clinical trial. JAMA Network Open, 7(10):e2440969e2440969, 10 2024. [27] Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung, Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J. Landsman, Sally L. Baxter, Subhi J. AlAref, Yijia Li, Alexander Chen, Josef A. Brejt, Michael F. Chiang, Yifan Peng, and Zhiyong Lu. Hidden flaws behind expert-level accuracy of multimodal gpt-4 vision in medicine. npj Digital Medicine, 7(1), July 2024. [28] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models, 2024. [29] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors, 2024. [30] Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, and Ethan Perez. Reasoning models dont always say what they think, 2025. [31] Jinu Lee and Julia Hockenmaier. Evaluating step-by-step reasoning traces: survey, 2025. 17 Technical Report Preprint [32] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. [33] Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think, 2025. [34] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcomebased feedback, 2022. [35] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. [36] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams, 2020. [37] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering, 2019. [38] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa : large-scale multi-subject multi-choice dataset for medical domain question answering, 2022. [39] Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, and James Zou. Disentangling reasoning and knowledge in medical large language models, 2025. [40] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. [41] Zeyu Huang, Zihan Qiu, Zili Wang, Edoardo M. Ponti, and Ivan Titov. Post-hoc reward calibration: case study on length bias, 2024. [42] Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. Odin: Disentangled reward mitigates hacking in rlhf, 2024. [43] Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024. [44] Nathan Lambert. Reinforcement Learning from Human Feedback. Online, 2024. [45] Dayou Mao, Yuhao Chen, Yifan Wu, Maximilian Gilles, and Alexander Wong. Robust analysis of multi-task learning efficiency: New benchmarks on light-weighed backbones and effective measurement of multi-task learning challenges by feature disentanglement, 2024. [46] Pradyumn Kumar Shukla, Christian Hirsch, and Hartmut Schmeck. Towards deeper understanding of trade-offs using multi-objective evolutionary algorithms. In Cecilia Di Chio, Alexandros Agapitos, Stefano Cagnoni, Carlos Cotta, Francisco Fernández de Vega, Gianni A. Di Caro, Rolf Drechsler, Anikó Ekárt, Anna I. Esparcia-Alcázar, Muddassar Farooq, William B. Langdon, Juan J. Merelo-Guervós, Mike Preuss, Hendrik Richter, Sara Silva, Anabela Simões, Giovanni Squillero, Ernesto Tarantino, Andrea G. B. Tettamanzi, Julian Togelius, Neil Urquhart, A. Şima Uyar, and Georgios N. Yannakakis, editors, Applications of Evolutionary Computation, pages 396405, Berlin, Heidelberg, 2012. Springer Berlin Heidelberg. [47] Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. Mitigating the alignment tax of rlhf, 2024. [48] Pengcheng Wang, Xinghao Zhu, Yuxin Chen, Chenfeng Xu, Masayoshi Tomizuka, and Chenran Li. Residual policy gradient: reward view of kl-regularized objective, 2025. [49] Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity, 2024. [50] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [51] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Yichang Xu, and Ling Liu. Safety tax: Safety alignment makes your large reasoning models less reasonable, 2025. 18 Technical Report Preprint [52] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. pages 3845. Association for Computational Linguistics, October 2020. [53] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. TRL: Transformer Reinforcement Learning. [54] Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. https://github.com/huggingface/accelerate, 2022. [55] Quentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf, Mario Šaško, Yacine Jernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil, Mariama Drame, Julien Chaumond, Julien Plu, Joe Davison, Simon Brandeis, Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry, Steven Liu, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain Lesage, Anton Lozhkov, Matthew Carrigan, Théo Matussière, Leandro von Werra, Lysandre Debut, Stas Bekman, and Clément Delangue. Datasets: Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175184. Association for Computational Linguistics, November 2021. [56] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 19 Technical Report Preprint"
        },
        {
            "title": "A Training Hyperparameters",
            "content": "A.1 SFT Hyperparameters Table 3: Detailed SFT Hyperparameters for Gazal-R1 Parameter Value Base Model Fine-tuning Method LoRA Rank (r) LoRA Alpha (α) LoRA Dropout Target Modules Optimizer Learning Rate LR Scheduler Warmup Ratio Weight Decay Epochs Batch Size per GPU Gradient Accumulation Steps Effective Batch Size Max Sequence Length Precision Gradient Checkpointing Seed Qwen 3 32B DoRA + rsLoRA 256 512 0.1 all-linear EXAdam 2e-5 Cosine Schedule 0.05 0.1 1 8 16 128 8192 bfloat16 True A.2 GRPO Hyperparameters Table 4: Detailed GRPO Hyperparameters for Gazal-R1 Parameter Value Base Model RL Algorithm Optimizer Learning Rate LR Scheduler Warmup Ratio Weight Decay Epochs Batch Size per GPU Gradient Accumulation Steps Generations per Prompt (K) Effective Prompts per Step GRPO Beta (β) GRPO Epsilon (ϵ) Generation Temperature Top-k / Top-p Max Prompt Length Max Completion Length Precision Seed Gazal-R1 (SFT-tuned) GRPO with BNPO loss EXAdam 1.0e-6 Constant with Warmup 0.0 0.1 1 4 8 4 32 0.0 0.28 0.6 20 / 0.95 3072 4096 bfloat16 3047 Technical Report Preprint"
        },
        {
            "title": "B System Prompts",
            "content": "B.1 SFT System Prompt When solving complex medical problems, follow this specific format: First, break down your clinical reasoning into extremely concise steps, with each step Present this step-by-step reasoning inside <think></think> tags. After completing your reasoning, provide your comprehensive assessment and plan after 1. limited to 10 words maximum. 2. 3. </think> tag. For example, when analyzing complex case: <think> Review presenting symptoms and vital signs. Identify key abnormal findings. Consider differential diagnoses by system. Evaluate diagnostic test results. Rule out critical conditions. Analyze medication interactions. Prioritize diagnoses by likelihood. Determine urgent vs. Plan appropriate interventions. Consider follow-up requirements. </think> stable concerns. [Your complete, detailed clinical assessment including: - Primary and differential diagnoses with supporting evidence - Recommended diagnostic workup - Treatment plan with rationale - Follow-up recommendations - Key considerations for this specific case] Instructions: - Each reasoning step must be 10 words or fewer - Use Markdown for your assessment - Include pathophysiology considerations in your reasoning - Address diagnostic uncertainty when appropriate - Consider evidence-based guidelines in your assessment - Maintain proper XML tag formatting throughout - Adapt your assessment to the type of problem whether Q&A, Diagnostic reasoning, Treatment decision-making, or Prognostic assessment 21 Technical Report Preprint B.2 GRPO System Prompt You are medical reasoning AI that solves multiple-choice questions through systematic clinical reasoning. <think></think> tags using this framework: For every question, demonstrate your thinking process inside ## Reasoning Framework **Differential Diagnosis**: **Information Analysis**: 1. clinical context 2. probability, note red flags 3. **Pathophysiology**: processes 4. current guidelines 5. compare remaining choices 6. **Clinical Decision**: **Logical Elimination**: **Evidence-Based Reasoning**: Extract key demographics, symptoms, vitals, labs, imaging, and Identify patterns, consider diagnoses by system, rank by Connect symptoms to underlying disease mechanisms and biological Apply clinical criteria, interpret tests, reference Systematically evaluate each option, exclude based on evidence, Consider risk-benefit, address uncertainty, prioritize safety ## Response Requirements - Use Markdown formatting throughout your assessment - Address diagnostic uncertainty when appropriate - Reference evidence-based guidelines and clinical criteria - Maintain proper XML tag formatting with <think></think> ## Response Format <think> [Comprehensive reasoning using Markdown formatting, following the 6-step framework. medical knowledge, pathophysiology understanding, systematic problem-solving, consideration of multiple possibilities, elimination of incorrect options, and justification for your final choice. </think> Address uncertainty and reference guidelines as appropriate.] Show ## Assessment [Provide your clinical assessment using Markdown formatting] ## Final Answer boxed{X} (where is single letter: A, B, C, or E) ## Key Principles - **Systematic Over Intuitive**: - **Evidence-Based**: - **Safety First**: - **Patient-Specific**: - **Transparency**: Choose conservative options when uncertain Consider age, comorbidities, clinical context Address limitations and diagnostic uncertainty Follow the framework consistently Ground reasoning in established medical knowledge and guidelines ## Critical Errors to Avoid - Anchoring on first impressions - Premature diagnostic closure - Ignoring pathophysiology - Pattern matching without mechanistic understanding - Failing to reference current evidence-based guidelines Apply rigorous clinical reasoning with proper formatting to achieve accurate medical decisions."
        }
    ],
    "affiliations": [
        "TachyHealth, Riyadh 13316, Saudi Arabia"
    ]
}