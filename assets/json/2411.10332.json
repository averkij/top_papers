{
    "paper_title": "Number it: Temporal Grounding Videos like Flipping Manga",
    "authors": [
        "Yongliang Wu",
        "Xinting Hu",
        "Yuyang Sun",
        "Yizhou Zhou",
        "Wenbo Zhu",
        "Fengyun Rao",
        "Bernt Schiele",
        "Xu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9\\% in mIoU for moment retrieval and 8.5\\% in mAP for highlight detection. The code will be available at https://github.com/yongliang-wu/NumPro."
        },
        {
            "title": "Start",
            "content": "Number it: Temporal Grounding Videos like Flipping Manga Yongliang Wu1* Xinting Hu2* Yuyang Sun1 Yizhou Zhou3 Fengyun Rao3 Bernt Schiele2 Xu Yang1 3WeChat, Tencent Inc. Wenbo Zhu4 2Max Planck Institute for Informatics 4University of California, Berkeley 1Southeast University 4 2 0 N 5 1 ] . [ 1 2 3 3 0 1 . 1 1 4 2 : r {yongliangwu,yysun,xuyang palm}@seu.edu.cn {harryizzhou,fengyunrao}@tencent.com {xhu,schiele}@mpi-inf.mpg.de wenbo zhu@berkeley.edu Figure 1. Effectiveness of Adding Frame Numbers for Temporal Grounding: (a) Without numbered images or frames, both humans and Vid-LLMs struggle to locate specific timestamps accurately. (b) Once numbered, grounding temporal cues becomes as intuitive as flipping manga, where timestamps are accessible at glance."
        },
        {
            "title": "Abstract",
            "content": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this gap, we introduce Number-Prompt (NumPro), novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating video as sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to read event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on NumPro-enhanced dataset defines new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9% in mIoU for moment retrieval and 8.5% in mAP for highlight detection. The code will be available at https://github.com/ yongliang-wu/NumPro. 1. Introduction Imagine you are watching cooking video, and trying to locate the exact moment when the chef stirs in the spices. While recognizing such actions is feasible, translating that visual information into precise timing, i.e., specific second or frame number, is surprisingly difficult. This challenge is central to the field of Video Temporal Grounding (VTG) [4, 16, 21, 40, 44]. In the realm of Video Large Language Models (Vid-LLMs) [28, 33, 51, 67], the integration of VTG allows for fine-grained visual and temporal understanding and reasoning of videos, which is pivotal for developing end-to-end video dialogue systems. Despite advances of Vid-LLMs, endowing these models with effective VTG abilities presents unique challenge: enhancing the models visual recognition of an event within video does not inherently enable it to describe when the event begins and ends using language [21, 44]. For instance, advanced Vid-LLMs like Qwen2-VL [51], while excelling at video comprehension, can struggle with grounding specific events in time. When asked, e.g., to locate when does the woman eat food in 10-frame video, the model can hallucinate an illogical answer like from frame 000 to 580. This limitation arises because these models are primarily trained to align visual content with language descriptions (what happens) while lacking mechanisms to directly interpret the temporal boundaries (when does it happen). This gap in powerful Vid-LLMs leads us to think: How can we empower Vid-LLMs to extract temporal cues directly through visual recognition? familiar human experience flipping manga provides an intuitive solution. When flipping manga, each numbered panel guides readers to follow the sequence of the narrative, linking visual conInspired by this, we tent with clearly defined timeline. introduce Number-Prompt (NumPro), which places unique numerical identifiers on each video frame, similar to manga panel numbers. Current Vid-LLMs process videos by considering them as sequence of frame images. With NumPro, VTG is as intuitive as flipping manga. As shown in Figure 1, NumPro augments each frame with unique numerical identifier denoting its position in the temporal sequence. Given language query targeting an event, Vid-LLMs retrieve relevant visual features of video frames and associate with the frame numbers overlaid. These numerical identifiers are then directly translated into textual outputs. In practice, we strategically position frame numbers in the bottom-right corner, using medium font size and distinct color. This design ensures numbers visibility without obstructing essential visual content. Overall, NumPro allows Vid-LLMs to read the video timeline, effectively converting visual recognition into temporal narrative in language. NumPros elegance lies in its simplicity: by subtly adding frame numbers as temporal markers into video frames, we enable Vid-LLMs to naturally correlate each frame to its temporal location in the video sequence. Unlike previous approaches [17, 21, 22, 31, 42, 44, 52], NumPro does not introduce additional tokens or modify model vocabulary to provide temporal cues, thus avoiding additional learning complexities and maintaining strong transferability across various tasks and datasets. Temporal grounding, therefore, becomes an accessible, free-lunch enhancement for Vid-LLMs already proficient in understanding video content. Additionally, fine-tuning on specially curated NumPro-enhanced VTG dataset (NumPro-FT) further advances state-of-the-art performance. Our contributions can be summarized as follows: We introduce NumPro, novel approach that enhances Video Temporal Grounding (VTG) capabilities of Video Large Language Models (Vid-LLMs) by overlaying frame numbers onto video frames, making temporal grounding as intuitive as following numbered panels in flipping manga. Through an experimental study, we find suitable NumPro design (font size, color, and position) that ensures high detectability by the model while minimally interfering with the original video content. We thoroughly evaluate NumPro on standard VTG benchmarks and metrics in both training-free and fine-tuned scenarios, demonstrating its effectiveness across various models and datasets. 2. Related Work Video Temporal Grounding with Vid-LLMs. Video Temporal Grounding (VTG) [40] focuses on the precise identification of event timestamps within videos, covering tasks such as moment retrieval [3, 4, 7, 13, 15, 16, 34, 50, 58, 59, 64], dense captioning [4, 8, 23, 25, 50, 60], and highlight detection [27, 44]. For current Video Large Language Models (Vid-LLMs) [28, 51, 67], which leverage powerful LLMs [1] for cross-modal understanding and videobased reasoning, VTG is crucial for achieving fine-grained temporal and visual comprehension, enabling end-to-end video dialogue systems with integrated temporal reasoning [21, 44, 52]. To achieve this, some methods rely on refined instruction datasets with temporal information (timestamps or frame numbers) for model fine-tuning [21, 31], while others concatenate additional textual temporal timestamps tokens with visual inputs [19, 47] or introduce specific temporal embeddings [17, 44]. Additional strategies model video structure [18, 22, 52] to better segment or organize videos into parts suitable for VTG. However, these approaches often require extensive retraining or specialized model adaptations, limiting their flexibility and transferability. In contrast, our NumPro aims to improve VTG for existing Vid-LLMs without additional training costs or architectural modifications. Visual Prompt in VLMs. Visual prompts [54], taking various forms such as circles [5, 61], bounding boxes [9, 12, 35] and semantic masks [37, 63], enhance vision-language models (VLMs) [36, 48, 53, 56, 62, 66] to focus on and reason about specific visual regions and reduce the occurrence of hallucination [5]. For CLIP [43], simple red circle [46] or colored region [63] can effectively guide model attention. Multi-modal large language models (MLLMs) [2, 6, 30] are also sensitive to specific visual prompts [5]. For example, ViP-LLaVA [5] and SoM [61] prompt MLLMs to answer about specific image regions with graphic shapes or numeric tags. CoLLaVO [26] and DOrA [55] utilize pixel-level prompts in images or videos to enhance the semantic localization capability of MLLMs. Additionally, 2 ing into textual description that describes when the event begins and ends. Specifically, we take video and language query as input, and extract the attention scores from the final multihead self-attention layer of Qwen2-VL-7B [51]. For each frame within the video sequence, we aggregate the attention scores from all the visual tokens corresponding to that frame across all attention heads. As illustrated in Figure 2, the attention map reveals strong correlation between the text query of an event and targeted video segments. It indicates that Qwen2-VL-7B can effectively focus on query-relevant frames, which is consistent with the models strong performance in other content-related video understanding tasks [14, 29]. However, the model struggles to verbalize the correct temporal boundaries, and generates surprising hallucinations such as from 200 to 599.. This observation underscores the need for mechanisms that can bridge the gap between spatial feature alignment and temporal reasoning with Vid-LLMs, which we aim to address with our NumPro method. 3.2. NumPro and NumPro-FT Our approach, Number-Prompt (NumPro), empowers VidLLMs to directly associate specific visual content with its temporal information, turning temporal localization into visual alignment task. As shown in Figure 3, NumPro operates in both training-free and fine-tuned scenarios. In the training-free setting, each video frame is marked with its corresponding frame number. By utilizing the builtin Optical Character Recognition (OCR) capabilities of VidLLMs, we enable them to read the timeline through the frame numbers associated with visual content. To clarify the purpose of the added numbers to Vid-LLMs, we prepend simple instruction to each event query: The red numbers on each frame represent the frame number. This approach allows Vid-LLMs to accurately identify frame-level boundaries by directly linking the frame numbers to language queries. For improved performance, NumPro-FT fine-tunes VidLLMs on NumPro-augmented dataset. This stage aligns frame numbers with temporal spans within the training data, embedding temporal grounding capabilities into the models learned representations. During fine-tuning, we freeze the visual encoder and only fine-tune the visual projector and LLM components. To reduce parameter count and training overhead, we apply Low-Rank Adaptation (LoRA) [20] to adjust the LLM. Our training objective is to maximize the likelihood of generating the correct answer tokens via auto-regressive language modeling: (A V, Tinstruct) = (cid:89) j=1 Pθ(Aj V, Xinstruct, A<j) (1) Figure 2. Attention Analysis between Video Frames and Event Query. Although the model accurately attends to regions of interest related to the query, it struggles to generate precise temporal boundaries in its response. toolchain [45, 49, 57, 68] approaches aggregate various visual prompts into multi-step reasoning paradigm to support reasoning complex tasks. While prior works focus on enhancing the region-based visual understanding of VLMs with visual prompts, our NumPro is the first to employ simple numerical tags as visual prompts within video frames to improve the temporal grounding capability. 3. Number-Prompt Approach Our Number-Prompt (NumPro) approach provides simple yet effective solution to enhance Video Temporal Grounding (VTG) capabilities of existing Video Large Language Models (Vid-LLMs) in both training-free and fine-tuned settings, as shown in Figure 3. Section 3.1 presents an attention analysis based on Qwen2-VL [51] to highlight the challenge of aligning visual features with textual temporal boundaries. Section 3.2 describes the construction of NumPro and the fine-tuning process of Vid-LLMs on NumPro-augmented VTG dataset, referred to as NumProFT. Finally, Section 3.3 details the design optimization of NumPro for maximizing its effectiveness. 3.1. Attention Analysis Current Vid-LLMs process videos as sequence of frames. Visual representations of the video can be taken as the concatenated representations from each individual frame, aggregating the information from discrete frames into comprehensive video level. This allows Vid-LLMs to understand videos by aligning visual representations of frame images with the textual representations of language queries. To explore the challenge in video temporal grounding (VTG), we analyze the attention map between representations of the frame image tokens and the query language tokens, and then we assess the temporal description of relevant video frames. Using Qwen2-VL-7B [51] as case study, we highlight the challenge of VTG for Vid-LLMs: while Vid-LLMs can understand what event is happening within video, they struggle to translate this understand3 Figure 3. Framework of Our Approach in Two Settings: (1) Training-free VTG with NumPro, where frame numbers are directly added to video frames, enabling Vid-LLMs to locate events temporally without additional training, and (2) Fine-tuned VTG with NumPro-FT, which further improves VTG performance by fine-tuning Vid-LLMs on dataset NumPro-enhanced with no architectural modifications. Here, represents the input video, θ denotes the trainable parameters, Tinstruct is the text instruction, is the length of the answer sequence A, and A<j includes all preceding answer tokens before the current token Aj. 3.3. Design of Numerical Prompt An effective NumPro design must ensure: (1) numbers are easily recognized by the model, and (2) minimal interference with visual content. Previous research [5] indicates that the appearance and placement of visual prompts can influence model attention. Given that all Vid-LLMs operate at fixed resolution of 336 336, we optimize NumPro by assessing three factors: font size, color, and placement position of the frame number. To determine an effective NumPro design, we use two primary metrics: Number Accuracy, assessing how well the model identifies overlaid numbers, and Caption Accuracy, measuring how accurately the original caption aligns with frame content after adding numbers. Balancing these two metrics allows us to select NumPro configurations where the numbers are clearly recognizable without disrupting the main video content. To make the design choices robust across various models and datasets, we employ CLIP-based experiments on subset of MSCOCO dataset [32] to calculate Number Accuracy and Caption Accuracy separately. We use the CLIP ViT-B/32 [10, 43] model to generate visual and textual representations, as many Vid-LLMs utilize CLIP-style vision encoders [11, 51], allowing our findings to generalize well across Vid-LLMs. COCO image-caption pairs serve as proxies for video frames, avoiding the high costs and limited scalability of direct VTG testing. Specifically, we randomly select 1,000 distinct image-caption pairs from Figure 4. Illustration of Our NumPro Design Algorithm.We overlay different numbers onto COCO images and obtain visual and textual representations using CLIP encoders. For each configuration, we calculate Number/Caption Similarity and derive Number/Caption Accuracy, enabling us to identify the optimal NumPro design that balances recognizability and minimal disruption to the visual content. MSCOCO [32] and overlay numbers ranging from 0 to 99 onto the image in various configurations. As shown in Figure 4, we first obtain representations from CLIP [43] vision and text encoders and compute intermediate similarity scores (i.e., Number and Caption Similarity) between them. Using the added numbers and original captions as ground truth, we select the text numbers and captions with the highest similarity scores as predictions to calculate Number and Caption Accuracy. Configurations balancing these accuracies are optimal for NumPro design. As shown in Figure 5, our findings indicate that increasing the font size improves number accuracy but reduces caption accuracy, suggesting that moderate font size (40 or 60) is optimal. For color selection, caption accuracy re4 4. Experiments We evaluate our model on two Video Temporal Grounding (VTG) tasks: Moment Retrieval [4, 16] and Highlight Detection [27]. Moment Retrieval, given language query describing an event, identifies the specific start and end video frames of the event. We utilize Charades-STA [16] and ActivityNet [4] as evaluation datasets, following previous works [21, 42, 44, 52]. Evaluation metrics include the mean Intersection over Union (mIoU) and recall@1 at various IoU thresholds (R@m), where is set to {0.3, 0.5, 0.7} following previous work [21, 44]. For Highlight Detection, which aims to locate and rank video frames based on their relevance to the language query, we use QVHighlights [27] for evaluation. Evaluation metrics include mean Average Precision (mAP) and HIT@1 (the hit ratio of the highest-scored clip), as in [17, 27, 44]. 4.1. Implementation Details for NumPro-FT Dataset Preparation. Our temporal grounding dataset consists of 70k question-answer pairs from DiDeMo [39] and ActivityNet Caption [4] datasets. Additionally, we incorporate data from Stage 2 and Stage 3 of VTimeLLM dataset [21]. After filtering out invalid videos, we obtain comprehensive instruction dataset totaling 220k samples. Each video in our dataset is augmented with our NumPro method by overlaying frame numbers directly onto the video frames. The question-answer pairs follow consistent template: questions are formatted as During which frames can we see {query}? and answers are formatted as From to y, where and denote the start and end frame numbers of the query event. Training Details. We utilize the LongVA-7B-DPO [65] as our base model, taking into account its uncomplicated design and its extensive capacity to handle context length. Additionally, it has not been trained on any video data. The model is trained for 3 epochs over our curated dataset with total batch size of 128. We use the AdamW optimizer [24] with cosine learning rate decay. The learning rate is set to 1e-4, and the warm-up ratio is 0.05. The LLM component utilizes LoRA with parameters = 64 and α = 128. All experiments are conducted on 8 H100 GPUs. 4.2. Main Results 4.2.1. Comparison with State-of-the-Art Methods Table 1 presents comparative analysis of Vid-LLMs enhanced with our NumPro/NumPro-FT against existing state-of-the-art (SOTA) methods on Moment Retrieval and Highlight Detection tasks. Moment Retrieval: Applying training-free NumPro enables Vid-LLMs to approach or exceed previous SOTA performance, benefiting both closed-source and open-source Vid-LLMs. GPT-4o [41] already exhibits strong moment Figure 5. The Impact of Different Number-Prompt Designs. We categorize the design into three dimensions: font size, position, and color. BL stands for Bottom Left, BR for Bottom Right, TL for Top Left, TR for Top Right, and for Center. mains relatively stable across different colors. Red shows the best performance for number accuracy, while black was the least effective. This finding is also consistent with previous works [5, 46]. Additionally, positioning the text in the center of the image significantly reduced caption accuracy due to overlaps with key visual elements, while placing the numbers in the bottom-right corner provides the best balance between caption and number accuracy. Finally, we select font size of 40, the color red, and the bottomright position for our final NumPro design. This design search allows NumPro to better harness the inherent OCR and visual-language alignment capabilities of Vid-LLMs to empower video temporal grounding. In practice, CLIP-based designs provide approximate rather than definitive guidance, further testing on Vid-LLMs with VTG dataset may yield additional model-specific insights. In Sec 4.3, consistent results further validate the effectiveness of our design. Table 1. Comparison of performance on the video temporal grounding task with previous state-of-the-art methods. NumPro refers to the use of number prompts for augmentation during inference, while NumPro-FT indicates fine-tuning with the number prompt augmentation instruction dataset. The best results are highlighted in bold, and the second-best are underlined."
        },
        {
            "title": "Model",
            "content": "Charades-STA"
        },
        {
            "title": "QVHighlights",
            "content": "R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU mAP HIT@1 GroundingGPT [31] LITA [22] VTG-LLM [17] TimeChat [44] VTimeLLM [21] Momentor [42] HawkEye [52] GPT-4o [41] +NumPro Qwen2-VL-7B [51] +NumPro LongVA-7B-DPO [65] +NumPro +NumPro-FT - - 52.0 47.7 51.0 42.9 50.6 55.0 57.1 8.7 60.7 22.6 27.2 63.8 29.6 - 33.8 22.9 27.5 23.0 31.4 32.0 35. 5.4 36.8 10.1 10.3 42.0 VTG-Tuned Vid-LLMs 11.9 - 15.7 12.5 11.4 12.4 14.5 11.5 13.5 2.4 15. 2.2 2.9 20.6 - - - 30.6 31.2 29.3 33.7 - - - 30.2 44.0 42.6 49.1 General Vid-LLMs 35.4 37.6 7.9 38. 14.6 18.9 41.4 33.3 45.5 17.0 44.2 11.8 20.1 55.6 - 25.9 - 16.9 27.8 26.6 29.3 21.2 30. 9.4 26.4 5.3 10.8 37.5 - - - 8.2 14.3 11.6 10.7 10.4 18.4 3.9 14.4 1.9 5.4 20. - 28.6 - 21.8 30.4 28.5 32.7 23.7 33.6 12.5 31.3 8.2 15.2 38.8 - - 16.5 14.5 - 7.6 - 39.5 40. 21.5 23.6 14.2 15.3 25.0 - - 33.5 23.9 - - - 68.7 70.7 42.2 43.4 20.4 24.3 37. retrieval capabilities, and our NumPro further enhances the performance. In particular, NumPro achieves 9.9% increase in mIoU on ActivityNet, surpassing the previous SOTA by 0.9%. Qwen2-VL-7B performs poorly initially and also sees significant improvement with NumPro, averaging 24.7% increase in mIoU across datasets. Moreover, starting from relatively low baseline on LongVA-7B-DPO [65], our fine-tuning approach, NumProFT, establishes new SOTA across all metrics. On CharadesSTA, it surpasses previous SOTA by 11.8%, 8.2%, 4.9%, 7.7% (R@0.3, R@0.5, R@0.7, mIoU), and on ActivityNet, it surpasses previous SOTA by 6.5%, 8.2%, 6.3%, 6.1% (R@0.3, R@0.5, R@0.7, mIoU). These results demonstrate that NumPro and NumPro-FT can utilize the superior video understanding abilities of existing Vid-LLMs and significantly enhance their moment retrieval capabilities. Highlight Detection: In this task, models like GPT4o [41] and Qwen2-VL have already achieved state-of-theart (SOTA) performance. However, our NumPro approach consistently enhances their performance, with an average increase of 1.55% in mean Average Precision (mAP) and 1.6% in the hit ratio of the highest-scored clip (HIT@1). Additionally, applying NumPro-FT enables LongVA-7BDPO to surpass existing SOTA by large margin (+8.5% in mAP and +3.7% in HIT@1). These findings suggest that NumPro and NumPro-FT, which can be easily appended to current Vid-LLMs, hold substantial potential for further advancing temporal reasoning capabilities. 4.2.2. Effectiveness of NumPro across Vid-LLMs Beyond surpassing SOTA, Table 2 demonstrates the broad applicability and scalability of NumPro across various VidLLMs in Video Temporal Grounding. We apply NumPro to additional Vid-LLMs, including LLaVA-Video-7B [67], LLaVA-OneVision-7B [28], and Qwen2-VL-72B [51], and observe notable performance improvements, with average mIoU gains reaching up to 18.1% on Charades and 14.0% on ActivityNet. Moreover, we conduct fine-tuning experiments with and without NumPro-augmented data (indicated as +FT in Table 2). Results show that NumPro-FT consistently outperforms conventional fine-tuning, particularly on longer video datasets like ActivityNet, where it achieves substantial 9.8% gain in mIoU. Those observations underscore the effectiveness of NumPro across models and highlight its superior impact when combined with fine-tuning. 4.2.3. Qualitative Results In Figure 6, we compare our method with SOTA methods, TimeChat [44] and VTimeLLM [21], through two visualization cases from our ActivityNet dataset. The first example features minimal scene changes between video frames. TimeChat predicts an early start, while VTimeLLM fails to capture the full event duration. In contrast, our method 6 Table 2. Performance of Applying NumPro to Various Vid-LLMs and Ablation Results on NumPro-FT. Model LLaVA-OneVision-7B [28] +NumPro LLaVA-Video-7B [67] +NumPro Qwen2-VL-72B [51] +NumPro LongVA-7B-DPO [65] +FT +NumPro-FT Charades-STA ActivityNet R@0.3 R@0.5 R@0. mIoU R@0.3 R@0.5 R@0.7 mIoU 22.3 42.9 (+20.6) 11.8 56.7 (+44.8) 0.0 25.8 (+25.8) 22.6 62.0 63.8 (+41.2) 7.9 19.4 (+11.5) 2.7 25.6 (+22.9) 0.0 9.9 (+9.9) 2.1 6.6 (+4.5) 0.1 8.6 (+8.5) 0.0 3.0 (+3.0) 10.1 41.6 42.0 (+31.9) 2.2 19.9 20.6 (+18.4) 15.9 28.1 (+12.2) 9.8 34.6 (+24.8) 0.2 17.4 (+17.2) 14.6 40.2 41.4 (+26.8) 7.1 14.4 (+7.3) 7.4 25.2 (+17.8) 1.0 35.5 (+34.5) 11.8 41.8 55.6 (+43.8) 3.1 7.9 (+4.8) 3.1 15.2 (+12.1) 0.6 21.4 (+20.8) 5.3 25.7 37.5 (+32.2) 1.1 3.8 (+2.7) 1.2 8.4 (+7.2) 0.3 11.0 (+10.7) 1.9 13.7 20.6 (+18.7) 6.1 11.3 (+5.2) 6.2 18.6 (+12.4) 1.0 25.5 (+24.5) 8.2 29.0 38.8 (+30.6) Table 3. Ablation study on various NumPro designs. We divide the designs into three dimensions: font size, color, and position. Size Color Position Charades-STA R@0.3 R@0.5 R@0.7 mIoU 40 40 40 40 20 40 60 80 40 40 40 40 Red Red Red Red Red Red Red Red Red Top Left Top Right Center Bottom Left Bottom Right Bottom Right Bottom Right Bottom Right Bottom Right Red Bottom Right Bottom Right Blue Black Bottom Right Green Bottom Right 56.7 58.2 53.7 61.6 60.7 53.6 60.7 58.0 58.0 60.7 57.8 56.6 56.0 32.9 34.0 29.5 37.8 36.8 34.0 36.8 34.5 33. 36.8 34.2 36.0 33.8 13.8 13.0 10.4 15.9 15.9 14.0 15.9 14.1 13.7 15.9 14.6 15.9 14.5 35.8 36.8 34.1 39.3 38.5 34.6 38.5 37.1 36. 38.5 36.6 36.6 36.0 precisely captures the correct event boundaries. The second case involves shorter event duration and frequent scene changes. TimeChat completely misses the event, and VTimeLLM overestimates the event duration by including irrelevant segments. Our approach, again, precisely delineates the event boundaries. These qualitative examples underscore the robustness and precision of our method in scenarios that are especially challenging for other SOTA methods. 4.3. Validation of NumPro Design Following our heuristic design process in Sec 3.3, we validate its effectiveness in temporal grounding tasks to confirm that these design choices generalize beyond the COCO dataset. We conduct moment retrieval experiments on Charades-STA [16] with Qwen2-VL-7B [51] in trainingfree setting. As shown in Table 3, the results align closely with our initial observations from COCO dataset, confirming the effectiveness of our design choices in VTG tasks. Specifically, (1) Position: Consistent with our CLIP-based findings, placing the text in the center has the largest impact on performance due to overlaps, while our choice of the bottom-right performs comparably to the best position; (2) Font Size: Both very large and very small fonts yield suboptimal results, supporting our balanced selection; (3) The performance on VTG is sensitive to number color, yet the red color consistently delivers the best performance, which may attribute to its high contrast against typical backgrounds [46]. Overall, the alignment between the CLIPbased design choices and the VTG results shows the validity and robustness of our NumPro design. 4.4. Investigation on the Sampling of NumPro Typically, we augment every frame in video with NumPro. In this section, we evaluate the impact of varying the sampling ratio and sampling method (randomly or uniformly) when selecting subset of frames from the video to augment NumPro. As depicted in Figure 7, performance increases with more labeled frames, with uniform sampling generally maintaining higher accuracy. Notably, labeling just 20% of the frames provides substantial performance boost and uniform sampling of 80% of the frames surpasses previous state-of-the-art, underscoring the robustness of our NumPro approach. 4.5. Influence on General Video-QA To explore the broader applicability of NumPro, we integrate it into general video-QA tasks, using VideoInstruct [38] as our benchmark. As detailed in Table 4, the incorporation of NumPro minimally affects general comprehension metrics, with slight decrease in Distraction Overlap (DO, -0.02) and an enhancement in Temporal Understanding (TU, +0.1). This indicates that Vid-LLMs equipped with NumPro maintain robust performance in general video-QA while excelling in precise video temporal grounding (VTG) tasks. This dual capability allows us 7 Figure 6. Qualitative Comparison with State-of-the-Art. Our LongVA-7B-DPO model, fine-tuned with NumPro-FT, outperforms TimeChat [44] and VTimeLLM [21] on ActivityNet by accurately identifying event boundaries in challenging scenes. Table 4. The influence of applying NumPro to general videoQA. CI stands for correctness of information, DO stands for detail orientation, CU stands for contextual understanding, TU stands for temporal understanding, and CO stands for consistency."
        },
        {
            "title": "Model",
            "content": "CI DO CU TU CO Qwen2-VL +NumPro 3.10 3.10 2.57 2.55 3.46 3.46 2.47 2.57 3.30 3. Figure 7. Performance Comparison of Sampling Strategies for NumPro. We compare the effects of NumPro with different sampling ratios and sampling methods (random vs. uniform), as tested on the Charades-STA [16] using the Qwen2-VL-7B [51] model. to harness powerful Vid-LLM for end-to-end video understanding that can flexibly adapt to both general and temporally nuanced questions within conversational AI systems. 5. Conclusion In this paper, we propose Number-Prompt (NumPro), simple yet efficient visual prompt designed to enhance the video temporal grounding (VTG) capabilities of Video Large Language Models (Vid-LLMs) with no effort. By overlaying frame numbers onto video content, NumPro leverages the inherent Optical Character Recognition (OCR) and visual-language alignment capabilities of Vid-LLMs, allowing them to accurately map events to specific temporal boundaries. Through systematic design informed by COCO-based heuristics and validated across VTG benchmarks, we demonstrated that NumPro effectively supports fine-grained temporal understanding while preserving general video comprehension. Through extensive evaluations, we demonstrated that NumPro consistently achieves state-of-the-art performance in both training-free and fine-tuned settings, enabling adaptable integration into both closed-source and open-source Vid-LLMs. NumProFT further refines temporal grounding performance, establishing new SOTA across VTG tasks. Besides, the minimal impact on general video-QA shows that NumPro can augment VTG while maintaining robust video understanding."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 2 [2] Yang Bai, Min Cao, Daming Gao, Ziqiang Cao, Chen Chen, Zhenfeng Fan, Liqiang Nie, and Min Zhang. Rasa: Relation and sensitivity aware representation learning for text-based person search. arXiv preprint arXiv:2305.13653, 2023. 2 [3] Meinardus Boris, Batra Anil, Rohrbach Anna, and Rohrbach Marcus. The surprising effectiveness of multimodal large language models for video moment retrieval. arXiv preprint arXiv:2406.18113, 2024. 2 [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 1, 2, 5 [5] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Vipllava: Making large multimodal models understand arbitrary visual prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12914 12923, 2024. 2, 4, 5 [6] Min Cao, Chen Chen, Hao Dou, Xiyuan Hu, Silong Peng, Progressive bilateral-context driven IEEE and Arjan Kuijper. model for post-processing person re-identification. Transactions on Multimedia, 23:12391251, 2020. [7] Min Cao, Shiping Li, Juntao Li, Liqiang Nie, and Min Zhang. Image-text retrieval: survey on recent research and development. arXiv preprint arXiv:2203.14713, 2022. 2 [8] Min Cao, Yang Bai, Ziyin Zeng, Mang Ye, and Min Zhang. An empirical study of clip for text-based person search. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 465473, 2024. 2 [9] Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, et al. Instructdet: Diversifying referring object detection with generalized instructions. arXiv preprint arXiv:2310.05136, 2023. 2 [10] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. arXiv preprint arXiv:2407.11691, 2024. 4 [12] Zhizhao Duan, Hao Cheng, Duo Xu, Xi Wu, Xiangxie Zhang, Xi Ye, and Zhen Xie. Cityllava: Efficient fine-tuning for vlms in city scenario. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71807189, 2024. 2 [13] Lin Geng Foo, Jia Gong, Zhipeng Fan, and Jun Liu. Systemstatus-aware adaptive network for online streaming video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 10514 10523, 2023. 2 [14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 3 [15] Junyu Gao and Changsheng Xu. Fast video moment retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15231532, 2021. 2 [16] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 1, 2, 5, 7, 8 [17] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, and Bo Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. arXiv preprint arXiv:2405.13382, 2024. 2, 5, 6 [18] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Qingbin Liu, and Xi Chen. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. 2 [19] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. [20] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Lowrank adaptation of large language models. In International Conference on Learning Representations, 2021. 3 [21] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1427114280, 2024. 1, 2, 5, 6, 8 [22] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. arXiv preprint arXiv:2403.19046, 2024. 2, 6 [23] Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. Do you remember? dense video captioning with cross-modal memory retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389413904, 2024. 2 [24] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [25] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 2 [26] Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro. Collavo: Crayon large language and vision model. arXiv preprint arXiv:2402.11248, 2024. 2 [27] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34: 1184611858, 2021. 2, 5 9 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 6, 7 [29] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. [30] Shiping Li, Min Cao, and Min Zhang. Learning semanticaligned feature representation for text-based person search. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 27242728. IEEE, 2022. 2 [31] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Vu Tu, et al. Groundinggpt: Language enhanced multi-modal grounding model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66576678, 2024. 2, 6 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 4 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1 [34] Yang Liu, Dingkang Yang, Yan Wang, Jing Liu, Jun Liu, Azzedine Boukerche, Peng Sun, and Liang Song. Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models. ACM Computing Surveys, 56(7):138, 2024. 2 [35] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. In European Conference on Computer Vision, pages 417435. Springer, 2025. 2 [36] Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike Zheng Shou, and Xiaoyan Sun. Visual perceparXiv preprint tion by large language models weights. arXiv:2405.20339, 2024. [37] Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai Huang, Qinghua Hu, and Bingzhe Wu. Invariant test-time adaptation for vision-language model generalization. arXiv preprint arXiv:2403.00376, 2024. 2 [38] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 7 [39] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit RoyChowdhury. Weakly supervised video moment retrieval from In Proceedings of the IEEE/CVF Conference text queries. on Computer Vision and Pattern Recognition, pages 11592 11601, 2019. 5 [40] Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng, Interventional video groundHao Zhang, and Wei Lu. In Proceedings of ing with dual contrastive learning. the IEEE/CVF conference on computer vision and pattern recognition, pages 27652775, 2021. 1, 2 [41] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. Accessed: 2024-10-21. 5, [42] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, TatSeng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. arXiv preprint arXiv:2402.11435, 2024. 2, 5, 6 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 4 [44] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 1, 2, 5, 6, 8 [45] Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, and Nenghai Yu. Towards more unified in-context visual unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13362 13372, 2024. 3 [46] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? viIn Proceedings of the sual prompt engineering for vlms. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1198711997, 2023. 2, 5, 7 [47] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [48] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models, 2024. 2 [49] Georgios Tziafas and Hamidreza Kasaei. Towards openworld grasping with large vision-language models. arXiv preprint arXiv:2406.18722, 2024. [50] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temarXiv poral grounding in video large language models. preprint arXiv:2410.03290, 2024. 2 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 3, 4, 6, 7, 8 [52] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video10 adapter: graph-based post-processing approach for scene text image super-resolution. In Proceedings of the 31st ACM International Conference on Multimedia, pages 21682179, 2023. 2 [67] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 1, 2, 6, [68] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-ofarXiv preprint imagination for simulated-world control. arXiv:2403.12037, 2024. 3 text llms for grounding text in videos. arXiv:2403.10228, 2024. 2, 5, 6 arXiv preprint [53] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision, 2024. 2 [54] Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan Rossi, Ruiyi Zhang, et al. Visual prompting in multimodal large language models: survey. arXiv preprint arXiv:2409.15310, 2024. 2 [55] Tung-Yu Wu, Sheng-Yu Huang, and Yu-Chiang Frank Wang. Dora: 3d visual grounding with order-aware referring. arXiv preprint arXiv:2403.16539, 2024. 2 [56] Yongliang Wu and Xu Yang. glance at in-context learning. Frontiers of Computer Science, 18(5):185347, 2024. 2 [57] Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Jian Wu, and Philip Torr. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. arXiv preprint arXiv:2403.12488, 2024. [58] Li Xu, He Huang, and Jun Liu. Sutd-trafficqa: question answering benchmark and an efficient network for video reasoning over traffic events. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 98789888, 2021. 2 [59] Li Xu, Haoxuan Qu, Jason Kuen, Jiuxiang Gu, and Jun Liu. Meta spatio-temporal debiasing for video scene graph generation. In European Conference on Computer Vision, pages 374390. Springer, 2022. 2 [60] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1071410726, 2023. 2 [61] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. 2 [62] Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. Advances in Neural Information Processing Systems, 36, 2024. 2 [63] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, TatSeng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. AI Open, 5:3038, 2024. [64] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. 2 [65] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 5, 6, 7 [66] Wenyu Zhang, Xin Deng, Baojun Jia, Xingtong Yu, Yifan Chen, Jin Ma, Qing Ding, and Xinming Zhang. Pixel"
        }
    ],
    "affiliations": [
        "WeChat, Tencent Inc.",
        "Max Planck Institute for Informatics",
        "University of California, Berkeley",
        "Southeast University"
    ]
}