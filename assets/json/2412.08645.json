{
    "paper_title": "ObjectMate: A Recurrence Prior for Object Insertion and Subject-Driven Generation",
    "authors": [
        "Daniel Winter",
        "Asaf Shul",
        "Matan Cohen",
        "Dana Berman",
        "Yael Pritch",
        "Alex Rav-Acha",
        "Yedid Hoshen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces a tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into a scene specified by either an image or text. Existing methods struggle to fully meet the task's challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the object's identity. We hypothesize that achieving these goals requires large scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train a straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composited image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using a single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 5 4 6 8 0 . 2 1 4 2 : r ObjectMate: Recurrence Prior for Object Insertion and Subject-Driven Generation Daniel Winter1, Asaf Shul1,2 Matan Cohen1 Dana Berman1 Yael Pritch1 Alex Rav-Acha1 Yedid Hoshen1, 1Google 2The Hebrew University of Jerusalem https://object-mate.com Figure 1. Our method composes objects into scenes with photorealistic pose and lighting, while preserving their identity. The scene can be specified via an image or text. We do not use test-time tuning."
        },
        {
            "title": "Abstract",
            "content": "This paper introduces tuning-free method for both object insertion and subject-driven generation. The task involves composing an object, given multiple views, into scene specified by either an image or text. Existing methods struggle to fully meet the tasks challenging objectives: (i) seamlessly composing the object into the scene with photorealistic pose and lighting, and (ii) preserving the objects identity. We hypothesize that achieving these goals requires large scale supervision, but manually collecting sufficient data is simply too expensive. The key observation in this paper is that many mass-produced objects recur across multiple images of large unlabeled datasets, in different scenes, poses, and lighting conditions. We use this observation to create massive supervision by retrieving sets of diverse views of the same object. This powerful paired dataset enables us to train straightforward text-to-image diffusion architecture to map the object and scene descriptions to the composited image. We compare our method, ObjectMate, with state-of-the-art methods for object insertion and subject-driven generation, using single or multiple references. Empirically, ObjectMate achieves superior identity preservation and more photorealistic composition. Differently from many other multi-reference methods, ObjectMate does not require slow test-time tuning. {daniel.winter, yedid.hoshen}@mail.huji.ac.il 1 1. Introduction This paper proposes new method for composing objects into scenes. This merges two popular sub-tasks: object insertion and subject-driven generation (from now, subject generation). In object composition, the user provides one or more reference views of an object and description of the target scene. For object insertion, the scene description includes background image and target location within this image, while for subject generation, the scene description is text prompt. The objective is to photorealistically compose the reference object into the scene while preserving its identity. Current generative models often struggle to preserve the fine details of the object and scene, and they frequently fail to harmonize the object with the scenes geometry and lighting. Due to the tasks complexity and industrial significance, it has attracted research interest for several decades. Supervised learning is natural solution, but there are no large-scale paired datasets available for training. Therefore, current solutions tackle this in 2 ways: (i) fine-tuning on the provided object views and scene descriptions at inference time only, and (ii) using video or image augmentations to create synthetic datasets for supervised learning. However, both approaches have limitations. Test-time tuning suffers from slow inference times and hyperparameter sensitivity, while synthetic data often lacks diversity in object poses and lighting conditions between the inputs and outputs of training examples, compared to real-world testing data. In this paper, we introduce the object recurrence prior and use it to create massive supervised dataset for object composition. Reminiscent of classical priors on the recurrence of patches [4, 17] and landmarks [1], we postulate that many everyday objects recur in large internet-based datasets across various scenes, poses, and lighting conditions. We use 2 tools unavailable in the past to find these recurrences: (i) deep global features that represent object instance identity rather than semantics, and (ii) very large dataset. Based on the object recurrence prior, we introduce ObjectMate, new method for object composition. It first detects objects within large image datasets and extracts deep identity features for each one. For each object, ObjectMate retrieves other objects with high feature similarity. The result is large dataset containing diverse objects, each with multiple views, scenes, lighting conditions, and poses. While extracting text description of the scene merely requires image captioning, extracting the background image for object insertion is more challenging. Other methods suggest masking the object region or inpainting it, but this leaves shadows and reflections intact and loses background information. Instead, ObjectMate uses counterfactual object removal [63] model, which also removes the objects shadows and reflections, overstepping these limitations. ObjectMate uses this dataset to train diffusion Figure 2. Retrieval feature comparison. Retrieval with DINO features (right) produces semantic matches, while instance retrieval features [51] (middle) find identical objects. model that maps scene descriptions and object views to the composite images. Excitingly, the high quality of our dataset creation procedure enables even straightforward architecture to achieve state-of-the-art results (see Fig. 1). ObjectMate achieves state-of-the-art results in both object insertion and subject generation. Unlike other fast, zero-shot methods, it can benefit from multiple reference views. To ensure sound evaluation, we improve current protocols and datasets as follows: (i) We introduce new evaluation dataset for object insertion, carefully crafted to include ground-truth examples. (ii) Our analysis reveals that current protocols do not accurately measure object identity preservation; thus, we suggest new metric that faithfully captures this aspect and validate it through user study. Our key contributions are: 1. Studying the object recurrence prior: many everyday object instances recur exactly in large internet-based datasets with diverse poses and scene conditions, providing valuable resource for multi-view learning. 2. Proposing new method, ObjectMate, that creates supervised dataset for object composition using the prior and trains state-of-the-art models on this dataset. 3. Improving evaluation protocols by: (i) capturing new object composition evaluation dataset containing ground-truth, and (ii) introducing metric for identity preservation that better aligns with human perception. 2. Related Works Subject-driven generation. There are two main approaches: test-time tuning (tuning) and zero-shot (ZS) methods. Tuning approaches fine-tune diffusion model 2 on several reference views of an object [6, 13, 15, 16, 20, 27, 28, 47, 48, 60, 61]. These approaches vary in the parameters they tune, such as text embeddings (Textual Inversion [15]), full denoiser weights (DreamBooth [47]), and cross-attention layers (Custom Diffusion [27]). This approach is typically slow. In contrast, zero-shot methods use fixed subject encoder instead of test-time tuning [26, 31, 39, 40, 54, 62, 64, 67]. These methods are faster at test time but often struggle to preserve subject identity. Object insertion. Early object insertion methods used generative adversarial networks [18, 23, 25, 35, 69], but more recent approaches use diffusion [41, 44, 46, 55, 56]. Most insertion methods are zero-shot and use fixed encoder for the reference object. Paint-by-Example [65], ControlCom [68], and ObjectStitch [57] use CLIP [43] encoder. AnyDoor [10] uses DINO embeddings [42] along with high-frequency maps to improve results. line of works extract supervision from videos [10, 11] or combine video and image data, such as IMPRINT [58]. Finally, some insertion methods use test-time-tuning [32, 38, 49]. Instance retrieval for generative models. Several works [2, 7, 8, 24, 30, 53] leverage nearest-neighbor retrieval to improve generation fidelity. For instance, SuTI [8] creates supervised dataset by clustering an internet dataset based on CLIP similarity. However, since these methods rely on semantic features such as BM25 [45] and CLIP [43], they tend to generate objects that are similar but not identical to the reference. Classical recurrence priors. Repeating patches across images, or even within an image,have been cornerstone of image processing for decades. Examples include nonlocal means [4] and example-based super-resolution [17]. Additionally, significant work has been done on landmark retrieval for 3D reconstruction [1]. We extend these works by showing that that many everyday objects recur across image collections. 3. Background 3.1. Task definition Object composition takes two main inputs: (i) set of reference views of the target object = {o1, o2, . . . , on}, and (ii) scene description S. For object insertion, consists of scene background image and target position p, i.e., = (b, p). For subject generation, is simply text prompt t. The objective is to learn model that outputs an image of the object composited into the scene: = g(S, O) (a) object identity Models should satisfy 2 objectives: preservation and (b) photorealistic composition, harmonizing the objects geometry and lighting with the scene. 3.2. Data for supervised learning. Learning end-to-end requires supervised pairs of object views O, scene description S, and composite image y. As no such datasets exist, creating this data is critical step. The three main approaches to data collection are manual collection, single-image augmentation, and video-based methods. The manual approach [63] simply captures counterfactual pairs (S, O, y) using tripod-mounted camera. While this method produces the highest-quality data, it is not scalable. Single-image augmentation [65] involves extracting an object from composite image and applying augmentations to simulate multiple views O. However, such augmentations typically fail to capture the full diversity of real-world data. Video-based approaches [10, 11] track an object across video to obtain multiple views O. These methods suffer from limited pose, lighting, and scene diversity (especially for inanimate objects), as well as low resolution and motion blur. In this work, we extract large-scale multi-view data from unsupervised image datasets, addressing the limitations of: 1) high manual collection costs, 2) the distributional mismatch between augmented and real data, and 3) the limited diversity of video data. 4. The object recurrence prior Classical work in computer vision observed that patches and landmarks recur across image collections. They used this prior to solve inverse problems [4, 17] in image processing and structure-from-motion [37, 50]. In this paper, we use modern tools to demonstrate that many everyday objects recur in large-scale unlabeled datasets across multiple images with diverse lighting conditions, poses, and scenes. We term this the Object Recurrence Prior. kNN Retrieval. To establish this prior, we count recurring objects across datasets. We first extract objects from the datasets COCO [34], Open Images [29], and subset of WebLI [9] with 55M objects. To encode each object, we extracted deep features using ViT encoder [14] specifically designed for instance retrieval (IR) [5, 51, 66]. The choice of features is critical, as semantic encoders like CLIP [43] or DINO [42] do not retrieve the same object, but only semantically similar ones, which are unsuitable for our analysis. We test 2 encoders: public model [51] and similar internal model trained on collection of IR datasets. Finally, we retrieved the top k-nearest neighbor objects for each object using the cosine similarity of the deep features. Fig. 2 presents several retrieval results, with diverse poses, illumination conditions and backgrounds. Retrieval filtering. We classify two objects as recurring if their feature distance is below threshold. To determine this threshold, we randomly selected 1,000 retrieved pairs 3 (a) (b) (c) Figure 3. Object recurrence analysis: (a) Retrieval precision vs. similarity threshold. threshold of 0.93 yields 70% precision. (b) Similarity score distribution for 3 datasets between an object and its 3 nearest neighbors. The legend shows the percentage of objects within the range of [0.93, 0.975]. (c) The percentage of objects in this range grows super-linearly as we use larger subsets of WebLI. 5. ObjectMate: Leveraging object recurrence 5.1. Dataset creation with the recurrence prior Our method, ObjectMate, first converts an unsupervised dataset into supervised object composition dataset using the recurrence prior (Sec. 4). Retrieving multiple object views. We first runs object detection over the entire dataset, retaining only objects with high detection confidence. We use subset of WebLI [9] consisting of 55M detected objects. An encoder extracts features from each object. The choice of encoder is critical (see Sec. 4). To accurately retrieve object matches, we use encoders trained specifically for instance retrieval (IR) rather than semantic retrieval. We then construct sparse kNN graph, providing for each object its most similar objects. To refine this graph, we threshold neighbors that are either too similar (likely near-duplicates) or too dissimilar (likely different objects), as detailed in Sec. 4. We denote by Oi, the set of retrieved objects for target object at location pi of image yi. Typically, each neighboring object in the set Oi is different instance of the same object captured under different pose, lighting, and background. We represent each object view by cropping the image according to the object bounding box. This procedure results in final object composition dataset of 4.5M objects, each with at least 3 retrieved distinct views. Fig. 5 shows an overview of our data pipeline. Scene description for object insertion. We extract the background image using the object removal model ObjectDrop [63]. This model removes the object, as well as its shadows and reflections. Previous methods simply replaced the object bounding box with gray pixels [10, 65], or used inpainting [21, 59]. However, these approaches often lose valuable background information or leave shadows and reflections intact, resulting in lower fidelity outputs. Scene description for subject generation. We extract text description using an image-to-text model. Figure 4. Recurring mass-produced objects. Percentage of instances within classes of everyday objects with at least 3 retrieved recurrences in WebLI. and manually labeled them as as exact matches (true) or not (false). Note that even false retrievals had very similar objects. Fig. 3a shows the retrieval precision versus similarity threshold. We selected threshold of 0.93, corresponding to precision of 70%, which we found sufficient for downstream tasks. Similarity values above 0.975 often indicated near-duplicates. Thus, we retain object pairs with similarity values between 0.93 and 0.975. Evaluating dataset recurrence. We show the distribution of retrieval scores across different datasets in Fig. 3b. We can see that all have significant recurrence fraction. Fig. 3c shows the recurrence rate for random subsets of WebLI of different sizes. Revealing that as the dataset size increases, the fraction of recurring objects also grows. Interestingly, COCO has higher recurrence fraction than Open Images, likely due to its superior object annotation quality. Which objects recur? We present breakdown of the percentage of repeating objects for each object category in Fig. 4. We see that many mass produced objects have high recurrence rate. There are some retrieval failure modes, as the encoder fails to differentiate between lookalike animals. Figure 5. Creating supervised dataset. For each unlabeled image, we detect and crop objects with high detection confidence. Next, we extract the kNN of these objects based on IR feature similarity. To generate the background image, we apply an object removal model. the standard UNet architecture. It trains the model to take grid of 22 images, each with resolution of 512512, resulting in composite input image of size 10241024. The grid consists of the 3 reference images and noisy target image in the top-left quarter (see Fig. 6). The model transfers information between the references and the noisy target image through self-attention layers. As the models objective is to denoise only the top left quarter of the grid, ObjectMate computes the loss only on these pixels. For object insertion, it takes two additional images, each populating only the top-left quarter and the rest is filled with zeros. The first is the background image and the second, the boundingbox mask indicating which pixels of the noised image should contain the object. Finally, ObjectMate concatenates the three images along the channel axis. For subject generation, it conditions the model on the text description via cross-attention. Implementation details. We train separate diffusion models for object insertion and subject generation. ObjectMates architecture is similar to Stable Diffusion XL [12]. To leverage large-scale pretraining, we initialize the object insertion model from an inpainting checkpoint and the subject generation model from text-to-image checkpoint. Both models are trained for 100K steps with batch size of 128 on 128 V4 TPUs, taking approximately 24 hours. 6. Experiments 6.1. Evaluation protocol Evaluating editing methods is notoriously challenging. Effective methods must edit as the user intended while preserving object identity and maintaining photorealistic composition. Here, we address gaps in the evaluation protocols for both object insertion and subject generation Subject generation. Evaluation protocols for this task must address 2 objectives: subject identity preservation and alignment with the text prompt. While the CLIP-T metric, the distance between the CLIP embeddings of the text prompt and the output image, measures alignment effectively, current metrics (CLIP-I, DINO) do not capture obFigure 6. Architecture. We use an unmodified standard UNet. The input is 2 2 grid of 3 reference images and noisy target image. We calculate the loss only for the target image pixels. In object insertion, we concatenate the mask and background along the channel axis. 5.2. Training Having large paired datasets makes object insertion and subject generation simpler. Even straightforward diffusion architecture trained on such large-scale supervised data achieved excellent performance. Following latent diffusion [46], ObjectMate performs the diffusion process in lowerdimensional latent space. Unless specified otherwise, it first maps all images in the diffusion optimization procedure to latents. It trains denoising network with UNet architecture that takes as input noised image, multiple reference object views Oi and scene description Si. For object insertion, Si consists of scene background image and location mask. For subject generation, Si it is text prompt describing the scene. The timestamp is τ , and ατ , στ parameterize the noising schedule. The UNet denoiser, Dθ, learns to map these inputs to the denoised target image y. The diffusion objective uses Euclidean loss: L(θ) = τ ([0,T ]) ϵN (0,1) (cid:34) (cid:88) i=1 Dθ(ατ yi + στ ϵ, Oi, Si, τ ) ϵ2 (cid:35) (1) Conditioning on multiple object references. To condition the generation on multiple reference images, ObjectMate takes straightforward approach, without modifying Figure 7. Object insertion results. Our method better harmonizes the pose and lighting with the scene while preserving object identity. Composition Identity Method CLIP-I DINO IR Paint-by-Example ObjectStitch AnyDoor Ours - 1 Ref. Ours - 3 Ref. 0.898 0.905 0.916 0.934 0.940 0.800 0.793 0.822 0.868 0.885 0.544 0.564 0.738 0.803 0.858 Table 1. Object insertion: baseline comparison. Our method achieves better composition and identity preservation. ject identity preservation adequately. To address this limitation, we propose measuring identity preservation using the IR features from [51]. Specifically, we propose cropping the 2 images to the subjects detection bounding boxes and measuring the cosine similarity between their IR features. We run user study asking users to rank identity preservation between two edits (see SM). Tab. 3 shows that using IR feature similarity is more accurate in predicting user perceptions of identity preservation, indicating better alignment. Object insertion. Insertion methods require photorealistic 6 object and scene composition. Currently, reliable evaluation depends on user studies. To automate this, we created supervised test set of 34 objects, each captured in 4 poses and scenes. Using tripod-mounted camera, we photographed each view with and without the object. We extract 4 samples per quadruplet: 1 ground truth image y, its background as scene description S, and the 3 remaining images as reference views O, yielding 136 samples. This dataset enables comparison of composite images to ground truth using DINOs semantic similarity as score. Our protocol includes two metrics: (i) object identity preservation using IR features, and (ii) DINO similarity between the composite outputs and ground truth. 6.2. Object insertion Baselines. We compare our method with Paint-by-Example [65], ObjectStitch [57] (we use an unofficial implementation [33] as the official implementation is unavailable), and AnyDoor [10]. Automatic metrics. Tab. 1 shows that ObjectMate outperforms all object insertion baselines in both composition and identity preservation. User study. We used the CloudResearch platform to gather Figure 8. Subject-driven generation results. ObjectMate can composite the object into the scene given 3 reference views and prompt describing the scene. Our method does not require test-time tuning. Method Tuning-free CLIP-T CLIP-I DINO Text Semantic TI DreamBooth DisenBooth ELITE BLIP-Diff. Ours - 1 Ref. Ours - 3 Ref. 0.306 0.291 0.301 0.293 0.288 0.322 0.322 0.775 0.767 0.784 0.767 0.788 0.770 0.773 0.564 0.576 0. 0.569 0.581 0.606 0.607 Id. IR 0.655 0.674 0.728 0.638 0.664 0.739 0.750 Table 2. Subject-driven generation: baseline comparison. While many methods perform well on semantic similarity (CLIPI, DINO), our method performs the best at identity presentation (IR) and alignment to the text prompt (CLIP-T). user preferences from 45 randomly selected participants. Each participant scored composition realism and identity preservation on 25 examples of our method versus random baseline. See SM for more details. Tab. 4 shows that users preferred our method over all baselines. Qualitative evaluation. Fig. 7 presents qualitative comparison with the baselines. See more examples in the SM. 7 6.3. Subject-driven generation Baselines. We compare our method with test-time-tuning approaches (Textual-Inversion [15], DreamBooth [47], DisenBooth [6]), and zero-shot methods (Blip-Diffusion [31], ELITE [62]) on the public benchmark DreamBench [47]. Automatic metrics. Tab. 2 shows that ObjectMate achieves the highest text alignment score. For identity preservation, the story is more nuanced. While ObjectMate does not outperform all methods in CLIP-I and DINO, it shows significant improvement in IR feature similarity. This suggests that while other methods generate semantically similar subjects, ObjectMate generates subjects with the same identity, aligning better with the task objective. User study. We conducted user study similar to Sec. 6.2. Tab. 5 shows that users preferred our method in terms of object preservation and text alignment. The results also confirm that the IR metric aligns better with user preferences compared to CLIP-I and DINO. Qualitative evaluation. Fig. 8 provides qualitative comparison with the baselines. Additional examples are provided in the SM."
        },
        {
            "title": "Task",
            "content": "CLIP-I DINO IR"
        },
        {
            "title": "Subject Generation\nObject Insertion",
            "content": "64.7% 68.4% 72.9% 60.4% 71.8% 79.5% Table 3. Identity metric comparison. Accuracy of metrics in predicting user responses. IR is the most accurate. Method ObjectStitch Paint-by-Example AnyDoor Identity Composition 86% 86% 100% 80% 76% 81% Table 4. Object insertion: user study. Percentage of users preferring our method over the baseline using 1 reference image. 6.4. Ablation study Public features and data. While we conducted experiments using internal datasets and retrieval features, public datasets and features exhibit similar behavior. To demonstrate this, we create paired dataset based on the annotated objects in the public Open Images dataset [29]. Instead of using an object removal model for the background condition, we mask the target image, similarly to [10, 65]. Furthermore, we compute the distance between image pairs for the kNN retrieval using the publicly available IR features [51]. We trained ObjectMate on these features and data, the results are shown in Fig. 9. Notably, this setup outperformed AnyDoor, the strongest baseline, using either one or three references. Internal and public IR features demonstrated comparable performance. Dataset size. We trained our entire object insertion pipeline end-to-end based on unsupervised object datasets of varying sizes. The object identity preservation and ground truth composition metrics are presented in Fig. 10. The results clearly show that larger datasets lead to improved performance. Interestingly, the performance has not yet saturated, suggesting that scaling up existing datasets could further enhance future systems. Retrieval and DINO features. We trained ObjectMate on the WebLI-55M with retrieval based on both DINO and IR features. We compared the two models by user study. Users preferred the identity preservation of ObjectMate that used the IR features dataset over the DINO dataset 63% of the time, demonstrating its effectiveness. Comparison to ObjectDrop. We do not directly compare to ObjectDrop as it merely copies the object into the new scene, while adding its shadows and reflection. It does not attempt to harmonize the lighting and pose of objects. In user study we ran, users responded that ObjectDrop preserved identity better in 71% of the time, as it copies the reference view directly and must preserve identity. However, Figure 9. Open features and data. Using data based on IR features outperforms CLIP and DINO. Public datasets and feature encoders achieve strong performance. Figure 10. Effect of dataset size on object insertion metrics. Larger unsupervised datasets yield better results. Method ELITE BLIP-Diff. TI DisenBooth DreamBooth Identity Text align 83% 95% 67% 79% 69% 56% 64% 67% 61% 91% Table 5. Subject-driven generation: user study. Percentage of users preferring our method over the baseline. users preferred ObjectMates composition 76% of the time as ObjectDrop does not hormonize the object. We believe ObjectMate is preferable when the scene context requires adjustments to the object. 7. Discussion and Limitations Other use cases for the dataset. While this paper focuses on object composition, we anticipate that our dataset creation method will also benefit tasks such as 3D geometry and object editing. We leave this exploration to future work. Number of references. Although our retrieval procedure can identify an arbitrary number of reference images, ObjectMates architecture currently supports up to 3 references. Future work could address this limitation by using cross-attention over references instead of self-attention. Retrieval for human subjects. The IR features used in this work were not designed for retrieving images of humans, and the inclusion of humans is beyond the scope of this study. However, we anticipate that using face recognition features could effectively retrieve multiple views of the 8 same individual. Additionally, since the number of humans is limited and their popularity varies significantly, we expect the object repetition prior to apply to them as well. We leave this exploration for future work. Limits on identity preservation. ObjectMate achieves better than state-of-the-art results for identity preservation, but it is constrained by VAE compression. For instance, VAEs often do not perfectly reconstruct text. While this is limitation of all latent diffusion models, using larger VAEs or performing pixel-space diffusion can mitigate this. 8. Conclusion We proposed the object recurrence prior, which states that object instances recur exactly across different scenes, poses, and lighting conditions in large unsupervised image collections. This is mostly due to mass-produced objects. We used this to create massive supervised datasets for object composition. These datasets were sufficient for making simple architectures achieve excellent performance. Concretely, our method, ObjectMate, outperforms state-of-theart methods in object insertion and subject driven generation. Additionally, we enhanced automated evaluation protocols by introducing supervised benchmark dataset for object insertion and proposing new metric for object identity preservation. Our analysis suggests that further scaling of dataset sizes and improving retrieval features will likely improve results. 9. Acknowledgement We would like to thank Amir Hertz, Andrey Voynov, Eliahu Horwitz, Jonathan Kahana, Tal Reiss, Yuval Bahat, and Nadav Magar for their invaluable feedback and discussions. We also appreciate the insights provided by Shmuel Peleg and Dani Lischinski, which helped improve this work."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. Communications of the ACM, 54(10):105112, 2011. 2, 3 [2] Andreas Blattmann, Robin Rombach, Kaan OkJonas Muller, and Bjorn Ommer. SemiarXiv preprint tay, parametric neural image synthesis. arXiv:2204.11824, 2022. 3, 13 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editIn Proceedings of the IEEE/CVF ing instructions. Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 13 IEEE computer society conference on computer vision and pattern recognition, pages 6065. Ieee, 2005. 2, 3 [5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep local and global features for image search. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages 726743. Springer, 2020. 3 [6] Hong Chen, Yipeng Zhang, Simin Wu, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Identity-preserving disentangled tuning arXiv for subject-driven text-to-image generation. preprint arXiv:2305.03374, 2023. 3, 7 [7] Wenhu Chen, Hexiang Hu, Chitwan Saharia, Re-imagen: RetrievalarXiv preprint and William Cohen. augmented text-to-image generator. arXiv:2209.14491, 2022. 3, [8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. Advances in Neural Information Processing Systems, 36, 2024. 3, 13 [9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: jointly-scaled multilingual language-image model, 2022. 3, 4, 13 [10] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. arXiv preprint arXiv:2307.09481, 2023. 3, 4, 6, 8 [11] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. arXiv preprint arXiv:2406.07547, 2024. 3 [12] Diffusers. Stable diffusion xl inpainting 0.1. https: / / huggingface . co / diffusers / stable - diffusion-xl-1.0-inpainting-0.1, 2023. 5 [13] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image generation via positive-negative prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 3 [14] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [4] Antoni Buades, Bartomeu Coll, and J-M Morel. In 2005 non-local algorithm for image denoising. [15] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel 9 Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3, 7 [16] Rinon Gal, Moab Arar, Yuval Atzmon, Amit and Daniel Cohen-Or. Bermano, Gal Chechik, Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023. [17] Daniel Glasner, Shai Bagon, and Michal Irani. SuperIn 2009 IEEE 12th resolution from single image. international conference on computer vision, pages 349356. IEEE, 2009. 2, 3 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 3 [19] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 38873896. PMLR, 2020. 13 [20] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73237334, 2023. 3 [21] Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, and Si Liu. Freeedit: Mask-free reference-based image editing with multi-modal instruction. arXiv preprint arXiv:2409.18071, 2024. 4 [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 13 [23] Yan Hong, Li Niu, and Jianfu Zhang. Shadow generation for composite image in real-world scenes. In Proceedings of the AAAI conference on artificial intelligence, pages 914922, 2022. [24] Hexiang Hu, Kelvin CK Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue InstructBen, Boqing Gong, William Cohen, et al. imagen: Image generation with multi-modal instrucIn Proceedings of the IEEE/CVF Conference tion. on Computer Vision and Pattern Recognition, pages 47544763, 2024. 3, 13 [25] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Image-to-image translation with Alexei Efros. In Proceedings of conditional adversarial networks. the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 3 [26] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 3 Ivan Krasin, [27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3 [28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 3 [29] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 3, 8, 13 [30] Bowen Li, Philip HS Torr, and Thomas Lukasiewicz. arXiv Memory-driven text-to-image generation. preprint arXiv:2208.07022, 2022. 3, 13 [31] Dongxu Li, Junnan Li, and Steven Hoi. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. 3, 7 [32] Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. arXiv Dreamedit: Subject-driven image editing. preprint arXiv:2306.12624, 2023. 3 [33] Bo Zhang Li Niu. Objectstitch-imagecomposition. https://github.com/bcmi/ ObjectStitchImageComposition, 2024. 6 [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: ComIn Computer VisionECCV mon objects in context. 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 3, 13 [35] Daquan Liu, Chengjiang Long, Hongpan Zhang, Hanning Yu, Xinzhi Dong, and Chunxia Xiao. Arshadowgan: Shadow generative adversarial network for augmented reality in single light scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81398148, 2020. 3 [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 15 [37] David Lowe. Distinctive image features from scaleinvariant keypoints. International journal of computer vision, 60:91110, 2004. 3 [38] Lingxiao Lu, Jiangtong Li, Bo Zhang, and Li Niu. Dreamcom: Finetuning text-guided inpaintarXiv preprint ing model for image composition. arXiv:2309.15508, 2023. 3 [39] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-toimage generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 1 12, 2024. 3 [40] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal latent diffusion for joint subject and text conditional image generation. arXiv preprint arXiv:2303.09319, 2023. 3 [41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [42] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin Dinov2: Learning robust viEl-Nouby, et al. arXiv preprint sual features without supervision. arXiv:2304.07193, 2023. 3 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, textCasey Chu, and Mark Chen. Hierarchical conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 3 [45] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3 (4):333389, 2009. 3 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF Conference els. on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 3, 5 [47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 3, [48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 3 [49] Nataniel Ruiz, Yuanzhen Li, Neal Wadhwa, Yael Pritch, Michael Rubinstein, David Jacobs, and Shlomi Fruchter. Magic insert: Style-aware drag-anddrop. arXiv preprint arXiv:2407.02489, 2024. 3 [50] Johannes L. Schonberger and Jan-Michael Frahm. In Proceedings of Structure-from-motion revisited. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 3 [51] Shihao Shao and Qinghua Cui. 1st place solution in google universal images embedding. arXiv preprint arXiv:2210.08473, 2022. 2, 3, 6, 8, 15, 16 [52] Shihao Shao and Qinghua Cui. 1st solution in https : / / google universal image embedding. www . kaggle . com / datasets / louieshao / guieweights0732, 2023. 15 [53] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. 3, [54] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation In Proceedings of the without test-time finetuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. 3 [55] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised In learning using nonequilibrium thermodynamics. International conference on machine learning, pages 22562265. PMLR, 2015. 3 [56] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Score-based generative modeling through Poole. arXiv preprint stochastic differential equations. arXiv:2011.13456, 2020. 3 [57] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and Daniel Aliaga. Objectstitch: Object compositing with diffuIn Proceedings of the IEEE/CVF Consion model. ference on Computer Vision and Pattern Recognition, pages 1831018319, 2023. 3, 6 [58] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, He Zhang, 11 net: Zero-shot object customization with variableviewpoints in text-to-image diffusion models. arXiv preprint arXiv:2310.19784, 2023. 3 [68] Bo Zhang, Yuxuan Duan, Jun Lan, Yan Hong, Huijia Zhu, Weiqiang Wang, and Li Niu. Controlcom: Controllable image composition using diffusion model. arXiv preprint arXiv:2308.10040, 2023. 3 [69] Shuyang Zhang, Runze Liang, and Miao Wang. Shadowgan: Shadow synthesis for virtual objects with conditional adversarial networks. Computational Visual Media, 5:105115, 2019. 3 Wei Xiong, and Daniel Aliaga. Imprint: Generative object compositing by learning identity-preserving representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80488058, 2024. 3 [59] Gemma Canet Tarres, Zhe Lin, Zhifei Zhang, Jianming Zhang, Yizhi Song, Dan Ruta, Andrew Gilbert, John Collomosse, and Soo Ye Kim. Thinking outside the bbox: Unconstrained generative object compositing. arXiv preprint arXiv:2409.04559, 2024. 4 [60] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image In ACM SIGGRAPH 2023 Conferpersonalization. ence Proceedings, pages 111, 2023. [61] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 3 [62] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. 3, 7 [63] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic In Computer Vision object removal and insertion. ECCV 2024, pages 112129, Cham, 2024. Springer Nature Switzerland. 2, 3, 4, 14 [64] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuningfree multi-subject image generation with localized atInternational Journal of Computer Vision, tention. pages 120, 2024. 3 [65] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editIn Proceedings of the ing with diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 3, 4, 6, 8 [66] Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Bingyi Cao, Mario Lipovsk`y, Pelin Dogan-Schonberger, Grzegorz Makosa, Boris Bluntschli, Mojtaba Seyedhosseini, Ondˇrej Chum, and Andre Araujo. Towards universal image embeddings: large-scale dataset and challenge for generic image representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1129011301, 2023. [67] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, and Ying Shan. Custom12 ObjectMate: Recurrence Prior for Object Insertion and Subject-Driven Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "and leveraging both modes during inference."
        },
        {
            "title": "In object",
            "content": "Object insertion. insertion, we modify the training process by zeroing out the reference condition in 10% of the training examples, while keeping the scene condition (background images and masks) unchanged. During inference, the models output is adjusted using the following formula: Figure 11. Subject-driven generation models architecture. Dθ(xt, O, S) =Dθ(xt, , S) A. Implementation details Training. As detailed in Sec. 5, we train two separate models: one for object insertion and another for subjectdriven generation. Fig. 6 in the main manuscript illustrates the architecture of our object insertion model. Additionally, App. Fig. 11 provides diagram for the subject-driven generation model. The primary difference between these architectures lies in how the input is integrated into the UNet. For object insertion, the scene description, background image and mask are concatenated along the channel axis with the noise inIn contrast, for subject-driven generation, the scene put. description is provided as text prompt and incorporated into the UNet via standard cross-attention layers. During object insertion training, we use an empty text prompt. The mask indicating the target objects location is the bounding box of the object rather than precise mask. k-Nearest Neighbors (kNN) search. For each detected object in our dataset, we compute retrieval-specific features designed for instance retrieval without local feature matching. This design makes them well-suited for large-scale kNN searches. Using the Python library ScaNN [19], we calculate the cosine similarity of features between all object pairs in the dataset. In the final dataset, we retain the top 5 nearest neighbors with similarity scores ranging from 0.93 to 0.975, as detailed in Section 4. A.1. Classifier-Free Guidance Following Brooks et al. [3], we apply classifier-free guidance (CFG) [22] to both text and image conditions. CFG is widely used method to enhance the models adherence to its conditioning inputs. This involves jointly training the model for both conditional and unconditional generation + γI (Dθ(xt, O, S) Dθ(xt, , S)) Here, γI controls the influence of the reference condition, we empirically set γI = 2. Subject-driven generation. For subject-driven generation, in 10% of the training examples, we zero out the reference condition O, and in another 10%, we use an empty prompt for the scene description S. During inference, the models output is adjusted as follows: Dθ(xt, O, S) =Dθ(xt, , ) + γtxt (Dθ(xt, O, S) Dθ(xt, O, )) + γI (Dθ(xt, O, ) Dθ(xt, , )) Here, γtxt, γI controls the strength of the text condition (scene description) and references condition respectively. We use constant values of γI = 1.5 and γtxt = 7.5. A.2. Dataset statistics In Sec. 4 we use the train split of the datasets COCO [34], Open Images [29], and subset of WebLI [9] of 48M images. We provide dataset statistics in App. Tab. 6. B. Additional comparisons Retrieval augmented models. As discussed in Sec. 2, several studies [2, 7, 8, 24, 30, 53] have used nearest neighbor (NN) retrieval to enhance generation fidelity. Specifically, [2, 7, 30, 53] retrieve the NNs based on the text prompt provided during inference to improve the generation of rare concepts. SuTI [8] and Instruct-Imagen [24] cluster images from the same URL and refine them using CLIP image similarity calculated at the whole-image level. Our approach differs in two key ways: (1) we employ an instance retrieval (IR) model that better distinguishes between identities with"
        },
        {
            "title": "Dataset",
            "content": "# Images # Objects"
        },
        {
            "title": "3 NNs",
            "content": "COCO Open Images Web-based 108,151 1,743,042 47,992,480 362,684 8,067,907 55,232,441 Object detection model"
        },
        {
            "title": "Human annotations\nHuman annotations",
            "content": "31,445 (8.7%) 471,091 (5.8%) 9,947,017 (18%) 17,119 (4.7%) 64,991 (2.4%) 4,550,770 (8.2%) # Examples with at least Table 6. Datasets statistics. similar semantics compared to CLIP, and (2) we calculate similarity at the object level rather than for the entire image. These differences result in object clusters with higher likelihood of representing the same identity. Since SuTI and Instruct-Imagen have not released their models, we compare our results with those reported in their manuscripts. App. Fig. 19 compares results where SuTI uses 5 references and our model uses 3. Our approach consistently achieves better identity preservation. Additionally, App. Fig. 20 compares our results with SuTI where both models use either 1 or 3 references. App. Fig. 21 qualitatively compares our model with Instruct-Imagen, demonstrating superior preservation of fine object details. Counterfactual object insertison. Similarly to ObjectDrop [63], we trained an object removal model using 2,000 counterfactual examples. We then used this model to synthesize the backgrounds for object insertion training. ObjectDrops approach involves training an object insertion model by first removing objects from images and then reinserting them into their original positions. For comparison, we implemented this approach in our experiments. When inserting objects into scene, the ObjectDrop model pastes them and generates only their effects on the surroundings. While this ensures identity preservation, it does not allow for adjustments to the pose or lighting of the inserted objects. In contrast, our model incorporates these capabilities, enabling more realistic harmonization of the object with the scene. App. Fig. 17 highlights our models superior performance in harmonizing lighting and pose. Retrieval and DINO features. We conducted an ablation study to assess the importance of instance retrieval (IR) features in our models performance. Specifically, we used DINO features to perform kNN search on the same image dataset used in our primary experiments. Subsequently, we trained subject generation model using the retrieval results based on these features. Notably, DINO features tend to identify objects with only semantic similarities (as illustrated in Fig. 2), which substantially influences the downstream performance of the model. To complement the findings of the user study presented in the main manuscript, App. Fig. 15 provides qualitative evidence showing that our model achieves superior identity preservation compared to model trained using DINO-based retrievals. More results. We extend the qualitative comparisons presented in the main manuscript with the following figures: Fig. 14 complements the quantitative comparison between different retrieval features made in Fig. 9 of the main manuscript. Fig. 16 shows that using publicly available dataset and IR features outperforms current SOTA insertion method. Fig. 22 shows creative application. Fig. 23 presents failure cases. Fig. 18, 24, and 25 show additional examples of object insertion. Fig. 26 and 27 present additional examples of subjectdriven generation. C. User study To evaluate the performance of our models, we conducted detailed user study on the CloudResearch platform. For the object insertion task, we had 50 participants, randomly selected, primarily from the United States. Each participant reviewed 25 examples drawn from our benchmark dataset comprising 136 examples. For each example, participants were presented with two images in random order: one generated by our model and another by baseline model. Participants were asked to answer the following questions: 1. Which image looks more realistic and natural? 2. In which image the subject is more similar to the reference? The responses to the first question were used to compute the Composition score, while the responses to the second question contributed to the Identity score. The results of this study are presented in Tab. 4 of the main manuscript. For the subject-driven generation task, 45 participants completed similar questionnaire with the following questions: 1. Which image matches the text prompt more? 2. In which image the subject is more similar to the references? In this evaluation we used the public benchmark DreamBench, which includes 30 unique objects and 25 textual 14 Figure 13. Example of quadruplet from out test set. From each quadruplet we extract 4 samples, where one object is used as the ground truth and the remaining 3 serve as the reference condition. these features are represented as E( Ig) and E(Iref ), respectively. Finally, the IR identity preservation score is determined by calculating the cosine similarity between E( Ig) and E(Iref ). The weights of the encoder are publicly available to download from [52]. To validate this protocol, we analyzed user study responses regarding identity preservation (see Sec. C). Each response comprises triplet (Iref , Ig1, Ig2 ), where Ig1 is the output of our model, Ig2 is the output or one of the baselines, and {1, 2} indicates the users choice for better identity preservation. For evaluating the validity of the metrics, the user responses serve as ground truth and we measure the accuracy of each metric in predicting user preferences. As presented in Tab. 3 of the main manuscript, IR demonstrates significantly improved performance over existing metrics, confirming the strong alignment between our automated evaluation method and human judgment. E. Object insertion benchmark We introduce new benchmark for object insertion. The benchmark comprises test set of 34 distinct objects, each captured in 4 different poses and scenes, representing variations such as indoor/outdoor settings and different times of day (e.g., daytime vs. nighttime). For each scene, we use tripod-mounted camera to capture images both with and without the object. From each quadruplet of images, we extract 4 samples: ground truth image (y), the background of the scene as scene description (S), and three reference images (O). This results in total of 136 samples. To the best of our knowledge, this is the first object insertion dataset that includes ground truth images and three reference views of the inserted object. An example of one such quadruplet is shown in Fig. 13. We will make this test set publicly available, along with the outputs of our model. Figure 12. screenshot of the user study questionnaire. prompts, resulting in total of 750 examples. The results are summarized in Tab. 5 of the main manuscript. Fig. 12 shows screenshot of the questionnaire. D. Quantitative evaluation protocol As outlined in Sec. 6, existing quantitative metrics, such as CLIP and DINO, primarily evaluate semantic similarity rather than the preservation of identity. To address this, we propose using the instance retrieval (IR) features from [51], which we demonstrate to be more closely aligned with user preferences for identity preservation (see Tab. 3 in the main manuscript). Below, we detail the evaluation protocol used in our approach. Given generated image Ig and reference image of the subject Iref , we begin by detecting the bounding box of the subject in Ig using [36] with the objects class name as input. The generated image Ig is then cropped to this bounding box, resulting in Ig. Next, we compute the IR features, denoted as E, for both Ig and Iref . Specifically, 15 Figure 14. Ablation study on the importance of IR features for object insertion. Using CLIP or DINO features for instance retrieval during object insertion training is insufficient to achieve identity preservation. Using specialized instance-retrieval (IR) features achieve much stronger results. In addition, the publicly available IR model from [51] is comparable to our internal model. Figure 15. Ablation study on the importance of IR features for subject generation. Our subject generation model, denoted as IR, demonstrates superior identity preservation compared to model trained using DINO-based retrievals. 16 Figure 16. Ablation study on data sources. We compare the effectiveness of different data sources for training. Training on Open Images with publicly available IR features and on web-scraped dataset using our internal IR model both outperform the current state-of-the-art insertion model, AnyDoor. Figure 17. Comparison with counterfactual object insertion. We compare to model similar ObjectDrop. Our model is able to realistically harmonize the objects pose and lighting, while the counterfactual model pastes the object without adjustments. 17 Figure 18. Additional in-the-wild object insertion results. Figure 19. Comparison with SuTI. Our method better preserves the fine details of the subjects. SuTI uses semantic features (CLIP) for retrieval, while we use specialized instance-retrieval features. This makes our paired data more suitable for identity preservation. Results of SuTI are taken from their manuscript. Here, SuTI uses 5 references, while we use 3. Figure 20. Comparison with SuTI. Our model demonstrates superior capability in preserving fine details of the object, regardless of whether 1 or 3 reference images are provided by the user. Results of SuTI are taken from their manuscript. 19 Figure 21. Comparison with Instruct-Imagen. Our method better preserves the fine details of the bowl (e.g., text decoration). InstructImagen uses similar data to SuTI, which is based on semantic clustering. Results of Instruct-Imagen are taken from their manuscript. Figure 22. Creative application. We test the models generalization by providing it with three references of different objects. This setup represents significant deviation from the training distribution, where the model received three references of the same object. Remarkably, the model demonstrates an ability to generalize beyond its training data by either synthesizing the references into single unified object or generating the three objects separately. (a) (b) Figure 23. Limitations. (a) This study primarily focuses on preserving subject identity, which may result in quality variability in scenarios that require changing some of the subjects properties, such as changes in color or shape. (b) Given that the training data is predominantly composed of real photographs, the model occasionally generates photos of paintings when the prompt specifies an artistic style. 20 Figure 24. Additional object insertion comparisons on our benchmark with the provided ground truth. 21 Figure 25. Additional object insertion comparisons on our benchmark with the provided ground truth. 22 Figure 26. Additional subject-driven generation comparisons. 23 Figure 27. Additional subject-driven generation comparisons."
        }
    ],
    "affiliations": [
        "Google",
        "The Hebrew University of Jerusalem"
    ]
}