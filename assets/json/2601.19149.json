{
    "paper_title": "GPCR-Filter: a deep learning framework for efficient and precise GPCR modulator discovery",
    "authors": [
        "Jingjie Ning",
        "Xiangzhen Shen",
        "Li Hou",
        "Shiyi Shen",
        "Jiahao Yang",
        "Junrui Li",
        "Hong Shan",
        "Sanan Wu",
        "Sihan Gao",
        "Huaqiang Eric Xu",
        "Xinheng He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "G protein-coupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding affinity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, a deep learning framework specifically developed for GPCR modulator discovery. We assembled a high-quality dataset of over 90,000 experimentally validated GPCR-ligand pairs, providing a robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-fidelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptor-ligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compound-protein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identified micromolar-level agonists of the 5-HT\\textsubscript{1A} receptor with distinct chemical frameworks. These results establish GPCR-Filter as a scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 2 ] . [ 1 9 4 1 9 1 . 1 0 6 2 : r GPCR-Filter: deep learning framework for efﬁcient and precise GPCR modulator discovery Jingjie Ning1 , Xiangzhen Shen2 , Li Hou3 , Shiyi Shen3 , Jiahao Yang4 , Junrui Li3 , Hong Shan3 , Sanan Wu5 , Sihan Gao6 , Huaqiang Eric Xu3 , Xinheng He3 1 School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA 2 College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan 410082, China 3 The State Key Laboratory of Drug Research, Shanghai Institute of Materia Medica, Chinese Academy of Sciences, Shanghai, China 4 Lingang Laboratory, Shanghai, China 5 Research Center for Medicinal Structural Biology, National Research Center for Translational Medicine at Shanghai, State Key Laboratory of Medical Genomics, Ruijin Hospital, Shanghai Jiao Tong University School of Medicine, Shanghai, China 6 School of Pharmacy, Fudan University, Shanghai 201203, China"
        },
        {
            "title": "Abstract",
            "content": "G proteincoupled receptors (GPCRs) govern diverse physiological processes and are central to modern pharmacology. Yet discovering GPCR modulators remains challenging because receptor activation often arises from complex allosteric effects rather than direct binding afﬁnity, and conventional assays are slow, costly, and not optimized for capturing these dynamics. Here we present GPCR-Filter, deep learning framework specifically developed for GPCR modulator discovery. We assembled high-quality dataset of over 90,000 experimentally validated GPCRligand pairs, providing robust foundation for training and evaluation. GPCR-Filter integrates the ESM-3 protein language model for high-ﬁdelity GPCR sequence representations with graph neural networks that encode ligand structures, coupled through an attention-based fusion mechanism that learns receptorligand functional relationships. Across multiple evaluation settings, GPCR-Filter consistently outperforms state-of-the-art compoundprotein interaction models and exhibits strong generalization to unseen receptors and ligands. Notably, the model successfully identiﬁed micromolar-level agonists of the 5-HT1A receptor with distinct chemical frameworks. These results establish GPCRFilter as scalable and effective computational approach for GPCR modulator discovery, advancing AI-assisted drug development for complex signaling systems. These authors contributed equally to this work. Corresponding authors. Email: xinheng.he@simm.ac.cn, eric.xu@simm.ac.cn."
        },
        {
            "title": "1\nG proteincoupled receptors (GPCRs) constitute the largest\nand most diverse family of cell-surface receptors, with more\nthan 800 members that orchestrate a wide spectrum of physio-\nlogical and pathological processes[1]. Through coupling with\nG proteins and arrestins, GPCRs regulate key pathways in the\nnervous, endocrine, and immune systems[2, 3, 4]. Owing to\ntheir central roles in signaling transduction, GPCRs consti-\ntute one of the most important classes of drug targets in mod-\nern medicine: up to 36% of all drugs approved by the U.S.\nFood and Drug Administration (FDA) act on GPCRs, and\nmore than 300 GPCR-targeting agents are currently in clin-\nical development[5, 6]. The discovery of new GPCR modula-\ntors is increasingly driven by the integration of computational\nand experimental approaches[7].",
            "content": "Despite decades of research, identifying GPCR modulators remains major challenge[8]. Unlike enzymes or ion channels, GPCRs exhibit highly dynamic conformational landscapes and allosteric mechanisms that can decouple ligand binding from downstream signaling efﬁcacy[9, 10]. Consequently, traditional computational screening methods often fail to capture the complex relationship between binding and functional modulation, resulting in uncertain or inactive hits, especially in unpublished negative results[11]. Experimental assays designed to probe these effects are typically costly, low-throughput, and labor-intensive, further limiting the discovery rate of novel modulators[12]. Recent advances in artiﬁcial intelligence (AI) offer new opportunities to address these challenges. AI models are capable of learning the complex features and approaching ligand discovery in data-driven manner. Broadly, AI-based methods can be categorized into structure-based and sequencebased approaches. Structure-based strategies leverage threedimensional receptor conformations to extract pocket information, ﬁlter potential binders, and capture detailed molecular interactions through AI-assisted docking[13, 14]. However, the number of experimentally determined GPCRligand complex structures remains limited (fewer than 1,800, and largely conserved), and the inaccuracies of computationally predicted structures constrain the ability to ﬁne-tune such models effectively[15, 16]. In contrast, sequence-based screening approaches offer access to far larger datasets and enable modeling of the relationships between ligand chemistry, receptor sequence, and downstream functional activity. General sequence-based drugtarget interaction (DTI) models such as TransformerCPI[17], TransformerCPI2.0[18], and ConPLex[19] combine protein language models with molecular representations to predict binding likelihoods across vast chemical spaces. Nonetheless, these frameworks are primarily optimized for generic binding prediction and often lack the biochemical sensitivity required to capture the nuanced sequencestructurefunction relationships that govern GPCR pharmacology. Combining sequence and structure based screening method, especially in GPCR domain, can improve success rate via different aspects [11]. To overcome these limitations and provide new ﬁlter during screening, we curated large, experimentally grounded dataset comprising over 90,000 GPCRligand pairs. Based on this dataset, we developed GPCR-Filter, deep learning framework dedicated to GPCR modulator discovery. GPCRFilter integrates the ESM-3 pre-trained model[20] for ﬁnegrained GPCR sequence embeddings with graph neural networks (GNNs) that capture ligand structure and chemistry. An attention-based fusion mechanism learns functional correspondence between receptor and ligand features, enabling prediction of modulatory potential beyond simple binding. Across multiple evaluation scenarios, GPCR-Filter surpasses state-of-the-art DTI models in both accuracy and generalization. Furthermore, experimental validation demonstrated that GPCR-Filter can identify micromolar-level agonists of the 5-HT1A receptor with distinct scaffolds, conﬁrming its potential to guide functional ligand discovery. By bridging GPCR biology and deep learning, this work provides scalable computational framework for mapping modulator space and accelerating GPCR-targeted drug development."
        },
        {
            "title": "2 Results\n2.1 GPCR-Filter overview\nAs illustrated in Fig. 1a, GPCR modulator discovery typi-\ncally begins with selecting a GPCR target and assembling\na ligand library suitable for virtual screening. Following\nconventional structure-based ﬁltering, GPCR-Filter operates\npurely on sequence and SMILES information to further pri-\noritize candidate ligands before experimental testing, thereby\nincreasing downstream hit rates. The overall architecture is\nshown in Fig. 1b.\nIt takes a GPCR amino-acid sequence\nand a ligand SMILES string as input, encoding them into per-\nresidue and per-atom representations using the pretrained pro-\ntein language model ESM-3 and a molecular graph neural net-\nwork, respectively. These representations are projected into\na shared latent space and fused through a Transformer-style\ndecoder with ligand-to-protein cross-attention, which aggre-\ngates the coupled features into a ﬁnal interaction probability.",
            "content": "To train and evaluate GPCR-Filter, we curated large, highquality dataset of 91,396 validated human GPCRdrug interactions, integrating records from GPCRdb and GtoPdb[15, 21], aligning 527 unique GPCRs to UniProt sequences, and standardizing 72,177 distinct ligands into canonical SMILES. The interaction distribution exhibits pronounced long-tailed pattern in which small number of highly promiscuous GPCRs dominate the dataset, whereas most receptors exhibit sparse ligand coverage (Appendix Fig. S1 and Table S1). Because negative labels are not experimentally available, we constructed negatives by enumerating all GPCRligand combinations, removing known positives, and sampling to obtain balanced 1:1 dataset. Across these settings, we compared GPCR-Filter with two state-of-the-art sequence-based DTI models, ConPLex and TransformerCPI2.0, using identical sequence+SMILES inputs; detailed dataset statistics and split sizes are provided in Methods[18, 19]."
        },
        {
            "title": "2.2 Predictive performance across data splits\nWe compared GPCR-Filter with two competitive sequence-\nbased DTI frameworks, ConPLex and TransformerCPI2.0,\nunder three evaluation regimes designed to probe increasingly\nchallenging levels of generalization (Fig. 2a). These include\na random split for in-distribution assessment, an intra-target\nsplit evaluating generalization to unseen ligand combinations\nfor previously encountered GPCRs, and an inter-target split\ntesting cross-target transfer to entirely unseen receptors (see\nMethods for details). We used accuracy (ACC), area under\nthe ROC curve (AUC), average precision (AP) and precision\nas evaluation metrics, where larger values indicate better per-\nformance. As shown in Table 1, GPCR-Filter achieved near-\nceiling performance under the random split (AUC of 98.93%\nand AP of 98.70%), substantially outperforming the two base-\nlines. Its robustness was further reﬂected in the intra-target\nsetting (Table 1), where GPCR-Filter maintained high dis-\ncriminative power (AUC of 97.16% and AP of 96.86%) de-\nspite the increased difﬁculty of predicting unseen ligand com-\nbinations for the same receptor. By contrast, both ConPLex\nand TransformerCPI2.0 showed consistently low AUCs (all\nbelow 63%), suggesting limited capacity to generalize across\nligand space.",
            "content": "We next examined the inter-target scenario, the most stringent generalization task in which receptors in the test set never appear during training. In this setting  (Table 1)  , GPCRFilter again delivered the strongest performance, with an AUC of 73.44% and AP of 64.04%, markedly higher than TransformerCPI2.0 and substantially above ConPLex, whose AUC fell below 50%. Such an AUC value shows ConPLex may have overﬁtted on its training set. This indicates that GPCR-Filter is more capable of capturing receptor-level sequence determinants relevant for ligand recognition. Collectively, these results demonstrate that GPCR-Filter consistently outperforms established DTI baselines across all three evaluation regimes, with particularly pronounced advantages in out-of-distribution settings requiring transfer across ligand chemical space or GPCR sequence space. We further visualized the three partitioning strategies and model performance using ROC curves  (Fig. 2)  , which illustrate the strong separation achieved by GPCR-Filter across all 2 Figure 1: Application of GPCR-Filter and overview of its architecture. (a) Workﬂow illustrating how GPCR-Filter integrates into GPCR modulator discovery. After ligand preparation and initial structure-based virtual screening, GPCR-Filter uses only GPCR sequences to further reﬁne docking outputs and prioritize candidates for downstream activation assays. (b) Schematic of the GPCR-Filter architecture. GPCR sequences are embedded into per-residue representations using pretrained protein language model, while ligand SMILES are encoded into per-atom features through molecular graph neural network. cross-attention module couples ligand and receptor representations, and the fused features are aggregated to produce ﬁnal interaction probability. protocols. Together, these ﬁndings highlight the robustness and generalizability of GPCR-Filter for GPCRligand interaction prediction under diverse distributional shifts."
        },
        {
            "title": "2.3 Interpretability of GPCR-Filter\nTo understand why GPCR-Filter generalizes and what pat-\nterns it relies on, we performed two complementary inter-\npretability analyses. At the dataset level, we characterized\neach GPCR by the chemical distribution of its known binders.\nFor the top 20 GPCRs with sufﬁcient ligand data, each lig-\nand was encoded using a 2048-bit Morgan (ECFP4) ﬁnger-\nprint [22], and a receptor-level representation was obtained\nby averaging the ﬁngerprints of all ligands associated with\nthat receptor. Pairwise receptor similarity was estimated us-\ning Tanimoto distance [23] between these averaged proﬁles,\nand hierarchical clustering (average linkage) produced a den-\ndrogram and a ligand-proﬁle similarity heatmap (Fig. S2). Re-\nceptors with chemically similar modulators group together in\nthis space.",
            "content": "Although ligand-proﬁle similarity is an indirect proxy for receptor structural similarity, this analysis highlights that GPCRs occupy partially organized regions of chemical space. Such organization provides plausible context for cross-target generalization: if GPCR-Filter learns chemically grounded patterns shared across subsets of receptors, then unseen receptors whose ligand chemistry resembles that of training receptors may fall within regions where these learned patterns remain applicable. Thus, the clustering results offer an interpretive framework for understanding how transferable chemical preferences could support inter-target predictive performance. At the structural level, we focused on whether the model has learned transferable binding principles rather than merely memorizing receptorligand pairs. Because cross-attention plays central role in ligandprotein interaction modeling, we assess whether residues receiving high attention weights correspond to crystallographic pocket residues [24, 25]. Specifically, we extract the decoders last-layer ligand-to-protein cross-attention using the ligand CLS token (t = 0) as query and residue embeddings as keys/values, average across heads, 3 Figure 2: Dataset split strategies and performance overview. (a) Random Split: Each GPCR (receptor icon) and its ligands are randomly assigned to training and testing sets. Both receptors and ligands can appear in both sets, simulating fully mixed in-distribution evaluation. (b) Intra-Target Split: Each receptor is present in both training and testing, but its associated ligands are divided into mutually exclusive subsets. This evaluates generalization to unseen ligands for the same receptor. (c) Inter-Target Split: The receptor set is partitioned into disjoint training and held-out target subsets (e.g., 9:1). Validation examples are drawn from the held-out subset to keep target identities disjoint between training and evaluation. On the right, the ROC curves illustrate the discriminative performance of GPCR-Filter and baseline models under the three data partitioning protocols. Receptor and ligand icons are for schematic illustration only, with color denoting training or testing samples. Figure 3: Binding-pocket analysis across two GPCR complexes. (ac) correspond to PDB 9bsb; (df) correspond to PDB 9jcl. (a,d) Model prediction scores under the three evaluation settings (Random, Intra-target, Inter-target). All predicted probabilities exceed 0.5, indicating consistent positive predictions across splits. (b,e) Top-20 attended residues ranked under the three evaluation settings. Each entry is reported as indexresidue; highlighted cells indicate residues that are repeatedly prioritized across splits. (c,f) Spatial mapping of the Inter-Target models Top-20 attended residues onto the corresponding structure and binding pocket. Inset panels show the ligand in sticks and the pocket region enlarged (red box), illustrating which high-attention residues lie within the crystallographic binding site. All residue indices correspond to the numbering used in the PDB sequences. mask padded positions, L1-normalize the resulting residuewise vector, and rank residues to obtain Top-20 list. Crystallographic pocket residues are deﬁned as those whose heavyatom distance to the ligand is < 5 Å. For each PDB structure, we report the number of pocket residues appearing in the Top-20 (Pocket hits@20). Models trained under random, intra-target, and inter-target protocols yield broadly consistent top hits, suggesting that the attention patterns are stable across training regimes and provide useful signal for pocket localization. We illustrate these patterns using two recently solved GPCRligand complexes: the dopamine D2 receptor (DRD2; PDB 9bsb [26]) and the purinergic receptor (PDB 9jcl In both cases, GPCR-Filter conﬁdently predicts the [27]). co-crystallized ligand as active under all three training protocols (Fig. 3a,d), providing suitable examples for qualitative interpretability. For 9bsb (Fig. 3ac), there are nine pocket residues within 5 Å of the ligand. The Top-20 attention ranks recover 6, 8, and 7 pocket residues for the random, intra-target, and inter-target models, respectively, indicating that attention emphasizes residues mediating direct interactions. For 9jcl (Fig. 3df), we observe similar qualitative trend: pocket residues are consistently enriched among the Top-20 attention positions across all splits, again showing that attention tends to focus on residues involved in diverse ligandreceptor interactions. Taken together, these case studies demonstrate that the cross-attention mechanism preferentially highlights experimentally validated binding-site 5 Table 1: Performance comparison of GPCR-Filter and baseline models across the three evaluation regimes. Method Random Intra-target Inter-target ACC AUC AP Prec ACC AUC AP Prec ACC AUC AP Prec ConPLex 52.82 53.03 55.29 57. 52.01 52.89 55.26 56.92 57.85 44. 40.78 39.93 TransformerCPI2.0 GPCR-Filter 61.46 95. 65.94 99.02 68.10 98.87 62.50 95. 58.14 93.53 62.02 97.98 63.34 97. 58.77 92.72 63.33 74.57 66.31 80. 58.50 70.99 53.35 75.93 Figure 4: Experimental validation of GPCR-Filter-predicted agonists on the 5-HT1A receptor. (a) Chemical structures of 5-HT and the four validated hits. (b) Single-concentration screening at 30 µM using GloSensor-cAMP assay. Bars represent luminescence (LUM) values for individual compounds; dashed lines denote Forskolin (black, baseline) and 5-HT (red, canonical agonist). Four compounds prioritized by GPCR-Filter (D24, D29, D34, D47) exhibited reduced LUM, indicative of receptor activation. (c) Normalized concentrationresponse curves for 5-HT and the four hits, showing comparable or higher maximal effects (Emax) but right-shifted potencies (higher EC50), indicating increased efﬁcacy but lower potency relative to 5-HT. Together, these results conﬁrm that GPCR-Filter successfully identiﬁed four true agonists activating 5-HT1A in dose-dependent manner. residues, supporting that GPCR-Filter captures proteinligand interaction patterns through pocket-relevant attention signals rather than memorizing receptorligand pairs."
        },
        {
            "title": "2.4 Wet-lab validation on 5-HT1A receptor\nTo validate GPCR-Filter in wet-lab experiments and assess its\nability to ﬁlter GPCR modulators, a virtual screening pipeline\nwas established. In this pipeline, 1,644,833 ChemDiv com-\npounds were docked into the SEP-363856 binding pocket",
            "content": "of the 5-HT1A receptor (PDB ID: 8W8B). The top-ranked docking candidates (8,705 molecules) were then screened using GPCR-Filter; among them, 97 compounds with predicted binding probability > 0.5 were selected for purchase, and, due to availability, 52 were obtained and tested. We conducted in-vitro validation of these 52 GPCR-Filterprioritized compounds targeting the human 5-HT1A recepIn this readout, lower tor using GloSensor-cAMP assay. luminescence (LUM) indicates stronger receptor activation. 6 Forskolin served as high-LUM reference, and 5-HT as an agonist control. We ﬁrst performed single-concentration screen at 30 µM for the 52 compounds prioritized by GPCR-Filter. Four ligands (D24, D29, D34, D47) produced more pronounced LUM reductions compared to that of 5-HT, indicating strong receptor activation. These four ligands were then selected for multiple-concentration pharmacological characterization. For D24, D29, D34, and D47, concentrationresponse curves were measured and normalized to the 5-HT maximal effect (100%). All four compounds elicited sigmoidal inhibition curves consistent with Gi-coupled receptor activation. Their maximal effects (Emax) approached or exceeded that of 5-HT, though with lower apparent potency as reﬂected by right-shifted EC50 values around the micromolar level, suggesting that these compounds represent promising hits for afﬁnity optimization."
        },
        {
            "title": "3 Discussion",
            "content": "In this work, we presented GPCR-Filter, GPCR-focused interaction model trained on curated human GPCRligand dataset. On this benchmark, the model delivers strong performance under random, intra-target, and inter-target splits and maintains clear margin under the most stringent inter-target setting (Tab. 1), consistent with the designed difﬁculty gradient of these protocols  (Fig. 2)  . Wet-lab assays on 5-HT1A validated four agonists among the top-ranked predictions  (Fig. 4)  , and attention-based analyses align model decisions with crystallographic binding regions  (Fig. 3)  . Because near-ceiling performance under the random split (AUC 0.99) can partly reﬂect the memorization-prone nature of random partitioning and potential leakage, we further evaluated BemisMurcko scaffold split and GPCR-family hold-out protocol to more rigorously assess generalization. Together, these results support GPCR-Filter as practical in-silico screening component for GPCR discovery pipelines. Across all three evaluation regimes, GPCR-Filter exhibits strong representational and predictive capability. Notably, in the inter-target regime ConPLex exhibits AUC below 0.5 while maintaining accuracy above 50%, likely reﬂecting overﬁtting inside their model evaluation. We attribute this robustness to combination of high-quality data and multi-level model design. The curated GPCRligand dataset spans diverse targets and chemotypes, uses balanced supervision, and applies carefully controlled split strategies. At the dataset level, GPCRs display structured organization in ligand-proﬁle space (Fig. S2), providing plausible basis for cross-target transfer by placing unseen receptors near regions where learned chemical patterns remain applicable. On the modeling side, GPCR-Filter integrates pretrained protein language model (ESM-3) with lightweight graph neural network over ligand atoms, coupled through coordinated selfand ligandprotein cross-attention. These mechanisms allow the decoder to focus on functionally relevant residues: Top-20 attention-ranked residues are enriched near crystallographic pockets across two newly solved complexes and across all split types  (Fig. 3)  , yielding residue-level maps that offer mechanistic interpretability. This synergy between chemically structured data and attention-based fusion supports high predictive accuracy even under challenging out-ofdistribution settings. Despite these strengths, several limitations remain. Negative labels are obtained through 1:1 negative sampling and inevitably include unobserved actives; exploring alternative decoy strategies, sampling ratios, and re-sampling variance may further clarify performance bounds. In the inter-target split, ligands are allowed to recur across different targets; although the receptors themselves are disjoint, ligand reuse may provide an easier signal for certain chemotypes, and stricter ligand or scaffold-level de-duplication could further stresstest the model. Also, broader comparisons involving deeper calibration strategies or structure-aware models remain to be explored. Overall, this study demonstrates that combining curated, GPCR-speciﬁc data with multi-level fusion of protein language models and ligand graph networks yields dataefﬁcient and interpretable virtual screening model with strong out-of-distribution performance. Future work will expand validation to additional protein families, reﬁne negative sampling and calibration protocols, and incorporate structural or pocket-level constraints to further strengthen inter-target generalization. More broadly, the results suggest that chemically grounded data organizationcombined with attention mechanisms that expose ligandreceptor interaction patternsoffers promising foundation for next-generation GPCR drug discovery pipelines."
        },
        {
            "title": "4.3 GPCR embedding calculation\nFor GPCRs, we start from precomputed per-residue represen-\ntations (e.g., ESM-3 outputs [20]), of dimension ht (in our",
            "content": "7 runs ht=1536). linear projection Rht Rd maps each residue to the shared hidden size (we use d=256). The sequence is then encoded by Transformer encoder with layers (default L=2), each layer using 8 attention heads, feed-forward width of 4d, dropout 0.1, and standard residual/LayerNorm; padding mask is applied to ignore padded residues. This encoded sequence serves as the memory for cross-attention in the decoder."
        },
        {
            "title": "4.4 Ligand embedding calculation\nLigand SMILES are converted into molecular graphs G =\n(V, E) via RDKit with standard atom/bond descriptors. Node\nfeatures of size hd are ﬁrst linearly projected to the shared hid-\nden size d, then passed through a single Graph Convolutional\nNetwork (GCN) layer. At layer ℓ,",
            "content": "x(ℓ+1) d,i = σ vj (vi){vi} 1 cij W(ℓ)x(ℓ) d,j , (1) where x(ℓ) d,i denotes the node feature of atom at layer ℓ, W(ℓ) is the trainable weight matrix, σ() is ReLU, and cij is degree-based normalization term. After GCN, we prepend learnable graph-level token (used for graph-level readout) to the atom sequence and pad to the batch maximum length to form the target sequence Xd R(1+V )d with corresponding target-side padding mask (see Decoder for fusion)."
        },
        {
            "title": "4.5 Decoder and attention-based fusion\nAt this stage, we have per-residue GPCR features xt ∈\nR|S|×d (after linear projection and an encoder) and per-atom\nligand features xd ∈ R|V |×d. Before fusion, we prepend a\nlearnable graph-level token to the ligand sequence and pad\nto batch length; with this convention we overload notation\nand still write xd ∈ R(1+|V |)×d, where index 0 is the graph-\nlevel (CLS-like) token. To perform feature fusion and gen-\nerate the ﬁnal prediction, we adopt a Transformer-based de-\ncoder architecture [29]: the ligand sequence xd serves as the\ntarget/query, while the encoded protein sequence xt serves as\nthe memory (key/value). Multi-head scaled dot-product at-\ntention with residual connections, LayerNorm, dropout, and\npadding masks is used in all sublayers (equations below are\nsingle-head forms for compactness).",
            "content": "First, the ligand target undergoes self-attention to reﬁne its representation, and the protein sequence is reﬁned by encoder self-attention (written here in the same notation for brevity). In the self-attention mechanism, each feature vector attends to others within the same set: αij = exp (xdi Wq xdj ) exp (xdi Wq xdj ) , (2) where αij denotes the attention weight between ligand features xdi and xdj , and Wq, Wk are learnable projections. The updated ligand features are di = αij(xdj Wv) + xdi , d R(1+V )d. (3) 8 Similarly, protein features are updated by encoder selfattention: βij = exp (xti Wq xtj ) exp (xti Wq xtj ) RSd, , βij(xtj Wv) + xti , = x ti (4) (5) where Wv is another trainable projection. Next, the ligand acts as the query and the protein as γij = key/value in cross-attention. The attention scores are exp (x di exp (x di and the ligand features are updated by aggregating protein information: di Wq tj Wq T ) tj ) R(1+V )d. Wv) + di γij(x tj = (7) (6) , , Finally, we extract the graph-level token at index 0, d[0] Rd, and pass it through multi-layer perceptron (MLP) to obtain the ﬁnal prediction (two-class logits; we denote the scalar score by for brevity). This attention pathway enables the model to capture ligandGPCR interactions by dynamically integrating information from relevant regions on both sides; the MLP maps the fused representation to the interaction likelihood. For interpretability, we use the last-layer cross-attention weights from the ligand graph-level token to residues as residue-level importance scores."
        },
        {
            "title": "4.7 Dataset distribution analysis\nBecause the ligand set is much larger than the GPCR set,\nindividual receptors are associated with very different num-\nbers of ligands. We quantiﬁed this imbalance by examining\nhow frequently each GPCR appears in the interaction dataset.\nThe overall distribution is presented as a binned frequency-\nof-frequency histogram in the Appendix (Fig. S1), where\nGPCRs are grouped into logarithmic bins based on ligand-\noccurrence counts. This view reveals a pronounced long-\ntailed pattern: most receptors interact with only a handful\nof ligands, while a relatively small subset of GPCRs accounts\nfor a large fraction of all recorded interactions. This heavy-\ntailed structure highlights the imbalance that any predictive\nmodel must address.",
            "content": "To make the head of the distribution concrete, Table S1 lists the Top-10 most frequent receptors (by interaction counts); these highly active receptors dominate the interaction space, whereas the vast majority of GPCRs have far fewer associated ligands (as captured by the histogram)."
        },
        {
            "title": "4.8 Evaluation protocols and negative sampling\nWe consider three evaluation scenarios to comprehensively\nassess GPCR-Filter: (1) a random split for in-distribution\nperformance; (2) an intra-target split to test generalization\nto new drug combinations over seen targets; and (3) an inter-\ntarget split to evaluate cross-target generalization to entirely\nunseen GPCRs. Task difﬁculty increases from (1) to (3). All\nmethods operate on the same sequence+SMILES inputs.",
            "content": "Negative sampling (general). We evaluated GPCR-Filter under three complementary scenarios including random, intra-target, and inter-target splits to characterize both indistribution performance and the models ability to generalize across ligand and receptor space. All models operated on identical protein-sequence and SMILES inputs. Because the curated dataset contains only experimentally veriﬁed GPCRligand pairs, negative examples were generated through systematic negative sampling. For each evaluation setting, we enumerated all possible drugtarget combinations, removed known positives, and randomly sampled an equal number of negatives to maintain 1:1 class balance. Negative sampling was performed independently for each split to avoid information leakage. In the random split, all positive and sampled negative pairs were pooled, stratiﬁed by label, and partitioned into training, validation, and test subsets using an 80/10/10 ratio. This yielded 146,233 training examples, 18,279 validation examples, and 18,280 test examples. The random split measures in-distribution performance when ligands and targets follow the same global distribution across all partitions. The intra-target split evaluates generalization to unseen ligand combinations for GPCRs that appear during training. Positive pairs were grouped by receptor ID, and negative samples were generated individually for each GPCR by pairing it with all ligands and removing known positives. Targets with fewer than ten positive samples were assigned exclusively to the training set, whereas targets with sufﬁcient data were divided into training, validation, and test subsets using the same 80/10/10 policy. This procedure produced 145,613 training examples, 18,110 validation examples, and 18,225 test examples. This setting preserves target identity across all partitions and challenges models to infer new ligandtarget relationships. The inter-target split tests cross-target generalization to completely unseen GPCRs. All receptor IDs were partitioned into disjoint training and held-out sets at 9:1 ratio. Positive examples were collected according to this partition, and negative samples were generated separately within the training and held-out target pools. To maintain strict separation of receptor identities, both the validation and test sets were drawn exclusively from the held-out target subset. This resulted in 167,521 training examples, 15,271 validation examples, and 15,271 test examples. The inter-target split provides the most challenging evaluation, requiring models to transfer receptorlevel features to entirely novel GPCRs. language model (PLex) alongside contrastive learning to improve generalization to previously unseen proteins and ligands. TransformerCPI2.0[18] integrates BERT-based protein encoder (TAPE-BERT) with graph neural network for ligand representations and has shown strong performance in virtual screening and target identiﬁcation tasks. For all methods, hyperparameters, optimization strategies, and thresholding procedures were matched unless otherwise speciﬁed."
        },
        {
            "title": "4.12 GloSensor cAMP assay\nWe fused a ﬂag-tag into the N terminal of full-length 5-HT1A\nreceptor, and cloned into pcDNA 6.0. 293T cells were cul-\ntured in DMEM supplemented with 10% FBS until reach-",
            "content": "9 ing 9095% conﬂuency and then seeded into 6well plates at density of 450,000 cells per well. Prior to transfection, the medium was exchanged with DMEM containing 10% charcoal-stripped FBS. For each well, mixture containing 1.5 of the 5-HT1A receptor plasmid and 1 of the cAMP biosensor GloSensor-22F (Promega) construct was combined with 150 OPTI-MEM and 7.5 of 1 mg/ml polyethylenimine (Yeasen), incubated at room temperature for 1520 minutes, and then added to the cells. Twenty hours post-transfection, cells were washed with PBS, detached using 50 trypsin per well (12 minutes), and the digestion was halted by adding 250 DMEM with 10% charcoal-stripped FBS (Vivacell). After centrifugation, cells were resuspended with HBSS and starved at 37ˇrC for 1 hour. Following starvation, cells were pelleted, resuspended in 500 CO2-independent medium (Gibco), and counted using 20 aliquot. Cells were then mixed with the GloSensor cAMP substrate (Promega) and distributed into 384-well plates, with ﬁnal cell density 6000/well. After 1-hour incubation at 37ˇrC, ligands with different fold diluted concentrations was added per well, followed by 10-minute incubation at room temperature after brief centrifugation. Finally, Forskolin (ﬁnal concentration 1 M) was added to each well, and luminescence was measured by Envision to monitor real-time cAMP responses. single-concentration GloSensor-cAMP assay was employed to evaluate GPCR-Filter-prioritized compounds. All 52 available candidates were tested at 30 µM to identify potential receptor activators. Luminescence (LUM) signals were benchmarked against 5-HT (agonist control) and Forskolin (baseline control), and four ligands (D24, D29, D34, D47) showing signiﬁcant LUM reductions were selected for subsequent doseresponse analysis. References [1] Christian Munk, Eshita Mutt, Vignir Isberg, Louise Nikolajsen, Janne Bibbe, Tilman Flock, Michael Hanson, Raymond Stevens, Xavier Deupi, and David Gloriam. An online resource for gpcr structure determination and analysis. Nature methods, 16(2):151 162, 2019. [2] Louis Mueller, Barbara Slusher, and Takashi Tsukamoto. Empirical analysis of drug targets for nervous system disorders. ACS chemical neuroscience, 15(3):394399, 2024. Jia Duan, Xin-Heng He, Shu-Jie Li, and Eric Xu. Cryo-electron microscopy for gpcr research and drug discovery in endocrinology and metabolism. Nature Reviews Endocrinology, 20(6):349365, 2024. [3] [4] Kaitlyn Dickinson, Elliott Yee, Isaac Vigil, Richard Schulick, and Yuwen Zhu. Gpcrs: emerging targets for novel cell immune checkpoint therapy. Cancer Immunology, Immunotherapy, 73(12):253, 2024. [5] Alexander Hauser, Misty Attwood, Mathias RaskAndersen, Helgi Schiöth, and David Gloriam. Trends in gpcr drug discovery: new agents, targets Nature reviews Drug discovery, and indications. 16(12):829842, 2017. Javier Sánchez Lorente, Aleksandr Sokolov, Gavin Ferguson, Helgi Schiöth, Alexander Hauser, and David Gloriam. Gpcr drug discovery: new agents, targets and indications. Nature Reviews Drug Discovery, pages 122, 2025. [6] [7] Mayako Michino, Jeremie Vendome, and Irina Kufareva. Ai meets physics in computational structurebased drug discovery for gpcrs. npj Drug Discovery, 2(1):16, 2025. [8] Mingyang Zhang, Ting Chen, Xun Lu, Xiaobing Lan, Ziqiang Chen, and Shaoyong Lu. protein-coupled receptors (gpcrs): advances in structures, mechanisms and drug discovery. Signal transduction and targeted therapy, 9(1):88, 2024. [9] Sonja Peter, Lydia Siragusa, Morgan Thomas, Tommaso Palomba, Simon Cross, Noel OBoyle, Dávid Bajusz, Gyorgy Ferenczy, Gy\"orgy Keseru, Giovanni Bottegoni, et al. Comparative study of allosteric gpcr binding sites and their ligandability potential. Journal of Chemical Information and Modeling, 64(21):81768192, 2024. [10] Shaoyong Lu, Xinheng He, Zhao Yang, Zongtao Chai, Shuhua Zhou, Junyan Wang, Ashfaq Ur Rehman, Duan Ni, Jun Pu, Jinpeng Sun, et al. Activation pathway of protein-coupled receptor uncovers conformational intermediates as targets for allosteric drug design. Nature communications, 12(1):4721, 2021. [11] Flavio Ballante, Albert Kooistra, Stefanie Kampen, Chris de Graaf, and Jens Carlsson. Structure-based virtual screening for ligands of proteincoupled receptors: what can molecular docking do for you? Pharmacological Reviews, 73(4):16981736, 2021. 10 [12] Shimeng Guo, Tingting Zhao, Ying Yun, and Xin Xie. Recent progress in assays for gpcr drug discovery. American Journal of Physiology-Cell Physiology, 323(2):C583C594, 2022. [13] Yingzhou Lu, Yaojun Hu, and Chenhao Li. Drugclip: Contrastive drug-disease interaction for drug repurposing. arXiv preprint arXiv:2407.02265, 2024. [14] Jaemin Sim, Dongwoo Kim, Bomin Kim, Jieun Choi, and Juyong Lee. Recent advances in ai-driven proteinligand interaction predictions. Current Opinion in Structural Biology, 92:103020, 2025. [15] Luis Taracena Herrera, Søren Andreassen, Jimmy Caroli, Ismael Rodríguez-Espigares, Ali Kermani, György Keseru, Albert Kooistra, Gáspár PándySzekeres, and David Gloriam. Gpcrdb in 2025: adding odorant receptors, data mapper, structure similarity search and models of physiological ligand complexes. Nucleic acids research, 53(D1):D425D435, 2025. [16] Shi-yi Shen, Jun-rui Li, Yu-song Wang, Shao-ning Li, Eric Xu, and Xin-heng He. An update for alphafold3 versus experimental structures: assessing the precision of small molecule binding in gpcrs. Acta Pharmacologica Sinica, pages 110, 2025. [17] Lifan Chen, Xiaoqin Tan, Dingyan Wang, Feisheng Zhong, Xiaohong Liu, Tianbiao Yang, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, and Mingyue Zheng. Transformercpi: improving compoundprotein interaction prediction by sequence-based deep learning with self-attention mechanism and label reversal experiments. Bioinformatics, 36(16):44064414, 2020. [18] Lifan Chen, Zisheng Fan, Jie Chang, Ruirui Yang, Hui Hou, Hao Guo, Yinghui Zhang, Tianbiao Yang, Chenmao Zhou, Qibang Sui, et al. Sequence-based drug design as concept in computational drug design. Nature communications, 14(1):4217, 2023. [19] Rohit Singh, Samuel Sledzieski, Bryan Bryson, Lenore Cowen, and Bonnie Berger. Contrastive learning in protein language space predicts interactions between drugs and protein targets. Proceedings of the National Academy of Sciences, 120(24):e2220778120, 2023. [20] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas J. Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Q. Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul S. Molina, Neil Thomas, Yousuf A. Khan, Chetan Mishra, Carolyn Kim, Liam J. Bartie, Matthew Nemeth, Patrick D. Hsu, Tom Sercu, Salvatore Candido, and Alexander Rives. Simulating 500 million Science, years of evolution with language model. 387(6736):850858, 2025. [21] Simon Harding, Jane Armstrong, Elena Faccenda, Christopher Southan, StephenaP Alexander, Anthony Davenport, Michael Spedding, and Jamie The iuphar/bps guide to pharmacology in Davies. 2024. Nucleic Acids Research, 52(D1):D1438D1449, 10 2023. [22] David Rogers and Mathew Hahn. ExtendedJournal of Chemical Inforconnectivity ﬁngerprints. mation and Modeling, 50(5):742754, 2010. PMID: 20426451. [23] Dávid Bajusz, Anita Rácz, and Károly Héberger. Why is tanimoto index an appropriate choice for ﬁngerprintJournal of Cheminforbased similarity calculations? matics, 7(1), May 2015. [24] Fatemeh Nazem, Reza Rasti, Afshin Fassihi, Alireza Mehri Dehnavi, and Fahimeh Ghasemi. Deep attention network for identifying ligand-protein Journal of Computational Science, binding sites. 81:102368, 2024. [25] Zhijun Zhang, Lijun Quan, Junkai Wang, Liangchen Peng, Qiufeng Chen, Bei Zhang, Lexin Cao, Yelu Jiang, Geng Li, Liangpeng Nie, et al. Labind: identifying protein binding ligand-aware sites via learning interactions between ligand and protein. Nature Communications, 16(1):7712, 2025. [26] Manojkumar Jain, Ryan Gumpper, Samuel Slocum, Grant Schmitz, Jakob Madsen, Thomas Tummino, Carl Suomivuori, Xiaoqin Huang, Levi Shub, Joseph DiBerto, Kyeonghwan Kim, Carlos DeLeon, Brian Krumm, Jonah Fay, Michael Keiser, Andreas Hauser, Ron Dror, Brian Shoichet, David Gloriam, David Nichols, and Bryan Roth. The polypharmacology of psychedelics reveals multiple targets for potential therapeutics. Neuron, 113(14):31293142.e9, 2025. [27] Q.C. Gu, T.X. Wang, and W.Q. Tang. Adp-bound purinergic receptor 1 with l266p mutant in complex with minigs/q. Worldwide Protein Data Bank, Aug 2024. [28] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks, 2017. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [30] The UniProt Consortium. Uniprot: the universal protein knowledgebase in 2025. Nucleic Acids Research, 53(D1):D609D617, 11 2024. [31] Heng Liu, You Zheng, Yue Wang, Yumeng Wang, Xinheng He, Peiyu Xu, Sijie Huang, Qingning Yuan, Xinyue Zhang, Ling Wang, et al. Recognition of methamphetamine and other amines by trace amine receptor taar1. Nature, 624(7992):663671, 2023. [32] Mats HM Olsson, Chresten Søndergaard, Michal Rostkowski, and Jan Jensen. Propka3: consistent treatment of internal and surface residues in empirical predictions. Journal of chemical theory and computation, 7(2):525537, 2011."
        },
        {
            "title": "Appendix",
            "content": "Figure S1: Distribution of target occurrences per GPCR. Binned frequency-of-frequency histogram summarizing the curated GPCR ligand dataset. The x-axis represents the number of ligand occurrences per GPCR (binned), and the y-axis represents the number of GPCRs (population) within each bin. The pronounced long-tail pattern indicates that while most GPCRs interact with only few ligands, small subset of receptors accounts for disproportionately large number of interactions. Figure S2: Hierarchical clustering of GPCRs based on aggregated drug ﬁngerprints. Hierarchical clustering was performed on the GPCR set using drug-ﬁngerprint similarity proﬁles aggregated over all associated ligands. Each cell in the heatmap represents the pairwise similarity between GPCRs ligands, while the dendrograms illustrate hierarchical relationships among targets. Closely clustered receptors share comparable ligands, revealing learnable latent structure within the dataset that supports cross-target generalization in GPCR-Filter. Index UniProt ID Protein Name GPCR Family Frequency Table S1: Top-10 most frequent GPCRs in the dataset. 1 2 3 4 6 7 8 9 10 Q9HBX Relaxin receptor 1 P34972 P21453 P41145 P41594 Cannabinoid receptor 2 Sphingosine 1-phosphate receptor 1 Kappa-type opioid receptor Metabotropic glutamate receptor 5 Mu-type opioid receptor Orexin receptor type 1 GPCR 1 family GPCR 1 family GPCR 1 family GPCR 1 family GPCR 3 family GPCR 1 family GPCR 1 family Q8TDV5 Glucose-dependent insulinotropic receptor GPCR 1 family O14842 Free fatty acid receptor 1 Orexin receptor type 2 GPCR 1 family GPCR 1 family 5097 1982 1940 1917 1799 1735 1479"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Fudan University",
        "Hunan University",
        "Lingang Laboratory",
        "National Research Center for Translational Medicine at Shanghai",
        "Ruijin Hospital, Shanghai Jiao Tong University School of Medicine",
        "Shanghai Institute of Materia Medica, Chinese Academy of Sciences"
    ]
}