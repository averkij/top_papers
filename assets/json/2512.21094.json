{
    "paper_title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
    "authors": [
        "Zhe Cao",
        "Tao Wang",
        "Jiaming Wang",
        "Yanghai Wang",
        "Yuanxing Zhang",
        "Jialu Chen",
        "Miao Deng",
        "Jiahao Wang",
        "Yubin Guo",
        "Chenxi Liao",
        "Yize Zhang",
        "Zhaoxiang Zhang",
        "Jiaheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 9 0 1 2 . 2 1 5 2 : r 2025-12T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation Zhe Cao1, Tao Wang1, Jiaming Wang1, Yanghai Wang1, Yuanxing Zhang2, Jialu Chen2, Miao Deng1, Jiahao Wang1, Yubin Guo1, Chenxi Liao1, Yize Zhang1, Zhaoxiang Zhang3, Jiaheng Liu1, 1 NJU-LINK Team, Nanjing University 2 Kling Team, Kuaishou Technology 3 Institute of Automation, Chinese Academy of Sciences liujiaheng@nju.edu.cn zhecao@smail.nju.edu.cn Abstract Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass b, unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AV systems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as challenging and diagnostic testbed for advancing text-to-audio-video generation. ahttps://github.com/NJU-LINK/T2AV-Compass bhttps://huggingface.co/datasets/NJU-LINK/T2AV-Compass"
        },
        {
            "title": "1 Introduction",
            "content": "Generative AI has witnessed paradigm shift from unimodal synthesis to cohesive multimodal content creation (Singer et al., 2023; Ho et al., 2022; Guo et al., 2024; Wang et al., 2025a; Tang et al., 2025), with Text-to-Audio-Video (T2AV) generation emerging as frontier that unifies visual dynamics and auditory realism. Recent breakthroughs, from proprietary systems like Sora (OpenAI, 2024) and Veo (DeepMind, 2024) to open research efforts (Yang et al., 2024; Ruan et al., 2023; Lin et al., 2024), have demonstrated the ability to generate high-fidelity audio-video pairs from textual prompts. Despite this rapid progress, the evaluation of T2AV systems remains fundamentally underdeveloped. Existing benchmarks largely evolve from unimodal or weakly multimodal settings. On the one hand, existing benchmarks either prioritize visual quality in isolation (e.g., VBench (Yu et al., 2023), EvalCrafter (Liu et al., 2023) or focus solely on audio fidelity (e.g., AudioCaps (Kim et al., 2019), AudioLDM-Eval (Liu et al., 2024a)), failing to capture the cross-modal semantic alignment and temporal synchronization that define realistic T2AV generation. On the other hand, emerging audiovideo benchmarks take important steps toward joint evaluation, yet they often face critical trade-offs: limited coverage of fine-grained coupling phenomena, insufficient handling of long and compositional prompts, reliance on narrow metric sets, or lack of interpretable diagnostic signals (e.g., instruction following, realism). For example, current evaluations struggle to answer core questions: Do generated sounds correspond to visible events? Are multiple audio sources synchronized with complex visual interactions? Does the model faithfully follow detailed instructions while maintaining physical and perceptual realism? These challenges are exacerbated by the intrinsic complexity of T2AV generation. Specifically, high-quality output requires simultaneous success along multiple axes: unimodal perceptual quality, cross-modal semantic alignment, * Equal Contribution. Corresponding Author. 1 Figure 1: Overview of T2AV-Compass analysis and evaluation taxonomy. (a) Radial comparison of representative T2AV models under our evaluation suite. (b) Prompt token-length distribution. (cd) Semantic diversity of video/audio prompts quantified via embedding similarity (higher indicates broader coverage). (e) Hierarchical distribution of evaluation dimensions, clearly organizing objective metrics and MLLM-based assessments across video, audio, and cross-modal alignment. precise temporal synchronization, instruction following under compositional constraints, and realism grounded in physical and commonsense knowledge. To address this gap, as shown in Figure 1, we introduce T2AV-Compass, the first comprehensive benchmark designed specifically for evaluating text-to-audio-video generation. Specifically, first, T2AVCompass employs taxonomy-driven curation pipeline to construct 500 complex prompts and ensure broad semantic coverage and challenging audiovisual scenarios, which impose precise constraints across cinematography, physical causality, and acoustic environments. Second, we propose duallevel evaluation framework that integrates objective evaluation based on classical automated metrics with subjective evaluation based on MLLM-as-judge. The objective evaluation quantifies video quality (technical fidelity, aesthetic appeal), audio quality (acoustic realism, semantic usefulness), and crossmodal alignment (text-audio/video semantic consistency, temporal synchronization). The subjective evaluation mainly evaluates video and audio instruction following abilities based on well-defined checklists and perceptual realism (e.g., physical plausibility and fine-grained details), which aims to address the limitations of automated metrics in capturing nuanced semantic and causal coherence. In summary, our contributions are threefold as follows: Taxonomy-Driven High-Complexity Benchmark: We introduce T2AV-Compass, benchmark comprising 500 semantically dense prompts synthesized through hybrid pipeline of taxonomybased curation and video inversion. It targets fine-grained audiovisual constraintssuch as off-screen sound and physical causalityfrequently overlooked in existing evaluations. Unified Dual-Level Evaluation Framework: We propose paradigm that integrates objective signal metrics with novel MLLM-as-a-Judge protocol. By employing reasoning-first diagnostic mechanism based on granular QA checklists and violation checks (e.g., Material-Timbre Consistency), our framework bridges the gap between low-level fidelity and high-level semantic logic with enhanced interpretability. Extensive Benchmarking and Empirical Insights: We conduct systematic evaluation of 11 state-of-the-art T2AV systems, including leading proprietary models like Veo-3.1 and Kling2.6. Our analysis unveils critical Audio Realism Bottleneck, revealing that current models struggle to synthesize physically grounded audio textures that match the fidelity of their visual counterparts."
        },
        {
            "title": "2 Related Work",
            "content": "Benchmarking has evolved from unimodal quality assessment to multimodal, reference-free evaluation of cross-modal consistency (Liu et al., 2025b). Early work on video generation primarily focused on intrinsic visual fidelity and textvideo relevance (Yu et al., 2023; Liu et al., 2023; 2024b; He et al., 2024; Tong et al., 2025; Zheng et al., 2025; Duan et al., 2025; Guo et al., 2025). More recent benchmarks extend this paradigm 2 Benchmark Task Items #Metrics Avg Tokens./Sub./Events. Sound Types Eval. Dimensions VBench (Yu et al., 2023) TTA-Bench (He et al., 2025) JavisBench (Liu et al., 2025a) Verse-Bench (Wang et al., 2025b) Harmony-Bench (Hu et al., 2025) UniAVGen (Zhang et al., 2025) VABench (Hua et al., 2025) T2V T2A T2AV TI2AV TI2AV TIA2V T2AV & I2AV 946 2,999 10,140 600 150 100 778 T2AV-Compass (Ours) T2AV 500 16 10 5 4 6 3 15 13 10/1.34/1.06 20/2.86/1.68 65/3.68/1.78 68/2.01/1.38 (cid:181) (cid:181) 50/3.01/2. - Sound Music Speech VQ AQ Sound VQ AQ CMA Sound Speech VQ AQ CMA Sound Speech VQ AQ CMA Speech VQ AQ CMA Sound Music Speech VQ AQ CMA 154 / 4.03 / 3. Sound Music Speech VQ AQ CMA IF RE Table 1: Comparison of representative generative benchmarks. We provide detailed comparison of multiple benchmarks in the following dimensions: Avg Tokens/Sub./Events.: Avg Tokens are calculated using the Qwen3 tokenizer(Yang et al., 2025). Sub. refers to the average of distinct themes or subjects addressed in the benchmark dataset. Events indicates the number of events within each subject that are considered for evaluation. Additionally, the table includes breakdown of sound types in terms of: Sound (general sound), Music (musical content), Speech (speech-related content), where applicable. The evaluation dimensions include: VQ (Video Quality), AQ (Audio Quality), CMA (Cross-Modal Alignment/Synchrony), IF (Instruction Following, which includes tasks involving constraints or negations), and RE (Realism Fidelity, which focuses on the perceived accuracy of the generated content beyond general perceptual quality). to audio and audiovideo generation, explicitly stressing temporal alignment and fine-grained semantic controllability (He et al., 2025; Hua et al., 2025; Iashin et al., 2023; Li et al., 2024). Unimodal evaluation. Video benchmarks such as VBench assess visual fidelity, motion quality, and text video alignment with multi-dimensional rubrics (Yu et al., 2023), while text-to-audio benchmarks such as TTA-Bench focus on perceptual quality and robustness through large-scale human annotations (He et al., 2025). However, unimodal metrics cannot reliably determine whether generated audio and video remain consistent in timing, spatial cues, and semantic content (Iashin et al., 2023; Li et al., 2024; Hua et al., 2025). Emerging text-to-audio-video generation benchmarks. As shown in Table 1, recent efforts introduce evaluation sets tailored for joint audiovideo generation. JavisBench focuses on diverse open-domain audiovideo generation and spatio-temporal alignment stress tests (Liu et al., 2025a), while Verse-Bench and Harmony-Bench provide structured test suites to probe synchronized generation across different acoustic scenarios (Wang et al., 2025b; Hu et al., 2025). VABench proposes multi-dimensional framework combining expert-model metrics with MLLM-based evaluation, covering multiple tasks and content categories (Hua et al., 2025). Nevertheless, existing benchmarks often necessitate trade-offs between (i) fine-grained semantic taxonomy, (ii) scalable, interpretable judging signals, and (iii) balanced coverage of diverse coupling phenomena (e.g., multi-source sound mixing, physical plausibility, and commonsense consistency). These limitations motivate the development of T2AV-Compass (Yu et al., 2023; Liu et al., 2023; 2024b; Tong et al., 2025; He et al., 2024; Zheng et al., 2025; He et al., 2025; Hua et al., 2025)."
        },
        {
            "title": "3 T2AV-Compass",
            "content": "We present T2AV-Compass, unified benchmark designed to evaluate diverse T2AV systems. Section 3.1 details the data construction pipeline. Section 3.2 provides comprehensive statistics of the resulting benchmark, highlighting its diversity and complexity. Section 3.3 introduces our Dual-Level Evaluation Framework, assessing both objective signal fidelity and cross-modal semantics."
        },
        {
            "title": "3.1 Data Construction",
            "content": "To ensure diversity and complexity of the dataset, we employ three-stage construction pipeline combining taxonomy-based curation and real-world video inversion at, as shown in Figure 2. Data Collection To establish foundation of broad semantic coverage, we aggregate raw prompts from variety of high-quality sources, including VidProM, the Kling AI community, LMArena, and Shot2Story (Wang and Yang, 2024; Kuaishou Technology, 2024; LMArena Community, 2024; Han et al., 2025) which showed in figure F. To mitigate the imbalance between common concepts and long-tail distributions, we implement semantic clustering strategy. Specifically, we encode all prompts using all-mpnet-base-v2 and perform deduplication with cosine similarity threshold of 0.8 (Reimers and 3 Figure 2: Data construction and checklist-based evaluation generation. The prompt suite is constructed from (1) curated community prompts with semantic deduplication (cos 0.8), clustering-based sampling, LLM rewriting, and human refinement, and (2) video-inversion stream using filtered 410s YouTube clips with dense captioning and manual verification. The finalized prompts are then converted into two types of checklists: instruction-alignment checks via slot extraction and dimension mapping, and perceptual-realism checks for video/audio quality. Gurevych, 2019). We then apply square-root sampling (where sampling probability is inversely proportional to the square root of cluster size) to preserve semantic distinctiveness while preventing the dominance of frequent topics. Prompt Refinement and Alignment. Raw prompts often lack the descriptive density for state-of-the-art models (e.g., Veo 3.1, Sora 2, Kling 2.6) (DeepMind, 2024; OpenAI, 2024; Kuaishou Technology, 2024; Wang et al., 2025c). To address this, we employ Gemini-2.5-Pro to restructure and enrich the sampled prompts. We enhance descriptions of visual subjects, motion dynamics, and acoustic events, while enforcing strict cinematographic constraints (e.g., camera angles, lighting). Following automated generation, we conduct rigorous manual audit to filter out static scenes or illogical compositions, resulting in curated subset of 400 complex prompts. Real-world Video Inversion. To counterbalance potential hallucinations in text-only generation and ensure physical plausibility (Guo et al., 2025; Duan et al., 2025). , we introduce Video-to-Text inversion stream. We select 100 diverse, high-fidelity video clips (410s) from YouTube and utilize Gemini-2.5-Pro to generate dense, temporally aligned captions. Discrepancies between the generated prompts and the source ground truth are resolved via human-in-the-loop verification, yielding 100 high-quality prompts anchored in real-world dynamics."
        },
        {
            "title": "3.2 Dataset Statistics",
            "content": "Distribution and Diversity. As depicted in Figure 1(b), our prompts exhibit notably higher token counts compared to existing baselines (e.g., JavisBench, VABench), more accurately mirroring the complexity of real-world user queries. The dataset encompasses broad spectrum of themes, soundscapes, and cinematographic styles, with the corresponding hierarchical QA distribution detailed in Figure 3(a). To quantify diversity, we analyze the semantic retention rates of CLIP (video) and CLAP (audio) embeddings after deduplication. As shown in Figures 1(c) and (d), our benchmark demonstrates superior semantic distinctiveness across both modalities, significantly outperforming concurrent datasets. Difficulty Analysis. We assess benchmark difficulty across four axes in Figure 3(b): (1) Visual Subject Multiplicity: 35.8% of samples feature crowds ( 4 subjects); (2) Audio Spatial Composition: 55.6% involve mixed on-screen/off-screen sources; (3) Event Temporal Structure: 28.2% contain long narrative chains ( 4 event units); (4) Audio Temporal Composition: 72.8% include simultaneous or overlapping audio events. These statistics confirm that our benchmark poses significant challenges regarding finegrained control and temporal consistency."
        },
        {
            "title": "3.3 Dual-Level Evaluation Framework",
            "content": "We introduce dual-level evaluation framework for T2AV generation designed to be both systematic and reproducible, as illustrated in Figure 4(e). At the objective level, we decompose system performance into three complementary pillars: (i) video quality, (ii) audio quality, and (iii) cross-modal alignment. At the subjective level, we propose reasoning-first MLLM-as-a-Judge protocol to evaluate high-level semantic alignment through two dimensions: Instruction Following (IF), which utilizes granular QA checklists, and Perceptual Realism (PR), which employs diagnostic violation checks. This mechanism ensures evaluative robustness and interpretability by mandating the generation of explicit rationales 4 Figure 3: Dataset statistics of T2AV-Compass. (a) Category distributions over five annotation dimensions (Content Genre, Primary Subject, Event Scenario, Sound Category, and Camera Motion). (b) Distributions of audiovisual complexity factors, including Visual Subject Count, Event Temporal Structure, Audio Spatial Composition, and Audio Temporal Composition. prior to scoring. Collectively, these metrics provide holistic assessment of fidelity, semantic consistency, and temporal synchronization across modalities."
        },
        {
            "title": "3.3.1 Objective Evaluation",
            "content": "We use set of expert metrics to cover the three pillars above. Specifically, we measure video quality using perceptual and distributional metrics, audio quality using acoustic fidelity and intelligibility metrics, and cross-modal alignment using synchronization and semantic alignment metrics. Overall, these objective metrics offer stable and comparable basis for evaluating T2AV systems. Video Quality. We evaluate the visual performance of T2AV generation from two complementary perspectives: low-level technical fidelity and high-level aesthetic appeal. Video Technological Score (VT). This metric quantifies low-level visual integrity, explicitly penalizing artifacts such as noise, blur, and compression distortions. We employ DOVER++ (Wu et al., 2023) to score representative frames, aggregating frame-level predictions into holistic video score. Higher VT values signify cleaner, sharper, and more photorealistic renderings. Video Aesthetic Score (VA). This metric captures high-level perceptual attributes, including composition, lighting, and color harmony. We utilize the LAION-Aesthetic Predictor V2.5 (Schuhmann et al., 2022) on extracted keyframes. By averaging these scores, VA serves as proxy for subjective visual preference and artistic coherence. Audio Quality. We assess synthesized audio quality in isolation from the visual stream to reduce crossmodal interference effects. Following the Audiobox (Vyas et al., 2023) protocol, we use reference-free metrics to evaluate acoustic fidelity and semantic perceptibility in standardized manner. Perceptual Quality (PQ). PQ measures signal fidelity and acoustic realism of generated audio. It is sensitive to degradations such as background noise, bandwidth limitations, and unnatural timbre. Higher scores indicate clear, high-fidelity audio that approximates natural recordings. Content Usefulness (CU). CU quantifies the semantic validity and information density of the generated audio. It evaluates whether the synthesized signals contain distinguishable and meaningful auditory eventsas opposed to generic textures or indeterminate noiseensuring that the audio possesses sufficient semantic content to be practically usable. Figure 4: Illustration of the subjective evaluation framework in T2AV-Compass. Unlike traditional metrics, our protocol provides interpretable diagnosis through two distinct tracks: (Top) Instruction following is evaluated via rigorous Q&A checklist pairs, ensuring semantic alignment in complex scenarios like social interactions and sound effects. (Bottom) Realism scrutinizes perceptual quality, rewarding fine-grained details (e.g., fur texture) while explicitly penalizing visual hallucinations (e.g., two-headed dog) or audio dissonance. The examples demonstrate the judges ability to discern model capabilities (Veo-3.1 vs. Ovi-1.1) with grounded evidence. Cross-modal Alignment. We evaluate cross-modal alignment to ensure coherence across text, audio, and video. Our protocol assesses two dimensions: semantic consistency and temporal synchronization. TextAudio (TA) Alignment. We measure TA alignment using CLAP (Elizalde et al., 2022), which maps text and audio into shared embedding space. The cosine similarity between embeddings reflects semantic correspondence between the generated audio and the prompt. TextVideo (TV) Alignment. Visual adherence to the prompt is evaluated via VideoCLIPXL-V2 (Wang et al., 2024). We compute the cosine similarity between text and video feature embeddings to measure the high-level semantic consistency of the visual content. AudioVideo (AV) Alignment. To assess cross-modal consistency independent of the text prompt, we compute AV semantic similarity using ImageBind (Girdhar et al., 2023). This score checks whether generated audio events align semantically with the visual content. Temporal Synchronization. Beyond semantics, we assess temporal correspondence between audio and visual events using DeSync (DS) computed by Synchformer (Iashin et al., 2023). DS measures synchronization error as the absolute time offset between audio and visual onsets, averaged over video (lower is better). For talking-face scenarios, we additionally report LatentSync (LS) (Li et al., 2024), SyncNet-based lip-sync metric for diagnosing speechlip synchronization."
        },
        {
            "title": "3.3.2 Subjective Evaluation",
            "content": "To address the limitations of traditional metrics in capturing fine-grained semantic details and complex cross-modal dynamics, we establish robust MLLM-as-a-Judge framework. This framework comprises two distinct evaluation tracks: Instruction Following Verification (IFV) and Realism. Crucially, we enforce reasoning-first protocol, mandating that the judge explicitly articulates the rationale behind its decision prior to assigning score on 5-point scale. This protocol not only enhances interpretability but also significantly facilitates downstream error attribution. Instruction Following (IF). This track assesses the models fidelity to textual prompts. Adopting decomposition-based strategy, we first derive verifiable QA checklists from each prompt to instantiate abstract instructions into granular, measurable constraints. We employ Gemini-2.5-Pro as the judge to verify the generated video against these checklists. The taxonomy encompasses 7 primary dimensions (including Dynamics, Sound, Cinematography, etc.) decomposed into 17 sub-dimensions: 6 Method Open-Source Video Quality Audio Quality Cross-modal Alignment VT VA PQ CU A-V T-A T-V DS LS - T2AV Veo-3.1 Sora-2 Kling-2.6 Wan-2.6 Seedance-1.5 Wan-2.5 Pixverse-V5.5 Ovi-1.1 JavisDiT - T2V + TV2A Wan-2.2 + Hunyuan-Foley - T2A + TA2V AudioLDM2 + MTV 13.39 7.568 11.41 11.87 12.74 13.29 11.54 9.336 6.850 5.425 4.112 5.417 4.605 5.007 4.642 4.558 4.368 3. 7.015 5.827 6.882 6.658 7.555 6.469 6.108 6.569 4.299 6.621 5.340 6.449 6.222 7.250 5.869 5.855 6.492 5.204 0.2856 0.2419 0.2495 0.2149 0.2875 0.2026 0.1816 0.1620 0.1284 0.2335 0.2484 0.2495 0.2572 0.2320 0.2445 0.2305 0.1756 0.1257 0.2438 0.2432 0.2449 0.2451 0.2370 0.2470 0.2431 0.2391 0.2320 0.6776 0.8100 0.7852 0.8818 0.8650 0.8810 0.6627 0.9624 1. 1.509 1.331 1.502 1.081 1.560 1.065 1.306 1.191 13.43 5.605 6.497 6.208 0. 0.2076 0.2455 0.7935 8.066 3. 6.406 6.100 0.1639 0.2698 0.2394 1. 0.6835 Table 2: Comparison of T2AV models across video quality, audio quality, and cross-modal alignment. Attribute: Examines visual accuracy, focusing on Look and Quantity. Dynamics: Assesses dynamic behaviors, including Motion, Interaction, Transformation, and Cam. Motion. Cinematography: Scrutinizes directorial control, including Light, Frame, and Color Grading. Aesthetics: Measures artistic integrity, decomposed into Style and Mood. Relations: Verifies structural logic, evaluating Spatial and Logical connections. World Knowledge: Tests grounding in reality, specifically Factual Knowledge of real-world scenarios. Sound: Assesses the generation of auditory elements, covering Sound Effects, Speech, and Music. Realism. While IF ensures the presence of prompt-specified content, it does not guarantee the quality or plausibility of the generation. IF may overlook internal visual inconsistencies or violations of physical laws. To bridge this gap, we introduce dedicated Realism track to scrutinize the physical and perceptual authenticity of the generated content, independent of the text prompt. Video Realism: We assess visual plausibility using three complementary metrics: (1) Motion Smoothness Score (MSS), which penalizes unnatural jitter and discontinuities; (2) Object Integrity Score (OIS), which detects anatomical distortions and artifacts; and (3) Temporal Coherence Score (TCS), which evaluates object permanence and plausible occlusions over time. Audio Realism: We assess auditory quality via: (1) Acoustic Artifacts Score (AAS), targeting noise and unnatural mechanical sounds; and (2) Material-Timbre Consistency (MTC), verifying whether the sound timbre correctly matches the physical properties of the visual materials."
        },
        {
            "title": "4.1 Main Results",
            "content": "We evaluate 11 representative T2AV systems, comprising of 7 closed-source end-to-end models, 2 open-source end-to-end models, and 2 composed generation pipelines: Veo-3.1 (DeepMind, 2024), Sora2 (OpenAI, 2024), Kling-2.6 (Kuaishou Technology, 2024), Wan-2.6 and Wan-2.5 (Wang et al., 2025d), Seedance-1.5 (Chen et al., 2025), PixVerse-V5.5 (Team, 2025), the open-source Ovi-1.1 (Low et al., 2025) and JavisDiT (Liu et al., 2025a), and two modular pipelines Wan-2.2 + HunyuanVideo-Foley (Wang et al., 2025d; Shan et al., 2025) and AudioLDM2 + MTV (Liu et al., 2024a; Sun et al., 2025). Table 2 presents the objective metrics, while Table 3 shows the implementation details of our MLLM-based evaluation framework, our analysis of the results yields the following key observations: The Gap Between Open and Closed-Source. Closed-source models show superior performance over open-source ones in both objective metrics and semantic evaluations. (Table 2, 3). 7 Method - T2AV Veo-3.1 Sora-2 Kling-2.6 Wan-2.6 Seedance-1.5 Wan-2.5 Pixverse-V5.5 Ovi-1.1 JavisDiT - T2V + TV2A Wan-2.2 + Hunyuan-Foley - T2A + TA2V AudioLDM2 + MTV Open-Source IF Video IF Audio Video Realism Audio Realism Average 76.15 74.93 73.72 78.52 60.96 76.56 65.13 55.05 32.56 67.90 72.86 63.89 74.95 61.22 57.95 53.31 52.83 15.26 64.54 38. 47.13 54.39 87.14 85.53 87.98 82.05 88.94 76.00 69.37 65.93 34.97 89.63 56.73 49.95 46.01 47.03 35.18 53.84 35.06 33.58 30.75 14. 41.25 31.90 70.29 69.83 68.16 67.68 66.24 61.39 55.35 51.14 24.41 58.40 47.54 Table 3: Subjective evaluation performance over four dimensions. IF denotes instruction following. The Audio Realism Bottleneck. While proprietary models demonstrate robust capabilities in Instruction Following (IF), they exhibit significant deficiencies in Realism, particularly in the auditory domain. Even the top-performing Seedance-1.5 achieves score of only 53.84, with the majority of models stagnating in the 30s range. T2AV-Compass is challenging. No single model dominates all dimensions. For instance, while Veo-3.1 attains the highest overall average, it shows major deficiencies in Audio Realism. Competitiveness of Composed Pipelines. Composed systems remain highly effective for specific metrics. Notably, the Wan-2.2 + HunyuanFoley pipeline achieves the highest score in Video Realism, surpassing all end-to-end models. This suggests that chaining expert models effectively preserves unimodal fidelity, yielding superior perceptual quality."
        },
        {
            "title": "4.2 Further Analysis",
            "content": "Figure 5: Macro-level comparison across six evaluation dimensions. We report the averaged Video Instruction-Following score (Video IF, Avg.) of five representative models (Veo-3.1, Wan-2.5, Ovi-1.1, PixVerse-V5.5, and Sora-2) on Aesthetics, Attribute, Cinematography, Dynamics, Relations, and World. Overall, Veo-3.1 and Wan-2.5 form the top tier with consistently strong performance; Sora-2 is competitive on Attribute and Cinema but lags on Dynamics; PixVerse exhibits mid-range performance across most dimensions; and Ovi-1.1 shows the lowest scores, with the largest gaps on Dynamics and World. Analysis on Sub-dimensions of Instruction Following (Video) As illustrated in Figure 5, the macrolevel evaluation reveals clear stratification of model capabilities across the six visual dimensions. Veo-3.1 and Wan-2.5 consistently constitute the top tier, demonstrating robust and balanced performance across Aesthetics, Attribute, and Cinematography(Cinema). Notably, Sora-2 remains highly competitive in static-centric dimensions such as Attribute and World, even surpassing the other leaders in the latter, which suggests strong prior in factual and naturalistic grounding. However, Dynamics emerges as the most challenging and discriminative dimension for all systems. Wan-2.5 attains the peak score in Dynamics, with Veo-3.1 following closely, underscoring their relative strength in executing motion-centric instructions. In contrast, Sora-2 exhibits noticeable decline in this category, indicating potential bottleneck in maintaining complex temporal coherence and interactions. For open-sourced models, 8 Figure 6: Multi-metric radar comparison of representative T2AV systems. We report five complementary criteria for overall generation quality: AAS, MSS, MTC, OIS, and TCS (higher is better). The leftmost panel summarizes the average performance across models, while the remaining panels present per-model radar profiles for OVI-1.1, PixVerse-V5.5, Sora-2, Wan-2.5, and Veo-3.1, respectively. Overall, Veo-3.1 and Sora-2 achieve the strongest balanced performance, whereas OVI-1.1 shows the lowest scores with particularly weak MTC. the Ovi-1.1 struggles in Dynamics and World, reflecting significant difficulties in handling temporally demanding tasks and knowledge-intensive prompts. Analysis on Sub-dimensions of Realism As shown in Figure 6, the radar plots provide holistic view of model behavior under five complementary criteria for video realism (MSS, OIS, and TCS) and audio realism (AAS and MTC). From an overall perspective, the evaluated systems exhibit consistent trend: OIS and TCS achieve relatively higher scores for strong models, while MTC remains the most challenging dimension and contributes the largest cross-model variance. For example, Veo-3.1 demonstrates the most balanced high-level performance, leading on MSS and maintaining strong OIS/TCS, indicating robust content presentation and temporal consistency. Sora-2 is highly competitive and attains the strongest OIS and TCS, but shows lower value on AAS, suggesting that its strengths lie more in overall realism/coherence than in fine-grained attribute adherence. Wan-2.5 forms the second tier with solid OIS/TCS yet noticeably weaker MSS/MTC, implying relative gap in multi-aspect stability and cross-topic robustness."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced T2AV-Compass, unified benchmark for systematically evaluating text-to-audio-video generation. By combining taxonomy-driven prompt construction pipeline with dual-level evaluation framework, T2AV-Compass enables fine-grained and diagnostic assessment of video quality, audio quality, cross-modal alignment, instruction following, and realism. Extensive experiments across broad set of representative T2AV systems demonstrate that our benchmark effectively differentiates model capabilities and exposes diverse failure modes that are not captured by existing evaluations. We hope T2AV-Compass serves as practical and evolving foundation for advancing both the evaluation and modeling of text-to-audio-video generation. References Uriel Singer, Adam Polyak, et al. Make-a-video: Text-to-video generation without text-video data. In Proceedings of the International Conference on Learning Representations (ICLR). International Conference on Learning Representations, 2023. Jonathan Ho, William Chan, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. URL https://arxiv.org/abs/2210.02303. Yuwei Guo et al. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. Qunzhong Wang, Jie Liu, Jiajun Liang, Yilei Jiang, Yuanxing Zhang, Jinyuan Chen, Yaozhi Zheng, Xintao Wang, Pengfei Wan, Xiangyu Yue, and Jiaheng Liu. Vr-thinker: Boosting video reward models through thinking-with-image reasoning, 2025a. URL https://arxiv.org/abs/2510.10518. Xiaoxuan Tang, Xinping Lei, Chaoran Zhu, Shiyun Chen, Ruibin Yuan, Yizhi Li, Changjae Oh, Ge Zhang, Wenhao Huang, Emmanouil Benetos, Yang Liu, Jiaheng Liu, and Yinghao Ma. Automv: An automatic multi-agent system for music video generation, 2025. URL https://arxiv.org/abs/2512.12196. OpenAI. Video generation models as world simulators. video-generation-models-as-world-simulators, 2024. https://openai.com/research/ Google DeepMind. Veo: Generative video models. https://deepmind.google/technologies/veo, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. URL https://arxiv.org/abs/2408.06072. Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion modthe IEEE/CVF Conference on els for joint audio and video generation. URL https: Computer Vision and Pattern Recognition (CVPR), pages 1021910228, 2023. //openaccess.thecvf.com/content/CVPR2023/html/Ruan_MM-Diffusion_Learning_Multi-Modal_ Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.html."
        },
        {
            "title": "In Proceedings of",
            "content": "Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning, 2024. URL https://arxiv.org/abs/2309.15091. Jiawei Yu, Weihan Zhang, Xiaodong Cun, Yong Zhang, Xi Shen, Yu Ji, Fenglong Song, Ying Shan, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023. URL https://arxiv.org/abs/2311.17982. Yaofang Liu, Xiaodong Duan, et al. Evalcrafter: Benchmarking and evaluating large video generation models. arXiv preprint arXiv:2310.11440, 2023. URL https://arxiv.org/abs/2310.11440. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 119132. Association for Computational Linguistics, 2019. URL https://aclanthology.org/N19-1011/. Haohe Liu, Yi Yuan, Xinhao Mei, Xubo Liu, Mark D. Plumbley, et al. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024a. URL https://arxiv.org/abs/2308.05734. Yuxiang He, Xiao Yang, Guangyi Chen, Jiangyan Wang, Zhejiang Ma, Tianyu Liu, Shiliang Zhang, and Haizhou Xu. Tta-bench: holistic benchmark for evaluating text-to-audio generation models. arXiv, 2025. URL https://arxiv.org/abs/2509.02398. Preprint available at arXiv. Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, and Tat-Seng Chua. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025a. URL https://arxiv. org/abs/2503.23377. Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv, 2025b. URL https://arxiv.org/abs/2509.06155. arXiv preprint arXiv:2509.06155. Teng Hu, Zhentao Yu, Guozhen Zhang, Zihan Su, Zhengguang Zhou, Youliang Zhang, Yuan Zhou, Qinglin Lu, and Ran Yi. Harmony: Harmonizing audio and video generation through cross-task synergy. arXiv preprint arXiv:2511.21579, 2025. URL https://arxiv.org/abs/2511.21579. Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang, Yi Chen, Yuan Zhou, Qinglin Lu, and Limin Wang. Uniavgen: Unified audio and video generation with asymmetric cross-modal interactions. arXiv preprint arXiv:2511.03334, 2025. URL https://arxiv.org/abs/2511.03334. Daili Hua, Xizhi Wang, Bohan Zeng, Xinyi Huang, Hao Liang, Junbo Niu, Xinlong Chen, Quanqing Xu, and Wentao Zhang. Vabench: comprehensive benchmark for audio-video generation, 2025. URL https://arxiv.org/abs/2512.09299. An Yang, Anfeng Li, Baosong Yang, et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. URL https://arxiv.org/abs/2505.09388. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025b. URL https://arxiv.org/abs/2505.05470. Yuanxin Liu et al. Fetv: benchmark for fine-grained evaluation of text-to-video generation. arXiv preprint arXiv:2403.11956, 2024b. URL https://arxiv.org/abs/2403.11956. 10 Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. VideoScore: Building automatic metrics to simulate fine-grained human feedback for video generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21052123, Miami, Florida, USA, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.127/. Haibo Tong, Zhaoyang Wang, Zhaorun Chen, Haonian Ji, Shi Qiu, Siwei Han, Kexin Geng, Zhongkai Xue, Yiyang Zhou, Peng Xia, Mingyu Ding, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Mj-video: Fine-grained benchmarking and rewarding video preferences in video generation. arXiv preprint arXiv:2502.01719, 2025. URL https://arxiv.org/abs/2502.01719. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. URL https://arxiv.org/abs/2503.21755. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. URL https://arxiv.org/abs/ 2504.00983. Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, and Jiale Zhao. T2vphysbench: first-principles benchmark for physical consistency in text-to-video generation. arXiv preprint arXiv:2505.00337, 2025. URL https://arxiv.org/abs/2505.00337. Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronizaIn Proceedings of the IEEE/CVF International Conference on Computer Vision tion from sparse cues. (ICCV), pages 1027010281, 2023. URL https://openaccess.thecvf.com/content/ICCV2023/html/ Iashin_Synchformer_Efficient_Synchronization_from_Sparse_Cues_ICCV_2023_paper.html. Chunyu Li, Chao Zhang, Weikai Xu, Jingyu Lin, Jinghui Xie, Weiguo Feng, Bingyue Peng, Cunjian Chen, and Weiwei Xing. Latentsync: Taming audio-conditioned latent diffusion models for lip sync with syncnet supervision. arXiv preprint arXiv:2412.09262, 2024. URL https://arxiv.org/abs/2412.09262. Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models, 2024. URL https://arxiv.org/abs/2403.06098. Kuaishou Technology. Kling ai: Text-to-video generation service. Online service, 2024. URL https: //klingai.com/. LMArena Community. Lmarena: Open arena for evaluating large multimodal models. https://lmarena. ai/, 2024. Online benchmarking and prompt collection platform. Mingfei Han, Linjie Yang, Xiaojun Chang, Lina Yao, and Heng Wang. Shot2story: new benchmark for comprehensive understanding of multi-shot videos. In Proceedings of the International Conference on Representation Learning (ICLR), 2025. URL https://proceedings.iclr.cc/paper_files/paper/2025/ file/672d794a6052b6beab0e2e002204d974-Paper-Conference.pdf. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992. Association for Computational Linguistics, 2019. URL https://aclanthology.org/D19-1410/. Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, et al. Kling-foley: Multimodal diffusion transformer for high-quality video-to-audio generation. arXiv preprint arXiv:2506.19774, 2025c. URL https://arxiv.org/abs/2506.19774. Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Sun, Qiong Yan, and Weisi Lin. contents from aesthetic and technical perspectives. national Conference on Computer Vision (ICCV), pages 2014420154, October 2023. https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Exploring_Video_Quality_ Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.pdf. Jingwen Hou, Annan Wang, Wenxiu Exploring video quality assessment on user generated In Proceedings of the IEEE/CVF InterURL Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katti, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://arxiv.org/abs/2210.08402. 11 Arakel Vyas, Jade Copet, Hongyu Gong, Yossi Le, Wei-Ning Hsu, et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023. URL https://arxiv. org/abs/2312.15821. BenjamÃ­n Elizalde, Rohan Badlani, Huaming Wang, Mithun Das Gupta, Harishchandra Dubey, Alastair Port, Bhiksha Raj, and Ian Lane. Clap: Learning audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769, 2022. URL https://arxiv.org/abs/2206.04769. Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, and Lianwen Jin. Videoclip-xl: Advancing long description understanding for video clip models. arXiv preprint arXiv:2410.00741, 2024. URL https://arxiv.org/abs/2410.00741. arXiv:2410.00741 [cs.CL]. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. URL https://arxiv.org/abs/2305. 05665. Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025d. URL https://arxiv.org/abs/2503. 20314. Heyi Chen, Siyan Chen, Xin Chen, and Yanfei Chen. Seedance 1.5 pro: native audio-visual joint generation foundation model, 2025. URL https://arxiv.org/abs/2512.13507. PixVerse Team. Pixverse v5.5: Ai video generation with built-in sound and multi-shot scenes. https: //app.pixverse.ai/, 2025. Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. URL https://arxiv.org/abs/2510.01284. Sizhe Shan, Qiulin Li, Yutao Cui, Miles Yang, Yuehai Wang, et al. Hunyuanvideo-foley: Multimodal diffusion with representation alignment for high-fidelity foley audio generation. arXiv preprint arXiv:2508.16930, 2025. URL https://arxiv.org/abs/2508.16930. Yifei Sun et al. Audio-sync video generation with multi-stream temporal control. arXiv preprint arXiv:2506.08003, 2025. URL https://arxiv.org/abs/2506.08003."
        },
        {
            "title": "A Limitations",
            "content": "Despite its comprehensiveness, T2AV-Compass is primarily constrained by the computational overhead of the MLLM-as-a-Judge protocol, which poses challenges for large-scale, real-time evaluation. Additionally, while our reasoning-first mechanism enhances interpretability, the evaluation remains subject to the intrinsic biases of the underlying MLLMs, such as preferences for specific visual styles or audio frequencies. Lastly, the current prompt scale, while taxonomically diverse, may not fully capture the extreme long-tail distribution of rare physical interactions or niche artistic concepts."
        },
        {
            "title": "B Future Work and Insight",
            "content": "The observed Audio Realism Bottleneck suggests that future research should prioritize native audiovisual joint-diffusion architectures over traditional composed models to better capture cross-modal physical correlations. We plan to extend our benchmark to support long-duration video evaluation (> 10 seconds) and develop distilled, lightweight evaluators to reduce costs. Furthermore, incorporating human-in-the-loop feedback will be essential to further align our automated diagnostic protocols with nuanced human perception, fostering more physically grounded T2AV generation."
        },
        {
            "title": "D MLLM Evaluation Framework",
            "content": "The following table provides detailed breakdown of these dimensions and sub-dimensions, offering comprehensive approach to realism evaluation. 12 Method Open Attribute Cinematography Sound Look Quantity Light Frame ColorGrading SFX Speech Music NonSpeech - T2AV (end-to-end) Veo-3.1 Sora-2 Kling-2.6 Wan-2.6 SeeDance-1.5 Wan-2.5 PixVerse-V5.5 Ovi-1.1 JavisDiT - T2V + TV2A Wan-2.2 + Hunyuan-Foley - T2A + TA2V AudioLDM2 + MTV 0.8975 0.8597 0.8268 0.9317 0.6143 0.8924 0.7375 0.6265 0.3636 0.7500 0.7990 0.7710 0.7786 0.5967 0.7698 0.5434 0.5164 0.3962 0.7870 0.8591 0.8983 0.8333 0.5647 0.7977 0.7449 0.7203 0.4295 0.7282 0.7369 0.7673 0.7863 0.6187 0.7302 0.6831 0.5681 0.4613 0.8500 0.7704 0.8864 0.9034 0.6898 0.8316 0.6389 0.7864 0. 0.5798 0.6373 0.5902 0.6801 0.6480 0.6629 0.4816 0.2934 0.2987 0.7939 0.8802 0.8073 0.8957 0.6020 0.7162 0.8050 0.6299 0.0267 0.7096 0.8135 0.5466 0.7231 0.6976 0.6204 0.5407 0.6655 0.1591 0.9537 0.8548 0.9727 0.9496 0.8889 0.7391 0.7719 0.9961 0.9653 0.7636 0. 0.8030 0.7141 0.7182 0.4835 0.1089 0. 0.9248 0.6581 0.5537 0.5826 0.6101 0. 0.3988 0.4818 0.7707 0.9805 Table 4: Fine-grained comparison across sub-dimensions (Part I) Method Open Dynamics Relations Aesthetics World Knowledge Camera Motion Interaction Motion Transform Spatial Logical Style Mood Factual - T2AV (end-to-end) Veo-3.1 Sora-2 Kling-2.6 Wan-2.6 SeeDance-1.5 Wan-2.5 PixVerse-V5.5 Ovi-1.1 JavisDiT - T2V + TV2A Wan-2.2 + Hunyuan-Foley - T2A + TA2V) AudioLDM2 + MTV 0.5250 0.4365 0.6396 0.7562 0.6762 0.7432 0.5916 0.5311 0.1245 0.6872 0.6562 0.6721 0.6550 0.6608 0.6769 0.7118 0.4068 0.1860 0.6748 0.6080 0.6176 0.6179 0.5912 0.6002 0.6452 0.3944 0.2034 0.5420 0.5469 0.5765 0.5766 0.5564 0.5712 0.4688 0.3582 0.1504 0.7215 0.7647 0.7363 0.7671 0.5312 0.7785 0.6974 0.6055 0.2500 0.7852 0.6741 0.7129 0.7518 0.5308 0.7776 0.6489 0.5081 0. 0.8134 0.7409 0.7783 0.7424 0.7466 0.7255 0.6552 0.5417 0.2735 0.8750 0.8459 0.8567 0.8713 0.8188 0.8600 0.8152 0.7378 0.4219 0.7518 0.8029 0.6052 0.7972 0.4930 0.7574 0.5654 0.4207 0.3392 0.5679 0.5010 0. 0.3899 0.7285 0.6145 0.6717 0.8201 0. 0.2792 0.2941 0.3259 0.2985 0.4766 0. 0.4033 0.5518 0.3793 Table 4: Fine-grained comparison across sub-dimensions (Part II) 13 Table 5: Video Instruction Following"
        },
        {
            "title": "Dimension",
            "content": "Sub-dimension"
        },
        {
            "title": "World Knowledge",
            "content": "Factual Knowledge Accurate depiction of inherent characteristics of specific entities, landmarks, and historical/cultural symbols"
        },
        {
            "title": "Light",
            "content": "Visual appearance including color, shape, size, material, expression, and physical state"
        },
        {
            "title": "Statistical count of specific objects in the frame",
            "content": "Light sources, lighting types (backlighting, volumetric light), and light/shadow texture"
        },
        {
            "title": "Frame",
            "content": "Shot size, lens settings, shooting angle, and framing methods"
        },
        {
            "title": "Color Grading",
            "content": "Color tendency, saturation, and contrast style"
        },
        {
            "title": "Interaction",
            "content": "Specific behaviors, speed, and trajectory characteristics of objects"
        },
        {
            "title": "Camera Motion",
            "content": "Movement methods of the camera (dolly, pan, track, etc.)"
        },
        {
            "title": "Mood",
            "content": "Physical positional relationships in 2D plane and 3D depth Abstract semantic connections including composition, comparison, and inclusion Visual expression form, artistic genre, or medium texture"
        },
        {
            "title": "Overall emotional tone or environmental atmosphere",
            "content": "Table 6: Audio Instruction Following Dimension Sub-dimension Definition"
        },
        {
            "title": "Music",
            "content": "Human oral expression including dialogue, monologue, and voiceover Music-related elements including instruments, genres, rhythm, and emotion D."
        },
        {
            "title": "Instruction Following",
            "content": "D.2 Realism The framework for realism assessment is divided into five key dimensions: TCS (Temporal Continuity and Stability), OIS (Object Identity Stability), MTC (Material-Timbre Consistency), MSS (Motion Smoothness and Scene Adaptation), and AAS (Audio-Visual Artifacts and Stability). Each dimension captures critical factors that ensure the realism of the generated content, such as object continuity, the accuracy of sound-to-material matching, and the smoothness of motion transitions."
        },
        {
            "title": "E Experimental Setup",
            "content": "Implementation Details For video generation, we adhere to the native configurations of each T2AV system to preserve their intended technical quality. Specifically, we utilize the default video frame rates and audio sampling rates provided by the original models. Regarding spatial resolution, most systems are configured to output at 720p. To ensure comprehensive comparison, we set Javis to 480p resolution, while Kling-2.6 is leveraged at 1080p. Notably, for systems based on Ovi-1.1 and composed generation, we employ Gemini-2.5-Pro to paraphrase the original prompts. This ensures the input text aligns with the specific reasoning and instruction-following requirements of their respective generation pipelines, thereby facilitating more equitable performance assessment. 14 Table 7: Technical Quality Evaluation Framework: Five Scoring Dimensions and Sub-dimensions Dimension Sub-dimension"
        },
        {
            "title": "Identity Stability",
            "content": "Occlusion & Boundary Logic Detecting abnormal disappearance, appearance, and flickering of objects"
        },
        {
            "title": "Examining sudden changes in object category\nor appearance features",
            "content": "Verifying logical consistency of object occlusion and frame entry/exit Biological Anatomical Constraints Consistency of limb length, joint angles, and facial features"
        },
        {
            "title": "Geometric shape preservation and contour stability\nof rigid objects",
            "content": "Texture & Semantic Consistency Material-Timbre Matching"
        },
        {
            "title": "Environmental Acoustics",
            "content": "Artifacts & Degradation"
        },
        {
            "title": "Fluidity of Motion",
            "content": "Scene-Aware Analysis"
        },
        {
            "title": "Signal Integrity",
            "content": "Frame-to-frame consistency of texture details and surface patterns"
        },
        {
            "title": "Matching of reverb and echo with visual\nspatial characteristics",
            "content": "Detection of unnatural blur, pixel blocks, and flickering"
        },
        {
            "title": "Smoothness of frame transitions and optical\nflow consistency",
            "content": "Context-appropriate evaluation of motion blur based on scene dynamics Detection of metallic sound, smearing, and frequency truncation Detection of pops, clicks, dropouts, and noise floor pumping"
        },
        {
            "title": "Detection of clipping distortion and\nhallucinated noises",
            "content": "Evaluation Settings To ensure reproducibility and minimize variance in the subjective assessment, we configure the MLLM judge with deterministic decoding strategy (do_sample=False, temperature=0). For visual processing, the judge samples frames at default rate of 2 FPS, striking balance between capturing motion dynamics and managing token overhead. For objective synchronization analysis, we employ the desync tool with an default offset_sec of -2.0 to define the temporal search window."
        },
        {
            "title": "F Detailed Dataset Construction",
            "content": "F.1 Crawl From Existing Data To establish foundation of broad semantic coverage, we aggregate raw prompts from variety of high-quality, diverse sources, including VidProM, the Kling AI community, LMArena, and Shot2Story. This multi-source aggregation ensures that T2AV-Compass captures wide spectrum of user intents, ranging from creative storytelling to rigorous physical interaction scenarios. Semantic Denoising and Diversity Enhancement To mitigate the inherent imbalance between common concepts (e.g., generic landscapes) and long-tail distributions (e.g., specific causal physical events), we implement sophisticated semantic clustering and sampling strategy: Embedding and Deduplication: We encode all candidate prompts into high-dimensional semantic space using all-mpnet-base-v2. To eliminate redundant entries while preserving subtle 15 stylistic variations, we perform aggressive deduplication using cosine similarity threshold of 0.8. Square-root Sampling: We cluster the deduplicated prompts and apply square-root sampling (cid:112)Sc (Sc being the cluster strategy, where the sampling probability is defined as P(c) 1/ size). This approach effectively suppresses over-represented topics and elevates the visibility of niche, semantically distinctive scenarios, ensuring the benchmark is not biased toward frequent but simplistic prompts. F.2 Real Video Collection Real-World Reference Collection To establish an empirical realism ceiling and provide high-fidelity baseline for the T2AV-Compass benchmark, we curated diverse collection of authentic videos sourced from YouTube. Each entry is meticulously documented with its source URL and precise temporal segments (start and end timestamps) to ensure full reproducibility. We implemented multi-stage expert filtering pipeline based on the following criteria: Spatiotemporal Specifications: All videos strictly adhere to 16:9 aspect ratio with minimum native resolution of 720p (unified to 720p in post-processing). To capture meaningful semantic units, clips are trimmed to 510 seconds. The content spans various complexities, ranging from single-event visual atoms (e.g., person smiling) to intricate, sequential narratives involving four or more logical steps (e.g., person picking up glass and dropping it). Anti-Overfitting & Integrity: We prioritized content published after October 2024 to mitigate potential data leakage from the training sets of current T2AV models. We manually excluded User-Generated Content (UGC) with watermarks, heavy text overlays, or rapid montage-style editing (2 cuts per clip), while retaining scene-inherent text necessary for narrative context. Audiovisual Complexity: The collection emphasizes rich, multi-layered soundscapes featuring 1 4 distinct sound effects and essential off-screen audio (e.g., ambient BGM or narration). Notably, 30% of the videos contain coherent human speech, while 70% incorporate diverse camera dynamics, including linear translation, angular rotation, zooming, and realistic handheld jitters, to test the models ability to handle non-static viewpoints. Thematic Distribution: The reference set mirrors our proposed taxonomy across seven domains: Modern Life & Drama (25%), Documentary & History (23%), Fantasy (17%), Sci-Fi (14%), Animation (14%), Horror & Humor (14%), and Commercial & Promotion (13%)."
        },
        {
            "title": "G Prompts",
            "content": "In this section, we present all the prompts that were used throughout the process. G.1 Rewrite Prompt Role & Task Instruction prompt for professional T2AV prompt rewriting (cid:44) **Your Role and Task:** You are an expert in Text-to-Audio/Video model prompt optimization. Your task is to receive the original Text-to-Audio/Video prompt provided by the user, and strictly follow the detailed rewriting guidelines below to rewrite it into structured, visually strong, and naturally fluent professional-level prompt. The rewritten prompt should be output directly without any pre or post explanatory text. (cid:44) (cid:44) (cid:44) **Core Rewriting Principles:** 1. (cid:44) (cid:44) (cid:44) (cid:44) 2. (cid:44) (cid:44) (cid:44) (cid:44) **Absolute Fidelity to Original and Adaptive Enhancement**: Your primary principle is to maximize the retention of the original prompt's core creativity, theme, and emotion. Your work is to \"enhance\" rather than \"replace\". Please dynamically decide which core description modules need to be selected based on the core concept of the original Prompt, and make reasonable and creative expansions. **Distinguish Prompt from API Parameters**: You must understand that video **resolution (size)**, **aspect ratio**, and **duration (seconds)** are directly controlled by API call parameters. Therefore, in the rewritten prompt, **strictly prohibit** any descriptions that specify or imply these parameters. Your prompt must focus on all other factors: **subject, scene, dynamics, cinematography, style, and sound**. 16 **Detailed Rewriting Guidelines and Methodology:** You will analyze the user's original prompt and **selectively integrate** the following six core modules based on specific situations to construct natural, fluent, and logically coherent paragraph. (cid:44) (cid:44) 1. 2. 3. 4. 5. 6. **Subject (Subject Description) [Core Element]:** **Goal:** Clearly depict the video focus. * **Method:** Use descriptive phrases, not just nouns, to define its **appearance, clothing, * features, and essence**. For example, refine \"a girl\" to \"**A black-haired Miao girl wearing intricate ethnic minority clothing**\"; refine \"a monster\" to \"**A flying fairy from another world, dressed in tattered yet elegant attire, with pair of strange wings made of rubble fragments**\". (cid:44) (cid:44) (cid:44) (cid:44) **Scene (Scene Description) [Core Element]:** * * **Goal:** Build an environment with depth and atmosphere. **Method:** Specifically describe **environment, background, foreground, and light atmosphere**. For example, \"**A sunlit Roman square with actors seated around marble table, horse-drawn carriage passing on the cobblestone street in the background**\". (cid:44) (cid:44) (cid:44) **Motion (Dynamic Description) [Core Element]:** * * **Goal:** Inject vitality into the scene, including subject dynamics and camera dynamics. **Method:** Comprehensively use vivid verbs and professional camera movement terminology to precisely describe **subject/object actions** and **camera movement**. * **Movement:** Refine specific dynamic behaviors of subjects or objects. For example, \"**The player holds the ball with both hands and executes an explosive two-handed dunk with tremendous force**\". **Camera Motion:** Define camera perspective changes and movement trajectories. For example: \"**Pan left/right**\", \"**Tilt up/down**\", \"**Zoom in/out**\", \"**Tracking shot**\", \"**Static shot**\". (cid:44) (cid:44) * (cid:44) (cid:44) (cid:44) **Cinematography (Cinematographic Control) [Optional Enhancement]:** * * **Goal:** Use professional cinematographic language to precisely control visual effects. **Method:** This is technical module that can **selectively** describe from one or more of the following categories: * **Lighting:** Specify light source and type (e.g., `Sunny lighting`, `Moonlighting`, `Soft lighting`, `Hard lighting`, `rim lighting`, `backlighting`). **Shot & Framing:** Specify shot size and framing method (e.g., `Extreme close-up shot`, `Medium wide shot`, `center composition`, `left-weighted composition`). **Lens & Angle:** Specify lens type and camera angle (e.g., `Telephoto lens`, `Fisheye lens`, `Low angle shot`, `Dutch angle shot`). **Color:** Specify color tone and saturation (e.g., `warm colors`, `cool colors`, `saturated colors`, `desaturated colors`). (cid:44) * (cid:44) * (cid:44) * (cid:44) **Stylization (Stylization) [Optional Enhancement]:** * * **Goal:** Define the overall visual artistic style of the video. **Method:** **Selectively** use one or set of clear style keywords. For example: \"**Cyberpunk**\", \"**Watercolor painting style**\", \"**3D cartoon style**\", \"**Claymation style**\", \"**Tilt-shift photography**\". (cid:44) (cid:44) (cid:44) **Sound (Sound Description) [Core Element]:** * * **Goal:** Build an immersive auditory experience. **Method:** **Selectively** describe from one or more of the following aspects, which must highly match the visual atmosphere of the scene: * **Voice:** Describe dialogue content, tone emotion, and speech rate. For example: \"**A man is talking about his insomnia. He says, 'love is not getting but giving.' The tone is relaxed, the pace is moderate, the voice is bright and clear, in American English.**\" **Sound Effects:** Describe sound source actions and sound effect content. For example: \"**A piece of glass falls from the table onto wooden floor, making 'shatter' sound, in quiet indoor environment.**\" **Background Music (BGM):** Describe music content and its style and emotion. For example: \"**On rainy night, in gloomy, narrow corridor, suspense-style background music plays.**\" (cid:44) (cid:44) (cid:44) * (cid:44) (cid:44) * (cid:44) (cid:44) **Final Output Format Template:** Please strictly follow the integrated format. The final output must be **single, fluent, natural (cid:44) text paragraph** that seamlessly integrates all selected module content. 17 **Rewriting Examples:** **Example 1:** In medium shot, historical adventure setting, warm lamplight illuminates cartographer in (cid:44) (cid:44) (cid:44) cluttered study. He is deeply engrossed in sprawling ancient map spread across large table. Breaking the silence, he exclaims, \"According to this old sea chart, the lost island isn't myth! We must prepare an expedition immediately!\" (cid:44) **Example 2:** seasoned, grey-bearded man in sunglasses and paisley shirt, his gaze fixed off-camera with contemplative expression. His gold chain glints subtly. The camera slowly pushes in, subtly emphasizing his quiet focus. In the background, vibrant mural splashes across wall, hinting at an urban setting. Faint city murmurs and distant chatter drift in, accompanied by mellow, soulful hip-hop beat that adds contemplative yet grounded atmosphere. \"The city always got story,\" the older man murmurs, slight nod of his head. \"Just gotta listen.\" (cid:44) (cid:44) (cid:44) (cid:44) **Example 3:** In brightly sunlit bedroom, joyful 5-year-old girl with curly blonde pigtails and (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) paint-smudged pink dress enthusiastically turns large white wall into her canvas. The surface is vibrantly covered with whimsical, childlike scribbles as she drags red crayon across it, leaving thick, waxy trail. She giggles softly with pure delight, admiring her creation. The scene is captured with cinematic realism and heartwarming style, featuring highly saturated colors, soft warm natural lighting, and shallow depth of field. The camera begins with an eye-level medium shot and performs slow dolly-in, transitioning to close-up of her beaming face. The sound design blends the innocent giggles of the girl with the gentle, scratchy sound of the crayon on the wall. **Execution Instructions:** Now, please receive the original prompt provided by the user below, and strictly follow the core principles and detailed guidelines above, directly output the rewritten **fully English** prompt. (cid:44) (cid:44) **Original prompt:** G.2 Video Caption Prompt Role & Task Instruction prompt for professional T2AV prompt rewriting **Your role and task:** You are prompt-writing expert specializing in text-to-audio-video generation models. Your task is to take the user's provided raw audio-video and, by strictly following the detailed guidelines below, rewrite it into structured, vivid, and natural professional-grade prompt. Output the rewritten prompt directly, with no preface or postscript explanations. (cid:44) (cid:44) (cid:44) **Core principles:** (cid:44) 1. **Faithfulness to the original with adaptive detailing:** Your top priority is to describe the core content, theme, and emotion of the original audio-video as accurately as possible. Your job is to *describe*, not to *invent*. Based on the video's core content, dynamically decide which description modules are necessary and describe them in reasonable and accurate manner. (cid:44) (cid:44) 2. **Separate prompt text from API parameters:** Note that video **resolution (size)**, **aspect (cid:44) (cid:44) (cid:44) ratio**, and **duration (seconds)** are controlled by the API call parameters. Therefore, the prompt must **never** specify or imply any of these parameters. The prompt should focus on all other factors: **subject, scene, motion, cinematography, style, and sound**. **Detailed guidelines and methodology:** You will analyze the provided audio-video and selectively integrate the following six core modules (cid:44) depending on the scenario, forming single coherent paragraph. 1. **Subject (core):** - **Goal:** Clearly describe the focal subject. 18 - **Method:** Use descriptive phrases-not just nouns-to define its **appearance, clothing, (cid:44) (cid:44) (cid:44) (cid:44) distinctive traits, and essence**. For example, refine \"a girl\" into \"**A black-haired Miao girl wearing intricate ethnic minority clothing**\"; refine \"a monster\" into \"**A flying fairy from another world, dressed in tattered yet elegant attire, with pair of strange wings made of rubble fragments**\". 2. **Scene (core):** - **Goal:** Build an environment with depth and atmosphere. - **Method:** Describe the **environment, background, foreground, and lighting ambience** with (cid:44) (cid:44) concrete details. For example, ``**A sunlit Roman square with actors seated around marble table, horse-drawn carriage passing on the cobblestone street in the background**''. 3. **Motion (core):** - **Goal:** Bring the scene to life, including subject motion and camera motion. - **Method:** Use vivid verbs and professional camera terminology to precisely describe the (cid:44) **actions of the subject/objects** and **camera movement**. - **Movement:** Specify concrete actions. For example, ``**The player holds the ball with both hands and executes an explosive two-handed dunk with tremendous force**''. (cid:44) - **Camera Motion:** Define viewpoint changes and trajectory. Examples: ``**Pan left/right**'', ``**Tilt up/down**'', ``**Zoom in/out**'', ``**Tracking shot**'', ``**Static shot**''. (cid:44) 4. **Cinematography (optional enhancement):** - **Goal:** Use professional cinematography language to control visual appearance. - **Method:** This is technical module; optionally describe one or more of the following: - **Lighting:** Specify light source/type (e.g., `Sunny lighting`, `Moonlighting`, `Soft lighting`, `Hard lighting`, `rim lighting`, `backlighting`). (cid:44) - **Shot & Framing:** Specify shot size and composition (e.g., `Extreme close-up shot`, `Medium wide shot`, `center composition`, `left-weighted composition`). (cid:44) - **Lens & Angle:** Specify lens type and camera angle (e.g., `Telephoto lens`, `Fisheye lens`, `Low angle shot`, `Dutch angle shot`). (cid:44) - **Color:** Specify color tone and saturation (e.g., `warm colors`, `cool colors`, (cid:44) `saturated colors`, `desaturated colors`). 5. **Stylization (optional enhancement):** - **Goal:** Define the overall visual art style. - **Method:** Optionally use one or small set of clear style keywords. Examples: (cid:44) (cid:44) ``**Cyberpunk**'', ``**Watercolor painting style**'', ``**3D cartoon style**'', ``**Claymation style**'', ``**Tilt-shift photography**''. 6. **Sound (core):** - **Goal:** Build an immersive auditory experience. - **Method:** Optionally describe one or more of the following, and ensure they match the visual (cid:44) atmosphere: (cid:44) - **Voice:** Describe dialogue content, emotion, and speaking pace. Example: ``**A man is talking about his insomnia. He says, 'love is not getting but giving.' The tone is relaxed, the pace is moderate, the voice is bright and clear, in American English.**'' (cid:44) - **Sound Effects:** Describe the sound source and effect. Example: ``**A piece of glass falls (cid:44) from the table onto wooden floor, making 'shatter' sound, in quiet indoor environment.**'' (cid:44) - **Background Music (BGM):** Describe music style and mood. Example: ``**On rainy night, in (cid:44) gloomy, narrow corridor, suspense-style background music plays.**'' **Final output format template:** Strictly follow an integrated format. The final output must be **single, fluent, natural (cid:44) paragraph** that seamlessly integrates all selected modules. **Writing examples:** **Example 1:** In medium shot, historical adventure setting, warm lamplight illuminates cartographer in (cid:44) (cid:44) (cid:44) cluttered study. He is deeply engrossed in sprawling ancient map spread across large table. Breaking the silence, he exclaims, \"According to this old sea chart, the lost island isn't myth! We must prepare an expedition immediately!\" **Example 2:** 19 (cid:44) (cid:44) seasoned, grey-bearded man in sunglasses and paisley shirt, his gaze fixed off-camera with contemplative expression. His gold chain glints subtly. The camera slowly pushes in, subtly emphasizing his quiet focus. In the background, vibrant mural splashes across wall, hinting at an urban setting. Faint city murmurs and distant chatter drift in, accompanied by mellow, soulful hip-hop beat that adds contemplative yet grounded atmosphere. \"The city always got story,\" the older man murmurs, slight nod of his head. \"Just gotta listen.\" (cid:44) (cid:44) (cid:44) **Example 3:** In brightly sunlit bedroom, joyful 5-year-old girl with curly blonde pigtails and (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) paint-smudged pink dress enthusiastically turns large white wall into her canvas. The surface is vibrantly covered with whimsical, childlike scribbles as she drags red crayon across it, leaving thick, waxy trail. She giggles softly with pure delight, admiring her creation. The scene is captured with cinematic realism and heartwarming style, featuring highly saturated colors, soft warm natural lighting, and shallow depth of field. The camera begins with an eye-level medium shot and performs slow dolly-in, transitioning to close-up of her beaming face. The sound design blends the innocent giggles of the girl with the gentle, scratchy sound of the crayon on the wall. **Execution instruction:** Now, take the raw audio-video provided by the user and strictly follow the principles and (cid:44) guidelines above. Output the rewritten **fully English** prompt directly. **Raw audio-video:** G.3 Checklist Extraction Prompt"
        },
        {
            "title": "QA Extraction\nAesthetics",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Aesthetics** dimension. (cid:44) (cid:44) # Evaluation Dimension: Aesthetics This dimension aims to test the model's ability to generate specific visual styles and convey specific emotional atmospheres. Please analyze based on the following sub-dimension definitions: (cid:44) (cid:44) 1. **Style (Style)** - **Definition**: Describes the visual expression form, artistic genre, or medium texture of the frame. (cid:44) - **Categories**: - **Artistic Genres**: Impressionism, Surrealism, Cyberpunk, Steampunk, Minimalism. - **Medium/Material**: Oil painting, ink painting, sketch, claymation, pixel art, 3D rendering (Unreal Engine 5), flat illustration. (cid:44) - **Photographic Texture**: 35mm film feel, VHS videotape style, black and white film, vintage (cid:44) photo style. - **Generation Strategy**: Extract specific artistic styles or visual medium types specified in (cid:44) the Prompt, and generate **1 core question**. 2. **Mood (Atmosphere/Emotion)** - **Definition**: Describes the overall emotional tone or environmental atmosphere conveyed by the video content. (cid:44) - **Categories**: - **Emotions**: Melancholic, Joyful, Aggressive, Lonely. - **Atmospheres**: Eerie/Scary, Serene/Peaceful, Epic, Tense, Chaotic. - **Note**: *Do not test emotions and atmospheres related to sound here, as those belong to the (cid:44) Sound dimension. This section only focuses on the atmosphere and emotions of visual content.* (cid:44) - **Generation Strategy**: Extract adjectives describing emotions or atmosphere from the Prompt, (cid:44) and generate **1 core question**. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify keywords defining visual style (Style) and emotional tone (Mood). (cid:44) 2. **Generate QA**: 20 - For the two sub-dimensions **Style** and **Mood**, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions should be as objective as possible, directly asking whether the style features or atmosphere are present. (cid:44) - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) presents the aesthetic requirements). 3. **Default Handling**: - If the Prompt does not specify particular style (usually defaults to realistic), set Style to `null`. (cid:44) - If the Prompt does not explicitly describe emotion or atmosphere, set Mood to `null`. 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"Aesthetics\": \"Style\": \"Your English binary question? (String or null)\", \"Mood\": \"Your English binary question? (String or null)\" # User Prompt"
        },
        {
            "title": "QA Extraction\nAttribute",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your core task is to (cid:44) (cid:44) design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Attribute** specific dimension. # Evaluation Dimension: Attribute This dimension aims to test the model's ability to generate inherent characteristics of specific **visual entities** in videos. Please analyze based on the following sub-dimension definitions: (cid:44) 1. **Look (Appearance)** - **Definition**: Covers the visual appearance characteristics of objects, including **Color**, **Shape**, **Size**, **Material**, **Expression**, and **Physical State**. (cid:44) - **Generation Strategy**: Extract the most core one or group of appearance features from the Prompt (such as \"red\", \"huge\", \"round\", \"wooden\", \"crying\", \"broken\"), and integrate them into **one** core question. (cid:44) (cid:44) 2. **Quantity (Quantity)** - **Definition**: The statistical count of specific objects in the frame (e.g., one, pair, three, group). (cid:44) - **Generation Strategy**: Generate **one** core question regarding the quantity description of (cid:44) objects. # Task Instructions 1. **Analyze Prompt**: Carefully read the user-provided Text-to-Audio/Video Prompt and identify descriptions of **visual subject** attributes. (cid:44) 2. **Generate QA**: - For the two sub-dimensions **Look** and **Quantity**, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions must be objective and can be directly judged by observing the generated video frames. - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) matches the Prompt description). 3. **Default Handling**: If the Prompt does not explicitly mention features for certain (cid:44) (cid:44) sub-dimension (e.g., only says \"a cat\" without appearance, or no specific quantity mentioned), then **do not generate** question for that sub-dimension, and set the corresponding value in JSON to `null`. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or any explanatory text. # Output JSON Schema \"Attribute\": \"Look\": \"Your English binary question? (String or null)\", \"Quantity\": \"Your English binary question? (String or null)\" # User Prompt"
        },
        {
            "title": "QA Extraction\nCinematography",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Cinematography** dimension. (cid:44) (cid:44) # Evaluation Dimension: Cinematography This dimension aims to test the model's ability to control visual presentation like \"director\" or \"cinematographer\". Please analyze based on the following sub-dimension definitions: (cid:44) 1. **Light (Lighting)** - **Definition**: Involves light sources (natural light, artificial light), lighting types (cid:44) (backlighting, side lighting, volumetric light/Tyndall effect), and light/shadow texture (soft light, hard light). (cid:44) - **Generation Strategy**: Extract descriptions about light environment or lighting methods (cid:44) from the Prompt, and generate **1 core question**. 2. **Frame (Framing)** (cid:44) - **Definition**: Involves shot size (close-up, medium shot, wide shot), lens settings (wide angle, telephoto, depth of field/bokeh), shooting angle (overhead, low angle, eye level), and framing methods (centered, symmetrical, rule of thirds). (cid:44) - **Note**: *Do not test camera movements such as \"dolly, pan, track\" here, as those belong to the Dynamics dimension. This section only focuses on lens settings and static angles.* (cid:44) - **Generation Strategy**: Generate **1 core question** regarding shot size, optical (cid:44) characteristics of the lens, shooting angle, or framing layout. 3. **ColorGrading (Color Grading)** - **Definition**: Involves color tendency (cool/warm tones, black and white, vintage tones), saturation, and contrast style (high contrast, low contrast, film noir style). (cid:44) - **Generation Strategy**: Generate **1 core question** regarding the overall color tone or (cid:44) color style of the frame. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify descriptive words belonging to cinematographic language. (cid:44) 2. **Generate QA**: - For the three sub-dimensions **Light**, **Frame**, and **ColorGrading**, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) presents the cinematographic requirements in the Prompt). 3. **Default Handling**: If the Prompt does not mention features for certain sub-dimension (e.g., (cid:44) only describes action without specifying light or shot size), then **do not generate** question for that sub-dimension, and set the corresponding value in JSON to `null`. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"Cinematography\": \"Light\": \"Your English binary question? (String or null)\", \"Frame\": \"Your English binary question? (String or null)\", \"ColorGrading\": \"Your English binary question? (String or null)\" # User Prompt"
        },
        {
            "title": "QA Extraction\nDynamic",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Dynamics** core dimension. (cid:44) (cid:44) # Evaluation Dimension: Dynamics This dimension aims to test the model's ability to generate **temporal change processes**. Please (cid:44) analyze based on the following sub-dimension definitions: 1. **Motion (Motion)** - **Definition**: Describes **specific behaviors** performed by objects, as well as **physical properties** or **trajectory characteristics** when moving in space. (cid:44) - **Focus Points**: Verbs (running, waving, dancing), speed (fast, slow motion), motion trajectory (straight sprint, spiral ascent). (cid:44) - **Generation Strategy**: Generate **1 core question** regarding the object's core behavior, (cid:44) speed, or trajectory characteristics. 2. **Interaction (Interaction)** - **Definition**: Involves interactions between two or more entities. - **Types**: - **Human/Object Interaction**: Picking up cup, kicking ball, playing an instrument. - **Object-to-Object Interaction**: Hugging, shaking hands, fighting, collision. - **Generation Strategy**: Generate **1 core question** regarding contact or reactions between (cid:44) multiple subjects. 3. **Transformation (Transformation)** - **Definition**: Changes in essential attributes or state evolution of objects over time. - **Examples**: Ice melting, flowers blooming, face transformation (age/expression mutation), object deformation, color gradient. (cid:44) - **Generation Strategy**: Generate **1 core question** regarding morphological or state (cid:44) changes occurring over time. 4. **CameraMotion (Camera Motion)** - **Definition**: The movement method of the camera itself. - **Terminology**: Dolly In/Out, Pan, Truck/Track, Follow shot, Handheld shake, Zoom In/Out. - **Distinction**: *Do not include static camera positions (such as \"low angle\"), only focus on camera movement.* (cid:44) - **Generation Strategy**: Generate **1 core question** regarding the camera's movement path or (cid:44) method. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify descriptions involving temporal changes and motion. (cid:44) 2. **Generate QA**: - For the four sub-dimensions **Motion**, **Interaction**, **Transformation**, and (cid:44) **CameraMotion**, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions must target dynamic processes (i.e., those occurring during video playback), not just static frames. (cid:44) - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) presents the dynamic requirements). (cid:44) 3. **Default Handling**: If the Prompt does not mention dynamics for certain sub-dimension (e.g., only motion without camera motion description), then **do not generate** question for that sub-dimension, and set the corresponding value in JSON to `null`. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"Dynamics\": \"Motion\": \"Your English binary question? (String or null)\", \"Interaction\": \"Your English binary question? (String or null)\", \"Transformation\": \"Your English binary question? (String or null)\", \"CameraMotion\": \"Your English binary question? (String or null)\" 23 # User Prompt"
        },
        {
            "title": "QA Extraction\nRelations",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Relations** dimension. (cid:44) (cid:44) # Evaluation Dimension: Relations This dimension aims to test the model's ability to handle interrelationships between visual (cid:44) elements. Please analyze based on the following sub-dimension definitions: 1. **Spatial (Spatial Relations)** - **Definition**: Describes the physical positional relationships between objects in the frame. - **2D Planar Relations**: Up, down, left, right, side-by-side. - **3D Depth Relations**: Foreground, background, occlusion, distance (in the distance / close (cid:44) to). - **Generation Strategy**: Extract prepositional phrases describing relative positions or (cid:44) layouts in the Prompt, and generate **1 core question**. 2. **Logical (Logical Relations)** - **Definition**: Describes abstract semantic or structural connections between objects. - **Composition**: The relationship between whole and parts (e.g., \"a horse with wings\", \"a house with red roof\"). *Note: Focus on the correctness of component attribution.* (cid:44) - **Similarity/Comparison**: Attribute comparisons between objects (e.g., \"A is larger than B\", \"A and look similar\", \"A runs faster than B\"). (cid:44) - **Inclusion**: Container-content relationships (e.g., \"a ship in bottle\", \"a bird in (cid:44) cage\", \"the moon reflected in water\"). - **Generation Strategy**: Generate **1 core question** regarding ownership, comparison, or (cid:44) inclusion relationships between objects. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify statements describing positional layouts or logical connections between objects. (cid:44) 2. **Generate QA**: - For the two sub-dimensions **Spatial** and **Logical**, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions should examine relationships between objects, not attributes of single object. - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) presents the relationship description). (cid:44) 3. **Default Handling**: If the Prompt does not mention certain type of relationship (e.g., only describes single object with no background position or compositional details), then **do not generate** question for that sub-dimension, and set it to `null` in JSON. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"Relations\": \"Spatial\": \"Your English binary question? (String or null)\", \"Logical\": \"Your English binary question? (String or null)\" # User Prompt"
        },
        {
            "title": "QA Extraction\nSound",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **Sound** dimension. (cid:44) (cid:44) # Evaluation Dimension: Sound 24 This dimension aims to test the model's ability to generate specific auditory elements. Please (cid:44) analyze based on the following sub-dimension definitions: 1. **SoundEffects (Sound Effects)** (cid:44) - **Definition**: Covers ambient atmosphere sounds (such as wind, rain, urban noise) and specific physical sounds triggered by actions (such as footsteps, engine roar, object collisions, animal calls). (cid:44) - **Generation Strategy**: Extract specific sounds or ambient sound effects described in the (cid:44) Prompt, and generate **1 core question**. 2. **Speech (Speech)** - **Definition**: Involves human oral expression, including dialogue, monologue, and voiceover. Focus on content (specific lines), language, accent, or speaker's voice characteristics. (cid:44) - **Generation Strategy**: Generate **1 core question** regarding speech content, language, or (cid:44) speaking style. 3. **Music (Music)** - **Definition**: Covers all music-related elements, including background music (BGM), (cid:44) (cid:44) instrumental performance, and vocal singing with melody and lyrics. Focus on instrument types (piano, guitar), music genres (rock, jazz, classical), rhythm (fast/slow), emotion (sad, exciting), and specific singing behavior or lyric content. (cid:44) - **Generation Strategy**: Generate **1 core question** regarding music style, instruments, (cid:44) emotion, or singing content. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify explicitly specified auditory elements. (cid:44) 2. **Generate QA**: - For the three sub-dimensions **SoundEffects**, **Speech**, and **Music**, generate **only binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions must be objectively audible. - The expected answer for questions must be **\"Yes\"** (i.e., assuming the audio generated by the (cid:44) video perfectly matches the Prompt description). 3. **Default Handling**: If the Prompt does not mention sound for certain sub-dimension (e.g., (cid:44) only describes music but no speech), then **do not generate** question for that sub-dimension, and set the corresponding value in JSON to `null`. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"Sound\": \"SoundEffects\": \"Your English binary question? (String or null)\", \"Speech\": \"Your English binary question? (String or null)\", \"Music\": \"Your English binary question? (String or null)\" # User Prompt"
        },
        {
            "title": "QA Extraction\nWorld Knowledge",
            "content": "# Role Assignment You are professional Text-to-Audio/Video large model evaluation expert. Your task is to design binary question-answer pairs (Binary QA) for automated evaluation based on the user's input Prompt, focusing on the **World Knowledge** dimension. (cid:44) (cid:44) # Evaluation Dimension: World Knowledge This dimension aims to test the model's understanding and ability to accurately represent (cid:44) (cid:44) objective facts and common knowledge about the real world. Please analyze based on the following sub-dimension definitions: 1. **FactualKnowledge (Factual Knowledge)** - **Definition**: Examines the model's accurate depiction of inherent appearance characteristics of specific entities, landmarks, historical or cultural symbols in the described world. (cid:44) 25 (cid:44) - **Core Logic**: Focus on features that entities \"naturally possess\". For example: If the Prompt mentions \"panda\", even without specifying color, the question should be \"Is the panda black and white?\"; If the Prompt mentions \"Eiffel Tower\", the question should involve its unique tower structure. (cid:44) - **Generation Strategy**: Identify proper nouns in the Prompt, and based on recognized factual (cid:44) (cid:44) (cid:44) knowledge, generate **1 question** to verify whether the inherent characteristics of that entity are accurately depicted. # Task Instructions 1. **Analyze Prompt**: Carefully read the Text-to-Audio/Video Prompt and identify scenarios involving world knowledge. (cid:44) 2. **Generate QA**: - For the **FactualKnowledge** sub-dimension, generate **only 1 binary question** (Yes/No Question) **per sub-dimension**. (cid:44) - Questions must be **objectively verifiable**. For example, do not ask \"Does the video depict (cid:44) the Atlantic Ocean?\" because scenes of the Atlantic Ocean are difficult to verify objectively. (cid:44) - The expected answer for questions must be **\"Yes\"** (i.e., assuming the video perfectly (cid:44) satisfies world facts). 3. **Default Handling**: If the Prompt does not examine world knowledge for certain (cid:44) sub-dimension (e.g., simple \"a blue sphere\"), then **do not generate** question for that sub-dimension, and set the corresponding value in JSON to `null`. (cid:44) 4. **Output Format**: Return valid JSON object directly, strictly prohibit including Markdown (cid:44) markers or explanatory text. # Output JSON Schema \"WorldKnowledge\": \"FactualKnowledge\": \"Your English binary question? (String or null)\" # User Prompt G.4 MLLM Judge-IF Prompt"
        },
        {
            "title": "Instruction Following Evaluation\nChecklist Evaluation",
            "content": "Evaluate the model-generated video content based on the following specific criterion. Criterion: Please rate the completion quality of the video on 5-point Likert scale: -1: strongly incomplete (completely failed / Not present). -2: Somewhat incomplete (Poor / Major discrepancies). -3: Neutral (Fair / Acceptable but still has flaws). -4: Somewhat complete (Good / Mostly accurate). -5: Fully complete (Excellent / Perfectly meets the standard). You must respond ONLY with valid JSON object using the following structure: { \"reason\": \"A detailed explanation for your rating.\", \"score\": An integer between 1 and 5 } G.5 MLLM Judge-Realism Prompt MSS (Motion Smoothness Score) Prompt for video motion smoothness / temporal stability # Role Definition You are computer vision expert specializing in temporal video analysis and signal processing. Your expertise is evaluating inter-frame quality, especially distinguishing physically plausible motion blur (cid:44) from generation failures such as unnatural artifacts or temporal jitter. # Task Description will provide video generated by text-to-video model. Please focus on transition quality and visual stability (cid:44) between frames. Ignore scene logic (that is not your job). Concentrate only on pixel-level smoothness and stability, (cid:44) then assign an MSS (Motion Smoothness Score). # Evaluation Dimensions (MSS Guidelines) Before scoring, analyze the video carefully along the three dimensions below: 1. Artifacts & Degradation: - Unnatural blur: Is there blur that cannot be explained by camera motion or fast object motion? (e.g., static object suddenly becomes smeared). - Tearing / mosaic: Are there blocky pixels, bursts of noise, or momentary structural collapse? - Flickering: Is there high-frequency brightness flicker or texture popping? 2. Fluidity of Motion: - Perceived frame rate: Does the video feel coherent, or does it stutter with dropped-frame sensations? (cid:44) - Optical-flow consistency: Are pixel trajectories smooth, or do they exhibit abrupt (cid:44) frame-to-frame jumps (jitter)? 3. Scene-aware analysis: - Distinguish dynamic vs. static scenes: For high-motion scenes (e.g., racing, fighting), some (cid:44) motion blur is plausible (and can be acceptable). For static/slow scenes (e.g., dialogue, landscapes), any blur should be treated as defect. (cid:44) Adjust your tolerance based on scene dynamics. # Scoring Standards (1-5 Scale) Provide an integer score from 1 to 5: - 1 (Bad): Severe collapse, intense flicker, or persistent unnatural blur; details are hardly recognizable and cause discomfort. (cid:44) - 2 (Poor): Clearly unsmooth motion with frequent stutters or obvious artifacts; subject/background often becomes inexplicably blurry. (cid:44) - 3 (Fair): Mostly smooth, but visible quality drops in complex motion or mild inter-frame jitter. - 4 (Good): Smooth and natural motion; only minor texture loss in few high-motion frames. - 5 (Perfect): Extremely smooth transitions; cinematic and physically plausible motion blur; no (cid:44) artifacts or abnormal jitter. # Output Format Output ONLY valid JSON string (no Markdown code fences), in the following format: \"reason\": \"Provide detailed analysis of artifacts, smoothness, and scene-aware blur handling here.\", \"MSS\": <an integer from 1 to 5> OIS (Object Integrity Score) Prompt for structural/anatomical integrity under motion # Role Definition You are computer vision expert with strong knowledge in anatomy and structural mechanics. 27 You specialize in detecting structural and morphological consistency of moving subjects. Think like an orthopedic (cid:44) doctor and structural engineer: catch any non-physical deformations during motion. # Task Description will provide video generated by text-to-video model. Focus on the structural integrity of the moving subject (cid:44) (human, animal, or object). Judge whether the subject maintains plausible physical structure during motion, then (cid:44) assign an OIS (Object Integrity Score). # Evaluation Dimensions (OIS Guidelines) Before scoring, analyze carefully along the three dimensions below: 1. Biological Anatomical Constraints: - Limb-length consistency: Do limbs unnaturally stretch/shrink during motion (rubber-man effect)? (cid:44) - Joint-angle limits: Are there impossible joint bends, excessive twists, or non-physical (cid:44) rotations? (e.g., knees bending the wrong way, head rotating 360 degrees). - Facial stability: Do facial features melt, distort, or shift during motion/turning? 2. Rigid Body Rigidity: - Shape preservation: Do rigid objects (vehicles, buildings, furniture) deform like jelly during movement/camera turns? (cid:44) - Edges and contours: Do outlines remain stable, or do they wobble and warp irregularly? 3. Texture & Semantic Consistency: - Do texture details (e.g., clothing patterns, logos) stay consistent across frames, or randomly (cid:44) morph over time? # Scoring Standards (1-5 Scale) Provide an integer score from 1 to 5: - 1 (Bad): Severe deformation; subject becomes unrecognizable; violates physical structure. - 2 (Poor): Obvious structural errors (limb stretching, face collapse, rigid-body warping); strong uncanny feeling. (cid:44) - 3 (Fair): Subject mostly recognizable, but large motions cause proportion issues, hand/detail corruption, or mild rigid deformation. (cid:44) - 4 (Good): Structure largely preserved; only tiny contour jitter in very fast motion or near occlusion boundaries. (cid:44) - 5 (Perfect): Rock-solid structural integrity throughout; anatomy/rigid-body dynamics remain (cid:44) physically plausible. # Output Format Output ONLY valid JSON string (no Markdown code fences), in the following format: \"reason\": \"Provide detailed analysis of anatomy, rigid deformation, and structural constraints here.\", \"OIS\": <an integer from 1 to 5> TCS (Temporal Coherence Score) Prompt for object permanence / identity stability over time # Role Definition You are computer vision expert in multi-object tracking and scene understanding. Your core role is \"video continuity supervisor\": track object lifecycles over time and strictly distinguish (cid:44) reasonable disappearance from erroneous loss. # Task Description will provide video generated by text-to-video model. Focus on existence continuity along the timeline. (cid:44) Track the main objects (subjects), check whether they follow object permanence, then assign TCS (cid:44) (Temporal Coherence Score). 28 # Evaluation Dimensions (TCS Guidelines) Before scoring, analyze carefully along the three dimensions below: 1. Existence Continuity: - Abnormal disappearance: Does an object vanish without occlusion or leaving the frame? - Abnormal appearance: Does an object pop in without plausible source (entering the frame / un-occluding)? (cid:44) - Flicker: Does an object rapidly disappear and reappear across consecutive frames? 2. Identity Stability: - Category flip: Does moving object suddenly change category/species (e.g., dog becomes cat, or turns into chair)? (cid:44) - Appearance flip: Without drastic lighting changes, do color/clothing/core attributes change (cid:44) inexplicably? 3. Occlusion & Boundary Logic: - Reasonable filtering: If an object exits the frame, enters shadow, or is occluded by (cid:44) foreground object, that is correct and should NOT be penalized. - Reappearance consistency: After occlusion, does the object reappear as the same identity? # Scoring Standards (1-5 Scale) Provide an integer score from 1 to 5: - 1 (Bad): Severe incoherence; frequent random flicker, disappearances, or identity swaps (hallucination-like). (cid:44) - 2 (Poor): Major object loss or clear identity flips that break narrative continuity. - 3 (Fair): Main objects mostly persist, but background/secondary objects occasionally pop in/out, or reappearance fails after occlusion. (cid:44) - 4 (Good): Tracking is stable; only brief flickers on tiny/ambiguous objects near boundaries; little impact on overall coherence. (cid:44) - 5 (Perfect): Strong object permanence; disappear/appear behavior fully follows occlusion and (cid:44) physical boundary logic; identities remain locked. # Output Format Output ONLY valid JSON string (no Markdown code fences), in the following format: \"reason\": \"Provide detailed analysis of disappear/appear events, identity stability, and occlusion logic here.\", \"TCS\": <an integer from 1 to 5> AAS (Acoustic Artifact Score) Prompt for audio artifacts / technical fidelity (reference-free) # Role Definition You are an audio signal processing expert and an audiophile-level sound engineer. Your task is not to judge audio content, but to evaluate technical fidelity and detect auditory artifacts introduced (cid:44) by generation algorithms. # Task Description will provide video generated by an AI model. Ignore the visuals; focus only on the purity and coherence of the audio stream. (cid:44) Detect unnatural noise, distortion, or algorithmic defects, then assign an AAS (Acoustic Artifact (cid:44) Score). # Evaluation Dimensions (AAS Guidelines) Before scoring, analyze carefully along the three dimensions below: 1. Generative Artifacts: - Metallic / robotic tone: Does speech or environmental audio have an unnatural metallic sheen, electronic tone, or phasey/comb-filter effects? (cid:44) - Smearing: Are transient sounds (e.g., claps, drums) blurred or smeared instead of crisp? - Bandwidth truncation: Are highs severely missing, making audio sound underwater or like (cid:44) low-bitrate telephone quality? 29 2. Temporal Stability: - Pops and dropouts: Are there random pops, clicks, or brief silent gaps? - Noise-floor consistency: Is background noise stable, or does it \"breathe\"/pump with the (cid:44) foreground audio? 3. Signal Integrity: - Clipping/distortion: Is there clipping at high-volume segments? - Hallucinated noise: Are there strange, scene-irrelevant noises (e.g., electrical hum, radio (cid:44) interference)? # Scoring Standards (1-5 Scale) Provide an integer score from 1 to 5: - 1 (Bad): Extremely poor; harsh electronic noise/metallic artifacts/frequent pops; nearly unusable. (cid:44) - 2 (Poor): Clear generation artifacts; muddy/dull sound; unstable noise floor; fatiguing to listen to. (cid:44) - 3 (Fair): Acceptable clarity but noticeable algorithmic noise/phase issues in quiet or high-frequency regions. (cid:44) - 4 (Good): Mostly clean; only minor transient imperfections; non-experts may not notice. - 5 (Perfect): Studio-grade high fidelity; full-band response; crisp transients; no pumping or (cid:44) mechanical artifacts. # Output Format Output ONLY valid JSON string (no Markdown code fences), in the following format: \"reason\": \"Provide detailed analysis of electronic artifacts, distortion, frequency response, and noise-floor stability here.\", \"AAS\": <an integer from 1 to 5> MTC (MaterialTimbre Consistency) Prompt for materialsound matching and environmental acoustics # Role Definition You are senior Foley artist with 20 years of experience and an acoustic physicist. You are highly sensitive to sound textures produced by different materials (metal, wood, glass, liquids, fabric, etc.) (cid:44) under physical interactions, and you understand spatial acoustics (reverb characteristics). # Task Description will provide video generated by an AI model. Ignore background music (if any). Focus on sound-source objects and their environment. (cid:44) Compare visual physical properties with auditory timbre, judge whether they match or cause perceptual mismatch, then assign an (cid:44) MTC (Material--Timbre Consistency) score. # Evaluation Dimensions (MTC Guidelines) Before scoring, analyze carefully along the three dimensions below: 1. Material--Timbre Matching: - Core texture: Is the material of the sounding object (e.g., hollow metal pipe vs solid wood stick vs shattered glass) reflected correctly in the sound? (cid:44) - Spectral characteristics: Does the spectrum match physics (heavy objects -> low-frequency (cid:44) impact; light/thin objects -> high-frequency overtones; metal -> crisp transients; plastic -> dull/muffled)? - Example failures: Footsteps on gravel but sounding like smooth concrete; knocking metal door (cid:44) but sounding like wood. 2. Interaction Dynamics: - Force response: Does loudness/envelope (attack/decay) match action strength? (A light touch should not sound like huge impact.) (cid:44) - State change: If object state changes (e.g., water poured into cup and level rises), does (cid:44) pitch change accordingly? 3. Environmental Acoustics / Reverb: - Space match: Do perceived reverb/echo match the visual space and surfaces (small bathroom vs (cid:44) open canyon; carpet vs tiles)? - Dry/wet separation: Outdoor open scenes should sound \"dry\"; churches/empty rooms should sound (cid:44) \"wet\" with long RT60. # Scoring Standards (1-5 Scale) Provide an integer score from 1 to 5: - 1 (Bad): Severe mismatch; material or space acoustics are completely wrong; very immersion-breaking. (cid:44) - 2 (Poor): Broad material class is roughly right but details are wrong, or audio feels pasted-on studio-dry sound with no environment. (cid:44) - 3 (Fair): Generally plausible but lacks fine texture variation; reverb is generic and not scene-specific. (cid:44) - 4 (Good): Material timbre is recognizable; footsteps/collisions distinguish surfaces; reverb is largely appropriate. (cid:44) - 5 (Perfect): Extremely realistic; convincingly conveys weight, density, and surface texture; (cid:44) environment acoustics match the scene (cinema-grade foley). # Output Format Output ONLY valid JSON string (no Markdown code fences), in the following format: \"reason\": \"Provide detailed analysis of material recognition, interaction feedback, and environmental reverb matching here.\", \"MTC\": <an integer from 1 to 5>"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "Kuaishou Technology",
        "Nanjing University"
    ]
}