{
    "paper_title": "Inductive Moment Matching",
    "authors": [
        "Linqi Zhou",
        "Stefano Ermon",
        "Jiaming Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), a new class of generative models for one- or few-step sampling with a single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256x256 with 1.99 FID using only 8 inference steps and achieves state-of-the-art 2-step FID of 1.98 on CIFAR-10 for a model trained from scratch."
        },
        {
            "title": "Start",
            "content": "Linqi Zhou 1 Stefano Ermon 2 Jiaming Song 1 5 2 0 2 1 1 ] . [ 2 5 6 5 7 0 . 3 0 5 2 : r Figure 1. Generated samples on ImageNet-256256 using 8 steps."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion models and Flow Matching generate high-quality samples but are slow at inference, and distilling them into few-step models often leads to instability and extensive tuning. To resolve these trade-offs, we propose Inductive Moment Matching (IMM), new class of generative models for oneor few-step sampling with single-stage training procedure. Unlike distillation, IMM does not require pre-training initialization and optimization of two networks; and unlike Consistency Models, IMM guarantees distribution-level convergence and remains stable under various hyperparameters and standard model architectures. IMM surpasses diffusion models on ImageNet-256256 with 1.99 FID using only 8 inference steps and achieves state-ofthe-art 2-step FID of 1.98 on CIFAR-10 for model trained from scratch. 1Luma AI 2Stanford University. Correspondence to: Linqi Zhou <alexzhou@lumalabs.ai>. 1 Generative models for continuous domains have enabled numerous applications in images (Rombach et al., 2022; Saharia et al., 2022; Esser et al., 2024), videos (Ho et al., 2022a; Blattmann et al., 2023; OpenAI, 2024), and audio (Chen et al., 2020; Kong et al., 2020; Liu et al., 2023), yet achieving high-fidelity outputs, efficient inference, and stable training remains core challenge trilemma that continues to motivate research in this domain. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b), one of the leading techniques, require many inference steps for high-quality results, while step-reduction methods, such as diffusion distillation (Yin et al., 2024; Sauer et al., 2025; Zhou et al., 2024; Luo et al., 2024a) and Consistency Models (Song et al., 2023; Geng et al., 2024; Lu & Song, 2024; Kim et al., 2023), often risk training collapse without careful tuning and regularization (such as pre-generating data-noise pair and early stopping). To address the aforementioned trilemma, we introduce Inductive Moment Matching (IMM), stable, single-stage training procedure that learns generative models from scratch for singleor multi-step inference. IMM operates on the time-dependent marginal distributions of stochastic interpolants (Albergo et al., 2023) continuous-time stochastic processes that connect two arbitrary probability Inductive Moment Matching density functions (data at = 0 and prior at = 1). By learning (stochastic or deterministic) mapping from any marginal at time to any marginal at time < t, it can naturally support oneor multi-step generation (Figure 2). IMM models can be trained efficiently from mathematical induction. For time < < t, we form two distributions at by running one-step IMM from samples at and t. We then minimize their divergence, enforcing that the distributions at are independent of the starting time-steps. This construction by induction guarantees convergence to the data distribution. To help with training stability, we model IMM based on certain stochastic interpolants and optimize the objective with stable sample-based divergence estimators such as moment matching (Gretton et al., 2012). Notably, we prove that Consistency Models (CMs) are single-particle, first-moment matching special case of IMM, which partially explains the training instability of CMs. On ImageNet-256256, IMM surpasses diffusion models and achieves 1.99 FID with only 8 inference steps using standard transformer architectures. On CIFAR-10, IMM similarly achieves state-of-the-art of 1.98 FID with 2-step generation for model trained from scratch. 2. Preliminaries 2.1. Diffusion, Flow Matching, and Interpolants For data distribution q(x), Variance-Preserving (VP) diffusion models (Ho et al., 2020; Song et al., 2020b) and Flow Matching (FM) (Lipman et al., 2022; Liu et al., 2022) construct time-augmented variables xt as an interpolation between data q(x) and prior ϵ (0, I) such that xt = αtx + σtϵ where α0 = σ1 = 1, α1 = σ0 = 0. VP 2 t(cid:1) diffusion commonly chooses αt = cos(cid:0) π and FM chooses αt = 1 t, σt = t. Both v-prediction diffusion (Salimans & Ho, 2022) and FM are trained by matching the conditional velocity vt = α tϵ such that neural network Gθ(xt, t) approximates Ex,ϵ [vtxt]. Samples can then be generated via probability-flow ODE (PF-ODE) dxt dt = Gθ(xt, t) starting from ϵ (0, I). 2 t(cid:1), σt = sin(cid:0) π tx + σ Stochastic interpolants. Unifying diffusion models and FM, stochastic interpolants (Albergo et al., 2023; Albergo & Vanden-Eijnden, 2022) construct conditional interpolation qt(xtx, ϵ) = (It(x, ϵ), γ2 I) between any data q(x) and prior ϵ p(ϵ) and sets constraints I1(x, ϵ) = ϵ, I0(x, ϵ) = x, and γ1 = γ0 = 0. Similar to FM, deterministic sampler can be learned by explicitly matching the conditional interpolant velocity vt = tIt(x, ϵ) + γtz where (0, I) such that Gθ(xt, t) Ex,ϵ,z[vtxt]. Sampling is performed following the PF-ODE dxt dt = Gθ(xt, t) similarly starting from prior ϵ p(ϵ). When γt 0 and It(x, ϵ) = αtx + σtϵ for αt, σt defined 2 Figure 2. Using an interpolation from data to prior, we define one-step sampler that moves from any to < t, directly transforming qt(xt) to qs(xs). This can be repeated by jumping to an intermediate < before moving to < r. in FM, the intermediate variable xt = αtx + σtϵ becomes deterministic interpolation and its interpolant velocity vt = α tϵ reduces to FM velocity. Thus, its training and inference both reduce to that of FM. When ϵ (0, I), stochastic interpolants reduce to v-prediction diffusion. tx + σ 2.2. Maximum Mean Discrepancy Maximum Mean Discrepancy (MMD, Gretton et al. (2012)) between distribution p(x), q(y) for x, RD is an integral probability metric (Muller, 1997) commonly defined on Reproducing Kernel Hilbert Space (RKHS) with positive definite kernel : RD RD as MMD2(p(x), q(y)) = Ex[k(x, )] Ey[k(y, )]2 (1) where the norm is in H. Choices such as the RBF kernel imply an inner product of infinite-dimensional feature maps consisting of all moments of p(x) and q(y), i.e. E[xj] and E[yj] for integer 1 (Steinwart & Christmann, 2008). 3. Inductive Moment Matching We introduce Inductive Moment Matching (IMM), method that trains model of both high quality and sampling efficiency in single stage. To do so, we assume timeaugmented interpolation between data (distribution at = 0) and prior (distribution at = 1) and propose learning an implicit one-step model (i.e. one-step sampler) that transforms the distribution at time to the distribution at time for any < (Section 3.1). The model enables direct onestep sampling from = 1 to = 0 and few-step sampling via recursive application from any to any < and then to any < until = 0; this allows us to learn the model from its own samples via bootstrapping (Section 3.2). 3.1. Model Construction via Interpolants Given data q(x) and prior ϵ p(ϵ), the timeaugmented interpolation xt defined in Albergo et al. (2023) follows xt qt(xtx, ϵ). This implies marginal interpolating distribution (cid:90) (cid:90) qt(xt) = qt(xtx, ϵ)q(x)p(ϵ)dxdϵ. (2) Inductive Moment Matching We learn model distribution implicitly defined by onestep sampler that transforms qt(xt) into qs(xs) for some t. This can be done via special class of interpolants, which preserves the marginal distribution qs(xs) while interpolating between and xt. We term these marginalpreserving interpolants among class of generalized interpolants. Formally, we define xs as generalized interpolant between and xt if, for all [0, t], its distribution follows qst(xsx, xt) = (Ist(x, xt), γ2 stI) (3) and satisfies constraints Itt(x, xt) = xt, I0t(x, xt) = x, γtt = γ0t = 0, and qt1(xtx, ϵ) qt(xtx, ϵ). When = 1, it reduces to regular stochastic interpolants. Next, we define marginal-preserving interpolants. Definition 1 (Marginal-Preserving Interpolants). generalized interpolant xs is marginal-preserving if for all [0, 1] and for all [0, t], the following equality holds: (cid:90) (cid:90) qs(xs) = where qst(xsx, xt)qt(xxt)qt(xt)dxtdx, (4) qt(xxt) = (cid:90) qt(xtx, ϵ)q(x)p(ϵ) qt(xt) dϵ. (5) That is, this class of interpolants has the same marginal at regardless of t. For all [0, 1], we define our noisy model distribution at [0, t] as (cid:90) (cid:90) pθ st(xs) = qst(xsx, xt)pθ st(xxt)qt(xt)dxtdx (6) where the interpolant is marginal preserving and pθ st(xxt) is our clean model distribution implicitly parameterized as one-step sampler. This definition also enables multistep sampling. To produce clean sample given xt qt(xt) in two steps via an intermediate s: (1) we sample ˆx pθ st(xxt) followed by ˆxs qst(xsˆx, xt) and (2) if the marginal of ˆxs matches qs(xs), we can obtain by pθ 0s(xˆxs). We are therefore motivated to minimize divergence between Eq. (4) and (6) using the objective below. Naıve objective. As one can easily draw samples from the model, it can be naıvely learned by directly minimizing L(θ) = Es,t (cid:104) D(qs(xs), pθ st(xs)) (cid:105) (7) with time distribution p(s, t) and sample-based divergence metric D(, ) such as MMD or GAN (Goodfellow et al., 2020). If an interpolant xs is marginal-preserving, then the minimum loss is 0 (see Lemma 3). One might also notice the similarity between right-hand sides of Eq. (4) and (6). However, qs(xs) = pθ st(xs) does not necessarily imply pθ st(xxt) = qt(xxt). In fact, the minimizer pθ st(xxt) is not unique and, under mild assumptions, deterministic minimizer exists (see Section 4). 3.2. Learning via Inductive Bootstrapping While sound, the naıve objective in Eq. (7) is difficult to optimize in practice because when is far from s, the input distribution qt(xt) can be far from the target qs(xs). Fortunately, our interpolant construction implies that the model definition in Eq. (6) satisfies boundary condition qs(xs) = pθ ss(xs) regardless of θ (see Lemma 4), which indicates that pθ st(xs) qs(xs) when is close to s. Furthermore, the interpolant enforces pθ sr(xs) for any < close to as long as the model is continuous around t. Therefore, we can construct an inductive learning algorithm for pθ st(xs) by using samples from pθ st(xs) pθ sr(xs). For better analysis, we define sequence number for parameter θn and function r(s, t) where r(s, t) < such that pθn (xs).1 We omit rs arguments when context is clear and let r(s, t) be finite decrement from but truncated at (see Appendix B.3 for well-conditioned r(s, t)). st(xs) learns to match pθn1 sr General objective. With marginal-preserving interpolants and mapping r(s, t), we learn θn in the following objective: L(θn) = Es,t (cid:104) w(s, t)MMD2(pθn1 sr (xs), pθn st(xs)) (cid:105) (8) where w(s, t) is weighting function. We choose MMD as our objective due to its superior optimization stability and show that this objective learns the correct data distribution. Theorem 1. Assuming r(s, t) is well-conditioned, the interpolant is marginal-preserving, and θ is minimizer of Eq. (8) for each with infinite data and network capacity, for all [0, 1], [0, t], MMD2(qs(xs), θ st(xs)) = 0. lim (9) In other words, θn eventually learns the target distribution qs(xs) by parameterizing one-step sampler pθn st(xxt). 4. Simplified Formulation and Practice We present algorithmic and practical decisions below. 4.1. Algorithmic Considerations Despite theoretical soundness, it remains unclear how to empirically choose marginal-preserving interpolant. First, we present sufficient condition for marginal preservation. Definition 2 (Self-Consistent Interpolants). Given s, [0, 1], t, an interpolant xs qst(xsx, xt) is selfconsistent if for all [s, t], the following holds: qst(xsx, xt) = (cid:90) qsr(xsx, xr)qrt(xrx, xt)dxr (10) 1Note that is different from optimization steps. Advancing from 1 to can take arbitrary number of optimization steps. 3 Inductive Moment Matching In other words, xs has the same distribution if one (1) directly samples it by interpolating and xt and (2) first samples any xr (given and xt) and then samples xs (given and xr). Furthermore, self-consistency implies marginal preservation (Lemma 5). DDIM interpolant. Denoising Diffusion Implicit Models (Song et al., 2020a) was introduced as fast ODE sampler for diffusion models, defined as DDIM(xt, x, s, t) = αs (cid:18) (cid:19) αt + σs σt σs σt xt (11) and sample xs = DDIM(xt, Ex[xxt], s, t) can be drawn when Ex[xxt] is approximated by network. We show in Appendix C.1 that DDIM as an interpolant, i.e. γst 0 and Ist(x, xt) = DDIM(xt, x, s, t), is self-consistent. Moreover, with deterministic interpolants such as DDIM, there exists deterministic minimizer pθ st(xxt) of Eq. (7). Proposition 1. (Informal) If γst 0 and Ist(x, xt) satisfies mild assumptions, there exists deterministic pθ st(xxt) that attains 0 loss for Eq. (7). See Appendix B.6 for formal statement and proof. This allows us to define pθ st(xxt) = δ(x gθ(xt, s, t)) for neural network gθ(xt, s, t) with parameter θ by default. Eliminating stochasticity. We use DDIM interpolant, deterministic model, and prior p(ϵ) = (0, σ2 dI) where σd is the data standard deviation (Lu & Song, 2024). As result, one can draw xs from model via xs = θ s,t(xt) := DDIM(xt, gθ(xt, s, t), s, t) where xt qt(xt). Re-using xt for xr. Inspecting Eq. (8) and (6), one requires xr qr(xr) to generate samples from the target Instead of sampling xr given new (x, ϵ) distribution. pair, we can reduce variance by reusing xt and such that xr = DDIM(xt, x, r, t). This is justified because xr derived from xt preserves the marginal distribution qr(xr) (see Appendix C.2). Stop gradient. We set to optimization step number, i.e. advancing from 1 to is single optimizer step where θn is initialized from θn1. Equivalently, we can omit from θn and write θn1 as the stop-gradient parameter θ. Simplified objective. Let xt, be i.i.d. random variables from qt(xt) and xr, are variables obtained by reusing xt, respectively, the training objective can be derived from the MMD definition in Eq. (1) (see Appendix C.3) as (cid:104) (cid:16) (cid:17) LIMM(θ) = Ext,x (cid:17) (cid:16) + ys,r, s,r t,xr,x (cid:16) r,s,t (cid:104) w(s, t) (cid:17) ys,t, s,r (12) s,t ys,t, (cid:16) s,t, ys,r (cid:17)(cid:105)(cid:105) where ys,t = θ s,r = θ s,r (x prior weighting function. s,t(xt), s,t(x s,r (xr), r), k(, ) is kernel function, and w(s, t) is t), ys,r = θ s,t = θ st(xs) are obtained by drawing from pθ Figure 3. With self-consistent interpolants, IMM uses particle samples (M = 2 is shown above) for moment matching. Samples from pθ st(xxt) followed by qst(xsx, xt). Solid and dashed red lines indicate sampling with and without gradient propagation respectively. After samples are drawn, sample xs is repulsed by and attracted towards samples of xs and through kernel function k(, ). An empirical estimate of the above objective uses particle samples to approximate each distribution indexed by t. In practice, we divide batch of model output with size into B/M groups within which share the same (s, t) sample, and the objective is approximated by instantiating B/M number of matrices. Note that the number of model passes does not change with respect to (see Appendix C.4). = 2 version is visualized in Figure 3 and simplified training algorithm is shown in Algorithm 1. full training algorithm is shown in Appendix D. 4.2. Other Implementation Choices We defer detailed analysis of each decision to Appendix C. Flow trajectories. We investigate the two most used flow trajectories (Nichol & Dhariwal, 2021; Lipman et al., 2022), Cosine. αt = cos(cid:0) 1 2 πt(cid:1), σt = sin(cid:0) 1 OT-FM. αt = 1 t, σt = t. 2 πt(cid:1). Network gθ(xt, s, t). We set gθ(xt, s, t) = cskip(t)xt + cout(t)Gθ(cin(t)xt, cnoise(s), cnoise(t)) with neural network Gθ, following EDM (Karras et al., 2022). For all choices we let cin(t) = 1/ /σd (Lu & Song, 2024). Listed below are valid choices for other coefficients. + σ2 α2 (cid:112) Identity. cskip(t) = 0, cout(t) = 1. Simple-EDM (Lu & Song, 2024). cskip(t) = αt/(α2 α2 Euler-FM. cskip(t) = 1, cout(t) = tσd. This is specific σ2 ), cout(t) = σdσt/ + σ2 . (cid:112) + to OT-FM schedule. We show in Appendix C.5 that θ s,t(xt) similarly follows the 4 Inductive Moment Matching EDM parameterization of the form θ cout(s, t)Gθ(cin(t)xt, cnoise(s), cnoise(t)). s,t(xt) = cskip(s, t)xt+ Noise conditioning cnoise(). We choose cnoise(t) = ct for some constant 1. We find our model convergence relatively insensitive to but recommend using larger c, e.g. 1000 (Song et al., 2020b; Peebles & Xie, 2023), because it enables sufficient distinction between nearby and t. Mapping function r(s, t). We find that r(s, t) via constant decrement in ηt = σt/αt works well where the decrement is chosen in the form of (ηmax ηmin)/2k for some appropriate (details in Appendix C.7). Kernel function. We use time-dependent Laplace kernels of the form ks,t(x, y) = exp( w(s, t) max(x y2, ϵ)/D) for x, RD, some ϵ > 0 to avoid undefined gradients, and w(s, t) = 1/cout(s, t). We find Laplace kernels provide better gradient signals than RBF kernels. (see Appendix C.8). Weighting w(s, t) and distribution p(s, t). We follow VDM (Kingma et al., 2021; Kingma & Gao, 2024) and define p(t) = U(ϵ, ) and p(st) = U(ϵ, t) for constants ϵ, [0, 1]. Similarly, weighting is defined as (cid:19) αa + σ2 α2 σ(b λt) w(s, t) = dt (13) 1 2 λt (cid:18) where σ() is sigmoid function, λt denotes log-SNRt and {1, 2}, σ(), are constants (see Appendix C.9). 4.3. Sampling Pushforward sampling. sample xs can be obtained by directly pushing xt qt(xt) through θ s,t(xt). This can be iterated for an arbitrary number of steps starting from ϵ (0, σ2 dI) until = 0. We note that, by definition, one application of θ s,t(xt) is equivalent to one DDIM step using the learned network gθ(xt, s, t) as the prediction. This sampler can then be viewed as few-step sampler using DDIM where gθ(xt, s, t) outputs realistic sample instead of its expectation Ex[xxt] as in diffusion models. Restart sampling. Similar to Xu et al. (2023); Song et al. (2023), one can introduce stochasticity during sampling by re-noising sample to higher noise-level before sampling again. For example, two-step restart sampler from xt requires (0, t) for drawing sample ˆx = θ 0,s(xs) where xs qs(xsf θ 0,t(xt)). Classifier-free guidance. Given data-label pair (x, c), during inference time, classifier-free guidance (Ho & Salimans, 2022) with weight replaces conditional model output Gθ(xt, s, t, c) by reweighted model output via wGθ(xt, s, t, c) + (1 w)Gθ(xt, s, t, ) (14) Algorithm 1 Training (see Appendix for full version) Input: parameter θ, DDIM(xt, x, s, t), B, , Output: learned θ while model not converged do Sample data x, label c, and prior ϵ with batch size and split into B/M groups. Each group shares (s, r, t) sample. For each group, xt DDIM(ϵ, x, t, 1). For each group, xr DDIM(xt, x, r, t). For each instance, set = with prob. p. Minimize the empirical loss ˆLIMM(θ) in Eq. (67). end while Algorithm 2 Pushforward Sampling (details in Appendix F) i=0, (0, σ2 dI), (optional) Input: model θ, {ti}N Output: xt0 Sample xN (0, σ2 for = N, . . . , 1 do dI) xti1 θ ti1,ti(xti) or θ ti1,ti,w(xti). end for θ (xt, s, t, c) where Gw cskip(s, t)xt+cout(s, t)Gw θ (xt, s, t, c) is as defined in Eq. (14) and we drop cin() and cnoise() for notational simplicity. We justify this decision in Appendix E. Similar to diffusion models, is randomly dropped with probability during training without special practices. We present pushforward sampling in Algorithm 2 and detail both samplers in Appendix F. 5. Connection with Prior Works Our work is closely connected with many prior works. Detailed analysis is found in Appendix G. Consistency Models. Consistency models (CMs) (Song et al., 2023; Song & Dhariwal, 2023; Lu & Song, 2024) uses network gθ(xt, t) that outputs clean data given noisy input xt. It requires point-wise consistency gθ(xt, t) = gθ(xr, r) for any < where xr is obatined via an ODE solver from xt using either pretrained model or groundtruth data. Discrete-time CM must satisfy gθ(x0, 0) = x0 and trains via loss Ext,x,t[d(gθ(xt, t), gθ (xr, r))] where d(, ) is commonly chosen as L2 or LPIPS (Zhang et al., 2018). We show in the following Lemma that CM objective with L2 distance is single-particle estimate of IMM objective with energy kernel. r, k(x, y) = y2, Lemma 1. When xt = t, xr = and > 0 is small constant, Eq. (12) reduces to CM loss Ext,x,t + for valid r(t) < and some constant C. w(t)gθ(xt, t) gθ (xr, r)2(cid:105) (cid:104) where denotes the null-token indicating unconditional output. Similarly, we define our guided model as θ s,t,w(xt) = Notice that single-particle estimate ignores the repulsion force imposed by k(, ), thus further biasing the estimate of 5 Inductive Moment Matching MMD. Energy kernel also only matches the first moment, ignoring all higher moments. These decisions can be significant contributors to training instability and performance degradation of CMs. Improved CMs (Song & Dhariwal, 2023) propose pseudohuber loss as d(, ) which we justify in the Lemma below. Lemma 2. Negative pseudo-huber loss kc(x, y) = (cid:113) y2 + c2 for > 0 is conditionally positive definite kernel that matches all moments of and where weights on higher moments depend on c. From moment-matching perspective, the improved performance is explained by the loss matching all moments of the distributions. In addition to pseudo-huber loss, many other kernels (Laplace, RBF, etc.) are all valid choices in the design space. We also extend IMM loss to the differential limit by taking r(s, t) t, the result of which subsumes the continuoustime CM (Lu & Song, 2024) as single-particle estimate (Appendix H). We leave experiments for this to future work. Diffusion GAN and Adversarial Consistency Distillation. Diffusion GAN (Xiao et al., 2021) parameterizes the generst(xsxt) = (cid:82)(cid:82) qst(xsx, xt)δ(x ative distribution as pθ Gθ(xt, z, t))p(z)dzdx for as fixed decrement from and p(z) noise distribution. It defines the interpolant qst(xsx, xt) as the DDPM posterior distribution, which is self-consistent (see Appendix G.2) and introduces randomness to the sampling process to match qt(xxt) instead of the marginal. Both Diffusion GAN and Adversarial Consistency Distillation (Sauer et al., 2025) use GAN objective, which shares similarity to MMD in that MMD is defined as an integral probability metric where the optimal discriminator is chosen in RKHS. This eliminates the need for explicit adversarial optimization of neural-network discriminator. Generative Moment Matching Network. GMMN (Li et al., 2015) directly applies MMD to train generator Gθ(z) where (0, I) to match the data distribution. It is special case of IMM in that when = 1 and r(s, t) = 0 our loss reduces to naıve GMMN objective. 6. Related Works Diffusion, Flow Matching, and stochastic interpolants. Diffusion models (Sohl-Dickstein et al., 2015; Song et al., 2020b; Ho et al., 2020; Kingma et al., 2021) and Flow Matching (Lipman et al., 2022; Liu et al., 2022) are widely used generative frameworks that learn score or velocity field of noising process from data into simple prior. They have been scaled successfully for text-to-image (Rombach et al., 2022; Saharia et al., 2022; Podell et al., 2023; Chen et al., 2023; Esser et al., 2024) and text-to-video (Ho et al., 2022a; Blattmann et al., 2023; OpenAI, 2024) tasks. Stochastic interpolants (Albergo et al., 2023; Albergo & Vanden-Eijnden, 2022) extend these ideas by explicitly defining stochastic path between data and prior, then matching its velocity to facilitate distribution transfer. While IMM builds on top of the interpolant construction, it directly learns one-step mappings between any intermediate marginal distributions. Diffusion distillation. To resolve diffusion models sampling inefficiency, recent methods (Salimans & Ho, 2022; Meng et al., 2023; Yin et al., 2024; Zhou et al., 2024; Luo et al., 2024a; Heek et al., 2024) focus on distilling one-step or few-step models from pre-trained diffusion models. Some approaches (Yin et al., 2024; Zhou et al., 2024) propose jointly optimizing two networks but the training requires careful tuning in practice and can lead to mode collapse (Yin et al., 2024). Another recent work (Salimans et al., 2024) explicitly matches the first moment of the data distribution available from pre-trained diffusion models. In contrast, our method implicitly matches all moments using MMD and can be trained from scratch with single model. Few-step generative models from scratch. Early onestep generative models primarily relied on GANs (Goodfellow et al., 2020; Karras et al., 2020; Brock, 2018) and MMD (Li et al., 2015; 2017) (or their combination) but scaling adversarial training remains challenging. Recent independent classes of few-step models, e.g. Consistency Models (CMs) (Song et al., 2023; Song & Dhariwal, 2023; Lu & Song, 2024), Consistency Trajectory Models (CTMs) (Kim et al., 2023; Heek et al., 2024) and Shortcut Models (SMs) (Frans et al., 2024) still face training instability and require specialized components (Lu & Song, 2024) (e.g., JVP for flash attention) or other special practices (e.g., high weight decay for SMs, combined LPIPS (Zhang et al., 2018) and GAN losses for CTMs, and special training schedules (Geng et al., 2024)) to remain stable. In contrast, our method trains stably with single loss and achieves strong performance without special training practices. 7. Experiments We evaluate IMMs empirical performance (Section 7.1), training stability (Section 7.2), sampling choices (Section 7.3), scaling behavior (Section 7.4) and ablate our practical decisions (Section 7.5). 7.1. Image Generation We present FID (Heusel et al., 2017) results for unconditional CIFAR-10 and class-conditional ImageNet-256256 in Table 1 and 2. For CIFAR-10, we separate baselines into diffusion and flow models, distillation models, and few-step models from scratch. IMM belongs to the last category in which it achieves state-of-the-art performance of 1. 6 Inductive Moment Matching Family Method FID () Steps () Family Method FID() Steps () #Params Diffusion & Flow Few-Step via Distillation DDPM (Ho et al., 2020) DDPM++ (Song et al., 2020b) NCSN++ (Song et al., 2020b) DPM-Solver (Lu et al., 2022) iDDPM (Nichol & Dhariwal, 2021) EDM (Karras et al., 2022) Flow Matching (Lipman et al., 2022) Rectified Flow (Liu et al., 2022) PD (Salimans & Ho, 2022) 2-Rectified Flow (Salimans & Ho, 2022) DFNO (Zheng et al., 2023) KD (Luhman & Luhman, 2021) TRACT (Berthelot et al., 2023) Diff-Instruct (Luo et al., 2024a) PID (LPIPS) (Tee et al., 2024) DMD (Yin et al., 2024) CD (LPIPS) (Song et al., 2023) CTM (w/ GAN) (Kim et al., 2023) SiD (Zhou et al., 2024) SiM (Luo et al., 2024b) sCD (Lu & Song, 2024) iCT (Song & Dhariwal, 2023) Few-Step from Scratch ECT (Geng et al., 2024) sCT (Lu & Song, 2024) IMM (ours) 3.17 3.16 2.38 4.70 2.90 2.05 6.35 2.58 4.51 4.85 3.78 9.36 3.32 5.57 3.92 3.77 2.93 1.87 1.92 2.06 2.52 2.83 2.46 3.60 2.11 2.97 2.06 3.20 1.98 1000 1000 1000 10 4000 35 142 GAN Masked & AR BigGAN (Brock, 2018) GigaGAN (Kang et al., 2023) StyleGAN-XL (Karras et al., 2020) VQGAN (Esser et al., 2021) MaskGIT (Chang et al., 2022) MAR (Li et al., 2024) VAR-d20 (Tian et al., 2024a) VAR-d30 (Tian et al., 2024a) Diffusion & Flow ADM (Dhariwal & Nichol, 2021) CDM (Ho et al., 2022b) SimDiff (Hoogeboom et al., 2023) LDM-4-G (Rombach et al., 2022) U-DiT-L (Tian et al., 2024b) DiT-XL/2 (w = 1.0) (Peebles & Xie, 2023) DiT-XL/2 (w = 1.25) (Peebles & Xie, 2023) DiT-XL/2 (w = 1.5) (Peebles & Xie, 2023) SiT-XL/2 (w = 1.0) (Ma et al., 2024) SiT-XL/2 (w = 1.5) (Ma et al., 2024) iCT (Song et al., 2023) Shortcut (Frans et al., 2024) IMM (ours) (XL/2, = 1.25) Few-Step from Scratch IMM (ours) (XL/2, = 1.5) 2 1 1 1 2 1 1 1 2 2 1 1 1 2 1 2 1 2 1 2 6.95 3.45 2.30 26.52 6.18 1.98 2.57 1.92 10.94 4.88 2.77 3.60 3.37 9.62 3.22 2.27 9.35 2.15 34.24 20.3 10.60 7.80 3.80 7.77 5.33 3.66 2.77 8.05 3.99 2.51 1.99 1 1 1024 8 100 10 10 250 8100 512 250 250 250 250 250 250 250 1 2 1 4 128 1 2 4 8 1 2 4 8 112M 569M 166M 227M 227M 400M 600M 2B 554M - 2B 400M 916M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M 675M Table 1. CIFAR-10 results trained without label conditions. Table 2. Class-conditional ImageNet-256256 results. Figure 4. FID convergence for different embeddings (left). CIFAR10 samples from Fourier embedding (scale = 16) (right). using pushforward sampler. For ImageNet-256256, we use the popular DiT (Peebles & Xie, 2023) architecture because of its scalability, and compare it with GANs, masked and autoregressive models, diffusion and flow models, and few-step models trained from scratch. We observe decreasing FID with more steps and IMM achieves 1.99 FID with 8 steps (with = 1.5), surpassing DiT and SiT (Ma et al., 2024) using the same architecture except for trivially injecting time (see Appendix I). Notably, we also achieve better 8-step FID than the 10-step VAR (Tian et al., 2024a) of comparable size and approaches the performance of its 2B parameter variant. However, different from VAR, IMM grants flexibility of variable number of inference steps, thus allowing scaling at inference time. The large improvement in FID from 1 to 8 steps additionally demonstrates IMMs efficient inferece-time scaling capability. Lastly, we similarly surpass Shortcut models (Frans et al., 2024) best performance with only 8 steps. We defer inference details to Section 7.3 and Appendix I.2. Figure 5. More particles indicate more stable training on ImageNet-256256. Figure 6. ImageNet256256 FID with different sampler types. 7.2. IMM Training is Stable We show that IMM is stable and achieves reasonable performance across range of parameterization choices. Positional vs. Fourier embedding. known issue for CMs (Song et al., 2023) is its training instability when using Fourier embedding with scale 16, which forces reliance on positional embeddings for stability. We find that IMM does not face this problem (see Figure 4). For Fourier embedding we use the standard NCSN++ (Song et al., 2020b) architecture and set embedding scale to 16; for positional embeddings, we adopt DDPM++ (Song et al., 2020b). Both embedding types converge reliably, and we include samples from the Fourier embedding model in Figure 4. Particle number. Particle number for estimating MMD is an important parameter for empirical success (Gretton et al., 2012; Li et al., 2015), where the estimate is more accurate with larger . In our case, naıvely increasing can slow down convergence because we have fixed batch size in which the samples are grouped into B/M groups 7 Inductive Moment Matching Figure 7. IMM scales with both training and inference compute, and exhibits strong correlation between model size and performance. Figure 8. Sample visual quality increases with increase in both model size and sampling compute. means fewer ts are sampled for each step, thus slowing convergence. general rule of thumb is to use large enough for stability, but not too large for slowed convergence. Noise embedding cnoise(). We plot in Figure 9 the log absolute mean difference of and r(s, t) in the positional embedding space. Increasing increases distinguishability of nearby distributions. We also observe similar convergence on ImageNet-256256 across different c, demonstrating the insensitivity of our framework w.r.t. noise function. 7.3. Sampling We investigate different sampling settings for best performance. One-step sampling is performed by simple pushforward from to ϵ (concrete values in Appendix I.2). On CIFAR-10 we use 2 steps and set intermediate time t1 such that ηt1 = 1.4, choice we find to work well empirically. On ImageNet-256256 we go beyond 2 steps and, for simplicity, investigate (1) uniform decrement in and (2) EDM (Karras et al., 2024) schedule (detailed in Appendix I.2). We plot FID of all sampler settings in Figure 6 with guidance weight = 1.5. We find pushforward samFigure 9. Log distance in embedding space for cnoise(t) = ct (left). Similar ImageNet-256256 convergence across different (right). of where each group shares the same t. The larger means that fewer ts are sampled. On the other hand, using extremely small numbers of particles, e.g. = 2, leads to training instability and performance degradation, especially on large scale with DiT architectures. We find that there exists sweet spot where few particles effectively help with training stability while further increasing slows down convergence (see Figure 5). We see that in ImageNet256256, training collapses when = 1 (which is CM) and = 2, and achieves lowest FID under the same computation budget with = 4. We hypothesize < 4 does not allow sufficient mixing between particles and larger 8 Inductive Moment Matching id/cos id/FM sEDM/cos sEDM/FM eFM CIFAR-10 3.77 3. ImageNet-256256 46.44 47.32 2.39 27.33 2. 28.67 2.53 27.01 Table 3. FID results with different flow schedules and network parameterization. plers with uniform schedule to work the best on ImageNet256256 and use this as our default setting for multi-step generation. Additionally, we concede that pushforward combined with restart samplers can achieve superior results. We leave such experiments to future works. 7.4. Scaling Behavior Similar to diffusion models, IMM scale with training and inference compute as well as model size on ImageNet256256. We plot in Figure 7 FID vs. training and inference compute in GFLOPs and we find strong correlation between compute used and performance. We further visualize samples in Figure 8 with increasing model size, i.e. DiT-S, DiT-B, DiT-L, DiT-XL, and increasing inference steps, i.e. 1, 2, 4, 8 steps. The sample quality increases along both axes as larger transformers with more inference steps capture more complex distributions. This also explains that more compute can sometimes yield different visual content from the same initial noise as shown in the visual results. 7.5. Ablation Studies All ablation studies are done with DDPM++ architecture for CIFAR-10 and DiT-B for ImageNet-256256. FID comparisons use 2-step samplers by default. Flow schedules and parameterization. We investigate all combinations of network parameterization and flow schedules: Simple-EDM + cosine (sEDM/cos), Simple-EDM + OT-FM (sEDM/FM), Euler-FM + OT-FM (eFM), Identity + cosine (id/cos), Identity + OT-FM (id/FM). Identity parameterization consistently fall behind other types of parameterization, which all show similar performance across datasets (see Table 3). We see that on smaller scale (CIFAR-10), sEDM/FM works the best but on larger scale (ImageNet-256256), eFM works the best, indicating that OT-FM schedule and Euler paramaterization may be more scalable than other choices. /σ2 Mapping function r(s, t). Our choices for ablation are (1) constant decrement in ηt, (2) constant decrement in t, (3) (cid:1), (4) constant increconstant decrement in λt = log(cid:0)α2 ment in 1/ηt (see Appendix C.6). For fair comparison, we choose the decrement gap so that the minimum r(s, t) is 103 and use the same network parameterization. FID progression in Figure 11 show that (1) consistently outperforms other choices. We additionally ablate the mapping gap using = 4 in (1). The constant decrement is in w(s, t) = 1 + ELBO weight + αt + 1/(α2 + σ2 ) FID-50k 40. 96.43 33.44 27.43 Table 4. Ablation of weighting function w(s, t) on ImageNet-256256. Figure 10. ImageNet-256256 FID progression with different t, gap with = 4. Figure 11. FID progression on different types of mapping function r(s, t) for CIFAR-10 (left) and ImageNet-256256 (right). the form of (ηmax ηmin)/2k for an appropriately chosen k. We show in Figure 10 that the performance is relatively stable across {11, 12, 13} but experiences instability for = 14. This suggests that, for given particle number, there exists largest for stable optimization. + σ2 Weighting function. In Table 4 we first ablate the weighting factors in three groups: (1) the VDM ELBO factors 1 2 σ(bλt)( dt λt), (2) weighting αt (i.e. when = 1), and (3) weighting 1/(α2 ). We find it necessary to use αt jointly with ELBO weighting because it converts v-pred network to ϵ-pred parameterization (see Appendix C.9), cont + σ2 sistent with diffusion ELBO-objective. Factor 1/(α2 ) upweighting middle time-steps further boosts performance, helpful practice also known for FM training (Esser et al., 2024). We leave additional study of the exponent to Appendix I.4 and find that = 2 emphasizes optimizing the loss when is small while = 1 more equally distributes weights to larger t. As result, = 2 achieves higher quality multi-step generation than = 1. 8. Conclusion We present Inductive Moment Matching, framework that learns few-step generative model from scratch. It trains by first leveraging self-consistent interpolants to interpolate between data and prior and then matching all moments of its own distribution interpolated to be closer to that of data. Our method guarantees convergence in distribution and generalizes many prior works. Our method also achieves stateof-the-art performance across benchmarks while achieving orders of magnitude faster inference. We hope it provides new perspective on training few-step models from scratch and inspire new generation of generative models. Inductive Moment Matching 9. Impact Statement This paper advances research in diffusion models and generative AI, which enable new creative possibilities and democratize content creation but also raise important considerations. Potential benefits include expanding artistic expression, assisting content creators, and generating synthetic data for research. However, we acknowledge challenges around potential misuse for deepfakes, copyright concerns, and impacts on creative industries. While our work aims to advance technical capabilities, we encourage ongoing discussion about responsible development and deployment of these technologies. 10. Acknowledgement We thank Wanqiao Xu, Bokui Shen, Connor Lin, and Samrath Sinha for additional technical discussions and helpful suggestions."
        },
        {
            "title": "References",
            "content": "Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Auffray, Y. and Barbillon, P. Conditionally positive definite kernels: theoretical contribution, application to interpolation and approximation. PhD thesis, INRIA, 2009. Berthelot, D., Autef, A., Lin, J., Yap, D. A., Zhai, S., Hu, S., Zheng, D., Talbott, W., and Gu, E. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Brock, A. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020. Esser, P., Rombach, R., and Ommer, B. Taming transformers In Proceedings of the for high-resolution image synthesis. IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Frans, K., Hafner, D., Levine, S., and Abbeel, P. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. Geng, Z., Pokle, A., Luo, W., Lin, J., and Kolter, J. Z. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., WardeFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM, 63(11): 139144, 2020. Gretton, A., Borgwardt, K. M., Rasch, M. J., Scholkopf, B., and Smola, A. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. Heek, J., Hoogeboom, E., and Salimans, T. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47): 133, 2022b. Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: Endto-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1012410134, 2023. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. Inductive Moment Matching Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Kingma, D., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. Li, C.-L., Chang, W.-C., Cheng, Y., Yang, Y., and Poczos, B. Mmd gan: Towards deeper understanding of moment matching network. Advances in neural information processing systems, 30, 2017. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Li, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In International conference on machine learning, pp. 17181727. PMLR, 2015. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D., Wang, W., and Plumbley, M. D. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Lu, C. and Song, Y. ing continuous-time consistency models. arXiv:2410.11081, 2024. Simplifying, stabilizing and scalarXiv preprint Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. Luhman, E. and Luhman, T. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Luo, W., Hu, T., Zhang, S., Sun, J., Li, Z., and Zhang, Z. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024a. 11 Luo, W., Huang, Z., Geng, Z., Kolter, J. Z., and Qi, G.-j. One-step diffusion distillation through score implicit matching. arXiv preprint arXiv:2410.16794, 2024b. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Meng, C., Rombach, R., Gao, R., Kingma, D., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1429714306, 2023. Muller, A. Integral probability metrics and their generating classes of functions. Advances in applied probability, 29(2):429443, 1997. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. OpenAI. Video generation models as world simulators. https: //openai.com/sora/, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Salimans, T., Mensink, T., Heek, J., and Hoogeboom, E. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2025. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Song, Y. and Dhariwal, P. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. Inductive Moment Matching Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Steinwart, I. and Christmann, A. Support vector machines. Springer Science & Business Media, 2008. Tee, J. T. J., Zhang, K., Yoon, H. S., Gowda, D. N., Kim, C., and Yoo, C. D. Physics informed distillation for diffusion models. arXiv preprint arXiv:2411.08378, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via nextscale prediction. arXiv preprint arXiv:2404.02905, 2024a. Tian, Y., Tu, Z., Chen, H., Hu, J., Xu, C., and Wang, Y. U-dits: Downsample tokens in u-shaped diffusion transformers. arXiv preprint arXiv:2405.02730, 2024b. Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Xu, Y., Deng, M., Cheng, X., Tian, Y., Liu, Z., and Jaakkola, T. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:7680676838, 2023. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6613 6623, 2024. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and Anandkumar, A. Fast sampling of diffusion models via operator learning. In International conference on machine learning, pp. 42390 42402. PMLR, 2023. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 12 Inductive Moment Matching A. Background: Properties of Stochastic Interpolants We note some relevant properties of stochastic interpolants for our exposition. Boundary satisfaction. For an interpolant distribution qt(xtx, ϵ) defined in Albergo et al. (2023), and the marginal qt(xt) as defined in Eq. (2), we can check that q1(x1) = p(x1) and q0(x0) = q(x0) so that x1 = ϵ and x0 = x. (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) q1(x1) = = = q1(x1x, ϵ)q(x)p(ϵ)dxdϵ δ(x1 ϵ)q(x)p(ϵ)dxdϵ q(x)p(x1)dx = p(x1) (cid:90) (cid:90) q0(x0) = q0(x0x, ϵ)q(x)p(ϵ)dxdϵ (cid:90) (cid:90) (cid:90) (cid:90) = = δ(x0 x)q(x)p(ϵ)dxdϵ q(x0)p(ϵ)dx = q(x0) Joint distribution. The joint distribution of and xt is written as (cid:90) (cid:90) (cid:90) qt(x, xt) = q1(x, x1) = = qt(xtx, ϵ)q(x)p(ϵ)dϵ q1(x1x, ϵ)q(x)p(ϵ)dϵ δ(x1 ϵ)q(x)p(ϵ)dϵ = q(x)p(x1) = q(x)p(ϵ) Indepedence of joint at = 1. in which case x1 = ϵ. B. Theorems and Derivations (15) (16) (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) B.1. Divergence Minimizer Lemma 3. Assuming marginal-preserving interpolant and metric D(, ), minimizer θ of Eq. (7) exists, i.e. pθ qt(xxt), and the minimum is 0. st(xxt) = Proof. We directly substitute qt(xxt) into the objective to check. First, (cid:90) (cid:90) (cid:90) (cid:90) pθ st(xs) = = qst(xsx, xt)pθ st(xxt)qt(xt)dxtdx qst(xsx, xt)qt(xxt)qt(xt)dxtdx (a) = qs(xs) 13 (28) (29) (30) where (a) is due to definition of marginal preservation. So the objective becomes Inductive Moment Matching (cid:104) Es,t w(s, t)D(qs(xs), pθ st(xs)) (cid:105) = Es,t [w(s, t)D(qs(xs), qs(xs))] = 0 (31) In general, the minimizer qt(xxt) exists. However, this does not show that the minimizer is unique. In fact, the minimizer is not unique in general because deterministic minimizer can also exist under certain assumptions on the interpolant (see Appendix B.6). Failure Case without Marginal Preservation. We additionally show that marginal-preservation property of the interpolant qst(xsx, xt) is important for the naıve objective in Eq. (7) to attain 0 loss (Lemma 3). Consider the failure case below where the constructed interpolant is generalized interpolant but not necessarily marginal-preserving. Then we show that there exists such that pθ st(xs) can never reach qs(xs) regardless of θ. Proposition 2 (Example Failure Case). Let q(x) = δ(x), p(ϵ) = δ(ϵ 1), and suppose an interpolant Ist(x, xt) = (1 )x + st(xs)) > 0 for all 0 < < < 1 regardless of the learned distribution pθ 1(t < 1), then D(qs(xs), pθ xt and γst = st(xxt) given any metric D(, ). (cid:112)1 t Proof. This example first implies the learning target (cid:90) (cid:90) (cid:90) (cid:90) qs(xs) = = qs(xsx, ϵ)q(x)p(ϵ)dxdϵ δ(xs ((1 s)x + sϵ))δ(x)δ(ϵ 1)dxdϵ = δ(xs s) (32) (33) (34) is delta distribution. However, we show that if we select any < 1 and 0 < < t, pθ st(xs) can never be delta distribution. pθ st(xs) = (cid:90) (cid:90) (cid:90) (cid:90) qst(xsx, xt)pθ st(xxt)qt(xt)dxtdx = = (cid:90) ((1 ((1 s )x + )x + s, s2 t2 )I)pθ xt, s2 t2 (1 s2 t2 (1 s2 t2 )I)pθ st(xxt = t)dx st(xxt)δ(xt t)dxtdx Now, we show the model distribution has non-zero variance under these choices of and s. Expectations are over pθ or conditional interpolant qst(xsx, xt) for all equations below. st(xs) Var(xs) = (cid:2)x2 (cid:3) [xs]2 (cid:2)x2 sx, xt (cid:3) pθ st(xxt)qt(xt)dxdxt [xs] (cid:2)x2 sx, xt (cid:3) pθ st(xxt)qt(xt)dxdxt [xs]2 (cid:90) (cid:90) (cid:90) (cid:90) = = = = (cid:90) (cid:90) (cid:104) (cid:90) (cid:90) (cid:104) Var(xsx, xt) + [xsx, xt]2(cid:105) Var(xsx, xt) + [xsx, xt]2(cid:105) (cid:90) (cid:90) st(xxt)qt(xt)dxdxt 2E [xs]2 + [xs]2 pθ pθ st(xxt)qt(xt)dxdxt [xs] [xsxxt] pθ st(xxt)qt(xt)dxdxt + [xs] (cid:90) (cid:90) (cid:104) = (cid:124) Var(xsx, xt) + [xsx, xt]2 [xs] [xsx, xt] + [xs]2(cid:105) (cid:123)(cid:122) (cid:125) (a) pθ st(xxt)qt(xt)dxdxt (43) 14 (35) (36) (37) (38) (39) (40) (41) (42) where (a) can be simplified as Inductive Moment Matching Var(xsx, xt) + (cid:16) [xsx, xt]2 [xs] (cid:17)2 > 0 (44) because Var(xsx, xt) > 0 for all 0 < < < 1 due to its non-zero Gaussian noise. Therefore, Var(xs) > 0, implying pθ st(xs) can never be delta function regardless of model pθ st(xs) and qs(xs) implies st(xxt). valid metric D(, ) over probability pθ D(qs(xs), pθ st(xs)) = 0 pθ st(xs) = qs(xs) which means st(xs) = qs(xs) = D(qs(xs), pθ pθ st(xs)) > 0 B.2. Boundary Satisfaction of Model Distribution The operator output pθ st(xs) satisfies boundary condition. Lemma 4 (Boundary Condition). For all [0, 1] and all θ, the following boundary condition holds. qs(xs) = pθ ss(xs) (45) Proof. (cid:90) (cid:90) pθ ss(xs) = qs,s(xsx, xs) (cid:123)(cid:122) (cid:125) (cid:124) δ(xsxs) pθ ss(xxs)qs,1(xs)dxsdx (cid:90) = pθ ss(xxs)qs(xs)dx (cid:90) = qs(xs) = qs(xs) pθ ss(xxs)dx B.3. Definition of Well-Conditioned r(s, t) For simplicity, the mapping function r(s, t) is well-conditioned if r(s, t) = max(s, (t)) (46) where (t) ϵ > 0 is positive function such that r(s, t) is increasing for + c0(s) where c0(s) is the largest that is mapped to s. Formally, c0(s) = sup{t : r(s, t) = s}. For + c0(s), the inverse w.r.t. exists, i.e. r1(s, ) and r1(s, r(s, t)) = t. All practical implementations follow this general form, and are detailed in Appendix C.6. B.4. Main Theorem Theorem 1. Assuming r(s, t) is well-conditioned, the interpolant is marginal-preserving, and θ for each with infinite data and network capacity, for all [0, 1], [0, t], is minimizer of Eq. (8) MMD2(qs(xs), θ st(xs)) = 0. lim (9) Proof. We prove by induction on sequence number n. First, r(s, t) is well-conditioned by following the definition in Eq. (46). Furthermore, for notational convenience, we let r1 (s, ) := r1(s, r1(s, r1(s, . . . ))) be nested application of r1(s, ) on the second argument. Additionally, r1 0 (s, t) = t. 15 Base case: = 1. Given any 0, r(s, u) = for all < c0(s), implying Inductive Moment Matching MMD2(pθ0 ss(xs), θ su(xs)) 1 (a) = MMD2(qs(xs), θ su(xs)) (b) = 0 (47) for c0(s) where (a) is implied by Lemma 4 and (b) is implied by Lemma 3. Inductive assumption: 1. We assume θ su (xs) = qs(xs) for all r1 n1 n2(s, c0(s)). We inspect the target distribution can apply r(s, ) to the inequality and get = r(s, s) r(s, u) r(s, r1 increasing. And by inductive assumption θ sr(s,u)(xs) in Eq. (8) if optimized on r1 n1 n1(s, c0(s))) = r1 θ sr(s,u)(xs) = qs(xs) for r(s, u) r1 n1 (cid:104) θ sr(s,u)(xs), pθn n1 su(xs)) Es,u w(s, u)MMD2(p n1(s, c0(s)). On this interval, we n2(s, c0(s)) since r(s, ) is n2(s, c0(s)), this implies minimizing (cid:105) on r1 n1(s, c0(s)) is equivalent to minimizing (cid:104) Es,u w(s, u)MMD2(qs(xs), pθn su(xs)) (cid:105) (48) for n1(s, c0(s)). Lemma 3 implies that its minimum achieves θ su(xs) = qs(xs). Lastly, taking implies limn r1 Therefore, limn MMD(qs(xs), θ st(xs)) = 0 for all 0 1. (s, s) = 1 and thus the induction covers the entire [s, 1] interval given each s. B.5. Self-Consistency Implies Marginal Preservation Without assuming marginal preservation, it is important to define the marginal distribution of xs under generalized interpolants qst(xsx, xt) as (cid:90) (cid:90) qst(xs) = qst(xsx, xt)qt(xxt)qt(xt)dxtdx (49) and we show that with self-consistent interpolants, this distribution is invariant of t, i.e. qst(xs) = qs(xs). Lemma 5. If the interpolant qst(xsx, xt) is self-consistent, the marginal distribution qst(xs) as defined in Eq. (49) satisfies qs(xs) = qst(xs) for all [s, 1]. Proof. For [s, 1], qst(xs) = = = = = (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) (cid:90) qst(xsx, xt))qt(xxt)qt(xt)dxtdx qst(xsx, xt))qt(xt) qst(xsx, xt)q(x) (cid:90) (cid:90) qt(xtx, ϵ)q(x)p(ϵ) qt(xt) dϵdxtdx qt(xtx, ϵ)p(ϵ)dϵdxtdx (cid:18)(cid:90) (cid:18)(cid:90) q(x)p(ϵ) q(x)p(ϵ) qst(xsx, xt)qt(xtx, ϵ)dxt dϵdx (cid:19) qst(xsx, xt)qt,1(xtx, ϵ)xt dϵdx (cid:19) (cid:90) (cid:90) (cid:90) (cid:90) (a) = (b) = qs,1(xsx, ϵ)q(x)p(ϵ)dϵdx qs(xsx, ϵ)q(x)p(ϵ)dϵdx = qs(xs) where (a) uses definition of self-consistent interpolants and (b) uses definition of our generalized interpolant. 16 (50) (51) (52) (53) (54) (55) (56) (57) Inductive Moment Matching We show in Appendix C.1 that DDIM is an example self-consistent interpolant. Furthermore, DDPM posteior (Ho et al., 2020; Kingma et al., 2021) is also self-consistent (see Lemma 6). B.6. Existence of Deterministic Minimizer We present the formal statement for the deterministic minimizer. Proposition 3. If for all [0, 1], [0, t], γst 0, Ist(x, xt) is invertible w.r.t. x, and there exists C1 < such that (cid:13) (cid:13)It1(x, ϵ) (cid:13) (cid:13) < C1x ϵ, then there exists function hst : RD RD such that (cid:90) (cid:90) qs(xs) = qst(xsx, xt)δ(x hst(xt))qt(xt)dxdxt. (58) st (, xt) be the inverse of Ist w.r.t. such that 1 Proof. Let 1 st (y, xt), xt) = for all x, xt, RD. Since there exists C1 < such that (cid:13) (cid:13) < C1x ϵ for all [0, 1], the PF-ODE of the original interpolant It1(x, ϵ) = It(x, ϵ) exists for all [0, 1] (Albergo et al., 2023). Then, for all [0, 1], [0, t], we let st (Ist(x, xt), xt) = and Ist(I 1 (cid:13)It1(x, ϵ)(cid:13) ˆhst(xt) = xt + (cid:90) t Ex,ϵ (cid:20) (cid:12) (cid:12) Iu(x, ϵ) (cid:12) (cid:12) (cid:21) xu du, which pushes forward the measure qt(xt) to qs(xs). We define: hst(xt) = 1 st (ˆhst(xt), xt). Then, since γst 0, qst(xsx, xt) = δ(xs Ist(x, xt)) where δ(x hst(xt)). Therefore, xs = Ist(hst(xt), xt) = Ist(I st (ˆhst(xt), xt), xt) = ˆhst(xt) whose marginal follows qs(xs) due to it being the result of PF-ODE trajectories starting from qt(xt). (59) (60) (61) Concretely, DDIM interpolant satisfies all of the deterministic assumption, the regularity condition, and the invertibility assumption because it is linear function of and xt. Therefore, any diffusion or FM schedule with DDIM interpolant will enjoy deterministic minimizer pθ st(xxt). C. Analysis of Simplified Parameterization C.1. DDIM Interpolant We check that DDIM interpolant is self-consistent. By definition, qst(xsx, xt) = δ(xs DDIM(xt, x, t, s)). We check that for all t, (cid:90) (cid:90) qsr(xsx, xr)qrt(xrx, xt)dxr = δ(xs DDIM(xr, x, r, s))δ(xr DDIM(xt, x, t, r))dxr where DDIM(DDIM(xt, x, t, r), x, r, s) = αsx + (σs/σr)([αrx + (σr/σt)(xt αtx)] αrx) = δ(xs DDIM(DDIM(xt, x, t, r), x, r, s)) = αsx + (σs/σr)(σr/σt)(xt αtx) = αsx + (σs/σt)(xt αtx) = DDIM(xt, x, t, s) Therefore, δ(xs DDIM(DDIM(xt, x, t, r), x, r, s)) = δ(xs DDIM(xt, x, t, s)). So DDIM is self-consistent. It also implies Gaussian forward process qt(xtx) = (αtx, σ2 (cid:90) σ2 dI) as in diffusion models. By definition, qt,1(xtx) = qt(xtx) = qt(xtx, ϵ)p(ϵ)dϵ so that xt is deterministic transform given and ϵ, i.e., xt = DDIM(ϵ, x, t, 1) = αtx + σtϵ, which implies qt(xtx) = (αtx, σ σ2 dI). 17 C.2. Reusing xt for xr Inductive Moment Matching We propose that instead of sampling xr via forward flow αrx + σrϵ we reuse xt such that xr = DDIM(xt, x, r, t) to reduce variance. In fact, for any self-consistent interpolant, one can reuse xt via xr qrt(xrx, xt) and xr will follow qr(xr) marginally. We check qr(xr) (a) = qrt(xr) = (cid:90) (cid:90) qrt(xrx, xt)qt(xxt)qt(xt)dxdxt = (cid:90) (cid:90) (cid:90) qrt(xrx, xt) qt(xtx, ϵ)q(x)p(ϵ)dϵdxdxt where (a) is due to Lemma 5. We can see that sampling x, xt first then xr qrt(xrx, xt) respects the marginal distribution qr(xr). C.3. Simplified Objective We derive our simplified objective. Given MMD defined in Eq. (1), we write our objective as LIMM(θ) = Es,t (cid:104) w(s, t) (cid:105) s,t(xt), )] Exr [k(f θ (cid:13) Ext[k(f θ (cid:13) (cid:13) (cid:13) s,t(xt), ) k(f θ Ext,xr [k(f θ (cid:13) (cid:13) (cid:68) s,t(xt), ) k(f θ Ext,xr [k(f θ (cid:104) (cid:13) 2 (cid:13) s,r (xr), )] (cid:13) (cid:13) (cid:105) 2 (cid:13) s,r (xr), )] (cid:13) s,r (xr), )], Ex (cid:104) w(s, t) (a) = Es,t (cid:104) w(s, t) (cid:104) w(s, t) Ext,xr,x = Es,t = Es,t t,x k(f θ s,t(xt), ), k(f θ s,t(xt), ), k(f θ k(f θ s,t(x = Ext,xr,x t,x r,s,t (cid:104) w(s, t) (cid:104) k(f θ s,t(x s,t(xt), θ s,t(xt), θ s,r (x k(f θ s,r (x t)) + k(f θ r), ) k(f θ s,r (xr), θ t), θ s,t(x r)) k(f θ s,r (x r)) (cid:105)(cid:105) s,r (xr)) (cid:69)(cid:105) [k(f θ t,x t), ) + k(f θ t), ) k(f θ s,r (x r), )] s,t(x s,r (xr), ), k(f θ t), ), k(f θ s,t(x s,r (x r), ) (cid:105)(cid:105) s,r (xr), ) (62) (63) (64) (65) (66) where , is in RKHS, (a) is due to the correlation between xr and xt by re-using xt. C.4. Empirical Estimation As proposed in Gretton et al. (2012), MMD is typically estimated with V-statistics by instantiating matrix of size such that batch of samples, {x(i)}B i=1, is separated into groups of (assume is divisible by ) particles {x(i,j)}B/M,M i=1,j=1 where each group share (si, ri, ti) sample. The Monte Carlo estimate becomes ˆLIMM(θ) = 1 B/M B/M (cid:88) i=1 w(si, tj) 1 2 (cid:88) (cid:88) (cid:104) j=1 k=1 k(f θ si,ti(x(i,j) ti ), θ si,ti(x(i,k) ti )) + k(f θ si,ri(x(i,j) ri ), θ si,ri(x(i,k) ri )) (67) 2k(f θ si,ti(x(i,j) ti ), θ si,ri(x(i,k) ri (cid:105) )) Computational efficiency. First we note that regardless of , we require only 2 model forward passes - one with and one without stop gradient, since the model takes in all instances together within the batch and produce outputs for the entire batch. For the calculation of our loss, although the need for particles may imply inefficient computation, the cost of this matrix computation is negligible in practice compared to the complexity of model forward pass. Suppose forward pass for single instance is O(K), then the total computation for computation loss for batch of instances is O(BK) + O(BM ). Deep neural networks often has , so O(BK) dominates the computation. C.5. Simplified Parameterization We derive θ s,t(xt) for each parameterization, which now generally follows the form θ s,t(xt) = cskip(s, t)xt + cout(s, t)Gθ(cin(s, t)xt, cnoise(s), cnoise(t)) 18 Identity. This is simply DDIM with x-prediction network. Inductive Moment Matching θ s,t(xt) = (αs (cid:32) σs σt αt)Gθ xt + σ2 α2 (cid:112) σd (cid:33) , cnoise(s), cnoise(t) + σs σt xt Simple-EDM. θ s,t(xt) = (αs (cid:34) σs σt αt) αt + σ2 α2 xt (cid:112) = αsαt + σsσt + σ2 α2 xt αsσt σsαt (cid:112) + σ2 α2 σt + σ2 α2 (cid:32) σdGθ (cid:32) σdGθ xt + σ2 α2 (cid:112) σd (cid:33)(cid:35) , cnoise(s), cnoise(t) + σs σt xt xt + σ2 α2 (cid:112) σd (cid:33) , cnoise(s), cnoise(t) When noise schedule is cosine, θ 2 π(t s)(cid:1)xt sin(cid:0) 1 And similar to Lu & Song (2024), we can show that predicting xs = DDIM(xt, x, s, t) with L2 loss is equivalent to v-prediction with cosine schedule. 2 π(t s)(cid:1)σdGθ s,t(xt) = cos(cid:0) 1 , cnoise(s), cnoise(t) +σ2 xt α2 σd (cid:18) (cid:19) . w(s, t) (cid:13) (cid:13) θ s,t(xt) (cid:13) (cid:13) (cid:18) (αs σs σt αt)x + σs σt xt (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) = w(s, t) αsαt + σsσt + σ2 α2 xt αsσt σsαt (cid:112) + σ2 α2 σdGθ (cid:32) xt + σ2 α2 (cid:112) σd (cid:33) (cid:18) , cnoise(s), cnoise(t) (αs σs σt αt)x + σs σt xt (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) = w(s, t) σdGθ (cid:32) xt + σ2 α2 (cid:112) σd (cid:33) (cid:32) , cnoise(s), cnoise(t) (cid:124) (cid:112) α2 + σ2 αsσt σsαt (cid:33) (cid:18) (αs σs σt αt)x + σs σt xt αsαt + σsσt + σ2 α2 xt (cid:123)(cid:122) Gtarget (cid:19) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) where (cid:32) Gtarget = (cid:32) = (cid:32) = (cid:32) = (cid:112) α α2 + σ2 αsσt σsαt (cid:112) + σ2 αsσt σsαt (cid:112) + σ2 αsσt σsαt (cid:112) + σ2 αsσt σsαt α α2 (cid:33) (cid:18) (αs σs σt αt)x + σs σt (cid:33) (cid:18) (αs αt)x + σs σt (cid:33) (cid:18) (αsσt σsαt)(α2 (cid:19) xt xt αsαt + σsσt α2 + σ2 αsαtσt (cid:8)(cid:8)(cid:8) + (cid:8)(cid:8)(cid:8) σsσ2 σsσ2 + σ2 σt(α2 ) σsα2 + σ2 ) + σsα3 + σ2 ) σt(α αsα2 σt + σsα2 αsαtσt + σ2 α2 (cid:19) ϵ (cid:33) (cid:18) (αsσt σsαt)(α2 ) (αsσt σsαt)α2 + σ2 α2 + σ2 (αsσt σsαt)αt σt(α2 + σ2 ) (cid:19) ϵ (cid:19) (αtx + σtϵ) = αtϵ σtx (cid:112) + σ2 α2 This reduces to v-target if cosine schedule is used, and it deviates from v-target if FM schedule is used instead. 19 Euler-FM. We assume OT-FM schedule. Inductive Moment Matching θ s,t(xt) = ((1 s) (cid:34) (cid:32) (1 t)) xt tσdGθ xt + σ2 α2 (cid:112) σd (cid:33)(cid:35) , cnoise(s), cnoise(t) + xt = ((1 ) (cid:34) (cid:32) xt tσdGθ xt + σ2 α2 (cid:112) σd (cid:33)(cid:35) , cnoise(s), cnoise(t) + xt = xt (t s)σdGθ (cid:32) xt + σ2 α2 (cid:112) σd (cid:33) , cnoise(s), cnoise(t) This results in Euler ODE from xt to xs. We also show that the network output reduces to v-prediction if matched with xs = DDIM(xt, x, s, t). To see this, = w(s, t) xt (t s)σdGθ w(s, t) = w(s, t) s,t(xt) (cid:13) (cid:13)f θ (cid:13) s,t(xt) (cid:13) (cid:13)f θ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) σdGθ σdGθ (cid:32) (cid:32) = w(s, t) = w(s, t) (cid:16) ((1 s) (cid:16) (1 )x + (1 t))x + (cid:17)(cid:13) 2 (cid:13) (cid:13) xt t (cid:32) xt + σ2 α2 (cid:112) σd (cid:17)(cid:13) 2 (cid:13) (cid:13) xt (cid:33) , cnoise(s), cnoise(t) (cid:16) (1 )x + (cid:17) xt (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) xt + σ2 α2 (cid:112) σd (cid:33) , cnoise(s), cnoise(t) (cid:18) 1 (t s) (cid:19) (cid:16) (1 )x + ( xt + σ2 α2 (cid:112) σd (cid:33) , cnoise(s), cnoise(t) (cid:18) 1 (t s) (cid:19) (cid:16) (cid:124) (1 (cid:123)(cid:122) Gtarget )x + ( t 1)xt 1)xt (cid:17) (cid:17) (cid:125) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) where (cid:18) Gtarget = (cid:18) = (cid:18) = 1 (t s) 1 (t s) 1 (t s) = ϵ (1 (1 s (cid:19) (cid:16) (cid:19) (cid:16) (cid:19) )x + ( )x + ( s 1)((1 t)x + tϵ) (cid:17) 1 + t)x + (s t)ϵ (cid:17) ((t s)x + (s t)ϵ) which is v-target under OT-FM schedule. This parameterization naturally allows zero-SNR sampling and satisfies boundary condition at = 0, similar to Simple-EDM above. This is not true for Identity parametrization using Gθ as it satisfies boundary condition only at > 0. C.6. Mapping Function r(s, t) We discuss below the concrete choices for r(s, t). We use constant decrement ϵ > 0 in different spaces. Constant decrement in η(t) := ηt = σt/αt. This is the choice that we find to work better than other choices in practice. First, let its inverse be η1(), r(s, t) = max (cid:0)s, η1(η(t) ϵ)(cid:1) We choose ϵ = (ηmax ηmin)/2k for some k. We generally choose ηmax 160 and ηmin 0. We find = {10, . . . , 15} works well enough depending on datasets. 20 Constant decrement in t. Inductive Moment Matching r(s, t) = max (s, ϵ) We choose ϵ = (T ϵ)/2k. Constant decrement in λ(t) := log-SNRt = 2 log(αt/σt). Let its inverse be λ1(), then r(s, t) = max (cid:0)s, λ1(λ(t) ϵ))(cid:1) We choose ϵ = (λmax λmin)/2k. This choice comes close to the first choice, but we refrain from this because r(s, t) becomes close to both when 0 and 1 instead of just 1. This gives more chances for training instability than the first choice. Constant increment in 1/η(t). r(s, t) = max (cid:18) s, η (cid:18) (cid:19)(cid:19) 1 1/η(t) ϵ We choose ϵ = (1/η(t)min 1/η(t)max)/2k. C.7. Time Distribution p(s, t) In all cases we choose p(t) = U(ϵ, ) and p(st) = U(ϵ, t) for some ϵ 0 and 1. The decision for time distribution is coupled with r(s, t). We list the constraints on p(s, t) for each r(s, t) choice below. Constant decrement in η(t). We need to choose < 1 because, for example, assuming OT-FM schedule, ηt = t/(1 t), one can observe that constant decrement in ηt when 1 results in r(s, t) that is too close to due to ηts exploding gradient around 1. We need to define < 1 such that r(s, ) is not too close to for reasonably far away. With ηmax 160, we can choose = 0.994 for OT-FM and = 0.996 for VP-diffusion. Constant decrement in t. No constraints needed. = 1, ϵ = 0. Constant decrement in λt. One can similarly observe exploding gradient causing r(s, t) to be too close to at both 0 and 1, so we can choose ϵ > 0, e.g. 0.001, in addition to choosing = 0.994 for OT-FM and = 0.996 for VP-diffusion. Constant increment in 1/ηt. This experience exploding gradient for 0, so we require ϵ > 0, e.g. 0.005. And = 1. C.8. Kernel Function For our Laplace kernel k(x, y) = exp( w(s, t) max(x y2, ϵ)/D), we let ϵ > 0 be reasonably small constant, e.g. 108. Looking at its gradient w.r.t. x, xe w(s,t) max(xy2,ϵ)/D = (cid:40) w(s,t) w(s,t)xy2/D xy xy2 , 0, if y2 > ϵ otherwise one can notice that the gradient is self-normalized to be unit vector, which is helpful in practice. In comparison, the gradient of RBF kernel of the form k(x, y) = exp 1 (cid:17) (cid:16) , 2 w(s, t)x y2/D xe w(s,t)xy2/D = w(s, t) w(s,t) 1 2 xy2/D(x y) (68) whose magnitude can vary lot depending on how far is from y. For w(s, t), we find it helpful to write out the L2 loss between the arguments. For simplicity, we denote ˆGθ(xt, s, t) = 21 Inductive Moment Matching , cnoise(s), cnoise(t) (cid:19) (cid:18) Gθ σd xt α2 +σ2 w(s, t) = w(s, t) s,r (x r) (cid:13) (cid:13) s,t(xt) θ (cid:13)f θ (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13)cskip(s, t)xt + cout(s, t) ˆGθ(xt, s, t) (cid:13) = w(s, t)cout(s, t) (cid:13) (cid:13) ˆGθ(xt, s, t) (cid:13) (cid:13) (cid:16) 1 cout(s, t) (cid:16) cskip(s, r)x + cout(s, r) ˆG(x r, s, r) (cid:17)(cid:13) (cid:13) (cid:13) cskip(s, r)x + cout(s, r) ˆG(x r, s, r) cskip(s, t)xt (cid:17)(cid:13) (cid:13) (cid:13) (cid:13)2 We simply set w(s, t) = 1/cout(s, t) for the overall weighting to be 1. This allows invariance of magnitude of kernels w.r.t. t. C.9. Weighting Function w(s, t) To review VDM (Kingma et al., 2021), the negative ELBO loss for diffusion model is LELBO(θ) = Ex,ϵ,t 1 2 (cid:20)(cid:18) (cid:19) dt λt ϵθ(xt, t) ϵ2 (cid:21) (69) where ϵθ is the noise-prediction network and λt = log-SNRt. The weighted-ELBO loss proposed in Kingma & Gao (2024) introduces an additional weighting function w(t) monotonically increasing in (monotonically decreasing in log-SNRt) understood as form of data augmentation. Specifically, they use sigmoid as the function such that the weighted ELBO is written as Lw-ELBO(θ) = Ex,ϵ,t (cid:20) σ(b λt) (cid:18) (cid:19) dt λt ϵθ(xt, t) ϵ2 (cid:21) 1 (70) where σ() is sigmoid function. The αt is tailored towards the Simple-EDM and Euler-FM parameterization as we have shown in Appendix C.5 that the networks σdGθ amounts to v-prediction in cosine and OT-FM schedules. Notice that ELBO diffusion loss matches Inspecting the gradient of Laplace kernel, we have (again, for simplicity we let ˆGθ(xt, s, t) = ϵ instead of v. Gθ( , cnoise(s), cnoise(t))) xt α2 +σ2 σd w(s,t) (cid:13) (cid:13)f θ (cid:13) s,t(xt)f θ s,r (xr) (cid:13) (cid:13) (cid:13) /D θ w(s, t) = w(s,t) (cid:13) (cid:13)f θ (cid:13) s,t(xt)f θ s,r (xr) = w(s, t) w(s,t) (cid:13) (cid:13)f θ (cid:13) s,t(xt)f θ s,r (xr) (cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) /D θ (cid:13) (cid:13)f θ s,t(xt) θ s,r (xr) (cid:13) s,t(xt) θ s,r (xr) (cid:13)2 /D ˆGθ(xt, s, t) ˆGtarget (cid:13) (cid:13) ˆGθ(xt, s, t) ˆGtarget (cid:13) (cid:13) (cid:13)2 (cid:13) θ θ s,t(xt) θ θ s,t(xt) for some constant ˆGtarget. We can see that gradient s,t(xt) is guided by vector ˆGθ(xt, s, t) ˆGtarget. Assuming ˆGθ(xt, s, t) is v-prediction, as is the case for Simple-EDM parameterization with cosine schedule and Euler-FM parameterization with OT-FM schedule, we can reparameterize vto ϵ-prediction with ϵθ as the new parameterization. We omit arguments to network for simplicity. We show below that for both cases ϵθ ϵtarget = αt( ˆGθ ˆGtarget) for some constants ϵtarget and ˆGtarget. For Simple-EDM, we know x-prediction from v-prediction parameterization (Salimans & Ho, 2022), xθ = αtxt σt ˆGθ, and we also know θ θ 22 x-prediction from ϵ-prediction, xθ = (xt σtϵθ)/αt. We have Inductive Moment Matching = αtxt σt ˆGθ xt αtσt ˆGθ )xt + αtσt ˆGθ = σtϵθ xt σtϵθ αt xt σtϵθ = α2 (1 α2 σ2 ϵθ = σtxt + αt ˆGθ ϵθ ϵtarget = σtxt + αt ˆGθ xt + αtσt ˆGθ = σtϵθ (cid:16) σtxt + αt ˆGtarget (cid:17) ϵθ ϵtarget = αt( ˆGθ ˆGtarget) (71) (72) (73) (74) (75) (76) (77) For Euler-FM, we know x-prediction from v-prediction parameterization, xθ = xt ˆGθ and we also know x-prediction from ϵ-prediction, xθ = (xt tϵθ)/(1 t). We have xt tϵθ 1 = xt ˆGθ xt tϵθ = (1 t)xt t(1 t) ˆGθ txt + t(1 t) ˆGθ = tϵθ ϵθ = xt + (1 t) ˆGθ ϵθ = xt + αt ˆGθ ϵθ ϵtarget = xt + αt ˆGθ (cid:16) xt + αt ˆGtarget ϵθ ϵtarget = αt( ˆGθ ˆGtarget) (78) (79) (80) (81) (82) (83) (84) (cid:17) In both cases, ( ˆGθ(xt, s, t) ˆGtarget) can be rewritten to (ϵθ(xt, s, t) ϵtarget) by multiplying factor αt, and the guidance vector now matches that of the ELBO-diffusion loss. Therefore, we are motivated to incorporate αt into w(s, t) as proposed. The exponent for αa the gradient gradient takes value of either 1 or 2. We explain the reason for each decision here. When = 1, we guide s,t(xt), with score difference (ϵθ(xt, s, t) ϵtarget). To motivate = 2, we first note that the weighted θ θ w(s, t) θ θ s,t(xt) = 1 cout(s, t) θ θ s,t(xt) = θ ˆGθ(xt, s, t) and as shown above that ϵ-prediction is parameterized as ϵθ(xt, s, t) = xt + αt ˆGθ(xt, s, t). ˆGθ(xt, s, t) therefore implicitly reparameterizes our model into an ϵ-prediction model. Multiplying an additional αt to θ The case of = 2 therefore implicitly reparameterizes our model into an ϵ-prediction model guided by the score difference (ϵθ(xt, s, t) ϵtarget). Empirically, α2 additionally downweights loss for larger compared to αt, allowing the model to train on smaller time-steps more effectively. Lastly, the division of α2 + σ2 Matching training. This is purely an empirical decision. is inspired by the increased weighting for middle time-steps (Esser et al., 2024) for Flow D. Training Algorithm E. Classifier-Free Guidance We refer readers to Appendix C.5 for analysis of each parameterization. Most notably, the network Gθ in both (1) SimpleEDM with cosine diffusion schedule and (2) Euler-FM with OT-FM schedule are equivalent to v-prediction parameterization 23 Inductive Moment Matching Algorithm 3 IMM Training Input: model θ, data distribution q(x) and label distribution q(cx) (if label is used), prior distribution (0, σ2 dI), time distribution p(t) and p(st), DDIM interpolator DDIM(xt, x, s, t) and its flow coefficients αt, σt, mapping function r(s, t), kernel function k(, ), weighting function w(s, t), batch size B, particle number , label dropout probability Output: learned model θ s,t Initialize 0, θ0 θ while model not converged do i=1 and ri = r(si, ti) for each i. This results in tuple {(si, ri, ti)}B/M Sample batch of data, label, and prior, and split into B/M groups, {(x(i,j), c(i,j), ϵ(i,j))}B/M,M i=1,j=1 For each group, sample {(si, ti)}B/M x(i,j) ti DDIM(ϵ(i,j), x(i,j), ti, 1) = αtix(i,j) + σtiϵ(i,j), (i, j) x(i,j) ri DDIM(x(i,j) , x(i,j), ri, ti), (i, j) (Optional) Randomly drop each label c(i,j) to be null token with probability θn+1 optimizer step by minimizing ˆLIMM(θn) using model θn (see Eq. (67)) (optionally inputting c(i,j) into network) i=1 ti end while in diffusion (Salimans & Ho, 2022) and FM (Lipman et al., 2022). When conditioned on label during sampling, it is customary to use classifier-free guidance to reweight this v-prediction network via Gw θ (cin(t)xt, cnoise(s), cnoise(t), c) = wGθ(cin(t)xt, cnoise(s), cnoise(t), c) + (1 w)Gθ(cin(t)xt, cnoise(s), cnoise(t), ) with guidance weight so that the classifier-free guided θ s,t,w(xt) is s,t,w(xt) = cskip(s, t)xt + cout(s, t)Gw θ θ (cin(t)xt, cnoise(s), cnoise(t), c) (85) (86) F. Sampling Algorithms Pushforward sampling. We assume series of time steps {ti}N the maximum time and minimum time ϵ. Denote σd as data standard deviation. i=0 with = tN > tN 1 > > t2 > t1 > t0 = ϵ for Algorithm 4 Pushforward Sampling Input: model θ, time steps {ti}N Output: xt0 Sample xN (0, σ2 for = N, . . . , 1 do dI) i=0, prior distribution (0, σ2 dI), (optional) guidance weight (Optional) 1 if = 1 ti1,ti(xti) or θ xti1 θ ti1,ti,w(xti) end for // can optionally discard unconditional branch for = Restart sampling. Different from pushforward sampling, time steps {ti}N i=0 do not need to be strictly decreasing for all time steps, e.g. = tN tN 1 t2 t1 t0 = ϵ (assuming > ϵ). Different from pushforward sampling, restart sampling first denoise clean sample before resampling noise to be added to this clean sample. Then clean sample is predicted again. The process is iterated for steps. G. Connection with Prior Works G.1. Consistency Models Consistency models explicitly match PF-ODE trajectories using network gθ(xt, t) that directly outputs sample given any xt qt(xt). The network explicitly uses EDM parameterization to satisfy boundary condition gθ(x0, 0) = x0 and trains via loss Ext,x,t where xr is deterministic function of xt from an ODE solver. gθ(xt, t) gθ (xr, r)2(cid:105) (cid:104) 24 Algorithm 5 Restart Sampling Inductive Moment Matching i=0, prior distribution (0, σ2 dI), DDIM interpolant coefficients αt and σt, (optional) Input: model θ, time steps {ti}N guidance weight Output: xt0 Sample xN (0, σ2 for = N, . . . , 1 do dI) (Optional) 1 if = 1 θ t0,ti(xti ) or θ if = 1 then t0,ti,w(xti) ϵ (0, σ2 dI) xti1 αti1 + σti1 ϵ else xt0 = end if end for // can optionally discard unconditional branch for = 1 // or more generally xti1 qt(xti1x, ϵ) We show that CM loss is special case of our simplified IMM objective. Lemma 1. When xt = Ext,x,t w(t)gθ(xt, t) gθ (xr, r)2(cid:105) t, xr = (cid:104) + for valid r(t) < and some constant C. r, k(x, y) = y2, and > 0 is small constant, Eq. (12) reduces to CM loss Proof. Since xt = k(f θ s,r(xr), θ t, xr = s,t(x r). So k(f θ r)) = 1 by definition. Since k(x, y) = y2, it is easy to see Eq. (12) reduces to r, we have θ s,r(xr) = θ s,t(xt) = θ t) and θ s,r(x s,r(x s,t(xt), θ s,t(x t)) = (cid:104) Ext,t w(s, t)(cid:13) (cid:13)f θ s,t(xt) θ s,r(xr)(cid:13) (cid:13) 2(cid:105) + (87) where = 2 and w(s, t) is weighting function. If is small positive constant, we further have θ s,t(xt) gθ(xt, t) where we drop as input. If gθ(xt, t) itself satisfies boundary condition at = 0, we can directly take = 0 in which case θ 0,t(xt) = gθ(xt, t). And under these assumptions, our loss becomes Ext,x,t w(t)gθ(xt, t) gθ(xr, r)2(cid:105) (cid:104) + C, (88) which is simply CM loss using ℓ2 distance. However, one can notice that from moment-matching perspective, this loss is problematic in two aspects. First, it assumes single particle estimate, which now ignores the entropy repulsion term in MMD that arises only during multi-particle estimation. This can contribute to mode collapse and training instability of CM. Second, the choice of energy kernel only matches the first moment, which is insufficient for matching two complex distributions! We should use kernels that match higher moments in practice. In fact, we show in the following Lemma that the pseudo-huber loss proposed in Song & Dhariwal (2023) matches higher moments as kernel. Lemma 2. Negative pseudo-huber loss kc(x, y) = that matches all moments of and where weights on higher moments depend on c. y2 + c2 for > 0 is conditionally positive definite kernel y2 + c2 is conditionally positive definite kernel (Auffray Proof. We first check that negative pseudo-huber loss & Barbillon, 2009). By definition, k(x, y) is conditionally positive definite if for x1, , xn RD and c1, , cn RD with (cid:80)n i=1 ci = (cid:113) (cid:113) (cid:88) (cid:88) i=1 j= cicjk(xi, xj) 0 25 (89) We know that negative L2 distance y is conditionally positive definite. We prove this below for completion. Due to triangle inequality, y y. Then Inductive Moment Matching (cid:88) (cid:88) i=1 j=1 cicj (xi xj) (cid:88) (cid:88) i=1 j=1 cicj (xi xj) ci (cid:88) j=1 (cid:88) i=1 cjxj (cid:88) j=1 cj (cid:32) (cid:88) i= (cid:33) cixi = (a) = 0 where (a) is due to (cid:80)n i=1 ci = 0. Now since (cid:113) z2 + c2 for all > 0, we have (cid:88) (cid:88) i=1 j= (cid:18) cicj (cid:113) xi xj2 + c2 (cid:19) (cid:88) (cid:88) i=1 j=1 cicj (xi xj) (90) (91) (92) (93) So negative pseudo-huber loss is valid conditionally positive definite kernel. Next, we analyze pseudo-huber losss effect on higher-order moments by directly Taylor expanding = (cid:113) z2 + c2 at (cid:113) z2 + c2 = = z2 1 8c3 y2 1 2c 1 2c 1 8c3 1 16c5 y4 + 1 16c5 5 128c7 y6 z4 + z6 z8 + O(z9) 5 128c7 y8 + O(x y9) (94) (95) (96) where we substitute = y. Each higher order yk for > 2 expands to polynomial containing up to k-th moments, i.e., {x, x2, . . . xk}, {y, y2, . . . yk}, thus the implicit feature map contains all higher moments where contributes to the weightings in front of each term. Furthermore, we extend our finite difference (between r(s, t) and t) IMM objective to the differential limit by taking r(s, t) in Appendix H. This results in new objective that similarly subsumes continuous-time CM (Song et al., 2023; Lu & Song, 2024) as single-particle special case. G.2. Diffusion GAN and Adversarial Consistency Distillation Diffusion GAN (Xiao et al., 2021) parameterizes its generative distribution as (cid:90) pθ st(xsxt) = qst(xsGθ(xt, z), xt)p(z)dz where Gθ is neural network, p(z) is standard Gaussian distribution, and qst(xsx, xt) is the DDPM posterior qst(xsx, xt) = (µs,t, σ2 s,tI) (97) µQ = αtσ2 αsσ2 xt + αs(1 α2 α2 σ2 σ2 )x = σ2 σ2 (1 α2 α2 σ2 σ2 ) Note that DDPM posterior is stochastic interpolant, and more importantly, it is self-consistent, which we show in the Lemma below. Lemma 6. For all 0 < 1, DDPM posterior distribution from to as defined in Eq. (97) is self-consistent Gaussian interpolant between and xt. 26 (98) (99) (100) Proof. Let xr qrt(xrx, xt) and xs qsr(xsx, xr), we show that xs follows qst(xsx, xt). Inductive Moment Matching xr = xs = αtσ2 αrσ2 αrσ2 αsσ2 xt + αr(1 xr + αs(1 (cid:115) )x + σr 1 (cid:115) )x + σs 1 α2 α2 σ2 σ2 α2 α2 σ2 σ2 α2 α2 σ2 σ2 ϵ1 α2 α2 σ2 σ2 ϵ where ϵ1, ϵ2 (0, I) are i.i.d. Gaussian noise. Directly expanding xt + αr(1 α2 α2 σ2 σ2 (cid:115) )x + σr 1 (cid:35) ϵ1 + αs(1 α2 α2 σ2 σ2 α2 α2 σ2 σ2 (cid:115) )x + σs 1 α2 α2 σ2 σ2 ϵ2 (1 α2 α2 σ2 σ2 ) + αs(1 (cid:21) ) + α2 α2 σ2 σ2 (cid:34) (cid:115) σr 1 α2 α2 σ2 σ2 (cid:35) (cid:115) ϵ1 + σs 1 α2 α2 σ2 σ2 ϵ2 (101) (cid:34) αtσ2 αrσ2 (cid:20) α2 rσ2 αsσ2 xt + xs = = = αrσ2 αsσ2 αtσ2 αsσ2 αtσ2 αsσ2 xt + αs(1 α2 α2 σ2 σ2 )x + (a) = αtσ2 αsσ2 xt + αs(1 α2 α2 σ2 σ2 (cid:115) 1 α2 α2 σ2 σ2 αrσ2 αsσr (cid:115) α2 α2 σ2 σ2 ϵ3 )x + σs 1 αrσ2 αsσ2 (cid:115) ϵ1 + σs 1 α2 α2 σ2 σ2 ϵ (102) (103) where (a) is due to the fact that sum of two independent Gaussian variables with variance a2 and b2 is also Gaussian with variance a2 + b2, and ϵ3 (0, I) is another independent Gaussian noise. We show the calculation of the variance: α2 α2 σ4 σ2 (1 α2 α2 σ2 σ2 ) + σ2 (1 α2 α2 σ2 σ2 ) = (cid:0)α2 σ4 (cid:0) σ2 α2 (cid:0) + σ2 (cid:0)α2 σ4 (cid:0) σ2 α2 (cid:0) t σ4 α2 σ2 α2 σ2 σ2 ) α2 α2 This shows xs follows qst(xsx, xt) and completes the proof. = σ2 (1 This shows another possible design of the interpolant that can be used, and diffusion GANs formulation generally complies with our design of the generative distribution, except that it learns this conditional distribution of given xt directly while we learn marginal distribution. When they directly learn the conditional distribution by matching pθ st(xxt) with qt(xxt), the model is forced to learn qt(xxt) and there only exists one minimizer. However, in our case, the model can learn multiple different solutions because we match the marginals instead. GAN loss and MMD loss. We also want to draw attention to similarity between GAN loss used in Xiao et al. (2021); Sauer et al. (2025) and MMD loss. MMD is an integral probability metric over set of functions in the following form DF (P, Q) = sup (cid:12) (cid:12)ExP (x)f (x) EyQ(y)f (y)(cid:12) (cid:12) where supremum is taken on this set of functions. This naturally gives rise to an adversarial optimization algorithm if is defined as the set of neural networks. However, MMD bypasses this by selecting as the RKHS where the optimal can be analytically found. This eliminates the adversarial objective and gives stable minimization objective in practice. However, this is not to say that RKHS is the best function set. With the right optimizers and training scheme, the adversarial objective may achieve better empirical performance, but this also makes the algorithm difficult to scale to large datasets. G.3. Generative Moment Matching Network It is trivial to check that GMMN is special parameterization. We fix = 1, and due to boundary condition, r(s, t) = 0 implies training target pθ st(xs) is simple pushforward of prior p(ϵ) through network gθ(ϵ) where drop dependency on and since they are constant. sr(s,t)(xs) = qs(xs) is the data distribution. Additionally, pθ 27 H. Differential Inductive Moment Matching Inductive Moment Matching Similar to the continuous-time CMs presented in (Lu & Song, 2024), our MMD objective can be taken to the differential limit. Consider the simplifed loss and parameterization in Eq. (12), we use the RBF kernel as our kernel of choice for simplicity. Theorem 2 (Differential Inductive Moment Matching). Let θ s,t(xt) be twice continuously differentiable function with bounded first and second derivatives, let k(, ) be RBF kernel with unit bandwidth, x, q(x), xt qt(xtx), qs(x tx), xr = DDIM(xt, x, t, r) and t, x, t, r), the following objective = DDIM(x LIMM-(θ, t) = lim rt 1 (t r)2 Ext,x t,xr,x (cid:16) (cid:104) s,t(xt), θ θ (cid:16) s,t(x t) (cid:17) (cid:16) + s,r(xr), θ θ (cid:16) (cid:17) s,t(xt), θ θ s,r(x r) s,t(x θ t), θ s,r(xr) (cid:17)(cid:105) s,r(x r) (cid:17) (104) can be analytically derived as (cid:34) Ext,x 2 θ s,t(x t)f θ s,t(xt) 1 (cid:32) df θ s,t(xt) dt df θ s,t(x t) dt df θ s,t(xt) dt (cid:16) s,t(xt) θ θ s,t(x t) (cid:17)(cid:16) s,t(xt) θ θ s,t(x t) (cid:17) df θ s,t(x t) dt (105) (cid:33)(cid:35) Proof. Firstly, the limit can be exchanged with the expectation due to dominated convergence theorem where the integrand consists of kernel functions which can be assumed to be upper bounded by 1, e.g. RBF kernels are upper bounded by 1, and thus integrable. It then suffices to check the limit of the integrand. Before that, let us review the first and second-order Taylor expansion of exy2 . We let a, RD be constants to be expanded around. We note down the Taylor expansion to second-order below for notational convenience. exb2/2 at = a: eab2/2 eab2/2(a b)(x a) + eya2/2 at = b: eba2/2 eba2/2(b a)(y b) + 1 2 1 2 exy2/2 at = a, = b: (x a)eab2/2 (cid:0)(a b)(a b) I(cid:1) (x a) (y b)eba2/2 (cid:0)(b a)(b a) I(cid:1) (y b) eab2/2 eab2/2(a b)(x a) eba2/2(b a)(y b) 1 2 1 2 + (y b)eba2/2 (cid:0)(b a)(b a) I(cid:1) (y b) (y b)eba2/2 (cid:0)(b a)(b a) I(cid:1) (y b) + + (x a)eba2/2 (cid:0)I (a b)(a b)(cid:1) (y b) Putting it together, the above results imply exy2/2 + eab2/2 exb2/2 eya2/2 (x a)eba2/2 (cid:0)I (a b)(a b)(cid:1) (y b) since it is easy to check that the remaining terms cancel. 28 Substituting = θ s,t(xt), = θ s,r(xr), = θ s,t(x t), = θ s,r(x r), we furthermore have Inductive Moment Matching lim rt 1 (x a) = lim rt = lim rt 1 1 = df θ s,t(xt) dt s,t(xt) θ s,r(xr)(cid:3) (cid:2)f θ (cid:34) (t r) df θ s,t(xt) dt (cid:35) + O(t r2) Similarly, lim rt 1 (y b) = lim rt 1 Therefore, LIMM-(θ, t) can be derived as (cid:2)f θ s,t(x t) θ s,r(x r)(cid:3) = df θ s,t(x t) dt (cid:34) Ext,x 2 θ s,t(x t)f θ s,t(xt)2 df θ s,t(xt) dt (cid:16) (cid:16) s,t(xt) θ θ s,t(x t) (cid:17)(cid:16) s,t(xt) θ θ s,t(x t) (cid:35) (cid:17)(cid:17) df θ s,t(x t) dt (cid:34) 1 2 θ s,t(x t)f θ s,t(xt) (cid:32) df θ s,t(xt) dt df θ s,t(x t) dt df θ s,t(xt) dt (cid:16) s,t(xt) θ θ s,t(x t) (cid:17)(cid:16) s,t(xt) θ θ s,t(x t) (cid:17) df θ s,t(x t) dt = Ext,x H.1. Pseudo-Objective (106) (107) (108) (109) (110) (111) (112) (cid:33)(cid:35) Due to the stop-gradient operation, we can similarly find pseudo-objective whose gradient matches the gradient of LIMM-(θ, t) in the limit of t. Theorem 3. Let θ k(, ) be RBF kernel with unit bandwidth, x, q(x), xt qt(xtx), = DDIM(x s,t(xt) be twice continuously differentiable function with bounded first and second derivatives, tx), xr = DDIM(xt, x, t, r) and t, x, t, r), the gradient of the following pseudo-objective qt(x θL IMM-(θ, t) = lim rt 1 (t r) θExt,x t,xr,x (cid:16) (cid:104) s,t(xt), θ θ (cid:16) s,t(x t) (cid:17) (cid:16) + s,r (xr), θ θ (cid:16) s,r (x r) (cid:17) (cid:17) s,t(xt), θ θ s,r (x r) s,t(x θ t), θ s,r (xr) (cid:17)(cid:105) can be used to optimize θ and can be analytically derived as (cid:34) 1 2 Ext,x (cid:13) (cid:13)f θ (cid:13) s,t (x t)f θ s,t (xt) 2 (cid:32) (cid:13) (cid:13) (cid:13) (cid:32) (cid:32) df θ s,t (x t) dt df θ s,t (xt) dt (cid:104) (cid:104) s,t (xt) θ θ s,t (x t) (cid:105) df θ s,t (x t) dt s,t (xt) θ θ s,t (x t) (cid:105) df θ s,t (xt) dt (cid:104) (cid:104) 29 s,t (xt) θ θ (cid:105)(cid:33) s,t (x t) θf θ s,t(xt)+ s,t (xt) θ θ (cid:105)(cid:33) s,t (x t) θf θ s,t(x t) (cid:33)(cid:35) (113) (114) Inductive Moment Matching Proof. Similar to the derivation of LIMM-(θ, t), let = θ s,t(xt), = θ s,r (xr), = θ s,t(x t), = θ s,r (x r), we have lim rt 1 (t r) θ(x a)eba2/2 (cid:0)I (a b)(a b)(cid:1) (y b) = lim rt 1 (t r) eba2/2 (cid:34)(cid:32) (y b) (cid:17) (cid:16) (a b)(y b) (cid:33) (a b) θx+ (cid:32) (x a) (cid:16) (a b)(x a) (cid:17) (a b) (cid:33) (cid:35) θy where lim rt 1 (t r) (x a) = df θ s,t (xt) dt , lim rt 1 (t r) (y b) = df θ s,t (x t) dt s,t (xt) Note that df θ s,t(xt) outside of the brackets, so (x a) and (y b) merely require evaluation at current θ with no gradient information, which θ satisfies. The objective can be derived as is now parameterized by θ instead of θ because the gradient is already taken w.r.t. θ dt θL IMM-(θ, t) (cid:34) (cid:13) (cid:13)f θ (cid:13) 1 2 = Ext,x s,t (x t)f θ s,t (xt) 2(cid:32) (cid:13) (cid:13) (cid:13) (cid:32) (cid:32) df θ s,t (x t) dt df θ s,t (xt) dt (cid:104) (cid:104) s,t (xt) θ θ s,t (x t) (cid:105) df θ s,t (x t) dt s,t (xt) θ θ s,t (x t) (cid:105) df θ s,t (xt) dt (cid:104) (cid:104) H.2. Connection with Continuous-Time CMs s,t (xt) θ θ (cid:105)(cid:33) s,t (x t) θf θ s,t(xt)+ s,t (xt) θ θ (cid:105)(cid:33) s,t (x t) θf θ s,t(x t) (cid:33)(cid:35) Observing Eq. (105) and Eq. (110), we can see that when xt = s,t(xt)(cid:13) (cid:13) s,t(x θ (cid:13)f θ (cid:13) the dependency on as input. Then, Eq. (105) reduces to s,t(xt), and exp t) = θ t) θ s,t(x 1 2 2(cid:17) (cid:16) = 1, and θ and xr = s,t(x r, being small positive constant, then t) gθ(xt, t) where since is fixed we discard Ext (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) df θ s,t(xt) dt 2(cid:35) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) which is the same as differential consistency loss (Song et al., 2023; Geng et al., 2024). And Eq. (110) reduces to (cid:34) df θ s,t (xt) dt Ext (cid:35) θf θ s,t(xt) (115) (116) which is the pseudo-objective for continuous-time CMs (Song et al., 2023; Lu & Song, 2024) (minus weighting function of choice). I. Experiment Settings I.1. Training & Parameterization Settings We summarize our best runs in Table 5. Specifically, for ImageNet-256256, we adopt latent space paradigm for computational efficiency. For its autoencoder, we follow EDM2 (Karras et al., 2024) and pre-encode all images from Parameterization Setting Architecture GFlops Params (M) cnoise(t) 2nd time conditioning Flow Trajectory gθ(xt, s, t) σd Training iter Training Setting Dropout Optimizer Optimizer ϵ β1 β2 Learning Rate Weight Decay Batch Size Kernel r(s, t) Minimum t, gap p(t) EMA Rate Label Dropout x-flip EDM augment (Karras et al., 2022) Inference Setting Sampler Type Number of Steps Schedule Type FID-50K (w = 0) FID-50K (w = 1.0, i.e. no guidance) FID-50K (w = 1.5) Inductive Moment Matching CIFAR-10 DDPM++ 21.28 55 1000t OT-FM Simple-EDM 0.5 400K 0.2 RAdam 108 0.9 0.999 0.0001 0 4096 4 Laplace max(s, η1(ηt (ηmaxηmin) - U(0.006, 0.994) 1 5 0.9999 - True True 215 )) DiT-S 6.06 33 1000t OT-FM Euler-FM 0.5 1.2M 0 AdamW 108 0.9 0.999 0.0001 0 4096 4 Laplace - U(0, 0.994) 1 4 0.9999 0.1 False False ImageNet-256256 DiT-B 23.01 130 1000t OT-FM Euler-FM 0.5 1.2M DiT-L 80.71 458 1000t OT-FM Euler-FM 0.5 1.2M DiT-XL 118.64 675 1000t OT-FM Euler-FM 0.5 1.2M 0 AdamW 108 0.9 0.999 0.0001 0 4096 4 Laplace 0 AdamW 108 0.9 0.999 0.0001 0 4096 4 Laplace max(s, η1(ηt (ηmaxηmin) - - U(0, 0.994) U(0, 0.994) 1 1 4 4 0.9999 0.9999 0.1 0.1 False False False False 212 0 AdamW 108 0.9 0.999 0.0001 0 4096 4 Laplace )) - U(0, 0.994) 1 4 0.9999 0.1 False False DiT-XL 118.64 675 1000t OT-FM Euler-FM 0.5 1.2M 0 AdamW 108 0.9 0.999 0.0001 0 4096 4 Laplace 104 U(0, 0.994) 2 4 0.9999 0.1 False False Pushforward 2 t1 = 1.4 1.98 - - Pushforward 8 Uniform - 42.28 20.36 Pushforward 8 Uniform - 26.02 9. Pushforward 8 Uniform - 9.33 2.80 Pushforward 8 Uniform - 7.25 2.13 Pushforward 8 Uniform - 6.69 1.99 Table 5. Experimental settings for different architectures and datasets. ImageNet into latents without flipping, and calculate the channel-wise mean and std for normalization. We use Stable Diffusion VAE2 and rescale the latents by channel mean [0.86488, 0.27787343, 0.21616915, 0.3738409] and channel std [4.85503674, 5.31922414, 3.93725398, 3.9870003]. After this normalization transformation, we further multiply the latents by 0.5 so that the latents roughly have std 0.5. For DiT architecture of different sizes, we use the same hyperparameters for all experiments. Choices for and ϵ. By default assuming we are using mapping function r(s, t) by constant decrement in ηt, we keep ηmax 160. This implies that for time distribution of the form U(ϵ, ), we set = 0.996 for cosine diffusion and = 0.994 for OT-FM. For ϵ, we set it differently for pixel-space and latent-space model. For pixel-space on CIFAR-10, we follow Nichol & Dhariwal (2021) and set ϵ to small positive constant because pixel quantization makes smaller noise imperceptible. We find 0.006 to work well. For latent-space on ImageNet-256256, we have no such intuition as in pixel-space. We simply set ϵ = 0 in this case. Exceptions occur when we ablate other choices of r(s, t), e.g. constant decrement in λt in which case we set ϵ = 0.001 to prevent r(s, t) for being too close to when is small. Injecting time s. The design for additionally injecting can be categorized into 2 types injecting directly and injecting stride size (t s). In both cases, architectural designs exactly follow the time injection of t. We simply extract positional time embedding of (or s) fed through 2-layer MLP (same as for t) before adding this new embedding to the embedding of after MLP. The summed embedding is then fed through all the Transformer blocks as in standard DiT architecture. 2https://huggingface.co/stabilityai/sd-vae-ft-mse 31 Inductive Moment Matching 1-step 2-step 4-step 8-step = 1 = 2 7. 8.28 4.01 4.08 2.61 2.60 2. 2.01 Table 6. Ablation of exponent in the weighting function on ImageNet-256256. We see that = 2 excels at multi-step generation while lagging slightly behind in 1-step generation. 1-step 2-step 4-step 8-step TF32 w/ = 1 FP16 w/ = 1 FP16 w/ = 2 7.97 8.73 8. 4.01 4.54 3.99 2.61 3.03 2. 2.13 2.38 1.99 Table 7. Ablation of lower precision training on ImageNet-256256. For lower precision training, we employ both minimum gap = 104 and (t r) conditioning. Improved CT baseline. For ImageNet-256256, we implement iCT baseline by using our improved parameterization with Simple-EDM and OT-FM schedule. We use the proposed pseudo-huber loss for training but find training often collapses using the same r(s, t) schedule as ours. We carefully tune the gap to achieve reasonable performance without collapse and present our results in Table 2. I.2. Inference Settings Inference schedules. For all one-step inference, we directly start from ϵ (0, σ2 dI) at time to time ϵ through pushforward sampling. For all 2-step methods, we set the intermediate timestep t1 such that ηt1 = 1.4; this choice is purely empirical which we find to work well. For 4 steps we explore two types of time schedules: (1) uniform decrement in with η0 < η1 < ηN where and (2) EDM (Karras et al., 2022) time schedule. EDM schedule specifies η0 < η1 < ηN where ti = + (ϵ ) (cid:16) 1 ρ max + η ηi = 1 ρ min η (η (cid:17)ρ 1 ρ max) and ρ = 7 (117) (118) We slightly modify the schedule so that η0 = ηmin is the endpoint instead of η1 = ηmin and η0 = 0 as originally proposed, since our η0 can be set to 0 without numerical issue. We also specify the time schedule type used for our best runs in Table 5 and their results. I.3. Scaling Settings Model GFLOPs. We reuse numbers from DiT (Peebles & Xie, 2023) for each model architecture. Training compute. Following Peebles & Xie (2023), we use the formula model GFLOPs batch size training steps 4 for training compute where, different from DiT, we have constant 4 because for each iteration we have 2 forward pass and 1 backward pass, which is estimated as twice the forward compute. Inference compute. We calculate inference compute via model GFLOPs number of steps. I.4. Ablation on exponent We compare the performance between = 1 and = 2 on full DiT-XL architecture in Table 6, which shows how affects results of different sampling steps. We observe that = 2 causes slightly higher 1-step sampling FID but outperforms = 1 in the multi-step regime. I.5. Caveats for Lower-Precision Training For all experiments, we follow the original works (Song et al., 2020b; Peebles & Xie, 2023) and use the default TF32 precision for training and evaluation. When switching to lower precision such as FP16, we find that our mapping function, i.e. constant decrement in ηt, can cause indistinguishable time embedding after some MLP layers when is large. To mitigate this issue, we simply impose minimum gap between any and r, for example, = 104. Our resulting mapping function becomes Inductive Moment Matching (cid:32) r(s, t) = max s, min (cid:16) (cid:17) , η1(η(t) ϵ) (cid:33) Optionally, we can also increase distinguishability between nearby time-steps inside the network by injecting (r s) instead of as our second time condition. We use this as default for FP16 training. With these simple changes, we observe minimal impact on generation performance. Lastly, if training from scratch with lower precision, we recommend FP16 instead of BF16 because of higher precision that is needed to distinguish between nearby and r. We show results in Table 7. For FP16, = 1 causes slight performance degradation because of the small gap issue at large t. This is effectively resolved by = 2 which downweights losses at large by focusing on smaller instead. At lower precision, while not necessary, = 2 is an effective solution to achieve good performance that matches or even surpasses that of TF32. J. Additional Visualization We present additional visualization results in the following page. 33 Inductive Moment Matching Figure 12. Uncurated samples on CIFAR-10, unconditional, 2 steps. 34 Inductive Moment Matching Figure 13. Uncurated samples on ImageNet-256256 using DiT-XL/2 architecture. Guidance = 1.5, 8 steps. 35 Inductive Moment Matching Figure 14. Uncurated samples on ImageNet-256256 using DiT-XL/2 architecture. Guidance = 1.5, 8 steps."
        }
    ],
    "affiliations": [
        "Luma AI",
        "Stanford University"
    ]
}