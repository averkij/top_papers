{
    "paper_title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA",
    "authors": [
        "Seanie Lee",
        "Sangwoo Park",
        "Dong Bok Lee",
        "Dominik Wagner",
        "Haebin Seong",
        "Tobias Bocklet",
        "Juho Lee",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA), which introduces a product of two trainable low-rank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update ($BA$) intensifies this effect. Freezing one matrix (e.g., $A$) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, a simple yet effective method that introduces a global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the $B$ matrix and transmits it to the server. The server aggregates the $B$ matrices, computes the product $BA$ using the previous $A$, and refactorizes the result via SVD. This yields a new adaptive $A$ composed of the orthonormal right singular vectors of $BA$, and an updated $B$ containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing $A$ to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of $A$ bounds the gradient norms of $B$ and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As a result, FedSVD consistently improves stability and performance across a variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 5 0 8 2 1 . 5 0 5 2 : r FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA Seanie Lee1 Sangwoo Park1* Dong Bok Lee1* Dominik Wagner2 Haebin Seong1 Tobias Bocklet2 Juho Lee1 Sung Ju Hwang1 1KAIST 2Technische Hochschule Nürnberg Georg Simon Ohm 3DeepAuto.ai {lsnfamily02, swgger, markhi}@kaist.ac.kr dominik.wagner@th-nuernberg.de, hbseong97@gmail.com tobias.bocklet@th-nuernberg.de, {juholee, sjhwang82}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA), which introduces product of two trainable lowrank matrices into frozen pre-trained weights, is widely used for efficient fine-tuning of language models in federated learning (FL). However, when combined with differentially private stochastic gradient descent (DP-SGD), LoRA faces substantial noise amplification: DP-SGD perturbs per-sample gradients, and the matrix multiplication of the LoRA update (BA) intensifies this effect. Freezing one matrix (e.g., A) reduces the noise but restricts model expressiveness, often resulting in suboptimal adaptation. To address this, we propose FedSVD, simple yet effective method that introduces global reparameterization based on singular value decomposition (SVD). In our approach, each client optimizes only the matrix and transmits it to the server. The server aggregates the matrices, computes the product BA using the previous A, and refactorizes the result via SVD. This yields new adaptive composed of the orthonormal right singular vectors of BA, and an updated containing the remaining SVD components. This reparameterization avoids quadratic noise amplification, while allowing to better capture the principal directions of the aggregate updates. Moreover, the orthonormal structure of bounds the gradient norms of and preserves more signal under DP-SGD, as confirmed by our theoretical analysis. As result, FedSVD consistently improves stability and performance across variety of privacy settings and benchmarks, outperforming relevant baselines under both private and non-private regimes."
        },
        {
            "title": "Introduction",
            "content": "Language models have demonstrated remarkable performance across various tasks [31, 22, 6]. While these models provide strong general capabilities, adapting them to specific domains or tasks typically requires fine-tuning with domain-specific datasets [3]. In real-world deployments, however, training data is frequently fragmented across various organizations or user devices, and strict privacy regulations often prohibit direct data sharing [8]. Federated Learning [FL; 19] provides viable solution by allowing clients to fine-tune models locally on their private data, while central server aggregates model updates without accessing raw training data, enabling privacy-preserving collaborative training. In FL, individual clients often lack the computational and memory capacity required for full finetuning of large models, making such approaches impractical. Parameter-efficient fine-tuning addresses this by freezing most model parameters and updating only small subset, enabling scalable model Equal Contribution. Preprint. Under review. Figure 1: (a) At communication round i, the server computes the SVD of Bi ˆAi1, i.e., UiΣiV and initializes ˆAi Each client updates only the matrix ˆBpkq The locally optimized ˆBpkq Bi ˆAi1, and ˆBi UiΣi. These reparameterized matrices are then broadcast to all clients. (b) 0 , initialized with ˆBi, using its local dataset, while keeping ˆAi fixed. (c) τ matrices are aggregated at the server to update the global model. adaptation in resource-constrained settings. In particular, Low-Rank Adaptation [LoRA; 14] has been widely adopted for fine-tuning models in FL environments due to its low local computation and communication requirements [38, 30, 10, 33]. Although FL improves privacy by exchanging model updates instead of raw data, it does not provide formal guarantees against information leakage. Sophisticated attacks such as membership inference [26] or model inversion [9], can potentially reconstruct sensitive information from shared updates, particularly given the capacity of language models to memorize training data [4, 5]. Therefore, integrating differential privacy [DP; 7] is essential to provide formal privacy guarantees and enhance the trustworthiness of collaborative model training. common approach to enforcing DP in deep neural networks is DP-SGD [29, 2, 1], which clips the norm of each per-sample gradient to predefined threshold and adds Gaussian noise to the average of clipped gradients. Recent work [30] has shown that naïve integration of LoRA into DP-SGD significantly degrades model performance. Following single DP-SGD update of the LoRA adapter matrices and B, the noise added to both matrices is amplified through their matrix product BA, as shown in Eq. 5. To mitigate this amplification, FFA-LoRA [30] proposes freezing the matrix with fixed, randomly initialized value, while optimizing only the matrix. However, using fixed random matrix for limits the learning capability of LoRA, and we observe that optimizing only leads to significantly slower convergence. Ideally, we would like to adapt over time to better capture the principal direction of aggregated updates without incurring noise amplification under DP-SGD. To this end, we propose FedSVD, simple yet effective method. In the first round, the server randomly initializes A0 and B0 and broadcasts them to the participating clients. Each client then optimizes only the matrix using its local data, and the server aggregates the updated matrices. In each subsequent round, the server refactorizes the product of the aggregated and the previous using singular value decomposition (SVD) to obtain the matrices for the next iteration. As shown in Fig. 1-(a), the rows of are re-initialized with orthonormal right singular vectors (i.e., ) of BA obtained from the SVD. The re-initialization of uses the remaining components of the SVD, namely the left singular vectors and singular values (i.e., UiΣi). The newly initialized matrices ˆAi and ˆBi are then broadcast to clients. Each client k, its local matrix ˆBpkq is initialized with ˆBi. It is further optimized to produce ˆBpkq τ matrices are aggregated at the server (Fig. 1-(c)). , while keeping ˆAi fixed (Fig. 1-(b)). Finally, the ˆBpkq 0 τ This SVD-based reparameterization has several merits. First, it enables to adapt based on the aggregated without amplifying noise. Since SVD is post-processing step after local updates with DP-SGD, it preserves the differential privacy guarantee. Secondly, the orthonormality of beneficially bounds gradient norms for B, preserving more update signal under DP-SGD compared to randomly initialized matrix. Lastly, we theoretically show that the orthonormal rows of induce lower condition number of the Hessian compared to random matrix in simple logistic regression, implying better-conditioned loss landscape that can potentially lead to faster convergence. Empirically, we observe accelerated accuracy improvement of deep models with orthonormal rows of  (Fig. 3)  . We empirically evaluate FedSVD on several benchmark datasets under both private and non-private settings. Across both regimes, it consistently outperforms baselines over most communication rounds and achieves the highest final accuracy. We summarize our findings and contributions as follows: We propose FedSVD, simple yet effective method allowing LoRA matrix to adapt over time based on aggregated updates of using SVD, while eliminating noise amplification under DP-SGD. 2 We theoretically show that orthonormal rows of improve the condition number of the Hessian in logistic regression. We empirically demonstrate that our FedSVD approach achieves better accuracy and faster convergence than relevant baselines under DP-SGD across several benchmark datasets."
        },
        {
            "title": "2 Method",
            "content": "This section reviews the necessary background, including federated learning with LoRA, DP-SGD, and FFA-LoRA in Sec. 2.1, before introducing our proposed method, FedSVD, and its theoretical foundations in Sec. 2.2. detailed discussion of related work on federated learning, DP-guaranteed federated learning, and parameter-efficient fine-tuning is deferred to Appendix B. 2.1 Background Federated learning with LoRA. Let pθ : Ñ be language model (e.g., [18, 6]) parameterized by θ, which maps an input token sequence to an output class label Y. In the FL framework, each client rKs : t1, . . . , Ku has access only to its local training dataset Dk tpxpkq Dk1 for all k, k1 rKs with k1. Furthermore, the central server never accesses any local datasets directly. At each update round rRs, random subset of client indices Si Ă rKs is selected such that Si 1. Each selected client Si then receives copy of the current global model parameters θi from the central server and trains its local model pθpkq using its private dataset Dk as follows: i1, where Dk , ypkq qunk Ş i,t`1 θpkq θpkq i,t ηθLpθpkq i,t ; Dkq, Lpθpkq i,t ; Dkq 1 nk ÿ px,yqPDk log pθpkq i,t py xq, (1) for 0, . . . , τk 1, where η ą 0 is the learning rate and θpkq 0,0 is initialized with θi. Since full fine-tuning of pθ is computationally expensive, Low-Rank Adaptation [LoRA; 14] is commonly employed to reduce overhead by injecting trainable low-rank matrices into the weight matrix of each layer l: pk,lq i,t plq 0 ` Apk,lq i,t Bpk,lq i,t , (2) is frozen pre-trained weight matrix of pθi, and Apk,lq i,t tpApk,lq where plq Rdoutˆr 0 are the corresponding low-rank matrices. We denote θpkq l1 as the set of LoRA adapter weights for client at step of round i, where each pair pApk,lq represents the LoRA matrices in layer l. In methods such as FedAvg [19] and FedIT [38], the server updates its parameters θi tpAplq Rrˆdin and Bpk,lq , Bpk,lq quL i,t , Bpk,lq i,t l1 by aggregating the weights from the participating clients as follows: quL , Bplq i,t i,t i,t i,t nk mi Apk,lq i,τk , Bplq i`1 nk mi Bpk,lq i,τk , (3) ÿ kPSi Aplq i`1 ÿ kPSi ř where mi updated weight matrix for each layer rLs as follows: kPSi nk and nk Dk. At round ` 1, the central server model pθi`1 uses the plq i`1 plq 0 ` Aplq i`1Bplq i`1, i`1Bplq where plq 0 denotes the frozen pre-trained weights and Aplq i`1 is the aggregated low-rank update. Differential privacy. Language models tend to memorize training data, which can lead to the leakage of private information from local client datasets [4, 5]. Differential privacy (DP) [7] provides formal privacy guarantee by limiting the influence of any individual data point on the model, thereby mitigating such leakage risks. Definition 1 (pϵ, δq-DP). randomized algorithm is pϵ, δq-differentially private if, for all neighboring datasets D, D1 that differ in exactly one entry, and all subsets of the possible outputs of , we have PrpM pDq Eq ď eϵPrpM pD1q Eq ` δ, where ϵ is the privacy budget, and δ is the probability of privacy violation. (4) 3 In FL, privacy guarantees depend on whether the central server is trusted. In the centralized DP setting, clients send raw updates without local privacy measures, and DP is applied during global aggregation [20]. In the local DP setting, which assumes an untrusted server, each client ensures its update is differentially private before communication [35, 16, 23]. Our work adopts this stronger local DP setting: we apply DP at the client level so that any shared updates (i.e., model parameters) are already privatized. By the composition property of DP, the final global model also satisfies DP. Fixed LoRA matrix. common approach to ensuring the differential privacy of deep neural networks is DP-SGD [29, 2, 1]. DP-SGD first clips each per-sample gradient gpxiq from sampled mini-batch to have bounded norm by applying gpxiq{ maxp1, gpxiq2 {Cq, where is predefined threshold. Gaussian noise ξ p0, σ2C 2Iq is then added to average of the clipped gradients. Finally, the noisy gradients are averaged to update the model. However, updating both and B, and aggregating them poses challenge for fine-tuning models with DP-SGD. During client-side fine-tuning, Gaussian noise is added to the average of the clipped gradients of and B, which becomes amplified through their post-update matrix product after single DP-SGD step: pBpk,lq i,t ` ξpk,lq qpApk,lq i,t ` ξpk,lq Bpk,lq i,t Apk,lq i,t ` ξpk,lq Apk,lq i,t ` Bpk,lq ` ξpk,lq ξpk,lq ξpk,lq , (5) and ξpk,lq where ξpk,lq represent the Gaussian noise added by DP-SGD. To mitigate the noise amplification caused by the LoRA matrix product, FFA-LoRA [30] fixes as randomly initialized matrix and performs aggregation only over B: plq i`1 plq 0 ` Bpk,lq i,τk Aplq fixed. (6) ÿ kPSi nk mi This removes the quadratic noise (i.e., ξpk,lq under DP-SGD. However, using fixed random matrix Aplq LoRA, potentially leading to suboptimal performance. ), as well as ξpk,lq ξpk,lq , thereby stabilizing model training fixed may impair the learning capacity of 2.2 Our Method: FedSVD While FFA-LoRA mitigates noise amplification by freezing matrix A, this can lead to suboptimal adaptation, as fixed random projection may not align well with the data distribution or the dynamics of local model updates. Ideally, should adapt over time to better capture the principal directions of the aggregated updates, while avoiding noise amplification under DP-SGD. Periodic re-initialization of via SVD. To this end, we propose FedSVD, simple yet effective approach that avoids direct optimization of matrix by periodically resetting it to new matrix with orthonormal rows, obtained via SVD of the aggregated product BA. Specifically, before broadcasting the newly aggregated matrix Bi to the clients, the server computes the SVD of Bi ˆAi1, where ˆAi1 is the matrix from the previous round 1, and initializes ˆAi and ˆBi as follows: ˆBi : Uir:, : rsΣir: r, : rs, ˆAi : Jr: r, :s, UiΣiV Bi ˆAi1, (7) where ˆB0 0, ˆA0 is initialized with Kaiming uniform [12], r:, : rs and r: r, :s denote the first columns and rows of the matrix , respectively. Note that we omit the superscript for brevity. Each client receives ˆAi and ˆBi, and optimizes only ˆBi, using Eq. 1 on its local dataset Dk. The server then aggregates the optimized ˆBi matrices from all participating clients. We outline our complete method in Alg. 1. Importantly, this reparameterization does not change the value of Bi ˆAi1, i.e., Bi ˆAi1 ˆBi ˆAi, since rankpBi ˆAi1q ď by the design of LoRA. Therefore, the rank-r SVD exactly recovers Bi ˆAi1. As result, all clients receive consistent, globally synchronized initialization after SVD, while benefiting from updated, data-informed ˆA matrices instead of relying on fixed random projection. As demonstrated in Sec. 3, this strategy empirically stabilizes training and accelerates optimization. Bounding the gradient norm. Moreover, the orthonormality of ˆA ensures that its spectral norm is exactly 1, which leads to tighter bound on the gradient norm of B. Denoting the output as 4 Algorithm 1 FedSVD 1: Input: Pre-trained language model pθ, client datasets tDkuK learning rate η, batch size b, rank r, the number of participating clients 1. k1, total optimization rounds R, for for 1, . . . , do Ui, Σi, if ą 0 then 2: for 0, . . . , 1 do 3: 4: 5: 6: 7: 8: 9: 10: 11: end if ˆBplq else 0 Ð 0, ˆAplq 12: 13: 14: 15: 16: end for for 1, . . . , do i`1 Ð kPSi ř Bplq end for 17: 18: 19: end for Ð SVDpBplq Ź Broadcast global parameters i1q, ˆBplq ˆAplq Ð Uir:, : rsΣir: r, : rs, ˆAplq Ð r: r, :s 0 Ð Kaiming_Uniformpd, dq end for Sample set of clients Si Ă t1, . . . , Ku with Si 1, mi Ð 0. for each client Si do Initialize the client parameter θpkq Optimize ˆBpk,lq i,0 uL nk Ð Dk, mi Ð mi ` nk, l1 on Dk with SGD for τk steps with Eq. 1. i,0 tp ˆApk,lq i,0 , ˆBpk,lq i,0 quL l1 Ð tp ˆAplq Ź Done in parallel , ˆBplq l1. quL Ź Aggregation of parameters updated by the clients nk mi ˆBpk,lq i,τk (cid:13) (cid:13) (cid:13) (cid:13) Bℓpzq Bz (cid:13) (cid:13) (cid:13) (cid:13)2 pW0 ` ˆAqx, we compute: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Bℓpzq BB Bℓpzq Bz ˆAxqJ (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:13) (cid:13) (cid:13) Bℓpzq Bz (cid:13) (cid:13) (cid:13) (cid:13)2 ˆAx2 ď ˆA2 x2 x2 , (8) where ℓpzq is loss function with corresponding label y, is the Frobenius norm, ˆA2 is the spectral norm of ˆA, and x2 is the l2-norm of x. Under DP-SGD, each per-example gradient is clipped to fixed norm before noise addition. Thus, any implicit amplification introduced by ˆA directly increases the amount of clipping, distorting the original gradient and weakening the update signal. Since ˆA2 1, gradients reach the clipping threshold with minimal norm, preserving more genuine update signal under given privacy budget. In contrast, random initializations typically yield A2 ą 1, necessitating more aggressive clipping and slowing optimization. (cid:13) (cid:13) (cid:13) (cid:13) Bℓpzq Bz (cid:13) (cid:13) (cid:13) (cid:13)2 Privacy guarantee of FedSVD. Due to the post-processing invariance property of DP [7, Proposition 2.1], FedSVD guarantees DP by design, as the SVD is applied only after has already been privatized. Corollary 2.1 (Privacy guarantee). By Theorem 1 and the moment accountant from Abadi et al. [1], FedSVD with DP-SGD and FedAvg aggregation satisfies pϵ, δq-DP, given sampling rate q, the total logpq{δq{ϵ for some number of local updates τ per client, and noise multiplier σ ě constant c. Proof. This is direct application of the post-processing invariance property of DP [7, Proposition 2.1] and Theorem 1 in Abadi et al. [1]. Theoretical analysis. We analyze how the reparameterization of and via SVD affects the optimization dynamics of logistic regression. Consider labeled dataset Dk tpxi, yiqunk i1, and define ř nk the binary cross entropy loss as LkpB; Aq 1 i1 pyi logpσpziqq p1 yiq logp1 σpziqqq, nk where σptq 1{p1`expptqq is the sigmoid function and the logit is given by zi pW `BAqxi with R1ˆdx , Rrˆdx , R1ˆr, and Rdx . The Hessian HkpB; Aq of the loss function LkpB; Aq with respect to is given by HkpB; Aq : 2 BLkpB; Aq AMkAJ, where Mk 1 . We define the condition number of the Hessian as nk κ2pHkpB; Aqq λmaxpHkpB; Aqq{λminpHkpB; Aqq, where λmax and λmin denote the largest and smallest non-zero singular values, respectively. Theorem 2.2. The condition number of the Hessian is upper bounded by κ2pHkpB; Aqq ď κ2pAq2 λminpMkRpAJ qq , where κ2pAq is the condition number of and λminpMkRpAJqq denotes nk i1 σpziqp1 σpziqqxixJ λmaxpMkq ř 5 Table 1: Results on 5 GLUE tasks without privacy constraints. We report average accuracy and 95% confidence intervals over 5 runs. The best and second-best results are highlighted in bold and underline, respectively. Method Comm. Cost (# params.) MNLI Matched Mismatched SST-2 QQP QNLI Average FedAvg FFA-LoRA FLoRA FedEX-LoRA 786,432 393,216 52,169,730 52,169,730 74.79 14.92 82.75 1.72 50.49 14.93 56.85 14.41 75.09 15.04 83.45 1.84 50.81 15.27 57.74 14.81 85.89 12.12 94.06 0.18 58.99 12.47 59.43 12. 61.75 10.06 78.00 3.08 57.91 7.31 64.86 2.39 71.40 12.78 86.61 1.22 62.16 10.41 64.90 12.84 73.78 6.61 84.98 0.99 56.07 9.26 60.76 4.06 FedSVD (ours) 393,2162 83.96 2. 84.32 2.27 94.26 0.51 79.82 2.43 88.98 1.43 86.27 1.44 Figure 2: Accuracy vs. communication rounds without privacy constraints across 5 GLUE tasks. Curves show average accuracy over 5 runs, with shaded regions indicating 95% confidence intervals. the smallest eigenvalue of Mk restricted to the row space of A. If the rows of are orthonormal, the upper bound of the condition number of the becomes tighter: κ2pHkpB; Aqq ď λmaxpMkq λminpMkRpAJ qq . The proof is deferred to Appendix A. By reparameterizing with SVD as ˆA Jr: r, :s and ΣV BA, the singular values of ˆA are all 1; thus, κ2p ˆAq 1. This eliminates the factor κ2pAfixedq2 ě 1, which is present when using fixed matrix with random initialization. In general, if Afixed is initialized randomly (e.g., from Gaussian or uniform distributions), its condition number satisfies κ2pAfixedq ş 1 with high probability. lower condition number for the Hessian generally implies better-behaved optimization landscape, which can lead to faster and more stable convergence for gradient-based methods to update B. Therefore, our SVD-based reparameterization improves the stability of local client optimization steps by promoting well-conditioned projection matrix A."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setups Datasets. Following FFA-LoRA [30], we use four datasets from the GLUE benchmark [32]: Multi-Genre Natural Language Inference [MNLI; 34], sentence-pair classification task for textual entailment with three labels (entailment, neutral, contradiction), evaluated on both matched (indomain) and mismatched (cross-domain) test sets; Stanford Sentiment Treebank v2 [SST-2; 28], single-sentence sentiment classification task with two labels (positive, negative); Quora Question Pairs [QQP; 25], paraphrase detection task with two labels (duplicate, not duplicate); and Question Natural Language Inference [QNLI; 32], binary classification task with two labels (entailment, not entailment) that determines whether context sentence answers given question extracted from [24]. We use the validation set for evaluation. Baselines. We compare our method, FedSVD, against the following baselines: 1. FedAvg [19, 38]: Both and matrices are fine-tuned locally and averaged independently, as described in Eq. 3. 2. FFA-LoRA [30]: The matrices are initialized with Kaiming uniform initialization [12] and remain fixed during training. Only the matrices are fine-tuned and aggregated. 3. FLoRA [33]: Both and matrices are fine-tuned locally and aggregated by stacking the individual matrices from all clients, rather than averaging them independently. The central server 2The communication cost is evaluated under the setting where SVD computations are performed by both the server and clients, avoiding transmitting to clients. Please refer to Sec. 4 for more details. 6 Table 2: Results on 5 GLUE tasks with DP (ϵ 6, δ 105). We report average accuracy and 95% confidence intervals over 10 runs. The best and second-best results are highlighted in bold and underline, respectively. Method Comm. Cost (# params.) MNLI Matched Mismatched SST-2 QQP QNLI Average FedAvg FFA-LoRA FLoRA FedEX-LoRA 786,432 393,216 52,169,730 52,169,730 65.45 6.14 55.56 8.58 48.01 10.76 54.98 8.16 67.02 5.93 56.39 8.94 48.86 11.22 56.02 8.10 89.41 2.18 91.42 0.87 91.83 1.13 87.34 1. 58.59 5.27 64.35 3.26 63.18 5.16 53.29 8.46 60.70 5.27 72.39 4.96 49.48 0.03 49.86 0.35 67.17 2.63 68.02 3.37 59.78 4.87 60.86 3.05 FedSVD (ours) 393,216 71.68 3. 73.03 2.90 91.32 0.85 72.42 2.36 75.50 4.20 76.79 1.81 Figure 3: Accuracy vs. communication rounds with DP (ϵ 6, δ 105) across five GLUE tasks. Curves show average accuracy over 10 runs, with shaded regions indicating 95% confidence intervals. computes the product BA from the stacked matrices and adds it to the pre-trained weight matrix W0. After aggregation, randomly re-initialized A, and updated W0 are sent back to the clients. 4. FedEx-LoRA [27]: Both and matrices are fine-tuned and aggregated individually as described in Eq. 3. The residual, which is defined as the difference between the aggregated BA and the product of the aggregated and fixed A, is added to the frozen pre-trained matrix W0. Implementation details. To better emulate realistic federated learning scenarios, only half of the clients are randomly sampled for participation in each communication round. Unless specified otherwise (see Fig. 4b), we use six clients in total. To emulate non-i.i.d data conditions, we sample client data proportions from Dirichlet distribution [13], with α = 0.5 by default (see Fig. 4a for an ablation on varying α values). For the GLUE benchmark, we use RoBERTa-large [18] as the base model and apply LoRA [14] with rank 8 and scaling factor α 8 to the query and value projections, using LoRA dropout rate of 0.05. All non-LoRA parameters, including the classification head, are frozen. We run 100 communication rounds, with participating clients in each round updating their weights via vanilla SGD for τ 10 local steps. Due to the absence of validation split in the GLUE benchmark, we refrain from extensive hyperparameter tuning. Instead, we adopt values that work reasonably well for FedAvg: learning rate η 0.5, clipping norm 2, and δ 105. We use the Opacus library [36] to automatically compute the noise multiplier σ given ϵ, δ, and the total number of steps Rτ . The same hyperparameters are applied to all methods for fair comparison. We use 3 NVIDIA RTX A6000 GPUS for all experiments. 3.2 Main Results Effectiveness of FedSVD without privacy constraints. We first assess FedSVD on the GLUE benchmark in non-private setting. As shown in Table 1, FFA-LoRA outperforms FedAvg, which we attribute to reduced aggregation error. In contrast, FLoRA, which transmits large number of parameters, underperforms due to the frequent random re-initialization of the and matrices in our experimental setup. We observe similar pattern in FedEX-LoRA. The proposed FedSVD further improves performance by periodically adapting through SVD of the product BA rather than using fixed A. As result, FedSVD achieves the highest average accuracy across all tasks, outperforming the second-best baseline (FFA-LoRA) by 1.29 percentage points. Fig. 2 illustrates accuracy as function of communication rounds for FedAvg, FFA-LoRA, and FedSVD, with FedSVD consistently outperforming the baselines throughout training. This robustness to early stopping makes FedSVD well-suited for scenarios with limited communication budgets or uncertain convergence points. Effectiveness of FedSVD with DP-SGD. We further evaluate the performance of FedSVD under DP constraints (ϵ 6, δ 105). Table 2 shows that the performance gain of FedSVD over FFA- (a) α (b) Total number of clients (K) Figure 4: (a): Results of varying α t0.1, 0.2, 0.3, 0.4, 0.5u on the MNLI dataset. (b): Results of varying the total number of clients (K t6, 9, 12u, and 1 3) on the MNLI dataset. LoRA increases substantially in the DP setting, rising from 1.29 percentage points without privacy to 8.77 percentage points under DP constraints. We attribute this improvement to the SVD-based re-initialization in FedSVD. This allows to capture the principal directions of the aggregated updates more reliably. Furthermore, orthonormal rows of bound the gradient norm of B, making more robust to gradient clipping under DP-SGD. Fig. 3 further demonstrates the effectiveness of SVD re-initialization: FedSVD consistently exhibits better convergence behavior compared to FFA-LoRA across most training rounds. Although we observe slight performance drop on SST-2 after round 80, FedSVD maintains strong overall accuracy, showing its robustness to DP noise and suitability for real-world federated learning deployments. Σ0r: r, : rsV 3.3 Analysis Initialization of A. To better understand the effect of initialization strategies for matrix A, we compare three classes of configurations. First, we randomly initialize with orthonormal rows and keep it fixed during training (Afixed w/ random orthonormal). Second, following PiSSA [21], we factorize the frozen pre-trained matrix using SVD: U0Σ0V 0 W0 and initialize and with Σ0r: r, : rs, respectively. The base matrix W0 is re-initialized with its residual component 1 0 U0r:, r`1 :sΣ0rr`1 :, r`1 :sV 0 rr`1 :, :s. Both this updated W0 and are frozen, while only is updated during training (Afixed w/ PiSSA). Lastly, we consider an Σr: r, : rsV Jr: r, :s alternative SVD-based initialization where is periodically re-initialized with and with r:, : rs Σr: r, : rs from the SVD of BA, which does not preserve the orthonormality of As rows (FedSVD w/o orthonormal). We report both matched and mismatched accuracy on the MNLI dataset for each configuration. 0 r: r, :s and U0r:, : rs Table 3: Accuracy on the MNLI dataset with different initializations of A. The methods with : indicate that matrices are periodically updated. Table 3 shows that introducing structural priors into matrix A, such as enforcing orthonormality or initializing via PiSSA, helps stabilize training and yields better performance compared to unstructured baselines. However, when is kept fixed throughout the training, the improvements are limited, suggesting that adaptivity plays crucial role beyond the prior itself. In addition, we find that removing the orthonormality constraint from our FedSVD degrades performance, indicating that the orthonormal structure of is not merely beneficial for initialization but remains important throughout the learning dynamics. Afixed (FFA-LoRA) Afixed w/ random orthonormal Afixed w/ PiSSA FedSVD w/o orthonormal: 55.56 8.58 55.58 5.97 66.32 2.87 70.76 3.75 56.39 8.94 56.96 5.98 67.57 2.79 71.86 3.79 Matched Mismatched FedSVD: (Ours) 71.68 3. 73.03 2.90 Method Heterogeneity of data distribution. To assess the robustness of FedSVD under varying degrees of non-i.i.d. data, we partition the MNLI dataset across clients using Dirichlet distribution with concentration parameter α t0.1, 0.2, . . . , 0.5u. For each setting, we train models using DP-SGD and report the mean and standard deviation over 5 independent runs. We compare FedSVD with FedAvg and FFA-LoRA across all levels of heterogeneity. As shown in Fig. 4a, our proposed FedSVD consistently outperforms the baselines all tested levels of data heterogeneity, except for α 0.1 where all methods fail due to extreme data heterogeneity. These experimental results highlight the robustness of FedSVD to varying degrees of data heterogeneity and its superior performance compared to the baselines under such conditions. 8 Varying the number of clients. To evaluate the robustness of each method in more realistic federated settings, we vary the total number of clients 6, 9, 12 while keeping the number of participating clients per round fixed at 1 3. We compare the performance of FedAvg, FFA-LoRA, and FedSVD with DP-SGD on the MNLI dataset and report the mean and standard deviation over 5 independent runs for each configuration. Fig. 4b shows that FedSVD consistently outperforms the baselines across all values of K. Notably, the performance degradation with increasing is significantly smaller for FedSVD, highlighting its robustness to the number of clients, which is crucial in realistic federated learning scenarios. SVD re-initialization frequency. To examine the effect of the re-initialization frequency in FedSVD, we conduct an ablation study where our SVD-based initialization is applied every 1, 2, 5, 10 communication rounds. Each configuration is denoted as FedSVD (p), where denotes re-initialization interval in rounds. As shown in Fig. 5, all FedSVD variants exhibit better convergence than FFA-LoRA, confirming the benefit of SVD re-initialization and its robustness to the choice of re-initialization interval. Given their comparable performance, variants with less frequent re-initialization offer favorable tradeoff when computational efficiency is prioritized. Moreover, the final accuracy remains stable across different re-initialization schedules, demonstrating the robustness of FedSVD to this hyperparameter. Figure 5: Results of varying the period of SVD using the MNLI dataset."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we proposed FedSVD, simple yet effective method for fine-tuning language models with DP-SGD in FL. Instead of using fixed random matrix for LoRA, we periodically refactorize the product of two LoRA adapter matrices BA with SVD and initialize with the right singular vectors of BA. As remains untrained and SVD is applied post-privatization of B, our method preserves differential privacy without incurring additional noise from matrix multiplication. Empirically, FedSVD consistently outperforms relevant baselines, often achieving faster convergence. Since our method is compatible with any architecture employing LoRA, extending the empirical evaluation of FedSVD to wider range of foundation models across different modalities is promising direction for future work. Furthermore, deeper theoretical analysis FedSVDs convergence dynamics, particularly for complex non-linear models, could provide valuable insights. Limitations. Although our approach shows promising results in both private and non-private federated learning settings, the computation of SVD incurs additional overhead on the server side. However, since SVD is performed on low-rank matrices, this overhead can be significantly reduced by employing randomized low-rank approximation methods, such as the algorithm proposed by Halko et al. [11, Algorithm 5.1]. Another limitation is the additional communication overhead from broadcasting the newly initialized ˆA matrix to clients after each SVD step. However, this cost, can be avoided by decentralizing the SVD computation. After aggregating Bi, the server computes ˆAi via SVD on the product Bi ˆAi1 and transmits only Bi to the clients. Each client then reconstructs ˆAi locally using the same procedure and obtains the updated pair ˆBi, ˆAiq. Since only ˆBi is optimized during training while ˆAi remains fixed, there is no need to transmit or aggregate ˆAi at the server. Broader impact. FedSVD advances data privacy in AI development by enabling more stable and effective training of large-scale models under differential privacy within federated learning framework, ensuring that sensitive data remains local to each client. By improving the robustness of privacy-preserving fine-tuning for foundation models, FedSVD contributes to reducing the risk of information leakage and supports the responsible deployment of AI systems in sensitive domains."
        },
        {
            "title": "References",
            "content": "[1] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308318, 2016. [2] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science, pages 464473. IEEE, 2014. [3] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. 30th USENIX Security Symposium (USENIX Security 21), 2021. [5] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. International Conference Learning Representations (ICLR), 2023. [6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology. org/N19-1423/. [7] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy, volume 9 of Foundations and Trends in Theoretical Computer Science. Now Publishers Inc., Hanover, MA, 2014. ISBN 9781601988188. [8] European Parliament and Council of the European Union. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation). https://eur-lex.europa.eu/ legal-content/EN/TXT/HTML/?uri=OJ:L:2016:119:FULL, 2016. OJ 119, 4.5.2016, p. 188. [9] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 13221333, 2015. [10] Pengxin Guo, Shuang Zeng, Yanran Wang, Huijie Fan, Feifei Wang, and Liangqiong Qu. Selective aggregation for low-rank adaptation in federated learning. International Conference on Learning Representations (ICLR), 2025. [11] Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53 (2):217288, 2011. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. International Conference on Computer Vision (ICCV), 2015. [13] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. International Conference on Learning Representations (ICLR), 2022. 10 [15] Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language models based on low-rank and sparse approximation. International Conference on Machine Learning (ICML), 2023. [16] Zitao Li, Bolin Ding, Ce Zhang, Ninghui Li, and Jingren Zhou. Federated matrix factorization with privacy guarantee. Proceedings of the VLDB Endowment, 15(4), 2021. [17] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. International Conference on Machine Learning (ICML), 2024. [18] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [19] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera Arcas. Communication-efficient learning of deep networks from decentralized data. Artificial intelligence and statistics (AISTATS), 2017. [20] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language models. International Conference on Learning Representations (ICLR), 2018. [21] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. Neural Information Processing Systems (NeurIPS), 2024. [22] Gemma Team Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, L. Sifre, Morgane Riviere, Mihir Kale, Christopher Love, Pouya Dehghani Tafti, Leonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vladimir Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Brian Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeffrey Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [23] Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork. Natural language understanding with privacy-preserving bert. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 14881497, 2021. [24] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras, editors, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264/. [25] Lakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. Natural language understanding with the quora question pairs dataset. arXiv preprint arXiv:1907.01041, 2019. [26] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 318. IEEE, 2017. [27] Raghav Singhal, Kaustubh Ponkshe, and Praneeth Vepakomma. Fedex-lora: Exact aggregation for federated parameter-efficient fine-tuning of foundation models. In NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability, 2024. [28] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://aclanthology.org/D13-1170/. [29] Shuang Song, Kamalika Chaudhuri, and Anand Sarwate. Stochastic gradient descent with differentially private updates. In 2013 IEEE global conference on signal and information processing, pages 245248. IEEE, 2013. [30] Youbang Sun, Zitao Li, Yaliang Li, and Bolin Ding. Improving loRA in privacy-preserving federated learning. Internationl Conference on Learning Representations (ICLR), 2024. [31] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [32] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. International Conference on Learning Representations (ICLR), 2019. [33] Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, and Ang Li. Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations. Neural Information Processing Systems (NeurIPS), 2024. [34] Adina Williams, Nikita Nangia, and Samuel Bowman. broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 11121122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/N18-1101/. [35] Nan Wu, Farhad Farokhi, David Smith, and Mohamed Ali Kaafar. The value of collaboration in convex machine learning with differential privacy. In 2020 IEEE Symposium on Security and Privacy (SP), pages 304317. IEEE, 2020. [36] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021. [37] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500, 2021. 12 [38] Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Tong Yu, Guoyin Wang, and Yiran Chen. Towards building the federatedgpt: Federated instruction tuning. International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. [39] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameterefficient fine-tuning. International Conference on Learning Representations (ICLR), 2023. [40] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Deltalora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Proof of Theorem 2.2 Proof. For notational brevity, we write HkpB; Aq as Hk. Using eigenvalue bounds for matrix products involving positive definite matrices: λmaxpAMkAJq (cid:13) (cid:13)2 ď A2 Mk2 (cid:13) (cid:13)AMkAJ(cid:13) (cid:13)2 A2 2 Mk2 , (cid:13)AJ(cid:13) where 2 is the spectral norm, i.e.A2 σmaxpAq. Since Mk is positive definite matrix, Mk2 λmaxpMkq. Moreover, A2 2 σmaxpAq2, where σmaxpAq is maximum singular value of A. Thus, λmaxpAMkAJq ď σmaxpAq2λmaxpMkq. By Rayleigh quotient characterization, minx0 The image of AJ is RpAJq : tAJx : Rdx and RpAJq. Then, we get xJAMkAJx xJx λminpAMkAJq. Now let AJx. vJMkv vJv ě λminpMkRpAJqq and also xJAAJx AJx2 ě λminpAAJqx2. Hence, combining these, we get xJAMkAJx pAJxqJMkpAJxq ě λminpMkRpAJqqAJx ě λminpMkRpAJqqλminpAAJqx2. Therefore, xJAMkAJx xJx ě λminpMkRpAJqqλminpAAJq λminpMkRpAJqqσminpAq2. Taking the minimum of the left hand side over non-zero vector gives λminpAMkAJq ě λminpMkRpAJqqσminpAq2. The condition number κ2pHkq λmaxpHkq{λminpHkq is upper-bounded: κ2pHkq ď σmaxpAq2λmaxpMkq σminpAq2λminpMkRpAJqq κ2pAq2 λmaxpMkq λminpMkRpAJqq . When the row vectors of are orthonormal, we have AAJ Ir, which implies every singular value of equals 1. Consequently, σminpAq σmaxpAq 1 ñ κ2pAq σmaxpAq σminpAq 1. In this case, neither stretches nor compresses any direction, so the Hessians condition number is governed solely by the spectrum of Mk on RpAJq."
        },
        {
            "title": "B Related Work",
            "content": "Federated learning. Federated Learning (FL) enables decentralized clients to collaboratively train models without sharing raw data. FedAvg [19] averages locally updated model weights to form global model, offering simple yet effective baseline. Built upon FedAvg, recent work has explored integrating Low-Rank Adaptation [LoRA; 14] into FL to reduce communication and computation overhead during model fine-tuning. For instance, Fed-IT [38] updates the adapter matrices and of LoRA, averages each matrices separately. To aggregate product of and A, several methods have been proposed. FedEx-LoRA [27] introduces an additional correction matrix to mitigate aggregation error. FLoRA [33] stacks adapter matrices and reinitializes them randomly at the end of each communication round. FFA-LoRA [30] proposes to use fixed randomly initialized matrix A, while training and aggregating only B. Lastly, Fed-SA [10] proposes learning both matrices and B, but shares only during aggregation. Our method is based on FFA-LoRA; however, we reinitialize the adapter matrices after aggregation to promote gradient stability and learning efficacy. Instead of using fixed random matrix for A, we periodically reinitialize using orthonormal bases via singular value decomposition (SVD) of BA, which empirically accelerates optimization. pϵ, δq-differential privacy [DP; 7] provides Differential privacy guaranteed federated fine-tuning. rigorous framework ensuring that models trained on neighboring datasets, differing by only one data point, produce similar outputs, thereby preserving individual privacy. DP-SGD [29, 2, 1] brings this guarantee to deep learning by adding noise to stochastic gradient updates. In FL, privacy guarantees depend on whether the central server is trusted. In the centralized DP setting, clients send raw updates without local privacy, and DP is applied during global aggregation [20]. In the local DP setting, which assumes an untrusted server, each client ensures its update is differentially private before communication [35, 16, 23]. Our work adopts this stronger setting: we apply DP at the client level, so any shared updates (i.e., model parameters) are already privatized. By the composition property of DP, the final global model also satisfies DP. DP-SGD is unstable with large numbers of trainable parameters due to increased gradient sensitivity and noise injection [1, 37]. To address this, FFA-LoRA [30] fixes the adapter matrix in LoRA to reduce trainable parameters, limiting noise amplification and avoiding quadratic noise growth. Parameter efficient fine-tuning. To mitigate the computational cost of fine-tuning language models, LoRA [14] injects trainable low-rank adapter matrices into some of model components. Subsequent works have proposed variants to improve adaptability and efficiency. For example, DeltaLoRA [40] improves LoRAs expressivity by combining original weights with adapter outputs, thereby enhancing the representational power. LoSparse [15] integrates LoRA with sparsity constraints to prevent the pruning of essential neurons. DoRA [17] separates the magnitude and direction of the update by learning scaling factor for the update , while keeping the direction determined by the LoRA update BA. Unlike these approaches, which aim to learn expressive low-rank approximations of weight updates, PiSSA [21] takes more structural approach. It first decomposes the original weight matrix using SVD, then fine-tunes only the low-rank components corresponding to the top-r singular values, while freezing the residual parts. Our method differs from PiSSA in two key aspects. First, rather than decomposing the pretrained weights, we perform SVD on the aggregated adapter product BA to reinitialize low-rank components after aggregation of optimized on the client side. This is distinct from PiSSAs fixed decomposition of model weights. Second, we enforce the rows of to be orthonormal by initializing them with right singular vectors of BA, which empirically stabilizes training and accelerates optimization compared to non-orthonormal structure. AdaLoRA [39] dynamically learns the optimal rank by parameterizing incremental updates through an SVD to dynamically prune and reallocate rank budget across layers based on the magnitude of the singular values during training. Unlike AdaLoRA, we employ SVD to refactor the aggregated adapter product BA and enforce the rows of to be orthonormal by initializing them with right singular vectors."
        },
        {
            "title": "C Dataset Statistics",
            "content": "Table 4: An overview of datasets used in our experiments. Dataset # Classes # Train # Val # Test MNLI (matched) MNLI (mismatched) SST-2 QQP QNLI 3 2 2 2 392,702 67,349 363,846 104,743 9,815 9,832 872 40,430 5, - - - - - Table 4 summarizes the statistics of the datasets used in our experiments."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: The papers not including the checklist will be desk rejected. The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit. Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist: You should answer [Yes] , [No] , or [NA] . [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available. Please provide short (12 sentence) justification right after your answer (even for NA). The checklist answers are an integral part of your paper submission. They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper. The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided proper justification is given (e.g., \"error bars are not reported because it would be too computationally expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering \"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to question, in the justification please point to the section(s) where related material for the question can be found. 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The claims made in the abstract and introduction accurately reflect the papers contributions and scope. We summarize our contribution in the introduction and support all the claims in the experiments. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have included limitations of our proposed method in conclusion. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. 16 The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: We have provided complete proof in the supplemental material. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have specified all the implementation details in section 4.1. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. 17 Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We use public benchmark datasetes for our experiments and include our code in supplementary file. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? 18 Answer: [Yes] Justification: We have specified all the implementation details in section 4.1. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We perform experiments multiple times with different random seeds and provide means and confidence intervals. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide all details in Section 4.1. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics 19 Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS COde of Ethics and make sure the reserach conform with it. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have included societal impacts in Sec. 4. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No, we do not describe any safeguards in our paper. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. 20 Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, we properly use public benchmark datasets. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not introduce any new assets. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [No] Justification: We do not perform crowdsourcing experiments or research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not perform research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: We only use LLMs for writing and editing. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST",
        "Technische Hochschule Nürnberg Georg Simon Ohm"
    ]
}