{
    "paper_title": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback",
    "authors": [
        "Nour Jedidi",
        "Yung-Sung Chuang",
        "Leslie Shing",
        "James Glass"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using a Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate a large number of tokens for each query. To address these challenges, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as a relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domain-specific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output a single token, thereby improving search latency. Our experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across a wide range of low-resource retrieval datasets while also making significant improvements in latency per-query."
        },
        {
            "title": "Start",
            "content": "Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback Nour Jedidi1 Yung-Sung Chuang2 Leslie Shing1 James Glass2 1MIT Lincoln Laboratory 2Massachusetts Institute of Technology {nour.jedidi, leslie.shing}@ll.mit.edu {yungsung, glass}@mit.edu 4 2 0 2 8 2 ] . [ 1 2 4 2 1 2 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Building effective dense retrieval systems remains difficult when relevance supervision is not available. Recent work has looked to overcome this challenge by using Large Language Model (LLM) to generate hypothetical documents that can be used to find the closest real document. However, this approach relies solely on the LLM to have domain-specific knowledge relevant to the query, which may not be practical. Furthermore, generating hypothetical documents can be inefficient as it requires the LLM to generate large number of tokens for each query. To address these challenges, we introduce Real Document Embeddings from Relevance Feedback (ReDERF). Inspired by relevance feedback, ReDE-RF proposes to re-frame hypothetical document generation as relevance estimation task, using an LLM to select which documents should be used for nearest neighbor search. Through this re-framing, the LLM no longer needs domainspecific knowledge but only needs to judge what is relevant. Additionally, relevance estimation only requires the LLM to output single token, thereby improving search latency. Our experiments show that ReDE-RF consistently surpasses state-of-the-art zero-shot dense retrieval methods across wide range of lowresource retrieval datasets while also making significant improvements in latency per-query."
        },
        {
            "title": "Introduction",
            "content": "Information Retrieval (IR) aims to identify relevant documents from large collection of text given users information needs. With recent advancements in transformer-based language models, dense retrieval techniques (Karpukhin et al., 2020; Xiong et al., 2020; Qu et al., 2020) which map queries and documents to shared semantic embedding space that captures relevance patterns have demonstrated significant success compared to traditional retrieval approaches based on exact-term matching, such as BM25 (Robertson et al., 2009). Despite great performance, it remains difficult to build dense retrieval systems in settings that do not have large amounts of dedicated training data (Thakur et al., 2021). Recent work has explored improving unsupervised dense retrieval systems, such as Contriever (Izacard et al., 2021), by leveraging Large Language Models (LLMs) to enrich the query embedding for nearest neighbor search. HyDE (Gao et al., 2023), for example, prompts an LLM to generate hypothetical documents that are used to search for the closest real documents. This casts dense retrieval as document similarity task, which aligns well with pre-training techniques of unsupervised dense retrieval methods (Gao et al., 2023; Izacard et al., 2021). While HyDE demonstrates strong zero-shot1 performance, it is highly reliant on the LLMs parametric knowledge, which can be barrier in deployment for out-of-domain corpora settings (e.g., proprietary documents). potential solution could be to leverage top-retrieved documents as context when prompting the LLM to generate hypothetical documents (Shen et al., 2024). However, this increases search latency due to the longer input context. Additionally, even with better prompt context, hypothetical documents generated by LLMs remain susceptible to common issues such as overlooking or ignoring the provided information (Zhou et al., 2023; Shi et al., 2023; Liu et al., 2024; Simhi et al., 2024). To address these challenges, we propose reframing the task as relevance estimation rather than hypothetical document generation. Drawing inspiration from relevance feedback, we introduce Real Document Embeddings from Relevance Feedback (ReDE-RF). ReDE-RF first retrieves an initial set of documents from fully unsupervised 1In this paper, zero-shot and unsupervised are used without distinction. 1 Figure 1: An illustration of the ReDE-RF approach. hybrid sparse-dense retrieval system and prompts an LLM to mark the returned documents as relevant or non-relevant. Then, given the set of relevant documents, ReDE-RF fetches the document embeddings which are precomputed offline from the dense index and generates an updated query vector. When updating the query representation with LLM relevance feedback, the new representation is based strictly on real documents from the corpus; the LLM does not generate any content that is used to refine the query representation. Importantly, we also note that ReDE-RFs goal is similar to that of Gao et al. (2023): we want to develop full zero-shot dense retrieval pipeline that requires no relevance supervision. The core motivation behind our approach is simif we can easily access top retrieved docuple: ments, we do not need to exclusively rely on LLMs to generate hypothetical documents. First, employing an LLM to generate hypothetical document for every query is inefficient and introduces unnecessary latency costs. Second, we argue that the task of generating hypothetical document is highly complex and requires the LLM to (1) already memorize the domain-specific knowledge relevant to the query and (2) replicate the structure of relevant document. In contrast, knowing what is relevant is much simpler task. Furthermore, when making use of real documents, we guarantee that the content used to refine the query representation is inherently grounded in the corpus, enabling our method to more seamlessly generalize across different domains. We empirically evaluate ReDE-RF on wide range of retrieval tasks. Our findings reveal that for low-resource tasks, ReDE-RF surpasses zero-shot dense retrieval methods that use LLMs for hypothetical document generation by up to 6% when LLMs are prompted to generate hypothetical document with top-retrieved documents as context and 14% when prompted without. Furthermore, ReDERF reduces retrieval latency by as much as 7.511.2 compared to hypothetical document generation with top-retrieved documents as context and 4.4 without. Our contributions are summarized as follows: We propose ReDE-RF, method that enhances query embeddings for unsupervised dense retrieval systems while addressing key challenges associated with approaches that rely entirely on hypothetical document generation. We comprehensively evaluate ReDE-RF on variety of search tasks and show that ReDERF improves upon state-of-the-art zero-shot dense retrieval approaches in low-resource domains while also improving latency. We demonstrate an approach for distilling ReDE-RF to smaller, more efficient unsupervised dense retriever, DistillReDE. DistillReDE is able to make 33% improvement on Contriever while not requiring any update to the Contriever document index or relying on LLMs at inference time."
        },
        {
            "title": "2 Methodology",
            "content": "In this section, we first provide brief overview of HyDE, method that performs zero-shot dense retrieval through the generation of hypothetical documents. We then describe how we leverage LLMs to perform relevance feedback, critical component that allows us overcome the challenges of hypothetical documents. Lastly, we describe how we use relevance feedback outputs to update our query representation. 2 2.1 Preliminaries: HyDE The main challenge in zero-shot dense retrieval is learning query and document embedding functions that capture relevance when human-annotated relevance scores are not available. HyDE (Gao et al., 2023) seeks to overcome this by re-casting the task as document-document similarity task. Given query, q, HyDE first samples hypothetical documents { ˆd1, . . . , ˆdN } from generative LLM denoted by LLMDocGen via zero-shot prompting: ˆdi = LLMDocGen(q), 1 (1) where LLMDocGen(q) denotes the stochastic output of LLMDocGen given q. One could optionally provide the top-k documents = {d1, d2, . . . , dk} from an unsupervised retrieval system (e.g., BM25) as context for LLMDocGen: ˆdi = LLMDocGen(D, q), 1 (2) We refer to this as HyDEPRF 2. Subsequently, the hypothetical documents, { ˆd1, . . . , ˆdN }, are encoded by an unsupervised contrastive encoder, , and averaged to generate an updated query embedding, ˆvqHyDE. When generating ˆvqHyDE, the original query is also considered. More formally, ˆvqHyDE = 1 + 1 (cid:32) (q) + (cid:88) i=1 (cid:33) ( ˆdi) (3) ˆvqHyDE is then searched against the corpus embeddings to retrieve the most similar real documents. Through this two-step process, zero-shot dense retrieval moves from directly modeling querydocument similarity to modeling documentdocument similarity. Without the need for relevance supervision, HyDE is able to out-perform state-of-the-art unsupervised dense retrievers. 2.2 ReDE-RF Similar to HyDE, ReDE-RF models zero-shot dense retrieval as document similarity task. Unlike HyDE, the LLM is leveraged for relevance feedback rather than document generation. ReDERF has two key components: (1) relevance feedback with LLMs and (2) updating the query representation. These two components are described in this subsection and illustrated in Figure 1. 2We note that this shares similarity with LameR (Shen et al., 2024), but LameR is built on BM25 and has slight implementation differences. Figure 2: Prompt for relevance feedback, which is modified version of the prompt used in Upadhyay et al. (2024). {} denotes the placeholder for the corresponding text. 2.2.1 Relevance Feedback with LLMs Given query, we first retrieve the top-k documents, D, from an unsupervised retrieval system. Subsequently, we employ zero-shot prompting to score the relevance of given document, di, to the query. Based on the prompt, generative LLM, denoted by LLMRel-Judge, returns list of the documents classified as relevant Dr = {dr1, dr2, . . . , drk }, where ri {1, 2, . . . , k} for 1 k. With the recent success of using LLMs for patching up missing relevance judgements, we use modified version of the prompt from Upadhyay et al. (2024), which is shown in Figure 2. 2.2.2 Updating the Query Representation Given the list of documents, Dr, that LLMRel-Judge deems relevant, we follow Equation 3 to update the query embedding. One key difference between ReDE-RF and HyDE is that (dri) where 1 already exists as it is the embedding of real document that was pre-computed offline. As such, we denote CE[dri] as the action of retrieving the embedding for specified document, dri, from the set of corpus embeddings CE. Thus, to update our query: ˆvqReDE = 1 + 1 (cid:32) (q) + (cid:88) i=1 (cid:33) CE[dri] (4) If no relevant documents are found in the top-k, i.e., Dr = , simple option could be to default to the unsupervised contrastive encoder, and just return (q). However, in these cases we also argue defaulting to hypothetical document generation can be viable option as it would only hurt latency for difficult queries that the initial retrieval struggles 3 with. We compare the trade-off between effectiveness and efficiency of these two options in Section 3.2 and 3.3."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Setup requires ReDE-RF Implementation an instruction-tuned LLM and dense retriever. For the instruction-tuned LLM (i.e., LLMRel-Judge) we leverage Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) and for dense retrieval we use the unsupervised Contriever (Izacard et al., 2021). When prompting LLMRel-Judge relevance feedback, we truncate the input document to at most 128 tokens and generate relevance score by applying softmax on the logits of the 1 and 0 tokens as shown in Nogueira et al. (2020). Only documents that LLMRel-Judge scores as 1 are used for updating the query representation. In cases in which Dr = , we consider two defaults: Contriever and HyDEPRF. for set To generate an initial document for LLMRel-Judge, we retrieve the top-20 documents from hybrid, sparse-dense, retrieval model (BM25 + Contriever)3. Retrieval experiments were performed with Pyserini (Lin et al., 2021) and LLM implementations in HuggingFace (Wolf et al., 2019). Datasets In our experiments, we evaluate on two web search datasets: TREC DL19 (Craswell et al., 2020) and TREC DL20 (Craswell et al., 2021). We also evaluate on seven low-resource retrieval datasets from BEIR (Thakur et al., 2021). The tasks include news retrieval (TREC-News, Robust04), financial question answering (FiQA), entity retrieval (DBpedia), biomedical IR (TREC-Covid, NFCorpus), and fact checking (SciFact). For metrics, we report NDCG@10, the offical metric for the TREC and BEIR datasets. Baselines We first compare ReDE-RF to unsupervised retrievers that do not leverage LLMs: BM25, Contriever, and hybrid retrieval model (BM25 + Contriever). We also include pseudo-relevance feedback (PRF) baseline which averages all top-k initially retrieved documents to update the query representation: ContrieverAvgPRF (Li et al., 2022). ContrieverAvgPRF is equivalent to ReDE-RF if all top-k retrieved documents are considered relevant. For ContrieverAvgPRF, the initially retrieved docu3We provide implementation details in Appendix ments are from the hybrid retrieval model (BM25 + Contriever). We then compare ReDE-RF to methods that use LLMs for zero-shot dense retrieval and require no training. Our main point of comparison for ReDE-RF is HyDE and HyDEPRF. For HyDEPRF we prompt the LLM using the top-20 initially retrieved documents from the same hybrid retrieval system as ReDE-RF. We also compare ReDE-RF to the dense retrieval version of PromptReps (Zhuang et al., 2024b), which generates query and document representations by prompting an LLM to generate single token that describes the text. Lastly, we compare against dense retrieval systems that have been fine-tuned with supervised data: DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2021), and ContrieverFT, fine-tuned version of Contriever. 3.2 Results on Benchmarks Table 1 presents the evaluation results on the TREC and BEIR datasets and reveals several insights: (1) ReDE-RF outperforms ContrieverAvgPRF, which uses all initially retrieved documents to enhance the query embeddings. This exemplifies that simply leveraging the top-k retrieved documents is not sufficient, and demonstrates the value of leveraging an LLM to filter out non-relevant documents. (2) Comparing ReDE-RF to HyDE, we find that using real documents for zero-shot dense retrieval consistently outperforms hypothetical documents based solely on LLM knowledge (i.e., without top documents as context). (3) When incorporating corpus text as guide for HyDE (i.e., HyDEPRF), the performance gap between ReDE-RF and HyDE decreases. However, ReDE-RF still provides substantial improvements in low-resource domains (6.0% when defaulting to HyDEPRF and 4.6% when defaulting to Contriever). For high-resource domains DL19 and DL20 HyDEPRF yields better results, which we hypothesize is due to the advantages of combining the LLMs parametric knowledge with corpus knowledge in domains the LLM is well-versed. (4) As the performance of ReDE-RF (Default: HyDEPRF) is equivalent to HyDEPRF for queries that ReDE-RF defaults, the performance gains of ReDE-RF in low-resource domains can be attributed to the benefits of doing nearest-neighbor search in the real document embedding space that LLMRel-Judge deemed relevant versus the hypothetical document space generated by LLMDocGen Model Init. Retrieval DL19 DL20 News Covid FiQA SciFact DBPedia NFCorpus Robust04 BEIR (Avg.) High Resource Low Resource (BEIR) BM25 Contriever Hybrid (BM25 + Contriever) ContrieverAvgPRF PRF-Depth: 3 PRF-Depth: 20 Zero-Shot Dense Retrieval PromptReps (Llama3-8B-I) HyDE (Mistral-7B-Instruct) HyDEPRF (Mistral-7B-Instruct) ReDE-RF (Ours) Default: Contriever Default: HyDEPRF Supervised Dense Retrieval DPR ANCE ContrieverFT N/A N/A N/A Hybrid (3) Hybrid (20) N/A N/A Hybrid (20) Hybrid (20) Hybrid (20) N/A N/A N/A 50.6 44.5 52. 52.1 49.2 - 57.8 63.5 60.3 62.8 62.2 64.5 62.1 48.0 42.1 50.9 46.4 46. - 53.9 62.0 59.4 60.4 65.3 64.6 63.2 39.5 34.8 42.2 43.4 39.9 - 44.0 46. 46.1 47.1 16.1 38.2 42.8 59.5 27.3 52.9 48.1 51.3 59.5 56.9 59.1 65.6 65. 33.2 65.4 59.6 23.6 24.5 28.4 22.8 12.0 27.1 21.6 28.3 28.2 29.3 11.2 29.5 32. 67.9 64.9 71.6 59.9 36.9 52.7 65.1 64.5 67.4 66.9 31.8 50.7 67.7 31.8 29.2 34. 31.9 28.6 31.5 35.3 35.2 37.0 37.6 26.3 28.1 41.3 32.2 31.7 33.7 32.3 21. 29.6 27.7 35.0 34.8 35.5 18.9 23.7 32.8 40.7 31.6 43.0 38.6 39.2 - 41.5 45. 49.8 51.7 25.2 39.2 47.3 42.2 34.9 43.8 39.6 32.8 - 41.7 44.9 47.0 47. 23.2 39.3 46.3 Table 1: Results (NDCG@10) on TREC and BEIR. We report the mean NDCG@10 across three runs for HyDE, HyDEPRF, and ReDE-RF (Default: HyDEPRF). The average standard deviation across all datasets for HyDE, HyDEPRF and ReDE-RF (Default:HyDEPRF) was 0.4%, 0.5% and 0.1% respectively. Exact numbers can be found in Appendix C. (5) While ReDE-RF remains competitive with fine-tuned dense retrieval systems (DPR, ANCE, and ContrieverFT) in high-resource domains, in low-resource domains, ReDE-RF outperforms DPR and ANCE on all but one dataset, and surpasses ContrieverFT on four of the seven lowresource datasets. 3.3 Comparing Latencies In Figure 3, we empirically compare the average query latency for HyDEPRF, HyDE and ReDERF. We also include HyDEPRF with 10 initially retrieved documents HyDEPRF (10 Docs) as an additional comparison4. All experiments were run on one A100 GPU and measure the time from input query to retrieval of results. Comparing the latencies across systems, we find that ReDE-RF (Default: Contriever) consistently reduces latency compared to HyDE and HyDEPRF. Specifically, on average, ReDE-RF (Default: Contriever) is 3.8 faster than HyDE and 6.7 to 9.7 faster than HyDEPRF, depending on whether 10 or 20 documents are used as context. This finding is true even when ReDE-RF defaults to HyDEPRF, improving latency by 2.4 compared to HyDE and and 4.1 to 5.9 compared to HyDEPRF. These results confirm our hypothesis that leveraging hypothetical document generation for every query introduces unnecessary latency costs and that it is possible to improve performance while also im4NDCG@10 results can be found in Appendix D.2 proving efficiency. For ReDE-RF, Table 1 showed that defaulting to HyDEPRF can provide performance boost as compared to defaulting to Contriever. However, these improvements come with higher latency. Ideally, ReDE-RF can achieve the performance of ReDERF (Default: HyDEPRF) while fully removing the need for generating hypothetical documents. We investigate this in Section 5."
        },
        {
            "title": "4 Ablation Study on ReDE-RF",
            "content": "There are many design decisions that one can make when implementing ReDE-RF. In this section, we study the effects of these different choices on ReDE-RFs performance. As some approaches may default more frequently than others, the result will be affected by how strong the chosen default is. Thus, in the ablation study, we choose no default: return no results for the query if = 0 (yielding an NDCG@10 of 0) to limit our study solely to the relevance feedback portion of ReDE-RF. We refer to this as ReDE-RF (No Default.) Effect of Initial Retrieval Method How does the initial retriever effect ReDE-RF accuracy? The results for this experiment are in Table 2. Feeding documents using hybrid retrieval consistently improves results compared to only sparse or only dense retrieval. As ReDE-RF is highly dependent on the initial retrieval, these results suggest that leveraging multiple unsupervised retrievers can improve performance. 5 DL20 News DBpedia Max 1 5 10 20 54.2 58.5 60.0 59. 35.8 40.8 41.1 41.2 31.5 35.0 36.1 35.4 Table 3: Impact of the number of documents used to update the ReDE-RF (No Default) query embedding. Method DL19 DL20 ReDE-RF (No Default) w/ Mistral-7B-Instruct w/ Mixtral-8x7B-Instruct w/ Gemma-2-2B-it w/ Gemma-2-9B-it w/ Llama-3.2-3B-I w/ Llama-3.1-8B-I 59.1 62.1 58.9 62.0 56.0 57.2 59.0 58.7 56.7 58.9 54.1 56.1 Table 4: Impact of Relevance Feedback model on NDCG@10. For Mixtral-8x7B-Instruct, we perform inference with 4-bit quantization. Prompt DL19 DL Figure 2 pointwise.yes_no (Zhuang et al., 2024c) RG-YN (Zhuang et al., 2024a) RG-YN Thomas et al. (2024) 59.1 61.4 57.8 59.8 61.4 59.0 56.1 51.8 54.9 56.9 Table 5: Impact of different prompts on ReDE-RF (No Default). For Thomas et al. (2024), we make the relevance options binary. Prompts are in Appendix G. feedback at the benefit of faster inference times. Prompt Variations In Table 5 we study how different prompts impact ReDE-RFs retrieval effectiveness. The results show that, on average, performance across prompts is similar. The only prompt that performs worse is RG-YN, which asks \"For the following query and document, judge whether they are relevant. Output Yes or No. We hypothesize that the drop in performance stems from the prompt not giving clear definition of what relevance is (which the other prompts do more clearly, e.g., \"the passage answers the query\"). To test this, we augment the prompt with the description relevance from Figure 2 (RG-YN). The results show that this simple augmentation improves performance of RG-YN by 4.6%, on average. This finding hints that when creating prompts for ReDERF it is important the prompt includes clear definition of what should be classified as relevant versus not. Figure 3: Latency per query for HyDEPRF, HyDE and ReDE-RF. Speedup is relative to slowest method (HyDEPRF). Method DL20 Covid FiQA Init. Retrieval: BM25 ReDE-RF (No Default) Init. Retrieval: Contriever ReDE-RF (No Default) Init. Retrieval: Hybrid ReDE-RF (No Default) 56. 65.0 20.6 52.4 37.3 21.6 59. 65.5 22.9 Table 2: Impact of initial retriever on NDCG@10. Impact of In Table 3 we study how the number of relevant documents (k as defined in 2.2.1) used to update the query representation influences the accuracy of ReDE-RF. Accuracy generally increases up to and stabilizes around = 10. We hypothesize this may be due to the increased number of potential false positives returned as the number of relevant documents increases, which may push ReDE-RFs query embedding further away from the true positives. Effect of Relevance Feedback Model In Table 4 we investigate ReDE-RF with different LLMs. We generally find similar trends across model families: as the model size increases, performance improves. Generally, smaller models are competitive with their larger counterparts, demonstrating the potential for using smaller LLM for relevance as drop-in replacement for Contriever in the ReDE-RF system. Table 6 shows the results of this experiment. When fully leveraging DistillReDE as the initial retriever and ReDE-RF default (row 3), performance is very competitive compared to ReDE-RF when defaulting to HyDEPRF (row 2). When performing initial retrieval using hybrid (BM25 + DistillReDE) initial retriever (row 4), performance improves compared to row 3 and increases slightly over row 2, while being significantly less costly (as we remove any need for hypothetical document generation). These results demonstrate that with simple offline training scheme, ReDE-RF can match the performance of ReDE-RF (Default: HyDEPRF) while fully removing the need for hypothetical document generation."
        },
        {
            "title": "6 ReDE-RF vs. Pointwise Reranking",
            "content": "The LLMRel-Judge component of ReDE-RF (discussed in 2.2.1) is similar to LLM-based pointwise re-rankers (Zhuang et al., 2024c). In this subsection, we ask: What benefits do we achieve by feeding relevant documents to improve the query representation as described in 2.2.2 versus simply re-ordering the initial retrieval based on the logits from LLMRel-Judge? To answer this, we focus on comparing pointwise re-ranking to ReDE-RF (Default: HyDEPRF) in equal settings: Both systems have access to the top-20 passages from hybrid (BM25 + Contriever) retriever and employ the same prompt as shown in Figure 2. Note, for the rest of this section we refer to ReDE-RF (Default: HyDEPRF) as ReDE-RF. In Table 7 we present the results of this experiment across three backbone LLMs. Based on the results, we can make the following observations: (1) When comparing NDCG@10, ReDE-RF and pointwise re-ranking are generally on par outside of Llama-3.1-8B-I on DL19. (2) ReDE-RF consistently outperforms pointwise re-ranking in terms of NDCG@20 by large amounts. This demonstrates that ReDE-RFs improvements extend beyond the top-ranked results and is not confined to the ini- (3) Besides the evaluation on the tial retrieval. TREC News dataset where it appears pointwise re-ranking is not well calibrated re-ranking and ReDE-RF are generally complementary. ReDE-RF + PR outperforms Hybrid + PR eight out of nine times (six out of six if excluding TREC News) and outperforms ReDE-RF five out of nine times (five out of six if excluding TREC News). Figure 4: Comparison of DistillReDE to Contriever and ReDE-RF (Default: HyDEPRF)."
        },
        {
            "title": "5 Can we Distill ReDE-RF?",
            "content": "As noted in Section 3.3, ReDE-RF improves latency per query as compared to HyDE and HyDEPRF. However, if ReDE-RF is implemented with HyDEPRF as its default, it still occasionally needs to default to hypothetical document generation if no relevant documents are found, thus making it costly for certain queries. In this section, we explore if we can improve the latency of ReDE-RF without trading off accuracy. With this in mind, we aim to answer two questions: 1) Can we distill ReDE-RFs performance to Contriever (DistillReDE)? 2) Can using DistillReDE in tandem with ReDE-RF remove the need for defaulting to HyDEPRF while matching the performance of ReDE-RF (Default: HyDEPRF)? Distilling ReDE-RF We aim to explore whether ReDE-RF can be distilled to student Contriever model, DistillReDE. Since ReDE-RFs embeddings are an average of the Contriever document embeddings, one advantage is that the student model can be trained without the need to re-index the corpus. To generate the training set, we first run ReDE-RF offline using LLM-generated synthetic queries and treat the corresponding ReDE-RF embeddings as the target representation. For training, we follow the framework from Pimpalkhute et al. (2024): we optimize combination of MSE loss and contrastive loss with in-batch random negatives. See Appendix for training details. The results, shown in Figure 4, indicate that DistillReDE can achieve significant improvements on Contriever, narrowing its performance gap with ReDE-RF (Default: HyDEPRF) while removing the need for LLMs at inference time. ReDE-RF with DistillReDE We next explore the possible advantages of leveraging DistillReDE 7 Default Init. Retrieval Covid NFCorpus FiQA Robust04 DBPedia Contriever HyDEPRF Hybrid Hybrid DistillReDE DistillReDE DistillReDE Hybrid 65.6 65. 63.8 66.3 34.8 35.5 35.6 35.8 28.2 29.3 30.2 30.9 49.8 51. 47.9 49.2 37.0 37.6 37.8 38.4 Table 6: NDCG@10 of ReDE-RF when implemented with DistillReDE. Hybrid is hybrid system that combines results from BM25 and DistillReDE. Hybrid is BM25 + Contriever, as in Table 1. Method Mistral-7B-Instruct ReDE-RF Hybrid + PR ReDE-RF + PR Gemma-2-9B-it ReDE-RF Hybrid + PR ReDE-RF + PR Llama-3.1-8B-I ReDE-RF Hybrid + PR ReDE-RF + PR Covid DL19 NDCG@10/20 NDCG@10/20 NDCG@10/20 News 65.6 / 57.9 63.6 / 49.6 68.8 / 58. 67.9 / 60.0 65.0 / 50.8 72.3 / 61.2 65.7 / 58.7 67.1 / 51.8 72.9 / 61.1 62.8 / 60.3 60.0 / 53.7 62.6 / 59.9 62.0 / 61.3 63.7 / 55.8 70.7 / 64.8 59.0 / 59.0 66.8 / 57.4 70.1 / 64.3 47.1 / 43.8 45.6 / 42.5 45.8 / 43. 46.6 / 42.8 46.3 / 41.9 43.4 / 41.5 49.9 / 45.7 47.2 / 42.8 48.6 / 45.4 Table 7: Comparing ReDE-RF (Default: HyDEPRF) to pointwise reranking (PR). ReDE-RF + PR reranks the top-20 passages returned from ReDE-RF. Bold denotes best overall system. Underline denotes best between ReDE-RF and Hybrid + PR. These results exemplify the difference in the roles of passage re-ranking and ReDE-RF. While passage re-ranking primarily enhances the ordering of the top-k passages, ReDE-RF improves the overall quality of candidates from the first-stage retrieval. Other work has looked into improving the outputs of LLM query expansions through query reranking (Chuang et al., 2023) or further training an LLM with preferences from the target retrieval systems (Yoon et al., 2024). Zero-Shot Dense Retrieval With advancements in deep learning, IR systems moved away from representations based on exact-term matching to dense vector representations generated from transformer language models (Lin et al., 2022), such as BERT (Devlin et al., 2019). However, learning these representations typically requires large, labeled datasets. As such, researchers have looked into methods for learning dense representations without manually labeled data through techniques such as using synthetic data (Izacard et al., 2021; Wang et al., 2023a; Sachan et al., 2023; Lee et al., 2024; Dai et al., 2022), addressing architecture limitations to improve zero-shot decoder-LLM embeddings (Springer et al., 2024), or leveraging LLMs to generate outputs that can be used to improve representations for zero-shot dense retrieval (Gao et al., 2023; Zhuang et al., 2024b)."
        },
        {
            "title": "8 Conclusion",
            "content": "Query Expansion with LLMs GAR (Mao et al., 2021) was among the first methods to demonstrate the effectiveness of LLMs for query expansion by training an LLM to expand queries through the generation of relevant contexts, such as the target answer or answer sentence. Recent work has looked into leveraging LLMs to generate query expansions via zero or few-shot prompting. This has been explored in contexts where the LLM generates hypothetical documents that can be used to augment the query (Gao et al., 2023; Wang et al., 2023b; Jagerman et al., 2023; Lei et al., 2024; Mackie et al., 2023; Shen et al., 2024). While some of these works generate hypothetical texts given real documents (e.g., Jagerman et al. (2023); Lei et al. (2024); Shen et al. (2024)) they still rely on LLMgenerated content to augment the query. We introduce ReDE-RF, zero-shot dense retrieval method that addresses key challenges associated with approaches that rely entirely on hypothetical document generation. Through extensive experiments, we show that ReDE-RF improves upon state-of-the-art zero-shot dense retrieval approaches in low-resource domains, while also lowering latency compared to techniques that rely only on hypothetical document generation. Further analysis shows that ReDE-RF can be easily distilled to smaller, more efficient unsupervised dense retriever, DistillReDE, removing any reliance on LLMs at inference time. In summary, ReDE-RF presents an approach that achieves the benefits of casting zero-shot dense retrieval as document similarity task while being more efficient and domainagnostic."
        },
        {
            "title": "Limitations",
            "content": "A limitation of ReDE-RF is its reliance on retrieved results from first-stage retrievers. If an initial retriever provides poor set of results, performance gains will not be as apparent as no relevant documents can be used to update the query embedding. This in turn makes ReDE-RF equivalent to Contriever or HyDE, depending on what default the user leverages. How to make ReDE-RF less reliant on retrieved results from unsupervised first-stage retrievers is question worth exploring in future work. Another simple improvement could be leveraging rules-based approach that keeps assessing retrieved documents if none of the top results are deemed relevant. We do note that this would likely increase latency. Another limitation is that while ReDE-RF seeks to minimize reliance on LLM-generated outputs, it does still depend on an LLM to be accurate in its relevance feedback. If the LLM provides inaccurate relevance assessments during the relevance feedback stage, it can further harm the query representation. Lastly, while ReDE-RF improves latency compared to previous approaches based on hypothetical document generation, the latency is still slower than approaches that do not rely on LLMs at inference time. However, we demonstrated the potential for doing offline training as way to mitigate this. Additionally, as LLMs advance, we may see improvements in the efficiency of LLM inference. Techniques such as flash-attention (Dao et al., 2022), can also significantly decrease the inference time of LLMs."
        },
        {
            "title": "Ethics Statement",
            "content": "While the ultimate goal of our work is to minimize reliance on LLM generated output, we do recognize that our system does still rely on LLMs, which means that there is risk that the LLM can produce biased, harmful, or offensive output. To mitigate this, we limit our LLM to only generate one token, which we hope can eliminate this risk. Additionally, our dense retrieval system is based on pre-trained language models which can potentially produce retrieval results that contain human biases. Our research solely uses publicly available datasets, and no personal information is collected. All datasets and models are used in accordance with its intended use and licenses. Our method is designed to improve the performance of information retrieval systems in settings in which there exists no data and proprietary LLMs may not be available. We hope this can enable the deployment of our system in similar settings."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely thank Charlie Dagli, John Holodnak, and Daniel Gwon for their discussion and help in this project. Research was sponsored by the Department of the Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "title": "References",
            "content": "Yung-Sung Chuang, Wei Fang, Shang-Wen Li, Wen-tau Yih, and James Glass. 2023. Expand, rerank, and retrieve: Query reranking for open-domain question answering. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1213112147, Toronto, Canada. Association for Computational Linguistics. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the trec 2020 deep learning track. Preprint, arXiv:2102.07662. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the trec 2019 deep learning track. Preprint, arXiv:2003.07820. Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. arXiv preprint arXiv:2209.11755. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. 9 Kingma Diederik. 2014. Adam: method for stochastic optimization. (No Title). Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise zero-shot dense retrieval without relIn Proceedings of the 61st Annual evance labels. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17621777, Toronto, Canada. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653. Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. Inpars-v2: Large language models as efficient dataset generators for information retrieval. arXiv preprint arXiv:2301.01820. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, et al. 2024. Gecko: Versatile text embeddings distilled from large language models. arXiv preprint arXiv:2403.20327. Yibin Lei, Yu Cao, Tianyi Zhou, Tao Shen, and Andrew Yates. 2024. Corpus-steered query expansion with large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 393401, St. Julians, Malta. Association for Computational Linguistics. In Proceedings of the 44th Interrepresentations. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2356 2362. Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2022. Pretrained transformers for text ranking: Bert and beyond. Springer Nature. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023. Generative relevance feedback with large lanIn Proceedings of the 46th Interguage models. national ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2026 2031. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021. Generation-augmented retrieval for opendomain question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 40894100, Online. Association for Computational Linguistics. Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document ranking with preIn Findings trained sequence-to-sequence model. of the Association for Computational Linguistics: EMNLP 2020, pages 708718, Online. Association for Computational Linguistics. Varad Pimpalkhute, John Heyer, Xusen Yin, and Sameer Gupta. 2024. Softqe: Learned representations of queries expanded by llms. In European Conference on Information Retrieval, pages 6877. Springer. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2010.08191. Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Hang Li, Shengyao Zhuang, Xueguang Ma, Jimmy Lin, and Guido Zuccon. 2022. Pseudo-relevance feedback with dense retrievers in pyserini. In Proceedings of the 26th Australasian Document Computing Symposium, pages 16. Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600616. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: python toolkit for reproducible information retrieval research with sparse and dense Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Yibin Lei, Tianyi Zhou, Michael Blumenstein, and Daxin Jiang. 2024. Retrieval-augmented retrieval: Large language models are strong zero-shot retriever. 10 Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text reIn International Conference on Learning trieval. Representations. Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. 2024. Ask optimal questions: Aligning large language models with retrievers preference in conversational search. arXiv preprint arXiv:2402.11827. Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1454414556, Singapore. Association for Computational Linguistics. Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui Wang, and Michael Bendersky. 2024a. Beyond yes and no: Improving zero-shot llm rankers via scoring fine-grained relevance labels. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 358370. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. 2024b. Promptreps: Prompting large language models to generate dense and sparse representations for zero-shot document retrieval. arXiv preprint arXiv:2404.18424. Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024c. setwise approach for effective and highly efficient zero-shot ranking with large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3847. In Findings of the Association for Computational Linguistics ACL 2024, pages 1593315946. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023. Trusting your evidence: Hallucinate less with context-aware decoding. arXiv preprint arXiv:2305.14739. Adi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. 2024. Constructing benchmarks and interventions for combating hallucinations in llms. arXiv preprint arXiv:2404.09971. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. 2024. Repetition improves language model embeddings. arXiv preprint arXiv:2402.15449. Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large language models can accurately predict searcher preferences. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 19301940. Shivani Upadhyay, Ehsan Kamalloo, and Jimmy Llms can patch up missing relearXiv preprint Lin. 2024. vance judgments in evaluation. arXiv:2405.04727. Ben Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: 6 billion parameter autoregressive language model. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023a. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Liang Wang, Nan Yang, and Furu Wei. 2023b. Query2doc: Query expansion with large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 94149423, Singapore. Association for Computational Linguistics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808."
        },
        {
            "title": "A Dataset Details",
            "content": "We show the number of test queries for each dataset used to evaluate ReDE-RF in Table 10. gemma-2-9b-it: 9B parameter model that is instruction fine-tuned. Huggingface ID: google/gemma-2-9b-it Dataset TREC DL19 TREC DL20 TREC-News TREC-Covid FiQA SciFact DBPedia NFCorpus Robust04 # Queries 43 54 57 50 648 300 400 323 249 Table 10: Dataset Details The above datasets have the following licenses. TREC DL19 and DL20 are under MIT License for non-commercial research purposes. Llama-3.2-3B-Instruct: that is parameter model tion Huggingface meta-llama/Llama-3.2-3B-Instruct fine-tuned. 3B instrucID: Llama-3.1-8B-Instruct: that is parameter model tion Huggingface meta-llama/Llama-3.1-8B-Instruct fine-tuned. 8B instrucID: The above models have the following licenses. Mistral-7B-Instruct-v0.2 is under the Apache 2.0 License. Mixtral-8x7B-Instruct-v0.1 is under the Apache 2.0 License. contriever is under CC BY-NC 4.0 license. gemma-2-2b-it is under the Apache 2.0 LiTREC News and Robust04 are under Copycense. right. gemma-2-9b-it is under the Apache 2.0 LiDBPedia is under CC BY-SA 3.0 license. cense. SciFact is under CC BY-NC 2.0 license. TREC Covid is provided under Dataset License Agreement Llama-3.2-3B-Instruct is under the Llama 2 Community License Agreement Llama-3.1-8B-Instruct is under the Llama 2 Community License Agreement NFCorpus and FiQA do not report the dataset license as per Thakur et al. (2021). We also leverage Pyserini (Lin et al., 2021) which is under the Apache 2.0 License. The BEIR dataset is under the Apache 2.0 License."
        },
        {
            "title": "B Model Details",
            "content": "Mistral-7B-Instruct-v.02: parameter model tion mistralai/Mistral-7B-Instruct-v0.2 is Huggingface fine-tuned. that 7B instrucID: contriever: Based on bert-base-uncased which has 110M parameters. HuggingFace ID: facebook/contriever. Mixtral-8x7B-Instruct-v0.1: 8x7B parameter model (47B total parameters) that is instruction fine-tuned. Huggingface ID: mistralai/Mixtral-8x7B-Instruct-v0.1 gemma-2-2b-it: 2B parameter model that is instruction fine-tuned. Huggingface ID: google/gemma-2-2b-it 12 Due to the randomness of sampling hypothetical documents from an LLM, we run HyDE, HyDEPRF, and ReDE-RF (Default: HyDEPRF) three times. In Table 8 we report the mean and standard deviation of NDCG@10 across the runs for all TREC and BEIR datasets."
        },
        {
            "title": "D HyDE and HyDEPRF Implementation",
            "content": "D.1 Generation Details To re-implement HyDE and HyDEPRF with Mistral7B-Instruct we follow the same parameters that were mentioned in the original paper (Gao et al., 2023) and in the provided codebase. In particular, we sample eight hypothetical documents from Mistral-7B-Instruct with temperature of 0.7 and allow up to 512 maximum new generation tokens per hypothetical document. Model HyDE (Mistral-7B-Instruct) HyDEPRF (Mistral-7B-Instruct) ReDE-RF (Default: HyDEPRF) DL19 57.8 0.3 63.5 1.2 62.8 0. DL20 53.9 0.7 62.0 0.2 60.4 0.1 News 44.0 0.6 46.9 1.1 47.1 0.3 Covid 56.9 0.8 59.1 0.5 65.6 0.1 FiQA 21.6 0.2 28.3 0.2 29.3 0.1 SciFact DBPedia NFCorpus Robust04 65.1 0.2 64.5 0.5 66.9 0. 35.3 0.3 35.2 0.1 37.6 0.1 27.7 0.2 35.0 0.2 35.5 0.1 41.5 0.4 45.6 0.2 51.7 0.1 Table 8: NDCG@10 of HyDE-Mistral-7B-Instruct and ReDE-RF (w/ HyDE) with standard deviations across three runs. Model HyDEPRF (Mistral-7B-Instruct) Hybrid (20) HyDEPRF (Mistral-7B-Instruct) Hybrid (10) Init Retrieval DL19 DL20 News Covid FiQA SciFact DBPedia NFCorpus Robust 63.5 63.6 62.0 60.5 46.9 47.2 59.1 58.8 28.3 28.3 64.5 63. 35.2 34.6 35.0 34.6 45.6 44.5 Table 9: NDCG@10 of HyDEPRF across different number of initially retrieved documents used as context. D.2 HyDEPRF with less in-context documents We explore how less initially retrieved results impact performance of HyDEPRF. For results, see Table 9. Please write financial article passage to answer the question. Question: {} Passage: D.3 Prompts For HyDE (Gao et al., 2023), we leverage the same prompts from the original implementation. For HyDEPRF, we follow the format of the HyDE prompts and provide context following the format in Q2D/PRF from Jagerman et al. (2023). D.3.1 HyDE Prompts TREC DL19 and DL20 Please write passage to answer the question. Question: {} Passage: SciFact DBPedia Please write passage to answer the question. Question: {} Passage: TREC News and Robust04 Please write news passage about the topic. Topic: {} Passage: Please write scientific paper passage to support/refute the claim. Claim: {} Passage: D.3.2 HyDEPRF Prompts TREC DL19 and DL20 TREC Covid and NFCorpus Please write scientific paper passage to answer the question. Question: {} Passage: FIQA Please write passage to answer the question based on the context: Context: {} Question: {} Passage: SciFact 13 Please write scientific paper passage to support/refute the claim based on the context: Context: {} Claim: {} Passage: TREC Covid and NFCorpus Please write scientific paper passage to answer the question based on the context: Context: {} Question: {} Passage: FIQA Please write financial article passage to answer the question based on the context: Context: {} Question: {} Passage: scores. We implement using the default parameters. See here for more details."
        },
        {
            "title": "F DistillReDE Training Details",
            "content": "To train DistillReDE, we first need synthetic queries given our corpus. We leverage the filtered set of 10K synthetic queries provided by Jeronymo et al. (2023) which were generated using GPT-J (Wang and Komatsuzaki, 2021). Then, we run ReDE-RF on these synthetic queries and store the ReDE-RF embedding for each query. If ReDE-RF finds no relevant documents for given query, we remove that query from our training set. To train DistillReDE, following Pimpalkhute et al. (2024), we optimize the following objective: LDistillReDE = 0.5LMSE(ˆvqReDE, (q)) + 0.5LCont (5) contrastive objective where LCont (Karpukhin et al., 2020): is L(qi, ˆv+ qReDE,i, ˆv qReDE,i,1, . . . , ˆv qReDE,i,n) (6) DBPedia = log sim(qi,ˆv+ qReDE,i sim(qi,ˆv qReDE,i,j ) qReDE,i ) sim(qi,ˆv+ (cid:80) j=1 + ) Please write passage to answer the question based on the context: Context: {} Question: {} Passage: TREC News and Robust ReDE-RF embedding ˆv+ RF embeddings (ˆv where given query, q, we have one positive qReDE,i and negative ReDEqReDE,i,n). qReDE,i,1, , ˆv We train with in-batch negatives using training batch size of 256 and use the Adam optimizer (Diederik, 2014) with learning rate of 5e-5. Please write news passage about the topic based on the context: Context: {} Topic: {} Passage: ReDE-RF Prompt Varations Below are the prompts studied in the prompt variations subsection of Section 4. pointwise.yes_no (Zhuang et al., 2024c)"
        },
        {
            "title": "Details",
            "content": "We use the hybrid retrieval implementation from Pyserini, which scores the document by weighted average of the sparse retrieval and dense retrieval Passage: {} Query: {} Does the passage answer the query? Answer Yes or No. RG-YN (Zhuang et al., 2024a) 14 For the following query and document, judge whether they are relevant. Output Yes or No. Query: {} Document: {} Output: RG-YN For the following query and document, judge whether they are relevant. Output Yes if the passage is dedicated to the query and contains the exact answer and output \"No\" if the passage has nothing to do with the query. Query: {} Document: {} Output: Thomas et al. (2024) You are search quality rater evaluating the relevance of web pages. Given query and web page, you must provide score on an integer scale of 0 to 1 with the following meanings: 1 = highly relevant, very helpful for this query 0 = not relevant, should never be shown for this query Assume that you are writing report on the subject of the topic. If the web page is primarily about the topic, or contains vital information about the topic, mark it 1. Otherwise, mark it 0. Passage: {} Query: {} Score:"
        }
    ],
    "affiliations": [
        "MIT Lincoln Laboratory",
        "Massachusetts Institute of Technology"
    ]
}