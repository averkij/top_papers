{
    "paper_title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning",
    "authors": [
        "Yongchao Chen",
        "Yueying Liu",
        "Junwei Zhou",
        "Yilun Hao",
        "Jingquan Wang",
        "Yang Zhang",
        "Chuchu Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. A key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke a Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-by-step reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to 64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with Code Interpreter (70.9\\%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 6 6 1 2 . 5 0 5 2 : r R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning Yongchao Chen MIT / Harvard yongchaochen@fas.harvard.edu Yueying Liu University of Illinois Urbana-Champaign yl136@illinois.edu Junwei Zhou University of Michigan zhoujw@umich.edu Yilun Hao MIT yilunhao@mit.edu Jingquan Wang University of WisconsinMadison jwang2373@wisc.edu Yang Zhang MIT-IBM Watson AI Lab Yang.Zhang2@ibm.com Chuchu Fan MIT chuchu@mit.edu"
        },
        {
            "title": "Abstract",
            "content": "Despite advances in reasoning and planning of R1-like models, Large Language Models (LLMs) still struggle with tasks requiring precise computation, symbolic manipulation, optimization, and algorithmic reasoning, in which textual reasoning lacks the rigor of code execution. key challenge is enabling LLMs to decide when to use textual reasoning versus code generation. While OpenAI trains models to invoke Code Interpreter as needed, public research lacks guidance on aligning pre-trained LLMs to effectively leverage code and generalize across diverse tasks. We present R1-Code-Interpreter, an extension of textonly LLM trained via multi-turn supervised fine-tuning (SFT) and reinforcement learning (RL) to autonomously generate multiple code queries during step-bystep reasoning. We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 diverse questions. We fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies, investigating different answer formats, reasoning vs. non-reasoning models, cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs. Unlike prior RL work on narrow domains, we find that Code Interpreter training is significantly harder due to high task diversity and expensive code execution, highlighting the critical role of the SFT stage. Our final model, R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0% to 64.1%, outperforming GPT-4o (text-only: 58.6%) and approaching GPT-4o with Code Interpreter (70.9%), with the emergent self-checking behavior via code generation. Datasets, Codes, and Models are available at https://github.com/yongchao98/R1-Code-Interpreter and https://huggingface.co/yongchao98."
        },
        {
            "title": "Introduction",
            "content": "While reinforcement learning (RL)-based fine-tuning has significantly improved LLMs reasoning and planning [Wang et al., 2024, Guo et al., 2025, Jaech et al., 2024], models still struggle with seemingly simple tasks [Chen et al., 2025] and incur high token costs during inference-time search [Chen et al., 2024a]. Notably, many benchmark taskssuch as Blocksworld [Valmeekam et al., 2024] and Preprint. Under review. Figure 1: Training Code Interpreter-augmented reasoning models with supervised learning and GRPO on 144 tasks in reasoning and planning domains. (a) Our best model, R1-CI-14B, surpasses GPT-4o (text-only) and approaches GPT-4o with Code Interpreter. (b) We propose modified GRPO framework that integrates Code Interpreter for output generation. Game 24 [Zhou et al., 2023a]are easily solvable via code. Textual reasoning excels at semantics and commonsense, but falls short in precise computation, symbolic manipulation, and algorithmic processing [Valmeekam et al., 2022]. In contrast, symbolic code generation handles these rigorously and benefits from external tools (e.g., equation solvers). Prompting LLMs to generate and execute code often outperforms pure textual reasoning [Madaan et al., 2022, Liang et al., 2022, Chen et al., 2022]. key challenge is guiding LLMs to decide when to rely on textual reasoning versus programmatic solutions, given that most input questions lack explicit cues about which approach is best and the possible text/code solution space is large. OpenAIs GPT models address this by incorporating Code Interpreter, allowing iterative code generation and reasoning over outputs [Achiam et al., 2023]. However, recent work [Chen et al., 2024b] show that current Code Interpreter implementations struggle to effectively steer between text and code, underutilizing symbolic capabilities. Moreover, public research lacks comprehensive understanding of how to fine-tune LLMs to integrate with Code Interpreter for robust, generalizable performance. Whether training strategies used in models like DeepSeek R1 can enable such integration remains an open question. To tackle these challenges, we present R1-Code-Interpreter, framework for integrating Code Interpreter capabilities into open-source LLMs. We curate 144 reasoning and planning tasks and synthesize 6.5k multi-turn text/code trajectories for supervised fine-tuning (SFT), followed by Group Relative Policy Optimization (GRPO) [Shao et al., 2024]. The resulting model, R1-CI-14B, effectively combines symbolic code execution with textual reasoning, solving tasks through iterative executeand-explore interactions before producing final answers. We further explore various training strategies and uncover several notable phenomena. Our main contributions and findings are: 1) Broad domains: We curate 144 reasoning and planning tasks (107 for training, 37 for testing), each with over 200 samples of varying difficulty. All tasks are standardized into unified format to enable efficient rollout and automated correctness evaluation. They cover diverse reasoning skills, including mathematical, spatial, logical, ordinal, optimization, and search-based reasoning. Our analysis highlights key challenges in leveraging Code Interpreter, especially due to varying task characteristics that affect whether code, text, mixture, or both are most effective. 2) R1-Code-Interpreter: the first SFT + RL framework that teaches reasoning LLMs to leverage Code Interpreter across diverse tasks. We synthesize 6.5k multi-turn trajectories with interleaved reasoning and code execution for SFT, followed by RL for further optimization. We fine-tune Qwen-2.5 [Qwen et al., 2025] models (3B/7B/14B), achieving average success rate improvements of 28.8% on 107 train tasks and 27.2% on 37 test tasks. As shown in Figure 1, our best model, R1-CI-14B, raises accuracy from 44.0% to 64.1%, outperforming GPT-4o (text-only: 58.6%) and approaching GPT-4o with Code Interpreter (70.9%). Notably, during training the model exhibits emergent self-checking behavior via code generation. The trained model also requires less costs of inference times and exhibits better decisions of code/text choice. 3) Extensive experiments comparing training strategies: We conduct systematic experiments to investigate training strategies, leading to several key insights: 1) Pre-trained LLMs naturally 2 generate Python code; no additional output templating is needed. 2) Warm-starting with SFT significantly improves integration of Code Interpreter, compared to cold-start RL without the initial SFT stage, especially for the generalization across tasks and domains. 3) Surprisingly, initializing with reasoning-focused models (e.g., R1-distilled) degrades performance and generalization, suggesting that excessive textual reasoning training can impair inherent coding ability. 4) GRPO is better than PPO in training Code Interpreter across different model sizes and numbers of task types. 5) Masking code execution outputs during training improves stability in multi-turn RL involving interleaved reasoning and execution. 4) RL limitation analysis: By tuning number of training tasks, we analyze the limitations of RL in training and optimizing Code Interpreter. Unlike prior work focused on narrow domains such as math or retrieval, we find that RL for general-purpose Code Interpreter is substantially more challenging, as it must learn policy effective across 144 diverse tasks with varying characteristics."
        },
        {
            "title": "2 Task benchmark",
            "content": "Challenges in 144 Reasoning and Planning Tasks We compile 144 tasks from three major reasoning and planning benchmarks: 33 from SymBench [Chen et al., 2025], 27 from Big-BenchHard [Suzgun et al., 2022], and 84 from Reasoning-Gym1. After removing near-duplicates, each task retains over 200 diverse samples. All tasks are standardized into unified format and evaluated using rule-based criteria (e.g., exact match or constraint checks) for efficient rollout and testing. The tasks cover diverse reasoning and planning challenges for LLM evaluation. Detailed task descriptions are in Appendix Sec. D, and their associated capability categorieslogic, spatial, order, optimization, search, and mathare summarized in Table 4. For tasks involving computation, logic, symbolic manipulation, optimization, spatial reasoning, or constrained planning, code-based symbolic computing often outperforms pure text reasoning. However, recent work [Chen et al., 2024b] highlight key challenges in steering LLMs to choose text) depends on task type, effectively between code and text: (1) The better mode (code vs. complexity, and model capabilitiesoften hard to predict; (2) LLM-generated code frequently degenerates into hard-coded text-like scripts, limiting its symbolic utility [Yang et al., 2024]."
        },
        {
            "title": "3 R1-Code-Interpreter",
            "content": "Figure 2 shows representative example of R1-Code-Interpreter in action. The model iteratively reasons, optionally generates code for execution, and refines its reasoning based on the results, continuing this process until the final answer is produced. The Code Interpreter is invoked only when deemed beneficial; otherwise, the model relies on pure textual reasoning. The rollout process follows an interleaved, multi-turn framework, where the LLM alternates between generating text and issuing external code queries. The system instruction directs the model to enclose code between the natural tokens ``` python and ``` when execution is needed. Upon detecting code block, the system extracts and executes it via the Code Interpreter, then appends the resultprefixed with the special token Code Execution Results:to the ongoing generation. This loop continues until either (1) the maximum of 5 code calls is reached, or (2) the model emits final answer enclosed between <<< and >>>. For the training process, we first synthesize 6.5k multi-turn text/code trajectories for SFT by prompting GPT-4o following with pre-defined formats. We then apply GRPO for further optimization, masking code execution outputs during loss and gradient computation to account for the multi-turn structure. Response format To train R1-Code-Interpreter, we begin by designing simple head prompt that guides the initial LLM to follow our predefined structure. As shown in Table 1, the prompt organizes the output into three iterative parts: reasoning, optional Code Interpreter invocation, and the final answer. We avoid imposing content-specific constraintssuch as enforcing reflective reasoning or code callsto preserve the models natural learning dynamics during RL. Unlike prior work that enforces section tags like <think>, <answer>, or <search> [Guo et al., 2025, Jin et al., 2025, Zhang et al., 2025], we rely solely on the final answer marker <<<answer 1https://github.com/open-thought/reasoning-gym 3 Figure 2: Example response of R1-Code-Interpreter in Blocksworld task. content>>> for answer extraction. For code, we leverage the LLMs pretrained behavior of naturally starting code blocks with ``` python, which serves as implicit tagging. Our initial tests show this natural format performs better than forced tagging, as it aligns more closely with the models original distribution. Table 1: Head prompt for R1-Code-Interpreter. The User asks question, and you solve it. You first generate the reasoning and thinking process and then provide the User with the final answer. During the thinking process, **you can generate python code** for efficient searching, optimization, and computing with the format of starting the python block with ``` python. **A code query must involve only single script that uses print function for the output.**. Once the code script is complete, stop the generation. Then, the code interpreter platform will execute the code and return the execution output and error. Once you feel you are ready for the final answer, directly return the answer with the format <<<answer content>>> at the end of your response. Otherwise, you can continue your reasoning process and possibly generate more code query to solve the problem. Non-reasoning Models as Base While some open-source modelsespecially DeepSeek-distilled variants [Guo et al., 2025]are strong in long-chain textual reasoning, initializing from them can hurt performance. Our experiments show that starting from general model like Qwen-2.5 yields better results, as overly specialized reasoning models often lose coding ability. Further analysis is provided in Sec. 5. Warm Starts Recent studies suggest that RL training without extensive SFT can match or even outperform traditional pipelines [Guo et al., 2025, Jin et al., 2025, Wei et al., 2025, Chu et al., 2025]. However, we find SFT to be essential for Code Interpreter training, significantly boosting model capability. See Sec. 5 for details. 4 3.1 Dataset synthesis of multi-turn SFT We fine-tune R1-Code-Interpreter using SFT and GRPO on subset of 144 tasks. We randomly select 107 tasks for training: 26 from SymBench, 20 from Big-Bench-Hard, and 61 from Reasoning-Gym, ensuring no sample overlaps with the test set. The remaining 37 tasks are used for evaluation. To generate SFT supervision, we prompt GPT-4o to produce multiple reasoning/execution trajectories per task and retain only those yielding correct answers. To enhance diversity and adaptability, we use varied prompt formatssome allow free-form reasoning such as the prompt in Table 1, while others enforce transitions between text and code. Each trajectory is limited to 5 code execution rounds, and each task includes up to 70 valid trajectories, resulting in final dataset of 6.5k high-quality samples. 3.2 Multi-turn RL with Code Interpreter As shown in Figure 1b, we formulate our RL objective with Code Interpreter as: ExD, yπθ(x;C) [rϕ(x, y)] β DKL (cid:2)πθ(y x; C) (cid:13) (cid:13) πref (y x; C)(cid:3) , (1) max πθ where πθ is the policy LLM, πref is the reference model, rϕ is the reward, and DKL is the KL divergence [Shlens, 2014]. Unlike prior work [Guo et al., 2025] that samples from πθ( x), our policy πθ( x; C) integrates external code execution, enabling hybrid reasoning. This design enables more effective decision-making for reasoning with code execution. Our method builds on two established policy gradient algorithmsProximal Policy Optimization (PPO) [Schulman et al., 2017, Ouyang et al., 2022] and Group Relative Policy Optimization (GRPO) [Shao et al., 2024, Guo et al., 2025]to optimize this code-enhanced reasoning process. In the remainder of this paper, we primarily focus on GRPO-based training, which we find outperforms PPO in our experiments. comparative analysis is provided in Section 5. Loss masking for code execution results In both PPO and GRPO, token-level loss is typically computed over the entire rollout. However, each rollout in R1-Code-Interpreter includes both LLMgenerated tokens and code execution results. Optimizing the latter can lead to undesirable behavior, where the model tries to predict execution outputs itself. To prevent this, we mask code execution tokens and compute the policy gradient only over LLM-generated tokens. Based on our experiments, this approach stabilizes training and aligns with findings in retrieval-based RL [Jin et al., 2025]. GRPO + Code Interpreter To improve policy optimization stability and avoid value-function approximation, GRPO differs from PPO by using the average reward of multiple sampled outputs as baseline instead of learned value function. Specifically, for each input x, GRPO samples group of responses {y1, y2, . . . , yG} from the reference policy πref and optimizes the policy by maximizing: JGRPO(θ) = ExD, y1:Gπold(x;C) (cid:34) 1 (cid:88) i=1 1 yi yi (cid:88) t=1 min (cid:16) πθ πref clip (cid:16) πθ πref (cid:0)yi,t x, yi,<t; C(cid:1) (cid:0)yi,t x, yi,<t; C(cid:1) , 1 ϵ, 1 + ϵ (cid:17) (cid:17) ˆAi,t (cid:0)yi,t x, yi,<t; C(cid:1) (cid:0)yi,t x, yi,<t; C(cid:1) (cid:35) ˆAi,t, β DKL (cid:2)πθ πref (2) (cid:3), where ϵ and β are hyperparameters, and ˆAi,t is the advantage, computed from the relative rewards of responses within each group. This eliminates the need for separate value network. Note that yi,t is an LLM-generated token, not code execution token. GRPO incorporates the KL divergence term DKL between the policy and the reference model directly into the loss, rather than as reward penalty, and applies the same token masking when computing it. Reward function The reward function provides the core training signal for reinforcement learning. For R1-Code-Interpreter, we adopt rule-based reward scheme based solely on final outcome correctness. In factual reasoning tasks, this involves exact matching; in planning tasks, it checks whether all constraints and goals are satisfied. Unlike the recent work by DeepSeek [Guo et al., 2025], we exclude format-based rewards, as our model already adheres well to structural formats. We also avoid training neural reward models for either outcome or process evaluation, due to their vulnerability to reward hacking and the high cost of retraining, following the approach by DeepSeek [Guo et al., 2025]."
        },
        {
            "title": "4 Experiments",
            "content": "Experimental settings We conduct experiments using three default base models: Qwen2.5-14BInstruct-1M, Qwen2.5-7B-Instruct-1M, and Qwen2.5-3B-Instruct [Qwen et al., 2025]. Additional ablations use DeepSeek-distilled reasoning models (3B/7B/14B) [Guo et al., 2025]. Code execution is performed on 48-core CPUs, with each question allowing up to 5 code calls and 60-second timeout per script. For training, we sample 50 questions per each of the 107 train tasks, generating 6.5k SFT trajectories via GPT-4o rejection sampling with varied prompts, followed by GRPO fine-tuning. Evaluation uses disjoint set of 100 questions per train task and all 37 test tasks to assess in-domain and out-of-domain generalization. For each training, we select the final model based on the checkpoint with the lowest validation loss for SFT, and the highest training reward for GRPO and PPO at the converging stage. SFT is trained for 3 epochs to prevent overfitting. GRPO uses learning rate of 1e-6 with 5 sampled responses per prompt and KL penalty of 0.001. For PPO, the actor and critic learning rates are set to 1e-6 and 1e-5, respectively. Learning rates are tuned in early-stage experiments. Training and inference temperatures are set to 1.0 and 0.6, respectively. We use batch size of 32 for SFT and 128 for GRPO and PPO. Both stages perform full-parameter fine-tuning on 8 H100 GPUs. The RL training is based on the VeRL framework [Sheng et al., 2025]. The complete training of R1-CI-14B takes approximately 1600 GPU hours (see Sec. 5 for further discussion). Unless stated otherwise, GRPO is the default RL method. Baselines To evaluate the effectiveness of R1-Code-Interpreter (R1-CI), we compare it against several baselines: All Text + CoT prompting LLMs to reason using only text with chain-of-thought (CoT); All Code + CoT prompting LLMs to first reason with CoT, then produce code as the answer; CI wo Fine-tune prompting non-fine-tuned LLMs to use the Code Interpreter with the prompt shown in Table 1; R1-CI wo GRPO our R1-Code-Interpreter trained only with SFT, without GRPO. For broader comparison, we also benchmark R1-CI against GPT-4o + All Text + CoT and GPT-4o + OpenAI Code Interpreter. The prompt for All Text + CoT and All Code + CoT is displayed in Appendix Sec. C. Evaluations Answers are evaluated using predefined rules, with GPT-4o assisting in format normalization when necessary. For methods that output code as the final answer, we extract and execute the code using predefined logic to obtain the final result or support further reasoning. To prevent infinite loops, each execution is limited to 60 seconds; timeouts result in failure or error handling in subsequent rounds. Task performance is measured using success rate. Figure 3: Score distribution across 144 training and test tasks. (a) Score frequency distribution for the four compared methods. (b) Method performance across six evaluation capabilities in 144 tasks. The evaluated models are either with the size of 14B or GPT-4o."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Performance Table 2 presents the experimental results of R1-Code-Interpreter (R1-CI) compared to baseline methods across 107 training and 37 test tasks. The full experimental results are in Appendix Sec. E. R1-CI significantly enhances the models reasoning and planning abilities, improving the average 6 Table 2: Scores of compared methods on 144 tasks across three benchmarks SymBench (SymB.), Big-Bench-Hard (BBH), and Reasoning-Gym (Rea.G.). Best result for each dataset is bold, and the second best is set in underline. We abbreviate R1-Code-Interpreter as R1-CI. 107 (26/20/61) Train Tasks Rea.G. BBH SymB. 37 (7/7/23) Test Tasks SymB. BBH Rea.G. Avg. Train Avg. Test Method (Acc %) GPT-4o All Text + CoT Code Interpreter 40.7 63.7 Qwen2.5-14B-Instruct-1M All Text + CoT All Code + CoT CI wo Fine-tune R1-CI wo GRPO R1-CI 24.0 40.0 27.9 69.0 71.1 Qwen2.5-7B-Instruct-1M 19.5 All Text + CoT 27.5 All Code + CoT 27.5 CI wo Finetune 65.6 R1-CI wo GRPO 67.7 R1-CI Qwen2.5-3B-Instruct All Text + CoT All Code + CoT CI wo Finetune R1-CI wo GRPO R1-CI 12.0 18.3 13.8 62.3 66.0 86.7 85.0 77.7 78.1 64.6 87.2 87. 66.0 70.2 69.0 83.7 85.2 55.1 60.4 56.9 78.6 81.1 45.5 63.3 35.8 40.4 30.0 48.3 52.4 25.7 25.7 20.0 45.6 49.1 11.2 5.0 5.4 36.9 40. 43.1 63.9 22.3 53.1 33.7 56.0 59.6 14.6 44.1 42.6 55.3 55.9 12.0 33.9 25.4 47.6 49.3 87.3 85.9 80.4 78.0 70.4 77.3 79. 69.1 63.1 69.7 70.1 74.1 49.7 52.3 51.9 61.6 64.7 54.6 68.4 39.6 35.7 39.7 56.1 60.6 26.3 34.0 27.0 53.6 57.9 10.7 8.9 8.8 44.0 48. 52.5 67.4 40.8 47.3 35.9 60.6 63.6 31.7 34.5 31.0 57.6 60.4 19.6 18.6 17.1 50.9 54.5 58.6 70.9 44.1 47.0 44.4 60.1 64. 32.2 41.5 38.1 57.0 60.6 18.3 21.8 20.1 48.0 51.5 Figure 4: Evolution of training rewards and testing scores during GRPO training. (a) For all model sizes (3B, 7B, and 14B), the training rewards generally increase in the early stepsdespite some fluctuationsbefore stabilizing at plateau. (b) In the 14B setting, the testing scores across individual tasks exhibit diverse trends. However, the average test score increases during the early training steps and then plateaus, mirroring the behavior of training rewards in (a). success rate by 28.8% on training tasks and 27.2% on test tasks across 3B, 7B, and 14B model sizes. Notably, R1-CI-14B achieves 64.1% success rate on test tasks, outperforming the much larger GPT4o with CoT-only reasoning (58.6%) and approaching GPT-4o with Code Interpreter (70.9%). These consistent improvements across all model sizes highlight the effectiveness of R1-Code-Interpreter. Score distribution and capability analysis Figure 3a shows the score distribution across 144 tasks for the four compared methods. We observe that SFT and GRPO training significantly reduce the number of tasks on which R1-CI models perform poorly. However, some tasks still yield low or Figure 5: Training-reward curves across training steps for progressively smaller task sets (50 questions per task). (a) All 107 training tasks. (b) Five tasksGame24, reasoning_gym_graph_color, reasoning_gym_group_anagrams, Blocksworld, and BoxLift. (c) Single taskGame24. even zero scores. This indicates that the inherent capabilities of the base LLM strongly affect overall performance, and training alone cannot overcome limitations on tasks beyond the models inherent reasoning or knowledge abilities. Figure 3b shows the average score for each capability, computed over all tasks labeled with that capability in Table 4 (Appendix D). R1-CI-14B performs comparably to GPT-4o+Code Interpreter across most capabilities, except for optimization, where performance is lower likely due to the increased complexity of coding and reasoning in those tasks. GRPO training curves Figure 4 presents the GRPO training curves for three model sizes, showing the evolution of training rewards and testing scores over training steps. For all model sizes, training rewards generally increase in the early stages and then plateau, though with noticeable fluctuations. In Figure 4b, the testing scores exhibit varied trends across individual tasks due to differing task characteristics. Nevertheless, the average testing score follows similar patterninitially rising and then convergingmirroring the trend observed in training rewards. In contrast to prior works on RL-enhanced LLM reasoning [Guo et al., 2025, Jin et al., 2025, Zhang et al., 2025, Shao et al., 2024], which often show smoother and more apparent reward improvements, our GRPO training for general-purpose Code Interpreter presents greater challenges. Most of these earlier studies focus on narrow domains such as math or retrieval, or rely on extremely large models (e.g., 600B in DeepSeek-R1) to handle task diversity. We hypothesize that the difficulty in our setting stems from the diversity of the 107 training tasks. As shown in Figure 5, training on fewer tasks leads to more stable and pronounced reward improvements. In particular, training on single task like Game24a task that requires fine-grained switching between text and code reasoning [Chen et al., 2024b]yields reward curves similar to those in domain-specific settings. This suggests that task diversity is key factor behind the instability in GRPO training for general-purpose models. Nonetheless, GRPO still satisfyingly improves average success rates by over 3% on both training and test tasks, comparable to the performance gap between 7B and 14B models  (Table 2)  . GRPO training cost GRPO training for the Code Interpreter is computationally expensive. For instance, training R1-CI-14B takes around 1600 GPU hours. The cost arises mainly from two factors: (1) GRPO requires multiple (e.g., 5) sampled rollouts per answer turn to enable reward-based comparison, which is further intensified in our multi-turn generation setting; (2) the Code Interpreter introduces additional overhead due to costly code execution, especially for scripts involving search, iteration, or optimization. Although we cap the execution time at 60 seconds per script, it remains major time sink. These highlight the need for more efficient RL training approaches. Figure 6: (a1a2) show the response length trajectories of Qwen-3B during GRPO fine-tuning after SFT, with (a1) trained on all 107 tasks and (a2) on the single task Game24. (b1b2) present the code usage ratio (b1) and the average number of LLM inference turns per question (b2). 8 5.2 Response characteristics Response length study Previous work [Guo et al., 2025, Jin et al., 2025] observed that LLM responses tend to grow longer during RL training, as the model learns to explore solutions through long-chain reasoning. Figure 6a1-a2 show the average response length over training steps. In contrast to prior findings, we observe no significant length increase even when training on single task. Possible reasons include: (1) The SFT stage already instills long-chain reasoning, as the data is synthesized by GPT-4o; (2) The multi-turn interaction spreads reasoning across turns, reducing per-turn response length; (3) Code-augmented reasoning reduces reliance on long CoT chains, as it does not require iterative textual search. Code usage ratio Figure 6b1 shows the average code usage ratio per question. After training, the model learns to better balance between text-based and code-based solutions, instead of purely relying on code execution. Figure 6b2 shows the average number of inference turns per question, indicating that the model becomes more efficient after SFT and GRPO by using fewer turns. Emergent behavior of Self-Checking During GRPO training, we observe emergent behavior where the model integrates textual reasoning and code execution to improve solution verification. For instance, in the final two reasoning turns of Figure 2, the model generates code to check whether the proposed solution satisfies the constraints. Across test samples, the model learns to verify answers using either textual reasoning or code execution, demonstrating an emergent self-checking behavior that enhances reasoning and planning. Table 3: Ablation studies on using DeepSeek-distilled reasoning models as the base model. Tested on 37 Test Tasks Model 7B, SFT 7B, All Text 7B, All Code 14B, All Text 14B, All Code DeepSeek Qwen-2.5 53.1 57.0 27.9 32.2 28.7 41.5 40.1 44. 43.4 47.0 Figure 7: GRPO vs. PPO and warmvs. cold-start. (ab) GRPO consistently beats PPO when training on (a) all 107 tasks and (b) the single task Game24 in the 3B model. (cd) With GRPO, warm start (preceded by SFT) outperforms cold start for both Qwen-7B and Qwen-3B. 5.3 Ablation study Reasoning vs. non-reasoning models as base: Table 3 compares training performance using the general-purpose Qwen-2.5 models versus the same sized long-chain reasoning models from DeepSeek. Whether after SFT or using the raw models, Qwen consistently outperforms DeepSeek, particularly in code generation for solving tasks. GRPO vs. PPO: Figure 7(ab) compares GRPO and PPO using the same dataset and initial model. GRPO consistently outperforms PPO in both full-task training and single-task training settings. Warm starts vs. cold starts: Figure 7(cd) compares GRPO training with and without the initial SFT stage. Unlike prior findings [Guo et al., 2025, Jin et al., 2025] suggesting SFT is unnecessary or only marginally helpful, we observe that SFT is crucial for enabling the model to reason effectively with the Code Interpreter, even with extensive GRPO training. Masked vs. unmasked code outputs: We also test removing the masking of code execution outputs during GRPO training, which degrades the performance without masking, as shown in Figure 8."
        },
        {
            "title": "6 Related Work",
            "content": "Code generation and symbolic computing in LLM tasks LLMs are widely used in agent tasks such as software/web interaction [Zhou et al., 2023b, Hao et al., 2024, Xu et al., 2024], robot planning [Chen et al., 2024c, Ahn et al., 2022], and logical inference [Suzgun et al., 2022]. Many benchmark tasks can in fact be solved directly through code [Suzgun and Kalai, 2024, Gao et al., 2023], and recent work extends coding to reasoning and semantic analysis [Li et al., 2023, Weir et al., 2024]. Most prior approaches use either text [Yao et al., 2024, Ahn et al., 2022] or code [Liang et al., 2022, Bairi et al., 2024, Zhou et al., 2023c] exclusively as output. Recent work [Chen et al., 2024b] emphasizes the need to dynamically switch between modalities, proposing CodeSteer [Chen et al., 2025] as guidance model. However, direct training of LLMs to use Code Interpreter remains largely unexplored, especially for CoT reasoning LLMs. LLM self-reflection and CoT reasoning models LLM self-evaluation can enhance task performance across domains [Yang et al., 2022, Welleck et al., 2022, Madaan et al., 2023]. Models like OpenAI o1 [Jaech et al., 2024] and DeepSeek R1 [Guo et al., 2025] showcase agentic behavior via Chain-of-Thought (CoT) reasoning and self-reflection. Extensions with retrieval [Jin et al., 2025, Li et al., 2025a] and tool use [Qian et al., 2025] further improve reasoning, but lack symbolic computing and code generation, limiting performance on complex symbolic tasks and incurring high token and time costs [Chen et al., 2024a]. Integrating Code Interpreter into reasoning LLMs is essential and challenging [Li et al., 2025b]. LLM fine-tuning with multi-turn SFT and RL SFT [Chen et al., 2024d] and RL [Ouyang et al., 2022] are widely used for LLM fine-tuning. To handle multi-turn agent tasks, these methods are extended with goal-conditioned rewards [Zhou et al., 2024, Zhai et al., 2024, Zhang et al., 2024]. Self-generated data, combined with search and rejection sampling [Zhou et al., 2023a, Guan et al., 2025], have become key for improving reasoning. Recent works, such as DeepSeek R1 [Guo et al., 2025], apply rule-based outcome rewards to enhance reasoning and planning in math [Shao et al., 2024], code [Wei et al., 2025], QA [Jin et al., 2025], and vision-language tasks [Wang et al., 2025, Zhang et al., 2025], using GRPO [Shao et al., 2024] and PPO [Ouyang et al., 2022]."
        },
        {
            "title": "7 Conclusion",
            "content": "We present framework that integrates Code Interpreter into LLM reasoning and planning via supervised and reinforcement learning. Our fine-tuned model, R1-CI-14B, surpasses GPT-4o without Code Interpreter and approaches its performance with one. Adapting GRPO for training enhances model capability, but its impact is constrained by task diversity, underscoring the need for strong base model and SFT stage. To our knowledge, this is the first open-source, general-purpose Code Interpreter trained with such methods. We further explore training strategies and observe emergent checking behaviors. Future work should address task diversity bottlenecks and reduce training costs."
        },
        {
            "title": "References",
            "content": "Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Yongchao Chen, Yilun Hao, Yueying Liu, Yang Zhang, and Chuchu Fan. Codesteer: Symbolicaugmented language models via code/text guidance, 2025. URL https://arxiv.org/abs/ 2502.04350. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024a. Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36, 2024. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023a. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still cant plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. arXiv preprint arXiv:2210.07128, 2022. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, and Chi Wang. Steering large language models between code execution and textual reasoning. arXiv preprint arXiv:2410.03524, 2024b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, and Faramarz Fekri. Can llms reason in the wild with programs? arXiv preprint arXiv:2406.13764, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Jonathon Shlens. Notes on kullback-leibler divergence and likelihood, 2014. URL https://arxiv. org/abs/1404.2000. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/3689031. 3696075. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b. Yilun Hao, Yongchao Chen, Yang Zhang, and Chuchu Fan. Large language models can plan your travels rigorously with formal verification tools. arXiv preprint arXiv:2404.11891, 2024. Tianqi Xu, Linyao Chen, Dai-Jie Wu, Yanjun Chen, Zecheng Zhang, Xiang Yao, Zhiqiang Xie, Yongchao Chen, Shilong Liu, Bochen Qian, et al. Crab: Cross-environment agent benchmark for multimodal language model agents. arXiv preprint arXiv:2407.01511, 2024. Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and Chuchu Fan. Scalable multi-robot collaboration with large language models: Centralized or decentralized systems? In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 43114317. IEEE, 2024c. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with taskagnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pages 1076410799. PMLR, 2023. 12 Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. arXiv preprint arXiv:2312.04474, 2023. Nathaniel Weir, Muhammad Khalifa, Linlu Qiu, Orion Weller, and Peter Clark. Learning to reason via program generation, emulation, and search. arXiv preprint arXiv:2405.16337, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023c. Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 43934479, 2022. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2022. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025a. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025b. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024d. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024. Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, and Min Lin. Chain of preference optimization: Improving chain-of-thought reasoning in llms. arXiv preprint arXiv:2406.09136, 2024. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 13 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 14 R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning 1 Introduction 2 Task benchmark 3 R1-Code-Interpreter"
        },
        {
            "title": "3.2 Multi-turn RL with Code Interpreter . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4 Experiments 5 Analysis"
        },
        {
            "title": "5.1 Performance .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Response characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Related Work 7 Conclusion Limitations and societal impacts Ablation study on masking code execution output Prompt for All Text + CoT and All Code + CoT Description of reasoning and planning tasks Full table of experimental results 3 3 5 5 6 6 9 9 10 10 16 16 17"
        },
        {
            "title": "A Limitations and societal impacts",
            "content": "Limitations 1) As discussed in the paper, training is costlyabout 1600 GPU hours for the 14B model. This reflects broader challenge in RL-based LLM training, and in our case, the overhead of code execution further exacerbates the cost. 2) While our Code Interpreter is designed to be general-purpose across reasoning and planning tasks, its scope does not cover broader application-specific tasks like plotting or software generation. In such cases, R1-CI may offer limited gains. 3) Due to high task diversity, GRPO training yields only modest improvements (around 3%), which is still acceptable. However, many tasks remain bottlenecked by the base models inherent limitations. Scaling to larger, more capable modelssimilar to what DeepSeek exploredmay help address this. Societal impacts This paper contributes to advancing Foundation Models by augmenting language models with Code Interpreter, which has strong potential to improve safety, performance, and alignment with human preferences. However, such capabilities are inherently dual-use, the same techniques that augment models toward harmless outputs can, with minor changes, be misused to generate harmful content. While misuse is concern, we believe the broader societal benefits, particularly in improving evaluation and control over language model outputsoutweigh the risks."
        },
        {
            "title": "B Ablation study on masking code execution output",
            "content": "Figure 8: Masked vs. unmasked code execution outputs during GRPO training on 3B models. The masked setting ascends while the unmasked setting degrades. Prompt for All Text + CoT and All Code + CoT Prompt for All Text + CoT Analyze the question step by step and try to list all the careful points. Then try to acquire the final answer with step by step analysis. In the end of your response, directly output the answer to the question. Do not output the code for execution. 16 Prompt for All Code + CoT You are helpful AI assistant. Solve tasks using your coding skills. In the following cases, suggest python code (in python coding block) for the user to execute. Dont include multiple code blocks in one response, only include one in the response. Do not ask users to copy and paste the result. Instead, use print function for the output when relevant. Think the task step by step if you need to. If plan is not provided, explain your plan first. You can first output your thinking steps with texts and then the final python code. Remember in the final code you still need to output each number in the final equation! Start the python block with ``` python."
        },
        {
            "title": "D Description of reasoning and planning tasks",
            "content": "Here we describe the 144 training and testing tasks. They require strong symbolic, mathematical, logical, geometrical, scientific, and commonsense reasoning capabilities. The T1 to T33 tasks originate from SymBench [Chen et al., 2025], T34 to T60 tasks originate from Big-Bench-Hard [Suzgun et al., 2022], and the last T61 to T144 tasks are from Reasoning-Gym2. We select questions with diverse difficulty and standardize them as unified format to support fast rollout and testing. T1 - 2048 Similarly to the 2048 game, in grid, numbers representing powers of 2 can move in any direction, combining when they encounter matching number to form the next power of 2. Given starting position and sequence of movements, the goal is to determine the resulting grid after executing the moves. T2 - Blocksworld In Blocksworld, the objective is to stack set of blocks (brown) according to specific order. The robot can perform four actions: (1) pick up block, (2) unstack block from the top of another block, (3) put down block, (4) stack block on top of another block. robot can only pick up, unstack, or stack block if it is clear, that is, the block has no other blocks on top and is not currently being held. T3 - BoxLift This task involves coordinating robots of various types to lift boxes of different sizes and weights. Each robot has specific lifting capacity and can collaborate with others to lift single box. box can only be lifted if the combined lifting capacity of the robots exceeds the boxs weight. The objective is to lift all the boxes in the minimum number of time steps. T4 - BoxNet This task involves coordinating robot arms to move colored boxes (squares) into corresponding colored goal locations (circles) in the fewest time steps. Each robot arm is assigned and restricted to cell indicated by the dotted lines. The arms have two possible actions: (1) move box within their cell to neighboring cell, or (2) move box within their cell to goal location within the same cell. The objective is to ensure all boxes are placed in their matching goal locations efficiently. T5 - Combinatoral Calculation Given set of integers, the goal is to use arithmetic operations (addition, subtraction, multiplication, division) and parentheses to arrange the numbers in such way that the final result matches specified target value. Each number must be used exactly once, and the order of the numbers cannot be changed. T6 - Constrained Linear Arrangement In two-player card game, the task is to deduce your opponents moves based on the games rules, your played cards, and the announced results of each round. Each card can only be used once, and the game follows specific interaction rules between different card types, where certain cards can defeat, be defeated by, or draw with others according to predefined relationships. T7 - Cryptanalysis In this task, you are provided with combination lock consisting of numbers and letters, where neither the numbers nor the letters repeat. Using series of guesses and feedback, the goal is to deduce the correct password based on the given conditions. T8 - Eight Queens Given grid with some queens already placed, the task is to place the remaining queens such that no two queens share the same row, column, or diagonal, while avoiding positions with obstacles in the grid. 2https://github.com/open-thought/reasoning-gym 17 T9 - Game 24 This task involves querying LLMs to use given set of integers to generate an equation that evaluates to 24. T10 - Gridworld This task involves querying LLMs to plan the robot actions in grid world, reaching all goals in any order while avoiding obstacles. T11 - GSM [Gao et al., 2023] This is the more challenging version of GSM8K [Cobbe et al., 2021] math reasoning dataset, where the numbers in the original questions of GSM8K are replaced with larger, less common values. T12 - Letter Logic Diagram The task is to complete an incomplete grid by selecting from list of letters, where each row and column must contain each letter exactly once, and all cells on the minor diagonal (top-right to bottom-left) must contain the same letter. Some cells are already filled in as constraints. T13 - Letters This task involves querying LLMs to count the total number of specific letters in long word and specify their positions. An example question can be How many rs in the word strawberry and what are their positions?. This task has recently gained significant attention because current LLMs struggle to perform it effectively and accurately. In this task, you are given an grid representing network of lights, T14 - Light Puzzles where lit light is represented by \"1\" and an unlit light by \"0\". Several buttons control the state of these lights by turning them on or off in certain positions. The state of each light can be affected by multiple buttons. The task is to follow series of button presses and determine the final state of the grid. T15 - Logical Puzzle The task involves querying LLMs to select specified number of different values from grid of numbers, ensuring that certain mathematical constraints (sum or product) are satisfied for selected numbers for each row and column. T16 - Logical Equation The task is to assign specific numeric value to each letter from given set, using predefined range of numbers and set of inequalities. Each letter corresponds to unique number, and the relationships between the letters are defined by mathematical equations or constraints. T17 - Mahjong Given an initial set of letter cards, in each round, new card is added and one card is removed. Some effects may happen when specific combinations of the cards appear after introducing the new card. result is determined based on these specific conditions. The goal is to determine result based on series of rounds. T18 - MATH-Count&Probability This dataset [Hendrycks et al., 2021], with specific focus on counting and probability questions. the math reasoning dataset is from MATH T19 - MATH-Geometry This is the math reasoning dataset from MATH dataset [Hendrycks et al., 2021], with specific focus on geometry questions. T20 - Matrix Transformation Rotate given matrix of characters based on given instruction (e.g., 90 degrees clockwise), preserving each characters position relative to others in the transformed output. The input matrix can be of any size and contain any character. T21 - New Operator This task introduces custom mathematical operations involving two numbers, defined with unique formulas. The goal is to use the given definitions of these operations to compute the result of specific expression. T22 - Number Multiplying This task involves querying LLMs to compute the product among integers. It represents classic problem that LLMs are not able to solve through pure textual reasoning. T23 - Pattern Recognition The task involves querying LLMs to find all squares in character matrix where each square consists of identical characters and has side length of at least 3. T24 - Permutation and Combination Given set of objects with specific positioning constraints, the task is to determine the correct arrangement of the objects on shelf. Each object must be placed in position according to the rules provided, ensuring that the conditions on adjacency, order, and specific positions are met. For example, rule about adjacency could be Book must be adjacent to book I. 18 T25 - Pooling This task involves applying pooling operation on numerical grid. The pooling operation uses an sliding window (n < ) that moves across the grid from left to right and top to bottom. The results from each window are then arranged based on their positions to create new output matrix. In this game similar to Reversi, players take turns placing pieces on an grid. T26 - Reversi After placing piece, any of the opponents pieces located between two of the players pieces (in the same row, column, or diagonal) will be flipped. The task is to determine the state of the board after rounds, starting from given configuration. T27 - Standard Sudoku Given partially filled Sudoku grid, the task is to fill the remaining empty cells with numbers between 1 and 9, ensuring that no number repeats in the same row, column, or 3 3 subgrid. T28 - Statistical Counting Calculate the total score of string by scanning it from left to right, where consecutive identical letters earn points (for example, two or more consecutive As add 1 point, Bs add 2 points, etc.). The task is to start with score of 0 and return the final summing value. T29 - String Deletion and Modification The task is to transform string by repeatedly applying set of ordered string manipulation rules until no more changes are possible, where each rule modifies the string based on specific patterns or conditions present in the current string state. For example, modification rule can be If the string ends with ba, replace it with ab. T30 - String Insertion The task is to transform string by scanning it from left to right and inserting specific characters after certain character patterns (e.g., each pattern WXYZ requires inserting immediately after it occurs). All operations are performed simultaneously on the original string. T31 - String Splitting dismantling engineer has old machines and can obtain machine parts through set of predefined methods. By continuously cycling through these methods in specific order, the engineer dismantles machines or combines parts to create new components, and the task is to determine the total number of parts and remaining machines after all possible cycles. T32 - String Synthesis Given an initial set of blocks and set of synthesis rules that combine different types of blocks, the task is to determine the final block(s) after repeatedly applying these rules in order until no more combinations are possible. T33 - Synthesis Decomposition farmer grows various crops and can exchange them for agricultural products. Using set of methods, he can trade specific combinations of crops for products, following cyclic pattern until no further exchanges are possible. The goal is to determine the synthesis result for each round. T34 - Boolean Expressions This task determines whether randomly generated Boolean expressionbuilt from the constants True and False and the operators and, or, and notevaluates to true or false. T35 - Causal Judgment This task involves querying LLMs to read brief scenario and predict the answer an average person would give to causal question about it, including moral, intentional, or counterfactual aspects. T36 - Date Understanding This task involves querying LLMs to interpret few sentences that reference dates and answer related question (e.g., compute and return specific date in MM/DD/YYYY format). T37 - Disambiguation QA For sentence containing potentially ambiguous pronoun, the task is to decide whether its reference is genuinely unclear; if it is clear, identify the noun to which the pronoun refers. T38 - Dyck Languages The task aims to complete Dyck-4 string by providing the missing closing parentheses that properly balance the given prefix. T39 - Formal Fallacies The task examines set of statements and judges whether the informal argument that follows is deductively valid or commits formal fallacy, with particular attention to negations. T40 - Geometric Shapes The task aim to analyze full SVG path description and identify the geometric figure that would be drawn. T41 - Hyperbaton Given two sentences, choose which of two sentences follows the natural English ordering of adjectives. T42-T44 - Logical Deduction 3/5/7 Objects Use spatial clues to determine the correct ordering of set of objects (3/5/7 objects). T45 - Movie Recommendation From four candidate films, select the one that best matches the preferences implied by the movies user has already enjoyed. T46 - Multi-Step Arithmetic Perform multi-step calculations involving addition, subtraction, multiplication, and division to obtain the correct result. T47 - Navigate Follow sequence of movement instructions and state whether the agent finishes at its starting point. T48 - Object Counting Given list of items and their quantities, count how many belong to specified category. T49 - Penguins in Table Refer to table of individual penguins and their attributes (possibly with extra information) to answer question about one of those attributes. T50 - Reasoning about Colored Objects Using the provided context, state the color of particular object on surface. T51 - Ruin Names Make single-character change to an artist, band, or movie name to create humorous new meaning. T52 - Salient Translation Error Detection Examine German sentence and its English translation, and classify the main translation error present. T53 - Snarks From pair of nearly identical sentences, identify the one that is sarcastic. T54 - Sports Understanding Judge whether fabricated sports-related statement is plausible. T55 - Temporal Sequences Given timeline of persons daily activities, identify time slot when they could have performed another specified task. T56-T58 - Tracking Shuffled 3/5/7 Objects Trace set of objects (3/5/7 objects) through series of pairwise swaps to determine their final positions. T59 - Web of Lies Decide whether Boolean function described in word problem evaluates to true or false. T60 - Word Sorting Arrange the provided words in standard alphabetical order. T61 - AB Rewrite an A::B token string by exhaustively applying neighbor collision rules and return the final sequence. T62 - Acre From example Blicket detector outcomes, decide whether new object set turns the detector on, off, or is undetermined. T63 - Advanced Geometry Solve analytic geometry questions (e.g. angle, orthocentre, in-circle radius) given vertex coordinates. T64 - AIW Answer small Alice-in-Wonderland social reasoning problems about siblings, friends, or colleagues. T65 - ARC_1D Infer the mapping rule that maps example 1D grids to output grids and apply it to test grid. T66 - ARC_AGI Same as ARC_1D but with rotations, mirrors and permutations on 2-D grids. T67 - Base Conversion The task is to convert integers between arbitrary bases. T68 - Basic Arithmetic The task is to evaluate the value of basic arithmetic expressions. T69 - BF Based on outputs of example BF codes, the task is to output the contents of new BF program. T70 - Binary Alternation The task is to produce the minimum number of character swaps to make binary string to be alternating, that is, no two adjacent characters are equal. 20 T71 - Binary Matrix Given binary matrices, the task is to find the distance to the nearest 0 for each cell in the matrix. T72 - Bitwise Arithmetic The task is to compute results of expressions with mixed bitwise and arithmetic operators. T73 - Caesar Cipher The task is to decrypt Caesar cipher text. T74 - Calendar Arithmetic Given description of the calendar, answer question by conducting arithmetic calculations such as adding or subtracting days / months / years or computing weekday differences. T75 - Chain Sum The task is to calculate simple arithmetic problem and output the answer. T76 - Circuit Logic Given logic circuit with logical operators, the goal is to evaluate the output of given inputs. T77 - Codeio The task is to read and reason about task description and pseudocode programs and report outputs of the program given inputs. T78 - Color Cube Rotation After rotating 3D colored cube, the task is to name the color on queried face. T79 - Complex Arithmetic The task is to perform arithmetic with complex numbers and report answers. T80 - Count Bits Given large number, the goal is to count the number of occurrence of 1 bits in the binary representation of this number. T81 - Count Primes The task is to count number of primes numbers within an interval. T82 - Countdown The task is to write an expression that can reach target integer using given numbers and the four operations. T83 - Course Schedule Given list of courses need to be taken and their prerequisites, the task is to determine if all courses can be finished. T84 - Decimal Arithmetic The task is to evaluate decimal expressions with given precision. T85 - Dice The task is to compute probabilities of rolling results in fair-dice experiments with various dices with different number of sides. T86 - Emoji Mystery The task is to deduce hidden sentences expressed with emoji symbols. T87 - Family Relationships The task is to answer kinship queries in family trees. T88 - Figlet Font The task is to read FIGlet banners and output the content as strings. T89 - Fraction Simplification The task is to simplify fractions to the lowest terms. T90 - Futoshiki The task is to fill in values to empty spaces of Futoshiki puzzles that have inequalities. T91 - Game of Life The task is to simulate Conways Game-of-Life for steps. T92 - Game of Life Halting The task is to decide whether Game-of-Life configuration halts within steps, that is, there are no cells alive. T93 - GCD The task is to compute greatest common divisors of numbers. T94 - Graph Color The task is to provide coloring for this graph such that every vertex is not connected to vertex of the same color. T95 - Group Anagrams Given list of words, the task is to cluster words that are anagrams. T96 - Intermediate Integration Given an expression, the task is to calculate the indefinite integral. T97 - Isomorphic Strings The task is to decide if two strings can be isomorphic if the characters in one string can be replaced to get the second string. T98 - Jugs Given empty jugs with different sizes, the task is to give plan of how to fill any of the available jugs with the target amount of water by filling, emptying, or pouring from jug to another. T99 - Knight Swap The task is to swap two knights on chessboard in the fewest moves. 21 T100 - Knights Knaves The task is to determine who is knight (truth-teller) or knave from their statements. T101 - Largest Island The task is to find max connected component size in binary grid. T102 - LCM The task is to find the Least Common Multiple (LCM) of numbers. T103 - Leg Counting The task is to count how many legs there are in total when given list of animals. T104 - Letter Jumble For each word in sentence, the letter may have been randomly shuffled. The task is to reconstruct original words from jumbled letters. T105 - List Functions Given examples of how inputs are mapped to outputs, reason and use the same mapping to generate output given an input. T106 - Manipulate Matrix Apply sequence of matrix transformations to matrix and output the result. T107 - Maze Compute the shortest path length from start to goal in an maze. T108 - Modulo Grid Identify the mathematical pattern which defines grid, then use that pattern to fill in the question marks in this grid. T109 - Needle Haystack The task is to locate short pattern inside longer string. T110 - Number Filtering Given list of numbers and requirement, remove numbers not satisfying this requirement. T111 - Number Format The task is to pick the largest/smallest number out of several options. T112 - Number Sequence Predict the next term of integer sequences based on previous patterns. T113 - Number Sorting The task is to sort number lists based on required order. T114 - Palindrome Generation The task is, given list of letters, to form valid palindrome. T115 - Palindrome Partitioning Given string, the task is to find all ways to partition it such that every substring is palindrome. T116 - Polynomial Equations The task is to find real values of variable in polynimial equation. T117 - Polynomial Multiplication The task is to calculate the result of multiplying two polynomials. T118 - Pool Matrix The task is to perform maxor average-pooling on numeric matrices. T119 - Products The task is to compute multiplications of numbers. T120 - Propositional Logic Given list of premises, the task is to infer correct conclusion from the premise. T121 - Quantum Lock There are some buttons, light, and number. The light will toggle between red and green whenever you press button. Each button performs mathematical operation to the number, but the operation may depend on the state of the light. You must press the shortest correct sequence of buttons to reach the target value. T122 - Ransom Note Given two strings representing ransom note and magazine, determine if the ransom note can be constructed using the letters in the magazine T123 - Rearc Find the common rule that maps input grids to output grids and apply the rule to predict corresponding output of test input grid. T124 - Rectangle Count The task is to count how many rectangles are present in an ASCII grid. T125 - Rotate Matrix Given square matrix, the task is to rotate it and output the rotated matrix. T126 - Rotten Oranges You are given an grid where each cell can be empty cell, contain fresh orange, or contain rotten orange. Every minute, any fresh orange that is 4-directionally adjacent to rotten orange becomes rotten. Your task is determine the minimum number of minutes that must elapse until no cell has fresh orange. 22 T127 - Rubiks Cube Given Rubiks cube, the task is to provide solution to solve this cube using Singmaster notation. T128 - Rush Hour Given rush hour parking grid, the task is to give plan of movements of cars to achieve required car positions. T129 - Self Reference The task is to evaluate self-referential arithmetic expressions to output the number of possible solutions. T130 - Shortest Path The task is to find the length of the shortest path in grid. T131 - Simple Equations The task is to solve equations with one variable. T132 - Simple Geometry Given polygon with different number of sides and all interior angles but one angle, the task is to calculate the remaining interior angle. T133 - Simple Integration The task is to find solution to indefinite integral problems with one variable. T134 - Sokoban The task is to find list of actions to solve Sokoban level. T135 - Spell Backward The task is to reverse input strings. T136 - Spiral Matrix Given matrix, the task is to generate list of elements in spiral order, starting from the top-left element. T137 - String Manipulation The task is to repeatedly transform string according to set of rules until no further transformations can be performed, or state is repeated. T138 - Syllogism Given some statements, answer the provided question by retrieving information from the statements. T139 - Time Intervals The task is to compute durations between two times / dates with various formats and complexities. T140 - Tower of Hanoi Output an optimal (or specified) move list to transfer disks between pegs to solve tower of Hanoi problem. T141 - Tsumego Choose the single correct Go move to capture or save stones. T142 - Word Ladder Transform one word to another by single-letter changes using dictionary words. T143 - Word Sequence Reversal Given list of words, the task is to reverse order of words. T144 - Zebra Puzzles Given some statements, solve the logic puzzle by gathering information from statements and deduce the answer of the question. Table 4: The evaluated capabilities of all tasks, classified as Execution, Planning, and Reasoning tasks. Logical Order Optimization Task 2048 u E Light Puzzles Mahjong Matrix Transform. New operator Number Multiplying Pattern Recognition Pooling Reversi Statistical Counting String Del. &Modi. String Insertion String Splitting String Synthesis Synthesis Decomp. Dyck Languages Math Spatial 23 Search Continued on next page Table 4 (continued from previous page)"
        },
        {
            "title": "Logical Order Optimization",
            "content": "Task Multi-Step Arithmetic Navigate Object Counting Ruin Names Tracking Shuffled Obj. Word Sorting AB ARC_1D ARC_AGI Base Conversion Basic Arithmetic BF Binary Alternation Binary Matrix Bitwise Arithmetic Caesar Cipher Chain Sum Codeio Color Cube Rotation Complex Arithmetic Count Bits Count Primes Decimal Arithmetic Dice Fraction Simplification GCD Group Anagrams Intermediate Integration Isomorphic Strings Largest Island LCM Leg Counting Letter Jumble List Functions Manipulate Matrix Number Filtering Number Format Number Sorting Palindrome Generation Palindrome Partitioning Poly. Equations Poly. Multiplication Pool Matrix Products Rectangle Count Rotate Matrix Rotten Oranges Simple Equations Simple Geometry Simple Integration Spell Backward Spiral Matrix String Manipulation Time Intervals Word Seq. Reversal Blocksworld Math n P Spatial Search Continued on next page 24 Table 4 (continued from previous page)"
        },
        {
            "title": "Logical Order Optimization",
            "content": "Task BoxLift BoxNet Combinatorial Calc. Const. Linear Arrange. Cryptanalysis Eight Queens Game 24 Gridworld Letter Logic Diagram Letters Logic Puzzle Permut. and Combina. Standard Sudoku Movie Recommendation Temporal Sequences Countdown Course Schedule Futoshiki Graph Color Jugs Knight Swap Maze Modulo Grid Quantum Lock Rubiks Cube Rush Hour Shortest Path Sokoban Tower of Hanoi Tsumego Word Ladder Logical Deduction s GSM MATH-Count&Prob. MATH-Geometry Hyperbaton Logical Deduction Penguins in Table Reasoning Colored Obj. Salient Trans. Err. Detect. Snarks Sports Understanding Web of Lies Acre Advanced Geometry AIW Calendar Arithmetic Circuit Logic Emoji Mystery Family Relationships Figlet Font Game of Life Game of Life Halting Knights Knaves Needle Haystack Number Sequence Math Spatial 25 Search Continued on next page Table 4 (continued from previous page)"
        },
        {
            "title": "Task\nPropositional Logic\nRansom Note\nRearc\nSelf Reference\nSyllogism\nZebra Puzzles",
            "content": "Math Spatial Search"
        },
        {
            "title": "E Full table of experimental results",
            "content": "Table 5: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-14BInstruct-1M, GPT-4o). Methods Qwen2.5-14B-Instruct-1M GPT-4o % r c k Ave. Norm., Seen Ave. Norm., Unseen Ave. Norm., Total SymBench Blocksworld BoxLift Combinatorial Calculation Constrained Linear Arrangement Cryptanalysis GSM Letter Logic Diagram Light Puzzles Logic Puzzle Logical Equation Mahjong Pattern Math Geometry Matrix Trans New Operator Pattern Recognition Permutations And Combinations Pooling Reversi Standard Sudoku Statistical Counting String Deletion And Modification String Insertion String Splitting String Synthesis Synthesis Decomposition 2048 Big Bench Hard Boolean Expressions Causal Judgement + T 40.8 44.1 41.7 + C 47.3 47.0 47.2 t - F C 35.9 44.4 38.1 G C - 1 60.6 60.1 60.5 Seen Tasks 0 0 40 36 7 47 0 56 27 24 41 58 38 36 43 58 26 11 45 29 23 10 27 1 32 11 74 48 63 81 78 18 79 42 90 77 63 68 76 95 40 80 63 50 65 100 97 93 99 64 44 76 46 99 71 0 0 37 29 7 79 0 86 34 21 54 73 53 35 74 61 29 29 65 70 67 18 20 6 53 39 98 69 0 0 39 31 9 74 0 19 26 18 55 77 28 35 12 50 33 2 0 22 1 0 16 0 47 31 98 26 + T 52.5 58.6 54.1 e e e 67.4 70.9 68.3 1 10 44 78 20 77 2 69 62 52 65 78 79 37 50 72 48 4 0 48 1 6 64 2 48 100 71 0 26 83 82 18 80 7 99 67 73 68 73 91 43 98 86 50 68 100 99 67 82 71 13 74 38 98 70 - 1 63.6 64.1 63.7 54 63 81 74 23 81 51 92 81 59 75 78 97 43 83 65 49 68 100 96 93 100 64 49 78 100 68 Table 5: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-14BInstruct-1M, GPT-4o). 5) Methods Qwen2.5-14B-Instruct-1M GPT-4o % r c k T Date Understanding Disambiguation QA Dyck Languages Formal Fallacies Geometric Shapes Hyperbaton Logical Deduction Five Objects Logical Deduction Seven Objects Logical Deduction Three Objects Movie Recommendation Multistep Arithmetic Two Navigate Object Counting Penguins In Table Tracking Shuffled Objects Seven Objects Tracking Shuffled Objects Three Objects Web of Lies Word Sorting Reasoning Gym Ab Acre Advanced Geometry Aiw Arc 1d Arc Agi Base Conversion Basic Arithmetic Bf Binary Alternation Codeio Color Cube Rotation Complex Arithmetic Count Primes Countdown Course Schedule Dice Emoji Mystery Family Relationships Fraction Simplification Futoshiki Game Of Life Gcd Graph Color Group Anagrams Isomorphic Strings Jugs Knight Swap Largest Island Lcm Leg Counting List Functions Manipulate Matrix + T T + C e - F C G C - 1 66 63 3 56 42 67 79 77 94 66 70 73 55 62 71 80 61 20 53 9 8 5 1 18 44 27 28 14 17 74 47 14 54 51 29 77 76 0 40 44 26 0 69 7 0 57 52 43 29 13 86 70 26 91 75 97 92 84 98 80 100 95 94 100 99 99 95 93 91 87 63 42 26 5 98 78 94 72 21 30 96 100 59 92 94 40 64 98 0 35 96 48 7 93 3 3 100 100 67 57 19 75 52 2 72 43 82 89 72 99 67 98 90 94 94 95 94 86 91 41 60 41 5 7 1 98 43 0 72 14 28 91 87 6 87 49 17 74 92 0 46 99 75 0 98 1 10 41 99 45 10 18 84 69 4 84 60 84 88 75 98 67 98 92 87 95 85 88 91 0 64 5 16 11 3 95 75 0 14 14 24 99 2 13 54 0 40 74 69 0 1 99 78 34 82 7 0 37 100 63 41 23 27 + T r r t d 89 70 4 83 71 99 96 92 100 76 98 97 100 99 99 100 100 90 6 84 11 27 27 9 96 85 27 19 18 61 98 3 16 87 0 0 79 40 0 9 100 98 96 83 57 5 58 100 78 50 84 61 9 77 66 96 98 90 99 74 97 98 99 97 95 98 98 96 68 82 39 10 13 7 99 80 51 62 10 56 97 100 65 97 95 93 78 99 0 84 100 63 99 100 42 30 100 100 86 68 48 - 1 86 78 23 94 79 97 97 85 96 77 98 96 95 98 100 100 96 93 93 84 61 61 30 5 100 87 96 72 27 34 97 100 58 97 96 42 75 99 0 40 93 49 62 89 1 10 100 100 65 61 23 Table 5: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-14BInstruct-1M, GPT-4o). 5) Methods Qwen2.5-14B-Instruct-1M GPT-4o % r c k Maze Needle Haystack Number Filtering Number Format Number Sorting Palindrome Generation Palindrome Partitioning Polynomial Multiplication Pool Matrix Propositional Logic Quantum Lock Ransom Note Rectangle Count Rotate Matrix Rotten Oranges Rush Hour Self Reference Shortest Path Simple Geometry Simple Integration Sokoban Spiral Matrix String Manipulation Syllogism Tower Of Hanoi Tsumego Word Ladder Zebra Puzzles SymBench BoxNet V2 Eight Queens Game24 Gridworld Letters Math Counting And Probability Number Multiply Big Bench Hard Reasoning About Colored Objects Ruin Names Salient Translation Error Detection Snarks Sports Understanding Temporal Sequences Tracking Shuffled Objects Five Objects Reasoning Gym Binary Matrix Bitwise Arithmetic Caesar Cipher Calendar Arithmetic Chain Sum Circuit Logic C + T T + C e - F C G C - 1 C + T r r t d I - 1 6 96 25 57 40 53 1 53 4 44 27 78 1 1 14 0 6 23 0 51 0 64 23 62 1 1 29 23 3 17 36 5 53 55 67 93 52 54 73 52 88 81 51 59 28 53 56 41 5 98 35 93 95 39 1 45 42 55 23 86 20 29 1 0 12 6 49 61 1 8 12 78 7 2 4 8 97 0 89 50 92 0 8 25 53 62 99 1 4 15 0 8 44 0 59 0 91 89 79 5 1 3 26 Unseen Tasks 5 63 21 5 99 79 100 95 58 56 81 66 95 95 0 28 1 60 69 16 1 2 23 4 17 82 90 64 60 83 78 97 91 3 9 0 67 100 42 28 12 92 50 98 81 33 7 57 21 46 32 92 8 0 16 0 8 25 12 27 0 95 47 75 8 3 1 41 9 64 81 4 74 80 80 84 61 56 78 71 92 44 50 24 77 44 86 21 95 49 98 81 48 2 55 29 47 40 94 13 1 17 0 6 44 27 31 0 97 66 74 7 0 3 44 12 67 89 7 80 78 84 90 58 54 82 78 99 98 64 56 36 77 54 86 26 100 99 100 100 74 7 75 43 55 45 94 38 8 10 0 13 16 99 43 0 31 14 83 23 10 8 32 7 36 26 82 85 34 100 80 68 90 74 100 99 12 40 14 78 69 9 70 100 100 98 100 59 99 84 33 49 58 100 38 3 94 0 9 100 100 29 0 100 31 82 60 14 5 59 54 65 25 30 93 82 98 91 81 67 86 76 100 14 100 53 88 69 100 Table 5: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-14BInstruct-1M, GPT-4o). 5) Methods Qwen2.5-14B-Instruct-1M GPT-4o % r c k T Count Bits Decimal Arithmetic Figlet Font Game of Life Halting Intermediate Integration Knights Knaves Letter Jumble Modulo Grid Number Sequence Polynomial Equations Products Rearc Rubiks Cube Simple Equations Spell Backward Time Intervals Word Sequence Reversal + T T + C e - F C G C - 1 4 48 0 100 49 72 8 0 69 70 61 0 0 100 26 62 57 15 0 100 65 60 0 0 56 94 100 0 0 0 13 53 33 54 51 0 100 40 52 2 0 51 53 52 0 0 40 66 32 31 77 0 100 35 60 2 0 64 86 98 0 0 0 91 86 71 99 - 1 85 0 100 42 68 4 0 66 94 97 14 0 0 92 85 78 98 + T A t r I C 59 1 100 75 72 53 0 72 48 58 32 0 0 99 84 81 100 100 1 100 45 74 55 0 70 99 99 30 0 0 100 82 96 100 Table 6: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-7BInstruct-1M, Qwen2.5-3B-Instruct). Methods Qwen2.5-7B-Instruct-1M Qwen2.5-3B-Instruct % r c k Ave. Norm., Seen Ave. Norm., Unseen Ave. Norm., Total SymBench Blocksworld BoxLift Combinatorial Calculation Constrained Linear Arrange. Cryptanalysis Gsm Letter Logic Diagram Light Puzzles Logic Puzzle Logical Equation Mahjong Pattern Math Geometry Matrix Trans + T 31.7 32.2 31. C + C 34.5 41.5 36.3 t - F C 31.0 38.1 32.8 G C - 1 57.6 57.0 57. Seen Tasks 0 0 29 14 2 67 0 15 18 14 49 67 58 50 53 77 71 7 83 42 74 75 42 73 71 89 0 0 29 16 7 74 0 23 20 1 44 52 72 0 0 42 15 2 37 0 39 10 24 39 53 64 - 1 60.4 60.6 60.5 51 58 75 71 7 83 45 86 78 61 70 74 86 + T 19.6 18.3 19.3 + C A 18.6 21.8 19.5 t - F C 17.1 20.1 17.9 G C - 1 50.9 48.0 30.2 0 0 10 8 1 58 0 8 5 8 42 59 0 0 13 14 1 53 0 27 9 6 29 49 47 0 0 20 6 1 42 0 19 6 15 39 47 29 27 34 76 64 4 76 37 94 56 41 70 66 83 - 1 54.5 51.5 53.7 32 37 84 65 3 81 42 95 66 39 74 72 Table 6: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-7BInstruct-1M, Qwen2.5-3B-Instruct). 6) Methods Qwen2.5-7B-Instruct-1M Qwen2.5-3B-Instruct % r c k T + T A C + C e - F C G C - 1 New Operator Pattern Recog Permut. And Combina. Pooling Reversi Standard Sudoku Statistical Counting String Deletion And Modifi. String Insertion String Splitting String Synthesis Synthesis Decomp 2048 Big Bench Hard Boolean Expression Causal Judgement Date Understanding Disambiguation QA Dyck Languages Formal Fallacies Geometric Shapes Hyperbaton Logical Deduction 5 Objects Logical Deduction 7 Objects Logical Deduction 3 Objects Movie Recommendation Multistep Arithmetic Two Navigate Object Counting Penguins In Table Tracking Shuffled Objects 7 Obj. Tracking Shuffled Objects 3 Obj. Web Of Lies Word Sorting Reasoning Gym Ab Acre Advanced Geometry Aiw Arc 1D Arc Agi Base Conversion Basic Arithmetic Bf Binary Alternation Codeio Color Cube Rotation Complex Arithmetic Count Primes Countdown Course Schedule Dice 39 11 26 10 4 0 10 1 0 9 0 30 35 92 56 75 61 0 60 43 82 60 57 88 57 84 85 75 94 64 79 82 0 38 5 0 3 1 94 57 0 5 8 24 77 0 3 47 0 49 82 39 49 63 100 97 88 98 68 39 77 53 99 69 84 71 16 86 72 91 84 72 98 70 97 91 88 96 99 100 99 95 92 79 60 39 15 4 97 83 92 76 10 31 97 95 45 92 75 42 16 44 38 23 30 65 26 27 19 0 17 30 88 57 57 60 3 53 35 77 75 55 77 66 91 89 91 87 95 85 69 3 21 41 1 3 1 96 48 0 36 4 19 82 93 1 34 27 34 44 35 39 5 19 82 22 29 25 0 28 28 98 57 83 65 3 60 40 76 72 60 82 60 88 73 89 84 57 83 70 80 5 19 0 0 3 1 21 4 10 8 13 1 1 2 4 9 2 30 + T A C + C e - F C G C - 1 20 5 20 2 7 0 3 4 5 0 0 14 21 92 51 69 49 3 54 21 71 48 51 73 58 75 64 67 72 31 66 60 27 0 12 0 0 0 0 26 0 0 0 5 2 5 0 0 5 41 7 25 19 3 31 34 17 5 2 2 13 29 89 53 55 59 6 58 21 58 38 45 72 46 92 64 83 76 74 75 55 89 0 0 0 0 0 0 2 0 0 0 0 4 4 0 0 0 0 30 4 15 16 9 13 10 2 4 3 0 9 19 88 53 68 52 4 56 24 71 54 42 71 63 86 57 79 66 38 64 67 34 0 0 0 0 0 0 13 2 0 0 5 0 3 0 2 3 44 62 42 47 71 99 98 87 98 58 57 82 46 100 61 81 58 11 73 67 87 78 73 89 78 93 83 77 97 99 99 100 68 92 80 63 33 7 1 92 72 82 60 15 28 92 96 29 78 51 - 1 43 74 51 39 73 98 99 90 98 65 65 84 58 100 65 87 67 13 75 65 90 76 71 95 73 90 86 87 95 99 99 99 92 81 59 35 1 1 93 74 87 74 10 21 97 97 29 85 73 - 1 50 84 49 50 48 100 98 91 98 63 46 71 53 98 70 85 72 20 83 70 92 91 80 99 73 98 94 95 98 98 100 96 96 89 81 60 38 19 4 96 85 92 70 5 28 94 97 19 97 77 Table 6: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-7BInstruct-1M, Qwen2.5-3B-Instruct). 6) Methods Qwen2.5-7B-Instruct-1M Qwen2.5-3B-Instruct % r c k Emoji Mystery Family Relationships Fraction Simplification Futoshiki Game Of Life Gcd Graph Color Group Anagrams Isomorphic Strings Jugs Knight Swap Largest Island Lcm Leg Counting List Functions Manipulate Matrix Maze Needle Haystack Number Filtering Number Format Number Sorting Palindrome Generation Palindrome Partitioning Polynomial Multiplication Pool Matrix Propositional Logic Quantum Lock Ransom Note Rectangle Count Rotate Matrix Rotten Oranges Rush Hour Self Reference Shortest Path Simple Geometry Simple Integration Sokoban Spiral Matrix String Manipulation Syllogism Tower Of Hanoi Tsumego Word Ladder Zebra Puzzles SymBench BoxNet V2 Eight Queens Game24 Gridworld Letters Math Count. And Probab. C + T T + C e - F C G C - 1 - 1 C + T A C + C e - F C G C - 1 - 1 0 38 26 0 3 68 70 42 69 0 2 25 53 22 6 14 13 93 45 73 88 11 0 29 27 46 13 71 13 34 4 0 25 0 21 31 1 6 13 70 2 2 0 36 0 0 16 0 0 6 31 100 0 30 80 2 0 30 0 3 82 96 29 0 13 5 69 16 29 4 40 0 2 0 19 21 100 0 0 15 0 2 5 0 13 0 79 54 72 5 1 0 7 1 47 26 1 59 75 4 21 1 0 24 95 25 10 29 2 16 17 6 2 26 14 16 86 60 73 89 36 0 0 2 37 26 86 0 26 6 0 14 19 1 36 0 75 37 71 7 1 0 24 41 70 96 0 43 97 49 85 89 0 11 96 96 56 53 24 12 87 68 82 83 41 7 53 20 43 38 92 9 5 13 0 9 39 4 23 0 94 25 74 3 0 3 40 Unseen Tasks 3 63 74 3 99 3 25 35 0 91 51 31 20 53 83 0 22 94 64 89 87 0 4 92 99 7 38 19 21 88 54 84 82 37 13 42 23 34 24 96 3 4 3 0 13 29 12 19 0 50 47 80 4 0 0 39 3 61 80 4 90 73 0 2 0 0 2 88 43 27 7 0 0 12 1 3 5 2 7 84 10 57 7 3 0 3 8 30 2 66 2 22 1 0 3 0 1 22 0 10 6 63 0 0 0 27 0 1 3 0 1 0 0 0 0 2 32 3 0 0 0 0 11 0 0 0 4 0 32 0 4 2 1 0 0 0 5 1 85 0 1 1 0 1 0 0 3 0 52 13 39 0 1 0 5 0 13 10 2 49 66 1 1 0 0 3 10 15 3 3 0 2 0 1 0 3 2 0 39 0 4 3 2 0 0 0 32 5 20 0 2 6 0 1 21 0 5 0 39 1 63 2 1 0 11 0 4 16 2 23 53 13 26 87 0 37 96 21 12 76 0 8 69 99 31 22 10 0 79 53 57 56 10 1 32 8 32 4 96 2 6 4 0 7 12 1 27 0 51 36 68 2 0 2 29 1 55 73 4 72 13 48 90 0 42 96 22 43 80 0 5 73 97 41 27 11 3 85 67 69 71 16 4 39 13 31 17 90 2 5 6 0 12 13 15 23 0 64 41 69 2 0 2 29 2 58 72 3 80 70 Table 6: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (Qwen2.5-7BInstruct-1M, Qwen2.5-3B-Instruct). 6) Methods Qwen2.5-7B-Instruct-1M Qwen2.5-3B-Instruct % r c k Number Multiply Big Bench Hard Reasoning About Colored Obj. Ruin Names Salient Transla. Error Detect. Snarks Sports Understanding Temporal Sequences Tracking Shuffled Obj. 5 Obj. Reasoning Gym Binary Matrix Bitwise Arithmetic Caesar Cipher Calendar Arithmetic Chain Sum Circuit Logic Count Bits Decimal Arithmetic Figlet Font Game Of Life Halting Intermediate Integration Knights Knaves Letter Jumble Modulo Grid Number Sequence Polynomial Equations Products Rearc Rubiks Cube Simple Equations Spell Backward Time Intervals Word Sequence Reversal + T 14 87 49 44 79 71 81 73 2 3 0 33 73 51 0 28 0 100 32 50 2 1 56 65 45 0 0 8 13 32 C + C 100 80 43 45 59 71 49 95 15 80 1 39 80 19 100 58 0 100 15 27 0 0 42 74 96 0 0 2 11 23 1 t - F C G C - 1 93 86 56 47 73 64 87 75 18 29 15 8 6 5 2 14 0 100 0 39 1 0 44 74 56 0 0 0 78 65 68 68 83 49 48 77 56 80 98 26 23 44 74 97 48 99 80 0 100 28 46 2 0 61 88 81 0 0 78 95 64 - 1 80 85 58 52 77 66 84 97 38 45 40 77 95 51 99 86 0 100 42 64 0 0 63 90 95 0 0 84 93 72 97 + T T + C A u - F C G C - 1 18 72 26 43 66 60 45 36 0 1 0 5 2 0 1 3 0 100 3 29 0 0 33 16 32 0 0 5 0 14 1 57 39 49 65 55 27 74 14 1 0 0 0 0 0 0 0 100 1 14 0 0 0 0 70 0 0 0 0 5 0 80 67 44 37 69 64 38 44 1 0 0 2 7 0 0 3 0 100 2 30 0 0 4 24 2 0 0 0 18 4 6 75 39 53 69 49 50 96 21 23 4 52 88 37 74 59 0 100 25 43 0 0 48 77 90 0 0 73 61 55 81 - 1 60 83 42 49 72 50 62 95 22 25 12 65 92 43 85 77 0 100 25 43 0 0 54 80 84 0 0 71 78 62 Table 7: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (DeepSeek-7B, DeepSeek-14B). Methods DeepSeek-7B DeepSeek-14B % r c k T + T A C + C e - F C + T T + C Ave. Norm., Unseen 27. 28.7 53.1 40.1 43.4 Unseen Tasks Table 7: Experimental results on SymBench, Big-Bench-Hard, and Reasoning-Gym (DeepSeek-7B, DeepSeek-14B). 7) Methods DeepSeek-7B DeepSeek-14B % r c k T + T A C + C e - F C + T T + C SymBench BoxNet V2 Eight Queens Game24 Gridworld Letters Math Counting And Probability Number Multiply Big Bench Hard Reasoning About Colored Objects Ruin Names Salient Translation Error Detection Snarks Sports Understanding Temporal Sequences Tracking Shuffled Objects Five Objects Reasoning Gym Binary Matrix Bitwise Arithmetic Caesar Cipher Calendar Arithmetic Chain Sum Circuit Logic Count Bits Decimal Arithmetic Figlet Font Game of Life Halting Intermediate Integration Knights Knaves Letter Jumble Modulo Grid Number Sequence Polynomial Equations Products Rearc Rubiks Cube Simple Equations Spell Backward Time Intervals Word Sequence Reversal 0 0 46 5 64 80 78 37 48 64 45 72 95 0 8 1 44 16 0 0 2 0 100 1 25 0 0 1 48 9 0 0 1 29 0 85 0 0 51 5 82 76 88 87 29 47 60 51 74 95 0 5 0 21 59 0 0 10 0 100 5 6 0 0 6 16 32 0 0 24 1 8 22 0 56 84 5 75 82 89 39 40 51 38 58 100 2 21 29 78 99 38 99 65 0 100 33 55 0 0 69 80 88 0 0 92 74 66 94 0 2 50 8 91 72 39 94 65 63 83 67 100 96 2 1 0 44 59 0 0 4 1 100 33 53 3 0 46 36 37 0 0 51 45 40 98 0 4 52 14 96 84 96 63 58 69 70 99 98 0 7 0 45 83 0 15 18 0 100 20 51 7 0 42 59 67 0 0 72 26 43"
        }
    ],
    "affiliations": [
        "Harvard",
        "MIT",
        "MIT-IBM Watson AI Lab",
        "University of Illinois Urbana-Champaign",
        "University of Michigan",
        "University of Wisconsin-Madison"
    ]
}