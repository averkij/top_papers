{
    "paper_title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines",
    "authors": [
        "Do Xuan Long",
        "Duong Ngoc Yen",
        "Do Xuan Trong",
        "Luu Anh Tuan",
        "Kenji Kawaguchi",
        "Shafiq Joty",
        "Min-Yen Kan",
        "Nancy F. Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In-context learning (ICL) is an important yet not fully understood ability of pre-trained large language models (LLMs). It can greatly enhance task performance using a few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the task language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong open- and closed-source LLMs by over 5% in both zero- and few-shot settings. We show that LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers."
        },
        {
            "title": "Start",
            "content": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines Do Xuan Long1,3, Duong Ngoc Yen2, Do Xuan Trong1*, Luu Anh Tuan2, Kenji Kawaguchi1, Shafiq Joty4, Min-Yen Kan1, Nancy F. Chen3 1National University of Singapore, 2Nanyang Technological University, Singapore, 3Institute for Infocomm Research (I2R), A*STAR, 4Salesforce AI Research xuanlong.do@u.nus.edu 5 2 0 2 2 ] . [ 1 5 6 2 1 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In-context learning (ICL) is an important yet not fully understood ability of large language models (LLMs). It can greatly enhance task performance using few examples, termed demonstrations, without fine-tuning. Although effective in question answering, ICL often underperforms in long-form generation tasks such as summarization. Under appropriately realistic assumptions, we empirically and theoretically show that ICL demonstrations alone are insufficient to teach LLMs the tasks language and format distributions for generation. We argue for explicit exposure to the task distributions and hypothesize that defining them by prompting enhances model performance. To this end, we present LongGuide, which efficiently generates two parallel streams of guidelines capturing task language and format properties: (i) Metric Guidelines (MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output Constraint Guidelines (OCGs) that constrain generation at both token and sentence levels. LongGuide automatically selects the best combination of guidelines, improving both strong openand closed-source LLMs by over 5% in both zeroand few-shot settings. LongGuide is generalizable, learnable by weak models to enhance strong ones, and integrates synergistically with automatic prompt optimizers1."
        },
        {
            "title": "Introduction",
            "content": "In recent years, pre-trained large language models (LLMs) have demonstrated impressive instructionbased performance through zeroand few-shot learning capabilities (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2022; Touvron et al., 2023; Jiang et al., 2023; Team et al., 2023). Notably, few-shot learning, termed as in-context learning (ICL), has proven highly effective and widely used to calibrate LLMs for applications (Dong *Works done during the internship at WING, NUS. 1Our codes and data will be made available at here. Figure 1: ChatGPT results on SAMSum example (Gliwa et al., 2019). With LongGuide, the generated output aligns better with ground truth, and the quality is also improved by removing verbose details: The dialogue... (ZS), ...discuss their day (FS). See Appx.-Figure 14 for full texts. et al., 2022). Formally, let be the vocabulary of the LM. For test-time task , the goal is to generate token sequence , given input token sequence . Then, ICL generation using an LLM is the generation conditioned on with task demonstrations {(x1, y1), ..., (xk, yk)} concatenated into df = [x1, y1, ..., xk, yk] . The probability distribution induced from : is: (cid:89) PM(ydf , x) := Myt([x1, y1, . . . , (1) t=1 xk, yk, x, y<t]) where = [y1, . . . , yy] with yt V. Several prior studies attempt to explain the ICL capabilities of LLMs, advocating for the sufficiency of well-chosen df as implicitly teaching the to perform the tasks, especially classification ones (Saunshi et al., 2020; Xie et al., 2021; Wang et al., 2024). Central to their theoretical analyses is strong assumption that the language model fully captures the underlying distribution of the tasks language; i.e., PM(X) = PT (X) where PT is the task-specific data distribution. However, this assumption is often not met, especially with domainspecific terminologies (Cheng et al., 2024) (also see C.1 for case study), raising concerns about the actual sufficiency of ICL. Moreover, recent studies empirically show that ICL underperforms in longform generation tasks involving multi-sentence or -paragraph answers (Sun et al., 2023a; Huang et al., 2024), highlighting significant gaps in our understandings of the causes of these limitations and how to effectively instruct LLMs for these tasks. These challenges remain unsolved to date. In this work, we first study the proficiency of ICL for long-form generation tasks. We empirically and theoretically highlight that if language model fails to capture the tasks text properties (language and format), providing demonstrations alone with such properties cannot entirely resolve this (2). This is because the model does not consistently apply them to all generated responses. Maintaining such properties in responses is crucial for accurately solving the task. Therefore, we argue that providing explicit task guidelines that capture these text properties is essential for improving LLM performance. Figure 1 illustrates such an example where instructing LLMs explicitly by guidelines carrying certain properties (e.g., conciseness, #sentences) of the task output distribution improves both alignment with ground truth and generation quality. We then propose LongGuide (3), guidelinelearning algorithm that efficiently2 generates two types of guidelines concurrently from limited task training data as supplementary instructions to enhance LLMs: (i) Metric Guidelines (MGs) that steer models to optimize self-evaluation guided metrics, inspired by prior studies in machine translation (Ranzato et al., 2015) and LLM selfevaluation (Ren et al., 2023); and (ii) Output Constraint Guidelines (OCGs) that impose constraints on generated outputs at the sentence and token levels, drawing on controllable generation research (Fan et al., 2018a). LongGuide is related to previous studies in task instruction construction (Wang 2The prompts and cost analysis are discussed in G. et al., 2022b) and task understanding through task definitions (Yin et al., 2023). However, it differs by offering post-hoc instructions that guide LLMs to enhance responses based on learned quality and quantitative criteria. LongGuide automatically identifies optimal guidelines, significantly enhancing distribution alignment and generation quality across seven generation tasks and one real-life chat LLM benchmark, including summarization, text simplification, translation, dialogue generation, and tableto-text generation. Its guidelines can enhance ICL performance through demonstrations (5.2), improve non-instruct LLMs (D.1), boost stronger models when learned by weaker ones (D.2), and can be further optimized for usage using prompt optimization algorithms (D.3). Notably, LongGuide is approximately at least 3.75 times more cost-efficient than prompt optimization algorithms (G.3-Table 20) as it requires only four prompt variants to verify on the validation set while delivering superior performance. 2 ICL Alone Is Insufficient for Long-form Generation i)}n i=1 where xt long-form generation task with samples is defined as = {(xt and yt i, yt are input contexts and ground-truth sentenceor paragraph-long responses (Fan et al., 2019). For such tasks, preserving language and format properties of task-specific data during generation is essential for aligning outputs with ground truth. This is unlike classification, where outputs are predefined. We demonstrate that ICL fails to enable LLMs to maintain these properties during generation. Setups. We first select metrics as properties commonly used for dialogue summarization. We follow Fu et al. (2023) to choose six: (1) Semantic Coverage (COV); (2) Factuality (FAC); (3) Consistency (CON); (4) Informativeness (INF); (5) Coherence (COH); (6) Relevance (REL). We also measure (7) # tokens (NT) and (8) # sentences (NS) of ICL responses, as these format metrics can significantly impact model performance (Fan et al., 2018a). For each metric, we select the demonstrations having the same score and evaluate whether the ICL-generated responses maintain that score. Our experiments are performed on 100 random SAMSum samples (Gliwa et al., 2019) for each metric. We use ChatGPT (gpt-3.5-turbo-1106) (OpenAI, 2022) with Self-consistency (Wang et al., ICL w/ 5 demos Expected outcome Mistral-7B-v0.3 Llama-3.1-8B Qwen2.5-7B Mistral-7B-it-v0.2 Llama-3.1-8B-it (1) COV (2) FAC (3) CON (4) INF (5) COH (6) REL (7) NT (mean) 100% 100% 100% 100% 100% 100% 12% 12% 43% 38% 44% 27% 42% 90% 80% 86% 28% 50% 85% 78% 82% 8% 4% 40% 17% 26% 20% 32% 78% 75% 81% 35% 47% 96% 88% 87% 17.00 87.74 271.81 281.38 50.25 34.72 (7) NT (std) 0. 144.91 379.48 264.59 55.54 45.29 Table 1: % of responses scored 5 on the (1)-(6) metrics, and the (mean, std) of the (7) #tokens of the responses. Qwen scored high on metrics (1)(6) because it copies the input dialogue as the summarization outcome. 2022a) to evaluate metrics (1)(6) on scale of 15, as it is an effective evaluator (Wang et al., 2023a). NLTK (Bird and Loper, 2004) assesses metrics (7) (8). For ICL experiments, we examine five instruct and non-instruct models: Mistral-7B-v0.3 (Jiang et al., 2023), Llama-3.1-8B (Dubey et al., 2024), Qwen-2.5-7B (Qwen Team, 2024), ans Mistral7B-it-v.02 (Jiang et al., 2023) and Llama-3.1-8B-it (Dubey et al., 2024). For metrics (1)(6), we select demonstrations having perfect score of 5, and for metrics (7)(8) having 17 response tokens, spanning 2 sentences. For each metric, we further examine whether simple guideline: The output must maintain...{property}. (w/ guideline) can help instruct models (Mistral) maintain that property better during generation. Findings. We present the = 5 demonstration results in Table 1 and the case of Mistral-7B-itv.02 in Figure 7 with metrics (1)(7)3. We derive three surprising findings. Firstly, the ICL models do not achieve 100% score of 5 on any metric and instruct models generally outperform non-instruct models. The highest percentage of score 5 on average is on COH and REL, where ICL models already excel while for critical summarization metrics such as INF and COV, they achieve only up to 20% to 40%. Notably, although all demonstrations contain 17 output tokens, fewer than 5% answers achieve this property. Secondly, increasing # demonstrations does not rectify this issue; the same trends persist across 3, 5, and 10. Finally, by adding simple guideline shown in Appx.Figure 7, the percentages of answers maintaining the metrics are mostly improved, especially (7) and (8), verifying that adding guidelines is indeed helpful for instruct models to maintain these properties. Without ICL (and without instruction in our consideration), the model is entirely unable to solve the task. 3We also tested demonstration counts of = 3, 10, all follow similar trends, see Appendix C.3s Figure 8. Theoretical intuitions. Our theoretical intuitions explaining the above observations are provided in B. In summary, we prove that when an LLM does not fully capture the true task distribution (PM = PT ), demonstrations cannot recover the true task distribution in the limit, causing certain task-specific language and format properties may not be preserved in generation. We term this as the text property transfer (PT) problem. To address this, we define text property task as reformulation of the original task, where responses are mapped via property-specific metric. We hypothesize that the original task can be approximated by optimizing set of well-chosen text property tasks, allowing the model to better align with desired text characteristics and improve generation quality."
        },
        {
            "title": "Generation Algorithm",
            "content": "Motivations. As we have seen, providing textual guidelines instructing LLMs to optimize certain text property metrics can enhance them on responses, possibly because LLMs are optimizers (Yang et al., 2024). We propose LongGuide (Figure 2 and Algorithm 1), an algorithm that efficiently generates guidelines for LLMs to optimize self-evaluated text properties during generation. Specifically, Steps 13 focus on generating the Metric Guideline (MG) capturing the intrinsic language properties of the task via reference-free metrics. In parallel, Step 4 analyzes the answer format of the task and translates it to Output Constraint Guideline (OCG). The best combination of MG and OCG is selected for inference (Step 5). To ensure LongGuides generalizability to new tasks, we assume access to at most 50 training samples: Dtrain = {(xt i, yt i)}n i=1. Step 1: Metric Collection & Selection. To learn tasks language properties, this step reasons to select appropriate language evaluation metrics for Figure 2: Overview of LongGuide. Orange and blue boxes denote the learned metric guideline and output constraint guideline. self-evaluation. For this purpose, we first construct pool of evaluation metrics, S, applicable to any text generation task. consists of 27 distinct metrics from 4 main sources (Appx.-Table 13 for details). Specifically, we collect 3 metrics from ABCs of Communication (Wagner, 1963), 12 metrics from (Fu et al., 2023) for dialogue generation, summarization, data2text generation, and machine translation, and propose 12 more metrics for broader evaluation coverage. We do not collect LM-based metrics, such as FactScore (Min et al., 2023), because it is challenging for LLMs to define and self-evaluate them. Additionally, we do not gather definitions of collected metrics, as their interpretations may vary across different tasks. With Dtrain and S, we then perform iterations to select the metrics. At each iteration, we randomly sample batch of data from Dtrain and instruct to generate the top-5 most important metrics in for evaluating batch data properties via Chain-of-Thought prompting (Wei et al., 2022). We implement the top-5 constraint to avoid excessive metrics being selected. The final set of selected metrics, denoted by , consists of the metrics chosen across all iterations sorted in alphabetic order. Step 2: Metric Score Collection via Self-evaluation. This step focuses on evaluating the selected metrics from on Dtrain to capture the task properties. Motivated by prior studies (Wang et al., 2023a; Ren et al., 2023), we utilize to score the metrics on scale of 15. Specifically, for each train sample, scores its ground-truth answer on all Ms metrics via Self-Consistency (Wang et al., 2022a). The final metrics scores, denoted as scoresM , are the average of scores over all train samples. Note that we separate this step from Step 1s metric selection because we want to evaluate each chosen metric on Dtrain instead of the samples that led to select it. Step 3: Generating Metric Guideline (MG). This step aims to generate textual metric guideline (MG) that guides to align generation outputs with task-specific properties from scoresM . MG is formed by concatenating metrics definitions generated by and tailored by scoresM via the LLM instruction Based on these scores on scale of 5...define the expected quality of the output for each metric in natural language. We use these moderated definitions instead of raw scoresM because LLMs better capture contextual nuances through descriptions rather than numerical scores (Singh and Strouse, 2024). Figure 2 illustrates an instance where Informativeness in the task dialogue sum. achieving 4/5 score from Step 2 is defined as ...good amt. of informative.... Step 4: Output Constraint Guideline (OCG). Research on controllable generation has extensively proposed constraints including ones on the length, which are broadly applicable, as well as linguistic or keyword, which are more task-specific (Fan et al., 2018a; He et al., 2022). In this step, we aim to establish robust set of output constraints that apply universally to long-form generation tasks. We focus on six key constraints related to two distributions: the number of sentences and tokens in ground-truth answers. These constraints include minimum, maximum, and average counts, serving as basic statistics for length and expected values. The Output Constraint Guideline (OCG) instructs SAMSum CNN SWiPE Method Zero-shot (ZS) + OCG + MG + MG-OCG + LongGuide Few-shot (FS) + OCG + MG + MG-OCG + LongGuide R-L 22.20 27.55 27.81 28.35 28.35 27.13 27.84 27.50 30.65 30.65 B-1 20.05 28.64 28.81 28.79 28.79 27.21 29.91 30.15 31.72 31. BS Avg.JS R-L B-1 BS Avg.JS R-L B-1 BS Avg.JS 58.98 60.38 60.06 60.66 60.66 61.70 61.08 62.24 62.73 62.73 0.1014 0.0402 0.0388 0.0375 0.0375 0.0502 0.0336 0.0352 0.0318 0. 19.23 22.46 18.35 22.05 22.46 17.56 15.20 18.13 19.19 19.19 20.43 27.82 19.66 26.97 27.82 20.55 17.58 20.94 22.30 22.30 60.59 61.37 59.79 61.18 61.37 57.74 58.12 57.89 57.95 57. 0.1262 0.0718 0.1413 0.0789 0.0718 0.0844 0.0922 0.0830 0.0814 0.0814 36.60 32.48 38.21 35.47 38.21 39.47 29.54 41.36 38.56 41.36 39.01 32.88 40.83 36.95 40.83 39.76 30.32 41.22 37.87 41. 71.18 67.32 70.87 68.77 70.87 70.56 68.82 71.14 68.54 71.14 0.0565 0.0650 0.0550 0.0554 0.0550 0.0469 0.0596 0.0450 0.0529 0.0450 Table 2: Mistral performance verifying LongGuide mitigates the text property transfer (PT) problem (2): (1) the trends of ROUGE-L (R-L), BLEU-1 (B-1), BERTScore (BS), and JensenShannon divergence (Avg. JS) show strong correlations, supporting our hypothesis; (2) LongGuide substantially enhances Avg. JS scores, thereby mitigating the PT problem. to adhere to these statistics during generation. Step 5: MGOCG selection. Models have varying inherent knowledge per task, resulting in different improvements gained by using MG and OCG  (Table 4)  . This step determines the optimal MG and OCG combination by evaluating model performance on Dtrain across configurations. The bestperforming guideline configuration is then selected as the final output of LongGuide."
        },
        {
            "title": "4 Experiments",
            "content": "Benchmarks. We benchmark LongGuide on seven widely evaluated long-form generation tasks from four main categories, summarization, text simplification, machine translation and generation, and one real-life chat LLM benchmark. These tasks are SAMSum (Gliwa et al., 2019), CNN/- Daily Mail (3.0.0) (See et al., 2017) and XLSUM (Hasan et al., 2021) for summarization, SWiPE (Laban et al., 2023) for text simplification, IWSLT-2017 en-ja (Cettolo et al., 2017) for machine translation, Synthetic-Persona-Chat (Jandaghi et al., 2023) for dialogue generation, CommonGen-Challenge (Lin et al., 2020) for data-to-text generation, and (a subset of) AlpacaEval2 (Dubois et al., 2024). We also benchmark the reasoning tasks in E.3. See for details. Baselines and evaluations. Since LongGuide is the first method to self-learn guidelines as additional instructions for generation, we compare it with the zero-/few-shot prompting baselines in this section, and many-shot prompting in E.1. We also evaluate it against three of the strongest prompt optimization algorithms to date: APO (Pryzant et al., 2023) in this section, and EvolPrompt (Guo et al., 2024) and adv-ICL (Do et al., 2024) in D.3, both of which optimize the input prompt on the Dtrain. We also compare LongGuide with General Guidelines in 5.2 where we ask the models to reason over demonstrations to generate task guidelines. For models, we empirically examine both strong openand closed-source LLMs: Mistral-7B-it v0.2 (Jiang et al., 2023) as an open-source model and ChatGPT (gpt-3.5-turbo1106) (OpenAI, 2022) as closed-source model. For evaluations, we use ROUGE-L (Lin, 2004) (recall-based) following Bai et al. (2024) (also for LongGuides Step 5), and GPT-4o-Judge (OpenAI, 2024) as our main evaluation metrics. For GPT-4o-Judge, we evaluate how aligned the generated answer is with the reference answer and its quality on five criteria: (i) Format consistency; (ii) Content completeness; (iii) Factuality; (iv) Style adherence; (v) Generation quality on scale of 1 10, following Zheng et al. (2023) (see G.1 for full prompt). We also report BLEU-1 (Papineni et al., 2002) (precision-based), BERTScore (Zhang et al., 2020) (meaning-based), and Human evaluation verifying the metric optimization and generation quality in 5.1. The results are averaged over three runs, with 95% CI of the t-test. 4.1 Findings LongGuide enhances PT which correlates with improved performance. LongGuide effectively addresses the PT problem identified in 2. Our experiments are conducted on 3 benchmarks SAMSum, CNN, and SWiPE with Mistral under the zero-shot and few-shot settings. For each task, we first obtain the set of selected text properties from LongGuide that the model needs to optimize. We then measure the average of Jensen-Shannon divergence (Lin, 1991) between their score distributions (judged by ChatGPT) between the generated answers and the ground truth answers, across all Sum. Simplification Translation Dialogue Gen. Table2Text Method SAMSum CNN (3.0.0) XL-Sum SWiPE IWSLT17 en-ja Syn. Persona Comm.-Chall. #shots (ran.) 3 3 5 3 3 5 ) 2 . 0 ( - t T a Zero-shot + APO + LongGuide % gain (+) Few-shot + APO + LongGuide % gain (+) Zero-shot + APO + LongGuide % gain (+) Few-shot + APO + LongGuide % gain (+) 22.20 / 7.43 23.77 / 7.31 28.35 / 7.73 6.15 / 0.30 27.13 / 7.66 26.23 / 7.44 30.65 / 7.72 3.52 / 0.06 23.83 / 7.43 25.05 / 7.45 30.47 / 7.59 6.64 / 0. 22.21 / 7.32 24.22 / 7.28 31.46 / 7.72 9.25 / 0.40 19.23 / 7.38 19.53 / 7.40 22.46 / 7.45 3.23 / 0.07 17.56 / 5.84 18.18 / 5.89 19.19 / 5.99 1.63 / 0.15 20.12 / 7.44 20.34 / 7.39 22.19 / 7.67 2.07 / 0.23 14.51 / 4.38 15.20 / 4.01 18.17 / 4.42 3.66 / 0.04 9.19 / 5.96 12.06 / 5.85 14.38 / 6.29 5.19 / 0. 9.79 / 4.46 11.99 / 4.55 15.23 / 5.06 5.44 / 0.40 10.80 / 5.96 12.19 / 6.07 20.93 / 6.36 10.13 / 0.40 11.42 / 5.95 14.07 / 6.19 19.95 / 6.36 8.53 / 0.41 36.60 / 7.21 36.92 / 7.21 38.21 / 7.32 1.61 / 0.11 39.47 / 7.12 39.55 / 7.11 41.36 / 7.24 1.89 / 0.12 45.09 / 7.28 46.32 / 7.51 45.09 / 7.28 0.00 / 0. 33.72 / 5.07 34.46 / 5.13 37.60 / 5.25 3.88 / 0.18 13.12 / 2.82 14.45 / 2.91 16.53 / 3.45 3.41 / 0.63 12.69 / 2.66 14.08 / 2.92 16.62 / 3.40 3.66 / 0.74 36.13 / 7.62 37.74 / 7.44 41.22 / 8.11 5.09 / 0.49 31.93 / 7.25 33.72 / 7.31 38.43 / 7.91 6.50 / 0.66 12.76 / 2.68 10.66 / 2.41 14.69 / 4.45 1.93 / 1. 3.56 / 1.00 4.26 / 1.05 5.25 / 3.93 1.69 / 2.93 19.46 / 6.04 19.91 / 6.12 22.98 / 6.41 3.52 / 0.37 16.10 / 4.67 17.68 / 4.55 22.36 / 5.26 6.53 / 0.59 10.12 / 5.14 11.21 / 4.68 25.20 / 6.81 15.08 / 1.67 3.98 / 1.34 5.45 / 2.05 25.05 / 6.65 21.07 / 5.31 24.21 / 6.53 23.63 / 6.53 34.41 / 7.23 10.20 / 0. 22.08 / 4.19 25.09 / 6.12 38.21 / 7.21 16.13 / 3.02 Table 3: ROUGE-L / GPT-4o-Judge results on seven long-form generation tasks. LongGuide remarkably outperforms baselines on most tasks and substantially enhances LLMs. BLUE-1 scores are reported in Appx.-10. selected properties, denoted as Avg.JS. The results are presented in Table 2. LongGuide significantly lowers the Avg.JS scores compared to the baselines, demonstrating the effectiveness of guidelines for enhancing property transfer. Furthermore, our findings corroborate our hypothesis: across benchmarks, Avg.JS exhibits moderate to strong positive correlations with the performance metrics (ROUGE-L, BLEU-1, BERTScore) measured by the Pearson correlation coefficient (Pearson, 1895) (Appx.-Figure 9). In E.7, we present the density plots for all metrics measured on the results with and without LongGuide. Table 3 details our main experiment results on downstream tasks. Firstly, for baselines, zero-shot performance is interestingly higher than the fewshot for both models, and the gaps are huge for Synthetic Persona and CommonGen-Challenge. We hypothesize that models were partly exposed to task data during training, causing few-shot demonstrations to push the prompts out of distribution, leading to frequent refusals to answer. Meanwhile, LongGuide helps models overcome this issue. Secondly, LongGuide substantially improves zeroand few-shot baselines by 6% on ROUGE-L and 0.8 on GPT-4o-Judge on average: improvement for fewshot prompting is surprisingly higher than in zeroshot, possibly because improving stronger baseline is harder than weaker one. Notably, LongGuide outperforms APO in most benchmarks, especially under the zero-shot setting, demonstrating that our strategy of optimizing text property tasks is markedly more effective than APO optimizing only Figure 3: GPT-4-Judge scores over criteria. ROUGE-L on limited data. Thirdly, we observe that LongGuide achieves the highest improvements on CommonGen-Challenge with 15.62% and lowest on SWiPE with 1.85% on ROUGE-L. These improvements are mainly because the answers generated by the baselines are often far longer than the ground truth. LongGuide rectifies this issue by controlling the output length and quality, leading to substantial performance gains. Fourthly, among the two models, LongGuide interestingly improves Mistral by 5.39%, while ChatGPT, regarded as stronger, improves by larger margin, 6.58%. This suggests that LongGuide has the potential to benefit stronger models in the future. Among five GPT4o-Judge criteria in Figure 3, LongGuide notably improves Format, Style, and Factuality, confirming its effectiveness in aligning model generation with ground-truth distributions. Finally, the significant gains in Quality, together with the ROUGE-L scores from Table 3 further verify that LongGuide also strongly enhances the generation quality. Where do the enhancements come from? To identify the primary source of performance gains, we present the results of LLMs with LongGuides components in Table 4. Firstly, MG-OCG combination (w/ MG-OCG) is the most useful for LLMs, observed to be the best 15 times, followed by OCG Method SAMSum CNN (3.0.0) XL-Sum SWiPE IWSLT17 en-ja Synthetic Persona CommGen-Chall. ) 2 . 0 ( - 7 - t M ) 6 0 1 1 ( t Zero-shot (ZS) + OCG + MG + MG-OCG MG-OCG sel. Few-shot (FS) + OCG + MG + MG-OCG MG-OCG Sel. Zero-shot (ZS) + OCG + MG + MG-OCG MG-OCG Sel. Few-shot (FS) + OCG + MG + MG-OCG MG-OCG Sel. 22.200.43 27.550.98 27.811.17 28.351.66 MG-OCG 27.130.26 27.840.88 27.502.08 30.650.88 MG-OCG 23.830.54 29.190.77 25.380.79 30.471.57 MG-OCG 22.212.35 30.001.07 29.430.83 31.461.34 MG-OCG 19.230.34 22.460.64 18.350.60 22.050.84 OCG 17.560.63 15.205.28 18.135.28 19.190.49 MG-OCG 20.120.27 22.390.82 20.370.41 22.190.65 MG-OCG 14.510.80 18.171.32 15.452.16 14.842.58 OCG 9.190.03 14.380.15 9.370.25 13.640.38 OCG 9.790.18 12.221.19 11.802.06 15.230.33 MG-OCG 10.800.18 20.930.52 10.421.15 20.020.89 OCG 11.420.13 19.951.38 12.490.59 18.580.44 OCG 36.600.59 32.481.91 38.211.72 35.472.89 MG 39.470.45 29.541.90 41.361.37 38.561.39 MG 45.091.45 37.761.44 45.062.96 41.384.91 ZS 33.722.61 16.681.29 19.361.40 37.602.85 MG-OCG 13.121.39 16.530.59 8.710.53 15.761.85 OCG 12.691.82 16.620.81 8.670.62 15.830.95 OCG 36.130.87 38.861.11 37.882.42 41.220.46 MG-OCG 31.931.88 38.571.81 39.453.55 38.432.37 MG-OCG 12.761.54 14.350.47 12.530.58 14.691.08 MG-OCG 3.560.36 5.061.05 4.320.39 5.250.94 MG-OCG 19.460.40 22.982.65 19.910.59 20.951.91 MG-OCG 16.102.61 22.360.89 18.640.49 19.471.20 OCG 10.120.02 24.160.11 21.547.50 25.201.89 MG-OCG 3.980.17 25.050.76 14.582.24 5.941.00 OCG 24.210.37 34.411.01 17.232.57 31.570.99 OCG 22.080.63 38.121.99 22.187.50 38.213.70 MG-OCG Table 4: ROUGE-L results with 95% CI from t-test. The gains of LongGuides components vary across different models and tasks. The MG-OCG selection results are reported in Appx.-Table 14. (w/ OCG) with 10, and MG (w/ MG) twice. While these statistics underscore the effectiveness of MGOCG, OCG particularly proves itself highly effective in tasks such as summarization, translation, and table-to-text generation. Secondly, individual MG or OCG strengthens the prompting baselines, with OCG showing slight edge. This is because while MG focuses on the language properties of answers, it does not directly control the output format, sometimes causing longer/shorter answers than the ground truths. Exceptionally, on SWiPE, OCG harms all models, whereas MG shows notably strong effectiveness with Mistral. Manual investigations reveal that ground-truth answers in SWiPE exhibit high variances in #sentences and #tokens which explains why OCG may not be effective for this benchmark. Thirdly, an interesting case is ChatGPT with few-shot prompting on SWiPE, where individual MG and OCG impair performance but their combination enhances it. This shows evidence that MG and OCG complement each other. As discussed above, due to the uneven nature of answers in SWiPE, using MG or OCG alone may not work well for multiple samples, as MG and OCG only provide expected statistics. However, combining them could enhance performance by allowing them to complement each other. such complement SWiPE example is outlined in Appx.-Figure 16."
        },
        {
            "title": "5 Discussion",
            "content": "We address several key questions about the usefulness, applicability, and generalizability of LongGuide. Additional properties are provided in and more method and attention analyses in E. 5.1 Human Evaluation: Does LongGuide Enhance Generation Quality? Figure 4: Win/Draw/Loss rates of w/ versus w/o LongGuide. We perform human evaluation to quantify whether LongGuide helps LLMs optimize the selected metrics and enhance generation quality, as no automatic methods can address this need to date. For this purpose, we randomly select 50 zero-shot generated samples from the SAMSum and Syn. Persona (since MG-OCG is the best for these datasets, Table 4). Three excellent Englishnative undergrads are hired to rate whether ZS + LongGuide improves ZS on each of the selected MG and OCG metrics. Due to limited resources, we evaluate 5 random MG metrics. As shown in Figure 4, we notice that ZS + LongGuide outperforms ZS on 27.8% MG metrics on average, draws on 64.2%, and loses on only 8%. Specifically, among the MG metrics, Brevity shows the highest winning rate of 73% while Relevance obtains the lowest winning rate of 12%, Methods SAMSum SWiPE CommGen-Chall. Zero-shot (ZS) + LongGuide + LongGuide w/o step 2 Few-shot (FS) + LongGuide + LongGuide w/o step 22.20 / 7.43 28.35 / 7.73 26.99 / 7.49 27.13 / 7.66 30.65 / 7.72 30.37 / 7.70 36.60 / 7.21 38.21 / 7.32 36.90 / 7.22 39.47 / 7.12 41.36 / 7.24 35.54 / 6.28 10.12 / 5.14 25.20 / 6.81 25.03 / 6.66 3.98 / 1.34 25.05 / 6.65 24.15 / 5. Table 5: Mistral ROUGE-L / GPT-4o-Judge main ablation study with LongGuide when Step 2 is skipped. Methods LC Win Rate Win Rate Zero-shot (ZS) + OCG + MG + MG-OCG + LongGuide Few-shot (FS) + OCG + MG + MG-OCG + LongGuide 11.08% 4.73% 19.13% 8.42% 19.13% 8.08% 7.73% 12.65% 12.63% 12.65% 3.17% 2.44% 7.07% 3.90% 7.07% 2.68% 3.45% 4.88% 4.88% 4.88% Table 6: AlpacaEval2 experiments. ting Step 1 transforms it into OCG, whereas excluding Step 3 yields MG, and skipping Step 4 becomes MG-OCG. Here, we investigate LongGuide effectiveness when skipping Step 2, Metrics scores collection: for selected metrics from Step 1, we directly task the models to optimize them for the generated answers. We experiment with Mistral on SAMSum, SWiPE, and CommGen-Chall. datasets because for these datasets, the best guideline combination includes MG. The results in Table 5 verify that without Step 2, the model performs worse, particularly for SAMSum and SWiPE in the zero-shot setting. We attribute these drops to the incorrect task properties captured and metric conflicts. case study is provided in Appx.-Figure 18. 5.4 LongGuide on Real-Life Chat LLM Benchmark We evaluate the effectiveness of LongGuide in aligning LLMs with desired real-world chats. Our experiments are conducted on subset of 203 random samples from the AlpacaEval2 benchmark (Dubois et al., 2024) with ChatGPT (1106). Since AlpacaEval2 lacks training data, we select 5 random samples from the public Alpaca-GPT4 instruction-tuning dataset (alpaca-gpt4), despite it being relatively out-of-distribution (OOD) compared to AlpacaEval2. Table 6 presents our findings. Few-shot demonFigure 5: LongGuide learned from demonstrations substantially enhances Mistral performance (ROUGE-L). possibly because ZS models can already generate highly relevant outcomes. Meanwhile, on the OCG metrics, LongGuide achieves superior win of 95% on average. Finally, regarding the generation quality, our annotators prefer LongGuide output by up to 92%. These indicate that LongGuide not only aligns the outputs with the ground truths but also enhances the generation quality. The fine-grained scores of MG metrics are provided in E.10, and annotator agreement (Krippendorffs alpha (Krippendorff, 2022)) is α=68.9%. 5.2 LongGuide Learns From Demonstrations To Boost ICL Performance Here, we revisit the question posed in 2 and verify that LongGuide learned from demonstrations substantially increases ICL performance. Our experiments using Mistral cover CNN, IWSLT17 en-ja, and CommGen-Chall datasets. Our experiments involve averaging the performance under zeroand few-shot settings. For Baseline, no guideline is utilized. For LongGuide on Demos, we train LongGuide on demonstrations used in Table 3, in contrast to the Dtrain for the case of LongGuide. We add one more baseline, General Guidelines (Gen. Gui.) on Demos, where we ask the models to generate general task guidelines from demonstrations. The results are summarized in Figure 5, with details in Appx.-Table 15. Specifically, LongGuide trained on Dtrain outperforms it on demonstrations, suggesting its possible scalability with more training data. Moreover, while Gen. Gui. slightly worsens the Baseline on CNN, both LongGuide and LongGuide on Demos notably surpass the Baseline, and Gen. Gui., highlighting the effectiveness of LongGuide in capturing task-specific properties, thereby boosting ICL performance. 5.3 Ablation Studies of LongGuides Steps From Table 4, we identify the unique contributions of each step within LongGuide. Specifically, omitstrations and OCG negatively impact performance, likely due to the OOD nature of Alpaca-GPT4 compared to AlpacaEval2. In contrast, with just 5 Alpaca-GPT4 samples, MG metrics, and LongGuide enhance performance by capturing certain response properties from GPT-4 (OpenAI, 2023), nearly doubling the zero-shot points."
        },
        {
            "title": "6 Related Work",
            "content": "Automatic prompt design for long-form generation. Long-form generation tasks are essential and have been studied extensively (Li et al., 2024). With LLM advancements, adapting these models for such tasks using prompt-based methods is critical yet challenging. Previous studies (Bang et al., 2023; Yang et al., 2023a; Hadi et al., 2023; Zhou et al., 2023b; Pan et al., 2024) highlight the limited efficacy of LLMs in producing outputs that resemble ground truths, as evaluated by ROUGEL (Lin, 2004). Our approach autonomously composes supplementary contexts, integrating text evaluation metrics and format constraints. In addition, studies regarding enhancing instructions for LLMs (Wang et al., 2022b; Yin et al., 2023; Long et al., 2024; Wang et al., 2023b), automatic prompt optimization (Zhou et al., 2023a; Pryzant et al., 2023), and demonstration selection (Yang et al., 2023b; Qin et al., 2023) are also related areas that can be developed in parallel and combined with our work (D.3). Controllable generation with LLMs. Controllable generation during fine-tuning has been studied extensively (Fan et al., 2018a; Lakew et al., 2019; Martin et al., 2020; He et al., 2022). More recently, researchers have explored prompting methods to control LLM generation. For instance, Sun et al. (2023b) found that LLMs struggle to meet fine-grained hard constraints, while Fonseca and Cohen (2024) proposed controlling stylistic features like keywords and narrative during generation, leading to improved LLM summarization outcomes. Although (Lu et al., 2023; Fonseca and Cohen, 2024) are closely related to our OCG, our approach goes beyond summarization and openended only features, as discussed in 3. We focus on universally applicable features."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we demonstrate that in-context learning (ICL) falls short in implicitly ensuring that large language models (LLMs) consistently preserve essential language and format properties in long-form generation tasks. To address this challenge, we introduce LongGuide, an efficient algorithm that automatically learns the critical language and format properties from task-specific data, converting them into textual guidelines for LLMs. Our results show that LongGuide significantly improves LLM performance across seven generation tasks and is highly generalizable, offering strong potential for various downstream applications with minimal data. This work paves the way for adaptive, task-specific prompt generation, advancing LLM adaptation."
        },
        {
            "title": "Generalizability and Customization of\nLongGuide",
            "content": "LongGuide facilitates flexible generalization that allows customization and extension of guidelines MG and OCG for specific tasks, which we strongly recommend. For instance, in summarization, MG can focus on only 4-5 standard metrics from while integrating summary-specific metrics like Summary Structure and Retention of Core Supporting Evidence. Simultaneously, OCG can impose stricter constraints on topics, keywords, grammar, or tones (Fan et al., 2018a; Lakew et al., 2019; Martin et al., 2020). Although LongGuide is primarily presented for general long-form generation, we strongly advise for these customizations to enhance its effectiveness."
        },
        {
            "title": "Limitations",
            "content": "limitations. Firstly, Our study has several our theoretical analysis focuses solely on the task language distribution which is PM(X) or PM(XDf ) instead of the actual output distribution, which is arg maxyY PM(Y = X) or arg maxyY PM(Y = Df , X). In our study, while leveraging the task language distribution allows us to hypothesize and highlight the limitations of demonstrations, shifting focus to the actual output distribution could yield more insights. Secondly, LongGuides learned guidelines are based on task-level and average statistics rather than sample-based details. We designed our framework at the task level to address limited data constraints, as we found that sample-based learning under these conditions leads to high errors. While task-level guidelines already demonstrate significant improvements for LLMs, sample-based guidelines could offer more tailored guidance, potentially leading to optimal results. Moreover, this average guidance approach may be ineffective for tasks with high variance in the statistics that LongGuide learns. In such cases, the final step of LongGuide can prevent performance decline by likely choosing no guideline. For example, we found this applies to Code2Text (Richardson et al., 2017) & StoryGeneration (Fan et al., 2018b). Thirdly, LongGuide relies on models having certain level of task knowledge to perform selfevaluation effectively, and LongGuide necessitates LLMs with strong instruction-following capabilities. However, we anticipate that cutting-edge AI language models will overcome this limitation both now and in the near future. Lastly, the guidelines learned by LongGuide may not be useful for the tasks the models are trained on. This is because these guidelines might introduce out-of-distribution context relative to the training data, thereby reducing the effectiveness of the testing inference. For instance, while we see notable enhancements on the CommonGen-Challenge dataset (Lin et al., 2020), its intriguing that we dont observe any improvements on the WebNLG (Gardent et al., 2017) and E2E NLG (Puzikov and Gurevych, 2018) datasets, despite their expected similarity. Given the popularity of these datasets, we suspect the models we tested may have been previously trained on them."
        },
        {
            "title": "Ethical Considerations",
            "content": "This method could be misused to optimize prompts for harmful purposes such as generating misinformation, hate speech, or privacy violations. While our method is not intended for such uses, it is impossible to completely prevent misuse. Although our method could enhance the efficiency and efficacy of bad actors, we do not anticipate that LongGuide is inherently more effective in these negative contexts than in positive applications. Finally, we employ annotators at an hourly rate of $20, which exceeds the local minimum wage requirements."
        },
        {
            "title": "Acknowledgement",
            "content": "This research is supported by the National Research Foundation Singapore under the AI Singapore Programme (AISG Award No: AISG2-GC-2022-005, AISG Award No: AISG2-TC2023-010-SGIL) and the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207). DXL is supported by the A*STAR Computing and Information Science (ACIS) scholarship. We thank members of WING and Deep Learning Lab at NUS and the ACL RR anonymous reviewers for the constructive feedback."
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand. Association for Computational Linguistics. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the AsiaPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675718, Nusa Dua, Bali. Association for Computational Linguistics. Steven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214217, Barcelona, Spain. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 214, Tokyo, Japan. International Workshop on Spoken Language Translation. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2024. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1240:113. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Shieh, and Junxian He. 2024. Prompt optimization via adversarial in-context learning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73087327, Bangkok, Thailand. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. survey on in-context learning. arXiv preprint arXiv:2301.00234. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Cantón Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Ken-591 neth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline C. Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollár, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Yann Dubois, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple debiasing of automatic evaluators. In First Conference on Language Modeling. Angela Fan, David Grangier, and Michael Auli. 2018a. Controllable abstractive summarization. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 4554, Melbourne, Australia. Association for Computational Linguistics. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 35583567, Florence, Italy. Association for Computational Linguistics. Angela Fan, Mike Lewis, and Yann Dauphin. 2018b. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889898, Melbourne, Australia. Association for Computational Linguistics. Marcio Fonseca and Shay Cohen. 2024. Can large language model summarizers adapt to diverse arXiv preprint scientific communication goals? arXiv:2401.10415. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166. Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124133, Santiago de Compostela, Spain. Association for Computational Linguistics. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: humanannotated dialogue dataset for abstractive summaIn Proceedings of the 2nd Workshop on rization. New Frontiers in Summarization, pages 7079, Hong Kong, China. Association for Computational Linguistics. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. 2024. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In The Twelfth International Conference on Learning Representations. Surafel Melaku Lakew, Mattia Di Gangi, and Marcello Federico. 2019. Controlling the output length of neural machine translation. In Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong. Association for Computational Linguistics. Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. 2023. survey on large language models: Applications, challenges, limitations, and practical usage. Authorea Preprints. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 46934703, Online. Association for Computational Linguistics. Junxian He, Wojciech Kryscinski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2022. CTRLsum: Towards generic controllable text summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 58795915, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, and Bhuwan Dhingra. 2024. Calibrating long-form generations from large language models. arXiv preprint arXiv:2402.06544. Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. 2023. Faithful persona-based conversational dataset generation with large language models. arXiv preprint arXiv:2312.10007. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Klaus Krippendorff. 2022. The reliability of generating data. Chapman and Hall/CRC. Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023. SWiPE: dataset for document-level simplification of Wikipedia pages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10674 10695, Toronto, Canada. Association for Computational Linguistics. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. Pre-trained language models for text generation: survey. ACM Computing Surveys, 56(9):139. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 18231840, Online. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Jianhua Lin. 1991. Divergence measures based on the shannon entropy. IEEE Transactions on Information theory, 37(1):145151. Do Xuan Long, Duong Ngoc Yen, Anh Tuan Luu, Kenji Kawaguchi, Min-Yen Kan, and Nancy F. Chen. 2024. Multi-expert prompting improves reliability, safety and usefulness of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2037020401, Miami, Florida, USA. Association for Computational Linguistics. Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. 2023. Bounding the capabilities of large language models in open text generation In Findings of the Assowith prompt constraints. ciation for Computational Linguistics: EACL 2023, pages 19822008, Dubrovnik, Croatia. Association for Computational Linguistics. Louis Martin, Éric de la Clergerie, Benoît Sagot, and Antoine Bordes. 2020. Controllable sentence simplification. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4689 4698, Marseille, France. European Language Resources Association. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. OpenAI. 2022. Introducing chatgpt. OpenAI. 2023. Gpt-4 is openais most advanced system, producing safer and more useful responses. OpenAI. 2024. Hello gpt-4o. Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2024. Lost in translation: study of bugs introduced by large language models while translating code. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, Online. Association for Computational Linguistics. Karl Pearson. 1895. Vii. note on regression and inheritance in the case of two parents. proceedings of the royal society of London, 58(347-352):240242. Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with gradient descent and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79577968, Singapore. Association for Computational Linguistics. Yevgeniy Puzikov and Iryna Gurevych. 2018. E2E NLG challenge: Neural models vs. templates. In Proceedings of the 11th International Conference on Natural Language Generation, pages 463471, Tilburg University, The Netherlands. Association for Computational Linguistics. Chengwei Qin, Aston Zhang, Anirudh Dagar, and In-context learning with itarXiv preprint Wenming Ye. 2023. erative demonstration selection. arXiv:2310.09881. Qwen Team. 2024. Qwen2.5: party of foundation models. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732. Jie Ren, Yao Zhao, Tu Vu, Peter Liu, and Balaji Lakshminarayanan. 2023. Self-evaluation improves selective generation in large language models. In Proceedings on, pages 4964. PMLR. Kyle Richardson, Sina Zarrieß, and Jonas Kuhn. 2017. The Code2Text challenge: Text generation in source libraries. In Proceedings of the 10th International Conference on Natural Language Generation, pages 115119, Santiago de Compostela, Spain. Association for Computational Linguistics. Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. 2020. mathematical exploration of why language models help solve downstream tasks. arXiv preprint arXiv:2010.03648. Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073 1083, Vancouver, Canada. Association for Computational Linguistics. Aaditya Singh and DJ Strouse. 2024. Tokenization counts: the impact of tokenization on arithmetic in frontier llms. arXiv preprint arXiv:2402.14903. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Wieting, Nanyun Peng, and Xuezhe Ma. 2023a. Evaluating large language models on controlled generation tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31553168, Singapore. Association for Computational Linguistics. Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu, Qian Hu, Rahul Gupta, John Frederick Wieting, Nanyun Peng, and Xuezhe Ma. 2023b. Evaluating large language models on controlled generation tasks. arXiv preprint arXiv:2310.14542. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971. Sara Wagner. 1963. The abcs of communication. American Association of Industrial Nurses Journal, 11(8):811. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is ChatGPT good NLG evaluator? preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 111, Singapore. Association for Computational Linguistics. The Twelfth International Conference on Learning Representations. Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei Cheng. 2023a. Exploring the limits of chatgpt for query or aspect-based text summarization. arXiv preprint arXiv:2302.08081. Zhao Yang, Yuanzhe Zhang, Dianbo Sui, Cao Liu, Jun Zhao, and Kang Liu. 2023b. Representative demonstration selection for in-context learning with twostage determinantal point process. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54435456, Singapore. Association for Computational Linguistics. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023. Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 30633079, Toronto, Canada. Association for Computational Linguistics. Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems. Tianyi Zhang, Varsha Kishore, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023a. Large language models are human-level In The Eleventh International prompt engineers. Conference on Learning Representations. Yongxin Zhou, Fabien Ringeval, and François Portet. 2023b. Can gpt models follow human summarization guidelines? evaluating chatgpt and gpt-4 for dialogue summarization. arXiv preprint arXiv:2310.16810. Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Ruifeng Xu, and Kam-Fai Wong. 2023b. Self-critique prompting with large language models for inductive instructions. arXiv preprint arXiv:2305.13733. Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2024. Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning. Advances in Neural Information Processing Systems, 36. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022b. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP In Proceedings of the 2022 Conference on tasks. Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online. Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc Le, Denny Zhou, and Xinyun Chen. In 2024. Large language models as optimizers. Algorithm 1 LongGuide Input: and its generation func. GM, train data Dtrain = {(xt i=1, linguistic processor L. Input: Task instruction I, instruction to select metrics IM , score metrics Iscore, generate MG IM G. i)}n i, yt Step 1: Metric Collection & Selection 1: Collect the set of widely-used evaluation metrics 2: = [] 3: for training iterations do 4: Sample batch from Dtrain Ssub := GM([IM , B, S]) 5: 6: = Ssub 7: end for 8: = M.sort() the set of selected metrics top-5 metrics selected from for best evaluating Step 2: Metric Score Collection via Self-evaluation 9: sM1 = = sMm = 0 10: for i, (x, y) in enumerate(Dtrain) do 11: } := GM([Iscore, x, y, ]) M1 , ..., si {si Mm Update sMj = sMj + (si 12: 13: end for 14: scoreM = [sM1, , sMm] sMj )/(i + 1) for all in range(m) Mj the self-evaluated average scores of selected metrics self-evaluation Step 3: Generating Metric Guideline 15: {dM1, ..., dMm} := GM([IM G, scoresM , ]) 16: MG = joined with newline ({dM1, ..., dMm}) Step 4: Output Constraint Guideline generate metrics definitions w.r.t scores 17: Using to compute (mins, maxs, avgs) of #sentences in yt 18: OCG = The response must have from {mins} to {maxs} sentences and from {mint} to {maxt} i, and (mint, maxt, avgt) of #tokens words with an average of {avgt} words and {avgs} sentences. Step 5: MGOCG selection 19: = {w/o guideline, G, OCG, & OCG} 20: LongGuide = arg maxgG(perf ormance(MI, g, Dtrain)) Output: LongGuide automatic guideline selection"
        },
        {
            "title": "A LongGuide Algorithm",
            "content": "Algorithm 1 outlines the LongGuide algorithm, which generates task-specific guidelines to enhance LLM performance. It selects relevant evaluation metrics, computes self-evaluated scores, formulates metric-based and structural constraints, and automatically chooses the most effective guideline. For detailed step descriptions, see 3."
        },
        {
            "title": "B Theoretical Intuitions",
            "content": "We now present theoretical analysis to explain the observed phenomena. i, yt i)}n i=1, where xt Task formulation. long-form generation train dataset for task with samples is defined as = {(xt are the input context and ground-truth sentenceor paragraph-long responses. In the instruction-based setting, an LLM is expected to generate given and an input instruction I. Recall that PM and PT are the probability functions of Ms generation and the task data distribution, then: and yt Remark B.1. Under mild assumptions and PM = PT , there exists such that PM(X = xDf ) = PT (X = x). The proof is provided in B.1. In essence, Remark B.1 asserts that when fails to capture the true task data distribution, df cannot recover the desired alignment in the limit. As result, certain task language and format properties, even when well-presented in demonstrations, may not be implicitly captured and preserved during LLM generation. We term this unsolved issue the text property transfer (PT) challenge: ensuring that captures and preserves specific desired text properties observed in limited set of labeled data to responses. We hypothesize that explicitly guiding the model with essential text property criteria mitigates the mismatch identified in Remark B.1 between the induced and task-specific language distributions, leading to improved performance. = {D, L}, text property T1 of task is defined as the task T1 Definition B.1. For the task {D1, L1}, where D1 = {(xt i))}n i, f1(yt (feature map) f1 : R, with and L1 being the objectives of and T1. = i=1 is the train data of T1 mapped from by text property metric Suppose that the objective is minimizeθΘL(θ, ) with θ being tunable factor of M, which can be its parameters or input instruction, and Θ being its space. We propose the following hypothesis verified in 4.1: Hypothesis B.1. can be approximated by well-chosen text property tasks T1, ..., Tr (T T1 ...Tr) with corresponding objectives L1, .., Lr such that by jointly optimizing them during the generation process, we can approximately optimize objective, i.e., arg minθΘ arg minθΘ (cid:80)r i=1 Li. B.1 Proof of Remark B.1 Assumption B.1. There exists for which PM(X = x) = PT (X = x). This assumption is intuitive and realistic, recognizing that LLMs cannot fully capture the vast and nuanced complexity of real-world language beyond their training data. It contradicts the common assumption PM(X) = PT (X) made by prior studies (Xie et al., 2021; Wang et al., 2024). simple empirical evidence is provided in C.1. We also assume: Assumption B.2. Two probability functions are functionally zero equivalent if they act on the same input space and any arbitrary event causes both functions to be zero or non-zero. We assume that PT and PM are functionally zero equivalent, i.e., PM(X = x) = 0 PT (X = x) = 0 . Note that Assumption B.2 is relaxed version of the common assumption PM(X) = PT (X), and does not conflict with Assumption B.1. Proof of Remark B.1. We prove this theorem by contradiction. Suppose the negation of Remark B.1 is true, i.e., there exists D1 such that , PM(XD1) = PT (X) (S1). Now, let us consider the event Dc 1 where Dc 1 is the conjugate of event D1, or Dc PM(X Dc since PM and PT are functionally zero equivalent, we have PM(X Dc 1D1) = 0. From the assumption (S1), we derive PT (X Dc 1) = 0. 1 = DD1. We have 1) = 0. From the Assumption B.2, Dc Dc 1 and Dc 1 form disjoint union of Dc 1) = 0 + 0 = 0. Since D1 and Dc Similarly, we can consider the event Dc where is the conjugate of X, we arrive at PM(X 1) = 0. Since the two Dc 1, we derive PM(Dc 1) = PM(X 1) + PM(X Dc 1 form disjoint union of D, we have PM(D1) = 1. From the negation statement (S1), we have PM(XD1) = PT (X) . Since D1 and Dc 1) = PM(X D1) + 0 = PM(X D1). We also have PM(XD1) = PM(XD1) from Bayess theorem, meaning that PM(D1) PM(XD1) = PM(X D1) = PM(X) (since PM(D1) = 1). Meanwhile, from the negation statement (S1), we have PM(XD1) = PT (X), thus PM(X) = PT (X) for all , which contradicts to our Assumption B.1. Therefore, our negation statement (S1) is false, leading to Remark B.1 is true. 1 form disjoint union of X, we have PM(X) = PM(X D1) + PM(X Dc Empirical Case Studies Supporting 2 C.1 Empirical Case Study for Assumption B.1 Here, we provide empirical evidence for Assumption B.1. It posits that for given long-form generation test task , the language model does not entirely encapsulate true language distribution. This assumption aligns with real-world scenarios since LLMs are not infinitely powerful: there exist task instances where they are out-of-distribution (OOD) for M. For example, let us consider Remark B.1 as the task and be ChatGPT (OpenAI, 2022). The true language distribution is the specific vocabulary, technical jargon, and nuanced writing style typically found in mathematics. The full dialogue can be found at https://chatgpt.com/share/ 0189761f-202b-46e6-bab0-39a80bd5340e. Input: Given two probability functions P1 and P2. Suppose that there exists at least such that P1(X) = P2(X). Prove that for any D, there exists at least such that P1(X D) = P2(X ). ChatGPT output (30 Jul 24): To prove the statement, we need to understand the conditions under which the equality of the probability functions P1 and P2 given condition breaks down. Lets start by stating the given conditions more clearly: 1. We have two probability functions P1 and P2. 2. There exists at least one event such that P1(X) = P2(X). We need to show that for any event D, there exists at least one event such that P1(X D) = P2(X D). Lets break it down into steps. . . Based on the above example, we can see that ChatGPT does not focus on proving P1(X D) = P2(X ). Instead, it misunderstood as proving P1(X D) = P2(X D), showing that the ChatGPTs language distribution when solving the task is different from the true task language distribution. C.2 Empirical Case Study for Remark B.1 Empirical evidence supporting Remark B.1 is not possible to establish since it is an extremal result concerning existence and non-existence. Essentially, Remark B.1 says that if at the beginning, the two distributions of the task and language model are not the same (first not the same) but functionally zero equivalent, then for any demonstrations, the two distributions of the task and language model conditioned on those demonstrations are not the same (second not the same). Its important to note that the data point causing the first not the same\" can differ from the data point causing the second not the sam, and this second not the same data point needs to be examined by all possible demonstrations. This makes it difficult to empirically verify the theorem since the demonstration space is vast. LongGuides Extra Preliminary Properties D.1 LongGuide can Improve Non-instruct Models Using guidelines learned by LongGuide, we add more instructions to models. Therefore, we aim to examine whether non-instruct models can benefit from these guidelines. Our final conclusion is yes, LongGuide has strong potential to enhance non-instruct models. Setups. Since non-instruct models inmight struggle to follow our structions to generate the guidelines 7, we utilize the guidelines learned by an instruct model instead. We run our experiments with Mistral7B-v0.14(Jiang et al., 2023) using the guidelines learned by Mistral-7BInstruct-v0.2. Methods CNN (3.0.0) IWSLT17 CommGen-Chall. Zero-shot (ZS) + OCG + MG + MG-OCG + LongGuide Few-shot (FS) + OCG + MG + MG-OCG + LongGuide 7.600.58 6.600.74 9.041.02 8.380.91 9.041.02 3.140.32 2.240.21 3.240.26 2.990.29 2.240.21 2.990.83 3.700.29 5.390.93 4.590.97 5.390.93 3.440.83 3.860.61 6.650.97 7.880.91 7.880.91 10.960.36 10.120.56 8.550.74 7.990.70 10.960.36 4.670.33 8.110.63 10.710.80 9.390.89 10.710.80 Findings. The results are provided in Table 7. We observe that LongGuide improves more than half of the experiments, showing its potential effectiveness in enhancing even non-instruct models, especially for the translation task. Table 7: ROUGE-L performance of Mistral-7B-v0.1 using LongGuide learned by Mistral-7B-Instruct-v0.2. We observe that LongGuide improves more than half of the experiments, showing its potential effectiveness in enhancing even non-instruct models, especially for the translation task. 4https://huggingface.co/mistralai/Mistral-7B-v0.1 D.2 LongGuide can be Transferable from Weaker to Stronger Models We find that the guidelines learned by LongGuide are transferable from weaker to stronger models. weaker model can learn the guidelines at low cost, which can then be used to enhance the performance of stronger models. This is particularly advantageous because powerful models are often closed-source and expensive to query, whereas open-source models are weaker but free to use. Methods CNN (3.0.0) IWSLT17 en-ja CommGen-Chall. ChatGPT Zero-shot (ZS) 20.120.27 ChatGPT ZS w/ Mistrals MG 21.410.62 36.130.87 39.662.47 ChatGPT Few-shot (FS) 14.510.80 ChatGPT FS w/ Mistrals MG 13.9611.50 31.931.88 32.3413.79 Mistral Zero-shot (ZS) Mistral w/ ChatGPTs MG 19.230.34 19.670.71 17.560.63 Mistral Few-shot (FS) Mistral FS w/ ChatGPTs MG 19.007.82 13.121.39 7.981.49 12.691.82 11.862.79 24.210.37 29.9523.66 22.080.63 33.3413.56 10.120.02 6.291.06 3.890.17 3.610.38 Table 8: LongGuide can be transferable from weaker to stronger models, evaluated by ROUGE-L. Setups. We demonstrate this through experiments on CNN (3.0.0), IWSLT17 en-ja, and CommGenChall, representing all the tasks. We used the MG generated by Mistral for experiments on ChatGPT and vice versa under both zero-shot and few-shot settings. Findings. Table 8 outlines the results. We observe that Mistrals MG generally improves ChatGPT performance, but not vice versa. Explaining these phenomena, firstly, the OCG is transferable across models because it is independent of any specific model. Secondly, the MG, while it helps models capture task distributions, an MG learned from stronger model may not benefit weaker model, as the weaker model may misinterpret it. In contrast, the stronger model, with better text comprehension, can generalize task distributions from MG even when MG is poor and/or not well expressive generated by the weaker model. D.3 LongGuide can be Compared and Combined with Automatic Prompt Optimization Algorithms The MG and OCG learned by LongGuide may not be fully optimized for LLMs. Hence, its intuitive to suggest that LLMs could achieve even greater performance by adopting optimal guidelines. In this section, we illustrate that the guidelines learned by LongGuide can be further refined through discrete prompt optimization algorithms. This capability is advantageous for LongGuide, enabling its concurrent development and integration with automatic prompt optimization algorithms. Setup. We employ two strong prompt optimizers, APO (Pryzant et al., 2023) and adv-ICL (Do et al., 2024), in our experiments. We also compare LongGuide with EvolPrompt (Guo et al., 2024) in this section. Here is our methodology: we integrated the guidelines generated by LongGuide into the prompt, including the input instruction and demonstrations. Subsequently, we applied the prompt optimizers to refine the input instruction, demonstrations, and guidelines. Our experiments were conducted using Mistral on datasets including CNN, IWSLT 2017 en-ja, and CommonGen-Challenge. Following our findings in Table 4, the guideline being optimized for CNN and IWSLT 2017 en-ja is OCG, while for CommonGen-Challenge is MG-OCG. Findings. Our results are detailed in Table 9. In summary, when further optimizing the OCG using APO and advICL for CNN and IWSLT 2017, we observed slight improvement. This could be attributed to the OCG already being concise and straightforward, making it easier for models to grasp. However, for the CommonGenChallenge dataset, which utilizes the Methods Zero-shot (ZS) + APO + EvolPrompt + adv-ICL + LongGuide CNN (3.0.0) IWSLT17 CommGen-Chall. 19.230.34 19.532.08 20.163.44 18.872.69 22.460.64 13.121.39 14.451.84 15.042.12 15.011.72 16.530.59 10.120.02 11.212.02 14.063.02 13.122.21 25.201.89 27.011.01 26.183.47 + LongGuide + APO + LongGuide + adv-ICL 22.761.04 21.973.21 17.131.05 16.902.15 Table 9: Guidelines learned by LongGuide are further optimized by discrete prompt optimization frameworks bringing even better performance, with Mistral, evaluated by ROUGE-L. Sum. Simplification Translation Dialogue Gen. Table2Text Method SAMSum CNN (3.0.0) XL-Sum SWiPE IWSLT17 en-ja Syn. Persona Comm.-Chall. Avg. #shots (ran.) 3 5 3 3 5 ) 2 . 0 ( - t T a Zero-shot (ZS) + APO + LongGuide % gain (+) Few-shot (FS) + APO + LongGuide % gain (+) Zero-shot (ZS) + APO + LongGuide % gain (+) Few-shot (FS) + APO + LongGuide % gain (+) 22.20 / 20.05 23.77 / 22.02 28.35 / 28.79 6.15 / 8.74 27.13 / 27.21 26.23 / 25.88 30.65 / 31.72 3.52 / 4.51 23.83 / 20.23 25.05 / 22.90 30.47 / 28.37 6.64 / 8.14 22.21 / 25.37 24.22 / 22.77 31.46 / 30.04 9.25 / 4. 19.23 / 20.43 19.53 / 21.46 22.46 / 27.82 3.23 / 7.39 17.56 / 20.55 18.18 / 21.32 19.19 / 22.30 1.63 / 1.75 20.12 / 24.11 20.34 / 21.88 22.19 / 30.79 2.07 / 6.68 14.51 / 17.52 15.20 / 17.04 18.17 / 18.52 3.66 / 1.00 9.19 / 8.82 12.06 / 11.50 14.38 / 14.13 5.19 / 5.31 9.79 / 8.32 11.99 / 11.71 15.23 / 14.02 5.44 / 5. 10.80 / 11.46 12.19 / 12.52 20.93 / 22.61 10.13 / 11.15 11.42 / 10.83 14.07 / 15.69 19.95 / 22.49 8.53 / 11.66 36.60 / 39.01 36.92 / 39.41 38.21 / 40.83 1.61 / 1.82 39.47 / 39.76 39.55 / 39.56 41.36 / 41.22 1.89 / 1.46 45.09 / 43.28 46.32 / 44.89 45.09 / 43.28 0.00 / 0.00 33.72 / 32.69 34.46 / 33.18 37.60 / 35.66 3.88 / 2. 13.12 / 13.72 14.45 / 15.49 16.53 / 18.81 3.41 / 5.09 12.69 / 13.78 14.08 / 14.70 16.62 / 17.92 3.66 / 4.14 36.13 / 38.32 37.74 / 39.01 41.22 / 43.79 5.09 / 5.47 31.93 / 32.68 33.72 / 35.50 38.43 / 42.84 6.50 / 10.16 12.76 / 11.79 10.66 / 10.05 14.69 / 12.86 1.93 / 1.07 3.56 / 2.67 4.26 / 2.91 5.25 / 4.46 1.69 / 1. 19.46 / 19.75 19.91 / 19.80 22.98 / 23.79 3.52 / 4.04 16.10 / 18.10 17.68 / 17.77 22.36 / 20.31 6.53 / 2.21 10.12 / 6.19 11.21 / 7.12 25.20 / 24.03 15.08 / 17.84 3.98 / 1.94 5.45 / 3.76 25.05 / 21.90 21.07 / 19.96 24.21 / 24.04 23.63 / 24.18 34.41 / 36.84 10.20 / 12.80 22.08 / 23.52 25.09 / 24.70 38.21 / 37.64 16.13 / 14. 17.38 18.26 23.37 5.99 16.32 17.12 21.92 5.61 25.77 26.45 31.91 6.13 22.34 23.65 29.55 7.21 Table 10: Supplemetary ROUGE-L / BLEU-1 results on seven long-form generation tasks showing that the trends of ROUGE-L and BLEU-1 scores are nearly identical. MG-OCG guideline with more detail, APO and adv-ICL have greater amount of material to optimize within the prompts. This led to substantial improvement in performance compared to the other datasets."
        },
        {
            "title": "E Supplementary Results and Discussions",
            "content": "E.1 Additional Baselines: Using more Shots for ICL #shot SWiPE CNN (3.0.0) 3-5 shots + LongGuide We supplement the results for CNN (3.0.0), SWiPE, and Comm.-Chall. in Table 11 where we use 10 shots for CNN, 50 shots for SWiPE, and Comm.-Chall up to the window size limit of gpt-3.5-turbo-1106 evaluated by ROUGE-L / GPT-4o-Judge scores. We observe that while supplementing more shots to ChatGPT improves models performance, LongGuide further boosts the ICL performance significantly for all three benchmarks. Table 11: Performance comparison of models with and without LongGuide across different datasets and shot settings. 10-50 shots + LongGuide 28.18 / 4.85 42.55 / 7.72 22.08 / 4.19 38.21 / 7.21 20.55 / 6.67 21.69 / 6.82 44.04 / 6.07 46.17 / 6. 14.51 / 4.38 18.17 / 4.42 33.72 / 5.07 37.60 / 5.25 Comm.-Chall. E.2 How Does LLM Handle LongGuide, and Context Given LongGuide? To analyze LongGuides impact on LLMs, we perform simple attention analysis to investigate (1) how LLMs attend to LongGuide and (2) utilize the input context when conditioning on LongGuide. Specifically, for (1), we calculate the average attention scores across all heads and layers for each guideline token. For (2), we evaluate the entropy of the attention scores overall context tokens. We experiment with Mistral on 100 SAMSum random samples. We learn two key findings. Firstly, Mistral shows substantial attention to the guidelines. By using MG, 37.81% of attention is on guideline tokens. For OCG, it is 22.56%, and MG-OCG, 37.87%. Notably, the average attention on OCG tokens is higher than on context, while MG and MG-OCG receive fair amount, confirming mode attention on guidelines (Appx.-Table 16). Figure 6: Entropy of attention over the input context across 32 Mistral layers. Secondly, from Figure 6, Mistral exhibits more selective context attention when conditioned on guidelines. The largest entropy gap occurs in the first layer, where with guidelines, the model sparsely processes the context but, without them, is biased towards focusing narrowly on specific context parts. In Figure 7: Property maintenance experiments with ICL. See Appx.-Figure 19 for full example. Figure 8: Property maintenance experiments with ICL full results. IT is the adding simple guideline baseline. the final layer, the model distributes attention more evenly with guidelines than without. Generally, MG stabilizes context use across layers, while OCG shows greater variance, likely because it does not directly control generation quality, therefore, the model bias almost exists as origin, as we can see the trends of using OCG and no guidelines are relatively similar. These findings indicate that guidelines potentially improve context utilization and mitigate token bias. E.3 LongGuide on Reasoning Tasks We conduct experiments comparing LongGuide to various baselines on reasoning tasks. We select Mistral as our LLM, and GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021) as benchmarks for evaluation. For each benchmark, we randomly sampled 200 instances from the test set for assessment and 50 instances from the train set to train the prompt optimizers and LongGuide. Methods GSM8k SVAMP Zero-shot (ZS) + APO + adv-ICL + LongGuide 39.66 41.83 42.66 40. 60.33 62.33 62.83 63.33 Few-shot (FS) + APO + adv-ICL + LongGuide 32.33 34.33 35.00 34.83 61.66 63.00 62.66 62.83 The results are averaged over three runs, and outlined in Table 12. LongGuide slightly outperforms the Zeroshot and Few-shot baselines but falls short compared to prompt optimizers. Nonetheless, the findings confirm that additional instructions for LLMs can potentially improve the init model, leading to further enhanced reasoning performance with prompt optimization. Table 12: Performance of LongGuide with Mistral on reasoning tasks. Figure 9: Pairwise Pearson correlation coefficient of metrics. Source Metrics The ABCs of Communication (Wagner, 1963) Accuracy, Brevity, Clarity BARTScore (Yuan et al., 2021) Relevance, Coherence GPTScore (Fu et al., 2023) We propose Total Semantic Coverage, Factuality, Fluency, Informativeness, Consistency, Engagement, Specificity, Correctness, Understandability, Diversity Completeness, Conciseness, Neutrality, Naturalness, Readability, Creativity, Rationalness, Truthfulness, Respect of Chronology, Non-repetitiveness, Indicativeness, Resolution Table 13: Metrics collected for LongGuides metric guideline (MG). # 3 2 10 27 E.4 Supplementary Results for 2 E.5 Understanding MG and OCG: How Do They Work (Together)? Metric Guideline (MG) (Step 1-3). To understand how models select and evaluate metrics, we analyze the specific metrics chosen for each task, their selection frequencies, and their average scores (Appx.- Table 17 and figs. 12 and 13 respectively). Overall, each of the 27 metrics is selected and evaluated in at least one task. Among them, common linguistic metrics such as Clarity are frequently selected, while task-specific metrics like Creativity are less frequently chosen. By examining the scores of selected metrics, we find that common linguistic metrics generally achieve high scores, as anticipated. However, task-specific metrics like Creativity exhibit varying scores across tasks, indicating their differing importance and relevance. Additionally, we also find that within MG can conflict with each other, such as Conciseness and Informativeness (see Appx.-Figure 17 for an example). This underscores the importance of LongGuides Step 2 in weighting the metrics. Output Constraint Guideline (OCG) (Step 4). We find that both the token and sentence constraints are crucial for LLMs (Appx.-E.13), with the sentence being more beneficial. We hypothesize that LLMs have better control over the number of sentences than tokens, as counting sentences is intuitively simpler than tokens. This can be observed in our experiment in 2. MG and OCG are complementary and non-interchangeable. MG and OCG complement each other rather than conflict, as partially discussed in 4.1. This is because MG language metrics primarily concern the characteristics of responses rather than their structural aspects such as sentence and token count, which is the main focus of the OCG. In addition, the MG and OCG are not interchangeable. One might question whether adopting conciseness and brevity metrics could sufficiently alter the OCG, or if the OCG could effectively encompass the MG guideline. Our answer is no. While MG can steer LLMs towards brevity in responses, it lacks precise quantification for conciseness. Modern LLMs, often trained to generate verbose responses, may struggle to meet human conciseness without explicit statistics. Meanwhile, the OCG supplies them in the form of bins and means, yet these statistics alone do not directly address linguistic qualities. We provide examples as evidence supporting our arguments in Appx.-Figures 15 and 16. E.6 Collected Metrics in LongGuides Step 1 (3) Table 13 presents our 27 metrics collected for LongGuides Step 1. E.7 JS Divergence over all LongGuide Metrics with SAMSum (4.1) Figure 10: Density plots of MG and OCG metrics selected by Mistral under the few-shot (FS) setting, measured on ground-truth, FS, and FS w/ LongGuide answers. For JensenShannon divergence, lower is better. Figure 10 presents density plots of MG and OCG metrics selected by Mistral under the few-shot (FS) setting, measured on ground-truth, FS, and FS w/ LongGuide answers. For JensenShannon divergence, the lower is better. E.8 Step 5 CD-MG Selection Results of LongGuide (4.1) Method SAMSum CNN (3.0.0) XL-Sum SWiPE IWSLT17 en-ja Synthetic Persona CommGen-Chall. Summarization Simplification Translation Dialogue Generation Table2Text #shots (random) 3 - 7 - t M t Zero-shot (ZS) + OCG + MG + MG-OCG MG-OCG Sel. Few-shot (FS) + OCG + MG + MG-OCG MG-OCG Sel. Zero-shot (ZS) + OCG + MG + MG-OCG MG-OCG Sel. Few-shot (FS) + OCG + MG + MG-OCG MG-OCG Sel. 21.25 27.43 27.68 28.34 MG-OCG 25.55 27.31 27.88 30.01 MG-OCG 24.21 28.81 25.12 29.79 MG-OCG 27.44 29.98 28.89 30.65 MG-OCG 3 18.96 21.92 18.02 21.63 OCG 17.30 16.45 18.47 19.87 MG-OCG 19.54 21.88 20.02 21.99 MG-OCG 13.77 17.55 14.03 13.12 OCG 5 8.88 14.22 10.26 13.90 OCG 9.85 12.47 12.01 14.89 MG-OCG 10.78 20.66 10.42 19.91 OCG 12.11 19.26 12.75 18.64 OCG 3 36.21 31.19 36.74 35.12 MG 39.29 29.85 41.07 39.40 MG 45.11 37.58 45.09 42.72 ZS 33.30 16.22 19.14 37.24 MG-OCG 5 14.05 16.93 11.06 15.49 OCG 13.52 17.58 14.09 17.02 OCG 36.22 38.45 37.72 41.50 MG-OCG 28.76 35.73 36.09 36.22 MG-OCG 5 12.93 12.99 13.74 14.14 MG-OCG 6.19 6.45 6.47 8.06 MG-OCG 19.68 23.09 19.81 20.82 MG-OCG 17.12 21.50 19.12 18.99 OCG 9.12 20.67 19.98 20.87 MG-OCG 4.01 20.50 11.16 5.18 OCG 24.23 35.04 18.50 30.09 OCG 24.12 36.51 21.99 38.33 MG-OCG Table 14: MG-OCG selection results on Dtrain set for the main experiments in Table 3, evaluated by ROUGE-L. The numerical MG-OCG selection results on Dtrain are presented in Table 14, as also noted in Table 4. Overall, the performance of LongGuide on Dtrain closely mirrors its performance on the testing tasks in Table 4. The only discrepancy is for the IWSLT17 en-ja task with ChatGPT using few-shot prompting: the optimal guideline combination on Dtrain is MG-OCG (see Table 14), whereas the best on the testing set is MG (see Table 4). E.9 LongGuide can Generalize from Demonstrations (5.2) Table 15 presents the numerical results of Figure 5 in 5.2. Even with only 3-5 exemplars as demonstrations, LongGuide effectively derives MG and OCG guidelines, benefiting the model. In this case, Dtrain is the set of demonstrations, and the rest of LongGuides steps remain unchanged. Methods CNN (3.0.0) IWSLT17 en-ja CommGen-Chall. Zero-shot (ZS) + OCG trained on Dtrain + MG trained on Dtrain + MG-OCG trained on Dtrain + LongGuide trained on Dtrain + OCG trained on Demos + MG trained on Demos + MG-OCG trained on Demos + LongGuide trained on Demos Few-shot (FS) + OCG trained on Dtrain + MG trained on Dtrain + MG-OCG trained on Dtrain + LongGuide trained on Dtrain + OCG trained on Demos + MG trained on Demos + MG-OCG trained on Demos + LongGuide trained on Demos 19.230.34 22.460.64 18.350.60 22.050.84 22.460.64 20.460.10 18.330.25 19.160.37 20.460.10 17.560.63 19.171.27 17.182.01 21.181.07 21.181.07 16.881.44 15.590.59 19.890.39 19.890.39 13.121.39 16.530.59 8.710.53 15.761.85 16.530.59 17.271.83 8.631.08 14.003.42 14.002.42 12.691.82 19.862.93 12.820.15 18.700.73 19.862.93 19.401.39 12.072.68 17.783.23 17.7818.43 10.120.02 24.160.11 21.547.50 25.201.89 25.201.89 23.970.47 18.980.52 24.462.43 24.462. 3.980.17 25.050.76 21.795.20 25.435.28 25.050.76 28.280.69 23.994.66 27.410.87 23.994.66 Table 15: LongGuide learns the guidelines from only demonstrations with Mistral, evaluated by ROUGE-L. Figure 11: Fine-grained human evaluation results on evaluated MG metrics. Per token All MG 0.0019 37.81% Context (MG) 0.0064 62.19% OCG 0.0133 22.56% Context (OCG) MG-OCG Context (MG-OCG) 0.0077 77.44% 0.0017 37.87% 0.0064 62.13% Table 16: Attention score over guideline and context tokens of Mistral. E.10 Human Evaluation Fine-grained Results (5.1) Figure 11 presents our fine-grained human evaluation results. Overall, LongGuide shows the best in terms of Accuracy and Clarity, with significant number of winning ratings. This suggests that the generated text is factually correct and easy to understand. Meanwhile, LongGuide shows more mixed results in terms of Clarity and Coherence. While there is still high winning rating, the proportion of draw and loss ratings is also relatively high, possibly because improving Brevity can somehow reduce the Clarity. E.11 Attention Analysis for Guideline Tokens (E.2) Table 16 shows our simple attention analysis. Task SAMSum CNN Model Mistral ChatGPT Mistral ChatGPT XLSum Mistral ChatGPT SWiPE Mistral ChatGPT IWSLT17 en-ja Mistral ChatGPT Synthetic Persona Mistral ChatGPT Selected Metrics [Accuracy, Brevity, Clarity, Relevance, Understandability] [Accuracy, Brevity, Clarity, Relevance, Understandability] [Accuracy, Brevity, Clarity, Coherence, Completeness, Engagement, Readability, Relevance, Truthfulness, Understandability] [Accuracy, Brevity, Clarity, Coherence, Completeness, Conciseness, Engagement, Neutrality, Readability, Relevance, Specificity] [Accuracy, Brevity, Clarity, Coherence, Completeness, Consistency, Correctness, Diversity, Engagement, Factuality, Fluency, Indicative, Informativeness, Neutrality, Non-repetitiveness, Relevance, Resolution, Respect of Chronology, Semantic Coverage, Specificity, Understandability] [Accuracy, Brevity, Clarity, Coherence, Completeness, Consistency, Correctness, Diversity, Engagement, Factuality, Fluency, Indicative, Informativeness, Neutrality, Non-repetitiveness, Rationalness, Relevance, Resolution, Respect of Chronology, Semantic Coverage, Specificity, Understandability] [Accuracy, Brevity, Clarity, Relevance, Understandability] [Accuracy, Brevity, Clarity, Coherence, Conciseness, Consistency, Correctness, Readability, Understandability] [Accuracy, Clarity, Coherence, Consistency, Correctness, Factuality, Fluency, Relevance, Understandability] [Accuracy, Clarity, Coherence, Consistency, Correctness, Factuality, Fluency, Relevance, Understandability] [Accuracy, Brevity, Clarity, Coherence, Completeness, Consistency, Correctness, Diversity, Engagement, Factuality, Fluency, Indicative, Informativeness, Neutrality, Non-repetitiveness, Relevance, Resolution, Respect of Chronology, Semantic Coverage, Specificity, Understandability] [Accuracy, Clarity, Coherence, Consistency, Correctness, Diversity, Engagement, Fluency, Indicative, Informativeness, Neutrality, Non-repetitiveness, Relevance, Resolution, Respect of Chronology, Specificity, Understandability] CommGenChall. Mistral [Coherence, Conciseness, Fluency, Relevance, Understandability] ChatGPT [Clarity, Coherence, Completeness, Conciseness, Consistency, Creativity, Engagement, Fluency, Naturalness, Relevance] Table 17: Selected metrics by tasks by Mistral and ChatGPT. E.12 Which Metrics Were Selected the Most for MG? (E.5) To better understand how models select and evaluate metrics, we analyze the specific metrics chosen for each task  (Table 17)  , their selection frequencies (Figure 12), and their average scores (Figure 13). Figure 12: Frequency of metrics selected as the metric guideline. Figure 13: Average scores of metrics as the metric guideline. Methods CNN (3.0.0) IWSLT17 en-ja CommGen-Chall. Zero-shot (ZS) + LongGuide + LongGuide w/o Token Constraint + LongGuide w/o Sentence Constraint Few-shot (FS) + LongGuide + LongGuide w/o Token Constraint + LongGuide w/o Sentence Constraint 19.230.34 22.460.64 21.540.52 20.920.23 17.560.63 21.181.07 20.301.46 15.892.26 13.121.39 16.530.59 14.091.07 10.024.17 12.691.82 19.862.93 19.751.47 12.572.99 10.120.02 25.201.89 21.492.15 13.320.73 3.980.17 25.050.76 20.301.46 12.203.91 Table 18: Mistral results when omitting OCGs Token or Sentence Information, showing the importance of OCGs Token and Sentence information, evaluated by ROUGE-L. E.13 Extra Ablation Studies: without OCGs Token or Sentence Constraint (E.5) Since OCGs token information and sentence information are the two types of information emphasized in OCG, we further investigate the importance of each type of information. The empirical experiments are conducted with Mistral on CNN, IWSLT-2017 en-ja, and CommonGen-Challenge. We present the results in Table 18. We observe that skipping OCGs token information or sentence information would hurt the performance. Specifically, the results drop more significantly when sentence information is omitted, and even fall below the Zero-shot score in CNN Few-shot with LongGuide and IWSLT17 en-ja Few-shot with LongGuide. The performance drops significantly in the CommonGen-Challenge Few-shot case, with fall of 55.20%. Due to the volatility of the token count in sentence, it is hard to estimate the other information with only one type of information given. Therefore, both types of information should be provided to better capture the text distribution."
        },
        {
            "title": "F Implementation Details",
            "content": "Task benchmark preprocessing. We chose the newest versions of the above datasets. For each dataset except Synthetic-Persona-Chat, we sample 200 samples from the test set for our evaluation, following Bai et al. (2024), and 50 random samples from the train set for Dtrain. For Synthetic-Persona-Chat, we randomly sample 25 dialogues from its test set for our evaluation (678 utterances in total) and 3 dialogues from its train set where 50 random utterances are selected for Dtrain. Prompting baselines hyperparameters. We present the implementation and hyperparameters details for our proposed LongGuide as well as prompting baselines below. LongGuide. We set the batch size is 5 and number of iterations is also 5 for LongGuides step 1. For steps 2, 3, and 4, no hyperparameter involves. For the evaluations by Self-consistency (Wang et al., 2022a), we sample 3 results. APO (Pryzant et al., 2023). We set the number of optimization iterations is 5. We use 1 sample with the lowest ROUGE-L score as the error sample for generating gradients, following (Do et al., 2024). At each iteration, 5 textual gradients are generated, and 5 new prompts are sampled from textual gradients. Finally, 1 paraphrase of the input prompt is sampled at each optimization iteration. adv-ICL (Do et al., 2024). We use 3 iterations with batch size of 5 as suggested by (Do et al., 2024). At each iteration, the number of new prompts sampled is 5. Models hyperparameters. The models hyperparameters are presented below. ChatGPT. We use gpt-3.5-turbo-1106 for our experiments. We use window size of 1500 and Nucleus Sampling (Holtzman et al., 2019) as our decoding strategy with value of 1. We use the system role as You are helpful assistant!. Mistral-7B-it-v0.2. We use window size of 1500, and Sampling decoding strategy (Holtzman et al., 2019) (do_sampling = rue). We load the model from Huggingface Transformers library (Wolf et al., 2020) with the model id is mistralai/Mistral-7B-Instruct-v0.2. We do not set any explicit system role."
        },
        {
            "title": "G Prompts and Prompting Analysis",
            "content": "G.1 GPT-4o-Judges Prompt Our GPT-4o-Judge prompt evaluating the generated response and the reference is heavily motivated by Zheng et al. (2023). Please act as an impartial judge and evaluate how well an assistants answer aligns with the reference answer and the quality of the assistants answer. You will be given user prompt, reference answer and an assistants answer. Your evaluation must consider the following criteria: - Format consistency: ensuring the generated response matches the length and structure of the reference. - Content completeness: evaluating whether all key points present in the reference are included in the assistants answer. - Factuality: checking for factual correctness of the assistants answer. - Style adherence: ensuring that the tone, style, and level of detail of the of the assistants answer match the reference. - Assistants answer quality: assessing how well the response satisfies the users requirements. Begin your evaluation by providing short explanation for each. Be as objective as possible. After providing your explanation, please rate the response on all the criterion on scale of 1 to 10 by strictly following this format: [The Start of Explanation] ... [The End of Explanation] [The Start of Ratings] { \"Format\": 1-10, \"Content\": 1-10, \"Factuality\": 1-10, \"Style\": 1-10, \"Quality\": 1-10, } [The End of Ratings] [User Prompt] user_prompt [The Start of Reference Answer] answer_ref [The End of Reference Answer] [The Start of Assistants Answer] answer_a [The End of Assistants Answer] G.2 ChatGPT Property Scorer Prompt You are an expert in evaluating the quality of text generation task. You possess nuanced understanding of various critical aspects. Brevity is paramount for you, ensuring concise expression without sacrificing essential information. Clarity is essential for comprehension, ensuring that your text is easily understood by the intended audience. Relevance ensures that the generated content aligns closely with the given context or prompt. Neutrality is crucial, maintaining an impartial tone devoid of bias. Coherence ties together ideas seamlessly, fostering logical flow within your text. Completeness guarantees that all relevant points are addressed adequately. Specificity enhances precision, providing detailed and accurate information. Respect of chronology ensures temporal coherence, maintaining the chronological order of events. Accuracy demands factual correctness, avoiding errors or misinformation. Non-repetitiveness prevents redundancy, ensuring freshness in your expression. Indicative language aids in signaling key points or conclusions. Lastly, resolution ensures that your text concludes satisfactorily, resolving any questions or issues raised throughout. Input: {dialogue} Output: {generated_summary} Your task is to evaluate the following criteria in scale of 1-5, with 1 is worst and 5 is best. { \"Semantic Coverage\": 1-5, \"Factuality\": 1-5, \"Consistency\": 1-5, \"Informativeness\": 1-5, \"Coherence\": 1-5, \"Relevance\": 1-5 } The definitions of the criteria are: Semantic Coverage (COV): The extent to which dialogue summary captures the main ideas and topics discussed in the conversation. Factuality (FAC): The accuracy and truthfulness of the information presented in the dialogue summary, reflecting fidelity to the original conversation. Consistency (CON): The degree to which the summary maintains logical and contextual coherence throughout, avoiding contradictory or conflicting information. Informativeness (INF): The richness and depth of information conveyed in the dialogue summary, including key details and relevant context. Coherence (COH): The overall clarity and organization of the summary, ensuring smooth transitions between ideas and coherence in the narrative flow. Relevance (REL): The pertinence of the information included in the dialogue summary to the intended purpose or topic, ensuring alignment with the users interests or needs. Your output must be in Python dictionary format. G.3 LongGuides Prompts Prompting templates for LongGuide. Let Q, C, I, Df be the input query, context, instruction, and demonstration token sequence respectively (1, 2), and Gbest is the learned guideline(s), the prompt for is formatted: {I}n{Df }n{C}n{Q}n{Gbest}. Models Method SAMSum CNN (3.0.0) XL-Sum SWiPE IWSLT17 en-ja Synthetic Persona CommGen-Chall. Summarization Simplification Translation Dialogue Generation Table2Text #shots (random) #tokens consumed US$ consumed 3 642 3 1110 0 5 811 0 3 1020 5 915 0 5 855 0 5 939 #tokens consumed US$ consumed 1866 insignificant 7683 insignificant 4863 insignificant 2380 insignificant 1370 insignificant 1344 insignificant 1272 insignificant t T a Table 19: Total number of tokens consumed and US$ consumed for models to learn the metric guideline (MG) and output constraint guideline (OCG). Prompting costs. Table 19 presents the total number of tokens consumed for models to learn the metric guidelines and output constraint guideline (OCG) for both models with the hyperparameters of LongGuide specified in F. We observe that the number of tokens needed to learn the guidelines is insignificant, demonstrating that LongGuide is cost-effective solution and potentially beneficial for wide range of applications. Table 20 presents the prompting cost comparision between LongGuide and other PO algorithms. We compare the number of new prompts sampled by each algorithm for validation set verification, as these prompts are the primary cost bottleneck in PO algorithms. We observe that LongGuide is approximately at least 3.75 times cheaper than adv-ICL in both settings and 18.75 times cheaper than APO. For SAMSum, the validation of one prompt using 50 samples involves approximately 22K tokens, which incurs cost of 0.02 USD as of November 19, 2024. Prompt for step 1, metric selection. Below is the prompt we use for step 1 selecting metrics for given task. Select top-5 metrics that are the most important from the list below to evaluate special way of {TASK_NAME}. {str(PRE_DEFINED_ASSESSMEN_METRICS)}. Here are some demonstrations of the task {TASK_NAME}: {DEMONSTRATION_STRING}. Output your list of metrics in Python list format without any explanation: [...]. Prompt for step 2, metric score collection. Below is the prompt we use for step 2 for evaluating selected metrics on the task. You are given an input and an output of {TASK_NAME} task. Input: {input} Output: {output} Your task is to evaluate the following criteria on scale of 1-5, with 1 being worst and 5 being best. {EVALUATION_FORMAT} The definitions of the criteria are: {METRICS_DEFINITIONS} Your output must be in Python dictionary format without explanation. Prompt for step 2, collecting metrics definitions. Below is the prompt we use for step 2 collecting METRICS_DEFINITIONS for step 2. Define the list of following metrics in details as the quality of the output expected for the {TASK_NAME} task. {metrics} Give me the list in bullet points. Prompt for step 3, generating metric guideline (MG). Below is the prompt we use for step 3, generating the metric guideline (MG). Now you are given the following metrics: {metrics_string} for the {TASK_NAME} task. Based on these scores on scale of 5 for the quality of the output: {str(metrics_collected_scores)}, define the expected quality of the output for each metric in natural language. Give me the list in bullet points."
        },
        {
            "title": "H Examples",
            "content": "Method #Prompts Sampled adv-ICL APO LongGuide (3 iterations) (1 instruction) (5 variants) (5 iterations) (15 prompts sampled) (1 instruction) 4 prompts (MG, OCG, MG-OCG, No guideline) Cost 15 prompt validation cost 75 prompt validation cost 4 prompt validation cost adv-ICL APO LongGuide (3 iterations) (3 demonstrations + 1 instruction) (5 variants) (5 iterations) (15 prompts sampled) (3 demonstrations + 1 instruction) 4 prompts (MG, OCG, MG-OCG, No guideline) 60 prompt validation cost 300 prompt validation cost 4 prompt validation cost F Table 20: Prompting cost comparison between PO methods and LongGuide based on # new prompts sampled to test over the validation set. Figure 14: Full example of ChatGPT results on SAMSum example (Gliwa et al., 2019) w/ LongGuide guidelines (Zero-shot + LongGuide and Few-shot + LongGuide) from Figure 1 Figure 15: sample from SAMSum dataset where MG and OCG supplement each other and are not interchangeable to increase the performance in final answer. Figure 16: An example of SWiPE (Laban et al., 2023) where the record contains fewer tokens than the expected average. This reduces the effectiveness of OCG and MG individually, but their combination could enhance performance. Figure 17: CommonGen-Challenge example (Lin et al., 2020), where output with high Conciseness score could have low Informativeness score and vice versa Figure 18: SAMSum example, where skipping step 2 worsens the performance due to lack of clarity in metrics Figure 19: Full text for an example in 2."
        }
    ],
    "affiliations": [
        "Institute for Infocomm Research (I2R), A*STAR",
        "Nanyang Technological University, Singapore",
        "National University of Singapore",
        "Salesforce AI Research"
    ]
}