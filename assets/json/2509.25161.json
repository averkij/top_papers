{
    "paper_title": "Rolling Forcing: Autoregressive Long Video Diffusion in Real Time",
    "authors": [
        "Kunhao Liu",
        "Wenbo Hu",
        "Jiale Xu",
        "Ying Shan",
        "Shijian Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, a novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design a joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep key value states of initial frames as a global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on a single GPU, with substantially reduced error accumulation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 6 1 5 2 . 9 0 5 2 : r ROLLING FORCING: AUTOREGRESSIVE LONG VIDEO DIFFUSION IN REAL TIME Kunhao Liu1 Wenbo Hu2 Jiale Xu2 Ying Shan2 Shijian Lu1 1Nanyang Technological University 2ARC Lab, Tencent PCG Figure 1: Rolling Forcing performs real-time streaming text-to-video generation at 16 fps on single GPU and is capable of producing multi-minute-long videos with minimal error accumulation. More results, code, and demo can be found at the project page."
        },
        {
            "title": "ABSTRACT",
            "content": "Streaming video generation, as one fundamental component in interactive world models and neural game engines, aims to generate high-quality, low-latency, and temporally coherent long video streams. However, most existing work suffers from severe error accumulation that often significantly degrades the generated stream videos over long horizons. We design Rolling Forcing, novel video generation technique that enables streaming long videos with minimal error accumulation. Rolling Forcing comes with three novel designs. First, instead of iteratively sampling individual frames, which accelerates error propagation, we design joint denoising scheme that simultaneously denoises multiple frames with progressively increasing noise levels. This design relaxes the strict causality across adjacent frames, effectively suppressing error growth. Second, we introduce the attention sink mechanism into the long-horizon stream video generation task, which allows the model to keep keyvalue states of initial frames as global context anchor and thereby enhances long-term global consistency. Third, we design an efficient training algorithm that enables few-step distillation over largely extended denoising windows. This algorithm operates on non-overlapping windows and mitigates exposure bias conditioned on self-generated histories. Extensive experiments show that Rolling Forcing enables real-time streaming generation of multi-minute videos on single GPU, with substantially reduced error accumulation. Work done during internship at ARC Lab, Tencent PCG. Corresponding authors. 1 Figure 2: Different paradigms in autoregressive video generation. History corruption (Chen et al., 2024; Guo et al., 2025) in (a) compromises temporal consistency, while planning generation (Zhang & Agrawala, 2025; Xiang et al., 2025) in (b) is incompatible with sequential streaming video generation. Self Forcing (Huang et al., 2025) in (c) can achieve consistent sequential streaming but suffers from severe error accumulation while generating long videos. The proposed Rolling Forcing in (d) supports streaming long video generation with superior temporal consistency and minimal error accumulation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern video diffusion models (OpenAI, 2024; Polyak et al., 2024) have demonstrated impressive capabilities in generating short video clips with rich detail and coherent motion. However, interactive applications such as world models (Bruce et al., 2024), neural game engines (Valevski et al., 2024), and immersive XR environments require the ability to stream each frame with minimal latency while maintaining visual quality and temporal coherence over long horizons. Unlike offline video generation, where the entire sequence is synthesized together at one go, the streaming video generation operates in an online fashion: frames are generated sequentially and immediately consumed by downstream tasks or displayed to users. Such online nature imposes unique challenges, as the model must maintain long-horizon consistency while accommodating real-time constraints in an autoregressive manner. Real-time streaming video generation methods, such as CausVid (Yin et al., 2025) and Self Forcing (Huang et al., 2025) (illustrated in Fig. 2(c)), distill pretrained bidirectional video diffusion model into fast, causal autoregressive generator. While they enable consistent sequential generation, their strictly causal frame prediction causes each frame to inherit errors from its predecessors, allowing small imperfections to compound over long horizons and eventually leading to noticeable drift and quality degradation. Two representative approaches have been explored for improving video generation over long horizons, as illustrated in Fig. 2(a,b). The first approach explores history corruption, which injects noise into past frames to reduce over-reliance on histories (Chen et al., 2024; Guo et al., 2025). History corruption mitigates drift by narrowing the gap between selfgenerated and ground-truth context, but it deprives the model of clean references and compromises temporal consistency. The second approach explores planning generation by first synthesizing distant key frames and then interpolating intermediates (Zhang & Agrawala, 2025; Xiang et al., 2025). Anchoring distant frames to the initial context mitigates drift, but the introduced out-of-order schedule violates strict sequential emission, which is unsuitable for real-time streaming. We design Rolling Forcing, an autoregressive long video generation technique that mitigates error accumulation while maintaining real-time performance as illustrated in Fig. 2. Rolling Forcing comes with three new designs. First, instead of iteratively denoising single frame at time as in most existing work, Rolling Forcing introduces rolling-window denoising to process multiple consecutive frames simultaneously. Within each window, frames are connected by bidirectional attention and assigned progressively increasing noise levels. Such mutual refinement corrects local errors before any frame is finalized, thereby suppressing long-horizon drift. In addition, this design 2 allows us to emit clean frame after each single forward pass, achieving real-time throughput on single GPU despite much larger attention window. Second, we adapt the attention sink mechanism (Xiao et al., 2023) to the streaming video generation task, thereby strengthening long-term global consistency. Specifically, we persist the keyvalue states of the initial frames as global context anchor and dynamically adjust their Rotary Position Embeddings (RoPE) (Su et al., 2024), which freezes the relative positions of initial frames to the current denoising frames and prevents excessive offsets. Note that the KV caching is applied to the recent clean frames as well to reduce latency and maintain temporal consistency. Third, we design an efficient training algorithm that enables few-step distillation over the extended denoising windows. This algorithm operates on non-overlapping windows that collectively cover all video frames, mitigating exposure bias by conditioning on self-generated histories during training. Extensive experiments show that Rolling Forcing achieves real-time streaming generation of multi-minute videos on single GPU, with substantially reduced error accumulation as illustrated in Fig. 1. The contributions of this work can be summarized in three key aspects. First, we introduce rollingwindow joint denoising technique that processes multiple frames in single forward pass, enabling mutual refinement while preserving real-time latency. Second, we introduce the attention sink mechanism into the streaming video generation task, pioneering effort that enables caching the initial frames as consistent global context for long-term coherence in video generation. Third, we design an efficient training algorithm that operates on non-overlapping windows and conditions on self-generated histories, enabling few-step distillation over extended denoising windows and concurrently mitigating exposure bias."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Bidirectional Video Generation Models. Video generation has advanced rapidly in recent years, with modern approaches mostly adopting the paradigms of denoising diffusion. Video diffusion has been explored in both pixel space (Ho et al., 2022; Singer et al., 2022) and latent space (Blattmann et al., 2023b;a), with architectures evolving from early SpaceTime U-Nets (Blattmann et al., 2023a; Hong et al., 2022) to more recent DiT-based designs (Peebles & Xie, 2023; Gupta et al., 2024). Significant industrial investment has driven the development of large video diffusion models, leading to several multi-billion parameter models, including open-source models such as Wan (Wan et al., 2025) and Hunyuan (Kong et al., 2024), and closed-source models such as Sora (OpenAI, 2024), Movie-Gen (Polyak et al., 2024), and Seaweed (Seawead et al., 2025). Notably, these models operate as bidirectional video diffusion models, as they have access to both past and future frames during denoising. While this bidirectional context enables high-quality synthesis for offline generation, it is incompatible with the causality that is necessitated in real-time streaming video generation. Autoregressive Video Generation Models. To enable long video generation, several studies have extended the generation paradigm from bidirectional to autoregressive, which naturally supports gradual rollout over extended time horizons. Autoregressive models are typically trained with nexttoken prediction objectives and generate spatiotemporal tokens sequentially at inference time (Bruce et al., 2024; Kondratyuk et al., 2023; Wang et al., 2024; Weissenborn et al., 2019; Yan et al., 2021). More recently, separate line of research combines autoregressive modeling with denoising diffusion (Chen et al., 2024; Gu et al., 2025; Guo et al., 2025; Jin et al., 2024; Li et al., 2024; Liu et al., 2024; Weng et al., 2024; Yin et al., 2025; Zhang et al., 2025; Zhang & Agrawala, 2025; Huang et al., 2025; Henschel et al., 2025), where frames are generated one-by-one in an outer loop and each frame is gradually denoised in an inner loop. Within this family, Rolling Diffusion (Ruhe et al., 2024) and its variants (Kim et al., 2024; Teng et al., 2025; Sun et al., 2025; Xie et al., 2025; Chen et al., 2025; Teng et al., 2025) merge the outer and inner loops: the diffusion model jointly denoises multiple frames at progressively increasing noise levels. However, these methods mostly suffer from exposure bias and error accumulation when generating long videos. Another line of research addresses error accumulation with planning generation (Long et al., 2024; Zhao et al., 2024; Hu et al., 2024; Xie et al., 2024; Zhang & Agrawala, 2025; Bansal et al., 2024; Yang et al., 2024; Xiang et al., 2025), which predicts distant future frames first and then interpolates the intermediate frames. While effective for reducing drift, it breaks the strict sequential order required for real-time streaming. In contrast, our work enables much longer streaming video generation with minimal error accumulation while addressing exposure bias. 3 Concurrent and Closed-Source Work. Two concurrent works are also devoted to the streaming generation of long videos. Specifically, StreamDiT (Kodaira et al., 2025) adopts the FIFO-style denoising (Kim et al., 2024) for streaming video generation. It modifies the pretrained model architecture by introducing micro-steps and window attention, necessitating extensive additional pretraining with large-scale data and computation. In contrast, our method keeps the pretrained model architecture unchanged, can be trained efficiently in only 3,000 steps, and does not require any video data. APT2 (Lin et al., 2025b) instead explores adversarial distillation (Lin et al., 2025a) for streaming video generation. It denoises videos block-by-block and involves multiple costly post-training stages, including diffusion adaptation, consistency distillation, adversarial training, and long-video training. APT2 is trained on one-minute-long videos, whereas ours is trained only on 5-second clips, yet can extend to multi-minute sequences during inference. Note that both StreamDiT and APT2 are closed-source and trained based on internal video diffusion models (Movie-Gen (Polyak et al., 2024) and Seaweed (Seawead et al., 2025)), while our model is trained on public datasets and relies on an open-source model (i.e., Wan2.1 (Wan et al., 2025)) as its foundation."
        },
        {
            "title": "3 METHODS",
            "content": "3.1 PRELIMINARIES: EXPOSURE BIAS IN AUTOREGRESSIVE VIDEO DIFFUSION MODELS An autoregressive video diffusion model is hybrid generative framework that integrates autoregressive chain-rule decomposition with denoising diffusion for video generation. Formally, given sequence of video frames x1:N = (x1, x2, . . . , xN ), their joint distribution can be factorized using the chain rule: p(x1:N ) = (cid:81)N i=1 p(xi x<i). Each conditional distribution p(xi x<i) is modeled through diffusion process, where each frame is generated by progressively denoising Gaussian noise while conditioning on the previously generated frames. In practice, one may also generate chunk of consecutive frames instead of single frame at each step (Yin et al., 2025; Teng et al., 2025; Huang et al., 2025). For clarity, we refer to each chunk simply as frame in the following text. Autoregressive video diffusion models are trained either (1) from scratch with frame-wise denoising loss or (2) by distilling pretrained bidirectional model. The first approach is trained under the paradigm of Teacher Forcing (TF) or Diffusion Forcing (DF) (Chen et al., 2024). In TF, the conditional distribution for the ith frame at noise level tj is p(xi 0 ), where all conditional histj tory frames are the ground-truth clean frames from the training data. While in DF, the conditional distribution is p(xi ), where the history frames are the ground-truth frames corrupted with tj independent noise levels. Since training relies on ground-truth histories while inference relies on the models own predictions, traintest gap known as exposure bias arises (Schmidt, 2019). Mitigating the exposure bias is difficult because the denoising loss requires pairs of model predictions and the corresponding ground truth conditioned on them, which are unavailable. x<i t0 x<i The second approach of distillation, however, provides way to bypass the denoising loss and mitigate exposure bias. CausVid (Yin et al., 2025) distills pretrained bidirectional model into few-step causal model. It adopts Distribution Matching Distillation (DMD) loss (Yin et al., 2024b) that minimizes the reverse KL divergence across randomly sampled timesteps between the smoothed data distribution pdata(xt) and the student generators output distribution pgen(xt). The gradient of the reverse KL can be approximated as the difference between two score functions: θLDMD Et (θKL (pgen,tpdata,t)) (cid:18)(cid:90) Et (sdata (Ψ (Gθ(ϵ), t) , t) sgen (Ψ (Gθ(ϵ), t) , t)) dGθ(ϵ) dθ (cid:19) dϵ , (1) where Ψ represents the forward diffusion process, ϵ is random Gaussian noise, Gθ is the generator parameterized by θ, and sdata and sgen represent the score functions trained on the data and generators output distribution, respectively. Since training with DMD loss does not require ground-truth image or video data (Yin et al., 2024a), Self Forcing (Huang et al., 2025) mitigates the exposure bias by conditioning each frame on previously self-generated histories during training. However, although exposure bias is alleviated, severe error accumulation still occurs once generation extends beyond the trained temporal window. 4 Figure 3: Illustration of the Rolling Forcing denoising process with = 4. Rolling Forcing jointly denoises short window of consecutive frames that are assigned progressively higher noise levels and connected by bidirectional attention. The KV cache of recent frames is preserved as temporal context to maintain short-term consistency, while the KV cache of the initial frames is preserved as global context to ensure long-term consistency. During training, only subset of denoising windows requires gradient computation, as highlighted by the red windows. These windows are mutually exclusive yet collectively cover all video frames. 3.2 AUTOREGRESSIVE VIDEO GENERATION VIA ROLLING DIFFUSION WINDOW In Self Forcing (SF), videos are generated frame-by-frame in strict causal manner. Consider noise schedule {t0 = 0, t1, . . . , tT = 1000} with total noise levels + 1. At each denoising step tj and frame index i, the model denoises an intermediate noisy frame xi tj conditioned on previous clean frames x<i 0 and then injects Gaussian noise with lower noise level into the predicted denoised clean frame via the forward diffusion process Ψ. This produces noisy frame xi tj1 which will be used as the input to the next denoising step. Formally, in SF, the denoising process is achieved (cid:1), and xi by: xi (0, I). However, this formulation has no tT bidirectional attention between the current denoising frame xi and its history x<i, where the strict causality forces every frame to inherit and compound the errors from its predecessors over time. = Ψ(cid:0)Gθ(xi tj 0 ), tj1 , tj, x<i tj The proposed Rolling Forcing relaxes this constraint by extending the single-frame denoising window into rolling window spanning multiple frames, as illustrated in Fig. 3. Each denoising window contains consecutive frames with progressively higher noise levels in temporal order, akin to Rolling Diffusion (Ruhe et al., 2024). The length of the denoising window Lwin is set to the number of denoising time steps, i.e., Lwin = . To ensure continuity, the next noise level of the ith frame is aligned with the current noise level of the (i 1)th frame, allowing the window to roll forward infinitely. At each roll, clean frame is generated, and pure Gaussian noise is appended as the next frame to be synthesized. Formally, for the denoising window starting at the ith frame, the denoising distribution of Rolling Forcing can be defined by: (cid:16) pθ xi:i+T 1 t0:T 1 xi:i+T 1 t1:T , x<i (cid:17) (cid:16) = Ψ Gθ(xi:i+T 1 t1:T , t1:T , x<i 0 ), t0:T (cid:17) , (2) t1:T denotes the noisy frames in the denoising window, and xi:i+T 1 where xi:i+T 1 denotes the window output with each frame denoised to lower noise level. The generator Gθ predicts clean frames conditioned on the input noisy frames, their noise levels t1:T , and the clean history frames x<i 0 . Ψ injects Gaussian noise ϵt0:T 1 at noise levels t0:T 1 into the predicted clean frames, producing frames with reduced noise levels. t0:T Since the length of the denoising window equals the number of denoising steps , which is typically large (i.e., 50) in video diffusion models (Wan et al., 2025), the denoising window itself becomes prohibitively large. To manage such large windows, previous work either processes every frame independently on multiple GPUs (Kim et al., 2024), or reduces to 30 using few-step samplers (Xie et al., 2025). In contrast, we adopt diffusion distillation (Yin et al., 2024b;a), which reduces the number of denoising steps to just 5 while preserving generation quality, thereby making the denoising windows compact enough to fit on single GPU while maintaining real-time latency."
        },
        {
            "title": "3.3 TEMPORAL AND GLOBAL HISTORY CONTEXT",
            "content": "As the clean history frames x<i 0 accumulate during generation, handling them directly becomes computationally expensive. To address this, following Huang et al. (2025), we cache the key and value states of the history frames, thereby avoiding redundant recomputation when generating new frames, as illustrated in Fig. 3. Note that although the attention within the denoising window is bidirectional, the attention between the frames in the denoising window and the KV cache of history frames remains causal. While KV caching reduces computation, the computational complexity still grows quadratically with the cache size as frames accumulate, and the cache may become large enough to cause out-ofmemory errors. Given denoising window starting at the ith frame xi:i+T 1 , we address this issue by retaining only the KV cache of the most recent Ltem history frames xiLtem:i1 as temporal context to preserve short-term temporal consistency. However, relying solely on short-term history causes gradual drift of long-range properties of the generated video (like exposure, color tone, white balance, etc.) as generation proceeds. t1:T 0 Algorithm 1 Rolling Forcing Training Require: Denoise timesteps {t0, t1, . . . , tT } Require: Number of video frames Require: AR diffusion model Gθ (returns KV embeddings via GKV ) θ Initialize model output Xθ [] Initialize KV cache KV [] Initialize x1:T 1 t1:T 1 with Gθ Sample Uniform{0, 1, . . . , 1} for = 1, . . . , do Sample xi+T 1 (0, I) tT Set xi:i+T 1 xi:i+T 2 t1:T t1:T 1 Select and apply RoPE to KV (Sec. 3.3) if (mod ) then xi+T 1 tT Enable gradient computation ˆxi:i+T 1 0 Xθ.append(ˆxi:i+T 1 Disable gradient computation Gθ(xi:i+T 1 ) t1:T 0 , t1:T , KV) else ˆxi:i+T 1 0 end if KV.append(GKV xi+1:i+T 1 t1:T 1 Gθ(xi:i+T 1 t1:T , t1:T , KV) θ (ˆxi Ψ(ˆxi+1:i+T 1 0 0, t0, KV)) , t1:T 1) 1: loop 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end loop end for Update θ via DMD loss (Eq. (1)) 0 To maintain long-term global consistency, we cache the KV states of the initial Lglo generated frames x1:Lglo as global context, analogous to attention sink tokens in streaming language models (Xiao et al., 2023). The cache sizes Ltem and Lglo are chosen such that the total attention window size matches that of the bidirectional teacher model, i.e., Ltem+Lglo+Lwin = Lbidirectional. However, directly caching the initial frames leads to spilling problems. Modern video diffusion DiTs (Peebles & Xie, 2023) typically use RoPE (Su et al., 2024) for relative positional encoding. As the indices of the denoising frames : + 1 increase, their relative distance to the initial cached frames grows, eventually exceeding the trained range of RoPE and producing unnatural artifacts. To resolve this, we cache the key states of the global context frames x1:Lglo before applying the RoPE transformation. During generation, we dynamically apply RoPE to these cached key states at the effective indices Ltem Lglo : Ltem 1, treating them as being positioned immediately before the temporal context frames xiLtem:i1 . This adjustment preserves fixed relative position w.r.t. the denoising frames, preventing excessive offsets. 0 0 3.4 ROLLING FORCING POST-TRAINING Rolling Forcing distills pretrained bidirectional video diffusion model (Wan et al., 2025) to fewstep causal autoregressive generator using the DMD loss (Eq. (1)). As DMD matches the holistic distribution of the entire video sequence to the data distribution D(pdata(x1:N )pθ(x1:N )), the calculation of the DMD loss requires predicted clean video ˆx1:N during training. In SF, the predicted clean video is generated by: 0 ˆx1:N 0 = (cid:110) 0 = Gθ(xi ˆxi tj , tj, x<i 0 ) = 1, 2, . . . , (cid:111) , (3) where Uniform{0, 1, . . . , 1} indicates each frames noise level tj before denoising. For Rolling Forcing, as the denoising window consists of multiple frames at different noise levels, we select the jth frame in each window and combine the selected frames as the predicted clean video: ˆx1:N 0 = (cid:110) 0 = (ˆxi:i+T 1 ˆxi )j = Gθ(xi:i+T 1 t1:T , t1:T , x<i 0 )j = 1, 2, . . . , (cid:111) , (4) where Uniform{0, 1, . . . , 1} represents both the frames index within the denoising window and the frames noise level tj. However, Eq. (4) incurs times higher computational complexity 6 Table 1: Comparisons with relevant baselines. We compare Rolling Forcing with representative open-source autoregressive video generation models of similar parameter sizes. Model #Params Throughput Latency (FPS) (s) Evaluation Scores Temporal Background Motion Aesthetic Imaging Flickering Consistency Consistency Smoothness Quality Quality Subject Quality Drift Diffusion Forcing Causal SkyReels-V2 (Chen et al., 2025) MAGI-1 (Teng et al., 2025) 1.3B 4.5B Distilled Causal CausVid (Yin et al., 2025) Self Forcing (Huang et al., 2025) Rolling Forcing (Ours) Numbers adopted from Huang et al. (2025). 1.3B 1.3B 1.3B 0.49 0.19 15.38 15.38 15.79 112 282 0.78 0.78 0.76 97.43 98.21 89.23 90.86 96.84 97.49 97. 87.99 86.48 92.80 93.45 93.25 89.99 90.29 93.71 98.76 99.20 61.55 62.90 59.91 59.87 98.09 98.47 98. 60.95 66.38 60.54 68.68 62.39 70.75 5.59 2.15 2.18 1.66 0.01 than Eq. (3), because the query size is times larger. Given that DMD loss is already computationally expensive, this additional cost can easily lead to out-of-memory error even on GPUs with 80G of memory. To address this issue, instead of backpropagating through every window (which requires gradients for each forward pass), we sample subset of non-overlapping windows to construct the predicted clean video, as illustrated in Fig. 3. Gradient computation is performed only on these selected windows, significantly reducing memory usage while retaining effective supervision. Formally, the predicted clean video is given by: ˆx1:N 0 = (cid:110) ˆxi:i+T 1 0 = Gθ(xi:i+T 1 t1:T , t1:T , x<i 0 ) (mod ), 1 (cid:111) , (5) where Uniform{0, 1, . . . , 1}. In each iteration, we reduce the number of forward passes requiring gradient computation from in Eq. (4) to N/T 1. The Rolling Forcing training is illustrated in Alg. 1. Similar to SF, the input noisy frames xi:i+T 1 during training are generated by the model rather than taken from ground truth, thus mitigating the exposure bias. However, unlike Eq. (3) or Eq. (4), where every frame in the predicted clean video ˆx1:N is denoised from the same noise level tj, the frames in Eq. (5) are denoised from varying noise levels t1:T . Consequently, frames denoised from different noise levels have different quality and clarity, leading to unnatural video ˆx1:N and camera movement in DMD training. To address this issue, we adopt mixed training strategy that alternates between SF training (Eq. (3)) and Rolling Forcing training (Eq. (5)) with equal probability. The SF objective serves as regularizer, encouraging the model to produce videos with natural camera movement. The inference adopts the Rolling Forcing paradigm alone as elaborated in Alg. 2. t1:T 0 4 EXPERIMENTS 4.1 IMPLEMENTATION DETAILS Model. We implement Rolling Forcing with Wan2.1-T2V-1.3B (Wan et al., 2025) as our base model, which generates 5s videos at 16 FPS with resolution of 832 480. Following CausVid (Yin et al., 2025) and Self Forcing (Huang et al., 2025), we first initialize the base model with causal attention masking on 16k ODE solution pairs sampled from the base model. For both ODE initialization and Rolling Forcing training, we sample text prompts from filtered and LLM-extended version of VidProM (Wang & Yang, 2024). We set = 5 and perform chunk-wise denoising with each chunk containing 3 latent frames. The model is trained for 3,000 steps with batch size of 8 and trained temporal window of 27 latent frames. We use the AdamW optimizer for both the generator Gθ (learning rate 1.5 106) and the fake score sgen (learning rate 4.0 107). The generator is updated every 5 steps of fake score updates. Evaluation. We adopt the VBench (Huang et al., 2024) quality matrices to evaluate the generation quality over 200 randomly sampled MovieGen (Polyak et al., 2024) prompts, where the matrices measure multiple dimensions, including temporal flickering, subject consistency, background consistency, motion smoothness, aesthetic quality, and imaging quality. For fairness, all videos for 1We omit the denoising windows at the start of the video for clarity in Eqs. (2) and (5), where the window has fewer than frames. Gradient computation is still required if the window index satisfies (mod ). 7 Figure 4: Qualitative comparisons. We compare Rolling Forcing with representative open-source autoregressive video generation models on long video generation. quantitative evaluation are generated with the same length (30s), frame rate (16 fps), and resolution (832 480). To assess quality drift in long video generation, following Zhang & Agrawala (2025); Yin et al. (2025), we compute the absolute difference in imaging quality, Quality , between the first and the last 5 seconds of each video. The magnitude of Quality directly reflects the severity of error accumulation. Following Huang et al. (2025), we evaluate real-time performance in terms of both throughput and latency. Unlike prior work that reports the first-frame latency, we measure latency after the generation process reaches stable speed. Drift Drift 4.2 COMPARISONS We compare Rolling Forcing against several relevant open-source video generation models of comparable scale. Specifically, SkyReels-V2 (Chen et al., 2025) is trained under the Diffusion Forcing paradigm (Chen et al., 2024), which corrupts historical frames during inference to alleviate error accumulation. MAGI-1 (Teng et al., 2025) adopts FIFO-style denoising paradigm (Kim et al., 2024) in both training and inference. We also compare against prior distillation-based approaches, including CausVid (Yin et al., 2025) and Self Forcing (Huang et al., 2025). Note that SkyReelsV2, CausVid, Self Forcing, and our Rolling Forcing are all initialized from the same base model, Wan2.1-T2V-1.3B (Wan et al., 2025). As shown in Table 1, Rolling Forcing achieves the highest overall quality scores. In particular, it obtains substantially lower Quality , demonstrating its effectiveness in suppressing error accumulation. Qualitative comparisons in Fig. 4 further highlight that Rolling Forcing preserves high-fidelity Drift Figure 5: Ablation studies on rolling diffusion window, mixed training strategy, and attention sink. and consistent video quality over 2 minutes of autoregressive generation, while the compared models exhibit pronounced degradation, such as color shifts, artifacts, unnatural motion, etc. In addition, Rolling Forcing achieves real-time generation with sub-second latency, marginally faster than Self Forcing and CausVid, thereby establishing its suitability for long-horizon video streaming applications. 4.3 ABLATION STUDIES We conduct ablation studies to assess the contribution of several design options, as summarized in Table 2. Table 2: Ablation studies. RF refers to Rolling Forcing, and SF refers to Self Forcing. Model Evaluation Scores Quality Drift Temp. Subj. Back. Mot. 95.45 86.01 89.94 97.36 57.59 65.19 w/o RF inference 95.91 87.50 90.86 98.05 60.41 69.24 w/o RF training w/o SF training 90.83 83.27 88.14 95.63 55.30 62.00 w/o attention sink 97.53 83.22 87.99 98.56 58.99 67.30 Rolling diffusion window. We evaluate two variants: w/o RF inference and w/o RF training. In w/o RF inference, we remove the rolling denoising window and adopt frameby-frame denoising during inference, while keeping the same training procedure and model weights as our full method. In w/o RF training, the model is trained and inferred entirely with the frame-by-frame paradigm. As shown in Fig. 5, both variants suffer from noticeable error accumulation within 30s, demonstrating that the rolling window is crucial for suppressing long-term drift. 97.61 92.80 93.71 98.70 62.39 70.75 5.53 0.89 1.62 4.63 Ours full 0.01 Img. Aes. Mixed training strategy. To assess its effect, we remove the Self Forcing training objective (w/o SF training). As reported in Table 2, this leads to substantial degradation in consistency and overall quality, primarily due to unnatural camera motion. Attention sink. Finally, removing the global context frame (w/o attention sink) results in noticeable drift in the generated videos, as illustrated in Fig. 5."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented Rolling Forcing, framework for real-time long-horizon video generation that mitigates error accumulation while sustaining sub-second latency. By introducing rolling-window joint denoising strategy, Rolling Forcing enables mutual refinement across consecutive frames, effectively reducing long-term drift. The integration of the attention sink mechanism further enhances global consistency by anchoring initial frames as persistent context, while our efficient training algorithm enables few-step distillation over extended denoising windows while mitigating exposure bias. Extensive experiments demonstrate that Rolling Forcing achieves state-of-the-art temporal coherence and visual fidelity over multi-minute streaming sequences, significantly outperforming prior streaming approaches in both quality and efficiency."
        },
        {
            "title": "REFERENCES",
            "content": "Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, and Kai-Wei Chang. Talc: Time-aligned captions for multi-scene text-to-video generation. arXiv preprint arXiv:2405.04682, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2256322575, 2023b. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pp. 393411. Springer, 2024. Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, In Proceedings of the Computer Vision and and extendable long video generation from text. Pattern Recognition Conference, pp. 25682577, 2025. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. arXiv preprint arXiv:2411.04925, 2024. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. 10 Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. Advances in Neural Information Processing Systems, 37: 8983489868, 2024. Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, and Yue Zhao. Streamdit: Real-time streaming text-to-video generation. arXiv preprint arXiv:2507.03745, 2025. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, and Furu Wei. Arlon: Boosting diffusion transformers with autoregressive models for long video generation. arXiv preprint arXiv:2410.20502, 2024. Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025a. Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videostudio: Generating consistent-content and multi-scene videos. In European Conference on Computer Vision, pp. 468485. Springer, 2024. OpenAI. Sora. https://openai.com/sora, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. In International Conference on Machine Learning, 2024. Florian Schmidt. Generalization in generation: closer look at exposure bias. arXiv preprint arXiv:1910.00292, 2019. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 11 Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7364 7373, 2025. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Advances in Neural Information Processing Systems, 37:6561865642, 2024. Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024. Dirk Weissenborn, Oscar Tackstrom, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019. Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, et al. Art-v: Auto-regressive text-to-video generation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73957405, 2024. Xunzhi Xiang, Yabo Chen, Guiyu Zhang, Zhongyu Wang, Zhe Gao, Quanming Xiang, Gonghu Shang, Junqi Liu, Haibin Huang, Yang Gao, et al. Macro-from-micro planning for high-quality and parallelized autoregressive long video generation. arXiv preprint arXiv:2508.03334, 2025. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 63226332, 2025. Zhifei Xie, Daniel Tang, Dingwei Tan, Jacques Klein, Tegawend Bissyand, and Saad Ezzini. Dreamfactory: Pioneering multi-scene long video generation with multi-agent framework. arXiv preprint arXiv:2408.11788, 2024. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Dingyi Yang, Chunru Zhan, Ziheng Wang, Biao Wang, Tiezheng Ge, Bo Zheng, and Qin Jin. Synchronized video storytelling: Generating video narrations with structured storyline. arXiv preprint arXiv:2405.14040, 2024. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024b. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. 12 Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William Freeman, and Hao Tan. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024."
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "KV Cache. We configure the KV cache and denoising window sizes as Ltem = 3, Lglo = 3, and Lwin = 15 latent frames. When updating the KV cache with GKV θ , the attention window attends only to recent frames, excluding the global context. This design reflects that, apart from the initial frames serving as the global context anchor, other cached frames are retained solely for preserving short-term temporal consistency. During inference, we persist the KV states of the global context frames while discarding obsolete temporal frames, thereby maintaining constant memory usage. At the start of the video, when the denoising window still includes the first frame, no temporal or global context is used. Algorithm 2 Rolling Forcing Inference Require: Denoise timesteps {t0, t1, . . . , tT } Require: Number of video frames Require: AR diffusion model Gθ (returns KV embeddings via GKV ) t1:T 1 with Gθ θ 1: Initialize model output Xθ [] 2: Initialize KV cache KV [] 3: Initialize x1:T 1 4: for = 1, . . . , do Sample xi+T 1 (0, I) 5: tT xi:i+T 2 Set xi:i+T 1 6: t1:T t1:T 1 Select and apply RoPE to KV (Sec. 3.3) 7: Gθ(xi:i+T 1 ˆxi:i+T 1 8: 0 9: Xθ.append(ˆxi 0) 10: KV.append(GKV xi+1:i+T 1 11: t1:T 1 12: end for θ (ˆxi Ψ(ˆxi+1:i+T 1 0 xi+T 1 tT 0, t0, KV)) , t1:T , KV) , t1:T 1) t1:T Training. During training, the number of generated frames is randomly sampled between 21 (the sequence length of the bidirectional teacher model) and 27 latent frames. The DMD loss is computed on the last 21 frames. Since in Wan2.1 (Wan et al., 2025) the first VAE-encoded frame is not temporally compressed and thus exhibits different statistics, we decode frames 0:N 21 to RGB and re-encode the (N 21)-th frame into the latent space. This re-encoded frame is then concatenated with latent frames 21:N 1 for loss computation. In this way, the first frame is only spatially compressed, ensuring consistency with the statistical distribution of the bidirectional teacher model. Noise schedule and model parameterization. Following the Wan2.1 and Self Forcing, we adopt the flow matching framework (Lipman et al., 2022; Liu et al., 2022), with time step shifting t(k, t) = (kt/1000)/(1 + (k 1)(t/1000)) 1000 and shift factor = 5. The forward process is specified as xt = 1000 ϵ, ϵ (0, I) with [0, 1000]. The data prediction model is given by: 1000 + 1t Gθ(x, t, c) = cskip ϵ cout vθ(cin xt, cnoise(t), c). (6) We keep the preconditioning coefficients the same as the base models configuration, i.e., cskip = cin = cout = 1 and cnoise(t) = t. Our few-step diffusion process employs uniform 5-step schedule [t5, t4, t3, t2, t1] = [1000, 800, 600, 400, 200]. We adopt 5-step schedule rather than 4, as our method achieves comparable and even slightly faster generation speed than 4-step Self Forcing."
        },
        {
            "title": "B VBENCH SCORES ACROSS ALL DIMENSIONS",
            "content": "Table 3: Full quality evaluation on VBench. Model Subject Background Temporal Motion Dynamic Aesthetic Imaging Quality Consistency Consistency Flickering Smoothness Degree Quality Quality Score CausVid (Yin et al., 2025) Self Forcing (Huang et al., 2025) Rolling Forcing (Ours) 89.50 88.61 94. 90.00 89.53 95.69 99.41 98.90 98.93 98.06 98.57 98.63 63.88 61.82 65.30 80.89 68.05 60.60 68.98 81.39 60.14 62.81 72.31 84.08 Table 4: Full semantic evaluation on VBench. Model Object Multiple Human Class Objects Action Color Spatial Relationship Scene Temporal Appearance Overall Style Consistency Style CausVid (Yin et al., 2025) 78.56 58.84 81.00 81.02 Self Forcing (Huang et al., 2025) 80.06 62.88 83.00 79.80 85.92 64.86 73.00 88.16 Rolling Forcing (Ours) 59.62 74.76 78.84 31.32 22.51 30.59 23.78 30.52 23.52 20.04 20.41 19.45 23.16 24.80 24. 14 Semantic Score 65.85 69.17 69.78 Figure 6: Interactive Video Streaming. Rolling Forcing allows the users to change prompts while streaming to steer the video content. We conduct comprehensive evaluation on the full VBench benchmark (Huang et al., 2024), using all 946 prompts and covering all 16 metrics reported in Tables 3 and 4. For detailed metric definitions, we refer readers to the VBench paper. All values are computed with the official standardized evaluation scripts. Our method achieves substantial improvements in overall quality, particularly in frame-wise fidelity, and also outperforms distilled baselines on semantic scores. Indices of the 200 sampled MovieGen prompts. 0, 5, 10, 15, 24, 30, 34, 38, 44, 48, 53, 60, 67, 71, 75, 79, 84, 88, 92, 98, 103, 108, 112, 116, 122, 126, 131, 137, 142, 146, 150, 157, 165, 171, 176, 182, 188, 196, 200, 207, 211, 215, 219, 225, 229, 233, 237, 242, 246, 250, 256, 261, 267, 272, 277, 284, 288, 294, 299, 303, 308, 312, 317, 321, 327, 331, 336, 340, 344, 348, 353, 357, 362, 367, 372, 376, 380, 388, 392, 396, 400, 408, 415, 423, 428, 433, 437, 441, 446, 452, 456, 460, 464, 469, 473, 477, 482, 487, 491, 495, 502, 507, 511, 515, 521, 525, 529, 533, 540, 544, 548, 553, 558, 569, 574, 578, 585, 590, 598, 602, 609, 614, 619, 626, 632, 636, 641, 647, 651, 657, 661, 666, 671, 677, 681, 686, 690, 695, 699, 704, 708, 712, 717, 722, 726, 730, 734, 739, 743, 747, 752, 756, 761, 766, 772, 776, 781, 786, 791, 795, 799, 803, 808, 812, 816, 820, 825, 829, 834, 838, 845, 849, 855, 860, 865, 870, 875, 880, 884, 888, 892, 897, 904, 908, 915, 924, 928, 933, 937, 942, 946, 954, 959, 964, 970, 976, 980, 986, 991, 996."
        },
        {
            "title": "C INTERACTIVE VIDEO STREAMING",
            "content": "In Fig. 6, we demonstrate that Rolling Forcing enables interactive video streaming, allowing users to modify prompts during generation to steer the video content. Implementing this functionality is straightforward: we discard the cross-attention cache of previous text prompts and apply the new prompts in cross-attention."
        },
        {
            "title": "D DYNAMIC ROPE",
            "content": "The placement of RoPE indices for the global context frames is critical. Suppose the indices of the denoising window are i:i + 1, and the temporal context frames are Ltem:i1. We investigate several options for assigning indices to the global context frames: 1. immediately preceding the temporal context, Ltem Lglo:i Ltem1 (our adopted design); 2. fixed at 0:Lglo1 without dynamic RoPE adjustment; 3. overlapping with the temporal context, Lglo:i1; 15 4. within the denoising window, within i:i + 1; 5. after the denoising window, beyond + . Empirically, option 2 produces strong jumping artifacts due to relative positions exceeding the trained offset range, as shown in Fig. 7. Option 3 introduces flickering, as the model confuses global and temporal contexts. Option 4 collapses into static outputs, since the generated frames are forced to replicate the global context. Option 5 induces unnatural motion, as the model attempts to converge toward the misplaced global anchor. Among these, only option 1 yields consistent videos with minimal artifacts. Figure 7: Fixed RoPE indices produce strong jumping artifacts, where the video abruptly resets to the initial frame during streaming."
        },
        {
            "title": "E LIMITATIONS",
            "content": "While Rolling Forcing substantially suppresses error accumulation in real-time streaming video generation, several limitations remain. First, although the global context helps stabilize long-horizon consistency, frames generated in the middle are discarded once they leave the temporal context. As result, the model retains no memory of mid-sequence content, suggesting that incorporating more advanced memory mechanisms is promising direction for future exploration. Second, training Rolling Forcing is computationally demanding: the enlarged attention window and the DMD loss significantly increase GPU memory usage, which may limit scalability to higher-capacity models. Developing more efficient training or distillation strategies to mitigate these costs is therefore an important avenue for future work. Third, as mentioned in Huang et al. (2025), the rolling diffusion window may increase latency in interactive applications, as future frames are partially pre-generated before the current frame is finalized. As Rolling Forcing natively supports both inference strategies, future work may consider mixed inference strategy that interactively switches between frame-byframe denoising during interaction and rolling denoising otherwise."
        },
        {
            "title": "F BROADER SOCIETAL IMPACT",
            "content": "This work introduces real-time, long-horizon text-to-video generation, which can broaden access to interactive media, live storytelling, and educational tools by enabling continuous and responsive video synthesis. However, the ability to produce realistic long-duration content in real time also heightens risks of misuse, such as generating misleading live streams or amplifying harmful biases over extended outputs. We encourage future research to explore safeguards, including content filtering, bias mitigation, and responsible deployment practices, to ensure these capabilities are used for positive impact."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "Nanyang Technological University"
    ]
}