{
    "paper_title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection",
    "authors": [
        "Sungheon Jeong",
        "Jihong Park",
        "Mohsen Imani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 3 9 3 2 0 . 5 0 5 2 : r Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection Sungheon Jeong University of California, Irvine sungheoj@uci.edu Jihong Park MOLOCO qkrwlghddlek@gmail.com Mohsen Imani University of California, Irvine m.imani@uci.edu"
        },
        {
            "title": "Abstract",
            "content": "Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cueskey indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), framework that synthesizes event representations directly from RGB videos and fuses them with image features through principled, uncertainty-aware process. The system (i) models heavytailed sensor noise with Students-t likelihood, deriving value-level inversevariance weights via Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in deep learning have led to significant progress in multimodal data analysis [72, 38, 23, 54, 37, 26, 1], enabling systems to effectively integrate diverse sensory inputs for complex tasks such as video anomaly detection [17, 76, 60, 32, 14]. Traditionally, research in this area has predominantly focused on leveraging static image information, which provides rich spatial context. However, dynamic event datacapturing subtle motion dynamics, abrupt changes, and transient temporal patternsremains largely underexplored. Event data inherently offers high temporal resolution and can capture fleeting anomalies that static images might miss [19, 55, 63, 5]. Although the event modality has recently been integrated into multimodal learning frameworks [31, 82, 71], its practical potential remains underrealized, primarily due to the scarcity of real-world event datasets. In this work, we introduce to use synthetic event data extracted from videos to overcome this limitation. The challenge of effectively combining heterogeneous data sourcesstatic image information and dynamic event cuesunder conditions of uncertainty remains critical open problem [22, 77, 20]. Recent transformer-based fusion methods for video anomaly detection [16, 66, 83, 85, 12, 22] still focus on the rich spatial details in images and, as result, under-utilize the transient yet crucial temporal cues in event streams. Because multimodal attention mechanisms naturally gravitate toward the more expressive modality, this bias suppresses the complementary information that other inputs Preprint. Under review. could provide [53, 62, 41]. These limitations highlight the need for fusion strategy that can balance modality contributions under uncertainty. To address these challenges, we propose an approach that explicitly fuses the event modality with image data through an uncertainty-aware framework. Our Uncertainty-Weighted Image-Event Fusion framework for Video Anomaly Detection (IEF-VAD) balances the two modalities by assigning inverse-variance weights derived from Bayesian uncertainty estimates. By modelling each modalitys latent features alongside their predictive variance, IEF-VAD down-weights less reliable signals and prevents the image streamricher in spatial content but often dominantfrom overshadowing the temporally informative event cues. Concretely, we capture the heavy-tailed sensor noise of events with Students likelihood and obtain Gaussian approximation via Laplace method [47, 85, 11, 65], while drawing on established Bayesian techniques for uncertainty estimation [85, 58, 33, 49, 18]. The resulting uncertainty-weighted fusion dynamically modulates each modalitys contribution, allowing high-resolution temporal cues from events to complement, rather than be suppressed by, the detailed spatial context of images. Furthermore, our framework incorporates sequential update mechanism and an iterative refinement process. The sequential update merges each new imageevent observation with the prior fused state via inverse-variance (Kalman-gain) weights, closely mirroring the Kalman filter update step [64, 27]. Iterative refinement then targets the fine-grained residual errors that the fusion step cannot fully resolvesuch as feature-level mismatches, modality-specific noise, and minor scale imbalances that arise when combining two streams with different noise profiles. By repeatedly estimating and subtracting these residuals, the refinement network progressively denoises and re-balances the fused representation, yielding robust and cohesive latent state. Our contributions can be summarized as follows: Precision-Weighted Fusion Mechanism: Drawing inspiration from Bayesian inference and Kalman filtering, our framework employs inverse variance (precision) weighting to dynamically fuse image and event representations. This principled approach allows our model to prioritize the contribution of each modality based on its estimated uncertainty, addressing key limitations of prior fusion strategies that fail to adaptively account for heterogeneous uncertainty and often overly rely on dominant modalities. Practical Integration of Synthetic Event Data: We present the successful use of synthetic event streams extracted from conventional videos, addressing the shortage of real-world event datasets. Our results demonstrate that this approach enables the integration of event modality into virtually any video-based dataset, substantially broadening the scope and applicability of multimodal anomaly detection. Empirical Validation on Real-World Datasets: We validate our approach on four major real-world video anomaly detection datasetsUCF-Crime, XD-Violence, ShanghaiTech, and MSADachieving AUC scores of 88.67%, 87.63%, 97.98%, and 92.90%, respectively, surpassing state-of-the-art methods on each benchmark. Our results further demonstrate that integrating event modality enables the detection of motion-centric and transient anomalies that static-image-based approaches often fail to capture."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Data Fusion: Multimodal fusion aims to integrate complementary signals from diverse modalities such as vision, audio, and language [72, 87, 29]. Early approaches relied on simple feature concatenation [48], but recent advances have increasingly leveraged transformer-based architectures [62, 1, 24, 36] and contrastive pretraining [54, 23, 10] to capture complex cross-modal dependencies. Although attention-based fusion methods offer fine-grained alignment [38, 36], they often suffer from modality dominance, where one modality disproportionately influences the joint representation [72, 87]. In response, emerging works have explored using large language models (LLMs) to dynamically modulate modality usage based on context [56, 78, 25, 35, 13], thereby highlighting the need for adaptive and selective integration. Building on this direction, our work emphasizes uncertainty as an explicit control signal for fusion, enabling robust integration under modality imbalance and real-world noise conditions. 2 Utilization of Event-Based Data: Recent neuromorphic sensors and datasets [34, 50, 39] have renewed interest in the event modality, characterized by its asynchronous, sparse, and high-temporalresolution nature that captures only salient changes [19, 79, 5, 4]. This distinctive property has been exploited across wide range of tasks [57, 73, 44, 9, 52, 55], with recent methods leveraging CLIP [54] to achieve effective modality alignment between image and event data [71, 82, 31]. This alignment demonstrates the capability to process paired image and event inputs, thereby enabling the extraction of synthetic event data from videos that capture the motion cues essential for anomaly detection [31]. Video Anomaly Detection: The video anomaly detection literature has witnessed significant shifts towards weakly-supervised learning frameworks driven by multiple instance learning (MIL) strategies [59, 66, 80, 6, 46]. Recent transformer-based models, which predominantly rely on image-based representations, aim to detect subtle, transient anomalies in long video streams, yet emerging work suggests that incorporating richer temporal information can greatly improve performance [42, 21, 3]. In this regard, several recent studies have begun exploring the integration of additional modalities, including text inputs via large language models (LLMs), to provide extra contextual information that can aid temporal understanding [2, 13, 78, 74, 75, 45]. In the context of modality integration, the extraction of synthetic event data from video [31, 55, 84, 3] has emerged as promising method for enriching temporal cues, focusing on capturing motion-centric signals rather than static background details. This refined focus enables models to better attend to the salient dynamics of the scene, thereby enhancing the detection of critical anomalous behavior. Uncertainty Estimation & Bayesian Fusion: In parallel with advances in multimodal fusion, there has been growing trend in incorporating uncertainty estimation directly into deep learning frameworks [33, 18, 51]. Recent works utilize Bayesian principles to quantify prediction uncertainty, thereby enabling models to weigh each modalitys contribution based on confidence measures [58, 47]. Approaches leveraging Monte Carlo dropout [18] and precise inverse variance weighting have been reported [58, 11], demonstrating improved performance in challenging conditions [51]. Our method adopts Bayesian fusion framework where the inverse variancecomputed via Laplace approximation over Students noise modelis used to weight the fusion of image and event features [47, 65]. This not only aligns with recent trends but also provides principled mechanism to enhance robustness, especially in the presence of heavy-tailed noise [65]."
        },
        {
            "title": "3 Uncertainty-Weighted Fusion for Multimodal Learning",
            "content": "Figure 1: Overview of IEF-VAD framework. Each video frame and its corresponding synthetic event representation are processed by CLIP encoders to obtain feature embeddings zm. These are further encoded by modality-specific transformers fm to produce ˆzm, which are then passed through projection heads gm and hm to estimate µm and σm. The estimated σm is used to compute the uncertainty-aware fusion weight wm, which is used to obtain the initial fused representation µt,0 . This is refined over iterative steps through refinement network to produce the final output µt,N . In this work, we extract image and event embeddings (zx, ze) separately from videos, assuming that both modalities observe the same underlying scene and share common spatial structure while exhibiting complementary expressive and modality-specific features. zx aggregates complex attributes such as color, background, and spatial details, whereas ze encapsulates transient changes and temporal 3 dynamics. To effectively combine these heterogeneous representations, we propose fusion strategy that integrates Bayesian uncertainty estimation with Kalman filter update principles, where uncertainty quantifies the reliability of each value-level feature. Specifically, we estimate latent features and their associated uncertainties for each modality, and subsequently fuse them through refinement network designed to iteratively correct residual noise, misalignments, and modality-specific artifacts, yielding robust final representation µf . 3.1 Modality Design and Uncertainty Estimation We design each modality as noisy observation influenced by both shared scene content and modalityspecific factors. zm = µm + δm, δm tν(0, Σm), {x, e}, where µm Rd is the central estimate and Σm = diag(σ2 m) encodes value-level uncertainty. We choose the heavy-tailed Students-t distribution because purely Gaussian model underestimates uncertainty in the presence of outliers; its thicker tails make the fusion rule more conservative when the input is degraded (see Appendix A). Given embedding sequences zm RBT D, we transpose them to (T, B, D), pass each through modality-specific transformer fm, and transpose back, yielding ˆzm = (fm(z Layer normalization aligns scales across modalities. This ensures that the downstream uncertainty estimation operates on comparable feature scales, improving numerical stability when computing variances. Linear projection heads gm, hm then predict the posterior mean and log-variance: m)). µm = gm(ˆzm), log σ2 = hm(ˆzm). (1) Predicting log σ2 guarantees positivity and numerical stability. Conceptually, zm is sampled from the posterior tν(µm, Σm), where µm represents the fused central tendency and σ2 quantifies epistemic uncertainty. These uncertainty estimates drive the Kalman-style update and the subsequent refinement loop, enabling dynamic, reliability-aware fusion that adapts to modality quality in real time. 3.2 Inverse Variance Calculation In this stage, our goal is to quantify the confidence in each modalitys predictions by computing the inverse variance. Recall that our model predicts the log-variance values (Eq. 1), which are then exponentiated to recover the variance. In the case of Gaussian noise model, this is given by (cid:1). For the Students noise model, we first apply the Laplace approximation to = exp (cid:0) log σ2 σ2 obtain the effective variance (see Appendix B). The Students probability density function (up to normalization constant) is given by Eq. 2, where σ2 is the variance parameter for the underlying Gaussian scale, and ν is the degree of freedom. (cid:18) p(δ) 1 + δ2 νσ (cid:19) ν+1 2 , log p(δ) = ν + 1 2 (cid:18) log 1 + (cid:19) δ2 νσ2 + C. (2) Since the mode of the distribution occurs at δ = 0, we perform second-order Taylor expansion of the logarithm around δ = 0. For small x, recall that log(1 + x) for 1. In our case, set = δ2/(νσ2). Then, for small δ, and log-density we have (cid:18) log 1 + (cid:19) δ2 νσ δ2 νσ2 , log p(δ) ν + 1 2 δ2 νσ2 + = ν + 1 2νσ2 δ2 + C. The Laplace approximation approximates probability density near its mode by Gaussian distribution. The log-density of Gaussian with variance σ2 is given by log pG(δ) = 1 2σ2 δ2 + C. By matching the quadratic terms in the Taylor expansion, we set 2νσ2 . This immediately implies σ2 = ν ν+1 σ2. Taking the logarithm of both sides gives 2σ2 = ν+1 1 log σ2 = log σ2 + log (cid:18) ν ν + (cid:19) . (3) This derivation shows that, under the Laplace approximation, the effective variance used in downν ν+1 , reflecting the heavy-tailed nature of the Students stream computations is scaled by the factor 4 distribution. This effective variance is then used in place of the original variance σ2 when computing inverse variance weights and other related measures, ensuring that the fusion process properly accounts for the increased uncertainty due to heavy-tailed noise. The variance (or effective variance in the Students case) represents the uncertainty associated with the prediction; lower values indicate higher confidence. To leverage this notion of confidence in manner consistent with Bayesian principles, we compute the inverse variance as measure of precision. Specifically, for the Students model, we use the effective variance: wm = 1 σ2 + ϵ . (4) where ϵ is small positive constant added for numerical stability, ensuring that we do not encounter division by zero. These weights, wx and we, essentially serve as confidence scoresmodalities with lower uncertainty (i.e., lower σ2 or σ2) yield higher weights and, consequently, contribute more significantly in subsequent fusion steps. This approach is theoretically grounded in Bayesian inference (detailed in Appendix C), where the inverse variance (or precision) is used to weight the contributions of different measurements according to their reliability. As result, by explicitly modeling and incorporating the inverse varianceor the effective inverse variance under the Students assumptionthe fusion process becomes more robust, effectively balancing the contributions of each modality based on their estimated uncertainties. 3.3 Uncertainty-Weighted Fusion We compute the fused representation µf RBT by taking weighted average of the modalityspecific means according to their computed confidence scores Eq. 4. µf = wxµx + weµe wx + we In the case of the Students distribution, the inverse variance weights are computed based on the effective variances obtained via Laplace approximation around the mode of the distribution. Specifically, by applying the logarithm to the effective variance as derived in Eq. 3, and then exponentiating, we have (cid:18) σ2 = exp log σ2 + log (cid:18) ν (cid:19)(cid:19) ν + Thus, in both cases, modalities with lower uncertainty (i.e., lower variance or effective variance) yield higher precision scores and contribute more significantly in the fusion process. The rationale behind this formulation is twofold. First, by weighting each modalitys mean by its (effective) inverse variance, we ensure that modalities with higher confidence have greater impact on the final fused representation. This is direct application of Bayesian fusion principles, where the posterior estimate of latent variable is precision-weighted average of the individual estimates. Second, this fusion strategy closely mirrors the update step in the Kalman filtera well-established method for sequential data fusionwhere the Kalman gain, derived from the inverse variances, dictates the contribution of each measurement in updating the state estimate, as further detailed in Appendix C. 3.4 Time Step Sequential Update Building upon our uncertainty-weighted fusion framework (Figure 1), we extend the method to handle the time steps (T ) by incorporating temporal dependencies in sequential update process. The intuition is analogous to the recursive estimation in Kalman filtering, except that here we account for the heavy-tailed nature of the Students noise via Laplace approximation. Under the Students model, the predicted variance is corrected to obtain an effective variance. Specifically, if the predicted log-variance is log σ2 at time t, the effective variance and state variance at the previous time step are given by σ2 = exp (cid:18) log σ2 + log (cid:18) ν (cid:19)(cid:19) ν + (5) 5 = µ0, f,0 = σ2 σ2 At the initial time step (t = 0), we set the state and its effective uncertainty directly from the first input: µ0 0. For each subsequent time step (t 1), we update the state estimate by fusing the previous state with the current input. To do so, we compute the inverse effective variance weights. The weight for the previous state is given by wt1 = 1/(σ2 f,t1 + ϵ) and the weight for the current input is wt = 1/(σ2 + ϵ), where ϵ is small constant for numerical stability. The updated state and its effective uncertainty are computed as: µt = wt1 µt1 wt1 + wtµt + wt , σ2 f,t = 1 wt1 + wt , 1. This sequential update process naturally extends the static fusion methodology by incorporating temporal continuity, using effective variances that account for the heavy-tailed nature of the Students noise. Just as in the static casewhere each modality is weighted according to its (effective) precisionthe sequential update fuses the previous state with new observations based on their relative effective uncertainties. This approach enhances the robustness and consistency of the state estimates over time, effectively capturing both current observations and historical context. 3.5 Iterative Refinement of Fused State After performing sequential fusion based on uncertainty-weighted averaging of multimodal latent representations, residual noise and subtle discrepancies between modalities inevitably persist in the fused latent state. Specifically, since each modality introduces distinct noise profiles and uncertainty estimates, their combination inherently results in microscopic residual errors. To systematically address these fusion-induced residuals, we introduce an iterative refinement procedure, inspired by iterative denoising methods such as the Denoising Diffusion Probabilistic Model [28]. Formally, starting from an initial fused state µ0 obtained after the sequential uncertainty-weighted update, we iteratively predict and subtract the residual error. At each refinement iteration (r = 0, 1, . . . , ), dedicated refinement network () estimates the residual error µt,r = (µt,r ) based solely on the current fused state. The estimated residual represents the remaining discrepancy between the current fused representation and the underlying true latent state. To ensure stable and progressive refinement without overshooting, the state is updated using fixed attenuation parameter λr (0, 1): µt,r+1 = µt,r λrµt,r . This attenuation parameter modulates the magnitude of each residual correction, preventing overcorrection and ensuring stable convergence. As iterative refinement progresses, residual errors decrease monotonically, gradually aligning the fused latent representation closer to the underlying true latent posterior distribution. Thus, iterative refinement serves as theoretically and empirically justified mechanism to denoise and enhance the multimodal fused representation, yielding more precise and reliable latent embedding suitable for subsequent downstream tasks (detailed in Appendix D). 3.6 Video Anomaly Detection with Loss Functions Classification Loss (Lcls): After obtaining the final refined fused representation µt,N RBT D, we perform binary classification at each time step to detect anomalies. Specifically, we apply lightweight classification head (H) to µt,N to produce logits: The logits are then passed through sigmoid activation to obtain anomaly probabilities and binary cross-entropy loss Lcls is computed against the ground-truth labels. ˆy = H(µt,N ) RBT 1. Lcls = 1 BT (cid:88) (cid:88) b= t=1 [yb,t log(ˆyb,t) + (1 yb,t) log(1 ˆyb,t)] , To accommodate variability in temporal granularity and enable weakly supervised learning without requiring precise frame-level annotations, we adopt segment-based aggregation strategy following [59]. Specifically, the temporal sequence is divided into non-overlapping segments of 16 frames. 6 Let length denote the number of valid time steps. Then, the number of segments is computed as (cid:106) length = + 1, and the predictions within each segment are averaged to yield an instance-level 16 prediction [30, 59]. (cid:107) KL Divergence Loss (LKL): Under the Students noise model, each modalitys measurement noise is assumed to follow δm tν(0, σ2 m), where ν denotes the degrees of freedom. Direct computation of the KL divergence between Students distribution and the standard normal prior is intractable due to the heavy-tailed nature of the distribution. To address this, we leverage the effective variance σ2 Eq. 5 obtained via Laplace approximation (Eq. 3) and approximate the latent distribution as Gaussian (µm, σ2 m). The resulting closed-form KL divergence with respect to (0, I) is given by (see Appendix for detailed derivation): LKL = KL(cid:0)N (µm, σ2 m)N (0, I)(cid:1) = 1 2 (cid:0)σ2 + µ 1 log σ2 (cid:1) This regularization encourages the latent distribution to remain close to the prior, providing robustness even under heavy-tailed noise conditions. Regularization Loss (Lreg): To ensure that the latent representations from both modalities are aligned both in direction and magnitude, we introduce regularization loss. This term comprises two components: one that minimizes the angular difference between µx and µe (using cosine similarity), and another that penalizes discrepancies in their norms. Formally, it is defined as Lreg = λ1(1 cos(µx, µe)) + λ2µx µe where λ1 and λ2 are hyperparameters that balance the contributions of the two terms. Lreg promotes shared latent space across modalities by jointly regularizing the direction and magnitude of their embeddings. The cosine term enforces semantic alignment by minimizing angular deviations, while the norm consistency term balances embedding scales to prevent modality domination. Together, these constraints enable stable and coherent multimodal fusion. Overall Loss: The final loss function is sum of the classifier loss, the KL divergence losses for both modalities, and the regularization loss: = Lcls + (cid:88) Lm KL + Lreg (6) This composite loss ensures that the model not only produces accurate binary predictions at each time step but also learns latent representations that are both robust and well-regularized. The use of the effective variance derived via Laplaces approximation allows us to maintain closed-form KL divergence expression, thereby combining the robustness of the Students model with the computational efficiency of Gaussian-based methods. An ablation study on the loss components is provided in Appendix G."
        },
        {
            "title": "4 Experiment Results",
            "content": "We evaluate our method on four public video anomaly detection datasetsUCF-Crime [59], XDViolence [66], ShanghaiTech [40], and MSAD [86]covering diverse real-world surveillance scenarios. Following standardized preprocessing and independent transformer encoding for each modality (see Appendix F), we perform evaluations using AUC and AP metrics. Ablation studies presented in Appendix analyze the sensitivity of performance to key hyperparameters (ν, , λref, and ϵ) and investigate the effects of loss configuration choices (Eq. 6). Robustness to outlier injection and the behavior of uncertainty weights under perturbations are evaluated, with additional uncertainty-aware metrics, including KL divergence and Brier score, reported in Appendix H. Our method consistently demonstrates strong performance by effectively leveraging complementary spatial and temporal cues from image and event modalities in weakly supervised setting. 4.1 Real-World Anomaly Detection in Surveillance Video Table 1 shows that IEF-VAD outperforms all prior weakly supervised detectors on every benchmark. On UCF-Crime [59], the Gaussian variant already matches the best published AUC (88.11%), while 7 Method Sultani et al. [59] Wu et al. [66] AVVD [67] RTFM [61] UR-DMU [81] UMIL [46] VadCLIP [70] STPrompt [69] OVVAD [68] UCF-Crime [59] AUC (%) Ano-AUC (%) XD-Violence [66] AP (%) Shanghai-Tech [40] AUC (%) Method MSAD [86] AUC (%) 84.14 84.57 82.45 85.66 86.97 86.75 88.02 88.08 86.40 63.29 62.21 60.27 63.86 68.62 68.68 70.23 - - 75.18 80.00 - 78.27 81.66 - 84.51 - 66.53 91.72 95.24 - 97.21 97.57 96.78 97.49 97.81 96.98 MIST (I3D) [15] MIST (SwinT) [15] UR-DMU [81] UR-DMU (SwinT) [81] MGFN (I3D) [8] MGFN (SwinT) [8] TEVAD (I3D) [7] TEVAD (SwinT) [7] EGO [12] 86.65 85.67 85.02 72.36 84.96 78.94 86.82 83.6 87.36 70.48 0.66 71.50 1.02 88.11 0.28 88.67 0.45 92.27 0.34 IEF-VAD (Gaussian) 92.90 0.27 IEF-VAD (Student-T) Table 1: Comparison of various methods on multiple anomaly detection benchmarks, including UCF-Crime [59], XD-Violence [66], ShanghaiTech [40], and MSAD [86]. All metrics are reported as the mean (1 standard deviation) of 10 runs. Our approach (IEF-VAD) consistently achieves the highest performance across datasets, demonstrating its robustness for video anomaly detection. IEF-VAD (Gaussian) IEF-VAD (Student-T) 97.91 0.08 97.98 0. 87.18 0.55 87.63 0.54 the Students extension lifts AUC to 88.67% and raises the anomaly-focused AUC (Ano-AUC) to 71.50%a 1.3 pp absolute gain over the previous record (70.23% of VadCLIP [70]). Similar trends appear on XD-Violence [66] (87.63 AP) and ShanghaiTech [40] (97.98 AUC), where both variants surpass the strongest competitors. On the more recent MSAD [86] benchmark, our Studentst model achieves 92.90 AUC, exceeding the best I3D-based baseline (86.82 AUC) by over 6 pp. Standard-deviation margins indicate that gains are statistically consistent across 10 independent runs. These results confirm two key insights of our framework. First, value-level uncertainty weighting enables practical exploitation of the heterogeneous synthetic-event modality, turning its complementary cues into measurable performance gains whenever the RGB stream is degraded. Second, modelling heavy-tailed noise via Students-t likelihood yields further, systematic improvements, especially on long-tailed datasets (UCF-Crime [59], XD-Violence [66]) where RGB frames frequently contain motion blur or illumination changes. The consistent gains across four diverse datasetswith no task-specific tuningunderscore the generality of uncertainty-aware fusion and highlight its promise for real-world surveillance settings in which sensor quality and scene dynamics vary unpredictably. Figure 2: Radar charts showing per-class anomaly detection performance (AUC and AP) for image-only (blue), event-only (orange), and fused (green) approaches. Each radial axis represents an anomaly category, and values are normalized per class by the maximum score. The fused approach consistently covers larger area, highlighting improved detection across anomaly types. These trends are further illustrated in Figure 2, which provides class-wise comparison of image-only, event-only, and fused approaches. In most anomaly classes, the image-based modality achieves higher performance than the event-based modality, reflecting the substantial differences in the underlying information each modality encodes. However, in anomaly categories that align more closely with the characteristics of ze (e.g., Fighting, Assault in UCF-Crime [59]), the event-based modality outperforms the image-based one, confirming our initial intuition (Figure 2). By fusing zx and ze, our proposed method consistently achieves higher detection accuracy than either single modality alone. In particular, this fusion harnesses the strengths of image while absorbing the advantages of event for those classes where it excels, leading to further performance gains in categories already well-handled by image. Consequently, as illustrated in Figure 2, the fused approach covers broader area in the radial plots, surpassing the capacity of single-modality baselines in most anomaly classes (see Appendix for detailed numbers). 4.2 Fusion: Uncertainty Behavior Under Value-Level Masking Figure 3: Change in image-side uncertainty weights wx under modality-specific masking perturbations across four datasets. The horizontal axis denotes the latent dimension index, and the vertical axis ρ indicates the proportion of values in zx or ze that are masked to zero. Each surface visualizes wx(i, ρ) = wmasked (i) for given masking ratio. The top row corresponds to masking applied to zx (image modality), while the bottom row applies masking to ze (event modality), with both measuring the resulting change in wx. Positive values (blue) indicate increased confidence in the image modality under corruption, while negative values (red) reflect reduction. The non-uniform patterns across highlight dimension-specific responses to value-level degradation. (i) wclean To investigate how the uncertainty weights wm respond to modality-specific degradation, we perturb single modality by masking random subset of its latent features zm RBT D. masked sequence is defined as zmasked m,i = (cid:40)0, Iρ zm,i, otherwise , Iρ = ρD. where ρ (0, 1) denotes the masking ratio and indexes feature dimensions. We then measure the change in the image-side uncertainty weight, wx = wmasked , with the normalisation wx + we = 1 enforced. We prefer masking over additive noise because it produces deterministic, localised degradation that preserves the Student-t noise assumption and avoids the non-linear propagation artifacts that Gaussian perturbations introduce in attention layers, yielding cleaner causal probe of the learned uncertainty mechanism. wclean Figure 3 plots wx for three masking ratios (ρ 0.05, 0.10, 0.20) on four datasets. The top row masks zx, consistently decreasing wxindicating higher image uncertaintywhereas the bottom row masks ze, symmetrically increasing wx. The magnitude of wx grows with ρ, showing that the fusion network scales its confidence shift with corruption severity. Value-level curves expose pronounced heterogeneity: some dimensions (e.g., indices 1227) react strongly, whereas others remain flat. We attribute this to distributed encoding: dimensions dominated by stable appearance cues are robust, while those capturing transient, modality-specific dynamics are fragile. The non-uniform yet directionally consistent responses provide empirical evidence that the model estimates uncertainty on fine-grained basis rather than collapsing it into scalar. Supplementary statistics, including KL divergence and Brier score, are reported in Appendix H."
        },
        {
            "title": "5 Conclusion",
            "content": "IEF-VAD proves that motion-centric synthetic event streams distilled from ordinary videos can be fused with RGB features to push video-anomaly detection beyond the limits of frame-based models. 9 The approach establishes new state-of-the-art scores on UCF-Crime, XD-Violence, ShanghaiTech, and MSAD, while value-level masking study shows its precision weights shift adaptively across latent dimensionsmaking the fusion logic transparent and reliable. We expect these findings to catalyse broader use of synthetic event data and to drive future work on uncertainty-aware multimodal learning for real-time surveillance. Limitations and Future Work. While our study advances uncertainty-guided fusion, several limitations invite further investigation. First, approximating each modalitys noise with diagonal covariance neglects cross-feature correlations; future work could adopt lightweight structured or low-rank covariance estimators to enrich value-level fusion. Second, the fixed regularization weights λ1,2 and Student-t degrees-of-freedom ν limit adaptability; jointly learning or meta-learning these parameters would enable the model to recalibrate trust as modality quality shifts. Third, replacing simple weight averaging with uncertainty-conditioned cross-modal attention, hierarchical gating, or iterative message passing may yield more expressive fusion mechanism that leverages inter-modal disagreement as informative signal, paving the way for stronger anomaly detection and broader multimodal perception capabilities."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the DARPA Young Faculty Award, the National Science Foundation (NSF) under Grants #2127780, #2319198, #2321840, #2312517, and #2235472, and by the Semiconductor Research Corporation (SRC). Additional support was provided by the Office of Naval Research through the Young Investigator Program Award and Grants #N00014-21-1-2225 and #N00014-22-1-2067, as well as the Army Research Office under Grant #W911NF2410360. This research was also supported by the Air Force Office of Scientific Research under Award #FA9550-221-0253, and through generous gifts from Xilinx and Cisco."
        },
        {
            "title": "References",
            "content": "[1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in neural information processing systems, 34:2420624221, 2021. [2] Jean-Baptiste Alayrac, Jeff Donahue, et al. Flamingo: visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. [3] Marcella Astrid, Muhammad Zaigham Zaheer, and Seung-Ik Lee. Synthetic temporal anomaly guided end-to-end video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2021. [4] Dario Cazzato and Flavio Bono. An application-driven survey on event-based neuromorphic computer vision. Information, 15(8):472, 2024. [5] Bharatesh Chakravarthi, Aayush Atul Verma, Kostas Daniilidis, Cornelia Fermuller, and Yezhou Yang. Recent event camera innovations: survey. arXiv preprint arXiv:2408.13627, 2024. [6] Junxi Chen, Liang Li, Li Su, Zheng-Jun Zha, and Qingming Huang. Prompt-enhanced multiple In Proceedings of the instance learning for weakly supervised video anomaly detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [7] Weiling Chen, Keng Teck Ma, Zi Jian Yew, Minhoe Hur, and David Aik-Aun Khoo. Tevad: Improved video anomaly detection with captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 55495559, 2023. [8] Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton Fok, Xiaojuan Qi, and Yik-Chung Wu. Mgfn: Magnitude-contrastive glance-and-focus network for weakly-supervised video anomaly detection. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 387395, 2023. [9] Hoonhee Cho, Hyeonseong Kim, Yujeong Chae, and Kuk-Jin Yoon. Label-free event-based object recognition via joint learning with image reconstruction from events. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1986619877, 2023. [10] Yong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang, Zhangyin Feng, Fan Zhang, Xueyu Hu, and Shuming Shi. One model, multiple modalities: sparsely 10 activated approach for text, sound, image, video and code. arXiv preprint arXiv:2205.06126, 2022. [11] Erik Daxberger et al. Laplace reduxeffortless bayesian deep learning. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [12] Dexuan Ding, Lei Wang, Liyun Zhu, Tom Gedeon, and Piotr Koniusz. Learnable expansion of graph operators for multi-modal feature fusion, 2025. [13] Danny Driess, Fei Xia, Siddhartha Srinivasa, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. [14] Chao Feng, Ziyang Chen, and Andrew Owens. Self-supervised video forensics by audio-visual anomaly detection. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1049110503, 2023. [15] Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. Mist: Multiple instance self-training In Proceedings of the IEEE/CVF conference on framework for video anomaly detection. computer vision and pattern recognition, pages 1400914018, 2021. [16] Xinyang Feng, Dongjin Song, Yuncong Chen, Zhengzhang Chen, Jingchao Ni, and Haifeng Chen. Convolutional transformer based dual discriminator generative adversarial networks for video anomaly detection. arXiv preprint arXiv:2107.13720, 2021. [17] Alessandro Flaborea, Luca Collorone, Guido Maria DAmely Di Melendugno, Stefano DArrigo, Bardh Prenkaj, and Fabio Galasso. Multimodal motion conditioned diffusion model for skeletonbased video anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1031810329, 2023. [18] Yarin Gal and Zoubin Ghahramani. Dropout as bayesian approximation: Representing model uncertainty in deep learning. International Conference on Machine Learning (ICML), 2016. [19] Guillermo Gallego, Tobi Delbrück, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jörg Conradt, Kostas Daniilidis, et al. EventIEEE transactions on pattern analysis and machine intelligence, based vision: survey. 44(1):154180, 2020. [20] Daniel Gehrig, Henri Rebecq, Guillermo Gallego, and Davide Scaramuzza. End-to-end learning of representations for asynchronous event-based data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 56335643, 2019. [21] Mihai Georgescu, Radu Ionescu, et al. Anomaly detection in video via self-supervised and multi-task learning. In CVPR, 2021. [22] Ayush Ghadiya, Purbayan Kar, Vishal Chudasama, and Pankaj Wasnik. Cross-modal fusion and attention mechanism for weakly supervised video anomaly detection. arXiv preprint arXiv:2412.20455, 2024. [23] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518015190, 2023. [24] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens Van Der Maaten, Armand Joulin, and Ishan Misra. Omnivore: single model for many visual modalities. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1610216112, 2022. [25] Zixiang Gong, Shuohang Li, Yuning Shao, et al. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [26] Wenzhong Guo, Jianwen Wang, and Shiping Wang. Deep multimodal representation learning: survey. Ieee Access, 7:6337363394, 2019. [27] Tuomas Haarnoja, Pieter Abbeel, and Sergey Levine. Backprop kf: Learning discriminative deterministic state estimators. In Advances in Neural Information Processing Systems (NeurIPS), 2016. [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [29] Keli Huang, Botian Shi, Xiang Li, Xin Li, Siyuan Huang, and Yikang Li. Multi-modal sensor fusion for auto driving perception: survey. arXiv preprint arXiv:2202.02703, 2022. 11 [30] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In International conference on machine learning, pages 21272136. PMLR, 2018. [31] Sungheon Jeong, Hanning Chen, Sanggeon Yun, Suhyeon Cho, Wenjun Huang, Xiangjian Liu, and Mohsen Imani. Expanding event modality applications through robust clip-based encoder. arXiv preprint arXiv:2412.03093, 2024. [32] Tianchen Ji, Sri Theja Vuppala, Girish Chowdhary, and Katherine Driggs-Campbell. MultiarXiv preprint modal anomaly detection for unstructured and uncertain environments. arXiv:2012.08637, 2020. [33] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems (NeurIPS), 2017. [34] Junho Kim, Jaehyeok Bae, Gangin Park, Dongsu Zhang, and Young Min Kim. N-imagenet: Towards robust, fine-grained object recognition with event cameras. In Proceedings of the IEEE/CVF international conference on computer vision, pages 21462156, 2021. [35] Junnan Li, Hexiang Hu, Xiang Shen, et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. International Conference on Learning Representations (ICLR), 2023. [36] Xiang Li, Xiang Zhang, Hang Xu, Zhen Lan, Jing Sun, Jianmin Wang, and Guo-Jun Qi. Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS, 2021. [37] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [38] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [39] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck. 128128 120 db 15µs latency asynchronous temporal contrast vision sensor. IEEE journal of solid-state circuits, 43(2):566 576, 2008. [40] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detectiona new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 65366545, 2018. [41] Yixuan Liu, Linchao Zhang, Yabiao Wang, Ying Wang, Gang Wang, and Hongsheng Li. Towards multimodal model generalization: Visual-audio temporal alignment for video classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097010979, 2021. [42] Yuchen Liu, Yumin Tian, Yanning Zhang, and Kai Chen. Weakly supervised temporal anomaly localization in surveillance videos. In ACM MM, 2021. [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [44] Xinglong Luo, Kunming Luo, Ao Luo, Zhengning Wang, Ping Tan, and Shuaicheng Liu. Learning optical flow from event camera with rendered dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 98479857, 2023. [45] Hui Lv and Qianru Sun. Video anomaly detection and explanation via large language models. arXiv preprint arXiv:2401.05702, 2024. [46] Hui Lv, Zhongqi Yue, Qianru Sun, Bin Luo, Zhen Cui, and Hanwang Zhang. Unbiased multiple instance learning for weakly supervised video anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80228031, 2023. [47] Magnus Malmström, Isaac Skog, Daniel Axehill, and Fredrik Gustafsson. Fusion framework and multimodality for the laplacian approximation of bayesian neural networks. arXiv preprint arXiv:2310.08315, 2023. [48] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. Multimodal deep learning. In ICML, pages 689696, 2011. 12 [49] Sebastian Ober, Christoph Posch, Max Welling, Yarin Gal, and Fabio Cuzzolin. The promises and pitfalls of bayesian deep learning in computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [50] Garrick Orchard, Ajinkya Jayawant, Gregory Cohen, and Nitish Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. Frontiers in neuroscience, 9:437, 2015. [51] Yaniv Ovadia and et al. Can you trust your models uncertainty? evaluating predictive uncertainty under dataset shift. In NeurIPS, 2019. [52] Federico Paredes-Vallés and Guido CHE De Croon. Back to event basics: Self-supervised learning of image reconstruction for event cameras via photometric constancy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34463455, 2021. [53] Kyunghyun Park et al. Assessing modality bias in video question answering benchmarks with multimodal large language models. arXiv preprint arXiv:2408.12763, 2024. [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [55] Henri Rebecq, René Ranftl, Vladlen Koltun, and Davide Scaramuzza. Events-to-video: Bringing modern computer vision to event cameras. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38573866, 2019. [56] Sheng Shen, Shijie Tang, Di Niu, and et al. Multimodal foundation models: From specialists to general-purpose learners. arXiv preprint arXiv:2306.05425, 2023. [57] Shintaro Shiba, Yoshimitsu Aoki, and Guillermo Gallego. Secrets of event-based optical flow. In European Conference on Computer Vision, pages 628645. Springer, 2022. [58] Mahesh Subedar, Ranganath Krishnan, Paulo Lopez Meyer, Omesh Tickoo, and Jonathan Huang. Uncertainty aware audiovisual activity recognition using deep bayesian variational inference. arXiv preprint arXiv:1811.10811, 2018. [59] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 64796488, 2018. [60] Jiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and Yingcong Chen. Hawk: Learning to understand open-world video anomalies. Advances in Neural Information Processing Systems, 37:139751139785, 2024. [61] Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 49754986, 2021. [62] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 65586569, 2019. [63] Zuowen Wang, Yuhuang Hu, and Shih-Chii Liu. Exploiting spatial sparsity for event cameras with visual transformers. In 2022 IEEE International Conference on Image Processing (ICIP), pages 411415. IEEE, 2022. [64] Greg Welch and Gary Bishop. An introduction to the kalman filter. University of North Carolina at Chapel Hill, Department of Computer Science, 7(1):116, 1995. [65] Jianxiang Wu, Kailun Ren, Yisen Wang, Xiaotong Liu, and Jinfeng Wang. Student-t processes for bayesian deep learning. In International Conference on Learning Representations (ICLR), 2021. [66] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXX 16, pages 322339. Springer, 2020. 13 [67] Peng Wu, Xiaotao Liu, and Jing Liu. Weakly supervised audio-visual violence detection. IEEE Transactions on Multimedia, 25:16741685, 2022. [68] Peng Wu, Xuerong Zhou, Guansong Pang, Yujia Sun, Jing Liu, Peng Wang, and Yanning Zhang. Open-vocabulary video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1829718307, 2024. [69] Peng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, and Yanning Zhang. Weakly supervised video anomaly detection and localization with spatiotemporal prompts. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 93019310, 2024. [70] Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. Vadclip: Adapting vision-language models for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 60746082, 2024. [71] Ziyi Wu, Xudong Liu, and Igor Gilitschenski. Eventclip: Adapting clip for event-based object recognition. arXiv preprint arXiv:2306.06354, 2023. [72] Peng Xu, Xiatian Zhu, and David Clifton. Multimodal learning with transformers: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1211312132, 2023. [73] Yan Yang, Liyuan Pan, and Liu Liu. Event camera data pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1069910709, 2023. [74] Muchao Ye, Weiyang Liu, and Pan He. Explainable video anomaly detection via verbalized learning of vision-language models. arXiv preprint arXiv:2412.01095, 2024. [75] Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, and Elisa Ricci. Harnessing large language models for training-free video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [76] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Xiaonan Huang, Changxin Gao, Shanjun Zhang, Li Yu, and Nong Sang. Holmes-vau: Towards long-term video anomaly understanding at any granularity. arXiv preprint arXiv:2412.06171, 2024. [77] Yuhuang Zhang, Guillermo Gallego, Davide Scaramuzza, et al. Event-based vision: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):154180, 2022. [78] Wayne Zhao, Xisen Wang, and et al. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. [79] Xu Zheng, Yexin Liu, Yunfan Lu, Tongyan Hua, Tianbo Pan, Weiming Zhang, Dacheng Tao, and Lin Wang. Deep learning for event-based vision: comprehensive survey and benchmarks. arXiv preprint arXiv:2302.08890, 2023. [80] Zheng Zhong, Wenhao Li, Xiaojun Zhao, et al. Gtad: semi-supervised learning framework for temporal anomaly detection in surveillance videos. In ECCV, 2022. [81] Hang Zhou, Junqing Yu, and Wei Yang. Dual memory units with uncertainty regulation for weakly supervised video anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 37693777, 2023. [82] Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. Eventbind: Learning unified representation to bind them all for event-based open-world understanding. In European Conference on Computer Vision, pages 477494. Springer, 2024. [83] Yao Zhou, Xiaodong Liu, Yadong Wang, and Weiming Wang. Anomalynet: An anomaly detection network for video surveillance. In 2019 IEEE International Conference on Image Processing (ICIP), pages 17001704, 2019. [84] Alex Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. The multivehicle stereo event camera dataset: An event camera dataset for 3d perception. In IEEE RA-L, 2018. [85] Hao Zhu, Henry Leung, and Zhongshi He. variational bayesian approach to robust sensor fusion based on student-t distribution. Information Sciences, 221:201214, 2013. [86] Liyun Zhu, Lei Wang, Arjun Raj, Tom Gedeon, and Chen Chen. Advancing video anomaly detection: concise review and new dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 14 [87] Yongshuo Zong, Oisin Mac Aodha, and Timothy Hospedales. Self-supervised multimodal learning: survey. arXiv preprint arXiv:2304.01008, 2023. 15 Bounded Influence of Students t-Noise Proposition 1 (Robustness of Students to Outliers). Let δ be noise or residual term drawn from univariate Students distribution with ν > 0 degrees of freedom, location 0, and scale σ: The probability density function of δ is proportional to δ tν(0, σ2). p(δ) (cid:16) 1 + δ2 ν σ2 (cid:17) ν+1 2 . Hence, the negative log-likelihood (omitting constant terms that do not depend on δ) is log p(δ) = ν + 1 2 (cid:16) 1 + δ2 ln ν σ2 (cid:17) + (constant). Differentiating with respect to δ yields the score function (cid:2) log p(δ)(cid:3) = dδ ν + 1 2 1 1 + δ2 ν σ2 2 δ ν σ2 = (ν + 1) δ ν σ2 + δ2 . As δ , the denominator ν σ2 + δ2 is dominated by δ2, so (ν + 1) δ ν σ2 + δ2 (ν + 1) δ δ2 = ν + 1 δ 0. Thus, the derivative (i.e., the slope or pull of the residual on the log-likelihood) remains bounded and actually tends to zero for large outliers. In contrast, consider Gaussian noise model, δ (0, σ2). The corresponding negative loglikelihood is log p(δ) = δ2 2 σ2 + (constant), whose derivative is (cid:2) log p(δ)(cid:3) = dδ δ σ2 , which grows unboundedly as δ . Therefore, large outliers in Gaussian model have much stronger influence on parameter estimation, making it less robust to extreme residuals. Hence, the bounded slope in the Student-t model demonstrates greater robustness against outliers: as δ becomes large, its influence on parameter updates (through gradient-based or maximum likelihood methods) remains finite. This property is central to why Student-t-based methods are often preferred in situations where occasional extreme values are expected. Laplace Approximation for the Students Noise Model We derive an effective variance for the Students noise model via the Laplace approximation. Our goal is to approximate the heavy-tailed log-density by quadratic (Gaussian) form in the vicinity of its mode, thereby obtaining an effective variance that scales the underlying Gaussian variance by factor of ν/(ν + 1). Students Distribution The probability density function of the Students distribution with degrees of freedom ν, location parameter µ, and scale parameter > 0 is given by (x; ν, µ, s) = where: (cid:1) Γ(cid:0) ν+1 νπ Γ(cid:0) ν 2 2 (cid:32) 1 + (cid:1) (cid:18) µ 1 ν (cid:19)2(cid:33) ν+1 2 , Γ() denotes the Gamma function. ν > 0 is the degrees of freedom, controlling the heaviness of the tails. µ is the location parameter (here, assumed to be zero in our derivation). > 0 is the scale parameter; setting σ2 = s2 allows us to interpret σ2 as the variance of the underlying Gaussian scale. In our derivation we assume µ = 0 for simplicity. Ignoring the normalization constant, the unnormalized density can then be written as (cid:18) p(δ) 1 + (cid:19) ν+1 2 , δ2 ν σ2 where δ represents the noise term. Derivation via Laplace Approximation We now present detailed derivation of the effective variance via the Laplace approximation. Proposition 2 (Effective Variance under the Laplace Approximation). Let δ be noise term with distribution (ignoring the normalization constant) (cid:18) p(δ) 1 + (cid:19) ν+1 , δ2 ν σ2 where σ2 is the scale (variance) parameter of the underlying Gaussian and ν > 0 is the degrees of freedom. Then, by performing second-order Taylor expansion of the log-density about its mode at δ = 0, the local approximation is equivalent to that of Gaussian distribution with effective variance ν ν + 1 σ2 = σ2. We start with the unnormalized density and take the natural logarithm to obtain the log-density as (cid:18) p(δ) 1 + δ2 ν σ2 (cid:19) ν+1 , log p(δ) = ν + 1 2 (cid:18) log 1 + (cid:19) δ2 ν σ2 + C, where is constant independent of δ. Since the mode of p(δ) occurs at δ = 0, we perform Taylor expansion of log p(δ) around δ = 0. For small x, we have the approximation = δ2 ν σ2 , Thus, for small δ, log(1 + x) (first-order Taylor expansion). (cid:18) log 1 + (cid:19) δ2 ν σ2 δ2 ν σ2 . Substituting this into the log-density expression yields log p(δ) ν + 1 2 δ2 ν σ2 + = ν + 1 2ν σ2 δ2 + C. Now, consider the log-density of Gaussian distribution with mean zero and variance σ2: log pG(δ) = 1 2σ2 δ2 + , where is constant independent of δ. To match the local quadratic approximation of log p(δ), we equate the coefficients of δ2: Multiplying both sides by 2 gives Taking reciprocals, we obtain 1 2σ2 = ν + 1 2ν σ2 . 1 σ2 = ν + 1 ν σ2 . ν ν + 1 σ2. σ2 ="
        },
        {
            "title": "Inference",
            "content": "In this appendix, we provide detailed derivation and theoretical justification of the inverse variance (or precision) weighting scheme used in our fusion model. This method is firmly rooted in Bayesian inference, where each measurement contributes to the estimation of the latent variable based on its reliability. C.1 Measurement Fusion Under Gaussian Noise Assume that we wish to estimate latent variable from two independent noisy measurements zx and ze. Each measurement is modeled as: zm = µm + δm, δm (0, σ2 m), {x, e}. Here, µm represents the central estimate predicted by modality m, and σ2 (variance). The likelihood of observing zm given is then is the associated uncertainty p(zm z) exp (cid:18) (zm z)2 2σ2 (cid:19) . Assuming flat prior p(z), the posterior is proportional to the product of the likelihoods: p(z zx, ze) p(zx z) p(ze z). Taking the logarithm, we obtain the joint log-likelihood: (zx z)2 2σ2 log p(z zx, ze) = (ze z)2 2σ2 + C. Differentiating with respect to to find the maximum posteriori (MAP) estimate ˆz: (cid:20) (zx z)2 2σ2 (ze z)2 2σ2 (cid:21) = 0, zx σ2 + ze σ2 = 0. Rearranging terms gives: (cid:18) 1 σ2 + 1 σ2 (cid:19) = zx σ2 + ze σ2 , and hence the fused estimate is: zx σ2 1 σ2 This derivation shows that the optimal fusion under Gaussian noise is achieved by weighting each measurement with its inverse variance wm = 1 . σ2 + ze σ2 + 1 σ2 ˆz = . C.2 Bayesian Justification via the Kalman Filtering Framework The inverse variance weighting rule is further supported by the Bayesian update formulations seen in Kalman filtering. Consider scenario where prior estimate ˆz with variance σ is updated with measurement zm having uncertainty σ2 m. The Kalman update is given by: ˆz = ˆz + K(zm ˆz), where the Kalman gain is: = σ σ + σ2 . measurement with lower uncertainty (higher precision) results in larger Kalman gain, thus exerting greater influence on the updated state. Extending this idea to the fusion of multiple modalities, the final fused estimate can be expressed as: µf = wxµx + weµe wx + we , which is exactly the precision-weighted average obtained via the MAP estimation under the assumed likelihood models. 18 C.3 Fusion Formula: Dual Theoretical Foundations Our fusion formula is derived based on two theoretical foundations: Bayesian inference and the Kalman filter. First, assume that the two modalities provide independent estimates of the same latent variable z. For the Gaussian case, the estimates are given by p(z µi, σ2 ) = (z; µi, σ2 ) and p(z µe, σ2 ) = (z; µe, σ2 ). Because these estimates are independent, the joint likelihood (or the unnormalized posterior under uniform prior) is proportional to the product of the two Gaussians: µi2 2σ2 By combining the exponents and completing the square, we find that the value of that maximizes the posteriori.e., the fused meanis given by µe2 2σ2 p(z µi, µe) exp exp . (cid:19) (cid:18) (cid:18) (cid:19) µf = µi/σ2 1/σ2 . + µe/σ2 + 1/σ2 with their effective counterparts σi 2, as For the Students model, we replace σ2 derived via the Laplace approximation (see below), leading to the same formulation in terms of the inverse variances. and σ2 2 and σe From the perspective of the Kalman filter, suppose that one modality provides prediction µi with 2 in the Students case) and another modality provides prediction µe with variance σ2 2 in the Students case). The Kalman filter update for the state estimates is given variance σ2 by (or σi (or σe where the Kalman gain is defined as µf = µi + K(µe µi), = 2 σi 2 + σe 2 , σi which leads to an equivalent expression for the fused mean: 2µe 2 2µi + σi 2 + σe σi By defining the inverse variance weights as wi = 1/(σ2 t) and we = 1/(σ2 + ϵ) (or we = 1/( σe µf = σe . + ϵ) (or wi = 1/( σi 2 + ϵ) for the Students 2 + ϵ) for the Students t), our fusion formula becomes µf = wiµi + weµe wi + we . Thus, both the Bayesian derivation and the Kalman filter interpretation lead to the same uncertaintyweighted fusion formula, with the only difference being that for the Students noise model we use corrected (effective) variance. To summarize, our fusion formula is derived based on two theoretical foundations: Bayesian inference and the Kalman filter. Both derivations lead to the same uncertainty-weighted fusion expression: µf = wiµi + weµe wi + we , with the inverse variance weights defined as wm = 1/(σ2 + ϵ) for Gaussian noise and wm = 1/(σ2 +ϵ) for the Students model. This dual theoretical basis justifies our approach, as it effectively leverages the inverse variances (or precisions) of the modality-specific estimates to account for their respective uncertainties, resulting in robust and reliable fused representation for downstream tasks."
        },
        {
            "title": "D Iterative Refinement of Fused State",
            "content": "After performing the sequential update to fuse the modalities over time using effective variances (i.e., σ2 = exp (cid:16) log σ2 + log (cid:16) ν (cid:17)(cid:17) ν + 1 19 ) to account for the heavy-tailed Student-T noise, small residual errors or microscopic uncertainties may still persist in the fused state. To address this, we introduce an iterative refinement step that further denoises and adjusts the fused representation. The intuition behind this approach is similar to iterative error correction or gradient descent-based optimization: rather than relying solely on the initial fusion, we continuously refine the estimate to better capture the true latent state. Specifically, starting from the initial fused state x0 fusion obtained after the sequential update, we iteratively predict and subtract residual correction. In each refinement step (for = 0, 1, . . . , 1), dedicated network () takes the current state xi along with additional contextual information ci (which may include time-step context, current effective uncertainty estimates, and modality weights) and predicts residual xi: xi = (xi, ci). This residual represents the remaining error in the current fused estimate. The state is then updated by subtracting fraction of this residual, controlled by an attenuation parameter λi: xi+1 = xi λixi. After refinement steps, the final fused representation xN is obtained, which is expected to be more robust and accurate. Theoretical Justification: Even after uncertainty-weighted fusion and sequential updateswhere effective variances derived via the Student-T model are usedthe fused state may still contain imperfections due to noise, model approximation errors, or unmodeled dynamics. The refinement network is motivated by the following principles: Residual Learning: The initial fused state x0 fusion is an approximation of the true latent state y, such that = fusion + δ, where δ is the residual error. The refinement network is designed to learn this residual: so that the final estimate becomes (xi, ci) E[y xi xi, ci], xN = x0 fusion 1 (cid:88) i=0 λiF (xi, ci). This formulation is analogous to residual learning in deep networks, where modeling the error is often easier than directly predicting the target. Diffusion Model Inspiration: Diffusion models iteratively denoise data by progressively removing noise from corrupted input. Similarly, our iterative refinement can be viewed as denoising process where each refinement step removes part of the residual error, thereby driving the fused state closer to the true latent representation. Optimization Perspective: The refinement step can be interpreted as performing an additional optimization in function space. The subtraction of λixi is akin to gradient descent update that reduces an implicit error loss. Over multiple iterations, this results in more accurate estimate, provided that the refinement network is properly designed and trained. Empirical Benefits and Future Directions: initial fusion offers several benefits: In practice, introducing refinement network after the Error Reduction: By learning the residual δ, the final output xN achieves lower prediction error than the initial fused state. Robustness: The iterative refinement is effective in mitigating the effects of heavy-tailed noise and unmodeled dynamics, leading to more stable fused representation. Enhanced Detail Recovery: Fine-grained details that might be lost in the initial fusion can be recovered through successive refinement, improving both quantitative metrics and qualitative performance. 20 Future work may explore: Iterative or Recurrent Refinement: Extending the refinement process with additional iterative steps or recurrent architecture that shares weights across iterations. Uncertainty-Guided Refinement: Incorporating explicit uncertainty measures to guide the refinement network to focus on regions with high residual error. Enhanced Loss Functions: Employing perceptual or adversarial losses in the refinement stage to better capture fine details and enhance the realism of the final output. In summary, the iterative refinement network not only removes residual errors remaining after the initial uncertainty-weighted fusion but also draws strong inspiration from diffusion models denoising principles. This two-stage approachfirst, coarse fusion and then fine, iterative refinementprovides theoretically grounded and empirically validated method to enhance the final fused representation for downstream tasks."
        },
        {
            "title": "E Derivation of KL Divergence",
            "content": "We present detailed derivation of the KL divergence between two Gaussian distributions. Specifically, the KL divergence between Gaussian distribution (µm, σ2 m) and standard normal distribution (0, I) is derived in closed form as follows: m)N (0, I)(cid:1) = The KL divergence between two probability distributions p(x) and q(x) is defined as: KL(cid:0)N (µm, σ2 1 log σ2 + µ2 m). (σ 1 2 KL(pq) = (cid:90) p(x) log p(x) q(x) dx. Consider two Gaussian distributions: p(x) = (x; µm, σ2 m), q(x) = (x; 0, 1). Expanding explicitly, we obtain: KL(pq) = (cid:90) (cid:34) p(x) log KL(pq) = 1 (cid:112)2πσ2 1 2 log 1 σ2 (cid:18) exp (cid:90) + (x µm)2 2σ2 (cid:20) p(x) (x µm)2 2σ2 + (cid:21) x2 2 dx. (cid:19) log 1 2π (cid:18) exp x2 2 (cid:19)(cid:35) dx. Evaluating the expectations under p(x): Ep[x] = µm, Ep[x2] = σ2 + µ2 m, Ep[(x µm)2] = σ2 m. Substituting these expectations back into the expression gives: 1 KL(pq) = σ2 + log 1 2 1 2σ2 1 σ2 (σ2 + µ2 m). Simplifying further, we obtain the final closed-form expression: KL(cid:0)N (µm, σ m)N (0, I)(cid:1) ="
        },
        {
            "title": "F Experiment Details",
            "content": "(σ2 + µ2 1 log σ2 m). Experiments compute resources. All experiments were conducted on local workstation equipped with an AMD Ryzen Threadripper PRO 5955WX 16-Core Processor (32 threads) and single NVIDIA RTX 6000 Ada Generation GPU (48GB VRAM). The system had 256GB of system RAM and 5GB VRAM and ran on Ubuntu 22.04. To improve training efficiency, we first precompute the video frame embeddings using the image and event encoders before training. This preprocessing step takes approximately 3 to 5 days. Once embeddings are extracted, training on the XD-Violence dataset takes around 3 hours, while training on the ShanghaiTech, UCF-Crime, and MSAD datasets completes within 1 hour. F.1 Dataset Details We evaluate our weakly supervised learning approach on four commonly used benchmark datasets for video anomaly detection: UCF-Crime [59], XD-Violence [66], ShanghaiTech [40], and MSAD [86]. UCF-Crime consists of 1,900 real surveillance videos, totaling 128 hours and covering 13 anomaly classes. XD-Violence comprises 4,754 untrimmed videos (217 hours), featuring 6 distinct anomalous or violent actions. ShanghaiTech includes 330 training and 107 test videos (approximately 317,000 frames), recorded in 13 scenes and labeled with 11 anomaly classes. MSAD features 720 videos from 14 different scenarios, annotated with 11 anomalies. Due to the data imbalance and the rarity of violent incidents in XD-Violence, we report the Average Precision (AP, %) to assess precisionrecall balance, while for the other datasets, we measure performance using the Area Under the ROC Curve (AUROC, %). Preprocessing We employ spatial augmentation strategy inspired by multi-crop evaluation. Specifically, each input video is first resized to resolution of 224 224, followed by the generation of 10 spatial crops: five fixed regions (top-left, top-right, bottom-left, bottom-right, and center) and their horizontally flipped counterparts. From each video, image frames are extracted and processed by the CLIP [54] (ViT-L/14) image encoder to obtain latent representations. For every 16 frames, we compute the average latent vector, denoted as zx. In parallel, we generate synthetic events by computing pixel-wise changes between consecutive frames within each 16-frame segment using threshold of 10/255 and clamp value of 10. The resulting binary event maps are stacked and fed into the event encoder [31] (aligned with image encoder) to produce event representations ze. We refer to these as \"synthetic\" since they are derived from RGB frames rather than captured by true event sensor. F.2 Implementation Details Architecture Detail Figure 1 illustrates the complete architecture used in our framework. Both the image and event encoders are implemented using the ViT-L/14 architecture from CLIP [54], with an embedding dimension of 768. The function , responsible for modality-specific feature encoding, is implemented as multi-layer attention module with 8 attention heads and 2 transformer layers. The functions and h, used to predict the mean and variance parameters for fusion, are each implemented as single linear layer. The refinement network consists of simple feedforward structure with LinearReLULinear sequence to iteratively refine the fused representation. Hyperparameters Detail To reproduce the results reported in Table 1, we configure the model with the following hyperparameters: the degrees of freedom is set to ν=8, the number of iterative refinement steps is =10, and the uncertainty refinement weight is λr=0.5. The optimization uses the AdamW [43] optimizer with learning rate of 2 105 and batch size of 64 for 10 training epochs. We apply MultiStepLR scheduler with milestones at epochs 4 and 8, and decay factor of 0.1. The numerical stability term is set to ϵ=108. Additionally, for the regularization loss Lreg, we fix both λ1 and λ2 to 0.5 throughout all experiments. F.3 Results Detail UCF-Crime [59] XD-Violence [66] ShanghaiTech [40] MSAD [86] Class Image Event Fusion Class Image Event Fusion Class Image Event Fusion Class Image Event Fusion Abuse Arrest Arson Assault Burglary Explosion Fighting RoadAccident Robbery Stealing Shooting Shoplifting Vandalism AUC Ano AUC 68.02 72.21 65.49 56.44 68.02 56.33 58.14 57.41 76.03 74.91 60.95 64.27 66.89 86.77 66.56 70.09 47.09 66.75 72.03 65.88 57.64 79.27 59.11 62.39 61.51 38.42 73.29 63.05 78.67 63.94 70.74 75.05 72.68 72.58 74.99 63.46 62.81 66.32 76.29 75.43 62.26 85.72 69. 89.13 72.49 Fighting Shooting Riot Abuse Car Accident Explosion 79.59 54.59 97.62 59.42 50.83 64.32 67.81 42.94 86.07 54.49 32.53 39.22 84.76 61.99 97.67 64.96 51.70 68.54 Car Chasing Fall Fighting Monocycle Robbery Running Skateboard Throwing_object Vehicle Vaudeville 70.07 94.49 72.96 76.91 67.23 76.74 37.95 76.04 89.63 79.39 44.04 74.76 84.36 65.30 63.48 75.32 87.26 60.95 78.29 83.14 67.38 53.66 74.83 91.33 82.17 83.64 75.46 90.43 60.78 82.38 91.95 79.87 53.61 Water_incident Assault Explosion Fighting Fire Object_falling People_falling Robbery Shooting Traffic_accident Vandalism 54.78 50.86 71.74 71.97 90.52 60.64 68.10 71.20 62.23 83.40 97.95 66.03 66.25 79.75 49.44 75.92 42.50 66.90 86.87 70.08 75.82 88. 58.73 57.20 81.14 71.23 90.92 56.93 70.95 77.88 70.93 87.05 98.75 AP 84.22 55.96 86.54 AUC 97.58 93.69 98.24 AUC 91.52 82. 92.16 Table 2: Per-class performance on UCF-Crime [59], XD-Violence [66], ShanghaiTech [40], and MSAD [86] for Figure 2. 22 Figure2 summarizes the per-class performance across four benchmark datasets: UCF-Crime, XDViolence, ShanghaiTech, and MSAD. For each anomaly class, we report detection performance using the image modality, event modality, and their fusion. In most cases, the fusion consistently outperforms both unimodal inputs, highlighting the complementarity between image and event information. Notably, on UCF-Crime, classes such as Shoplifting and Arson show substantial gains from fusion. For XD-Violence, categories like Riot and Fighting exhibit strong improvements with fusion, despite relatively weak event-only performance. ShanghaiTech also shows consistent pattern of fusion superiority across diverse scene types. In the MSAD dataset, fusion leads to higher detection scores for complex dynamic events such as Water_incident and Vandalism. These results emphasize the effectiveness of our uncertainty-guided multimodal fusion strategy in adapting to diverse scene contexts and anomaly types."
        },
        {
            "title": "G Ablation Study",
            "content": "G.1 Sensitivity to Hyperparameter Settings We conduct comprehensive ablation study to examine the effect of key hyperparameters in our uncertainty-guided fusion framework. Specifically, we analyze the impact of the degrees of freedom ν in the Student-T distribution, the number of refinement steps , the Laplace approximation precision ϵ, and the refinement weight λr, all metrics is reported 10-times average with 1-standard deviation. ν 2 4 8 10 87.980.33 AUC Ano-AUC 70.060.72 88.340.30 70.690.72 88.380.40 70.880.86 88.670.45 71.501. 88.110.47 70.220.86 0 10 20 40 86.770.18 AUC Ano-AUC 67.410.43 88.670.45 71.501.02 ϵ 104 88.460.42 AUC Ano-AUC 71.061. λr 0.1 0.3 88.410.43 71.270.61 10 88.500.38 71.450.81 0.5 88.480.28 71.360.59 0. 88.600.31 71.450.58 108 88.670.45 71.501.02 0.9 87.920.38 AUC Ano-AUC 69.610.98 88.130.24 70.130.58 88.670.45 71.501. 88.210.34 70.320.90 87.910.34 69.800.86 Table 3: Ablation study results on UCF-Crime with varying hyperparameters. : Varying the degrees of freedom ν shows gradual increase in both AUC and UCF-Crime Ano-AUC scores, peaking at ν = 8. For the number of refinement steps , performance increases significantly when changes from 0 to 10, and remains relatively stable for 10. The Laplace approximation precision ϵ results in only marginal differences across all values tested. In the case of the refinement weight λr, the best results are obtained at λr = 0.5, while performance slightly degrades when the weight is set to more extreme values (0.1 or 0.9) Table 3. : In the XD-Violence dataset, the degrees of freedom ν show stable performance XD-Violence across different settings, with AP values consistently around 87, peaking at 87.63 when ν = 8. Increasing the number of refinement steps leads to noticeable improvements, with AP rising from 86.01 at = 2 to 87.71 at = 30. The Laplace precision parameter ϵ exhibits minimal influence on performance, with AP differences within 0.36 across tested values. Adjusting the refinement weight λr shows moderate effects, where the best AP of 87.63 is achieved at λr = 0.5 Table 4. ShanghaiTech : For the ShanghaiTech dataset, the performance remains stable across different settings of the degrees of freedom ν, with AUC scores ranging narrowly between 97.90 and 97.98. Varying the number of refinement steps does not lead to significant changes, though slight increase is observed at = 10. The Laplace precision parameter ϵ shows minimal effect on AUC, 23 Dataset Hyperparameter 2 / 0 / 1e-4 / 0.1 4 / 10 / / 0.3 6 / 20 / 1e-6 / 0. 8 / 30 / / 0.7 10 / 40 / 1e-8 / 0.9 XD-Violence ShanghaiTech MSAD ν (AP) (AP) ϵ (AP) λr (AP) ν (AUC) (AUC) ϵ (AUC) λr (AUC) ν (AUC) (AUC) ϵ (AUC) λr (AUC) 87.310.77 86.011.10 87.270.76 86.890.80 97.910.10 97.940.08 97.910.12 97.860.07 91.780.34 90.690.35 91.770.27 90.730.74 87.390.65 87.630.54 86.740.48 97.910.10 97.980.07 97.900.11 91.790.29 92.900.27 91.410. 87.030.52 87.520.40 87.440.79 87.630.54 97.900.10 97.900.12 97.900.08 97.980.07 91.840.42 92.290.30 91.810.33 92.900.27 87.630.54 87.710.70 87.190.67 97.980.07 97.900.11 97.900.11 92.900.27 92.720.29 92.120. 86.640.51 87.540.60 87.630.54 87.231.13 97.920.06 97.930.07 97.980.07 97.920.13 91.640.27 92.690.21 92.900.27 92.280.44 Table 4: Ablation study results across datasets (XD-Violence, ShanghaiTech, MSAD) under varying hyperparameters. with differences remaining within 0.08. Adjusting the refinement weight λr yields the highest AUC of 97.98 at λr = 0.5, while other values produce slightly lower but comparable performance Table 4. MSAD : In the MSAD dataset, increasing the degrees of freedom ν generally results in marginal improvements, peaking at ν = 8 with an AUC of 92.90. The number of refinement steps has more pronounced impact, with AUC improving steadily from 90.69 at = 0 to 92.72 at = 30, then maintaining similar level at = 40. The Laplace precision parameter ϵ again leads to only small variations, with values ranging from 91.77 to 92.90. For λr, AUC increases consistently as the parameter increases, with the highest value (92.28) observed at λr = 0.9 Table 4. G.2 Loss Configuration Dataset UCF-Crime Metric Lcls 88.040.28 Ano-AUC 69.790.85 AUC Lcls + Lreg 87.780.46 69.731.00 Lcls + LKL 88.240.19 70.460.69 Lcls + Lreg + LKL 88.670.45 71.501.02 XD-Violence ShanghaiTech AP AUC 87.400.72 87.450.49 87.460.64 87.630.54 97.970. 97.910.09 97.850.14 97.980.07 MSAD 92.900.27 Table 5: Performance comparison under different loss configurations across datasets. 91.650. 92.390.22 92.380.22 AUC We assess the contribution of each loss component by evaluating four configurations: classification loss only (Lcls), classification plus regularization (Lcls + Lreg), classification plus KL divergence (Lcls + LKL), and the full combination (Lcls + Lreg + LKL) across UCF-Crime, XD-Violence, ShanghaiTech, and MSAD datasets  (Table 5)  . In UCF-Crime, adding the KL loss improves AUC from 88.04% to 88.24% and Ano-AUC from 69.79% to 70.46%, while the full combination further boosts AUC and Ano-AUC to 88.67% and 71.50%, respectively. In XD-Violence, ShanghaiTech, and MSAD, performance remains largely stable across settings, with slight improvements under the full loss configuration. In particular, MSAD AUC increases from 92.39% to 92.90% with the full loss. Overall, adding LKL consistently yields benefits, while the effect of Lreg alone is minor. The full configuration achieves the best or comparable results across all benchmarks."
        },
        {
            "title": "H Fusion Details",
            "content": "To evaluate whether the proposed framework performs as intended, we analyze the behavior of the uncertainty weights wm (Eq. 4) when one modality is partially corrupted. Specifically, we aim to test whether the models dynamic uncertainty estimation mechanism can correctly respond to degraded sensor input. Rather than injecting additive noisewhich may lead to complex and unpredictable interactions within attention-based encoderswe opt for controlled masking strategy. 24 In attention networks, even small perturbations can propagate nonlinearly across dimensions, making it difficult to interpret the resulting change in uncertainty due to entangled feature dependencies. Additionally, adversarial perturbations rely on gradients computed after the refinement stage, making it difficult to isolate the direct effect on modality-specific uncertainty. In contrast, masking fixed proportions of input features allows us to deterministically degrade the modality in localized and interpretable manner, providing clean testbed for evaluating the reliability and sensitivity of uncertainty estimation. To assess the reliability of our uncertainty-weighted fusion mechanism, we conduct controlled modality-specific perturbation experiments across four datasets. We simulate degradation by randomly masking proportion ρ {0.05, 0.10, 0.20, 0.30, 0.50} of the latent feature dimensions in either the image modality (zx) or the event modality (ze). All experiments report standard performance metrics, including AUC or AP for detection quality, Brier score for probabilistic calibration, and KL divergence to quantify the shift between predictions made on clean inputs and those made under masking. Additionally, we track uncertainty weights wx and we for both modalities, including breakdowns on abnormal and normal video segments, to understand how the model reallocates modality-level confidence under degradation. Noise Type Masked Level AUC (%) Brier KL we wab wn wx wab wn CLEAN EV_NOISE IMG_NOISE 0.05 0.10 0.20 0.30 0.50 0.05 0.10 0.20 0.30 0.50 89.09 88.92 88.80 88.68 88.64 88.35 88.57 87.90 87.05 86.57 85.74 0. 0.0000 0.4760 0.4744 0.4761 0.5240 0. 0.5239 0.1238 0.1249 0.1252 0.1238 0.1216 0.1261 0.1295 0.1290 0.1238 0.1097 0.0032 0.0062 0.0104 0.0133 0.0198 0.0258 0.0548 0.1066 0.1445 0.2021 0.4757 0.4756 0.4757 0.4757 0. 0.4764 0.4769 0.4779 0.4788 0.4808 0.4742 0.4742 0.4743 0.4743 0.4745 0.4749 0.4755 0.4766 0.4776 0.4798 0.4758 0.4758 0.4758 0.4758 0.4759 0.4766 0.4771 0.4780 0.4790 0.4809 0.5243 0.5244 0.5243 0.5243 0. 0.5236 0.5231 0.5221 0.5212 0.5192 0.5258 0.5258 0.5257 0.5257 0.5255 0.5251 0.5245 0.5234 0.5224 0.5202 0.5242 0.5242 0.5242 0.5242 0.5241 0.5234 0.5229 0.5220 0.5210 0.5191 Table 6: Fusion metrics on the UCF-Crime dataset under varying noise settings. AUC is reported as percentage. Noise Type Masked Level AP (%) Brier KL we wab wn wx wab wn CLEAN EV_NOISE 0 0.05 0.10 0.20 0.30 0.50 88.26 88.12 88.00 87.74 87.51 87.13 0.0735 0. 0.4661 0.4623 0.4672 0.5339 0.5377 0. 0.0741 0.0747 0.0761 0.0769 0.0792 0.0020 0.0040 0.0083 0.0121 0.0194 0.4661 0.4661 0.4660 0.4660 0.4658 0.4623 0.4623 0.4624 0.4624 0.4623 0.4672 0.4671 0.4671 0.4670 0.4668 0.5339 0.5339 0.5340 0.5340 0. 0.5377 0.5377 0.5376 0.5376 0.5377 IMG_NOISE 0.05 0.10 0.20 0.30 0.50 0.5372 0.5367 0.5356 0.5345 0.5323 Table 7: Fusion metrics on the XD-Violence dataset under varying noise settings. 0.0267 0.0546 0.1162 0.1788 0.3082 0.5335 0.5330 0.5321 0.5313 0. 0.4665 0.4670 0.4679 0.4687 0.4705 0.4676 0.4680 0.4689 0.4697 0.4714 0.0794 0.0852 0.0986 0.1109 0.1366 0.4628 0.4633 0.4644 0.4655 0.4677 87.95 87.64 86.60 85.86 84.23 0.5328 0.5329 0.5329 0.5330 0. 0.5324 0.5320 0.5311 0.5303 0.5286 Across all four datasets, our method consistently demonstrates robust uncertainty-guided fusion behavior. When degradation is applied to the event modality (ze), performance remains stableAUC or AP typically drops by less than 1%, and uncertainty weights show minimal change. This suggests that the model is not overly sensitive to event corruption and maintains reliable fusion under partial degradation. In contrast, masking the image modality (zx) produces more pronounced effects, especially on appearance-dependent datasets such as UCF-Crime and XD-Violence. At the highest masking level, UCF-Crime experiences 2.35% drop in AUC, and XD-Violence shows nearly 5% drop in AP, accompanied by KL divergence increases up to 0.3082. Uncertainty weights reflect this asymmetry: wx consistently decreases while we increases, indicating that the model dynamically downweights unreliable image features and reallocates confidence toward the event modality. 25 Noise Type Masked Level AUC (%) Brier KL we wab wn wx wab wn CLEAN EV_NOISE 0 0.05 0.10 0.20 0.30 0. 98.17 98.13 98.11 98.07 98.06 98.06 0.0402 0.0000 0.4718 0. 0.4723 0.5282 0.5367 0.5277 0.0401 0.0398 0.0399 0.0393 0.0383 0.0006 0.0022 0.0038 0.0053 0. 0.4720 0.4722 0.4726 0.4729 0.4734 0.4634 0.4635 0.4638 0.4639 0.4641 0.4725 0.4727 0.4731 0.4734 0.4739 0.5280 0.5278 0.5274 0.5271 0.5266 0.5366 0.5365 0.5362 0.5361 0.5359 IMG_NOISE 0.05 0.10 0.20 0.30 0.50 0.5353 0.5349 0.5322 0.5304 0.5246 Table 8: Fusion metrics on the ShanghaiTech dataset under varying noise settings. 0.0162 0.0308 0.0758 0.1084 0.2180 0.4731 0.4738 0.4754 0.4768 0.4803 0.5274 0.5267 0.5250 0.5236 0.5199 0.0430 0.0434 0.0477 0.0520 0. 0.4726 0.4733 0.4750 0.4764 0.4801 0.4647 0.4651 0.4678 0.4696 0.4754 97.66 97.54 96.29 95.09 90.72 0.5275 0.5273 0.5269 0.5266 0.5261 0.5269 0.5262 0.5246 0.5232 0.5197 Noise Type Masked Level AUC (%) Brier KL we wab wn wx wab wn CLEAN EV_NOISE IMG_NOISE 0 0.05 0.10 0.20 0.30 0.50 0.05 0.10 0.20 0.30 0.50 92.39 92.27 92.19 91.94 91.89 91.86 92.27 92.10 91.86 91.63 91. 0.1119 0.0000 0.4807 0.4786 0.4814 0. 0.5214 0.5186 0.1126 0.1129 0.1149 0.1153 0.1168 0.1117 0.1118 0.1111 0.1116 0.1106 0.0024 0.0048 0.0088 0.0116 0.0158 0.0066 0.0125 0.0220 0.0303 0. 0.4808 0.4810 0.4812 0.4814 0.4817 0.4811 0.4816 0.4824 0.4832 0.4848 0.4787 0.4788 0.4791 0.4791 0.4794 0.4790 0.4795 0.4804 0.4813 0.4827 0.4815 0.4816 0.4819 0.4821 0.4824 0.4817 0.4822 0.4830 0.4838 0. 0.5192 0.5190 0.5188 0.5186 0.5183 0.5189 0.5184 0.5176 0.5168 0.5152 0.5213 0.5212 0.5209 0.5209 0.5206 0.5210 0.5205 0.5196 0.5187 0.5173 0.5185 0.5184 0.5181 0.5179 0.5176 0.5183 0.5178 0.5170 0.5162 0. Table 9: Fusion metrics on the MSAD dataset under varying noise settings. On datasets with higher modality redundancysuch as ShanghaiTech and MSADboth performance and uncertainty remain relatively stable under perturbation. ShanghaiTech maintains an AUC above 98% under event noise and above 90% under severe image masking, while MSAD exhibits only minor fluctuations across all metrics. This confirms that the model can maintain balanced modality fusion when both modalities offer sufficient information. While the average change in uncertainty weights (wm) is numerically smalltypically below 1%this is largely due to aggregation over all vector indices and time steps. finer-grained analysis reveals that individual latent dimensions can shift by as much as 30% under value-level masking, demonstrating substantial feature-wise modulation. Moreover, uncertainty reallocation is more pronounced in abnormal segments compared to normal ones, indicating that the model adapts more sensitively in semantically critical regions. Finally, increases in Brier score under corruption reflect growing misalignment between predicted probabilities and ground truth labels, particularly under image degradation, further confirming that the models confidence dynamically adjusts in response to input quality."
        },
        {
            "title": "I Visualizatation of Anomaly Detection with Uncertainty",
            "content": ""
        }
    ],
    "affiliations": [
        "MOLOCO",
        "University of California, Irvine"
    ]
}