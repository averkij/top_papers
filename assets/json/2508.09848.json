{
    "paper_title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts",
    "authors": [
        "Mo Yu",
        "Tsz Ting Chung",
        "Chulun Zhou",
        "Tong Li",
        "Rui Lu",
        "Jiangnan Li",
        "Liyan Xu",
        "Haoshu Lu",
        "Ning Zhang",
        "Jing Li",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 2 8 4 8 9 0 . 8 0 5 2 : r PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts PRELUDE: BENCHMARK DESIGNED TO REQUIRE GLOBAL COMPREHENSION AND REASONING OVER LONG CONTEXTS , Liyan Xu 1 , Jiangnan Li 1 , Rui Lu 3 , Tong Li 3 , Mo Yu 1 , Chulun Zhou 3 , Tsz Ting Chung 2 Haoshu Lu 4, Ning Zhang 1, Jing Li 4, Jie Zhou 1 Project Page: https://gorov.github.io/prelude 1WeChat AI, Tencent Spoiler alert: We show it is possible to Measure Fluid Intelligence in natural language space. 2HKUST 3CUHK 4NJIT Figure 1: (Left) Two examples from our PRELUDE. (Right) Comparison of existing benchmarks along the different criteria for long context understanding assessment. We report the normalized measure along each criterion, with the details presented in Appendix A."
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce PRELUDE, benchmark for evaluating long-context understanding through the task of determining whether characters prequel story is consistent with the canonical narrative of the original book. Our task poses stronger demand for global comprehension and deep reasoning than existing benchmarks as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "The emergence of new LLM-driven applications, such as multi-document analysis (Google DeepMind, 2024; Wang et al., 2024d; Gutierrez et al., 2024), personal assistants with chat histories (Wu et al., 2024; Xu et al., 2025a), autonomous agents (Hu et al., 2025; OpenAI, 2025; Wang et al., 2025), and repository-level coding tools (Jimenez et al., 2024), has created increasing demands for robust long-context understanding and reasoning. Equal contribution. Correspondence to: moyumyu@tencent.com,ttchungac@connect.ust.hk. 1 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts To better support long inputs, many techniques have been proposed, primarily focusing on efficient attention mechanisms (Xiong et al., 2021; 2023) and retrieval-augmented generation (RAG) (Lewis et al., 2020; Xu et al., 2024a; Edge et al., 2024; Gutierrez et al., 2024; Asai et al., 2023). Alongside these technical advances, there is growing need for effectively evaluating long context understanding and reasoning capabilities. To this end, several benchmarks have recently been introduced (see Section 2). Building on this progress, recent work has extensively discussed the criteria that strong benchmark for long-context understanding and reasoning should satisfy (Press et al., 2023; Yu et al., 2023; Liu et al., 2024; Yen et al., 2024; Fang et al., 2024; Wu et al., 2025b). To rigorously evaluate models capabilities in this domain, several key criteria have emerged as essential: Beyond Memorization. LLMs memorize content from pretraining, especially for popular texts (Tirumala et al., 2022; Del√©tang et al., 2023; Sutskever, 2023), enabling answers without true comprehension. The existence of this shortcuts blurs the line between long-context understanding and mere activation of parametric knowledge memorized during pretraining. As training data grows, this issue worsens. As Necessity Condition, robust benchmark must prevent solutions based on memorization alone, ensuring full-context reasoning remains essential. Global Dependency. The task should require aggregating evidence scattered across the context or exhibiting global dependencies; otherwise, it reduces to short-context problem focused on retrieval rather than true long-text understanding. Depth of Reasoning. Long-context reasoning should inherently require synthesizing multiple pieces of evidence and multi-step deduction. However, many existing benchmarks focus on shallow reasoning, such as decomposition or enumeration (e.g., counting facts (Xu & Ma, 2025), or sequential sub-questions (Yang et al., 2018)), which reduces the need for global, multi-step inference and oversimplifies the task. Human-Machine Gap. To highlight essential capabilities that general-purpose intelligent systems should possess, benchmark should show significant gap between humans and machines. Low human accuracy or agreement typically signals poor annotation quality or high subjectivity, undermining reliable evaluation. Beyond Summarization/Salience. Often overlooked but crucial, strong benchmark should require attention to fine-grained details beyond high-level abstraction to remain challenging and meaningful. Otherwise, it risks reducing to summarization task that is solvable without longcontext understanding (Chang et al., 2023). Figure 1 evaluates several widely-used benchmarks for story understanding across the aforementioned dimensions. It shows that all existing benchmarks fall short in at least one aspect, with particularly notable limitation in mitigating the effects of Memorization and encouraging Deep Reasoning. Detailed evaluation metrics and experimental settings are provided in Appendix A. To address these limitations, we propose novel task format that enables comprehensive assessment across all the identified criteria. The task involves presenting the model with list of hypothetical prequels for important supporting book characters, and asking it to determine whether each prequel is consistent with the canonical story. Each hypothesis is presented as concise bullet point (see Figure 1) summarizing proposed setting. These hypotheses are annotated by human experts who have read the canonical stories multiple times and performed literary analysis before. This annotation process is both efficient and high-quality, ultimately yielding dataset of 1K labeled examples. Intuitively, our task design naturally mitigates the key limitations observed in existing benchmarks: First, the Memorization shortcut is alleviated by construction, as the input prequels are newly generated and do not appear in the training data of any existing LLMs. The limited performance of OpenAIs Deep Research further supports this, suggesting that it is difficult to locate humansummarized evidence on the internet to solve our task. Second, our task encourages global reasoning. This is because (1) determining whether consistent prequel aligns with the canonical story typically requires aggregating evidence across the whole character story; and (2) contradictory prequels often involve inconsistencies that span several scattered events due to the narrative structure of the original work. Empirically, our annotation analysis reveals that 88% of the examples in PRELUDE require non-local evidence to resolve. Finally, our task encourages deep reasoning, because of the fact that the canonical story reflects non-immediate consequences of the prequels. To solve our task, LLMs must unfold the implications of prequel and align them with the story, often requiring multi-step inference. For instance, the 2 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Type Definition Example Contradict Local detail in the original novel directly contradicts the prequel Contradict Global The prequel causes part of the original plot or the characters motivations to become broadly unreasonable. Book: The Chronicles of Narnia Char: Eustace Scrubb Prequel: Born in London and raised as an only child, Eustace was taught to call his parents by their names \"Harold\" and \"Alberta\". The novel introduces Eustace as being born in Cambridge, England. Book: The Count of Monte Cristo Char: Faria Prequel: He joined the Jesuits and was sent to Goa, Indian. During this time, he secretly studied the ancient Indian medical text \"Charaka Samhita,\" which laid the groundwork for his later expertise in toxicology. (Throughout the story, Faria never demonstrates any expertise in medicine or toxicology. Moreover, he does not speak Hindi, making such an event unlikely.) Contradict Global II The prequel does not contradict the original plot but is inconsistent with the tone or stylistic setting of the novel Book: Romance of the Three Kingdoms Char: Xiahou Yuan Prequel: He was taught magical speed-enhancing techniques by mysterious hermit, which allowed him to march 1000 li in six days. (The book is historical novel with largely realistic style, and it never depicts any general using magic or supernatural powers.) Consistent Irrelevant The prequel is not closely relevant to the original plot but doesnt contradict it. It is side story of the character. Consistent Core The prequel fills in missing detail without introducing contradictions. Book: The Count of Monte Cristo Char: Faria Prequel: At the age of 12, he entered the University of Bologna to study theology and chemistry, and caused sensation in the academic world by deciphering the ciphers in 15th-century alchemists manuscript. (Farias academic background and influence are never depicted in the novel. However, this is consistent with his characterization as learned man.) Book: Harry Potter Char: Sirius Black Prequel: Rejecting pure-blood elitism, Sirius frequently showed admiration for Gryffindor and spent time with Muggles and so-called \"blood traitors\" to annoy his parents even before attending Hogwarts. (Sirius is introduced as key figure against pure-blood elitism, but little detail is provided. This fills in gaps without introducing contradictions.) Table 1: Definitions of our annotation labels in PRELUDE. second example in Figure 1 involves reasoning that Faria was arrested when Napoleon was still emperor, and then inferring contradiction from the fact that the Bourbon Restoration removed Napoleon from power. This kind of non-immediate causality resists shallow reasoning shortcuts that decomposes the problem into subquestions. We conduct extensive experiments on our proposed task using ICL, RAG, and DeepResearch across state-of-the-art commercial and open-source LLMs. The results reveal several key findings: (1) The best-performing system lags human performance by over 15%; (2) LLMs often arrive at correct predictions with flawed reasoning, resulting in reasoning accuracy gap of >30% compared to humans; (3) Our task cannot be solved by searching for information on the web, making the advanced DeepResearch systems struggle and underperform RAG; (4) Supervised training and many-shot ICL yield no performance gains, highlighting LLMs intrinsic limitations in long-context reasoning. These findings highlight that PRELUDE requires deeper long-context reasoning capabilities beyond what current methods offer, pointing to important directions for future research."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Tasks over Synthetic Long Contexts Following the needle-in-a-haystack task that examines LLMs in-context searching (Kamradt, 2023), line of works have since focused on probing the LLM ability to trace and utilize information pieces in stretched long context (Hsieh et al., 2024; Li et al., 2025b; Yu et al., 2025b), while others also fuse reasoning in their task design such as sorting or relation identification, beyond the mere retrieval aspect (Kuratov et al., 2024; Wang et al., 2024a; Dong et al., 2024; Wang et al., 2024c; Lee et al., 2025). Besides these works that specifically stresstest long context utilization, other related LLM tasks could also reflect such ability, e.g., many-shot in-context learning (Agarwal et al., 2024; Xu et al., 2024b; Li et al., 2025c). Realistic Long Context Understanding Tasks Orthogonal to those synthetic stress-testing tasks, another line of works target the more natural question answering settings for realistic long context evaluation, primarily utilizing stories in various domains, such as NarrativeQA (KoÀácisk`y et al., 2018), NovelQA (Wang et al., 2024b), DetectiveQA (Xu et al., 2025b), CharToM-QA (Zhou et al., 2025). Realistic long context QA has gained particular attention in many recent LLM benchmarks, such as LongBench (Bai et al., 2024a;b), XLBench (Ni et al., 2024), CLongEval (Qiu et al., 2024), LooGLE (Li et al., 2024), Bench (Zhang et al., 2024), LaRA (Li et al., 2025a), etc. 3 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Document-Level Entailment Our task is closely related to fact verification over multiple documents or web sources, as exemplified by FEVER (Thorne et al., 2018) and its extensions (Wadden et al., 2020; Yin et al., 2021; Schlichtkrull et al., 2023). Among this line of work, NoCha (Karpinska et al., 2024) is the most relevant, as it also uses book narratives as context. However, key distinction lies in the nature of the hypotheses: NoCha uses summaries or conclusions of the original story, which often share semantic overlap with the canonical book. Therefore, this task design is vulnerable to memorization or summarization shortcuts, as shown in Figure 1. To mitigate this, NoCha uses recently published books. Yet as training corpora expand, newer LLMs inevitably become familiar with these texts, reducing the tasks effectiveness. As shown in our experiments, while the public subset of NoCha has been largely conquered by LLMs, our subset comprising works from the same period or earlier remains challenging. This shows that our task is not becoming easier for models over time."
        },
        {
            "title": "3 THE PROPOSED PRELUDE TASK",
            "content": "In this section, we introduce the construction process of our PRELUDE (PRequel Entailment for Long context Understanding and DEduction) task. 3.1 TASK FORMULATION Our task is formulated as binary classification. The input consists of book that is split into consecutive chunks = {b1, b2, ..., bM }; and prequel for character c, which is short text describing an experience of the character prior to the story of happens. The task is then predict whether aligns with B. The labels to predict thus are {consistent, contradict}. 3.2 WHY PREQUELS? Our prequel entailment task is naturally long-context understanding task and form of everyday research task. To solve the task, model needs to judge whether characters prequel remains consistent with the behaviors and experiences throughout the narrative and makes counterfactual reasoning when necessary. These requirements make our task well-suited for benchmarking longcontext reasoning for the following desirable properties: Natural long-context reasoning: The task requires holistic understanding of narrative arc, including tracking characters psychological continuity, goals, and situational influences across temporally distant events. Cognitive research practices in daily life: While formal research is often confined to scientific domains, its core cognitive components, such as gathering evidence, forming hypotheses, and drawing conclusions, are deeply embedded in daily reasoning. Our task scenario mirrors this real-life cognition, as humans frequently make similar judgments while watching films, reading novels, or engaging in social interactions. Light dependency on background knowledge: This task requires little reliance on external or specialized knowledge. reader with full understanding of the story, even as middle school student, can often make accurate judgments. As result, the task emphasizes fluid intelligence rather than crystallized knowledge acquired through prior learning. 3.3 DATASET CONSTRUCTION Label Definitions To facilitate human annotation, we categorize the consistent and contradictory cases into fine-grained types. Definitions and representative examples are provided in Table 1. Guidelines for Human Annotation Annotators are instructed to follow the definitions and examples provided in Table 1. They are guided by flowchart that first identify Contradict - Local, Contradict - Global I, and Contradict - Global II respectively. If no contradiction is identified, they then determine whether the consistent prequel provides key missing information. During annotation, annotators are required to carefully consult the original book to identify any contradictions. PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Name Author Genre Lang. Public #Char #Instances The Count of Monte Cristo Demi-Gods and Semi-Devils The Return of the Condor Heroes Investiture of the Gods Romance of the Three Kingdoms Love in the Time of Cholera Pinball, 1973 Rebecca In Search of the Castaways The Redeemer Drawing Sword Dwelling Narrowness Distant Saviour English Adventure fiction Yes Alexandre Dumas Martial art fiction Chinese No Louis Cha Martial art fiction Chinese No Louis Cha Chinese Yes Mythology Xu Zhonglin Chinese Yes Historical fiction Luo Guanzhong English Gabriel Garc√≠a M√°rquez Magical realism No Chinese No Haruki Murakami No English Daphne du Maurier Yes English Jules Verne English Jo Nesbo No Chinese No Du Liang Chinese No Liu Liu Chinese No Dou Dou Surrealism Gothic fiction Adventure fiction Crime fiction Historical fiction Social critique Romance Total English Chinese Public English Chinese Yes Table 2: Statistics of PRELUDE. Label Count Label Count Consistent Core Irrelevant 434 270 164 Contradict Local Global 361 94 267 Table 3: Statistics of annotated labels. 2 3 4 4 2 1 2 3 4 3 3 3 4 40 13 25 54 78 44 94 28 15 22 82 86 82 70 92 48 795 319 476 262 Annotation following this flowchart is generally sufficient for people familiar with the book. However, during trial annotation, we identified three issues hence introduced the following rules: First, judgments must be based solely on the content of the original novel. Adaptations, derivative works, or historical inspirations behind the characters should not be considered. Otherwise, annotators might incorrectly flag contradiction on case consistent with the novel based on, for example, historical figures biography. Second, annotators should assume that the prequel is followed immediately by the original story, with no additional text or events in between. This rule addresses tendency among human annotators to over-interpret. For instance, when facing clear contradiction, they might imagine that some unseen event occurred between the prequel and the original story to make it eventually consistent. This rule reduces such subjectivity. Finally, unless characters statements are later explicitly revealed to be deceptive, they should be treated as factual, akin to narrative exposition. Otherwise, one could dismiss any contradiction with the canonical text as intentional deceit. Candidate Prequel Generation We prompt DeepSeek-R1 and GPT-4o to generate prequel samples, using the prompt provided in Appendix B.1. In the prompt, we explicitly instruct the LLMs to generate prequel for each character in Markdown list format. Each bullet point is treated as an individual prequel example for annotators to label according to the types defined in Table 1. Annotation Details Four annotators labeled the prequels for 40 characters across 13 books (see Appendix B.2), as shown in Table 2. These books were selected to represent diversity in genre, popularity, and original language. Two annotators are graduate students majoring in literature or related fields, while the other two major in computer science. Each annotator worked on books they were familiar with to ensure high-quality annotations.1 The label distribution in Table 3 shows that humans identified contradictions in nearly half of the generated prequels. The annotation process resulted in total of 795 instances, with each case taking approximately 10 minutes to complete. After training, the annotators reached substantial agreement, with Kappa score of 0.7828, though some subjectivity in interpretation remained. Most unresolved cases were due to differing interpretations of characters, ambiguities left by the original authors, or inherently fuzzy logic. Representative examples are shown in Appendix D.1, Examples and II. 1 By familiar, we require that the annotator has read the book multiple times and can recall the overall plot in reasonable detail. PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Model Qwen2.5-72B + RAG top-40 Qwen3-32B + RAG top-40 Qwen3-235B-A22B + RAG topDeepSeek-R1 + RAG top-40 GPT4o + RAG top-40 o3-mini + RAG top-40 Gemini-2.5-Flash + RAG top-40 Gemini-2.5-Pro + RAG topHumans Overall F1-Scores Macro-Avg Consistent Contradict Chinese-Set English-Set R1-Set GPT-Set Public-Set F1-Scores on Subsets 55.7 56.4 53.5 61.3 57.3 59.7 61.3 59. 57.8 62.9 53.5 60.0 61.8 57.8 65.1 60.7 81.7 66.4 57.9 69.7 67.3 70.1 59.7 69.2 51.3 69.9 61.2 68.8 67.0 62.4 48.9 61.4 53.7 79.5 45.0 54.9 37.4 55.4 44.5 59.7 53.4 66.9 45.8 64.6 38.2 53.0 61.1 66.8 68.9 67.8 83.9 54.8 56.9 55.3 61.7 54.9 58.8 60.3 63. 57.2 62.3 52.5 58.5 66.4 62.8 67.1 61.0 57.1 55.5 50.7 60.7 60.9 60.8 62.7 53.0 58.9 63.7 54.8 62.3 54.9 50.2 62.2 60.2 56.8 60.8 58.3 64.8 62.9 60.5 56.2 61.2 55.1 64.5 52.2 63.7 64.6 58.3 66.4 63.6 54.7 52.6 50.7 59.2 53.9 59.2 63.9 57. 58.8 61.3 53.3 57.2 59.6 57.1 64.1 58.4 59.7 56.0 55.9 64.1 59.1 60.7 66.2 61.8 62.7 63.2 56.4 64.0 60.5 52.0 62.0 59.9 Table 4: Comparison of different LLMs on the full set of PRELUDE. *: Experiments conducted on subset for reference."
        },
        {
            "title": "4 COMPARED METHODS",
            "content": "We compare multiple state-of-the-art LLMs under the following settings. The implementation details can be found in Appendix C. LLMs with Vanilla Few-Shot ICL This is the vanilla prompting approach that first presents the LLMs with the task instruction and then examples (k = 5 in our case). It does not provide book context in the input so the LLMs need to rely on their inherent parametric knowledge to solve the task. We use the prompts shown in Figure 9 and 11 from appendix, but with the field Original Excerpt omitted from the input. For open-source LLMs, we use the Instruct versions of the Qwen2.5/3 models and DeepSeek-R1. We also compare against commercial LLMs, including GPT-4o, o3-mini, and the Gemini 2.5 family, accessed via API calls. Retrieval-Augmented LLMs We enable the LLMs to access the canonical novels via retrievalaugmented generation (RAG) (Lewis et al., 2020; Guu et al., 2020). We experiment with various embedding models and hyperparameters, as detailed in Section 5.3. Our final system uses the Qwen3-Embedding-8B model to retrieve the top 40 chunks, each with length of 500 tokens. In-Domain Post-Training Prior work has shown that fine-tuning with as few as 1K examples can elicit specific capabilities in LLMs, such as mathematical reasoning or general instruction following (Zhou et al., 2023; Zhao et al., 2024; Muennighoff et al., 2025). These successes rely on the assumption that such capabilities are already present in the model acquired by pretraining thus can be activated with minimal supervision. In contrast, if an LLM lacks the potential for given capability, training with small number of examples is unlikely to produce meaningful gains. Thus, the effectiveness of low-resource in-domain training can serve as diagnostic tool to assess the intrinsic difficulty of task, as demonstrated in (Yu et al., 2025a). Following this idea, we fine-tune on our labeled dataset (excluding the human study subset, 700 examples) and evaluate on the held-out human study subset. Many-Shot ICL Similar to the in-domain training approach, many-shot ICL (Agarwal et al., 2024; Bertsch et al., 2025) provides large number of examples in the input context to elicit the latent capabilities of the LLM. We use the same data split as in the in-domain training experiment. 6 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Methods Model Source F1-Scores Accuracy Avg Consistent Contradict Answer Reason No-Context RAG (top-40, total length: 20K) Qwen3-32B Qwen3-235B-A22B DeepSeek-R1 GPT4o o3-mini Gemini-2.5-Flash Gemini-2.5-Pro Qwen3-32B Qwen3-235B-A22B DeepSeek-R1 GPT4o o3-mini Gemini-2.5-Flash Gemini-2.5-Pro In-Domain Training Qwen3-32B Many-Shot ICL DeepSeek-R1 Gemini-2.5-Pro OpenAI DeepResearch Humans o3 Book Book Book Book Book Book Book Book 53.81.6 57.43.5 65.00.7 58.81.1 50.02.0 63.43.7 64.31.6 60.50.5 63.12.5 59.14.1 60.22.7 55.71.9 60.71.5 55.81. 52.41.5 59.70.9 62.32.3 59.51.4 63.21.7 62.13.3 65.01.9 64.71.5 55.70.6 55.26.0 52.02.8 60.02.1 56.94.2 42.46.7 50.83.7 54.62.2 45.83.3 37.52.5 58.73.0 60.10.2 62.01.9 46.02. Web 62.5 58.4 44.41.5 52.83.9 65.02.0 52.91.4 44.33.7 71.61.6 76.80.4 61.03.2 69.32.3 75.91.7 69.61.8 56.71.7 75.60.4 74.10.8 46.11.5 59.22. 62.72.9 73.00.3 66.7 55.71.7 57.73.1 65.00.8 59.71.2 50.71.7 65.32.9 68.70.9 60.70.5 64.02.2 66.02.8 62.02.4 55.71.9 66.30.5 63.31.2 53.31.9 59.70.9 62.32.4 64.00. 63 Book 81.74.4 79.57.6 83.94.9 823. 47 43 45 47 47 51 Table 5: Comparing different systems on the human-study subset. : We conduct human verification on the most accurate results from the three runs. For in-domain training experiments, we use the largest model (32B) supported on our infrastructure (8xA800). For many-shot ICL experiments, we compare with the two best-performed models in the No-Context block. Commercial DeepResearch Commercial deep research services are offered by several companies, notably OpenAI DeepResearch2 and Google Gemini DeepResearch3. These services showcase the ability to retrieve and synthesize information from multiple sources to generate reports using an agentic approach. We use the web interface of OpenAI DeepResearch, which has demonstrated strong performance across wide range of everyday tasks."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "5.1 HUMAN PERFORMANCE We selected 100 examples to compute human performance. Three participants who had not involved in our task annotations and have similar backgrounds to our annotators were asked to annotate examples from books they were familiar with. The results show strong performance, with an F1 score of 81.7% (an accuracy of 82%), indicating that the task is largely solvable by humans. Upon examining the disagreements, we found that most could be resolved, as they were often due to annotators overlooking information (either from fatigue after extended work or the the fallibility of human memory). The remaining unresolved cases are of similar types to those analyzed in Section 3.3. Our study further reveals that humans tend to adopt DeepResearch-style approach, which involves iteratively generating hypotheses and resolving them by locating relevant supporting evidence. 5.2 RESULTS OF LLMS Comparison across LLMs Table 4 compares state-of-the-art open-source and commercial LLMs on our task. The Gemini-2.5-Pro model shows clear advantage over the others, yet still falls short of human performance by >15%. Another key finding is that, except for the Gemini-2.5 models, all other LLMs tend to overpredict the Consistent label when not given access to the original books, resulting in unbalanced performance. 2https://openai.com/index/introducing-deep-research/ 3https://gemini.google/overview/deep-research/ 7 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Figure 2: Accuracy the same methods when taking reasoning correctness into consideration. This suggests that these models struggle to encode details of the novels within their parametric knowledge, underscoring the importance of incorporating canonical books in our task. per-book breakdown of the results from this study and the following RAG study is provided in Appendix D.2. Impact of RAG Table 4 also presents the performance of various LLMs under the RAG setting. For most models, RAG improves the F1 score on the Contradict class. However, we also observe tendency toward over-rejection, where LLMs predict Contradict more frequently with hypercritical reasoning that focuses on minor or debatable inconsistencies. One illustrative example can be found in Appendix D.1, Example III. Notably, RAG results in worse performance for the Gemini-2.5-Pro model on both classes. This suggests that the retrieved contexts do not effectively contribute new or useful information for the strong Gemini-Pro model. It also highlights broader limitation that despite recent advancements, long-context reasoning remains persistent challenge for LLMs. Results of In-Domain Training and Many-Shot ICL As shown in Table 5, on our held-out subset, neither in-domain fine-tuning nor many-shot ICL improves performance over the baseline usage of the same LLMs respectively. It indicates that current LLMs are still fundamentally limited in the type of reasoning required for our task. Results of DeepResearch Finally, Table 5 shows that DeepResearch performs worse than the best LLMs, both with and without RAG. It is also less effective at identifying contradictory evidence compared to most RAG-based systems. Since DeepResearch primarily relies on retrieving humanwritten analyses from the Internet, these results suggest that our task cannot be solved using existing external commentary or interpretations alone. 5.3 ANALYSIS Correct Answer with Incorrect Reasoning We manually verify the outputs of the LLMs and find that, although they correctly answer large portion of the tasks, they often fail to arrive at the correct answers through valid reasoning. Due to the labor-intensive nature of this evaluation, we select the best outputs from systems that achieve the highest performance on at least one metric in each block of Table 5. We also include DeepSeek-R1 with RAG, as its best run yields the highest answer accuracy. The final column of Table 5 and Figure 2 present the results, revealing clear gap between answer accuracy and reasoning accuracy. such example can be found in Appendix D.1, Example IV. In contrast, human annotators generally agree on their reasoning, suggesting that current models still lack true comprehension in solving our task. It is noteworthy that, despite its lower answer accuracy, DeepResearch exhibits the smallest performance drop when considering reasoning accuracy. This suggests combining strong reasoning model with reflective mechanisms can lead to more reliable reasoning traces. Impact of Context Length in RAG Figure 3(a) investigates the impact of retrieved context lengths. The RAG system achieves its best performance when the input length is around 20k tokens. With shorter contexts, the retriever often fails to include important evidence due to limited capacity. 8 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Figure 3: (a) Effect of retrieved context length on RAG performance. We use Qwen3-Embedding-8B and GPT4o to generate the results on the human study subset. (b) Study of the impact of model sizes under the best setting from (a). Retriever Top-k Len Macro-Avg Consis. Contra. Overall F1-Scores No-RAG Qwen3-8B + sorted Qwen3-8B BGE-M3 BGE-M 40 40 20 40 20 500 500 1k 500 1k 57.8 62.9 60.1 61.9 61.4 59.7 69.9 61.2 58.8 62.3 61.9 59.1 45.8 64.6 61.5 61.4 60.9 60.4 Table 6: Comparison of different retrieval methods with the same context length. We use GPT4o to generate the results. Conversely, with longer contexts, the excess information can overwhelm the LLM and hinder its ability to effectively utilize the extended context. Different Retrieval Methods Table 6 provides ablation study on our retrieval method using the following variations: Sorting the retrieved chunks according to their order in the books: While this intuitively provides more coherent context, it overlooks chunk relevance, resulting in decreased performance. Doubling the chunk size while keeping the input length: This causes slight performance drop, likely due to the reduced effectiveness of embedding models on longer chunks (Wu et al., 2025a). Replacing the embedding model with BGE-M3: This slightly reduces performance. Replacing with BGE-M3 while doubling the chunk size: This results in further decrease in performance, likely due to BGE-M3s weaker handling of long inputs. Effect of Model Scaling Figure 3(b) shows how model performance changes with increasing model size. We experiment with both the Qwen2.5 and Qwen3 series. As shown in Table 4, Qwen models rely on RAG to incorporate additional knowledge and improve performance. Therefore, all experiments in this section are conducted under the RAG setting, with each model evaluated over three runs. The results indicate that for both Qwen series, performance consistently improves as model size increases. However, this improvement begins to plateau beyond the 32B model. Notably, the 235B model even outperforms the larger 671B R1 model on certain metrics, suggesting that simply scaling up model size is not efficient to our task."
        },
        {
            "title": "6 DISCUSSIONS",
            "content": "Limited Long Context Reasoning Capability in Recent LLMs Our results in Table 4 show that while some LLMs benefit from retrieved contexts, advanced reasoning models, such as DeepSeek-R1 and Gemini-2.5, exhibit notable performance drop when context is provided. This gap becomes more pronounced as the base models grow stronger. 9 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts These findings suggest that recent improvements in LLMs general reasoning capabilities do not necessarily translate to better long-context reasoning. One possible explanation is that as models become more powerful, their internal (parametric) knowledge is more efficient to solve tasks, making them prone to ignore external inputs during post-training. This highlights the need for improved training data and strategies specifically designed to encourage long-context reasoning. Model Bias in Our Dataset Construction Method If our dataset construction method introduced bias, we would expect model to perform worse on examples it generated itself, assuming that LLMs inherently trust their own outputs. However, the results in Table 4 show that this is not the case, indicating that our construction process does not introduce significant bias toward any particular model family. This also suggests that state-of-the-art LLMs do not inherently trust their own generations and remain susceptible to hallucination, even when evaluating content they previously produced. Measuring Fluid Intelligence in Natural Language Space Combined with the observation that DeepResearch performs poorly on our task, it becomes evident that the task cannot be solved simply by retrieving existing information from the web. Instead, it requires generating new knowledge through reasoning based on learned rules, aligning with the notion of fluid intelligence tests (Chollet, 2019; Chollet et al., 2025; Yu et al., 2025a). Unlike prior work, our task represents the first fluid intelligence assessment conducted entirely in the natural language space."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduce PRELUDE, new benchmark for evaluating long-context comprehension and reasoning in LLMs. Our task design addresses several key shortcuts present in prior long-context benchmarks. Experiments show that state-of-the-art models still fall significantly short of human performance, particularly in generating valid reasoning. PRELUDE calls for further research into robust longcontext understanding and the development of models with stronger global reasoning capabilities."
        },
        {
            "title": "LIMITATIONS",
            "content": "Our task empirically mitigates the shortcuts observed in prior work (as shown in Figure 1). However, due to the inherent complexity of long-context reasoning and the subjective nature of interpreting literary narratives, human performance on our task is also non-perfect. In future work, we aim to improve the annotation framework to further enhance inter-annotator consistency and reduce the annotation and human study workload of the task."
        },
        {
            "title": "REFERENCES",
            "content": "Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie C.Y. Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=AB6XpMzvqH. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2023. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024a. URL https://doi.org/10.18653/v1/2024. acl-long.172. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024b. 10 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 1211912149, 2025. Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. Booookscore: systematic exploration of book-length summarization in the era of llms. arXiv preprint arXiv:2310.00785, 2023. Fran√ßois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025. Gr√©goire Del√©tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. BAMBOO: comprehensive benchmark for evaluating long text modeling capacities of large language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 20862099, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.188/. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. Google DeepMind. NotebookLM: Your AI-Powered Research Assistant. https://notebooklm. google/, 2024. Accessed: 2025-07-24. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. Advances in Neural Information Processing Systems, 37:5953259569, 2024. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrievalaugmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. Shousheng Jia Haosheng Zou, Xiaowei Lv and Xiangzheng Zhang. 360-llama-factory, 2024. URL https://github.com/Qihoo360/360-LLaMA-Factory. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. URL https://arxiv.org/abs/2505.23885. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. 11 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Gregory Kamradt. Needle in haystack - pressure testing llms. https://github.com/ gkamradt/LLMTestNeedleInAHaystack/tree/main, 2023. Accessed: 2025-07-23. Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer. One thousand and one pairs: \"novel\" challenge for long-context language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024. emnlp-main.948. Tom√°≈° KoÀácisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G√°bor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. URL https://aclanthology. org/Q18-1023.pdf. Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Igorevich Sorokin, Artyom Sorokin, and Mikhail Burtsev. BABILong: Testing the limits of LLMs with long context reasoning-in-ahaystack. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=u7m2CG84BQ. Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, and Jaewoo Kang. ETHIC: Evaluating large language models on long-context tasks with high information coverage. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 54975512, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/ 2025.naacl-long.283. URL https://aclanthology.org/2025.naacl-long.283/. Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle: Can long-context language models understand long contexts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.859. Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, and Minhao Cheng. Lara: Benchmarking retrieval-augmented generation and long-context llmsno silver bullet for lc or rag routing. arXiv preprint arXiv:2502.09977, 2025a. Mo Li, Songyang Zhang, Taolin Zhang, Haodong Duan, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in information-dense context?, 2025b. URL https:// arxiv.org/abs/2407.11963. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context LLMs struggle with long in-context learning. Transactions on Machine Learning Research, 2025c. ISSN 2835-8856. URL https://openreview.net/forum?id=Cw2xlg0e46. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, and Piji Li. Xl2 bench: benchmark for extremely long context understanding with long-range dependencies. arXiv preprint arXiv:2404.05446, 2024. OpenAI. Deep research system card. Technical report, OpenAI, 2025. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 56875711, 2023. 12 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, and Irwin King. Clongeval: In Findings of the chinese benchmark for evaluating long-context large language models. Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024.findings-emnlp.230. Michael Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. Averitec: dataset for real-world claim verification with evidence from the web. Advances in Neural Information Processing Systems, 36: 6512865167, 2023. Ilya Sutskever. theory of unsupervised learning, 2023. URL https://www.youtube.com/ watch?v=AKMuA_TVz3A. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:3827438290, 2022. David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. arXiv preprint arXiv:2004.14974, 2020. Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen. Ada-LEval: In Kevin Duh, Helena Evaluating long-context LLMs with length-adaptable benchmarks. Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 37123724, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.205. URL https: //aclanthology.org/2024.naacl-long.205/. Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Qian Wang, and Yue Zhang. Novelqa: benchmark for long-range novel question answering. CoRR, abs/2403.12766, 2024b. URL https://doi.org/10.48550/arXiv.2403. 12766. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 56275646, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 322. URL https://aclanthology.org/2024.emnlp-main.322/. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 56275646, 2024d. Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Xing Jin, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, and Mo Yu. Sitemb-v1.5: Improved context-aware dense retrieval for semantic association and long story comprehension, 2025a. URL https://arxiv.org/abs/2508.01959. Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, and Roy Ka-Wei Lee. Shifting long-context llms research from input to output. arXiv preprint arXiv:2503.04723, 2025b. 13 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. Nystr√∂mformer: nystr√∂m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1413814148, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/17664. Liyan Xu, Jiangnan Li, Mo Yu, and Jie Zhou. Fine-grained modeling of narrative context: coherence perspective via retrospective questions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 58225838, 2024a. Nan Xu and Xuezhe Ma. Llm the genius paradox: linguistic and math experts struggle with simple word-based counting problems. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 33443370, 2025. Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025a. Xiaoyue Xu, Qinyuan Ye, and Xiang Ren. Stress-testing long-context language models with lifelong ICL and task haystack. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id= j6PTT6NB2O. Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. DetectiveQA: Evaluating long-context reasoning on detective novels. In Workshop on Reasoning and Planning for Large Language Models, 2025b. URL https://openreview.net/forum?id=9ExIs5ELlk. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of EMNLP 2018, pp. 23692380, 2018. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. Wenpeng Yin, Dragomir R. Radev, and Caiming Xiong. Docnli: large-scale dataset for documentlevel natural language inference. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, 2021. URL https://doi.org/10. 18653/v1/2021.findings-acl.435. Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Zhou Xiao, Fandong Meng, and Jie Zhou. Personality understanding of fictional characters during book reading. arXiv preprint arXiv:2305.10156, 2023. Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, Dit-Yan Yeung, and Jie Zhou. The stochastic parrot on llms shoulder: summative assessment of physical concept understanding. arXiv preprint arXiv:2502.08946, 2025a. Yifei Yu, Qian-Wen Zhang, Lingfeng Qiao, Di Yin, Fang Li, Jie Wang, Zengxi Chen, Suncong Zheng, Xiaolong Liang, and Xing Sun. Sequential-niah: needle-in-a-haystack benchmark for extracting sequential needles from long contexts. arXiv preprint arXiv:2504.04713, 2025b. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, 2024. URL https://doi.org/10.18653/v1/2024.acl-long.814. 14 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: simple but tough-to-beat baseline for instruction fine-tuning. In International Conference on Machine Learning, pp. 6067460703. PMLR, 2024. Chulun Zhou, Qiujing Wang, Mo Yu, Xiaoqian Yue, Rui Lu, Jiangnan Li, Yifan Zhou, Shunchi Zhang, Jie Zhou, and Wai Lam. The essence of contextual understanding in theory of mind: study on question answering with story characters. arXiv preprint arXiv:2501.01705, 2025. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. 15 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts ASSESSMENT OF ASSESSMENTS: WHAT MAKES LONG-CONTEXT BENCHMARK TRULY MEANINGFUL? For each criterion discussed in Section 1, we first go through the definition and examples of the criterion, then propose its delegate measure: Beyond Memorization. LLMs are known to memorize large amounts of training data (Tirumala et al., 2022; Del√©tang et al., 2023; Sutskever, 2023). For popular texts (e.g., widely read books), models may recall content or associated analysis from pretraining, bypassing the need for actual comprehension. As training datasets continue to expand, this issue becomes increasingly problematic. Therefore, benchmark should be designed such that it cannot be solved purely through memorized knowledge, ensuring that the full length and structure of the context remain necessary for reasoning. Measurement: We first evaluate the memorization performance of GPT-4o by having it answer questions without using RAG. We then compute its quantile within the range defined by human and random performance. This measurement is illustrated in Figure 4. Figure 4: Illustration of the measurement for the criterion of Beyond Memorization. Global Dependency. The task should require aggregating information from multiple pieces of evidence that are scattered across the context or exhibit global dependencies. Otherwise, it reduces to short-context problem solvable by retrieving the relevant passage for given question. In such cases, the task becomes more about improving retrieval quality than longcontext understanding ability. Measurement: This dimension differs slightly from the previous one in how the performance interval is defined. To evaluate how much better model performs compared to using single document, the reference point should be the models performance with sufficient evidence, rather than human performance. To approximate this, we retrieve the top-20 documents using both the question and the answer, and treat this as the upper bound of performance with sufficient evidence. We then ask GPT-4o to select the best supporting document from the top-20 retrieved using both the question and answer, treating it as the strongest single piece of evidence. We compute the quantile of the models RAG performance using only this selected document, relative to the interval defined by the QA-top-20 RAG performance and the random baseline. Figure 5: Illustration of the measurement for the criterion of Global Dependency. Depth of Reasoning. By nature, long-context reasoning involves synthesizing multiple pieces of evidence across the input. The complexity of reasoningespecially multi-step deductionis closely tied to task difficulty. Many existing benchmarks emphasize shallow reasoning, primarily requiring decomposition or enumeration (e.g., counting mentions of fact (Xu & Ma, 2025), or multi-hop QA where the questions are often constructed as sequences of relatively simple subquestions (Yang et al., 2018). This limits the need for global reasoning and makes tasks easier than intended. Measurement: Similar to the previous dimension, the key in this measurement lies in identifying representative performance interval and suitable reference model for computing the quantile. We choose QwQ-32B as strong reasoning model and treat its RAG performance using the top-20 documents retrieved with both the question and answer (as defined in the previous measurement) as the upper bound. We then compute the quantile of Qwen2.5-7B under the same RAG setting. This gap reflects the potential for Figure 6: Illustration of the measurement for the criterion of Depth of Reasoning. 16 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts Criterion Metric Beyond Memorization Global Dependency Depth of Reasoning Random Human Performance Human-Random Gap Memorization Performance Quantile of Memorization RAG (qa-top20) RAG (best one from qa-top20) Quantile of RAG (best-one) Qwen2.5-7B w/ RAG qa-top20 QwQ-32B w/ RAG qa-top20 Quantile of Qwen2.5-7B Beyond Summarization RAG over chunks (max-10K) RAG over chunk summaries (max-10K) Relative Improvement Human-Machine Gap Best Machine Performance Quantile of Machine Performance Average Doc Length InfBench.MC DetectiveQA NarrativeQA NoCha (public) PRELUDE Datasets 25.0 100.0 75. 77 30.7 85.0 84.0 1.7 68.0 86.0 29.5 83.0 80.1 3.6 83.0 22.7 228K 25.0 94.0 69.0 50.5 63.1 73.3 55.2 37.4 68.6 81.0 22.1 77.4 77.2 0.3 77.4 24. 97K 7.1 64.3 57.2 36.1 49.3 35.9 32.9 10.6 24.0 42.3 52.1 23.9 27.1 -11. 36.1 49.3 104K 50.0 97.0 47.0 80.2 35.8 73.8 57.4 68.9 60.3 66.7 38. 68.3 73.8 -7.5 80.2 35.8 50.0 82.0 32.0 56 81.3 59.0 53.0 66.7 52.0 56.0 66. 61.0 59.0 3.4 61.0 65.6 153K 408K Table 7: The detailed experimental results we used to compute the measurement of representative criteria for good long-context understanding benchmarks. Except for the Qwen2.5-7B and QwQ-32B results, we generate the results with Qwen3-Embedding-8B and GPT4o. *The paper did not provide the human performance but claimed humans can achieve near-perfectly so we trust their claim. improvement attributable to stronger reasoning capabilities. Thus, the deeper the reasoning required by dataset, the larger this gap is expected to be. Note that this proxy becomes less informative if the dataset is so challenging that neither model significantly outperforms the random baseline. However, as shown in Table 7, this is not currently the case. Therefore, this measurement remains meaningful indicator. Beyond Summarization/Salience. This often-overlooked criterion is crucial: Tasks that can be resolved simply by generating summary of long input are less challenging and may no longer probe deep understanding, especially given recent advances in summarization (e.g., hierarchical or iterative methods (Chang et al., 2023)). In such cases, the challenge of long-context understanding is effectively reduced to summarization task over short segments, creating shortcut. highquality benchmark should instead require attention to fine-grained details that go beyond high-level abstraction, or it can be reduced to relatively easier task of summarization thus is less meaningful. Measurement: This dimension is measured by comparing the performance of RAG using original text chunks against RAG using chunk summaries of the same input length. We do not use quantiles here but instead directly report the relative improvement, as some datasets actually perform worse when using the original texts, indicating that the task questions primarily target salient events, making summarization sufficient for answering. Human-Machine Gap. To highlight essential capabilities that general-purpose intelligent systems ought to possess, meaningful benchmark should show significant gap between humans and machines. At the same time, poor human accuracy or inter-annotator agreement typically signals low annotation quality or high subjectivity both detrimental to robust evaluation. Measurement: We identify the best machine performance from the evaluations in the Beyond Memorization and Beyond Summarization dimensions and compute its quantile within the interval defined by human performance and the random baseline. Figure 7: Illustration of the measurement for the criterion of HumanMachine Gap. Compared Datasets. We compare our PRELUDE with representative benchmarks from prior work, including (KoÀácisk`y et al., 2018; Karpinska et al., 2024; Zhao et al., 2024; Xu et al., 2025b). These widely used datasets span multiple genres such as classic and 17 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts detective novels, support both English and Chinese languages, and cover range of task formats including free-form QA, multiple choice, and true-or-false questions. For fair comparison, we sample 100 questions from each benchmark: InfBench.MC (25 books), DetectiveQA (10 books), and NarrativeQA (10 books from the development set), ensuring similar number of examples across datasets."
        },
        {
            "title": "B DETAILS OF THE DATASET",
            "content": "In this section we provide the detailed characters and questions used in our study in both English and Chinese dataset. The public subset will be included in our code and data released upon publication. B.1 PROMPT FOR GENERATING PREQUELS We simple prompt the LLMs with no additional context to generate the prequels for human annotation. Figure 8 demonstrates the English translation of our prompt. You are writing assistant who is deeply familiar with the plots of various renowned literary works and highly skilled in literary analysis. Please write prequel story for the character \"{{ char }}\" based on the plot of the novel {{ book_name }}. The story should provide background that does not contradict the original work. It should explain all of the characters major actions in the original novel, their key personality traits, and their major relationships. Your prequel should focus on reasonably answering unresolved questions about the character that are raised in the original novel. These include: important issues that cannot be explained using the original content alone, plot points that appear inconsistent with the characters established traits or internal logic, and significant past experiences that are mentioned but not elaborated upon. The background story must remain coherent with the original plot and should not introduce settings or elements that are clearly inconsistent with the style of the original work. You only need to write an outline. The outline should summarize key elements of the characters backstory and major life events. Each item in the outline should include specific story-level details, rather than general concepts or attributes. The total length of the outline should be within 1,000 words. Please present your answer in Markdown list format. Figure 8: The prompt we used to generate prequels for human annotation (English translation). B.2 SELECTED CHARACTERS Table 8 lists the main characters used to in our PRELUDE for annotation. Name Author Characters The Count of Monte Cristo Demi-Gods and Semi-Devils The Return of the Condor Heroes Investiture of the Gods Romance of the Three Kindoms Love in the Time of Cholera Pinball, 1973 Rebecca In Search of the Castaways The Redeemer Drawing Sword Dwelling Narrowness Distant Saviour Attack on Titan Alexandre Dumas Louis Cha Louis Cha Xu Zhonglin Luo Guanzhong Gabriel Garc√≠a M√°rquez Haruki Murakami Daphne du Maurier Jules Verne Jo Nesbo Du Liang Liu Liu Dou Dou Hajime Isayama & the Fans Faria, Noirtier, General de Quesnel Bo Murong, Yellow-Browed Monk, Sweeper Monk Yin Zhiping, Yelv Qi, Huo-Du, Gongsun Zhi Wen Zhong, Zhao Gongming, Zhang Kui, Huang Feihu Xiahou Yuan, Taishi Ci Don Pius Loayza, Lotario Thugut, Rosalba The Rat, J, Naoko Mrs. Van Hopper, Mrs. Danvers, Jack Favell Jacques Paganel, Tom Ayrton/Ben Joyce, Thalcave, Kai-Koumou Christo Stankic, Martine Eckhoff, Jon Karlsen Tian Moxuan, Ding Wei, Yamamoto Kazuki Chen Sifu, Mrs. Song, Su Chun Liu Bing, Han Chufeng, Xiao Yawen, Lin Yufeng Eren Kruger Table 8: Main characters considered in our PRELUDE."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 HYPERPARAMETERS OF IN-DOMAIN TRAINING We apply LoRA (Hu et al., 2021) to all available target modules for in-domain training with RAG and non-RAG settings. The training is conducted on the dataset excluding the human-study subset, 18 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts resulting in total of 695 instances. Since the average token length in the RAG setting is above 20k, we perform LoRA tuning on both settings using 360-LLaMA-Factory (Haosheng Zou & Zhang, 2024). We fine-tune each model for 2 epochs with batch size of 1 on single machine with 8 H20 GPUs. The dimension of LoRAs attention layer is set to 64, while Œ± is configured to 128. In addition, the learning rate and warmup ratio are set to 1e-4 and 0.1 respectively. C.2 PROMPT TEMPLATES We use the prompt template in Figure 9 and 11 for the RAG experiment, depending on whether the instance originates from Chinese book or an English book. For the No Context experiment, we remove the original excerpt part both in the ICL examples and the actual instance. The exact content of ICL in both languages is in Figure 10 and 12."
        },
        {
            "title": "D ADDITIONAL RESULTS",
            "content": "D."
        },
        {
            "title": "ILLUSTRATIVE EXAMPLES FOR QUALITATIVE STUDY",
            "content": "Subjective Example Fuzzy Logic: Example shows an example that our annotators felt difficult to arrive an agreement due to the fuzzy reasoning. This setting results in the fact that the character appears in the story at around 120 years old. The novel does feature several characters in their 100s who remain healthy and skilled in combat. Still, as the most advanced martial artist in the story, whether 120 is reasonable age is matter of fuzzy logic. Example I. An example of disagreement among human annotators due to fuzzy reasoning. Book: Demi-Gods and Semi-Devils Char: Sweeper Monk Groundtruth: Consistent Prequel: His birth name was Li Xuankong, collateral member of the imperial family of Later Zhou during the late Five Dynasties period. As child, he was orphaned by war and taken in by hermit from Mount Song, who was secretly first-generation disciple of the Xiaoyao Sect (unaffiliated with the internal conflict within Wuya Zis lineage). Subjective Example Different Interpretation of Persona (Example II): In the story, the Sweeper Monk is not formal disciple of Shaolin; he is more like hermit. So the disagreement arises that when others mention that many Shaolin disciples had learned Xiaowuxiang Gong, whether the Sweeper Monk would interpret that as referring to himself? The annotators would project themselves into the characters perspective when making predictions, which introduces subjectivity. Example II. An example of disagreement among human annotators due to subjective interpretation of character persona. Book: Demi-Gods and Semi-Devils Char: Sweeper Monk Groundtruth: Consistent Prequel: As young prodigy, he simultaneously studied the incomplete version of Shaolins Yijin Jing and the Xiaoyao Sects Xiaowuxiang Gong, though he never formally joined any sect. Example that RAG Performs Worse: Example III gives an example in which the vanilla LLM can predict correct answer but fails when equipped with RAG. The prequel is consistent because it does not violate any part of the story. Specifically, in flashback scene from the original novel, it is revealed that Murong Bos mother subjected him to harsh training in order to raise him as future monarch for national restoration. When the RAG system fails to recognize that this passage describes Murong Bos childhood, it tends to rely solely on the more prominent information in the input (e.g., Document 0) to make its judgment. Example III. An example that RAG makes the performance worse. Book: Demi-Gods and Semi-Devils Char: Murong Bo Groundtruth: Consistent Prequel: On her deathbed, his mother pressed bloodstained jade pendant engraved with the Yan dynasty dragon emblem into his hand, repeatedly stressing that the Xianbei Murong clan and the Zhao-Song regime cannot coexist. Models Reason for Contradict: In the original novel, Murong Bos restorationist education clearly stems from paternal lineage. His mother is portrayed merely as an ordinary family member. The added element of \"the mother reinforcing the restorationist ideology\" has no textual basis and contradicts the Murong clans patrilineal transmission system of from father to son, from grandfather to grandson (see Document 0). Example with Correct Answer but Incorrect Reasoning: In Example IV, the model correctly answers the question, but bases on incorrect reasons. The actual basis for judging this as contradiction lies in the fact that both characters are portrayed as emphasizing personal martial prowess, with no 19 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts demonstration of strategic ability or any depiction of possessing military treatises in the original novel. The act of gifting The Six Secret Teachings (Liu Tao)a text regarded in Chinese tradition as one of the most profound and authoritative works on military strategystands in sharp contrast to their characterization. The models explanation is invalid because it fails to grasp that Wu Anguo was representing Beihais forces (under Kong Rong) in the battle at Hulao Pass. Instead, it superficially interprets the text and incorrectly assumes that Wu Anguos sphere of activity was limited to Hulao Pass. This reflects the same underlying issue as in the RAG failure example: LLMs are heavily influenced by the provided context and tend to overanalyze non-informative clues, often becoming overly literal and fixating on minor wording differences. Example IV. An example that LLMs make correct prediction with incorrect reasoning. Book: Romance of the Three Kingdoms Char: Taishi Ci Groundtruth: Contradict Prequel: At the age of seventeen, while escorting his mother on visit to relatives, he encountered remnants of the Yellow Turban rebels. In the midst of fierce battle, he rescued Wu Anguo, the gate captain of Beihai, who was under siege. Moved by his bravery, Wu Anguo gifted him handwritten copy of The Six Secret Teachings by Jiang Ziya. Models Reason for Contradict: In the original novel, Wu Anguos only appearance in Romance of the Three Kingdoms is in Chapter 5, where he has his wrist severed by L√º Bu during the battle at Hulao Pass. He is never mentioned again, nor is there any record of him serving as gate captain in Beihai. This character setting not only fabricates an encounter between Wu Anguo and teenage Taishi Ci (violating Rule 1), but also forcibly relocates Wu Anguoa defeated general from the Hulao Pass battleto post in Beihai, disrupting the narrative continuity of that battle (violating Rule 2). D.2 PERFORMANCE DECOMPOSITION TO BOOKS Book Name #Samples GPT4o + RAG R1 + RAG Gemini-2.5-Pro + RAG The Count of Monte Cristo Demi-Gods and Semi-Devils The Return of the Condor Heroes Investiture of the Gods Romance of the Three Kingdoms Love in the Time of Cholera Pinball, 1973 Rebecca In Search of the Castaways The Redeemer Drawing Sword Dwelling Narrowness Distant Saviour 54 78 44 94 28 15 22 82 86 82 70 92 48 66.7 50.0 56.8 59.6 64.3 46.7 68.2 67.1 70.9 42.7 64.3 70.7 54.2 70.4 65.4 61.4 62.8 46.4 53.3 68.2 64.6 62.8 53.7 65.7 60.9 60.4 68.5 56.4 65.9 61.7 64.3 66.7 54.5 68.3 74.4 48.8 64.3 65.2 56.2 70.4 66.7 65.9 67.0 75.0 60.0 68.2 47.6 48.8 58.5 60.0 57.6 64. 72.2 75.6 72.7 69.1 57.1 73.3 63.6 64.6 51.2 64.6 67.1 64.1 60.4 75.9 64.1 77.3 69.1 46.4 53.3 54.5 65.9 47.7 61.0 62.9 53.3 66.7 Table 9: Performance decomposition to books. We report accuracy on each book. 20 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts ‰Ω†ÊòØ‰∏Ä‰ΩçÂØπÂêÑÁßçËëóÂêçÊñáÂ≠¶‰ΩúÂìÅÂâßÊÉÖ‰∫ÜÂ¶ÇÊåáÊéå‰∏îÁ≤æÈÄöÊñáÂ≠¶ÂàÜÊûêÁöÑÂÜô‰ΩúÂä©Êâã ÊàëÊ≠£Âú®‰∏∫Â∞èËØ¥{{ book_name }}‰∏≠ÁöÑ‰∫∫Áâ©{{ char }}ÂÜôËØ•‰∫∫Áâ©ÁöÑÂâç‰º†ÊïÖ‰∫ã‰Ω†Â∞ÜÈòÖËØªÂà∞ÊàëÂÜôÁöÑÂâç‰º†‰∏≠ÁöÑ‰∏ÄÊù°ÂâßÊÉÖËÆæÂÆöÂπ∂Â∏ÆÂä© ÊàëÂà§Êñ≠Ëøô‰∏ÄÈ°πÂâßÊÉÖÊòØÂê¶‰ºö‰∏éÂéü‰π¶‰∏≠ÁöÑÂâßÊÉÖÂíåËÆæÂÆöÂΩ¢ÊàêÁüõÁõæ Âà§Êñ≠ËßÑÂàôÂ¶Ç‰∏ã 1. Â¶ÇÊûúÂéü‰π¶‰∏≠Â≠òÂú®‰∏Ä‰∏™ÁªÜËäÇ‰∏éÊàëÊèê‰æõÁöÑÂâßÊÉÖËÆæÂÆöÁõ¥Êé•ÊûÑÊàêÁüõÁõæËØ∑ÂõûÁ≠îÁüõÁõæ 2. Â¶ÇÊûúÊàëÊèê‰æõÁöÑÂâßÊÉÖËÆæÂÆöÂØºËá¥Âéü‰π¶‰∏≠ÈÉ®ÂàÜÂâßÊÉÖÊàñËÄÖ‰∫∫Áâ©Ë°å‰∏∫Âä®Êú∫Êï¥‰Ωì‰∏çÂêàÁêÜËØ∑ÂõûÁ≠îÁüõÁõæ 3. Â¶ÇÊûúÊàëÊèê‰æõÁöÑÂâßÊÉÖËÆæÂÆö‰∏éÂéü‰π¶‰∏≠ÂâßÊÉÖ‰∏çÊûÑÊàêÁüõÁõæ‰ΩÜÊòØ‰∏éÂéü‰π¶ÁöÑËÆæÂÆöÈ£éÊ†º‰∏çÁªü‰∏ÄËØ∑ÂõûÁ≠îÁüõÁõæ 4. Â¶ÇÊûúÊàëÊèê‰æõÁöÑÂâßÊÉÖËÆæÂÆö‰∏éÂéü‰π¶‰∏çÂ≠òÂú®‰ª•‰∏äÁ±ªÂûãÁöÑÊòéÊòæÁüõÁõæËØ∑ÂõûÁ≠î‰∏ÄËá¥ 5. ËØ∑Âú®‰Ω†ÁöÑÂõûÁ≠î‰∏≠ÁªôÂá∫‰Ω†ÁöÑÂà§Êñ≠Âπ∂Êèê‰æõÁÆÄË¶ÅÁöÑÂà§Êñ≠‰æùÊçÆ 6. ‰Ω†Â∞ÜË¢´Êèê‰æõ‰∏Ä‰∫õÂéü‰π¶‰∏≠ÂèØËÉΩÁöÑÁõ∏ÂÖ≥ÁâáÊÆµ‰Ωú‰∏∫ÂèÇËÄÉ‰ΩÜÊòØËøô‰∫õÂÜÖÂÆπÂèØËÉΩ‰∏çÂ§üÂÖ®Èù¢Âõ†Ê≠§ËØ∑ÁªìÂêà‰Ω†ÂØπËØ•‰π¶ÁöÑÁü•ËØÜÂíåÊâÄÊèê‰æõÁöÑÂéüÊñáÁâáÊÆµËøõË°å ÂõûÁ≠î 7. ‰Ω†Â∫îËØ•Âè™Âü∫‰∫éÂ∞èËØ¥Âéü‰π¶ÁöÑÂÜÖÂÆπËøõË°åÂà§Êñ≠‰∏çË¶ÅËÄÉËôëÂÖ∂‰ªñÁõ∏ÂÖ≥‰ΩúÂìÅÊîπÁºñÊàñËÄÖ‰∫∫Áâ©ÂéÜÂè≤ÂéüÂûã 8. Âú®Âà§Êñ≠Êó∂‰Ω†ÈúÄË¶ÅÂÅáËÆæÊï¥‰∏™ÊïÖ‰∫ãÁî±ËØ•ÂâßÊÉÖËÆæÂÆöÁöÑÂâç‰º†ÊïÖ‰∫ãÂíåÂéü‰π¶Áõ¥Êé•ÊâøÊé•ÊûÑÊàê‰∏§ËÄÖ‰∏≠Èó¥Ê≤°ÊúâÂÖ∂‰ªñÊïÖ‰∫ãÂèëÁîü 9. Èô§ÈùûÊüê‰∏™‰∫∫Áâ©ÁöÑÂè∞ËØçÂú®ÂêéÈù¢Ë¢´ÊòæÂºèËØÅÊòéÊòØË∞éË®ÄÂê¶Âàô‰Ω†Â∫îËØ•Â∞ÜÂÖ∂ËÆ§‰∏∫ÊòØ‰∫ãÂÆûÂêåÁêÜÊâÄÊúâÊóÅÁôΩ‰πüÂ∫îË¢´ËÆ§‰∏∫ÊòØ‰∫ãÂÆû ‰ª•‰∏ã‰∏∫ËØ•‰ªªÂä°ÁöÑ‰∏Ä‰∫õÁ§∫‰æãÊØè‰∏™Á§∫‰æãÂØπÂ∫îÁöÑÁõ∏ÂÖ≥ÂéüÊñáËæÉÈïøÊïÖÁï•Âéª <example_1> [example_1] </example_1> <example_2> [example_2] </example_2> <example_3> [example_3] </example_3> <example_4> [example_4] </example_4> <example_5> [example_5] </example_5> ËØ∑Ê†πÊçÆ‰ª•‰∏äÁ§∫‰æãÂÆåÊàê‰ª•‰∏ã‰ªªÂä° [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥{{ book_name }} ‰∫∫Áâ©{{ char }} Áõ∏ÂÖ≥ÂéüÊñá <chunks>{{ chunks }}</chunks> Ê†áÈ¢ò{{ subject }} ËÆæÂÆö{{ content }} ËØ∑‰ªøÁÖß‰ª•‰∏ä‰æãÂ≠êÁöÑËæìÂá∫Ê†ºÂºèÁªôÂá∫‰Ω†ÁöÑÂõûÁ≠î Figure 9: Chinese prompt template used for the RAG experiment in Table 5, with contents of [example_i] presented in Figure 10. 21 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts [example_1]: [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥Ë•øÊ∏∏ËÆ∞ ‰∫∫Áâ©ÈáëËßíÂ§ßÁéã‰∏éÈì∂ËßíÂ§ßÁéã Áõ∏ÂÖ≥ÂéüÊñáÁúÅÁï• Ê†áÈ¢ò‰∏ãÁïåÂõ†Êûú ËÆæÂÆö‰∫îÁôæÂπ¥ÂâçÂ≠ôÊÇüÁ©∫Ë∏¢ÁøªÂÖ´Âç¶ÁÇâÊó∂ÈÅóËêΩ‰∏âÂùóÁÅ´Á†ñÂØπÂ∫îÂπ≥È°∂Â±±‰∏âÂ≥∞ÂèóËÄÅÂêõÂØÜ‰ª§‰∏ãÁïåÂõûÊî∂ÁÅ´Á†ñ [ËæìÂá∫] Think: Âéü‰π¶‰∏≠ÂÖ´Âç¶ÁÇâÈÅóËêΩÁÅ´Á†ñÂΩ¢ÊàêÁöÑÊòØÁÅ´ÁÑ∞Â±±ËÄå‰∏çÊòØÂπ≥È°∂Â±±Âõ†Ê≠§Á¨¶ÂêàËßÑÂàôÁ¨¨‰∏ÄÊù°‰∏éÂéü‰π¶ÁªÜËäÇÁüõÁõæ Answer: ÁüõÁõæ [example_2]: [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥Ë•øÊ∏∏ËÆ∞ ‰∫∫Áâ©ÈªÑÁãÆÁ≤æ Áõ∏ÂÖ≥ÂéüÊñáÁúÅÁï• Ê†áÈ¢òÂàùÁîü‰∏éÁÅµÊ†πÊ∏äÊ∫ê ËÆæÂÆöÊú∫ÁºòÂ∑ßÂêà‰∏ãÂÆÉÂæóËé∑‰∏ÄÂè™ÈªÑÁéâÁãÆÁà™Ê≥ïÂô®ÈõèÂΩ¢ÂêéË¢´ÂÖ∂‰øÆÁÇº‰∏∫‰∏ªË¶ÅÊ≠¶Âô®‰∏∫Êó•ÂêéÊàòÊñóÂ¢ûÊ∑ªÁã¨ÁâπÂ®ÅÂäõ [ËæìÂá∫] Think: Âéü‰π¶‰∏≠ÈªÑÁãÆÁ≤æ‰ªéÊù•Ê≤°ÊúâÁî®ËøáÈªÑÁéâÁãÆÁà™ËøôÁßçÊ≥ïÂô®‰ªñÁöÑÊ≠¶Âô®ÊòØÂõõÊòéÈì≤Âõ†Ê≠§ËØ•ËÆæÂÆöÁ¨¶ÂêàËßÑÂàôÁ¨¨‰∫åÊù°‰∏éÂéü‰π¶Â§öÂ§ÑÂâßÊÉÖÊûÑÊàêÊï¥‰Ωì‰∏ç‰∏Ä Ëá¥ Answer: ÁüõÁõæ [example_3]: [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥‰∏âÂõΩÊºî‰πâ ‰∫∫Áâ©Â§è‰æØÊ∏ä Áõ∏ÂÖ≥ÂéüÊñáÁúÅÁï• Ê†áÈ¢òÂØíÈó®Â¥õËµ∑ ËÆæÂÆöÂæóÂºÇ‰∫∫‰º†ÊéàÁ•ûË°åÊ≥ïÈÅìÊúØÂõ†Ê≠§Êó•ÂêéÂ∏¶ÂÖµÂèØ‰ª•‰∏âÊó•‰∫îÁôæÂÖ≠Êó•‰∏ÄÂçÉ [ËæìÂá∫] Think: ËôΩÁÑ∂‰∏éÂéü‰π¶ÂâßÊÉÖ‰∏çÁüõÁõæ‰ΩÜÊòØÂéü‰π¶‰∏âÂõΩÊºî‰πâÊï¥‰Ωì‰øùÊåÅÂÜôÂÆûÈ£éÊ†ºÂÖ∂‰∏≠ÁöÑÂ∞ÜÂÜõÂ∏¶ÂÖµ‰ΩúÊàò‰∏ªË¶ÅÂá≠ÂÄü‰∏™‰∫∫Ê≠¶Âãá‰ªéÊù•Ê≤°Êúâ‰ΩøÁî®Ê≥ïÊúØ ‰ΩúÊàòÁöÑ‰æãÂ≠êÂõ†Ê≠§ËØ•ËÆæÂÆöÁ¨¶ÂêàËßÑÂàôÁ¨¨‰∏âÊù°‰∏éÂéü‰π¶ÁöÑËÆæÂÆöÈ£éÊ†º‰∏çÁªü‰∏Ä Answer: ÁüõÁõæ [example_4]: [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥‰∏âÂõΩÊºî‰πâ ‰∫∫Áâ©ÂæêÂ∫∂ Áõ∏ÂÖ≥ÂéüÊñáÁúÅÁï• Ê†áÈ¢òÂàùÈú≤ÈîãËäí ËÆæÂÆö‰∏≠Âπ≥ÂÖÉÂπ¥184Âπ¥ÈªÑÂ∑æ‰πã‰π±Êä§ÈÄÅÊØç‰∫≤ÈÄÉ‰∫°ÈÄî‰∏≠ÁõÆÁùπÂÆòÂÜõÂ±†ÊùëÈ¶ñÊ¨°ÊùÄ‰∫∫17Â≤ÅÂõ†ÊïëË¢´Êé≥Â¶áÂ•≥ÂæóÁΩ™ÂΩìÂú∞Ë±™Âº∫ÂºÄÂßãÂåñÂêç\"Âçï Á¶è\" [ËæìÂá∫] Think: Âéü‰π¶‰∏≠ÊúâÂæêÂ∫∂Âπ¥Â∞ë‰ªª‰æ†ÊùÄ‰∫∫ÂíåÂåñÂêçÂçïÁ¶èÁöÑÊèèÂÜô‰ΩÜÊòØÊ≤°ÊúâÁªôÂá∫ÂÖ∑‰ΩìÁªÜËäÇËØ•ËÆæÂÆöË°•ÂÖ®‰∫ÜÂéü‰π¶Áº∫Â∞ëÁöÑÁªÜËäÇ‰∏îÊ≤°ÊúâÊûÑÊàêÁüõÁõæÂõ† Ê≠§ÊòØ‰∏Ä‰∏™‰∏éÂéüËëóÂ≠òÂú®È´òÂ∫¶ÂÖºÂÆπÊÄßÁöÑÂâßÊÉÖËÆæÂÆö Answer: ‰∏ÄËá¥ [example_5]: [Ââç‰º†ÂâßÊÉÖËÆæÂÆö] Â∞èËØ¥Ë•øÊ∏∏ËÆ∞ ‰∫∫Áâ©ÈªÑÁãÆÁ≤æ Áõ∏ÂÖ≥ÂéüÊñáÁúÅÁï• Ê†áÈ¢òÁü≠ÊöÇÊãúÂ∏à‰∏éÁÇºÊ≥ï ËÆæÂÆö‰∏∫Ê±ÇÊõ¥È´òÊ≥ïÂäõÈªÑÁãÆÁ≤æÊõæÁßÅËá™ÈóØÂÖ•Âë®ËæπÂÖ∂‰ªñÂ±±Â§¥‰∏éÂ§ö‰ΩçÂ∞èÂ¶ñÂØπÂÜ≥Èî§ÁÇºÊ≥ïÊúØ‰πüÊ†ë‰∏ãÂÜ§‰ªá [ËæìÂá∫] Think: ÂéüÊñá‰∏≠Âú®Áõ∏ÂÖ≥Á´†ËäÇ‰ªéÊú™ÊèêËøáÈªÑÁãÆÁ≤æ‰∏éÂë®ËæπÂÖ∂‰ªñÂ±±Â§¥ÁöÑÂÜ§‰ªáËØ•ËÆæÂÆö‰∏éÂéü‰π¶ÂâßÊÉÖËÑ±Á¶ª‰ΩÜ‰πüÂõ†Ê≠§‰∏ç‰ºö‰∏éÂéü‰π¶ÂâßÊÉÖÊûÑÊàêÁüõÁõæÂõ†Ê≠§ÊòØ ‰∏Ä‰∏™‰∏éÂéüËëóÂâßÊÉÖÊó†Á¥ßÂØÜËÅîÁ≥ªÁöÑÂâßÊÉÖËÆæÂÆö Answer: ‰∏ÄËá¥ Figure 10: Content of the in-context chinese examples in Figure 9. PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts You are writing assistant who is deeply familiar with the plots of various renowned literary works and highly skilled in literary analysis. am writing prequel story for the character {{ char }} from the novel {{ book_name }}. You will read plot point from this prequel and help me determine whether it conflicts with the original novels plot and setting. Please follow the rules below when making your judgment: 1. If detail in the original novel directly contradicts the plot point provide, please respond with Contradict; 2. If the plot point provide causes part of the original plot or the characters motivations to become broadly unreasonable, please respond with Contradict; 3. If the plot point does not contradict the original plot but is inconsistent with the tone or stylistic setting of the novel, please respond with Contradict; 4. If none of the above conflicts exist between the plot point and the original novel, please respond with Consistent; 5. In your response, clearly state your judgment and briefly explain your reasoning; 6. You will be provided some snippets from the original book for reference, which may not be comprehensive. Please consider both the snippets and your knowledge about the book to provide the answer; 7. You should base your judgment only on the content of the original novel --do not consider adaptations, derivative works, fandoms, or historical inspirations behind the characters; 8. You should assume that the prequel is followed immediately by the original story, with no additional text or events in between; 9. Unless characters statements are later explicitly revealed to be deceptive, they should be treated as factual, similar for narrative expositions. Here are some examples for reference (the corresponding original texts are long, so they are omitted ): <example_1> [example_1] </example_1> <example_2> [example_2] </example_2> <example_3> [example_3] </example_3> <example_4> [example_4] </example_4> <example_5> [example_5] </example_5> Please follow the format above and complete the task below: [Prequel Plot Point] Novel: {{ book_name }} Character: {{ char }} Original Excerpt: <chunks>{{ chunks }}</chunks> Title: {{ subject }} Setup: {{ content }} Please give your answer following the format shown in the examples above. Figure 11: English prompt template used for the RAG experiment in Table 5, with contents of [example_i] presented in Figure 12. 23 PRELUDE: Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts [example_1]: [Prequel Plot Point] Novel: The Chronicles of Narnia Character: Eustace Scrubb Original Excerpt: (omitted) Title: Early Life Setup: Born in London and raised as an only child, Eustace was taught to call his parents by their names --\"Harold\" and \"Alberta--\"rather than \"Mother\" and \"Father\". [Output] Think: In the original novel, the Scrubb family lives in Cambridge and Eustace was born in Cambridge England. Therefore, this detail is consistent with Rule 1 and contradicts factual detail from the original story. Answer: Contradict [example_2]: [Prequel Plot Point] Novel: The Chronicles of Narnia Character: Eustace Scrubb Original Excerpt: (omitted) Title: Adventure in Underworld Setup: The Lady of the Green Kirtle directs Eustace to the Underland, helping him find Prince Rilian and kill the serpent responsible for the princes enslavement and intent on killing him. [Output] Think: In the story, Eustace is actually directed by her to the giants castle and accidentally discovers the Underland. Since the Lady of the Green Kirtle is herself the serpent, it would be impossible for her to intentionally lead them to the Underland and help them find the prince. Therefore, this scenario matches Rule 2 and introduces inconsistencies across multiple parts of the original plot. Answer: Contradict [example_3]: [Prequel Plot Point] Novel: Romance of the Three Kingdoms Character: Xiahou Yuan Original Excerpt: (omitted) Title: Rise from Humble Origins Setup: He was taught magical speed-enhancing techniques by mysterious hermit, which allowed him to march 500 li in three days, and 1000 li in six days. [Output] Think: Although not contradictory to the plot, the original novel maintains realistic --tonegenerals fight with martial skills and do not use magic. This setup violates Rule 3 by being stylistically inconsistent with the novel. Answer: Contradict [example_4]: [Prequel Plot Point] Novel: Harry Potter Character: Sirius Black Original Excerpt: (omitted) Title: Belief and Value Setup: Rejecting pure-blood elitism, Sirius frequently showed admiration for Gryffindor and spent time with Muggles and so-called \"blood traitors\" to annoy his parents even before attending Hogwarts. [Output] Think: In the original novel, Sirius is introduced as key figure against pure-blood elitism and is sorted into Gryffindor in 1971. However, the relevant chapters do not describe these specific actions. This addition fills in missing details without introducing contradictions. Therefore, it is compatible with the original. Answer: Consistent [example_5]: [Prequel Plot Point] Novel: Journey to the West Character: Yellow Lion Demon Original Excerpt: (omitted) Title: Brief Apprenticeship and Spell Training Setup: Seeking greater power, he once secretly ventured to other nearby mountains, fought with lesser demons, honed his magic, and made enemies along the way. [Output] Think: The relevant chapters in the original text do not mention these rivalries. While this setup is detached from the original plot, it doesnt contradict it. Thus, it is side story that remains compatible with the canon. Answer: Consistent Figure 12: Content of the in-context english examples in Figure 11."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "NJIT",
        "WeChat AI, Tencent"
    ]
}