{
    "paper_title": "Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models",
    "authors": [
        "Haoxiang Sun",
        "Yingqian Min",
        "Zhipeng Chen",
        "Wayne Xin Zhao",
        "Zheng Liu",
        "Zhongyuan Wang",
        "Lei Fang",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1 and OpenAI's o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 0 8 3 1 2 . 3 0 5 2 : r Technical Report on Slow Thinking with LLMs: Evaluation Benchmark Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models Haoxiang Sun1, Yingqian Min2, Zhipeng Chen2, Wayne Xin Zhao2, Zheng Liu3, Zhongyuan Wang3, Lei Fang4, Ji-Rong Wen2 1School of Information, Renmin University of China. 2Gaoling School of Artificial Intelligence, Renmin University of China. 3BAAI. 4DataCanvas Alaya NeW. {hxiang.sun, batmanfly}@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-theart models including DeepSeek-R1 and OpenAIs o3-mini demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilitiesa critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the OlymMATH benchmark at the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."
        },
        {
            "title": "Introduction",
            "content": "The advent of large language models (LLMs) [1] has marked significant leap forward in the capabilities of artificial intelligence, showcasing exceptional performance across broad spectrum of tasks, and in some cases, even rivaling or exceeding human-level proficiency [2, 3]. Among the myriad of capabilities demonstrated by LLMs, mathematical reasoning has surfaced as particularly pivotal and demanding area of research [4, 5, 6]. In recent years, the evaluation and enhancement of mathematical reasoning abilities have become central focus in the development of advanced language models [7]. Correspondence to Xin Zhao. Preprint. Under review. Figure 1: Performance comparisons of mainstream reasoning models between our OlymMATH (English version) and other Olympiad-level mathematical benchmarks. OlymMATH-HARD emerges as the most challenging, with significantly higher difficulty than existing evaluation benchmarks. To effectively assess the reasoning capacities of LLMs, it is essential to develop evaluation benchmarks that are both reliable and verifiable. Reliability, in this context, means that the problems and their corresponding solutions must be meticulously designed to ensure accuracy, consistency, and alignment with the evaluations intended objectives. This involves crafting problems that are free from ambiguities, biases, or errors, while also providing well-defined, correct answers that serve as definitive reference for assessing model performance. For verifiability, the evaluation process should be designed to be easily constructed, replicated, and validated through simple automated procedures. This necessitates that the answer format be easily parsable, allowing the correctness of model-generated answers to be efficiently and effectively verified by comparing them against ground-truth answers. To achieve this, many existing benchmarks adopt single-answer evaluation format, where answers are presented in fixed and standardized form. For example, answers are often formatted in boxed form, such as The answer is 3 , to facilitate straightforward parsing and comparison. This approach not only simplifies the evaluation process but also enhances reproducibility and scalability, making it easier to assess the mathematical reasoning capabilities of LLMs across diverse datasets. The primary purpose of establishing evaluation benchmarks is to identify the limitations and shortcomings of LLMs, thereby guiding future research directions for improvement. In the past years, extensive high-quality mathematical benchmarks have been developed, playing pivotal role in advancing the reasoning capabilities of LLMs (e.g., [8, 9]). Notable examples include GSM8K [10] and MATH [11], which have been instrumental in pushing the boundaries of LLMs mathematical reasoning. However, significant trend has emerged: many of these benchmarks, including those currently in use, are becoming saturated due to rapid advancements in LLMs. For instance, GSM8K [10], once standard for evaluating early-generation models like Llama 1 and 2, has now been largely mastered by state-of-the-art models. Similarly, MATH [11], initially designed to challenge GPT-4-level models, has also reached saturation with todays best-performing models. This issue is further exacerbated by the development of slow-thinking models, such as DeepSeek-R1 [4] and OpenAIs o1 [12], which encourage deliberate, step-by-step reasoning. These models demonstrate that enhancing the reasoning process can lead to substantial performance improvements, rendering existing benchmarks less effective at distinguishing cutting-edge capabilities."
        },
        {
            "title": "MATH Dataset",
            "content": "Problem: Compute: 1 2 + 3 4 + 5 + 99 100. Answer: 50. Problem: What is the distance, in units, between the points (2, 6) and (4, 3)? Express your answer in simplest radical form. Answer: 3 13. Problem: Let x, y, and be positive real numbers such that xyz = 2. Find the minimum value of x4 + 4y2 + 4z4. Answer: 16. Problem: Let be positive integer. Simplify the expression 4 )(44 + 1 (24 + 4 ) [(2n)4 + 1 4 ] 4 ) [(2n 1)4 + 1 4 ] . (14 + 1 4 )(34 + 1 8n2 + 4n + 1. Answer: OlymMATH-HARD (Ours) (cid:0)20161234 (cid:1) Find the remainder of (cid:80)1234 Problem: k=0 2016k modulo 20172 (provide the value in the range [0, 20172)). Answer: 1581330. Problem: 设数列 {an} 满足 a0 = 0, an+1 = k=0 ak 的小数部 8 5 an + 6 5 分(用小数表示). Answer: 0.84. (n N). 求 (cid:80)2005 (cid:112)4n a2 OlymMATH-EASY (Ours) 5 , cos BAD = cos CAD = 1 Problem: sphere is circumscribed around tetrahedron ABCD, and another sphere with radius 1 is tangent to plane ABC. The two spheres are internally tangent at point D. If AD = 3, cos BAC = 4 find the volume of tetrahedron ABCD. Answer: Problem: 设 为 ABC 的内心, AB = 3, AC = 4, BC = 5, OC, OP = 0 x, y, 1. 求动点 的轨迹所覆盖的平面区 域的面积. Answer: 12. OB + OA + 18 5 . 2 , Figure 2: Examples from the MATH dataset and our OlymMATH dataset. To better evaluate the performance of advanced reasoning models, more rigorous and challenging benchmarks are needed to assess their mathematical reasoning capabilities. The AIME dataset has emerged as more demanding benchmark by incorporating problems from the American Invitational Mathematics Examination (AIME), which presents higher level of difficulty. Due to their complexity and rigor, AIME problems continue to challenge state-of-the-art models under standard prompting. Nevertheless, the AIME dataset has three major limitations. First, the limited scale of the current dataset (containing merely 30 problems from AIME 2024) may compromise the statistical reliability and robustness of the evaluation results. Second, as reasoning models rapidly improvethrough methods like fine-tuning with long chain-of-thought data [13] or reinforcement learning scaling [4]the benchmarks original performance ceiling is being surpassed. For example, models such as OpenAIs o1 and DeepSeek-R1 now achieve nearly 80% accuracy, demonstrating that current top-performing models are approaching the limits of what AIME can effectively measure. Third, the dataset exclusively features English problems, leaving multilingual reasoning capabilities unassessed despite their importance for comprehensive evaluation. To overcome these limitations, we present OlymMATH, rigorously curated bilingual mathematical benchmark designed to assess LLMs reasoning capabilities at Olympiad level. The benchmark comprises 200 carefully curated problems divided into two difficulty levels: easy (OlymMATHEASY) and hard (OlymMATH-HARD), with each problem available in both English and Chinese versions, resulting in two parallel 200-problem evaluation sets (i.e., OlymMATH-EN and OlymMATHZH). To prevent data contamination, all problems were manually sourced from printed publications rather than online resources, with each problem undergoing expert verification and annotation. The dataset requires precise numerical answers, ensuring reliable evaluation through answer verification. OlymMATH covers four major mathematical fields, enabling thorough evaluation across diverse reasoning tasks. Maintaining compatibility with existing benchmarks, it follows the established MATH dataset [11] format (see the detailed comparison in Figure 2). By leveraging the OlymMATH benchmark, we conduct extensive experiments to evaluate the performance of several state-of-the-art models (see Figure 1). Experimental results demonstrate the difficulty of our benchmark: even the most advanced reasoning models, such as DeepSeek-R1 [4] 3 Table 1: Comparison of different math evaluation benchmarks. EN and ZH denote English and Chinese, respectively. Univ. denotes university-level problems. # Problems Evaluation Language Difficulty # Field Name GSM8K [10] MATH [11] AIME 2024 [14] AIME 2025 [15] HMMT 202502 [16] Olympiad Bench [17] MATH Odyssey [18] Omni-MATH [19] OlymMATH-EN OlymMATH-ZH 1319 5000 30 30 8476 387 4428 200 200 - 6 - - - 3 33+ 4 4 EN EN EN EN EN ZH & EN EN EN EN ZH Rule Rule Rule Rule Rule Rule LLM LLM Rule Rule Grade School Competition Olympiad Olympiad Olympiad Olympiad High School & Univ. Olympiad Olympiad Olympiad and OpenAI o3-mini (high) [12], achieve only 21.2% and 30.3% accuracy, respectively, on the OlymMATH-EN-HARD subset. These findings clearly indicate that Olympiad-level math problems remain significant challenge for current LLMs, highlighting the need for continued research and development in mathematical reasoning capabilities. Our multilingual comparison reveals consistent performance gap, with models typically achieving higher accuracy on English-version problems compared to their Chinese counterparts. This finding highlights the necessity of multilingual evaluation for comprehensive capability assessment. Furthermore, we conduct case studies to closely examine the solution process. Our analysis reveals that, for certain problems, models sometimes rely on heuristic guessing strategies to indirectly arrive at the final answers, often without providing rigorous proofs or logical reasoning. This observation underscores the importance of incorporating process-level inspection and evaluation to more accurately assess the true capabilities of LLMs. In summary, our contribution are as follows: We present OlymMATH, manually curated and rigorously verified Olympiad-level math dataset, specifically designed to objectively assess the mathematical reasoning capabilities of LLMs. To maximize research utility and ensure broad accessibility, the benchmark features fully parallel English and Chinese problem sets that enable rigorous multi-lingual evaluation of mathematical reasoning capabilities. We demonstrate the rigor and effectiveness of our benchmark, highlighting its potential to drive future research and advancements in the mathematical reasoning capabilities of LLMs. Our experiments reveal that even the most advanced models currently available achieve relatively poor performance on OlymMATH. We conduct detailed experimental analyses and case studies, identifying the key limitations and challenges faced by current state-of-the-art models in solving complex mathematical problems. Additionally, we compare the linguistic effects by evaluating the same model across both Chinese and English versions of the dataset, providing insights into the impact of language on mathematical reasoning performance."
        },
        {
            "title": "2 Benchmark Construction",
            "content": "In this section, we describe the OlymMATH dataset in detail, including its construction methodology, problem composition, categorical distribution, and evaluation approach. Our dataset is specifically designed to provide rigorous yet objectively verifiable benchmark for assessing the mathematical reasoning capabilities of LLMs. Additionally, we offer two parallel evaluation sets containing 200 problems each in English and Chinese as supplementary data to facilitate comparative analysis of performance gaps between the two languages. Table 1 presents basic comparison of our proposed OlymMATH benchmark and other mathematical reasoning benchmarks. 4 Problem: Given that two vertices of an equilateral triangle are on the parabola y2 = 4x, and the third vertex is on the directrix of the parabola, and the distance from the center of the triangle to the directrix equals 1 9 of the perimeter. Find the area of the triangle. Subject: Geometry Figure 3: geometry problem described precisely in text from OlymMATH. Table 2: The distribution of contest problems by category. Category Topic Algebra (Alg.) Inequality, Sequence, Trigonometry, etc. Geometry (Geo.) Solid Geometry, Analytic Geometry, etc. Number Theory (Num.) Divisibility, Diophantine Equation, etc. Combinatorics (Com.) Graph Theory, Permutation, etc. Count HARD EASY Total 25 25 25 25 33 29 50 58"
        },
        {
            "title": "Total",
            "content": "100"
        },
        {
            "title": "2.1 Dataset Construction",
            "content": "OlymMATH comprises 200 high-quality mathematical problems at the Olympiad level, meticulously curated from printed resources to ensure both quality and originality. These problems were manually gathered from range of authoritative sources, including specialized magazines, textbooks, and official competition materials. To minimize the risk of data contamination, online repositories and forums were intentionally excluded from the sourcing process. This methodology ensures that the problems are intellectually challenging and representative of advanced mathematical reasoning, while also minimizing prior exposure on publicly accessible digital platforms. Consequently, OlymMATH serves as reliable benchmark for evaluating the real capabilities of LLMs in solving complex mathematical tasks."
        },
        {
            "title": "2.2 Problem Categories and Distribution",
            "content": "To ensure comprehensive coverage of reasoning skills, the problems in OlymMATH span four primary mathematical fields: algebra, geometry, number theory, and combinatorics. These fields are selected for two key reasons: they encompass highly challenging problems and are often amenable to verification through simple-answer formats. While geometry problems typically rely on figures for their descriptions, our emphasis on language models requires that these problems be reformulated into text-based descriptions to ensure compatibility with the evaluation framework. Hence, we exclude geometry problems that cannot be equivalently converted into plain text from the dataset. Figure 3 illustrates an example of geometry problem that has been successfully adapted and included in our benchmark. Although the current benchmark consists of 200 problems, we aim to maximize diversity by targeting wide range of topics within each field. For instance, in algebra, we include problems on inequalities, sequences, trigonometry, and more. To further refine the evaluation, we categorize the problems into two distinct difficulty levels: easy and hard. The easy problems are designed to challenge standard prompting approaches used by mainstream models, while the hard problems are tailored to test advanced reasoning capabilities, such as slow-thinking modes, in state-of-the-art reasoning models. The distribution of problems across categories and difficulty levels is detailed in Table 2."
        },
        {
            "title": "2.3 Format and Verification Methodology",
            "content": "The OlymMATH dataset adheres to format that aligns with the widely used MATH dataset, ensuring seamless integration with existing benchmarks and evaluation pipelines. This consistency not only simplifies the evaluation procedure but also enhances the datasets utility for researchers and 5 Table 3: The included and excluded formats of the final answer. Type Examples (cid:112) 4 + Included Excluded Real number Interval Set Operations Variable Complex number Text 5, 16, 22017 + 1 [ 33, +), (4, 5π] 2 , arctan 2 3 {4, 5} {1, 8} 5a2, p2 pq, n! + 2 9 + 4i, East, Alice Original problem: If the distances from the eight vertices of cube to certain plane are 0, 1, 2, 3, 4, 5, 6, 7 respectively, what is the possible edge length of this cube? After transformation: If the distances from the eight vertices of cube to certain plane are 0, 1, 2, 3, 4, 5, 6, 7 respectively, consider all possible edge lengths of this cube. Assuming the possible edge lengths form set S, find the sum of squares of all elements in S. Figure 4: An example from OlymMATH-HARD designed to assess whether the model can identify all possible answers. practitioners. Each problem is meticulously organized in structured format, as outlined in Figure 5, to maintain clarity and facilitate efficient processing. Furthermore, the dataset is composed exclusively of text-based problems. As previously mentioned, for geometry problems that traditionally rely on diagrams, we have meticulously designed problem descriptions to ensure that all essential geometric relationships and details can be accurately inferred from the text alone. This approach maintains the integrity of the problems while aligning with the text-focused nature of language model evaluation. To ensure consistent and objective evaluation, OlymMATH restricts answers to real numbers and intervals, as illustrated in Table 3. This simplifies the verification process and minimizes ambiguity in assessing model performance. We deliberately exclude other answer formats that are more challenging to verify, such as set operations, variables, complex numbers, and free-form text, as these can introduce subjectivity or require additional interpretation. By focusing on real numbers and intervals, we maintain clear and standardized evaluation framework that enhances the reliability and reproducibility of results. This standardization enables reliable verification using sympy based 2)/2 = 2 sin 15 = verification tools. Also, for instance, expressions like e1/2 can be numerically evaluated to judge the models output. 3 = ( 6 2 (cid:112) To make the evaluation more challenging, OlymMATH specifically curates problems that yield challenging numerical answers, such as those with multiple possible answers. These problems are then modified to require summary of all potential outcomes, such as sums, sums of squares, and more (see Figure 4). This method allows for the assessment of whether models consider all scenarios while providing an effective means of evaluating their performance. By requiring detailed exploration of various answers, we ensure that models can effectively demonstrate their reasoning capabilities."
        },
        {
            "title": "2.4 Bilingual Extension",
            "content": "The OlymMATH benchmark originates with Chinese-language problems. To enable comprehensive bilingual evaluation, we preserve both the original Chinese and translated English versions of all problems. Our translation pipeline employs two-stage LLM-based approach: First, we generate initial English translations using Claude Sonnet 3.7, then iteratively refine them through multiple rounds of review with GPT-4o until convergence. Finally, expert human annotators verify and polish all translations for mathematical accuracy and linguistic fluency. 6 Problem (EN): Let $a_1, a_2, a_3, a_4, a_5 in [0, 1]$, find the maximum value of $prod_{1 le < le 5} a_i - a_j$. Answer (EN): frac{3sqrt{21}}{38416} Subject (EN): Algebra Problem (ZH): 设 $a_1,a_2,a_3,a_4,a_5 in [0, 1]$, 求 $prod_{1le i<j le 5} a_i-a_j$ 的最 大值. Answer (ZH): frac{3sqrt{21}}{38416} Subject (ZH): 代数 Figure 5: Example of the format used in OlymMATH. These parallel evaluation setsdenoted as OlymMATH-EN (English) and OlymMATH-ZH (Chinese) respectively (see Figure 5)enable systematic comparison of reasoning capabilities across languages, while the union of the two sets are denoted as OlymMATH (full set)."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we assess the performance of leading reasoning models using the OlymMATH benchmark and then provide detailed analysis of their capabilities."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Models. To conduct thorough evaluation, we assess range of representative large language models (LLMs), including both open-source and closed-source models. Specifically, for open-source models, we select DeepSeek-R1 [4], DeepSeek-R1-Distill-Qwen-32B [4], and QwQ-32B [20]. For closed-source models, we include o3-mini (high) [12] in our evaluation. Additionally, to simplify model identification, we use the abbreviation Qwen2.5-32B-R1D to refer to DeepSeek-R1-DistillQwen-32B. Evaluation Details. To ensure robust evaluation, we adopt two complementary metrics: Pass@1 and Cons@10. Our evaluation pipeline follows systematic approach: for each problem, we generate 10 distinct responses from each comparison model, with the exception of o3-mini (high), for which we limit sampling to three responses per question due to API stability constraints. For the Pass@1 metric, we compute the mean accuracy across all sampled responses to derive the final accuracy score. For the Cons@10 metric, we implement majority voting to determine consensus answer for each problem, subsequently calculating the average accuracy across the entire problem set. For generation hyperparameters, we adhere to established practices from previous research [4, 20], configuring locally-evaluated models with temperature, top_p, min_p, and max_token set to 0.6, 0.95, 0, and 32768, respectively. For api-evaluated models (i.e., DeepSeek-R1 and o3-mini (high)), we use their default hyperparameters to unleash their reasoning capabilities better. This evaluation method efficiently assesses the models performance on the benchmarks, but it still leaves room for improvement in the accuracy and reliability of model evaluation."
        },
        {
            "title": "3.2 Evaluation Results on OlymMATH",
            "content": "In this part, we assess the performance of the comparison models on our OlymMATH benchmark. We present the evaluation results of OlymMATH-EN and OlymMATH-ZH in Table 4 and Table 5, respectively. First, we observe that all tested models exhibit relatively poor performance, with even OpenAI o3-mini (high) achieving only 30.3% on OlymMATH-EN-HARD. This underscores the high overall difficulty of our benchmark, which demands stronger reasoning abilities and deeper understanding of mathematical knowledge to solve the problems effectively. In contrast, the performance of these 7 Table 4: Model performance on the English version of OlymMATH. The abbreviations Alg., Geo., Num., and Com. represent the four problem categories: Algebra, Geometry, Number Theory, and Combinatorics, respectively. Due to o3-minis API instability, we sampled only three responses per question and cannot report its Cons@10 performance. Models Pass@ Cons@10 Alg. Geo. Num. Com. Avg. Alg. Geo. Num. Com. Avg. Qwen2.5-32B-R1D 17.2 25.6 QwQ-32B 32.8 DeepSeek-R1 24.0 o3-mini (high) Qwen2.5-32B-R1D 73.6 84.8 QwQ-32B 88.4 DeepSeek-R1 90.0 o3-mini (high) 18.8 18.4 28.8 32.7 69.1 79.7 84.8 89.4 The Hard Subset 13.6 18.5 21.2 30.3 0.8 5.2 4.0 20.0 17.6 24.8 19.2 44. The Easy Subset 66.7 78.1 80.0 89.7 48.3 67.9 62.4 85.1 88.5 83.8 90.8 100.0 36.0 36.0 48.0 / 20.0 12.0 36.0 / 92.0 96.0 100.0 / 97.0 100.0 97.0 / 24.0 24.0 24.0 / 92.3 92.3 92.3 / 0.0 4.0 4.0 / 58.6 79.3 75.9 / 20.0 19.0 28.0 / 84.0 92.0 91.0 / Table 5: Model performance on the Chinese version of OlymMATH. The abbreviations Alg., Geo., Num., and Com. represent the four problem categories: Algebra, Geometry, Number Theory, and Combinatorics, respectively. Due to o3-minis API instability, we sampled only three responses per question and cannot report its Cons@10 performance."
        },
        {
            "title": "Models",
            "content": "Pass@1 Cons@10 Alg. Geo. Num. Com. Avg. Alg. Geo. Num. Com. Avg. Qwen2.5-32B-R1D QwQ-32B DeepSeek-R1 o3-mini (high) 5.6 19.6 21.6 22.0 Qwen2.5-32B-R1D 45.2 76.8 QwQ-32B 79.2 DeepSeek-R1 88.0 o3-mini (high) 6.8 12.4 24.4 26.0 40.3 73.3 74.8 92.4 The Hard Subset 5.4 1.2 12.1 1.2 16.1 1.6 27.2 12.0 8.0 15.2 16.8 48.7 The Easy Subset 23.4 48.6 50.7 79.9 38.5 68.9 71.3 88. 54.6 87.7 93.1 96.2 4.0 16.0 20.0 / 56.0 92.0 92.0 / 8.0 12.0 36.0 / 57.6 97.0 90.9 / 12.0 24.0 20.0 / 61.5 92.3 100.0 / 0.0 0.0 0.0 / 31.0 62.1 65.5 / 6.0 13.0 19.0 / 50.0 85.0 85.0 / advanced reasoning models on OlymMATH-EN-EASY is more modest and comparable to that on AIME 2024 and AIME 2025, suggesting that OlymMATH-EN-EASY is well-suited for evaluating the capabilities of less advanced reasoning models. Second, by comparing the performance of LLMs on OlymMATH-EN and OlymMATH-ZH, we find that language can influence the reasoning performance of LLMs to some extent. Overall, all models tend to achieve higher performance on the English benchmarks. potential reason for this is that English corpora still dominate existing pre-training datasets, making the English-based task-solving capabilities of LLMs generally more superior compared to other languages. This finding highlights the importance of considering performance across different languages when conducting comprehensive evaluation of LLMs."
        },
        {
            "title": "3.3 Analysis of Different Benchmarks",
            "content": "To compare OlymMATH with existing benchmarks and provide thorough analysis, we compare the performance of various LLMs on MATH-500 [11], AIME 2024, AIME 2025, and HMMT 202502 in Table 6. Results marked with an asterisk (*) originate either from their respective reports [4, 12, 20] or 8 Table 6: Model performance on different mathematical benchmarks (Pass@1). The results with (*) denote that they are copied from their reports or the third-party website MathArena. Benchmarks o3-mini (high) DeepSeek-R1 QwQ-32B Qwen2.5-32B-R1D MATH-500 AIME 2024 AIME 2025 HMMT OlymMATH-EN-HARD OlymMATH-EN-EASY OlymMATH-ZH-HARD OlymMATH-ZH-EASY / 87.3 86.5 67.5 30.3 89.7 27.2 88.2 97.3 79.8 70.0 41.7 21.2 80.0 16.1 71.3 / 79.5 66.0 47.5 18.5 78.1 12.1 68.9 94.3 72.6 59.0 33.3 13.6 66.7 5.4 38.5 Table 7: The average length of the responses generated from DeepSeek-R1. Output Length refers to the length of the final solution provided by the model, while Reasoning Length indicates the length of models thought process."
        },
        {
            "title": "Dataset",
            "content": "AIME 2024 AIME 2025 OlymMATH-EN-HARD OlymMATH-EN-EASY Avg. Output Length Avg. Reasoning Length"
        },
        {
            "title": "Total",
            "content": "1591 1547 1608 1588 1707 1625 1610 1594 1617 1575 1610 19958 23350 39024 29132 50836 53006 43850 46008 26820 33894 42827 from the third-party website MathArena2. Additionally, we report the average length of the generated responses from DeepSeek-R1  (Table 7)  . In Table 6, due to the higher difficulty of our benchmark, models achieve lower accuracy compared to previous benchmarks. When examining model performance on MATH-500 and AIME 2024, all LLMs attain high scores (exceeding 90% and 70% accuracy, respectively), rendering these test sets less effective in distinguishing the reasoning abilities of the models. In contrast, performance on OlymMATH is more varied, enabling the evaluation metrics to better differentiate the models reasoning capabilities. For instance, DeepSeek R1 and Qwen2.5-32B-R1D achieve similar accuracy on MATH-500 (i.e., 97.3% v.s. 94.3%), while their performance on the hard subset of OlymMATH shows significant difference (i.e., 21.2% v.s. 13.6% on OlymMATH-EN-HARD). Since longer responses from LLMs often correlate with harder problems [21], we analyze the response length (i.e., character length) of DeepSeek-R1 across different benchmarks to further assess the difficulty of OlymMATH compared to others. As shown in Table 7, DeepSeek-R1 generates significantly more tokens for questions in OlymMATH than for those in AIME 2024 and AIME 2025 on average. This indicates that questions in OlymMATH require the model to engage in more extensive reasoning to arrive at solutions. Even for questions the model answers correctly, substantial amount of reasoning is necessary, with an average of 39024 and 29132 characters for OlymMATH-EN-HARD and OlymMATH-EN-EASY, respectively. As illustrated in Figure 6, the reasoning length distribution of OlymMATH-EN-EASY closely resembles that of AIME, suggesting comparable level of difficulty. In contrast, OlymMATH-EN-HARD exhibits significantly greater total response length, with an average length of 42.8K. This underscores the high difficulty of our benchmark and the greater demand it places on the reasoning capabilities of models."
        },
        {
            "title": "3.4 Rigorous Reasoning or Empirical Guessing?",
            "content": "During our problem collection and preliminary experiments, we have empirically observed that LLMs sometimes resort to making empirical guesses rather than engaging in rigorous reasoning. Specifically, for certain mathematical problems, these models may arrive at correct answers through pattern matching, heuristic methods, proposition simplification, or even fabrication, rather than 2https://matharena.ai/ 9 Figure 6: The reasoning length distribution of DeepSeek-R1 in AIME 2024, AIME 2025, OlymMATH-EN-EASY and OlymMATH-EN-HARD. The vertical dashed line represents the mean. systematic derivation. For instance, in Figure 7, the model (o3-mini) only guessed that = at optimal because of symmetry, while solving geometric optimization problem. Although such empirical guesses may yield correct answers, this intuitive approach lacks the necessary rigor of logical reasoning and would likely be deemed incorrect or, at best, not fully correct by human evaluators. This case becomes problematic when employing rule-based evaluation or LLM-as-judge methods, as neither can effectively assess the quality of rigorous reasoning. This implies that we should be cautious of LLMs potentially taking shortcutssolutions that are not well-derived or properly formulated during problem-solvingwhich could lead to an illusory improvement in answer accuracy. We further investigated this issue in the AIME 2025 and Omni-MATH benchmark using the o3-mini (high), and also observed similar occurrences (see Figure 8, 9). Despite significant performance improvements, LLMs still exhibit deficiencies in deliberative thinking. This underscores the importance of processlevel supervision or evaluation, which remains challenging to scale in practice due to the high human effort involved. Currently, we do not accurately measure the proportion of empirical guesses in these benchmarks, leaving this as an important direction for future work. Interestingly, however, when confronted with the problems in our dataset, these guessing strategies often fail. Figure 10 illustrates case where o3-mini (high) attempts to solve complex optimization problem from OlymMATH. The model incorrectly assumes symmetry and produces the answer 3081, while the correct answer (verified using Mathematica) is 2625. The test samples OlymMATHHARD are generally more challenging than those in existing benchmarks, as the intermediate reasoning steps are difficult to hack (i.e., some steps are directly taken without proper reasoning) through empirical guessing."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we introduce OlymMATH, challenging benchmark for evaluating mathematical reasoning capabilities of LLMs. Unlike existing math benchmarks, which are often sourced directly from the web or digital platforms, our dataset is meticulously curated from printed math magazines, books, and documents. The benchmark comprises 200 carefully selected problems spanning four 10 major mathematical fields, organized into easy (AIME-level) and hard (challenging for state-of-theart reasoning models) difficulty levels. To enable comprehensive bilingual evaluation, we provide parallel versions in both English and Chinese. Through extensive experiments, we assess the reasoning performance of state-of-the-art language models, particularly those operating in slow-thinking modes. Our findings reveal that even the most advanced models, such as DeepSeek-R1 and OpenAIs o3-mini, achieve relatively poor performance on OlymMATH. Additionally, we conducted case studies on several challenging problems, demonstrating that mainstream models often struggle to solve these complex tasks. These results underscore the rigor and significance of OlymMATH as benchmark for pushing the boundaries of mathematical reasoning in LLMs. The creation of this benchmark is part of our STILL project, and we believe that establishing appropriate benchmarks is of immense value for driving the advancement of language intelligence. In fact, benchmarks should evolve at faster pace than methodologies, as they provide essential guidance and direction toward achieving the ultimate objectives of the field. As part of future work, we plan to further expand the scale of the OlymMATH benchmark, thereby contributing to the development of more powerful and robust reasoning models."
        },
        {
            "title": "References",
            "content": "[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. survey of large language models, 2025. [2] Qwen and:, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [3] Abhinav Jauhri el. Aaron Grattafiori, Abhimanyu Dubey. The llama 3 herd of models, 2024. [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [5] OpenAI. Openai o1 system card, 2024. [6] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025. [7] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [8] Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data, 2024. [9] Daman Arora, Himanshu Gaurav Singh, and Mausam. Have llms advanced enough? challenging problem solving benchmark for large language models, 2023. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. [12] OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning, 1 2025. [13] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems, 2024. [14] AIME. Aime 2024, 2024. [15] AIME. Aime 2025, 2025. [16] HMMT. Hmmt 202502, 2025. [17] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 38283850, 2024. [18] Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. arXiv preprint arXiv:2406.18321, 2024. [19] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. [20] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [21] Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. Promptcot: Synthesizing olympiadlevel problems for mathematical reasoning in large language models, 2025. 12 In the triangle ABC, the lengths of the three sides are a, b, c, satisfying the Problem: condition 2a2 + b2 + c2 = 4. Find the maximum value of the area of ABC. Answer: , = = 5 . (When = 2 (cid:113) 6 5 ) 5 5 Model Output: We wish to maximize the area of ABC, subject to 2a2 + b2 + c2 = 4. Because the condition is symmetric in and (but not in a), it is reasonable to look for an optimum among triangles having = c. (Later one can check that assuming = indeed gives the maximum.) The problem involves maximizing the area of an isosceles triangle with sides a, b, under the constraint 2a2 + b2 + c2 = 4. By assuming = c, we derive formula for the area in terms of and find the maximum area by expressing it as function of = b2. We then determine the critical points of this quadratic function, identify the maximum area, and finally recover the side 5 5 . lengths to verify that they satisfy the original constraints. The maximum area achieved is Figure 7: An example during our data collection. o3-mini (high) found the correct answer without rigorously solving the problem. Due to length constraints, we provide only summary of its original solution steps. (cid:16) Problem: Let the sequence of rationals x1, x2, . . . be defined such that x1 = 25 xk+1 = 1 3 and n. Find the remainder when + is divided by 1000. Answer: 248. 11 and for relatively prime positive integers . x2025 can be expressed as xk + 1 xk 1 (cid:17) (cid:16) (cid:17) 3 xk + 1 xk , then after many 11 and xk+1 = 1 (in lowest terms) satisfies + 248 (mod 1000). Model Output: We will show that if x1 = 25 iterations the answer to x2025 = In what follows we briefly describe one way (via miraculous change-ofvariables) to solve the recurrence; the details need not be reproduced fully here. (cid:17) good hint is that the recurrence xk+1 = 1 3 (cid:16) 1 1 3 naturally appears. In fact, one may rewrite the recurrence in the form 3xk+1 + 1 = xk + 1 xk 2, so that (which is symmetric in and its reciprocal) . has the curious symmetry (x) = , so the expression + 1 xk + 1 xk 1 + = (cid:16) 1 (cid:17)2 (cid:16) (cid:17) (cid:17) (cid:16) useful idea is then to complete the square by writing (cid:16) (cid:17)2 xk 1 xk xk 1 xk +2. Defining uk = , one obtains the linear-looking = 3xk+1 1. With some additional work it turns out that if we write xk = mk nk 3xk+1 +1 = relation u2 (in lowest terms), then one may prove by induction that mk + nk = 36 12(2 k11). (In our case one checks that m1 + n1 = 25 + 11 = 36, m2 + n2 = 432, m3 + n3 = 62208, and indeed 36, 432, 62208 equal 36 120, 36 121, 36 123 respectively.) Thus, for = 2025 we have + = 36 12(220241). Even though this number is astronomically large, we are only asked to compute + (mod 1000). Using modular reductions, we compute + modulo 1000. First, modulo 8, we find 36 12(220241) 0 (mod 8). Modulo 125, Eulers theorem helps reduce 12N to 1215 118 (mod 125), giving 36 12N 123 (mod 125). Using the Chinese Remainder Theorem, solving 125k + 123 0 (mod 8) gives 1 (mod 8), leading to the final answer: 248 . = xk + 1 xk xk 1 xk Figure 8: An example from AIME 2025. o3-mini (high) forgot to prove that (mk, nk) = 1 and it claimed that mk + nk = 36 12(2k11) without providing the complete proof (highlighted in red). Instead, the model guessed this expression by calculating the first several terms in the sequence one by one. Due to length constraints, we provide only summary of its unimportant steps. 13 Source: 2016 IMO Shortlist A7 Problem: Find all functions : such that (0) = 0 and for all x, R, (x + y)2 = 2f (x)f (y) + max (cid:8)f (x2 + y2), (x2) + (y2)(cid:9) . Answer: (x) = 1 and (x) = 1. Solution: functional equation, we start with the condition: (Provided in Omni-MATH) To find all functions : satisfying the given and the functional equation for all x, R: (0) = 0, (x + y)2 = 2f (x)f (y) + max (cid:8)f (x2 + y2), (x2) + (y2)(cid:9) . Step 1: Test Simple Functions Lets start by testing the functions given in the reference answer: (x) = 1 and (x) = 1. Case 1: (x) = 1. Substitute (x) = 1 into the equation: Case 2: (x) = 1. Substituting (x) = 1: confirming (x) = 1 is another valid solution. Conclusion After verifying the two functions, we conclude that the functions satisfying the original functional equation are: Thus, the solutions are: (x) = 1 and (x) = 1. (x) = 1 and (x) = 1 . Model Output: We will show that the only functions : with (0) = 0, satisfying (x + y)2 = 2f (x)f (y) + max{ (x2 + y2), (x2) + (y2) } for all x, R, are (x) = 1 for all or (x) = 1. Below is an outline of one way to arrive at this result. Step 1. Plug in = 0 Let = 0 in the given equation: (x)2 = 2f (x)f (0) + max{f (x2), (x2) + (0)}. Then (x2) + < (x2) so we have (x)2 = 2c (x) + (x2). Then (x2) + > (x2) so the equation becomes (x)2 = 2c (x) + Write = (0) = 0. Notice that the two arguments inside the maximum differ by (0); therefore, the larger one depends on the sign of c: - **If > 0:** (x2) + c. - **If < 0:** Thus, in each case one obtains relationship between (x) and (x2). Step 2. Check constant solutions Thus, the constant solution is (x) = 1 for all x. Step 3. Look for non-constant solutions natural next guess is to try linear function. quick check verifies that (x) = 1 satisfies the original equation. which equals (x + 1)2, matching the left-hand side. Conclusion The only functions : with (0) = 0 that satisfy for all real numbers and are: (x) = 1 for all or (x) = 1. These are the complete solutions. Figure 9: An example from Omni-MATH. The solution provided by Omni-MATH itself is flawed and problematic. Both the reference solution and o3-mini (high) failed to prove that there are no other possible function that satisfies the condition (highlighted in red). Due to length constraints, we have omitted its unimportant steps. Problem: Let a, b, R, a3b + b3c + c3a = 3, find the minimum value of the expression (a, b, c) = ((cid:80) a4)4 + 1000 (cid:80) a2b2. Reference Answer: 2625. (Equality condition: = 4 7 4 7 7 and its cyclic permutations) sin2 π sin2 4π 7 , = 4 7 sin2 2π 7 , = Model Output: We wish to minimize (a, b, c) = (cid:0)a4 + b4 + c4(cid:1)4 + 1000 (cid:0)a2b2 + b2c2 + c2a2(cid:1) subject to the (cyclic) constraint a3b + b3c + c3a = 3, with a, b, R. natural idea is to look for \"symmetric-like\" solution. (Even though the constraint is cyclic rather than fully symmetric, the very \"even\" form of the objective all terms are even suggests that the optimum might occur when the \"building blocks\" have equal absolute value.) One obvious candidate is to try = = = t."
        },
        {
            "title": "Then the constraint becomes",
            "content": "a3b + b3c + c3a = 3t4 = 3 = t4 = 1. Thus = 1 or = 1. (In the equation t4 = 1 has the two solutions = 1 and = 1.) Now compute the value of at = = = t. Notice that a4 + b4 + c4 = t4 + t4 + t4 = 3, a2b2 + b2c2 + c2a2 = t4 + t4 + t4 = 3 (since t4 = 1). Then = (3)4 + 1000 3 = 81 + 3000 = 3081. Before concluding that 3081 is the minimum, one may ask: Could there be choice of a, b, satisfying a3b + b3c + c3a = 3 for which (cid:0)a4 + b4 + c4(cid:1) < 3 or (cid:0)a2b2 + b2c2 + c2a2(cid:1) < 3? Since a4, b4, c4 are nonnegative and the constraint (a homogeneous form of degree 4) \"forces\" the variables to be large enough in certain sense, one may show by careful (often inequalitybased) argument that if a3b + b3c + c3a = 3, then one must have a4 + b4 + c4 3 and a2b2 + b2c2 + c2a2 3. In fact, equality in these \"loss-inequalities\" forces = = = 1, and it turns out (by checking the signs in the constraint) that the only possibility for equality is = = = 1 or = = = 1. Also, one might try \"extremal\" cases such as letting one variable vanish (say, = 0). In that case, the constraint reduces to a3b = 3 so that and cannot be arbitrarily small (or close in absolute value to 1), and short calculation shows that the value of then exceeds 3081. Thus, the candidate = = = 1 (or = = = 1) gives the minimal value. In conclusion, the minimum value of is 3081, and it is attained exactly when a, b, are all equal with = 1 and the signs arranged so that a3b = 1 (that is, = = = 1 or = = = 1). Figure 10: An example from OlymMATH-EN-HARD subset. o3-mini (high) attempted to guess the equality conditions but failed to produce correct solution (highlighted in red)."
        }
    ],
    "affiliations": [
        "BAAI",
        "DataCanvas Alaya NeW",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "School of Information, Renmin University of China"
    ]
}