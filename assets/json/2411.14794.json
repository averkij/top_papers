{
    "paper_title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
    "authors": [
        "Songhao Han",
        "Wei Huang",
        "Hairong Shi",
        "Le Zhuo",
        "Xiu Su",
        "Shifeng Zhang",
        "Xu Zhou",
        "Xiaojuan Qi",
        "Yue Liao",
        "Si Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, a novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs a semantic-aware method to reduce redundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose a Hybrid LVLMs Collaboration framework, featuring a Frame Selector and a two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/VideoEspresso"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 2 ] . [ 1 4 9 7 4 1 . 1 1 4 2 : r VideoEspresso: Large-Scale Chain-of-Thought Dataset for Fine-Grained"
        },
        {
            "title": "Video Reasoning via Core Frame Selection",
            "content": "Songhao Han1, Wei Huang2, Hairong Shi1, Le Zhuo3, Xiu Su4, Shifeng Zhang5, Xu Zhou5, Xiaojuan Qi2, Yue Liao6, Si Liu1 1Beihang University 2The University of Hong Kong 3Shanghai AI Lab 4Central South University 5Sangfor Technologies Inc. 6CUHK {hshjerry,liusi}@buaa.edu.cn, {aaron.weihuang, liaoyue.ai}@gmail.com Figure 1. Overview of VideoEspresso. (a) Comparison of annotation pipelines: Unlike traditional videoQA datasets, VideoEspresso features an automatic pipeline for constructing complex reasoning QA tasks and multimodal Chain-of-Thought (CoT) annotations. This enhances the diversity of QA data and significantly improves scalability. (b) Examples from VideoEspresso: Illustrated are sample questionanswer pairs, along with CoT bounding boxes and evidence annotations, demonstrating the datasets richness. (c) Benchmark performance: Comparative results on our benchmark highlight the video reasoning capabilities of our model."
        },
        {
            "title": "Abstract",
            "content": "The advancement of Large Vision Language Models (LVLMs) has significantly improved multimodal understanding, yet challenges remain in video reasoning tasks due to the scarcity of high-quality, large-scale datasets. Existing video question-answering (VideoQA) datasets often rely on costly manual annotations with insufficient granularity or automatic construction methods with redundant frame-by-frame analysis, limiting their scalability and effectiveness for complex reasoning. To address these challenges, we introduce VideoEspresso, novel dataset that features VideoQA pairs preserving essential spatial details and temporal coherence, along with multimodal annotations of intermediate reasoning steps. Our construction pipeline employs semantic-aware method to reduce reCorresponding Author 1 dundancy, followed by generating QA pairs using GPT-4o. We further develop video Chain-of-Thought (CoT) annotations to enrich reasoning processes, guiding GPT-4o in extracting logical relationships from QA pairs and video content. To exploit the potential of high-quality VideoQA pairs, we propose Hybrid LVLMs Collaboration framework, featuring Frame Selector and two-stage instruction fine-tuned reasoning LVLM. This framework adaptively selects core frames and performs CoT reasoning using multimodal evidence. Evaluated on our proposed benchmark with 14 tasks against 9 popular LVLMs, our method outperforms existing baselines on most tasks, demonstrating superior video reasoning capabilities. Our code and dataset will be released at: https://github.com/hshjerry/ VideoEspresso 1. Introduction In recent years, the development of Large Vision Language Models (LVLMs) [3, 11, 18, 24] has brought significant advancements to multi-modal understanding tasks. By integrating visual and language information through extensive data training, more advanced LVLMs families [16, 31] are able to generate reasonable outputs while fully leveraging the rich knowledge of LLMs, thus demonstrating excellent performance in tasks like image captioning and visual question answering. Recent research has started to explore extending LVLMs to the domain of video content understanding [19, 22, 27]. Although these efforts have shown great potential on certain basic video understanding benchmarks [20, 28, 42, 47], their performance on complex video reasoning tasks remains less than satisfactory. primary limitation in video question-answering (VideoQA) research is the scarcity of high-quality, largescale datasets. Current VideoQA datasets [15, 41, 47] rely on costly manual annotations that often lack the granularity needed for detailed understanding, limiting scalability. Yet, LVLMs require vast amounts of multimodal QA pairs for effective training. Recently, advancements in large language models (LLMs) like GPT-4 [30] and Gemini-Pro [12] have allowed for the automatic generation of QA pairs through carefully designed prompts. straightforward approach is to use video metadatatypically high-level descriptionsand leverage LLMs to generate QA pairs based on this coarse information. However, the missing of crucial video details restricts the QA pairs effectiveness for finegrained reasoning. Alternatively, analyzing video frames for more granular understanding is feasible, but video content is often redundant, with key information dispersed sparsely, making frame-by-frame analysis computationally expensive and prone to information overload. To address these challenges, we propose novel automatic VideoQA construction method and introduce new dataset, VideoEspresso. By preserving important spatial details with temporal coherence, we create fine-grained reasoning-enabled VideoQA dataset that fosters more effective multimodal understanding. As shown in Fig. 1, we first design semantic-aware key information extraction method to extract key information from the video. Unlike traditional methods that extract key frames based on image representations, we first map video frames to the linguistic space using an LVLM. We then remove similar frames based on semantic similarity, which reduces redundancy in the video data. To retain frame-level details and inter-frame correlation, we sequentially group the video frames and input them into GPT-4o [31]. With carefully designed prompts, we instruct the model to generate initial QA pairs and filter out low-quality data. To further expand the intermediate reasoning steps, we introduce video Chain-of-Thought annotations. We design prompts to guide GPT-4o in extracting logical relationship evidence from QA pairs and videos that are helpful for answers, including interactions of key objects in spatial and temporal flow. By annotating these logical processes, we ultimately aim to expand the chain of inference evidence on QA datasets. To fully leverage the potential of the high-quality VideoQA pairs in our proposed VideoEspresso, we introduce novel framework, Hybrid LVLMs Collaboration for VideoQA, achieving cost-effective and accurate video LVLM reasoning. The framework is composed of tiny Frame Selector and fine-grained reasoning LVLM. The Frame Selector adaptively selects the most relevant core frames for the question based on the image-to-language mapping. These core frames are then submitted to the reasoning LVLM, where the model first extracts multimodal evidence based on the frame information, and ultimately provides the answer to the question through chainof-thought reasoning, leveraging this evidence. This dataset provides explicit annotations of key reasoning steps and image regions through both text and bounding boxes, which enables models to effectively use text and image localization information when answering questions. Based on our dataset, we have constructed an evaluation benchmark that includes set of GPT-4o-based open-ended evaluation metrics. We evaluated 9 popular LVLMs as our comparison baselines. To assess video reasoning capabilities from different perspectives, we categorized the evaluation into 14 tasks. Our method demonstrates significant advantages over baseline methods across most tasks. 2. Related Work VideoQA Dataset. Traditional videoQA datasets [15, 42, 47] rely heavily on manual annotations, where annotators watch videos, summarize content, and generate QA pairs based on set guidelines. This labor-intensive process limits scalability. With advancements in LLM capabilities [1, 12, 29], recent approaches use tailored prompts to utilize LLMs for annotation, often relying on metadata or detailed captions [6, 36] to construct QA data [20, 33, 35]. However, these methods often lack fine-grained video information and depend heavily on raw video data. In contrast, we introduce an automatic pipeline for QA pair generation that processes and annotates raw data without manual input, enhancing scalability. Video LVLMs. In recent years, large vision-language models (LVLMs) have advanced VideoQA tasks significantly. Prior works have enhanced model performance by experimenting with various architectures, with some aligning visual and textual features through Q-Former [5, 16, 48], while others concatenate frame-level features directly [14, 19]. Balancing frame count with token efficiency has become key focus, typically addressed by uniform sampling [5, 49] or additional modules [22, 35]. Our approach 2 Figure 2. The automatic generation pipeline of VideoEspresso. (i) Question-Answer Pair Construction: We use video frame-leveled captions to extract the key frames of the video and group descriptions of these frames. Then, we prompt GPT-4 to design questions for each group of video frames. (ii) Multimodal Chain-of-Thought Annotation: We extract key evidence text and generate captions with the highest relevance to the question with GPT-4o. Additionally, we annotate spatial and temporal information for key items, which results in multimodal Chain of Thought data pairs grounded in both temporal and spatial dimensions. employs tiny model that selectively captures frames relevant to the question, minimizing context length while preserving essential spatiotemporal information. Visual CoT. Chain-of-Thought (CoT) techniques enhance the reasoning abilities of LLMs by guiding them through intermediate reasoning steps to produce more accurate answers. While prior works [45, 51] have applied CoT to visual tasks, they primarily focus on text-level reasoning, often overlooking visual comprehension. Recent studies [34, 39] improved performance by targeting specific image regions. In videoQA, VideoCoT [38] focuses on textlevel reasoning, while VoT [10] emphasizes spatial relationships. Our approach integrates key object regions and core frames in the video CoT reasoning, capturing both spatial and temporal details to enhance video understanding. 3. VideoEspresso In this section, we introduce VideoEspresso, large-scale VideoQA dataset designed to facilitate high-level reasoning over macroscopic video semantics. This dataset is generated through scalable, fully automated generation pipeline that produces high-quality reasoning VideoQA pairs from distilled video content. The VideoEspresso construction pipeline consists of the following key stages: (1) We collect raw video data, followed by redundancy reduction streamline video frames that encapsulate essential content; (2) Based on these frames, we generate QA pairs that capture the core semantics of each video; (3) To further enable interpretability and bolster the benchmark for complex reasoning capabilities, we incorporate fine-grained Chain-ofThought (CoT) annotations, which connect core visual elements through spatial and temporal interactions, bridging the reasoning gaps in traditional VideoQA pairs. 3.1. Video Data Curation We leverage the vast amount of unannotated Internet videos for building scalable datasets. To build videoQA dataset with complex semantic reasoning, selecting appropriate data sources and types is essential. As illustrated in Fig. 3, we collect raw videos from 7 datasets [9, 26, 35, 37, 40, 44, 52] encompassing rich temporal dynamics, logical sequences, and causal relationships. These high-level semantics provide strong foundation for constructing complex and coherent question-answering dataset. We conduct manual review of these videos to evaluate their reasoning potential. The dataset includes diverse video types, covering genres such as news, movies, documentaries, animation, 3 Figure 3. The statistical analysis of our VideoEspresso dataset. and educational content. Based on these characteristics, we predefined 14 tasks to assess the models capabilities across various dimensions of video reasoning tasks. 3.2. Redundancy Removal in Video Frames The goal of this module is to eliminate redundant information in the video and retain the essential content by selecting concise sequence of frames. Different videos exhibit Initially, we varying rates of content and scene changes. determine an appropriate sampling interval based on the type of video. For instance, for rapidly changing dynamic scenes, we set the FPS between 2 and 4, whereas for static scenes, we choose longer sampling interval, with FPS set to 1. Then, to capture the fine-grained semantic information of the video as input for constructing QA pairs, we use InternVL2-8B [7] to perform frame-level captioning on all sampled frames. To filter out redundant frames in the video, we leverage the language retrieval model BGE-M3 [4] to preliminarily remove highly similar frames through finegrained semantic filtering. Specifically, for all sampled frame descriptions C, if the cosine similarity between the textual features ϕT (c) of adjacent captions exceeds preset threshold τ , we apply Last-In-First-Out (LIFO) filtering approach. This process results in concise caption sequence and the corresponding frames. The process is formulated as follows: = arg max ci,cj cos(ϕT (ci), ϕT (cj)), (c C, if S(c) < τ ), (1) (2) where denotes the similarity matrix, and ϕT (c) represents the feature computation for the caption c. 3.3. Question-Answer Pair Construction This module aims to leverage the powerful language reasoning capabilities of LLMs to automatically construct highFigure 4. The dataset attributes comparison between our VideoEspresso and MVbench. quality video reasoning QA pairs based on detailed descriptions of video frames. To maintain semantic continuity within the groups and avoid issues such as model hallucinations and failure to follow instructionscaused by an excessive number of tokenswe adopt continuous grouping approach to streamline frames. Specifically, for all captions of single video, every 15 consecutive frame captions are grouped into group Gi that preserves both frame-level details and inter-frame correlations. To obtain complex reasoning QA pairs, we design and iteratively refine prompts to ensure that the LLM follows the rules in constructing them. As shown in the top-right corner of Fig. 2, our prompts instruct GPT-4o [31] to generate question-answer pairs based on multi-frame descriptions, while also ensuring that GPT-4o maintains consistency between the descriptions to construct complex video reasoning questions. To improve the quality of the QA pairs, we validate them by designing an additional LLM [1] to verify the quality of both the questions and answers, including eliminating hallucinations in the QA pairs, checking the factual accuracy of the answers, and filtering out answers to highly subjective or difficult-to-evaluate open-ended questions. Ultimately, we obtain high-quality reasoning QA pairs for these videos and record the frame sequence grouping corresponding to each QA pair. 4 Figure 5. Two-Stage Video Evidence of Thought Training Procedure. The Frame Selector comprises tiny LVLM and tiny LLM, tasked with generating captions for videos and selecting the most relevant frame to as core video token for large reasoning model. twostage supervised fine-tuning technique is employed. During stage-1, set of cue prompts is introduced to guide the model in producing evidence, while in stage-2, the evidence generated from stage-1 is concatenated and used directly to guide the answer generation. 3.4. Multimodal Chain-of-Thought Annotation assist in answering questions. To further enhance the reasoning capabilities of models, this module focuses on annotating multimodal evidence that contains key spatiotemporal information. First, we group the Q-A pairs obtained in Sec. 3.3 along with the corresponding frame sequences as input, and design the prompt shown in the lower-left corner of Fig. 2 to guide GPT4o [31] in extracting key information. Sparse core frames are sufficient to capture enough information to answer the question; hence, we need to obtain the core frames most relevant to the question. Accordingly, our designed prompt primarily guides GPT-4o to extract the following key in- (1) From the captions group, select the capformation: tions most relevant to the question, i.e., the captions of core frames; (2) extract key objects from these captions, i.e., the key items; and (3) organize these key objects into natural language description as evidence to support answering the question, i.e., the evidence. To expand the dimensions of reasoning, we annotate these key elements with temporal and spatial information. For spatial annotation, we apply GroundingDINO [25] to mark bounding boxes around all key items and leverage CLIP-ViT-B/32 [32] to verify consistency between labels and objects within the bounding boxes. For temporal annotation, since GPT-4os generated captions of core frames GGP in (1) cannot directly match the original captions at the string level, we employ BGE-M3 [4] to retrieve the caption in the original set Gi and obtain temporal grounding information t. The process is formulated as follows: = arg max cos(ϕT (cj), ϕT (ck)), (3) where cj GGP , ck Gi. Finally, we obtain both textual evidence and multimodal key information that includes temporal and spatial dimensions, which can serve as the intermediate reasoning step to 3.5. Data Analysis In Fig. 3 and Fig. 4, we provide visualization of the data analysis and comparisons. To investigate the temporal distribution of key information in videos, we first examine the distribution of distances between adjacent key frames across different tasks. As shown in Fig.3.(a), the distance distribution between key frames varies significantly across tasks, indicating that the conventional strategy of uniformly sampling video frames is suboptimal and introduces substantial redundancy. As shown in Fig. 3.(b), the number of key items in the CoT of our dataset is diverse, encompassing range from few to numerous critical elements, reflecting the complexity and diversity of the visual content. In addition to the unique characteristics of our dataset, we also compare it with the QA content of popular dataset, MVBench [20]. As shown in Fig. 4.(a), we illustrate the differences between the datasets in terms of token length. The QA set length in MVBench (right) is shorter, while the answer set in our VideoEspresso (left) is on average much longer and exhibits greater diversity in distribution. As illustrated in Fig. 4.(b), we further present comparison of the word clouds for VideoEspresso and MVBench. In the Question set, our VideoEspresso emphasizes reasoning based on visual facts, with keywords like considering, based and inferred In contrast, MVBench emphasizes basic inquiries such as object, person and action In the Answer set, VideoEspresso includes not only reasoningrelated keywords as previously described but also terms associated with reasoning steps, such as Initially and Finally On the other hand, MVBench focuses on object definitions and spatial relationships within videos, with keywords like object, left and forward. 5 4. Hybrid LVLMs Collaboration for VideoQA 4.2. Fine-Grained Reasoning via LVLM To fully unleash the potential of high-quality video QA pairs offered by VideoEspresso, we propose an efficient video reasoning framework with hybrid LVLMs collaboration to enable cost-effective and accurate video LVLM reasoning. As shown in Fig. 5 the framework consists of two core components: lightweight selector that identifies core frames that are closely related to the input question, and powerful LVLM that performs content understanding and reasoning based on these selected core frames. 4.1. Core Frames Selection via Tiny LVLM We propose Lightweight Selector designed to extract core frames that are closely aligned with the question from input videos. Unlike traditional keyframe extraction methods, which primarily filter out semantically similar frames, our approach dynamically selects question-driven core frames to meet diverse task requirements. This enables reduction in the number of frames passed to large models compared to conventional methods that rely solely on frame semantic similarity. Moreover, this selector serves as plug-and-play module that can be inserted before any LVLMs. Our architecture comprises lightweight LVLM with 1 billion parameters and an LLM with 0.5 billion parameters in sequential setup. The LVLMs function is to convert video frames into language descriptions, while the LLM selects frames most relevant to the question based on these descriptions. Specifically, the process consists of two steps. (1) Frame Captioning: Given video and specified frames-per-second (FPS) sampling rate, the LVLM samples frames and generates caption ci for each frame fi. This process can be formulated as: {fi}N {ci}N i=1 = SampleFrames(V, FPS) i=1 = LVLM({fi}N i=1) (4) (5) Given key frames extracted from the first stage, our goal is to enable the model to effectively leverage multimodal spatiotemporal evidence for answering complex reasoning tasks. We design two-stage supervised fine-tuning paradigm. In the first stage, we guide the model to extract essential visual evidence from video data relevant to the question, establishing foundation for deeper reasoning. This is achieved through supervised fine-tuning using instructions like Please provide evidence to help answer the question. to guide the model in generating evidence. This evidence-based generation process not only filters core information but also enhances multimodal alignment and prepares the model for subsequent reasoning tasks. In the second stage, we further fine-tune the model to directly generate answers based on the extracted multimodal evidence. This is achieved through supervised fine-tuning using instructions like Please answer the question with the help of evidence. to guide the model in answering with evidence. Unlike traditional single-stage question-answering methods, this two-stage structure divides evidence generation and answer generation, enhancing transparency in reasoning and boosting response accuracy. Besides, it ensures that the model gradually integrates multimodal information for complex spatiotemporal reasoning, significantly improving performance in video question-answering tasks by producing more logically coherent answers. 4.3. Inference During the inference phase, we first use the Lightweight Selector to extract core frames from the video that are closely related to the question, serving as input for subsequent reasoning. We then leverage fine-grained reasoning LVLM, which generates evidence through chain-of-thought process to support the final answer generation. This workflow enables efficient question answering, from frame selection to answer generation. where is the total number of sampled frames and {ci}N represents the collection of captions for these frames. i=1 5. Experiments (2) Core Frame Selection: Using the set of captions {ci}N i=1 and question q, the LLM identifies the subset of captions most relevant to q, resulting in set of core frame captions {c j}M j=1: {c j}M j=1 = LLM({ci}N i=1, q) (6) where and {c j}M j=1represents the final set of core captions selected for their relevance to the question. Since this step requires minimal reasoning from the model, we adopt cost-efficient solution to address the challenges posed by excessive token length when handling video input in large models. 6 5.1. Overview of the Evaluation Benchmark Our VideoEspresso includes 14 predefined tasks, with each constructed QA pair matched to corresponding task using GPT-4o. If no suitable task alignment is identified, the pair is categorized as Others. To establish comprehensive benchmark, the defined tasks encompass diverse perspectives, including time, logic, scene, behavior, and state, illustrated by examples such as Event Dynamics, Causal Analysis and Theme Analysis. Additionally, the framework incorporates real-world application tasks, such as Cooking Process and Traffic Analysis. The benchmark assesses the performance of LVLMs through both objective and subjective evaluations, providing multifaceted Models #Frames Param TFLOPs Narra. Event Ingre. Causal Theme Conte. Influ. Role Inter. Behav. Emoti. Cook. Traff. Situa. Avg. Closed-source LVLMs GPT-4o [31] Qwen-VL-Max [3] FPS=3 FPS=3 - - - - 32.3 33.9 16.7 22. 25.5 23.5 22.8 21.4 32.8 26.2 27.5 30.3 37.5 28.6 24.2 41.7 30.2 27.4 19.3 26. 30.8 20.0 30.2 20.0 22.0 26.4 20.8 16.7 24.0 26.0 Opened-source LVLMs 4 LLaVA-1.5 [23] FPS=1 InternVL2 [7] FPS=1 LLaVA-N-Inter [17] FPS=1 Qwen2-VL [3] 128 LongVA-DPO [49] mPLUG-Owl3 [46] FPS=1 LLaVA-N-Video [50] FPS=1 2.36 Ours 7B 8B 7B 7B 7B 7B 7B 8.5B 14.50 73.23 62.78 64.60 465.4 89.78 60.42 9. 32.3 33.9 24.2 27.4 35.5 30.6 31.2 45.2 21.3 24.1 23.6 23.0 14.9 23.6 20.2 27.0 19.4 27.6 26.5 24.5 16.3 20.4 16.2 33.7 17.1 24.4 19.2 23.5 19.0 22.3 17.6 26.1 26.2 42.6 31.1 29.5 34.4 37.7 36.5 39.3 20.2 33.0 32.1 31.2 22.0 29.4 32.7 36. 36.1 33.3 21.0 45.8 28.6 19.4 31.9 17.5 24.2 47.2 31.7 22.6 37.5 23.8 29.0 48.6 34.9 30.6 30.6 24.5 26.4 55.6 41.3 30.6 21.1 22.8 21.1 28.1 22.8 24.6 24.5 29.8 20.0 21.5 26.2 40.0 20.0 27.7 34.7 30.8 35.8 16.7 18.0 24.2 34.0 20.0 24.0 28.7 30.2 13.3 20.0 24.4 22.6 30.0 18.0 28.5 37.7 16.7 12.0 24.4 24.5 13.3 24.0 28.0 20.8 20.3 17.0 25.2 35.8 20.0 26.0 34.1 Table 1. Main Result on Our Objective Benchmark. We report results of closed-source and opened-source LVLMs with ours. The process of constructing task evaluations is shown in the supplementary. TFLOPs refers to the total computational cost of inference, measured under the same 16-second video input. Models 61.66 48.43 GPT-4o Qwen-VL-Max Log. Acc. Fac. Closed-source LVLMs 73.15 63.11 50.33 62.46 Open-source LVLMs LLaVA 1.5 49.56 60.53 InternVL2 56.32 70.64 52.34 LLaVA-N-inter 63.27 53.67 66.31 Qwen2-VL-7B 54.72 LongVA-7B-DPO 67.98 mPLUG-Owl3 53.05 66.14 54.11 63.42 LLaVA-N-Video 61.28 72.25 Ours 49.93 54.53 48.45 50.84 52.78 50.97 49.55 59.68 Con. Overall 70.02 60.21 66.13 53.37 62.1 66.76 66.78 68.88 58.38 67.3 63.31 75.73 52.12 60.05 55.16 57.66 57.19 57.14 56.43 65.84 Table 2. Results on Subjective Benchmark. We report the metrics of Logic (Log.), Factuality (Fac.), Description Accuracy (Acc.), and Conciseness (Con.). analysis of their capabilities. Experimental Setting. To comprehensively evaluate the capabilities of LVLMs on VideoQA tasks, we selected: (1) closed-source large models, such as GPT-4o [31] and Qwen-VL-Max [3]; (2) general-purpose LVLMs that claim strong video capabilities on video benchmarks, such as InternVL [7] and Qwen2-VL [3]; and (3) popular video LVLMs, such as LongVA [49] and mPLUG-Owl3 [46].To ensure the fairness of the reported accuracies, the video frame sampling scheme, temperature, and other parameters follow the settings from the original paper. Additionally, we standardize the maximum token length of the outputs to 512. As our model training details, the learning rate is set to 2e-5, the warmup rate is 0.03, and we train the model for one epoch with global batch size of 16. The training and evaluation process is facilitated on 8 NVIDIA-A100 GPUs. Evaluation. To more accurately evaluate the open-ended responses of LVLMs, we propose two-step evaluation In the method based on fine-grained semantic similarity. 7 first step, we assess the semantic similarity between the models output and the reference answer. If the similarity exceeds 80%, the output is considered potentially correct In the second step, we infrom semantic perspective. troduce three highly confounding distractors for each reference answer. We then calculate the similarity between the models output and each distractor. If the similarity with any distractor surpasses that of the models output to the reference answer, the response is deemed incorrect. Only if the output passes both steps is it classified as correct. Meanwhile, we incorporate subjective evaluation by assessing the generated content across multiple dimensions, including logic, factuality, desciption accuracy, and conciseness. To facilitate this, we designed an evaluation framework that prompts GPT-4o [31] to rate the models output on scale of 1 to 10 based on the ground truth, along with providing an overall score. Moreover, during evaluation, we do not directly input the options into the model, which effectively prevents potential information leakage. Finally, we report the accuracy for each task across the entire VideoEspresso dataset. 5.2. Results on Benchmark Objective Evaluation Results. We evaluated 7 opensource and 2 closed-source LVLMs on 14 video reasoning tasks in objective evaluation. As shown in Tab. 1, our method achieves the state-of-the-art performance across 12 tasks, and an average accuracy of 34.1%. This performance surpasses that of the top-performing open-source model, InternVL2 [7], and the closed-source GPT-4o [31] by 5.4% and 7.7%, respectively. Compared to our selected backbone, LLaVA-Next-interleave, the performance improves by nearly 10% after fine-tuning with reasoning instructions. In addition to its advantage in video reasoning QA, our method also demonstrates the leading efficiency. Specif-"
        },
        {
            "title": "Setting",
            "content": "Acc. baseline GT-CoT w/o Bbox w/o CoT 34.13 72.95+38.82 33.140.99 31.322."
        },
        {
            "title": "Selector",
            "content": "#Frame Add. GPU hr."
        },
        {
            "title": "Inference Memory",
            "content": "Acc. Uniform GT 1B/1.5B 1B/0.5B 8 2.98 2.77 2.36 - - 2.5B 1.5B - - 1.33 0.37 0G + 14G + 40G 0G + 14G +15G 5G +14G +14G 3G +14G +12G 33.74 37.54+3.80 34.76+1.02 34.13+0.29 Table 3. Ablation Studies on different CoT strategies. GT is ground truth. Table 4. Ablation studies on Selector. GT refers to the ground truth time of the core frames annotation. 1B/1.5B represents the selector consists of InternVL2-1B and QwenLM-1.5B, and similarly for 1B/0.5B. Add. stands for additional parameters. ically, the average number of input frames is only 1.8% of that used by LongVA-DPO [49], and the FLOPs calculated on the same video input are only 14.74% of those for LLaVA-Next-interleave. Notably, InternVL2 and LongVADPO excel in the Theme Analysis and Cooking Process tasks, which is likely due to their exposure to large size of the same type of data during training process. Subjective Evaluation Results. We evaluated the quality of LVLMs answers in subjective evaluation across four aspects, including logic consistency, factuality, accuracy, and conciseness. As shown in Tab. 2, the results align closely with the observations from the objective evaluation. GPT4o exhibits strong performance in both logical reasoning and factual accuracy, due to its robust language reasoning capabilities and extensive prior knowledge. However, among all open-source LVLMs, our method outperforms the approaches presented in the table across all four dimensions. Notably, in the Conciseness evaluation, our method surpasses GPT-4o by 5%, further demonstrating the significant contribution of our VideoEspresso dataset in enhancing models ability to learn video reasoning. 5.3. Ablation Study Ablation on CoT. Our results in Tab. 3 further prove the effectiveness of visual grounding in CoT evidence. Moreover, ablation experiments involving the ground truth of CoT and the ablation of the CoT process further demonstrate the potential of CoT in enhancing performance on visual tasks. The performance boost achieved by the CoT ground truth is significant, highlighting the importance of endowing LVLMs with reasoning QA capabilities. Ablation on Selector. As shown in Tab. 4, we conducted ablation experiments under different selector combination settings, specifically InternVL2-1B [7] + QwenLM1.5B [2] and InternVL2-1B + QwenLM-0.5B. In this set of experiments, our core frame selection significantly improved video understanding capabilities compared to the uniform sampling method. Although selectors of different sizes may increase memory usage by 3GB or 5GB, the optimization of redundant keyframe tokens led to reduction of 26-28GB in memory usage, resulting in significant improvement in overall video understanding efficiency. We Model GPT-4o GPT-4o GPT-4o Sample #Frame Ratiotok TFLOPs Acc. 26.86 1 Uniform 16 28.26 0.17 1B/0.5B 2.77 29.45 0.15 1B/1.5B 2.36 - - - InternVL2 Uniform 16 InternVL2 1B/0.5B 2.77 InternVL2 1B/1.5B 2.36 LongVA LongVA LongVA Uniform 128 1B/0.5B 2.77 1B/1.5B 2. LLaVA-N-i Uniform 16 LLaVA-N-i 1B/0.5B 2.77 LLaVA-N-i 1B/1.5B 2.36 1 0.17 0.15 1 0.02 0.02 1 0.17 0.15 73.23 12.68 10.80 28.57 29.23 30. 465.44 24.41 23.18 10.07 23.85 8.58 62.78 10.86 9.26 24.37 24.20 24.26 Table 5. Evaluations results with selector adoption. also tested more lightweight LVLMs, such as LLaVA-Nextinterleave-0.5B [17] and Qwen-VL-1.5B [3]. However, the results did not meet expectations, likely due to the Tiny LVLMs processing too many image tokens, exceeding their capacity for handling them effectively. 5.4. Adapting Selector to other LVLMs We further apply the selector to other LVLMs to explore whether the extracted core frames can be effectively generalized to other models in zero-shot manner. As shown in Tab. 5, we evaluated the methods performance on GPT-4o and several open-source LVLMs. The results demonstrate performance improvements and reduction in the number of input frames, with the frame input reduced by approximately 15% on both GPT-4o and InternVL2. For the other two models, experiments show that introducing the selector leads to slight loss in performance, but substantial gains in frame input. Notably, LongVA achieved 98% reduction in frame input, which highlights that our proposed selector still aids in reducing computational overhead for reasoning in LLMs, as plug-and-play module. 6. Conclusion In this paper, we presented VideoEspresso, novel dataset designed to enhance video reasoning by addressing the limitations of existing datasets in terms of scale and gran8 ularity. Our approach employs semantic-aware key-frame extraction and leverages GPT-4o to generate fine-grained VideoQA pairs with Chain-of-Thought evidence. By integrating Hybrid LVLMs Collaboration framework, we achieve cost-effective and accurate video reasoning, outperforming baseline models on the majority of tasks VideoEspresso sets across our proposed benchmark. new starting point in video reasoning, offering rich annotations that facilitate advanced multimodal understanding. We hope our contributions can facilitate future exploration and development of more sophisticated models capable of tackling complex video reasoning challenges."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024. 2, 4 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 8 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. ArXiv preprint, 2023. 2, 7, 8 [4] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Bge m3-embedding: MultiLian, and Zheng Liu. lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024. 4, 5 [5] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. [6] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479, 2024. 2 [7] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 4, 7, 8 [8] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. 11 [9] Hang Du, Sicheng Zhang, Binzhu Xie, Guoshun Nan, Jiayang Zhang, Junrui Xu, Hangyu Liu, Sicong Leng, Jiangming Liu, Hehe Fan, et al. Uncovering what why and how: comprehensive benchmark for causation understanding of video anomaly. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18793 18803, 2024. 3 [10] Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Forty-first International Conference on Machine Learning, 2024. [11] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. ArXiv preprint, 2023. 2 [12] Gemini Team. Gemini: family of highly capable multimodal models. ArXiv preprint, 2023. 2 [13] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. 11 [14] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. ArXiv preprint, 2023. 2 [15] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. TVQA: Localized, compositional video question answering. In EMNLP, 2018. 2, [16] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [17] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 7, 8, 11 [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [19] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. ArXiv preprint, 2023. 2 [20] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. ArXiv preprint, 2023. 2, 5 [21] Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: multi-task benchmark for video-and-language understanding evaluation. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks, 2021. 11 [22] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. ArXiv preprint, 2023. 9 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 7 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv preprint, 2023. 2, 11 [25] Siyi Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chun yue Li, Jianwei Yang, Hang Su, Jun-Juan Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. ArXiv preprint, 2023. 5 [26] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated In International Conferquestion answering in 3d scenes. ence on Learning Representations, 2023. 3 [27] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. [28] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. In NeurIPS, 2024. 2 [29] OpenAI. Introducing chatgpt. 2022. 2 [30] OpenAI. Gpt-4 technical report, 2023. 2 [31] OpenAI. GPT-4o system card, 2024. 2, 4, 5, 7, 12 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 5 [33] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. ArXiv preprint, 2023. 2 [34] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. arXiv preprint arXiv:2403.16999, 2024. 3 [35] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 2, 3, 11 [36] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum: Summarizing web videos using titles. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 51795187, 2015. 2 [37] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. 3 [38] Yan Wang, Yawen Zeng, Jingsheng Zheng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. Videocot: video chain-ofthought dataset with active annotation tool. arXiv preprint arXiv:2407.05355, 2024. 3, [39] Penghao Wu and Saining Xie. V?: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 3 [40] Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXX 16, pages 322339. Springer, 2020. 3 [41] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, pages 97779786, 2021. 2, 11 [42] Junbin Xiao, Yao Angela, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In arXiv, page preprint, 2023. [43] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia, 2017. 11 [44] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal arXiv long story generation with large language model. preprint arXiv:2407.08683, 2024. 3 [45] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3 [46] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding arXiv preprint in multi-modal large language models. arXiv:2408.04840, 2024. 7 [47] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 2, 11 [48] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. ArXiv preprint, 2023. [49] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2, 7, 8 [50] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 7 [51] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 3 [52] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 3 10 A. Details of VideoEspresso"
        },
        {
            "title": "Task",
            "content": "# Train Set # Test Set Benchmark Core Frames CoT # Questions How2QA [21] ActivityNet-QA [47] NExT-QA [41] MovieChat [35] TVQA [15] MSRVTT-QA [43] VideoCoT [38] VideoEspreeso 2,852 8,000 8,564 13,000 15,253 72,821 11,182 T&V 203,546 Table 6. Dataset comparison between videoQA datasets. and represent the textual and visual elements in the CoT, respectively. Dataset Comparison. Existing VideoQA datasets [15, 21, 35, 38, 41, 43, 47] are limited by manual annotations, making it challenging to scale up to meet the demands of LVLM training. In contrast, our proposed dataset, VideoEspresso, contains over 200K question-answer pairs (Tab. 6), significantly enhancing the dataset scale. Moreover, we annotate highly relevant core frames within the videos, providing fine-grained representation of temporal information. While VideoCoT [38] only introduces text-level chains of thought (CoT), we address the gap in previous work by incorporating visual elements into CoT process. Dataset details. As shown in Tab. 7, VideoEspresso comprises 14 tasks, with the training and testing sets divided according to specific proportions. The detailed question design for each task is presented in Tab. 9. As shown in Fig. 6, traditional videoQA datasets sample all frames of video at equal intervals. In contrast, VideoEspresso only focuses on the core frames of the video, which are highly relevant to the question. Unlike conventional videoQA tasks, which predominantly focus on querying actions or participants within the video, our dataset prioritizes the finegrained logical reasoning, requiring deeper understanding of complex temporal and contextual relationships. Moreover, the analysis of multimodal evidence integrated within the Chain-of-Thought reasoning process enhances both the accuracy and robustness of the generated answers, ensuring they are substantiated by comprehensive contextual understanding."
        },
        {
            "title": "Causal Inference\nContextual Interpretation\nEvent Process\nInteraction Dynamics\nBehavior Profiling\nEmotional Recognition\nInfluence Tracing\nRole Identification\nNarrative Structuring\nThematic Insight\nSituational Awareness\nCooking Steps\nIngredient Details\nTraffic Analysis",
            "content": "87,009 20,057 29,227 7,322 660 3,505 5,749 9,134 3,940 10,650 1,018 276 22,552 1,065 426 109 174 62 57 65 72 63 62 61 50 53"
        },
        {
            "title": "Total",
            "content": "202,164 1,382 Table 7. Tasks distribution and dataset split in VideoEspresso. Stage2 224 6144 Stage1 224 6144 config input resolution max token length LoRA weight ratio learning rate schedule learning rate batch size warmup epochs total epochs Table 8. Training Hyperparameters for different stages. True 0.02 cosine decay 0.03 1 0.03 1 2e-5 1e-5 C. Prompt Details In this section, we present the complete set of prompts utilized in the data generation pipeline, alongside those employed for subjective evaluation. Specifically, these include the prompt designed for QA construction in Fig. 7, the prompt aimed at filtering low-quality QA pairs in Fig. 8, the prompt used for constructing CoT evidence in Fig. 9, and the prompt applied for subjective evaluation in Fig. 10. B. Training Implementation The hyperparameters used at different training stages are listed in Tab. 8, following LLaVA-Next architecture [17, 24]. During both stage, we leverage diverse instruction data and integrate LoRA modules [13] into the LLM with rank of 16, an alpha value of 32, and dropout rate of 0.1. Flash attention [8] is applied to accelerate the training process. D. Evaluation Analysis Construction of Test set. For all questions, we devised three distractor options that maintain consistent contextual relevance and similar linguistic structures to the correct answer while presenting distinct factual inaccuracies, thereby enhancing the robustness of the objective process. Furthermore, to mitigate potential biases arising from significant token-length disparities in the second step of the objective 11 Logical Reasoning Causal Inference How did the actions of the robot and display on the screen contribute to the successful resolution in the control room? Contextual Interpretation How does the presence of the small cat and Georges exploration relate to the chefs activities? Event Process What transition do the rabbits experience from the time the moon rose to when they drift off to sleep? Social Understanding Interaction Dynamics Considering the atmosphere and expressions depicted, what can be concluded about the progression of the interaction between the man and the woman? Behavior Profiling Discuss how the actions of the baby triceratops with different dinosaurs reveal aspects of its behavior and the responses of the other dinosaurs. Emotional Recognition How does the emotional journey of the small purple dinosaur from feeling lost to excitement tie into the groups decision to explore the cave? Influence Tracing How did the presence of the dolphin and the sea monster influence the dinosaurs experience at the waterbody? Discourse Comprehension Role Identification How does the womans role in coordinating town safety relate to the devices activation with green checkmark and an orange flame? Narrative Structuring Considering the changes between the two frames, what can you infer about the narrative progression between the two depicted scenes? Thematic Insight How do the changing production logos contribute to the thematic preparation for the viewer before the main storyline begins? Situational Awareness Based on the sequence of events, how does the situation described contribute to the visual effect observed in the third frame? Cooking Steps Ingredient Details Traffic Analysis Considering the sequence of actions, what cooking technique is being employed, and how is it crucial for the fried chicken? If the person is preparing chili con carne, what is the purpose of the liquid being poured into the pan? Analyze the potential destinations of the visible vehicles based on their types and cargo as inferred from the images. Reality Application Table 9. Our proposed task categories with question prototypes. Figure 6. Comparison between VideoEspresso and other VideoQA dataset. evaluation, we employed GPT-4o [31] to standardize the length and ensure balanced distribution across all answer options for each question (shown in Fig. 12). Details of Objective Evaluation. As illustrated in Algorithm 1, our objective evaluation is divided into two distinct steps. In the first step, the semantic similarity between the models output and the reference answer is computed. If the similarity score SR falls below the predetermined threshold tau = 80%, the output is deemed incorrect. In the second step, set of three carefully selected con12 Figure 7. QA-Construction Prompt. Figure 10. Subjective Evaluation Prompt. Algorithm 1 Objective Evaluation for Open-Ended Output Require: Model output O, reference answer R, threshold τ = 80%, distractors {D1, D2, D3} Figure 8. QA-Filter Prompt. Return: Incorrect Ensure: Evaluation result: Correct or Incorrect Step 1: Semantic Similarity Assessment 1: Compute semantic similarity SR = Sim(O, R) 2: if SR < τ then 3: 4: end if Step 2: Confounding Distractor Analysis 5: for each distractor Di in {D1, D2, D3} do 6: 7: 8: Compute semantic similarity SDi = Sim(O, Di) if SDi > SR then Return: Incorrect end if 9: 10: end for 11: Return: Correct tween reference answers and the longest distractor option, as shown in Fig. 12, predominantly are confined to the interval [10, +10], indicating that the disparity in length between correct answers lie within is relatively minor. The distribution shows near symmetry along the y-axis, indicating balanced pattern: in about half of the cases, reference answers are longer than distractors, while in the remaining cases, distractors are longer. E. Case Study Leveraging VideoEspresso, the LVLM demonstrates superior performance in fine-grained reasoning tasks for video understanding. As shown in Fig. 13, while GPT-4o generates seemingly rich and plausible answers, including detailed analyses of elephant and monkey behaviors, it incorporates significant amount of video-irrelevant information, such as and their tusks for tasks like stripping bark or digging for roots and social foraging. This diminFigure 9. CoT-Evidence Construction Prompt. founding distractors {D1, D2, D3} is introduced for each reference answer. The semantic similarity SDi between the models output and each distractor is then computed. If any distractors similarity score SDi exceeds SR, the output is categorized as incorrect. Only outputs meeting the criteria in both steps are ultimately classified as correct. Analysis of test set. As depicted in Fig. 11, we present the example of reference answers and distractor options within the test set. The figure highlights factual inaccuracies in the distractor options using red annotations, while the correct answers are distinctly marked in green for clarity and emphasis. The token length disparities be13 Figure 11. Example of test set. represent the Reference Answer, while Di stand for the i-th Distractor. Figure 13. Example of over-analysis with GPT-4o. Figure 12. The Distribution of token length disparities between reference answers and the longest distractor option. ishes the proportion of visually grounded outputs. This issue is more pronounced in Fig. 14, where GPT-4o performs extensive analysis based on erroneous reasoning due to its failure to account for fine-grained inter-frame relationships, e.g., If the jacket is visibly marked by soot, burns, or other damage, it suggests the individual was close to the fire. From these examples, we underscore the critical role of visual information in video reasoning QA tasks. Figure 14. Example of Non-factual response with GPT-4o."
        }
    ],
    "affiliations": [
        "Beihang University",
        "CUHK",
        "Central South University",
        "Sangfor Technologies Inc.",
        "Shanghai AI Lab",
        "The University of Hong Kong"
    ]
}