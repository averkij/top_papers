{
    "paper_title": "COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training",
    "authors": [
        "Haocheng Xi",
        "Han Cai",
        "Ligeng Zhu",
        "Yao Lu",
        "Kurt Keutzer",
        "Jianfei Chen",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54x compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a 1.43x end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 3 1 3 9 1 . 0 1 4 2 : r COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training COAT: COMPRESSING OPTIMIZER STATES AND ACTIVATION FOR MEMORY-EFFICIENT FP8 TRAINING Haocheng Xi1, Han Cai2, Ligeng Zhu2, Yao Lu2, Kurt Keutzer1, Jianfei Chen4, Song Han2,3 1 University of California, Berkeley https://github.com/NVlabs/COAT 2 NVIDIA 3 MIT 4 Tsinghua University"
        },
        {
            "title": "ABSTRACT",
            "content": "FP8 training has emerged as promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (Compressing Optimizer States and Activations for FP8 Training), novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1) Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2) Mixed-Granularity Activation Quantization, which optimizes activation memory using combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by 1.54 compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves 1.43 end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngines speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing practical solution for scaling large-scale model training. The code is available at https://github.com/NVlabs/COAT. Figure 1: (a,b) Comparing the quantization flow of Transformer Engine and COAT. Both the optimizer states and activations are quantized to FP8 in COAT. (c) End-to-end per-GPU memory comparison when training Llama-2-13B on 880G H100 using FSDP."
        },
        {
            "title": "INTRODUCTION",
            "content": "Foundation Models (FMs), such as Large Language Models (LLM) and Vision Language Models (VLM), have made significant breakthroughs in various tasks such as reasoning, understanding, and Part of the work done during an internship at NVIDIA. 1 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training summarization (Dubey et al., 2024; Adler et al., 2024; Team et al., 2024; Lin et al., 2024). However, the training of such models, which often comprise billions of parameters, demands substantial computational resources and memory. This presents substantial challenges, making the training of these foundation models very challenging (Smith et al., 2022; Hoffmann et al., 2022). Low-precision training has emerged as promising approach to make FMs training more efficient (Micikevicius et al., 2017; Wang et al., 2018; Zhu et al., 2020; Xi et al., 2023; Wortsman et al., 2023; Xi et al., 2024). By quantizing tensors used in deep neural networks into lower precision, low-precision training effectively speed up the training process and reduce the memory footprint. Currently, BF16 training (Kalamkar et al., 2019; Micikevicius et al., 2017) is the most prevalent low-precision method, and is widely adopted in large-scale training frameworks like DeepSpeed (Rasley et al., 2020) and Megatron-LM (Shoeybi et al., 2019). With the advent of Nvidias H100 GPU (NVIDIA, 2024a), FP8 training Micikevicius et al. (2022) is emerging as the next-generation low-precision technique. Compared to BF16, FP8 training has the potential to (1) double the speed and (2) halve the memory footprint. To achieve practical speedup, Transformer Engine (NVIDIA, 2024b) performs matrix multiplications in FP8 precision, leading to faster training. Transformer Engines memory footprint can be further improved by reducing optimizer states, gradients, weights, and activations to lower precision. As illustrated in Figure 1, FP8-LM (Peng et al., 2023) advances this by further quantizing the gradients, weight master copy, and first-order momentum into FP8. This reduces memory and communication overhead, partially improving memory efficiency. However, they do not tackle the memory consumption of activations and still leave the optimizers second-order momentum in higher precision. The memory problem of activations becomes even more critical when optimizer, gradient, and weights are sharded across multiple GPUs using ZeRO or FSDP. Besides, second-order momentum is more sensitive to quantization than first-order momentum (Fishman et al., 2024), and activations large spikes also make them hard to quantize to FP8 (Yang et al., 2024). This potential accuracy degradation makes them missing crucial opportunity to optimize memory further. In this work, we propose COAT: Compressing Optimizer states and Activations for memoryefficient FP8 Training to address the aforementioned issue. COAT significantly reduces the overall memory footprint by quantizing optimizer states and activations into FP8. For optimizer states, we observe that FP8 formats representation range is under-utilized when quantizing them, as illustrated in Figure 2(a). To address this, we introduce novel Dynamic Range Expansion method which adjusts the distribution of optimizer states to better fit within the FP8 range, thereby minimizing quantization error. For activations, we propose Mixed-Granularity Activation Quantization to achieve efficient and accurate quantization. We apply fine-grained quantization to non-linear layers and apply per-tensor quantization to linear layers. Per-tensor quantization for matrix multiplications is more efficient and better suited for TensorCores, while fine-grained quantization helps maintain accuracy. These two approaches tackle high memory consumption while ensuring minimal performance degradation. We provide an overview of COAT in Figure 1(b) for demonstration. We demonstrate the accurate performance of COAT on wide range of tasks, including LLM pretraining, LLM fine-tuning, and VLM training. COAT achieves nearly lossless performance on all of these tasks. For efficiency results, COAT achieves 1.54 end-to-end memory reduction compared with BF16, and 1.43 end-to-end training speed up on Llama 7B, 13B, and 30B models compared to BF16. COAT also doubles the batch size in all realistic distributed training settings, which is crucial for higher speedup and support for longer context length, leading to more efficient training process for large-scale models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Low-precision Training Low precision training (Wang et al., 2018; Chen et al., 2020; Lin et al., 2022; Wortsman et al., 2023; Xi et al., 2024) has become prominent technique in modern deep learning, offering reductions in both computational costs and memory requirements. FP16 (halfprecision) training (Micikevicius et al., 2017) is the most prevalent low-precision method nowadays. It introduces loss scaling to address FP16s narrower representation range problem. BF16 training (Kalamkar et al., 2019) refines this approach, as BF16 has larger representation range and is more stable for large-scale training. In these approaches, forward and backward passes are computed in FP16 or BF16 precision, while master weights, gradients, and optimizers are stored in FP32. 2 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training FP8 training (Fishman et al., 2024; Micikevicius et al., 2022) aims to push these efficiency gains further. With the introduction of Nvidias Hopper GPU architecture, FP8 is emerging as practical datatype for next-generation low-precision training. Nvidias Transformer Engine (TE) (NVIDIA, 2024b) is the first framework designed for FP8 mixed-precision training that employs FP8 Tensorcore for linear layer calculation. FP8-LM (Peng et al., 2023) extends FP8 quantization to gradients and optimizer states, further improving the training throughput. However, they fail to reduce the memory usage of activations stored for the backward pass using FP8, and leave second-order momentum in FP16, limiting the full potential of FP8s memory advantages. Memory Efficient Optimizers While 32-bit optimizer (Kingma, 2014; Loshchilov, 2017) states are widely adopted, several research efforts have been made to reduce the memory footprint of optimizer states through quantization. The 8-bit Adam (Dettmers et al., 2021) introduces novel data format called dynamic exponent (DE) for quantization, but the adoption of this new data format limits its flexibility. 4-bit Optimizer (Li et al., 2024) further pushes the limit of optimizer quantization to 4-bit by addressing the zero-point problem, but is restricted to fine-tuning tasks. FP8-LM (Peng et al., 2023) quantizes the first-order momentum to FP8 while leaving second-order momentum in FP16, which limits the overall memory savings. (Fishman et al., 2024) finds that second-order momentum is more sensitive to quantization, and proposes to quantize it using E5M2 format. In addition to quantization, there are other approaches that aim to reduce the memory footprint of the optimizer states (Shazeer & Stern, 2018; Anil et al., 2019; Chen et al., 2024; Zhao et al., 2024), such as low-rank decomposition and optimizer simplification that only store the first-order momentum. These methods are orthogonal to our approach. Activation Quantization Activations consume significant amount of memory during training Cai et al. (2020), with memory usage scaling in proportion to sequence length and batch size. ActNN (Chen et al., 2021) is memory-efficient training framework designed to reduce the memory footprint by quantizing activations to 2 bits. Few-bit Backward (Novikov et al., 2023) focus on quantizing activation functions while achieving higher performance. Jetfire proposes INT8 data flow to quantize the activation of both linear and non-linear layers to reduce memory footprint."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "FP8 Quantization Quantization compresses high-precision tensor to low-precision to achieve speedup and save memory footprint, at the cost of lower precision and lower representation range. FP8 format consists of two encodings - E4M3 and E5M2 (Open Compute Project, 2023). E4M3 has higher precision, while E5M2 has larger representation range. We define E4M3s min and max min = 216 values as E4M3 and E5M2 min = 57344. To quantize an FP32 tensor into E4M3 precision, we use quantizer Q() to map the tensor into FP8s representation range. This process can be formulated as max = 448, while E5M2s min and max values are E5M2 min = 29 and E4M3 XFP8, SX = Q(XFP32), where XFP8 = (cid:23) (cid:24) XFP32 SX , SX = max (XFP32) E4M3 max , where SX is the scaling factor, refers to round-to-nearest. Quantize into E5M2 follows similar procedure, where we only replace E4M3 max . To map the quantized tensor back to FP32 precision, the dequantize operation DQ() can be formulated as XFP32 = DQ(XFP8, SX ) = SX XFP8. max with E5M2 Optimizer Update Rule Optimizers are widely used in deep learning to update parameters. The most common gradient-based optimizer is Adam/AdamW (Kingma, 2014; Loshchilov, 2017), which uses first-order and second-order momentum to achieve better convergence. The update rule of AdamW at time step can be formulated as: mt = β1mt1 + (1 β1)gt1 vt = β2vt1 + (1 β2)g2 t1 ˆmt = mt 1 βt 1 ˆvt = vt 1 βt 2 (cid:19) wt+1 = wt η (cid:18) ˆmt ˆvt + ϵ 3 + λwt (1) COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Figure 2: (a) Visualization of optimizer states dynamic range under per-group quantization. FP8 E4M3s representation range is under-utilized in this case. (b) After dynamic range expansion, FP8s representation range is well utilized. (c) Distribution of for optimizer states. The second orders is larger than the first orders k, since the second-order momentums dynamic range is smaller. where mt is the first-order momentum, vt is the second-order momentum, gt is the gradient, β1 and β2 are betas of AdamW, η is learning rate, λ is weight decay, and ϵ is used to prevent NaN or Inf. To perform optimizer state quantization, we adopt per-group quantization for both first-order and second-order momentum, following previous works (Dettmers et al., 2021; Li et al., 2024). Every consecutive element forms group (G is defined as the group size), and each group is quantized independently with its own statistics. Optimizer states are stored in FP8 precision, while its scaling factor is stored in BF16. More details can be found in Appendix A."
        },
        {
            "title": "4 DYNAMIC RANGE EXPANSION FOR ACCURATE OPTIMIZER QUANTIZATION",
            "content": "In subsequent sections, we explain how COAT utilizes FP8 quantization to achieve memory-efficient FP8 training without compromising accuracy. Section 4 focuses on optimizer states quantization, while Section 5 discusses activation quantization. 4.1 UNDERSTANDING THE ISSUE OF CURRENT OPTIMIZER STATES QUANTIZATION METHOD Under per-group quantization, we find that one significant drawback of current quantization methods is that, they can not fully utilize the representation range of FP8 and therefore lead to large quantization error. Take the E4M3 data format as an example, the ratio between E4M3s maximum repre512 = 229376 2105. sentable value and minimum representable value is E4M3 Therefore, for quantization group X, if we want to fully utilize the 256 representable value1 of FP8, we hope the dynamic range of the quantization group should cover the entire span between E4M3 min = 448 1 max /E4M3 min and E4M3 max . To make it more formally, we define dynamic range as the ratio between the maximum absolute value and the minimum absolute value within quantization group X: Definition 1 (dynamic range) Given set that consists of real numbers = {x1, x2, . . . , xG}, the dynamic range is defined as RX = max(x1, x2, . . . , xG) min(x1, x2, . . . , xG) , where denotes the absolute value. That is to say, E4M3s dynamic range is RE4M3 = 448 512 = 229376 2 105. However, in practice, many quantization groups within the optimizer states fail to effectively map values across this wide range. We observe that optimizer states are highly sparse, with fewer than 1% of values having large magnitudes, while the majority are relatively small and closely clustered. Most groups exhibit low dynamic ranges since large values are so few. As visualized in Figure 2(a), the dynamic 1Actually among them small amount of values represents NaN and Inf. 4 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Table 1: Quantization error of under different quantization settings. +Expand means applying our Dynamic Range Expansion method. MSE of First Order Second Order E4M3 E4M3+Expand E5M2 E5M2+Expand E4M3 E4M3+Expand E5M2 E5M2+Expand 20.10 15.13 37.02 17. 18.08 12.31 35.96 15.48 25.65 21.96 40.30 23.84 18.16 12.43 36.00 15. Figure 3: Dynamic Range Expansion can better utilize E4M3 representation range. range for first-order momentum is typically less than 1e4, and for second-order momentum, it is usually less than 1e1both far below the available range of FP8. As result, substantial portion of FP8s representational capacity is wasted, leading to large quantization error."
        },
        {
            "title": "4.2 DYNAMIC RANGE EXPANSION",
            "content": "To address this problem, we introduce expand function () before quantization to expand the dynamic range of the quantization group and align it with E4M3, which can be formalized as XFP8, SX = Q(f (XFP32)). The expand function we use is defined as where is used to control the strength of the expansion. (x) = sign(x)xk, For quantization group X, after applying the expand function to X, the dynamic range becomes Rf (X) = max(f (X)) min(f (X)) = max(sign(X)X k) min(sign(X)X k) = (cid:16) max(X) min(X) (cid:17)k = (RX )k. Therefore, when > 1, RX will be enlarged and become closer to the ideal RE4M3. The optimal satisfy that (RX )k = RE4M3, which means that = logRX (RE4M3). With this optimal k, (X) can fully utilize the representation range of E4M3, while the original can only utilize small portion of it. As shown in Figure 2(c), the second-order momentum typically has larger value (5 15) compared to the first-order momentums (1 3). This corresponds to our previous observation, that second-order momentum usually has smaller dynamic range compared with firstorder momentum, so it requires larger to align well with E4M3s dynamic range. We calculate on-the-fly for every optimizer step and for every quantization group for accuracy consideration. When dequantize, we apply the inverse of expand function 1(x) = 1 after dequantization to recover its original value, which can be expressed as FP32 = 1(DQ(XFP8, SX )). We apply regular quantizer and our dynamic range expansion method to both the first-order momentum and second-order momentum v. As visualized in Figure 3(b), the distribution after expansion can fully utilize the FP8 (E4M3) representation range, which proves the effectiveness of our method. We further quantify the effectiveness of our method in Table 1. In AdamW optimizer step, as stated in Eq. 1, v+ϵ to quantify the performance of quantization method. We find that E4M3 is more suitable for firstorder momentum than E5M2. For second order momentum, although E4M3 better than E5M2, their quantization error is nearly the same after applying our expand function. Our Dynamic Range Expansion can effectively reduce the MSE by 1.63. Appendix provide more results. v+ϵ is the actual effective term for weight update, so we report the MSE of m"
        },
        {
            "title": "5 MIXED-GRANULARITY ACTIVATION QUANTIZATION",
            "content": "5.1 DECOMPOSE THE ACTIVATION MEMORY FOOTPRINT In the forward pass of neural networks, activations must be preserved for the backward pass to calculate gradients. As illustrated in Table 2, non-linear layers such as LayerNorm/RMSNorm (Ba, 2016; Zhang & Sennrich, 2019) and Activation Functions (Hendrycks & Gimpel, 2016; Shazeer, 2020) 5 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Table 2: Activation memory footprint of different operators. is unit to measure memory usage, where 1U = Batch Size Sequence Length Hidden Size 2 bytes (for BF16). For Llama-style model, Act Func refers to SiLU & Multiply, and Linear refers to the summation of QKV/Attn/Up/Gate/Down projection. RMSNorm is upcast to float32 in transformers implementation, so the memory usage of LayerNorm in BF16 is 4U. Our method reduces activation memory by quantizing them to FP8. More details about FlashAttention in Appendix D. Non-Linear Attention Reduction Ratio RMSNorm Act Func RoPE FlashAttn Linear Total Ideal Achieved Llama-style BF16 TE COAT 4U 2U 1U 8U 8U 4U 2U 2U 2U 3U 3U 3U 5.66U 22.66U 1.00 3.33U 18.33U 1.23 3.33U 13.33U 1.69 1.00 1.20 1.65 typically account for approximately 50% of the memory footprint in the Llama model series Touvron et al. (2023). In contrast, linear layers contribute less than 25%. Therefore, it is essential to optimize both linear and non-linear layers to reduce activation memory footprint.2 One straightforward approach to achieve this is to quantize the activations to FP8 format prior to each non-linear and linear layer and only save the quantized tensors for backward. However, this introduces significant overhead due to the additional quantization step. Furthermore, non-linear layers can be sensitive to quantization. For example, GLU activation will amplify the spike in activations, resulting in significant quantization errors if not carefully handled (Yang et al., 2024; Fishman et al., 2024). This necessitates us to develop efficient and accurate quantization techniques. 5.2 MIXED GRANULARITY FP8 PRECISION FLOW To address the inefficiency and inaccurate problem, we propose to use mixed granularity FP8 precision flow to improve the accuracy without introducing too much overhead. FP8 precision flow requires the input and output of all linear and non-linear layers in FP8. By directly saving the input tensor in FP8 format for the backward pass, we eliminate the need for an extra quantization operation, which reduces the associated overhead. However, this method still suffers from accuracy degradation and necessitates further refinement. We propose to vary the quantization granularity across different layers to balance precision and efficiency in mixed-granularity manner. For non-linear layers, VS-Quant (Dai et al., 2021) or PerBlock Quant (Xi et al., 2024) methods are well-suited due to their fine-grained and precise nature. For linear layers, we apply per-tensor quantization to maximize the performance of Tensor Cores. We observe that quantizing the input of layernorm across multiple token axes is detrimental to accuracy. As illustrated in Figure 4(a), when the number of elements that share scaling factor is fixed, the quantization error increases significantly when quantization is performed across the token axis. Therefore instead of using per-block quantization with block size as proposed in (Xi et al., 2024), we propose to use per-group quantization with group size 1 G, where = B2 to keep the granularity the same. This approach enhances the accuracy of non-linear layers while maintaining efficiency. Our precise FP8 precision flow is visualized in Figure 1(a), where we display the full precision flow for Llama-style decoder layer, both forward and backward pass. We also propose Group Scaling, an efficient per-tensor scaling method that balances the performance and precision. To perform per-tensor quantization, the maximum absolute value of the tensor needs to be calculated through max reduction, adding lot of overhead. In our Group Scaling, we address these problems by splitting the max reduction into two stages: (1) performing max reduction on each 1 element and storing the results as intermediate values; (2) applying max reduction on the intermediate tensor to obtain the per-tensor max value. The first stage can be seamlessly fused with the previous operation, adding minimal overhead, while the second stage is more efficient than doing max reduction on the entire tensor, as the intermediate result is smaller than the original tensor. As illustrated in Figure 4(b), Group Scaling successfully reduces the max reduction overhead compared with just-in-time scaling. 2RoPE and FlashAttention is more as distinct module and should be handled separately. 3Finer-grained methods can also be applied since they are generally compatible with FP8 precision flow. 6 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Figure 4: (a) Quantization Error in forward pass. (b) Time comparison of various scaling methods. In comparison, TransformerEngine proposes delayed scaling to avoid the on-the-fly max reduction required for per-tensor quantization, and can also fuse the division process into the previous operator to optimizes memory accesses. However, we find that using the current tensors statistics to compute the scaling factor is no worse, and could be even better in precision, than using the delayed scaling heuristic. Therefore, we advocate for Group Scaling as simpler and more flexible alternative that can potentially offer improved numerical stability and precision while not being significantly slower than Delayed Scaling."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "6.1 ACCURACY EXPERIMENTS Setups We compare COAT with BF16 training and TransformerEngine (NVIDIA, 2024b) baselines. For all experiments, we adopt the default hyperparameters in the official training recipe. We validate the effectiveness of our method on multiple tasks, including Large Language Model (LLM) pretraining and fine-tuning, and Vision Language model (VLM) training. For LLM pertaining, we report the perplexity on Wikitext 103 (Merity et al., 2016), C4 (Raffel et al., 2020), and Pile (Gao et al., 2020), and the accuracy on COPA (Gordon et al., 2012), ARC (Clark et al., 2018), SciQ (Welbl et al., 2017), and HellaSwag (Zellers et al., 2019). For LLM fine-tuning, we conduct experiments in math corpus, and evaluate on Mathmeticas (Davies et al., 2021), SVAMP (Patel et al., 2021), NumGLUE (Mishra et al., 2022), and GSM8K (Cobbe et al., 2021). For VLM training, we report the score on VideoMME (Fu et al., 2024), POPE (Li et al., 2023b), VizWiz (Gurari et al., 2018), GQA (Hudson & Manning, 2019), VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019), SEED (Li et al., 2023a), and MMMU Validation Set (Yue et al., 2024). We use 1 128 per-group quantization for optimizer states and 1 16 per-group quantization for non-linear layer activations. 6.1.1 LLM PRETRAINING To evaluate our method when pretraining LLMs, We train OLMo-1B (Groeneveld et al., 2024) and OLMo-7B on Dolma (Soldaini et al., 2024). Following the official report, we use global batch size of 4M tokens (2048 macro batch size, with sequence length of 2048 tokens). We use PyTorch FSDP in our experiments. For OLMo-1B, we pretrain for 5500 steps, which corresponds to approximately 22B tokens. For OLMo-7B, due to the resource limitation, we perform continue pretraining for 1000 steps (about 4B tokens) and resume from the official checkpoint at 5000 steps. We report the training curve in Figure 5, and report the perplexity and accuracy result in Table 3. The training curve and downstream task performance were consistent with BF16 training and TransformerEngine baseline, validating the effectiveness of COAT. For OLMo-7B experiment, we report the training curve in Figure 6, where COAT also aligns well with the baseline and is nearly lossless. 6.1.2 LLM FINE-TUNING We further evaluate our method on LLM fine-tuning task. We focus on math corpus and fine-tune Llama-2-7B model on the MAmmoTH (Yue et al., 2023) dataset. We train for 3 epochs, and report the downstream tasks performance in 4. After fine-tuning, COAT still performs consistently with the baselines on the downstream task performance, proving the accurateness of our method. 7 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Table 3: OLMo-1B pretraining performance on downstream tasks. TE refers to TransformerEngine. Train Loss WikiText Pile Avg ppl BF16 TE COAT BF16 TE COAT 2.995 3.001 3.008 30.499 30.736 30. 27.966 28.064 28.099 17.405 17.434 17.453 25.290 25.411 25.391 COPA ARC(Easy) SciQ HellaSwag Avg Acc 60.0% 62.0% 61.0% 45.6% 45.4% 44.2% 67.3% 63.9% 67.6% 33.7% 33.8% 33.7% 51.6% 51.3% 51.5% Figure 5: OLMo-1B pretraining loss curve. Table 4: Evaluation result of fine-tuning Llama-2-7B on math corpus. Llama-2-7B refers to the evaluation metric before fine-tuning. TE refers to TransformerEngine. Mathmeticas SVAMP NumGLUE GSM8k Avg Llama-2-7B BF16 TE COAT 6. 46.3 45.3 47.8 14.6 64.2 66.1 64.4 34.5 54.8 53.5 53.3 29. 57.7 57.7 56.6 21.3 55.7 55.6 55.5 Figure 6: OLMo-7B training curve. 6.1.3 VLM TRAINING We also evaluate COAT on vision language models. We conduct experiments on VILA (Lin et al., 2024) and perform stage-3 SFT of VILA1.5-7B using the same SFT data mixture employed by VILAs original paper (Chen et al., 2023; Xu et al., 2024) and set the global batch size to 1024. We pad the sequence length to multiple of 4 for efficiency consideration. The training loss curve is visualized in Figure 7. We report the downstream task performance and their average in Table 5. We find that COAT performs on par with BF16 training and is better than the TransformerEngine baseline, which demonstrates the accurateness of our method. We further visualize the VLM captioning experiment in Appendix to prove the effectiveness of COAT on generation tasks. 6.2 MEMORY SAVING AND SPEEDUP We test the memory saving and speedup result of COAT in two settings: The results on single transformer layer help to accurately analyze the capabilities for memory saving and speedup, while the end-to-end results reflect the practical benefits of our method in real-world applications. 6.2.1 MEMORY SAVING AND SPEEDUP FOR SINGLE TRANSFORMER LAYER Table 6 highlights the speedup and memory reduction achieved for single transformer layer. We conducted experiments with batch size of 4, varying the hidden sizes between 2048 and 4096, and sequence lengths of 2048 and 4096. COAT demonstrates better speedup compared with TE, and significantly better memory reduction ability compared with BF16 and TE. Our approach achieves up to 1.57 speedup over BF16 and achieves consistent 1.65 memory reduction compared to BF16, which is very close to the theoretically 1.69 reported in Table 2. The speedup ratio becomes larger with larger hidden sizes and longer sequence lengths. 6.2.2 SPEEDUP AND MEMORY SAVING FOR END-TO-END TRAINING Table 7 presents detailed comparison of end-to-end memory reduction and speedup results across different configurations for transformer models, specifically Llama-2-7B, Llama-2-13B, and Llama30B, with variations in the number of GPUs used (1, 2, 4, and 8). It highlights COATs effectiveness 8 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Table 5: VILA1.5-7B Stage-3 SFT performance on downstream tasks. * means it has seen the training data. Stage 3 VideoMME POPE VizWiz GQA* VQAv2* BF16 TE COAT 42.96 43.19 44.56 86.90 87.64 87.43 61.42 57.61 61. SEED 64.55 64.53 64.44 81.47 81.34 81.20 Stage 3 TextVQA Image Video MMMU Val Average BF16 TE COAT 65.60 64.70 64.65 73.40 73.51 73.36 45.65 43.12 43.76 38.56 35.89 37. 62.80 61.88 62.51 Figure 7: VILA1.5-7B Stage-3 SFT loss curve. Table 6: Memory Saving and Speedup for single Transformer Layer. Memory refers to Activation Memory. Our method achieves better speedup than TransformerEngine and significantly reduces the activation memory footprint by 1.65. Hidden Size = 2048, Batch Size = 4 Sequence Length = 2048 Sequence Length = Forward Backward Total Ratio Memory Ratio Forward Backward Total Ratio Memory Ratio BF16 TE COAT 3.36 2.96 2.88 8.47 5.32 5.16 11.83 8.28 8. 1.00 1457 MB 1.42 1210 MB 1.47 883 MB 1.00 1.20 1.65 6.88 5.94 5.89 17.24 11.29 10.82 24.12 17.23 16.71 1.00 1.00 2914 MB 1.39 2420 MB 1.20 1.44 1766 MB 1.65 Hidden Size = 4096, Batch Size = 4 Sequence Length = 2048 Sequence Length = 4096 Forward Backward Total Ratio Memory Ratio Forward Backward Total Ratio Memory Ratio BF16 TE COAT 7.77 6.19 5.89 18.78 11.79 10.96 26.55 17.98 16.85 1.00 2914 MB 1.00 1.20 1.47 2420 MB 1.57 1766 MB 1.65 16.37 12.66 12.16 38.43 24.58 23. 54.80 37.24 35.6 1.00 5828 MB 1.00 1.20 1.47 4840 MB 1.53 3533 MB 1.65 in reducing end-to-end memory footprint and the speedup compared to standard BF16 and TransformerEngine (TE) setups under varying conditions of batch size and context length. COAT allows full-parameter training of Llama-2-7B on single GPU, where BF16 and TE both out of memory (OOM). Similarly, for the Llama-2-13B and Llama-30B models, our method enables 2-GPU training for Llama-2-13B and 8-GPU training for Llama-30B when Batch Size = 1. In all multi-GPU training setting, COAT can double the micro-batch size and therefore lead to even higher speedup. For example, our method can achieve 2.25 speedup when training Llama-2-13B on 4-GPUs since we can effectively increase the batch size to 2. Overall, COAT significantly reduces end-to-end memory usage by up to 1.55 and speeds up the end-to-end training by nearly 1.44. This facilitates full-parameter training on fewer GPUs, which is particularly beneficial for larger language models. 6.3 ABLATION STUDIES 6.3.1 DYNAMIC RANGE EXPANSIONS COMPATIBILITY WITH OTHER DATA FORMATS Dynamic Exponent Quantization (DE) was proposed in (Dettmers et al., 2021) to quantize the optimizer states since its representation range has range of 7 orders of magnitude and is very suitable for optimizer states quantization. Therefore, they proposed to quantize both first-order and second-order momentum with 8-bit DE. We report the quantization error of in Table 8, and find that applying our dynamic range expansion method to 8-bit DE can further reduce the quantization error by 1.41. Specifically, the lowest quantization error is achieved when first-order momentum is quantized with 8-bit DE + Dynamic Range Expansion, and second-order momentum is quantized with E4M3/E5M2 + Dynamic Range Expansion. This proves the effectiveness of our method. 9 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Table 7: End-to-end memory reduction and speedup results. BS refers to batch size. CL refers to context length. We report token/s per GPU for speed results. means CL=1024. Llama-2-7B Context Length = 2048 Maximum Batch Size, Context Length = 2048 Optimizer Activations Peak Ratio Max BS Speed Ratio 1 GPUBS=1 2 GPUBS=2 4 GPUBS=2 8 GPUBS= BF16 TE COAT BF16 TE COAT BF16 TE COAT BF16 TE COAT - - 13.1 GB - - 6.5GB 13.1 GB 13.1 GB 3.2 GB 6.5 GB 6.5 GB 1.6 GB - - 8.1 GB - - 16.9 GB 25.8 GB 21.9 GB 16.9 GB 25.8 GB 21.9 GB 16.9 GB OOM OOM 79.3 GB OOM OOM 52.8 GB - - - - 1.00 55.1 GB 1.08 51.1 GB 35.6 GB 1.54 1.00 41.2 GB 1.11 37.2 GB 27.0 GB 1.52 - - 1 1 1 4 2 2 4 4 4 8 OOM OOM 5906 token/s 6130 token/s 6842 token/s 11351 token/s 7730 token/s 9577 token/s 11257 token/s 8238 token/s 11704 token/s 11241 token/s - - 1.00 1.11 1.85 1.00 1.24 1.45 1.00 1.42 1.36 Llama-2-13B Context Length = 2048 Maximum Batch Size, Context Length = 2048 Optimizer Activations Peak Ratio Max BS Speed Ratio 2 GPU BS=1 4 GPUBS=1 8 GPUBS= BF16 TE COAT BF16 TE COAT BF16 TE COAT - - 12.6 GB 25.1 GB 25.1 GB 6.3 GB 12.6 GB 12.6 GB 3.1 GB - - 10.1 GB 20.1 GB 17.2 GB 13.2 GB 20.1 GB 17.2 GB 13.2 GB OOM OOM 73.2 GB - - 1.00 76.1 GB 1.04 73.0 GB 49.1 GB 1.55 1.00 49.4 GB 1.06 46.5 GB 32.5 GB 1.52 - - 1 1 1 2 2 2 4 OOM OOM 2137 token/s 2345 token/s 2851 token/s 5295 token/s 3907 token/s 5604 token/s 5650 token/s - - 1.00 1.21 2.25 1.00 1.43 1.44 Llama-30B Context Length = Maximum Batch Size, Context Length = 2048 Optimizer Activations Peak Ratio Max BS Speed Ratio 8 GPUBS=1 BF16 TE COAT - - 7.8 GB - - 24.2 GB OOM OOM 70.5 GB - - - - 1 OOM OOM 1363 token/s - - Table 8: Dynamic Range Expansion is compatible with DE8 (8-bit dynamic quantization). First Order E4M E4M3 + Expand E5M2 E5M2 + Expand DE8 DE8 + Expand E4M3 + Expand DE8 DE8 + Expand 15.13 12.11 11.57 12.31 8.27 7.47 21.96 20.02 19.69 12.43 8.43 7.65 14.01 10.54 9.91 18.84 16.25 15. Second Order"
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we present COAT, memory-efficient FP8 training framework for foundation models by quantizing both optimizer states and activations into FP8 format. We observe that the FP8 formats representation range is not fully utilized when quantizing optimizer states, prompting us to propose Dynamic Range Expansion to align their dynamic range. We then identify the importance of quantizing non-linear layers, and propose mixed-granularity FP8 precision flow to quantize the activations accurately without introducing too much overhead. Extensive experiments on LLM and VLM training and fine-tuning demonstrate that COAT can achieve nearly lossless performance. In end-to-end training, COAT achieves 1.54 memory reduction and 1.43 speedup compared to BF16, and is comparable or even faster to TransformerEngines training speed. Our method also enables full-parameter training of billion-scale models on fewer GPUs and is able to double the batch size in realistic settings. These results highlight COATs capability to enable memory-efficient, large-scale model training without sacrificing accuracy, providing highly effective solution for memory-constrained environments. Future work could further explore combining our proposed approach with other low-precision gradient compression methods to reduce communication overhead. 10 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training"
        },
        {
            "title": "REFERENCES",
            "content": "Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization. Advances in Neural Information Processing Systems, 32, 2019. Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce activations, not trainable parameters for efficient on-device learning. arXiv preprint arXiv:2007.11622, 2020. Jianfei Chen, Yu Gai, Zhewei Yao, Michael Mahoney, and Joseph Gonzalez. statistical framework for low-bitwidth training of deep neural networks. Advances in neural information processing systems, 33:883894, 2020. Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael Mahoney, and Joseph Gonzalez. Actnn: Reducing training memory footprint via 2-bit activation compressed training. In International Conference on Machine Learning, pp. 18031813. PMLR, 2021. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, and Brucek Khailany. Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference. Proceedings of Machine Learning and Systems, 3:873884, 2021. Alex Davies, Petar Veliˇckovic, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomaˇsev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andras Juhasz, et al. Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):7074, 2021. Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trilliontoken llms. arXiv preprint arXiv:2409.12517, 2024. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. 11 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394398, Montreal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Bingrui Li, Jianfei Chen, and Jun Zhu. Memory efficient optimizers with 4-bit states. Advances in Neural Information Processing Systems, 36, 2024. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023b. Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, and Song Han. On-device training under 256kb memory. Advances in Neural Information Processing Systems, 35:22941 22954, 2022. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2668926699, 2024. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. 12 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. Numglue: suite of fundamental yet challenging mathematical reasoning tasks. arXiv preprint arXiv:2204.05660, 2022. Georgii Sergeevich Novikov, Daniel Bershatsky, Julia Gusak, Alex Shonenkov, Denis Valerievich Dimitrov, and Ivan Oseledets. Few-bit backward: Quantized gradients of activation functions for memory footprint reduction. In International Conference on Machine Learning, pp. 26363 26381. PMLR, 2023. NVIDIA. Nvidia h100 tensor core gpu, 2024a. URL https://www.nvidia.com/en-us/ data-center/h100/. Accessed: 2024-09-19. NVIDIA. Transformerengine: An efficient library for training transformer models, 2024b. URL https://github.com/NVIDIA/TransformerEngine. Accessed: 2024-09-19. Open Compute Project. December ocp-8-bit-floating-point-specification-ofp8-revision-1-0-2023-12-01-pdf-1. Accessed: 2024-10-09. Ocp 8-bit floating point revision 1.0, https://www.opencompute.org/documents/ specification (ofp8), 2023. URL Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 45964604. PMLR, 2018. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. 13 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31, 2018. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:1027110298, 2023. Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:4914649168, 2023. Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang. Vision-flan: Scaling human-labeled tasks in visual instruction tuning. arXiv preprint arXiv:2402.11690, 2024. Jaewoo Yang, Hayun Kim, and Younghoon Kim. Mitigating quantization errors due to activation spikes in glu-based llms. arXiv preprint arXiv:2405.14428, 2024. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Jintao Zhang, Pengle Zhang, Jun Zhu, Jianfei Chen, et al. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv preprint arXiv:2410.02367, 2024. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024. Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19691979, 2020. 14 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training"
        },
        {
            "title": "A DETAILS ABOUT OPTIMIZER STATES QUANTIZATION",
            "content": "When performing optimizer.step(), We first dequantize the optimizer states into FP32, then update the optimizer states and weights in FP32 precision. In the end, we quantize the updated optimizer states back to FP8 and store it in GPU memory. This can be formulated as: (Dequantize to FP32) (Dequantize to FP32) t1, Smt1 ) t1, Svt1 ) mt1 = DQ(mq vt1 = DQ(vq mt = β1 mt1 + (1 β1) gt vt = β2 vt1 + (1 β2) g2 ˆmt = ˆvt = mt 1 βt 1 vt 1 βt 2 wt+1 = wt η mq , Smt = Q(mt) vq , Svt = Q(vt) (cid:18) ˆmt ˆvt + ϵ (cid:19) + λwt (Quantize to FP8) (Quantize to FP8) QUALITATIVE EXAMPLE - VISION LANGUAGE MODEL CAPTIONING Figure 8: Comparison of BF16 and COAT on VLM captioning. COAT can accurately summarize the figure and identify the key points in the figure. 15 COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training"
        },
        {
            "title": "C VISUALIZATION OF EXPAND FUCTION",
            "content": "We further visualize Figure 3 by flattening it. This helps to understand the effectiveness of our method since floating point numbers can be more easily understood in binary manner. Figure 9: Axis is of base 2. DETAILED EXPLANATION FOR TABLE 2 We mainly explain why the reduction ratio of linear layers is not exactly 50% here. 5.66U comes from: QKV Projection - 1U, Attention Projection - 1U, Up&Gate Projection - 1U, Down Projection - 2.66U. Among them, the attention projections input is exactly the output of FlashAttention. However, FlashAttention will save its input (QKV) and its output itself, so FlashAttention and Attention Projection actually share the same tensor when saving it. In Python, this tensor will only be saved for 1 time. So we should not further store an FP8 version of the Attention Projections input, since this will even increase the memory usage by 0.5U if we do not change the code of FlashAttention. Although we need to quantize the input to FP8 again in backward pass, we can store the scaling factor to reduce this additional overhead. Therefore, after FP8 quantization, the memory usage of linear layers comes from: QKV Projection - 0.5U, Attention Projection - 1U, Up&Gate Projection - 0.5U, Down Projection - 1.33U. They sum up to 3.33U. Our FP8 precision flow method is also compatible with other quantized attention method Shah et al. (2024); Zhang et al. (2024) by directly quantize the output of the RoPE function. DETAILED EXPLANATION FOR FIGURE 1(C) We mainly discuss FP8-LM in this section. FP8-LM reduces the gradient communication precision from FP32 to FP8 and therefore greatly reduces the communication overhead. It also reduces the master weights precision from FP32 to FP16/BF16, and reduces the optimizer states precision from FP32 to BF16/FP8. For optimizer states and master weights, the memory footprint reduction result is consistent with our bar. However, for gradient, although FP8-LM can quantize the gradient into FP8, it still needs to preserve the main gradient in FP32 for gradient accumulation. Therefore it can not reduce the memory footprint of the weight gradient."
        }
    ],
    "affiliations": [
        "MIT",
        "NVIDIA",
        "Tsinghua University",
        "University of California, Berkeley"
    ]
}