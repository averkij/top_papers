{
    "paper_title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
    "authors": [
        "Xuezhe Ma",
        "Shicheng Wen",
        "Linghao Jin",
        "Bilge Acun",
        "Ruihang Lai",
        "Bohan Hou",
        "Will Lin",
        "Hao Zhang",
        "Songlin Yang",
        "Ryan Lee",
        "Mengxi Wu",
        "Jonathan May",
        "Luke Zettlemoyer",
        "Carole-Jean Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to $4\\times$ longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 1 3 6 4 6 0 . 1 0 6 2 : r Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths Xuezhe Ma Shicheng Wen Linghao Jin University of Southern California Bilge Acun Meta AI Research Ruihang Lai Bohan Hou Carnegie Mellon University Will Lin Hao Zhang University of California San Diego Songlin Yang MIT CSAIL Ryan Lee Mengxi Wu Jonathan May University of Southern California Luke Zettlemoyer Carole-Jean Wu Meta AI Research xuezhema@usc.edu wenshich@usc.edu linghaoj@usc.edu acun@meta.com ruihangl@cs.cmu.edu bohanhou@cs.cmu.edu wlsaidhi@ucsd.com haozhang@ucsd.edu yangsl66@mit.edu ryantlee@usc.edu mengxiwu@usc.edu jonmay@usc.edu lsz@meta.com carolejeanwu@meta.com Abstract Designing unified neural network to efficiently and inherently process sequential data with arbitrary lengths is central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to 4 longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm . Equal Contribution. Correspondence to xuezhema@usc.edu and acun@meta.com Figure 1: Negative log-likelihood for Gecko-7B, Megalodon-7B, Llama2-7B and Llama2-13B, w.r.t processed tokens during training. 1. Introduction The capability of efficient long-context modeling is crucial for the neural architecture design of next-generation large language models (LLMs). In many real-world applications, such as multi-turn conversation, mathematical reasoning, and video generation, large language models (LLMs) must efficiently process long sequential data, understand internal long-range dynamics, and generate coherent output. The Transformer architecture (Vaswani et al., 2017), despite its remarkable capabilities, faces challenges with quadratic computational complexity and weak length extrapolation, making it inefficient for long sequence modeling (Wang et al., 2024; Zhou et al., 2024; Lu et al., 2025). Techniques like sparse attention mechanisms (Tay et al., 2020; Ma et al., 2021), structured state space models (Gu et al., 2022; Poli et al., 2023; Gu and Dao, 2023) and linear Transformers (Katharopoulos et al., 2020; Dao and Gu, 2024; Yang et al., 2025) have been introduced to overcome these limitations, with the aim of improving model efficiency and performance. However, the practical application of these methods still falls short of Transformers, particularly for in-context retrieval-oriented tasks (Arora et al., 2024; Wen et al., 2025; Yang et al., 2025). This work aims at introducing model architecture that is capable of efficiently and inherently processing sequences with unlimited context length, and outperforms the canonical Transformer architecture on real-world language modeling. Moving average gated attention (Mega and Megalodon) (Ma et al., 2023, 2024), which harnesses the gated attention mechanism (Hua et al., 2022; Qiu et al., 2025) with the classical exponential moving average (EMA) approach (Hunter, 1986) (2), have gained significant interest due to their impressive Table 1: Performance on standard academic benchmarks of Gecko on two model scales (1.3B & 7B), compared to open-source base models. We reported model size, context length (CTX) and total data tokens during model pretraining. indicates that the number was not reported in the original paper. Model OLMo1 Gecko Mamba RWKV MPT Mistral Gemma Llama2 Llama2 Megalodon Gecko Size Data CTX MMLU BoolQ HellaSw PIQA SIQA WinoG Arc-e Arc-c NQ TQA 1B 1B 3B 7B 7B 7B 8B 13B 7B 7B 7B 3T 2T 0.6T 1.1T 1T 6T 2T 2T 2T 2T 2K 32K 2K 4K 4K 16K 8K 4K 4K 32K 32K 27.4 26.2 26.8 60.1 64.3 54.8 45.3 49.8 49.4 67.5 67. 71.0 75.0 83.2 83.2 81.7 77.4 80.5 81.2 66.9 65.0 71.0 70.8 76.4 81.3 81.2 80. 77.2 77.5 78.1 74.9 75.5 78.1 77.3 80.6 82.2 81.2 80.5 78.8 80.1 80.3 45. 48.5 47.0 51.8 50.3 48.3 49.6 50.2 61.4 59.6 65.9 68.4 68.3 74.2 72.3 72. 69.2 71.4 73.1 55.3 66.4 68.2 74.9 70.2 80.0 81.5 77.3 75.2 79.8 79.3 36.5 39. 41.7 46.1 42.6 54.9 53.2 49.4 45.9 53.1 53.5 16.2 21.9 20.8 23.2 23.0 31.2 25.7 25.7 28.5 50.4 62.5 63.4 65.1 58.5 60.5 61.4 model capacity, distributed scalability, and numerical stability. However, the existing problems in timestep normalization (2.2) and chunk-wise gated attention (2.3) have restricted the utilization of long-context information in Mega. In this paper, we propose Gecko, novel model architecture built on the Megalodon backbone, incorporating several new techniques to further enhance the capability and efficiency of large-scale long-context pretraining and inference. First, Gecko introduces timestep decay normalization layer that controls the influence of the current mean and variance on the cumulative statistics, keeping it at fixed ratio (3.1). Then, Gecko extends chunk-wise attention to sliding chunk attention (SCA), which caches sliding segments on chunk-by-chunk basis (3.2). To capture long-term information that lies outside the sliding chunks, Gecko incorporates an adaptive working memory component, implemented using linear attention mechanism with position-aware online softmax activation (3.3). Empirically, we demonstrate the potential of Gecko as general architecture for modeling long sequences, by evaluating its performance across multiple scales of language modeling, as well as downstream domain-specific tasks. Through direct comparison by controlling for data and compute, Gecko-7B significantly outperforms the state-of-the-art variant of Transformer and Mega used to train Llama2-7B (Touvron et al., 2023) and Megalodon-7B (Ma et al., 2024) on both training perplexity (Figure 1) and across downstream benchmarks  (Table 1)  . Evaluation on long-context modeling, including perplexity in various context lengths up to 4 million and long-context QA tasks in Scrolls (Parisotto et al., 2020) prove Geckos ability to model sequences of unlimited length. To further assess Geckos retrieval capability, we evaluate it on two standard needle-in-a-haystack (NIAH) benchmarks: passkey retrieval and vanilla NIAH (Kamradt, 2023; Hsieh et al., 2024). Across both tasks, Gecko retrieves information from sequences that are roughly four times longer than its attention context. 2. Background: Backbone Architectures in Mega In this section, we set the notation and discuss existing problems of Mega and Megalodon (Ma et al., 2023, 2024). We use = {x1, x2, . . . , xn} Rnd and = {y1, y2, . . . , yn} Rnd to denote the input and output sequences with length n, and assume that the representations of the input and output sequences have the same dimension d. 3 2.1 CEMA: Complex Multi-dimensional Damped EMA Mega embeds an EMA component into the calculation of the attention matrix to incorporate inductive biases across the timestep dimension, while Megalodon further extends to work over the complex number system to improve capacity. Concretely, CEMA first expands each dimension of the input sequence individually into dimensions via an expansion matrix β Rdh, then applies damped EMA to the h-dimensional hidden space. Formally, for each dimension {1, 2, . . . , d}: u(j) = βjxt,j = αj u(j) h(j) + (1 αj δj)(cos θj + sin θj) h(j) h(j) yt,j = Re(ηT ) t1 (1) where is the element-wise product and u(j) Rh is the expanded h-dimensional vector for the j-th dimension at timestep t. α (0, 1)dh, δ (0, 1)dh are the decaying and damping factors, respectively. h(j) Ch is the complex hidden state for the j-th dimension at timestep t. η Cdh is the projection matrix to map the h-dimensional hidden state back to 1-dimensional output yt,j R. θj Rh, {1, 2, . . . , d} are the arguments uniformly spaced over the period 2π: θj,k = 2πk ωj, {1, 2, . . . , h} (2) where the learnable parameter ω Rd depicts the base angles.1 2.2 Timestep Normalization Megalodon proposed Timestep Normalization, which extends Group Normalization (Wu and He, 2018) to the auto-regressive case by computing the cumulative mean and variance. Similar to Group Normalization, each input vector xt is split into groups along the feature dimension with dg = d/k elements per group. We use µt and σ2 to denote the mean and variance of the first group of the input vector at timestep {1, 2, . . . , n}. Then, the cumulative mean (mt) and variance (vt) are the average of the means and variances from the previous timesteps: µt = 1 dg dg (cid:88) j=1 xi,j, σ2 = 1 dg dg (cid:88) j=1 (xi,j µt)2 mt = 1 t (cid:88) i=1 µt, vt = 1 (cid:88) i=1 σ2 (3) (4) Existing Problem. However, clear drawback of Timestep Normalization is that the influence of the current mean and variance (µt and σ2 ) on the cumulative statistics (mt and vt) decreases monotonically as the timestep increases. It limits the ability to inherently scale Megalodon to long sequences. 1. See Megalodon (Ma et al., 2024) for more details. 4 2.3 Chunk-wise Normalized Gated Attention To improve efficiency and stability, Megalodon simply splits the sequences of queries, keys and values in (7-9) into chunks of length c, and embeds the CEMA component into the calculation of normalized gated attention. Formally, = CEMA(X) = Wz + bz, = Q = κq + νq = κk + νk = ϕsilu(XWv + bv) Rnd Rnz Rnz Rnz Rnv (5) (6) (7) (8) (9) where κq, νq, κk, νk Rz are the learnable scalars and offsets of queries and keys. The attention is individually applied to each chunk, yielding linear complexity O(kc2) = O(nc): Os = fsoftmax (cid:0)QsKs (cid:1) Rcv (10) where {1, 2, . . . , n/c} is the index of chunks with chunk size c. Qs the queries, keys and values in the s-th chunk. and Ks and are Existing Problems. Technically, the CEMA sub-layer in Megalodon helps capture local contextual information near each token, mitigating the problem of losing contextual information beyond chunk boundaries in the chunk-wise attention. Despite its impressive successes, Megalodon still suffers from at least two limitations: (i) losses near the context boundaries of chunk-wise attention (both preceding and following chunk) increase substantially, due to the limited expressiveness of the CEMA sub-layer (Figure 4b); and (ii) Megalodon is unable to effectively capture historical information outside the chunk-wise attention context, resulting in suboptimal performance on in-context retrievaloriented tasks. 3. Gecko 3.1 Timestep Decay Normalization In order to control the influence of the current mean and variance of the cumulative statistics in Timestep Normalization (4), Gecko incorporates decaying mechanism into the calculation of cumulative statistics. Formally, we introduce two hyper-parameters, β1, β2 [0, 1), to fix the ratio of current mean and variance, respectively: mt = β1mt1 + (1 β1)µt, where µt and σ2 are the current mean and variance defined in (3). Directly inspired from the first and second order momentums in Adam (Kingma and Ba, 2015), we apply the initialization bias correction terms to both mt and vt: vt = β2vt1 + (1 β2)σ2 (11) = µt 1 βt 1 , = vt 1 βt (12) In practice, good default settings are β1 = 0.999, β2 = 0.9999. Similar to Timestep Normalization in Megalodon, we provide hardware-friendly implementation of Timestep Decay Normalization on modern hardware (GPU). 5 Figure 2: Comparison of various sparse attention patterns. (a) Chunk-wise Attention: attention is restricted to separated chunks; (b) Sliding Window Attention: attention is restricted to fixed-size windows; (c) Sliding Chunk Attention: attention is assigned to both current and previous chunks. 3.2 Sliding Chunk Attention To mitigate the issues of truncated context at chunk boundaries inherent in chunk-wise attention, straightforward approach is to replace it with sliding window attention (SWA) (Beltagy et al., 2020), which restricts each tokens attention context to fixed-size local window. Although SWA has lower theoretical complexity, its practical implementation often relies on many small matrix multiplicationseffectively one per tokendue to the per-token shifted attention window. These operations are inefficient on modern hardware devices such as GPUs, which are designed and optimized for large, contiguous matrix computations rather than numerous small ones. Motivated by the recurrent segment mechanism in Transformer-XL (Dai et al., 2019) and the block sliding window in LongFormer (Beltagy et al., 2020), we extend sliding window attention by incorporating chunked contexts, yielding the sliding chunk attention (SCA) mechanism. Similar to chunk-wise attention, SCA partitions the input sequence = {x1, x2, . . . , xn} Rnd into non-overlapping chunks of fixed length c. Specifically, the s-th chunk is defined as = {x(s1)c+1, x(s1)c+2, . . . , xsc} Rcd, where {1, . . . , n/c}, with query, key, and value chunks constructed in the same manner. For ease of exposition, we denote = {xs,1, xs,2, . . . , xs,c}, where xs,i = x(s1)c+i, {1, . . . , c}. We use analogous notation for Qs , Ks and s. For each chunk, attention is computed jointly over the current chunk and the preceding chunk, enabling efficient local context propagation across chunk boundaries: Os = fsoftmax (cid:0)Qs[Ks1, Ks]T (cid:1) [V s1, s] Rcv (13) where [Ks1, Ks] R2cd denotes the concatenation of keys vectors from the previous and current chunks; similar definition for [V s1, s] R2cv. Despite having theoretical complexity comparable to SWA, SCA performs matrix multiplications at the chunk level rather than per token, resulting in more efficient implementation on modern GPU hardware. Figure 2 illustrates the three different sparse attention patterns. 6 The keys and values from the previous chunk (Ks1, s1) effectively serve as lossless short-term memory, retaining the most recent contextual information. However, both SCA and SWA remain limited in capturing long-range dependencies due to their constrained receptive fields, even with deep layer stacking (Xiao et al., 2024). Consequently, there remains an urgent need for an effective and efficient long-term memory mechanism to process context information beyond sliding chunks. Importantly, sliding chunk attention is well Efficient Context Parallelism for SCA. suited for efficient context parallelism in distributed pretraining. As with CEMA and Timestep Decay Normalization, SCA requires communication only of the previous keyvalue chunks among devices within each context-parallel group. Through asynchronous communication, these transfers can be overlapped with the computation of other submodules in the same block, effectively hiding communication latency and reducing parallelization overhead. 3.3 Adaptive Working Memory Fixed-size compressive memory mechanism promises stable and efficient computations tailored to specific contexts for extremely long sequences (Kanerva, 1988; Munkhdalai et al., 2019). Recent work, such as Flash (Hua et al., 2022) and Infini-attention (Munkhdalai et al., 2024), proposed to implement compressive memory as learnable module with linear attention mechanism (Katharopoulos et al., 2020; Schlag et al., 2020). Compressive Memory with Linear Attention. With Infini-attention as an example, with linear attention as compressive memory, information that moves beyond attention window is recurrently compressed into fixed-size state. Formally, Ms Rdv denote the memory state for the s-th chunk. Once moving to the next chunk of information, compressive memory is updated by applying linear attention with delta rule (Widrow and Hoff, 1988; Schlag et al., 2021): Ms = Ms1 + ϕ(Ks)T (cid:18) (cid:88) (cid:88) τ = ϕ (kt,i) = τ s1 + t=1 i=1 ψ(Qs)Ms1 ψ(Qs)τ s1 Os = (cid:19) ψ(Ks)Ms1 ψ(Ks)τ s1 (cid:88) ϕ (ks,i) i=1 Rdv Rd1 Rdv (14) (15) (16) where τ Rd is the normalization term, ϕ() and ψ() are the nonlinear feature kernels for keys and queries, respectively. Commonly used featur1349415e kernels are element-wise nonlinear activation functions, such as SiLU (Ramachandran et al., 2017). The output Os is computed by retrieving information from the memory Ms1 with the query Qs in (16). The delta update rule dynamically erases less redundant information associated with current keys and values and memory content, to make space for new ones. Technically, the compressive memory Ms in linear attention maintains bounded-capacity storage. As tokens move beyond the attention context, their information is incrementally accumulated in the memory. When modeling very long sequences, however, the constrained memory capacity inevitably leads to memory collisions, degrading information fidelity. To 7 address this limitation, prior work introduces gated mechanisms that act as decay factors, enabling the model to selectively forget historical information (Dao and Gu, 2024; Yang et al., 2025). While these approaches achieve strong empirical performance, the deliberate forgetting of historical information conflicts with the design motivation of Gecko, which aims to retain long-term memory. Adaptive Working Memory (AWM) with Position-aware Online Softmax Kernel. To effectively compress long-term information into fixed-size memory while avoid forgetting historical one, Gecko introduces the position-aware online softmax kernel to linear attention. For clarity of presentation, we denote the current and accumulative denominators of the softmax function in chunk as (cid:88) i=1 (cid:88) ws = zs = exp(ks,i) (cid:88) exp(kt,i) = zs1 + ws t=1 i=1 Rd1 (17) Rd (18) Note that for each input vector kt,i Rd, exp() is the element-wise exponential function. Then, we define the local and global feature kernels of keys with online softmax: Rd1 (19) ϕ(ks,t) = fsoftmax(ks,t) = exp(ks,t) ws ϕs(ks,t) = fsoftmax(ks,t; k<s,t) = exp(ks,t) zs For both ϕ(), ϕs(), the softmax functions perform normalization over the timestep dimension of the sequence. In contrast to the element-wise feature kernels (ϕ() and ψ() in (14)), the position-aware kernel ϕs(ks,t) integrates all historical information into the denominator of the online softmax function (20). ϕ(ks,t) Rd1 ws zs (20) = The normalization term ψ()τ in (14) and (16) might result in numerical instabilities, and has been removed it from recent work (Yang et al., 2024a,b). Note that with the online softmax ϕs() (20)), we have τ = 1, = 1, 2, . . .. Gecko further proposes to use the softmax function along the feature dimension as the query feature kernel ψ(): ψ(q) = fsoftmax(q) = exp(q)/ (cid:88) j=1 exp(qj) Rd (21) where softmax normalization in ψ() is performed along the feature dimension of query vector Rd1. Together with ϕs(), Gecko essentially eliminates the normalization term by constraining it to be constant of value one: ψ(Qs)τ s1 = 1, = 1, 2, . . . , (22) Then, by incorporating ϕ(), ϕs() into the memory update rule in (14), we have Ms = zs1 zs Ms1 + ws zs ϕ(Ks)T (Vs ψ(Ks)Ms1) Rdv (23) with the initial memory M0 = 0 (detailed derivations in Appendix A). 8 (a) Adaptive Working Memory (b) Attention from timestep to + 1. Figure 3: Adaptive working memory in Gecko. (a) illustrates how the memory is updated by compressing information from all previous and current chunks. (b) depicts how Gecko stores contextual information in shortand long-term memories in different components. Integrating Memory with Sliding Chunk Attention. To integrate the short-term memory in SCA with the long-term adaptive working memory (AWM) in (23), we need to shift the chunk index to the left by one so that the adaptive working memory captures = Ms1, information moving beyond the sliding context in SCA. Concretely, denoting we have the new memory updated rule with shifted chunk index s: = zs2 zs1 with the initial memory 1 = 0. Figure 3 depicts the compressive memory update mechanism (3a) as well as the coordination between shortand long-term memory components across different Gecko modules (3b) in maintaining contextual information. s1 + ϕs1(Ks1)T (cid:0)Vs1 ψ(Ks1)M (24) s1 (cid:1) Inspired by Infiniti-Attention, we reuse intermediate represenNeural Parameterization. tations from SCA computation. In particular, we reuse the normalized shared representation (6) and the value vectors (9). For memory computation, we introduce two additional parameter sets that act as learnable scalars and offsets to derive new keys and queries: Qm = ηq + ρq Km = ηk + ρk , ρq , ηk where ηq content Om from the memory s1 Om = ψ(Qm)M , ρk Rz are the learnable parameters for Qm : by using the query Qm s1 Rnz Rnz (25) (26) and Km. Finally, we retrieve Rcv (27) The AWM output Om is directly added to the SCA output in (13) for the final output. Relation to Previous Work. The channel-wise ratio vector zs1 is reminiscent of the zs \"writing strength\" and forget gate in Gated DeltaNet (Yang et al., 2025) and Kimi Delta Attention (Zhang et al., 2025). However, AWM differs from them in two fundamental aspects. First, AWM leverages zs1 to globally compress information from current and all zs previous chunks into memory, rather than discarding historical information through forgetting. Second, Gated DeltaNet and Kimi Delta Attention are proposed as drop-in replacements for causal full attention and perform hidden-state updates at every token step. By contrast, AWM operates at the chunk level, updating memory on chunk-by-chunk basis. and ws zs 4. Experiments 4.1 LLM Pretraining Setup Gecko-1B vs. OLMo1. For controlled comparison study, we configure the Gecko-1B model to closely follow the architectural hyperparameters of OLMo-1B (Groeneveld et al. 2024, July 2024 release). Both models have 16 blocks with feature dimension of = 2048. We pretrained Gecko 1B with 2 trillion tokens on the same Dolma v1.7 dataset (Soldaini et al., 2024) that was used to train OLMo-1B. We also use OLMo-1Bs tokenizer with vocabulary size of 50,304. Training is performed using the AdamW optimizer (Loshchilov and Hutter, 2019) with β1 = 0.9, β2 = 0.95, ϵ = 1e8, and peak learning rate of 4e4. Cosine decay schedule is used, along with 2,000 warmup steps, weight decay of 0.05, gradient clipping of 1.0, and no dropout. Additional details and experiments are provided in Appendix C.1. Gecko-7B vs. Llama2. In designing the Gecko-7B model, we closely follow the architectural hyperparameters of Llama2-7B and Megalodon-7B to enable fair comparison. The model comprises 32 blocks with feature dimension of = 4096. As in Megalodon-7B, we employ SwiGLU (Shazeer, 2020) in the feed-forward layers and rotary positional embeddings (RoPE; Su et al. 2021). Llama2 is pretrained with 4096-token context window, and Megalodon-7B uses chunk size of = 4096 for its chunk-wise attention. To maintain comparable effective context length, we set the chunk size of Geckos sliding chunk attention to = 2048, yielding the same 4096-token attention context. Consistent with Ma et al. (2024), we use the same mixture of publicly available data as Llama2, ensuring that all models are trained on an identical corpus of 2 trillion tokens. We also adopt the Llama2 tokenizer with 32K vocabulary. Training is performed using AdamW with β1 = 0.9, β2 = 0.95, ϵ = 1e8, and peak learning rate of 3.5e4. We use cosine decay schedule with 2,500 warmup steps, along with weight decay of 0.1, gradient clipping of 1.0, and no dropout. Following Megalodon-7B, we pretrain with 32K context window and global batch size of 4M tokens. Training is distributed across 256 NVIDIA H100 GPUs (16K tokens per GPU), with data parallelism of 128, chunk parallelism of 2. Pretraining Results. Figure 1 presents the negative log-likelihood (NLL) curves for Gecko-7B in comparison with Llama2-7B, Megalodon-7B, and Llama2-13B over the course of training. Across all stages, Gecko-7B attains noticeably lower NLL than Llama27B and Megalodon-7B for the same number of processed tokens, demonstrating superior data efficiency. At convergence, Gecko achieves training loss of 1.68, outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and nearly matching Llama2-13B (1.67). 4.2 Short-Context Evaluation on Academic Benchmarks We evaluate Gecko-7B against Llama2 and Megalodon-7B on standard short-context academic benchmarks (< 4K tokens), following the evaluation protocol of Llama2 (Touvron et al., 2023). The benchmarks span four categories: Commonsense Reasoning (0-shot): HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e and -c (Clark et al., 2018). 10 (a) PPL in various context lengths. (b) NLL Loss by token positions. Figure 4: PPL/NLL over long sequences. (a) shows the perplexity (PPL) in various context lengths. (b) plots averaged negative log-likelihood (NLL) broken down by token positions. World Knowledge (5-shot): NaturalQuestions (NQ, Kwiatkowski et al. (2019)) and TriviaQA (TQA, Joshi et al. (2017)). Reading Comprehension (0-shot): BoolQ (Clark et al., 2019). Aggregated Evaluation (5-shot): MMLU (Hendrycks et al., 2020). Table 1 reports results for Gecko-7B, Megalodon-7B, and Llama2 models, along with other open-source baselines such as MPT (MosaicML, 2023), RWKV (Peng et al., 2023), Mamba (Gu and Dao, 2023), Mistral (Jiang et al., 2023) and Gemma (Mesnard et al., 2024). Under identical pretraining conditions (2T tokens), Gecko-7B consistently outperforms both Llama2-7B and Megalodon-7B across all benchmarks. On several tasks, its performance is comparable to or even exceeds that of Llama2-13B. We emphasize that Mistral-7B and Gemma-8B were pretrained on significantly larger datasets, so comparison with Gecko-7B is not entirely apples-to-apples. 4.3 Long-Context Evaluation Validation loss over Long Sequences To assess Geckos ability to intrinsically exploit very long contexts for next-token prediction, we evaluate validation perplexity over range of context lengths. Following Ma et al. (2024), we construct validation set of 1,500 books, each containing sequences of at least 4M tokens. Figure 4a presents the perplexity (PPL) of Gecko and Megalodon on this dataset as the context length increases from 4K to 4M tokens. Across all settings, Gecko exhibits markedly better context utilization than Megalodon. Moreover, the consistent decrease in PPL with longer contexts demonstrates Geckos inherent abilitywithout relying on any explicit context-extension mechanismsto model extremely long sequences. To further analyze the effectiveness and robustness of Gecko in long-context modeling, we examine the average negative log-likelihood as function of token position (up to 32K), shown in Figure 4b. Because chunk-wise attention discards contextual information across chunk boundaries, Megalodon suffers from pronounced increase in loss near these 11 (a) Passkey (b) NIAH Figure 5: Evaluate Gecko-7B Long context on Passkey Retrieval and Needle in Haystack. boundaries. By contrast, Gecko exhibits steadily decreasing loss with increasing context length, highlighting its improved utilization of long-range context. Long-Range Retrieval-Oriented Tasks. To assess Geckos information retrieval capability, we evaluate it on single needle-in-a-haystack (NIAH) task (Kamradt, 2023). Following the Ruler benchmark suite (Hsieh et al., 2024), Figure 5 presents results under two haystack settings: (i) passkey retrieval with repeated synthetic context, and (ii) NIAH with real-world essay context. For both settings, we report performance across varying needle depths. With only 4K attention context, Gecko achieves near-perfect performance up to 16K sequence length, under the passkey-retrieval setting. Under the more different NIAH setting, the performance on 16K sequence length is slightly worse than that of passkey retrieval, but still obtains 100% performance for 8K length across all depth levels. These results demonstrate the strong inherent retrieval capability of Gecko without relying on any context-extension methods. Model NaQA Qasper QMSum Table 2: Results on Scrolls. Llama2-L (Xiong et al., 2023) continually trains Llama2 on 500B tokens for length extension. Long-Context QA tasks in Scrolls We further evaluate Gecko on longcontext open-book question answerfrom the Scrolls benching tasks mark (Shaham et al., 2022), including NarrativeQA (Kočiský et al., 2018), Qasper (Dasigi et al., 2021) and QMSum (Zhong et al., 2021). Following the evaluation protocol of Xiong et al. (2023), we report 0-shot F1 on NarrativeQA, 2-shot F1 on Qasper, and 1-shot geometric ROUGE (the geometric mean of ROUGE-1, ROUGE-2, and ROUGEL) on QMSum, using unified prompt format across tasks.2 Table 2 compares Gecko-7B and Megalodon-7B against other open-source long-context models of similar scale, including Xgen-7B-8K (Nijkamp et al., 2023), MPT-7B-8K (MosaicML, 2023), YaRN-7B-128k (Peng et al., 2024), Llama2-7BLlama2 Llama2-L Megalodon Xgen MPT Yarn 6.8 8.8 11.4 19.8 28.3 28.0 20.5 24.7 26.2 17.4 18.8 20. 18.8 23.5 23.9 10.1 14.5 13.1 Gecko 15.8 34.2 27. 2. Prompt format: {CONTEXT} Q: {QUESTION} A: 12 4K (Touvron et al., 2023), and Llama2-7B-32K (Llama2-L, Xiong et al. (2023)). Across all three benchmarks, Gecko-7B consistently achieves the strongest performance. Notably, Llama2-7B-32K extends the base Llama2-7B models context window from 4K to 32K through continued pretraining on an additional 500B long-context tokens. 5. Conclusion We present Gecko, novel architecture built upon the Megalodon backbone that introduces several key innovations to enhance the efficiency and effectiveness of large-scale long-context pretraining and inference. Through the integration of timestep decay normalization, sliding chunk attention, and adaptive working memory, Gecko inherently supports efficient processing of sequences with effectively unbounded context length, surpassing the canonical Transformer in real-world language modeling. Compared directly with Llama2 and Megalodon, Gecko delivers consistent improvements in training perplexity and downstream benchmark performance. Importantly, without any explicit context-extension mechanisms, Gecko achieves robust long-context processing and retrieval, stably handling sequences of up to 4 million tokens and retrieving information from contexts 4 longer than its nominal attention context."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Chunting Zhou and Xiaomeng Yang for their helpful feedback and discussion during this work."
        },
        {
            "title": "References",
            "content": "Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff. In Proceedings of the 41st International Conference on Machine Learning (ICML-2024), pages 17631840, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 74327439, 2020. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In 13 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29782988, 2019. Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021), pages 45994610, Online, June 2021. Association for Computational Linguistics. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. Olmo: Accelerating the science of language models. Preprint, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR2022), 2022. Yuling Gu, Luca Soldaini, Shengyi Jha, Dirk Groeneveld, Dustin Schwenk, and Jesse Dodge. Olmes: standard for language model evaluations. arXiv preprint arXiv:2406.08446, 2024. URL https://arxiv.org/abs/2406.08446. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning (ICML-2022), pages 90999117. PMLR, 2022. Stuart Hunter. The exponentially weighted moving average. Journal of quality technology, 18(4):203210, 1986. 14 Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2017. Gregory Kamradt. Needle in haystack - pressure testing llms., 2023. URL https://github. com/gkamradt/LLMTest_NeedleInAHaystack. Pentti Kanerva. Sparse distributed memory. 1988. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. Yi Lu, Jing Nathan Yan, Songlin Yang, Justin Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander Rush. controlled study on long context extension and generalization in LLMs. In Second Conference on Language Modeling (COLM-2025), 2025. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:24412453, 2021. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International Conference on Learning Representations (ICLR-2023), 2023. Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. Advances in Neural Information Processing Systems, 37:7183171854, 2024. 15 Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology, 2024. MosaicML. Introducing mpt-7b: new standard for open-source, commercially usable llms, 2023. Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned neural memory. Advances in Neural Information Processing Systems, 32, 2019. Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143, 101, 2024. Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryściński, Lidiya Murakhovska, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Joty, and Caiming Xiong. Xgen-7b technical report, 2023. Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International conference on machine learning, pages 74877498. PMLR, 2020. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. 16 Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In International Conference on Learning Representations (ICLR-2024), 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International conference on machine learning (ICML-2023). PMLR, 2023. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. In Advances in Neural Information Processing Systems, 2025. Prajit Ramachandran, Barret Zoph, and Quoc Le. Swish: self-gated activation function. arXiv preprint arXiv:1710.05941, 7(1):5, 2017. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9): 99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Imanol Schlag, Tsendsuren Munkhdalai, and Jürgen Schmidhuber. Learning associative inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In 38th International Conference on Machine Learning, ICML 2021, pages 93559366, 2021. Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP-2022), pages 1200712021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. 17 Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. arXiv preprint arXiv:2009.06732, 2020. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi. Beyond the limits: survey of techniques to extend the context length in large language models, 2024. Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. Rnns are not transformers (yet): The key bottleneck on in-context retrieval. In The Thirteenth International Conference on Learning Representations (ICLR-2025), 2025. Bernard Widrow and Marcian Hoff. Adaptive switching circuits. In Neurocomputing: foundations of research, pages 123134. 1988. Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV-2018), pages 319, 2018. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations (ICLR-2024), 2024. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In Proceedings of the 41st International Conference on Machine Learning (ICML-2024), pages 5650156523, 2024a. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024b. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. In The Thirteenth International Conference on Learning Representations (ICLR-2025), 2025. 18 Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL-2019). Association for Computational Linguistics, 2019. Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, Y. T. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, and Yulun Du. Kimi linear: An expressive, efficient attention architecture, 2025. Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-2021), pages 59055921, Online, June 2021. Association for Computational Linguistics. Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly, 2024. 19 Appendix: Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths Appendix A. Adaptive Working Memory Ms = = = = = s1 (cid:88) c=1 s1 (cid:88) c=1 s1 (cid:88) c=1 zs1 zs zs1 zs ϕs(Kc)T Vc + ϕs(Ks)T Vs (cid:18) ϕs(Kc) ϕs1(Kc) (cid:19)T ϕs1(Kc) Rdv (28) Vc + ϕs(Ks)T Vs Rdv (29) (cid:18) zs1 zs (cid:19)T ϕs1(Kc) Vc + ϕs(Ks)T Vs Ms1 + ϕs(Ks)T Vs ws zs Ms1 + ϕ(Ks)T Vs Rdv Rdv Rdv (30) (31) (32) Applying delta rule, we have: Ms = zs1 zs Ms1 + ϕs(Ks)T (Vs ψ(Ks)Ms1) Rdv (33) where M0 = 0. The output of chunk is: Os = ψ(Qs)Ms1 Rcv (34) Incorporating with Sliding Chunk. we have: = Ms1, via incorporating with sliding chunks, = zs2 zs1 s1 + ϕs1(Ks1)T (cid:0)Vs1 ψ(Ks1)M where 1 = 0. The output of chunk is: Os = ψ(Qs)M s1 (cid:1) Rdv (35) Rcv (36) Appendix B. Implementation Details B.1 Efficient Fused CUDA Operators Implementation The CEMA recurrence poses challenge for efficient training on GPUs due to its sequential dependency. Standard PyTorch implementations suffer from high memory I/O and low core utilization. To address this, we implement highly optimized CUDA Kernel that reformulates the recurrence as parallel associative scan, utilizing custom warp-level primitives and hardware-aware memory layouts. 20 B.1.1 Parallel Associative Scan Formulation To enable parallelization of Equation 1 via the prefix scan algorithm, we map this linear recurrence to complex affine algebra. For given feature dimension j, we define the multiplicative term qt and the additive term pt as: qt = (1 αj δj)(cos θj + sin θj), pt = αj u(j) (37) The recurrence allows us to define binary associative operator acting on tuples St = (qt, pt). The operation (qb, pb) (qa, pa) combines current state with previous state a: (qb, pb) (qa, pa) = (qb qa, qb pa + pb) (38) This formulation reduces the sequence modeling complexity from linear span O(L) to logarithmic span O(log L) on parallel hardware. Zero-Cost Boundary Handling. To handle document boundaries inherent in variablelength processing, we avoid control flow divergence by embedding boundary logic directly into the algebra. Using pre-computed mask Mt {0, 1} (where 0 indicates document = qt Mt. When Mt = 0, the state reset), we compute the effective multiplicative term as qeff naturally resets (ht = pt), allowing uniform instruction execution across irregular boundaries. B.1.2 Kernel Architecture and Thread Mapping Our kernel grid minimizes kernel launch overhead and maximizes instruction-level parallelism by exploiting the specific dimensions of the hidden state CBDN L. Feature-Major Grid: We map the feature dimension to the CUDA Grid, where each thread block processes tile of features. Warp-Level Parallelism: We adopt \"one-warp-per-feature\" strategy. Each warp (32 threads) is assigned unique feature index and processes the sequence L. Crucially, and dimensions are handled sequentially within the warp via loops. This Parameter Reuse strategy enables time-invariant parameters (p, q, γ) to be loaded once into Shared Memory/L1 Cache and reused times, significantly increasing arithmetic intensity. Chunked Warp Scan: We partition sequence into chunks of size 32. Inside each chunk, threads utilize warp shuffle intrinsics to perform register-level cooperative scans. This avoids the latency of round-tripping to Shared Memory for intra-chunk dependencies. B.1.3 Memory Hierarchy and Vectorization To saturate HBM, we implement strictly coalesced access patterns: Vectorized float4 Storage: Since CEMA operates on complex numbers, we utilize float4 data types to load the affine tuple (q, p). Each float4 stores 128 bits representing [Re(q), Im(q), Re(p), Im(p)]. This vectorization ensures that every global memory transaction is 128-bit aligned and coalesced. 21 Shared Memory Tiling: Input chunks and parameters are prefetched into Shared Memory buffers. This hides global memory latency behind the arithmetic computations of the prefix scan. B.1.4 Memory-Efficient Backward Pass Storing the full hidden state CBDN for gradient computation is prohibitive for long-context training. We employ chunk-level rematerialization strategy: Forward Pass: We only store the chunk boundaries (the accumulated state at the end of every 32-element chunk) to global memory. Backward Pass: The kernel reconstructs intermediate ht values on-the-fly by rerunning the forward prefix scan from the nearest stored chunk boundary, immediately followed by backward adjoint scan. This approach reduces the activation memory footprint by factor of 32, enabling the scaling of GECKO to long sequences without memory overflow. Appendix C. Experimental Details Table 3: Performance on standard academic benchmarks of Gecko-1B compared to OLMo1-1B at matched training checkpoints (approximately 2T tokens). We report model size, context length (CTX) and total data tokens during model pretraining. Model Size Data CTX BoolQ HellaSw PIQA WinoG Arc-e Arc-c OLMo1 Gecko 1B 1B 2T 2T 2K 32K 63.9 67.1 49.5 65.0 73.1 75.5 62.4 59. 55.1 66.4 35.8 39.8 C.1 Experiment on Gecko-1B As reported in Table 1 and Section 4.1, we conducted controlled comparison between 1B version of Gecko and OLMo1-1B (0724). Due to resource constraints, we did not pretrain Gecko-1B up to 3T tokens as OLMo1-1B did, instead training up to 2T tokens using the Dolma v1.7 dataset. For as close comparison as possible, during training we also matched OLMo1-1Bs global batch size, i.e. number of tokens per optimizer step. Even with less training, we see comparable results against the published OLMo1 1B benchmark performance and better results for PIQA, ARC-Easy, and ARC-Challenge. For an additional point of comparison, we report in Table 3 Gecko-1B at 2024B tokens vs. OLMo1-1B at 2025B tokens trained. OLMo1-1B results are reported by evaluating AI2s checkpointed model (step966000-tokens2025B) with AI2s Open Language Model Evaluation System (OLMES, Gu et al. 2024). It is important to note that neither model is fully annealed at 2T tokens (Like OLMo1-1B, Gecko-1B also fully anneals between 3T and 4T tokens), however the comparison provides additional insight on training progress."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "MIT CSAIL",
        "Meta AI Research",
        "University of California San Diego",
        "University of Southern California"
    ]
}