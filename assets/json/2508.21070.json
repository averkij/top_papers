{
    "paper_title": "Dress&Dance: Dress up and Dance as You Like It - Technical Preview",
    "authors": [
        "Jun-Kun Chen",
        "Aayush Bansal",
        "Minh Phuoc Vo",
        "Yu-Xiong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Dress&Dance, a video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a user wearing desired garments while moving in accordance with a given reference video. Our approach requires a single user image and supports a range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in a single pass. Key to our framework is CondNet, a novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and a larger, more readily available image dataset, in a multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables a high quality and flexible try-on experience."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 0 7 0 1 2 . 8 0 5 2 : r Dress&Dance: Dress up and Dance as You Like It Technical Preview Jun-Kun Chen1 Aayush Bansal2 Minh Phuoc Vo2 Yu-Xiong Wang1 1University of Illinois Urbana-Champaign 2SpreeAI {junkun3,yxw}@illinois.edu {aayush.bansal,minh.vo}@spreeai.com immortalco.github.io/DressAndDance Figure 1. Given single image of user, garment (dress) that they would like to wear, and an example video showing how they would like to animate themselves (dance) as shown in the first row. Our method, Dress&Dance, generates high-quality 5s video (1152 720, 24 FPS) of the user wearing the target garment containing desired motion while maintaining their accessories such as bag and shoes."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We present Dress&Dance, video diffusion framework that generates high quality 5-second-long 24 FPS virtual try-on videos at 1152 720 resolution of user wearing desired garments while moving in accordance with given reference video. Our approach requires single user image and supports range of tops, bottoms, and one-piece garments, as well as simultaneous tops and bottoms try-on in single pass. Key to our framework is CondNet, novel conditioning network that leverages attention to unify multi-modal inputs (text, images, and videos), thereby enhancing garment registration and motion fidelity. CondNet is trained on heterogeneous training data, combining limited video data and larger, more readily available image dataset, in multistage progressive manner. Dress&Dance outperforms existing open source and commercial solutions and enables high quality and flexible try-on experience. Imagine yourself trying new dress in fitting room. It is natural to move around to get feel for how the garment fits and flows. Current computational methods that enable virtual garment try-on typically generate static 2D images. Viewing oneself in single image does not provide the full In this paper, we and expressive experience of try-on. introduce Dress&Dance, video diffusion framework that generates 5-second, 24 FPS video at 1152 720 resolution of user wearing desired garment. The motion in the generated video is guided by an example video selected by the user. Our approach allows the user to choose the garment (Dress) and control how they would like to animate (Dance) with it, as illustrated in Fig. 1. Dress&Dance supports wide range of tops, bottoms, and one-piece garments as well as their combinations at once. Our model allows for try-ons of clothing borrowed from another human and garments and accessories modification using text prompts. 1 Figure 2. Dress&Dance can be used in multiple ways: (a) Dress&Dance can generate complicated dancing motions; (b) Dress&Dance performs virtual try-on for both top and bottom garments simultaneously; and (c) Dress&Dance can input garments worn by other users. Fig. 2 highlights few applications of Dress&Dance. The emergence of diffusion models [20] has allowed the generation of videos from text [9, 30] and images [1, 2, 32]. Generating videos from text requires highly detailed descriptions; otherwise, the model struggles to capture essential details. In our context, it is difficult to describe the content textually such that both the users appearance and the integrity of the target garment are preserved in the generated results. To sidestep the problem, one strategy is to first perform single image try-on [15, 24, 26, 28] and then animate the output with video diffusion models like CogVideoX [30]. We observe temporally incoherent outputs due to error propagation from the first frame (Fig. 3- (a)), particularly when there are significant pose changes, leading to occlusions of the garment or human body. Another challenge is to instruct the machine learning model to generate desired motion. Simple movements, such as turn-around, can be described textually. Capturing more nuanced movements is difficult with text, even when using state-of-the-art commercial methods such as Kling [1] and Ray2 [19], as shown in Fig. 3-(b). We overcome these challenges by learning an end-to-end approach that leverages exemplar videos to describe nuanced motion. Our results faithfully preserve garment and user appearance details and strictly follow the direction, as shown in Fig. 3-(c). The key to our work is conditioning network, CondNet, that leverages cross-attention for conditioning on multiple modalities (text, images, and videos). By converting heterogeneous conditions into homogeneous attention sequences, we can concatenate them with the sequences of video and text for cross-attention, enabling the model to handle them in unified manner. CondNet implicitly connects all pairs of the input images pixels to the generated videos pixels through cross-attention. This allows us to better transfer garment details, supports multiple garment types, including garment combination simultaneously, and is robust to varying garment capture methods. Whether the garment is casually captured as flat image or worn by an individual, our model can effectively handle both scenarios. Due to the high compute cost of attention modules in generating high-resolution visual output, we design stagewise, multi-phase, resolution progressive training strategy to effectively and efficiently train the model. First, we adopt curriculum garment warm-up learning to guide the coarse garment try-on location on the user body and progressive resolution training to preserve the user and garment identities. Secondly, we further boost the resolution and quality, by introducing an auto-regressive video refiner stage to upsample the video from 8 FPS to 24 FPS while further refining the appearance details. We show ablation results that demonstrate the effectiveness of this training strategy. Our contributions: (1) We propose CondNets, simple yet effective conditioning strategy that unifies multimodal inputs through cross-attention, improving both garment registration and overall try-on quality. (2) We introduce compute-effective and data-efficient training strategies that allow us to generate high-resolution outputs with limited compute and data. (3) We show experimentally that our simple method can effectively handle arbitrary garments and is robust to various garment capture methods, enabling the user to simultaneously try on set of garments or point to garment worn by another person and perform tryon. Our method achieves state-of-the-art quality in various challenging scenarios, outperforming open source baselines and commercial models such as Kling [1] and Ray2 [19]. Figure 3. Given user image, desired garment, and an indicated reference video, we extract detailed text description via GPT [18] as shown in the first row. (a) Single image try-on methods TPD [28], OOTD [26], and GP-VTON [24] struggle to generate meaningful try-on, whilst error propagation happens when motion is applied via CogVideoX [30]. (b) State-of-the-art commercial models, such as Kling [1], are able to perform try-on but still struggle in capturing the nuanced motion description, as text alone is not sufficient for motion description. This is also evident when we use Ray2 [19] for motion generation even when using Kling for try-on. Also, as the users right hand covers part of the garment, the information of the covered patterns is lost for the video generation model, leading to incorrect garment appearances when the hand moves in the video, for all baselines in (a) and (b). (c) Our Dress&Dance generates high-quality virtual try-on results that faithfully preserve both garment and user appearance with precise motion, even when the hand moves off. 2. Related Work Videos from Single Image. As representative method, Stable Video Diffusion (SVD) [2] generates short videos from single image. However, it supports only landscape videos and is limited to short video lengths. Similarly, I2VGen-XL [32] uses image and text input to generate short videos, but is restricted to landscape formats. more flexible solution, CogVideoX-I2V [30], supports portrait videos and allows image-to-video synthesis with text guidance, providing more control over generated content. Commercial models, such as Kling Video 1.6 [1] and Ray2 [19], offer similar capability for image-to-video synthesis, supporting portrait videos with image and text inputs, enabling more customizable output. These methods focus on generating video sequences from still images but do not address garment try-on specifically or the need for motion-guided generation. Note that Kling has standalone image try-on model which we use for analysis in this work. Single 2D Image Try-On. This is the most popular virtual try-on work stream and supports both one or multiple target garment(s) to produce single try-on image as output [3, 7, 13, 15, 22, 2426, 28, 29, 34, 35]. These methods typically require paired training data consisting of the garment and corresponding human model images wearing that garment. To prevent information leakage from such paired training data, they extract garment agnostics human intermediates, such as the pose or try-on region mask, from the human model images, and train the model to resynthesize the original RGB human model images in self-supervised setting. However, at inference time, the model is given the unpaired setting where it needs to try-on chosen garment on user image wearing different garment, making inference harder than training due to information leakage at training stage. Moreover, the use of the intermediate representations also propagate their pre-processing artifacts to the try-on results. Our method sidesteps both problems using synthetically generated unpaired triplets to not only eliminate the need for these intermediates, but also to enable consistent training setting as evaluation, potentially improving the try-on results in the evaluation. Video-to-Video Translation. Video-to-video translation methods have explored both supervised and unsupervised approaches to generate temporally consistent videos. VideoShop [5] is training-free approach that relies on pre-trained diffusion model to edit the first frame of video, which is then propagated across subsequent frames to maintain temporal consistency. Similarly, BIVDiff [21] performs per-frame editing, refining the video using pre-trained diffusion model to ensure consistency across frames. Another relevant approach is CogVideoX-V2V [30], employing pre-trained text-to-video diffusion model combined with SDEdit [17] for text-guided video editing. These methods focus on video editing and manipulation, but do not address the specific needs of garment try-on or the combination of motion and garment generation. Dress&Dance Figure 4. Our Dress&Dance supports transferring garment from another given image via segmentation, regardless of the pose of that image. Notably, our Dress&Dance generates high-resolution videos at 1152 720 with clear appearance and more details, while the baseline Fashion-VDM [12] exhibits color fading and generates low-resolution 512 384 videos. seamlessly integrates garment try-on within video generation, allowing users to choose desired garment and manipulate motion, effectively combining the adaptability of video-to-video translation with the added functionality of virtual try-on. Video Virtual Try-On (VVT). Early work of VVT [4, 11, 14, 33] utilizes generative adversarial networks (GANs) [8] to perform virtual try-on, and warp previous frames to new frames to form videos. After the emergence of diffusion models [2], follow-up work [6, 10, 12, 23, 27] leverages image or video diffusion models for VVT. However, these methods produce low-resolution (lower than 512512) VVT videos with fewer frames. Additionally, the diffusion-based methods work only on UNet-based diffusion models, which limits their scalability. In comparison, our Dress&Dance generates high 1152720 resolution, 121-frame videos, through powerful DiT-based video diffusion model. 3. Dress&Dance: Methodology Overview We propose Dress&Dance, novel video diffusion framework for virtual try-on, as illustrated in Fig. 5. Given user image, garment or garment set to try on, reference motion video, and an optional text prompt, Dress&Dance synthesizes realistic videos of the user performing the indicated motion while wearing the desired garments. As shown in Fig. 5-(a), all inputs are encoded into token sequences and fed into unified diffusion backbone. To support rich multi-modal conditioning, we introduce CondNets (Fig. 5- (b)), modular attention-based adapters that inject garment, user, and motion cues into the diffusion process. dedicated video refiner (Fig. 5-(c)), fine-tuned from our main Dress&Dance model, further upsamples the initial 8-FPS output to 24 FPS, while enhancing visual quality and removing artifacts. To enable supervised training despite limited paired video data, we construct synthetic triplets, which eliminates the need for intermediates like agnostic masks or Dense Poses used in existing works, bridging the gap between training and inference formats. During optimization, we employ two key strategies: (1) garment warm-up training phase based on curriculum learning, which guides the model to quickly learn garment registrationa core capability for try-on; and (2) multi-stage progressive training scheme that gradually increases resolution and condition complexity, improving training stability and convergence. Together, these designs allow Dress&Dance to deliver high-fidelity and controllable try-on results under diverse settings. 4. Experiments We study the performance of Dress&Dance using three different try-on modes  (Fig. 2)  : (1) single garment mode allows the user to try on the garment shown in single flat garment image, (2) set of garments or multiple garment mode allows simultaneous try-on of all the garments provided as flat garment images, and (3) garment transfer mode allows to transfer garment from an existing image through segmentation. Datasets. We curated two video datasets for training and (1) Internet video dataset is constructed by evaluation. crawling publicly available fashion videos with paired flat garment images, which contains around 80K garmentvideo pairs. (2) Our captured video dataset is constructed by hiring 183 human models to record the try-on videos for various sets of garments. Each model records the try-on videos for around 100 different sets of garments, so we can construct unpaired multi-garment try-on data with ground 4 Figure 5. Dress&Dance supports multi-modal conditioning through our unified CondNets architecture based on attention mechanism. In generation, the main diffusion model generates an 8-FPS video, and then the refiner model upsamples it to 24-FPS while removing artifacts. Method Property Ours Commercial Method + CogVideoX I2V TPD + CogVideoX I2V OOTDiffusion ML-VTON + CogVideoX I2V Dress&Dance Image Try-On + CogVideoX I2V Kling Image Try-On + Kling Video 1.6 Dress&Dance Image Try-On + Kling Video 1.6 Dress&Dance Image Try-On + Ray2 Dress&Dance, Direct Train Dress&Dance Evaluation with Ground Truth on Captured Dataset LPIPSAlexNet PSNR 0.2461 14.47 0.2457 14.68 0.2520 14.49 0.1635 17.26 0.1683 17.33 0.2066 15.21 0.1990 15.16 0.1338 17.14 0.0624 22.41 SSIM LPIPSVGG 0.8305 0.8282 0.8270 0.8515 0.8651 0.8297 0.8290 0.8678 0. 0.2840 0.2779 0.2961 0.2812 0.2296 0.2835 0.2938 0.2854 0.2382 Table 1. In the quantitative evaluations, Dress&Dance significantly outperforms the open-sourced baselines under most of the metrics, while achieving comparable or even better quality metrics than commercial models Kling AI [1] and Ray2 [19]. The bold numbers mark the best for each metric and the underlined numbers mark the second best. test subsets and performed training on the combination of all training subsets. For evaluation, we either sample garments and models from the evaluation subsets or use other garments, human images, and motion reference videos from the Internet. Baselines. As there are no publicly available methods that support virtual try-on video generation, we compose the baseline methods using an image try-on + image animation pipeline: we first apply virtual try-on to the user image with state-of-the-art image try-on method and then animate it according to the prompt with video generation method. We consider two different types of baselines: (1) open-source models. To perform single image try-on, we use state-of-the-art image try-on methods, TPD [28] and OOTDiffusion [26]. We also use ML-VTON, an engineered version of GP-VTON [24] that combines the original warping module with the HR-VTON [15] try-on module. For comparison, we also provide the single image try-on generated by Dress&Dance as single-frame video. We then animate these try-on images with CogVideoX 1.5 I2V [30], one of the best open-source image-to-video (I2V) generation models available with text prompt descriptions. We cannot use other methods, including Stable Video Diffusion [2] and I2VGen-XL [32], as they do not support (2) commercial models. We also portrait videos well. compare with two state-of-the-art commercial methods for video generation: Kling Video 1.6 [1] and Ray2 [19]. We once again perform image try-on with our Dress&Dance on Figure 6. Dress&Dance allows user to dress up themselves with desired garment, and perform the desired dance. It is difficult to express these moves by text which makes the generation of motion by Kling [1] and Ray2 [19] quite challenging. truth, by cross-matching the frames of different garments of the same person. We also collected an image dataset on the Internet with around 4M pairs of garment images paired for hybrid training. We divided each dataset into training and 5 Figure 7. Dress&Dance supports simultaneously trying on set of top and bottom garments, while correctly understanding and representing both garments without explicit labeling. On the contrary, Kling AI [1] misrepresents the trousers as skirt. Figure 8. Dress&Dance significantly outperforms existing video virtual try-on methods, with much more detailed and precise textures and better support of transparent garments. the user image and use these models to animate according to the text prompt. Kling AIs platform also provides an image try-on model. We also use it to perform tryon for the subsequent video generation. (3) video try-on baselines. We compare with ViViD [6], WildFit [10], Tunnel Try-On [27], GPD-VVTO [23], and ClothFormer [11] in single-garment setting, and Fashion-VDM [12] in the aligned garment transfer try-on task. Due to the lack of publicly available code, we compare our Dress&Dance with the tasks presented on their website. 4.1. Results and Analysis Qualitative Comparison. We show the qualitative results in Figs. 3, 4, and 7. Fig. 3 show the try-on results in the single garment mode. More results are on our project page. In the user image of Fig. 3, the hand of the user covers the top-right corner of the garment. Therefore, any baseline, including Kling [1] and Ray2 [19], that first apply try-on to the user image and then animate it with another model, failed to generate the correct top-right garment pattern. The information contained in the occluded garment is unable to be recovered, as the animation model does not consider the garment image. On the contrary, Dress&Dance sees the garment image during the generation of the entire video and generates correct and consistent video try-on results. On our project page, we evaluate our model with challenging dancing motions with two different dances, which are difficult to describe in text. This leads to the failure of all the baselines. Using the skeleton video extracted from the dancing video, our Dress&Dance is able to generate an accurate and smooth dancing motion with the correct try-on results. The results of the multiple garment mode are shown in Fig. 7. Dress&Dance is able to perform the virtual try-on of both top and bottom garments simultaneously, without 6 any explicit information indicating the garments type (e.g., which one is the top or the bottom). Regardless of the types and the orders of garments to try on, Dress&Dance consistently generates high-quality try-on results. Kling AI, though officially supports multi-garment try-on, incorrectly tries on the trousers as dress. In Fig. 8, we compare our model with multiple video try-on baselines in single garment mode; in Fig. 4, we compare our model with Fashion-VDM [12] in garment transfer mode. Dress&Dance generates the videos at higher resolution (1152 720), which better preserves the details and textures of the garments, and superior quality in rendering semi-transparent garments, while any other baselines results are limited to lower 512384 resolution with blurred and gloomy textures. Quantitative Comparison. We show the quantitative comparison in Tab. 1, where we conduct experiments on tasks constructed from our captured dataset, and compare the generated video with the ground truth using PSNR, SSIM, and LPIPS [31]. Dress&Dance achieves results better than most methods. It also remains competitive to commercial models, Kling Video 1.6 [1] and Ray2 [19]. For our Internet dataset, it is challenging to evaluate the try-on quality and visual quality, due to so many degrees of freedom in the generation. Inspired by VQAScore [16], we leverage GPT [18]s strong capability in vision-language reasoning to grade the generated videos from the following aspects: garment try-on fidelity and quality (GPTTry-On), user appearance fidelity (GPTUser), human and garment motion quality (GPTMotion), visual quality (GPTVisual), and finally the overall quality considering all the aspects above (GPTOverall). We provide detailed instructions and rubrics to guide the GPT to grade, while re-grading each video 40 times and taking the average to avoid randomness and ensure fairness. The results are shown in Tab. 2. Virtual Try-On Capability. As shown in the GPTTry-On row, our Dress&Dance significantly outperforms all the baseline models in garment fidelity and try-on quality. Also, using our Dress&Dance for image try-on in baselines pipeline consistently improves the try-on quality with large margins, compared to the one using other image tryon methods, including Kling AI [1]. This shows that our Dress&Dance has superior capability in virtual try-on. Visual Quality. In all other metrics, our Dress&Dance achieves highly comparable results as the commercial models. Notably, as the commercial models are trained with much more video data and have the flexibility to generate motions only constrained by the text prompt, they are intrinsically easier to achieve high scores, but unable to perfectly present the indicated motion from the reference video. Even in this case, our Dress&Dance still achieves very similar scores in all of the metrics, while the baseline, which achieves most of the best results, combines our try-on capability with Klings animation. Ablation Study. We study the effectiveness of our training strategy, including the garment warm-up and multi-stage training. We define the Direct Training (DT) variant for Dress&Dance, which is directly trained with our final-stage inputs and outputs with full resolution. We compare full Dress&Dance with the DT-variant in Tab. 1 and project page. Without the garment warm-up training and multistage progressive training, the model is unable to faithfully preserve the contents of both user and garment images, even after extended training, resulting in low quantitative metrics. We observe that the training strategy is crucial for the convergence and final performance of the model. 5. Conclusion novel video diffusion framework, We introduce Dress&Dance, try-on and that enables both garment temporally consistent motion generation. As the first work to achieve high-resolution (1152 720) video virtual try-on, our framework creates high-quality videos showing the user wearing target garment, with motion guided by an example video. Our approach tackles several key challenges, including preserving the likeness of the user and garment, and generating complex motion with minimal error propagation. By utilizing unified conditioning network with cross-attention, we effectively handle heterogeneous inputs, improving garment registration, and supporting various garment capture methods. Additionally, our data-efficient training strategy, coupled with synthetic triplet data generation and multi-stage progressive approach, enables the generation of high-resolution videos with reduced artifacts."
        },
        {
            "title": "References",
            "content": "[1] Kling AI. Kling ai: Next-generation ai creative studio, 2024. 2, 3, 5, 6, 7, 8 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. 2, 3, 4, 5 [3] Chieh-Yun Chen, Yi-Chung Chen, Hong-Han Shuai, and Wen-Huang Cheng. Size does matter: Size-aware virtual try-on via clothing-oriented transformation try-on network. In Proceedings of the IEEE/CVF international conference on computer vision, pages 75137522, 2023. 3 [4] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin. Fw-gan: Flow-navigated In Proceedings of warping gan for video virtual try-on. the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. 4 [5] Xiang Fan, Anand Bhattad, and Ranjay Krishna. Videoshop: Localized semantic video editing with noise-extrapolated diffusion inversion, 2024. 3 Method Property Ours Commercial Method + CogVideoX I2V TPD + CogVideoX I2V OOTDiffusion ML-VTON + CogVideoX I2V Dress&Dance Image Try-On + CogVideoX I2V Kling Image Try-On + Kling Video 1.6 Dress&Dance Image Try-On + Kling Video 1.6 Dress&Dance Image Try-On + Ray2 Dress&Dance, Direct Train Dress&Dance Evaluation of Try-On and Visual Quality GPTTry-On GPTUser GPTMotion GPTVisual GPTOverall FIDInternet FIDCaptured 65.45 70.78 69.95 76.88 84.70 84.94 83.48 71.24 84.48 73.98 76.41 76.62 87.54 89.97 89.77 88.99 78.47 88.89 69.67 70.57 69.69 85.05 80.10 86.85 86.79 79.48 87.41 68.64 70.76 70.50 80.71 84.38 85.85 84.18 74.85 84.95 68.15 68.71 68.50 75.68 85.48 82.59 79.31 72.00 80.35 1146 1089 1185 1109 982 1008 1094 1073 753 884 739 760 655 700 735 788 691 Table 2. Our Dress&Dance significantly outperforms all the baselines in garment fidelity in virtual try-on (GPTTry-On), showing superior try-on capability, while achieving highly comparable or even better visual quality in other metrics to powerful commercial baselines Kling Video 1.6 [1] and Ray2 [19], which outperforms all open-sourced baselines with large margin. [6] Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and ZhengJun Zha. Vivid: Video virtual try-on using diffusion models, 2024. 4, 6 [7] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling In Proceedings of the IEEE/CVF conappearance flows. ference on computer vision and pattern recognition, pages 84858493, 2021. 3 [8] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. 4 [9] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, and Hang Xu. Reuse and diffuse: Iterative denoising for text-to-video generation, 2023. 2 [10] Zijian He, Peixin Chen, Guangrun Wang, Guanbin Li, Philip HS Torr, and Liang Lin. Wildvidfit: Video virtual tryon in the wild via image-based controlled diffusion models. arXiv preprint arXiv:2407.10625, 2024. 4, [11] Jianbin Jiang, Tan Wang, He Yan, and Junhui Liu. Clothformer: Taming video virtual try-on in all module. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 4, 6 [12] Johanna Karras, Yingwei Li, Nan Liu, Luyang Zhu, Innfarn Yoo, Andreas Lugmayr, Chris Lee, and Ira KemelmacherShlizerman. Fashion-vdm: Video diffusion model for virtual try-on. In Proceedings of ACM SIGGRAPH Asia 2024, December 2024. 4, 6, 7 [13] Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In CVPR, 2024. 3 [14] Gaurav Kuppa, Andrew Jong, Vera Liu, Ziwei Liu, and Teng Moh. Shineon: Illuminating design choices for practical video-based virtual clothing try-on, 2020. 4 [15] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. High-resolution virtual try-on with In Eumisalignment and occlusion-handled conditions. ropean Conference on Computer Vision, pages 204219. Springer, 2022. 2, 3, 5 [16] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In ECCV, 2024. 7 [17] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 3 [18] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 7 [19] Ray2. Ray 2 - advanced ai video generation tool, 2025. 2, 3, 5, 6, 7, 8 [20] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [21] Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, and Limin Wang. Bivdiff: training-free framework for general-purpose video synthesis via bridging image and video diffusion models, 2024. 3 [22] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. 3 [23] Yuanbin Wang, Weilun Dai, Long Chan, Huanyu Zhou, Aixi Zhang, and Si Liu. Gpd-vvto: Preserving garment details In Proceedings of the 32nd ACM in video virtual try-on. International Conference on Multimedia, MM 24, page 71337142, New York, NY, USA, 2024. Association for Computing Machinery. 4, [24] Zhenyu Xiel, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. Gpvton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning. pages 2355023559, 06 2023. 2, 3, 5 [25] Jiazheng Xing, Chao Xu, Yijie Qian, Yang Liu, Guang Dai, Baigui Sun, Yong Liu, and Jingdong Wang. Tryon-adapter: Efficient fine-grained clothing identity adaptation for highfidelity virtual try-on. International Journal of Computer Vision, pages 122, 2025. [26] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779, 2024. 2, 3, 5 [27] Zhengze Xu, Mengting Chen, Zhao Wang, Linyu Xing, Zhonghua Zhai, Nong Sang, Jinsong Lan, Shuai Xiao, and Changxin Gao. Tunnel try-on: Excavating spatial-temporal tunnels for high-quality virtual try-on in videos, 2024. 4, 6 8 [28] Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang, Jin Tao, and Xiangmin Xu. Texture-preserving diffusion models for high-fidelity virtual try-on, 2024. 2, 3, [29] Zhaotong Yang, Zicheng Jiang, Xinzhe Li, Huiyu Zhou, Junyu Dong, Huaidong Zhang, and Yong Du. 4-vton: Dynamic semantics disentangling for differential diffusion based virtual try-on. In European Conference on Computer Vision, pages 3652. Springer, 2024. 3 [30] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5 [31] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In 2018 IEEE/CVF deep features as perceptual metric. Conference on Computer Vision and Pattern Recognition, pages 586595, 2018. 7 [32] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. 2023. 2, 3, 5 [33] Xiaojing Zhong, Zhonghua Wu, Taizhe Tan, Guosheng Lin, and Qingyao Wu. MV-TON: memory-based video virtual try-on network. CoRR, abs/2108.07502, 2021. 4 [34] Luyang Zhu, Yingwei Li, Nan Liu, Hao Peng, Dawei Yang, and Ira Kemelmacher-Shlizerman. M&m vto: MultiIn Proceedings of the garment virtual try-on and editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13461356, 2024. [35] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46064615, 2023."
        }
    ],
    "affiliations": [
        "SpreeAI",
        "University of Illinois Urbana-Champaign"
    ]
}