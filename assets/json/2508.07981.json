{
    "paper_title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation",
    "authors": [
        "Fangyuan Mao",
        "Aiming Hao",
        "Jintao Chen",
        "Dongxia Liu",
        "Xiaokun Feng",
        "Jiashu Zhu",
        "Meiqi Wu",
        "Chubin Chen",
        "Jiahong Wu",
        "Xiangxiang Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects."
        },
        {
            "title": "Start",
            "content": "Preprint. Omni-Effects: UNIFIED AND SPATIALLY CONTROLLABLE VISUAL EFFECTS GENERATION Fangyuan Mao1 Aiming Hao1 Jintao Chen1,2 Jiashu Zhu1 Meiqi Wu1,4 Chubin Chen1,3 1 AMAP, Alibaba Group 2 PKU 3 THU 4 CASIA Project Page: https://amap-ml.github.io/Omni-Effects.github.io/ Jiahong Wu1 Xiangxiang Chu1 Dongxia Liu1,3 Xiaokun Feng1,4 5 2 0 A 1 1 ] . [ 1 1 8 9 7 0 . 8 0 5 2 : r Figure 1: Capabilities for Diverse Customized Visual Effects. Omni-Effects supports both (a) single-VFX and (b) multi-VFX generation through pure prompt-guided generation. Integrated with the Spatial-Aware Prompt, Omni-Effects enables (c) precise spatial VFX control and (d) intricate object-based visual effects with targeted environmental transformations."
        },
        {
            "title": "ABSTRACT",
            "content": "Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: Work done during the internship at AMAP, Alibaba Group. Equal Contribution Corresponding author 1 Preprint. (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs group of expert LoRAs, integrating diverse effects within unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct comprehensive VFX dataset Omni-VFX via novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects."
        },
        {
            "title": "INTRODUCTION",
            "content": "Visual effects (VFX) play crucial role in modern filmmaking, enabling the creation of immersive narratives and fantastical worlds. While traditional VFX pipelines, especially for composite effects requiring simultaneous coordination across different spatial locations, are notoriously complex and resource-intensive Chabanova (2022). Recent rapid advances in video generation technologies Yang et al. (2024); Bao et al. (2024); Kong et al. (2024); Wan et al. (2025) are driving paradigm shift in VFX creationtransitioning from conventional methods to generative model-powered dynamic, efficient synthesis. The inherent scarcity of VFX data and pronounced variability in dynamic characteristics across effects pose significant challenges to training generative models. Consequently, current methods Liu et al. (2025b) focus on single-effect generation, employing dedicated LowRank Adaptation (LoRA) Hu et al. (2022) tailored to individual effects. However, this paradigm struggles with multi-VFX scenes, exhibiting two critical limitations: Limitation 1. Cross-Adapter Interference where joint multi-LoRA activation Mangrulkar et al. (2022) induces spatial occlusion artifacts (Figure 2 (a)) and shared-subspace hybrid training triggers fidelity-degrading cross-effect confusion via task interference Zhang et al. (2025a) (Figure 2 (b, c)). Limitation 2. Spatial-Semantic Misalignment wherein the text-pixel space gap prevents precise spatial cue encoding for VFX placement (Figure 2 (d)). These limitations fundamentally constrain conventional video generation adaptation to complex multiple VFX compositions (multi-VFX). Figure 2: Defects in standard video generation models. (a) VFX disappearance, (b) quality degradation, (c) confusion between VFX elements, and (d) spatial uncontrollability. To address these limitations, we propose Omni-Effects, unified framework modeling multi-VFX generation as multi-condition video generation problem, where textual prompts specify effect categories while spatial masks define their precise locations. First, to tackle Limitation 1, we introduce LoRA-based Mixture of Experts (LoRA-MoE) module Shazeer et al. (2017); Dou et al. (2023); Zhang et al. (2025c), which partitions effects into specialized subspaces, each optimized by dedicated expert branches, with gating router dynamically activating relevant subspaces to minimize cross-task interference and enhance effect fidelity. Second, to overcome Limitation 2, we present 2 Preprint. the Spatial-Aware Prompt (SAP), which integrates explicit mask-based spatial conditioning with textual inputs via full-attention mechanisms for accurate effect placement. To mitigate SAP crossinterference during concurrent application, we introduce the Independent-Information Flow (IIF) module, which isolates condition-specific information flows through IIF Attention Mask, preventing unintended effect blending. Collectively, these innovations enable Omni-Effectsto our knowledge, the first VFX frameworkto achieve high-fidelity multi-VFX compositions with pixel-level spatial control. (Figure 1). To advance this research, we construct high-quality VFX dataset Omni-VFX and develop specialized VFX data pipeline. This pipeline utilises image editing models Liu et al. (2025a) to generate source image pairs depicting initial/final effect states, which are then synthesised into VFX videos via the FLF2V framework (built on Wan2.1 Wan et al. (2025)). Rigorous manual filtering ensures quality while expanding coverage to 55 distinct effect categories. Further, we introduce an evaluation framework specifically designed for controllable VFX generation tasks. Comprehensive experiments validate the Omni-Effects frameworks superior performance across three core capabilities: single-VFX, multi-VFX, and controllable VFX generation. In summary, the major contributions of our work are as follows: unified VFX framework, Omni-Effects, is proposed to enable high-fidelity spatial controllable multi-VFX generation through dual-core architecture: (1) LoRA-MoE modules unifying multi-VFX training, and (2) IIF-augmented SAP mechanisms enabling independent multi-condition control without interference. The most comprehensive VFX dataset Omni-VFX is developed with an automated production pipeline to support the generation of diverse high-quality VFX video, complemented by comprehensive evaluation framework for rigorous controllable VFX assessment. Extensive experiments demonstrate that our Omni-Effects achieves precise spatial control and enables diverse VFX generation, thereby allowing users to specify both the category and location of desired effects."
        },
        {
            "title": "2.1 VIDEO GENERATION MODELS",
            "content": "Recent advances in diffusion-based video generation Chen et al. (2023); He et al. (2023); Zeng et al. (2024); Kong et al. (2024); Yang et al. (2024); HaCohen et al. (2024); Bao et al. (2024); Polyak et al. (2025); Wan et al. (2025); Teng et al. (2025); Seawead et al. (2025) have enabled text-to-video (T2V) and image-to-video (I2V) synthesis, where input images establish spatial context. As critical I2V application, VFX generation creates unrealizable fantastical visuals. However, VFX data scarcity forces reliance on Low-Rank Adaptation (LoRA) Hu et al. (2022) for limited-data fine-tuning Liu et al. (2025b). This necessitates separate LoRA models per effect, while combined training causes performance degradation, fundamentally limiting multi-VFX generation within single video. Our architecture unifies multi-VFX training, avoids degradation, and enables concurrent multi-VFX generation."
        },
        {
            "title": "2.2 CONDITIONAL VIDEO GENERATION",
            "content": "Condition-guided diffusion models leverage auxiliary inputs for precise output control, falling into two paradigms: spatial-fusion guidance and high-level semantic guidance. Spatial-fusion methods, exemplified by ControlNet Zhang et al. (2023), integrate condition inputs with denoising inputs. These methods Li et al. (2024); Bai et al. (2024); Bian et al. (2025); Xu et al. (2025b); Lei et al. (2025); Jiang et al. (2025) enable fine-grained spatial alignment while preserving the generation quality of pre-trained diffusion models. High-level semantic methods exploit latent interactions between conditions and the denoising process. Techniques include cross-attention based mechanisms Ye et al. (2023); Zhang et al. (2024); Zhou et al. (2025); Yuan et al. (2025) and conditional token concatenation strategies Wang et al. (2024); Huang et al. (2024b); Tan et al. (2025); Zhang et al. (2025b), dynamically modulate generation through semantic embeddings. Crucially, while these methods effectively handle individual conditions, they struggle to simultaneously and independently control multiple conditions, critical requirement for professional VFX generation. 3 Preprint. Figure 3: Flowchart of proposed Omni-Effects. Given reference image and composite conditions of arbitrary length, Omni-Effects first encodes each input into corresponding tokens. These tokens are concatenated and processed sequentially through downstream DiT blocks. These blocks incorporate two key technologies: (a) LoRA-MoE, MoE plugin replacing standard FFN linear layers to enable collaborative expert task-solving and (b) SAP, which fuses effect descriptors with spatial trigger information during the attention stage while mitigating cross-condition information leakage via an IIF mechanism. Note that, in the IIF, dashed lines represent blocked information flow, while solid lines indicate active information transmission. Our model employs IIF-powered SPA control mechanisms to support independent, non-interfering control of multiple conditions within the same video."
        },
        {
            "title": "3.1.1 VIDEO DIFFUSION MODELS",
            "content": "Video generation models usually utilize the diffusion paradigm Ho et al. (2020); Lipman et al. (2022), which generates samples from data distribution (x0) by progressively denoising samples that are initially drawn from Gaussian distribution (xT ). During training, clean samples x0 (x0) undergo iterative corruption through diffusion steps: xt = αtx0 + σtϵ, (1) where αt, σt > 0 are scalars that jointly define the Signal-to-Noise Ratio (SNR). The denoiser with parameter θ is optimized to predict the target noise ϵ. The optimization process is defined as: ϵ (0, I) , = 1, ..., Lmse (θ) = Ext,t,c,ϵ (cid:104) ϵ ϵθ (xt, t, τ (c))2 (cid:105) (2) where is conditions (e.g., text and spatial location), and τ denotes the condition encoder. By replacing target noise ϵ with v, which is weighted combination of x0 and ϵ, as the prediction target, the v-prediction Salimans and Ho (2022) is derived, which is adopted in our Omni-Effects framework. Moreover, mainstream video generation leverages Diffusion Transformer (DiT) Peebles and Xie (2023); Ma et al. (2024); Chu et al. (2024) architecture by employing attention mechanisms to model spatiotemporal consistency while aligning conditional inputs with visual outputs Zheng et al. (2024). By integrating diffusion processes with Transformer architectures, the video generation performance is improved, leading to high-quality and accurate video synthesis results."
        },
        {
            "title": "3.1.2 SPATIALLY CONTROLLABLE MULTI-VFX GENERATION",
            "content": "In practical applications, its often necessary to display different VFX at distinct locations throughout video. We formalize this task as multi-conditional video generation, wherein video diffusion 4 Preprint. models take reference image and set of control signals = {ci}N i=1 as inputs. Each condition ci = (ei, si) couples an effect descriptor ei with spatial trigger si, whereby the generated video x0 applies effect ei at the location specified by si. Specifically, we use text prompt to describe the VFX, while using spatial mask RHW to serve as the spatial trigger. set of conditions is incorporated into the denoising process, and the denoiser prediction becomes: ˆv = ϵθ (cid:18) xt, t, (cid:110) τ (i) (ei) (cid:111)N i= (cid:110) τ (i) , (si) (cid:19) , (cid:111)N i=1 (3) where τe and τs denote the text and spatial mask encoder, respectively. Notice that, when = 1 and the spatial trigger is empty, the above task reduces to the traditional single-VFX generation task Liu et al. (2025b)."
        },
        {
            "title": "3.2 Omni-Effects",
            "content": "To model the above task, video diffusion models require simultaneous support for multi-VFX inference and spatial control capabilities. We accordingly propose Omni-Effects, building upon the CogVideoX Yang et al. (2024) architecture and incorporating two core components: LoRA-MoE and Spatial-Aware Prompt. The overview is illustrated in Figure 3, and the details are as follows."
        },
        {
            "title": "3.2.1 LORA-MOE",
            "content": "As mentioned in Figure 2, both multi-LoRA parallel inference and single-LoRA unified training degrade performance. Crucially, we observe the synergistic mechanism in VFX compatible training: VFX-combination training enhances single-VFX generation quality (Figure 4). This discovery motivates task-space our adaptive partitioning strategy: inspired by MoE Shazeer et al. (2017) architectures, we partition distinct effects into specialized subspaces and deploy gating router subspace for selection. adaptive Figure 4: FVD scores for diverse VFX trained with shared LoRA. VFX performance exhibits an initial improvement followed by progressive degradation with increasing numbers of co-trained effects. This indicates inherent effect clustering: synergistic groups (e.g., Melt-like effects) improve co-training performance, while incompatible combinations (e.g., Deflate + Squish) suffer from mode collapse and underperform relative to compatible sets. Note that, lower FVD values indicate superior performance, with optimal VFX results uniformly achieved when the number of co-trained VFX equals 4. Specifically, LoRA-MoE Dou et al. (2023) integrates MoE with LoRA (Figure 3 (a)), which employs an expert ensemble where each LoRA specializes in distinct VFX manifolds. Formally, for input token Rd, weight is obtained by gating network : Rd (cid:55) Rn for each expert, resulting in (x) = [G (x)1 , (x)2 , ..., (x)n], where represents the number of experts. Each expert Ei implements LoRA decomposition: Ei(x) = α xAiBi, Ai Rdr, Bi Rrd, (4) where denotes the low-rank and α is scaling factor. The final prediction combines base model and expert outputs: (cid:88) = Base (x) + (x)i Ei (x) , (5) i=1 This MoE-structured plugin replaces standard FFN linear layers, enabling collaborative expert tasksolving. During training, Top-k routing strategy (k n) is adopted to enforce exactly non-zero entries in (x). At inference, all experts are activated to prevent effect suppression caused by Top-k 5 Preprint. Figure 5: Visualization of controllable VFX performance and attention maps. (a) Position description lacks spatial control; (b) ControlNet faces inter-condition interference, leading to VFX leakage and artifacts; (c) Proposed SAP+IIF achieves precise positional controllability while preventing mutual interference between multi-VFX. filtering, omitting critical experts, which is essential for multi-VFX combination generation. Moreover, to mitigate workload imbalance caused by the gating network favoring few experts during training, we also employ balanced routing auxiliary loss Laux Fedus et al. (2022). Comprehensive details are provided in Supplement A. The final training objective is expressed as: = Lmse+βLaux, where β is hyperparameter."
        },
        {
            "title": "3.2.2 SPATIAL-AWARE PROMPT",
            "content": "For condition ci = (ei, si), embedding positional descriptors within text prompts proves insufficient for precise spatial control. To investigate this phenomenon, we visualize attention maps across diverse prompts. Crucially, attention consistently activates identical regions regardless of prompt semantics (Figure 5 (a)), evidencing textual position cues failure to direct activation toward specified targets. Prior work Liu et al. (2025b); Jiang et al. (2025) mitigates this via ControlNet Zhang et al. (2023) to extract mask sequence for generation guidance. However, this solution suffers from two critical limitations: 1. Significant parameter overhead: ControlNet duplicates portion of the base models parameters (typically half), requiring substantial extra trainable weights; 2. Severe cross-condition interference: During multi-VFX generation, parallel ControlNet inference suffers from information leakage, manifesting as erroneous co-occurrence of effect ei and ej at positions si and sj respectively (Figure 5 (b)). In summary, signals within composite conditions must be integrated while preventing crosscondition interference to ensure robust performance. We address these challenges by proposing the Spatial-Aware Prompt to directly inject spatial information into prompts tokens via enhanced spatial-text condition token interactions within attention mechanism, enabling controllable generation with minimal parameter/computational overhead. Building on this, we introduce InformationIndependent Flow, which utilizes designed attention mask to restrict cross-condition information exchange, thereby preventing interference between distinct control streams. Formally, given set of conditions C, encoder processing yields text condition tokens and spatial condi- (cid:110) τ (i) (cid:111)N (si) , which are sequentially concatenated with the noisy latent xt to form the tion tokens inputs Q, and . Then we define an attention mask {0, }ll (l is the total sequence length) to regulate attention flow (Figure 3 (b), details are in Supplement A) that blocks condition-toi=1 (cid:110) τ (i) (ei) (cid:111)N i= 6 Preprint. condition and noise-to-condition interactions, eliminating cross-condition leakage to prevent effect misalignment or blending. The final output of attention is expressed as: = Softmax (cid:16) QKT / (cid:112) dk + (cid:17) , (6) where dk denotes the feature dimension. To enhance spatial conditioning alignment with noisy latents, we inject positional embeddings from xts initial frame into τ (i) (si), coupled with deds icated Spatial-Condition LoRA. Crucially, all spatial conditions share identical LoRA parameters while maintaining common base LoRA across other branches, ensuring efficient conditional injection without disrupting pretrained representations. Each text condition is individually processed through text encoder while sharing identical positional encoding. As shown in Figure 5 (c), our SAP+IIF achieves precise VFX targeting in target regions with non-overlapping activation zones."
        },
        {
            "title": "4.1 DATASET COLLECTION",
            "content": "VFX fundamentally manifest as radical spatio-temporal state transformations (e.g., explosion). Despite modern techniques like animation and Computer Graphics Interface (CGI) Chabanova (2022), modeling such dynamics remains challenging. We introduce novel pipeline: for any input image, Step1X-Edit Liu et al. (2025a) produces its modified counterpart to establish boundary frames defining VFXs initial and terminal states. This constraint provides strong transformation priors for generative models. The FLF2V framework Wan et al. (2025) then synthesizes the final video by compressing VFX production into boundary-constrained state-transition path search, significantly reducing modeling complexity. Through curated manual selection, we build comprehensive dataset Omni-VFX spanning 55 distinct VFX across instantaneous environmental shifts, artistic styles, human emotions, and so on, enabling diverse creative applications. For more data details, please refer to Supplement B."
        },
        {
            "title": "4.2 TRAINING",
            "content": "Since our training dataset contains only single-VFX without multi-VFX data, empirical observations reveal that standard training fails to achieve controllable multi-VFX generation. We overcome this with tri-level solution. At the data level, through random cropping and splicing with two videos, and random temporal freezing, we generate pseudo multi-VFX videos with corresponding masks. At the scheduler level, Non-Uniform Sampling prioritizes denoising steps [900, 1000](early stage) for spatial control learning with increased batch allocation, while dedicating fewer batches to detail refinement in lower steps [0, 900], motivated by empirical findings that enhanced focus on early denoising accelerates model convergence. At the training strategy level, iterative single to multiVFX (N = 2) fine-tuning ensures stable convergence and performance gains. For more training details, please refer to Supplement C."
        },
        {
            "title": "5.1.1 EVALUATION METRICS",
            "content": "Following previous work Liu et al. (2025b), for single-VFX evaluation, we employ two established metrics: Frechet Video Distance (FVD) Unterthiner et al. (2018) for overall fidelity and Dynamic Degree Huang et al. (2024c) for motion dynamics. For controllable VFX, we introduce three novel metrics. Regional Dynamic Degree (RDD), which utilizes optical flow and masks, quantifies the strength of motion within the target region, thereby quantifies motion strength within target regions to measure visual impression. Effect Occurrence Rate (EOR), which is computed by inputting both the video and given prompt template into Gemini2.5 Comanici et al. (2025) to obtain the answer, measures intended effect trigger frequency, indicating generation reliability. Building upon EOR, Effect Controllability Rate (ECR) assesses spatial precision by verifying VFX confinement to designated areas. Complete metric details appear in Supplement D. 7 Preprint. Metrics Methods Cake-ify Crumble Crush Decapitate Deflate Dissolve Eye-pop Harley Inflate Levitate Melt Squish Ta-da Venom Avg. Param.# FVD Single LoRA 2138 Mix LoRA 2947 2552 1496 1772 LoRA-MoE 1506 1641 Dynamic Degree Single LoRA Mix LoRA LoRA-MoE 0.8 0.8 1. 0.8 0.8 1.0 0.0 0.0 0.6 1190 1299 0.6 0.6 0.6 913 886 839 0.0 0.0 0. 1770 1995 1995 1725 3576 4496 1505 2042 1401 1006 2827 1415 2748 1053 1240 4146 2026 132.1M 9.4M 3923 2041 1118 1460 3330 736 2512 1561 1064 3339 1628 28.5M 0.8 0. 0.4 0.0 0.0 0.0 1.0 1.0 1.0 0.8 0. 1.0 0.0 0.0 0.0 0.6 0.6 0.6 1.0 1. 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.60 132.1M 9.4M 0. 0.66 28.5M Table 1: Performance comparison on OpenVFX dataset. Param.# represents the average training parameters per effect. And the highest metric values are highlighted in bold, with the second-best underlined. Methods RDD EOR ECR Melt Levitate Explode Avg. Melt Levitate Explode Avg. Melt Levitate Explode Avg. CogVideoX LTX-Video Wan-2.1 CogV+CN Ours 0.91 0.12 2.06 3.80 2. 0.99 0.11 1.57 2.39 2.22 1.11 0.14 2.38 2.09 3.87 1.00 0.12 2.00 2.76 2. 0.06 0.05 0.11 0.95 0.99 0.09 0.02 0.02 0.80 0.94 0.11 0.05 0.03 0.82 0. 0.09 0.04 0.05 0.86 0.97 0.00 0.00 0.02 0.56 0.93 0.00 0.00 0.00 0.36 0. 0.00 0.00 0.00 0.70 0.89 0.00 0.00 0.01 0.54 0.88 Table 2: Quantitative Results of Single-VFX Control Generation. We compare Omni-Effects with representative open-source video generation models under three controllable VFX scenarios: Melt, Levitate, and Explode. 5.1."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Training employs CogVideoX-5B backbone with LoRA rank of 128 with total of = 8 experts, generating 49 480 720 resolution videos. For loss, β is set to 0.01. We utilize 8 H20 GPUs (96GB) with batch size of 1 per GPU. We use AdamW Loshchilov and Hutter (2017) at constant 104 learning rate for 5, 000 steps. During inference, DDIM Nichol and Dhariwal (2021) sampling (50 steps) with CFG Ho and Salimans (2022) scale 6.0 is applied, which can be perform on single GPU. Extended details are in Supplement C."
        },
        {
            "title": "5.2 QUANTITATIVE RESULTS",
            "content": "In the following, we evaluate the effectiveness of Omni-Effects by comparing it with baseline models on unified and controllable VFX generation tasks."
        },
        {
            "title": "5.2.1 UNIFIED VFX GENERATION",
            "content": "We evaluate the LoRA-MoE against VFX-specific training LoRA and mix training LoRA for all VFX on the public OpenVFX dataset as detailed in Table 1. LoRA-MoE achieves the best performance across different types of VFX, while significantly reducing the number of trainable parameters. This demonstrates the effectiveness of the designed VFX task-subspace partitioning strategy. Qualitative results are shown in Supplement E."
        },
        {
            "title": "5.2.2 CONTROLLABLE VFX GENERATION",
            "content": "To evaluate our model, we perform comprehensive experiments for singleand multi-VFX control, comparing with state-of-the-art methods including (a) CogVideoX Yang et al. (2024), (b) LTXVideo HaCohen et al. (2024), (c) Wan2.1 Wan et al. (2025), and (d) CogVideoX integrated with ControlNet (CogV+CN). Evaluation targets three spatially localized VFX typesExplode, Melt, and Levitateto ensure contamination-free assessment. Single-VFX Control. Table 2 demonstrates baseline methods fundamental limitations in synthesizing target VFX and achieving precise spatial control. While CogV+CN can synthesize VFX, it exhibits limited controllability. In comparison, Omni-Effects achieves the best performance with 0.97 EOR and 0.88 ECR, significantly outperforming all baselines in both generation quality and spatial control precision. This validates that our proposed SAP effectively integrates VFX descrip8 Preprint. Figure 6: Qualitative Comparison of MultiVFX Generation. The desired outcome requires the left chair to melt while the right levitates simultaneously. Figure 7: Effect of Different Attention Masks in SAP. Attention Masks are progressively removed while information flow constraints are relaxed from top to bottom. Figure 8: Scalable VFX augmentation. Omni-Effects supports inference-time extension to diverse VFX composition. tors with spatial triggers without introducing substantial additional training parameters. Qualitative comparisons are shown in Supplement E."
        },
        {
            "title": "Methods",
            "content": "Melt+Levitate Melt+Explode RDD EOR ECR RDD EOR ECR CogVideoX 1.80 LTX-Video 0.11 Wan-2.1 1.93 3.18 CogV+CN Multi-VFX Control. For multiVFX generation using two effects combinations, Table 3 shows baseline models consistently failing to generate or spatially control VFX. Omni-Effects achieves precise spatial control over simultaneous VFX. Moreover, Figure 6 demonstrates Omni-Effects superiority: when instructed to melt the left chair while levitating the right chair, CogVideoX erroneously applies melting to both objects; CogV+CN correctly renders melting but fails to generate levitation; whereas Omni-Effects simultaneously executes both VFX through spatial condition. This performance validates our proposed IIFs efficacy in mitigating cross-condition interference. The user study is shown in Supplement E. Table 3: Quantitative Results of Multi-VFX Generation. 0.00 0.00 0.01 0.09 0.03 0.00 0.01 0.08 0.00 0.00 0.00 0.08 0.00 0.00 0.00 0.05 0.96 0.13 3.10 3."
        },
        {
            "title": "Ours",
            "content": "2.63 0.62 0.41 4.59 0.68 0."
        },
        {
            "title": "5.2.3 GENERALIZATION",
            "content": "Despite being trained with only = 2 effects, our model generalizes to diverse mask conditions during inference using the shared Spatial-Condition LoRA, thereby extending to the generation of more concurrent control VFX (N > 2). Omni-Effects demonstrates robust extensibility, successfully handling complex effect combinations (Figures 1 (d) and 8), validating its test-time scalable VFX control capability. 9 Preprint."
        },
        {
            "title": "5.3.1 LORA-MOE",
            "content": "Ablation study on expert count and Topk selection  (Table 4)  reveals that scaling experts improves generation quality at increased parameter cost. Crucially, our MoE architecture with minimal experts surpasses LoRA baselines  (Table 1)  , demonstrating efficient VFX adaptation through parameter-optimized expert aggregation."
        },
        {
            "title": "Model",
            "content": "FVD"
        },
        {
            "title": "4 Experts+Top1\n8 Experts+Top2",
            "content": "Dynamic Degree"
        },
        {
            "title": "4 Experts+Top1\n8 Experts+Top2",
            "content": "Avg. 1762 1628 0.65 0.66 Param.# 18.9 28.5 18.9 28. Table 4: Ablation study on LoRA MoE settings. Ablation study on SAP+IIF reveal critical insights in attention mechanisms corresponding to information flow. Removing SAP attention masks from regions {➁,➅,➆} causes melting artifacts on levitating objects (Figure 7 (b, c)), exposing information leakage, while complete attention induces uncontrolled object melting, demonstrating excessive information interaction degrades control. Strategic masking of {➁,➅,➆} prevents leakage while preserving independent information flow in target regions. Additional ablation studies are detailed in the Supplement F."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose Omni-Effects, unified framework for generating customized VFX videos. It supports the creation of diverse VFX, ranging from single-VFX, multi-VFX to spatially controllable multi-VFX. To achieve these, our framework integrates two core modules: LoRA-MoE and SAP-IIF. Specifically, the LoRA-MoE module mitigates cross-condition interference arising during mix training of multi-VFX. The SAP module, on the other hand, fuses VFX descriptors with spatial trigger information and tackles cross-condition information leakage via an IIF mechanism. Through the synergistic integration of LoRA-MoE and SAP-IIF, Omni-Effects enables precise spatial control and produces high-fidelity multi-VFX composites. We also develop comprehensive VFX dataset Omni-VFX with specialized data production pipeline and an evaluation framework tailored for controllable VFX generation to further validate our approach. Extensive experiments demonstrate the robustness of Omni-Effects across complex, multi-condition VFX generation scenarios. MultiVFX generation represents domain of substantial practical value coupled with persistent technical challenges. To the best of our knowledge, this work pioneers the first comprehensive framework explicitly addressing this complex problem. Our methodology substantively advances controllable multi-VFX synthesis capabilities while unlocking novel applications across film production, game development, and advertising creatives."
        },
        {
            "title": "REFERENCES",
            "content": "Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. arXiv preprint arXiv:2412.07760, 2024. Jinze Bai, Shuai Bai, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Any-length video inpainting and editing with plug-and-play context control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. Anastasia Chabanova. Vfx new frontier: The impact of innovative technology on visual effects, 2022. PhD thesis, University of Westminster. 10 Preprint. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. Rui Chen, Lei Sun, Jing Tang, Geng Li, and Xiangxiang Chu. Finger: Content aware fine-grained evaluation with reasoning for ai-generated videos. In ACM MM, 2025. Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In ECCV, 2024. Xiangxiang Chu, Renda Li, and Yong Wang. Usp: Unified self-supervised pretraining for image generation and understanding. In ICCV, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, et al. Loramoe: Alleviate world knowledge forgetting in large language models via moe-style plugin. arXiv preprint arXiv:2312.09979, 2023. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Xiaokun Feng, Haiming Yu, Meiqi Wu, Shiyu Hu, Jintao Chen, Chen Zhu, Jiahong Wu, Xiangxiang Chu, and Kaiqi Huang. Narrlv: Towards comprehensive narrative-centric evaluation for long video generation models. arXiv preprint arXiv:2507.11245, 2025. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation, 2023. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Hailang Huang, Yong Wang, Zixuan Huang, Huaqiu Li, Tongwen Huang, Xiangxiang Chu, and Richong Zhang. Mmgenbench: Fully automatically evaluating lmms from the text-to-image generation perspective. arXiv preprint arXiv:2411.14062, 2024a. Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024b. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024c. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Rui Lan, Yancheng Bai, Xu Duan, Mingxing Li, Lei Sun, and Xiangxiang Chu. Flux-text: simple and advanced diffusion transformer baseline for scene text editing, 2025. Guojun Lei, Chi Wang, Rong Zhang, Yikai Wang, Hong Li, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2794627956, 2025. 11 Preprint. Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. ConImproving conditional controls with efficient consistency feedback, 2024. URL https: trolnet++: //arxiv.org/abs/2404.07987. Xinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu. Vmbench: benchmark for perception-aligned video motion generation. In ICCV, 2025. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025a. Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. Vfx creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin PEFT: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/ Bossan. huggingface/peft, 2022. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. IEEE/CVF international conference on computer vision, pages 41954205, 2023. In Proceedings of the Adam Polyak, Amit Zohar, et al. Movie gen: cast of media foundation models, 2025. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer, 2025. URL https://arxiv.org/abs/2411.15098. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, arXiv preprint WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv:2505.13211, 2025. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Team Wan, Ang Wang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024. Ryan Xu, Dongyang Jin, Yancheng Bai, Rui Lan, Xu Duan, Lei Sun, and Xiangxiang Chu. Scalar: Scale-wise controllable visual autoregressive learning. arXiv preprint arXiv:2507.19946, 2025a. 12 Preprint. Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1590915919, 2025b. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyang Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1297812988, 2025. Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. Juzheng Zhang, Jiacheng You, Ashwinee Panda, and Tom Goldstein. Lori: Reducing cross-task interference in multi-task low-rank adaptation. arXiv preprint arXiv:2504.07448, 2025a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 38133824, 2023. Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer, 2025b. Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025c. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, et al. Light-a-video: Training-free video relighting via progressive light fusion. arXiv preprint arXiv:2502.08590, 2025. 13 Preprint."
        },
        {
            "title": "A METHOD",
            "content": "A.1 BALANCED ROUTING AUXILIARY LOSS LAUX Drawing inspiration from Switch Transformers Fedus et al. (2022), we integrate balanced routing auxiliary loss Laux into LoRA MoE training. Specifically, for batch with tokens, define fi as the fraction of tokens routed to expert i: fi = 1 (cid:88) {arg max (x) = i} , xB and Pi is the mean router probability for expert i: Pi = 1 (cid:88) xB pi (x). The auxiliary loss encourages load balancing through: Laux = (cid:88) fi Pi, (7) (8) (9) i=1 which reaches its theoretical minimum of 1 when pi(x) = 1/n uniformly (x, i). When the gating network outputs an average probability distribution of [1/n, , 1/n] for tokens in batch, Laux achieves its minimum value as (cid:80)n i=1 1/n 1/n = 1 A."
        },
        {
            "title": "IIF ATTENTION MASK",
            "content": "The IIF Attention Mask is partitioned into two primary components: Condition Interaction: Within each condition pair ci, comprising text (τ (i) (ei)) and spatial condition (τ (i) (si)) tokens, tokens attend to each other. However, tokens within one condition pair are masked from all tokens in other condition pairs and from the noisy latent token xt to prevent information leakage. Information Aggregation: The noisy latent token xt attends to all text tokens ( ) k=1 and to itself. This aggregates textual information for updating its representation. Crucially, xt is (ek) (cid:110) τ (k) (cid:111)N masked from all spatial condition tokens ( redundancy. (cid:110) τ (k) (sk) (cid:111)N k=1 ), preventing direct access and avoiding Formally, the IIF Attention Mask Mij for tokens xi and xj is defined as: Mij = 0, 0, , otherwise if xi and xj belong to the same condition ck if xi xt and xj xt (cid:110) τ (k) (ek) (cid:111)N k=1 (10)"
        },
        {
            "title": "B DATASET",
            "content": "B.1 DATASET COLLECTION To augment our dataset, we employ three strategies: novel first-last frame generation method and integrating external datasets. Our dataset construction pipeline, outlined in Figure 9, follows multi-stage generative approach. Firstly, we define target VFX categories spanning diverse styles (e.g., seasonal transformations, claymation, 3D doll rendering). An initial semantic analysis using Qwen Bai et al. (2023) classifies input images by content and style. Secondly, for each category, Step1X-Edit Liu et al. (2025a) generates stylistically consistent frame pairs (initial-final) conditioned on dynamically constructed prompts. These pairs are then analyzed by Qwen to produce descriptive text prompts, which undergo iterative refinement through Wan2.1-14Bs Video Prompt Extender Wan et al. (2025) for temporal coherence. Thirdly, the optimized prompts and frames jointly drive Wan2.1-14B to synthesize augmented video sequences. Through this pipeline, we achieve coverage of 55 effect categories while maintaining quality via: (1) automated style-consistency checks, and (2) manual validation of visual fidelity. B.2 DATASET VISUALIZATION 14 Preprint. Figure 9: Synthetic VFX Video Generation via Keyframe Editing and WAN 2.1 Interpolation. Figure 10: Some examples of our dataset curation pipeline. 15 Preprint. Figure 11: Some examples of our Omni-VFX dataset. Preprint. Our dataset consists of 55 diverse VFX samples, systematically categorized into five groups: 1. Environmental Shifts: Including seasonal transitions (spring, summer, autumn, winter) and weather variations (e.g., rain). 2. Dynamic Transformations: Featuring simulated phenomena such as explosions and fluid dynamics. 3. Artistic Styles: Comprising stylized renderings (e.g., oil painting effects, 3D doll aesthetics). 4. Human Emotion Depictions: Capturing facial expressions (e.g., smiles, crying). 5. Complex Effects: Integrating multiple visual elements across categories to produce sophisticated composites. Figure 12: Distribution of our OmniVFX dataset. The representative visualizations are provided in Figure 11 while the distribution of samples across categories is illustrated in Figure 12."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 TRAINING DATA AUGMENTATION To address the scarcity of multi-VFX (Multiple Visual Effects) data, we perform data augmentation on singleVFX data. We sample single-VFX data along with their corresponding complete mask data with probability of 20%; with 40% probability, we sample single-VFX data and perform random cropping and splicing; and with 40% probability, we sample two types of VFX and perform random cropping and splicing. During the splicing process, there is 20% probability of applying temporal freezing to any segment of the spliced video (turning it into static video with the corresponding mask set to empty, simulating the condition where no VFX is generated). The example of data augmentation is shown in Figure 13. C.2 NON-UNIFORM TIMESTEP SAMPLING Through experiments, we observe that for video generation with strict control requirements, the denoising steps in the early stages of the diffusion model play critical role, as they determine whether the target region achieves precise control. Traditional uniform timestep sampling requires extensive training to sufficiently optimize the models accuracy in the initial steps. To accelerate training convergence, we enhance the focus on early denoising by allocating 3/4 of the batch to the crucial initial steps ([900, 1000]), while dedicating the remaining 1/4 to refining details in later steps ([0, 900]). C.3 DUAL-PHASE TRAINING STRATEGY Figure 13: Visualization of Data augmentation. To enhance the models capability in controlling both single-VFX and multi-VFX videos generation, we adopt dual-stage progressive training strategy during the training phase: Stage 1: We train the model using single-VFX videos with single masks, allowing it to learn how to control single VFX. This stage runs for 2, 000 steps. Stage 2: In addition to single-VFX videos, we introduce multi-VFX videos by combining two VFXs (as described in Sec. C.1) and perform data augmentation on these samples. The model is fine-tuned for an additional 3,000 steps under this setting. This training approach improves the models robustness and enables it to generalize to larger number of control VFXs (N > 2) during inference."
        },
        {
            "title": "D METRICS",
            "content": "With the development of diffusion-based generative models Chu et al. (2025); Lan et al. (2025); Xu et al. (2025a), series of evaluation benchmarks have emerged to assess the quality of generated content Ling et al. 17 Preprint. Figure 14: Flowchart of the metric design for controllable visual effect. (2025); Feng et al. (2025); Chen et al. (2025); Huang et al. (2024a). However, existing benchmarks struggle to meet the evaluation requirements for controllable VFX. Therefore, we propose an evaluation framework comprising three components: VFX detection, VFX controllable estimation, and video motion estimation, with the specific architecture illustrated in the Figure 14. These components correspond to the calculation processes of the metrics Effect Occurrence Rate (EOR), Effect Controllability Rate (ECR), and Regional Dynamic Degree (RDD). D.1 VISUAL EFFECTS DETECTION To determine whether the desired visual effect is present in the generated videos, we leverage the Gemini 2.5 Comanici et al. (2025) large multimodal model as an evaluation assistant. Specifically, for each test video, we pair it with predefined prompt template that explicitly describes the target effect, and input both into Gemini. We then request Gemini to provide binary answer as to whether the specified effect has appeared in the video. Each query is repeated three times, and the most frequently occurring answer among the three is selected as the final result. By applying this process to all videos in the evaluation set, we compute the Effect Occurrence Rate (EOR), defined as the percentage of videos in which Gemini confirms the occurrence of the designated visual effect. D.2 VISUAL EFFECTS CONTROLLABILITY ESTIMATION Controllability estimation is performed only on videos that are identified as having the target effect by the Visual Effects Detection step. For these videos, we compute the absolute pixel-wise difference between the first and last frames and compute the resulting difference binary map. In the masked region, we select the top 80% of difference values; for the non-masked region, the bottom 80%. The mean squared error (MSE) is then computed as Inner Diff and Outer Diff. An effect is regarded as controllable if Inner Diff is below threshold of 0.5 and Outer Diff is below threshold of 0.1. The Effect Controllability Rate (ECR) is defined as the fraction of detected-effect videos that meet these criteria. D.3 VIDEO MOTION DETECTION The Regional Dynamic Degree (RDD) measures the strength of motion caused by VFX within mask-specified region of video. We use the RAFT algorithm Teed and Deng (2020) to estimate optical flow between consecutive frames. Given binary mask for the region of interest, we calculate the mean motion magnitude within this region as follows: RDD = 1 (cid:88) t=1 1 (cid:88) (x,y)s 18 Flow(It, It+1)x,y2 , (11) Preprint. Figure 16: Qualitative Comparison of Different LoRA Settings. where is the number of consecutive frame pairs and is the number of pixels in the masked region. higher RDD indicates stronger or more dynamic effects within the specified area, enabling precise, regionspecific evaluation of effect intensity."
        },
        {
            "title": "E EXPERIMENTS RESULTS DETAILS",
            "content": "E.1 QUALITATIVE RESULTS OF LORA MOE Qualitative results of Different LoRA settings are visualized in Figure 16. LoRA-MoE demonstrates superior visual performance. E.2 QUALITATIVE RESULTS OF SINGLE-VFX CONTROL Qualitative results of single-VFX control are visualized in Figure 15. CogV+CN incorrectly causes both cups to explode, while our proposed Omni-Effects explodes the correct cup while keeping the other cup intact. E.3 USER STUDY OF MULTI-VFX GENERATION Figure 15: Qualitative Comparison of SingleVFX Generation. The desired outcome requires the right cup to explode while the left stays static. To achieve more reliable evaluation results, we select subset of videos from the test set for user study. We choose the state-of-the-art open-source Wan2.1-I2V model as the representative base model, and evaluate it alongside CogV+CN and our proposed method on multi-effect videos. We design two questions: one regarding user preference (i.e., which video the user considers to be of the highest overall quality?), and another asking whether each video demonstrates precise controllability of the specified visual effect, which can directly reflect the effectiveness of our approach. Six professional raters participated in the evaluation, and the results are shown in Figure 17 19 Preprint. Method VFX Nums Deflate Melt Crumble Dissolve Ta-da Squish Crush Cake-ify FVD LoRA 1 2 4 8 14 913 857 779 787 949 2827 3606 3957 3456 2030 2947 - 2324 2756 1770 - 1559 2103 1722 1053 1415 - - - - 1206 1699 1501 1527 1496 - - 1540 2138 - - 1355 1388 LoRA MoE 14 839 2512 1641 1064 1561 1213 1506 Table 5: Ablation Study on Co-trained with Different VFX Number. Figure 17: User Study for MultiVFX Generation. Omni-Effects exceeds other baseline. MORE ABLATION STUDY F.1 VFX-COMBINATION TRAINING Ablation studies on diverse VFX-combination training regimes (Table 5 and Table 6) reveal inherent effect clustering, demonstrating that compatible VFX-combination training enhances single-VFX generation quality. The proposed LoRA-MoE framework effectively leverages this property to boost performance across all VFX types. Figure 18: Ablation study on Non-uniform Timestep Sampling Steps. NUS stands for Non-Uniform Sampling, and DA stands for Data Augmentation. The training of first stage = 1 with epoch=70. F.2 DATA AUGMENTATION Figure 19: Ablation Study on Different Training Strategy. Ablation study on data augmentation (Figure 18) reveals that models struggle to achieve spatial controllability using singleVFX data alone without augmentation. Our custom-designed data augmentation protocol enables effective controllability for both singleand multi-VFX generation. Deflate Deflate+Melt Deflate+Squish FVD 913 857( 6%) 1611( 76%) Table 6: Ablation Study on Co-trained with Different VFX combination. F.3 TIMESTEP SCHEDULER Ablation study on different time schedulers (Figure 18) reveals that traditional uniform sampling necessitates extensive training efforts to sufficiently optimize model accuracy in initial steps, whereas our employed nonuniform timestep sampling empirically accelerates model convergence. F.4 TRAINING STRATEGY Ablation study on different training strategies (Figure 19) reveals that our employed Dual-phase Training Strategy effectively enhances model robustness for multi-VFX generation, with Stage 2 demonstrably mitigating interference artifacts compared to Stage 1."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "CASIA",
        "PKU",
        "THU"
    ]
}