{
    "paper_title": "Video Creation by Demonstration",
    "authors": [
        "Yihong Sun",
        "Hao Zhou",
        "Liangzhe Yuan",
        "Jennifer J. Sun",
        "Yandong Li",
        "Xuhui Jia",
        "Hartwig Adam",
        "Bharath Hariharan",
        "Long Zhao",
        "Ting Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We explore a novel video creation experience, namely Video Creation by Demonstration. Given a demonstration video and a context image from a different scene, we generate a physically plausible video that continues naturally from the context image and carries out the action concepts from the demonstration. To enable this capability, we present $\\delta$-Diffusion, a self-supervised training approach that learns from unlabeled videos by conditional future frame prediction. Unlike most existing video generation controls that are based on explicit signals, we adopts the form of implicit latent control for maximal flexibility and expressiveness required by general videos. By leveraging a video foundation model with an appearance bottleneck design on top, we extract action latents from demonstration videos for conditioning the generation process with minimal appearance leakage. Empirically, $\\delta$-Diffusion outperforms related baselines in terms of both human preference and large-scale machine evaluations, and demonstrates potentials towards interactive world simulation. Sampled video generation results are available at https://delta-diffusion.github.io/."
        },
        {
            "title": "Start",
            "content": "2024-12-"
        },
        {
            "title": "Video Creation by Demonstration",
            "content": "Yihong Sun1,2, Hao Zhou1, Liangzhe Yuan1, Jennifer J. Sun1,2, Yandong Li1, Xuhui Jia1, Hartwig Adam1, Bharath Hariharan2, Long Zhao1 and Ting Liu1 1Google DeepMind, 2Cornell University We explore novel video creation experience, namely Video Creation by Demonstration. Given demonstration video and context image from different scene, we generate physically plausible video that continues naturally from the context image and carries out the action concepts from the demonstration. To enable this capability, we present ùõø-Diffusion, self-supervised training approach that learns from unlabeled videos by conditional future frame prediction. Unlike most existing video generation controls that are based on explicit signals, we adopts the form of implicit latent control for maximal flexibility and expressiveness required by general videos. By leveraging video foundation model with an appearance bottleneck design on top, we extract action latents from demonstration videos for conditioning the generation process with minimal appearance leakage. Empirically, ùõø-Diffusion outperforms related baselines in terms of both human preference and large-scale machine evaluations, and demonstrates potentials towards interactive world simulation. Sampled video generation results are available at https://delta-diffusion.github.io. Keywords: Generative AI, Controllable Video Generation, Video Foundation Models, World Simulation 4 2 0 2 2 1 ] . [ 1 1 5 5 9 0 . 2 1 4 2 : r Figure 1 Video Creation by Demonstration. Given demonstration video, our proposed ùõø-Diffusion generates video that naturally continues from context image and carries out the same action concepts. Corresponding author(s): Ting Liu (liuti@google.com) and Long Zhao (longzh@google.com). 2024 Google DeepMind. All rights reserved Video Creation by Demonstration 1. Introduction When given visual demonstration, humans can naturally imagine what it would look like if these actions were to take place in different environment. This leads to natural question, can we teach machines to simulate realistic visual world with fine-grained action controls, all from provided demonstration and specified environment? As an initial step towards answering this question, we propose Video Creation by Demonstration, video creation experience that empowers users to generate videos by providing demonstration video that showcases desired action concepts and an initial scene context image to carry out the action concepts from. As shown in Figure 1, our system generates new video that integrates the demonstrated action into the provided context, ensuring both temporal continuity and physical plausibility. With the recent advances in diffusion models (Ho et al., 2020; Song et al., 2021), video generation (Brooks et al., 2024; Gupta et al., 2024; Polyak et al., 2024) emerges as frontier in the goal of building interactive world simulators (Alonso et al., 2024; Brooks et al., 2024; Bruce et al., 2024; Meng et al., 2024; Valevski et al., 2024). Compared to our proposed Video Creation by Demonstration, most existing controllable video generation approaches are typically restricted to synthesizing dynamics through explicit control signals. These control signals, either abstract by nature (e.g., text prompts (Gupta et al., 2024), moving keypoints (Wu et al., 2024)) or difficult to acquire (e.g., dense depth (Chen et al., 2023c) or segmentation maps (Han et al., 2022)), limit user expressiveness or experience in interactive video creation. Unlike previous approaches that focused on gaming (Bruce et al., 2024; Valevski et al., 2024) or domain-specific content like dancing or talkinghead videos (Siarohin et al., 2021; Song et al., 2019), we target general videos in this work, which presents significant challenges to video generation because actions in general videos are naturally contextualized and highly complex. Consider Figure 1, the same action concept can appear drastically different depending on the subject performing the action, the object being acted upon, and the surrounding environment involved. This misalignment, along with inherent complexities in videos such as camera view changes and motion blurs, poses significant challenges for transferring action concepts between contexts. On one hand, this fundamentally differentiates our work from conventional action transfer and retargeting (Ren et al., 2021; Siarohin et al., 2019, 2021), which typically assume strict alignment between both actions and contexts of reference and target scenes. On the other hand, this misalignment prevents us from mining paired training data as in previous methods (Ren et al., 2021; Siarohin et al., 2021), making it challenging for model training. To address these challenges, we propose three key designs. First, we enable deep understanding of complex actions and scenes in demonstration video by leveraging the advancement of state-of-the-art video foundation models (Bardes et al., 2024; Wang et al., 2022, 2024c; Zhao et al., 2024a). Second, we adopt the form of implicit action latents to condition the generation process in place of explicit control signals to maximize the flexibility and expressiveness required by general videos. Third, we propose self-supervised training paradigm for learning Video Creation by Demonstration. From single video, we sample context frame and its following clip as the demonstration to guarantee the context-action alignment, and task the model to generate the same clip. With these designs, we unlock new capabilities for video generation and manipulation beyond explicit signals, allowing for more nuanced and flexible control of video content. In this paper, we introduce ùõø-Diffusion, novel two-stage training approach. In the first stage, we extract spatiotemporal semantic representations of demonstration video using pre-trained video foundation model (Zhao et al., 2024a). Directly conditioned on such representations during generation model training would lead to degenerate solutions where the context image is ignored. To tackle this issue, we learn an appearance bottleneck module on top of the video representations, by which we are able to extract action latents with minimal appearance/contextual Video Creation by Demonstration information. In the second stage, we train diffusion model to predict future frames given the action concepts. By conditioning the generation model on both the extracted control latents and context image, we generate videos with realistic motion that seamlessly integrates with the In contrast with specified action and context. supervised methods that require paired training data, ùõø-Diffusion can potentially leverage large amounts of unlabeled video data for scaling. We demonstrate the effectiveness of our approach through extensive experiments on diverse video datasets. Our method is evaluated in terms of visual quality, action transferability, and context consistency through both machine and human evaluations. Notably, ùõø-Diffusion is capable of generating high-fidelity videos spanning wide range of action concepts, from everyday activities and ego-centric perspectives to complex robotic movements. We also show that creating videos by visual demonstration yields better controllability and concept transferability compared to text control. Furthermore, we are able to use different demonstration videos simply concatenated together to drive the generation of coherent sequence, indicating the potential of leveraging ùõø-Diffusion as an alternative to generative interactive environment (Bruce et al., 2024). In summary, we make the following contributions. (i) We introduce Video Creation by Demonstration, new creation experience for controllable video generation, which enables directly using videos as driving control signals for transferring action concepts. (ii) To the best of our knowledge, we are the first to leverage out-of-thebox video foundation models for latent control of video generation. (iii) We propose novel selfsupervised approach for model training, which achieves compelling controllable video generation results. Although some limitations remain (e.g., the result might not fully follow physical laws under complex scenes), we hope the proposed paradigm for controllable video generation will open new doors to interactive world simulation. 2. Related Works 2.1. Video Generation Recent years have witnessed significant progress in video generation (Blattmann et al., 2023; Brooks et al., 2024; Gupta et al., 2024; Harvey et al., 2022; Ho et al., 2022a,b; Polyak et al., 2024; Singer et al., 2023), with range of methods for controlling the generated videos, including text-to-video (Ho et al., 2022a; Ramesh et al., 2021; Singer et al., 2023), image-to-video (Singer et al., 2023; Zhao et al., 2018), image+text-tovideo (Gupta et al., 2024; Wang et al., 2024d; Xiang et al., 2024), and image+video-to-video (I+V2V) (Zhao et al., 2024b). Specifically, I+V2V methods include animating still input image to create video, conditioned on another video itself or some signals derived from another video. Our work, Video Creation by Demonstration, falls into the I+V2V category, where we aim to generate videos that continues from the context image while integrating the action concepts from the demonstration video. (e.g., signals control Existing I+V2V techniques typically extract keyeither explicit points (Chang et al., 2024)) or implicit control signals (e.g., learned embeddings (Bruce et al., 2024)) from the condition video to influence the generated motion. These signals are then used alongside the initial frame to achieve I+V2V. common method that existing works have used to tackle I+V2V is to extract explicit control signals from the demonstration video, and use those to animate the input context frame. These extracted signals include text prompts (Hu et al., 2023; Wang et al., 2024b; Xiang et al., 2024; Yang et al., 2024), depth maps and edge maps (Chen et al., 2023c; Wang et al., 2024b), box or point tracks (Chen et al., 2023b; Li et al., 2024; Wang et al., 2024a,d; Wu et al., 2024), human keypoints (Hu, 2024; Park et al., 2024), or segmentation masks (Davtyan and Favaro, 2022; Huang et al., 2022; Xiao et al., 2024). This also includes works that learn to leverage multiple types of control signals (e.g., sketch, depth, style), such as (Chen et al., 2023c; Wang et al., 2024b)."
        },
        {
            "title": "Compared to extracting explicit signals from",
            "content": "3 Video Creation by Demonstration the control video, extracting implicit signals is comparatively less well-explored. One example is MotionDirector (Zhao et al., 2024b), which finetunes video diffusion model for each demonstration video at test-time to generate the reference motion pattern that contains textures consistent with given image. In comparison, our ùõø-Diffusion aims to generate videos that naturally continue from the given context image, with no optimization during inference. Another example is Genie (Bruce et al., 2024), which learns reusable latent action codebook from video games. In addition to interactive generation where sequence of discrete latent codes are inputted by users, these action codes can also be extracted from demonstration video to control generation. In comparison, ùõø-Diffusion extracts and utilizes action concepts that are more abstract than consecutive frame changes, while applying to the real-world visual domain. 2.2. Modeling Motion and Action key challenge in our framework is to effectively capture the action concept from the condition video, and transfer it to different initial context. One line of prior work has studied task named action transfer or action re-targeting (Ren et al., 2021; Siarohin et al., 2019, 2021; Song et al., 2019), which often involves decomposing videos into motion and content representation, then transferring learned motion representations from one video into the content of another. While these methods have shown promising results in transferring motion patterns, they often operate under the assumption of certain degree of alignment between the source and target videos (e.g., humans facing camera at the same distance). This alignment allows the low-level motion information to effectively represent higher-level actions. However, this assumption does not hold in our setting of Video Creation by Demonstration, where the condition video and the target context can be misaligned. In such cases, directly transferring low-level motion patterns may not accurately convey the intended action concept (e.g., video of robot arm closing the top open drawer, to robot arm closing the bottom open drawer). 2.3. Video Foundation Models The emergence of powerful video foundation models, such as VideoPrism (Zhao et al., 2024a), InternVideo (Wang et al., 2022, 2024c), and VJEPA (Bardes et al., 2024), has led to new directions for video understanding. These models are trained on massive datasets, learning to capture both low-level visual features and high-level semantic concepts. Unlike traditional video models that often focus on specific tasks like action recognition or object tracking, video foundation models are designed to be robust and adaptable, enabling them to be used for wide range of downstream tasks (Yuan et al., 2024). One of the key strengths of video foundation models is their ability to extract rich representations that capture both action and context information. This is demonstrated through their strong performance on various video understanding tasks, even when the foundation model is frozen. For example, VideoPrism (Zhao et al., 2024a) has shown state-of-the-art results on various tasks such as video captioning, question answering, and action localization using frozen features. To the best of our knowledge, we are the first to study leveraging video foundation models for controlling I+V2V. 3. Methodology 3.1. Task Formulation For the proposed task of Video Creation by Demonstration, the input is context image ùêº providing contextual information and demonstration video ùëâ providing the control signal for generation. The goal is to generate video ÀÜùëâ that naturally continues from the context image ùêº and carries out the action concepts in similar manner as those found in the demonstration video ùëâ (Figure 1). We only consider this task valid when the action concepts are compatible with the input context. For instance, we do not target simulating cutting action in context image without any hands in it. 4 Video Creation by Demonstration Figure 2 (a) Overview of ùõø-Diffusion. The context frame ùêº is provided to the generation model along with the action latents ùõøùëâ extracted from the demonstration video ùëâ. (b) Extracting action latents. spatial-temporal vision encoder is applied to extract temporally-aggregated spatiotemopral representations from an input video ùëâ, with ùë° denoting the temporal dimension. In parallel, spatial vision encoder extracts per-frame representations from ùëâ, which is aligned to by feature predictor as h. The appearance bottleneck then computes the action latents ùõøùëâ by subtracting the aligned spatial representations from the spatiotemporal representations. 3.2. Method Overview Figure 2(a) shows the overview of our proposed model ùõø-Diffusion. The generation model takes context image ùêº and control latents extracted from demonstration video ùëâ as inputs, and outputs desired video ÀÜùëâ. We apply vision encoders to compute spatiotemporal semantic representations of ùëâ, and one of our key designs is an appearance bottleneck applied on top to extract action-rich latents with minimal preservation of appearance information to condition the generation process (Section 3.3). Conditioned on the bottlenecked control latents, ùõø-Diffusion is trained in self-supervised manner, where both the context image ùêº and the demonstration video ùëâ are sampled from the same video, with ùêº being starting frame followed by ùëâ, and the generation model is tasked to reconstruct ùëâ as the target ÀÜùëâ (Section 3.4). 3.3. Extracting Action Latents Figure 2(b) illustrates our design for extracting action control latents. Given demonstration video ùëâ, we apply pre-trained video encoder to extract its temporally-aggregated semantic representations ‚Ñùùëá ùëÅ ùê∑, where ùëá, ùëÅ, and ùê∑ represent the temporal, spatial, and feature dimension, respectively. Such representations typically is entangled with both appearance and action information, from which we propose to learn an appearance bottleneck to extract the desired actionrich control latents ùõøùëâ. The appearance bottleneck module consists of feature predictor and removal operator. The feature predictor takes per-frame representations extracted from each frame of ùëâ using spatial encoder, and computes the best-effort approximation of the temporallyaggregated representations independently for each frame. The removal operator then subtracts the output of from the to obtain the actionrich control latents ùõøùëâ. Our appearance bottleneck is designed following two principles: (i) the feature predictor extracts appearance representations from single frame; (ii) the extracted appearance representations are compatible with the temporallyaggregated representations z. Principle (i) guarantees that extracts minimal action information, which is collectively defined by multiple consecutive frames. Principle (ii) makes the design of the removal operator easy. Based on these two principles, we formulate as per-frame estimator that minimizes the difference between per-frame feature zùë° and hùë° over dataset as = arg min ùëá 1 ùëâ ùë°=0 zùë° hùë° 1, (1) where zùë° ‚ÑùùëÅ ùê∑ denotes the ùë°-th per-frame slice of z, and hùë° denotes the feature of the ùë°-th frame 5 Video Creation by Demonstration from the input video ùëâ predicted by P. Intuitively, Equation (1) learns that tries to reconstruct temporally-aggregated information based on individual frame with best effort. Without having access to the inter-frame action information, only captures the contextual appearance information. Through reconstruction, hùë° is aligned to the zùë° space, and the removal operator can be simply designed as subtraction operation. In practice, we first extract visual features using spatial encoder from each frame. takes these visual features instead of raw frames as input, which makes its learning easier in practice. The action control latents ùõøùëâ are then computed as ùõøùëâ = [h0, h1, . . . , hùëá 1], (2) where [] is concatenation operation. In Equation (2), ùõøùëâ, the difference between the temporally-aggregated and per-frame information, represents the collective temporal surprisal that models the action concepts from the demonstration video, retrievable only through sequence of frames. 3.4. Training ùõø-Diffusion We train the generation model in selfsupervised manner. During training, we sample ùëá + 1 frames ùë£0:ùëá from video ùëâ in the training set, where ùë£ùë° denotes the ùë°-th frame of ùëâ. The first frame ùë£0 is used as the context image ùêº, and the following frames ùë£1:ùëá are used as the reconstruction target ÀÜùëâ. ùë£1:ùëá are also used as the demonstration video, and the control latents are also extracted using the vision encoders and appearance bottleneck (Section 3.3). In practice, we apply additional action-preserving random augmentations on ùë£1:ùëá into ùëâ before extracting the action latents ùõøùëâ to further reduce appearance leakage. The augmentations, such as random spatial cropping, result in misalignment and disparity between the demonstration video and the target output during training, making it harder for to copy the appearance information directly from the demonstration video. is trained to predict ùë£1:ùëá from ùë£0 and ùõøùëâ by minimizing the loss = arg min ùë¢ (G(ùë£0, ùõøùëâ ), ùë£1:ùëá ) . (3) The recent success of diffusion models for generating high quality videos (Brooks et al., 2024; Gupta et al., 2024; Polyak et al., 2024) motivates us to adopt latent diffusion models (LDMs) as G. We first employ tokenizer to compress both ùë£0 and ùë£1:ùëá into low dimensional latent space. The LDM then takes the corrupted latents of ùë£1:ùëá together with other conditions as inputs and is trained with denoising loss function L. Following the practice in Gupta et al. (2024); Salimans and Ho (2022), we use the velocity as the prediction target of L. During inference, generates videos by iteratively denoising samples drawn from noise distribution. 3.5. Implementation Details Vision encoders. We adopt the video foundation model VideoPrism (Zhao et al., 2024a) (Base, 0.1B parameters) as the vision encoders . VideoPrism adopts the factorized encoder architecture design from ViViT (Arnab et al., 2021), which first computes the spatial representations per frame and then temporally aggregates each spatial location to output the final spatiotemporal representations. This factorization in space and time allows the appearance bottleneck to directly take the per-frame spatial representations in VideoPrism as input, which reduces both training and inference cost. VideoPrism takes 16 288 288 frames as input and outputs 16 16 16 768 spatiotemporal representation tensor. Appearance bottleneck feature predictor. The per-frame feature predictor is constructed as stack of 4 Transformer encoder blocks from ViViT (Arnab et al., 2021) with 1024 hidden dimensions and 8 heads for multi-head attention. takes the intermediate VideoPrism spatial encoder output for particular frame ùë£ùë° as input, and predicts its associated frame representations hùë°. During training, we adopt reconstruction objective with L1 loss and trained for 30k iterations with the Adam optimizer (Kingma, 2015) with 104 base learning rate and 4.5 102 weight decay. Video Creation by Demonstration Video generation model. For the video generation model G, we adopt WALT (Gupta et al., 2024) (Large, 0.3B parameters). is trained to generate videos of ùëá = 16 frames with 128128 spatial resolution. The context image ùêº is natively passed into the architecture as the image for conditional generation. The control latents ùõøùëâ from demonstration video ùëâ is projected into sequence of 2048-dimensional latent vectors in place of the original text embeddings for conditioning. We initialize with pre-trained I+T2V checkpoint, and fine-tune on downstream datasets in 500k iterations. During inference, we use classifierfree guidance scale of 1.25 (Ho and Salimans, 2022). 4. Experiment 4.1. Datasets We conduct our experiments primarily on three datasets, namely Epic Kitchens 100 (Damen et al., 2018), Something-Something v2 (SSv2) (Goyal et al., 2017), and Fractal (Brohan et al., 2022). We choose these datasets because of their rich human-object interactions and state changes in the video clips, making them good testbed to demonstrate the proposed action concept transferring. Epic Kitchens 100 is an egocentric video dataset, mostly focuses on kitchen scene. It features 55 hours of videos with ground-truth annotations of fine-grained actions. SomethingSomething v2 is collection of labeled video clips of humans performing pre-defined basic actions with everyday objects. It contains 174 labeled actions in total with both first-person-view and third-person-view video clips and rich contents depicting human-object interactions. Lastly, Fractal is real-world robotics manipulation dataset firstly introduced in (Brohan et al., 2022). It contains around 130k episodes over 700 tasks. Each task is annotated by simple natural language, such as pick redbull can from middle drawer and place on counter. To train our method, we first train shared per-frame feature predictor on mixture of the training split of all the datasets mentioned above. We additionally include Ego4D (Grauman et al., 2022) during this training stage due to its rich and diversified content. Our video generation model is then trained on each of the three datasets individually, yielding one model for each dataset. The proposed Video Creation by Demonstration task requires pair of <context image, demonstration video>, and trivially picking one image paired with random video during inference might lead to incompatible action concepts. This is because the action conducted in the demonstration video could be completely infeasible to execute in the context image. To mitigate this issue, we reorganize the existing datasets and curate meaningful pairs for validation. Specifically, for each dataset, we first pick the examples with top-10 most frequent labels. Within each label category, we then randomly sample 1k video pairs. In each video pair (A, B), we take video as the demonstration video and the first frame from video as the context image. The generated video is evaluated against video as we assume the videos with the same labeled description contains transferable action concept. We use this curated large scale evaluation dataset for machine evaluation. Additionally, we manually select small set of video pairs by carefully verifying the transferability of their action concepts. Please refer to appendix for details of this selection process. These selected video pairs are used for human evaluation. 4.2. Evaluation Setup We conduct human and machine evaluations to study the effectiveness of different methods. Machine evaluation. Pre-trained image/video understanding models are leveraged to quantify the quality of generated examples in machine evaluation. It brings scalability advantages over the human evaluation which can be used to evaluate large cohort of generated examples. In this study, we apply three quantitative machine evaluation metrics to evaluate the action concept transfer quality in different aspects. We apply the commonly used Fr√©chet Video Distance (FVD) (Unterthiner et al., 2018) to measure the overall generation fidelity. However, FVD only reflects the distribution shift between generated and reference videos and cannot measure whether the genVideo Creation by Demonstration Table 1 Ablation study on ùõø-Diffusion. We ablate the proposed appearance bottleneck, and consider alternative variants that serve as competitive baselines. For the applied bottleneck, None indicates no bottleneck is placed while Temp. Norm. indicates temporal normalization applied to the spatiotemporal features. We evaluate on Something-Something v2, Epic Kitchens 100, and Fractal for comparisons in terms of generation quality (FVD) and context-generation alignment via both embedding cosine similarity (ES) and retrieval Hit@k (%). Bottleneck SSv2 FVD () ES () Hit@100/500/1k () Epic Kitchens 100 FVD () ES () Hit@100/500/1k () Fractal FVD () ES () Hit@100/500/1k () None Temp. Norm. Ours 47.3 38.4 38.0 0.838 0.846 0.853 57.7 / 74.3 / 81.7 63.4 / 79.1 / 85.4 66.9 / 81.8 / 87. 46.2 41.7 42.3 0.847 0.850 0.854 38.9 / 58.9 / 69.1 41.7 / 61.3 / 70.7 44.9 / 64.6 / 74.2 41.9 42.1 44.9 0.902 0.906 0.907 58.0 / 75.7 / 82.8 62.2 / 78.2 / 85.0 63.0 / 78.8 / 85. Table 2 Human evaluation preference rate on ùõø-Diffusion. We compare against two baseline methods in condition video generation that are relevant our task. We ask human raters to evaluate the performance in terms of visual quality (VQ), action transferability between demonstration and generated videos (AT), and context image consistency (CC). Method Condition Epic Kitchens 100 VQ () AT () CC () VQ () AT () CC () VQ () AT () CC () Fractal SSv vs. WALT vs. MotionDirector image+text image+text+video 0.74 0.86 0.77 0.98 0.70 0.91 0.70 0. 0.83 0.98 0.65 0.98 0.82 0.96 0.80 1.00 0.74 0.97 erated video faithfully follow action concept of reference video. To complement this, we apply I3D (Carreira and Zisserman, 2017) to compute features from the generated and reference videos and report their averaged embedding cosine similarity (ES). Finally, we also construct retrievalbased evaluation to measure the similarity between the generated and reference video in the context of all reference videos. We use generated video features and attempt to retrieve the corresponding reference video via embedding cosine similarity. The retrieval performance is evaluated in terms of hit-rate at 100, 500, and 1k, which correspond to 1%, 5%, and 10% of the retrieval set, respectively. Human evaluation. On the other hand, human evaluation captures the human preference on the task. In this studies, we hire five human raters who are trained to assess the overall visual quality (VQ), action transferability (AT) and content consistency (CC) of the generated videos. The detailed description of each rubrics could be found in the appendix. We curate 20 video examples from each dataset, resulting in 60 examples in total, as the human evaluation set. In each turn, reference image, demonstration video, and two generated videos (one from the proposed method and the other from the method to compare) are provided to the rater. The rater is then asked to choose the better video from the two generated ones based on each of the three rubrics mentioned above. We then compute the preference rate of human rater favoring our method against the baseline under each dataset and rubric as the evaluation metric. 4.3. Ablation Study In this section, we ablate the design choice of our training algorithm first. We use large scale evaluation data as described in Section 4.1 and machine metrics as described in Section 4.2 for the ablation study. The results are presented in Table 1. We compare two variant bottleneck designs with our proposed appearance bottleneck. None stands for using features as the condition signal and we do not apply any bottleneck training on the features. In the temporal normalization setup, following Xiao et al. (2024), the condition signal is obtained by firstly applying average pooling along the temporal dimension of and then subtracting it from z. From the results, we observe that ùõø-Diffusion Video Creation by Demonstration In our study, we take WALT (Gupta et al., 2024) as the representative of video generation conditioned on image and text. For fair comparison, we fine-tune it on each dataset individually with ground truth captions. We also compare the proposed method with MotionDirector (Zhao et al., 2024b) which uses image, text, and video as conditional signals. When generating videos, MotionDirector learns per-instance appearance and motion from context image and demonstration video via LoRA fine-tuning (Hu et al., 2022), and generate videos based on the given text prompt. For both WALT and MotionDirector, we use the groundtruth caption label as the prompt during video generation. Please refer to the appendix for more details of these two baseline. We report the human evaluation results of ours against two baselines in Table 2. The numbers indicate the preference rate of human rater favoring our method against the baseline under each dataset and rubric. We notice that ùõø-Diffusion is clearly favored by the human raters across all the datasets and rubrics, which underlines its superior performance. We compare the generated videos by our methods and other baselines in Figure 4. We note that MotionDirector would fail in many cases, possibly due to its difficulties in optimizing the LoRA parameters. WALT, although preserves content consistency well, could carry out in-genuine action to the demonstration video. This could due to the limited expressiveness in text description on demonstration videos. On our results, we show both faithful action execution and content preservation in the generated videos. Auto-regressive generation. We further explore auto-regressively generating coherent video that starts from single context image, conditioned on sequence of demonstration videos, as shown in Figure 5. At each step, the generation model uses the last frames from the previous step as the context. Each conditioning video is selected from different scene, but overall they demonstrate complete rollout of complex task. We show that by sequentially applying the action latents from the demonstration videos, we are able to auto-regressively simulate the task execution in one consistent environment. Figure 3 Qualitative results for bottleneck ablation on the Something-Something v2 dataset (Goyal et al., 2017). Applying no or temporal normalization bottleneck suffers from appearance leakage, while generation based on our appearance bottleneck preserves the input context. performs better than two counter-parts. On instance-wise metrics, retrieval hit-rate (Hit@k) and embedding cosine similarity (ES), ùõøDiffusion consistently outperforms baselines with none or temporal normalization as the bottleneck. This indicates an improved action concept transferability with our proposed appearance bottleneck design. On the other hand, we do not observe consistent win among the three methods in terms of visual quality. This shows that while increasing the action faithfulness and temporal consistency, the proposed method does not hurt the visual quality of the generated videos. Figure 3 further shows visual comparisons for different bottleneck designs. We notice that both without bottleneck and temporal norm. bottleneck suffer from appearance leakage; in contrast, the proposed appearance bottleneck successfully retain action concepts from the demonstration video while minimizing the appearance information. 4.4. Main Results Comparison with baselines. Other than video, text is also natural format to describe actions. Video Creation by Demonstration Figure 4 Qualitative comparisons of ùõø-Diffusion against MotionDirector (Zhao et al., 2024b) and WALT (Gupta et al., 2024) on (a) Something-Something v2 (Goyal et al., 2017), (b) Epic Kitchens 100 (Damen et al., 2018), and (c) Fractal (Brohan et al., 2022) datasets. 5. Conclusion"
        },
        {
            "title": "Broader Impact",
            "content": "In this work, we introduce Video Creation by Demonstration, video creation experience that enables users to generate videos by providing an initial context frame and demonstration video for action concept conditioning. The main challenge in this task is how to handle the misalignment of appearances and contexts between the demonstration video and the context image during the transferring of action concepts. To address this, we propose to leverage the video foundation model to provide semantic representations for actions and contexts in videos and build an appearance bottleneck on top of them to extract latent control signals with rich action concept and minimum appearance information. Then, video generation model is learned to take the context image and the control signals as inputs and generate videos that continue from the context image and carry out the action concept in the similar manner as shown in the demonstration video. Our extensive experiments on three separate datasets using both machine and human evaluations demonstrate the effectiveness of our proposed ùõø-Diffusion. One of the limitations is that ùõø-Diffusion at its current form does not always strictly preserves physical realism under complex scenes, which needs further exploration and can be potentially alleviated by scaling up the generation model. Societal impact. Our proposed Video Creation by Demonstration improves the ease of interactive generation workflow, by offering novel video creation experience that bridges the gap between abstract (less controllable) and detailed (hard to obtain) control signals. In addition, ùõø-Diffusion demonstrates capabilities of world modeling beyond the transfer of action concepts, by simulating possible effects of these actions on the entire scene (e.g., actions would cause additional interactions in the given context that is not captured by the demonstration video.) Potential negative impact. Our work does not introduce any negative societal impacts beyond those commonly found in controlled video generation. These may include reinforcement of existing dataset bias, misuse of generated content for creating misleading or inappropriate materials, and privacy concerns involving individuals without their explicit consent. We highlight the importance of vigilance and responsible implementation to mitigate these impacts and guarantee ethical use."
        },
        {
            "title": "Acknowledgements",
            "content": "We sincerely thank Anqi Huang at UC Irvine, and Boqing Gong, Bohyung Han, David Hendon, Hex10 Video Creation by Demonstration Figure 5 Auto-regressive generation controlled via concatenation of three different demonstration videos of varying lengths. The sequence of demonstrated action concepts (picking something from drawer and placing it on the table, closing drawer, and opening drawer) are coherently transferred to the input context. iang Hu, Jimin Pi, Luke Friedman, Luming Tang, Mikhail Sirotenko, Ming-Hsuan Yang, Mingda Zhang, Sanghyun Woo, Willis Ma, Yukun Zhu, Yuxiao Wang, and Ziyu Wan at Google DeepMind for their feedback, discussion, and support. We also thank Florian Schroff, Huisheng Wang, Caroline Pantofaru, and Tomas Izo for their leadership support for this project. Video Creation by Demonstration"
        },
        {
            "title": "References",
            "content": "E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. Storkey, T. Pearce, and F. Fleuret. Diffusion for world modeling: Visual details matter in Atari. NeurIPS, 2024. A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luƒçiƒá, and C. Schmid. ViViT: video vision transformer. In CVPR, 2021. A. Bardes, Q. Garrido, J. Ponce, X. Chen, M. Rabbat, Y. LeCun, M. Assran, and N. Ballas. Revisiting feature prediction for learning visual representations from video. TMLR, 2024. A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. RT-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, Video generation modURL https://openai.com/research/ T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. els as world simulators. video-generation-models-as-world-simulators. OpenAI Blog, 2024. J. Bruce, M. D. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps, et al. Genie: Generative interactive environments. In ICML, 2024. J. Carreira and A. Zisserman. Quo vadis, action recognition? new model and the Kinetics dataset. In CVPR, 2017. D. Chang, Y. Shi, Q. Gao, H. Xu, J. Fu, G. Song, Q. Yan, Y. Zhu, X. Yang, and M. Soleymani. MagicPose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In ICML, 2024. T. Chen, R. Zhang, and G. Hinton. Analog Bits: Generating discrete data using diffusion models with self-conditioning. In ICLR, 2023a. T.-S. Chen, C. H. Lin, H.-Y. Tseng, T.-Y. Lin, and M.-H. Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023b. W. Chen, Y. Ji, J. Wu, H. Wu, P. Xie, J. Li, X. Xia, X. Xiao, and L. Lin. Control-A-Video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023c. D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. Scaling egocentric vision: The EPIC-KITCHENS dataset. In ECCV, 2018. A. Davtyan and P. Favaro. Controllable video generation through global and local motion dynamics. In ECCV, 2022. R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In ICCV, 2017. K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4D: Around the world in 3,000 hours of egocentric video. In CVPR, 2022. Video Creation by Demonstration A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, F.-F. Li, I. Essa, L. Jiang, and J. Lezama. Photorealistic video generation with diffusion models. In ECCV, 2024. L. Han, J. Ren, H.-Y. Lee, F. Barbieri, K. Olszewski, S. Minaee, D. Metaxas, and S. Tulyakov. Show me what and tell me how: Video synthesis via multimodal conditioning. In CVPR, 2022. W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood. Flexible diffusion modeling of long videos. In NeurIPS, 2022. J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al. Imagen Video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In NeurIPS, 2022b. A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado. GAIA-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. L. Hu. Animate Anyone: Consistent and controllable image-to-video synthesis for character animation. In CVPR, 2024. J. Huang, Y. Jin, K. M. Yi, and L. Sigal. Layered controllable video generation. In ECCV, 2022. D. P. Kingma. Adam: method for stochastic optimization. In ICLR, 2015. Y. Li, X. Wang, Z. Zhang, Z. Wang, Z. Yuan, L. Xie, Y. Zou, and Y. Shan. Image Conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. F. Meng, J. Liao, X. Tan, W. Shao, Q. Lu, K. Zhang, Y. Cheng, D. Li, Y. Qiao, and P. Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. G. Y. Park, H. Jeong, S. W. Lee, and J. C. Ye. Spectral motion alignment for video motion transfer using diffusion models. arXiv preprint arXiv:2403.15249, 2024. A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang, et al. Movie Gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In ICML, 2021. J. Ren, M. Chai, O. J. Woodford, K. Olszewski, and S. Tulyakov. Flow guided transformable bottleneck networks for motion retargeting. In CVPR, 2021. 13 Video Creation by Demonstration C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. T. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. A. Siarohin, S. Lathuili√®re, S. Tulyakov, E. Ricci, and N. Sebe. First order motion model for image animation. In NeurIPS, 2019. A. Siarohin, O. J. Woodford, J. Ren, M. Chai, and S. Tulyakov. Motion representations for articulated animation. In CVPR, 2021. U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al. Make-A-Video: Text-to-video generation without text-video data. In ICLR, 2023. Y. Song, J. Zhu, D. Li, X. Wang, and H. Qi. Talking face generation by conditional recurrent adversarial network. In IJCAI, 2019. Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. J. Wang, Y. Zhang, J. Zou, Y. Zeng, G. Wei, L. Yuan, and H. Li. Boximator: Generating rich and controllable motions for video synthesis. arXiv preprint arXiv:2402.01566, 2024a. X. Wang, H. Yuan, S. Zhang, D. Chen, J. Wang, Y. Zhang, Y. Shen, D. Zhao, and J. Zhou. VideoComposer: Compositional video synthesis with motion controllability. In NeurIPS, 2024b. Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu, Z. Wang, et al. InternVideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. Y. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, et al. InternVideo2: Scaling video foundation models for multimodal video understanding. In ECCV, 2024c. Z. Wang, Z. Yuan, X. Wang, Y. Li, T. Chen, M. Xia, P. Luo, and Y. Shan. MotionCtrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH, 2024d. W. Wu, Z. Li, Y. Gu, R. Zhao, Y. He, D. J. Zhang, M. Z. Shou, Y. Li, T. Gao, and D. Zhang. DragAnything: Motion control for anything using entity representation. In ECCV, 2024. J. Xiang, G. Liu, Y. Gu, Q. Gao, Y. Ning, Y. Zha, Z. Feng, T. Tao, S. Hao, Y. Shi, et al. Pandora: Towards general world model with natural language actions and video states. arXiv preprint arXiv:2406.09455, 2024. Z. Xiao, Y. Zhou, S. Yang, and X. Pan. Video diffusion models are training-free motion interpreter and controller. In NeurIPS, 2024. M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel. Learning interactive real-world simulators. In ICLR, 2024. Video Creation by Demonstration L. Yuan, N. B. Gundavarapu, L. Zhao, H. Zhou, Y. Cui, L. Jiang, X. Yang, M. Jia, T. Weyand, L. Friedman, et al. VideoGLUE: Video general understanding evaluation of foundation models. TMLR, 2024. L. Zhao, X. Peng, Y. Tian, M. Kapadia, and D. Metaxas. Learning to forecast and refine residual motion for image-to-video generation. In ECCV, 2018. L. Zhao, N. B. Gundavarapu, L. Yuan, H. Zhou, S. Yan, J. J. Sun, L. Friedman, R. Qian, T. Weyand, Y. Zhao, et al. VideoPrism: foundational visual encoder for video understanding. In ICML, 2024a. R. Zhao, Y. Gu, J. Z. Wu, D. J. Zhang, J.-W. Liu, W. Wu, J. Keppo, and M. Z. Shou. MotionDirector: Motion customization of text-to-video diffusion models. In ECCV, 2024b. 15 Video Creation by Demonstration"
        },
        {
            "title": "Appendix",
            "content": "A. Author Contributions We list authors alphabetically by last name. Please direct all correspondence to Ting Liu (liuti@ google.com) and Long Zhao (longzh@google.com). Core Contributors Ting Liu: Video Creation by Demonstration (VCBD) concept, project leadership, dataset curation, evaluation, action model research, model demo. Jennifer J. Sun: Generation model research, infrastructure, evaluation. Yihong Sun: Generation model research, action model research, infrastructure, evaluation, model demo. Liangzhe Yuan: VCBD concept, action model research, evaluation. Long Zhao: Project leadership, generation model research, action model research, evaluation. Hao Zhou: Action model research, evaluation, infrastructure."
        },
        {
            "title": "Partial Contributors and Advisors",
            "content": "Bharath Hariharan: Technical advice. Xuhui Jia: Technical advice. Yandong Li: Technical advice."
        },
        {
            "title": "Sponsors",
            "content": "Hartwig Adam: Strategic advice. B. Improving Generation Quality To support demonstration video with length longer than the generation length ùëá = 16, we follow the auto-regressive generation setup in WALT (Gupta et al., 2024). During model training, we swap out the input context image with probability of 0.5 and replace it with multiple consecutive frames to encourage smoother continual generation. At inference time, we cut long demonstration video into multiple segments as needed, each of length ùëá. This allows the generated segment to remain aligned with the appropriate demonstration segment. Here, the first segment is generated from the input context image, while the subsequent segments are conditioned on the last 4 generated frames from previous segment. During training, we enable self-conditioning (Chen et al., 2023a) with probability of 0.9 and randomly mask out the control signal with probability of 0.2. At inference time, we adopt classifierfree guidance consistent with Saharia et al. (2022) with guidance weight of 1.25 and drop both the self-conditioning and control signal for the unconditional generation. C. Human Evaluation Setup C.1. Prompt Selection For all three datasets, we consider the demonstration video to be between 16 to 24 frames at 12 frames per second (FPS) (Something-Something v2 and Epic Kitchens 100) or 10 FPS (Fractal). For 16 Video Creation by Demonstration Table 3 Human evaluation instructions and rubrics. Instructions Given an initial image (context frame) and demonstration video (reference video), the task of Video Creation by Demonstration is defined as to create plausible video clip initiating from the context frame and contains similar content dynamics as in the demonstration video. In this user study, you will be provided reference video, context frame, and two generated videos from two methods side-by-side in each turn. You would assess the generated video quality on the following three rubrics and determine which one better recreates the actions and dynamics of given demonstration video, while also appearing realistic and continuous from context image. Rubrics Descriptions Visual Quality How realistic does the video look compared to real-world video? Consider factors like smooth motion, accurate details, and believable physics. Choose the video that is better. Action Transferability How well does the generated video recreate the actions and movements shown in the reference video? Note that the action concept can include the camera motions. Choose the video that is better. Frame Continuity How seamlessly does the generated video flow from the provided context frame? Does it look like natural continuation of the scene? Choose the video that is better. Something-Something v2 (Goyal et al., 2017), we select the top-10 action classes. For each action class, we manually select 20 out of 500 randomly sampled pairs for human evaluation. For Epic Kitchens 100 (Damen et al., 2018), we apply automatic filtering to first narrow down the search. For each video, we retrieve top-1 video with the same action label and different participant ID via first-frame CLIP (Radford et al., 2021) similarity. Then, 20 pairs are randomly sampled from top-25 action classes. Finally, 20 examples are manually selected from pool of pairs with CLIP similarity greater than 0.9. For Fractal, we sample 500 same-action pairs uniformly from each of the 8 action classes (defined by the verb and preposition from the associated captions) and conduct manual selection in random order to select 20 pairs for evaluation. The main manual selection principle is to confirm that the action concept in the demonstration video can be potentially applied in reasonable way from the context image. This process is conducted without any considerations of the methods to be evaluated. From each selected video pair, one video is randomly selected as demonstration, and the first of frame of the other video is used as the context image to construct prompt. C.2. Rating Instructions and Rubrics In Table 3, we list the instructions and rubrics shown to the human raters before presenting them with the generated videos for side-by-side comparisons. 17 Video Creation by Demonstration C.3. Additional Details When presenting the visual examples, we show Demonstration Video, Context Image, Video A, and Video in order. As we conduct the user study via Colab, we randomly assign Video to be either ùõø-Diffusion or baseline method. We hired 5 human raters to examine 2 baselines to compare against ùõø-Diffusion on 3 datasets, 20 examples per dataset, and 3 metrics per example. In total, we collected 1800 human preferences. D. Details on Baselines MotionDirector (Zhao et al., 2024b). When testing MotionDirector on Something-Something v2 (SSv2) (Goyal et al., 2017), Epic Kitchens 100 (Damen et al., 2018), and Fractal (Brohan et al., 2022), we use the official code published by the authors at https://github.com/showlab/ MotionDirector. We first train the spatial path with the context image for 300 epochs with input resolution being 384 384. Then we train the temporal path following the 16-frame single video setup. During inference, noise prior of 0.0 is applied. Ground truth caption labels are used as the prompt for both training and inference. WALT (Gupta et al., 2024). We fine-tune WALT on each dataset in accordance with the official procedures and use ground truth captions to tune video generation from context image and text caption. We keep all hyper-parameters the same and fine-tune for 200k steps. During inference, the ground truth caption of the demonstration video is used along with the context image for video generation. As shown in the visualizations, WALT generations, while preserving context consistency, carry out ungenuine action due to limited expressiveness in text descriptions. E. Qualitative Results The original videos for creating Figures 1 and 5 can be found at https://delta-diffusion. github.io. The original videos for creating Figures 3 and 4 can be found at https:// delta-diffusion.github.io/additional.html, along with additional examples. F. Controllability by Demonstration As shown in Figures 6, 7, 8, and 9, we showcase the controllability of ùõø-Diffusion using different demonstration videos for the same context image. The video samples can be found at https: //delta-diffusion.github.io/#cbd. G. Failure Cases As shown in Figure 10, we identify three primary failure modes of our method. In the first row, we show case where the semantics of the action concept in the demonstration videos are not fully carried out. Specifically, the generated object is placed in-between the existing objects instead of next-to them. In the second row, we show case where permanence is not held when the object in the demonstration video undergoes fast appearance changes. Here, fast object rotation causes appearance leakage in the generation. In the third row, we show inconsistent generations where the demonstration videos and context images are mis-matched significantly. On the left, the perspectives of the moving hand are mis-matched and cause another hand in the same demonstrated perspective to be generated. 18 Video Creation by Demonstration Figure 6 Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset (Goyal et al., 2017). 19 Video Creation by Demonstration Figure 7 Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Something-Something v2 dataset (Goyal et al., 2017). 20 Video Creation by Demonstration Figure 8 Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset (Brohan et al., 2022). 21 Video Creation by Demonstration Figure 9 Qualitative results of driving alternative generation from the same context image with different demonstration videos from the Fractal dataset (Brohan et al., 2022). 22 Video Creation by Demonstration Figure 10 Failure cases generated by ùõø-Diffusion."
        }
    ],
    "affiliations": [
        "Cornell University",
        "Google DeepMind"
    ]
}