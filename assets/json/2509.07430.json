{
    "paper_title": "The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward",
    "authors": [
        "Long Li",
        "Jiaran Hao",
        "Jason Klein Liu",
        "Zhijian Zhou",
        "Xiaoyu Tan",
        "Wei Chu",
        "Zhe Wang",
        "Shirui Pan",
        "Chao Qu",
        "Yuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as a proactive solution. We argue that standard RLVR objectives -- both those using the mode-seeking reverse KL-divergence and those forgoing a divergence term entirely -- lack a crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose a fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as a rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights a crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of a divergence measure is a powerful tool for building more general and diverse reasoning models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 0 3 4 7 0 . 9 0 5 2 : r THE CHOICE OF DIVERGENCE: NEGLECTED KEY TO MITIGATING DIVERSITY COLLAPSE IN REINFORCEMENT LEARNING WITH VERIFIABLE REWARD Long Li1, Jiaran Hao1, Jason Klein Liu1, Zhijian Zhou2, Xiaoyu Tan1 Wei Chu1, Zhe Wang3, Shirui Pan3, Chao Qu1,2, Yuan Qi1,2 1INFLY TECH, 2Fudan University, 3Griffith University {seamoke111@gmail.com} https://github.com/seamoke/DPH-RL"
        },
        {
            "title": "ABSTRACT",
            "content": "A central paradox in fine-tuning Large Language Models (LLMs) with Reinforcement Learning with Verifiable Reward (RLVR) is the frequent degradation of multi-attempt performance (Pass@k) despite improvements in single-attempt accuracy (Pass@1). This is often accompanied by catastrophic forgetting, where models lose previously acquired skills. While various methods have been proposed, the choice and function of the divergence term have been surprisingly unexamined as proactive solution. We argue that standard RLVR objectivesboth those using the mode-seeking reverse KL-divergence and those forgoing divergence term entirelylack crucial mechanism for knowledge retention. The reverse-KL actively accelerates this decay by narrowing the policy, while its absence provides no safeguard against the model drifting from its diverse knowledge base. We propose fundamental shift in perspective: using the divergence term itself as the solution. Our framework, Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences (like forward-KL and JS-divergence) to function as rehearsal mechanism. By continuously referencing the initial policy, this approach forces the model to maintain broad solution coverage. Extensive experiments on math and SQL generation demonstrate that DPH-RL not only resolves the Pass@k degradation but improves both Pass@1 and Pass@k inand out-of-domain. Additionally, DPH-RL is more training-efficient because it computes f-divergence using generator functions, requiring only sampling from the initial policy and no online reference model. Our work highlights crucial, overlooked axis for improving RLVR, demonstrating that the proper selection of divergence measure is powerful tool for building more general and diverse reasoning models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated significant success in enhancing the mathematical and coding capabilities of Large Language Models (LLMs) (Yue et al., 2025a; OpenAI, 2023; Li et al., 2024d; Guo et al., 2025). Despite significant successes, critical paradox shadows the field: While RLVR-tuned models consistently improve the probability of generating correct solution in single attempt (Pass@1), they often fail to enhanceand in some cases, actively degradeperformance when multiple solution attempts are permitted (Pass@k), especially when compared to their base models (Yue et al., 2025a). This puzzling observation has given rise to the troubling hypothesis that RLVR may not be endowing models with genuinely new reasoning capabilities, but rather re-weighting and narrowing their focus onto few known reasoning paths at the expense of solution diversity. This discrepancy is often linked to entropy collapse, where the models output distribution narrows, reducing solution diversity. Some approaches tackle this by forgoing KL-divergence term entirely Corresponding author. 1 Figure 1: The left panel shows the performance of model trained exclusively on the Bird dataset, evaluated across different test sets. The X-axis represents the degree of domain and task divergence from the training set. Specifically, Bird is the original SQL dataset, Spider is another dataset for the same SQL task but from different domain, and Math represents the average result for class of mathematical tasks. The datasets are ordered from right to left, showing an increasing divergence from the distribution of our RL training set. The Y-axis shows the relative Pass@k score, calculated as (Current Models Pass@k) / (Base Models Pass@k). The right panel visualizes the distributions of reverse-KL and forward-KL. (Yu et al., 2025), yet still observe decline in diversity. As shown in the left panel of Figure 1, both GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) without KL exhibit significant decline in Pass@k after training. The issue of maintaining exploration is not new; traditional reinforcement learning has rich history of using entropy regularizers, such as in Soft Actor-Critic (SAC), which is based on the soft Bellman equation (Haarnoja et al., 2018a). However, in practice, the temperature coefficient α that balances the reward and entropy is notoriously difficult to tune. Subsequent work on SAC reformulated the problem as dual optimization with lower bound on entropy (Haarnoja et al., 2018b), but this often amounts to transforming one difficult problem into another . Within the LLM domain, entropy regularization has also been explored, but it can create an undesirable tradeoff by negatively impacting Pass@1 performance. As an alternative, separate line of research has focused on directly optimizing the Pass@k metric itself (Mahdavi et al.; Walder & Karkhanis, 2025). Another research line resorts to the external help from the stronger reasoning model like deepseek-R1 to break through the boundary of the base model (Yan et al.; Dong et al., 2025a) In this paper, we approach this from different perspective, focusing on the KL-divergence term used in most RL fine-tuning for LLMs. The standard choice is the reverse-KL divergence, DKL(πθπref ) = Eπθ log πθ (Kazemnejad et al., 2024; Schulman et al., 2017; Shao et al., 2024). πref It is well-established that reverse-KL is mode-seeking (Bishop & Nasrabadi, 2006) (as illustrated in Figure 1), property that theoretically encourages the policy to converge on single high-probability solution, thus suppressing diversity. We validated this with simple experiment in Figure 5: When an RL model is trained with reverse-KL objective, it almost exclusively produces single solution style. However, training with forward-KL objective allows it to generate three or more styles in 60% of cases. Furthermore, we observe that models trained with reverse-KL or no KL term suffer from catastrophic forgettinga gradual decline in accuracy on problems the model could previously solve correctly. As illustrated in Figure 4, after RL training, GRPO and DAPO policy only solve around 85% queries it previously solved correctly. This phenomenon is known challenge in sequential learning paradigms with neural networks, not fundamental flaw of RL, which in principle does not suffer from this in tabular cases (Hamadanian et al., 2025). Based on these observations, we propose several alternatives to the reverse-KL. straightforward example is the forward-KL divergence, DKL(πref πθ). Theoretically, forward-KL is masscovering, which penalizes the policy for failing to assign probability mass to any solution present in the reference distribution, thereby preserving diversity. From practitioners standpoint, the forward-KL objective Eπref log πref , which computes its expectation over samples from the referπθ ence policy, effectively creates an anchor dataset. This forces the model to continuously revisit and rehearse its original knowledge base. In contrast, the reverse-KL objective samples from the current 2 policy πθ , making it impossible to sample and reinforce solution strategy once it has been forgotten. This rehearsal mechanism mirrors how humans periodically review learned material to prevent forgetting. We generalize this concept by extending our analysis from forward and reverse KL to the broader family of f-divergences (Liese & Vajda, 2006; Nowozin et al., 2016), including measures like the Jensen-Shannon (JS) divergence and α-divergence. Our experiments demonstrate that employing mode-covering divergences, specifically forward-KL and JS-divergence, leads to significant improvements in both Pass@1 and Pass@k performance, effectively addressing the diversity and forgetting issues that have challenged the field. Furthermore, Moreover, our method maintains strong performance on out-of-domain tasks, in stark contrast to methods like GRPO and DAPO whose performance collapses. Critically, all these gains are achieved without requiring external knowledge from stronger model. our contributions are three-folds: 1. Systematic Analysis of Diversity Collapse: We provide systematic analysis of the solution diversity collapse in RLVR, identifying the standard reverse-KL divergence as primary cause. We show that its mode-seeking nature not only suppresses Pass@k performanceoften to levels below the base modelbut also exacerbates catastrophic forgetting and leads to poor out-of-domain generalization. 2. Novel Diversity-Preserving Framework (DPH-RL): We reframe the role of the KL-divergence, proposing its use not as mere policy constraint but as an active diversity-preserving mechanism. Based on this principle, we introduce DPH-RL, novel framework that employs mass-covering fdivergences (e.g., Forward-KL and JS-divergence) to serve as rehearsal mechanism, effectively improving both Pass@1 and Pass@k performance. 3.Extensive Empirical Validation: Through extensive experiments on range of models (Llama and Qwen, 7B to 32B) and complex reasoning tasks (mathematics and SQL), we demonstrate the robustness and superiority of DPH-RL. Our method consistently outperforms prior work on both in-domain and out-of-domain benchmarks, successfully mitigating the trade-off between greedy performance and solution diversity."
        },
        {
            "title": "2 RELATED WORK",
            "content": "A more recent development is RLVR (Yue et al., 2025a), promising strategy for boosting LLM reasoning, especially in areas like mathematics and coding (Shao et al., 2024; Guo et al., 2025; Team, 2024; Lambert et al., 2025; Li et al., 2024c). The first model to use RL to effectively encourage large-scale reasoning was OpenAIs o1 (OpenAI, 2024). Following its success, models like DeepSeek R1 (Guo et al., 2025), QwQ (Qwen, 2024), Kimi k1.5 (Team et al., 2025), and Qwen3 (Yang et al., 2025) have been developed with the goal of matching or exceeding its performance. Notably, DeepSeek R1 demonstrated that robust reasoning can arise from optimizing based on outcomes using the GRPO online RL algorithm (Shao et al., 2024). Inspired by these findings, subsequent methods such as VAPO (Yue et al., 2025b), SimpleRLZoo (Zeng et al., 2025), and OpenReasoner-Zero (Hu et al., 2025) have further explored RL-based reasoning. significant challenge that has received attention is policy entropy collapse in RLVR. This occurs in the later stages of training, as the models outputs become too similar and its output diversity declines, hindering its ability to explore new solutions (Wang et al., 2025; Yue et al., 2025a). To address this, methods like DAPO (Yu et al., 2025) and CLIP-Cov (Cui et al., 2025) attempt to solve the issue by adjusting the clipper and controlling entropy during the training process, while other methods (Chen et al., 2025; Dong et al., 2025b) maintain training diversity by controlling the models Pass@k metric. Our work is most closely related to recent studies that explore f-divergences for policy optimization. Notably, Wang et al. (2024); Han et al. (2025) replace the reverse KL-divergence in the offline DPO objective with generalized f-divergence, deriving new family of f-PO algorithms. However, our work is distinct in both its objective and methodology. The f-PO methods operate in an offline setting using human preference data to improve alignment. In contrast, our work operates in an online RLVR setting, using verifiable rewards from reasoning tasks. Our primary goal is not preference alignment, but specifically to solve the Pass@k diversity collapse by leveraging the properties of mass-covering f-divergences."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "3.1 -DIVERGENCE In information theory, an f-divergence is function Df (pq) that measures the difference between two probability distributions and q. Given convex function : R+ such that (1) = 0, the f-divergence is defined as: (cid:90) Df (pq) = q(x)f (cid:19) (cid:18) p(x) q(x) dx This general form allows for the unification of many common divergence measures under single theoretical framework. The condition (1) = 0 ensures that Df (pq) = 0 if and only if = q. Many common divergences are special cases of f-divergences, corresponding to specific choice for the generator function . For instance, the KL divergence, JS divergence are all instances of fdivergences. summary of some prominent f-divergences and their corresponding generator funcIn this paper, we let πref be q, and the formula for f-divergence tions is provided in Table 1. references an improved version (Wang et al., 2024), the definitions for forward-KL and reverse-KL are swapped compared to Wikipedia. Table 1: Summary of some typical f-divergences Df (pq) together with their generator functions (u). For all divergences, the generator function : R+ {+} is strictly convex, lowersemicontinuous, and satisfies (1) = 0. Forward-KL and reverse-KL are the special case of alpha divergence. Name Reverse-KL(α = 0) Forward-KL(α = 1) α-divergence (α / {0, 1}) q(x) dx Divergence Definition Df (pq) (cid:82) p(x) log p(x) (cid:82) q(x) log q(x) p(x) dx (cid:34) (cid:82) q(x) (cid:17)α 1 α(α1) (cid:16) p(x) q(x) (cid:35) α (cid:16) p(x) q(x) (cid:17) dx Generator (u) log log (u1α (1 α)u α)/(α(α 1)) Jensen-Shannon (cid:32) (cid:82) 1 2 p(x) log 2p(x) p(x)+q(x) +q(x) log 2q(x) p(x)+q(x) dx (cid:33) 2 log (u+1) log u+1 2 3.2 MARKOV DECISION PROCESS We consider discounted Markov Decision Process (MDP) defined by the tuple (S, A, P, r, ρ0, γ), where is the state space, is the action space, (ss, a) is the transition probability, r(s) is the reward function, ρ0 is the initial state distribution, and γ [0, 1) is the discount factor. stochastic policy π(as) defines probability distribution over actions for each state. The goal is to maximize the performance objective, i.e., the expected cumulative discounted reward: J(π) = Eτ π (cid:34) (cid:88) (cid:35) γtr(st) t=0 , where s0 ρ0(s0), at π(atst), st+1 (st+1st, at). (1) seminal result from Kakade & Langford (2002) expresses the performance of policy π in terms of an older policy πold using the advantage function Aπold(s, a): J(π) = J(πold) + Eτ π (cid:34) (cid:88) (cid:35) γtAπold(st, at) . t=0 (2) This can be rewritten using the discounted state visitation distribution, ρπ(s) (cid:80) sπ), which gives the probability distribution over states encountered under policy π: t=0 γtP (st = J(π) = J(πold) + (cid:88) sS ρπ(s) (cid:88) aA π(as)Aπold(s, a). (3) Directly optimizing this expression is difficult because the state distribution ρπ depends on the new policy π. Therefore, algorithms like TRPO and PPO optimize surrogate objective Lπold(π) by approximating ρπ with the distribution from the old policy, ρπold: Lπold (π) = J(πold) + (cid:88) sS ρπold (s) (cid:88) aA π(as)Aπold(s, a). (4) This approximation is reliable when π is close to πold, forming the basis of modern policy gradient methods."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we elaborate on how to implement reinforcement learning (RL) with an f-divergence regularizer. Our aim is to find the optimal policy for the following optimization problem: EqD (cid:2)Eaπθ(q)[r(aq)] ηDf (πθ(q)πref (q))(cid:3) . max πθ (5) The expected reward term in Equation 5 can be readily optimized with standard algorithms like PPO; therefore, our subsequent discussion mainly concentrates on the estimation of the f-divergence term. The overall methodology is divided into two distinct phases: pre-sampling stage and an online training stage. 4.1 PRE-SAMPLING STAGE Lets consider concrete instance of f-divergence: Forward-KL, which we define as: Dforward-KL(πθπref ) DKL(πref πθ) = Eaπref [log(πref (aq)) log(πθ(aq))]. To facilitate the computation of this expectation during the main reinforcement learning loop, we adopt pre-sampling strategy. Specifically, prior to the commencement of policy πθ training, we generate static dataset. For each query in our initial dataset D, we generate independent samples. We then evaluate the correctness of each of these samples. query is classified as near-perfect if the number of entirely correct samples is close to k. This allows us to create clear separation. The initial dataset is then partitioned into two distinct datasets: Dpef: The perfect dataset contains queries where the base model consistently produces near-perfect results. For each instance, we save one correct response generated by the base model and its corresponding log probability (log p). The format for each data point is (Question, Response, log p). Dexp: The exploration dataset comprises queries that require further improvement through reinforcement learning. Each data point is formatted as pair: (Question, GroundTruth). This partitioning ensures that our RL agent can focus on improving performance on challenging examples (Dexp) while KL divergence constraint is applied to maintain proficiency on mastered examples (Dpef). 4.2 ONLINE TRAINING STAGE In the online training stage, we simultaneously train the model using two distinct loss functions tailored for the Dexp and Dpef datasets. For samples from Dexp, we want the model to have maximum 5 freedom for exploration. Conversely, for samples from Dpef, we want the model to retain its capabilities. To achieve this, we employ two key f-divergences: Forward-KL and JS divergence. Our method is thus divided into two approaches: DPH-F for forward-KL and DPH-JS for JS divergence. Forward-KL divergence, as we define it, penalizes instances where the reference policy πref assigns high probability to an action, but the new policy πθ assigns near-zero probability. This property encourages the new policy πθ to maintain coverage of all modes of the reference policy πref , thereby preserving its original diversity. JS divergence provides symmetric and more stable alternative to KL divergence. It encourages the new policy πθ to maintain high similarity with the reference policy πref while achieving high performance, effectively preventing policy collapse."
        },
        {
            "title": "4.2.1 GENERATOR-BASED IMPLEMENTATION",
            "content": "Our implementation relies on pre-sampling from the reference policy. This approach allows us to compute the divergence term using static dataset, eliminating the need to run inference with the reference model during the online training loop. Loss for Dexp For challenging samples in Dexp, we remove the KL divergence penalty from the loss function entirely. This allows the model to perform pure policy optimization based solely on the reward signal, enabling more aggressive exploration. Specifically, the loss function for these samples is the standard PPO-clip objective: LDPH-exp(θ) = qDexp oiπθold (q) (cid:34) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min ρi,t(θ) ˆAi,t, clip (ρi,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:35) (cid:17) Please refer to Appendix for the definitions of the symbols in the formula. Loss for Dpef For all f-divergences, the general formula for calculating the loss on Dpef is: Lpef(θ) = EqDpef [Df (πθπref )]. For DPH-F, the loss term is: Lpef(θ) = EqDpef (cid:34) (cid:88) πref (aq) log (cid:18) πref (aq) πθ(aq) (cid:19)(cid:35) . For DPH-JS, the loss term is: Lpef(θ) = EqDpef (cid:34) (cid:88) πref (aq) (cid:18) log + 1 2 log (cid:18) + 1 2 (cid:19)(cid:19)(cid:35) . where = πθ/πref . 4.2.2 OVERALL LOSS FUNCTION (6) (7) (8) (9) The total loss for given batch is combination of these two objectives. For mixed batch, we determine whether the data comes from Dpef or Dexp and then calculate the corresponding loss: LDPH-RL(θ) = Lexp(θ) + ηLpef(θ), (10) where the losses are applied sample-wise depending on their origin, and η is hyperparameter to balance the two components."
        },
        {
            "title": "4.3 ENHANCED MONOTONIC IMPROVEMENT GUARANTEE",
            "content": "In this section, we derive an enhanced monotonic improvement guarantee for TRPO-style algorithms that incorporate our proposed method. Our analysis demonstrates that the policy improvement in each update step is augmented by term related to reference policy. To begin, we introduce mild assumption concerning policy, πpef, which is induced by our near perfect dataset Dpef. Assumption 1. For policy π encountered during training, there exists constant δ 0 such that for any state s, the expected advantage of actions from the reference policy πpef evaluated under π is lower-bounded by δ: Eapefπpef(s)[Aπ(s, apef)] δ. This assumption is well-founded in regions of the state space covered by the distribution close to the dataset Dpef, where the prefect actions are expected to be superior, thus yielding δ > 0. Conversely, for states outside this domain, our method does not apply regularization, and this bound naturally holds with δ = 0. Leveraging this assumption, the following theorem provides stronger lower bound on policy improvement compared to the counterpart in the original TRPO analysis (Schulman et al., 2015). Theorem 1. Let α1 = maxs DKL(π(s)πold(s)). For α2 = maxs Df (π(s)πpef(s)), our proof of the following bound considers Df to be one of three specific divergences: the forward-KL divergence, the α-divergence, or the Jensen-Shannon divergence. If Assumption 1 holds, then the inequality is: J(π) Lπold(π) δ 1 γ 2γ(α2 1ϵπ + α2 (1 γ)2 2ϵpef) , (11) where ϵπ = maxs,a Aπ(s, a) and ϵpef = maxs,a Aπpef (s, a). The proof of Theorem 1 follows similar line of reasoning to the analyses in Schulman et al. (2015) and Kang et al. (2018). The detailed derivation is deferred to the Appendix G."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP In this section, we conduct extensive experiments to demonstrate the effectiveness and generalization of DPH-RL. The experimental setups were applied to two types of tasks: SQL and mathematical reasoning, using both Llama 3.1 Meta AI (2024) and Qwen2.5 Team (2024) series models. As our method is plug-and-play approach that adds an auxiliary loss term and is orthogonal to existing GRPO variants, we selected GRPO, DAPO and reverse-KL (RKL) as our baselines. Please note that for the SQL task, we only trained on the Bird dataset. Please refer to Appendix for training details. 5.2 SQL 5.2.1 MAIN RESULTS As shown in the table 2, in the Bird dataset, the Pass@8 scores for both GRPO and DAPO are lower than the baseline, while our DPH-F and DPH-JS methods surpass the base model. This indicates that our strategy possesses more robust ability to maintain model diversity. Specifically, DPH-JS shows Pass@8 scores that are 4.3% and 2.9% higher than GRPO, respectively. For the Spider dataset, which is an out-of-domain (OOD) SQL task, the Pass@k metrics for all models generally show performance collapse. However, DPH-F and DPH-JS can maintain accuracy levels closer to the original baseline. While DAPO performs better than GRPO on the Bird dataset, its performance is more unstable on the OOD data. Regarding OOD performance preservation, DPH-F demonstrates more powerful capability, with its Pass@16 scores being 9.0% and 1.6% higher than DAPO, respectively. This suggests that for simple tasks that the model can already handle correctly, preservation of its original capabilities is more crucial than pure exploration. General Models vs. Fine-Tuned Models It is worth noting that the Llama-3.1-8B-Instruct model, with its poor prompt adaptation and high randomness, shows significant improvement in the 7 Table 2: Results in SQL tasks. All results are averaged over three experimental runs (inferences). For the RL Models, the highest score for each metric is highlighted in color . For each method, we conducted three reinforcement learning training runs. The resulting error bars are shown in the figure 6. Model Bird Spider Greedy Pass@8 Pass@16 Greedy Pass@8 Pass@16 Base Model 42.4 68.8 75.0 71.0 90.9 93. Llama-3.1-8B-Instruct RL Models GRPO DAPO RKL DPH-F α Divergence -α=0.2 -α=0.5 -α=0.8 58.5 60.0 60.0 60.4 60.1 60.8 60.5 DPH-JS (Generator) 59.8 61.3 62. -η=0.01 -η=0.05 -η=0.2 66.2 67.2 69.8 70.1 67.0 68.0 69.2 66.7 69.5 70.5 DPH-JS (Divergence Definition) 66.6 69.6 70.1 -η=0.01 -η=0.05 -η=0. 59.6 61.4 62.4 67.7 69.0 71.8 71.6 69.1 69.8 70.4 68.1 70.9 72.4 68.2 71.3 71.9 73.0 71.1 71.0 77. 75.4 75.2 75.8 76.4 75.3 76.0 76.7 75.9 78.6 79.5 75.3 79.0 84.5 78.8 80.4 81.7 79.1 82.7 82. 80.5 83.5 85.2 80.6 76.7 80.6 85.7 80.1 82.5 83.6 80.8 83.8 84.1 81.5 84.7 86.7 Greedy metric but decline in metrics like Pass@16. In contrast, the OmniSQL-32B model, which is fine-tuned from the Qwen2.5-32B-Instruct model on the Bird dataset, adapts well to the prompts, resulting in more stable performance before and after training. Table 3: Out-of-Distribution Performance. Evaluating SQL-trained models on mathematical tasks. The AIME dataset fluctuates more than the other datasets. For fair comparison, we removed AIME25. Model Base Model RL Models -GRPO -DAPO -RKL -α=0.2 -α=0.5 -α=0.8 -DPH-F -DPH-JS Pass@64 Pass@16 Pass@ AIME24 AMC23 Math500 Olympiad Minerva College Math Llama-3.1-8B-Instruct (SQL-trained) 40.0 33.3 30.0 23.3 40.0 36.7 43.3 46.7 40.0 95. 77.5 77.5 72.5 90.0 85.0 87.5 95.0 92.5 81.2 72.0 72.8 70.8 79.8 80.2 80.8 80.8 81.2 46.4 41.8 44.4 33.8 44.0 43.7 43.1 46.1 48.2 54. 51.2 52.3 49.2 51.8 50.7 54.0 54.3 53.8 45.5 38.4 38.8 41.1 41.8 43.4 44.2 43.0 45.7 Avg 60.35 52.37 52.63 48.45 57.90 56.62 58.82 60.98 60. Performance on OOD Tasks The results on out-of-distribution (OOD) tasks are presented in Table 3. To evaluate the diversity of the models after RL, we selected five mathematical datasets and tested their Pass@k scores using the Llama model. 8 Without incorporating additional general-purpose data during training, methods like GRPO and DAPO cause the models to become overly focused on the SQL domain of the training data. Their performance on other OOD tasks shows significant decline. In contrast, while the DPH-F and DPH-JS methods show slight decrease in some metrics compared to the Base Model, their average performance is remarkably higher, surpassing DApo by 8.35% and 7.6%, respectively. It is particularly noteworthy that DPH-F exhibits the best preservation ability in this context, aligning with our conclusion from Table 1 that it is more effective at maintaining consistency with the base model."
        },
        {
            "title": "5.2.2 COMPARISONS UNDER DIFFERENT f -DIVERGENCES",
            "content": "In Tables 2 and 3, we analyzed the impact of different -divergences on the Llama model. First, DAPO and GRPO, which lack any form of -divergence constraint, experienced severe Pass@k collapse on both in-domain and out-of-domain tasks. Conversely, while RKL could maintain high Pass@k on the training tasks test set, its performance collapse on tasks with different distributions was even more severe than methods without any KL penalty. This highlights the limitations of reverse-KL: it causes the model to over-focus on the training datas distribution, thereby completely sacrificing generalization. To prove the reliability of our experimental baseline, we show the performance of RKL with different η values in Figure 2. When η > 0.02, the models learning performance fails to surpass that of DAPO. This indicates that the chosen η value is too large, and therefore our selection of η = 0.01 is highly reasonable. Figure 2: η vs. Greedy Performance The α divergence sits between the forward-KL and reverse-KL, effectively preventing the model from veering toward extremes. Across all datasets, it shows clear trend: as the α value increases, it theoretically approaches the capabilities of forward-KL, while experimentally maintaining higher Pass@k. In contrast, DPH-F and DPH-JS methods demonstrated strong generalization across different tasks. The DPH-JS method, in particular, not only maintained higher greedy performance on training tasks but also preserved high Pass@k value on out-of-domain tasks, which fully demonstrates its significant advantage in preserving multi-modal distribution. 5.2.3 ABLATION STUDY Since our method is plug-and-play module, we mix the introduced -divergence loss with our sampled data. For the ablation study, we therefore adjusted the value of the η parameter. As shown in Table 2, when η is very small, the total loss approaches that of the DAPO method. We also compared the Generator and Divergence Definition implementations of -divergence. See the appendix for details on the Appendix C. The results in Table 2 show that with the same η parameter, the performance of the Generator and Divergence Definition models is similar. However, the Divergence Definition model requires resampling from πθ and introduces an additional reference model to calculate πref, which makes the generating function form significantly more timeefficient. When the value of η is small (e.g., η = 0.01), the effect is closer to that of GRPO. The -divergence constraint stabilizes training by limiting the exploration of DAPO. As the value of η increases, our Pass@16 score shows steady growth, demonstrating the effectiveness of our method. 5.3 MATHEMATICAL REASONING For the mathematical tasks, we present two types of results, shown in Table 4 and Table 8, respectively. Table 4 shows the Pass@k scores on various test sets, while Table 8 shows the Mean@k scores, which is the average accuracy calculated by sampling the entire dataset times. Regarding mathematical ability, different model families have completely different capabilities. For the Llama model, the improvement through RL is very limited; the average mean@k value on GRPO 9 Table 4: The Pass@k metric for models trained on math datasets. In the Llama experiments, the DAPO settings (ϵhigh=0.28) caused training to crash. To conduct effective experiments, we aligned the settings for all Llama experiments on the Math dataset with GRPO (ϵhigh=0.2). Model Base Model RL Models -GRPO -RKL -DPH-F -DPH-JS Base Model RL Models -GRPO -DAPO -RKL -DPH-F -DPH-JS Pass@64 Pass@16 Pass@8 AIME24 AIME25 AMC23 Math500 Olympiad Minerva College Math 40.0 33.3 36.7 36.7 40.0 63.3 56.6 63.3 66.7 73.33 66.7 23.3 26.7 16.7 26.7 26. 56.7 50.0 46.7 40.0 50.0 53.3 Llama-3.1-8B-Instruct 95.0 80.0 75.0 90.0 82.5 81. 79.6 80.0 80.6 81.4 Qwen2.5-Math-7B 87.5 97.5 97.5 97.5 97.5 100.0 88.8 93.0 92.2 92.0 92.4 92. 46.4 43.3 39.1 44.3 45.8 61.9 62.8 63.1 64.6 63.8 65.2 54.0 56.6 56.6 57.3 58. 56.6 64.0 64.3 64.0 64.8 66.2 45.5 43.1 43.4 45.5 45.1 42.9 50.1 48.7 51.3 50.9 51. Avg 55.06 51.80 49.64 54.44 54.23 65.39 67.71 67.97 68.01 70.39 70.74 only increased by 0.93, while Pass@k decreased by 3.26. This suggests that for the Llama architecture, RL struggles to improve performance without sacrificing the models original diversity and success rate. In contrast, the Qwen model can significantly improve both its mean@k (about 20%) and Pass@k just through RL. This indicates that the Qwen model is much more receptive to RLbased fine-tuning for this specific task. We selected these two models to explore the performance of the DPH method under different model strengths. For Llama, DPH-JS maintained the original Pass@k value and significantly improved mean@k on AIME, which demonstrates that DPH-JS is powerful and highly versatile method with strong capabilities in both exploration and preservation. Furthermore, DPH-JS provided more balanced improvement than GRPO, increasing both Pass@k and mean@k averages, showing it does not sacrifice one metric for the other. For the Qwen model, the overall performance showed decrease in Pass@k on the AIME test set but an increase on other datasets. In these two trends, DPH-JS performed the best. On AIME, it was closer to the diversity of the base model than both GRPO and DAPO, while on other datasets, it achieved higher Pass@k. This demonstrates DPH-JSs superior ability to navigate conflicting performance trends, preserving diversity where needed (AIME) and maximizing success rates on other, perhaps less challenging, datasets."
        },
        {
            "title": "6 ANALYSIS\n6.1 ANALYSIS OF TRAINING PROGRESS",
            "content": "In Figure 3, we visualize the evolution of the Llama models Pass@8 metric during training. On the Bird dataset, GRPO shows clear, gradual collapse, while DAPOs performance continuously oscillates. Over the long term, DPH-F and DPH-JS both consistently maintain higher Pass@8 value than the initial level. On the Spider dataset, DAPO performs extremely poorly, even falling below GRPO. Our experiments show that DAPO sacrifices out-of-domain generalization for improved performance on in-domain datasets. While RKL performs well on the Bird dataset, its performance also shows gradual collapse on the Spider dataset as training progresses. 6.2 ANALYSIS OF KEEP AND EXPLORATION To gain more granular understanding of why our method achieves higher Pass@k scores, we extracted correct and incorrect samples from the base models Pass@8 results. These were designated 10 (a) Llama Bird (b) Llama Spider Figure 3: Llama Pass@8 progress: Algorithms vs. Training Steps Figure 4: Exploring the differences between base model and RL-Tuned models in Llama. sub sub and Dwrong as data subsets Dcorrect , respectively. We then compared them with the model after reinforcement learning. The proportion of samples it got correct on Dcorrect is its keep rate, while the proportion it got correct on the incorrect samples from Dwrong is its additional exploration rate. As shown in Figure 4, the keep rate of GRPO and DAPO both decreased on the two datasets, while the KL method mainly maintained higher keep rate to maintain higher Pass@k value. DAPO had higher exploration within the domain than GRPO, but the keep rate outside the domain dropped significantly. RKL could only have high keep rate on the same Bird test set as the training set, while it dropped to level similar to GRPO on Spider. DPH-JS had higher exploration on the Bird test set, while DPH-F was higher on the Spider dataset. sub sub"
        },
        {
            "title": "7 CONCLUSION",
            "content": "Addressing the limitations of reverse KL divergence in current large language model reinforcement learning methods, we propose DPH-RL, which effectively solves the issue of diversity collapse using f-divergence. Our extensive experiments on Llama and Qwen models for both SQL and mathematical reasoning tasks show that DPH-RL achieves superior pass@k scores on both in-domain and out-of-domain datasets. Furthermore, we show that implementing f-divergence is achievable via two distinct methods: the Generator and the Divergence Definition approach. It is worth noting that the Generator method obviates the need for reference model during training while producing results comparable to the Divergence Definition method. DPH-RL trains more powerful and comprehensive model using fewer training resources."
        },
        {
            "title": "REFERENCES",
            "content": "Christopher Bishop and Nasser Nasrabadi. Pattern recognition and machine learning, volume 4. Springer, 2006. Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, and Guang Shi. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models, 2025. URL https://arxiv.org/abs/2508.10751. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models, 2025. URL https://arxiv.org/abs/2505.22617. Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025a. Yihong Dong, Xue Jiang, Yongding Tao, Huanyu Liu, Kechi Zhang, Lili Mou, Rongyu Cao, Yingwei Ma, Jue Chen, Binhua Li, et al. Rl-plus: Countering capability boundary collapse of llms in reinforcement learning with hybrid-policy optimization. arXiv preprint arXiv:2508.00222, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018a. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b. Pouya Hamadanian, Arash Nasr-Esfahany, Malte Schwarzkopf, Siddartha Sen, and Mohammad Alizadeh. Online reinforcement learning in non-stationary context-driven environments. In International Conference on Learning Representations, 2025. Jiaqi Han, Mingjian Jiang, Yuxuan Song, Stefano Ermon, and Minkai Xu. -po: Generalizing preference optimization with -divergence minimization. In International Conference on Artificial Intelligence and Statistics, pp. 11441152. PMLR, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In ACL (1), pp. 38283850. Association for Computational Linguistics, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pp. 267274, 2002. Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In International conference on machine learning, pp. 24692478. PMLR, 2018. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https: //arxiv.org/abs/2411.15124. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In NeurIPS, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q. Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. https: //huggingface.co/datasets/Numinamath, 2024a. Hugging Face repository, 13:9. Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024b. Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, and Liang He. How do humans write code? large models do it the same way too. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 46384649, Miami, Florida, USA, November 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.267. URL https://aclanthology.org/ 2024.emnlp-main.267/. Long Li, Xuzheng He, Haozhe Wang, Linlin Wang, and Liang He. How do humans write code? large models do it the same way too. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 46384649, Miami, Florida, USA, November 2024d. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.267. URL https://aclanthology.org/ 2024.emnlp-main.267/. Friedrich Liese and Igor Vajda. On divergences and informations in statistics and information theory. IEEE Transactions on Information Theory, 52(10):43944412, 2006. Sadegh Mahdavi, Muchen Li, Kaiwen Liu, Renjie Liao, and Christos Thrampoulidis. Beyond accuracy: policy gradient reweighting approach for pass@ maximization in llms. In 2nd AI for Math Workshop@ ICML 2025. Meta AI. Introducing meta Llama 3: The most capable openly available LLM to date, April 2024. URL https://ai.meta.com/blog/meta-llama-3/. Accessed: 2024-04-18. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems, 29, 2016. OpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Qwen. Qwq-32b: Embracing the power of reinforcement learning, 2024. URL https:// qwenlm.github.io/blog/qwq-32b/. 13 John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pp. 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: scaling instruction tuning for mathematical reasoning. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Christian Walder and Deep Karkhanis. Pass@ policy optimization: Solving harder reinforcement learning problems. arXiv preprint arXiv:2505.15201, 2025. Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=2cRzmWXK9N. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025. URL https://arxiv.org/abs/ 2506.01939. Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. URL https://arxiv. org/abs/2504.14945. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. 14 Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 39113921, 2018. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond In 2nd AI for Math Workshop @ ICML 2025, 2025a. URL https: the base model? //openreview.net/forum?id=upehLVgq1b. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025b. URL https://arxiv. org/abs/2504.05118. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. 15 Figure 5: On the left, we construct base model that outputs multiple solution styles for SQL problems. This model is then used for reinforcement learning training. We calculated the probability of the number of times the model outputted different styles across 32 samples."
        },
        {
            "title": "A MULTIPLE STYLE CAPABILITY EXPERIMENT",
            "content": "We construct base model that outputs five different solution styles for SQL problems. This model is then used for reinforcement learning training. Next, we sample each question 32 times. Based on the output prefix, we count the number of times each distinct style appears within these 32 samples. For prefixes that are not among our original five styles (e.g., the model combines several different prefixes), we categorize them as Others and treat them as an additional style. The result shows in Figure 5. On the base model, we find that it can fully output all styles. However, after training with reverse KL-constrained GRPO, most outputs degenerate into single style. In contrast, forward KL significantly mitigates the degradation of the models ability to produce different styles."
        },
        {
            "title": "B RLVR ALGORITHMS",
            "content": "B.1 GROUP RELATIVE POLICY OPTIMIZATION (GRPO) GRPO presents an innovative approach to policy learning that distinguishes itself from methods like Proximal Policy Optimization (PPO) by eliminating the need for an explicit value function. Instead, GRPO computes the advantage in group-relative manner, offering streamlined yet effective optimization strategy. For specific question-answer pair (q, a), GRPOs underlying behavior policy, πθold, generates group of individual responses, denoted as {oi}G i=1. The advantage for the i-th response within this ensemble is then precisely calculated by normalizing the rewards specific to that group, {Ri}G i=1: ˆAi,t = ri mean({Ri}G std({Ri}G i=1) i=1) . (12) Similar to the clipped surrogate objective found in PPO, GRPO also incorporates clipping mechanism to constrain policy updates. This helps maintain training stability and improve sample efficiency by ensuring that new policies dont deviate too drastically from previous ones. Beyond 16 this, GRPO further enhances regularization by directly adding Kullback-Leibler (KL) divergence penalty term to its objective function. This penalty helps prevent the policy from drifting too far from reference policy, promoting stable and controlled learning. The comprehensive objective function for GRPO is articulated as: LGRPO(θ) = (q,a)D oiπθold (q) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min 1 oi (cid:16) (cid:32) oi (cid:88) t=1 ρi,t(θ) ˆAi,t, clip (ρi,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:17) (13) (cid:33)(cid:35) ηDKL(πθπref) , where β is coefficient for the KL penalty, DKL(πθπref) quantifies the KL divergence between the current policy πθ and specified reference policy πref. The term ρi,t(θ) represents the importance sampling ratio for the i-th response at time t, which is defined as: ρi,t(θ) = πθ(oi,t q, oi,<t) πθold(oi,t q, oi,<t) . (14) critical design choice in GRPO is its sample-level objective computation. Specifically, the loss is first averaged within each generated sequence, and subsequently, these sequence-level losses are averaged across various samples. This distinct computational approach, particularly when compared to token-level optimizations, can significantly influence the algorithms empirical performance. Traditional GRPO applies single, unified loss function to all training samples. This loss includes KL divergence penalty, typically in the form of reverse KL divergence, which aims to keep the fine-tuned policy close to the base policy. This penalty is added directly to the policy gradient loss. When applied to our experimental setup, the KL penalty for samples from Dexp would be: LKL-GRPO = EqDexp [βDKL(πθπbase)] , where the reverse KL divergence is defined as: DKL(πθπbase) = (cid:88) πθ(x) log (cid:18) πθ(x) πbase(x) (cid:19) . (15) (16) Here, the term DKL(πθπbase) represents the Reverse KL Divergence. It measures the information lost when the base policy πbase approximates the new policy πθ. This choice heavily penalizes the new policy πθ for exploring actions that the base policy πbase considers to have low probability. In other words, if the new policy πθ assigns high probability to an action where the base policy πbase assigns near-zero probability, this divergence will be very large. The effect is to strongly encourage the new policy πθ to stick to the modes (high-probability regions) of the original base policy πbase, which restricts the exploration space. B.2 DYNAMIC SAMPLING POLICY OPTIMIZATION (DAPO) DAPO is an enhancement of the GRPO algorithm, incorporating several key improvements. DAPO eliminates the KL penalty and refines the clipping mechanism, changing the upper bound from (1 + ε) to fixed value of (1 + εupper), where εupper is set to 0.28. core innovation of DAPO is its dynamic sampling mechanism, which moves beyond the all-or-nothing approach to sampling. Additionally, the algorithm applies token-level policy gradient loss and uses an overlong reward shaping technique."
        },
        {
            "title": "C METHOD FOR DIVERGENCE DEFINITION",
            "content": "For f-divergences like forward KL, the Divergence Definition and Generator implementations are equivalent. However, for divergences such as JS divergence, which require sampling from both the 17 reference policy πref and the new policy πθ: JS(πbaseπθ) = (cid:18)"
        },
        {
            "title": "DKL",
            "content": "πbase πbase + πθ 2 (cid:19) + 1 2 (cid:18)"
        },
        {
            "title": "DKL",
            "content": "πθ (cid:19) πbase + πθ 2 1 2 (17) Since the JS divergence is composed of two parts, we use data from the Dpef loss to compute the (cid:1). For the training on the Dexp loss, we introduce reference model to (cid:0)πbase πbase+πθ first part DKL calculate the value of the second part DKL (cid:0)πθ πbase+πθ (cid:1). 2 2 The loss for DPH-JS on the Dexp can be written as: LDPH-exp(θ) = (q,a)Dexp oiπθold (q) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) (cid:16) min ρi,t(θ) ˆAi,t, clip (ρi,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:17) t=1 (cid:18) β1DKL πθ (cid:19) (cid:35) πbase + πθ 2 β1 = η Dpef Dexp (18) (19) By adjusting the value of β1, we can make the values of the two JS components within the same batch as close as possible, thereby achieving balance during training. β1 is not hyperparameter; it only depends on the dataset size and η. Its formula is: The loss for DPH-JS on the Dpef can be expressed as: (cid:18) LDPH-pef = EqDpef (cid:20) DKL πbase (cid:19)(cid:21) πbase + πθ (20)"
        },
        {
            "title": "D TRAINING DETAILS",
            "content": "D.1 TASK SETTINGS For the SQL task, we used the Llama-3.1-8B-Instruct and OmniSQL32B models. We performed RL training exclusively on the BIRD dataset Li et al. (2024b) and then evaluated the models on both the BIRD and Spider Yu et al. (2018) datasets. To validate the generalization of our method, we also tested it on mathematical reasoning task, treating it as an out-of-distribution (OOD) evaluation. For the mathematical reasoning task, we used the Llama-3.1-8B-Instruct and Qwen2.5-Math-7B models. We conducted RL training on the DAPO-17K Yu et al. (2025) dataset and performed testing on seven different datasets: AIME24 Li et al. (2024a), AIME25, AMC23 Li et al. (2024a), Math500 Hendrycks et al. (2021), Olympiad He et al. (2024), Minerva Lewkowycz et al. (2022), and College Math Tang et al. (2024). Evaluation For the SQL tasks, the output is typically tuple, such as [(string, int), (string, int)]. We compute the Cartesian product of the prediction and the ground truth. For the math tasks, we use the official Qwen2.5 evaluation tool1 for detailed assessment, as this tool provides tailored evaluations for each dataset. D.2 METHOD SETTINGS The implementation of RKL is consistent with the standard GRPO implementation. Our default implementation for DPH-RL uses the Generator method. The settings for our different methods are detailed in Table 5. For all RL algorithms, we consistently use token-level loss. In most of our experiments, the ϵhigh for our DPH methods was set to 0.28. However, due to training instability in the Llama math experiments, we set the ϵhigh to 0.2 for all methods. 1https://github.com/QwenLM/Qwen2.5-Math 18 Table 5: Configuration Comparison of RL Methods Method Online Data Offline Data Offline Loss η ϵhigh Dynamic Sampling GRPO DAPO RKL α-divergence DPH-F DPH-JS Dexp,Dpef Dexp,Dpef Dexp Dexp Dexp Dexp - - - Dpef Dpef Dpef - - - α-divergence Forward KL JS divergence - - 0.01 0.01 0.01 0. 0.2 0.28 0.28 0.28 0.28 0.28 D.3 HYPERPARAMETERS All experiments were conducted on 32 NVIDIA A800-80G GPUs using the VeRL framework for our RL algorithm implementations. For all experiments, we set the number of rollouts to 16. In our setup, the training batch size was 128, and the batch size for the Dpef data was 256. This resulted in using 2048 samples from Dexp and 256 samples from Dpef at each learning step, yielding an effective ratio of 8:1. comprehensive list of the specific hyperparameters used is provided in Table 6. In the offline phase, we used the current model to perform 8 rollouts on the training data. For the SQL tasks, we separated the perfectly correct examples from the rollouts and used the remaining ones to construct Dexp. For the DAPO-17k dataset used in the math tasks, we discarded all data where the models responses were entirely incorrect. During DAPO training, high temperature could easily cause model collapse, so we reduced the temperature and learning rate accordingly. Table 6: Hyperparameters for RL Training Hyperparameter Llama-SQL OmniSQL-32B-SQL Llama-Math Qwen2.5-Math Batch Size Learning Rate Rollout Temperature Rollout Top-p Validation Temperature Validation Top-p PPO Epochs Size of Dexp Size of Dpef Max Response Length Number of Rollouts Training Epochs 128 1e-6 1.0 0.95 1.0 0.95 1 6710 2248 4096 16 4 128 1e-6 1.0 0.95 1.0 0.95 1 5169 3789 4096 16 4 128 2e-7 0.7 0.95 0.6 0.95 1 4599 1248 2048 16 128 2e-7 0.7 0.95 0.6 0.95 1 7009 3199 2048"
        },
        {
            "title": "E ADDITIONAL EXPERIMENTS",
            "content": "E.1 32B EXPERIMENTS FOR SQL Table 7: Results on Omnisql-32B for SQL tasks. Model Bird Spider Greedy Pass@8 Pass@16 Greedy Pass@8 Pass@16 OmniSQL-32B Base Model RL Models GRPO DAPO DPH-F DPH-JS 65.0 69.4 69.9 70.4 70.5 77. 76.3 76.8 78.6 79.2 85.0 84.7 83.8 86.1 84.9 92.8 91.6 91.0 92.7 91.9 93. 92.4 91.7 93.3 93.1 79.8 78.8 79.2 80.8 81.9 19 Figure 6: Error bar in Llama Sql. For each method in our Llama SQL experiments, we conducted three separate reinforcement learning training runs. We then selected the model that achieved the highest pass@16 score on the Bird. We also tested the effectiveness of DPH-F and DPH-JS on larger 32B model and found them to be equally effective. As shown in Table 7, please note that greedy score of 70.5 on the Bird dataset is already approaching the performance limit of open-source Single-Models2. E.2 ERROR BAR E.3 MEAN@K METRIC FOR MATHEMATICAL REASONING TASK To verify that our reinforcement learning training was effective on the math tasks, we evaluated the performance using the mean@k metric in Table 8. 2https://bird-bench.github.io/ 20 Table 8: The Mean@k metric for models trained on math datasets. To accelerate training, we filtered the DAPO-17k dataset for data with Pass@8 score of 0. In the end, we trained Llama on 5.8k samples and Qwen on 10.2k samples. Model Base Model RL Models -GRPO -RKL -DPH-F -DPH-JS Base Model RL Models -GRPO -DAPO -RKL -DPH-F -DPH-JS Mean@64 Mean@16 Mean@ AIME24 AIME25 AMC23 Math500 Olympiad Minerva College Math Llama-3.1-8B-Instruct 5.5 5.6 6.4 6.2 8.4 12.7 25.8 25.9 25.8 26.0 26. 0.5 0.6 0.2 0.5 0.5 6.6 10.9 11.1 10.3 10.6 11.2 23.0 23.3 22.3 23.8 24. 32.2 60.6 61.3 61.5 60.4 62.7 46.5 47.7 45.6 48.1 48.4 Qwen2.5-Math-7B 48. 75.5 76.3 75.3 76.4 76.5 15.2 14.9 13.0 15.5 15.6 22.8 37.8 37.6 36.7 37.3 38.5 22. 25.4 24.8 25.4 27.4 13.3 38.8 39.0 34.6 38.1 38.9 27.3 29.1 28.6 29.7 29.9 19. 40.6 40.7 39.9 40.9 41.9 Avg 20.01 20.94 20.13 21.31 22.06 22.26 41.43 41.70 40.59 41.38 42."
        },
        {
            "title": "F CASE STUDY",
            "content": "We have listed the use cases and prompts for the two tasks in detail to facilitate future reproduction."
        },
        {
            "title": "Instruction",
            "content": "System Prompt You are Qwen, created by Alibaba Cloud. You are helpful assistant. User Prompt Task Overview: You are data science expert. Below, you are provided with database schema and natural language question. Your task is to understand the schema and generate valid SQL query to answer the question. Database Engine: SQLite Database Schema: CREATE TABLE frpm ( CDSCode text, example: [01100170109835, 01100170112607] Academic Year text, example: [2014-2015] County Code text, example: [01, 02] Free Meal Count (K-12) real, example: [565.0, 186.0] Percent (%) Eligible Free (K-12) real, example: [0.519779208831647, 0.470886075949367] 2013-14 CALPADS Fall 1 Certification Status integer, example: [1] PRIMARY KEY (CDSCode), CONSTRAINT fk frpm cdscode FOREIGN KEY (CDSCode) REFERENCES schools (CDSCode) ); CREATE TABLE.... This schema describes the databases structure, including tables, columns, primary keys, foreign keys, and any relevant relationships or constraints. Question: Eligible free rate for K-12 = Free Meal Count (K-12) / Enrollment (K-12). What is the highest eligible free rate for K-12 students in the schools in Alameda County? Instructions: - Make sure you only output the information that is asked in the question. If the question asks for specific column, make sure to only include that column in the SELECT clause, nothing more. - The generated query should return all of the information asked in the question without any missing or extra information. - Before generating the final SQL query, please think through the steps of how to write the query. Output Format: In your answer, please enclose the generated SQL query in code block: sql -- Your SQL query Take deep breath and think step by step to find the correct SQL query. Response To find the codes of the schools with total enrollment of over 500, we need to follow these steps: 1. Identify the table that contains the enrollment data: The frpm table contains the Enrollment (K-12) and Enrollment (Ages 5-17) columns, which represent the enrollment data. 2. Identify the relevant columns: We need to sum the Enrollment (K-12) and Enrollment (Ages 5-17) columns to get the total enrollment. 3. Identify the filter condition: We need to filter the schools with total enrollment of over 500. 4. Write the SQL query: We can use SELECT statement to retrieve the CDSCode column from the frpm table, filtering the schools where the sum of Enrollment (K-12) and Enrollment (Ages 5-17) is greater than 500. Based on these steps, the SQL query to find the codes of the schools with total enrollment of over 500 is: sql SELECT frpm.CDSCode FROM frpm WHERE frpm.Enrollment (K-12) + frpm.Enrollment (Ages 5-17) > 500; This query sums the Enrollment (K-12) and Enrollment (Ages 5-17) columns and filters the schools with total enrollment of over 500, returning the CDSCode column. Execution results [[01100170109835], [01100170112607], [01100170124172]] 22 F.1 MATH TASK"
        },
        {
            "title": "Instruction",
            "content": "System Prompt Please reason step by step, and put your final answer within boxed{{}}. User Prompt Every morning Aya goes for 9-kilometer-long walk and stops at coffee shop afterwards. When she walks at constant speed of kilometers per hour, the walk takes her 4 hours, including minutes spent in the coffee shop. When she walks + 2 kilometers per hour, the walk takes her 2 hours and 24 minutes, including minutes spent in the coffee shop. Suppose Aya walks at + 1 2 kilometers per hour. Find the number of minutes the walk takes her, including the minutes spent in the coffee shop. Response To solve this problem, we need to first determine the value of and t. We know the following: 1. When Aya walks at km/h, the total time including the coffee shop visit is 4 hours. 2. When Aya walks at + 2 km/h, the total time including the coffee shop visit is 2 hours and 24 minutes (which is 2.4 hours). First, we can set up the equations for the walking times: 1. For the first scenario: 2. For the second scenario: 9 We can solve these equations to find and t. Lets subtract the second equation from the first to eliminate t: + 2 + 60 = 4 + 60 = 2.4 9 + 2 = 1.6 This simplifies to: 9(s + 2) 9s = 1.6 s(s + 2) 18 s(s + 2) = 1.6 We can solve this quadratic equation using the quadratic formula = , where = 1.6, = 3.2, and = 18: 18 = 1.6s2 + 3.2s 1.6s2 + 3.2s 18 = 0 (cid:113) b24ac 2a 18 = 1.6s(s + 2) = 3.2 (cid:112)3.22 4 1.6 (18) 2 1.6 3.2 10.24 + 115.2 = = 3.2 3.2 125. 3.2 3.2 11.2 3.2 = 2.5 (positive root) This gives us two possible solutions: = = 8 3.2 So, = 2.5 km/h. Now we can substitute back into one of the original equations to find t: = 14.4 3. = 4.5 (negative root, not physically meaningful) 9 2.5 + 3.6 + 60 60 = 4 = 4 = 0.4 = 24 So, = 24 minutes. Now we need to find out how long the walk takes when Aya walks at + 1 coffee shop visit will be: 2 = 2.5 + 0.5 = 3 km/h. The total time including the Converting 3.4 hours to minutes, we get: So the final answer is 204 . 9 3 + 24 60 = 3 + 0.4 = 3.4 hours 3.4 60 = 204 minutes 23 PROOF OF THEOREM 1 Lemma 1. Consider forward KL, α-divergence and Jensen-Shannon divergence in f-divergence, we have 1 2 T (P Q) Df (P Q). Proof. 1. PROOF VIA GENERALIZED PINSKERS INEQUALITY The proof for the first three divergences relies on the generalized Pinskers inequality, which states that for any f-divergence, its relationship with the squared total variation distance is: We proceed by calculating (1) for each case. Df (pq) (1) D2 (pq) α-divergence (α = 0, 1): The corresponding function is fα(t) = tαα(t1)1 α(t) = tα2. At = 1, we have second derivative is α(1) = 1α2 = 1. α(α1) . The Forward KL Divergence: The corresponding function is (t) = log t. The second derivative is (t) = 1 . At = 1, we have (1) = 1. In both cases, since (1) = 1, the generalized Pinskers inequality directly yields the desired result: Df (pq) 1 2 D2 (pq) 2. TIGHTER BOUND FOR JENSEN-SHANNON DIVERGENCE The case of the Jensen-Shannon (JS) divergence is more intricate. Its corresponding function is (t) = 1 2 (cid:20) log (t + 1) log (cid:18) + 1 2 (cid:19)(cid:21) . 2t(t+1) , which gives (1) = 1 The second derivative is (t) = 4 . direct application of the generalized Pinskers inequality would only yield DJS(pq) 1 8 D2 (pq), which is looser bound than the one we aim to prove. We therefore require more specific proof for this tighter bound. 1 By the data processing inequality for f-divergences, it suffices to prove the inequality for the simplest non-trivial case: two Bernoulli distributions. Let = (p, 1 p) and = (q, 1 q). The total variation distance is DT (P, Q) = q. Let = and = (p + q)/2. The inequality to be proven becomes DJS(P Q) 1 Lets define function H(d) = DJS(P Q). It can be expressed in terms of the KL divergence with respect to the mean distribution = (m, 1 m): 2 d2. H(d) = 1 DKL(P ) + 1 2 DKL(QM ) H(d) is an even function of d, with H(0) = 0 and (0) = 0. We analyze its second derivative: (d) = = d2 dd2 H(d) (cid:20)(cid:18) 1 1 + d/2 8 + 1 1 d/2 (cid:19) (cid:18) + 1 d/2 + 1 1 + d/2 (cid:19)(cid:21) . Consider the function ϕ(x) = 1 (1mx)3 > 0 for in the valid domain, which means ϕ(x) is convex. 1 1mx . m+x + 2 Its second derivative is ϕ(x) = 2 (m+x)3 + By applying Jensens inequality to the expression for (d): (d) = (cid:18) ϕ(d/2) + ϕ(d/2) 2 1 4 (cid:19) 1 4 ϕ (cid:18) d/2 d/2 2 (cid:19) = 1 ϕ(0) = 1 4m(1 m) . Since [0, 1], the term m(1 m) is maximized at = 1/2, with maximum value of 1/4. Therefore, 4m(1m) 1, which implies (d) 1. Finally, let us define an auxiliary function G(d) = H(d) 1 2 d2. We examine its properties: 1. G(0) = H(0) 0 = 0. 2. G(0) = (0) 0 = 0. 3. G(d) = (d) 1 1 1 = 0. Since the function G(d) starts at zero with zero slope and is convex (non-negative second derivative), it must be non-negative for all d. Thus, G(d) 0, which implies H(d) 1 2 d2. This completes the proof. Throughout the following proof, we will repeatedly apply the lemma established in Schulman et al. (2015). Lemma 2 (Schulman et al. (2015)). Given two policies π and π, we have J(π) J(π) = Eτ π[ (cid:88) γtAπ(st, at)], t= where the expectation is over the trajectories τ := (s0, a0, s1, a1, ...) and the notion indicates that τ are sampled from π to generate τ . Lemma 3 (Schulman et al. (2015)). Given that π, π are α-coupled policies, define Aππ(s) = EaπAπ(s, a), for all s, we have Aππ(s) 2α max s,a Aπ(s, a) Lemma 4 (Kang et al. (2018)). let (π, π) be an α - coupled policy pair. Then Estπ[Aππ(st) 2α(1 (1 α)t) max s,a Aπ(s, a) Now we are ready to prove our theorem 1. 25 Proof. J(π) Lπ(π) (cid:88) a=Estπ[ t=0 γtAππ(st)] Estπ[ (cid:88) t=0 γtAππ(st)] = (cid:88) t=0 (cid:0)Estπ[γtAππ(st)] Estπpef [γtAπpef π(st)] + Estπpef [γtAπpef π(st)] Estπ(γtAππ(st))(cid:1) b=(η(π) η(π)) (η(πpef ) η(π)) + Estπpef [γtAπpef π(st)] Estπ[γtAππ(st)] =η(π) η(πpef ) + Estπpef [γtAπpef π(st)] Estπ[γtAππ(st)] c= (cid:88) t=0 (cid:88) t=0 (cid:0)Estπ[γtAππpef (st)] + Estπpef [γtAπpef π(st)] Estπ[γtAππ(st)](cid:1) (cid:0) 2γtβ(1 (1 β)t) max (s,a) Aπpef (s, a) + γtδ 2γtα(1 (1 α)t) max (s,a) Aπ(s, a)(cid:1) e= 2β2γϵp (1 γ)(1 γ(1 β)) 2γ(β2ϵp + α2ϵπ) (1 γ) + δ 1 γ 2α2γϵπ (1 γ)(1 γ(1 α)) + δ 1 γ (21) where equality a, b, holds from Lemma 2; inequality applies Lemma 4; equality holds from Lemma 1."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Griffith University",
        "INFLY TECH"
    ]
}