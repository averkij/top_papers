{
    "paper_title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "authors": [
        "Shobhita Sundaram",
        "John Quan",
        "Ariel Kwiatkowski",
        "Kartik Ahuja",
        "Yann Ollivier",
        "Julia Kempe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data."
        },
        {
            "title": "Start",
            "content": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability Shobhita Sundaram1,, John Quan2, Ariel Kwiatkowski2, Kartik Ahuja2, Yann Ollivier2, Julia Kempe2,3 1MIT, 2Meta FAIR, 3New York University Work done during an internship at Meta Can model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate fundamental question: Can pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: selfimprovement framework designed to surface these pedagogical signals through meta-RL. teacher copy of the model proposes synthetic problems for student copy, and is rewarded with its improvement on small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving principled path to escape reasoning plateaus without additional curated data. Date: January 27, 2026 Correspondence: Shobhita Sundaram at shobhita@mit.edu 6 2 0 2 6 ] . [ 1 8 7 7 8 1 . 1 0 6 2 : r Figure 1 Learning on hard problems by self-generating curriculum. We introduce SOAR: meta-RL framework for improving on difficult datasets where performance plateaus. (left) We initialize asymmetric teacher and student models from the same base model. The teacher generates synthetic problems for the student to train on with RL, and is rewarded by the students measurable improvement on small subset of the real, ground-truth problems. (right) RL training on problems generated with SOAR, using grounded teacher rewards, outperforms direct training on the hard problems and enables the student to break out of the performance plateau."
        },
        {
            "title": "1 Introduction",
            "content": "Reinforcement learning with verifiable rewards (RLVR) has recently spurred an impressive rise in LLM reasoning capabilities (DeepSeek-AI, 2025; Team et al., 2025), particularly in mathematics and programming. Though effective, this paradigm has key limitation: the model cannot learn from problems that it cannot already solve to some extent, since RLVR uses correct solutions to reinforce useful reasoning traces. When problems are too difficult, sparse or non-existent rewards lead to little useful training signal, leaving the model stuck\". Past work has shown that the order of training data strongly affects generalization in RL training (Bengio et al., 2009; Narvekar et al., 2020), with success in selecting maximally learnable\" problems for the current policy, adapting them to learning progress, and using easy-to-hard curricula (Parashar et al., 2025; Chen et al., 2025b). Such curricula can be fragile, however, and require careful design (Kordi et al., 2025) as well as curated intermediate datasets; in many settings, the best learnable problems may be unavailable or unknown. Recent work addresses sparse rewards by exploiting dense reward signals from test-case pass rates in coding problems (Sun et al., 2025), but still relies on curated test-cases that give intermediate signals. This motivates the need for self-generated curricula. Here, we ask: Can model break its reasoning plateau by generating its own stepping-stone curriculum? We posit that pretrained LLMs possess the capacity to directly generate stepping stone curriculum to tackle hard problems. To investigate if this pedagogical signal is present and extractable, we design SOAR: an asymmetric teacher-student meta-RL framework inspired by self-play (Silver et al., 2018; Sukhbaatar et al., 2017; OpenAI et al., 2021). Both the teacher and student are initialized from the target model; the teacher proposes questions-answer pairs that the student trains on with RL. The teacher is rewarded based on student improvement on difficult subset. Critically, rather than using intrinsic rewards common to self-play, we use the difficult training dataset as black-box grounding reward signal to guide the teacher towards producing useful questions for the student. Intuitively, pretrained model has already encountered vast array of easy problems. Consider difficult calculus question: While the model may be unable to directly generate correct answer, it might still possess the latent knowledge required to generate easy chain-rule exercises, without requiring human-in-the-loop to identify and source such questions. We find that by leveraging pretraining knowledge, RL can effectively surface and amplify these latent pedagogical signals to generate useful question-answer pairs. Importantly, we do so without actually showing the model the hard questions; our framework recovers useful curriculum just by using performance on the hard dataset as reward signal. Empirically, while directly training on the hard dataset fails, we find that the teacher in our framework learns to produce useful synthetic questions that can get the student unstuck on the hard dataset, without actually seeing the hard problems. Our main contributions, supported by an extensive multi-seed empirical study and ablations (over 600 runs), are the following: Decoupled teaching and solving: models ability to generate effective \"stepping stones\" for hard problems is distinct from its ability to solve them. Self-generated problems expand the learning frontier, enabling progress on hard problems where direct RL training fails. While the base model has the capacity to propose useful questions, meta-RL is essential to sharpen this noisy distribution into reliable learning signal. proof-of-concept of self-generated curricula with SOAR (Self-Optimization via Asymmetric RL), an asymmetric teacher-student framework that rewards the teacher for student progress on hard problems. With Llama-3.2-3B-Instruct, on hard subsets of MATH and HARP, self-generated problems improve performance (e.g., 4 pass@1 and 2 pass@32 on MATH, 2 pass@1 and 1.5 pass@32 on HARP). These problems also transfer to unlock learning on hard datasets that they were not optimized for. Grounded rewards over intrinsic rewards: Grounding teacher rewards in student progress on real problems improves performance over intrinsic rewards common in self-play, which are prone to instability and collapse of question diversity. 2 Question structure over solution correctness: Problem structure and difficulty calibration matter more for escaping plateaus than answer correctness; generated questions provide useful gradient signal even when the majority of answers are incorrect. These results, backed by comprehensive empirical study, show that grounded meta-RL can escape genuine learning plateaus by letting models discover for themselves what data they need to learn from to expand their learning frontier."
        },
        {
            "title": "2 Related Work",
            "content": "For an extended background and comparison to the literature see Section A, summarized here: Curriculum Learning in RL: Automated curriculum design has long history predating modern LLMs (Bengio et al., 2009; Graves et al., 2017; Narvekar et al., 2020; Parashar et al., 2025) focusing on reordering or selecting existing data to enable or accelerate learning, or, in the context of RL, to help agents acquire complex behaviors by first mastering simpler tasks. For LLM training, curricula are applied over curated prompts or problem categories, using proxy signals such as gradient norms or advantage/difficulty estimates to guide selection (Team et al., 2025; Dennis et al., 2020; Wen et al., 2025; Yu et al., 2025; Bae et al., 2025; Chen et al., 2025b; Jiang et al., 2025). By contrast, our goal is not to arrange data but to self-generate tasks to elicit learning on fixed, verifiable hard dataset where standard RLVF fails. Self-Play and Teacher-Student Setups: Self-play offers complementary lens on autonomous capability growth, classically exemplified by game-playing agents trained without external data, such as AlphaZero (Silver et al., 2018) and asymmetric teacher-student setups to induce powerful automatic curricula (Sukhbaatar et al., 2017; OpenAI et al., 2021). Self-play methods for LLMs must address specific challenges: rewards in language domains are extremely sparse and brittle. For mathematical problems, correctness is essentially binary and offers no gradient toward partial solutions. Thus, essentially all modern LLM self-play methods optimize for self-consistency or solution quality. Earlier works (Chen et al., 2024; Wang et al., 2025; Singh et al., 2024; Ye et al., 2024) still presuppose the existence of well-formed input prompts or curated high-quality questions. series of near-contemporary works leverages pre-trained LLMs themselves as an untapped resource for question generation to create \"fully data-free\" co-evolving systems (Zhao et al., 2025a; Huang et al., 2025; Kuba et al., 2025; Fang et al., 2025; Chen et al., 2025a). These works all leverage intrinsic or proxy rewards such as majority vote, learnability, reward-model preferences, or gradient magnitudes. Because these methods optimize intrinsic or proxy objectives, they risk drifting to degenerate or unlearnable tasks, are sensitive to reward hacking and lack guarantees of progress (Chae et al., 2025). Prolonged RL with self-rewards often results in sudden and complete performance collapse (Shafayat et al., 2025; Chae et al., 2025), when rewards vanish or when generator and solver objectives misalign, especially in discrete, symbolic domains with essentially binary correctness signals. This fragility mirrors earlier findings in unsupervised curriculum generation (Dennis et al., 2020; Racaniere et al., 2020; Jiang et al., 2021) and connects directly to the broader question of whether self-improvement driven by intrinsic or self-generated rewards can be sustained within RL. To our knowledge, our work is the first for LLM self-play to ground the curriculum generation in concrete failure regime instead of internal proxies of difficulty. Intrinsic Rewards versus Bilevel Optimization Yet the use of proxy rewards is often not merely design preference but pragmatic simplification, especially in teacher-student self-play setups: it avoids facing an explicit inner-loopouter-loop bilevel optimization probleman appealing but challenging objective where the output of one optimization (in this instance the optimization of the student trained with RLVF on the teachers question-answer pairs) is fed into another optimization loop (the performance improvement of the student on the hard dataset). Such bilevel optimization appears in meta-learning (Finn et al., 2017; Nichol et al., 2018), hyperparameter learning (Maclaurin et al., 2015) and - partially inspiring our work - in dataset distillation, where an outer loop optimizes generally small dataset that allows an inner training loop to achieve good target performance (Wang et al., 2018; Deng and Russakovsky, 2022; Feng et al., 2024). In general, such approaches become intractable, as the inner loop involves multi-step computation with large number of steps, which requires backpropagation through time (BPTT), unrolling the inner loop and taking meta-gradients. Our approach, however, avoids the need to unroll the inner loop thanks to the use of RLOO in the outer loop, using the performance improvement of the student as the reward to reinforce question-answer sets. This is the first instance of double meta-RL loop we are aware of in the context of self-play for LLMs."
        },
        {
            "title": "3 Method",
            "content": "Can pretrained LLM leverage latent knowledge to generate synthetic question-answer pairs for problems it cannot solve? And in particular, can this be achieved in domains with sparse, binary rewards lacking automatic question verification? To explore this, we introduce SOAR: meta-RL framework designed to surface such pedagogical signals. Critically, SOAR grounds the teacher reward in measured student progress rather than intrinsic proxy rewards. If the model can generate useful stepping stones despite being unable to solve the original problems, this would suggest that the latent knowledge exists, and is extractable without human curation. Let πθ be language model with parameters θ. We assume access to dataset = {(qi, ai)}D of difficult i=1 question-answer pairs (πθ produces 0/128 successful generations). is split into train and test sets: Dtrain, Dtest. To improve the performance of πθ on Dtest, the natural approach is to train πθ directly on Dtrain using RL (e.g., REINFORCE, GRPO, RLOO, etc). However, for difficult datasets, this may not improve performance due to the sparsity of positive rewards, as we illustrate in our experiments. We instead use this failure regime\" as testbed to see if the model can autonomously recover intermediate problems that make these hard problems more learnable."
        },
        {
            "title": "3.1 Overview",
            "content": "Our framework adopts teacher-student setup, inspired by asymmetric self-play, to kickstart\" learning on datasets where the initial success rate is too low for successful training. We instantiate two copies of the same . At step zero, θ = ϕ = θbase. model: teacher πT ϕ The teachers role is to generate synthetic problems that provide the student with the necessary gradient signal to escape the performance plateau. Intuitively, while the teacher may be unable to solve difficult problem directly, it may still possess the knowledge to generate easier problems that provide non-zero reward to the student and shift its policy towards progress on the original problem. and student πS θ We formulate this problem as bilevel optimization problem. The objective is to generate small synthetic dataset = {(qi, ai)}n on with RL improves performance on the target domain. of question-answer pairs such that training πS θ i=1 max ϕ πT ϕ (cid:104) (cid:16) πS θ(X ), Dtrain (cid:17)(cid:105) subject to θ(X ) = RL-update(θ, ), (1) where RL-update describes the RL training procedure of the student on , yielding parameters θ(X ), and denotes the updated students performance on Dtrain. Such bilevel optimization objectives have strong historical precedence in meta-learning (Finn et al., 2017; Nichol et al., 2018), hyperparameter learning (Maclaurin et al., 2015) and dataset distillation (Wang et al., 2018; Deng and Russakovsky, 2022; Feng et al., 2024). In general, such approaches become intractable, requiring backpropagation through gradient descent, unrolling the inner loop and taking meta-gradients. To avoid the computational difficulties of unrolling the inner loop, we instead instantiate objective (1) as nested meta-RL loop: Outer (teacher) RL loop: we train the teacher with RLOO (Ahmadian et al., 2024) to generate synthetic question-answer pairs. Inner (student) RL loop: we train the student with standard RLVR (also with RLOO) to answer the teacher-generated problems. We use the subsequent performance improvement of the student on Dtrain as the black-box reward signal for the teacher. 4 Critically, we do not assume automatic verification of synthetic question well-posedness or answer correctness (as e.g., in coding tasks in Zhao et al. (2025a)). Instead, the teacher generates both the question and answer, treating the usefulness of the question as an emergent property of the teachers reward signal. The key insight is to ground the teachers objective in measured student progress on Dtrain, rather than intrinsic proxies such as learnability, as done in prior work. SOAR only rewards synthetic question-answer pair (qi, ai) if training on it improves the students performance on ground-truth problems. This black-box grounding signal tethers question generation to real learning progress, implicitly penalizing degenerate problems and reward hacking. Notably, the teacher is not shown the hard problems during training, but rather discovers useful stepping stones purely from this student improvement signal. In the following sections we detail the outer and inner RL loops. Our high-level procedure is shown in Figure 2, with full algorithm in Algorithm 1. Figure 2 The SOAR meta-RL Loop. The teacher and student are initialized from the same model. In the outer RL loop the teacher generates candidate question-answer pairs that are partitioned into datasets. In the inner RL loop, the student is trained for 10 steps on the candidate problems and evaluated on sampled hard problems. The teacher is rewarded based on the resulting student improvement over the student baseline, grounding the synthetic curriculum in real learning progress."
        },
        {
            "title": "3.2 Outer Loop: Teacher Training",
            "content": "We train the teacher with RLOO to generate problems that demonstrably improve student performance. Let denote the RLOO group size and the size of the generated dataset . At each iteration, we sample rollouts y1, . . . , ygn from πT , subdivided into datasets of items each: X1 = {y1, . . . , yn}, . . . , Xg = ϕ {yg(n1), . . . , ygn)}. Since we cannot automatically verify the answers to proposed problems, we prompt the teacher to generate both the question and answer. Each rollout yi is parsed into yi = (qi, ai) (described in Appendix B.2; we may need to sample multiple times to obtain parseable yi). Each dataset Xk receives reward as follows. At each outer-loop iteration we subsample set of reward questions QR Dtrain from the original training set. For each dataset Xk, we execute the inner loop in Figure 2 by training the student for fixed number of steps on Xk, resulting in trained student πS (see θ Section 3.3). The dataset-level reward R(Xk) is then the average greedy success of trained student πS on the θ questions QR relative to the success of baseline student model πS θ : R(Xk) = Acc(πS θ (QR)) Acc(πS θ (QR)). where πS θ is the initial student when starting the inner loop. To mitigate student training noise and reward variance, we average rewards over parallel student trainings per dataset. This averaged reward is assigned to each rollout in Xk to update the teacher."
        },
        {
            "title": "3.3 Inner Loop: Student Training",
            "content": "The student πS trains on the teacher-generated dataset Xk using RLOO. We train the student for small θ number of RL updates (10 steps with batch size 8). This is long enough to induce measurable movement in the student, but short enough to keep the student-training computationally cheap. After each inner loop the student reverts to the baseline policy for the next iteration. key question is whether the teacher is capable of adapting to an improving student, while accumulating stepping stone questions over different learning stages. To address this, we introduce promotion mechanism to accumulate student improvement across inner loops. Precisely, we track moving average of teacher rewards Rt. When Rt exceeds fixed threshold τ , we promote the student trained on the best Xk: namely, we reset the baseline student πS to the improved student, so subsequent rewards measure improvement relative to this θ new baseline (further details in Appendix B.3). The accumulated datasets that led to student promotion, which we call Dbest, constitute the Promotion Questions (PQ) that we evaluate in our experiments."
        },
        {
            "title": "4.1 Models and Datasets\nAll experiments are conducted with Llama-3.2-3B-Instruct. To study the prototypical setting of sparse,\nbinary rewards, without automatic question-answer verification (as present in code, for instance) we focus on\nmath reasoning tasks, where this setting is common. We use three such benchmarks: MATH (Hendrycks\net al., 2021), HARP (Yue et al., 2024), and OlympiadBench (He et al., 2024). These datasets cover a range of\nwidely recognized math competitions (AMC, AIME, USA(J)MO, Olympiads).",
            "content": "For each dataset, we identify difficult problems by sampling 128 times with Llama-3.2-3B-Instruct, and retaining problems with 0/128 success rate. We choose 128 as practical but stringent threshold, and find empirically that it is sufficiently difficult such that direct training leads to only marginal performance improvement. We call these subsets fail@128 datasets. Each is randomly split 50-50 into training and held-out test sets. Given the low baseline pass rates on fail@128 problems, this larger test set is necessary to distinguish observed performance gains from stochastic variance. Further dataset details in Appendix B.5."
        },
        {
            "title": "4.2 Teacher-student training\nWe train with SOAR on MATH and HARP, keeping OlympiadBench held-out to test cross-dataset generaliza-\ntion. Both the teacher and student are initialized from Llama-3.2-3B-Instruct. We allocate a max budget of\n200 outer-loop steps based on compute constraints.",
            "content": "At every outer-loop iteration we sample = 64 problems (X ) from the teacher, and 64 reward questions (QR) from the fail@128 train set (Dtrain). We track the moving global average of teacher rewards over the most recent 3 steps, and promote the student baseline if the moving average exceeds τ = 0.01. Full hyperparameters are reported in Appendix B.7 with ablations sensitivity to τ and in Appendix D.2. Analysis of SOAR training dynamics is in Appendix E."
        },
        {
            "title": "4.3 Evaluation\nOnce training completes, we test if the generated problems improve performance on Dtest. Based on\nobservations of teacher reward plateaus in initial runs, we evaluate the teacher at checkpoints where training\nrewards stabilize: step 200 for MATH and step 170 for HARP.",
            "content": "We assess two aspects of SOAR: Promoted Student (PS). For training runs that reached multiple promotions, we evaluate the student model with the best validation performance (i.e., best Dtrain greedy accuracy) on the test set to measure direct performance gains from SOAR. In practice we observe maximum of four promotions; thus the PS model has been trained on one of {128, 192, 256} synthetic questions. 6 Promotion Questions (PQ). We train fresh base student on Dbest with standard RLOO on combination of PQ and the fail@128 train set. This isolates the value of the synthetic questions, separate from the specific training trajectory of the promoted student. We test two mixing strategies. Curriculum trains on synthetic questions only for 64 steps, then Dtrain questions only. Mixed trains with synthetic and Dtrain questions together for the full training period. Based on experiments with our baselines (Appendix B.6), we use curriculum training for MATH and mixed training for HARP and OlympiadBench across all methods. We use the same strategy for all methods on each dataset. We denote PQ from MATH and HARP training as PQ-MATH and PQ-HARP respectively."
        },
        {
            "title": "4.4 Baselines\nHard-Only. We train Llama-3.2-3B-Instruct directly on the Dtrain (real fail@128 train set) with a standard\ngroup size of 32. To disentangle the effects of the meta-RL loop from just using additional compute, we also\ntrain with group size 128 on MATH.",
            "content": "Intrinsic Teacher (Intrinsic-T). To isolate the effects of grounding rewards, we compare to an intrinsic, data-free baseline. We train using the same procedure and hyperparameters as SOAR, but replace the grounded signal with learnability objective (Zhao et al., 2025a; Sukhbaatar et al., 2017) that rewards questions of moderate difficulty. We evaluate by sampling 128 problems from learnability-trained teacher (Intrinsic-T ) and training fresh student on combination of the sampled questions and the fail@128 train set, using the same protocol as PQ evaluation. Details on learnability training in Appendix B.4. Upper bound. We train fresh student on combination of the official MATH train split (6750 problems) and the fail@128 train set. This shows what performance looks like with curated easier problems, providing reference for synthetic stepping stones."
        },
        {
            "title": "4.5 Metrics\nWe report the pass@k accuracy on the held-out fail@128 test set for k ∈ {1, 4, 8, 16, 32}, using 32 samples per\nproblem. We run all evaluations for 6-12 seeds, nested across teacher/student training, (Appendix B.8) and\nreport the median and standard deviation.",
            "content": "Student Early Stopping. For experiments where we train fresh students, on MATH/HARP we select student checkpoints at the convergence point of the smoothed training reward curve, specifically where the reward gradient falls below fixed threshold. This alleviates noise from small validation sets and ensures fair comparison between methods with differing convergence rates; full discussion is in Appendix B.6. On OlympiadBench, where convergence is more uniform, we report at 50 steps. Full training trajectories are in Figure 9."
        },
        {
            "title": "5.1 Meta-RL Discovers Effective Questions.",
            "content": "While curriculum learning is well-studied in RL, it is not obvious that synthetic questions can help model move \"beyond sharpening\" its existing distributions. Here, we show that self-generated stepping stones provide learnable gradient that unlocks improvement in stalled regimes. This occurs without the teacher seeing the target problems; instead, meta-RL sharpens the teachers policy, discovering useful curricula solely by optimizing for student progress. PQ Kickstarts Learning on Hard Subsets. Both PS and PQ substantially outperform Hard-Only and Instrinsic baselines, with larger gains at higher k. Figure 3 shows improvement over Hard-Only. Hard-Only test trajectories are in Figures 5; all absolute numbers and trajectories are in Appendix C.1-C.2. Inference with the base model achieves non-zero pass@k due to stochastic sampling with different seeds than were used for the initial fail@128 filtering; nonetheless, Hard-Only training cannot sustain learning and plateaus. Inference with PS achieves +8.5% pass@32 on fail@128-MATH over Hard-Only, and +3.6% pass@32 on fail@128HARP. PQ achieves higher mean performance (+9.3% pass@32 on MATH, +4.2% on HARP), indicating 7 Figure 3 Performance on MATH and HARP fail@128 (improvement over HardOnly). Synthetic problems generated with SOAR (PQ) and inference with the promoted student (PS) outperform direct training on fail@128 train sets (HardOnly), and sampling from teachers trained with intrinsic rewards (Intrinsic-T ). Performance is reported as the delta over Hard-Only. For reference, Hard-Only MATH pass@k for {1, 4, 8, 16, 32} is {0.5, 1.7, 3.2, 5.7, 9.6}. Hard-Only training curves are shown in Figure 5; absolute performance for all methods, and further evaluations, are in Tables 4-5. Shaded regions are 1 SD over 6-12 seeds nested across teacher/student training (see B.8). Figure 4 Transfer performance to OlympiadBench fail@128 subset (improvement over Hard-Only). Questions optimized for MATH and HARP transfer to held-out dataset. Performance is reported as the delta over Hard-Only; absolute performance, including PS evaluation, is in Table 6. that the synthetic questions, rather than fortunate student training trajectory, drive the performance gains. Intrinsic-T underperforms both, validating that grounded rewards are needed to discover the right questions. Synthetic questions do not just boost accuracy, but shift the student policy to make previously hard problems learnable. Student learning curves on MATH, where we use curriculum training, exhibit continued improvement after transitioning to fail@128 training (Figure 9). These effects significantly outstrip what can be achieved from repeated sampling alone on fail@128 data; Hard-Only with group size of 128 (4 extra compute) achieves only +2.8% pass@32  (Table 4)  . OOD generalization. Figure 4 shows that synthetic questions from PQ-MATH, PQ-HARP, and Intrinsic-T transfer to OlympiadBench, an OOD dataset (+6% and +3% respectively over Hard-Only). Cross-dataset transfer, despite no OOD optimization, suggests that synthetic curricula can capture generalizable reasoning pathways. Oracle comparison to real curated data. Our regime assumes that we only have access to hard problems, to study the case where additional expert-curated data is not available or not known. As strong upper-bound, we compare to the oracle\" case where curated extra data is available. We train students on fail@128 + the full official MATH training set (6750 problems) as representative pool of abundant, easier questions. We also compare to training with 128 random MATH/HARP questions in Appendix C.2, which performs similarly to training with the full dataset. Synthetic PQ-MATH questions recover 75% of the performance gains from full-MATH training, and PQ-HARP recover 50%. Notably, HARP-PQ (128/192 questions) outperforms 128 real HARP questions, and matches 128 real MATH questions. Direct inference on fail@128 test problems with the final trained teacher policy model does not improve over base model performance (Appendix C.2), indicating that generator and solver abilities are largely independent. Takeaway: models pedagogical ability can be decoupled from its task-solving ability. Grounded meta-RL (SOAR) expands the learnability frontier\" by surfacing synthetic questions that enable improvement over reasoning plateaus."
        },
        {
            "title": "5.2 Grounded rewards lead to stable and diverse teacher policies.\nWhile the main utility of SOAR is in surfacing a set of teacher-generated questions that unlock student learning\n(PQ), we now shift focus to the trained teacher policies themselves. In this section we perform a controlled",
            "content": "8 Figure 5 Grounded rewards lead to more stable teacher policies. We evaluate trained teacher policies by sampling questions and training fresh students. (Left) Test pass@32 comparison between students trained with questions sampled from Grounded-T and Base-T (Hard-Only also shown for reference). Grounded-T outperforms Base-T and exhibits more stable student trajectories. (Right) Pass@32 trajectories for fresh students trained with individual Grounded-T teacher seeds (red) and Intrinsic-T teacher seeds (green). Questions from Grounded-T yield consistent student trajectories, whereas Intrinsic-T exhibits higher variance across teachers, including failure mode where I-T (1) causes student collapse. Shading shows 1 SD. Curves for other pass@k and OlympiadBench are in Figures 10-12. study of teacher objectives to probe the effects of meta-RL, and show that grounded rewards (as in SOAR), versus intrinsic ones, yield stronger teacher policies. We evaluate teachers trained with grounded rewards (Grounded-T ), intrinsic rewards (Intrinsic-T ) and the base model (Base-T ) by sampling question-answer pairs from these policies and training fresh students. In Appendix C.3 we also ablate grounded teachers trained without the student-promotion mechanism, to validate its necessity. We evaluate four Grounded-T seeds per dataset to cover range of final promotion stages, and three Intrinsic-T teacher seeds. We sample 128 questions from these teachers and train 2-3 fresh students on the synthetic questions and real fail@128 train set ( 9 student runs per reported metric, see Appendix B.6). The teacher policy generates useful questions. Student test performance curves in Figure 5 reveal that questions sampled from Grounded-T improve over Hard-Only. Results are competitive with PQ on MATH and HARP, validating that the useful pedagogical signal is not just captured in the set of evolved questions, but is also learned by the teacher policy. Further ablations show that sampling larger datasets from Grounded-T reduces the variance of student outcomes (Appendix D.1) and that the student-promotion mechanism improves the teacher policy (Appendix C.3). Meta-RL sharpens the question distribution. In Figure 5 (left) we overlay student training curves for Grounded-T questions and Base-T questions. Grounded-T students consistently track the upper envelope of Base-T performance for MATH/HARP, with lower variance on MATH. The existence of successful runs from Base-T reveals the ability to generate useful stepping stone questions is latent in the model; meta-RL improves Grounded-T by sharpening the teacher to output questions that more reliably provide useful gradient signal. This is yet another example of the sharpening mechanism of RL (Yue et al., 2025; Zhao et al., 2025b; Tsilivis et al., 2025a,b), but here leveraged for curricula. On OlympiadBench, where the target distribution differs substantially from the teachers training domain, Grounded-T and Base-T learning curves overlap more (though Grounded-T on HARP achieves highest peak performance), suggesting that meta-RL primarily sharpens in-domain pedagogical signals. This is consistent with PQ results in Figure 4, in which PQ-HARP Figure 6 Qualitative Evolution of Generated Questions. (Left) Baseline student performance during SOAR run on HARP. The y-axis shows greedy accuracy on the fail@128 train set over promotion stages. (Right) Sampled teacher questions at different promotion points. Content and style shift from word problems and basic formulas (stage 1) to concise, equation-heavy problems in algebra and calculus (stage 2). Many effective stepping stones\" include incorrect solutions, suggesting that structural and conceptual content provide sufficient learning signal. outperforms Intrinsic-T whereas PQ-MATH matches it Fragility of intrinsic proxies. Figure 5 (right) compares aggregate student training curves for individual Grounded-T and Intrinsic-T teacher seeds. Students trained with questions from different Grounded-T seeds exhibit highly similar trajectories, indicating that grounded rewards lead to stable teacher policies. In contrast, Intrinsic-T teachers produce, on average, worse and more volatile outcomes. Across MATH, HARP, and OlympiadBench there is clear separation in performance between students trained with different Intrinsic-T seeds. MATH and OlympiadBench student trajectories exhibit consistent and significant ordering depending on the teacher. While some Intrinsic-T teachers produce highly effective curricula, the objective is subject to high-variance failure mode: one out of three teacher seeds exhibits collapse across all datasets, yielding little or no progress on the target problems. This reinforces observations from the literature that RL with self-rewards is prone to reward hacking, or the decoupling of the intrinsic reward from actual task mastery (Shafayat et al., 2025; Chae et al., 2025). Grounded Training Sustains Diversity. To probe how meta-RL shapes the teachers generative distribution, in Table 1 we measure the semantic diversity of datasets from different teachers with the Vendi Score (V S) (Friedman and Dieng, 2022) using Qwen3-8B embeddings (Zhang et al., 2025). Grounded-T (MATH) and Grounded-T (HARP) match the diversity of Base-T (V = 34.91), with PQ showing only small decline from the base model (V = 31.75). In contrast, Intrinsic-T collapses into narrow conceptual space (V = 10.82), providing evidence of reward-hacking and potential explanation for the observed fragility\". This suggests that grounded rewards successfully avoid the diversity collapse often seen in RL-loops (Song et al., 2025), while intrinsic rewards fall prey to it. Indeed, we also observe decline in the diversity of teacher completions during meta-RL with learnability rewards (Appendix E). Takeaway: Effective questions are latent in the base model, but hard to find. Grounding rewards in student progress \"sharpens\" the teachers noisy distribution of questions into stable, diversitypreserving policy, whereas intrinsic rewards are prone to instability and diversity collapse."
        },
        {
            "title": "5.3 Question structure matters more than answer correctness.",
            "content": "While conventional wisdom suggests that question-answer correctness is most important, our results suggest that the conceptual content and structure of questions is more important for models on learning plateaus. Figure 6 shows qualitative examples of PQ questions at different stages of sample SOAR training trajectory, exhibiting shifts in style and conceptual focus as the baseline student improves. We annotate synthetic 10 questions with Claude-4.5-Sonnet as an oracle judge, and observe that only 32.8% of PQ problems contain fully correct solution, while 63% are considered mathematically well-posed (Appendix C.4). This suggests that for models stalled on performance plateau, structural and contextual cues of question are more important for kickstarting learning than correct answer. Indeed, Intrinsic-T questions have higher correctness (55%) but perform worse, likely because of lack of diversity (Section 5.2). Our experiments with Base-T , which, like Grounded-T and Intrinsic-T , is filtered for correctly formatted questions, show that question format alone is not behind these effects. more detailed taxonomy of synthetic questions, including error types, is in Appendix C.4. Meta-RL decreases question ambiguity errors relative to Base-T , validating the importance of question coherence over answer correctness. Method Vendi Score (V S) Std. Dev (σ) Base-T Grounded-T (HARP) Grounded-T (MATH) PQ Intrinsic-T 34.91 34.66 31.99 28.33 10.82 1.74 1.74 1.54 1.55 1.01 Table 1 Semantic diversity analysis of synthetic datasets using Vendi Scores (V S). All metrics are standardized to 128 questions via bootstrap subsampling (k = 100 iterations). represents the effective number of unique semantic concepts. Our proposed teacher training (Grounded-T ) successfully expands the conceptual manifold. Takeaway: For models at learning plateaus, problems that have conceptually diverse and coherent questions can provide useful gradient signal even without having precisely correct answers."
        },
        {
            "title": "6 Discussion and Conclusions",
            "content": "Breaking the sparse-reward plateau in RL fine-tuning. Our work establishes way to kickstart RL fine-tuning when the initial success rate is too low to collect RLVR signal. Generating question-answer pairs (even if not correct) and training on those, with the right meta-RL self-play loop, can be enough to provide nonzero signal on the original hard problems. Contrary to learnability approaches that rely on pure internal rewards, as is the case in prior LLM self-play approaches, here the signal is ultimately grounded in measuring improvement on the original problems. central contribution of our work is that we show how to make this grounded bilevel meta-RL loop work in practice. The gap in performance shows the importance of this point. More importantly, our setup shows that generating stepping-stone questions to solve problem does not require the preexisting ability to solve that problem, and that meta-RL sharpens this latent ability in the pretraining distribution. This intuition lies at the core of the self-play idea, although we show that it is crucial to go beyond pure curiosity by grounding the process in actual performance. Our results tie to the broader debate on whether RL fine-tuning truly expands models learning frontier, or merely sharpens latent abilities (Yue et al., 2025; Zhao et al., 2025b; Tsilivis et al., 2025a,b). Our work indicates that meta-RL can expand the envelope of learnability beyond what direct RLVF can achieve. As North Star thought experiment, consider future model trained on the entire mathematical literature: proof of Millennium Problem such as the Riemann Hypothesis may already be latent in pretraining, yet successful learning would hinge on recovering the right sequence of intermediate lemmas and theorems that make the proof learnable to student reasoner. In this view, just as RL is believed to sharpen or amplify useful subsets of pretraining data, meta-RL could retrieve the stepping-stone questionanswer pairs embedded in the teachers vast training corpus. We believe our results provide concrete evidence that moderate amount of grounded meta-RL can elicit such capabilities that remain inaccessible through repeated sampling alone. Limitations. Our frameworks primary limitation is the computational cost of running bilevel RL loops (Appendix B.9). While inner loop training is relatively cheap (10-20 steps depending on the promotion stage) it necessitates training parallel students to compute stable teacher rewards. Importantly, our ablation in Table 4 shows that reallocating compute to direct training on hard problems via repeated sampling does not recover the improvements achieved by the bilevel framework. Our work serves as proof of concept for grounded rewards in this setting; investigating more efficient reward proxies or scaling beyond our 3B model experiments are rich avenues for further work."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Cansu Sancaktar and Phillip Isola for helpful discussions. JK thanks the Simons Foundation for support through the Collaborative Grant The Physics of Learning and Neural Computation. This work was supported by an NSF GRFP fellowship to SS."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1224812267, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.662. URL https://aclanthology. org/2024.acl-long.662/. AlphaProof and AlphaGeometry teams. Olympiad-level formal mathematical reasoning with reinforcement learning. Nature, 640:731739, November 2025. doi: 10.1038/s41586-025-09833-y. URL https://www.nature.com/articles/ s41586-025-09833-y. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. Online difficulty filtering for reasoning oriented reinforcement learning, 2025. URL https://arxiv.org/abs/2504.03380. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. Justin Yang Chae, Md Tanvirul Alam, and Nidhi Rastogi. Towards understanding self-play for llm reasoning, 2025. URL https://arxiv.org/abs/2510.27072. Lili Chen, Mihir Prabhudesai, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Self-questioning language models, 2025a. URL https://arxiv.org/abs/2508.03682. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for LLM reasoning, 2025b. URL https://openreview.net/forum? id=lNgSdqKFmU. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=O4cHTxW9BS. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https: //arxiv.org/abs/2501.12948. Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural networks. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2022. Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning, 2016. URL https://arxiv.org/abs/1611.02779. Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, and Dacheng Tao. Serl: Self-play reinforcement learning for large language models with limited data, 2025. Yunzhen Feng, Shanmukha Ramakrishna Vedantam, and Julia Kempe. Embarrassingly simple dataset distillation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= PLoWVP7Mjc. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 11261135. JMLR.org, 2017. Dan Friedman and Adji Bousso Dieng. The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410, 2022. Alex Graves, Marc G. Bellemare, Jacob Menick, Rémi Munos, and Koray Kavukcuoglu. Automated curriculum learning for neural networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, page 13111320. JMLR.org, 2017. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for 13 promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402. 14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data, 2025. URL https://arxiv.org/abs/2508.05004. Minqi Jiang, Michael Dennis, Jack Parker-Holder, Alexandre Bayen, Stuart Russell, Sergey Levine, and Andrew Critch. Replay-guided adversarial environment design. In Advances in Neural Information Processing Systems (NeurIPS), 2021. Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, and Zico Kolter. Adaptive data optimization: Dynamic sample selection with scaling laws. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=aqok1UX7Z1. Yeganeh Kordi, Nihal Nayak, Max Zuo, Ilana Nguyen, and Stephen Bach. Revisiting generalization across difficulty levels: Its not so easy. arXiv preprint arXiv:2511.21692, 2025. Jakub Grudzien Kuba, Mengting Gu, Qi Ma, Yuandong Tian, Vijai Mohan, and Jason Chen. Language self-play for data-free training, 2025. URL https://arxiv.org/abs/2509.07414. Hynek Kydlíček. Math-verify: Math verification library. https://github.com/huggingface/math-verify, 2025. Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through reversible learning. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pages 21132122, 2015. URL https://proceedings.mlr.press/v37/maclaurin15.html. Maren Mahsereci, Lukas Balles, Christoph Lassner, and Philipp Hennig. Early stopping without validation set. ArXiv, abs/1703.09580, 2017. URL https://api.semanticscholar.org/CorpusID:14520242. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: framework and survey. J. Mach. Learn. Res., 21(1), January 2020. ISSN 1532-4435. Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), pages 51865198, 2021. Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms, 2018. URL https: //arxiv.org/abs/1803.02999. OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben DSa, Arthur Petron, Henrique P. d. O. Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu, and Wojciech Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation, 2021. URL https://arxiv.org/abs/2101.04882. Shubham Parashar, Shurui Gui, Xiner Li, Hongyi Ling, Sushil Vemuri, Blake Olson, Eric Li, Yu Zhang, James Caverlee, Dileep Kalathil, and Shuiwang Ji. Curriculum reinforcement learning from easy to hard tasks improves llm reasoning, 2025. URL https://arxiv.org/abs/2506.06632. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak. Maximizing confidence alone improves reasoning, 2025. URL https://arxiv.org/abs/2505.22660. Sebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, and Timothy Lillicrap. Automated curriculum generation through setter-solver interactions. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1e0Wp4KvH. Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train?, 2025. URL https://arxiv.org/abs/2505.21444. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew H. Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419): 11401144, 2018. doi: 10.1126/science.aar6404. URL https://www.science.org/doi/10.1126/science.aar6404. Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alexander Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-solving with language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=lNAyUngGFK. Expert Certification. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning, 2025. URL https: //arxiv.org/abs/2509.06941. Sainbayar Sukhbaatar, Manohar Paluri, Gabriel Synnaeve, Arthur Szlam, and Rob Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In International Conference on Learning Representations (ICLR), 2017. Yiyou Sun, Yuhan Cao, Pohao Huang, Haoyue Bai, Hannaneh Hajishirzi, Nouha Dziri, and Dawn Song. Rl grokking recipe: How does rl unlock and transfer new algorithms in llms?, 2025. URL https://arxiv.org/abs/2509.21016. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Nikolaos Tsilivis, Eran Malach, Karen Ullrich, and Julia Kempe. How reinforcement learning after next-token prediction facilitates learning. In EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM), 2025a. URL https://openreview.net/forum?id=olUqaphLDa. Nikolaos Tsilivis, Eran Malach, Karen Ullrich, and Julia Kempe. How reinforcement learning after next-token prediction facilitates learning, 2025b. URL https://arxiv.org/abs/2510.11495. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. Yibo Wang, Hai-Long Sun, Guangda Huzhang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Lijun Zhang. Triplets better than pairs: Towards stable and effective self-play fine-tuning for LLMs. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=Hk4cCTukeI. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum SFT, DPO and RL for long COT from scratch and beyond. In Georg Rehm and Yunyao Li, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 318327, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-288-6. doi: 10.18653/v1/2025.acl-industry.24. URL https://aclanthology.org/2025.acl-industry.24/. Ziyu Ye, Rishabh Agarwal, Tianqi Liu, Rishabh Joshi, Sarmishta Velury, Quoc V. Le, Qijun Tan, and Yuan Liu. Scalable reinforcement post-training beyond static human prompts: Evolving alignment via asymmetric self-play, 2024. URL https://arxiv.org/abs/2411.00062. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Albert S. Yue, Lovish Madaan, Ted Moskovitz, DJ Strouse, and Aaditya K. Singh. Harp: challenging human-annotated math reasoning benchmark, 2024. URL https://arxiv.org/abs/2412.08819. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=4OsgYD7em5. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Zilong Zheng, Kai Fan, Jifan Chen, and Jindong Li. Absolute zero: Reinforced self-play reasoning with zero data. In Advances in Neural Information Processing Systems (NeurIPS), 2025a. URL https://openreview.net/forum?id=neZSGqhxDa. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025b. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards, 2025c. URL https://arxiv.org/abs/2505.19590. Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature regression. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2022. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, Biqing Qi, Youbang Sun, Zhiyuan Ma, Lifan Yuan, Ning Ding, and Bowen Zhou. TTRL: Test-time reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=VuVhgEiu20. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting language models, 2025."
        },
        {
            "title": "A Extended Related Work",
            "content": "A.1 Curriculum Learning in RL Automated curriculum design has long history predating modern LLMs, beginning with classical curriculum learning (Bengio et al., 2009; Graves et al., 2017). These methods assume access to labeled training set and focus on reordering or selecting existing data rather than generating new tasks. In the context of RL, curriculum learning helps agents acquire complex behaviors by first mastering simpler tasks (Narvekar et al., 2020; Parashar et al., 2025). Contemporary LLM post-training inherits this paradigm: curriculum is applied over curated prompts or problem categories, using proxy signals such as gradient norms or advantage estimates to guide selection. Examples include synthetic or self-training curricula like Kimi (Team et al., 2025), FastCuRL (Dennis et al., 2020), and LightR1 (Wen et al., 2025), as well as online difficulty-filtering strategies such as Dapo (Yu et al., 2025), Online Difficulty Filtering (Bae et al., 2025), and SEC (Chen et al., 2025b), which discretize problems into difficulty buckets and score categories by gradient-derived proxies. While these approaches improve learning efficiency in-distribution or OOD, they presuppose that difficulty can be meaningfully partitioned priori and provide only indirect rewards for student progress. Adaptive Data Optimization (ADO) (Jiang et al., 2025) leverages per-domain scaling laws to estimate the learning potential of various data sources online Jiang et al. (2025). By contrast, our goal is not to arrange data but to elicit learning on fixed, verifiable hard dataset where standard GRPO fails. A.2 Self-Play and Teacher-Student Setups Self-play offers complementary lens on autonomous capability growth, classically exemplified by game-playing agents trained without external data, such as AlphaZero (Silver et al., 2018). Our approach is inspired by line of research demonstrating that asymmetric self-play can induce powerful automatic curricula. In early work, Sukhbaatar et al. (2017) introduced the canonical AliceBob framework in which one agent (Alice) proposes tasks while another (Bob) attempts to solve them, yielding natural progression of justhard-enough challenges that drive learning. This idea was later extended to complex embodied domains in robotics, where asymmetric self-play enabled automatic discovery of diverse manipulation goals without manual task specification (OpenAI et al., 2021). Applying these ideas from robotics and control to large language models introduces fundamentally different challenges: LLMs operate over discrete, symbolic problem space with no environment simulator to evaluate intermediate progress; teacher must generate entire tasks, often requiring multi-step reasoning. Moreover, rewards in language domains are extremely sparse and brittlefor mathematical problems, correctness is essentially binary and offers no gradient toward partial solutions. Modern LLM self-play methods thus differ in mechanism: SPIN (Chen et al., 2024), Triplet self-play (Wang et al., 2025), and ReSTEM (Singh et al., 2024) optimize for self-consistency or solution quality. These methods generate responses and still presuppose the existence of well-formed input prompts or curated high-quality questions. Recent systems like AlphaProof (AlphaProof and teams, 2025) attempt to mitigate this sparsity at test-time by using an LLM to generate \"natural curriculum\" of auxiliary theorem variations for additional training (AlphaProof and teams, 2025). In the context of RLHF, eva (Ye et al., 2024) casts RLHF as an asymmetric creatorsolver game in which creator evolves prompts to expose alignment weaknesses and solver adapts to reward-model feedback. series of near-contemporary works leverages pre-trained LLMs themselves as an untapped resource for question generation. Such \"fully data-free\" co-evolving systemsincluding Absolute Zero (Zhao et al., 2025a), R-Zero (Huang et al., 2025), Language Self-Play (LSP) (Kuba et al., 2025), SeRL (Fang et al., 2025) and Self-Questioning Language Models (SQLM) (Chen et al., 2025a)jointly evolve task creators and solvers via intrinsic or proxy rewards such as majority vote, learnability, reward-model preferences, or gradient magnitudes. Because these methods optimize intrinsic or proxy objectives, they risk drifting to degenerate or unlearnable tasks, are sensitive to reward hacking where models learn to maximize training (pseudo-)reward, and lack guarantees of progress (see an analysis of AbsoluteZero in Chae et al. (2025)). This connects directly to line of works investigating the broader question of whether self-training the process where model learns from its own judgments can be sustained within RL, and how far self-improvement can be driven by intrinsic or self-generated rewards. Prolonged RL with self-rewards often results in sudden and complete performance collapse (Shafayat et al., 2025; Chae et al., 2025), when rewards vanish or when generator and solver objectives misalign, especially in discrete, symbolic domains with essentially binary correctness signals. This fragility mirrors earlier findings in unsupervised 17 curriculum generation (Dennis et al., 2020; Racaniere et al., 2020; Jiang et al., 2021). These observations motivate our design: we learn teacher policy via meta-RL that generates verifiable math questions directly optimized for student learning progress, grounding the curriculum in concrete failure regime instead of internal proxy of difficulty. A.3 Intrinsic Rewards versus Bilevel Optimization To our knowledge, essentially all recent fully data-free self-play approaches use intrinsic or proxy rewards to train the teacher/proposer, without anchoring to real student performance (with the exception of the self-adaptation work by Zweiger et al. (2025) which uses ReSTEM/SFT for outer/inner loop). Examples of intrinsic rewards include model confidence as proposed in Inuitor (Zhao et al., 2025c) or RENT (Prabhudesai et al., 2025) or the majority answer as in TTRL (Zuo et al., 2025) or Shafayat et al. (2025), as well as in SQLM (Chen et al., 2025a). Of course, the use of proxy rewards is often not merely design preference but pragmatic simplification, especially in teacher-student self-play setups: it avoids facing an explicit inner-loopouter-loop bilevel optimization problem - an appealing but challenging objective where the output of one optimization (in this instance the optimization of the student trained with RLVF on the teachers question-answer pairs) is fed into another optimization loop (the performance improvement of the student on the hard dataset). Such bilevel optimization objectives have strong historical precedence in meta-learning, in popular methods such as MaML (Finn et al., 2017) and Reptile (Nichol et al., 2018), which explicitly train through an inner-loopouter-loop structure to obtain efficient few-shot learners, following earlier research like RL2 (Duan et al., 2016), and works that meta-learn hyperparameters of neural nets via full backpropagation through the training loop (Maclaurin et al., 2015). similar bilevel formulation, which served as inspiration for our work, also appears in dataset distillation (Wang et al., 2018), where an outer loop optimizes generally small dataset that allows an inner training loop to achieve good target performance. Here, both proxy-based (e.g., NTK approximation (Nguyen et al., 2021) or feature-matching (Zhou et al., 2022)) and end-to-end bilevel formulations have been explored (Wang et al., 2018; Deng and Russakovsky, 2022; Feng et al., 2024). In general, such approaches become intractable, as the inner loop involves multi-step computation with large number of steps, which requires backpropagation through time (BPTT), or in fact backpropagation through gradient descent, unrolling the inner loop and taking meta-gradients. Our approach, however, avoids the need to unroll the inner loop thanks to the use of RLOO in the outer loop, using the reward (the performance improvement of the student) to reinforce question-answer sets. This is the first instance of double meta-RL loop we are aware of in the context of self-play for LLMs."
        },
        {
            "title": "B Method and Experiment Details",
            "content": "B.1 Prompts Teacher Prompt. At every outer-loop step, the teacher is given the same prompt. The prompt guides the model towards producing valid math problems using sample subjects/domains and provides explicit instruction regarding the expected format. We avoid seeding the teacher with sample math questions to preserve the data-free setup; the model only sees the black-box reward signal of student performance. We also observe in initial experiments that, when given seed questions, the teacher often collapses to copying them. 18 Teacher Prompt You are generating new math problem for math assistant. Allowed topics: Algebra, Counting and Probability, Geometry, Intermediate Algebra, Number Theory, Prealgebra, or Precalculus. Output rules (follow EXACTLY): - Provide the final formatted problem in this structure: <question>[full math question]<question><answer>boxed{[answer]}</answer> - Any explanations, steps, or reasoning about the problem goes OUTSIDE the <question> and <answer> tags. Constraints: - The problem must be original, challenging, and require at least 2--3 steps of reasoning. - Output exactly ONE problem. You MUST follow the specified format EXACTLY. Begin now: Student Prompt. The same prompt is used for fail@128 filtering, training the student in the inner-loop, and training the student in evaluation. Student Prompt conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first shows the complete reasoning process step by step, then provides the final answer in boxed{}. The assistant must always follow the format: User: [question] Assistant: [detailed reasoning] The final answer is: boxed{[answer]}. User: <QUESTION> Assistant: \" B.2 Parsing Teacher Outputs To parse the teacher rollouts into question-answer pairs, we require teacher responses to follow the promptspecified format. We filter out generations that do not follow this format, and resample until we have correctly-formatted problems. We filter for the following: Contains opening and closing question/answer tags. Contains the boxed\" notation (denoting an answer). Contents of the boxed answer are parsable by symbolic math verifier. Theoretically, rejection sampling does not affect the RLOO gradient update (Proposition 1); empirically, we find that this performs better than using teacher-format rewards or sequential question/answer sampling. Proposition 1 (RLOO update with rejection sampling). Let π0(z) be proposal distribution over some random variable z. Let be set of accepted values of z, and assume π0(S) > 0. Let π(z) = π0(z)1zS/π0(S) (2) be the distribution on obtained by rejection sampling, namely, sampling from π0 until S. Let R(z) be some reward function on z. Then the RLOO update on π can be computed from gradient of π0 only. Namely, for any g-tuple z1, . . . , zg sampled from π, one has where (cid:88) i=1 A(zi) ln π(zi) = (cid:88) i=1 A(zi) ln π0(zi) A(zi) = R(zi) 1 1 (cid:88) j=i R(zj) 19 (3) (4) is the RLOO advantage function, and where the gradients are with respect to the parameters of π. This is not true for simple Reinforce: it relies on the fact that RLOO advantages A(zi) sum to 0 over i. Proof. For any sampled from π, one has with probability 1. For S, one has ln π(z) = ln π0(z) ln π0(S). Therefore, (cid:88) i=1 A(zi) ln π(zi) = = = (cid:88) i=1 (cid:88) i=1 (cid:88) i= A(zi) ( ln π0(zi) ln π0(S)) (cid:33) A(zi) ln π0(S) (cid:32) (cid:88) i=1 A(zi) ln π0(zi) A(zi) ln π0(zi) (5) (6) (7) since the sum of advantages in RLOO satisfies (cid:80) A(zi) = 0. B.3 Training Details Algorithm 1 details our full algorithm. Input: Initial teacher πT Initialize timestep 0, EMA reward R0 0, Dbest while < do ϕ , initial student πS θ , threshold τ , group size g, dataset size n, repeats // 1. Teacher generation Sample QA pairs: {(qi, ai)}gn Partition into datasets: Xk = {(qj, aj)}nk Sample reward questions QR = {(qj, aj)}M i=1 πT ϕ j=n(k1)+1 for = 1, . . . , j=1 Dtrain // 2. Inner Loop for = 1 to do for = 1 to do k,j RLOO-Update(θ, Xk) {Student RL} θ Rk,j Acc(θ k,j, QR) Acc(θ, QR) end for Rk 1 (cid:80)r j=1 Rk,j end for // 3. Check for student promotion. (cid:80)g Update Rt EMA( Rt1, 1 if Rt > τ then k=1 Rk) arg maxk Rk Find such that Rk,j is the median reward in {Rk,j}r θ θ Dbest Dbest Xk k,j {Student Promotion} j= end if // 4. Teacher Policy Update (Outer-loop) ϕ RLOO-Update(ϕ, {(Xk, Rk)}g + 1 k=1) {Teacher RL} end while return Dbest, πS θ Algorithm 1: SOAR: Teacher-Student meta-RL Training Stabilizing teacher rewards. Training inner-loop students with RL can potentially lead to noisy trajectories, and thus noisy teacher rewards. To stabilize the teacher rewards, for each sampled dataset Xk we execute parallel student trainings and evaluations, and average their rewards to obtain the final reward: Rk = 1 j=1 Rk,j. In practice, we use = 4. (cid:80)r 20 Promotion mechanism. At each outer-loop timestep we train students on each dataset Xk, and promote\" the student baseline when the moving average of teacher rewards exceeds fixed threshold τ . We choose which trained student to promote by selecting the dataset Xk with the highest reward R(Xk) and then selecting the student with the median reward amongst those trained on Xk. Computing student rewards. For inner-loop and evaluation RL on the student, we use the Math-Verify package to compare the student-generated and ground-truth answers (Kydlíček, 2025). We assign reward following standard formulations for RLVR with math: R(y, a) = 120.0 20.0 10.0 0. if has_boxed(y) verify(y, a) if has_boxed(y) verify(. . . ) yans if has_boxed(y) verify(. . . ) / yans otherwise B.4 Learnability Reward. To ablate the effects of our grounded reward versus intrinsic rewards, we train teacher models using the well-studied learnability reward (Zhao et al., 2025a; Sukhbaatar et al., 2017). We use the same candidate- , we generation and dataset-partitioning procedure as SOAR. For each candidate dataset Xk = {qi, ai)}n sample 32 completions from the student for each qi and compute the average success rate si. The per-question reward is then computed as i=1 (cid:40) ri = 0, 1 si, if si = 0 otherwise. (8) We then compute the dataset-level reward as Rk = 1 i=1 ri. For consistency with SOAR, every rollout in Xk receives the averaged dataset-level reward. We train learnability teachers for 200 steps, and observe convergence of rewards. (cid:80)n B.5 Datasets Fail@128 Filtering. For each problem in the pool of candidates, we sample 128 solutions with Llama-3.2-3BInstruct using the student prompt in Appendix B.1, token budget of 1024 tokens, and temperature 1.0. We keep problems that obtained 0/128 success rate. OlympiadBench. For OlympiadBench, we source our fail@128 questions from the subset that is in English, text-only, and automatically verifiable (674 total questions). Since OlympiadBench was originally designed as test set, we construct random train/test split. HARP. We source our fail@128 problems from the full HARP dataset. Since HARP was originally designed as test set, we construct random train/test split. MATH. In preliminary experiments, we observed large gap between the zero-shot accuracy of Llama-3.2-3BInstruct on the official MATH training vs. test splits (60% vs. 37%), suggesting that the model may have partial exposure to the MATH training questions. To minimize confounding effects from such memorization, we draw our initial pool of hard problems from the 5000-problem official MATH test split. We then apply the fail@128 filter and construct our own internal train/test split from this filtered subset. All synthetic data generation and student-teacher training uses only the internal training split, and final results are reported exclusively on the held-out internal test split. Dataset sizes. In Table 2 we report the original size of each problem pool, and the sizes of our train/test splits. B.6 Evaluation Mixed synthetic-real training. We primarily evaluate generated questions by training fresh student model on combination of the synthetic questions, and the real fail@128 train set. We explore two mixing strategies: Table 2 Dataset sizes preand postfail@128 filtering. Dataset MATH HARP Olympiad Bench Initial problem pool 5000 4768 674 fail@128 train set 359 714 158 fail@128 test set 360 714 158 Curriculum training. We first train the student on synthetic questions for fixed number of training steps (64), and then switch to training on real fail@128 training questions, aiming to mirror the trajectory of training promoted student. Here, the synthetic questions act as warm-start\", enabling the student to obtain gradient signal on the harder problems. The synthetic training window was chosen as representative budget based on preliminary experiments. Mixed training. We train on mixture of synthetic and real questions throughout. To avoid biasing results, we select between curriculum/mixed training using our baseline methods. On MATH, while both exhibit similar training dynamics, we found that our Base-T baseline performed better with curriculum and thus adopt it for all MATH experiments (Figure 7). On OlympiadBench and HARP we observed that mixed training yields significantly more stable learning dynamics, even when adding real instead of synthetic data. Figure 8 compares mixed/curriculum training on HARP and OlympiadBench fail@128 with 128 real MATH problems. Curriculum training exhibits an early performance spike, followed by significant and sudden performance decline early in training. Thus for HARP and OlympiadBench we use mixed training in our evaluations. Figure 7 Mixed v. Curriculum training on MATH. We compare training the base student on fail@128 + 128 questions sampled from Base-T , for performance on MATH. Curriculum performs better across different inference budgets. Teacher sampling. At evaluation time, we sample problems from the trained teacher using the same prompt and format-filtering as in training. PQ/PS Evaluation. We evaluate PQ using mixed synthetic/real training, described above. We evaluate PS by simply running inference on the fail@128 test set, to evaluate how much the student baseline advanced during SOAR training. Student checkpoint selection. For evaluations involving fresh student models, we train for maximum of 1500 steps (observing convergence well before this point). For MATH and HARP experiments where we report performance at fixed point, we select the student checkpoint to evaluate at using the slope of the smoothed training reward curve, similarly to classic RL early stopping heuristics (Mahsereci et al., 2017). In particular, we smooth the average training reward curve (centered-moving-average, 25 steps) and compute the discrete slopes, normalized by the range of observed rewards. The early stopping step is defined as the earliest point where the normalized slope falls below 15% of the maximum observed slope. We selected 15% threshold to identify the beginning of the reward plateau; empirically, varying between 10% and 20% have negligible effects on the selected point. Test performance is averaged over 200 step window following the selected step, to account for variance. In Figure C.2 we show the full training curves. We choose this heuristic to account for differing convergence rates between methods on MATH and HARP, 22 Figure 8 Mixed v. Curriculum training on HARP/OlympiadBench. We compare training the base student on real fail@128 + 128 random MATH questions, for HARP and OlympiadBench. Mixed training exhibits significantly more stable training dynamics across inference budgets (Pass@8 and Pass@32) and converges to higher final performance points. For both datasets, curriculum training exhibits strong instability with large early performance spike and then crash. and our small dataset sizes. In initial experiments we found separate validation sets, and cross-validation with the train set, to be extremely noisy. On OlympiadBench we observe similar convergence across all methods, and report at fixed point of 50 steps. B.7 Hyperparameters In Table 3 we detail our training and evaluation hyperparameters. Outer-loop training. We performed the following sweeps in preliminary experiments, and tuned using student performance on the full train set. Once selected, the same hyperparameters are used across all training runs and datasets. See Appendix D.2 for ablations on sensitivity to threshold τ and dataset size n. LR: {1e-6, 5e-6, 1e-5, 5e-5} n: {8, 16, 32, 64} τ : {0.01, 0.015, 0.02} Moving avg window size: {1, 3} We train for maximum of 200 outer steps based on compute constraints. For teacher-sampling experiments we fix the evaluation checkpoint based on the point of decline of teacher rewards observed in initial runs (170 steps for all HARP-trained models, 200 steps for all MATH-trained models). Inner-loop training. We find that from the base student, 10 steps is sufficient to induce movement in student performance. As the student baseline is updated, it is helpful to train slightly longer (we use +5 steps). We use greedy decoding for evaluating on QR to reduce noise in the student reward. Evaluation. We use standard hyperparameters to train the student from scratch on combined real/synthetic data (Table 3c). For PQ with curriculum evaluation we use zero learning rate warmup to match the inner-loop environment. 23 B.8 Seeds To ensure statistical significance and account for both teacher-training and student-training variation, we employ nested seeding strategy. Teacher training. For our main SOAR experiments, we train four independent teachers each on MATH and HARP to cover range of teacher training outcomes. For teacher objective ablations (Intrinsic-T and Grounded-T (no promotion)) we trained three independent teachers each. Evaluation (student training). The Hard-Only baseline is evaluated over 6 student seeds. For PQ datasets (>2 promotions), we train at least three students per PQ dataset, totaling 6 seeds (2 PQ datasets 3 students) per reported metric. For PS students, we compute pass@k metrics using inference over three seeds. For teacher-sampling experiments (i.e., sampling data from trained teachers and then training fresh student) we train 2-3 independent students per teacher seed, resulting in 8 seeds per reported metric. For all metrics we report the aggregated mean and standard deviation over student seeds. Hyperparameter Teacher Student Optimizer KL coefficient LR schedule Learning rate Temperature LR warmup steps Batch size Group size Max generated tokens AdamW 0.001 Cosine decay 1e-5 1.0 20 2 4 512 meta-RL specific (teacher only) Promotion threshold (τ ) Moving avg window Dataset size (n) Student repeats (r) 0.01 3 64 4 Evaluation specific (student only) Max training steps Synthetic warmup steps (curriculum training) 0/20 8 32 1024 1500 64 Table 3 Hyperparameters for SOAR training and evaluation. B.9 Computational resources Each SOAR training run was executed on 4 nodes (each 8 NVIDIA H200 GPUs or 8 NVIDIA H100 GPUs) for 48-60 hours. Each RLOO evaluation run (training fresh student) was executed for 12 hours on 1 H200 node or 1 H100 node."
        },
        {
            "title": "C Evaluations",
            "content": "C.1 Full Student Training curves In Figure 9 we show full student training curves for PQ, Hard-Only, and the full MATH upper bound for MATH, HARP, and OlympiadBench. In Figures 10-12 we show these training curves for questions sampled 24 Figure 9 Fail@128 test performance during student training for MATH, HARP, and Olympiad. Student learning curves at different pass@k when trained on Hard-Only, PQ, or the Full MATH dataset (PS inference performance shown as horizontal line). PQ and PS improve performance on all inference budgets and datasets, with increased effect at higher k. On MATH, PQ exhibits performance gains even after the synthetic-training phase (64 steps), showing that synthetic problems make real hard problems more learnable. from Grounded-T , Base-T , Intrinsic-T , and Grounded-T (no promotion). All curves show the mean and standard deviation over seeds. 25 C.2 Full Evaluation on fail@128 MATH, HARP, and OlympiadBench. Method Base Model Inference Hard-Only Hard-Only (g = 128) SOAR-PQ (Ours) SOAR-PS (Ours) Grounded-T (Ours) Intrinsic-T HARP train (128) MATH train (128) MATH train (Full) 1 4 8 16 32 0.3 0.1 1.0 0.2 2.0 0. 3.9 0.8 7.5 1.3 0.5 0.1 1.4 1.0 1.7 0.4 3.9 2.6 3.2 0.8 6.1 3.9 5.7 1.5 8.9 5. 9.6 2.6 12.4 7.4 1.7 1.0 1.0 0.2 1.6 0.5 1.0 0.6 2.4 1.0 2.1 0.0 2.7 0.2 5.3 2.6 3.8 0.6 5.1 1.4 3.3 2.1 8.5 3.7 6.8 1.1 8.4 2.1 5.7 3.5 13.0 4.8 11.5 1.6 13.1 2.9 9.2 5. 18.9 5.3 18.1 2.4 19.1 3.7 14.1 7.5 7.2 2.4 6.6 0.1 7.6 0.7 11.3 3.1 10.5 0.3 11.5 1.2 16.5 3.6 15.7 0.5 16.4 1.8 23.0 3.9 21.8 0.9 22.0 2.4 Table 4 MATH Pass@k (%) Test Accuracy on Fail@128. Mean and SD over seeds are averaged over 200 step window determined by training reward convergence (see Appendix B.6) with full curves in Figure 9. PQ and PS consistently outperform inference-only, Hard-Only, and intrinsic baselines across all inference budgets, and recover the majority of performance gain from training with real curated problems. We boldface the best among data-free\" methods (i.e., only Dtrain available). The bottom three rows serve as upper bounds from using curated, expert-annotated data. PQ datasets contain one of {128, 192, 256} questions. Method Base Model Inference Hard-Only SOAR-PQ (Ours) SOAR-PS (Ours) Grounded-T (Ours) Intrinsic-T HARP train (128) MATH train (128) MATH train (Full) 1 4 8 16 32 0.2 0.0 0.9 0.0 1.7 0. 3.4 0.0 6.4 0.0 0.4 0.1 1.4 0.2 2.6 0.4 4.7 0. 8.2 1.0 0.7 0.3 0.6 0.1 0.5 0.2 0.4 0.1 0.4 0.0 0.6 0.1 1.7 0.2 2.5 0.8 2.1 0.3 2.0 0.5 1.6 0.5 1.4 0.1 2.1 0.4 5.1 0.4 4.5 1.3 3.9 0.6 3.8 0.9 3.1 0. 7.7 1.7 7.0 0.9 6.7 1.3 5.6 1.4 12.3 2.0 11.8 1.2 11.2 1.7 9.6 2.1 2.8 0.2 4.0 0.7 8.1 0.4 5.0 0.5 7.1 0.9 11.7 0.3 8.7 1.1 11.9 0.9 16.2 0.4 Table 5 HARP Pass@k (%) Test Accuracy on fail@128. Mean and SD over seeds are reported at the timestep determined by training reward convergence (see Appendix B.6) with full curves in Figure 9. PQ and PS consistently outperform inference-only, Hard-Only, and intrinsic baselines across all inference budgets. Notably, SOAR questions perform better on HARP than similar numbers of questions from the MATH/HARP datasets (which serve as curated, expert-annotated data source). In Tables 4-5 we report our full results from evaluating SOAR on MATH and HARP (in-domain datasets). In Table 6 we report full results from evaluating on OlympiadBench, an OOD dataset. Our PQ datasets have one of {128, 192, 256} questions, depending on the number of student promotions for each run. For Intrinsic-T we sample 128 questions, consistent with all of our teacher-sampling experiments. For the equal-data comparison between Intrinsic-T and Grounded-T (sampling from the SOAR-trained teacher), see Section 5.2 and Appendix C.3. In addition to the methods/baselines shown in Figure 3 we also report the following. Inference pass@k with the base model. Inference with the base model has non-zero pass@k due to stochastic sampling with different seeds than were used for the initial pass@128 = 0 filtering. Comparison with Hard-Only 26 Method Base Model Inference Hard-Only SOAR-PQ (MATH) (Ours) SOAR-PQ (HARP) (Ours) SOAR-PS (MATH) (Ours) SOAR-PS (HARP) (Ours) Grounded-T (MATH) (Ours) Grounded-T (HARP) (Ours) Intrinsic-T HARP train (128) MATH train (128) MATH train (Full) 1 4 8 32 0.2 0.0 0.3 0.1 0.8 0.1 1.1 0.3 1.6 0.3 2.1 0.6 3.1 0.5 3.9 1.3 5.8 1.0 6.9 2. 0.5 0.1 0.5 0.1 0.6 0.1 0.5 0.1 0.4 0.2 0.5 0.2 0.4 0.3 0.5 0.1 1.0 0.1 0.9 0.0 1.9 0.5 2.0 0.5 2.1 0.5 2.0 0.4 1.6 0.8 1.9 0.6 1.7 1.2 2.0 0.2 3.4 0.1 3.2 0.1 3.6 0.9 3.8 1.0 3.7 0.8 3.8 0.7 2.9 1.4 3.6 1.1 3.1 2.0 3.6 0.4 5.9 0.1 5.6 0. 6.4 1.6 7.0 1.8 6.2 1.3 6.9 1.1 5.3 2.4 6.5 1.8 5.5 3.4 6.5 0.8 9.6 0.4 8.8 0.7 10.6 2.7 12.0 3.0 9.9 2.2 11.7 1.6 9.0 4.0 11.1 2.9 9.1 5.2 10.6 1.7 14.6 1.4 13.1 0.9 Table 6 Olympiad Pass@k (%) Test Accuracy on fail@128. Mean and SD over seeds are reported timestep 50 with full curves in Figure 9. Despite being optimized with reward signals from HARP and MATH, PQ questions and PS inference transfer to improving performance on Olympiad, and match or outperform 128 questions sampled from the HARP train set (a curated/expert-annotated source of problems). PS and PQ transfer better when trained with HARP than with MATH, potentially indicating more shared structure between HARP and Olympiad. results shows that our fail@128 datasets are sufficiently difficult such that direct training yields very little improvement. Doing inference with the trained Grounded-T teacher model directly on fail@128 MATH test questions does not improve upon base model, further evidence for the decoupling of generation and solving abilities. Hard-Only with extra compute. natural question is whether we can improve direct training on fail@128 train questions simply by increasing compute. One strategy is to train for longer, however our learning curves in Figure 9 show that Hard-Only test performance decreases in the latter stages of training. Another strategy is to sample more from the base model by increasing the RLOO group size. On MATH, we increase the group size 4 (from our default = 32 to = 128), and find that it only yields marginal improvements over Hard-Only (e.g., +2.8% pass@32) and does not recover the improvements of PQ. Sampling curated oracle questions\". In addition to training with the full MATH train set, we also evaluate sampling 128 questions from the MATH and HARP train sets, which can be considered oracle (curated/expertannotated) data sources. We choose 128 to match our teacher sampling experiments (Section C.3) and roughly match the amount of PQ data, which varies between 128 and 256 questions. On MATH, training with these smaller subsets performs similarly to training with the full MATH dataset, suggesting saturation point. On HARP, these smaller subsets only recover 50% of the gains from training with the full MATH train set. Notably, PQ and PS both outperform 128 sampled questions from HARP, and match 128 questions from MATH. C.3 Sampling from Trained Teachers. While PQ comes from accumulated useful questions over the meta-RL trajectory, here we sample questions directly from the trained teacher policy. The similar performance of Grounded-T and PQ (Tables 4-5) provide evidence that the pedagogical signals captured in the PQ datasets are learned by the teachers distribution. In Figures 10-12 we show full test trajectories on MATH, HARP, and Olympiad for students trained with 128 questions sampled from Grounded-T , Intrinsic-T , Base-T , and Grounded-T (no promotion). Grounded-T outperforms all comparisons, particularly at higher inference budgets, and is competitive with PQ. Grounded-T also exhibits lower variance and greater stability across student and teacher seeds. Grounded-T (no promotion) performs worse than Grounded-T , PQ, and PS, validating the importance of the promotion mechanism. 27 In Figure 13 we also compare student trajectories for each Grounded-T and Intrinsic-T teacher seed. Consistent with MATH and HARP (Figure 5), students have similar trajectories across independent Grounded-T teachers, and high variance across different Intrinsic-T teachers, showcasing the instability of intrinsic rewards. C.4 Correctness of Synthetic Questions We categorize synthetic questions into correctness taxonomies using Claude-4.5-Sonnet as an oracle judge. The prompt given to Claude is shown below. In Table 7 we report taxonomy statistics for PQ datasets, and problems sampled from Grounded-T , Intrinsic-T , and Base-T teachers. We prompt Claude-4.5-Sonnet to categorize problems as follows: Well posed: If the problem is mathematically complete and solvable. Correct: If the proposed answer is correct (only if the problem is well posed). Error type: None Arithmetic error: Sound logic, but incorrect final calculation. Logical fallacy: Does not follow mathematical rules. Ill-posed/Impossibility: The question contains mathematical impossibility. Ambiguous: The question is missing data, variables, or context necessary for solving it. Our results show that the well-posedness of problem matters more than the correctness of the solution. While teacher-training does improve the correctness rate, the best-performing datasets (Grounded-T and PQ) only contain 32.8% and 36.5% correct solutions respectively, compared to 55.5% for Intrinsic-T . This indicates that question diversity is more important for success (see Table 1). Question structure and coherence is more important; meta-RL reduces question ambiguities while the rate of arithmetic errors remains the same or slightly higher. 28 Figure 10 Fail@128 test performance during student training for MATH with different teachers. Each column compares training fresh student with 128 questions from Grounded-T to 128 questions from different teacher (Hard-Only also included for reference). While all teachers outperform Hard-Only, Grounded-T performs best, with increasing effects at higher k. Grounded-T results in less variance across student outcomes, particularly compared to Base-T and Intrinsic-T . PQ learning curves are in Figure 9. 29 Figure 11 Fail@128 test performance during student training for HARP with different teachers. Each column compares training fresh student with 128 questions from Grounded-T to 128 questions from different teacher (Hard-Only also included for reference). Grounded-T performs best, with increasing effects at higher k. Students trained with Base-T and Intrinsic-T tend to decline more for higher in the later stages of training, while Grounded-T leads to more stable trajectories. 30 Figure 12 Fail@128 test performance during student training for Olympiad with different teachers. Each column compares training fresh student with 128 questions from Grounded-T (trained with MATH and HARP) to 128 questions from different teacher (Hard-Only also included for reference). Students trained with Grounded-T teachers have more similar mean performance to Base-T and Intrinsic-T than seen on HARP and MATH (Figures 10-11). However, Grounded-T (HARP) shows more stability and less variance between independent teachers than Intrinsic-T (see Figure 13). 31 Figure 13 Test Pass@32 on OlympiadBench for fresh students trained with individual Grounded-T teacher seeds (red) and Intrinsic-T teacher seeds (green). Questions from Grounded-T yield consistent student trajectories on OlympiadBench across different teachers, whereas Intrinsic-T exhibits high variance across teachers, including failure mode where I-T (1) causes student collapse. 32 Oracle Prompt You are evaluating generated math problems for their coherence and solvability. Your task is to determine if the given question is well-formulated, and if the given answer is correct. CRITICAL INSTRUCTION: Do not assume missing information. If the question is nonsensical, lacks clear problem/question/equation, is syntactically incorrect, is missing necessary information, or is missing variables, you MUST classify it as Ambiguous or Ill_Posed. Do not invent context to make the answer work. QUESTION: {question} PROPOSED_ANSWER: {proposed_answer} TAXONOMY OF ERRORS: - None: The question is mathematically complete and the answer is correct. - Arithmetic: The logic is sound, but the final calculation is wrong. - Logical_Fallacy: The steps taken do not follow mathematical rules. - Ill_Posed: The question contains mathematical impossibility. - Ambiguous: The question is missing necessary data, variables, or context (e.g., \"Solve the equation\" without providing the equation). TASK: 1. Analyze the QUESTION for completeness. If its \"fragment\" or \"nonsense,\" stop and flag it. 2. Solve the problem ONLY if it is well-defined. 3. Determine: - is_well_posed: boolean - Is the question mathematically complete and solvable? - is_correct: boolean - Is the proposed answer correct? (Only evaluate if is_well_posed is true) - error_type: one of [None, Arithmetic, Logical_Fallacy, Ill_Posed, Ambiguous] - verified_answer: string - The correct answer if the question is well-posed, or \"N/A\" if not well-posed OUTPUT FORMAT: First, provide your reasoning in <think> tags. Then, provide JSON object with the following exact structure: json {{ \"is_correct\": <boolean>, \"is_well_posed\": <boolean>, \"error_type\": \"<one of: None, Arithmetic, Logical_Fallacy, Ill_Posed, Ambiguous>\", \"verified_answer\": \"<string: the correct answer or N/A>\" }} EXAMPLE OUTPUT: <think> The question asks to solve 2x + 5 = 13. This is well-posed with all necessary information. Solving: 2x = 8, so = 4. The proposed answer is 4, which is correct. </think> json {{ \"is_correct\": true, \"is_well_posed\": true, \"error_type\": \"None\", \"verified_answer\": \"4\" }} Category Well-Posed Correct Base 53.6% 23.2% Intrinsic Grounded 63.5% 55.5% 70.0% 36.5% PQ 64.6% 32.8% Error Taxonomy (% of total samples) Arithmetic Error Logic Error Impossibility Error Ambiguity Error 23.7% 5.7% 4.7% 42.4% Total Samples 384 5.7% 2.3% 2.9% 33.6% 384 29.0% 6.9% 8.2% 21.3% 375 25.0% 6.5% 4.7% 31.3% 384 Table 7 Correctness analysis and error taxonomy of synthetic questions, evaluated by Claude-4.5-Sonnet. Teacher training (for both grounded and intrinsic rewards) improves the well-posedness and correctness of problems relative to the base model, with corresponding decrease in question ambiguity errors. Grounded-T and PQ have fewer correct questions than Intrinsic-T but perform better, potentially because of greater diversity (see Table 1.)"
        },
        {
            "title": "D Ablations",
            "content": "D.1 Sampled dataset size Figure 14 (Left) Sampling different-sized datasets from Grounded-T for MATH (fail@128) Mean and 1 SD across 2 teacher seeds and 2 student seeds. (Right) Sampling different-sized datasets from the MATH trainset for MATH (fail@128). Resampled for each seed, 3 seeds. When training with SOAR, teacher-generated problems are partitioned into datasets that the student is trained on in the inner loop. Thus the teacher rewards are based on specific dataset size (64 in our case). In evaluation, however, one could potentially sample any number of questions from the teacher policy. This raises the question of how the performance of sampled datasets changes with size. Is it best to sample the number of questions that the teacher was trained with, or does performance saturate at higher sampling rates? We evaluate two teacher models trained with MATH by sampling {32, 64, 128} questions from each teacher, and training fresh student on the sampled questions and the MATH fail@128 train set (3 seeds per run). Since teacher models are trained with = 64, this covers datasets smaller, equal to, and larger than the dataset size that the teacher was trained with. Results are shown in Figure 14 for different pass@k. Performance improves with increasing n. Sampling with 128 questions has similar mean performance as sampling 64 questions but with significantly smaller error. This illustrates benefits (namely, consistency/reliabilty) to sampling questions from the teacher at higher rates than it was trained with. As comparison we also perform the same experiment using real questions from the MATH training dataset. For all values of n, real MATH questions perform similarly or better, and exhibit diminishing variance with increasing numbers of questions. D.2 Sensitivity to Teacher Hyperaparameters We ablate τ (the teacher-reward threshold to determine if the student baseline should be promoted) and (the number of samples per dataset that teacher-generated problems are partitioned into). The teacher generates problems per outer-RLOO iteration. We train SOAR on MATH with τ {0.01, 0.015} and {32, 64}. For each combination we train two SOAR runs for 200 steps and evaluate the final teacher checkpoints by sampling varying amounts of questions (X {32, 64, 128}) and training two fresh students. Results are shown in Figure 15 for pass@8 and pass@32. Our default configuration (n=64, τ =0.01) performs best, with = 64 showing modest advantages over = 32 at larger evaluation dataset sizes, which is consistent with the teacher being trained to produce larger datasets. Figure 15 Hyperparameter sensitivity on MATH. We train SOAR with τ {0.01, 0.015} and {32, 64}, then evaluate by training students on datasets of size {32, 64, 128}. Shaded regions indicate 1 SD. D.3 Problem Generation Format. MATH Pass@k (%) 64 32 64 128 32 64 128 1 8 16 32 0.66 0.58 0.52 0.26 0.67 0.67 2.34 1.91 1.93 0.93 2.29 2.03 4.16 3.13 3.60 1.63 4.03 3. 7.06 4.75 6.44 2.66 6.82 4.91 11.42 6.66 10.99 3.96 11.06 7.05 0.44 0.12 0.38 0.04 0.43 0.12 1.61 0.42 1.49 0.15 1.55 0.36 2.95 0.76 2.85 0.28 2.80 0.57 5.16 1.39 5.29 0.48 4.83 0. 8.56 2.48 9.35 0.84 7.96 1.32 Table 8 MATH Pass@k results for multi-turn teacher sampling. We report mean and SD across four teacher seeds and 2 student seeds per teacher. Multiturn performs worse than our default single-turn setting across all pass@k and sampled dataset sizes. In our default setup, we sample problems from the teacher by prompting it to produce single completion that is parsed into question/answer, and filtering out outputs that do not match the necessary format. An alternative sampling method, however, is to generate problems in separate question-answer stages (multi-turn) such that filtering is not needed: 1. Sample πT 2. Sample πT ϕ (qip) where is teacher prompt to generate question. ϕ (aip, qi, p) where is prompt to generate an answer given the question. The logprob component of the teacher RLOO loss is then log(πT We execute SOAR across four seeds using this teacher-sampling formulation with our standard procedure and hyperparameters, ablating {32, 64}. We observe that the teacher reward quickly plateaus and does not exceed one promotion. In Table 8 we find that across different numbers of sampled problems and values of n, the multi-turn sampling strategy performs worse than our default single-turn sampling. ϕ (qip)) + log(πT ϕ (aip, qi, p))."
        },
        {
            "title": "E Teacher Training Dynamics",
            "content": "In Figure 16 we show representative teacher training curve for SOAR on HARP. We observe that SOAR follows pattern of search and exploitation. The training curve exhibits periods of oscillation (search), and then steady rise in reward from steps 18-27, culminating in student promotion. The reward declines after the promotion, due to the improved student baseline, oscillates as the teacher adapts to the improved student, and then exhibits another rise from steps 80-86 culminating in second promotion. Figure 17a shows teacher training curves for Intrinsic-T teachers, aggregated across teacher seeds, which exhibits smooth upward climb. Figure 17b shows that as the Intrinsic-T reward climbs, the diversity of teacher completions falls (diversity measured as the average pairwise cosine distance of embeddings). Meanwhile Grounded-T preserves the original model diversity throughout the full trajectory. This is consistent with findings in Section 5.2  (Table 1)  that Grounded-T achieves similar question diversity to Base-T , whereas Intrinsic-T teachers collapse to more narrow conceptual space. Figure 16 Annotated teacher reward dynamics when training SOAR with HARP. Shows sample teacher trajectory from SOAR run on HARP. The teacher follows cyclical search-exploitation pattern. Student promotions (updating the student baseline to trained student) are triggered when the 3-step moving average of teacher rewards exceeds τ = 0.01. After each promotion, the improved student baseline makes previous curricula less useful, causing rewards to drop, and then recover as the teacher adapts and discovers questions appropriate for the improved student. 36 Figure 17 (Left) Teacher training dynamics when training with Intrinsic-T. Mean and 1 SD over three independent training runs. (Right) Teacher completion diversity when training with intrinsic v. grounded rewards. Grounded rewards preserve diversity for the full run, while intrinsic teachers lose diversity as they converge. Mean and 1 SD over three training runs for intrinsic and four for grounded (two MATH, two HARP)."
        }
    ],
    "affiliations": [
        "MIT",
        "Meta FAIR",
        "New York University"
    ]
}