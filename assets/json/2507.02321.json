{
    "paper_title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback",
    "authors": [
        "Nina Konovalova",
        "Maxim Nikolaev",
        "Andrey Kuznetsov",
        "Aibek Alanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. ControlNet addresses this by introducing an auxiliary conditioning module, while ControlNet++ further refines alignment through a cycle consistency loss applied only to the final denoising steps. However, this approach neglects intermediate generation stages, limiting its effectiveness. We propose InnerControl, a training strategy that enforces spatial consistency across all diffusion steps. Our method trains lightweight convolutional probes to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. These probes efficiently extract signals even from highly noisy latents, enabling pseudo ground truth controls for training. By minimizing the discrepancy between predicted and target conditions throughout the entire diffusion process, our alignment loss improves both control fidelity and generation quality. Combined with established techniques like ControlNet++, InnerControl achieves state-of-the-art performance across diverse conditioning methods (e.g., edges, depth)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 2 3 2 0 . 7 0 5 2 : r Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback Nina Konovalova1 Maxim Nikolaev1,2 Andrey Kuznetsov1,3,4 Aibek Alanov2,1 1AIRI, Russia 2HSE University, Russia 3Sber, Russia 4Innopolis, Russia"
        },
        {
            "title": "Abstract",
            "content": "Despite significant progress in text-to-image diffusion models, achieving precise spatial control over generated outputs remains challenging. One of the popular approaches for this task is ControlNet, which introduces an auxiliary conditioning module into the architecture. To improve alignment of the generated image and control, ControlNet++ proposes cycle consistency loss to refine correspondence between controls and outputs, but restricts its application to the final denoising steps, while the main structure is introduced at an early generation stage. To address this issue, we suggest InnerControl training strategy that enforces spatial consistency across all diffusion steps. Specifically, we train lightweight control prediction probes small convolutional networks to reconstruct input control signals (e.g., edges, depth) from intermediate UNet features at every denoising step. We prove the efficiency of such models to extract signals even from very noisy latents and utilize these models to generate pseudo ground truth controls during training. Suggested approach enables alignment loss that minimizes the difference between predicted and target condition throughout the whole diffusion process. Our experiments demonstrate that our method improves control alignment and fidelity of generation. By integrating this loss with established training techniques (e.g., ControlNet++), we achieve high performance across different condition methods such as edge and depth conditions. The code is available at https: //github.com/ControlGenAI/InnerControl."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in diffusion models [6, 7, 9, 8] have enabled high-quality and diverse text-to-image (T2I) generation, producing images that closely align with input textual prompts [1013]. However, achieving precise spatial control during image generation remains challenging problem [16, 3, 17]. To address this issue, methods such as ControlNet [2] and T2I-Adapter [1] introduced conditional mechanisms to guide the generation process using provided control signals (e.g., edge maps, depth, or segmentation). Various methods focused on ControlNet [2] quality improvements through architecture advances [18], unified conditioning [21, 20], and efficient adaptation to new conditions [37]. However, these approaches often suffer from inconsistencies between the provided control signal and the final generated output. Recent work, such as ControlNet++ [4], reduces the difference between input control and output generation by incorporating additional reward losses that minimize discrepancies between the control signals extracted from the generated image (e.g., edges or depth) and the input condition. Ctrl-U [5] suggests building uncertainty-aware reward modeling to reduce the adverse effects of inaccurate feedback from the reward model. These methods focus on the alignment on late denoising steps, while the main spatial knowledge appears in the early generation steps [22, 23]. However, extending the reward losses to earlier steps leads to significant decrease in the generated image quality, producing visible artifacts on the generated images. These poor results are probably caused by inefficient signal extraction in the early sampling steps, producing inaccurate Preprint. Under review. signals for loss calculations. This analysis highlights critical limitation of the suggested approaches, making them applicable only to the late generation steps. To address temporal misalignment in prior methods, we introduce InnerControl novel training strategy that enforces consistency between input control (e.g., edges, depth maps) and extracted signals from intermediate diffusion features at the whole sampling trajectory. Based on previous research that leverages diffusion features for vision tasks such as depth estimation, semantic segmentation, and classification tasks [23, 24, 30], we propose to utilize lightweight convolutional neural nets to extract control from UNet decoder features. Inspired by Readout Guidance [34], which uses timestep-conditioned architectures for discriminative tasks, and demonstrates the effectiveness of such models at early denoising stages, where spatial structure predominantly emerges [22]. Using these estimation models, we calculate an additional penalty during ControlNet training, explicitly enforcing the spatial alignment during the whole generation process. We prove that InnerControl enhances the previous reward training approach, improving control alignment while maintaining perceptual quality. Our core contributions are: Early-stage control alignment: we introduce novel training objective that enforces consistency between the provided control signal (e.g., edge maps, depth) and the extracted signal from intermediate diffusion features throughout the entire diffusion process, including the early stages where structural content emerges. Improved Controllability: suggested training strategy improves the existing rewarding approach, providing better control alignment and image quality on different spatial control tasks, such as depth maps and edge control."
        },
        {
            "title": "2 Related",
            "content": "2.1 Controllable Text-to-Image Diffusion Models Diffusion models [6, 7, 9, 8] have achieved remarkable success in generating high-quality, diverse images conditioned on text prompts [1015]. However, traditional approaches rely solely on textual guidance, limiting precise spatial control over generated outputs. Several methods have been proposed to enable more precise spatial control without retraining the entire diffusion pipeline. ControlNet [2] proposed novel architecture that incorporates duplicate encoder of the pretrained diffusion model while introducing zero-convolution learnable layers to stabilize training. This approach enables alignment with diverse spatial conditions (e.g., edges, depth, or segmentation masks). Similarly, T2IAdapter [1] proposes framework that bridges the internal representations of text-to-image diffusion models with external control signals through additional adapter modules. Further methods have explored architectural refinements to improve efficiency [18, 19]. Other studies focus on developing unified frameworks capable of supporting multiple types of spatial controls within single model [20, 21], rapid adaptation to novel control types [37] or advanced backbones [39, 38]. Despite these advances, achieving consistent alignment between generated images and the input conditions remains challenging. To address this issue, ControlNet++ [4] introduces additional reward loss for ControlNet training that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Ctrl-U [5] proposes uncertainty-aware reward modeling to regularize reward fine-tuning through consistency construction. In particular, rewards with lower uncertainty receive higher loss weights, while those with higher uncertainty are given reduced weights to allow for larger variability. However, both approaches focus primarily on late stage alignment because of the suggested strategy of one-step prediction for signal estimation, neglecting the earlier phases of generation. At the same time, prior works demonstrate that the main structure appears during the early stage of generation [22]. That is why ensuring consistency across the entire generation trajectory is crucial for preserving fidelity to the input condition. Our approach directly addresses this gap by enforcing alignment at every denoising step. 2.2 Diffusion Model Representation Pretrained text-to-image diffusion models have demonstrated remarkable utility in extracting semantically rich representations from their internal features, enabling applications in diverse discriminative 2 tasks such as segmentation, semantic correspondence, classification, and depth [36, 2427]. Previous works have analyzed the quality of UNet layers at different diffusion denoising steps for downstream vision tasks [23, 22]. Recent efforts leverage diffusion features for discriminative tasks by aggregating information across layers and denoising steps for segmentation [31, 30, 29], semantic correspondences [28, 24, 29], classification [32, 29], detection tasks [33, 29] or depth estimation problem [29]. However, most existing approaches rely on aggregated features across denoising steps, potentially limiting their ability to capture task-specific information at individual stages. Recent work [34] improves the Diffusion Hyperfeatures [35] architecture by introducing additional timesteps conditioning. However, this approach focuses on convolutional features. In contrast, prior research suggests extensive exploration of geometry extraction from different parts of the diffusion UNet, finding that self-attention is more appropriate for the structure estimation [22]. Building on this insight, we slightly change the architecture to leverage self-attention outputs from the UNet decoder for signal estimation."
        },
        {
            "title": "3 Preliminaries",
            "content": "In this section, we introduce the background of diffusion models and spatially controllable generation, followed by an analysis of cycle consistency losses suggested in ControlNet++ [4]. 3.1 Controllable generation Diffusion models [6, 7] are class of generative models that synthesize data by iteratively denoising random noise through learned reverse process. The forward process defines sequence of noise adding steps that transform the data into isotropic Gaussian noise over steps. q(xtxt1) = (xt; αtxt1, (1 αt)I) (1) where αt (0, 1) is fixed variance schedule. The reverse process learns to invert this process using neural network that iteratively denoises samples: pθ(xt1xt) = (cid:0)xt1; µθ(xt, t), σ2 (cid:1) , where µθ() is learnable function approximating the mean of the true posterior. The standard training objective minimizes the noise prediction error: Ldiffusion = Ex0,ϵ,t ϵ ϵθ(xt, t)2(cid:105) (cid:104) (2) (3) where xt noisy sample from timestep t, ϵ (0, I) and ϵθ(xt, t) learned approximation using another parametrization. In case of additional control such as text prompt conditioning ctxt and spatial control cspatial (e.g., depth maps, edges), the objective can be expressed as: Ldiffusion = Ex0,ϵ,t,ctxt,cspatial (cid:104) ϵ ϵθ(xt, t, cspatial, ctxt)2(cid:105) (4) 3.2 ControlNet++ ControlNet [2] is one of the popular methods that utilize pretrained text-to-image diffusion model for controllable generation with additional spatial control. While ControlNet is trained using standard diffusion loss 4, it suffers from inconsistency between the final prediction and the input control. To mitigate this issue, ControlNet++ proposes cycle consistency loss that leverages discriminative reward model. To be more specific, the authors suggest minimizing the loss between the input control cspatial and the corresponding condition extracted from the generated image using the Reward model ˆcspatial = D(x0), where x0 denotes the generated image. During training, diffusion models sample timesteps [999, 0] to simulate the full denoising trajectory. However, computing rewards across 3 the entire trajectory would require prohibitive gradient accumulation. To address this issue, the authors suggest approximating x0 from noisy sample xt by performing single-step sampling: x0 0 = G(cspatial, ctxt, xt, t) = xt 1 αtϵθ(x t, cspatial, ctxt, t) αt (5) where ϵθ() - prediction of network and G() denotes the diffusion models single-step generation process. This allows to direct use of the denoised image 0 to perform reward fine-tuning: Lreward = (cspatial, [G(cspatial, ctxt, xt, t)]) (6) Due to single-step sampling, the authors suggest applying their rewarding loss only on the last 200 steps (t [0, 200]) of diffusion trajectory sampling."
        },
        {
            "title": "4 Method",
            "content": "In this section, we will discuss the main limitations of ControlNet++ and suggest the new training approach InnerControl the solution to mitigate these problems. 4.1 Motivation Figure 1: Visualizing the trade-off between control consistency (RMSE) and image fidelity (FID) when extending reward losses to early denoising stages. Left: Highlight visual artifacts in generated samples while applying rewarding loss (top) and additional alignment loss (bottom) on the early diffusion steps. Right: Quantitative analysis of metrics trade-off demonstrating the inverse relationship between control precision (RMSE ) and image fidelity (FID ). As we mentioned earlier, ControlNet++ [4] focuses on applying the reward loss to the final 200 denoising steps (t [0, 200]) due to the reliance on single-step prediction strategy. We analyze the trade-off between control consistency and image fidelity when extending the suggested loss to early denoising stages. To this end, we train ControlNet model conditioned on depth maps with Lreward loss applied on different number of diffusion denoising steps. We evaluated alignment using RMSE and perceptual quality via FID. Our experiments show that by extending the reward loss to earlier sampling steps, the control alignment increases  (Fig. 1)  , reducing RMSE. While RMSE improves, the quality of generated images is significantly decreased, leading to an increase in FID (Fig. 1 right). The perceptual metrics degradation can also be observed in the images, where artifacts appear as the reward loss is extended to earlier steps. As shown in Fig. 1, applying reward loss up to 400 denoising steps produces good-quality images. However, while increasing the number of steps to 600, small artifacts begin to appear, and for almost all steps (920) the image contains unexpected lines and distorted edges  (Fig. 1)  . We suggest that quality degradation is caused by poor-quality one-step prediction from highly noisy latents. The single-step image prediction strategy applied to images during early denoising steps provides very blurry output. Fig. 3 illustrates the predictions of DPT depth estimation on different diffusion steps. As we can see, starting from 400 generation steps, the single-step prediction appears to be blurry and out-of-domain for pretrained depth estimation model. Fig 2 further supports this claim: the RMSE loss increases at early steps (green line) for both SD1.5 and ControlNet generations. Thus, extracted depth maps are inaccurate and contain significant misalignment with the ground-truth depth map of an image, propagating errors during training. Figure 2: Left: RMSE between depth estimated from final image and DPT depth prediction for single-step predicted images (green line) and estimated depth from intermediate features (blue line) for SD1.5 generation. Right: RMSE between control depth and DPT depth prediction for singlestep predicted images (green line) and estimated depth from intermediate features (blue line) for ControlNet. Figure 3: Results of one-step prediction (up) at varying noise levels (from low to high), corresponding depth prediction generated using the DPT estimator [42] (middle) and depth, estimated from intermediate UNet features (bottom). 4.2 Alignment on early steps While standard discriminative models such as DPT [42] struggle to extract precise control signals from blurry images, prior works demonstrate that intermediate diffusion features contain information about spatial structure even at early stages of generation [22]. Building on this knowledge, instead of relying on discriminative models for images, we propose to train lightweight convolutional network H(, t) to estimate the control signals directly from intermediate features at every denoising step. This approach is inspired by Readout Guidance [34], which trains small, timestep-conditioned models to extract signals from diffusion features. To validate the effectiveness of intermediate feature-based signal estimation, we compare our lightweight convolutional estimator H(, t) with the standard DPT depth estimation model[42]  (Fig. 3)  . We evaluated depth prediction and control accuracy using RMSE between two approaches: DPT depth estimation from single-step prediction and H(, t) results. Our results demonstrate that H(, t) predicts results more aligned with the final depth prediction for SD1.5, especially at the early stage of generation Fig. 2. Additionally, H(, t) proves to be more stable for ControlNet generation, maintaining consistent signal estimation throughout the entire denoising trajectory (Fig. 2, Right). These results show that intermediate features provide more robust signal prediction, enabling accurate control signal estimation even in high-noise regimes. By leveraging H(, t) to enforce alignment at an early stage of generation, we introduce InnerControl the method that suggests the new training objective that addresses the misalignment that appeared in the previous approaches: 5 Figure 4: Pipeline overview. We schematically illustrate the main idea of our InnerControl framework, highlighting the integration of the alignment loss. The main difference from ControlNet++ is Alignment module, which processes intermediate features extracted from the UNet decoder. These features are passed through an aggregation network to predict spatial control signals (e.g., depth or edge maps), which are then compared to the input control cspatial to enforce consistency at every denoising step. Lalignment = (cspatial, [ControlNet(cspatial, ctxt, xT , t)] , t) (7) This loss is applied during training to increase control alignment. The additional alignment block is illustrated in Fig. 4. For the final training objective, we use weighted combination of standard diffusion loss 4, rewarding loss at the early denoising stage 6, and additional alignment loss 7: Ltraining = Ldiffusion + α Lreward + β Lalignment (8) This additional loss penalizes discrepancies between cspatial and ˆcspatial at each step t, enforcing spatial alignment at each denoising step, resulting improved control correspondence and image fidelity without visible artifacts compared to reward loss at the early steps Fig. 1."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment setup Datasets. We evaluate our method on the MultiGen-20M dataset [21] large-scale synthetic dataset containing paired images and control signals. This dataset is used for LineArt and HED conditioning. For depth estimation, we use the corresponding MultiGen-20M depth dataset, which contains precomputed depth maps generated using standard monocular depth estimation techniques. Implementation details. We follow the training protocol from ControlNet++ [4] and Ctrl-U [5] with modifications to incorporate our intermediate feature feedback mechanism. We begin with finetuning the pretrained ControlNet model for 10k iterations using the AdamW optimizer with learning rate 105. After finetuning, we use the same optimization settings and perform 10k training iterations with suggested loss 8. The proposed Lalignment loss is applied over [920 0] diffusion steps, while the reward loss is applied for the 200 steps for edge control tasks and for 400 steps for depth estimation. All experiments use 512 512 images with batch size of 256. See the supplementary material for detailed model settings 7.1. Baselines. We compare our method with several competitors, including T2I-Adapter [1], ControlNet v1.1 [2], GLIGEN [40], Uni-ControlNet [20] and and UniControl [21] and ControlNet++ [4] and CTRL-U [5]. Most of these methods are based on SD1.5 for text-to-image generation, but we additionally add several models based on SDXL [41]: ControlNet-SDXL and T2I-Adapter-SDXL 6 following the evaluation protocol suggested in CTRL-U [5]. For fair comparison, all models are evaluated under identical image conditions and text prompts, using two guidance scales: 7.5 and 3.0. Metrics and evaluation. We evaluate alignment fidelity using task-specific metrics: Structural Similarity Index (SSIM) between generated edges and input control signals and Root Mean Squared Error (RMSE) between predicted and ground-truth depth maps. All metrics are computed on the 512 512 images to ensure consistency. To reduce stochastic variance, we generate 4 independent sample batches using different random seeds and report the mean metrics. More information about evaluation models can be found in the supplementary material 7.1. 5.2 Experimental results Table 1: Unified comparison on the MultiGen-20M benchmark. Controllability is measured by SSIM () for HED/LineArt and RMSE() for Depth; fidelity by FID (); relevance by CLIP-score(). denotes training model from scratch using suggested in paper hyperparameters. Method T2I Model Hed Edge LineArt Edge Depth Map SSIM FID CLIP SSIM FID CLIP RMSE FID CLIP Guidance scale = 7.5 ControlNet T2I-Adapter T2I-Adapter Gligen Uni-ControlNet UniControl ControlNet ControlNet++ ControlNet++ Ctrl-U InnerControl (Ours) Guidance scale = 3.0 ControlNet ControlNet++ Ctrl-U InnerControl (Ours) SDXL SDXL SD1.5 SD1.4 SD1.5 SD1.5 SD1.5 SD1.5 SD1.5 SD1.5 SD1. SD1.5 SD1.5 SD1.5 SD1.5 0.5634 0.6910 0.7969 0.7621 0.8097 0.8401 0.8207 0.7752 0.8204 0.8522 0.8305 17.08 15.99 15.41 15.01 11.59 13.27 15.98 12.21 10.25 12.16 31.94 32.02 31.46 32.05 32.05 31. 31.34 31.32 31.50 31.28 0.6394 0.7054 0.8399 0.8208 0.8250 0.8258 0.7328 0.8515 0.8488 0.8395 17.44 13.88 12.04 12.08 13.81 13.22 11.99 11.38 31.26 31.95 31.88 31. 31.59 31.18 31.17 31.30 40.00 39.75 48.40 38.83 40.65 39.18 35.90 28.32 29.66 29.06 26.09 34.25 26.53 25.86 25.10 22.52 18.36 20.27 18.66 17.76 16.66 17.93 19.73 18.29 16.36 14.56 15.48 14.67 31.46 31.48 31.66 31.68 32.11 32.09 31.88 31. 31.37 31.32 31.18 31.18 Comparison of Controllability. We summarize the results for the quality of the control alignment in Table 1. For depth estimation, our method achieves significant improvements over baselines at both guidance scales: at the guidance scale 7.5 RMSE is reduced by 7.87% compared to ControlNet++ [4] and 10.22% compared to CtrlU [5], demonstrating stronger alignment under high guidance intensity. At the guidance scale 3.0, we observe 2.94% RMSE improvement over CTRL-U, indicating consistent performance across settings. For edge control tasks (LineArt and HED), our approach outperforms ControlNet++ on LineArt conditioning when trained with identical parameters and seeds, while maintaining competitive results on HED tasks. Although CTRL-U slightly exceeds our approach in HED and LineArt at low guidance (scale 3.0), our method exhibits greater stability for higher guidance scales, proving to be more efficient in overall generation. Additionally, it is important to mention that our alignment loss may be applied to the CTRL-U [5] pipeline, which is promising direction for future work. Comparison of Image Quality. To evaluate the perceptual quality of generated images, we provide the Fréchet Inception Distance (FID) [43] across all evaluated methods and two guidance scales Table 1. While achieving the best RMSE for depth estimation for guidance scale 3.0 we obtain FID metric almost as good as for ControlNet++. At the same time, for higher guidance scale (7.5) our method shows improvements for HED and LineArt compared to ControlNet++ [4] training strategy, while being more efficient than the Ctrl-U [5] method for depth control task. This means that our method not only improves the controllability, but also enhances the image quality. Comparison of CLIP Score. To estimate prompt alignment, we calculate CLIP-Score metrics, providing the results in Table 1. We calculate CLIP metrics for ControlNet++ and Ctrl-U using official checkpoints provided by the authors. We observe that while providing more aligned and quality images, we remain on the same CLIP-scores level in various setups, providing improvements in LineArt experiments. 7 Figure 5: Qualitative Comparison with Baselines: Side-by-side results for HED (top), depth (middle) and LeneArt control (bottom) using identical prompts and two guidance scales (3.0 and 7.5). Our method produces more accurate and aligned to input control results compared to competitors. Qualitative Analysis. We present qualitative comparison, showing side-by-side generations from our method, ControlNet++ [4], and CTRL-U [5] under identical prompts and control signals (e.g., depth maps, HED edges, and LineArt edges) at guidance scales 3.0 and 7.5. In the Fig. 5, we highlight the misalignment with input conditions and generated results. For example, Ctrl-U introduces additional HED edges at both guidance scales and generates noisy LineArt outputs under high guidance (jagged edges in dress folds). Similarly, ControlNet++ and CTRL-U fail to correctly generate images based on input depth maps, producing inconsistent object distances and noisy surface textures, while our method remains efficient for both high and low guidance scales. Intermediate features. We compared extracted features alignment with control for the depth estimation task Fig 6. The top row illustrates that after ControlNet training, extracted depth maps exhibit high correspondence with the input control signal across different steps. This visualization proves the efficiency of alignment across sampling trajectory, improving alignment not only of extracted features but also the resulting generated image. 8 Figure 6: Visualization of difference between extracted signal from intermediate features and input control after our training applied (top) and for standard ControlNet (bottom) 5.3 Ablation Alignment steps ablation. We conduct an ablation study to analyze how the application of alignment and reward losses across different diffusion denoising steps affects performance  (Table 2)  . Specifically, we train ControlNet models with alignment (Lalignment) and reward (Lreward) losses applied to varying subsets of denoising steps. All models are initialized from open-source ControlNet weights for depth estimation and trained under identical conditions (same seed, iteration count, optimizer settings). Our experiments show that integrating alignment loss into both training ControlNet and ControlNet++ pipelines improves control alignment (RMSE) and image quality (FID metric). However, using alignment loss alone is less effective compared to reward loss. We further explore the impact of extending alignment loss to different number of denoising steps. Compared to reward loss, alignment loss does not increase FID while applied to early denoising steps. That is why for our main experiments we utilize the 920 steps for alignment loss. Table 2: Ablation study for Depth Map control task from the MultiGen-20M benchmark. Explore the influence of timesteps number, where reward and alignment losses are applied. Method Steps Steps Reward RMSE FID CLIP Guidance scale = 7. ControlNet ControlNet ControlNet ControlNet ControlNet ControlNet ControlNet ControlNet 0 920 0 0 200 400 600 920 0 0 920 200 200 200 200 200 33.95 32.80 25.70 29.66 28.93 28.56 28.41 27.50 18.61 18.55 22.43 18.51 18.32 18.57 18.52 18.22 32.175 32.05 31.43 32.11 32.06 32.00 31.99 31."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the challenge of improving ControlNet controllability by refining its training objective to enforce consistency between input controls and intermediate diffusion features. We explore the limitations of previous reward-based approaches, ControlNet++[4] and CTRL-U [5], which focus on the last diffusion steps control alignment while neglecting early denoising steps, where spatial structure predominantly emerges [22]. To solve this limitation, we propose to use lightweight convolutional net that extracts control signals from intermediate features at every diffusion step, enabling explicit alignment on all sampling trajectories. We conducted our experiments on three benchmarks, including LineART, HED, and depth map control, proving that the alignment strategy significantly improves both the fidelity to input controls and stability under high guidance scales. These results underscore the importance of enforcing consistency across the entire diffusion trajectory and show the great potential of our approach for future studies."
        },
        {
            "title": "References",
            "content": "[1] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. [2] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [3] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [4] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, Chen Chen, and others. ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback: Project Page: liming-ai. github. io/ControlNet_Plus_Plus. In European Conference on Computer Vision, pages 129147. Springer, 2024. [5] Guiyu Zhang, Huan-ang Gao, Zijian Jiang, Hao Zhao, and Zhedong Zheng. Ctrl-u: Robust conditional image generation via uncertainty-aware reward modeling. arXiv preprint arXiv:2410.11236, 2024. [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [7] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [9] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [10] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [11] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. PMLR, 2021. [12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. HighIn Proceedings of the IEEE/CVF resolution image synthesis with latent diffusion models. conference on computer vision and pattern recognition, pages 1068410695, 2022. [13] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, and others. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [14] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, and others. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [15] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [16] Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, and Tat-Jen Cham. Cocktail: Mixing multi-modality control for text-conditional image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 10 [17] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. [18] Denis Zavadski, Johann-Friedrich Feiden, and Carsten Rother. ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models as Feedback-Control Systems. In European Conference on Computer Vision, pages 343362. Springer, 2024. [19] Ke Cao, Jing Wang, Ao Ma, Jiasong Feng, Zhanjie Zhang, Xuanhua He, Shanyuan Liu, Bo Cheng, Dawei Leng, Yuhui Yin, and others. Relactrl: Relevance-guided efficient control for diffusion transformers. arXiv preprint arXiv:2502.14377, 2025. [20] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023. [21] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, and others. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. [22] Yida Chen, Fernanda Viégas, and Martin Wattenberg. Beyond surface statistics: Scene representations in latent diffusion model, 2023. URL https://arxiv. org/abs/2306.05720. [23] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. [24] Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. Advances in Neural Information Processing Systems, 36:82668279, 2023. [25] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems, 36:5892158937, 2023. [26] Xingyi Yang and Xinchao Wang. Diffusion model as representation learner. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1893818949, 2023. [27] Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised learners. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1580215812, 2023. [28] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. [29] Nick Stracke, Stefan Andreas Baumann, Kolja Bauer, Frank Fundel, and Björn Ommer. CleanDIFT: Diffusion Features without Noise. arXiv preprint arXiv:2412.03439, 2024. [30] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging pixel-level semantic knowledge in diffusion models. arXiv preprint arXiv:2401.11739, 2024. [31] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885, 2022. [32] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22062217, 2023. [33] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffusiondet: Diffusion model for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1983019843, 2023. [34] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82178227, 2024. [35] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. Advances in Neural Information Processing Systems, 36:4750047510, 2023. [36] Frank Fundel, Johannes Schusterbauer, Vincent Tao Hu, and Björn Ommer. Distillation of In 2025 IEEE/CVF Winter Conference on diffusion features for semantic correspondence. Applications of Computer Vision (WACV), pages 67626774. IEEE, 2025. [37] Yifeng Xu, Zhenliang He, Shiguang Shan, and Xilin Chen. CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation. arXiv preprint arXiv:2410.09400, 2024. [38] Lingmin Ran, Xiaodong Cun, Jia-Wei Liu, Rui Zhao, Song Zijie, Xintao Wang, Jussi Keppo, and Mike Zheng Shou. X-adapter: Adding universal compatibility of plugins for upgraded diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87758784, 2024. [39] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. arXiv preprint arXiv:2404.09967, 2024. [40] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [42] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179 12188, 2021. [43] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [44] Patrick Von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. 2022. [45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016."
        },
        {
            "title": "7 Appendix",
            "content": "7.1 Implementation Details Training details. We utilize the same α for the reward loss weighs as suggested in ControlNet++: 0.5 for the depth control and 1 for the HED and LineArt control. For β alignment loss, we use the same value across all experiments: β = 1. For the reward loss, we utilize timesteps threshold 200 steps for HED and LineArt and 400 steps for depth estimation, for the alignment loss we always use 920 steps (see Table 4). Datasets information is illustrated in Table 3. The training was conducted on 8 H100 and took around 6 hours. Our codebase is based on the implementation in HuggingFaces Diffusers [44]. Reward models details. We additionally provide information about reward models in Table 4. Following ControlNet++ [4] and Ctrl-U [5] we utilize slightly weaker model as the reward model for depth estimation training and stronger model for evaluation. For HED and LineArt we use the same models as proposed in ControlNet [2]. Table 3: Datasets and evaluation details for explored tasks. denotes higher is better, lower is better. HED Edge LineArt Edge Depth Map Dataset MultiGen20M [21] MultiGen20M [21] MultiGen20M [21] Training Samples 2,560, Evaluation Samples Evaluation Metric 5,000 SSIM 2,560,000 5, SSIM 2,560,000 5,000 RMSE Table 4: Details about some training parameters and reward models. ControlNet denotes utilizing the same model to extract signal as ControlNet [2] Reward Model (RM) Depth Edge DPT-Hybrid RM Performance NYU(AbsRel): 8.69 Evaluation Model (EM) DPT-Large Hed Edge ControlNet - ControlNet LineArt Edge ControlNet - ControlNet EM Performance NYU(AbsRel): 8. - - Reward Loss Loss Weight α Steps threshold Alignment Loss Loss Weight β Steps threshold MSE Loss 0.5 400 MSE Loss 1.0 MSE Loss 1.0 200 MSE Loss 1.0 920 MSE Loss 1.0 200 MSE Loss 1.0 920 Alignment models details For our work, we utilize the architecture for H(, t) from the Readout Guidance [34]. Following [34] we build an aggregation network that takes features from the UNet decoder, and applies bottleneck layers [45] to standardize the channel count and aggregate with learned weighted sum. Additionally, authors use pretrained timesteps embedding for model conditioning to make predictions on each diffusion step. Our slight modification is applied to the depth control task, where we utilize the self-attention features from the UNet decoder instead of convolutional features, as it provides slight improvements in MSE metrics (see Figure 7). Additional visualization of the extracted depth end edges can be observed in Figs. 8,9. 7.2 More visualizations We also provide visualizations for different control types for InnerControl generation. The results are shown in Figures 10,11,12. 13 Figure 7: Quality comparison for attention-based and convolution-based predictions for depth maps extraction task Figure 8: Visualization of one-step prediction, corresponding DPT depth estimation, and depth extracted from intermediate features. Figure 9: Visualization of one-step prediction, corresponding DPT depth estimation and depth extracted from intermediate features. 7.3 Limitations. The main limitation of our approach is the quality of small convolution neural nets for signal estimation from intermediate features. Due to their small parameter count and shallow design, these models may struggle to predict fine-grained spatial details, such as thin edges. However, we emphasize that this limitation is not intrinsic to the method itself. Our framework may utilize any model that is able to extract signal at each timestep. This opens promising direction for searching for better model for future work. 14 Figure 10: More visualizations for InnerControl (ours) method (depth maps) 15 Figure 11: More visualizations for InnerControl (ours) method (LineArt) 16 Figure 12: More visualizations for InnerControl (ours) method (HED)"
        }
    ],
    "affiliations": [
        "AIRI, Russia",
        "HSE University, Russia",
        "Innopolis, Russia",
        "Sber, Russia"
    ]
}