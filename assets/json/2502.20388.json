{
    "paper_title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation",
    "authors": [
        "Sucheng Ren",
        "Qihang Yu",
        "Ju He",
        "Xiaohui Shen",
        "Alan Yuille",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a $k\\times k$ grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as \\textbf{continuous entity regression}, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20$\\times$ faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2$\\times$ faster than the previous best-performing model without relying on vision foundation modules (\\eg, DINOv2) or advanced guidance interval sampling."
        },
        {
            "title": "Start",
            "content": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation Sucheng Ren1 Qihang Yu2 Ju He2 Xiaohui Shen2 Alan Yuille1 Liang-Chieh Chen2 2ByteDance 1Johns Hopkins University https://oliverrensu.github.io/project/xAR 5 2 0 F 7 2 ] . [ 1 8 8 3 0 2 . 2 0 5 2 : r Figure 1. xAR: Autoregressive (AR) Visual Generation with Next-X Prediction. The proposed xAR adopts general next-X prediction framework, where is flexible prediction entity that can correspond to: (a) an individual image patch (as in vanilla AR [12]), (b) cell (a group of spatially contiguous tokens), (c) subsample (a non-local grouping), (d) an entire image (as in flow-matching [28]), or (e) scale (coarse-to-fine resolution, similar to VAR [48]). We use red, green, blue, yellow to illustrate the first four AR prediction steps for each entity example. The gray tokens represent the remaining tokens."
        },
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, token is treated as the smallest prediction unit, often discrete symbol in language or quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, generalized AR framework that extends the notion of token to an entity X, which can represent an individual patch token, cell (a grouping of neighboring patches), subsample (a non-local grouping of distant patches), scale (coarse-to-fine resolution), or even whole image. Additionally, we reformulate discrete token classification 1 as continuous entity regression, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20 faster inference. Meanwhile, xAR-H sets new state-of-the-art with an FID of 1.24, running 2.2 faster than the previous best-performing model without relying on vision foundation modules (e.g., DINOv2) or advanced guidance interval sampling. 1. Introduction Autoregressive (AR) models have driven major advances in natural language processing (NLP) through next-token prediction, where each token is generated from its preceding tokens. This framework enables coherent, context-aware text generation, with landmark models like GPT-3 [6] and its successors [33, 34] setting new benchmarks across diverse NLP applications. Building on the successes of AR modeling in NLP, researchers have extended this framework to computer vision, particularly for high-fidelity image generation [12, 27, 47, 58, 60]. In these approaches, image patches are discretized into tokens [52] and reshaped into 1D sequences, allowing AR models to predict each token sequentially. However, unlike language, where tokens correspond to semantically meaningful units such as words, vision lacks universally agreed-upon token definition. This naturally raises the question: How can next-token prediction be generalized to next-X prediction, and what constitutes the most suitable for image generation? Additionally, beyond token design, traditional AR models rely on teacher forcing [56] during training, where ground truth tokens are provided at each step instead of the models own predictions. While this stabilizes training, it introduces exposure bias [38], since the model is never exposed to potential errors. Consequently, during inference, without ground truth guidance, errors accumulate over time, leading to cascading errors and context drift as the model conditions solely on its past predictions. To address these challenges, we propose xAR, general next-X prediction framework that reformulates discrete token classification (conditioned on all preceding discrete ground truth tokens) into continuous entity regression problem conditioned on all previous noisy entities. The regression process is guided by flow-matching [28, 30] at each AR step. As illustrated in Fig. 1, within this framework, serves as flexible representation that can correspond to an individual patch token, cell (a group of surrounding tokens), subsample (a non-local grouping), scale (coarseto-fine resolution), or even an entire image. Unlike teacher forcing [56], which always provides ground truth inputs, xAR deliberately exposes the model to noisy contexts during training, allowing it to learn from imperfect, corrupted, or partially inaccurate conditions. We refer to this approach as Noisy Context Learning (NCL), reformulation that reduces reliance on ground truth inputs, improving robustness and mitigating exposure bias [38] by enabling the model to generalize better during inference. We demonstrate the effectiveness of xAR on the challenging ImageNet generation benchmark [10]. Through systematic experimentation with different configurations, we find that next-cell predictionwhere neighboring tokens are grouped into moderately sized cells (e.g., 88 Figure 2. ImageNet-256 Results. Our base model, xAR-B, outperforms DiT-XL [36] and SiT-XL [3] while achieving 20 faster inference, and our largest model, xAR-H, establishes new stateof-the-art with an FID of 1.24 on ImageNet-256. tokens)yields the best performance by capturing richer spatial-semantic relationships. Leveraging both next-cell prediction and Noisy Context Learning, our base model xAR-B (172M) outperforms the large DiT-XL [36] and SiTXL [3] (675M) while achieving 20 faster inference. Additionally, our largest model, xAR-H (1.1B), sets new state-of-the-art with an FID of 1.24 and runs 2.2 faster than the previous best-performing model [62] on ImageNet256 [10], without relying on vision foundation models (e.g., DINOv2 [35]) or extra guidance interval sampling [25]. 2. Related Work AR Modeling in NLP. Autoregressive language models [6, 33, 34, 37, 49] have driven significant progress toward general-purpose AI. Their core principle is simple yet powerful: predicting the next token based on preceding context. This approach has demonstrated impressive scalability, guided by scaling laws, and adaptability, enabling zero-shot generalization. These strengths have extended AR modeling beyond traditional language tasks, influencing wide range of modalities. AR Modeling in Vision. Inspired by the success of AR modeling in NLP, researchers have explored its application in vision [8, 39, 47, 48, 50, 58, 60]. pioneering effort in this direction was PixelCNN [50], which factorized the joint pixel distribution into product of conditionals, enabling the model to learn complex image distributions. This idea was further refined in PixelRNN [51], which incorporated recurrent layers to capture richer context in both horizontal and vertical directions. iGPT [8] extended this pixel-level approach by leveraging Transformers [53] for next-pixel prediction. Beyond next-pixel mod2 eling, AR methods have shifted toward more abstract token representations. VQ-VAE [52] introduced discrete latent codes that could be modeled autoregressively, offering compressed yet expressive representation of images. Later models like Parti [58] and LlamaGen [47] combined these learned tokens with Transformer-based architectures to generate high-fidelity images while maintaining scalable training. Recently, MAR [27] introduced diffusion-based approach [19, 46] to model per-token probability distributions in continuous space, replacing categorical cross-entropy with diffusion loss. VAR [48] extended next-token prediction to coarse-to-fine scale prediction paradigm, progressively refining image details. Our work unifies these approaches under general next-X prediction framework, where can flexibly represent tokens, scales, or our newly introduced cells, providing more flexible and generalizable formulation for autoregressive visual modeling. Diffusion and Flow Matching. Beyond autoregressive modeling, diffusion [19, 45, 46] and flow matching [13, 28, 30] have surpassed Generative Adversarial Networks (GANs) [15, 44] by employing multi-step denoising. Latent Diffusion Models (LDMs) [41] improve speed and scalability by operating in compressed latent space [24] instead of raw pixels. Building on this, DiT [36] and U-ViT [4] replace the traditional convolution-based U-Net [42] with Transformers [53] in latent space, further enhancing performance. Simple Diffusion [20, 21] introduces streamlined approach for scaling pixel-space diffusion models to highresolution outputs, while DiMR [29] progressively refines features across multiple scales, improving detail from low to high resolution. In parallel, flow matching [28, 30] reformulates the generative process by directly mapping data distributions to standard normal distribution, simplifying the transition from noise to structured data. SiT [3] builds on this by integrating flow matching into DiTs Transformer backbone for more efficient distribution alignment. Extending this approach, SD3 [13] introduces Transformer-based architecture that leverages flow matching for text-to-image generation. REPA [62] refines denoising by aligning noisy intermediate states with clean image embeddings extracted from pretrained visual encoders [35]. 3. Method In this section, we first provide an overview of autoregressive modeling with the next-token prediction paradigm in Sec. 3.1, followed by our proposed xAR framework with next-X prediction and Noisy Context Learning in Sec. 3.2. 3.1. Preliminary: Next-Token Prediction Autoregressive modeling with next-token prediction is fundamental approach in language modeling where the joint probability of token sequence is factorized into product of conditional probabilities. Formally, given sequence 3 = {x1, x2, . . . , xN }, the model estimates (x) = (cid:89) n=1 (cid:0)xn x1, x2, . . . , xn1 (cid:1). (1) In practice, an autoregressive language model predicts the next token xn through token classification, conditioned on all preceding tokens {x1, x2, . . . , xn1}. This process proceeds sequentially from left to right (i.e., = {1, . . . , }) until the full sequence is generated. For visual generation, VQ tokenizer [12, 52] discretizes an image into sequence of tokens. An autoregressive visual generation model then follows the next-token prediction paradigm, sequentially predicting tokens through classification conditioned on previously generated tokens. However, directly applying the next-token prediction paradigm to visual generation introduces several challenges: In NLP, each token (e.g., word) Information Density. carries rich semantic meaning. In contrast, visual tokens typically represent small image patches, which may not be as semantically meaningful in isolation. single patch can contain fragments of different objects or textures, making it difficult for the model to infer meaningful relationships between consecutive patches. Additionally, the quantization process in VQ-VAE [52] can discard fine details, leading to lower-quality reconstructions. As result, even if the model predicts the next token correctly, the generated image may still appear blurry or lack detail. Accumulated Errors. Teacher forcing [56], common training strategy, feeds the model ground truth tokens to stabilize learning. However, this reliance on perfect context causes exposure bias [2, 17]the model never learns to recover from its potential mistakes. During inference, when it must condition on its own predictions, small errors can accumulate over time, leading to compounding artifacts and degraded output quality. To address these challenges, we extend next-token prediction to next-X prediction, transitioning from traditional AR to xAR. This is accomplished by introducing more expressive prediction entity and training the model with noisy entities for improved robustness. 3.2. The Proposed xAR We introduce xAR, which consists of two key components: next-X prediction (Sec. 3.2.1) and Noisy Context Learning (Sec. 3.2.2). We first detail each component, then describe the inference strategy (Sec. 3.2.3), followed by discussion on how xAR enhances visual generation (Sec. 3.2.4). 3.2.1. Next-X Prediction Given an image, we use an off-the-shelf VAE [24] (instead of VQ-VAE [52] to avoid quantization loss) to convert it C, where and into continuous latent H denote image height and width, is the downsampling rate (we use = 16 [27]), and represents the number of channels. We then construct sequence of prediction entities = {X1, X2, . . . , XN } based on I. Each Xi is flexible entity that can represent an individual token (an image patch), cell (a group of surrounding tokens), subsample (a non-local grouping), scale (coarse-to-fine resolution), or even an entire image. We outline common choices for below and refer readers to Fig. 1 for visualization and Algorithm 1 for PyTorch pseudo-code implementation. Individual Patch Token (Fig. 1 (a)). When Xi corresponds to single image patch, xAR reduces to standard AR modeling, where each token is predicted sequentially. Cell (Fig. 1 (b)). The image is divided into an grid, where each cell has spatially adjacent tokens1. Subsample (Fig. 1 (c)). Entities are created by spatially and uniformly subsampling the image grid [12]. Entire Image (Fig. 1 (d)). As an extreme case, all tokens are grouped into single entity, i.e., = X1 = I, transforming xAR into flow matching method [28, 30]. Scale (Fig. 1 (e)). multi-scale hierarchical representation is constructed, similar to VAR [48]. Given any scale design {s1, . . . , sN }, we define Xi = resize(I, si), where resize refers to resizing the latent to the target scale si. By default, we set XN = ), and define Xi = resize(I, 1 2N ) which progressively refines predictions from coarse to fine scales. Unlike VAR [48], our approach generalizes next-scale prediction to any scale configuration and does not require specially designed multi-scale VQGAN tokenizer. Default Choice of X. Extensive ablation studies in Sec. 4.2 show that cell (with size of 88 tokens) achieves the best performance among all designs. Therefore, unless specified otherwise, xAR adopts 88 cells as the default X. 2N ) (i.e., si = C (i.e., sN = 1 3.2.2. Noisy Context Learning xAR transitions the paradigm from discrete token classification (conditioned on all preceding ground truth tokens) to continuous entity regression (conditioned on all previous noisy entities). Specifically, unlike traditional AR modeling, which directly classifies Xn based on all preceding ground truth entities {X1, . . . , Xn1}, xAR predicts Xn by minimizing regression loss derived from flow matching [28, 30], conditioned on all previous noisy entities. During training, we randomly sample noise time steps {t1, . . . , tn} [0, 1], and draw noise samples {ϵ1, . . . , ϵn} from the source Gaussian noise distribution. Specifically, at the n-th AR step, the noise samples are drawn as ϵn (0, I), where ϵn and Xn share the same 1We also experimented with rectangular cells (e.g., cells with shape k/2 2k or 2k k/2), but observed no significant difference compared to squared cells. Thus, we adopt the simpler squared cell design. Algorithm 1 PyTorch Pseudo-Code for General Entity from einops import rearrange import torch import torch.nn.functional as class xAR(nn.Module): # Construct sequence of entities based on the input latent. # Input: continuous latent with shape (b, c, h, w). # Return: sequence of entities with shape (b, s, c). def latent2token(self, latent): return latent.flatten(2).permute(0,2,1) def latent2cell(self, latent, k): # k: Group spatially neighboring tokens into one cell. return rearrange(latent, (h k1) (w k2) -> (h k1 k2) c, k1=k, k2=k) def latent2subsample(self, latent, distance): # distance: Group tokens based on evenly spaced distances. return rearrange(latent, (d1 h) (d2 w) -> (h d1 d2) c, d1=distance, d2=distance) def latent2scale(self, latent, scales): # scales: sequence of scale design. entities = [F.interpolate(latent, (i,i)).flatten(2).permute(0,2,1) for in scales] entities = torch.cat(entities, dim=1) return entities shape [41]. We construct the interpolated input tn = (cid:0)1 tn tn (cid:1)Xn + tnϵn. as: (2) Note that in , the superscript denotes the flow-matching noise time step, while the subscript represents the AR time step. We then define the velocity tn as: tn = dF tn dtn = ϵn Xn, (3) represents the directional flow from tn where tn toward Xn, guiding the transformation from the source to the target distribution. The model is trained to predict the velocity tn 1 , . . . , tn preceding and current noisy entities {F t1 using all }: = (cid:88) n=1 (cid:13) (cid:13)xAR(cid:0){F t1 (cid:13) 1 , . . . , tn }, tn; θ(cid:1) tn (cid:13) 2 (cid:13) (cid:13) , (4) where xAR denotes our xAR model parameterized by θ. We refer to this scheme as Noisy Context Learning (NCL), where the model is trained by conditioning on all previous noisy entities rather than perfect ground truth inputs. This effectively reduces reliance on clean training signals, improving robustness and mitigating exposure bias [38]. Fig. 3 (Training) provides an illustration of NCL. Notably, when sampling the time steps {t1, . . . , tn} [0, 1], no constraints are imposed (e.g., we do not enforce t1 > t2), allowing the model to experience varying degrees of noise in preceding entities, strengthening its adaptability during inference. 4 Figure 3. Conditioning Mechanism Comparison between Vanilla AR vs. xAR. During training, vanilla AR conditions on all preceding ground truth tokens (i.e., Teacher Forcing), whereas xAR conditions on all previous noisy entities, each with different noises (i.e., Noisy Context Learning). At inference, vanilla AR suffers from exposure bias, as errors accumulate over AR steps due to its exclusive training on ground truth tokens, leaving it unprepared for imperfect predictions. In contrast, xAR, trained to handle noisy inputs, reduces reliance on ground truth signals and improves robustness to prediction errors. 3.2.3. Inference Scheme xAR performs autoregressive prediction at the level of entity X. Since cell is the default choice for X, we use it as concrete example. As illustrated in Fig. 3 (Inference), xAR begins by predicting an initial cell ˆX1 from Gaussian noise sample ϵ1 (0, I) (where ϵ1 has the same shape as ˆX1) via flow matching [28, 30]. Conditioned on the clean estimate ˆX1, xAR generates the next cell ˆX2 from another Gaussian noise sample ϵ2. This process continues autoregressively, where at the i-th AR step, the model predicts the next cell ˆXi based on all previously generated clean cells { ˆX1, , ˆXi1} and the newly drawn Gaussian noise sample ϵi. This iterative approach progressively refines the image, ensuring structured and context-aware generation at the cell level. 3.2.4. Discussion As discussed in Sec. 3.1, traditional AR modeling for visual generation faces two key challenges: information density and accumulated errors. The proposed xAR is designed to address these limitations. Semantic-Rich Prediction Entity. cell (i.e., grouping of spatially contiguous tokens) aggregates neighboring tokens, effectively capturing both local structures (e.g., edges, textures) and regional contexts (e.g., small objects or parts of larger objects). This leads to richer semantic representations compared to single-token predictions. By modeling relationships within the cell, the model learns to generate coherent local and regional features, shifting from isolated token-level predictions to holistic patterns. Additionally, predicting cell rather than an individual token allows the model to reason at higher abstraction level, akin to how NLP models predict words instead of characters. The larger receptive field per prediction step contributes more semantic information, bridging the gap between lowlevel visual patches and high-level semantics. Robustness to Previous Prediction Errors. The Noisy Context Learning (NCL) strategy trains the model on noisy entities instead of perfect ground truth inputs, reducing over-reliance on pristine contexts. This alignment between training and inference distributions enhances the models ability to handle errors in self-generated predictions. By conditioning on imperfect contexts, xAR learns to tolerate minor inaccuracies, preventing small errors from compounding into cascading errors. Additionally, exposure to noisy inputs encourages smoother representation learning, leading to more stable and consistent generations. 4. Experimental Results In this section, we present the main results in Sec. 4.1, followed by ablation studies on key design choices in Sec. 4.2. 5 type GAN GAN GAN Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Flow-Matching Flow-Matching Flow-Matching Mask. Mask. Mask. Mask. AR AR AR AR AR AR AR AR AR AR AR MAR MAR MAR VAR VAR VAR xAR xAR xAR model BigGAN [5] GigaGAN [22] StyleGan-XL [44] ADM [11] LDM-4-G [41] Simple-Diffusion [20] DiT-XL/2 [36] L-DiT-3B [1] DiMR-G/2R [29] MDTv2-XL/2 [14] CausalFusion-H [9] SiT-XL/2 [3] REPA [62] REPA [62] MaskGIT [7] TiTok-S-128 [61] MAGVIT-v2 [59] MaskBit [55] VQVAE-2 [39] VQGAN [12] VQGAN [12] RQTran. [26] ViTVQ [57] DART-AR [16] MonoFormer [63] Open-MAGVIT2-XL [32] LlamaGen-3B [47] FlowAR-H [40] RAR-XXL [60] MAR-B [27] MAR-L [27] MAR-H [27] VAR-d16 [48] VAR-d20 [48] VAR-d30 [48] xAR-B xAR-L xAR-H #params 112M 569M 166M 554M 400M 2B 675M 3.0B 1.1B 676M 1B 675M 675M 675M 227M 287M 307M 305M 13.5B 227M 1.4B 3.8B 1.7B 812M 1.1B 1.5B 3.1B 1.9B 1.5B 208M 479M 943M 310M 600M 2.0B 172M 608M 1.1B FID 6.95 3.45 2.30 10.94 3.60 2.44 2.27 2.10 1.63 1.58 1.57 2.06 1.80 1.42 6.18 1.97 1.78 1.52 31.11 18.65 15.78 7.55 4.17 3.98 2.57 2.33 2.18 1.65 1.48 2.31 1.78 1.55 3.30 2.57 1.97 1.72 1.28 1.24 IS 224.5 225.5 265.1 101.0 247.7 256.3 278.2 304.4 292.5 314.7 - 277.5 284.0 305.7 182.1 281.8 319.4 328.6 45 80.4 74.3 134.0 175.1 256.8 272.6 271.8 263.3 296.5 326.0 281.7 296.0 303.7 274.4 302.6 323.1 280.4 292.5 301. Precision 0.89 0.84 0.78 0.69 - - 0.83 0.82 0.79 0.79 - 0.83 0.81 0.80 0.80 - - - 0.36 0.78 - - - - 0.84 0.84 0.81 0.83 0.80 0.82 0.81 0.81 0.84 0.83 0.82 0.82 0.82 0.83 Recall 0.38 0.61 0.53 0.63 - - 0.57 0.60 0.63 0.65 - 0.59 0.61 0.65 0.51 - - - 0.57 0.26 - - - - 0.56 0.54 0.58 0.60 0.63 0.57 0.60 0.62 0.51 0.56 0.59 0.59 0.62 0.64 Table 1. Generation Results on ImageNet-256. Metrics include Frechet Inception Distance (FID), Inception Score (IS), Precision, and Recall. denotes the use of guidance interval sampling [25]. The proposed xAR-H achieves state-of-the-art 1.24 FID on the ImageNet256 benchmark without relying on vision foundation models (e.g., DINOv2 [35]) or guidance interval sampling [25], as used in REPA [62]. 4.1. Main Results We conduct experiments on ImageNet [10] at 256256 and 512512 resolutions. Following prior works [27, 36], we evaluate model performance using FID [18], Inception Score (IS) [43], Precision, and Recall. xAR is trained with the same hyper-parameters as [27, 36] (e.g., 800 training epochs), with model sizes ranging from 172M to 1.1B parameters. See Appendix Sec. for hyper-parameter details. ImageNet-256. In Tab. 1, we compare xAR with previous state-of-the-art generative models. Out best variant, xAR-H, achieves new state-of-the-art-performance of 1.24 FID, outperforming the GAN-based StyleGANXL [44] by 1.06 FID, masked-prediction-based MaskBit [7] 6 model VQGAN [12] BigGAN [5] MaskGiT [7] DiT-XL/2 [36] DiMR-XL/3R [29] VAR-d36 [48] REPA [62] xAR-L FID #params 227M 26.52 8.43 158M 7.32 227M 3.04 675M 2.89 525M 2.63 2.3B 2.08 675M 1.70 608M IS 66.8 177.9 156.0 240.8 289.8 303.2 274.6 281.5 Table 2. Generation Results on ImageNet-512. denotes the use of DINOv2 [35]. by 0.28 FID, AR-based RAR [60] by 0.24 FID, VAR [48] by 0.73 FID, MAR [27] by 0.31 FID, and flow-matchingbased REPA [62] by 0.18 FID. Notably, xAR does not rely on vision foundation models [35] or guidance interval sampling [25], both of which were used in REPA [62], the previous best-performing model. Additionally, our lightweight xAR-B (172M), surpasses DiT-XL (675M) [36] by 0.55 FID while achieving an inference speed of 9.8 images per second20 faster than DiT-XL (0.5 images per second). Detailed speed comparison can be found in Appendix B. ImageNet-512. In Tab. 2, we report the performance of xAR on ImageNet-512. Similarly, xAR-L sets new stateof-the-art FID of 1.70, outperforming the diffusion based DiT-XL/2 [36] and DiMR-XL/3R [29] by large margin of 1.34 and 1.19 FID, respectively. Additionally, xAR-L also surpasses the previous best autoregressive model VARd36 [48] and flow-matching-based REPA [62] by 0.93 and 0.38 FID, respectively. Qualitative Results. Fig. 4 presents samples generated by xAR (trained on ImageNet) at 512512 and 256256 resolutions. These results highlight xARs ability to produce high-fidelity images with exceptional visual quality. 4.2. Ablation Studies In this section, we conduct ablation studies using xAR-B, trained for 400 epochs to efficiently iterate on model design. Prediction Entity X. The proposed xAR extends nexttoken prediction to next-X prediction. In Tab. 3, we evaluate different designs for the prediction entity X, including an individual patch token, cell (a group of surrounding tokens), subsample (a non-local grouping), scale (coarse-to-fine resolution), and an entire image. Among these variants, cell-based xAR achieves the best performance, with an FID of 2.48, outperforming the tokenbased xAR by 1.03 FID and surpassing the second best design (scale-based xAR) by 0.42 FID. Furthermore, even when using standard prediction entities such as tokens, subsamples, images, or scales, xAR consistently outperforms existing methods while requiring significantly fewer parameters. These results highlight the efficiency and effectivemodel LlamaGen-L [47] xAR-B PAR-L [54] xAR-B DiT-L/2 [36] xAR-B VAR-d16 [48] xAR-B xAR-B prediction entity token subsample image scale cell #params FID IS 343M 172M 343M 172M 458M 172M 310M 172M 172M 3.80 3.51 3.76 3.58 5.02 3.13 3.30 2.90 2.48 248.3 251.4 218.9 231.5 167.2 253.4 274.4 262.8 269.2 Table 3. Ablation on Prediction Entity X. Using cells as the prediction entity outperforms alternatives such as tokens or entire images. Additionally, under the same prediction entity, xAR surpasses previous methods, demonstrating its effectiveness across different prediction granularities. cell size (k tokens) grid 1 1 2 2 4 4 8 8 16 16 16 16 8 8 4 4 2 2 1 1 FID 3.51 3.04 2.61 2.48 3.13 IS 251.4 253.5 258.2 269.2 253.4 Table 4. Ablation on the cell size. In this study, 16 16 continuous latent representation is partitioned into an grid, where each cell consits of neighboring tokens. cell size of 8 8 achieves the best performance, striking an optimal balance between local structure and global context. ness of xAR across diverse prediction entities. Cell Size. prediction entity cell is formed by grouping spatially adjacent tokens, where larger cell size incorporates more tokens and thus captures broader context within single prediction step. For 256 256 input image, the encoded continuous latent representation has spatial resolution of 16 16. Given this, the image can be partitioned into an grid, where each cell consists of neighboring tokens. As shown in Tab. 4, we evaluate different cell sizes with {1, 2, 4, 8, 16}, where = 1 represents single token and = 16 corresponds to the entire image as single entity. We observe that performance improves as increases, peaking at an FID of 2.48 when using cell size 8 8 (i.e., = 8). Beyond this, performance declines, reaching an FID of 3.13 when the entire image is treated as single entity. These results suggest that using cells rather than the entire image as the prediction unit allows the model to condition on previously generated context, improving confidence in predictions while maintaining both rich semantics and local details. Noisy Context Learning. During training, xAR employs Noisy Context Learning (NCL), predicting Xn by conditioning on all previous noisy entities, unlike Teacher Forc7 Figure 4. Generated Samples. xAR generates high-quality images at resolutions of 512512 (1st row) and 256256 (2nd and 3rd row). previous cell clean increasing noise decreasing noise random noise noise time step ti = 0, < t1 < t2 < < tn1 t1 > t2 > > tn1 no constraint FID 3.45 2.95 2.78 2. IS 243.5 258.8 262.1 269.2 Table 5. Ablation on Noisy Context Learning. This study examines the impact of noise time steps (t1, , tn1 [0, 1]) in previous entities (t = 0 represents pure Gaussian noise). Conditioning on all clean entities (the clean variant) results in suboptimal performance. Imposing an order on noise time steps, either increasing noise or decreasing noise, also leads to inferior results. The best performance is achieved with the random noise setting, where no constraints are imposed on noise time steps. ing. The noise intensity of previous entities is contorlled by noise time steps {t1, . . . , tn1} [0, 1], where = 0 corresponds to pure Gaussian noise. We analyze the impact of NCL in Tab. 5. When conditioning on all clean entities (i.e., the clean variant, where ti = 0, < n), which is equivalent to vanilla AR (i.e., Teacher Forcing), the suboptimal performance is obtained. We also evaluate two constrained noise schedules: the increasing noise variant, where noise time steps increase over AR steps (t1 < t2 < < tn1), and the decreasing noise variant, where noise time steps decrease (t1 > t2 > > tn1). While both settings improve over the clean variant, they remain inferior to our final random noise setting, where no constraints are imposed on noise time steps, leading to the best performance. 5. Conclusion In this work, we introduced xAR, general next-X prediction framework for autoregressive visual generation. Unlike traditional next-token prediction, xAR reformulates discrete token classification as continuous entity regression, enabling more flexible and semantically meaningful prediction units. Through systematic exploration, we found that next-cell prediction provides the best balance between local structure and global coherence. To mitigate exposure bias, we proposed Noisy Context Learning (NCL), which trains the model on noisy entities instead of pristine ground truth inputs, improving robustness and reducing cascading errors. As result, xAR achieves state-of-the-art performance on ImageNet-256 and ImageNet-512."
        },
        {
            "title": "References",
            "content": "[1] Alpha-vllm. large-dit-imagenet. 2024. 6 [2] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language genIn Findings of the Association for Computational eration. Linguistics: ACL 2022, 2022. 3 [3] Sara Atito, Muhammad Awais, and Josef Kittler. Sit: arXiv preprint Self-supervised vision transformer. arXiv:2104.03602, 2021. 2, 3, 6, 1 [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 3 [5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 6, [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 2 [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 6, 7 [8] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. 2 [9] Chaorui Deng, Deyao Zh, Kunchang Li, Shi Guan, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. 6 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 2, 6 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34, 2021. 6 [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 1, 2, 3, 4, 6, 7 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 6 [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 3 [16] Jiatao Gu, Yuyang Wang, Yizhe Zhang, Qihang Zhang, Dinghuai Zhang, Navdeep Jaitly, Josh Susskind, and Shuangfei Zhai. Dart: Denoising autoregressive transformer for scalable text-to-image generation. In ICLR, 2025. 6 [17] Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. Exposure bias versus self-recovery: Are distortions 9 really incremental for autoregressive text generation? EMNLP, 2021. In [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. 6 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [20] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. 3, 6 [21] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion (sid2): 1.5 fid on imagenet512 with pixel-space diffusion. arXiv preprint arXiv:2410.19324, 2024. [22] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, 2023. 6 [23] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 1 [24] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 3 [25] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. 2, 6, [26] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. 6 [27] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. NeurIPS, 2024. 2, 3, 4, 6, 7, 1 [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 2, 3, 4, 5 [29] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024. 3, 6, 7 [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2, 3, 4, [31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 1 [32] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 6 [33] OpenAI. Introducing chatgpt. https://openai.com/ blog/chatgpt/, 2022. 2 [34] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, [52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017. 2, 3 [53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 2, 3 [54] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024. 7 [55] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: arXiv Embedding-free image generation via bit tokens. preprint arXiv:2409.16211, 2024. 6 [56] Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989. 2, 3 [57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 6 [58] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 2, [59] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 6 [60] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. 2, 6, 7 [61] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024. 6 [62] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 2, 3, 6, 7, 1 [63] Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. 6 Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 6, [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, 3, 6, 7, 1 [37] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by genhttps : / / cdn . openai . com / erative pre-training. research - covers / language - unsupervised / language_understanding_paper.pdf, 2018. 2 [38] MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In ICLR, 2016. 2, 4 [39] Ali Razavi, Aaron Van Den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. NeurIPS, 2019. 2, 6 [40] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. 6 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 4, [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 3 [43] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 6 [44] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. arXiv preprint arXiv:2201.00273, 2022. 3, 6 [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. 3 [46] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019. [47] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 3, 6, 7 [48] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. NeurIPS, 2024. 1, 2, 3, 4, 6, 7 [49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [50] Aaron Van Den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 2016. 2 [51] Aaron Van Den Oord, Nal Kalchbrenner, and Koray In ICML, Kavukcuoglu. Pixel recurrent neural networks. 2016. 10 Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation"
        },
        {
            "title": "Appendix",
            "content": "The supplementary material includes the following additional information: Sec. details the hyper-parameters used for xAR. Sec. provides comprehensive speed comparison. Sec. discusses the limitations and future directions. Sec. presents visualization samples generated by xAR. A. Hyper-parameters for xAR We list the detailed training and inference hyper-parameters in Tab. 6. config optimizer optimizer momentum weight decay batch size learning rate schedule peak learning rate ending learning rate total epochs warmup epochs dropout rate attn dropout rate class label dropout rate inference mode inference steps value AdamW [23, 31] (0.9, 0.96) 0.02 2048 cosine decay 4e-4 1e-5 800 100 0.1 0.1 0.1 SDE 50 Table 6. Detailed Hyper-parameters of xAR Models. B. Speed Comparison. We compare xAR with diffusion-, flow matching-, and autoregressive-based models in Tab. 7. Our most lightweight variant, xAR-B (172M), outperforms DiTXL (diffusion-based), SiT-XL (flow matching-based), and MAR (autoregressive-based), while achieving 20 speedup (9.8 vs. 0.5 images/sec). Additionally, xAR-L surpasses the recent state-of-the-art model REPA, running 5.3 faster (3.2 vs. 0.6 images/sec). Finally, our largest model, xAR-H, achieves 1.24 FID on ImageNet-256, setting new state-of-the-art, while still running 2.2 faster than REPA. C. Discussion and Limitations Our empirical evaluations indicate that square 88 cell configuration achieves the best performance, with no noticeable difference when using rectangular cells (e.g., k/2 2k or 2k k/2), which introduce additional complexity method #params FID steps images/sec type 675M 2.27 DiT-XL/2 [36] Diff. 675M 2.02 Flow. SiT-XL/2 [3] 479M 1.78 AR MAR-L [27] 172M 1.72 xAR xAR-B MAR-H [27] MAR 943M 1.55 675M 1.42 Flow. REPA [62] 608M 1.28 xAR xAR-L 1.24 1.1B xAR xAR-H 250 250 256 50 256 250 50 50 0.5 0.5 0.5 9.8 0.3 0.6 3.2 1.3 Table 7. Sampling Throughput Comparison. Throughputs are evaluated as samples generated per second on single A100 based on their official codebases. without clear benefits. Given that different regions in an image contain varying levels of semantic information (e.g., dense object areas vs. uniform sky regions), future research could explore whether dynamically shaped prediction entities provide additional benefits. However, in this work, we adopt simple yet effective square cell design, demonstrating state-of-the-art results on the challenging ImageNet generation benchmark. D. Visualization of Generated Samples Additional visualization results generated by xAR-H are provided from Fig. 5 to Fig. 13. Figure 5. Generated Samples from xAR. xAR is able to generate high-fidelity American eagle (22) images. 1 Figure 6. Generated Samples from xAR. xAR is able to generate high-fidelity macaw (88) images. Figure 8. Generated Samples from xAR. xAR is able to generate high-fidelity otter (360) images. Figure 7. Generated Samples from xAR. xAR is able to generate high-fidelity golden retriever (207) images. Figure 9. Generated Samples from xAR. xAR is able to generate high-fidelity lesser panda (387) images. 2 Figure 10. Generated Samples from xAR. xAR is able to generate high-fidelity coral reef (973) images. Figure 12. Generated Samples from xAR. xAR is able to generate high-fidelity valley (979) images. Figure 11. Generated Samples from xAR. xAR is able to generate high-fidelity geyser (974) images. Figure 13. Generated Samples from xAR. xAR is able to generate high-fidelity volcano (980) images."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Johns Hopkins University"
    ]
}