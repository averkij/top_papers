{
    "paper_title": "Einstein Fields: A Neural Perspective To Computational General Relativity",
    "authors": [
        "Sandeep Suresh Cranganore",
        "Andrei Bodnar",
        "Arturs Berzins",
        "Johannes Brandstetter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the \\emph{metric}, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural Tensor Fields} with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 9 8 5 1 1 . 7 0 5 2 : r EINSTEIN FIELDS: NEURAL PERSPECTIVE TO COMPUTATIONAL GENERAL RELATIVITY Sandeep S. Cranganore 1 Andrei Bodnar 2 Johannes Brandstetter 1,3 Arturs Berzins 1 Equal contribution 1LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2University of Manchester, United Kingdom 3Emmi AI GmbH, Linz, Austria {cranganore, berzins, brandstetter}@ml.jku.at andrei.bodnar@student.manchester.ac.uk"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Einstein Fields, neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the metric, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are Neural Tensor Fields with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at https://github.com/AndreiB137/EinFields."
        },
        {
            "title": "INTRODUCTION",
            "content": "General relativity (GR) is foundational theory of modern physics that describes gravity as the curvature of spacetime caused by mass and energy. This curvature, along with angles, distances, geodesics, and causality, is encoded in the metric. At the heart of GR lie the Einstein field equations (EFEs), set of coupled, tensor-valued, non-linear hyperbolic-elliptic partial differential equations (PDEs) that govern the metric behavior. EFEs are exceptionally complex, and exact analytical solutions exist only in idealized cases. To solve EFEs in more general settings, numerical relativity (NR) has emerged as powerful computational tool, whose predictive accuracy has repeatedly proven to be remarkable. Notable successes include the high-precision modeling of astrophysical phenomena such as mergers of binary black holes (Abbott et al., 2016a;b;c), binary neutron stars (Hayashi et al., 2025), and neutron starblack hole systems (Abbott et al., 2017). These numerical models have also played central role in the detection of gravitational waves (GWs) by the Laser Interferometer Gravitational-Wave Observatory (LIGO) (Collaboration et al., 2015) and Virgo (Acernese et al., 2014), leading to groundbreaking, Nobel-prize winning discoveries (Castelvecchi, 2017). Likewise, the precision of applied technologies, most notably the global positioning system (GPS), also depends on subtle relativistic effects, such as gravitational time dilation, which constitutes the influence of gravity on the flow of time. Incorporating these corrections and recalibrating the GPS satellites is crucial for providing accurate real-time positioning data, as even tiny relativistic discrepancies lead to significant errors in location estimates. Nonetheless, state-of-the-art NR is one of the most computationally intensive domains of scientific computing, typically requiring petascale computing infrastructures (Huerta et al., 2019). With new 1 Figure 1: conceptual overview of EinFields training and downstream pipeline. (i) Premise: The Einstein field equations (EFEs) in Equation (1) are highly non-linear partial differential equations defined on 4D spacetime manifold, describing the geometric nature of gravitation. Their solutions define the metric tensor field gαβ(xµ), which encodes the full spacetime geometry and serves as tensorial generalization of the gravitational potential. In this work, we parametrize gαβ(xµ) using neural network. (ii) Training: The training is conducted on the metric tensor fields defined on 4D spacetime grid. EinFields instead fit continuous signal on these discrete representations, thus modeling 4D spacetime as continuum, and returning the metric tensor field for 4D spacetime query coordinate (t, x) at arbitrary resolution. (iii) Sobolev supervision: The reconstruction quality of the metric and its derivatives is improved by augmenting Sobolev losses, i.e., metric Jacobian (neighborhood structure) and Hessian (curvatures). (iv) Validation and downstream tasks: Sobolev improved EinFields AD-based derivatives enable accurate point-wise retrieval of differential geometric quantities, such as the Levi-Civita connection (covariant derivative), geodesics, curvature tensors, and their invariants. generation of more sensitive detectors underway, e.g., the Einstein Telescope (Punturo et al., 2010; Abac et al., 2025) and the Laser Interferometer Space Antenna (Amaro-Seoane et al., 2017), there is strong demand for further improving the performance and accuracy of NR. With the growing success of machine learning-based approaches to scientific modeling, particularly neural PDE surrogates for spatiotemporal dynamics and forecasting (Thuerey et al., 2021; Zhang et al., 2023; Brunton et al., 2020; Li et al., 2021; Brandstetter et al., 2022; Bodnar et al., 2025; Brandstetter, 2025), and neural fields (NeFs) for compact representations of scenes (Mildenhall et al., 2021), shapes (Park et al., 2019; Chen & Zhang, 2019; Mescheder et al., 2019), images (Karras et al., 2021), audio, video (Sitzmann et al., 2020), or physical quantities (Raissi et al., 2019), it is worth exploring the potential for hybrid methodologies that integrate machine-learning and traditional NR frameworks. In the context of GR, we hypothesize that NeFs offer promising advantages for modeling continuum fields, where data are costly to store and accurate differentiation is crucial for capturing physical laws. However, the key difference to conventional NeFs is that when encoding the spacetime geometry of GR into NeFs representations, dynamics emerge naturally as byproduct. In this work, we introduce Einstein Fields or, for short, EinFields representation that can parameterize computationally intensive four-dimensional NR simulations as compact neural network weights. schematic of EinFields is shown in Figure 1. Contrary to traditional NR approaches, EinFields model the spacetime and the tensor-valued fields defined on it as continuum. EinFields are NeFs that parametrize the metric tensor field (t, x) R1,3 (cid:55) gαβ(t, x) R44 (or R10 due to symmetry). Since the metric and its first two derivatives entirely encode the geometric structure of the spacetime and, thus, many physical laws and phenomena, we leverage automaticdifferentiation (AD) that are native to NeFs (Griewank, 2003; Griewank & Walther, 2008; Baydin et al., 2018). Since many physical phenomena predicted by GR, such as the precession of orbits, the bending of light, or GW formation, require high precision when modeled computationally, achieving small errors is central quality criterion for EinFields. We show the impact of the model, the optimizer, the loss function, impact of the coordinate system, as well as the differentiation strategy, successfully achieving an agreement down to 1E8 relative error and recovering these sensitive physical phenomena in simulated experiments. It is worth stressing that this work is designed to encode existing simulations into neural network weights, rather than serving as surrogate solver. Unlike NeFs in existing applications, e.g., for representing 3D scenes or geometries, EinFields must respect the non-trivial aspects of differential geometry and gravitational physics: (i) encapsulate all features of (pseudo-) Riemannian metric, such as raising and lowering of contravariant and covariant indices of other tensors; (ii) capture the underlying physics, not coordinates-associated artifacts; (iii) yield highly accurate metric tensor derivatives, pivotal for reconstructing dynamics, such as geodesics (Equation (2)), curvature invariants (Equation (79)), and satisfying the EFEs (Equation (1)). Owing to the properties of NeFs, EinFields inherit the following advantages: Neural compression. EinFields encode the geometric information in compact neural network representations (typically less than 2 million parameters), agreeing with groundtruth metric tensor components up to 1E9 relative precision. These models offer highly memory-efficient approximations of complex geometric structures. Discretization-free. The continuity of NeFs enables the modeling of gravitational events without the discretization of the domain. These neural implicits can be fit with arbitrary point samples (e.g., regular grids, irregular grids, unstructured point clouds) and can be evaluated at arbitrary points of the spacetime. This avoids resolution-limited artifacts and increases flexibility and ease of use. Tensor differentiation. EinFields are smooth (C(M)) MLP parameterizations. This enables querying higher-order tensor derivatives, e.g., Christoffel symbols, Riemann tensor, and curvature invariants, via point-wise AD. We reconstruct differential geometric quantities with AD, which is accurate and easy to use in contrast to higher-order finite-difference (FD) methodologies that require adaptive mesh refinement (AMR) (Berger & Oliger, 1984) near high-curvature regions. Our secondary set of contributions mainly concerns the training and validation-related aspects: 3 Reconstructing seminal tests of GR. We use synthetic data generated from analytical solutions to validate and characterize EinFields. Beyond conventional ML evaluation metrics, we use the reparameterized metrics to evaluate derived physical quantities and reconstruct famous experimental predictions of GR. These include (i) perihelion precession around Schwarzschild metrics; (ii) geodesics around Kerr metrics with varying spins and ring-singularity through Kretschmann invariant; (iii) deformation of ring due to gravitational waves, Weyl scalar Ψ4, and radiated power of outgoing fields in the linearized gravity setting. These phenomena are due to small corrections from Newtonian gravity and serve as sensitive indicators of the metric quality. JAX-based differential geometry library. We introduce differential geometry library based on JAX (Bradbury et al., 2018), leveraging its AD and accelerated linear algebra (XLA) features (just-in-time compilation and automatic vectorization). Our library includes: 1. Tensor algebra: handling of coand contravariant indices, tensor contractions, and specialized operations; 2. Tensor calculus: suite of differential geometric quantities, operations (Lie derivatives, covariant derivatives), and identities common in GR; 3. Collection of common analytical solutions of EFEs."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Our work combines two primary ideas: general relativity from physics and neural fields from machine learning. We begin with brief introduction of GR, stressing three key properties that pertain to our work: GR is field theory, GR is coordinate independent, and physics is entirely contained in the metric and its first two derivatives. For this, we shall introduce the technical concepts of tensors and their coordinate transformations. For more complete introduction, we refer to Appendix and the sources therein. 2.1 GENERAL RELATIVITY GR is tensor field theory. GR, formulated by Albert Einstein, extends Newtonian gravity with geometric interpretation of gravity, where mass and energy tell spacetime how to curve, and curved spacetime tells objects how to move (Misner et al., 2017). This is formalized by Einsteins field equations (EFEs) Gαβ + Λgαβ = 8πG Tαβ . (1) EFEs are set of 10 coupled non-linear, tensor-valued, second-order partial differential equations and can be viewed as tensorial generalization of the Newton-Poisson equation for gravity 2Φ(r) = 4πGρ(r) (Misner et al., 2017; Poisson, 2004). In EFEs, Gαβ = Rαβ 1 2 Rgαβ is the Einstein tensor, formed from the metric tensor gαβ, the Ricci curvature tensor Rαβ, and the Ricci curvature scalar R. Thus, the left-hand side of EFEs is entirely described by the metric and its derivatives, with Λ being the cosmological constant. The right-hand side depends on the stress-energy tensor Tαβ describing the matter distribution, with being Newtons constant. The backbone of these highly complicated field equations are tensors, and the mathematical formalism is differential geometry. Spacetime is modeled as four-dimensional continuum with the structure of pseudo-Riemannian (Lorentzian) manifold M. pseudo-Riemannian manifold is equipped with an indefinite metric that allows for both positive and negative distances. Tensors. rank (r, s) tensor at point is the multilinear map from covectors and vectors to real number: : ... (cid:125) (cid:123)(cid:122) rcopies (cid:124) ... (cid:125) (cid:123)(cid:122) scopies (cid:124) . The vectors and covectors pair with the respective covariant and contravariant components of the tensor. As such, tensor is an element that lives in tensor product of vector and dual spaces, i.e., (V)r (V )s. 4 tensor in particular choice of basis {eαn }1nr and {ϑβn }1ns is given by = α1α2...αr β1β2...βs eα1 eαr ϑβ1 ϑβs , (ϑα1 , , ϑαr , eβs, , eβs ) are the components of the tensor in this where α1α2...αr particular basis. β1β2...βs Importantly, the abstract tensor itself is coordinate-independent object. Similarly, EFEs are diffeomorphism covariant, meaning physical laws remain unchanged under differentiable coordinate transformations (Misner et al., 2017; Tao, 2008). Tensor fields. tensor field is collection of tensor-valued quantities such that at each point M, the multilinear function associates value Tx . Consequently, the components α1...αr (x) are functions of the points of the manifold. For example, both the metric β1...βs tensor field : TxM TxM R, i.e., = gαβ(x) dxα dxβ and Ricci tensor Rαβ(x) are of rank (0, 2). The Ricci curvature scalar is rank 0 tensor: : R, and, similarly, velocity and gradient vector fields constitute rank (1, 0) and rank (0, 1) tensors, respectively1. (cid:0)V (cid:1)q Metric. The metric is rank (0, 2) symmetric bilinear form : TxM TxM that generalizes the notion of an inner product on the tangent space TxM of differentiable manifold (Jost, 2008). It enables the computation of angles between vector fields defined on the manifold and provides means to measure distances via the line element, expressed in local coordinates as ds2 = gαβ(xµ) dxαdxβ, where gαβ denotes the components of the metric tensor in chosen coordinate system, which may be curvilinear and need not be intrinsically Euclidean (see Appendix A.3.2 for detailed exposition). In relativistic physics, the metric is often denoted as ds2 or dτ 2, and in general relativity, it serves as the tensorial generalization of the gravitational potential, encoding the causal structure and geometric properties of spacetime. The spacetime metric tensor gαβ is 4 4 symmetric matrix with ten independent components, and together with its first and second derivatives, fully determines wide range of physical phenomena. These include gravitational wave propagation, gravitational lensing, time dilation, black hole dynamics, and the evolution of astrophysical systems under the influence of gravity. Derivative operators. The metric and its partial derivatives can be used to construct the Christoffel symbols Γρ µν(x) := gρσ(cid:0)µgσν(x) + νgσµ(x) σgµν(x)(cid:1) . 1 2 The Christoffel symbols denote how the metric varies across spacetime and define parallel transport machinery to translate tensor fields around the manifold. With these, it is possible to construct two pivotal modified tensor differentiation operators, namely: (i) The covariant derivative (also called the Levi-Civita connection), which can be seen as calibration of the partial derivative operator for parallel transportation in the curvilinear setting: µT α1...αr β1...βs = xµ α1...αr β1...βs + (cid:88) i= Γαi µσT α1...σ...αr β1...βs (cid:88) j=1 Γσ µβj α1...αr β1...σ...βs , (ii) The Lie derivative, which generalizes the notion of directional derivative that is connection independent (cf. Appendix A.3.3.2). The Lie derivative captures infinitesimal dragging of the tensor field along the flow generated by the vector field ξ: (LξT )α1...αr β1...βs = ξµµT α1...αr β1...βs (cid:88) i=1 α1...µ...αr β1...βs µξai + (cid:88) j=1 α1...αr β1...µ...βs βj ξµ . Differential geometric objects. Using the modified derivatives, we can construct hierarchy of higher-rank differential geometric quantities, such as the Riemannian curvature tensor Rδ αβγ or the 1Colloquially speaking, the components of vector transform contravariantly, i.e., opposite to how the base set transform, and the components of contravariant vector transform covariantly, i.e., in the same way as the base set transforms. 5 Ricci tensor Rαβ, via series of derivatives , covariant derivatives := + Γ, and tensor index contractions : r+1 (typically, Trg). s+1 α gαβ Γγ αβ Cαβγδ CCC CCC Rδ αβγ Rαβ LX R[αβγ]σ = 0 [σRαβ]γδ = 0 Figure 2: Differential geometry workflow in general relativity (only lhs of Equation (1)): We describe each quantity starting left: The metric tensor gαβ defines the spacetime geometry. Its partial derivatives yield the Christoffel symbols Γγ αβ, which describe the notion of parallel transport and defines covariant derivative operation α = α + Γα. The connection /JBnot defined also defines the Lie derivative Lv along vector fields v. The connections derivatives give the Riemann αβγ, which encodes tidal forces. The contraction operator = Trg contracts with curvature tensor Rδ the metric, producing the trace part, i.e., the Ricci tensor Rαβ. Its subsequent contraction yields the Ricci scalar R. The Riemann tensor further splits into the Weyl tensor Cαβγδ (trace-free curvature part) and satisfies the algebraic and differential Bianchi identities R[αβγ]σ = 0 and [σRαβ]γδ = 0. Together, these geometric objects form the backbone of general relativity, ultimately entering the Einstein field equations through the Einstein tensor Gαβ. It follows from the contracted Bianchi identities, i.e., cyclic sum of Riemann Conservation laws. curvature tensor covariant derivatives (II Bianchi identity see Equation (74b)) vanishes identically: αRβγδσ + βRγαδσ + γRαβδσ = 0 . This is geometric identity that holds for any (torsion-free) connection compatible with the metric. The identity above consequently leads to the covariant derivative of the stress-energy tensor vanishing, that is, βT αβ := 0 (see Equation (81)), which corresponds to the energy-momentum conservation in general relativity. If required, conservation laws typically feature as soft constraints in PDEs, and are relevant especially when matter distribution/fields are considered. Equations of motion. The geodesic equation is central second-order ODE that describes the motion of objects following geodesic paths in the curved spacetime background d2xµ dτ 2 + Γµ dxρ dτ Here, τ is some affine parameter (often the proper time; not to be confused with the coordinate time t). The geodesic equation is the general relativistic analogue of Newtons second law and generalizes the concept of straight line to curved spacetime by describing the trajectories of bodies under the influence of gravitational field. Related, the geodesic deviation equation describes how nearby geodesics diverge or converge due to the intrinsic curvature of the manifold, quantified by the separation vector Sµ(τ ): dxσ dτ = 0 . ρσ (2) D2Sµ Dτ 2 = Rµ αβγX αX βSγ , (3) where, α is vector field and Definition 2). Thus, it encodes the information about tidal forces of gravitation. Dτ = αα denotes the directional covariant derivative (see 2.1.1 ANALYTICAL SOLUTIONS Exact solutions of the EFEs are metric tensor fields gαβ that satisfy Equation (1). Many exact solutions are known, which can be classified into exterior (vacuum) solutions and interior solutions (Misner et al., 2017). While there exist several geometries that satisfy the EFEs, we shall focus on three 6 prominent vacuum solutions: the Schwarzschild metric, the Kerr metric, and gravitational waves. These geometries not only have high theoretical and historical relevance, but are also of great interest in computational black hole astrophysics and gravitational wave and multi-messenger astronomy. Appendix discusses these solutions in more detail, including other prominent coordinates, as well as real and fake (coordinate) singularities. From here on, we work in naturalized units by setting = = 1. Schwarzschild metric It is the simplest non-trivial solution to the EFEs and describes static spherically symmetric geometry around spherical non-rotating gravitating bodies, such as stars or black-holes. Although simple, the Schwarzschild metric predicts many phenomena beyond Newtonian gravity, most notably the precession of elliptical orbits and the bending of light rays. Both of these predictions have been experimentally verified in the Solar system, using the motion of Mercury perihelion and in the Eddington experiment during the 1919 Solar eclipse, respectively. The metric is typically written in spherical polar coordinates (t, r, θ, ϕ) where R, R+, θ (0, π), and ϕ [0, 2π): ds2 = (cid:18) 1 2M (cid:19) dt2 + (cid:18) 1 (cid:19)1 2M dr2 + r2(cid:0)dθ2 + sin2θdϕ2(cid:1) . (4) Kerr metric generalizes the Schwarzschild solution to rotating bodies with the angular momentum or, equivalently, the rotation parameter = . The solution forms rotating, stationary (but not static) geometry, which is oblate around the rotation axis that breaks spherical symmetry. This geometry again permits new phenomena, notably the geodetic effect and frame dragging, both of which have been experimentally verified in the Earths orbit by the Gravity Probe B. The metric can be described in the corresponding oblate spheroidal coordinates also known as the Boyer-Lidquist (BL) coordinates (t, r, ϑ, ϕ) (see Equation (93)) (Boyer & Lindquist, 1967): ds2 = (cid:16) 1 2M Σ (cid:17) dt2 4M arsin2ϑ Σ dtdϕ+ Σ dr2+Σdϑ2+ (cid:18) r2+a2+ (cid:19) 2M ra2sin2ϑ Σ sin2ϑdϕ2. Linearized gravity models the metric as tiny fluctuations or perturbations hαβ, hαβ 1 of the flat background metric ηαβ: gαβ ηαβ + hαβ + O(hαβ)2 . Famously, this model can describe GW propagation using periodic time-dependent perturbation, which has served as the theoretical basis for the Nobel-prize winning detection of GWs generated by binary black hole mergers (Abbott et al., 2016c). As detailed in Appendix B.3, the choice of (ϵ) certain gauge essentially transforms the vacuum EFEs into the wave equation αβ = 0 where ηαβαβ is the dAlembert or wave operator and 2 h(ϵ)ηαβ. This equation admits family of GW solutions: we will use the plane wave propagating in the z-direction with the angular frequency ω expressed in the Cartesian coordinates: αβ = h(ϵ) αβ 1 (ϵ) hTT αβ = 0 0 0 0 0 h+ 0 0 h+ 0 0 0 0 cos (cid:0)ω(t z)(cid:1) . (5) Here, h+ and are the amplitudes of the + (plus) polarization and (cross) polarization 2.2 NUMERICAL RELATIVITY In most cases, it is impossible to solve the EFEs analytically, and one has to resort to numerical methods. NR concerns with solving such problems by evolving the spacetime metric, cast as Cauchy initial value problem, by foliating the four-dimensional manifold into sequence of spatial hypersurfaces (3+1 decomposition). The seminal ArnowittDeserMisner (ADM) formalism (Arnowitt et al., 1959) provides Hamiltonian decomposition of the EFEs into constraint (elliptic-PDEs) and evolution equations (hyperbolic-PDEs) for the spatial metric γij and extrinsic curvature Kij, governed by lapse function α and shift vector βi. While ADM forms the foundation, its lack of numerical stability in strong-field regimes has led to the development of more robust schemes such as the BaumgarteShapiroShibataNakamura (BSSN) formalism (Shibata & Nakamura, 1995). BSSN introduces conformal rescalings and auxiliary variables to stabilize the evolution by decoupling constraint-violating modes and controlling the growth of numerical errors. Alternatively, generalized harmonic formulations (Lindblom et al., 2006) recast EFEs as manifestly hyperbolic wave equations for the metric components, introducing gauge source functions Hα to maintain coordinate conditions dynamically. These hyperbolic systems are particularly well-suited for modern numerical methods and have enabled stable evolutions of binary black hole mergers and gravitational waveforms (see Einstein Toolkit (Rizzo et al., 2025), NRPy+ (Ruchlin et al., 2018), SpeCTRE (Deppe et al., 2025) for open-source NR toolkits). Higher-order methods. Finite-difference (FD) methods have long served as foundational tool in NR, where high-order stencils (see Appendix C) discretize both spatial and temporal domains. An n-th order FD stencil introduces truncation error of O(hn), where is the grid spacing. Common choices, such as fourth-or sixth-order schemes, enhance accuracy but increase communication overhead due to their wider stencil footprint. To suppress spurious high-frequency artifacts, FD schemes typically employ filters, such as KreissOliger dissipation (Kreiss, 1994). Furthermore, while effective for smooth fields, FD methods are susceptible to Gibbs-like oscillations near discontinuities (e.g., neutron star surfaces or shocks), which can hinder convergence. State-of-the-art NR workflows increasingly rely on (pseudo-)spectral methods (Scheel et al., 2025; Boyle et al., 2025; Haas et al., 2016), which represent fields globally via expansion coefficients in bases such as Fourier or Chebyshev polynomials (Fornberg, 1996). Spectral methods are merely limited by double precision arithmetic, offering orders-of-magnitude improvements in efficiency and accuracy; for example, precessing binary black hole inspiral-merger-ringdown simulations are 10005000 times more efficient as compared to FD approaches at comparable accuracy (Rashti et al., 2025). Consequently, even modern GPU-accelerated FD codes remain uncompetitive relative to CPU-based spectral codes. 2.3 NEURAL FIELDS neural field (NeF), also known as implicit neural representation or coordinate-based NN, is neural network that models continuous signal or field over some domain, e.g., material density on 3D domain. Typically, NeFs are multilayer-perceptrons, which are highly flexible function approximators (Hornik et al., 1990) and allow for efficient access to computation of spatial and temporal derivatives via AD. Unlike traditional discrete representations like voxels or meshes, NeFs offer continuous signal approximations that can be queried at arbitrary resolutions, even when trained on unstructured or sparse data. NeFs are memory-efficient and capable of capturing high-fidelity detail across complex domains. These properties have motivated their primary development and adoption in computer vision domains for representation, generation, and inversion tasks. Examples include scene reconstruction and rendering (Mildenhall et al., 2021; Müller et al., 2022), shape generation (Park et al., 2019; Chen & Zhang, 2019; Mescheder et al., 2019), and image, video, and audio compression (Karras et al., 2021; Sitzmann et al., 2020). For comprehensive surveys, see Xie et al. (2021); Essakine et al. (2024); Papa et al. (2024). Beyond vision, NeFs are also applied in scientific domains, such as molecular dynamics (Behler & Parrinello, 2007) inverse problems in acoustics (Reed et al., 2021), and continuous PDE dynamics forecasting (Yin et al., 2023). Their integration with physics-informed loss functions makes them well-suited for modeling spatiotemporal dynamics governed by PDEs. In such settings, NeFs act as solvers, more commonly known as physics-informed neural networks (Raissi et al., 2019; Karniadakis et al., 2021; Wang et al., 2023). NeFs have also found applications in differential geometry to model continuous implicit surfaces, replacing discrete meshes and leveraging AD to compute normals and curvatures (Yang et al., 2021; Novello et al., 2022; Berzins et al., 2025). NeFs have also been applied to gravitational tasks. Izzo & Gómez (2022) use NeFs for gravity inversion task of irregular small bodies, such as asteroids and comets. Assuming the Newtonian 8 gravity model, they train neural density field to fit the acceleration field measurements in the orbit of the body, ultimately enabling the reconstruction of the bodys shape and interior. More recently, NeFs have been applied to geophysical modeling of planetary-scale fields. For instance, Smith et al. (2025) utilized NeFs to map the Earths gravitational potential field ϕ(x, y, z) for geophysics applications. Such models can interpolate scattered geophysical measurements and allow spatial gradients (corresponding to gravitational acceleration, i.e., xϕ(x)) to be queried at arbitrary coordinates. In general relativistic physics, PINNs have been used for solving the Teukolsky equation (Luna et al., 2023) and parameter estimation by learning the metric components of analytic EFEs (Li et al., 2023). Recently, semi-supervised machine learning approaches have been deployed to discover new Einstein metrics (metrics that satisfy Rαβ = λgαβ λ R) (Hirst et al., 2025)."
        },
        {
            "title": "3 EINSTEIN FIELDS",
            "content": "NR simulations necessitate substantial computational resources, encompassing not only fine-grained explicit tensor field representations and high-precision solvers for the EFEs, but also dynamic, hierarchical mesh structures. These render NR workflows heavily dependent on large-scale distributed computing infrastructures. Consider the four-dimensional spacetime, Hausdorff manifold equipped with Lorentzian metric (M , g), corresponding to an exact or numerical solution of the EFEs2: Gαβ = 8πG Tαβ. An EinField models the 10 independent components of the metric tensor field as compact NeF, ultimately mapping the spacetime coordinates (x0, x1, x2, x3) to the symmetric rank (0, 2) metric tensor field (see Definition 8): ˆg : gαβ(x) Sym2(T ) , (6) where, ˆg is the implicit representation of the metric, with NN parameters θ. Methodologically, this memory-efficient, continuous, and differentiable toolkit generalizes to an arbitrary rank (r, s) tensor field α1...αr (xµ). Thus, EinFields posit neural alternative to address one or more of the β1...βs challenges mentioned above. In this section, we analyze the different aspects and associated features of our methodology applied to computational GR tasks. Thus, our framework should be considered as an instance of Neural Tensor Fields. 3.1 DISTORTION We adopt simple decomposition strategy wherein the full spacetime metric gαβ is split into the flat Minkowski background ηαβ and the distortion αβ = gαβ ηαβ . (7) This decomposition is purely algebraic and should not be conflated with KerrSchild-type constructions, where the spacetime metric is expressed as non-trivial exact solution involving linear superposition of the flat background and squared null vector field ℓ, scaled by scalar function. One can view the decomposition of Equation (7) as form of bias mitigation during preprocessing, ensuring that features contributing to the learning task are on comparable scale. For example, the Schwarzschild metric expressed in spherical coordinates exhibits distinct scaling behaviors across components, with gtt 1/r and gθθ r2. This distortion training trick facilitates more efficient learning process by excluding trivial contributions of the flat background from the learning target. This is because the distortion components encapsulate the essential non-trivial geometric features of the spacetime, including curvature induced by mass-energy distributions, rotational effects, and structures such as event horizons. From the NeFs training perspective, this avoids redundancy in learning the signal of the well-understood and coordinate-dependent features of flat spacetime, thereby allowing the model to focus its representational capacity on the physically meaningful deviations encoded in the distortions. Coincidentally, for our analytical use cases, it is easily seen that the trivial Minkowski parts (see e.g., Equations ((107), (109), (110)) contain features with larger numerical scales dominating the learning process. Thus, this eliminates scaling-induced bias, improving convergence, while concentrating on the dynamic and geometric content of the metric during the training process. 2From now on, we omit the cosmological constant term Λgαβ. 9 The corresponding Minkowski metric is inserted back as part of the post-processing step, such that further analysis is always performed on the full metric."
        },
        {
            "title": "3.2 COMPRESSION AND CONTINUITY",
            "content": "We deploy an MLP ˆgθ with parameters θ, denoted ˆg for simplicity, to fit on the ground-truth signal, i.e., the metric tensor field g. This enables directly condensing the entire geometric information into storage-cheap NN weights, while allowing continuous access (different from the training grid points) to high-resolution metric information, devoid of meshes."
        },
        {
            "title": "3.3 RETRIEVING PHYSICS VIA NEURAL TENSOR FIELD DERIVATIVES",
            "content": "The first and second derivatives of the metric tensor play significant role in GR. Instead of implementing higher-order finite differencing methods (detailed in Appendix C), one can leverage the infinite differentiability of NeFs and access higher-order tensor derivatives via AD, i.e., (j) ˆg(xµ). Sobolev training (Czarnecki et al., 2017) refers to class of learning paradigms where NNs are trained not only to match target function values but also additionally its derivatives. Formally, given target function : R, and NN approximation ˆfθ, Sobolev training minimizes joint loss involving the functional and its derivatives: LSob(θ) = Ex λ0f (x) ˆfθ(x)2 + λj (cid:13) (cid:13) 2 (cid:13)D(j)f (x) D(j) ˆfθ(x) (cid:13) (cid:13) (cid:13) , (cid:88) j=1 (8) where D(j) denotes the jth derivative operator, which in our case could be the partial derivatives or covariant derivatives j, and λj are weighting coefficients. This loss promotes alignment not only in function space but also in the Sobolev space N,2(X ), which encodes both value and derivative information. Sobolev training enhances generalization, stability, and accuracy of NeF derivatives (Chetan et al., 2024). As we demonstrate in the next Section, Sobolev training can significantly improve EinFields, rectifying irregularities in the metric Jacobian µgαβ, matrix with 64 = 16 4 components, and the metric Hessian µνgαβ, matrix with 256 = 16 4 4 components. Since these indirectly describe the neighborhood structure and spacetime curvature, respectively, Sobolev training consequently improves the reconstruction accuracy of physical quantities of interest, such as Christoffel symbols, covariant derivatives, geodesics, and curvature invariants. Algorithm 1 describes the training strategy to improve tensor field derivatives using the loss in Equation (8). We use at most the Hessian (N = 2), for which our Sobolev loss on the spacetime metric reads: Lg Sob(θ) = Ex (cid:20) λ0gαβ(x) ˆgαβ(x)2 + 2 (cid:88) λj j= (cid:13) (cid:13)(j) (cid:13) gαβ(x) (j) ˆgαβ(x) 2 (cid:21) (cid:13) (cid:13) (cid:13) . (9) g, (j) ˆg(cid:1) are short-hand for Ex µ and (2) (cid:13) (cid:13)(j) (cid:13) We use the succinct notation (1) L(cid:0)(j) gαβ(x) (j) . Additionally, it is possible to incorporate soft constraints C, born out of physics considerations, akin to PINN loss. Examples include conservation laws, e.g., of the energy for matter distributions βT αβ = 0 (Section 2.1), vacuum Ricci tensor ˆRαβ = 0 for NeF parameterized Ricci tensor ˆRαβ, as well as specific symmetries associated with the system. νµ. The expected losses in Algorithm ˆgαβ(x) (cid:13) 2 (cid:13) (cid:13) 10 Algorithm 1: EinField training 1: Input: Training dataset {(cid:0)xi, g(xi), (1) g(xi), (2) g(xi)(cid:1)}m i=1, number of epochs Nepochs, learning rate η, optimizer O, Sobolev order {0, 1, 2} 2: Initialize neural field parameters θ on device (e.g., GPU) in single (FLOAT32) precision 3: for epoch = 1 to Nepochs do 4: 5: 6: 7: 8: for each mini-batch (xbatch, gbatch, (1) Move (xbatch, gbatch) to device ˆgbatch EinFields(xbatch; θ) loss L(gbatch, ˆgbatch) if 1 then {Jacobian supervision} gbatch) in dataset do gbatch, (2) 9: 10: 11: 12: 13: Compute (1) loss loss + λ1 L((1) ˆgbatch through AD gbatch, (1) ˆgbatch) end if if 2 then {Hessian supervision} Compute (2) loss loss + λ2 L((2) ˆgbatch through AD gbatch, (2) ˆgbatch) end if Compute gradients: θ θ loss Update parameters: θ O(θ, θ, η) Optionally: synchronize gradients across devices if using distributed training end for Optionally: evaluate on validation set, log MAE and memory usage for monitoring Optionally: checkpoint θ for fault tolerance and reproducibility 14: 15: 16: 17: 18: 19: 20: 21: 22: end for 23: return optimized parameters θ The impact of the modified training loss is particularly evident when accurately querying higher-order differential geometric quantities, which include the metric Jacobian, Christoffel symbols, metric Hessian, Riemann tensor, and curvature invariants on an arbitrary set of validation points. We evaluate the improvement in reconstruction quality and fidelity for these quantities on the Kerr metric in Cartesian KS coordinates, which are free of coordinate singularities, using 2D tomographic slices (see Section D.3 for details). Comparative results with and without Sobolev training are explicitly reported in Figures 2019. Reconstructing dynamics. Reconstructing free-falling trajectories of bodies and light rays necessitates solving the geodesic equation (see Equation (71)). This requires the accurate computation of Christoffel symbols Γ = Γ(g, g). Moreover, Christoffel symbols are included in the covariant derivative := + Γ, playing pivotal role in parallel transportation on (pseudo-) Riemannian manifolds (see Appendix A.3.3). Following our AD-based workflow depicted in Figure 3, EinFields (with Jacobian supervision) enable accurate AD-based reconstruction of the Christoffel symbols via Γ ˆΓ = ˆΓ(ˆg, ˆg), and subsequently the covariant derivative operator := + ˆΓ. Consequently, we can model trajectories governed by the geodesic equation and also query the Riemann tensor (Γ, see Appendix A.3.5.1 for details) on arbitrary points. Characterizing intrinsic geometry. Beyond accurately capturing the spacetime dynamics, EinFields must also faithfully reproduce the intrinsic geometry of the underlying manifold. This intrinsic structure is encoded in key geometric quantities such as the Riemann curvature tensor, the Ricci tensor Rαβ, the Ricci scalar R, and the Kretschmann invariant (cf. Figure 3). Our framework, trained with explicit supervision on Jacobian and Hessian fields, successfully meets these stringent geometric requirements. In particular, the learned fields demonstrate close quantitative agreement with their analytical counterparts across the computational domain. This fidelity holds especially well in regions sufficiently distant from zones of extreme curvature or curvature singularities, where numerical learning and representation are inherently more challenging. For detailed analysis of curvature tensors and invariants computed by our model, see Appendix A.3.5.2. 11 Covariant derivative Jacfwd (cid:76) Γ Bianchi II (Jacfwd (cid:76) Γ)Riem = 0 metric Jacfwd metric Jac Jac(g) Einsum Christoffel symbols Γ Jacfwd (cid:76) Γ Riemann tensor Riem Einsum Ricci tensor Ric Einsum Ricci scalar Lie derivative Jacfwd (cid:76) Γ Weyltensor Figure 3: Automatic differentiation-enabled directed-acyclic graph (DAG) for computational differential geometry analogue of Figure 2. The diagram illustrates the computational workflow for constructing geometric quantities from metric tensor g. The metrics Jacobian Jac(g) is obtained via forward-mode automatic differentiation (Jacfwd) and contracted using Einstein summation (Einsum) to compute the Christoffel symbols Γ. Covariant derivatives and Lie derivatives utilize Jacfwd (cid:76) Γ emerging as derivative operators acting on tensors. The Riemann tensor Riem is built through further application of Jacfwd and Γ, followed by contraction to yield the Ricci tensor Ric and scalar curvature R. Branches compute the Weyl tensor and enforce Bianchi identities (cid:0)Jacfwd (cid:76) Γ(cid:1)Riem = 0. This framework leverages automatic differentiation and tensor algebra to enable scalable symbolic-numeric computations in geometric deep learning and physics-informed models."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EVALUATION CRITERIA We flatten the ground truth tensor at point with its components indexed by be denoted by fk(p) Rn, with 1 and the corresponding EinFields parametrized tensors are denoted by ˆfk(p). The dimensionality depends on the tensor under consideration. For instance, for symmetric metric tensor = 10, corresponding to its independent components, while for the Riemann curvature tensor, = 256 when considering all components explicitly, or = 20 when accounting only for the independent components under the symmetries inherent to the tensor, respectively. We evaluated these quantities over set of 125, 000 validation collocation points = {pi}1im and use standard error criteria in discretized form, which includes double sums: one over the total number of tensor components {fk}1kn, while the other for the total number of collocation points {pi}1im: Mean-absolute error (MAE) = 1 mn (cid:88) (cid:88) i=1 k=1 ˆfk(pi) fk(pi) (10a) (cid:115) (cid:80)m i=1 (cid:80)n (cid:80)m k=1 ˆfk(pi) fk(pi)2 k=1 fk(pi)2 (cid:80)n i=1 Relative ℓ2 error (Rel. ℓ2) = 12 . (10b) These are applied to the metric tenors and their derived quantities, illustrated in Figures 2 and 3. Recall that the tensor components are coordinate-dependent (and even more so, the metric Jacobian, metric Hessian, and Christoffel symbols are not even tensors), and, hence, these errors lack an immediate physical meaning. This is improved with the consideration of scalar quantities such as the Ricci scalar, Kretschmann invariants, and Weyl scalars, which by definition are coordinate-independent quantities. The above error criteria are an aggregation of point-wise, i.e., local quantities. We additionally assess the quality of free-falling trajectories obtained by evolving the geodesic equation (Equation 2) from the same initial condition on the exact and approximate metrics. Even small errors quickly accumulate during the evolution of the trajectory, providing sensitive assessment of the metric quality. In addition, these trajectories provide strong physical intuition, e.g., the number of stable orbits."
        },
        {
            "title": "4.2 FLOATING POINT PRECISION",
            "content": "NR simulations are inherently high-precision endeavors, with the accurate modeling of complex gravitational phenomena critically reliant on high-fidelity numerical computations. In contrast to traditional machine learning domains, such as large language models (LLMs), where reducedprecision arithmetic (FLOAT16 or BFLOAT16) yields strong results in both training accuracy and memory efficiency (Dean et al., 2012), this paradigm does not extend to NR workflows, where floating-point precision is dominant factor influencing the fidelity of the results. While single-precision (FLOAT32) arithmetic is sufficient for training EinFields in most experiments and downstream tasks presented, geodesic simulations (see Figures 5 and 7) indicate the need for FLOAT64 precision results in MAE and relative ℓ2 error for the reconstructed metric and its derivatives. Only FLOAT64 ensures the mitigation of error accumulation during temporal rollout, preserving the accuracy necessary for reliable scientific inference in gravitational physics. 4.3 DATA Our use cases are exact analytic solutions to the EFEs, i.e., the set of metrics gαβ that satisfy the Equation (1). These solutions describe the exterior (vacuum) solutions around massive gravitating objects. For our main set of experiments, we fit NeF against the analytic solutions introduced in Section 2.1.1, each having different features and spatio-temporal symmetries: Schwarzschild metric in spherical coordinates (Equation (113)), Kerr metric in Boyer-Lindquist and Kerr-Schild coordinates (Equations (116, 96)), gravitational waves metric (TT gauge) in Cartesian coordinates (Equation (100)). For each, we compute the distortion after subtracting the flat background metric using the corresponding equations summarized in the Appendix. Detailed information on data specifications is provided in Table 1. Additionally, in Section 4.7, we train each geometry in different coordinate systems to investigate how the choice of coordinates impacts NeFs (recall: the physical laws do not depend on the coordinate system)."
        },
        {
            "title": "Schwarzschild",
            "content": "Spherical (t, r, θ, ϕ)"
        },
        {
            "title": "Kerr",
            "content": "BoyerLindquist (t, r, ϑ, ϕ) Kerr-Schild (t, x, y, z)"
        },
        {
            "title": "Linearized gravity",
            "content": "Cartesian (t, x, y, z) = 0 [2.5,150] θ (0, π) ϕ [0, 2π) = 0 [3,14] ϑ (0, π) ϕ [0, 2π) = 0 [-3, 3] [-3, 3] [0.1, 3] [0, 10] [0, 10] [0, 10] [0, 10] 1 128 128 128 1 128 128 128 1 128 128 140 10 10 140 = 1 = 1 [0.628,0.95] = 1 = 0.7 ω = 1 ϵ = 106 Table 1: Training data generation specifications: spacetime metric, coordinate system, domain extent, grid resolution, and physical parameters. 4.4 TRAINING Across the experiments, we consider several NeF models. The most effective and accurate models for our use cases are MLPs with SiLU (i.e., sigmoid-weighted linear units) activation function (Elfwing et al., 2018), which are especially suited when supervising over derivatives. Moreover, since the optimizer is known to impact the speed and convergence of the training strongly and subsequently the accuracy of the model (Wang et al., 2025), we utilize SOAP (Vyas et al., 2025), scalable quasiNewton method with consistently improved gradient alignment between losses in PINNs. However, since our training characteristics are different from PINNs, we do not expect SOAP to mitigate the problem of gradient conflict (see Appendix D.1) completely, but rather to leverage the second-order approximation combined with its advanced preconditioning and computational efficiency. That being said, using SOAP increases the optimization efficiency and reduces the training loss by at least two orders of magnitude compared to ADAM (Kingma & Ba, 2015). To mitigate gradient imbalances during Sobolev supervision, which is especially more pronounced in lowor high-curvature regions where the metric or its derivatives dominate, respectively, we adopt gradient normalization weighting scheme (GradNorm (Chen et al., 2018)), forcing all gradients to unit norm. This is slightly different weighting compared to Wang et al. (2020), in the sense that we do not multiply by the sum of the gradient norms, resulting, however, in better results for our use case. The investigated MLP widths and depths range from 64 3 to 512 7, with less than 1.9 106 parameters ( 7MiBs). We use the cosine annealing learning rate schedule, with the initial learning rate set to η = 1E-2. The number of training epochs is chosen between [100, 200], and the number of batches per epoch varies from 20 up to 300. The 100 epochs runs with 100 batches had runtime 20it/s for metric alone to 5it/s including Hessian supervision. In wallclock time, this ranges between 64 seconds for the former, and 2140 seconds for the latter. For more details on the training hyperparameters, see Table 10. 4.5 ACCURACY AND STORAGE EFFICIENCY OF EINFIELDS In this work, we adopt finite difference (FD) methods as our baseline, recognizing their limitations relative to spectral approaches as detailed in Section 2.2. The integration of spectral techniques will be the focus of future work. 14 In Table 2, we report the representation capacity of EinFields and its Sobolev-enhanced variants (trained according to Pseudocode 1) on the distortion component of the Schwarzschild metric expressed in spherical coordinates (see Equation (113)). We evaluate models using the discrete forms of the mean absolute error (MAE) and relative ℓ2 error as defined in Equations (10), achieving best-case accuracies of approximately 108 (MAE) and 107 (Rel. ℓ2) on validation points. The last two columns of Table 2 report the memory efficiency of NeFs and the associated compression factors relative to explicit grid representations. Notably, while modifying the loss functions does not directly impact storage requirements, the resulting differences in accuracy across schemes lead to variations in effective storage/compression values, as we select the highest achievable accuracy for each method. These trade-offs are visualized in Figure 4a, where the curves are obtained by connecting points to form decreasing trendlines. Representation Rel. ℓ MAE Storage Compression EinFields EinFields (+ Jac) EinFields (+ Jac + Hess) Explicit grid 2.37E-7 1.51E-7 1.40E-7 3.93E-7 2.20E-7 6.89E-8 85 KiB 1.1 MiB 202 KiB 343 MiB 4035 311 1698 Table 2: Performance evaluation (measured in Rel. ℓ2 and MAE metrics) and storage efficiency of EinFields parametrized metric tensor fields under different representations (i.e., with and w/o Sobolev trainings). The model with the lowest MAE is selected in each row. (a) Accuracy of EinFields for different NN sizes (measured in KiBs required to store all FLOAT32 parameters) as trendlines for different training schemes. Each metric tensor component has MAE values ranging from 1E-6 to 1E-8. Apart from high accuracy, one additionally acquires 1000 4000 times compression factors in storage memory gain, as detailed in Table 2. (b) Accuracy of EinFields Christoffel symbols derived from the metric tensor, shown as trendlines for different training schemes in FLOAT32 (KiBs). The trendlines show different training schemes (with and without Sobolev training) with MAE values ranging from 1E-4 to 1E-7. We benchmark against fourthorder and sixth-order stencils with truncation errors O(h5) and O(h7), respectively. Our framework outperforms FD stencils by more than an order of magnitude in accuracy. Figure 4: Trendlines of accuracy versus storage memory (KiB) requirement for the metric tensor and Christoffel symbols. For the explicit grid storage this is computed as num of grid collocation points 4, with 4 bytes for single precision (FLOAT32). For the NeFs, this corresponds to the storage memory of the compact implicit NN weights. 15 Accuracy and storage efficiency of tensor differentiation. High-fidelity reconstruction of higherrank tensors that are obtained via differentiation of neural tensor fields is essential for accurately recovering the underlying physics, including geodesics, tidal forces, and related geometric quantities. Here, we evaluate the performance of our framework by reporting accuracy versus memory trendlines for the metric tensor (Figure 4a) and the Christoffel symbols (Figure 4b, computed via AD). We apply the same setting as used for the results in Table 2. For the Christoffel symbols, the trendlines for the higher-order finite-differencing methodologies (4-th and 6-th order stencils) are compared against EinFields and Sobolev training variants. While EinFields exhibits decreasing MAE when plotted against increasing network sizes (# of floats), the FD baselines face notable limitations not only coming from truncation errors and numerical instabilities (increasing error for smaller values), but also accuracy and memory efficiency-related bottlenecks. Furthermore, we report the performance of AD-based Jacobians and covariant derivative operators, through which we obtain Christoffel symbols (see Equation (64)), the Riemann curvature tensor (see Equations ((72), (73)), the contracted curvature tensors (Weyl tensor, Ricci tensor, Ricci scalar), and the curvature invariants (see Section A.3.5.2). We compare against higher-order FD-stencils. Table 3 shows that EinFields reconstructed differential geometric quantities are up to 10 105 times more accurate than FD stencils for these differential geometric quantities. Geometric quantity Storage [GiB] MAE Full Sym. GT (FD) EinFields (AD) GT (AD) 26 Christoffel symbol 103 Christoffel Jacobian 103 Metric Lie derivative 103 Riemann tensor 103 Weyl tensor 6.5 Ricci tensor Ricci scalar 0.4 Kretschmann invariant 0.4 16 64 64 8.0 4.0 4.0 0.4 0.4 5.37E-6 6.24E-3 6.24E-3 1.78E-2 1.72E-2 4.81E-2 5.35E-2 1.33E-2 4.87E-7 1.63E-6 1.63E-6 2.53E-7 5.71E-7 1.02E-6 5.66E-5 1.71E5.83E-9 1.71E-8 1.71E-8 2.86E-8 5.89E-8 9.02E-8 1.31E-8 3.32E-8 Table 3: Performance evaluation of EinFields reconstructed differential geometric quantities obtained for the Schwarzschild geometry in spherical coordinates. Column 1 reports values on spherical grid with spacing = 0.01. Columns 2 and 3 report memory usage when storing full and symmetry-reduced components, respectively. Column 4 shows the MAE from EinFields (AD) measured against analytical solutions. Bold entries indicate significant accuracy gains over naive finite-difference baselines applied on the ground-truth (GT) metric (GT (FD), = 0.01). The last column (GT (AD)) applies AD to the analytic solution, which almost coincides with the analytic expressions for these differential geometric quantities. 4.6 RECONSTRUCTING GENERAL RELATIVISTIC DYNAMICS AND CURVATURE SCALARS We use synthetic data generated from analytical solutions to validate and characterize EinFields. We primarily focus on two interesting aspects of general relativity: (i) general relativistic dynamics, particularly geodesic motion around massive gravitating objects (see Section 2.1) and (ii) global curvature structures encoded in tensorial invariants. EinField-based geodesics. To compute geodesic motion, we numerically integrate the trajectories using fifth-order explicit singly diagonally implicit Runge-Kutta (ESDIRK) solver. Specifically, we evolve Equation (2) with respect to the affine parameter τ (proper time)3,generating ground-truth geodesics from the analytic Christoffel symbols for the Schwarzschild and Kerr spacetimes. These are then compared against the rollouts obtained using the EinField-reconstructed Christoffel symbols. To accurately retrace geodesic orbits, it is essential to incorporate Jacobian supervision within the Sobolev training framework. In contrast, additional Hessian supervision results in only marginal 3Not to be confused with the coordinate time t. 16 improvements for geodesic simulations and is not required in practice. Following Section 4.2, all geodesic solvers are executed in double precision to ensure numerical stability and high-fidelity trajectory reconstruction. Following the strategy presented in Section 3.3, we leverage EinFields ability to yield highprecision derivatives of the spacetime metric, which includes Christoffel symbols, Riemann curvature tensors, and scalar invariants. In this section, we demonstrate how to accurately model particle trajectories derived from geodesic integration with our implicit parameterizations."
        },
        {
            "title": "4.6.1 SCHWARZSCHILD METRIC",
            "content": "Geodesics in the Schwarzschild spacetime are of fundamental interest, as they underlie phenomena such as gravitational lensing and the perihelion precession of Mercury, as well as the motion of planets in the solar system more generally."
        },
        {
            "title": "The initial conditions of the trajectories chosen in the experiments are fully specified by the initial\nposition",
            "content": "(t, r, θ, ϕ)(t = 0) = (cid:0)0, a0rs, π/2, 0(cid:1) and the initial four-velocity (vt, vr, vθ, vϕ)(t = 0) = (cid:0) 1 (cid:112)(1 rs/r0)(1 v2 0) , 0, 0, v0 cos ϕ0 0(1 v2 0) (cid:112)r2 (cid:1) , (11) (12) (cid:112)1/(r0 rs) and a0, b0 can be chosen freely to select the desired orbit in the Where v0 = b0 θ = π/2 plane. The geodesics in Figure 5 demonstrate good qualitative agreement over several orbits. The error is quantified and discussed further in Section 4.8. (a) Perihilion precession orbits. (b) Circular orbit for = 3.85rs. (c) Eccentric orbits. Figure 5: Geodesics in Schwarzschild spacetime simulated in spherical coordinates (Equation (85)). The filled black circle indicates rs = 2M . The purple dashed line marks the innermost stable circular orbit (ISCO) at = 3rs. Green solid lines represent the ground-truth geodesics, while the red dotted lines represent EinFields reconstructed ones. We find very close agreement between the orbits up to chosen time of running the geodesics. Being able to compute geodesics is sufficient to perform rendering. We use the Schwarzschild EinFields metric to render black-hole on celestial background. This requires propagating geodesics from the camera observer via the spacetime terminating at the distant background. The resulting image in Figure 6 provides visual evidence for the global consistency and quality of the metric and the derived Christoffel symbols. 17 Figure 6: Render of Schwarzschild black-hole in front of celestial background constructed from the EinField metric. 4.6.2 KERR METRIC Geodesics in Kerr spacetime around rotating body (see details in Appendix B.2) play central role in several key astrophysical observations and experimental tests of GR. Notably, photon geodesics determine the black hole shadow images captured by the Event Horizon Telescope (Fuerst, S. V. & Wu, K., 2004), and frame-dragging (Lense-Thirring) effects (Misner et al., 2017) are hallmark of the Kerr geometry. These have been measured experimentally by the Gravity Probe mission (Everitt et al., 2011) and recently via radio pulses arriving from pulsars (Krishnan et al., 2020). We consider three different cases, namely, Zackiger orbits (retrograde geodesics stable geodesics with larger radii), prograde orbits (stable geodesics with smaller radii), and arbitrary eccentric orbits, which depend on the initial conditions, including choice of energy and angular momentum Lz of the test particle. The geodesics in Figure 7 demonstrate good qualitative agreement over several orbits. The error is quantified and discussed further in Section 4.8. (a) Retrograde orbit for = 0.95. (b) Prograde orbit for = 0.90. (c) Eccentric orbit for = 0.628. Figure 7: Geodesics in Kerr spacetime simulated in BoyerLindquist coordinates (Equation (95)). Distinct regions of the geometry are indicated in solid colors. Green solid lines represent ground-truth geodesics, while the red dotted lines represent our NeFs reconstructed orbits. Kretschmann invariant. The Kretschmann invariant (scalar), = Rαβγδ(xµ)Rαβγδ(xµ), is key curvature invariant distinguishing true (curvature) singularities from coordinate (apparent) singularities (see Appendix A.3.5.2). For the Kerr geometry, the rotation parameter induces ring singularity at radius on the equatorial plane θ = π/2, where the curvature diverges (see 18 Equation (99) and Section B.2.2). Accurately capturing this geometric structure requires isolating true singularities from coordinate artifacts, which can otherwise lead to incorrect classification of singularities. We perform training in Cartesian KS coordinates (see Equation (96)) to eliminate coordinate singularities that would otherwise impede convergence. We first train EinFields (+Jac + Hess) on Cartesian KS coordinates, subsequently constructing the Riemann tensor (see Section A.3.5.1) via successive automatic differentiation steps and raising indices using the parametrized metric ˆg (see Equation (99)). The NeF reconstructed ˆK captures the ring singularity structure and agrees well with the analytical solution, as shown in Figures 8a and 8b. However, the reconstruction remains sensitive to floating-point errors and requires high NeF accuracy for stability (see Limitations, Section 5). (a) Analytic form (b) EinFields reconstructed ˆK (c) Absolute error ˆK Figure 8: The Kretschmann scalar of the Kerr metric computed in Cartesian Kerr-Schild form (Equation (96)) in the x-y plane for = 0.3. 4.6.3 LINEARIZED GRAVITY Linearized gravity models the solution of the EFEs via periodic perturbations on fixed background metric. These linearized solutions are highly relevant in numerical relativity, as they describe the groundbreaking, experimentally verified discovery of gravitational waves generated by binary black hole mergers (Abbott et al., 2016c). The metric tensor can be written as gαβ ηαβ + hαβ + O(hαβ)2 , where hαβ 1 is the perturbation term. As detailed in Section B.3, plane gravitational wave propagating in the z-direction with angular frequency ω can be described in the tranverse-traceless (TT) gauge as (13) hTT αβ = 0 0 0 0 h+ 0 0 h+ 0 0 0 0 0 cos (cid:0)ω(t z)(cid:1) . (14) Here, h+ and are the amplitudes of the + (plus) polarization and (cross) polarization. Validation problems for GW metric and derivatives quality. Compared to Schwarzschild and Kerr metrics, key distinction of the linearized gravity setting describing gravitational waves is its time dependence (see Equation (100)). Although it does not depend on and y, the temporal dependence motivates us to consider our model trained on full spacetime grid of size Nt Nx Ny Nz. Distortion of ring of test-particles. When the described gravitational wave interacts with ring of freely falling test particles initially at rest in the x-y plane, it induces periodic deformations of the ring. For purely + polarized wave, the resulting motion causes the ring to stretch and squeeze along the xand y-axes, leading to characteristic plus deformation pattern. The motion of the test particles under the influence of this gravitational wave is obtained by solving the geodesic deviation equation, up to leading order in the strain amplitude h+. As result, the particle trajectories in the TT gauge are 19 (cid:18) x(t) = 1 + 1 2 h+ cos (cid:0)ω(t z)(cid:1) (cid:19) (cid:18) x(0) , y(t) = 1 h+ cos (cid:0)ω(t z)(cid:1) (cid:19) 1 2 y(0) . (15) Here, x(0) and y(0) denote the initial coordinates of test particle, and the time-dependent perturbations reflect the tidal nature of gravitational waves. The cosine dependence captures the periodic stretching and squeezing of spacetime caused by the wave as it traverses the particle ring. Figure 9 and Table 6 show how the famous ring oscillation experiment can be reproduced with EinFields. This is done by parametrizing the perturbation hTT αβ and captures the famous stretching and squeezing effect. Figure 9: Spatial deformations (stretching and squeezing) of circular ring of test particles due to + polarization. The NeF-parametrized perturbation terms h+ cos (cid:0)ω(t z)(cid:1) and cos (cid:0)ω(t z)(cid:1) show excellent agreement with the analytic result in the linearized gravity use case. See Table 6 for quantitative evaluation. Weyl scalars of gravitational radiation field. The Weyl scalars are five complex quantities Ψ0, Ψ1, Ψ2, Ψ3, Ψ4 that arise in the NewmanPenrose formalism of GR (Newman & Penrose, 1962). They encode all the independent components of the Weyl tensor Cαβγδ (see Equation (78)), representing the free gravitational field the part of spacetime curvature that can propagate as gravitational waves, distinct from the curvature directly caused by matter. In NR and GW modeling, Ψ4(t) is the primary scalar quantity used to extract observable GW signals from simulations. It is defined as Ψ4 := Cαβγδnαkβnγ kδ (16) with n, being particular choice of NewmanPenrose tetrads and its complex conjugate 4. The central relation in an asymptotically flat spacetime (cf. Boyle et al. (2019) for details) is that Ψ4(t) is equivalent to the second coordinate-time derivative of the strain h(t) = h+(t) + ih(t): Ψ4 h+ + ih . (17) We compute Ψ4 from the NeF-parameterized strain hαβ in two distinct ways: 1. indirectly via the Weyl tensor obtained with the differential-geometric chain (see also Figures 2 and 3): hTT αβ +ηαβ gαβ Γγ αβ Rδ αβγ Cαβγδ Eq. (16) Ψ4; 2. directly via the second time-derivative: hTT αβ Eq. (17) Ψ4. 4Note that the Weyl scalars are not invariant and depend on particular choice of the tetrad fields. 20 Spin-weighted spherical harmonic representation for GW extraction. quantity of central interest in gravitational waveform construction is the mode decomposition of the GW strain into its angular components. The complex strain h(t, r, θ, ϕ) h+(t, r, θ, ϕ) h(t, r, θ, ϕ) can be expanded in terms of spin-weighted spherical harmonics (SWSHs) as h(t, r, θ, ϕ) ="
        },
        {
            "title": "M\nr",
            "content": "(cid:88) ℓ (cid:88) ℓ=2 m=ℓ hℓ,m(t) 2Yℓm(θ, ϕ) , (18) where 2Yℓm(θ, ϕ) are the SWSHs (see Equation (105)) with spin-weight = 2 reflecting the helicity of GWs in the TT gauge (see Equations (103) and (104)). In practice, the dominant contributions to the strain arise from the quadrupole (ℓ = 2, = 2) modes, denoted by h2,2(t), which capture the leading-order gravitational radiation (detailed in Section B.3). Figure 10 Figure 10: The absolute error of the amplitude between the EinFields and analytic values R/M h2,2(t) (see Equation (104)) at fixed radial distance = 1 plotted against t/M . The amplitudes agree to 1E-8, indicating that EinFields can capture the complex strain and subsequently h2,2(t) GW signals. Radiated power of GWs. Another important physical observable for GWs is the radiated power loss given by the famous quadrupole formula (Carroll et al., 2004). The time-averaged power or luminosity radiated by GWs is given by dE dt = (cid:90) r2 32π dΩ hT ij hT ij = 1 4 h2 + + . (19) The particular perturbation metric in the above experiments (see Equation (100)) has equal amplitude = h+ = for both + and polarizations. As consequence, the radiated power loss simplifies to dE dt = ω2A2 . 21 (20) (a) Absolute error in the (z, t) plane, averaged over xy slices. (b) Temporal evolution of the Weyl scalar computed from the analytic and EinFields perturbations (left axis) and their absolute error (right axis) at fixed position. Figure 11: Comparison of the real part of the Weyl scalar ℜ(Ψ4) (Equation (16)) computed from the EinFields and the analytic metric. The errors are on the order of E-10 and E-11, respectively, indicating highly accurate gravitational waveform reconstruction capacities of EinFields. Refer to Table 6 for more details. 4.7 TRAINING ON VARIED COORDINATE CHARTS The freedom to choose coordinate systems for describing spacetime geometries is fundamental aspect of both theoretical GR and computational pipelines in NR. Consequently, it is essential to investigate how coordinate-dependent neural networks, i.e., NeFs, perform across input query coordinates that represent the same spacetime point when parameterized in different coordinate charts. To this end, we train EinFields on range of coordinate systems for Schwarzschild and Kerr spacetimes, enabling systematic examination of the impact of coordinate systems. We compare the performance of EinFields on the Schwarzschild and Kerr metrics, since each of them possesses different families of coordinate charts (spherical-like, Cartesian-like, and lightconelike) representations, as shown in the Table below: Furthermore, detailed analysis and numbers pertaining to validation are reported in Appendix D.4. Metric Schwarzschild Kerr Spherical-like Cartesian-like Lightcone-like Table 4: Col. 1 lists the spacetime metrics (Schwarzschild and Kerr). Cols. 24 indicate the coordinate charts used for NeF training: spherical-like, Cartesian-like, and lightcone-like. For Schwarzschild, these correspond to spherical coordinates (t, r, θ, ϕ), Cartesian Kerr-Schild (KS) coordinates (t, x, y, z), and ingoing Eddington-Finkelstein (EF) coordinates (v, r, θ, ϕ), trained on the metrics described in Equations ((113))((115)) respectively. For Kerr, these correspond to Boyer-Lindquist (BL) coordinates (t, r, ϑ, ϕ), Cartesian KS coordinates (t, x, y, z), and ingoing EF coordinates (v, r, θ, ϕ), trained on the metrics in described in Equations ((116))((117)) respectively. Results reported in Table 9 suggest that the choice of coordinates has strong impact on the metric up to three orders of magnitude. This aspect should be investigated further in future work."
        },
        {
            "title": "4.8 ACCUMULATION OF ROLLOUT ERRORS FOR GEODESICS",
            "content": "Minute floating-point inaccuracies (around 1E-5 to 1E-6) arising from Christoffel symbols retrieved via EinFields autoregressively accumulate when evolving the equations of motion for test particles along geodesics. To quantify the inaccuracies between the ground truth and NeF-evolved geodesics, we compute the deviation between the position vectors r(τ ) R3 as function of the affine parameter (proper time) τ in Cartesian coordinates. Specifically, for the ground truth trajectory, the spatial coordinates corresponding to the position vector are given by r(τ ) = (cid:0)x(τ ), y(τ ), z(τ )(cid:1), while for the NeFevolved trajectory, we denote ˆr(τ ) = (cid:0)ˆx(τ ), ˆy(τ ), ˆz(τ )(cid:1). The deviation at each proper time τ is then computed as the Euclidean norm, δr(τ ) = r(τ ) ˆr(τ )2 = (cid:113)(cid:0)x(τ ) ˆx(τ )(cid:1)2 + (cid:0)y(τ ) ˆy(τ )(cid:1)2 + (cid:0)z(τ ) ˆz(τ )(cid:1) . (21) In practice, the geodesic trajectories are computed in (r, ϑ, ϕ) (e.g., BoyerLindquist) coordinates and subsequently transformed into Cartesian coordinates before evaluating the deviation using the above expression. Given the high sensitivity of time-stepped trajectories to such numerical inaccuracies, we quantify this error accumulation by explicitly presenting the deviation as function of the affine parameter τ , especially for eccentric orbits for both Schwarzschild (see Figure 5c) and Kerr metric for = 0.628 (see Figure 7c). These are reported for the Schwarzschild use case in Figure 12a, and Figure 12b for the Kerr metric use case, respectively. For Schwarzschild, the error accumulates stably, while for Kerr it is erratic. We hypothesize this is likely due to the stable versus chaotic nature of orbits in the respective spacetimes. Eventually, orbits diverge significantly, especially when leaving the NeF training domain. (a) Schwarzschild eccentric orbits. (b) Kerr eccentric orbits for = 0.628. Figure 12: Geodesic rollout deviation δr over proper time τ . The results suggest that incorporating the Hessian supervision into training may introduce noise that can hinder convergence, performing worse than using metric Jacobian supervision or, for that matter, metric alone. For geodesic equations, supervising second derivatives is often unnecessary, and Jacobians alone provide significant improvements in trajectory reconstruction. However, Hessians become essential when computing Riemann tensors and curvature-related quantities, and are required in applications such as numerically solving the geodesic deviation equation (see Equation (3)), which are typically encountered for solving for the test ring oscillation in linearized gravity use cases. 4.9 ABLATION STUDIES 4.9.1 SCHWARZSCHILD We report detailed ablation results relative to our best-performing baseline configuration, systematically examining the effects of matrix representations (full metric instead of distortions), activation functions, optimizers, learning rate schedulers, and Sobolev regularization. All evaluations are conducted on spherical coordinates."
        },
        {
            "title": "Ablation",
            "content": "Baseline: Metric distortion, Jac + Hes, SiLU, SOAP, Cosine LR Metric distortion αβ Metric gαβ Cosine Const. LR SOAP ADAM SiLU WIRE Jac + Hes Jac Jac + Hes - Rel ℓ2 Wallclock time [s] 1.40E-7 2.13E-6 2.37E-5 4.16E-6 4.12E-6 1.51E-7 2.37E1400 1407 1397 1150 3045 509 364 Table 5: Ablation study results for the Schwarzschild metric. Row 1: Best-performing configuration across all grid ranges (Full config). Row 2: Equation (85) in place of Equation (113)). Row 3: Constant learning rate versus cosine annealing schedule. Row 4: ADAM optimizer in place of the default SOAP optimizer. Row 5: Baseline activation (WIRE) compared to our proposed SiLU activation. Rows 67: Omitted Sobolev training (only Jacobian supervision or no higherderivative supervision) on the full Schwarzschild metric. Column 2 reports the Rel ℓ2 error for the aforementioned rows. Column 3 reports the wall-clock time (seconds) required for training over 100 epochs (100 batches per epoch). 4.9.2 LINEARIZED GRAVITY For GW tasks, we also compare against two other relevant models, which utilize periodic activations functions: (i) WIRE architecture (Saragadam et al., 2023) with continuous real Gabor wavelet activation function ψ(x; ζ0, s0) = cos(ζ0x)e(s0x)2 , with ζ0 and s0 controlling the frequency of the wavelet and the spread, respectively (i.e., signal width localized in both the spatial and frequency domains), and (ii) SIREN (Sitzmann et al., 2020) architecture with the periodic activation function sin(ζ0x), and ζ0 being the modulating frequency. The SiLU activation function exhibits lower frequency bias and lacks oscillatory behavior in its higher-order derivatives, which directly feature in the Sobolev norms used during training. In contrast, sinusoidal activations employed in SIREN and WIRE architectures inherently possess oscillatory behavior, with their k-th derivatives scaling as ζ 0 , where ζ0 denotes the base frequency. This scaling amplifies high-frequency components and can lead to instability when minimizing Sobolev norms, as high-frequency errors become disproportionately emphasized. In practice, the smoothness and non-oscillatory nature of SiLU activations result in more stable training and improved generalization under Sobolev training objectives. These trends are quantitatively reflected in our experiments, as reported in Table 6. Model hT αβ (+Jac + Hess) (GradNorm) Ricci scalar Weyl scalar ℜ(Ψ4) Luminosity dE/dt 8.56E-4 SiLU SIREN 3.78E-2 1.68E-2 WIRE 5.90E-13 1.08E-12 1.55E-13 2.53E-5 9.56E-5 1.81E-5 2.71E-4 3.34E-4 3.69E-4 Table 6: Rel. L2 for key quantities in the linearized gravity case with two different NeF architectures: (i) perturbation metric, (ii) perturbation metric trained with Sobolev loss and gradient normalization, (iii) reconstructed Ricci scalar, and (iv) reconstructed real part of the Weyl scalar Ψ4, where Ψ4 h+(t, x). The final column reports the absolute difference in the predicted gravitational radiation energy loss, integrated over the unit sphere."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We have introduced Einstein Fields, short EinFields, which enable an efficient and differentiable modeling of 4D spacetime. EinFields leverage the concept of neural tensor fields together with the simplicity and stability of automatic differentiation, offering scalable, discretization-free, and storage-efficient framework for physics observables encountered in GR. We have tested EinFields across several canonical test beds of GR, observing strong potential in terms of efficiency, storage requirements, accuracy, and faithful modeling of physics. We release an open-source JAX-based library, paving the way for future developments in NR: https://github.com/AndreiB137/EinFields. Limitations. The current main limitation stems from the lossy compression of NeFs. This prevents pushing the MAE of the metric components beyond 1E8, outperforming the FD-methods only in the FLOAT32 setting. Despite using FLOAT64 training, our models cannot fit beyond 1E8 MAE. These errors propagate over to the Christoffel symbol components, affecting the geodesics over long temporal rollouts of the geodesic solvers (see Figure 7c) for diverging orbits) and subsequently to the curvature tensors. Moreover, NeFs suffer near singularities, e.g., 1/r, and are unable to fit near-divergent points on manifold. This and the aforementioned accumulation of floating point errors become even more pronounced for differential geometric quantities such as Kretschmann scalars, especially within spatial domains of high curvatures (see Figure 8c). Furthermore, since NeFs are coordinate-based NNs, training can be influenced by different choices of the basis sets and coordinate systems. Consequently, metric components may have different polynomial orders. For instance, in the spherical coordinate Schwarzschild metric gθθ, gϕϕ r2, whereas, gtt 1/r. Thus, finding NeF architecture, including the choice of activation functions that fit the metric tensor field in different coordinate systems, is challenging task. Future work. The litmus test of EinFields is large NR simulation data, including spirals and merger events of binary black holes or binary neutron stars. Furthermore, our neural implicit frameworks should be tested against adaptive mesh refinement baselines, and recent developments in NR, such as the Discontinuous Galerkin (DG) method and classical discretization-free representations, such as (pseudo) spectral methods that are known to be orders of magnitude more efficient than FD (see Section 2.2). While the primary focus of this work is on the representation, NR can benefit from hybrid framework that combines NeFs (compact smooth representation, automatic differentiation, physics-informed losses) with classical discrete or (pseudo) spectral solvers, such as Generalized Harmonic evolution or BSSN (see Section 2.2) to evolve dynamic spacetimes and solve EFEs. This necessitates the integration of NeFs with classical numerical integrators, such as explicit and implicit Runge-Kutta methods (Álvaro Fernández Corral et al., 2024; Chen et al., 2023). In summary, EinFields have the potential to open several new possibilities in the field of NR, potentially paving the way for next-gen NR simulations. ACKNOWLEDGMENTS We sincerely thank Nils Deppe for valuable feedback on several numerical relativity-related aspects of the paper. The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. We thank the projects FWF AIRI FG 9-N (10.55776/FG9), AI4GreenHeatingGrids (FFG899943), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank NXAI GmbH, Audi AG, Silicon Austria Labs (SAL), Merck Healthcare KGaA, GLS (Univ. Waterloo), TÜV Holding GmbH, Software Competence Center Hagenberg GmbH, dSPACE GmbH, TRUMPF SE + Co. KG. Sandeep S. Cranganore was supported by the FWF Bilateral Artificial Intelligence initiative under Grant Agreement number 10.55776/COE12."
        },
        {
            "title": "REFERENCES",
            "content": "Adrian Abac et al. The science of the einstein telescope, 2025. B. P. Abbott et al. Observation of gravitational waves from binary black hole merger. Phys. Rev. Lett., 116:061102, Feb 2016a. doi: 10.1103/PhysRevLett.116.061102. B. P. Abbott et al. Gw150914: The advanced ligo detectors in the era of first discoveries. Phys. Rev. Lett., 116:131103, Mar 2016b. doi: 10.1103/PhysRevLett.116.131103. B. P. Abbott et al. Properties of the binary black hole merger gw150914. Phys. Rev. Lett., 116:241102, Jun 2016c. doi: 10.1103/PhysRevLett.116.241102. B. P. Abbott et al. Multi-messenger observations of binary neutron star merger*. The Astrophysical Journal Letters, 848(2):L12, oct 2017. doi: 10.3847/2041-8213/aa91c9. Acernese et al. Advanced virgo: second-generation interferometric gravitational wave detector. Classical and Quantum Gravity, 32(2):024001, dec 2014. doi: 10.1088/0264-9381/32/2/024001. Pau Amaro-Seoane et al. Laser interferometer space antenna, 2017. R. Arnowitt, S. Deser, and C. W. Misner. Dynamical structure and definition of energy in general relativity. Phys. Rev., 116:13221330, Dec 1959. doi: 10.1103/PhysRev.116.1322. Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: survey, 2018. Jörg Behler and Michele Parrinello. Generalized neural-network representation of high-dimensional potential-energy surfaces. Phys. Rev. Lett., 98:146401, Apr 2007. doi: 10.1103/PhysRevLett.98. 146401. Marsha Berger and Joseph Oliger. Adaptive mesh refinement for hyperbolic partial differential ISSN 0021-9991. doi: equations. Journal of Computational Physics, 53(3):484512, 1984. https://doi.org/10.1016/0021-9991(84)90073-1. Arturs Berzins, Andreas Radler, Eric Volkmann, Sebastian Sanokowski, Sepp Hochreiter, and In Forty-second International Johannes Brandstetter. Geometry-informed neural networks. Conference on Machine Learning, 2025. G.D. Birkhoff and R.E. Langer. Relativity and Modern Physics. Harvard University Press, 1923. Cristian Bodnar, Wessel Bruinsma, Ana Lucic, Megan Stanley, Anna Allen, Johannes Brandstetter, Patrick Garvan, Maik Riechert, Jonathan Weyn, Haiyu Dong, Jayesh Gupta, Kit Thambiratnam, Alexander Archibald, Chun-Chieh Wu, Elizabeth Heider, Max Welling, Richard Turner, and Paris Perdikaris. foundation model for the earth system. Nature, 641(8065):11801187, May 2025. Raoul Bott and Loring W. Tu. de Rham Theory, pp. 1388. Springer New York, New York, NY, 1982. ISBN 978-1-4757-3951-0. doi: 10.1007/978-1-4757-3951-0_2. Robert H. Boyer and Richard W. Lindquist. Maximal analytic extension of the kerr metric. Journal of Mathematical Physics, 8(2):265281, 02 1967. ISSN 0022-2488. doi: 10.1063/1.1705193. Michael Boyle, Keefe Mitman, Mark Scheel, and Leo Stein. The sxs package, May 2025. Michael Boyle et al. The sxs collaboration catalog of binary black hole simulations. Classical and Quantum Gravity, 36(19):195006, sep 2019. doi: 10.1088/1361-6382/ab34e2. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. Johannes Brandstetter. Envisioning better benchmarks for machine learning PDE solvers. Nat. Mac. Intell., 7(1):23, 2025. doi: 10.1038/S42256-024-00962-Z. 26 Johannes Brandstetter, Daniel E. Worrall, and Max Welling. Message passing neural PDE solvers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. CoRR, abs/2104.13478, 2021. Steven Brunton, Bernd Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics. Annual review of fluid mechanics, 52(1):477508, 2020. S. Carroll, S.M. Carroll, and Addison-Wesley. Spacetime and Geometry: An Introduction to General Relativity. Addison Wesley, 2004. ISBN 9780805387322. Davide Castelvecchi. Gravitational wave detection wins physics nobel. Nature, 550(7674):1919, October 2017. S. Chandrasekhar. The Mathematical Theory of Black Holes, pp. 526. Springer Netherlands, Dordrecht, 1984. ISBN 978-94-009-6469-3. doi: 10.1007/978-94-009-6469-3_2. Honglin Chen, Rundi Wu, Eitan Grinspun, Changxi Zheng, and Peter Yichen Chen. Implicit neural spatial representations for time-dependent pdes. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 51625177. PMLR, 2023. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International conference on machine learning, pp. 794803. PMLR, 2018. Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 59395948, 2019. Aditya Chetan, Guandao Yang, Zichen Wang, Steve Marschner, and Bharath Hariharan. Accurate differential operators for neural fields, 2024. The LIGO Scientific Collaboration, Aasi, et al. Advanced ligo. Classical and Quantum Gravity, 32 (7):074001, mar 2015. doi: 10.1088/0264-9381/32/7/074001. Wojciech Marian Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan Pascanu. Sobolev training for neural networks. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, pp. 42814290, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Jeffrey Dean et al. Large scale distributed deep networks. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1, NIPS12, pp. 12231231, Red Hook, NY, USA, 2012. Curran Associates Inc. Nils Deppe, William Throwe, Lawrence E. Kidder, Nils L. Vu, Kyle C. Nelli, Cristóbal Armaza, Marceline S. Bonilla, François Hébert, Yoonsoo Kim, Prayush Kumar, Geoffrey Lovelace, Alexandra Macedo, Jordan Moxon, Eamonn OShea, Harald P. Pfeiffer, Mark A. Scheel, Saul A. Teukolsky, Nikolas A. Wittek, et al. SpECTRE v2025.04.21, 4 2025. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 107:311, 2018. doi: 10.1016/J.NEUNET.2017.12.012. Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, CarolaBibiane Schönlieb, and Angelica Aviles-Rivero. Where do we stand with implicit neural representations? technical and performance survey. arXiv preprint arXiv:2411.03688, 2024. 27 C. W. F. Everitt, D. B. DeBra, B. W. Parkinson, J. P. Turneaure, J. W. Conklin, M. I. Heifetz, G. M. Keiser, A. S. Silbergleit, T. Holmes, J. Kolodziejczak, M. Al-Meshari, J. C. Mester, B. Muhlfelder, V. G. Solomonik, K. Stahl, P. W. Worden, W. Bencze, S. Buchman, B. Clarke, A. Al-Jadaan, H. Al-Jibreen, J. Li, J. A. Lipa, J. M. Lockhart, B. Al-Suwaidan, M. Taber, and S. Wang. Gravity probe b: Final results of space experiment to test general relativity. Phys. Rev. Lett., 106:221101, May 2011. doi: 10.1103/PhysRevLett.106.221101. Bengt Fornberg. Introduction, pp. 13. Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press, 1996. V. Frolov and I. Novikov. Black Hole Physics: Basic Concepts and New Developments. Fundamental Theories of Physics. Springer Netherlands, 1998. ISBN 9780792351450. Fuerst, S. V. and Wu, K. Radiation transfer of emission lines in curved space-time*. A&A, 424(3): 733746, 2004. doi: 10.1051/0004-6361:20035814. A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition. Other Titles in Applied Mathematics. Society for Industrial and Applied Mathematics, 2008. ISBN 9780898716597. Andreas Griewank. mathematical view of automatic differentiation. Acta Numerica, 12:321398, 2003. doi: 10.1017/S0962492902000132. Roland Haas, Christian D. Ott, Bela Szilagyi, Jeffrey D. Kaplan, Jonas Lippuner, Mark A. Scheel, Kevin Barkett, Curran D. Muhlberger, Tim Dietrich, Matthew D. Duez, Francois Foucart, Harald P. Pfeiffer, Lawrence E. Kidder, and Saul A. Teukolsky. Simulations of inspiraling and merging double neutron stars using the spectral einstein code. Phys. Rev. D, 93:124062, Jun 2016. doi: 10.1103/PhysRevD.93.124062. Kota Hayashi, Kenta Kiuchi, Koutarou Kyutoku, Yuichiro Sekiguchi, and Masaru Shibata. Jet from binary neutron star merger with prompt black hole formation. Phys. Rev. Lett., 134:211407, May 2025. doi: 10.1103/PhysRevLett.134.211407. Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: neural network library and ecosystem for JAX, 2024. Edward Hirst, Tancredi Schettini Gherardini, and Alexander G. Stapleton. Ainstein: Numerical einstein metrics via machine learning, 2025. Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. Neural Networks, 3(5): 551560, 1990. ISSN 0893-6080. doi: https://doi.org/10.1016/0893-6080(90)90005-6. E. A. Huerta, Roland Haas, Sarah Habib, Anushri Gupta, Adam Rebei, Vishnu Chavva, Daniel Johnson, Shawn Rosofsky, Erik Wessel, Bhanu Agarwal, Diyu Luo, and Wei Ren. Physics of eccentric binary black hole mergers: numerical relativity perspective. Phys. Rev. D, 100:064003, Sep 2019. doi: 10.1103/PhysRevD.100.064003. Youngsik Hwang and Dong-Young Lim. Dual cone gradient descent for training physics-informed neural networks, 2025. C.J. Isham. Modern Differential Geometry for Physicists. World Scientific lecture notes in physics. World Scientific, 1999. ISBN 9789810235628. Dario Izzo and Pablo Gómez. Geodesy of irregular small bodies via neural density fields. Communications Engineering, 1(1):48, December 2022. Jürgen Jost. Riemannian Geometry and Geometric Analysis. Universitext. Springer Berlin, Heidelberg, Berlin, Heidelberg, 5th edition, 2008. ISBN 978-3-540-77341-2. doi: 10.1007/978-3-540-77341-2. George Em Karniadakis, Ioannis Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422440, June 2021. Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 852863. Curran Associates, Inc., 2021. Kerr and Schild. Republication of: new class of vacuum solutions of the einstein field equations. General Relativity and Gravitation, 41(10):24852499, October 2009. Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021. Patrick Kidger and Cristian Garcia. Equinox: neural networks in JAX via callable PyTrees and filtered transformations. Differentiable Programming workshop at Neural Information Processing Systems 2021, 2021. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. S. Kobayashi and K. Nomizu. Foundations of Differential Geometry. Number Bd. 1 in Foundations of Differential Geometry. Interscience Publishers, 1963. ISBN 9780470496480. Andrzej Krasinski. Ellipsoidal space-times, sources for the kerr metric. Annals of Physics, 112(1): 2240, 1978. ISSN 0003-4916. doi: https://doi.org/10.1016/0003-4916(78)90079-9. Heinz-Otto Kreiss. Difference Methods for Time-Dependent Partial Differential Equations, pp. 209238. Springer New York, New York, NY, 1994. ISBN 978-1-4612-0859-4. doi: 10.1007/ 978-1-4612-0859-4_7. V. Venkatraman Krishnan, M. Bailes, W. van Straten, N. Wex, P. C. C. Freire, E. F. Keane, T. M. Tauris, P. A. Rosado, N. D. R. Bhat, C. Flynn, A. Jameson, and S. Osłowski. Lensethirring frame dragging induced by fast-rotating white dwarf in binary pulsar system. Science, 367(6477): 577580, 2020. doi: 10.1126/science.aax7007. J. Lee. Introduction to Smooth Manifolds. Graduate Texts in Mathematics. Springer New York, 2012. ISBN 9781441999825. Zhi-Han Li, Chen-Qi Li, and Long-Gang Pang. Solving einstein equations using deep learning, 2023. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. Lee Lindblom, Mark Scheel, Lawrence Kidder, Robert Owen, and Oliver Rinne. new generalized harmonic evolution system. Classical and Quantum Gravity, 23(16):S447, jul 2006. doi: 10.1088/0264-9381/23/16/S09. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning, 2021. Qiang Lui, Mengyu Chu, and Nils Thuerey. Config: Towards conflict-free training of physics informed neural networks, 2025. Raimon Luna, Juan Calderón Bustillo, Juan José Seoane Martínez, Alejandro Torres-Forné, and José A. Font. Solving the teukolsky equation with physics-informed neural networks. Phys. Rev. D, 107:064025, Mar 2023. doi: 10.1103/PhysRevD.107.064025. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. In Proceedings of the Occupancy networks: Learning 3d reconstruction in function space. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 44604470, 2019. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 29 C.W. Misner, K.S. Thorne, J.A. Wheeler, and D.I. Kaiser. Gravitation. Princeton University Press, 2017. ISBN 9780691177793. Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM Transactions on Graphics, 41(4):115, July 2022. ISSN 1557-7368. doi: 10.1145/3528223.3530127. Ezra Newman and Roger Penrose. An approach to gravitational radiation by method of spin coefficients. Journal of Mathematical Physics, 3(3):566578, 05 1962. ISSN 0022-2488. doi: 10.1063/1.1724257. Tiago Novello, Guilherme Schardong, Luiz Schirmer, Vinícius da Silva, Hélio Lopes, and Luiz Velho. Exploring differential geometry in neural implicits. Computers & Graphics, 108:4960, 2022. ISSN 0097-8493. doi: https://doi.org/10.1016/j.cag.2022.09.003. Samuele Papa, Riccardo Valperga, David Knigge, Miltiadis Kofinas, Phillip Lippe, Jan-Jakob Sonke, and Efstratios Gavves. How to train neural field representations: comprehensive study and benchmark, 2024. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. DeepSDF: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 165174, 2019. Eric Poisson. Relativists Toolkit: The Mathematics of Black-Hole Mechanics. Cambridge University Press, 2004. Punturo et al. The einstein telescope: third-generation gravitational wave observatory. Classical and Quantum Gravity, 27(19):194002, sep 2010. doi: 10.1088/0264-9381/27/19/194002. M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686707, 2019. ISSN 0021-9991. doi: https://doi.org/10.1016/j.jcp.2018.10.045. Alireza Rashti, Rossella Gamba, Koustav Chandra, David Radice, Boris Daszuta, William Cook, and Sebastiano Bernuzzi. Binary black hole waveforms from high-resolution gr-athena++ simulations. Phys. Rev. D, 111:104078, May 2025. doi: 10.1103/n5pz-qv3x. Albert Reed, Thomas Blanford, Daniel C. Brown, and Suren Jayasuriya. Implicit neural representations for deconvolving sas images, 2021. Maxwell Rizzo et al. The einstein toolkit, May 2025. Ian Ruchlin, Zachariah B. Etienne, and Thomas W. Baumgarte. SENR/NRPy+: Numerical relativity in singular curvilinear coordinate systems. Phys. Rev. D, 97:064036, Mar 2018. doi: 10.1103/PhysRevD.97.064036. Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, and Richard G. Baraniuk. Wire: Wavelet implicit neural representations, 2023. Mark A. Scheel et al. The sxs collaborations third catalog of binary black hole simulations, 2025. Guangyuan Shi, Qimai Li, Wenlong Zhang, Jiaxin Chen, and Xiao-Ming Wu. Recon: Reducing conflicting gradients from the root for multi-task learning, 2023. Masaru Shibata and Takashi Nakamura. Evolution of three-dimensional gravitational waves: Harmonic slicing case. Phys. Rev. D, 52:54285444, Nov 1995. doi: 10.1103/PhysRevD.52.5428. Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 30 Luke Thomas Smith, Tom Horrocks, Naveed Akhtar, Eun-Jung Holden, and Daniel Wedge. Implicit neural representation for potential field geophysics. Scientific Reports, 15(1):9799, March 2025. Terence Tao. What is gauge?, 2008. Saul A. Teukolsky. The kerr metric. Classical and Quantum Gravity, 32(12):124006, jun 2015. doi: 10.1088/0264-9381/32/12/124006. Nils Thuerey, Philipp Holl, Maximilian Mueller, Patrick Schnell, Felix Trost, and Kiwon Um. Physics-based deep learning. arXiv preprint arXiv:2109.05237, 2021. Matt Visser. The kerr spacetime: brief introduction, 2008. Nikhil Vyas, Depen Morwani, Rosie Zhao, Mujin Kwun, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade. Soap: Improving and stabilizing shampoo using adam, 2025. Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient flow pathologies in physics-informed neural networks, 2020. Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An experts guide to training physics-informed neural networks, 2023. Sifan Wang, Ananyae Kumar Bhartari, Bowen Li, and Paris Perdikaris. Gradient alignment in physics-informed neural networks: second-order optimization perspective, 2025. Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling. Equivariant and Coordinate Independent Convolutional Networks. 2023. Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. CoRR, abs/2111.11426, 2021. Guandao Yang, Serge Belongie, Bharath Hariharan, and Vladlen Koltun. Geometry processing with neural fields. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2248322497. Curran Associates, Inc., 2021. Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. In International Continuous pde dynamics forecasting with implicit neural representations. Conference on Learning Representations, 2023. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning, 2020. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, et al. Artificial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023. Álvaro Fernández Corral, Nicolás Mendoza, Armin Iske, Andrey Yachmenev, and Jochen Küpper. Learning phase-space flows using time-discrete implicit runge-kutta pinns, 2024."
        },
        {
            "title": "A INTRODUCTION TO GENERAL RELATIVITY",
            "content": "This appendix provides the mathematical background and intuition on differential geometry (covering every aspect of the paper and the library), and general relativity. We remark that the reader may appreciate several related works, such as (Jost, 2008; Kobayashi & Nomizu, 1963; Isham, 1999; Lee, 2012) for mathematically rigorous coverage of differential geometry. For more physics-oriented readers, the following books extensively cover general relativity and numerical relativity (Misner et al., 2017; Carroll et al., 2004; Poisson, 2004) as alternative resources. Additionally, the Geometric Deep Learning (GDL) community can also find more ML-centric introduction to differential geometry in the following work (Bronstein et al., 2021; Weiler et al., 2023). Table 7: Table of notations Symbol Description ηµν xµ xµ eµ ϑµ eµ ϑµ xµ TpM Ω1(M) Γ(T M) Γ(T M) Φ Φ Riem(M) Diff(M) Dv Lv µ δµ ν Arbitrary manifold 4-dimensional spacetime manifold Flat Lorentzian metric Original coordinates Transformed coordinates Basis set Dual basis set Transformed basis set Transformed dual basis set Coordinate basis (equivalent to partial derivative operator) Tangent space at point Cotangent space at point Space of one-forms Smooth sections of tangent bundle (collection of vector fields) Smooth sections of cotangent bundle (collection of one-forms) Pullback operation Pushforward operation Set of (pseudo-)Riemannian metrics on Set of diffeomorphism maps on Cartesian (tensor) product Kronecker (tensor) product Directional derivative Lie derivative with respect to vector field Covariant derivative Kronecker delta (identity matrix) Speed of light Newtons constant A.1 FUNDAMENTAL CONCEPTS OF DIFFERENTIAL GEOMETRY & AND TENSOR CALCULUS The main concepts covered in this appendix are: 1. Fundamental concepts of differential geometry and tensor calculus: We introduce contravariance and covariance, and further vector and dual vector spaces. This allows us to define tangent and cotangent spaces. 2. Tensors and tensor fields: Next, we define tensors and tensor fields, operations on tensor fields, and the Lie derivative as generalization of the directional derivative for tensor fields. 3. Riemannian and Lorentzian geometry: This is the meat of Appendix A. We introduce 4-dimensional spacetime as continuous differentiable manifold. Via the metric, we can define Riemannian manifolds, and finally Lorentzian manifolds as pseudo-Riemannian manifold. Next, we discuss connections, covariant derivatives, and Christoffel symbols. This is all mathematical background that is required to introduce parallel transport, geodesics, the Riemann curvature tensor, the Ricci tensor, the Ricci scalar, the Weyl tensor, and finally curvature invariants and the stress-energy-momentum tensor. We end with Einstein field equations, reflecting back on the coordinate-independency of GR. A.1.1 CONTRAVARIANT AND COVARIANT COMPONENTS Loosely speaking, an ndimensional vector Rn can be expanded in its basis as = v1e1 +v2e2 + . . . + vnen, or = viei if we use Einstein sum convention. In general relativity, we write = vµeµ, where Greek indices indicate 4 dimensional space-time. Thus, in this non-Euclidean setting, it is necessary to distinguish objects that carry an upper index (contravariant) versus objects that carry lower index (covariant), since they satisfy different geometric properties and transformation laws. Definition 1 (Contravariance of vector components): Let Rn be coordinate system (frame) that is spanned by coordinate basis set {eµ}1µn, i.e., each basis vector can be expressed as eµ = xµ . vector can be expanded in its coordinate basis as = vµ(x)eµ := vµ(x) xµ . When transforming the vector to new coordinate system, spanned by another coordinate basis set {eν}1νn, i.e., eν = xν , one can express the vector components in the new coordinate system vν = vν(cid:0)x(cid:1) as vν(x) = xν xµ vµ(x) . (cid:88) µ (22) The ratio of change of the vector components is the inverse of the ratio of the base components. In other words, vector components transform inversely or contravariantly with respect to basis transformations, i.e., transform in the opposite way to the change in the coordinate system. Most contravariant objects represent physical quantities like displacement, velocity, and momentum, which must adjust when the coordinate basis changes. Definition 2 (Covariance of basis set): Let Rn be coordinate system (frame) that is spanned by coordinate basis set {eµ}1µn, i.e., each basis vector can be expressed as eµ = xµ . vector can be expanded in its coordinate basis as = vµ(x)eµ := vµ(x) xµ . When transforming the vector to new coordinate system, spanned by another coordinate basis set {eν}1νn, i.e., eν = xν , then, the basis set itself transforms as, xν = (cid:88) µ xµ xν xµ . (23) Note that we have introduced the concept of contravariant and covariant transformation by the example of vector components and the respective basis set. In general, we speak of contravariant w.r.t. their corresponding basis sets. I.e., contravariant components have covariant basis sets and covariant components have contravariant basis sets. As we introduce next, an object with covariant components is an object of the dual space. These covariant vectors, or covectors, typically represent gradients, such as the gradient of function. gradient represents the change w.r.t. an infinitesimal change in direction. It is intuitive that if we make the direction larger, the change becomes larger as well. In other words, if we change the basis vectors in which we measure this change, the gradient transforms covariantly w.r.t. the basis vectors. A.1.2 DUAL SPACE While the concept of vector space is well known in the machine learning community, there is closely associated concept of dual vector space (succinctly called dual space), which is an algebraic dual to the vector space itself with the same dimensions. 33 Definition 2 (Dual vector space): Let (cid:0)V, +, (cid:1) be vector space over field (e.g., R, C). The (algebraic) dual space (V , +, ) is vector space of linear functionals (maps) := (cid:8)vv(v) = V, (cid:9), satisfying: (v + w)(v) = v(v) + w(v) v(αv + βw) = αv(v) + βw(w) (24) (25) (26) for all v, , v, V, α, β, . Elements of the dual space are sometimes referred to as covectors or one-forms Bott & Tu (1982).5. (cv)(v) = c(v(v)) For example, suppose, we are given basis {e1, . . . , en} of vector space V. Then, one can introduce dual basis set {ϑ1, . . . , ϑn} of the dual space . Let = (α1e1 + α2e2 + . . . + αnen) V, {αi}1in . Thus, the action of the linear functionals ϑi on the vector reads: ϑi(v) = ϑi(α1e1 + α2e2 + . . . + αnen) = αiϑi(ei) = αi, = 1, . . . , and ϑi(ej) = δi j, where we have used the orthonormality condition, and δi is the Kronecker delta symbol. Conceptually, the covector ϕ is (complex-conjugated if = C) row-vector, which acts on column vector to produce α . Colloquially speaking, an element of the dual space eats up an element of the vector space and returns scalar (duality pairing). A.1.3 TANGENT AND COTANGENT SPACES Definition 1 (Tangent space): Let be smooth (C) manifold of dimension n. The tangent space TpM at point is set of d-dimensional vectors (called tangent vectors) attached at point p, defined as TpM := {(p, v) : Rd}, and carries the structure of real vector space. Every tangent space is spanned by an ordered basis {eµp}1µn = (cid:8) (cid:9) TpM, and vectors can be expanded in this basis as: , . . . , xn (cid:12) (cid:12) (cid:12)p (cid:12) (cid:12) (cid:12)p x1 vp = vµ(x) xµ (cid:12) (cid:12) (cid:12)p , (27) where vµ(x) are the components of the vector in this basis {eµp}1µn of the tangent space TpM. It is worth noting that dim(TpM) = dim(M). With this setup, we can now formally introduce the definition of tangent vectors. Definition 2 (Tangent vector): vector vp TpM is called as tangent vector if it acts as derivation, i.e, linear map acting on smooth functions C(M) at point M. Specifically, the map vp : C(M, R) satisfies 6: i) v(f + g) = v(f ) + v(g) f, C(M, R) (linearity) ii) v(f ) = 0, when is constant function, i.e., acts trivially on constants. iii) v(f g) = (p)v(g) + g(p)v(f ) f, C(M, R) (Leibniz product rule) From the definition above, it follows that tangent vectors should be regarded as derivation maps. This is equivalent to the notion of directional derivative (Isham, 1999) (Dvf )(p) := dt (p + tvp) (cid:12) (cid:12) (cid:12)t=0 = vµ(x) xµ (cid:12) (cid:12) (cid:12)p , = vµ(x) xµ (cid:12) (cid:12) (cid:12)p TpM, C(M, R) . (28) Directional derivatives are traditionally defined only for scalar-valued functions. This shall be revisited rigorously for more generalized concept called the Lie-derivatives, which operates on general tensors, c.f. Section A.2.3. 5A finite dimensional vector space is isomorphic to its double dual, i.e. = (cid:0)V (cid:1). 6For sake of ease, we will drop whenever possible. 34 Thus, TpM is the space of directional derivatives. The disjoint union of all the tangent spaces at every point forms structure called tangent bundles Isham (1999): = (cid:71) TpM = (cid:91) (cid:8)(p, vp) : vp TpM(cid:9) . (29) pM pM Tangent vectors as vector fields. In physics, quantities that vary spatiotemporally as continuum representation are defined as fields, featuring in domains such as electrodynamics, gravity, fluid dynamics, or continuum mechanics. vector field is smooth assignment of tangent vector vp to each point M. Thus, vector field is map : C(M) C(M), and is defined as: (cid:0)V (f )(cid:1)(p) = vp(f ) . (30) Theorem 1 (Cotangent space): Let be smooth (C)-manifold (differentiable). The cotangent := {(p, vp)vp, vp = κ, M, vp TpM, κ R} at point is the space set of all linear maps vp : TpM R, i.e., dual to the tangent space. The cotangent space is spanned by an ordered basis set (cid:8)dx1 p, ....., dxd can be expanded as: p}. Thus, any vp p, dx2 vp = µ(x)dxµ = µ(x)dxµ(cid:12) (cid:12) (cid:12)p . (31) It follows that dxµ(cid:12) (cid:12) = dim TpM = dim M. The (cid:12)p disjoint union of all the cotangent spaces at every point are known as cotangent bundles Isham (1999): ν , and dim = δµ (cid:12) (cid:12) (cid:12)p (cid:12) (cid:12) (cid:12)p := (cid:18) xµ xν (cid:18) xν (cid:19) (cid:19) = (cid:71) TpM = (cid:91) (cid:8)(p, vp) : vp M(cid:9) . (32) pM pM One can also construct fields of cotangent vectors (cotangent fields) by picking up an element of (M) in smooth manner. I.e., by assigning one cotangent vector smoothly at each point of the manifold, one obtains cotangent field (i.e., smooth section of the cotangent bundle). These cotangent fields are known in mathematical literature as one-forms. The set of all smooth one-forms on is commonly denoted as Ω1(M). A.2 TENSORS AND TENSOR FIELDS Definition 3 (Tensors): rank (r, s) tensor at point is described as multilinear map: : ... (cid:125) (cid:123)(cid:122) rcopies (cid:124) ... (cid:125) (cid:123)(cid:122) scopies (cid:124) , (33) where denotes the Cartesian product and the resultant tensor has total rank of +s. tensor takes in covectors and vectors, returning real number, in multilinear way (linear in each argument separately). The and input vectors and covectors pair with the and being the convariant and contravariant components, respectively. Equivalently, tensor is an element that lives in tensor product of vector and dual spaces, i.e., (V)r (V )s. tensor in particular basis choice {eαn}1nr and {ϑβn }1ns is given by = α1α2...αr β1β2...βs eα1 . . . eαr ϑβ1 . . . ϑβs , (34) := (ϑα1, . . . , ϑαr , eβs , . . . , eβs ) are the coefficients of the tensor w.r.t. the where α1α2...αr basis set. β1β2...βs 35 A.2.1 TENSOR TRANSFORMATION PROPERTIES pivotal criterion for an object to be classified as tensor(field) is that it transforms according to well-defined rule under changes of coordinates. Let {eαn }1nr V, {ϑβn }1ns , and {eαn}1nr V, { ϑβn }1ns be two coordinate systems on smooth manifold M, related by smooth invertible map. Consider tensor field of type (r, s) with components α1...αr in the original coordinate system {eαn }1nr V, {ϑβn }1ns . Under change of coordinate systems, the components in the new coordinate system {eαn }1nr V, { ϑβn }1ns transform according to the following tensor transformation law: β1...βs µ1...µr ν1...νs (x) = µ1 α1 µr αr α1...αr β1...βs (x) (cid:0)J 1(cid:1)β1 ν1 (cid:0)J 1(cid:1)βs νs , (35) xβl xαk and (cid:0)J 1(cid:1)βl xµk where µk xνl are the Jacobian and Jacobian inverse matrices in the αk νl coordinate basis, respectively. µk αk is the contravariant transformation of the contravariant components of α1...αr is the covariant transformation of the covariant components β1...βs of α1...αr . The indices µk, νl label components in the new coordinates and αk, βl are dummy indices summed over the old coordinates. key feature of tensor is that, if it is zero in one coordinate system, it is zero in every other coordinate system. This transformation law ensures that the tensorial nature of the object is preserved independent of the coordinate chart chosen. , whereas (cid:0)J 1(cid:1)βl νl β1...βs Tensor fields. tensor field is collection of tensor-valued rank quantities (r, s) such that at each point M, the multilinear function associates value Tp . Thus, the components α1...αr (p) are functions of the points of the manifold. (cid:0)V (cid:1)s β1...βs By definition, some known examples of tensor fields in physics and machine learning are: Rank 0 tensor, e.g., temperature field φ : Rm (scalar field) Rank (1, 0) tensor, e.g., (velocity, momentum, displacement) vector fields v: Rm Rn (contravariant vector field). These rank (1, 0) tensors have one component that transforms contravariantly, and eats up covariant component, e.g., vT to produce scalar. Rank (0, 1) tensor, e.g., gradient vector fields : Rm (covariant vector field). These rank (0, 1) tensors have one component that transforms covariantly, and eat up contravariant component to produce scalar. Rank (0, 2) tensor, e.g., matrix representing bilinear form that takes in two vectors and outputs scalar. We will see the metric tensor gµν as an example. In continuum and structural mechanics, known example is the strain tensor ϵij representing the deformation of crystal (body) caused by external forces such as stress. Rank (2, 0) tensor, e.g., matrix as multilinear map that takes in two covectors and outputs scalar. An example for rank (2, 0) tensor is the outer product of two vectors. An example is the Cauchy stress tensor σij from structural mechanics, which represents the internal forces per unit area acting inside material body. The stress tensor takes in two vectors, i.e., the normal vector to the surface (describing orientation), and the direction vector along which the force acts (projection), and returns scalar (force per unit area in that direction). A.2.2 OPERATIONS ON TENSOR FIELDS For multiple tensors of the same type (r, s), the algebraic operations such as addition, subtraction or multiplication by functions are straightforward. Here, we address multiplication of tensors of different ranks. Let be rank (r, s) tensor and rank (p, q) tensor. One can construct tensor product resulting in new tensor of rank (r + p, + q), defined by S(eα1 , . . . , eαr , eη1, . . . , eηp , φβ1 , . . . , φβs , φδ1 , . . . , φδq ) = (eα1 , . . . , eαr , eη1 , . . . , eηp )S(φβ1 , . . . , φβs , φδ1, . . . , φδq ) , (36) (37) and the components of this composite tensor read, (T S)α1...αrη1...ηp β1...βsδ1...δq := α1...αr β1...βs Sη1...ηp δ1...δq . (38) Another useful rule is that of contracting over repeated index/indices each from the vector and dual space respectively. Consider rank (r, s) tensor α1...αp...αr = α1...αr (39) . β1...αp...βs β1...βs I.e., αp is summed-over in the contravariant and covariant indices, and, thus it gets contracted. The resulting tensor is of rank (r 1, 1) . A.2.3 LIE DERIVATIVE: GENERALIZING THE NOTION OF DIRECTIONAL DERIVATIVES FOR TENSOR FIELDS Directional derivatives are of great importance and often appear in domains such as fluid dynamics, where scalar field is differentiated with respect to vector flow field, capturing infinitesimal dragging of scalar fields along flows generated by vector field. Flows can be viewed as diffeomorphisms Poisson (2004) induced by these vector fields. However, generalizing the notion of directional derivatives require defining derivatives of set of tensor fields of arbitrary rank (r, s) w.r.t. set of vector fields. This is often not possible on arbitrary manifolds, and requires concept of differentiating in tensorial setting. Geometrically, to compare tensors at infinitesimally separated points on manifold V, say at points p, requires to drag the tensor from to (also called parallel transporting, c.f. Section A.3.3.1. Alternatively, simpler approach to describe the dragging is via coordinate transformation from to q. This is the idea behind the Lie derivative. The Lie derivative along vector field vp TpM measures by how much the changes in tensor along differ from mere infinitesimal passive coordinate transformation of the tensor generated by v. In other words, the Lie derivative compares the actual rate of change of the tensor as you move along against the change youd get if everything were just shifted passively via coordinate transformation. We provide rough sketch of the derivation, but detailed explanations can be found here (Lee, 2012; Poisson, 2004). Consider an infinitesimal coordinate transformation which maps the vector with coordinates xµp at point to xµq at point q: xµq = xµp + δξ vµ(x)p . (40) It is to explicitly note that the original coordinates xµp and the transformed coordinates xµq are components of the same set of basis vectors. Such transformations fall under the category of active coordinate transformations that map points (or tensors at those points) at old locations to new locations in the old coordinate system in this case by moving small amount δξ along the vector field vp TpM. In other words, an active coordinate transformation maps points (and tensors) to new locations in the old coordinate system keeping the basis set intact. Whereas, passive transformations assign new coordinates to the old points (and tensors) by transforming the basis set itself. Assuming coordinate basis, one can differentiate the transformation w.r.t. the original coordinates, which yields xµ xν = δµ ν + δξ vµ(x) xν . (41) The result contains the identity matrix δµ first order, the inverse of the above Jacobian is ν and small correction due to the flow field vµ(x). To the xν xµ = δµ vµ(x) xν ν with respect to vµ follows similar pattern and is defined via ν δξ . The Lie-derivative of tensor field µ the limes: LvT µ ν = lim δξ0 ν (x) µ µ δξ ν (x) . (42) 37 In this scheme, it is important to distinguish three distinct tensor field evaluations: a) µ tensor in untransformed coordinates), b) µ and c) µ ν (x) (original ν (x) (transformed tensor in the transformed coordinates) ν (x) (original tensor in transformed coordinates). In order to compute Equation (42), we need two important concepts from differential geometry called push-forward and pull-back operations. We direct the interested readers to more advanced literature Isham (1999); Lee (2012); Kobayashi & Nomizu (1963) These three separate tensors fields can be related in the following manner: Firstly, the tensor mapped to the new set of coordinates µ ν (x) can be obtained via Equation(35), µ ν (x) = (J 1)µ ρ σ ν ρ σ (x) µ ν (x) + δξ (cid:18) vµ xσ σ ν (x) (cid:19) vσ xν µ σ (x) + O(δξ2) . (43) Secondly, the original tensor in transformed coordinates µ expansion: ν (x) can be evaluated at q, by Taylor ν (x) = µ µ ν (xσ) = µ ν (xσ + δξ vσ) = µ ν (x) + δξ vσ µ ν xσ + O(δξ2) . (44) Substituting Equations (43) (44) into the Lie-derivative definition of Equation (42), and δξ 0 one finds the following final expression: LvT µ ν = vσ µ ν xσ vµ xσ σ (cid:123)(cid:122) (cid:124) pullback ν (x) (cid:125) + vσ xν µ (cid:123)(cid:122) (cid:124) pushforward σ (x) (cid:125) . (45) The pushforward and pullback operations drag the transformed tensor field onto the original point, where differences can be computed. Thus, tensors are being compared in the same tangent/cotangent space. Mathematically, for smooth maps7 (diffeomorphisms) Φ : the pushforward Φ : TpM TΦ(p)N pushes vector fields forward from one tangent space of domain TpM to the tangent space of another tangent space TΦ(p)N . The pullback, dual linear map to pushforward, drags covectors (one-forms) living in cotangent spaces (Φ) Φ : in the reverse direction to the domain. Hence, the contributions from the pushforward on the vector field components and pullback on the covector field components jointly determine the structure of the Lie derivative of mixed tensor field, as expressed in Equation (45). These operations offer coherent mathematical framework for transitioning between tangent and cotangent bundles mapped onto other tangent and cotangent bundles via smooth maps, acting appropriately on vector fields and one-forms, respectively. Φ(p)N For any arbitrary rank (r, s) tensor, Equation (45) can be generalized to: (LvT )µ1...µr ν1...νs = vσ xσ µ1...µr ν1...νs (cid:88) i=1 µ1...σ...µr ν1...νs vµi xσ + (cid:88) j=1 µ1...µr ν1...σ...νs vσ xνj . (46) Lie-derivatives do not require the notion of connection. Connections will be introduced in detail in Section A.3.3 and intuitively stating, connects two distinct Tangent spaces at different points, which is not to be confused with pullback operation. Here, is an instructive comparison table for that compares different differentiation schemes: Feature Input function Connection dependence Captures curvature Measures Directional derivative Scalar fields Scalar changes Covariant derivative Tensor fields (Explicit) Lie derivative Tensor fields Intrinsic curvature Diffeomorphisms (flows) Table 8: Comparison between actions of directional, covariant, and Lie derivatives. 7for e.g., dragging of coordinates as in Equation (40) due to flows induced by vector fields. A.3 RIEMANNIAN AND LORENTZIAN GEOMETRY A.3.1 FOUR DIMENSIONAL SPACETIME AS CONTINUOUS DIFFERENTIABLE MANIFOLD The fabric of spacetime according to general relativity is combination of three-dimensional space and strictly positively progressing time direction into single four-dimensional continuum. Thus, space and time mix between each other through special orthogonal transformations SO(1, 3) called the Lorentz transformations. In order to rigorously define the four-dimensional spacetime, it is necessary to define the following: Definition 4 (Manifold): n-dimensional manifold is space, that, locally resembles the ndimensional Euclidean space Rn. However, combining these local patches together, globally, the space deviates from Rn. Definition 5 (Hausdorff space): Let be topological space. Then is said to be Hausdorff space if: For every pair of distinct points x, with = y, there exist open sets U, such that: U, V, and = . (47) Definition 6 (Differentiable manifold): An n-dimensional differentiable manifold is Hausdorff topological space such that: i) Locally is homeomorphic to Rn. Thus, there is an open set such that and homeomorphism ϕ : with an open subset of Rn. ii) For two subsets Uα and Uβ with Uα ϕα : Uα Zα and ϕβ : Uβ Zβ are compatible, i.e., the map ϕβ ϕ1 (cid:1) is smooth (infinitely differentiable C), and so is its inverse map. ϕβ (cid:84) Uβ = , the homeomorphisms (topologically isomorphic) (cid:1) α : ϕα (cid:84) Uβ (cid:84) Uβ (cid:0)Uα (cid:0)Uα The ϕα are often called charts and collection (union) of them (cid:83) provides coordinate system, labeling Uα K. The coordinate associated to Uα is: α ϕα is called an atlas. These charts ϕα(p) = (cid:0)x1(p), x2(p), ...., xn(p)(cid:1) Mathematically, the spacetime continuum denoted as M, is differentiable manifold with the structure of an Hausdorff topological space. To summarize, differentiable manifold is space that may be curved or complicated globally, but looks like Euclidean space up close, and allows for smooth calculus to be done on it. The Hausdorff space ensures than one can separate points nicely with open sets. This avoids weird pathological cases and makes limits and continuity well-behaved. Locally Euclidean means that one can do calculus as if we were on flat space even if the whole space is curved. And finally, the compatibility between overlapping charts ensures that one can do calculus consistently across different charts. A.3.2 METRIC TENSOR Definition 7 (Metric): metric is rank (0, 2) tensor field that is defined as symmetric bilinear map that assigns to each positive-definite inner product : TpM TpM such that i) g(vp, wp) = g(v, w) = g(w, v) v, TpM (symmetric) ii) For any M, g(v, w) = 0 wp TpM implying vp = 0 (non-degenerate). Represented in the basis set of the tangent space, the metric components at each point is given by (cid:32) (cid:33) gµν = gνµ := gp and the metric can be expanded as, xµ (cid:12) (cid:12) (cid:12)p , xν (cid:12) (cid:12) (cid:12)p (48) (49) = gµν(x) dxµ dxν. 39 Geometrically, the metric defined in Equation (49) generalizes the notion of distances and induces norm .p : TpM for generic coordinates such as curvilinear and/or manifolds possessing geometries that are intrinsically non-Euclidean in nature, for e.g., spaces of constant positive sectional curvature = 1 (e.g., 2-sphere S2 embedded in R3), spaces of constant sectional curvature = 1 such as hyperbolic geometry (Bolyai-Lobachevsky spaces H2). The distance between two points in such cases is called the line element, which is defined as, ds2 = gµν(x)dxµdxν. (50) For an n-dimensional manifold M, the metric tensor gµν is symmetric matrix, g(ϕµ, ϕν) := ϕµ, ϕν, with n(n+1) independent components (not necessarily expanded in the coordinate basis): 2 gµν = ϕ0, ϕ0 ϕ1, ϕ0 ... ϕn1, ϕ0 ϕ0, ϕ1 ϕ1, ϕ1 ... ϕn1, ϕ1 . . . ϕ0, ϕn1 ϕ1, ϕn1 ... ϕn1, ϕn1 . (51) Definition 8 (Metric bundle): Let be smooth manifold and (x0, , xn) be local coordinates on M. The bundle of symmetric (0, 2)-tensors on is the subbundle Sym2(T M) 0,2M = M. In fact, sections of Sym2(T M) contains all the symmetric bilinear forms, i.,e. symmetric (0, 2)- tensor fields, and includes the pseudo-Riemannian metrics on M. Riemannian manifolds. metric where all diagonal entries of the metric are positive, i.e., gµµ > 0, µ = 0, . . . , dim(M) 1 is called Riemannian metric. Thus, manifold endowed with Riemannian metric is known as Riemannian manifold denoted as tuple (cid:0)M, g(cid:1) (Jost, 2008). For the the Euclidean space Rn with Cartesian coordinates representation = dx1 dx1 + .... + dxn dxn (52) the metric tensor amounts to gij = δij. and boils down to Pythagoras theorem. general Riemannian metric prescribes method to measure the norm of vector as (cid:112)g(v, v) = and also allows for measuring angles between any two vectors v, at each point cosϑ = . Like any g(v,w) g(v,v)g(w,w) other tensor, the components of the metric tensor transform under coordinate change according to Equation (35): gαβ(x) = (cid:2)(J 1)µ α (cid:3)T gµν(x) (J 1)ν β . (53) Definition 9 (Arc length): Let γ : [0, 1] be piecewise smooth curve on differentiable manifold M, with γ(0) = and γ(1) = q. The velocity vector along the curve is denoted by γ(t), which lives in the tangent space Tγ(t)M. If the curve is expressed in local coordinates xµ(t), then the components of the tangent vector γ(t) are given by dxµ(t) . The arc length L(γ) (distance) of the curve is then defined by dt L(γ) = (cid:90) 1 0 γ(t)γ(t) dt = (cid:90) 1 (cid:114) gµν(x(t)) dxµ(t) dt dxν(t) dt dt . (54) This arc length8 is reparameterization invariant, i.e., it does not depend on the choice of parameterization of the curve γ(t). It is very important result that every smooth manifold admits Riemannian metric. 8It is also called an action in physics. 40 Lorentzian manifolds. Unlike Riemannian manifolds, spacetime is actually pseudo-Riemannian manifold9, that is, the metric is not positive definite. Thus, the underlying metric carries signature (, +, +, +), meaning, gtt < 0. Consequently, spacetime is Lorentzian manifold , and, forms the basis for electromagnetism and special relativity. The simplest example of Lorentzian manifold of arbitrary dimension is the Minkowksi metric, which is flat (meaning no curvature): η = dx0 dx0 + dx1 dx1 + .... + dxn1 dxn1 , (55) where, the components of the Minkowksi metric are ηµν = diag(1, +1, ..., +1). It is possible to find an orthonormal basis {eµ} of TpM around small neighborhood of point of Lorentzian manifold such that, locally, the metric resembles the Minkowski metric gµνp = ηµν . (56) In the case of Lorentzian manifolds , the arc length L(γ) in Eq.(54) is modified due to non positive-definiteness (cid:114) τ (σ) = (cid:90) σ 0 gµν(x) dxµ(σ) dσ dxν(σ) dσ dσ, (57) and is sometimes τ is referred to as proper-time. The minus sign under the square root ensures the integrand is positive for timelike paths, since for timelike intervals, the inner product of the velocity vector with itself (under the Lorentzian metric) is negative, i.e., ds2 = gµνdxµdxν. (58) Natural isomorphism between vector spaces and dual spaces. The metric provides natural isomorphism between vector spaces and dual spaces and allows the switch between contravariant and covariant components10. This is done via the following mapping : TpM M, where at each point p, one-form (covectors) is obtained via contraction operation of vector field vp with the metric g. For vector vp = vµ xµ and the covector vp = vµdxµ, the components are related by vµ = gµνvν . Since is non-degenerate, it is invertible. We denote the inverse metric as gµν, such that gµσgσν = δµ ν . This is rank (2, 0) tensor of the form ˆg = gµν xν . Through the inverse metric indices can be raised, e.g. xµ = gµνxν. xµ Such index contraction rules with the metric apply to tensors of rank (r, s) or even quantities that are not tensors: Sβ1....βs α1....αr = (cid:18) (cid:89) gβiγi (cid:19)(cid:18) (cid:89) (cid:19) gαiδi Sδ1....δr γ1....γs . (59) A.3.3 CONNECTIONS & COVARIANT DERIVATIVE i=1 i=1 Transporting vector and tensor fields systematically on manifolds requires mapping vector spaces at one point to vector spaces at another. While this can be done trivially in the Euclidean setting, for Riemannian and Lorentzian manifolds this is non-trivial since these vector fields and tensor fields live in different vector spaces. This necessitates geometric object that behaves as connector between vector spaces. This is achieved via geometric entity called the affine-connection, which is vector-valued one-form. Definition 10 (Affine connection): Let be smooth manifold and Γ(T M) be the space of vector fields on M, that is the space of smooth sections of the tangent bundle (i.e., the collection of all tangent spaces). An affine connection is bilinear map : Γ(T M) Γ(T M) Γ(T M) (v, w) (cid:55) vw . 9In general, pseudo-Riemannian manifold has signature (, ..., (cid:124) (cid:123)(cid:122) (cid:125) , +, ..., + (cid:124) (cid:123)(cid:122) (cid:125) ). 10In the context of numerical relativity the switch between contravariant and covariant components is called raising and lowering indices. 41 The differential operator is the covariant derivative satisfying the following for tangent vectors v, (short for vp, wp): i) v(w + z) = vw + vz ii) vw = vw C(M, R) iii) v(f w) = (Dvf )w + vw C(M, R), Dvf = v(f ) is the directional derivative. The affine connection is completely independent of the metric. However, if manifold is endowed with metric, this enables expressing the connection in terms of the metric. In GR, one looks at special subclass of affine connections called Levi-Civita connection, due to the symmetry property of the metric tensor. Definition 11 (Levi-Civita connection): An affine connection is an Levi-Civita connection for tangent vectors v, (short for vp, wp) if: vg = 0 Γ(T M) (metricity condition) (60a) vw wv = [v, w] v, Γ(T M) (torsion-free condition), (60b) where, [v, w] = (cid:0)vµµwν wµµvν(cid:1)ν is the Lie-bracket of vector fields Kobayashi & Nomizu (1963). Intuition behind the torsion-free condition. Imagine you are moving on smooth surface (like walking on hill), and you have two directions vp and wp at point p. Now: First move along tiny bit, then subsequently along w. Alternatively, move along first, then along v, akin to constructing parallelogram. In flat, Euclidean space, doing these two moves would land you at the same final point, because partial derivatives commute. On curved surface (a manifold M), they dont generally commute you end up slightly shifted. The Lie bracket [v, w] measures how far off (deficit) you are after moving in and then w, compared to and then v. It captures the non-commutativity of the vector transport along the two distinct directions, which leads to nonclosure of the parallelogram. Thus, the Lie bracket is intrinsic to the manifold, and shows how the transport of and interact. In torsion-free connections, the commutation failure is purely due to the manifolds structure not any extra twisting introduced by the connection itself. A.3.3.1 Parallel transport Definition 12 (Parallel transport): Let γ : [0, 1] be smooth curve on the manifold, and let be smooth (r, s)-rank tensor field defined along the curve γ. The parallel transport of along the curve γ(τ ) is defined by the condition that its directional covariant derivative along the curves tangent vector vanishes: γ(τ )T = 0, τ [0, 1] , γ(t) TpM . (61) In local coordinates {xµ(τ )}, the parallel transport condition for the components of the tensor field µ1...µr ν1...νs (τ ) along the curve γ(t) is: dτ µ1...µr ν1...νs (τ ) + (cid:88) i=1 Γµi λρ xρ(τ )T µ1...λ...µr ν1...νs (τ ) (cid:88) j=1 νj ρ xρ(τ )T µ1...µr Γλ ν1...λ...νs (τ ) = 0 . (62) These equations are set of coupled ODEs, and can be solved uniquely for an initial condition to find unique vector at each point along the curve γ(τ ). This ensures that as the tensor is transported along the curve, its components change in such way that their covariant rate of change along the curve vanishes. A.3.3.2 Christoffel symbols The Levi-Civita covariant derivative contains, apart from the partial derivative term, correction field that calibrates the deficit between vector (tensor) fields transported along path on the manifold. For basis {eµ} that is transported the covariant derivative is given by, (63) eν eµ(x) = Γσ µν(x)eσ(x) . The covariant derivative, denoted by eν ν = ν + Γν, defines modified differentiation operator that preserves tensorial character under general coordinate transformations. The quantities Γσ µν, known as the Christoffel symbols, represent the components of the Levi-Civita connection, which is uniquely determined by the requirement that the connection is torsion-free and compatible µν = Γσ with the metric. Notably, these symbols are symmetric in their lower two indices, i.e., Γσ νµ. The action of the covariant derivative on general tensor field of type (r, s) ensures that derivatives of tensors transform covariantly, thereby extending the notion of differentiation from vector calculus to curved manifolds. µT α1...αr β1...βs (x) = xµ α1...αr β1...βs (x) + (cid:88) i=1 Γαi µσ(x)T α1...σ...αr β1...βs (x) (cid:88) j=1 Γσ µβj (x)T α1...αr β1...σ...βs (x) . (64) (65) The action of the covariant derivative on scalar field, simply reduces to partial derivative µϕ(x) = ϕ(x) xµ . Christoffel symbols can be solely expressed in terms of the metric and its partial derivatives: Γρ µν(x) := 1 2 gρσ(cid:0)µgσν(x) + νgσµ(x) σgµν(x)(cid:1) = Γρ νµ(x) . (66) crucial feature of any connection is that it is not tensorial quantity. Connections dont obey the transformation law in Equation (35) under coordinate changes. This can be easily seen through the components of the Christoffel symbols in the coordinate basis: Γρ µν(x) = xρ xγ (cid:124) xα xµ xβ xν Γγ (cid:123)(cid:122) tensorial contribution αβ(x) (cid:125) + 2xσ xµ xν (cid:123)(cid:122) (cid:124) non-tensorial contribution xρ xσ (cid:125) . (67) Christoffel symbols play significant role in defining most stationary trajectories (shortest or longest) in the non-Euclidean setting. Lie derivatives revisited: Levi-Civita connection included. In case of nonzero Levi-civita connection, the partial derivatives of an ordinary Lie derivative in Equation(46) is replaced by the covariant derivatives: (LvT )µ1...µr ν1...νs = vσσT µ1...µr ν1...νs (cid:88) i=1 µ1...σ...µr ν1...νs σvµi + (cid:88) j=1 µ1...µr ν1...σ...νs νj vσ . (68) The first term advects (drags) the tensor along the flow of v, i.e., this is the naive directional derivative part. The second and third terms account for how the basis vectors themselves are changing, due to curvature and due to the vector field v, respectively. It is easy to show that the three terms lead to pair-wise cancellations between the Christoffel symbols present in the three different covariant derivative terms of Equation(68). Due to this, the whole expression boils down to Equation(46), thus corroborating the connection independence of this derivative operator. Differently put, in the covariant derivatives, the Christoffel symbols introduce extra terms. However, the extra Christoffel terms cancel out between the different contributions (first, second, and third terms). One ends up getting exactly the same final expression for the Lie derivative as if only partial derivatives had been used this is what is meant by connection independence. A.3.4 GEODESIC EQUATION Geodesics are paths that correspond to the most stationary trajectories (shortest or longest distance) that connect two points and on manifold. Often, we only consider locally distance minimizing curves, and refer to them as geodesics. Geodesics are obtained by solving calculus of variations problem on the distance metric L(γ) in Equation (54), i.e., 43 δL(γ) := 0 , (69) (or alternatively on τ (σ) in the Lorentzian setting of Equation (57)). Solving the calculus of variations problem boils down to solving the Euler-Lagrange equations, which mathematically is equivalent to the condition γ(t) γ(t) = 0 , (70) where γ(t) is the curve (path) on the manifold, γ(t) the tangent vector (velocity vector), and the covariant derivative. Equation (70) intuitively says that the tangent vector is parallel transported along itself meaning, one is moving without acceleration relative to the curved space. Thus, parallel transporting the tangent vector along the curve preserves the tangent vector. In numerical relativity, these corresponds to the equations of motion, i.e. generalization of the Newtons acceleration equation d2xµ dτ 2 = µ/m. For full derivations, we direct the readers to refer to Carroll et al. (2004); Misner et al. (2017); Poisson (2004). We shall present the final form of very central second-order ODE describing motion (acceleration) of objects executing geodesic paths around heavy gravitating bodies, namely, the geodesic equation d2xµ dτ 2 + Γµ ρσ(x) dxρ dτ dxσ dτ = 0 , (71) where, τ is some affine paramter (typically, chosen to be the proper-time in Equation (57)), d2xµ/dτ 2 is the four-acceleration vector, dxρ/dτ is the four-velocity and Γµ ρσ is the Christoffel symbols as seen in Equation (66)). Importantly, Equation (70) is the geometric statement of the geodesic equation. Its coordinate-free, i.e., its expressed entirely in terms of geometric objects. Equation (71) is the coordinate version of the same idea. Here, one chooses coordinate system xµ on the manifold, and the covariant derivative acting on vector becomes the partial derivative plus correction terms involving the Christoffel symbols11. A.3.5 CURVATURE TENSORS AND SCALARS Curvature tensors arise naturally in differential geometry as tensorial objects that capture the intrinsic and, where appropriate, extrinsic geometric properties of manifold. They provide coordinateindependent way to quantify the curvature of space or spacetime by encoding how the geometry deviates from flatness through the second derivatives, constructed out of Hessians of the metric tensor. Unlike artifacts that may arise from curvilinear coordinate choices on flat manifolds, curvature tensors reflect the true geometric content of space. These generalize classical notions such as Gaussian curvature to higher dimensions and arbitrary signature. Being multilinear objects containing several tensor components, curvature tensors systematically characterize the variation of the metric across different directions. We shall introduce the key curvature related quantities, which include the Riemann relevant ones used in our paper in the following section. A.3.5.1 Riemann curvature tensor The Riemann curvature tensor Rµ γαβ(x) eµ ϑγ ϑα ϑβ is rank (1, 3) tensor, which quantifies the measure to which vector that is transported along small loop (also called holonomy) fails to return to its original orientation due to the effect of the intrinsic curvature that the vector field picks up during the transport. The Riemann curvature tensor is defined via the commutators of covariant derivatives acting on components of vector field v: [α, β]vδ(x) = (cid:0)αβ βα (cid:1)vδ(x) = Rδ αβγ(x)vγ(x) . (72) The components of the Riemann curvature tensor are expressed in terms of the Christoffel symbols Rδ αβγ(x) = Γδ αγ(x) xβ Γδ βγ(x) xα + Γσ γα(x)Γδ βσ(x) Γσ βγ(x)Γδ σα(x) . (73) 11Ideally, concepts such as connections, parallel transport and covariant derivatives are metric-independent formulation 44 The Riemann tensor Rαβγδ = gασRσ βγδ obeys the following identities: Rαβγδ = Rαβδγ , Rαβγδ = Rβαγδ , Rαβγδ = Rγδαβ . The Riemann tensor in n-dimensional manifold has n2(n2 1)/12 independent components. Importantly, it satisfies two additional identities, called the Bianchi identities Rαβγδ + Rαγδβ + Rαδβγ = 0 (Bianchi Identity I) , αRβγδσ + βRγαδσ + γRαβδσ = 0 (Bianchi Identity II) . (74a) (74b) Unlike the Christoffel symbols, which may be non-zero purely due to the choice of coordinates e.g., when imposing curvilinear coordinates such as polar coordinates (r, ϑ) on the flat Cartesian plane the Riemann curvature tensor encapsulates the true geometric curvature of manifold. Since Christoffel symbols represent connection coefficients rather than tensorial objects, their non-vanishing components can give the false impression of intrinsic curvature, even on flat manifold. In contrast, the Riemann tensor is bona fide tensor and its vanishing is coordinate-invariant statement: if the Riemann tensor vanishes in one coordinate system, it vanishes in all coordinate systems. Thus, it provides definitive criterion for distinguishing truly curved spaces from flat ones, independent of coordinate artifacts. Geodesic deviation. An important consequence of the existence of non-zero Riemann tensor is that it encapsulates directional information about how geodesics path converge or diverge. Intuitively, it implies that in Euclidean space Rd, parallel lines always remain parallel, but in the case of spherical geometry, say Sd1 (constant positive curvature) the parallel lines converge at point, while for hyperbolic spaces Hd , the parallel lines continue diverging. This is captured by the geodesic deviation equation (sometimes referred to as Jacobi equation) Isham (1999); Jost (2008); Poisson (2004), which shows how an infinitesimal neighborhood of given geodesics diverge or converge. Here, we shall give the equation with brief sketch. Theorem 2 (Jacobi equation): Let γs(τ ) be family of closely spaced geodesics indexed by smooth one-paramter family and τ the affine parameter. Let xµ(s, τ ) be the coordinates of the geodesics γs(τ ), then the tangent vector field is directional derivative expressed in these coordinates as µ = dxµ(s,τ ) τ . Then, the deviation vector fields that satisfy the acceleration equation are called (Jacobi fields) and read . Let the set of deviation vector fields Sµ = xµ(τ,s) dτ D2Sµ Dτ 2 = Rµ αβγX αX βSγ , (75) where, given direction on manifold, while accounting for the manifolds curvature. Dτ = αα is the directional covariant derivative, i.e., the derivative of vector field along A.3.5.2 Contracted curvature tensors, scalars and invariants Ricci tensor. From the rank (1, 3) Riemann tensor, one can construct traced (contracted) symmetric curvature tensor of rank (0, 2), called the Ricci tensor Rαβ ϑα ϑβ, Rγ (76) αγβ = Trg(Rγ αδβ) := Rαβ . Mathematically, the Ricci tensor aggregates directional curvature along orthogonal planes. Thus, it can be considered as curvature average of the Riemann tensor. It is closely related to the concept of sectional curvature and reflects how volume deformations occurs as one evolve under geodesic flow. Ricci scalar. The traced (contracted) Ricci tensor yields scalar field called the scalar curvature, also called the Ricci scalar. It is defined as Rα α = Trg(Rαβ) := . (77) 45 Mathematically, the scalar curvature corresponds to the sum/average over all sectional curvatures, i.e., R(p) = (cid:80) α=β Sec(eα, eβ)p M. For point p, in an n-dimensional Riemannian manifold (M, g), it characterizes the volume of an ϵ-radius ball in the manifold to the corresponding ball in Euclidean space, given by, Vol(cid:0)Bϵ(p)) M(cid:1) = Vol(cid:0)Bϵ(0) Rn(cid:1) (cid:18) 1 6(n + 2) (cid:19) ϵ2 + O(ϵ3) . Weyl tensor. Another important tensor field of rank(0, 4) is the Weyl tensor, which is obtained as the trace-free part of the Riemann tensor. Physically, the Weyl tensor describes the tidal force experienced by body when moving along geodesics, and quantifies the shape distortion body experiences due to tidal forces (e.g., water tides caused by the gravitational pull of the moon). In an n-dimensional manifold it is defined as: Cαβγδ = Rαβγδ 1 (n 2) (cid:18) (cid:19) Rαδgβγ Rαγgβδ + Rβγgαδ Rβδgαγ (78) + 1 (n 1)(n 2) R(cid:0)gαγgβδ gαδgβγ (cid:1) . Mathematically, the Weyl tensor corresponds to the only non-zero components of the Riemann tensor when looking at Ricci-flat manifolds, i.e. Rαβ = 0. This become relevant for e.g., vaccum solutions, class of exact solutions where Rαβ = 0 of the Einstein equations in the absence of matter distribution. Curvature invariants. Curvature invariants play central role in the analysis of spacetime geometries in general relativity. These scalar quantities are constructed from contractions of curvature tensors and are manifestly invariant under general coordinate transformations. As such, they serve as powerful diagnostic tools for characterizing the local and global geometric and physical properties of spacetime, which includes the identification of true (genuine spacetime singularities) and false singularities (artifact of choice of coordinate charts). Among the most prominent quadratic curvature invariants that is relevant to our simulations and features in our paper is the Kretschmann scalar, defined as the full contraction of the Riemann curvature tensor with itself: (x) := Rαβγδ(x)Rαβγδ(x) = gαρ(x)gβσ(x)gγζ(x)gδη(x)Rρσζη(x)Rαβγδ(x) . (79) The Kretschmann scalar provides coordinate-independent measure of the magnitude of the curvature of spacetime and the singularity becomes blows-up, due to infinite curvature. Examples are Kretschmann scalars for blackholes, and Weyl scalars Ψ4 for gravitational wave astrophysics. They capture the presence of intrinsic curvatures even when the Ricci tensor itself vanishes. Thus, the Kretschmann scalar encodes geometric information in frame-independent manner. A.3.6 STRESS-ENERGY-MOMENTUM TENSOR The stress-energy-momentum tensor (or simply called the energy-momentum tensor) is symmetric rank (2, 0) tensor αβ = αβeα eβ . (80) Physically, αβ is generalization of the stress tensor in continuum and fluid mechanics. It stores the information of distribution of matter fields, i.e., sources or sinks as 4 4 tensor, such as energy-density, energy-flux, momentum density, and momentum flux. These matter fields satisfy the conservation laws, i.e., conservation of mass and energy via the four-dimensional continuity equation and corresponds to the divergence-free condition of the energymomentum tensor αT αβ(x) = 0 . (81) The stress-energy-momentum tensor features on the right hand side of the Einstein field equations, and influences spacetime geometry by causing distortions on it. A.3.7 EINSTEIN FIELD EQUATIONS The Einstein field equations (EFEs) are set of second-order non-linear PDEs containing geometry on the left hand side and the source on the right hand side. EFEs are obtained by combining all the differential geometric quantities from Equations ((51), (76), (77), (80)), together with the conservation laws for matter distribution of Equation (81), resulting in Gαβ = 8πG Tαβ . (82) Here, Gαβ := Rαβ 1 condition αGαβ = 0, which, is consequence of the IInd Bianchi identity of Equation (74b). 2 gαβR+Λgαβ is called the Einstein tensor and also satisfies the divergence-free A.3.8 COORDINATE-INDEPENDENCE OF GR Fundamentally, GR posits deeper symmetry class: diffeomorphism covariance (Misner et al., 2017). It asserts that the laws of physics are independent of any particular choice of coordinates or parametrization of the underlying smooth manifold. For example, the metric around the star, say sun, can be expressed in terms of the Schwarzschild metric (introduced in Section B.1). Here, the diffeomorphism acts as gauge transformation (Tao, 2008) between two sets of metrics defined on the Lorentzian manifold , in this case an equivalence class of Lorentzian metrics Riem(M ) describing the same spacetime geometry. This makes sure equations of motion, conservation laws, physical fields, etc. remain intact, hence, the term covariance. In mathematical terms, Let, Φ Diff(M ) be smooth, invertible map between with smooth inverse, Φ : = such that: (cid:0)Φg(cid:1)(v, w) := g(cid:0)Φ(v), Φ(w)(cid:1) . (83) Here, Φ : M , is the pushforward map defined on the tangent bundles. This means under diffeomorphisms the metric transforms via pullback operation Φg = g. I.e., gαβ(x) = xµ xν xβ gµν(x) are gauge equivalent. Additionally, GR also admits changes of local frames or bases xα (external symmetries) via the general linear group GL(4, R), i.e., invertible linear transformations at each point . Thus, GR enjoys coordinate independence from two symmetry transformations, i.e., (i) between any particular choice of coordinates or parameterization of the underlying smooth manifold , and (ii) general linear group transformations that locally change frames of reference."
        },
        {
            "title": "B EXACT SOLUTIONS OF THE EINSTEIN FIELD EQUATIONS",
            "content": "This Appendix contains detailed description of the exact solutions of EFEs corresponding to class of metrics gµν that are solutions of Equation (82). While there exist several geometries that satisfy the EFEs, we shall consider three prominent geometries: the Schwarzschild metric, the Kerr metric, and gravitational waves. These solutions not only have high theoretical and historical relevance, but are also of great interest in computational black hole astrophysics and gravitational wave and multi-messenger astronomy. From here on, we work in naturalized units by setting = = 1. Our work predominantly uses the exact solutions for generating synthetic training data, which are analytic expressions for (i) Schwarzschild, (ii) Kerr, and (iii) linearized gravity metrics, on which we fit the NeFs. B.1 SCHWARZSCHILD METRIC The Schwarzschild metric is the simplest non-trivial solution to the EFEs. It describes the geometry around non-rotating spherical body, such as star or black hole, constituting spherically symmetric vacuum solutions, i.e., Tµν = 0. famous result of GR called the Birkhoffs theorem (Birkhoff & Langer, 1923) proves that any spherically symmetric vacuum solution corresponds to static (nonrotating), time-independent (stationary), and asymptotically flat metric (i.e., for the metric converges to the flat Minkowski spacetime), and must essentially be equivalent to the Schwarzschild solution. 47 B.1.1 COORDINATE SYSTEMS FOR SCHWARZSCHILD METRICS Spherical polar coordinates. Schwarzschild solution is typically written in the convential spherical polar coordinates (t, r, θ, ϕ) where R, R+, θ (0, π), and ϕ [0, 2π). The metric can be written either using the quadratic line element (cid:18) (cid:19)1 (cid:19) (cid:18) ds2 = 1 dt2 + 1 dr2 + r2(cid:0)dθ2 + sin2θdϕ2(cid:1) . 2M 2M or in the equivalent matrix notation gSph µν = (cid:18) 1 2M (cid:19) 0 0 0 (cid:19)1 0 (cid:18) 1 2M 0 0 0 0 r2 0 . 0 0 0 r2sin2θ (84) (85) The true singularity of the Schwarzschild metric is at the origin and can be identified from the divergent Kretschmann scalar (Eq. (79)): (r) = 48M 2 r6 r0 . (86) Although (fake) coordinate singularity exists at = rs = 2M , where the Kretschmann scalar is well defined. This special radius rs is called the Schwarzschild radius. It demarcates the location of the event horizon of non-rotating black hole and delineates region from which no causal signal can escape to asymptotic infinity, meaning, it is point of no return for any body (including light) once it crosses this critical radius. Cartesian Kerr-Schild coordinates. The Kerr-Schild (KS) form is beneficial representation for finding exact solutions to the EFEs. These are perturbative corrections to spacetime metric only upto the linear order (Kerr & Schild, 2009). The KS form enables the following simplifications to the nonlinear field equations : (i) It expresses the resultant metric as linearized perturbation to the background metric, and (ii) gets rid of the coordinate singularities, which are mere artifacts of an unsuitable choice of coordinate systems. The corresponding line element expressed in the KS form reads ds2 = (gαβ + (x)ℓαℓβ)dxαdxβ , (87) where gαβ is some background metric, lα are the components of null vector ℓ with respect to the background metric and (x) is scalar. For spherically symmetric non-rotating blackhole such as Schwarzschild, the Cartesian Kerr-Schild line element is obtained by setting gαβ = ηαβ, ℓ = 1, (cid:16) (cid:17) and = 2M : , , ds2 = dt2 + dx2 + dy2 + dz2 + (cid:20) dt + 2M dx + dy + (cid:21)2 dz . (88) Unlike the spherical coordinate form in Equation (84), = 2M is not singular, hence removing the coordinate singularities. The metric tensor components read: 1 + 2M/r gKS µν = 2M r2 2M r2 2M r2 1 + 2M r2 2M x2 r3 2M xy r3 2M xz r3 2M r2 2M xy r3 2M y2 r3 2M yz r3 1 + 2M r2 2M xz r3 2M yz r3 2M z2 r3 1 + . (89) (90) 48 Ingoing (advanced) Eddington-Finkelstein coordinates The ingoing version of the EddingtonFinkelstein (EF) coordinates is obtained by replacing time with an advanced time coordinate (cid:12) = + r(r), where = + log(cid:12) (cid:12). Thus, dt in these transformed coordinates amounts to: (cid:12) r2M 2M dt = dv dr = dv 1 (cid:18) (cid:19)1 dr 2M The ingoing EF version of the Schwarzschild metric reads: ds2 = (cid:18) 1 (cid:19) 2M dv2 + 2dv dr + r2(cid:0)dθ2 + sin2θ dϕ2(cid:1) , (91) With the metric tensor being: gEF µν = (cid:18) 1 (cid:19) 2M 1 0 0 0 0 0 0 0 0 0 0 0 . r2 sin2 ϑ (92) This metric is smooth (and non-degenerate), devoid of coordinate singularities at the event horizon = rs = 2M , and can be continued down to the curvature singularity at = 0 (Carroll et al., 2004; Chandrasekhar, 1984; Frolov & Novikov, 1998). B.2 KERR METRIC The Kerr solution describes massive gravitating body rotating with an angular momentum J. From the physics perspective, it is not symmetric under time-reversal symmetry, i.e., t, hence corresponds to stationary but non-static solution (Teukolsky, 2015). Due to finite angular momentum J, or equivalently, rotation parameter = > 0, the Kerr metric introduces an asymmetry, and is oblate. Thus, the Kerr metric corresponds to an oblate spheroid geometry. B.2.1 COORDINATE SYSTEMS FOR KERR METRIC Boyer-Lindquist coordinates. The Boyer-Lindquist (BL) coordinates are special and convenient representation for the Kerr metric (Boyer & Lindquist, 1967; Visser, 2008; Teukolsky, 2015). The BL form (t, r, ϑ, ϕ) is described by oblate spheroidal coordinates (Krasinski, 1978): = = (cid:112) r2 + a2 sinϑ cosϕ (cid:112) r2 + a2 sinϑ sinϕ = cosϑ . (93a) (93b) (93c) Notice that the zenith angle ϑ = θ differs from the Schwarzschild case, while the azimuthal angle ϕ is the same in both. As 0, the Kerr metric boils down to the non-rotating spherical case of the Schwarzschild metric. ds2 = (cid:16) 1 (cid:17) 2M Σ dt2 4M arsin2ϑ Σ dtdϕ+ Σ dr2+Σdϑ2+ (cid:18) r2+a2+ (cid:19) 2M ra2sin2ϑ Σ sin2ϑdϕ2 , (94) where the length scales are = (angular momentum per unit mass), Σ r2 + a2cos2ϑ, and r2 2M + a2. The Kerr curvature singularity occurs at Σ := r2 + a2cos2ϑ = 0, implying = 0 and ϑ = π 2 . The metric tensor of Equation (94) is: 49 gBL µν = 1 (cid:19) 2M Σ 0 (cid:18) 2M ar sin2 ϑ Σ 0 0 0 Σ 0 Σ 0 2M ar sin2 ϑ Σ 0 0 (cid:18) r2 + a2 + 2M a2r sin2 ϑ Σ (cid:19) sin2 ϑ . (95) In the Boyer-Lindquist form of the metric, there also exist coordinate singularities at = r2 2M + a2 = 0. Thus, the roots of = 0 are = 2 a2, which demarcate the outer and inner horizons. It is easy to see the existence of curvature singularity at = 0 on the equatorial plane corresponding to the zenith angle ϑ = π 2 . Thus, unlike Schwarzschild, the singularity in Kerr geometry takes the form of ring, also known as ring singularity. Cartesian Kerr-Schild coordinates. The Cartesian KS form of the Kerr metric is obtained by (cid:17) setting in Equation (87) ℓ = r4+a2z2 in Equation (87). The Kerr metric in Kerr coordinates are often used to write initial data for hydro simulations. The line-element in the Cartesian Kerr-Schild form reads (Teukolsky, 2015): and = mr3 r2+a2 , ryax r2+a2 , 1, rx+ay (cid:16) ds2 = dt2 +dx2 + dy2 + dz2 + 2mr3 r4 + a2z2 (cid:20) dt + r(xdx + ydy) a2 + r2 + a(ydx xdy) a2 + + (cid:21)2 dz . (96) Here, r(x, y, z) is not coordinate, and is given implicitly by solving the quadratic equation x2+y2 r2+a2 + z2 r2 = 1: The solution for the implicit function is given by the discriminant (Visser, 2008): r2(x, y, z) = x2 + y2 + z2 a2 2 + (cid:114) (x2 + y2 + z2 a2)2 + 4a2z2 4 . The corresponding Cartesian coordinates are expressed as: = (r cosφ sinφ)sinϑ = = (r sinφ + cosφ)sinϑ = = cosϑ . (cid:112) (cid:112) r2 + a2 sinϑ cos(cid:0)φ + tan1(a/r)(cid:1) , r2 + a2 sinϑ sin(cid:0)φ + tan1(a/r)(cid:1) , In the BL coordinates the ring singularity for Kerr exists at = 0 & ϑ = π 2 , translating to = 0 (equatorial-plane), and the ring occurring at x2 + y2 = a2. In contrast, the KS representation is devoid of coordinate singularities, making it suitable to work in numerics, especially around the event-horizons. Ingoing Eddington-Finkelstein coordinates. In the original formulation, the Kerr metric is written in the advanced time coordinates/ingoing EF coordinates v. The line element in this representation reads (Teukolsky, 2015): ds2 = (cid:18) 1 2M r2 + a2cos2θ (cid:19) (dv + sin2θ ϕ)2 (97) + 2(dv + sin2θd ϕ)(dr + sin2θd ϕ) + (r2 + a2cos2θ)(dθ2 + sin2θd ϕ2) , where the ingoing EF coordinates are related to the Boyer-Lindquist coordinates Equation (93) by the following transformation: = + (cid:90) (r2 + a2) dr , ϕ = ϕ + (cid:90) dr , 50 where, r2 2M + a2. and the metric tensor components corresponding to the line element is given by (cid:19) 2M Σ 1 0 0 0 Σ 2M ar sin2 θ Σ sin2 θ 0 1 0 (cid:18) gEF µν = . (98) 2M ar sin2 θ Σ sin2 θ 0 (cid:18) r2 + a2 + 2M a2r sin2 θ Σ (cid:19) sin2 θ The coordinate (fake) singularities (K < ) of the Kerr metric is given by the zeros of r2 2M + a2 = 0. Solving for the zeros, one finds = (cid:112) 2 a2 , where, r+ is the outer event horizon, while demarcates the inner event horizon. Apart from that, rotating metrics also possess highly interesting region known as the ergosphere, which fundamentally captures the non-Euclidean and non-inertial nature of general relativistic effects induced by rotation. This domain is situated outside the outer event horizon r+, and is created owing to the frame-dragging (Lense-Thirring) effect. Consequently, no physical observer (test body) can remain static within the ergosphere and is compelled to co-rotate with the black hole depending on the value of a. The location of the ergosphere is given by rergo (ϑ) = where, rergo + is the outer ergosphere, while rergo following regions of the Kerr metric are demarcated: (cid:112) 2 a2cos2ϑ , demarcates the inner ergosphere. In Figure (13), the Figure 13: Kerr metric 2D slice in the x-z plane (y = 0) for spin parameter = 0.99. The following regions plotted are: i) inner ergosphere rergo : red region, ii) inner event-horizon r: green region, iii) outer event-horizon r+: blue region and iv) outer Ergosphere rergo + : purple region. B.2.2 KRETSCHMANN SCALAR ASSOCIATED WITH KERR SOLUTION The nontrivial part of the Kretschmann invariant for the Kerr metric reads: RαβγδRαβγδ = CαβγδC αβγδ = 48M 2(r2 a2cos2ϑ)(cid:2)(r2 + a2cos2ϑ)2 16r2a2cos2ϑ(cid:3) (r2 + a2cos2ϑ)6 . (99) This guarantees that the curvature singularity (K ) occurs at the ring Σ r2 + a2cos2ϑ = 0, with zeros at: π 2 . = 0 , and, ϑ = 51 B.3 GRAVITATIONAL WAVES Linearized gravity models the metric as tiny fluctuations or perturbations hαβ of the flat background metric ηαβ: gαβ ηαβ + hαβ + O(hαβ)2 , where hαβ 1. To describe gravitational wave propagation, it is often convenient to reduce the linearized field equations into simplified form via two gauge fixing conditions, namely, a) harmonic gauge, and b) transverse-traceless (TT) gauge. Thus, the Einstein field equations for gravitational waves assume succinct wave equation type form: (ϵ) αβ = 16πTαβ . (ϵ) αβ = h(ϵ) Here, be shown that the PDEs in Equation (B.3) produce gravitational wave solutions. 2 h(ϵ)ηαβ and ηαβαβ is the dAlembert operator (wave operator). It can αβ 1 The transverse-traceless (TT) perturbation (we drop the superscript (ϵ) for the sake of ease) satisfies the following conditions: Transverse: βhTT αβ = 0, i.e., wave propagates perpendicular to perturbation direction, Traceless: hTT α Purely spatial: hTT α = 0, 0α = 0, i.e., no time components. Thus, gravitational wave propagating in the z-direction with frequency ω is given in the TT gauge as: 0 0 0 hTT αβ = 0 0 h+ 0 0 h+ 0 0 0 cos(cid:0)ω(t z)(cid:1) . (100) h+ and are the amplitudes of the + (plus) polarization and (cross) polarization. The complete metric tensor in the linearized gravity setting is given by: gαβ = ηαβ + hTT αβ = 1 0 0 0 0 1 + h+ cos (cid:0)ω(t z)(cid:1) cos (cid:0)ω(t z)(cid:1) cos (cid:0)ω(t z)(cid:1) 1 + h+ cos (cid:0)ω(t z)(cid:1) 0 0 0 0 0 . (101) The corresponding line-element in the linearized gravity setting reads: ds2 = dt2 + (cid:2)1 + h+ cos(ω(t z)(cid:1)(cid:3) dx2 + (cid:2)1 h+ cos(ω(t z)(cid:1)(cid:3) dy2 + 2h cos(ω(t z)(cid:1) dx dy (102) Spin-weighted spherical harmonics (SWSH) metric representation. We start from the decomposition of the complex gravitational wave strain with the spherical harmonic basis-set expansion (Newman & Penrose, 1962). With the expansion in mode weights hlm(t, r), one can ignore (remove) the angular dependence: h(t, r, θ, ϕ) = h+(t, r, θ, ϕ) ih(t, r, θ, ϕ) = (cid:88) ℓ (cid:88) ℓ=s mℓ hℓm(t) 2Yℓm(θ, ϕ) , (103) 52 where, h(t, r, θ, ϕ) = h+(t, r, θ, ϕ) ih(t, r, θ, ϕ) is the complex strain. Thus, for each orbital and azimuthal indices (ℓ, m), one can extract the mode hℓm(t) at fixed radius r, one uses the orthogonality of the spin-weighted spherical harmonics (SWSHs) elements: hℓm(t) = (cid:90) 2π (cid:90) π 0 0 h(t, r, θ, ϕ) 2 Yℓm(θ, ϕ) dΩ (104) where, dΩ = sin θ dθ dϕ is the spherical volume element and 2 Yℓm denotes the complex conjugate of the = 2 = 2 where (n = 4 for Ψ4) spin-weighted spherical harmonics. One typically carries out the integral over the 2-sphere S2 numerically on finite angular grid. The general formula for SWSHs is: sYℓm(θ, ϕ) = (1)l+ms (cid:20) 2ℓ + 1 4π (ℓ + m)!(ℓ m)! (ℓ + s)!(ℓ s)! (cid:19)(cid:18) ℓ + (cid:21)1/2 sin2ℓ (cid:19) (cid:18) θ 2 eimϕ (105) ℓs (cid:88) (1)r r=0 (cid:18)ℓ (cid:19) cot2r+sm (cid:19) . (cid:18) θ 2 + where the parameters ℓ, are the familiar Laplace spherical harmonics (orbital-angular momentum and azimuthal indices), while is the additional spin-weight introduced by some underlying gauge group such as (1). We especially plot for the integers = 2, = = 2, since they are relevant for GWs and are depicted in Figure 14. Figure 14: Spin weighted spherical harmonics for = 2, and = 2 for l. The dominant contributions for the Weyl scalar Ψ4 and the associated metric coefficients in the spherical harmonic basis h2,2(t) are shown. B.4 MINKOWKSI METRIC B.4.1 COORDINATE SYSTEMS FOR MINKOWSKI METRIC The flat Minkowski metric, which is spacetime that has no curvature (M 0) can be expressed in other coordinate systems as well. Spherical polar coordinates. scribed by the quadratic line element, In spherical coordinates (t, r, ϑ, φ), the Minkowski metric is deds2 = c2dt2 + dr2 + r2(dθ2 + sin2dϕ2) (106) here, R+, θ (0, π), and, ϕ [0, 2π) are the usual spherical polar coordinates. Thus, the metric tensor describing the Schwarzschild solution reads: ηSph µν = 1 0 0 0 0 0 1 0 0 r2 0 0 0 0 0 r2sin2θ (107) Boyer-Lindquist coordinates. Setting 0 in the Boyer-Lindquist form of the Kerr metric (94), the corresponding line element reduces to an unfamiliar oblate-spheroidal\" represtation: ds2 = dt2 + r2 + a2cos2ϑ r2 + dr2 + (r2 + a2cos2ϑ) dϑ2 + (r2 + a2) sin2ϑ dϕ2 , (108) 53 and the components of the 0 0 r2+a2cos2ϑ 0 r2+a2 0 0 (r2 + a2) sin2ϑ 0 The usual Cartesian coordinates can be related to the oblate spheroid ones via: 0 0 r2 + a2cos2ϑ 1 0 0 0 ηBL µν = . (109) (cid:112) = r2 + a2 sin ϑ cos ϕ (cid:112) = = cos ϑ . r2 + a2 sin ϑ sin ϕ Eddington-Finkelstein coordinates. The Minkowski metric can be written in the ingoing (advanced) Eddington-Finkelstein form in two different cases, namely for the non-rotating and the rotating case. i) non-rotating, 0 (Schwarzschild): : ds2 = dv2 + 2 dv dr + r2(dθ2 + sin2ϑ dϕ2) , and the metric tensor reads: ηEF µν = 1 1 0 0 0 1 0 0 0 r2 0 0 . 0 0 0 r2 sin2θ ii) rotating: > 0 (Kerr): ds2 = (dv + sin2ϑdϕ)2 + 2(dv + sin2ϑdϕ)(dr + asin2ϑdϕ) + (r2 + a2 cos2ϑ)(dθ2 + sin2dϕ2) , and the metric tensor reads: ηEF µν = 1 1 0 0 1 0 0 0 0 Σ asin2ϑ 0 0 asin2ϑ 0 (r2 + a2) sin2ϑ . (110) (111) (112) B.5 TRAINING ON NON-TRIVIAL METRIC FIELDS (DISTORTIONS) Distortion part of Schwarzschild geometry in spherical coordinates: Spherical coordinates: obtained by subtracting (107) from (85): 0 0 0 0 rs (r rs) 0 µν ηSph gSph µν = rs 0 0 0 0 0 0 0 0 . (113) Kerr-Schild coordinates obtained by subtracting ηµν = diag(1, +1, +1, +1) from (89): gKS µν ηµν = 2M 2M r2 2M r2 2M 2M r2 2M x2 r3 2M xy r3 2M xz r3 2M r2 2M xy r3 2M y2 r3 2M yz r3 2M r2 2M xz r3 2M yz r3 2M z2 r3 . (114) Ingoing Eddington-Finkelstein coordinates obtained by subtracting (110) from (92): µν ηEF gEF µν = rs 0 0 0 0 0 0 0 0 0 0 0 0 0 . (115) Distortion part of Kerr geometry: Boyer-Lindquist coordinates: obtained by subtracting (109) from (95): µν ηBL gBL µν = 2M Σ 0 0 2M ar sin2 θ Σ 0 2M Σ 0 0 0 0 0 2M ar sin2 θ Σ 0 0 2M a2r sin4 ϑ Σ . (116) Kerr-Schild coordinates: obtained by subtracting η = diag (1, +1, +1, +1) from Equation (96): Eddington-Finkelstein coordinates: obtained by subtracting (112) from (98): µν ηEF gEF µν = 2M Σ 0 0 2M ar sin2 θ Σ 0 0 0 0 0 0 2M ar sin2 θ Σ 0 0 2M a2r sin4 ϑ Σ . (117) 55 FINITE-DIFFERENCE METHOD (FDM) FOR TENSOR DIFFERENTIATION The main concept in this appendix section details numerical differentiation methods for tensor-valued quantities, focusing on the practical use of higher-order finite-difference schemes (in particular, sixth-order stencils). We outline the treatment of discretization errors and the use of neighboring grid collocation points as part of numerical tensor calculus toolbox. To compare the performance against automatic differentiation on tensor fields defined on the four dimensional spacetime, throughout the paper we opt for the highly accurate sixth-order order forward difference (n = 6 accuracy). This scheme queries six neighboring points per evaluation and the general formula of the differential operators are given by: 15 2h (x + 2h) + (x + h) (x + 3h) 49 20h (x) + 6 (cid:2)xif (x)(cid:3) 15 4h (x + 4h) + 6 5h (x + 5h) (118) Here, = (x1, ..., xd) Rd and the = hei, s.t. ei = (0, .., , .., 0), depending with respect to the variable xi the partial derivative is performed over. Thus, this is accurate upto O(h6), and the truncation error occurs at seventh-order. 20 3h (x + 6h) + O(h7) 1 6h i-th index (cid:124)(cid:123)(cid:122)(cid:125) In general, for an n-th order finite-difference approximation, the stencil is constructed by querying neighboring collocation points on the voxel grid. The resulting truncation error on function evaluated on the grid (gridfunctions) scales as follows (Ruchlin et al., 2018): FD[f ] = O(cid:0)hnn+1 Thus, higher order stencils enable larger step size choices since the error scales exponential to the stencil order, i.e, hn. This results in not only better accuracy, but also lesser memory consumption due to lower grid resolution. (cid:1) . Finite-difference method bottlenecks in NR Higher-order finite-difference stencils require collection of padded grid points exterior to the cube as boundary handling. These are often called ghost cells (zones). For e.g., if ng ghost cells are required for the n-th order forward difference stencil, the grid size increases of 3D spatial voxel grid (Ruchlin et al., 2018) NxNyNz (Nx+ng)(Ny+ng)(Nz+ng). Sensitive to numerical noise especially for tensor-valued functions defined on multidimensional voxel grids. Comparing AD vs FD based methods. We quantify the performance of automatic-differentiation operations on the ground truth metric against the 6-th order finite difference stencils. We test it against the Kretschmann scalar = RαβγδRαβγδ, which is prone to errors, especially due to floating point errors accumulated in the Riemann curvature tensor: Figure 15: Absolute error Kanalytic KAD profile plotted for = 0.3 between the analytic Kretschmann scalar and the ground truth Kretschmann scalar obtained via AD implemented on the ground truth (analytic) metric."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "This appendix provides detailed experimental specifics, including: (i) gradient alignment aspects relevant to SOAP optimizers; (ii) error plots for the metric and higher-rank differential geometric quantities, with component-wise tomography where applicable; (iii) training across varied coordinate systems to illustrate the coordinate-choice flexibility of NeFs; (iv) the hyperparameter configurations employed; and (v) the hardware and software environments used for these experiments. D.1 GRADIENT ALIGNMENT Competing tasks is well-known problem in multi-objective learning (Yu et al. (2020), Liu et al. (2021), Shi et al. (2023)), the gradients of the loss functions pull the weights in different directions. In Scientific Machine Learning (SciML), lot of work emerged in analyzing and mitigating gradient conflicts in the context of PINNs (Wang et al. (2025), Lui et al. (2025), Hwang & Lim (2025)). Although Sobolev training differs from PINNs, particularly from supervision perspective, it exhibits the same problem where some loss terms dominate others. In PINNs, the training is highly dependent on first satisfying the initial/boundary conditions, which provide uniqueness to the solution. The different levels of complexity between these and the residual loss create different optimization priorities, but both losses are equally important. Similarly, Sobolev training faces analogous challenges with competing loss components. The Jacobian data serve to constrain the models derivatives, while the target function outputs determine the integration \"constant\", both components being equally valuable. However, the sources of complexity differ between these approaches. In PINNs, the primary challenge stems from determining solution through unsupervised learning on PDE losses, whereas in our Sobolev training specifically, the complexity arises from managing optimization stability in high-dimensional spaces: 16-dimensional output space, 64-dimensional Jacobian, and 256-dimensional Hessian. Moreover, this complexity is accompanied by the challenge of handling gradient imbalances. Depending on the point in spacetime, the metric or its derivatives dominate in the loss. Generally speaking, an analogy is to think gµν 1 r3 , making it clear how gradient magnitudes differ depending on the radius. r2 and σρgµν 1 ,ρgµν 1 Mitigating gradient conflicts does not necessarily result in better accuracy, but it explains possible reason why the loss does not improve further, the optimization being stuck in the local minimum of one of the objectives. The intra-step gradient alignment scores presented in Wang et al. (2025) demonstrate SOAP as far superior alternative compared to other well-established optimizers, or at least for the experiments considered in that study. To provide direct comparison using the same methodology as in Wang et al. (2025), we evaluated both ADAM and SOAP on the Cartesian KS representation of the Kerr metric, chosen as the most complex metric investigated in this work. The Sobolev training contains only two objectives: metric and Jacobian supervision. For our experimental setup, we employed an MLP architecture with 5 hidden layers, 190 hidden units per layer, and SILU activation function to compare gradient conflicts between the two optimizers. The training utilized cosine decay learning rate schedule, starting from an initial learning rate of 1E2 and decaying to final rate of 1E8 over 200 epochs. For weighting the losses, gradient normalization was used with and without an exponential moving average. As shown in Figure 15, even though Adam is providing twice as much gradient alignment score in almost all epochs, SOAPs second-order and preconditioning capabilities allow for 100x training loss improvement. 57 Figure 16: Average gradient alignment per epoch (left) and MSE loss during training for Adam and Soap optimizers (right). The shaded light color in the alignment plot represents minimum and maximum deviation compared to using an exponential moving average or not for the weights multiplying the gradients. D.2 KERR METRIC ERROR VISUALIZATION To quantify the quality of EinFields parametrized metric tensor fields for the Kerr metric with spin parameter = 0.712, we report the mean absolute error (MAE) between the ground truth and the NeF-fitted metric tensors in Figure 17. The evaluation is performed on validation grid with collocation points sampled arbitrarily within the training range but distinct from the training collocation points. Using model configured with SiLU activations, SOAP optimizer, GradNorm, and without Sobolev regularization, we observe agreement with the ground truth up to six decimal places, achieving an MAE on the order of 1E6. Figure 17: Kerr metric absolute error between ground truth (analytic) metric and the EinFields parametrized metric. The metrics are depicted in the Cartesian Kerr-Schild (KS) representation as presented in Equation (96). The 2D slice of all the metric components captured in the x-y plane at fixed = 1.4 for spin parameter value = 0.7. 12Cartesian Kerr-Schild coordinates are chosen to avoid coordinate singularities, enabling tomography over larger coordinate ranges. 58 D.3 TOMOGRAPHY: METRIC, METRIC JACOBIAN AND METRIC HESSIAN COMPONENTS The effect of introducing losses pertaining to metric Jacobian and Hessian supervision, apart from the metric loss that EinFields predominantly uses, can be quantified and visualized with the following plots below. Here, for the sake of visualization, we do tomography (2D cuts) of different metric components along particular axis for the Kerr metric in Cartesian KS coordinates (Equation (96)). The first, second, and third columns in each figure correspond to EinFields training without Sobolev supervision, EinFields (+Jac), and EinFields (+Jac + Hess) trained, respectively, for randomly sampled components of differential geometric quantities. Figure 18: 2D Tomography of Kerr Riemann tensor components in Cartesian KS representation. Figure 19: 2D Tomography of Kerr Kretschmann invariant in Cartesian KS representation. 59 Figure 20: 2D Tomography of Kerr metric Jacobian components in Cartesian KS representation. Figure 21: 2D Tomography of Kerr metric Hessian components in Cartesian KS representation. 60 Figure 22: 2D Tomography of Kerr Christoffel symbols components in Cartesian KS representation. D.4 TRAINING ON VARIED COORDINATE SYSTEMS NeFs take physical coordinates as inputs and map them directly to field values. Unlike traditional machine learning architectures that ingest abstract learned feature spaces (such as token embeddings or extracted features), INRs operate directly on the physical coordinate space, enabling them to represent continuous signals in domain-agnostic manner. In the context of GR, this implies that four-dimensional representation of metric tensor fields by an INR explicitly depends on the input coordinate system, or more generally, on the chosen frame of reference. Despite this apparent dependency, GR possesses the fundamental property of diffeomorphism covariance (see Section A.3.8), which asserts that the laws of gravitation remain invariant under smooth coordinate transformations. However, the choice of coordinate system remains an essential practical tool for simplifying the form of the metric tensor. For example, while the Schwarzschild metric is diagonal in spherical coordinates (albeit with coordinate singularity at the event horizon), transforming to Cartesian KS coordinates produces dense, off-diagonal metric representation, or, for that matter, moving to Eddington-Finkelstein coordinates, which both remove coordinate-related artifacts (see Paragraph A.3.5.2). Understanding this behavior is essential for developing robust INR-based frameworks for representing geometric quantities in numerical relativity, while respecting the underlying diffeomorphism invariance of general relativity. For the Schwarzschild case, we initiate the training by sampling query spacetime coordinates in spherical representation (t, r, θ, ϕ). These sampled collocation points are then transformed into their corresponding collocation points in Cartesian coordinates (t, x, y, z) and ingoing EddingtonFinkelstein coordinates (v, r, θ, ϕ) (see Section B.1.1 for explicit transformation details). Subsequently, EinFields outputs the metric tensors corresponding to these coordinate systems, yielding Equations ((85), (89), (92)). For the Kerr metric, which is characterized by its oblate spheroidal geometry, we sample query collocation points in the Boyer-Lindquist coordinates (t, r, ϑ, ϕ), followed by the collocation points transformed into Cartesian (t, x, y, z) and ingoing Eddington-Finkelstein coordinates (t, r, θ, ϕ). EinFields then outputs the Kerr metric tensors in these respective coordinate systems, resulting in Equations ((95), (96), (98)). This multi coordinate training strategy ensures that the neural tensor field learns consistent representations across coordinate systems while maintaining geometric and physical consistency under diffeomorphisms, facilitating generalization and stability in downstream geometric learning tasks. Metric Representation Coordinate EinFields Schwarzschild EinFields (+ Jac) EinFields (+ Jac + Hess) EinFields Kerr EinFields (+ Jac) EinFields (+ Jac + Hess) Spherical Cartesian Kerr-Schild Eddington-Finkelstein Spherical Cartesian Kerr-Schild Eddington-Finkelstein Spherical Cartesian Kerr-Schild Eddington-Finkelstein Boyer-Lindquist Cartesian Kerr-Schild Eddington-Finkelstein Boyer-Lindquist Cartesian Kerr-Schild Eddington-Finkelstein Boyer-Lindquist Cartesian Kerr-Schild Eddington-Finkelstein Rel. L2 2.26E-7 1.37E-5 9.21E-9 1.37E-7 3.00E-6 6.47E-9 1.20E-7 1.53E-6 9.08E-9 6.95E-8 4.47E-6 6.44E4.72E-8 8.83E-7 4.95E-8 4.69E-8 4.95E-7 4.72E-8 Table 9: Relative L2 error considered on grid of validation collocation points (i) EinFields, (ii) EinFields (+Jac) and, (iii) EinFields (+Jac + Hess) supervision. As described above in the text, we quantify the effect of inputs queries in varied coordinate charts and how EinFields training generalizes over these different metric (geometry) representations. 62 D.5 TRAINING HYPERPARAMETERS Table 10: Training configurations for Schwarzschild, Kerr, and GWs used in the geodesics, Kretschmann plots, Table 8, and linearized gravity section. Parameter Architecture Depth Width Activation Input dimension Output dimension # Parameters Optimizer β1 β2 Precondition frequency Learning rate schedule Initial learning rate Decay steps Final learning rate Training GWs 5 128 / 128 / 90 SiLU / SIREN / WIRE 4 16 85K Schwarzschild 3 / 3 / 5 / 7 64 / 128 / 256 / 512 SiLU 4 10 / 16 13.5K / 50K / 332K / 1.5M Kerr"
        },
        {
            "title": "MLP",
            "content": "5 190 SiLU 4 16 185K"
        },
        {
            "title": "SOAP",
            "content": "0.95 0.95 1 E-2 / E-3 104 E-5 / E-6 6 103 / 2, 4, 6 104 E-7 / E-8 4 103 / 4 104 E-9 Epochs Number of batches Gradient weighting scheme 100 100 200 30 / 100 / 200 / 300 None / GradNorm 200 20 / 200 D.6 HARDWARE For our primary computational work, we utilize high-performance CPU system equipped with 232-core Intel Xeon Platinum 8452Y+ processors, each operating at 4.1 GHz, and 2048 GiB of RAM. All NeF-related training is performed on single NVIDIA H200 SXM GPU with 144 GiB of HBM3e memory. For prototyping and preliminary experiments, we employ single NVIDIA Tesla A100 GPU with 40 GiB of memory."
        },
        {
            "title": "E LICENSES",
            "content": "This work would not have been possible without the open-source software ecosystem. Our implementation is built upon multiple community-maintained libraries, and we gratefully acknowledge their licenses below. The core computations were performed using JAX[cuda12] (Bradbury et al., 2018) with CUDA support, licensed under the Apache 2.0 License. For model definition and training, we relied on Equinox (Kidger & Garcia, 2021) and Flax.Linen (Heek et al., 2024), both also under Apache 2.0. For solving differential equations, we employed Diffrax (Kidger, 2021), distributed under the Apache 2.0 License. All libraries used are permissively licensed, enabling free academic and non-commercial research."
        }
    ],
    "affiliations": [
        "Emmi AI GmbH, Linz, Austria",
        "LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria",
        "University of Manchester, United Kingdom"
    ]
}