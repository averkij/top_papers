{
    "paper_title": "CamemBERT 2.0: A Smarter French Language Model Aged to Perfection",
    "authors": [
        "Wissam Antoun",
        "Francis Kulumba",
        "Rian Touchent",
        "Éric de la Clergerie",
        "Benoît Sagot",
        "Djamé Seddah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to a decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on a significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across a range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on Huggingface."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 8 6 8 8 0 . 1 1 4 2 : r CamemBERT 2.0: Smarter French Language Model Aged to Perfection"
        },
        {
            "title": "Wissam Antoun",
            "content": "Francis Kulumba Rian Touchent Éric de la Clergerie Benoît Sagot Djamé Seddah Inria, Paris, France {firstname,lastname}@inria.fr"
        },
        {
            "title": "Abstract",
            "content": "French language models, such as CamemBERT, have been widely adopted across industries for natural language processing (NLP) tasks, with models like CamemBERT seeing over 4 million downloads per month. However, these models face challenges due to temporal concept drift, where outdated training data leads to decline in performance, especially when encountering new topics and terminology. This issue emphasizes the need for updated models that reflect current linguistic trends. In this paper, we introduce two new versions of the CamemBERT base modelCamemBERTav2 and CamemBERTv2designed to address these challenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use of the Replaced Token Detection (RTD) objective for better contextual understanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked Language Modeling (MLM) objective. Both models are trained on significantly larger and more recent dataset with longer context length and an updated tokenizer that enhances tokenization performance for French. We evaluate the performance of these models on both general-domain NLP tasks and domain-specific applications, such as medical field tasks, demonstrating their versatility and effectiveness across range of use cases. Our results show that these updated models vastly outperform their predecessors, making them valuable tools for modern NLP systems. All our new models, as well as intermediate checkpoints, are made openly available on https://huggingface.co/ Huggingface almanach?search_models=camembert+v2."
        },
        {
            "title": "Introduction",
            "content": "In recent years, French language models such as CamemBERT (Martin et al., 2020) have become integral to businesses leveraging natural language processing (NLP) to boost productivity and efficiency. Since its release, CamemBERT has gained widespread adoption, receiving over 4 million downloads each month, and continues to be actively used by the NLP community. notable example is ENEDIS, which used CamemBERT to automate the dispatch of 100,000 customer requests per day across 1,500 operators, significantly reducing manual workload and achieving return on investment of approximately C3M per year (Gemignani et al., 2023; Akani et al., 2023). However, significant challenge that models like CamemBERT, developed in 2019, face is temporal concept drift (Loureiro et al., 2022; Agarwal and Nenkova, 2022; Jin et al., 2022) This phenomenon occurs when the data model was originally trained leading to decline in on becomes outdated, performance as new topics, events, and terminology emerge. For instance, when CamemBERT was trained, discussions around COVID-19, public health restrictions, and associated changes in language usage were not present in the training data. As result, models like CamemBERT, which have not been updated, struggle to understand or generate accurate responses to these newer concepts. Temporal drift impacts their ability to remain relevant in evolving real-world applications, highlighting the need for continuous updates to keep the models aligned with current linguistic and contextual trends. Given these challenges, it is essential to develop and deploy updated encoder models that can better serve modern NLP applications. In response, we aim to provide state-of-the-art models and fine-tuned versions for range of common NLP tasks, including Named Entity Recognition (NER), Question Answering (QA), Natural Language Inference (NLI), and Part-of-Speech (POS) tagging. These updated models will play major role in creating fast, efficient, and reliable AI systems. In this paper, we present two new versions of the CamemBERT base model: CamemBERTav2 and CamemBERTv2. CamemBERTav2 is built on the DeBERTaV3 (He et al., 2021a) architecture as an update to the CamemBERTa model (Antoun et al., 2023), using the Replaced Token Detection (RTD) training objective for enhanced context and positional representation, while CamemBERTv2 is based on the RoBERTa (Liu, 2019) architecture, trained using the Masked Language Modeling (MLM) objective. Both models benefit from training on much larger and more recent dataset, coupled with an updated tokenizer designed to better capture the nuances of the French language, and to support modern token requirements by adding new lines, tabulation and emojies to the vocabulary. This ensures improved tokenization performance across various NLP tasks. To evaluate the performance of these new models, we conducted extensive tests on both general-domain NLP tasks and domain-specific such as those in the medical applications, field. This dual approach demonstrates the versatility of our models, highlighting their capacity to excel in diverse use cases, including highly specialized areas where domain-specific knowledge is necessary. The contributions of this paper are as follows: We present CamemBERTav2 and CamemBERTv2 and trained on larger, up-to-date dataset with an enhanced tokenizer to better capture the complexities of the French language. We both the models evaluate and NLP general-domain domain-specific applications, particularly in the medical field, to demonstrate their robustness and versatility. on tasks We are releasing all model artifacts, including intermediate checkpoints and fine-tuned models, enabling the community to further experiment, fine-tune, and deploy these models across various applications.1 These contributions aim to advance French language modeling and provide the community with state-of-the-art tools for diverse NLP tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Pre-trained French Language Models French language models have predominantly been 1https://huggingface.co/almanach?search_ models=camembert+v2 trained using either Masked Language Modeling (MLM) or Causal Language Modeling (CLM) techniques. leading French Among the models, CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) have gained considerable traction, both relying on the MLM pre-training method. Other noteworthy models include FrALBERT (Cattan et al., 2021), French adaptation of ALBERT (Lan et al., 2020), and LePetit (Micheli et al., 2020), scaled-down version of CamemBERT. In addition, DAlemBERT (Gabay et al., 2022), derived from RoBERTa (Liu et al., 2020), is tailored for Early Modern French. BARThez (Kamal Eddine et al., 2021) is sequence-to-sequence model built on BARTs architecture (Lewis et al., 2020), while PAGnol (Launay et al., 2022) and Cedille (Müller and Laurent, 2022) represent models trained with CLM. Recently, CamemBERTa, French DeBERTa model based on the DeBERTaV3 architecture, has been introduced. CamemBERTa demonstrates superior performance on various French NLP tasks compared to traditional BERT-based models, despite using significantly fewer training tokens (Antoun et al., 2023). Additionally, CamemBERT-bio, specialized model fine-tuned for French biomedical data, has shown substantial improvements in named entity recognition tasks within the biomedical domain, addressing the limitations of general-purpose models like CamemBERT when applied to specialized texts (Touchent and de la Clergerie, 2024). DrBERT further extends this focus by pre-training on both public and private medical data, offering specialized performance for the French biomedical field (Labrak et al., 2023). Finally, CharacterBERT-French, character-based variant of BERT, offers robustness to noise by utilizing character embeddings instead of subword-based vocabularies. This model has shown promise, particularly in handling noisy experimental data (Riabi et al., 2021)."
        },
        {
            "title": "3 CamemBERT 2.0",
            "content": "In this paper, we introduce two updated versions of the CamemBERT base model, named CamemBERTav2 and CamemBERTv2, original improve developed to CamemBERTa and (Antoun et CamemBERT (Martin et al., 2020) models the al., 2023) upon respectively. CamemBERTav2 is built on the DeBERTaV3 (He et al., 2021a,b) architecture and the Replaced Token Detection (RTD) (Clark et al., 2020) training objective, leveraging its improved attention mechanism for better context and positional representation. CamemBERTv2, on the other hand, is based on RoBERTa (Liu, 2019), using the Masked Language Modeling (MLM) objective for training, and is meant to be drop-in replacement in task where computing having the pseudo-language modeling probability is needed. The models were trained on much larger and up-to-date dataset, accompanied by an updated tokenizer that better captures the linguistic complexities of the French language, ensuring improved tokenization performance for various downstream tasks."
        },
        {
            "title": "3.1 Pre-Training Dataset",
            "content": "Our new pre-training dataset is sourced mainly from the French subset of the CulturaX corpus (Nguyen et al., 2023). CulturaX is multilingual dataset that combines mC4 (Xue et al., 2021) and four OSCAR (Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022) snapshots.2 The documents are then deduplicated on the document level and filtered using language filters, URL block lists, and comprehensive set of metric-based filters (e.g. stopword ratio, perplexity score, word repetition ratio...). In addition, we make use of the French section of Wikipedia3, and French scientific papers and theses from the HALvesting corpus (Kulumba et al., 2024). In total, we gather 265B tokens from Culturax, 4.7B tokens from HALvesting, and 0.5B tokens from Wikipedia. During training, we upsample Wikipedia 10 times, and hence our final pre-training dataset has 275B tokens, compared to 32B which were used during the original CamemBERT and CamemBERTa training."
        },
        {
            "title": "3.2 Tokenizer",
            "content": "A key improvement in the CamemBERTv2 models is the development of an updated tokenizer. The primary goal was to improve tokenization efficiency by addressing the limitations of the previous version. This includes the introduction of newline and tab characters, as well as support for emojis, which are normalized by 2Culturax contains the following OSCAR corpora 20.19, 21.09, 22.01, and 23.01, and the version 3.1.0 of mC4 3We use the April 2024 dump tokens. removing zero-width joiner characters and splitting emoji sequences into individual To improve the handling of numerical data, we opted to split numbers into maximum of tokens, which we hypothesize will two-digit enhance the models ability to process dates and perform simple arithmetic tasksfunctions more commonly utilized in encoder models than in generative ones. Additionally, French and English elisions (e.g., l, lorsqu) are now treated as single tokens, including the apostrophe. We adopted the WordPiece tokenization algorithm (Devlin et al., 2019), which allows for flexible vocabulary adjustments and the easy addition of new tokens. The vocabulary size was set to 32,768, with around 400 tokens reserved for future expansion to maintain multiple of 8. We finally train the tokenizer on subsample of our pre-training dataset that include subsample of CulturaX and full French Wikipedia and HAL."
        },
        {
            "title": "3.3 Pre-Training Methodology",
            "content": "The pre-training process for both CamemBERTv2 and CamemBERTav2 models was done in two stages. Initially, both models were trained with sequence length of 512 tokens, which allowed for faster convergence during the early stages of training. In the second stage, the models were further pre-trained using sequence length of 1024 tokens to fully capture long-range dependencies and improve performance on tasks requiring extensive context. To create pre-training dataset for the long sequence training, we further filter our pretraining corpus to have only long documents, while also including short sequences with 5% chance to ensure the model retains the ability to correctly handle shorter sequences. Additionally, it was shown by (Antoun et al., 2023) that models trained with MLM require multiple epochs of pre-training to achieve optimal accuracy, due to the Masked Language modeling objective only being able to propagate the loss from the masked tokens. Hence, we train CamemBERTv2 for three epochs over our dataset. We set the token masking rate to 40%, which was found to be optimal by Wettig et al. (2023). In contrast, CamemBERTav2, being based on the more sample-efficient DeBERTaV3 pertaining methodology of replaced-token detection, reaches peak performance after just one epoch, making it significantly more efficient in terms of training time and computational resources. Details about MODEL GSD RHAPSODIE SEQUOIA FSMB FTB-NER UPOS LAS UPOS LAS UPOS LAS UPOS LAS F1 CamemBERT CamemBERTa 98.570.07 98.550.05 94.350.15 94.380.15 97.620.08 97.520.14 84.290.56 84.230.08 99.350.09 99.440.07 94.780.12 94.850. 94.800.16 94.800.09 81.340.63 80.740.25 89.970.50 90.330.54 CamemBERTv2 CamemBERTav2 98.600.05 98.540.03 94.180.12 94.350. 97.620.10 97.700.21 84.090.31 84.300.27 99.370.04 99.420.05 94.800.14 94.610.25 95.050.18 95.190.11 81.490.38 81.320. 91.990.96 93.400.62 Table 1: POS tagging, dependency parsing and NER results on the test sets of our French datasets. UPOS (Universal Part-of-Speech) refers here to POS tagging accuracy, and LAS measures the overall accuracy of labeled dependencies in parsed sentence. pre-training hyperparameters are available in Table"
        },
        {
            "title": "4.1 Downstream Evaluation",
            "content": "General Domain To evaluate our models we consider range of French downstream tasks and datasets, including Question Answering (QA) using FQuAD 1.0 (dHoffschmidt et al., 2020), Part-Of-Speech (POS) tagging and Dependency Parsing on GSD (McDonald et al., 2013), Rhapsodie (Lacheret et al., 2014), Sequoia (Candito and Seddah, 2012; Candito et al., 2014) in their UD v2.2 formats, and the French Social Media Bank (Seddah et al., 2012). We also assess Named Entity Recognition (NER) on the 2008 FTB version (Abeillé et al., 2000; Candito and Crabbé, 2009) with NER annotations by Sagot et al. (2012). To assess text classification capabilities we evaluate our models on the FLUE benchmark (Le et al., 2019). We re-used the same splits from Antoun et al. (2023), and performed hyper-parameter tuning on all models and datasets with 5 seed variations, except the dependency parsing and part-of-speech tasks where we only validate over 5 seeds using the same sets of parameters. subset Domain Specific To the models assess include the tasks, we on domain-specific French pseudoanonymized the of dataset for radicalization detection with NER annotations (Riabi et al., 2024) which we refer to as Counter-NER. For biomedical-domain datasets, we evaluate five distinct tasks: EMEA, MEDLINE, CAS1, CAS2, and E3C. EMEA and MEDLINE are part of the QUAERO corpus (Névéol et al., 2014), where EMEA consists of drug leaflets and MEDLINE includes scientific article titles, both annotated with 10 semantic groups from the UMLS. CAS1 and CAS2 are based on the CAS corpus (Grouin et al., 2019), focusing on pathology and symptoms in the first subtask, while the second subtask involves extracting additional clinical information such as anatomy and treatment. Finally, E3C (Magnini et al., 2020) focuses on clinical cases from scientific articles, using fully annotated texts to identify clinical entities. For consistency, we adopt the dataset splits and hyper-parameters proposed by Touchent and de la Clergerie (2024) for comparison with his model."
        },
        {
            "title": "4.2 General Domain Results",
            "content": "For general domain tasks, the results show clear performance trends between models: POS Tagging and Dependency Parsing: As shown in Table 1, all models performed well on Universal POS (UPOS) tagging and dependency parsing, where the updated CamemBERTv2 and CamemBERTav2 model maintaining the previous models scores. These results indicate possible saturation in the benchmark scores for current encoder-based transformer models. Named Entity Recognition (NER): In general domain NER, evaluated on the FTB dataset, CamemBERTaV2 outperformed all other models with an F1 score 93.4% showing significant improvement over the baseline CamemBERT model (89.97%), while also improving over the MLM-trained CamemBERTv2 model. Question Answering (QA): For the FQuAD 1.0 dataset  (Table 2)  , CamemBERTav2 achieved the highest F1 score (83.04%) and exact match (EM) score (64.29%), outperforming the other models by significant margin. The performance gap between CamemBERTv2 and CamemBERTav2 (80.39% vs 83.04%) suggests that the latters enhanced pre-training loss and architecture yielded more robust representations for machine comprehension tasks in French. Model EM CamemBERT CamemBERTa 80.980.48 81.150.38 62.510.54 62.010.45 CamemBERTv2 CamemBERTav2 80.390.36 83.040. 61.350.39 64.290.31 enhancements made in these newer versions. These gains showcase the models ability to generalize to challenging, niche domains with specialized vocabularies. Table 2: Question Answering results on FQuAD 1.0. Text Classification: Table 3 presents text classification results across the CLS, PAWS-X, and XNLI tasks from the FLUE benchmark. CamemBERTav2 consistently outperformed other models, achieving top scores in all tasks, with the highest accuracy on the CLS task (95.63%), PAWS-X (93.06%), and XNLI (84.82%). The massive increase in CamemBERTav2s XNLI scores compared to the previous CamemBERTa model shows that small transformer-based models, that use the sample-efficient RTD objective, can still benefit from increasing the unique token count during pretraining. Model CLS PAWS-X XNLI CamemBERT CamemBERTa 94.620.04 94.920.13 91.360.38 91.670.17 81.950.51 82.000. CamemBERTv2 CamemBERTav2 95.070.11 95.630.16 92.000.24 93.060.45 81.750.62 84.820.54 Table 3: Text classification results (Accuracy) on the FLUE benchmark."
        },
        {
            "title": "4.3 Domain Specific Results",
            "content": "In the evaluation of domain-specific tasks, Table 4, particularly in the medical fields, both CamemBERTv2 and CamemBERTav2 exhibited strong performance. On medical NER tasks, the new models were able to achieve results comparable to domain-specific models, namely CamemBERT-bio, showcasing their ability to handle specialized terminologies and complex entity recognition. Notably, CamemBERTv2 and CamemBERTav2 significantly outperformed their predecessors across all tasks, largely due to the inclusion of scientific and medical articles in their updated pre-training datasets. In the radicalization NER task, which involves identifying sensitive and domain-specific entities, both models demonstrated large improvements. original surpassed CamemBERTav2 CamemBERT model by 2 percentage points, while CamemBERTv2 exceeded CamemBERT further highlighting the by over 3 points, the Model Medical-NER Counter-NER CamemBERT CamemBERTa CamemBERT-bio CamemBERTv2 CamemBERTav2 70.960.13 71.860.11 73.960.12 72.770.11 73.980.11 84.181.23 87.370.73 - 87.460.62 89.530.73 Summary of NER F1 scores on the Table 4: domain-specific downstream tasks. Full scores are available in Table 5."
        },
        {
            "title": "4.4 Discussion",
            "content": "results from our experiments The clearly demonstrate the significant advancements that CamemBERTv2 and CamemBERTav2 bring to French NLP tasks, both in general and domain-specific contexts. In the general domain tasks, CamemBERTav2 consistently outperformed its predecessors, showcasing the effectiveness of the DeBERTaV3 architecture and the RTD objective in handling both contextual and positional representations. The improvements seen in tasks such as NER, QA, and text classification are particularly noteworthy. For example, in the FQuAD 1.0 dataset, the large performance gap between CamemBERTv2 and CamemBERTav2 illustrates the robustness of the latter in understanding complex queries and extracting relevant answers. The enhanced tokenizer, with its improved handling of French-specific linguistic features and expanded vocabulary, also played key role in these improvements. that the marginal gains over Interestingly, while the models achieved high accuracy in POS tagging and dependency parsing tasks, the original transformer-based CamemBERT suggest models may be approaching performance ceilings on these specific benchmarks. This observation indicates that future progress in these areas might require new approaches, such as task-specific architectures or training methodologies, rather than further refinements to existing models. In domain-specific tasks, the inclusion of scientific and medical articles in the pre-training dataset allowed both CamemBERTv2 and CamemBERTav2 to achieve strong results across specialized fields. Their ability to generalize to biomedical NER tasks, where they performed comparably to models specifically designed for the medical domain, shows the versatility of our updated models. The sizable improvements in the radicalization NER task also reflect the enhanced knowledge embedded in the new models, which is essential for identifying sensitive and rare entities within challenging domains. These results affirm the value of continual model updates, particularly in addressing the issue of temporal concept drift. As language evolves and new terminologies emerge, updating models with more recent datasets and architectures becomes crucial for maintaining their relevance and utility in real-world applications. Our decision to update the tokenizer to better handle modern language elements like emojis and numerical data further reinforces this point, allowing the models to stay aligned with contemporary communication patterns."
        },
        {
            "title": "5 Conclusion",
            "content": "a In conclusion, the development of CamemBERTv2 and CamemBERTav2 marks significant in French language modeling, advancement demonstrating improved performance across variety of general and domain-specific NLP tasks. By leveraging larger and more recent datasets, alongside an updated tokenizer, these models have shown enhanced versatility and robustness, particularly in tasks like NER, QA, and text classification. However, the marginal improvements seen in certain tasks like POS tagging and dependency parsing suggest that these benchmarks may be nearing saturation for current transformer-based models. issue. is also dataset Looking ahead, future work should not only focus on refining model architectures and training objectives but also prioritize updating datasets. Temporal concept drift is not solely model issueit Many benchmarks currently in use do not reflect the latest linguistic distributions, which can exacerbate the performance gap between models trained on outdated versus modern data. Ensuring that datasets are regularly updated to include contemporary topics, terminologies, and language use is essential for keeping models relevant and maximizing their real-world applicability. Such efforts will ensure that both models and benchmarks evolve together, addressing temporal drift more effectively and pushing the boundaries of what these systems can achieve."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partly funded by Benoît Sagots chair in the PRAIRIE institute funded by the French national reseach agency (ANR as part of the Investissements davenir programme under the reference ANR-19-P3IA-0001. This work also received funding from the European Unions Horizon 2020 research and innovation program under grant agreement No. 101021607. The authors are grateful to the OPAL infrastructure for providing from Université Côte dAzur resources and support. This work was also granted access by GENCI to the HPC resources of IDRIS under the allocation 2024-GC011015610. Finally, part of this work was funded by the DINUM through the AllIAnce program. We would also like to thank Nathan Godey, and Arij Riabi for the productive discussions."
        },
        {
            "title": "References",
            "content": "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, Towards cleaner and Benoît Sagot. 2022. document-oriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 43444355, Marseille, France. European Language Resources Association. Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2021. Ungoliant: An optimized pipeline for the generation of very large-scale multilingual web corpus. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pages 1 9, Mannheim. Leibniz-Institut für Deutsche Sprache. Building treebank for French. Anne Abeillé, Lionel Clément, and Alexandra Kinyon. 2000. In Proceedings of the Second International Conference on Language Resources and Evaluation (LREC00), Athens, Greece. European Language Resources Association (ELRA). Oshin Agarwal and Ani Nenkova. 2022. Temporal effects on pre-trained models for language processing the Association for Transactions of tasks. Computational Linguistics, 10:904921. E. Akani, R. Gemignani, and R. Abrougui. 2023. Enebert: state-of-the-art language model trained on corpus of texts generated from the set of dso In 27th International Conference on activities. Electricity Distribution (CIRED 2023), volume 2023, pages 29032907. Conference, pages 33673374, Marseille, France. European Language Resources Association. Wissam Antoun, Benoît Sagot, and Djamé Seddah. 2023. Data-efficient french language modeling with In Findings of the Association for camemberta. Computational Linguistics: ACL 2023, Toronto, Canada. Association for Computational Linguistics. Marie Candito and Benoît Crabbé. 2009. Improving generative statistical parsing with semi-supervised the 11th word clustering. International Conference on Parsing Technologies (IWPT09), France. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "138141, pages Paris, Marie Candito, Guy Perrier, Bruno Guillaume, Corentin Ribeyre, Karën Fort, Djamé Seddah, and Eric De La Clergerie. 2014. Deep syntax annotation of the sequoia french treebank. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), Reykjavik, Iceland. European Language Resources Association (ELRA). Marie Candito and Djamé Seddah. 2012. Le corpus sequoia : annotation syntaxique et exploitation pour ladaptation danalyseur par pont lexical (the sequoia corpus : Syntactic annotation and use for parser lexical domain adaptation method) [in In Proceedings of the Joint Conference French]. JEP-TALN-RECITAL 2012, volume 2: TALN, pages 321334, Grenoble, France. ATALA/AFCP. Oralie Cattan, Christophe Servan, and Sophie Rosset. 2021. On the Usability of Transformers-based models for French Question-Answering task. In Recent Advances in Natural Language Processing (RANLP), Varna, Bulgaria. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training transformers for language of deep bidirectional the 2019 understanding. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Martin dHoffschmidt, Maxime Vidal, Wacim Belblidia, FQuAD: French arXiv e-prints, and Tom Brendlé. 2020. Question Answering Dataset. arXiv:2002.06071. Simon Gabay, Pedro Ortiz Suarez, Alexandre Bartz, Alix Chagué, Rachel Bawden, Philippe Gambette, and Benoît Sagot. 2022. From FreEM to dAlemBERT: large corpus and language model In Proceedings of the for early Modern French. Thirteenth Language Resources and Evaluation R. Gemignani, E. Akani, J.P. Delrieux, and A. Sayouti Souleymane. 2023. Hape: optimizing customer relation by automatic task distribution using constrained optimization and natural language In 27th International Conference on processing. Electricity Distribution (CIRED 2023), volume 2023, pages 17641768. Cyril Grouin, Natalia Grabar, Vincent Claveau, and Thierry Hamon. 2019. Clinical case reports for NLP. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 273282, Florence, Italy. Association for Computational Linguistics. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. Preprint, arXiv:2111.09543. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021b. Deberta: Decoding-enhanced In International bert with disentangled attention. Conference on Learning Representations. Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2022. Lifelong pretraining: Continually adapting language models to emerging corpora. In Proceedings of BigScience Episode #5 Workshop on Challenges & Perspectives in Creating Large Language Models, pages 116, virtual+Dublin. Association for Computational Linguistics. Moussa Kamal Eddine, Antoine Tixier, and Michalis Vazirgiannis. 2021. BARThez: skilled pretrained French sequence-to-sequence model. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 93699390, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Francis Kulumba, Wissam Antoun, Guillaume Vimont, and Laurent Romary. 2024. Harvesting textual and structured data from the hal publication repository. Preprint, arXiv:2407.20595. Yanis Labrak, Adrien Bazoge, Richard Dufour, Mickael Rouvier, Emmanuel Morin, Béatrice Daille, and Pierre-Antoine Gourraud. 2023. DrBERT: robust pre-trained model in French for biomedical and the 61st clinical domains. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1620716221, Toronto, Canada. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Anne Lacheret, Sylvain Kahane, Julie Beliao, Anne Dister, Kim Gerdes, Jean-Philippe Goldman, Nicolas Obin, Paola Pietrandrea, and Atanas Tchobanov. 2014. Rhapsodie: un Treebank annoté pour létude de linterface syntaxe-prosodie en français parlé. In 4e Congrès Mondial de Linguistique Française, volume 8, pages 26752689, Berlin, Germany. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: lite bert for self-supervised learning In International of language representations. Conference on Learning Representations. Julien Launay, E.l. Tommasone, Baptiste Pannier, François Boniface, Amélie Chatelain, Alessandro Cappelli, Iacopo Poli, and Djamé Seddah. 2022. PAGnol: An extra-large French generative model. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 42754284, Marseille, France. European Language Resources Association. Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoit Crabbé, Laurent Besacier, and Didier Schwab. 2020. FlauBERT: Unsupervised In language model pre-training for French. Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 24792490, Marseille, France. European Language Resources Association. Hang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoît Crabbé, Laurent Besacier, and Didier Schwab. 2019. Flaubert: Unsupervised language model pre-training for french. Preprint, arXiv:1912.05372. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual the Association for Computational Meeting of Linguistics, pages 78717880, Online. Association for Computational Linguistics. Yinhan Liu. 2019. Roberta: robustly optimized arXiv preprint bert pretraining approach. arXiv:1907.11692. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Ro{bert}a: robustly optimized {bert} pretraining approach. Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and Jose Camacho-collados. 2022. TimeLMs: Diachronic language models from Twitter. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 251260, Dublin, Ireland. Association for Computational Linguistics. Bernardo Magnini, Begoña Altuna, Alberto Lavelli, Manuela Speranza, and Roberto Zanoli. 2020. The e3c project: Collection and annotation of multilingual corpus of clinical cases. Proceedings of the Seventh Italian Conference on Computational Linguistics CLiC-it 2020. Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric de la Clergerie, Djamé Seddah, and Benoît Sagot. 2020. CamemBERT: tasty French language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 72037219, Online. Association for Computational Linguistics. Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini, Núria Bertomeu Castelló, and Jungmee Lee. 2013. Universal Dependency annotation for multilingual parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 9297, Sofia, Bulgaria. Association for Computational Linguistics. Vincent Micheli, Martin dHoffschmidt, and François Fleuret. 2020. On the importance of pre-training data volume for compact language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages for Computational Linguistics. 78537858, Online. Association Martin Müller and Florian Laurent. 2022. Cedille: large autoregressive french language model. Preprint, arXiv:2202.03371. Aurélie Névéol, Cyril Grouin, Jeremy Leixa, Sophie Rosset, and Pierre Zweigenbaum. 2014. The quaero french medical corpus: ressource for medical entity recognition and normalization. In Bio text-mining workshop (BioTextM 2014), page 7p, Reykjavik, Iceland. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023. Culturax: cleaned, enormous, and multilingual dataset for large language models in 167 languages. Preprint, arXiv:2309.09400. Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Asynchronous pipelines for Romary. 2019. processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 16, Mannheim. Leibniz-Institut für Deutsche Sprache. Arij Riabi, Menel Mahamdi, Virginie Mouilleron, and Djamé Seddah. 2024. Cloaked classifiers: sensitive Pseudonymization the classification tasks. Fifth Workshop on Privacy in Natural Language Processing, pages 123136, Bangkok, Thailand. Association for Computational Linguistics. on In Proceedings of strategies Arij Riabi, Benoît Sagot, and Djamé Seddah. 2021. Can character-based language models improve downstream task performances in low-resource and In Proceedings of the noisy language scenarios? Seventh Workshop on Noisy User-generated Text (W-NUT 2021), pages 423436, Online. Association for Computational Linguistics. Benoît Sagot, Marion Richard, and Rosa Stern. 2012. Annotation référentielle du corpus arboré de Paris 7 en entités nommées (referential named entity annotation of the Paris 7 French TreeBank) [in In Proceedings of the Joint Conference French]. JEP-TALN-RECITAL 2012, volume 2: TALN, pages 535542, Grenoble, France. ATALA/AFCP. Djamé Seddah, Benoit Sagot, Marie Candito, Virginie Mouilleron, and Vanessa Combet. 2012. The French Social Media Bank: treebank of noisy user generated content. In Proceedings of COLING 2012, pages 24412458, Mumbai, India. The COLING 2012 Organizing Committee. Rian Touchent and Éric de la Clergerie. 2024. CamemBERT-bio: Leveraging continual pre-training for cost-effective models on French biomedical data. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 26922701, Torino, Italia. ELRA and ICCL. Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Should you mask 15% in Danqi Chen. 2023. In Proceedings of masked language modeling? the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 29853000, Dubrovnik, Croatia. Association for Computational Linguistics. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American the Association for Computational Chapter of Linguistics: Human Language Technologies, pages 483498, Online. Association for Computational Linguistics."
        },
        {
            "title": "A Full Results",
            "content": "A.1 Full Domain Specific NER Results Dataset Model CAS1 CAS2 E3C EMEA MEDLINE Counter-NER CamemBERT CamemBERTa Dr-BERT CamemBERT-Bio CamemBERTv2 CamemBERTav2 CamemBERT CamemBERTa Dr-BERT CamemBERT-Bio CamemBERTv2 CamemBERTav2 CamemBERT CamemBERTa Dr-BERT CamemBERT-Bio CamemBERTv2 CamemBERTav CamemBERT CamemBERTa Dr-BERT CamemBERT-Bio CamemBERTv2 CamemBERTav2 CamemBERT CamemBERTa Dr-BERT CamemBERT-Bio CamemBERTv2 CamemBERTav2 CamemBERT CamemBERTa CamemBERTv2 CamemBERTav2 F1 70.721.47 71.961.38 62.761.55 72.281.46 71.181.62 72.872.29 78.431.78 79.060.68 76.430.49 82.500.56 81.870.58 81.850. 67.012.13 67.011.85 56.992.40 69.871.21 69.270.90 70.120.87 73.532.04 75.990.51 71.330.84 76.962.00 76.301.00 77.280.57 65.110.56 65.330.30 58.900.51 68.210.91 65.260.33 67.770.44 84.181.23 87.370.73 87.460.62 89.530.73 Table 5: NER F1 scores on the domain-specific downstream tasks. Hyper-parameters B.1 Pre-training Hyper-parameters Hyper-parameter CamemBERTav2base CamemBERTv2base Number of Layers Hidden size Generator Hidden size FNN inner Hidden size Attention Heads Attention Head size Dropout Warmup Steps (p1/p2) Learning Rates (p1/p2) End Learning Rates (p1/p2) Batch Size Weight Decay Max Steps (p1/p2) Learning Rate Decay Adam ϵ Adam β1 Adam β2 Gradient Clipping Masking Probability Seq. Length (p1/p2) Precision 12 768 256 3072 12 64 0.1 10k/1k 7e-4/3e-4 1e-5 8k 0.01 91k/17k Polynomial p=0.5 1e-6 0.878 0.974 1.0 20% 512/1024 BF16 12 768 - 3072 12 64 0.1 10k/1k 7e-4/3e-4 1e-5 8k 0.01 273k/17k Polynomial p=0.5 1e-6 0.878 0.974 1.0 40% 512/1024 BF Table 6: Hyper-parameters for pre-training CamemBERTa and CamemBERT 2.0. B.2 Fine-Tuning Hyper-parameters Task FQuAD Learning Rate LR Sch. Epochs Max Len. Batch Size Warmup {0,0.1} {3, 5, 7}e-5 {32,64} 1024 6 CLS {3, 5, 7}e-5 PAWS-X {3, 5, 7}e-"
        },
        {
            "title": "FTB NER",
            "content": "{3, 5, 7}e-5 XNLI POS Dep. Pars. {3, 5, 7}e-5 3e-05 3e-05 Counter-NER {3, 5, 7}e-5 Med-NER 5e-5 cosine cosine linear cosine linear cosine linear cosine linear linear cosine linear linear 6 6 8 10 64 8 3 1024 {32,64} {32,64} 0 148 192 160 1024 1024 512 {16,32} {0,0.1} 32 8 8 0.1 100 steps 100 steps {16,32} {0,0.1} 8 0.224 Table 7: Hyperparameter Search During Fine-tuning of CamemBERTv2. All models were trained with FP"
        }
    ],
    "affiliations": [
        "Inria, Paris, France"
    ]
}