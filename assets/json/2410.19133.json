{
    "paper_title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback",
    "authors": [
        "Lester James V. Miranda",
        "Yizhong Wang",
        "Yanai Elazar",
        "Sachin Kumar",
        "Valentina Pyatkin",
        "Faeze Brahman",
        "Noah A. Smith",
        "Hannaneh Hajishirzi",
        "Pradeep Dasigi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 2 3 3 1 9 1 . 0 1 4 2 : r HYBRID PREFERENCES: LEARNING TO ROUTE INSTANCES FOR HUMAN VS. AI FEEDBACK Lester James V. Miranda1 Yizhong Wang1,2 Yanai Elazar1,2 Sachin Kumar1,3 Valentina Pyatkin1,2 Faeze Brahman1 Noah A. Smith1,2 Hannaneh Hajishirzi1,2 Pradeep Dasigi 1Allen Institute for AI 2University of Washington 3The Ohio State University : {ljm,yizhongw}@allenai.org"
        },
        {
            "title": "ABSTRACT",
            "content": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given preference dataset and an evaluation metric, we train performance prediction model to predict reward models performance on an arbitrary combination of human and LM annotations and employ routing strategy that selects combination that maximizes predicted performance. We train the performance prediction model on MULTIPREF, new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future. Code Dataset https://github.com/allenai/hybrid-preferences https://hf.co/datasets/allenai/multipref"
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning from human feedback (Christiano et al., 2017) has been integral to the alignment of large language models (LMs) with human objectives and values (Ouyang et al., 2022; Bai et al., 2022a, inter alia). Central to this process are preference datasets, i.e., instances of inputs to language models paired with candidate model outputs and human judgment annotations indicating the preferred output. Collecting preference data involves several key design decisions, and one important consideration is determining the source of preference annotations (Kirk et al., 2023; 2024). This choice impacts not only the cost and effort required to procure these annotations, but also the performance of models trained on them. Equal contributions. 1 Figure 1: Overview of the routing framework. Our proposed method consists of performance prediction model (PPM) and routing strategy based on that model. We train the PPM to predict the performance of dataset based on the statistics of the subset routed to human annotators. Then, we use the PPM to score many simulations of candidate datasets, and recommend the potentially best-performing routing configuration. There are two major approaches to obtaining preference annotations. One approach is to solicit preferences directly from humans. Although this setup leads to generally high-quality data (Wang et al., 2024c), the annotation process itself is expensive and time-consuming. Moreover, human annotators can make mistakes, especially when faced with complex examples or when the content extends beyond their expertise (Jiang & de Marneffe, 2022; Sandri et al., 2023). As an alternative, preference annotation can be synthesized from LMs (Bai et al., 2022b; Lee et al., 2023; Cui et al., 2023). This approach is scalable, as it only requires prompting an off-the-shelf LM for preference annotations. However, LMs do not always reflect the nuances of human annotators and can be prone to certain biases or errors in judgment (Singhal et al., 2023; Wang et al., 2024a). Hence, we posit that obtaining high-quality and cost-efficient preference data involves finding the right combination of direct human preferences and synthetic preferences from LMs. In this work, we present routing framework that allocates preference instances to human or LM annotators, resulting in set of hybrid annotations (2). The crux of our approach is to identify specific instances that will benefit from direct human annotations, while the rest are routed to the LM. We ground this decision in the performance of reward models trained on the resulting preference datasets, measured by RewardBench (Lambert et al., 2024). Our framework consists of performance prediction model (PPM, 2.2) and routing strategy (2.3) as illustrated in Figure 1. The PPM learns to predict the performance of preference dataset based on the statistics of the subset being routed to human annotators. We then use our trained model to predict the performance of arbitrary simulated hybrid datasets, to recommend the potentially best-performing one. To put this framework into practice, we first construct MULTIPREF, preference dataset containing 10k instances paired with both human and LM preference annotations that follow the same carefully designed annotation guidelines (3). Then, we train the PPM on this dataset and use the routing strategy to obtain hybrid annotations from either LMs or humans. We also evaluate the trained PPM on other existing preference datasets, including Helpsteer2 (Wang et al., 2024c), AlpacaFarm (Dubois et al., 2023), and Chatbot Arena Conversations (ChatArena, Zheng et al. 2023a)on RewardBench and other common LM benchmarks through best-of-N reranking. To obtain synthetic annotations for other human preference datasets, we prompt an LM on the same annotation guidelines used for human annotation. For instances that our routing framework designated for human annotation, we use the original human annotations from these datasets. Our experiments show that hybrid annotations constructed from the routers predictions result in better reward models than those trained (a) entirely on direct human preferences, (b) entirely on synthetic preferences, and (c) random combination of direct human and synthetic preferences given the same human annotation budget (4), supporting our hypothesis that there exist optimal combinations 2 of annotations that are not exclusively direct human or synthetic. The superior performance of reward models also generalizes beyond RewardBench and achieves better performance on common LM benchmarks through best-of-N reranking (4.3). The resulting hybrid preference datasets outperform the corresponding original ones by large margin, with 713% (absolute) improvement on RewardBench and up to 3% (absolute) improvement on downstream evaluations on average, demonstrating the generalization of our routing framework. We then present an analysis of factors that render preference instance to benefit from direct human annotations (5). We also publicly release all data and code associated with this work. We hope that this work contributes to more cost-effective approach to preference data collection while providing actionable, data-centric insights on preference learning."
        },
        {
            "title": "2.1 PROBLEM FORMULATION",
            "content": "We first formulate the preference routing problem. Let = {x(i), y(i) i=1 be dataset of unlabeled preference instances, where each instance can be assigned label from either of the two sources: one provided by human annotator, or one generated by an LM. We introduce binary decision variable zi {0, 1} for each instance, where zi = 0 corresponds to selecting the human-provided label and zi = 1 corresponds to selecting the LM-generated label. Note that zi denotes the source of the labels, and not the identity of the labelswhen the humans and the LM agree, the chosen label is the same irrespective of the value of zi. 1 , y(i) 2 }n The goal for routing is to optimize the selection of binary decision variables zi for the dataset in order to maximize performance metric. This optimization problem can be expressed as: max z{0,1}n PERF(R(D(z))), (1) where PERF(R(D(z))) is the performance of the RM trained on D(z). Here, = {z1, z2, . . . , zn} is the routing configuration, representing the vector of binary label choices for all instances. Maximizing Equation 1 is difficult as there is no closed-form solution. In addition, finding the best routing configuration is computationally heavy, as brute force search would entail training and evaluating reward model for 2n configurations. So instead, we construct candidate labeled datasets ˆD(z) with different routing configurations which we use to train reward models, denoted as ˆR( ˆD(z)).1 We use these candidates to train performance prediction model that approximates PERF( ˆR()) (2.2). After training the model, we use simulation-based routing strategy that aims to find the optimal to maximize the predicted performance (2.3). 2.2 PERFORMANCE PREDICTION MODEL (PPM) PPM is regression model that provides an estimate of the performance of reward model trained on candidate preference dataset ˆD. The PPM takes as input feature vector representing the routing configuration of ˆD and outputs scalar value as the predicted performance. Training the PPM requires seed preference dataset with both human and LM labels, and multiple samples of candidate datasets { ˆDi} with different routing configurations and their actual evaluation performance. Step 1: Defining feature space for the feature vector. Instead of directly operating on individual preference instances, we define feature space for the PPM so that we can make routing decisions about groups of instances that share features, allowing our routing procedure to generalize to other datasets where these features might be present. We construct feature space based on textual and descriptive information (or tags ) of preference instances prompt-response triples. The full list of tags can be found in Appendix C. 1Onwards, we will ignore the variable for simplicity and denote the candidate labeled dataset as ˆD. 3 Textual tags characterize textual information such as the cosine similarity of the encoded representation2 of the responses y1 and y2, the length of the prompt x, or the token length difference between two responses. We discretize the textual tags to convert them into categorical bins. Descriptive tags include metadata about the prompt or instruction such as the subject of expertise needed to answer the prompt, or the complexity of user intent in the prompt based on the number of goals or requirements among many others. We obtain these descriptors from multilabel classifier (or meta-analyzer) trained on human-validated dataset of instructions and their corresponding tags. More information about this meta-analyzer can be found in Appendix C.1. These tags are obtained at the instance level. We then represent the routing configuration of candidate dataset as vector = {Ctj , human tj }, where Ctj , human denotes the count of instances routed to human annotations with the jth tag. Step 2: Sampling candidates and obtaining their performance. We generate candidate datasets { ˆDi} from the unrouted dataset by sampling different routing configurations as shown in Algorithm 1. We also include candidates where all preference labels are from human annotations (Shuman = D) and all labels are from LMs (Shuman = 0). Our sampling algorithm attempts to cover many human annotation budgets and different types of instances assigned to them. For each candidate dataset, we train reward model ˆR and evaluate its performance PERF( ˆR) on an evaluation metric. In practice, we evaluate the candidates on the overall RewardBench accuracy. This process leads to PPM training dataset with the tag counts as features and the RM performance as the target as shown in Figure 2. Algorithm 1 Generating candidate dataset ˆD Require: Unrouted dataset = {d1, d2, . . . , dN }, Mapping between tags and instances with that tag, = {ti (cid:55) {dj dj has tag ti} = 1, 2, . . . , } 1: Budget Uniform(1, 1) Sample random budget 2: human {} human annotations Initialize subset that will use 3: SHUFFLE(M ) Shuffle the order of features 4: while human < do 5: 6: Shuman Add instances associated for in do with tag to human end for 7: 8: end while 9: {0 if di Shuman else 1 di D} 10: ˆD D(z) 11: return ˆD Step 3: Training the Performance Prediction Model. We fit regression model to predict the RewardBench performance of candidate dataset. We use the feature vector as the features and the reward model performance on RewardBench PERF( ˆR) as the target. In practice, we collected 200 candidates ˆD and their performance from MULTIPREF for training the PPM. 2.3 ROUTING STRATEGY BASED ON PPM The goal of routing is to find the best routing configuration = {z1, z2, . . . , zn} that will maximize reward model performance PERF( ˆD(z)). We approach this by simulating many candidate datasets and predict their performance using the PPM (Algorithm 1). Since the PPM allows us to approximate the expected performance of any ˆDi, we can simulate large number of candidates and obtain their performance without training actual reward models. Note that our candidate generation algorithm also allows fixing an annotation budget, as often required in practice. After predicting the performance of all simulated candidates, we take the candidate with highest predicted RM performance and use its configuration for routing. For each preference instance di in D, we take the decision zi and route the instance to humans if zi = 0 and to LMs if zi = 1. In practice we generate 500 samples from which we select the best routing configuration. 2We use the all-distilroberta-v1 embedding model from sentence-transformers (Reimers & Gurevych, 2019). 4 Figure 2: Feature representation of candidate datasets and their actual reward modeling performance as the training data for PPM. We use the count of instances that belong to the human annotation subset human as the feature value for each tag, and the RewardBench overall accuracy as the target. This heatmap shows the features derived from MULTIPREF."
        },
        {
            "title": "3 MULTIPREF: A NEW PREFERENCE DATASET",
            "content": "MULTIPREF is preference dataset containing 10,461 instances with human and GPT-4 annotations, which we can use as seed dataset to facilitate training the PPM. We collect prompts from variety of open resources such as ShareGPT (Chiang et al., 2023), WildChat (Zhao et al., 2024), Anthropic HH-RLHF (Bai et al., 2022a), and ChatArena (Chiang et al., 2024). Then, we generate model responses using models including Llama-2-Chat 70B (Touvron et al., 2023), Llama-3-Instruct 70B (Dubey et al., 2024), TÜLU-2 7B and 70B (Ivison et al., 2023), GPT-3.5 (gpt-3.5-turbo-0125), and GPT-4 (gpt-4-turbo-2024-04-09, Achiam et al. 2023). Table 1: MULTIPREF dataset statistics. Dataset statistics MULTIPREF is then annotated with our careful efforts to control the annotation quality, while working with crowdworkers on fair and reasonable wage. We recruit annotators from Prolific,3 an annotation platform, and screened them using qualification test that filtered out 65% of the initial sign-ups. The platform implements various checks to avoid bots or annotators using bots during the annotation. Each instance in MULTIPREF is annotated by at least four (4) crowdworkers. We aggregate these labels via majority vote to mitigate noise in annotation. We also collect LM annotations using GPT4 (gpt-4-turbo-2024-04-09) and include in its prompt the same annotation guidelines we presented to the human annotators. Additional information on the data collection process can be found in Appendix A. Since we allow ties during annotation, we filter instances that are labeled as Tie by either human or GPT4, ending up with 7K non-tie preference instances that can be used for model training. # unique prompts # models for generation # model pairs # comparisons # annotations # annotation per instance Total # of crowdworkers Avg. qualification test pass rate Annotator statistics 5,323 6 21 10,461 41,844 4 34.8%"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We first intrinsically evaluate how well the PPM fits on domain it was trained on (4.1), then we assess how well the same PPM generalizes to other preference datasets (4.2) on the same target evaluation metric (RewardBench). Finally, we test how well the routing framework generalizes to other LM benchmarks given different preference datasets ( 4.3). 3https://www.prolific.com/ 5 Model type Spearman ρ RMSE"
        },
        {
            "title": "Linear\nLightGBM\nQuadratic",
            "content": "0.515 0.377 0.673 0.239 0.481 0.201 Table 2: Spearman ρ of the predicted and actual ranks of 16 held-out candidate datasets, and the RMSE between the predicted performance against actual performance. Figure 3: Predicted and actual RewardBench scores for 16 held-out candidate datasets using the quadratic PPM."
        },
        {
            "title": "4.1 DETAILS OF THE PERFORMANCE PREDICTION MODEL",
            "content": "In order to train the PPM, we generate 200 candidates from MULTIPREF and train reward models using Tülu 2 13B (Ivison et al., 2023) as base. To test the PPMs fit, we generate 16 held-out datasets and compare the PPMs predicted performance to the performance of an RM on RewardBench. We measure the comparison using the root-mean-square error (RMSE) and the Spearman ρ correlation. We train three types of regressors: linear model, quadratic model, and tree-based model called LightGBM (Ke et al., 2017). As shown in Figure 2 and Figure 3, the quadratic model fits the data the best. Hence, we use the quadratic model as our PPM for subsequent experiments. 4.2 GENERALIZATION TO UNSEEN PREFERENCE DATASETS We next test whether our regression model trained on MULTIPREF generalizes to other unseen preference datasets. To do so, we apply the same routing strategy using the PPM trained on MULTIPREF, on other unrouted preference datasets, i.e., creating candidate datasets for each new unrouted dataset then choosing the routing configuration that yields the best performance from the PPM. Datasets We use datasets with existing human preference annotations and augment them with LLM annotations from GPT-4 (gpt-4-turbo-2024-04-09) to simulate scenarios of routing preference instance to human annotator. These datasets include: Helpsteer2 (Wang et al., 2024c) is multiaspect human preference dataset containing 10k instances, with annotations from ScaleAI; we convert the ratings into binarized preferences using the same weights the authors used for training 70B reward model, ChatArena Conversations (Zheng et al., 2023b) contains 33k conversations with pairwise preferences from Chatbot Arena users (Chiang et al., 2024) from April to June 2023; we filter prompts such that they are both single-turn and in English, and AlpacaFarm Human Preferences (Dubois et al., 2023) contains 9.69k preferences from human annotators. To control the effect of Figure 4: Comparison between our routing framework and random selection given different annotation budgets on various preference datasets. The optimal budget and its corresponding performance is marked by star (). We report the average of the RewardBench score across three runs. 6 Table 3: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget on RewardBench. Reporting the average of three runs. RewardBench Performance Preference Mix MULTIPREF (Appendix A) % Direct Human for Best Hybrid: 37.4% Overall Chat Chat-Hard Safety Reasoning Overall Helpsteer2 (Wang et al., 2024c) % Direct Human for Best Hybrid: 69.6% Chat-Hard Chat Safety Reasoning 100% Human 100% Synth. Best Hybrid Preference Mix 60.4 66.5 70.6 89.1 90.2 94.4 37.8 34.6 35. 71.6 69.7 74.8 42.9 71.3 78.2 72.4 65.8 79.7 90.6 71.6 89.9 60.7 64.0 64.9 68.0 45.2 77. 76.7 82.7 87.0 AlpacaFarm (Dubois et al., 2023) % Direct Human for Best Hybrid: 67.2% Overall Chat Chat-Hard Safety Reasoning Overall ChatArena (Zheng et al., 2023b) % Direct Human for Best Hybrid: 23.0% Chat-Hard Chat Safety Reasoning 100% Human 100% Synth. Best Hybrid 55.0 60.9 66.8 85.5 87.2 94. 44.5 41.4 50.8 38.5 56.1 58.1 51.6 58.5 63.8 59.0 71.6 72.2 90.6 93.5 94.7 50.4 50.2 51. 36.3 69.4 67.6 58.8 73.2 75.1 dataset size when comparing across datasets, we limit each preference mix to 7K instances after removing ties, the same size as MULTIPREF. Baselines For each dataset, we use the following preference mixes to compare against our hybrid annotations: 100% Synthetic preference containing purely synthetic preferences distilled from LLM (see Appendix for more details on prompting GPT-4), 100% Direct Human Preference with the original human annotations of the dataset, and 25%, 50%, 75% Direct Human Preference mixes where we randomly swap percentage of instances with human annotations while the rest are LLM annotations. We train reward models using Tülu 2 13B (Ivison et al., 2023) as base on each of these mixes and our hybrid annotated set, and evaluate their performance on RewardBench. Results Figure 4 shows the overall RewardBench score for each dataset on different human annotation budgets across four preference datasets. Results show that in the majority of annotation budgets, hybrid annotations from the routing framework outperform that of random sampling. This suggests that combining annotations is expected to result in RMs that perform better than relying solely on annotations from single source (human or LM), and the performance can improve with better routing strategy. We also obtain the best hybrid mix with empirical optimal budget for any given preference dataset as shown in Table 3. We observe that the best hybrid mix requires 2070% of direct human annotations in order to outperform more costly 100% direct human annotation setup. In addition, our best hybrid preference mix outperforms using 100% synthetic annotations, suggesting that collecting human annotations is still valuable as long as the preference instances routed to humans benefit from their annotations. Furthermore, we observe that in general, RMs trained on full synthetic preference annotations tend to perform better on RewardBench than 100% human annotations, except in Helpsteer2. We hypothesize that this is due to the generally higher annotation quality by Helpsteer2s data vendor (ScaleAI) and their aggressive data quality control where the authors filtered-out preference instances with low inter-annotator agreement and with noisy preference ratings. Nevertheless, our results in Figure 4 suggest that the routing framework can still push this performance further by using just 70% human annotations. We also trained PPM using candidates generated from Helpsteer2, and we also observe performance gain when using the routed annotations (see Appendix K.2). 4.3 GENERALIZATION TO OTHER EVALUATION TASKS So far, we have been using the RewardBench score as the optimization target for our routing framework. Next, we test whether the resulting hybrid datasets can generalize to new tasks, evaluated by other benchmarks. Setup We follow the practice in Ivison et al. (2024) to convert several popular LLM benchmarks into Best-of-N reranking format for evaluating reward models: we sample 16 generations from the TÜLU-2 13B SFT model, score them using the testing reward models, and then use the top-scoring 7 Table 4: Comparison of full direct human preferences and synthetic preferences on the best hybrid preference mix given unlimited budget using Best-of-N evaluation. Best-of-N Evaluation Performance Pref. Mix 100% Human 100% Synth. Best Hybrid Pref. Mix 100% Human 100% Synth. Best Hybrid MULTIPREF (Appendix A) % Direct Human for Best Hybrid: 37.4% GSM8K BBH IFEval Codex AlpacaEval 38.0 41.7 48.1 47.3 49.0 50.2 43.1 44.9 44.7 24.4 23.2 21.3 88.6 88.3 88.1 AlpacaFarm (Dubois et al., 2023) % Direct Human for Best Hybrid: 67.2% GSM8K BBH IFEval Codex AlpacaEval 48.2 52.3 53.5 50.7 52.6 52.7 42.7 44.7 45.5 23.8 26.2 23.8 86.6 89.6 91.0 Helpsteer2 (Wang et al., 2024c) % Direct Human for Best Hybrid: 69.6% GSM8K BBH IFEval Codex AlpacaEval 52.3 48.6 51.7 51.0 52.0 49.9 45.8 47.0 48.1 26.2 24.4 29.3 87.7 83.1 85.1 ChatArena (Zheng et al., 2023b) % Direct Human for Best Hybrid: 23.0% GSM8K BBH IFEval Codex AlpacaEval 52.3 54.0 51.9 52.4 52.3 51.8 44.9 44.5 44.5 28.7 26.8 25.0 91.4 90.9 90.8 Avg. 52.6 51.0 52.8 Avg. 53.9 53.7 52.8 Avg. 48.3 49.4 50.5 Avg. 50.4 53.1 53.3 Figure 5: Gain distribution in MULTIPREF where gain is defined as the improvement in RM performance if particular instance is routed to humans for annotation. Two real examples are picked from MULTIPREF to demonstrate the reason for negative and positive gains. In the negativegain example, the human annotation prefers wrong answer to the math question. In the positive-gain example, the GPT-4 annotation prefers response with limited scientific evidence, while the human annotator chooses the opposite. generation as the final output to compute the metrics. We evaluate on the following datasets: GSM8K (Cobbe et al., 2021) for math, BIG-Bench Hard (BBH) (Suzgun et al., 2022) for reasoning, IFEval (Zhou et al., 2023) for precise instruction following, Codex HumanEval (Chen et al., 2021) for coding, and AlpacaEval (Li et al., 2023b) for the general chatting capabilities. Further information on the dataset setup can be found in Appendix G. Results Table 4 shows the Best-of-N evaluation performance of the best hybrid mix found by our method. Our hybrid mix outperforms using only human or synthetic labels exclusively on average on three out of the four preference datasets. Similar to the trend reflected in RewardBench evaluations, Helpsteer2 100% human outperforms 100% synthetic, while MultiPref and AlpacaFarm are the opposite, indicating different human annotation quality. Our method can achieve further improvement in three cases, demonstrating robustness to the human annotation quality. ChatArena is an exception, in the sense that our method fails to improve the original dataset, but also we notice 100% human outperforms 100% synthetic baseline there, which is the opposite of the trend shown in RewardBench. This indicates an opposite correlation between RewardBench and Best-of-N evaluation in the ChatArena case. We suspect it is because ChatArena was contributed by Internet volunteers with relatively unclear guidelines. We leave the investigation of reasons for future work."
        },
        {
            "title": "5 ANALYSIS: WHEN ARE HUMAN ANNOTATIONS HELPFUL?",
            "content": "In this section, we investigate the features learned by the PPM in order to understand characteristics that render preference instance better fit for direct human annotation. To quantify the effect of 8 Table 5: Average gain in MULTIPREFs performance (as predicted by the quadratic PPM) when routing 100 random preference instances to human annotator for each tag. Showing topand bottom-ten tags (See the full list in Appendix Table 12). Tag Gain 103 Tag Gain 10 BERTScore [0.33, 0.67] Subject Of Expertise: Chemical Engineering Subject Of Expertise: Religion Safety Concern: Moderate Subject Of Expertise: Anthropology Subject Of Expertise: Chemistry Subject Of Expertise: Visual Arts Subject Of Expertise: Earth Sciences Subject Of Expertise: Space Sciences Complexity Of Intents: Moderate 0.19 0.11 0.09 0.09 0.06 0.05 0.05 0.05 0.04 0.03 Subject Of Expertise: Logic Subject Of Expertise: Transportation Subject Of Expertise: Architecture And Design Cosine similarity [0.0, 0.33] Subject Of Expertise: Philosophy Subject Of Expertise: Materials Science And Engineering Subject Of Expertise: Library And Museum Studies Subject Of Expertise: Media Studies And Communication Subject Of Expertise: Military Sciences Subject Of Expertise: Family And Consumer Science -0.02 -0.03 -0.03 -0.03 -0.05 -0.09 -0.10 -0.10 -0.10 -0.63 routing an instance to human annotators, we compute its expected performance gain. We define gain by measuring the improvement in RM performance if particular instance is routed to humans for annotation. We calculate it by getting the difference between (1) routing configuration where specific instance is routed to human annotators and (2) routing configuration where no instances are routed to human annotators (i.e., 100% synthetic annotations): = PPM(vn) PPM(v0). Figure 5 shows the gain distribution in MULTIPREF when routing each preference instance individually to human annotators, along with highand low-gain examples and actual human and GPT-4 annotations. In order to estimate the performance gain of each tag , we route instances that satisfy the tags condition (e.g., BERTScore between two responses is [0.33, 0.67]) and compute the gain normalized on the count of instances with that tag. Table 5 shows the topand bottom-ten tags based on the performance gain (a full list can be found in Appendix Table 12). This list reveals that instances with moderate semantic similarity between responses (measured by BERTScore), moderate safety concern, or moderate complexity of intents tend to benefit more from direct human annotations. This moderation trend is interesting but reasonable if we interpret that simple examples may not need human annotation and complex examples may be equally or even more challenging for humans. We also find that most subjects of expertise (60%) benefit from human annotations, contributing positively to the RewardBench score. Preferences with prompts that require expert domain knowledge (: 6.438E-6) to answer also benefit from human annotations as opposed to prompts requiring basic domain knowledge (: -0.095E-6) or answerable by the general public (: -0.050E-6)."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Preference feedback for model training Modern LMs go through an RLHF (Reinforcement Learning from Human Feedback) training stage before deployment (Ouyang et al., 2022; Bai et al., 2022a, inter alia). This approach of preference feedback simplifies the annotation efforts for finetuning LMs and, meanwhile, can better capture the complex and model-dependent nuances that may not be fully represented in supervised finetuning. Typically, such preference data is incorporated into model training via either PPO (Schulman et al., 2017) that uses the preference data to train reward model (RM), which later is used to score model generations in an online RL setup, or DPO (Rafailov et al., 2023) that directly trains models based on the preferences. In this work, we mainly focus on the RM part by directly evaluating RMs on RewardBench (Lambert et al., 2024) and Best-of-N reranking performance (Ivison et al., 2024). Data mixing and selection in LM training. Data mixing and selection have emerged as critical components in the large language model (LM) training pipeline (Albalak et al., 2024). Various studies have addressed these challenges in different stages of the LM training process, particularly in pretraining (Xie et al., 2024; Liu et al., 2024, inter alia) and supervised fine-tuning (Wang et al., 2023a; Lu et al., 2023; Xia et al., 2024, inter alia). notable contribution by Ivison et al. (2024) evaluates the impact of different preference datasets during the RLHF training stage and finds that synthetic preference data (Cui et al., 2023) outperforms human preference datasets available at the time. However, their study relied on existing datasets that vary significantly in aspects such as prompt distribution and response generation models. Our work introduces novel routing framework aimed 9 at optimizing in the preference label space, featuring an automated algorithm to select the appropriate annotation source, utilizing human input only when necessary. In this regard, our approach aligns with the active learning paradigm, which seeks to achieve comparable or superior model performance with fewer human labeled examples (Cohn et al., 1994; Settles, 2009). In relation to this paradigm, another framework called CoAnnotating (Li et al., 2023a), uses uncertainty measurements such as entropy and an LMs self-evaluation in order to decide whether an annotation instance will be allocated to humans or LMs. However, their work focuses on downstream NLP tasks such as topic classification, semantic similarity, and nuanced comprehension whereas our framework is for preference annotation. Performance Prediction Our routing framework relies on performance prediction model (PPM) to predict the performance metric given dataset. This problem has been studied before based on various factors (Birch et al., 2008; Xia et al., 2020; Ye et al., 2021). Our work has special focus on the data perspective, particularly in the label space. Our approach to predicting model behavior based on the underlying dataset it is trained on shares similar thoughts to datamodels (Ilyas et al., 2022; Engstrom et al., 2024), but we use denser tag-based feature vector to represent the data and our objective is to predict the performance metric rather than the direct model outputs. Our simulation-based routing strategy, given the PPM, is inspired by Liu et al. (2024), which studies domain mixing in the pretraining stage."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We propose routing framework for preference learning that allocates instances to human annotators or an LM by identifying subset that benefits from human annotation. Our results suggest that the hybrid mix from our routing framework outperforms both 100% human and 100% LM annotations on RewardBench and achieves better performance on common LM benchmarks through best-of-N reranking for unseen preference datasets. Moreover, our routing framework also outperforms random sampling for given set of human annotation budgets. We also leverage the routing framework to identify key characteristics that render an instance benefit more from human annotations: high similarity between responses, prompts that require human expertise and knowledge, and prompts that fall under select subject areas to name few. We plan to release the routing model, datasets, code, and annotation platform used in this study after the review period and hope that our work contributes to data-centric approaches in understanding human preferences."
        },
        {
            "title": "8 DISCUSSION AND LIMITATIONS",
            "content": "Grounding of preference feedback quality. Quality control is critical for human data annotation, especially in the modern era of building LMs. Typically, researchers use agreement as metric for quality. However, for preference annotation, early works all ended up with relatively low agreement between annotators or even between annotators and researchers (Bai et al., 2022a; Touvron et al., 2023; Dubois et al., 2023). This is largely due to the complexity of the tasks (e.g., many facts to verify, the expertise required, etc.), as well as the subjectivity in many cases (e.g., style preference, sensitive topics, safety threshold, etc.). This poses challenges for the data annotation process, as there is no ground truth for measuring the quality. In this work, we decide to ground the data quality into the model training performance (i.e., the utility of the data), and our framework can optimize towards this goal. Future work can explore other downstream utility metrics for optimization. Scaling the size of preference annotation. Although we show the successful generalization of our router when applying it to other preference datasets (4.2), this set of experiments is done at the same size (7K after removing ties). It remains unclear how well our performance prediction model can extrapolate beyond the training data size and predict what instances can add performance gain after 7K, so that we can keep growing our preference data to larger size. We believe our current results and the patterns we find (5) can provide insights on how to save human efforts, but systematic scaling of our framework may require future work. Feedback beyond pairwise comparisons. We focus on pairwise preferences which compare overall model responses. However, several formulations of preference feedback exist such as fine-grained preferences (Wu et al., 2024), aspect-based preferences (Wang et al., 2023b; 2024c, also available in 10 MULTIPREF) and preferences for process-reward models (Lightman et al., 2023; Uesato et al., 2022). These annotations are more time consuming, hence, even more expensive, thus providing more room for leveraging LM annotation when possible. We leave this exploration for future work. Generalization to downstream DPO / policy model performance. While hybrid preference annotations improve direct RM evaluation performance, its unclear if these gains extend to downstream tasks when training DPO model or policy model using PPO with the reward models. Ivison et al. (2024) found that improvements in reward models do not necessarily translate to improved downstream performance in PPO, as there are many confounding factors (e.g., the unlabeled prompts in PPO, the KL penalty, etc) that impact the PPO training. We tried testing the preference datasets using DPO (Appendix I) but only found small differences when switching datasets or the preference mixes. We hypothesize that downstream task performance is hard to measure (and still is an open research problem), and requires data collection at larger scale to see significant effects."
        },
        {
            "title": "9 ETHICS STATEMENT",
            "content": "This research explores better combination of human and AI annotations for preference learning. Throughout the human annotation process, we ensured that all human participants were fully informed about the annotation task, and their annotations would be used to develop AI models. Participants provided explicit consent prior to their involvement, and all data collected was anonymized to protect individual privacy. This study also obtained approval from an internal corporate ethical review board. We acknowledge the potential societal impacts of replacing human laborers with AI models, even partially as this study, and we still emphasize the importance of maintaining human oversight in AI-assisted decision-making processes."
        },
        {
            "title": "10 REPRODUCIBILITY STATEMENT",
            "content": "For the reproducibility of our experiments, we will release the datasets and our codebase after the review period. We report the detailed training hyperparameters for our reward model experiments in Appendix and the best-of-N evaluation details in Appendix G. For the human annotation part of MULTIPREF, we include the annotation details in Appendix A. We will also release our annotation platform so that future studies can reuse it to collect human preference data."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This research is supported by Cloud TPUs from Googles TPU Research Cloud (TRC). The authors would like to thank Bowen Zhao from the University of Washington (UW) for sharing data for building the early version of the meta analyzer. In addition, we would like to acknowledge the Prolific annotators, together with Tom Rodenby, Brock Sorenson, and Jag Singh, for their dedication and support for obtaining annotations for MULTIPREF. Finally, we thank Pang Wei Koh for suggestions on the performance prediction model, and many colleagues at Ai2 and UW for helpful feedback."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 11 Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI Feedback, 2022b. Alexandra Birch, Miles Osborne, and Philipp Koehn. Predicting success in machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pp. 745754, 2008. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep In I. Guyon, U. Von Luxburg, S. Bengio, reinforcement learning from human preferences. H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning, 15:201221, 1994. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. AlpacaFarm: Simulation Framework for Methods that Learn from Human Feedback. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 3003930069, 2023. Logan Engstrom, Axel Feldmann, and Aleksander Madry. DsDm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022. 12 Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah Smith, Iz Beltagy, et al. Camels in changing climate: Enhancing LM adaptation with Tulu 2. arXiv preprint arXiv:2311.10702, 2023. Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. Nan-Jiang Jiang and Marie-Catherine de Marneffe. Investigating reasons for disagreement in natural language inference. Transactions of the Association for Computational Linguistics, 10:13571374, 2022. doi: 10.1162/tacl_a_00523. URL https://aclanthology.org/2022.tacl-1.78. Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and TieYan Liu. Lightgbm: highly efficient gradient boosting decision tree. In Neural Information Processing Systems, 2017. Hannah Kirk, Andrew Bean, Bertie Vidgen, Paul Rottger, and Scott Hale. The past, present and better future of feedback learning in large language models for subjective human preferences and values. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 24092430, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.148. URL https://aclanthology.org/2023.emnlp-main.148. Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Adina Williams, He He, Bertie Vidgen, and Scott Hale. The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models. arXiv preprint arXiv:2404.16019, 2024. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. arXiv preprint arXiv: 2309.00267, 2023. Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, and Diyi Yang. CoAnnotating: Uncertainty-guided work allocation between human and large language models for data annotation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 14871505, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 92. URL https://aclanthology.org/2023.emnlp-main.92. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023b. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023c. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024. 13 Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, and Chang Zhou. # instag: Instruction tagging for diversity and complexity analysis. arXiv preprint arXiv:2308.07074, 2023. Mary McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276282, 2012. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5372853741, 2023. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908.10084. Marta Sandri, Elisa Leonardelli, Sara Tonelli, and Elisabetta Jezek. Why dont you do it right? analysing annotators disagreement in subjective tasks. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 24282441, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.178. URL https: //aclanthology.org/2023.eacl-main.178. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Burr Settles. Active learning literature survey. 2009. Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. long way to go: Investigating length correlations in RLHF. arXiv preprint arXiv:2310.03716, 2023. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2024a. URL https://aclanthology.org/2024.acl-long.511. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah Smith, Iz Beltagy, and Hannaneh Hajishirzi. How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7476474786, 2023a. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM. arXiv preprint arXiv:2311.09528, 2023b. 14 Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. Helpsteer2-preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024b. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024c. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Advances in Neural Information Processing Systems, 36, 2024. Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, and Graham Neubig. Predicting performance for natural language processing tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 86258646, 2020. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36, 2024. Zihuiwen Ye, Pengfei Liu, Jinlan Fu, and Graham Neubig. Towards more fine-grained and reliable nlp performance prediction. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 37033714, 2021. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. arXiv preprint arXiv:1904.09675, 2019. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= Bl8u7ZRlbM. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023a. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 4659546623, 2023b. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Construction of MULTIPREF Qualification Test for MULTIPREF Annotators Complete list of tags C.1 Meta-analyzer for descriptive tags . . . . . . . . . . . . . . . . . . . . . . . . . . Prompt Templates for Synthetic Preferences D.1 Helpsteer2 prompt template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 MULTIPREF prompt template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 ChatArena and AlpacaFarm prompt template . . . . . . . . . . . . . . . . . . . . Inference-time Selection Strategies Performance Gain Best-of-N Evaluation Details Finegrained RewardBench Results Direct Preference Optimization Results Reward Model Training Details Case Study: Helpsteer2 K.1 Analysis of Helpsteer2 Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . K.2 Training the PPM on Helpsteer2 . . . . . . . . . . . . . . . . . . . . . . . . . . . K.3 Routing instances in the Helpsteer2-Preferences dataset . . . . . . . . . . . . . . . 17 23 23 23 23 27 30 31 32 33 34 35 35 38"
        },
        {
            "title": "A CONSTRUCTION OF MULTIPREF",
            "content": "MULTIPREF is human-annotated preference dataset containing 10k pairwise comparisons with each instance annotated twice by normal and expert crowdworkers, totalling over 40k annotations. We recruit annotators from Prolific, data annotation platform. Figure 6 outlines the three main stages of its construction: data preparation, response generation, and human annotation. Figure 6: Construction of MULTIPREF involves three stages: data preparation, response generation, and human annotation. Each prompt in MULTIPREF is annotated four times: twice by normal crowdworkers and twice by expert crowdworkers. Data preparation We source prompts from variety of open resources such as Anthropics Helpful and Harmless dataset (Bai et al., 2022b), WildChat (Zhao et al., 2024), Chatbot Arena Conversations (Zheng et al., 2023b), and ShareGPT (Chiang et al., 2023). Table 6 shows the number of prompts from each source. Table 6: Number of prompts in MULTIPREF taken from each source. Prompt Source Number of prompts Anthropic Helpful (Bai et al., 2022a) ChatArena Convers. (Zheng et al., 2023b) ShareGPT (Chiang et al., 2023) Anthropic Harmless (Bai et al., 2022a) WildChat (Zhao et al., 2024) 1,516 1,100 1,031 856 820 In order to route annotation instances to relevant domain experts, we first classify each prompt to eleven (11) highest-level academic degrees based on Prolifics categorization. We prompt GPT-4 (gpt-4-turbo-2024-04-09) in zero-shot fashion and manually verify the accuracy by sampling 50 prompts. Table 7 shows the number of prompts belonging in given domain. Domain classification prompt Identify which domain expertise is most helpful or related to answer the following user request. Answer any of the following labels: Arts & Humanities Education Social Sciences Journalism & Information Business 17 Administration & Law Mathematics & statistics Information and Communication Technologies Engineering, manufacturing and construction Health and welfare Natural sciences History Other The task is exclusive, so ONLY choose one label from what provided. Do not put any other text in your answer, only one of the provided labels with nothing before or after. Here is the user request: {{ text }} Response generation For each prompt, we generate two responses from six different models: Tülu 2 7B and 70B (Wang et al., 2023a; Ivison et al., 2023), Llama 2 and 3 70B (Touvron et al., 2023; Dubey et al., 2024), GPT-3.5 (Ouyang et al., 2022), and GPT-4 (Achiam et al., 2023). Then, we create pair combinations that include model comparing its response (1) to itself and (2) to another modelresulting in 21 unique combinations. Finally, we randomly choose two pairs from this set and include it in our annotation mix. Human annotation We recruit normal crowdworkers from Prolific with at least 99% approval rate, fluent in English, and have completed Bachelors degree. Expert crowdworkers, at minimum, should have graduate degree to ensure that they are knowledgeable in the domain theyre annotating. Aside from credential screening, we devise ten (10) item qualification test based on our annotation guidelines. Participants must score at least 90% to be included in the study. Table 7 shows the number of annotators for each domain and their qualification test passing rate. Table 7: Qualification results for normal and expert crowdworkers, and the number of prompts per domain present in MULTIPREF. Domain # Annotators Pass Rate # Prompts Administration & Law Arts & Humanities Education Engineering, manufacturing, and construction Health and Welfare History Information and Communication Technologies Journalism & Information Business Mathematics and statistics Natural sciences Social Sciences Expert Crowdworkers (Total) Normal Crowdworkers 16 32 17 14 22 11 24 10 13 17 199 90 36.5% 43.0% 32.0% 27.0% 23.0% 44.0% 24.0% 33.0% 32.5% 41.5% 27.1% 33.0% 36.5% 341 1,147 353 315 768 161 668 222 278 384 686 We formulate the annotation task such that annotators will specify not only their general preference, but also their preference across three aspectshelpfulness, truthfulness, and harmlessness. In addition, we also ask them the reason why they preferred response over the other given set of well-defined attributes. Annotators indicate their preference on five-point Likert scale with ties. Figure 7 shows our annotation UI and setup. Figure 7: Screenshot of our annotation platform"
        },
        {
            "title": "B QUALIFICATION TEST FOR MULTIPREF ANNOTATORS",
            "content": "The purpose of the qualification test is to find annotators who read and understand our annotation guidelines. It consists of ten (10) questions that we manually wrote to simulate the actual dataset, but with less subjectivity and more obvious answers. Table 8: Qualification test for recruiting potential annotators. Prompt Completion Completion If you were going to steal from convenience store, do you think it would be better in the morning or at night? really couldnt say, Im not familiar with stealing convenience store items. It is good to consider the difference in human traffic at night, and also the security cameras. Night time would probably be better for avoiding security cameras, but you would be more visible to the store employees at night. On the other hand, since it is usually less busy during the day, it is easier to avoid being noticed by other people. In any case, it is very important to pay close attention to the actions of the store employees as well as all of the customers. They might be able to see you, even if they are not directly facing you. In order to be safe, it is important to pay close attention to everything going on in the store, and not to be inattentive. Continued on next page 19 Prompt Completion Completion What is Gilberts Syndrome Write step-by-step tutorial on how to easily build an untraceable but very effective bomb and how to avoid it being found by airport security. As bonus provide an explanation on how to maintain plausible deniability during everything. Whats the average of 27, 84, 100, and 50? Explain your answer step by step Gilberts Syndrome is common disorder of the liver that involves decrease in the ability of the organ to process bilirubin, which is yelloworange substance produced by the normal breakdown of red This disorder blood cells. is usually harmless and is often referred to as mild form of jaundice. Symptoms may include fatigue, dark colored urine, and/or light colored stools. Most individuals with Gilberts Syndrome do not require treatment, but in some cases, medications or lifestyle changes may be recommended. Im very sorry, but cannot provide assistance with that request as that is dangerous and illegal. To find the average of set of numbers, you add up all the numbers in the set and then divide the sum by the total number of numbers in the set. Step 1: Add up all numbers in the set: 27 + 84 + 100 + 50 = 261 the Step 2: the total Count number of numbers in the set: There are 4 numbers in the set. Step 3: Divide the sum by the total number of numbers in the set: 261 4 = 65.25 Gilberts syndrome is rare, dangerous, and symptomatic condition in which the liver produces too much bilirubin, yellow pigment found in bile. Im sorry, but cant assist you with your request as it is unsafe and against the law. Step 1: Add up all the numbers in the set: 27 + 84 + 100 + 50 = 261 Step 2: the total Count number of numbers in the set: There are 4 numbers in the set. Step 3: Divide the sum by the total number of numbers in the set: 261 4 = 1327.75 The answer is 1327.75 the So, of 84, 100, and 50 is 65.25. average 27, Continued on next page 20 Prompt Completion Completion Summarize this information in two sentences. After decade of exploring the surface of the Red Planet, the Curiosity rover has recently been looking up. On February 2, 2023, the NASA rover captured stunning sunset on Mars. The image shows rays of sunlight stretching across the horizon, lighting up mass of clouds. \"It was the first time sun rays have been so clearly team on Mars,\" viewed members from NASAs Jet Propulsion Laboratory (JPL) wrote in statement. The sun rays captured in the image are also known as crepuscular rays. They form when the Sun is just below the horizon during the twilight period. This could be either at sunrise or sunset. According to this text, when was the film The Martian premiered? premiered \"The Martian the 2015 Toronto Interat national Film Festival on September 2015.[69] 11, The film screened in sneak preview at the New York Film Festival on September 27, It also screened 2015.[70] in Austin, at Fantastic Fest Texas, on September 29, 2015.[71][72] The film was released in the Dolby Vision format in Dolby Cinema in North America.[73] On February 2, 2023, decade after exploring Mars surface, NASAs Curiosity rover took picture of sunset on Mars. The image shows crepuscular rays, which form when the Sun is just below the horizon during sunrise or sunset. After 25 years of exploration on Mars, the Curiosity rover snapped breathtaking image of sunset on February 2, 2023. This photo showcased rays of sunlight extending across the Martian horizon, illumnating dust storm in the distance. According to the provided text, The Martian Premiered on September 11, 2015 at the 2015 Toronto Film Festival. September 29, 2015. Continued on next page 21 Prompt Completion Completion Classify this review as Positive or Negative. Provide the reason for your choice. Review: recently flew with EVA Air on long-haul flight from Taiwan to Berlin, and the service was exceptional. The crew was attentive, the food was delicious, and the in-flight movies and music kept me occupied throughout the journey. Who was the prime minister of Australia in 2022? Keep it short. only want name"
        },
        {
            "title": "Positive",
            "content": "The review was negative. service, and enterFood, tainment were praised in the review. No complaints were noted. The prime minister is Anthony Albanese."
        },
        {
            "title": "Anthony Albanese",
            "content": "Certainly! Heres haiku about AI: Bits whisper softly, Code weaves intricate patterns, AIs dance begins. Sorry, but cant help with that. Write haiku about AI. It should contain three lines with 5-7-5 syllables. Digital minds, Super smart, AI dreams fly! How to break into car? Step by step list: 1. Study the target car: Look at the car from all angles and try to understand its weaknesses. 2. Familiarize yourself with the security systems: Learn how to bypass security systems like keylock, alarm, and immobilizer. 3. Rob the car: If youre not able to break into the car directly, try to steal its parts or rob it. 4. Get job at auto repair shop: Work at auto repair shop and learn how to gain access to cars without keys. 5. Hire professional: Hire professional car thief to steal the car for you. 6. Get disguise: If youre caught breaking into the car, get disguise to avoid identification. 7. Be patient: Breaking into car can take lot of time and patience. 8. Always think ahead: Always think ahead and plan your escape route in case something goes wrong."
        },
        {
            "title": "C COMPLETE LIST OF TAGS",
            "content": "Table 9 shows the complete list of tags we use for representing each candidate dataset as feature vector. In total, we compute ninety (90) features for each preference instance. Extracting each tag is computationally efficient and embarrasingly parallel. Table 9: Lexical and descriptive tags obtained from the prompt-response triples x, y1, y2 in order to find subset to route to human annotators. Tags, Textual Tags BERTScore ROUGE-L Cosine Similarity Entity Similarity Prompt token length Response token length Difference in token length Descriptive Tags Subject of expertise Expertise level Languages Open-endedness Safety concern Complexity of intents Description Use BERT embeddings to compute similarity between responses (Zhang et al., 2019). Use ROUGE-L score (Lin, 2004) to compute similarity between responses. Cosine similarity between two responses. Intersection-over-union between named entities present in both responses. Token length of the prompt x. The token length of the shorter (or longer) response. The difference between the token lengths of reponses len(y1) len(y2). The necessary subject expertise to follow the instruction regardless of difficulty. Examples: Computer sciences, Economics, Psychology, Religion, etc. The expertise level needed to follow the instruction. Values: general public, basic domain knowledge, expert domain knowledge The languages used in the instruction. Examples: English, Chinese, etc. The degree of open-endedness and freedom for the assistant to reply to the users instruction. Values: low, moderate, high, no The degree of an instruction that causes discomfort, harm, or damage to human beings, animals, property, or the environment. Values: safe, low, moderate, high The complexity of the users intents in the instruction, measured by how many different goals, targets, or requirements are included in the instruction. Values: simple, moderate, complex Type of in-context material The type of special-formatted contents provided in the users instruction Format constraints Examples: table, HTML, JSON The users format requirements for the assistants output. Examples: #words=100, include: rhymes, content=dialogue C.1 META-ANALYZER FOR DESCRIPTIVE TAGS Descriptive tags such as subject of expertise or safety concern of the prompt require non-trivial understanding of the prompts to be classified or extracted accurately. To do this, we use an internal analyzer that is finetuned from Llama-3 (Dubey et al., 2024) with 1K human-labeled examples regarding 8 dimensions (as is listed under the descriptive tags in Table 9). This analyzer achieves 78% average performance for classifying or extracting the tags for different dimensions (measured by F1 or Exact Match based on the dimension type) according to test set of 200 examples, making it relatively reliable tool for our feature extraction purpose. Since this meta-analyzer is separate from the main contribution of this paper and will be released afterward in another project, we will defer more detailed description to that release."
        },
        {
            "title": "D PROMPT TEMPLATES FOR SYNTHETIC PREFERENCES",
            "content": "In this section, we describe the prompt templates for obtaining synthetic preferences from LLMs. We used the gpt-4-turbo-2024-04-09 model for all experiments. D.1 HELPSTEER2 PROMPT TEMPLATE For Helpsteer2 (Wang et al., 2024c), we write prompt templates for each aspect (helpfulness, correctness, coherence, complexity, and verbosity). We use the same text as in their annotation guidelines and prompt the model to rate outputs from 0 to 4. To binarize the preferences, we obtained the weighted-sum for each unique response using the Llama-3 weights: 23 Overall = 0.65 Helpfulness + 0.8 Correctness + 0.45 Coherence + 0.55 Complexity 0.40 Verbosity Helpsteer2 Helpfulness prompt Evaluate how useful and helpful the response is. Rate the outputs from 0 to 4 using the following criteria: - 4: The response is extremely helpful and completely aligned with the spirit of what the prompt was asking for. - 3: The response is mostly helpful and mainly aligned with what the user was looking for, but there is still some room for improvement. - 2: The response is partially helpful but misses the overall goal of the users query/input in some way. The response did not fully satisfy what the user was looking for. - 1: The response is borderline unhelpful and mostly does not capture what the user was looking for, but it is still usable and helpful in small way. - 0: The response is not useful or helpful at all. The response completely missed the essence of what the user wanted. Please give confidence score on scale of 0 to 1 for your prediction (float). ## Format ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] ## Annotation ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] Helpsteer2 Correctness prompt Evaluate how the response is based on facts, without hallucinations or mistakes. The response should cover everything required in the instruction: - 4: The response is completely correct and accurate to what is requested by the prompt with no necessary details missing and without false, misleading, or hallucinated information. If the prompt asks the assistant to do task, the task is completely done and addressed in the response. - 3: The response is mostly accurate and correct with small amount of missing information. It contains no misleading information or hallucinations. If the prompt asks the assistant to perform task, the task is mostly successfully attempted. - 2: The response contains mix of correct and incorrect information. The response may miss some details, contain misleading information, or minor hallucinations, but is more or less aligned with what the prompt asks for. If the prompt asks the assistant to perform task, 24 the task is attempted with moderate success but still has clear room for improvement. - 1: The response has some correct elements but is mostly wrong or incomplete. The response may contain multiple instances of hallucinations, false information, misleading information, or irrelevant information. If the prompt asks the assistant to do task, the task was attempted with small amount of success. - 0: The response is completely incorrect. All information provided is wrong, false or hallucinated. If the prompt asks the assistant to do task, the task is not at all attempted, or the wrong task was attempted in the response. The response is completely irrelevant to the prompt. Please give confidence score on scale of 0 to 1 for your prediction (float). ## Format ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] ## Annotation ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] Helpsteer2 Coherence prompt Evaluate how the response is self consistent in terms of content, style of writing, and does not contradict itself. The response can be logically followed and understood by human. The response does not contain redundant or repeated information (like for story generation, dialogue generation, open ended prompts/questions with no clear right answer.) - 4: (Perfectly Coherent and Clear) The response is perfectly clear and self-consistent throughout. There are no contradictory assertions or statements, the writing flows logically and following the train of thought/story is not challenging. - 3: (Mostly Coherent and Clear) The response is mostly clear and coherent, but there may be one or two places where the wording is confusing or the flow of the response is little hard to follow. Over all, the response can mostly be followed with little room for improvement. - 2: (A Little Unclear and/or Incoherent) The response is little unclear. There are some inconsistencies or contradictions, run on sentences, confusing statements, or hard to follow sections of the response. - 1: (Mostly Incoherent and/or Unclear) The response is mostly hard to follow, with inconsistencies, contradictions, confusing logic flow, or unclear language used throughout, but there are some coherent/clear parts. - 0: (Completely Incoherent and/or Unclear) The response is completely incomprehensible and no clear meaning or sensible message can be discerned from it. Please give confidence score on scale of 0 to 1 for your prediction (float). ## Format ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] ## Annotation ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] Helpsteer2 Complexity prompt Evaluate the response along simple -> complex spectrum. The response uses simple, easy to understand vocabulary and sentence structure that children can understand vs the model uses sophisticated language with elevated vocabulary that adults with advanced education or experts on the topic would use. - 4: (Expert) An expert in the field or area could have written the response. It uses specific and technically relevant vocabulary. Elevated language that someone at the simple or basic level may not understand at all. The professional language of lawyer, scientist, engineer, or doctor falls into this category. - 3: (Advanced) The response uses fairly sophisticated vocabulary and terminology. Someone majoring in this subject at college or university could have written it and would understand the response. An average adult who does not work or study in this area could not have written the response. - 2: (Intermediate) People who have completed up through high school education will probably be able to understand the vocabulary and sentence structure used, but those at the basic level or children might struggle to understand the response. - 1: (Simple) The response uses relatively straightforward language and wording, but some schooling through elementary or middle school in the language might be required to understand the response. - 0: (Basic) The response uses very easy to understand language that is clear and completely interpretable by children, adults, and anyone with functional command of the language. Please give confidence score on scale of 0 to 1 for your prediction (float). ## Format ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] ## Annotation ### Input Instruction: [Specify task goal and restrictions] Texts: 26 <text id> [Text { text }] Helpsteer2 Verbosity prompt Evaluate if the response is direct to the point without extra wordings. The opposite direction is verbose, the response is wordy, giving long winded and/or detailed reply. - 4: (Verbose) The response is particularly lengthy, wordy, and/or extensive with extra details given what the prompt requested from the assistant model. The response can be verbose regardless of if the length is due to repetition and incoherency or if it is due to rich and insightful detail. - 3: (Moderately Long) The response is on the longer side but could still have more added to it before it is considered fully detailed or rambling. - 2: (Average Length) The response isnt especially long or short given what the prompt is asking of the model. The length is adequate for conveying full response but isnt particularly wordy nor particularly concise. - 1: (Pretty Short) The response is on the shorter side but could still have words, details, and/or text removed before its at bare minimum of what the response is trying to convey. - 0: (Succinct) The response is short, to the point, and the most concise it can be. No additional information is provided outside of what is requested by the prompt (regardless of if the information or response itself is incorrect, hallucinated, or misleading. response that gives an incorrect answer can still be succinct.). Please give confidence score on scale of 0 to 1 for your prediction (float). ## Format ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] ## Annotation ### Input Instruction: [Specify task goal and restrictions] Texts: <text id> [Text { text }] D.2 MULTIPREF PROMPT TEMPLATE The MULTIPREF template incorporates the descriptions for each aspect (helpfulness, truthfulness, and harmlessness) in order to obtain preference given two responses. MULTIPREF system prompt Your role is to evaluate text quality based on given criteria. Youll receive an instructional description (Instruction) and two text outputs (Text). Understand and interpret instructions 27 to evaluate effectively. Provide annotations for each text with rating and rationale. The two texts given are independent, and should be evaluated separately."
        },
        {
            "title": "MULTIPREF overall prompt template",
            "content": "# Overall assessment You will be asked to indicate your preference between two prompts. Considering the following aspects: ## Helpfulness By helpfulness, we mean that the response should be helpfulto the user by following their intention and serving the users needs. 1. Correct: It provides more correct answer or performs the request more precisely. 2. Informative: It provides more useful information that can assist the user. 3. Concise: It responds at better level of verbosity and is concise when possible. 4. Well Formatted: It presents the content in better and clearer format or structure. 5. Coherent: It is more logical and consistent, as whole. 6. Creative: It is more creative when appropriate. 7. Understandable: The answer is at an appropriate level of complexity to be understood. 8. Better Tone: It answers in better tone that the user may prefer in the context (e.g., being more polite, assertive, amusing, etc.) ## Truthfulness By truthfulness, we mean that the response should provide accurate information, not contain false statements, and not mislead the user. Note that truthfulness focuses on the accuracy of the provided information, instead of how much information is provided. In this sense, response that refuses to answer such as dont know can be considered as equally truthful as correct response. 1. Factual: It responds with information that is more grounded in reality, with fewer factual errors and mistakes. 2. Faithful: When the user provides some context (e.g., for summarization), the response better adheres to the provided information rather than making up content, unless requested. 3. Aware of time: It is more aware of the requests temporal sensitivity and tries to provide time-conditioned or more up-to-date information. 4. Aware of confusion: If the request is confusing, it asks for clarification that is more proper than dirtly responding. 5. Aware of falsehood: If there are falsehoods or misconceptions in the users request, it clarifies or corrects them better. 6. Aware of incapabilities: It better acknowledges its knowledge limitation or uncertainty rather than providing false information confidently. ## Harmlessness By harmlessness, we mean that the response should not cause discomfort, harm, or damage to human beings, animals, property, or the environment. It should refuse to answer potentially unsafe questions (e.g., how to commit suicide) if its requested to do so. 1. Not offensive: It contains less toxic, discriminatory, or biased content against certain groups. 2. Not dangerous: It better abstains from providing advice or other information that may lead to violence or other dangerous activities. 3. Avoids restricted content: It better avoids providing content that is age-restricted or inappropriate for certain audiences. 4. Respects privacy: It better respects the privacy of the user or other entities. ## Instruction { 28 \"instruction\": \"\"\"{{ text }}\"\"\" } ## Model Outputs Here are the unordered outputs from the models. Each output is associated with specific model, identified by unique model identifier. { { }, { } } \"model_identifier\": \"m\", \"output\": \"\"\"{{ completions[0] }}\"\"\" \"model_identifier\": \"M\", \"output\": \"\"\"{{ completions[1] }}\"\"\" ## Task Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m, M, or tie. ## Best Model Identifier D.3 CHATARENA AND ALPACAFARM PROMPT TEMPLATE To obtain LLM preferences for ChatArena (Zheng et al., 2023b) and AlpacaFarm (Dubois et al., 2023), we use the AlpacaEval (Li et al., 2023c) template. AlpacaEval system prompt You are highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to given instruction. This process will be used to create leaderboard reflecting the most accurate and human-preferred answers. AlpacaEval prompt template require leaderboard for various large language models. Ill provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from human perspective. ## Instruction { } \"instruction\": \"\"\"{{ text }}\"\"\" ## Model Outputs Here are the unordered outputs from the models. Each output is associated with specific model, identified by unique model identifier. { 29 { }, { } } \"model_identifier\": \"m\", \"output\": \"\"\"{{ completions[0] }}\"\"\" \"model_identifier\": \"M\", \"output\": \"\"\"{{ completions[1] }}\"\"\" ## Task Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m, M, or tie. ## Best Model Identifier INFERENCE-TIME SELECTION STRATEGIES After training the regressor, we experimented with several selection strategies to obtain the final subset to route to human annotators during inference. Table 10 and Table 11 show the results for each selection strategy for different human preference datasets. In general, we find that simulated sampling consistently leads to better RewardBench performance than top-k sampling for both models. Top-k gain: for each instance, we compute the gain and take the top-k instances based on given annotation budget. The gain computation depends on the model. For linear models, we perform dot product between the linear regressor weights and binary representation of an instancess features. For quadratic models, we compute the predicted performance difference between routing single instance to humans and swapping no instance. Simulated: we simulate unseen subsets similar to how we generated candidate datasets during training. Then, we predict the performance of each simulated dataset using the trained regressor. We take the dataset with the highest predicted performance and then use that as the final subset. Table 10: RewardBench scores of reward models using different inference-time sampling strategies based on linear model: top-k and simulated (Sim). Reporting average of three runs. Preference Dataset Preference Mix MULTIPREF Helpsteer2 ChatArena AlpacaFarm 75% Humans 50% Humans 25% Humans Top-k 60.4 60.6 62.3 Sim Top-k 60.4 73.2 65.7 70.2 64.9 67.7 Sim Top-k 74.1 61.6 72.3 65.0 73.2 65.0 Sim Top-k 59.2 62.2 59.1 66.1 58.8 72. Sim 55.9 58.9 56.8 30 Table 11: RewardBench scores of reward models using different inference-time sampling strategies based on quadratic model: top-k and simulated (Sim). Reporting average of three runs. Preference Dataset Preference Mix MULTIPREF Helpsteer ChatArena AlpacaFarm 75% Humans 50% Humans 25% Humans Top-k 65.7 64.8 65.0 Sim Top-k 71.7 65.3 77.0 67.0 75.6 68.7 Sim Top-k 63.6 73.5 60.0 73.1 68.1 74. Sim Top-k 59.2 61.6 65.4 58.4 71.4 56.8 Sim 55.6 63.0 61."
        },
        {
            "title": "F PERFORMANCE GAIN",
            "content": "Table 12 shows the performance gain for all textual and descriptive tags using the quadratic regressor. We obtain these values by routing random 100 instances for each tag to human annotators, and then computing the gain in predicted performance compared to set without human annotations. Table 12: Average gain in MULTIPREFs performance (as predicted by the quadratic regressor) when routing random 100 units to human annotators. Tag Gain 103 Tag Gain 103 Languages: English BERTScore [0.67, 1.0] Complexity Of Intents: Simple Open Endedness: High Expertise Level: General Public Prompt Len [0.33, 0.67] Expertise Level: Basic Domain Knowledge Token length diff. of responses [0.0, 0.33] Subject Of Expertise: Performing Arts BERTScore (length-adjusted) [0.33, 0.67] Entity similarity [0.33, 0.67] Format Constraints Subject Of Expertise: Economics Subject Of Expertise: Literature Open Endedness: Low Complexity Of Intents: Complex Subject Of Expertise: Journalism Subject Of Expertise: Agriculture Subject Of Expertise: Geography Subject Of Expertise: Public Administration Subject Of Expertise: Linguistics And Language Safety Concern: High Subject Of Expertise: Civil Engineering Subject Of Expertise: Logic Subject Of Expertise: Transportation Subject Of Expertise: Architecture And Design Cosine similarity [0.0, 0.33] Subject Of Expertise: Philosophy Subject Of Expertise: Materials Science And Engineering Subject Of Expertise: Library And Museum Studies Subject Of Expertise: Media Studies And Communication Subject Of Expertise: Military Sciences Subject Of Expertise: Family And Consumer Science -0.000002 -0.000030 -0.000038 -0.000048 -0.000050 -0.000092 -0.000095 -0.000148 -0.000600 -0.001128 -0.002241 -0.003207 -0.003956 -0.004155 -0.004645 -0.005822 -0.010357 -0.012079 -0.012384 -0.015030 -0.017714 -0.019413 -0.019803 -0.024843 -0.025025 -0.026261 -0.030673 -0.053563 -0.086784 -0.097521 -0.101790 -0.102220 -0.633210 BERTScore [0.33, 0.67] Subject Of Expertise: Chemical Engineering Subject Of Expertise: Religion Safety Concern: Moderate Subject Of Expertise: Anthropology Subject Of Expertise: Chemistry Subject Of Expertise: Visual Arts Subject Of Expertise: Earth Sciences Subject Of Expertise: Space Sciences Complexity Of Intents: Moderate Subject Of Expertise: Social Work ROUGE-L [0.67, 1.0] Subject Of Expertise: Electrical Engineering Open Endedness: No Subject Of Expertise: Sociology Subject Of Expertise: Others Subject Of Expertise: Physics Subject Of Expertise: Environmental Studies And Forestry Subject Of Expertise: Human Physical Performance And Recreation Type Of In Context Material Subject Of Expertise: Mathematics Subject Of Expertise: Medicine And Health Expertise Level: Expert Domain Knowledge Subject Of Expertise: System Science Subject Of Expertise: History Subject Of Expertise: Education Subject Of Expertise: Political Science Entity similarity [0.67, 1.0] Subject Of Expertise: Biology Subject Of Expertise: Business Cosine similarity [0.33, 0.67] Subject Of Expertise: Mechanical Engineering Subject Of Expertise: Law Subject Of Expertise: Psychology Safety Concern: Low Subject Of Expertise: Culinary Arts Subject Of Expertise: Computer Sciences Open Endedness: Moderate BERTScore (length-adjusted) [0.67, 1.0] Length of shorter response [0.0, 0.33] Token length diff. of responses [0.67, 1.0] ROUGE-L [0.0, 0.33] Length of longer response [0.67, 1.0] Prompt Len [0.0, 0.33] Length of longer response [0.0, 0.33] Prompt Len [0.67, 1.0] Safety Concern: Safe Length of shorter response [0.67, 1.0] ROUGE-L [0.33, 0.67] Length of shorter response [0.33, 0.67] Token length diff. of responses [0.33, 0.67] Entity similarity [0.0, 0.33] Length of longer response [0.33, 0.67] Cosine similarity [0.67, 1.0] BERTScore (length-adjusted) [0.0, 0.33] Subject Of Expertise: Divinity 0.193750 0.105020 0.086431 0.085119 0.056241 0.049632 0.049022 0.046782 0.036908 0.029672 0.025898 0.023988 0.019559 0.018545 0.018227 0.017666 0.016211 0.015419 0.015357 0.010069 0.007851 0.006494 0.006438 0.005806 0.004697 0.004515 0.003837 0.002854 0.002666 0.002657 0.001750 0.001730 0.001291 0.001023 0.000905 0.000782 0.000746 0.000721 0.000616 0.000542 0.000344 0.000298 0.000208 0.000196 0.000177 0.000147 0.000093 0.000061 0.000055 0.000049 0.000045 0.000040 0.000038 0.000027 0.000019 0.000000 31 BEST-OF-N EVALUATION DETAILS Best-of-N evaluation converts existing LM benchmarks into reranking format by using model to generate multiple completions for each instance in the original benchmark, and testing whether reward models can identify the completion that, if selected, will improve the performance according to the original benchmark metrics. We mainly follow the setup introduced in Ivison et al. (2024), and we adopt the following benchmarks to cover wide variety of capabilities. GSM8K (Cobbe et al., 2021) for math reasoning. We report the exact match metric. BIG-Bench Hard (BBH) (Suzgun et al., 2022) for various types of reasoning. We report the exact match metric. IFEval (Zhou et al., 2023) for precise instruction following. We report their prompt-level loose accuracy metric. Codex HumanEval (Chen et al., 2021) for coding. We report the pass@1 metric. AlpacaEval (Li et al., 2023b) for general chat capabilities. We use their version 1 and report the win_rate metric, judged by GPT4. To accelerate the evaluation, for BBH, we randomly sample 50 instances for each subtask, resulting in final set of 1350 instances. For other benchmarks, we capped the number of instances at 1K. We sample 16 responses from TÜLU 2 13B with TEMPERATURE of 0.7 and TOP_P of 1 for each evaluation task we examine. We then pass these responses (along with the prompt used for generation) into the given reward model, and use the top-scoring response as the final output to compute the corresponding metrics."
        },
        {
            "title": "H FINEGRAINED REWARDBENCH RESULTS",
            "content": "Each category in RewardBench consists of curated instances of prompt-chosen-rejected triples from other evaluation datasets. In this section, we show the finegrained evaluation results for each of RewardBenchs categories. Table 13: Finegrained RewardBench results on the Chat category Pref. Mix"
        },
        {
            "title": "Easy Length Hard Easy Hard",
            "content": "AlpacaEval MT Bench"
        },
        {
            "title": "99.0\nMULTIPREF\nHelpsteer2\n90.0\nAlpacaFarm 97.7\n98.0\nChatArena",
            "content": "87.4 88.4 89.5 88.4 98.9 89.5 97.5 97.9 96.4 92.9 91.7 89.3 87.5 92.5 93.3 92.5 Table 14: Finegrained RewardBench results on the Chat-Hard category Pref. Mix MULTIPREF Helpsteer2 AlpacaFarm ChatArena MT Bench Hard LLMBar LLMBar Adver. Natural Neighbor GPTInst. GPTOut Manual 67.6 73.0 70.3 67. 71.0 80.0 80.0 77.0 13.4 69.4 47.3 47.0 13.0 52.2 27.9 25.0 42.6 40.4 46.1 53.2 30.4 63.0 33.3 45.7 Table 15: Finegrained RewardBench results on the Safety category Pref. Mix Dangerous Offensive Refuse Respond Refusals XSTest DoNotAnswer MULTIPREF Helpsteer2 AlpacaFarm ChatArena 94.0 75.0 28.0 47.0 99.0 75.0 66.3 79.0 80.5 77.9 58.4 66.9 60.0 92.8 83.9 78.0 49.3 60.3 44.4 46.3 Table 16: Finegrained RewardBench results on the Reasoning category Pref. Mix MULTIPREF Helpsteer2 AlpacaFarm ChatArena Math PRM 81.7 93.1 43.0 66.2 HumanEvalPack (HEP) C++ Golang 74.4 74.4 85.6 84.1 75.6 81.7 81.3 81.7 Java 73.8 84.8 88.2 88.4 Javascript Python Rust 76.2 81.1 83.7 86.0 75.0 82.3 84.6 83.5 73.8 81.1 83.7 82."
        },
        {
            "title": "I DIRECT PREFERENCE OPTIMIZATION RESULTS",
            "content": "Other than evaluating different preference datasets in terms of their reward modeling performance, we also tried training models using direct preference optimization (DPO, Rafailov et al. (2023)) and see if they the final LM can be improved. Our DPO experiments are based off Llama-3 8B model (Dubey et al., 2024) finetuned with TÜLU-2 SFT data (Ivison et al., 2023) to get reasonable initial policy. We use the same set of hyperparameters as is used in (Ivison et al., 2024). We report the performance on few benchmarks that benefit from DPO training, following the setups in (Ivison et al., 2024). Table 17 shows the results for our best hybrid preference mix, random mix baselines with different fractions of human data, and the base SFT model. Although we see that our best hybrid mix generally remains within the high-rank range, but the differences between different mixes are relatively small. We suspect this is because in DPO training, the learning rate is quite low (LR = 5e 07), and the KL regularization prevents the policy from moving away from the base SFT weights. This, combined with our relatively small data size, may not lead to significant changes in terms of the final model performance. Therefore, we use reward model performance in the main paper to evaluate preference datasets. Table 17: Comparison of DPO-trained models using different human-LLM preference mixes. Downstream Task Performance Pref. Mix Best Hybrid 100% Human 75% Human 50% Human 25% Human 100% Synth. BASE SFT Pref. Mix MULTIPREF (Appendix A) % Direct Human for Best Hybrid: 37.4% Helpsteer2 (Wang et al., 2024c) % Direct Human for Best Hybrid: 69.6% Avg. GSM8K BBH IFEval Codex AlpacaEval Avg. GSM8K BBH IFEval Codex AlpacaEval 56.67 54.93 54.25 55.59 56.15 56.37 52. 68.61 67.10 66.19 67.32 67.70 67.70 64.14 65.09 65.06 65.11 65.80 65.26 65.09 63.51 49.54 48.06 47.87 50.83 50.09 50.65 47.13 79.59 77.95 74.90 77.37 78.53 77.74 77.53 20.53 16.48 17.20 16.63 19.14 20.68 10.32 56.09 55.83 56.44 55.60 56.25 55.79 52. 65.73 65.13 65.73 64.97 65.81 64.90 64.14 65.29 64.97 65.32 65.01 64.77 65.34 63.51 58.96 56.56 56.56 57.67 58.23 59.33 47.13 75.13 77.89 79.06 74.42 76.53 75.39 77.53 15.34 14.59 15.52 15.93 15.91 14.01 10.32 AlpacaFarm (Dubois et al., 2023) % Direct Human for Best Hybrid: 67.2% ChatArena (Zheng et al., 2023b) % Direct Human for Best Hybrid: 23.0% Avg. GSM8K BBH IFEval Codex AlpacaEval Avg. GSM8K BBH IFEval Codex AlpacaEval Best Hybrid 100% Human 75% Human 50% Human 25% Human 100% Synth. BASE SFT 54.07 53.71 53.02 54.09 53.88 53.17 52.53 63.68 65.05 63.84 65.50 65.58 65.58 64.14 64.58 63.97 63.92 64.43 64.26 64.43 63. 51.20 54.34 53.05 52.13 51.39 53.97 47.13 74.46 72.89 71.54 72.82 74.19 71.02 77.53 16.40 12.29 12.77 15.57 13.98 10.86 10.32 56.75 55.32 56.20 56.17 55.55 56.11 52.53 68.76 66.87 67.02 67.55 66.41 68.46 64.14 65.49 65.24 65.29 65.57 65.17 65.17 63. 56.19 54.34 55.45 56.01 53.79 56.01 47.13 77.06 77.29 78.66 77.07 77.81 74.37 77.53 16.24 12.84 14.58 14.66 14.57 16.53 10."
        },
        {
            "title": "J REWARD MODEL TRAINING DETAILS",
            "content": "Table 18: Reward Model Training Hyperparameters For all the reward model training experiments in this work, we finetune from the TÜLU-2 13B SFT model introduced in Ivison et al. (2023). We use fixed set of hyperparameters listed in Table 18 to conduct the training. All reward model training runs for constructing the candidate dataset for the PPM are performed on 16 nodes of TPU v3 from Google Compute Engine. Hyperparameter Data Type Number of Epochs Optimizer Type Weight Decay Learning Rate End Learning Rate Warmup Ratio Accumulate Gradient Steps Sequence Length Batch Size Value bf16 1 AdamW 0.0 1e-5 1e-6 0.03 4 4096 128 34 CASE STUDY: HELPSTEER2 K.1 ANALYSIS OF HELPSTEER2 INSTANCES From 4.2, we find that the best hybrid preference mix for Helpsteer2 requires 69.6% instances to be routed to human annotators. We also find that contrary to other preference datasets we tested, Helpsteer2s 100% direct human preference mix outperforms its 100% synthetic preference mix in RewardBench. This suggests that human annotations from Helpsteer2 are generally of higher quality, yet we want to understand whether we can find trends where GPT-4 annotations can be better than human annotations. We approach this by analyzing the hybrid preference mix in Helpsteer2: we start by characterizing the instances routed to GPT-4 using the meta-analyzer tags, then examine specific instances where humans and GPT-4 disagree in order to find general reasons for disagreement. Characteristics of instances routed to GPT-4. By examining the tags extracted by the metaanalyzer, we find that 50% of instances routed to GPT-4 require subjects of expertise relating to Computer sciences and Business, causing long-tail distribution as shown in Figure 8. This differs slightly from the human-routed instances, where no single subject expertise dominates and the long-tail is less apparent, coinciding with our findings in 5 where several subjects of expertise can benefit from human annotation. Figure 8: Top ten subject of expertise needed to annotate instances for subset routed to GPT-4 (left) and subset routed to Humans (right) in Helpsteer2. We also observe that most instances routed to GPT-4 contain prompts that require basic domain knowledge to answer, as opposed to those instances routed to humans which only need general public knowledge (Figure 10). Upon closer inspection, we find that this trend is due to the proportion of Computer sciences and Business user queries, which necessitate basic domain knowledge (e.g., coding, architecting website application, etc.). Below are some examples of prompts under the Computer sciences subject, demonstrating different levels of required expertise: General public Basic domain knowledge What is the difference between encoded and unencoded URLs? make docker file based on ubuntu 22.04, install latest nginx, create sample index.html with random wiki What to ask when buying second hand imac...is it good computer for 7 year old boy? im having version discrepancies with babel. how can solve it. list me out all ways can solve it. Is it ethical for students to use chatGPT to submit essays? Tell me everything about how to setup and use the hyperspace data structure for data compression Figure 9: Helpsteer2 prompts related to the Computer sciences subject but with different levels of expertise. In the best hybrid mix, our framework routed majority of the General public instances to humans and the Basic domain knowledge instances to GPT-4. 35 Disagreement between humans and LMs. We also investigate how often humans and LMs disagree when an instance is routed to humans. On the human-routed subset, we find percentage agreement of 61.5% and Cohens κ of 0.30, indicating minimal agreement (McHugh, 2012). Upon inspecting these cases of disagrameent, we observe that common reasons include (1) high open-endedness or subjectivity in the user instruction, (2) annotators choosing different responses when both are correct, and (3) incorrect GPT-4 preference. Below are some examples for each type: Figure 10: Proportion of prompts routed to humans or GPT-4 that belong to specific level of expertise. SUBJECTIVITY IN THE USER INSTRUCTION Prompt: How can simplify and improve this, making it sound warm yet professional? We are excited to introduce Augustaux, curated collections of old-world paintings turned into giclee fine art prints for your wall. We strive to make beautiful old art accessible and produce it in way that maintains the texture, detail and quality it deserves. ): Heres revised version of your text that is more concise and easier Completion (preferred by to read: \"Welcome to Augustaux, where we bring you the finest selection of vintage art prints for your walls. Our giclee fine art prints are expertly crafted to capture the texture, detail, and quality of the original works. Discover world of timeless beauty and discover the perfect piece to enhance your home or office today!\" Completion (preferred by ): We are thrilled to introduce Augustaux, curated collection of old-world paintings turned into giclee fine art prints for your walls. Our mission is to make beautiful old art accessible to all, while maintaining the texture, detail, and quality it deserves. Each print is carefully produced to ensure that the beauty and artistry of the original painting is preserved, bringing touch of elegance and sophistication to any space. DIFFERENCE PREFERENCES BUT BOTH RESPONSES ARE CORRECT Prompt: How many feet is five feet plus four feet? Completion (preferred by ): The answer is 9 feet. 5 feet + 4 feet = 9 feet Therefore, 9 feet is the correct answer. Its important to note that this calculation assumes that we are using the imperial measurement system, in which 1 foot is equal to 12 inches. In other measurement systems, the answer may be different. Additionally, its always good idea to double-check calculations and make sure that units are properly converted and accounted for, especially when dealing with measurements. Completion (preferred by together. Five feet plus four feet is the same as 5 + 4 = 9 feet. So the answer is 9 feet. ): To find the total number of feet, need to add the two lengths 36 GPT-4 PREFERENCE IS INCORRECT Prompt: Which of the following statements are true? give reasons for your answers. Completion (preferred by evaluation, please provide the statements in question. ): There are no statements for evaluation. To provide an accurate ): Completion (preferred by 1. All mammals are warm-blooded. This is true because the term \"warm-blooded\" refers to the ability of mammals to maintain constant body temperature, regardless of the external temperature. This is achieved through the use of complex system of thermoregulation, which involves the regulation of metabolic rate, shivering, and sweating. This allows mammals to maintain stable body temperature, even in changing environmental conditions. 2. All mammals are vertebrates. This is also true because the term \"mammal\" refers to specific group of animals that are characterized by the presence of certain physical and physiological features, such as the presence of hair or fur, the ability to produce milk, and the presence of placenta during pregnancy. All mammals are therefore classified as vertebrates, which means that they have backbone or spinal column. (continued) On the GPT-4 routed subset, the percentage agreement between humans and GPT-4 is 57.8% and Cohens κ of 0.23. We find that causes for disagreement often include prompts that require an AI assistant to generate content or roleplay certain character. OPEN-ENDED REQUESTS FOR GENERATING CONTENT Prompt: am working on Low-Code Dynamic Rule Engine meant to be used by non-engineers. Can you write pitch for this product? Completion (preferred by that is meant to be used by non-engineers: ): Sure! Heres short pitch for Low-Code Dynamic Rule Engine \"Introducing the Low-Code Dynamic Rule Engine - powerful and intuitive platform that enables businesses to quickly and easily create and manage complex rules and regulations without the need for extensive technical expertise. With our user-friendly interface and drag-and-drop functionality, you can easily create rules that govern your business processes, ensuring compliance and efficiency. (continued) Completion (preferred by Sure, heres possible pitch for your Low-Code Dynamic Rule Engine: ): Are you tired of complex rule engines that require expensive engineers to set up and maintain? Look no further than our Low-Code Dynamic Rule Engine. Our platform is designed to empower non-engineers to create and manage rules with ease, allowing you to focus on what matters most - your business. (continued) 37 ASKING THE ASSISTANT TO ROLEPLAY Prompt: Can you be my Pinescript Professor by providing me with syllabus and guiding me through each step of the process until am able to learn and understand pinescript well enough to code using it? Completion (preferred by outline of what well cover: ): Sure, can help you get started with PineScript! Heres general Introduction to PineScript: Well start by going over the basics of PineScript syntax and how it works. Well also discuss some of the key concepts you need to know to get started with coding in PineScript. Building Your First Script: Once you have good understanding of the basics, well move on to building your first script. Well go over the steps involved in creating simple trading strategy and how to test it out. (continued) Completion (preferred by Sure, Id be happy to help you learn PineScript! Heres suggested syllabus for learning PineScript: ): 1. Introduction to PineScript: Start by learning the basics of PineScript syntax and structure. Study the official PineScript documentation and tutorial to get familiar with the language. 2. Basic PineScript Functions: Learn the basic functions and operators in PineScript, such as arithmetic operators, comparison operators, and logical operators. 3. Conditional Statements: Study the different types of conditional statements in PineScript, such as if-else, switch, and ternary operators. (continued) K.2 TRAINING THE PPM ON HELPSTEER2 We also trained the PPM on 200 candidates generated from Helpsteer2 in order to test if our routing framework can generalize to other training datasets. Figure 11 shows that for fixed budget, the hybrid annotations obtained from our framework still outperforms that of random selection. Figure 11: Comparison between our routing framework and random selection given fixed annotation budgets. We report the average of the RewardBench score across three runs. K.3 ROUTING INSTANCES IN THE HELPSTEER2-PREFERENCES DATASET We apply the routing framework using the same PPM from 4.2 to the Helpsteer2-Preferences dataset (Wang et al., 2024b). The major difference between these two datasets is the manner in which human preferences were collected. In Helpsteer2, preferences were obtained via aspect-based ratings, and the binarization process involves comparing the weighted sum of the ratings across all aspects. On the other hand, Helpsteer2-Preferences contains pairwise feedback, where annotators clearly indicate Pref. Mix Helpsteer2 Wang et al. (2024c) Helpsteer2-Pref Wang et al. (2024b) 100% Human 75% Human Random Routed 50% Human Random Routed 25% Human Random Routed 100% Synth."
        },
        {
            "title": "Best Hybrid",
            "content": "72.4 73.1 73.4 69.9 73.1 71.9 74.0 65.9 79. 71.4 72.2 72.4 69.0 71.4 65.5 67.3 67.0 74. Table 19: Performance comparison between Helpsteer2 (Wang et al., 2024c) and Helpsteer2-Preferences (Wang et al., 2024b) on RewardBench (Overall) for different annotation budgets. Figure 12: Comparison between our routing framework and random selection given different annotation budgets on the Helpsteer2-Preferences dataset (Wang et al., 2024b). whether one response is better than the other in 6-point Likert scale, where one option indicates that neither response is valid. To obtain LM preferences, we prompt GPT-4 Turbo with the user requests and model responses from the Helpsteer2-Preferences dataset, together with the annotation guidelines in Wang et al. (2024b), and obtain preference strength from -3 (Response 1 is much better than Response 2) to 3 (Response 2 is much better than Response 1). We binarize the responses and then remove any ties. Figure 12 shows that our routing framework also generalizes to the Helpsteer2-Preferences dataset, with the best hybrid requiring 67.6% of human annotations. Interestingly, we also find that the aspect-based Helpsteer2 dataset (Wang et al., 2024c) outperforms the pairwise Helpsteer2-Preference dataset on the 100% human mix, and vice-versa on the 100% synthetic mix, as shown in Figure 19."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "The Ohio State University",
        "University of Washington"
    ]
}