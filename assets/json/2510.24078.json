{
    "paper_title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification",
    "authors": [
        "William Yang",
        "Xindi Wu",
        "Zhiwei Deng",
        "Esin Tureci",
        "Olga Russakovsky"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) models are increasingly used for synthetic dataset generation, but generating effective synthetic training data for classification remains challenging. Fine-tuning a T2I model with a few real examples can help improve the quality of synthetic training data; however, it may also cause overfitting and reduce diversity in the generated samples. We propose a fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for fine-grained classification. Given a small set of real examples, we first extract class-agnostic attributes such as scene background and object pose. We then explicitly condition on these attributes during fine-tuning of the T2I model and marginalize them out during generation. This design mitigates overfitting, preserves the T2I model's generative prior, reduces estimation errors, and further minimizes unintended inter-class associations. Extensive experiments across multiple T2I models, backbones, and datasets show that our method achieves state-of-the-art performance in low-shot fine-grained classification when augmented with synthetic data. Concretely, BOB outperforms DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning a CLIP classifier with five real images augmented with 100 synthetic images). In three of the four benchmarks, fine-tuning downstream models with 5 real images augmented with BOB achieves better performance than fine-tuning with 10 real images. Collectively, BOB outperforms prior art in 18 of 24 experimental settings, with 2+% accuracy improvements in 14 of these settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 8 7 0 4 2 . 0 1 5 2 : r Preprint BEYOND OBJECTS: CONTEXTUAL SYNTHETIC DATA GENERATION FOR FINE-GRAINED CLASSIFICATION William Yang Xindi Wu Zhiwei Deng Esin Tureci Olga Russakovsky Princeton University Google DeepMind williamyang@cs.princeton.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-image (T2I) models are increasingly used for synthetic dataset generation, but generating effective synthetic training data for classification remains challenging. Fine-tuning T2I model with few real examples can help improve the quality of synthetic training data; however, it may also cause overfitting and reduce diversity in the generated samples. We propose fine-tuning strategy BOB (Beyond OBjects) to mitigate these concerns for fine-grained classification. Given small set of real examples, we first extract class-agnostic attributes such as scene background and object pose. We then explicitly condition on these attributes during fine-tuning of the T2I model and marginalize them out during generation. This design mitigates overfitting, preserves the T2I models generative prior, reduces estimation errors, and further minimizes unintended inter-class associations. Extensive experiments across multiple T2I models, backbones, and datasets show that our method achieves stateof-the-art performance in low-shot fine-grained classification when augmented with synthetic data. Concretely, BOB outperforms DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning CLIP classifier with five real images augmented with 100 synthetic images). In three of the four benchmarks, fine-tuning downstream models with 5 real images augmented with BOB achieves better performance than fine-tuning with 10 real images. Collectively, BOB outperforms prior art in 18 of 24 experimental settings, with 2+% accuracy improvements in 14 of these settings. Code is available at https://github.com/princetonvisualai/BeyondObjects."
        },
        {
            "title": "INTRODUCTION",
            "content": "Powerful text-to-image (T2I) models trained on internet-scale datasets (Schuhmann et al., 2022; Rombach et al., 2022) have shown promise in the creation of synthetic data for representation learning (Tian et al., 2023; 2024), 3D synthesis (Poole et al., 2023), and image editing (Hertz et al., 2022; Mokady et al., 2022; Brooks et al., 2023). However, there remains considerable performance gap when using synthetic data as training data for downstream tasks such as classification (Burg et al., 2023; Fan et al., 2024; Geng et al., 2024). Ideally, given target classification task described in language (e.g., train an aircraft classifier to distinguish 747-300 from 747-400), T2I model can be directly used to generate training images of the desired classes. key challenge limiting T2I models from generating informative images is the model estimation errors caused by misalignment between the T2I models learned distribution and the target task (Geng et al., 2024). For example, the visual difference between 747-300 and 747-400 is subtle: the presence of winglet on slightly longer wing. In consequence, the introduction of low level artifacts and incorrect visual compositions introduce challenge for the task of fine-grained recognition. One approach to mitigate model estimation errors for these classes is to provide few real images to fine-tune the T2I model (Wang et al., 2024; Kim et al., 2024). The underlying T2I model needs to rely heavily on additional guidance from provided examples to generate not only accurate samples but also diverse enough samples to augment training of downstream classifier. However, operating in the few-shot regime requires special considerations (Yue et al., 2020). The increased expressivity from fine-tuning can introduce trade-off where the T2I model starts to overfit to the few examples, losing its strong world prior and hurting the diversity of the synthetic dataset. 1 Preprint In this work, we tackle fine-grained classification with synthetic data generation by introducing BOB (Beyond OBjects). BOB mitigates overfitting during T2I fine-tuning by obtaining the background and pose for each example via captioning model and incorporating them into the text condition. During data generation, background and pose pairs are sampled across the dataset, effectively marginalizing out any unintended associations across classes. We provide comprehensive evaluation across three backbones, two T2I models, four datasets, two data scales, and seven existing methods, to demonstrate the effectiveness of BOB. We observe the most considerable gain on Aircraft, dataset where T2I perform poorly and fine-tuning benefits the most. Training the CLIP model (Radford et al., 2021) using 5 real images augmented with 100 synthetic images per class result in 7.4% increase in classification accuracy from 50.0% when augmented with the previous fine-tuning method DataDream (Kim et al., 2024) to 57.4% with BOB. Further, across three of the four dataset (Aircraft, Cars, and CUB), using 5 real images augmented with BOB generated images results in better classification performance than using 10 real images: e.g., CLIP fine-tuned on CUB achieves accuracy of 75.8% with 5 real images augmented with BOB generated images and only 74.6% with 10 real images without augmentation. Overall, BOB outperforms existing state-of-the-art methods by at least 2% on 18 of the 24 experimental settings (backbone, dataset source, and dataset size). On the six remaining settings (on the Pets (Parkhi et al., 2012) dataset), BOB offers competitive performance within 1% of state-of-the-art. To summarize, we make the following contributions: 1. We introduce stronger supervision with more detailed captioning during T2I fine-tuning to mitigate model overfitting and enhancing prior preservation (3.1). 2. We marginalize out unintended inter-class associations by randomly sampling classagnostic features (background, pose) across the whole dataset (3.2). 3. We provide comprehensive evaluation (4) across seven previous methods, two T2I models, and 24 different experimental settings to demonstrate our methods outperforms previous methods in 18 of the 24 settings with competitive performance in the rest."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Personalization. Many personalization methods serve as inspiration for approaches aimed at synthetic data generation for classification. Personalization methods seek to guide and control T2I models beyond text descriptions, typically using few image exemplars. These approaches have proven effective at tailoring T2I models to reproduce highly specific visual concepts (Gal et al., 2023; Ruiz et al., 2023; Kumari et al., 2023; Ye et al., 2023; Zhang et al., 2023; Li et al., 2023b; Zong et al., 2024; Zhao et al., 2025). However, while they enable strong concept-level control, their objectives differ from those required for classification-oriented synthetic data. They emphasize concept fidelity over diversity, often limiting intra-class variation and inter-class separabilityboth essential for robust classifier training. Therefore, such personalization methods are insufficient for addressing the challenges of synthetic data generation for classification. Synthetic data for classification. The field of synthetic data generation for classification has initiated significant shift, moving toward leveraging powerful T2I models. This paradigm shift contrasts sharply with traditional data augmentation methods, such as CutMix (Yun et al., 2019) and Mixup (Zhang et al., 2018). These techniques interpolate between existing data, which helps smooth the decision function but is limited in sample diversity and fidelity. In contrast, pre-trained T2I model provides world prior that significantly enhances both sample diversity and fidelity. Early works demonstrated the utility of T2I models for classification. Real Guidance (He et al., 2023) demonstrated that utilizing these T2I models with simple class descriptions and few reference images can improve classification performance. Da-fusion (Trabucco et al., 2023) incorporate Textual Inversion (Gal et al., 2023) on the few reference images to generalize to unknown concepts. Subsequent research has largely focused on two parallel areas: improving fine-tuning and enhancing generation. For fine-tuning, methods like Diff-Aug, Diff-Gen, and Diff-Mix (Wang et al., 2024) and DataDream (Kim et al., 2024) focused on adapting different components of the T2I model (U-Net and text-encoder) to the target data. On the generation side, previous works primarily focus on better prompt design (Sariyildiz et al., 2023; Yu et al., 2025), prompt augmentation with image captions (Dunlap et al., 2023; da Costa et al., 2023), diffusion latents interpolation (Zhou et al., 2023; Wang & Chen, 2025), leveraging vision-language models (Michaeli & Fried, 2024), and hard examples generation (Koohpayegani et al., 2023; Hemmat et al., 2024; Askari-Hemmat et al., 2025). Unlike 2 Preprint Figure 1: Overview of BOB. We extract background and pose attributes from training images using captioning model (Step 1), apply context preservation by fine-tuning the T2I model with enriched captions containing class names and context attributes (Step 2), and then perform context marginalization by generating synthetic data through randomly sampling background-pose pairs across the entire dataset (Step 3-4). This preserves class-relevant features while reducing spurious class-context associations. previous work that focuses primarily on fine-tuning or generation, our approach integrates diverse captions across both the fine-tuning and generation stage for superior data quality and variety. Diffusion classifier. Using the diffusion model directly for image classification has shown promising performance (Li et al., 2023a; Clark & Jaini, 2023). The natural question of directly using the T2I model as classifier emerges, not only because it eliminates the need to train downstream classifier, but also because utilizing the T2I model introduces useful inductive biases such as the reduction of spurious correlations (Li et al., 2025) and better alignment with human vision (Jaini et al., 2024). However, the compute required to perform such classification is considerably more expensive, increasing each classification decision from seconds to over 10 minutes in the case of ImageNet (Russakovsky et al., 2015; Li et al., 2023a). Therefore, there is still need for research on how to distill these capabilities into downstream classifier."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We propose an effective approach for fine-grained synthetic data generation that addresses the critical problem of overfitting during T2I model fine-tuning and image generation (Figure 1). This overfitting exists in both modalities: in the texts, the classification tasks ambiguous text-to-image mapping inherently loses T2I controllability by failing to articulate the intra-class visual range; and in the images, the few-shot regime fosters unintended inter-class associations. Our method mitigates both overfitting issues via two stages: Context Preservation during fine-tuning and Context Marginalization during data generation. We describe both in detail in this section. 3.1 CONTEXT PRESERVATION Our first mitigation strategy Context Preservation aims to recover the intra-class visual range lost in the text modality. While T2I models require detailed text-to-image mappings, classification datasets only provide generic labels. Previous approaches use class-specific templates (e.g., photo of [classname]) but this simplification reduces diverse visual information to single description, leading to loss of T2I controllability. To address this, we propose associating each image with unique, descriptive text. We achieve this by extracting and explicitly encoding class-agnostic attributes (background and pose) into the text conditioning. This enables the model to learn the specific association between these attributes and the visual context during fine-tuning. Concretely, each image is associated with unique caption following the template: [descriptor] photo of [classname] in the [background] background with the [pose] pose. 3 Preprint The [descriptor] is dataset-level general descriptor such as aircraft or birds. [classname] is the name of the class provided by the dataset. The background/pose is extracted by captioning model for each image. We leverage the Qwen 2.5VL-7B, state-of-the-art vision-language model (Bai et al., 2025) to extract the background with the following prompt: describe the background of the [descriptor] in as few words as possible. Refer to the [descriptor] as simply [descriptor]. Similarly, we use the same prompt with background replaced by pose to extract the pose. We also store the extracted background and pose into caption bank = {(bi, pi)}N i=1, where bi and pi represent the background and pose attributes of the i-th training image, respectively (Fig. 1, steps 1-2). This prompting approach serves two purposes: (1) it provides necessary context to guide accurate attribute extraction, and (2) it prevents potential leakage of class-specific information by maintaining generic references to the object category. Once we established the image-text pairing, we fine-tune the diffusion model using the standard diffusion objective. We follow the standard parameter-efficient fine-tuning procedure by using LowRank Adaptation (LoRA) (Hu et al., 2022) to fine-tune the attention layers of both the U-Net (Ronneberger et al., 2015) and CLIP text encoder (Radford et al., 2021). Consider the following notations: θ as parameters of the attention layers, the image as x, the text as y, the CLIP text encoder as c(y), timestep of diffusion process as t, and the U-Net model as ϵθ(x, cθ(y), t). The parameters θ are updated by minimizing the following objective: E(x,y)D, ϵN , tU ϵ ϵθ(x, cθ(y), t)2 2. 3.2 CONTEXT MARGINALIZATION Our second stage Context Marginalization directly addresses the unintended inter-class associations in the image modality. Having established fine-tuned T2I model that explicitly associates classagnostic attributes (pose and background) with visual context via Context Preservation, we now leverage this learned attribute representation to generate diverse synthetic data. The key insight is that the contextual attributes preserved during fine-tuning can be strategically leveraged during generation to break the spurious class-to-context associations and produce robust synthetic images. Figure 2: Causal graph of generative process. To better understand why spurious inter-class associations emerge in the data-scarce setting, consider the generation process of our training data image is generated given the classthat our T2I model emulates: relevant attributes and the class-agnostic attributes Z. We introduce random variable corresponding to unique ID for every possible training data. The sample ID would consequentially describe the observed class-relevant attributes and class-agnostic attributes Z. This generative process can be formalized as structured causal model (Pearl, 2009) shown in Figure 2. When there are sufficient data, then becomes irrelevant because and become independent with (IY ) (I). Similar argument can be made between and Z. However, when data is scarce, the class-relevant attributes and the class-agnostic attributes become predictive of I. In consequence, this would introduce spurious correlation: X. These spurious correlations between class labels and contextual attributes are more prominent in fine-grained classification datasets due to the scarcity of training examples. To remove this spurious correlation and directly model the relationship between and , we would like to sample from the intervention distribution (Xdo(Y )) by invoking the back-door criterion (Pearl, 2009) for the following equivalence: (Xdo(Y )) = (cid:80) Our Context Marginalization procedure to implement this principle as illustrated in step 3 and 4 of Figure 1. We generate synthetic images using the sample template structure from Context Preservation, randomly sampling background-pose pairs (b, p) from our caption bank regardless of their original class. The random sampling from the caption bank (b, p) is equivalent to to sampling from (Z), and image generation conditioned on the class name with the sampled background and pose corresponds to (XY, Z). As such, our procedure approximates sampling from the interventional distribution (Xdo(Y )), effectively marginalizing out spurious correlations. (XY, Z)P (Z)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we will first go over the experimental setup for our comprehensive evaluation. Next, we will perform detailed quantitative analysis on the experimental results to demonstrate advantages 4 Preprint of our proposed approach with respect to previous methods. Lastly, we present additional analysis showing that our method produces synthetic data that better aligns with the target data distrbution and perform ablations showing the necessity of both the context preservation and content marginalization for generating highly informative data for the downstream classification task. 4.1 EXPERIMENTAL SETUP Datasets. We follow the standard settings for data-scarce augmentation where we have 5 and 10 real images per class as the few-shot setting. We follow the previous evaluation setting (Wang & Chen, 2025) where we use Aircraft (Maji et al., 2013), CUB (Wah et al., 2011), Car (Krause et al., 2013), and Pets (Parkhi et al., 2012) datasets for evaluation. Additionally, we also use the CUB-LT (Samuel et al., 2021) and Flower-LT (Wang et al., 2024) dataset to extend our proposed methodology to different data-scarce setting: long-tail classification. Backbones. We use three different backbones with different degrees of language supervision during pre-training for downstream task fine-tuning. For backbone with dense language supervision, we use the CLIP VIT-B/16 model (Dosovitskiy et al., 2020; Radford et al., 2021). For backbone with weak language supervision, we use the ImageNet classification-trained ResNet-50 model (He et al., 2016). Finally, for backbone with no language supervision, we use the masked auto-encoder (MAE) VIT-B/16 model pre-trained on ImageNet (He et al., 2022). Unlike previous works which typically focus on one of these types, we decided to have all three language supervision settings to provide wider perspective towards the behavior and usefulness of different methods. Baseline methods. We use seven popular existing data generation or augmentation methods for comparisons: RealGuidance (He et al., 2023), Da-fusion (Trabucco et al., 2023), Diff-Aug, DiffGen, Diff-Mix (Wang et al., 2024), DataDream (Kim et al., 2024), and Diff-II (Wang & Chen, 2025). In the synthetic data generation stage, we use the hyperparameters and procedure provided in the original paper. We utilize the fine-tuned T2I weights provided by the Diff-II paper for synthetic data generation on Stable Diffusion v1.5 (Rombach et al., 2022). We reproduce Diff-Aug, DiffGen, and Diff-Mix using the same T2I model from the paper: Stable Diffusion v1.5. Similar for Datadream, we reproduce their results using the same T2I model: Stable Diffusion v2.1-base. Since RealGuidance and Da-fusion are relatively older methods on older T2I models, we reproduce their results with the more recent T2I model of Stable Diffusion v2.1-base. In summary, when possible, we utilized the relevant T2I model for each prior method to compare with directly. Implementation details. For fair comparison with existing methods, we fine-tune our method on both Stable Diffusion v1.5 and Stable Diffusion v2.1-base. We utilize the same hyperparameters as DataDream with the exception of longer fine-tuning: 400 epochs instead of 200 epochs. For fair comparisons, we also extend the DataDream method to 400 epochs. We show in Section B.1 of the Appendix that both methods improve with the increased training. In the downstream classification fine-tuning process, we replicate the few-shot examples such that there is close to 50/50 split of real and synthetic images. The classification objective function is weighted average on the cross-entropy loss between real and synthetic data: = λ CE(fθ(xreal), yreal) + (1 λ) CE(fθ(xsyn), ysyn) (1) where λ is hyperparameter and fθ(x) is the classifier with parameters θ. Mixup (except for the Diff-Mix setting) and Cutmix augmentation is applied separately between the real and synthetic data. For fair comparison, we perform hyperparameter tuning by training only for 10 epochs and evaluating separate validation set before finally training the downstream classifier with the best hyperparameters for 100 epochs. Refer to Section of the appendix for more details. 4.2 FEW-SHOT CLASSIFICATION In the few-shot classification setting we use 5 or 10 real images per class which we use to fine-tune the pre-trained T2I model before generating 100 synthetic images per class. In training the downstream classifier, the real images are replicated such that there is 50/50 split between synthetic and real data. We present the performance of our method compared with seven existing baselines in Table 1: Diff-Aug, Diff-Gen, Diff-Mix, Diff-II using Stable Diffusion v1.5 and RealGuidance, Da-fusion and DataDream using Stable Diffusion v2.1. Downstream tasks include Aircraft classification, task with lowest maximum starting baseline performance of 44.37%, moderate maximum 5 Preprint Table 1: Few-shot classification accuracy. The best performing method is in bold and the second best is underlined. Across three different backbones used as downstream classifier, our method outperforms existing methods by considerable margin on Aircraft (AirC), Car, and CUB. On the Pets dataset, our method obtains similar performance of previous methods."
        },
        {
            "title": "Method",
            "content": "Real Only Diff-Aug Diff-Gen Diff-Mix Diff-II BOB (ours) RealGuidance Da-fusion DataDream BOB (ours) Real Only Diff-Aug Diff-Gen Diff-Mix Diff-II BOB (ours) RealGuidance Da-fusion DataDream BOB (ours) Real Only Diff-Aug Diff-Gen Diff-Mix Diff-II BOB (ours) RealGuidance Da-fusion DataDream BOB (ours) SD Ver. v1.5 v1.5 v1.5 v1.5 v1.5 v2.1 v2.1 v2.1 v2.1 v1.5 v1.5 v1.5 v1.5 v1.5 v2.1 v2.1 v2.1 v2. v1.5 v1.5 v1.5 v1.5 v1.5 v2.1 v2.1 v2.1 v2.1 5-shot 10-shot"
        },
        {
            "title": "Car",
            "content": "79.01 80.93 81.60 80.19 82.16 88.10 80.23 79.83 84.58 88.41 56.16 70.95 80.73 76.58 82.95 88.80 68.76 73.99 86.15 88.64 53.94 72.22 82.66 78.16 82.09 87.73 68.78 73.39 85.81 88."
        },
        {
            "title": "CUB",
            "content": "67.72 68.05 69.21 67.45 70.41 75.84 69.93 69.33 70.74 75.43 48.22 57.24 60.91 53.28 63.60 68.78 57.34 59.03 67.40 71.38 39.63 55.35 62.79 52.50 66.53 69.23 52.47 51.90 69.07 73."
        },
        {
            "title": "Pets",
            "content": "92.76 92.27 91.69 92.78 92.75 92.24 92.78 92.59 92.67 92.73 83.17 85.09 86.95 85.36 87.63 86.38 87.25 86.17 84.85 87.00 76.81 74.76 77.32 81.28 88.33 87.46 80.62 75.52 80.38 86."
        },
        {
            "title": "AirC",
            "content": "55.73 57.19 58.60 52.73 60.25 68.88 52.96 55.27 63.89 67.61 55.48 57.91 60.32 52.21 62.81 70.79 49.23 56.69 67.99 73.78 57.61 60.64 63.85 60.31 65.20 75.70 57.13 58.57 71.20 75."
        },
        {
            "title": "Car",
            "content": "84.87 86.07 88.43 87.31 89.02 92.42 85.36 79.83 90.26 92.00 78.50 85.34 88.85 86.41 88.53 92.60 83.13 85.80 91.29 92.52 79.12 86.79 90.92 88.29 90.39 93.16 84.58 87.61 92.12 92."
        },
        {
            "title": "CUB",
            "content": "74.59 77.29 76.45 73.60 77.05 81.26 76.45 76.02 78.90 80.95 68.05 72.74 72.40 68.16 73.60 78.62 70.43 71.38 77.48 79.52 62.50 75.72 77.10 69.09 77.05 80.17 73.33 73.33 79.15 81."
        },
        {
            "title": "Pets",
            "content": "93.65 93.44 93.57 93.34 93.02 93.31 92.79 94.04 93.90 93.77 86.75 87.40 89.93 88.63 89.95 89.04 87.23 88.98 88.38 89.40 82.97 84.41 85.15 86.40 89.21 89.56 86.94 83.21 86.35 88."
        },
        {
            "title": "AirC",
            "content": "44.37 44.67 47.54 42.09 49.02 55.85 43.12 42.39 50.04 57.37 39.62 43.27 48.42 38.27 52.28 60.02 35.53 42.60 54.58 60.31 41.13 44.28 51.79 41.46 54.90 62.32 38.70 46.98 58.54 61.21 C e I M baseline performance tasks of Car classification and CUB classification (79.01% and 67.72%), to Pets, with relatively high maximum baseline performance of 92.76% in the 5-shot setting. Our method improves performance over all the baseline and the best performing existing method, in all tasks with the exception of Pets. For Aircraft, Car and CUB downstream tasks, BOB improves performance by at least 6.36% and up to 34.54% over the baseline of training with only the real data, and at least 2.77% and up to 10.25% over the best performing existing method. Detailed analysis focusing on specific aspects of these experiments follow. Aircraft classification task. The pre-trained stable diffusion model has the least amount of knowledge about the Aircraft dataset, as indicated by the very poor performance of RealGuidance which is personalization and fine-tuning free method. Focusing on the 5-shot setting for the FGVCAircraft classification task, using the ImageNet trained ResNet-50, augmenting real images with RealGuidance generated images results in degradation in performance of the ImageNet pretrained model by 4.09% and 6.25% in 5and 10-shot settings. Improvements by other previous methods range in 3.65-14.96% with DataDream performing the best, while our method, BOB leads to 20.69% improvement raising the accuracy from 39.62% to 60.31%, 5.73% higher improvement than DataDream. Including the CLIP and MAE backbones for downstream tasks, BOB provides 3.787.33% improvement in the 5-shot and 4.65-5.79% in the 10-shot settings over the best performing previous method for this downstream task. 6 Preprint Table 2: Long-tail classification accuracy. The best performing method is in bold and the second best is underlined. The expected accuracy across all the classes is reported. Many reports classes with over 20 (30) examples for CUB-LT (Flower-LT). Medium reports classes with between 5-20 (10-30) examples for CUB-LT (Flower-LT). Few reports classes with under 5 (10) examples for CUB-LT (Flower-LT). Imbalanced factor (IF) are indicated in bold. Results from fine-tuning an ImageNet pre-trained ResNet-50 indicates that BOB outperforms existing methods. Method SD Ver. CUB-LT IF=100 Many Med Few All 50 10 Flower-LT IF=100 Many Med Few All 50 10 86.00 65.22 17.84 37.73 49.32 60.09 99.45 97.70 60.74 72.08 87.41 93.70 Real Only v1.5 87.22 68.69 26.06 43.95 59.47 67.78 99.79 96.71 71.25 79.17 92.93 95.12 Diff-Gen v1.5 87.70 73.12 32.76 49.46 60.61 67.06 99.61 98.47 73.17 80.93 91.99 94.77 Diff-Mix Diff-II v1.5 87.54 72.16 44.05 56.10 64.52 70.28 99.82 98.45 79.51 85.35 95.20 97.62 BOB (ours) v1.5 88.48 75.37 52.24 62.19 70.57 74.54 100.0 98.56 84.13 88.60 95.68 96.13 DataDream v2.1 87.25 71.23 39.72 53.42 66.05 72.32 100.0 98.67 79.96 85.73 94.13 96.08 BOB (ours) v2.1 88.43 75.56 53.47 63.06 73.00 76.28 99.45 98.41 83.48 88.07 96.85 97.80 Pets classification task. Pets classification task has the highest baseline performance of 76.81%, 83.17% and 92.76% with five real images and 82.97%, 86.75% and 93.65% with 10 real images for the MAE, ImageNet and CLIP backbones indicating that this downstream dataset distribution is represented much better in these backbone models compared to other datasets. We note two interesting observations. First, it appears that the Stable Diffusion model have additional knowledge of this dataset since RealGuidance can improve performance by more than 4%. Second, there is low variability in performance across all the methods in CLIP and ImageNet backbones. This is most likely due to significant overlap from the pre-training data. We comment on this in more detail in Section B.3 of the Appendix. Overall, our method BOB improves performance on par with existing methods in this task reaching performance of 87.46%, 87 and 92.73% in the 5-shot and 89.56, 89.40 and 93.77% in the 10-shot setting, within 1% of performance of best existing method. Comparison of 5-shot and 10-shot performance. An interesting comparison is to look at the informativeness of synthetic data with respect to additional real images. We observe that, the addition of synthetic data generated by funetuning with 5 real images to these images using BOB outperforms using 10 real images in all datasets with the exception of Pets, indicating that BOB allows for efficient sampling of training data for fine-tuning the target downstream task. 4.3 LONG-TAIL CLASSIFICATION To demonstrate that our method extends beyond the few-shot classification setting, we perform experiments in long-tail classification setting using the CUB-LT dataset (Samuel et al., 2021) and Flower-LT (Wang et al., 2024) using ImageNet pre-trained ResNet-50 backbone. In the long-tail classification settings, the number of images per class used for fine-tuning is artifically skewed to follow an exponential distribution specified in Samuel et al. (2021) for CUB-LT and Wang et al. (2024) for Flower-LT. For synthetic data generation, we set budget of 200 total images per class. Similar to the few-shot examples, we duplicate the real examples by constant factor such that the number of images in the head (classes with abundant real images) are close to 200 images, arriving at the number of images defined by the following equation: 200 number of real images (c is 6 for CUB-LT and 5 for Flower-LT). Table 2 summarizes results where BOB outperforms existing methods by considerable margin in the long-tail classes. Performance gains on CUB-LT. CUB-LT is relatively challenging datasets for long-tail classification with relatively lower accuracies when training with only real data. In this challenging setting, we observed across every imbalanced factor on CUB-LT datasets, our method BOB outperforms existing ones by margin of at least 4%. We observe the greatest improvement in performance when there is large class imbalance. For the setting with the largest imbalanced (IF=100), our method improves from previous method by at least 6%: 56.10% with Diff-II to 62.19% with ours and 53.42% with DataDream to 63.06% with ours. The source of the improvement is from the Preprint Figure 3: Visualizations. left. 737-400 images from real data and synthetic data generated by DiffII, DataDream, and BOB (ours). Diff-II generates images with aircrafts with high contrast in simple backgrounds. DataDream generates more realistic aircrafts that are only on the ground. Our method BOB generate realistic aircrafts in very diverse settings such as taking off, flying, or on the ground with mountainous background, resulting in images that are visually similar to real images. improved accuracy from classes with very few real examples. Although classes with many examples tend to have similar accuracies (87.54% with Diff-II vs. 88.48% with ours), there is large increase in performance in the classes with few examples: 44.05% with Diff-II to 52.24% with ours. These findings provide strong evidence that for challenging long-tail tasks such as CUB-LT, BOB generates more informative data than previous methods. Competitive performance on Flower-LT. In contrast to the dataset CUB-LT, Flower-LT is relatively easier task with fewer classes and higher accuracies. For the most difficult part of this benchmark with IF=100, our method achieves 2%-3% gain. For IF=50 and IF=10, our method performs competitively against previous methods with accuracies within 1% range. These results show that even for easier long-tail tasks, BOB generates the best synthetic datasets. 4.4 ANALYSIS Having demonstrated that our proposed method BOB outperforms existing methods for few-shot classification and long-tail classification, we next analyze why it works better. We perform analysis using the 10-shot Aircraft data setting with ImageNet-1K pre-trained ResNet-50 backbone to reveal that: (1) our method produces synthetic data that more closely resembles and aligns with real data. (2) the performance gains are not due to distilling class knowledge from the captioning model, and (3) both context preservation and marginalization are important for creating high performing data. Qualitative analysis. Visualization of images generated by our method shown in Figure 3 produces sharp contrast compared to existing methods. We observe in Figure 3 that the previous methods lack realism or diversity. With Diff-II, the aircrafts have high contrast with background that is typically monotonous. DataDream generates realistic looking images with many In contrast, our method complexities in the background but the aircrafts are all on the ground. BOB, produces images that are both realistic and diverse: we have that is on the ground with mountain in the background, an aircraft taking off, etc. If we compare with images from the real dataset, it is clear that the synthetic images generated from BOB resemble the closest: suggesting that perhaps the source of performance gain is better alignment with the real data distribution. Real vs. synthetic distribution. We provide an analysis of how well the synthetic dataset distributions align with the target dataset by computing the per-class Frechet Inception Distance (FID) (Heusel et al., 2017) between the whole training dataset and the synthetic datasets generated by either Diff-II, DataDream or BOB. lower value indicates that the generated dataset is closer to the real data distribution. We reveal that the synthetic dataset produced by our perform are better aligned to the training data distribution compared to DataDream and Diff-II. In Figure 4, we observe that, on average, the per-class FID is lower for our method BOB with the mode at around 26 compared to 31 with DataDream and 37 with Diff-II. In fact, of the 100 classes in aircraft, 91 of them 8 Figure 4: Density plot of FID of synthetic data against the real data for each class. Preprint exhibited decrease in FID using our method BOB compared to DataDream (see Section B.2 of the Appendix for more details). The lower FID further reinforces our qualitative analysis that the generated data is closer to the real data distribution. Is it distillation? Since the captioning models themselves have some fine-grained classification capabilities, the nature question on whether we are inadvertly distilling these capabilities down to the T2I model, and subsequently, to the downstream classification model. To test this hypothesis, we use two additional captioning model: Qwen VL2.5-3B (Bai et al., 2025) and GPT-4o (Hurst et al., 2024). If the source of performance gains is due to distillation, then the fine-grained classifcation capabilities of these captioning model should be strong indicator of downstream classification performance. However, Figure 5 demonstrates that there is no such association at any discernable level. We observe that GPT-4o has considerably higher fine-grained classification capability with accuracy close to 80%. However, when used as captioning model in our method, the downstream classification performance does not improve. Similarly for the weaker captioning model Qwen-3B, we do not exhibit considerable decrease with accuracy 70.88%, which is still over 3% higher than the DataDream baseline. All in all, this analysis suggests that the performance gains we observed are not due to distilling classification capabilities from the captioning model. Figure 5: Classification accuracy of caption model vs. downstream classifier trained on synthetic data from BOB. w/o preserv w/ preserv marginalization 68.00 68.01 70. without class-level dataset-level Table 3: Ablation on the effect of context preservation and context marginalization. Ablation studies. Finally, ablate on context preservation and context marginalization by directly including and excluding them from the pipeline. Results from Table 3 reveals three key findings. First, marginalization is necessary for generating highly informative images. Without preservation and marginalization, the algorithm is identical to DataDream baseline, which achieves only 68%. Adding marginalization without preservation result in 3% improvement in accuracy to 70.13 %. Including both marginalization and preservation results in the best accuracy at 73.78%. Second, preservation without marginalization might lead to worse performance. We observe that the performance decreased from 68% to 65.90% when using preservation without marginalization. Finally, we highlight the need for dataset-level marginalization. To accomplish this, we add an option of performing class-wise marginalization, where we only sample background and pose from images of the same class instead of across the entire dataset. We observe that without preservation, this had no impact on the downstream performance. With preservation, this results in decrease in performance from 65.90% to 64.39%. The decrease in performance is likely due to further exacerbation of spurious correlation in fewshot setting. In summary, these ablations reveal the necessity of both context preservation step and dataset-wide marginalization step for generating highly informative images. 65.90 64.38 73."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce BOB as fine-tuning strategy for text-to-image (T2I) models that mitigates overfitting and preserves the strong world prior of these models while addressing the unique challenges of finegrained classification. By leveraging more detailed captioning to extract class-agnostic background and pose information, conditioning on these features during fine-tuning, and marginalizing them out during data generation, our approach reduces unintended class associations and narrows the distribution gap between synthetic and real data. Extensive experiments across multiple backbones, datasets, and scales demonstrate consistent and significant performance gains, including over 7% improvement on the Aircraft dataset and state-of-the-art performance in nearly all settings. This work highlights the potential of caption-guided fine-tuning to improve synthetic data quality for downstream classification tasks and opens avenues for further research on scaling this approach to broader domains and modalities. 9 Preprint ACKNOWLEDGEMENTS This material is based on research supported by the National Science Foundation under Grant No. 2112562 and Solidigm AI SW. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We use publicly available datasets Aircraft (Maji et al., 2013), CUB (Wah et al., 2011), Car (Krause et al., 2013), Pets (Parkhi et al., 2012) as well as CUB-LT (Samuel et al., 2021) and Flower-LT (Wang et al., 2024) datasets for non-commercial research purposes, in line with dataset disclaimers (e.g. CUB). These datasets may include biases in representation as the data is limited in terms of their geographical or environmental context, and the models we train with this data are not intended to be used directly without such representation considerations. The goal of this research is to develop method for augmenting training for fine-grained classification especially in the few-shot regime. Such settings may naturally include biases in the data due to low sample size and the downstream effects of such use must be studied in detail when applying to real-world applications. In addition, since any such method can be misused to violate privacy and copyrights or creating deepfakes, we encourage use of watermarks for both real images as well as model generated images for their detection and mitigation."
        },
        {
            "title": "REFERENCES",
            "content": "Reyhane Askari-Hemmat, Mohammad Pezeshki, Elvis Dohmatob, Florian Bordes, Pietro Astolfi, Melissa Hall, Jakob Verbeek, Michal Drozdzal, and Adriana Romero-Soriano. Improving the scaling laws of synthetic data with deliberate practice. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=0LZRtvK871. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1839218402. IEEE, 2023. Max Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. Image retrieval outperforms diffusion models on data augmentation. arXiv preprint arXiv:2304.10253, 2023. Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=fxNQJVMwK2. Victor Turrisi da Costa, Nicola DallAsen, Yiming Wang, Nicu Sebe, and Elisa Ricci. Diversified in-domain synthesis with efficient fine-tuning for few-shot classification. arXiv preprint arXiv:2312.03046, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph Gonzalez, and Trevor Darrell. Diversify your vision datasets with automatic diffusion-based augmentation. Advances in neural information processing systems, 36:7902479034, 2023. 10 Preprint Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian. Scaling laws of synthetic images for model training... for now. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73827392, 2024. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2023. Scott Geng, Cheng-Yu Hsieh, Vivek Ramanujan, Matthew Wallingford, Chun-Liang Li, Pang Wei Koh, and Ranjay Krishna. The unmet promise of synthetic training images: Using retrieved real images performs better. Advances in Neural Information Processing Systems, 37:79027929, 2024. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiaojuan In International Is synthetic data from generative models ready for image recognition? Qi. Conference on Learning Representations, 2023. Reyhane Askari Hemmat, Mohammad Pezeshki, Florian Bordes, Michal Drozdzal, and Adriana Romero-Soriano. Feedback-guided data synthesis for imbalanced classification. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/ forum?id=IHJ5OohGwr. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Priyank Jaini, Kevin Clark, and Robert Geirhos. Intriguing properties of generative classifiers. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=rmg0qMKYRQ. Jae Myung Kim, Jessica Bader, Stephan Alaniz, Cordelia Schmid, and Zeynep Akata. Datadream: Few-shot guided dataset generation. In European Conference on Computer Vision, pp. 252268. Springer, 2024. Soroush Abbasi Koohpayegani, Anuj Singh, KL Navaneet, Hadi Jamali-Rad, and Hamed Pirsiavash. Genie: Generative hard negative images through diffusion. arXiv preprint arXiv:2312.02548, 2023. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pp. 554561, 2013. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19311941, 2023. 11 Preprint Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your difIn Proceedings of the IEEE/CVF International fusion model is secretly zero-shot classifier. Conference on Computer Vision, pp. 22062217, 2023a. Alexander Cong Li, Ananya Kumar, and Deepak Pathak. Generative classifiers avoid shortcut solutions. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=oCUYc7BzXQ. Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:3014630166, 2023b. S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013. Eyal Michaeli and Ohad Fried. Advancing fine-grained classification by structure and subject preserving augmentation. Advances in Neural Information Processing Systems, 37:2231622349, 2024. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pp. 34983505. IEEE, 2012. Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge University Press, USA, 2nd edition, 2009. ISBN 052189560X. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=FjNys5c7VyY. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234241. Springer, 2015. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. Dvir Samuel, Yuval Atzmon, and Gal Chechik. From generalized zero-shot learning to long-tail In Proceedings of the IEEE/CVF winter conference on applications of with class descriptors. computer vision, pp. 286295, 2021. Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 80118021, 2023. 12 Preprint Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic images from text-to-image models make strong visual representation learners. Advances in Neural Information Processing Systems, 36:4838248402, 2023. Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1588715898, 2024. Brandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data augmentation with diffusion models. In International Conference on Learning Representations, 2023. Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. Yanghao Wang and Long Chen. Inversion circle interpolation: Diffusion-based image augmentation for data-scarce classification. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2556025569, 2025. Zhicai Wang, Longhui Wei, Tan Wang, Heyu Chen, Yanbin Hao, Xiang Wang, Xiangnan He, and In Qi Tian. Enhance image classification via inter-class image mixup with diffusion model. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17223 17233, 2024. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, and Yong Jae Lee. Diversify, dont fine-tune: Scaling up visual recognition training with synthetic imISSN 2835-8856. URL https: ages. Transactions on Machine Learning Research, 2025. //openreview.net/forum?id=YCt8lsIDwA. Featured Certification. Zhongqi Yue, Hanwang Zhang, Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning. Advances in neural information processing systems, 33:27342746, 2020. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 60236032, 2019. Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, and Yunhao Ge. Dreamdistribution: Learning prompt distribution for diverse in-distribution generation. In International Conference on Learning Representations, 2025. Yongchao Zhou, Hshmat Sahak, and Jimmy Ba. Training on thin air: Improve image classification with generated data. arXiv preprint arXiv:2305.15316, 2023. Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, and Hongsheng Li. Easyref: Omni-generalized group image reference for diffusion models via multimodal llm. In Forty-second International Conference on Machine Learning, 2024. 13 Preprint"
        },
        {
            "title": "Appendices",
            "content": "A Additional implementation details A.1 Hyperparameter sweep . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Parameters for dataset generation . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Creation of validation dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional analysis B.1 Number of training epochs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Class differences in real vs. synthetic distribution . . . . . . . . . . . . . . . . . . B.3 Significant overlaps in the Pets dataset . . . . . . . . . . . . . . . . . . . . . . . . 15 15 16 16 16 16 14 Preprint Table 4: Hyperparameters for fine-tuning. Hyperparameters used for fine-tuning of T2I model and downstream classifier. list of parameters indicates the hyperparameter sweep using the validation set. The number of epochs indicated in parenthesis is the epochs used for fine-tuning on test set with the best hyperparameter on validation set. Learning rate Weight decay Layer decay λ Epochs Batch size Scheduler Warm up Max norm LoRA rank Mixed precision T2I 1e-4 1e-2 - - 400 80 Cosine 100 steps 1.0 16 No CLIP [1e-4, 1e-5, 1e-6, 1e-7] [5e-4, 1e-4] - [0.5, 0.8] 10 (100) 64 Cosine 3 epochs - 16 fp ImageNet [1e-3, 1e-4, 1e-5] [0.01, 1e-4, 0] - [0.5, 0.8] 10 (100) 64 Cosine 3 epochs - - fp16 MAE [1e-3, 5e-4] 0.05 [0.65, 0.75] [0.5, 0.8] 10 (100) 64 Cosine 5 epochs - - fp"
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "A.1 HYPERPARAMETER SWEEP In this section, we go over the hyperparameter used to produce the results in Table 1 and Table 2. The full hyperparameters are listed in Table 4. For fine-tuning the T2I model on the few-shot or long-tail images, we follow the procedure in DataDream paper (Kim et al., 2024) with two difference: using dense captions for text input following our template outline in Section 3.1, increasing the number of epochs from 200 to 400. For CLIP fine-tuning, we follow the pipeline in DataDream paper (Kim et al., 2024) We optimize LoRA layers in both the image and text encoder with rank 16. We sweep over learning rate {1e-4, 1e-5, 1e-6, 1e-7} and weight decay {5e-4, 1e-7}. The only difference is that we have an additional sweep for λ {0.5, 0.8}. This results in 16 different configurations we are sweeping over for CLIP fine-tuning. For ImageNet fine-tuning, we sweep over learning rate {1e-3, 1e-4, 1e-5}, weight decay {0, 0.01, 1e-4}, and λ {0.5, 0.8}. This results in 18 different configurations for ImageNet. For MAE fine-tuning, we use the fine-tuning recipe provided by the original authors. We sweep over base learning rate {1e-3, 5e-4} and layer-wise learning rate decay {0.65, 0.75} and due to discrepancy in the values provided by the original paper and default in their Github release. We also sweep over λ {0.5, 0.8}. In all, this results in 8 different configurations for MAE fine-tuning. We fine-tune the model for 10 epochs and select the configuration that results in the best validation accuracy across the 10 epochs. Using the hyperparameters that gave the best accuracy on the validation set, we fine-tune the pre-trained model again from scratch for 100 epochs. We report the test accuracy for the epoch that corresponds to the best validation accuracy during this training. A.2 PARAMETERS FOR DATASET GENERATION We follow the same parameters used in DataDream for generating the synthetic dataset shown in Table 5: guidance scale of 2.0, 50 inference steps, and fp16 mixed precision. The scheduler used is the default for Stable Diffusion v1.5 and Stable Diffusion v2.1-base. Other methods. For generating synthetic data for baselines used for comparisons, we use the default parameters used for data generation provided by their paper. For the Diff-Mix method, an additional CLIP filtering is used to remove problematic images as outlined in the original paper. 15 Table 5: Generation parameters. Hyperparameter Guidance scale Number of steps Mixed precision Value 2.0 50 fp16 Preprint A.3 CREATION OF VALIDATION DATASET For fair comparison, we perform hyperparameter tuning on the learning rate, weight decay, and the λ hyperparameters when evaluating downstream classification performance. In order to achieve this, we created our own validation sets. In the few-shot classification setting, since not all of the training data is used, we randomly select 16 non-overlapping images per class for FGVC-Aircraft and Oxford-Pets, 10 random non-overlapping images for Stanford-Cars and CUB. However, in the long-tail setting (CUB-LT and Flower-LT), there exist some classes where most of the examples are used for training. Therefore, it is no longer possible to create separate validation set. Therefore, we split the test set into smaller test set and held-out validation set with five images per class."
        },
        {
            "title": "B ADDITIONAL ANALYSIS",
            "content": "B.1 NUMBER OF TRAINING EPOCHS Table 6: Fine-tuning the T2I model longer helps. Epochs DataDream BOB (ours) 200 400 1000 2000 In comparison to DataDream, we increase the number of epochs from 200 to 400. To motivate this design decision, we fine-tune model for 2,000 epochs on 10-shot Aircraft and save the intermediate checkpoints to study the effect of longer fine-tuning towards the generation of informative samples. For the intermediate checks, we follow the same synthetic data generation and hyperparameter tuning procedure to obtain the final test accuracy. The result shown in Table 6 shows similar effect from number of epochs on both our method BOB and DataDream. Going from 200 epochs to 400 epochs, DataDream performance improves by 2%, from 66.49% to 68.87%. However, our method exhibits 6% increase from 68.65% to 74.41%. The considerably larger increases suggests that, while DataDream benefits from longer fine-tuning, our method BOB benefits from it more. Similarly for both methods, the performance peaks at the checkpoint fine-tuned for 1,000 epochs before it starts to decrease again using the 2,000 epoch checkpoint. Finally, at every epoch in Table 6, our method BOB outperforms DataDream. 68.65 74.41 75.22 73.87 66.49 68.87 69.11 67.28 B.2 CLASS DIFFERENCES IN REAL VS. SYNTHETIC DISTRIBUTION Following our analysis in Section 4.4 towards comparing the distribution of sythetic dataset against the real dataset, we directly compare the FID of each class between synthetic data generated by DataDream and our method BOB. We plot the histogram in Figure 6. We observe that there are for 91 classes out of 100 FID is lower for our method compared to DataDream. Of these, for 29 classes, FID decreases by over 5. For all of the classes where our method had higher FID, the increase is less than 5. This means that there are 3x as many classes that observed considerable decrease in FID than the classes with relatively low FID increase. This result suggests that our method provides fairly uniform improvement in FID across all the classes-FID either considerable increase or remain similar (within 5 FID). B.3 SIGNIFICANT OVERLAPS IN THE PETS DATASET Figure 6: Histogram of the FID difference from DataDream vs. BOB for each class. Recall from Table 1 that on the Pets dataset, fine-tuning classifier on synthetic data generated images results in very little performance gains across all of the methods in the CLIP and ImageNet classification pre-trained ResNet-50 backbone. In the case of CLIP, there is no performance gains with all of the methods arriving at an accuracy within 1% compared to just fine-tuning on real data. This is because the zero-shot classification accuracy on Pets using CLIP is already 91% as reported in the DataDream paper (Kim et al., 2024). As result, the model already have very strong classification capabilities, and therefore, additional synthetic data isnt as impactful, if at all. For 16 the ImageNet classification trained backbone, we make similar observation where most of the 39 pet classes are already present in ImageNet. To study this, we manually search up the pet names (as well as adjacent names since same pets have multiple names) in the ImageNet classes. The result is shown in Table 7. We observe that 22 of the 39 classes have corresponding ImageNet class. Similar to the CLIP setting, if the backbone very high classification capabilities, then it is not good evaluation metric for determining the strength of classification signals in the synthetic dataset. These findings explains why there the different trends observed in the MAE setting vs. ImageNet or CLIP setting from the Pets dataset in Table 1. Table 7: Oxford-IIIT Pets classes with ImageNet IDs ( if not present) Pet name ImageNet ID Pet name ImageNet ID Abyssinian Bombay British Shorthair Persian Ragdoll Siamese Boxer Havanese English Setter Chihuahua German Shorthaired Staffordshire Bull Terrier New Found Land Leonberger Wheaten Terrier Samoyed Shiba Inu Saint Bernard n02123394 n02123597 n02108089 n02100735 n02085620 n02100236 n02093256 n02111277 n02098105 n02111889 - Bengal Birman Maine Coon Egyptian Mau Russian Blue Sphynx Keeshond Basset Hound Miniature Pinscher Great Pyrenees Beagle English Cocker Spaniel Pomeranian American Pit Bull Terrier Japanese Chin Scottish Terrier Pug n02124075 n02112350 n02088238 n02107312 n02111500 n02088364 n02102318 n02112018 n02085782 n02097298 n02110958 Back to Table of Contents 17 Back to the First Page"
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Princeton University"
    ]
}