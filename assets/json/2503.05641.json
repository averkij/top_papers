{
    "paper_title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning",
    "authors": [
        "Justin Chih-Yao Chen",
        "Sukwon Yun",
        "Elias Stengel-Eskin",
        "Tianlong Chen",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Combining existing pre-trained expert LLMs is a promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and gradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained approach to selection by emphasizing skills, e.g., algebra in math or molecular biology in biomedical reasoning. We propose a skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in k outputs from k experts, which are then synthesized into a final high-quality response by an aggregator chosen based on its ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's instance-level expert selection improves performance by a large margin but -- when implemented naively -- can introduce a high computational overhead due to the need for constant model loading and offloading. To address this, we implement a batch inference strategy that groups instances based on their assigned experts, loading each model only once. This allows us to integrate 16 expert models on 1 GPU with a time cost comparable to or better than prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that Symbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, Symbolic-MoE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 1 4 6 5 0 . 3 0 5 2 : r Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning Justin Chih-Yao Chen Sukwon Yun Elias Stengel-Eskin Tianlong Chen Mohit Bansal"
        },
        {
            "title": "UNC Chapel Hill",
            "content": "https://symbolic-moe.github.io"
        },
        {
            "title": "Abstract",
            "content": "Combining existing pre-trained expert LLMs is promising avenue for scalably tackling large-scale and diverse tasks. However, selecting experts at the task level is often too coarse-grained, as heterogeneous tasks may require different expertise for each instance. To enable adaptive instance-level mixing of pre-trained LLM experts, we propose SYMBOLIC-MOE, symbolic, text-based, and gradient-free Mixture-of-Experts framework. SYMBOLIC-MOE takes fine-grained approach to selection by emphasizing skills, i.e., specialized subcategories or subtopics such as algebra in mathematics or molecular biology in biomedical reasoning. We propose skill-based recruiting strategy that dynamically selects the most relevant set of expert LLMs for diverse reasoning tasks based on their strengths. Each selected expert then generates its own reasoning, resulting in outputs from experts, which are then synthesized into final high-quality response by an aggregator. The aggregator is chosen based on its ability to integrate diverse reasoning outputs. We show that instance-level expert selection improves performance by large margin but when implemented naively can introduce high computational overhead due to the need for constant model loading and offloading. To address this, we implement batch inference strategy that groups instances based on their assigned experts, ensuring each model will only be loaded once. This allows us to integrate 16 models on single GPU with time cost comparable to prior multi-agent baselines using 4 GPUs. Through extensive evaluations on diverse benchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that SYMBOLIC-MOE outperforms strong LLMs like GPT4o-mini, as well as multiagent approaches, with an absolute average improvement of 8.15% over the best multi-agent baseline. Moreover, SYMBOLIC-MOE removes the need for expensive multi-round discussions, outperforming discussion baselines with less computation."
        },
        {
            "title": "Introduction",
            "content": "A core strength of humans is our ability to communicate and coordinate with each other using language (Clark, 1996; Yow & Lim, 2019; Xu et al., 2023). This allows diverse experts to contribute specialized knowledge towards solving problem, and is common across variety of settings, including research, medicine, and engineering. Like humans, large language models (LLMs) often have differing skills and strengths, derived from differences in their architectures and training regimens. For instance, math-specific models like MetaMath (Yu et al., 2023), WizardMath (Luo et al., 2023), and QwenMath (Yang et al., 2024) are post-trained with mathematical reasoning data, making them particularly adept at math tasks often at the cost of performance on out-of-distribution tasks (Kumar et al., 2022; Chu et al., 2025) like commonsense or medical reasoning (Lobo et al., 2024). Even within specialized domains, differences in pre-training data can lead to nuanced variations in expertise: one math-focused model may excel at algebra, while another is better suited for geometry. This motivates our development of an automated, skill-based framework designed to identify and select the most Equal contribution. 1 suitable set of expert models for given problem.1 Allowing multiple diverse models to combine their answers can improve even the strongest models (Chen et al., 2024c), and as tasks grow in complexity, leveraging specialized models at test time presents promising approach to scaling LLM capabilities. Indeed, combining multiple expert models, i.e., Mixture-of-Experts (MoE) is well-studied notion in machine learning (Jacobs et al., 1991; Eigen et al., 2013) and has been applied widely for large pre-trained models, enabling better performance at lower computing cost (Shazeer et al., 2017a; Fedus et al., 2022; Riquelme et al., 2021). However, in the conventional MoE settings, experts are typically sub-models, i.e. subsets of parameters within larger model, trained simultaneously in an end-to-end fashion, where at test time, experts are combined in the models parameter space. This generally requires end-to-end training from scratch, which is often computationally expensive and precludes the re-use of the vast pool of existing already-trained LLMs. Building on recent efforts in combining fixed set of models through multi-agent discussions (Chen et al., 2024c; Du et al., 2023; Liang et al., 2023; Wang et al., 2024a), we propose exploring new training-free paradigm for large-scale MoEs: symbolic mixture of experts (SYMBOLIC-MOE). Rather than using information encoded in the models hidden state, SYMBOLIC-MOE uses symbolic structures in two ways: First, SYMBOLIC-MOE infers set of discrete skills needed to solve problem, measuring the abilities of each model in pool of candidate expert models according to the set. It then uses skill-based performance as router to recruit sparse subset of experts for each problem. Secondly, SYMBOLIC-MOE combines pretrained experts through symbolic channel, i.e., language, which is common protocol already shared by all models involved. To take advantage of the diverse set of expert LLMs, two key challenges present: (1) Effective Expert Selection: Given large set of existing LLMs, how can we choose the best experts for each example? (2) Scalable Expert Mixing: How can we scalably serve large number of experts without increasing the demand for GPUs? Effective Expert Selection: The increasing diversity of benchmarks (Miranda et al., 2024) and the growing number of models means that experts must be selected not at the level of tasks, but at the level of individual queries. Even at the task level, manual selection can be labor-intensive, and the performance of multi-agent frameworks can be sensitive to the agents chosen for task (Chen et al., 2024c; Liang et al., 2023; Wang et al., 2024b). At the query level, manual selection becomes infeasible, especially in multi-model settings where different experts must collaborate to address complex queries (Wang et al., 2024c; Rein et al., 2023). Moreover, manually selecting models for each task does not guarantee optimal performance. For instance, as shown in Fig. 1 (a), while given subset of models may perform well on math tasks on average, their proficiency in specific subfields like algebra or probability might vary. Therefore, using of fixed subset of models on all math samples might hurt performance on particular subtasks. This underscores the need for an automated, fine-grained selection mechanism as shown in Fig. 1 (b). Given that evaluating all possible model subsets is computationally infeasible, we instead propose an efficient approach that activates only the most relevant experts while minimizing computational overhead. Scalable Expert Mixing: Unless models are retrained jointly, the integration of multiple models must occur through their outputs typically via symbolic channel, such as multi-agent discussion. Past work has often relied on multiple rounds of inference, leading to significant GPU demands. As shown in Fig. 3 (I), when selecting fixed set of models for given domain (e.g. as done by (Li et al., 2024b; 2025)), models could in principle be loaded onto parallel GPUs. This reduces the latency of the system but increases the number of GPUs needed. Moreover, it does not scale to dynamic setting like the one we consider, where the number of GPUs required would be equal to the number of potential models available (in our case, 16), making this option prohibitively expensive. Given models and queries, this solution requires GPUs. Alternatively, models could be run sequentially on single GPU (cf. Fig. 3 (II)), which minimizes GPU usage but incurs high overhead due to frequent loading and off-loading of models. In other words, this solution requires O(kn) loads. Given the high cost of loading models into GPU memory (Griggs et al., 2024; Li et al., 2024a), this solution is also untenable. Processing each query separately also fails to fully utilize GPU memory, further limiting its efficiency. Driven by these motivations, in SYMBOLIC-MOE, we achieve effective expert selection and scalable expert mixing through the following approach. First, to enhance expert selection, we introduce automated skill-based recruiting, which leverages an inferred skill-based model profile dictionary mapping the most relevant models to specific skills (e.g., keywords or subcategories) required to 1We refer experts to those LLMs that are selected to solve problem in this work. 2 Figure 1: Comparison between existing multi-agent work and our work. (a) In prior work, fixed set of task-level experts (e.g., Phi, Mistral, and Llama) is recruited to solve mathematical problems, where heterogeneous questions may differ in the skills required to solve them (e.g Q1 requires algebra, while Q2 focuses on probability). Expert models then engage in multiple rounds of discussion, making this approach resource-intensive. (b) In contrast, SYMBOLIC-MOE adaptively recruits instance-level experts based on skills needed (Algebra experts for Q1 and different set of Probability experts for Q2). By generating only single round of responses with selected aggregator to synthesize the final output, our approach is both more performant and more efficient. solve given query using small subset of available samples. Like router in standard MoE setting, SYMBOLIC-MOEs skill-based routing allows the most relevant experts to be activated while keeping the others inactive, optimizing both accuracy and efficiency. Next, to enable scalable expert mixing, we propose batch inference mechanism (illustrated in Fig. 3 (III)). For each query, the router first selects the necessary experts, and then examples are grouped into batches per model. We then run all queries for given model in single batch, allowing us to load and off-load O(k) times in total, which is far faster than sequential processings O(kn). Finally, while SYMBOLIC-MOEs batched implementation accommodates up to 16 models on single GPU, it can also be parallelized across multiple GPUs. This flexibility ensures both speedups with increased computing power and accessibility for users with limited hardware resources. As shown in Figure 1, existing multi-agent approaches use fixed set of experts for each task (e.g., all math problems), regardless of fine-grained topics (e.g., algebra or probability), and rely on multiple discussion rounds, making them non-adaptive and resource-intensive. In contrast, our approach introduces adaptive query-level recruitment based on fine-grained skills, selecting the most suitable models for each query. Furthermore, we improve efficiency through (1) batch inference strategy that processes all queries for model in single batch and (2) task-specific aggregator instead of multi-agent discussions, ensuring that expert models generate their answers only once. We evaluate SYMBOLIC-MOE on diverse benchmarks, including MMLU-Pro (Wang et al., 2024c), GPQA (Rein et al., 2023), AIME 2024 (MAA, 2024), and MedMCQA Pal et al. (2022), using diverse model pool. We show that integrating this model pool with automated skill-based recruiting yields an average accuracy improvement of 8.15% over the best multi-agent baseline. Moreover, despite primarily using LLMs with 7-8 billion (B) parameters, SYMBOLIC-MOE achieves comparable performance with larger 70B models, and on average, outperforms strong proprietary models like GPT4o-mini (OpenAI, 2024), Gemini 1.5 Pro (Team et al., 2024), and DeepSeek-V3 (DeepSeekAI et al., 2025b). Without any manual intervention, SYMBOLIC-MOE consistently surpasses all baselines, whereas the strongest baseline changes across settings, with some baselines performing competitively in one setting but poorly in others. Thus, SYMBOLIC-MOE eliminates the need for the user to evaluate and compare large number of possible baselines for each setting. Notably, SYMBOLIC-MOE obtains these benefits while reducing the amount of compute needed. Using single GPU, SYMBOLIC-MOE has 44% less run-time than mixture-of-agents baseline (Wang et al., 2024a). When four GPUs are available for both methods, we obtain an almost 2 speedup over this baseline. Our analysis highlights that SYMBOLIC-MOE shows strong robustness despite the variation in the best-performing models and optimal aggregators for each task. Additionally, we show that selecting task-specific aggregator based on its ability to integrate diverse answers can achieve performance comparable to multi-round discussions while requiring substantially less compute. 3 Figure 2: Overview of SYMBOLIC-MOE. (a) Preprocessing: Given validation set and pool of agents, we create model profiles and select an aggregator. (b) Inference-Time: For each test example, SYMBOLIC-MOE activates the most relevant models (experts) based on skill-based routing, using model profiles determined during preprocessing. These models generate CoT responses, which the aggregator (chosen based on its ability to select correct answers) synthesizes into final answer."
        },
        {
            "title": "2 Related Work",
            "content": "Mixture-of-Agents. Traditional Mixture-of-Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994; Chen et al., 1999; Yuksel et al., 2012) models distribute computation across multiple experts, with growing interest in sparsity-driven approaches. The Sparse MoE (SMoE) approach (Shazeer et al., 2017a) improves efficiency by activating only the most relevant experts per input, enhancing scalability for high-dimensional data. This method has been widely applied in vision tasks (Riquelme et al., 2021; Wang et al., 2020; Yang et al., 2019; Abbas & Andreopoulos, 2020), language tasks (Lepikhin et al., 2021; Zhang et al., 2021; Zuo et al., 2022; Jiang et al., 2021) and multimodal learning (Kudugunta et al., 2021; Yun et al., 2024). In contrast to past sparse MoE work, the models in our approach are combined through symbolic channels (e.g., language outputs) as opposed to in the parameter space, allowing us to re-use existing LLMs without training. MoA (Wang et al., 2024a) introduces framework for combining LLM agents into ensembles that relies on fixed set of agents across tasks and instances. This approach requires multiple rounds of generation and aggregation before producing final answer. Similarly, Self-MoA (SMoA; Wang et al., 2024a) posits that using multiple distinct LLMs is unnecessary, suggesting that optimal performance can be achieved by invoking the task-best model multiple times alongside the task-best aggregator. Our work differs from MoA and SMoA by introducing adaptive, instance-level, skill-based routing while avoiding costly multimodel discussions in favor of streamlined aggregation, significantly reducing computational overhead. We also find that mixing different LLMs is advantageous when paired with effective routing and aggregation strategies. Moreover, we show that the best aggregator for task is not necessarily the best-performing model overall, but can be identified through synthetic task we introduce, designed to evaluate aggregation effectiveness. Multi-Agent Reasoning. Multi-agent reasoning has emerged as promising paradigm for enhancing complex problem-solving and decision-making in AI systems. Early approaches employed reinforcement learning-based coordination (Lowe et al., 2017; Foerster et al., 2018; Jaques et al., 2019), while recent efforts leverage LLM-based multi-agent frameworks. One line of research explores student-teacher paradigms (Magister et al., 2022; Fu et al., 2023; Ho et al., 2022; Du et al., 2023; Chen et al., 2024a), where reasoning capabilities are distilled from stronger to weaker agents. Another approach investigates multi-agent debate frameworks, where agents interact to refine arguments and enhance collective decision-making; this has been explored with multiple instances of single model (Liang et al., 2023; Xiong et al., 2023; Chan et al., 2023) or debates between multiple LLM types (Chen et al., 2024c). In both cases, the set of models is predefined by the user. In contrast, our approach automatically selects models based on inferred skills. Additionally, our framework achieves superior efficiency by avoiding multi-round discussions while still outperforming debate-based methods."
        },
        {
            "title": "3 Methodology",
            "content": "SYMBOLIC-MOE consists of three stages: (I) model profile creation and aggregator selection (Fig. 2 (a)), which occur during preprocessing, followed by (II) expert recruitment and (III) final answer generation (Fig. 2 (b)), both of which take place during inference. First, we establish the problem setup in 3.1. Then, in the preprocessing stage (3.2), we describe how the model profile is created in 3.2.1, which enables fine-grained skill-based assessment of each model. We also describe the process for selecting the aggregator in 3.2.2, which is responsible for generating the final answer. Next, in the inference stage (3.3), we introduce the recruitment process in 3.3.1, where the experts are selected based on the skills needed for each problem, and how the answers from the recruited experts are combined using the aggregator to improve reasoning in 3.3.2. 3.1 Problem Setup Given pool of models = {Mi}n i=1, where each model represents distinct LLM with potentially different pre-training datasets and architectures, our goal is to optimize performance through dynamic allocation solving each problem with the most suitable subset of models and combined reasoning allowing experts to combine information to enhance reasoning. To achieve this, we assume access to small validation set to obtain (1) model profiles Pi n, and (2) aggregator profiles that benchmark the ability of each model to act as an aggregator. We use these profiles to recruit experts (at the instance level) and to select the aggregator (at the task level). 3.2 Preprocessing 3.2.1 Model Profile Creation To recruit the most suitable experts for given question, we assess each models specialized skills across various problem-solving domains, illustrated in Fig. 2 (a). This is done by evaluating their performance on the validation set for each task (see Table 6 for sizes), thereby constructing model profile Pi for each model Mi. For each question in the validation set, we first prompt an LLM referred to as the Keyword LLM to identify the essential skills required to solve the problem. The prompt used for generating these required skills is provided in Appendix G. For consistency, we use Qwen2.5-7B-Instruct (Team, 2024) as the Keyword LLM. To reduce noise, we generate keyword annotations for each question five times, and retain only those that appear more than once across the whole validation set. These extracted skills represent core knowledge areas necessary for solving the problem for instance, given college-level math problem may require skills such as algebra, calculus, and geometry. Once all questions are annotated with their required skills, each model Mi in the pool attempts to solve them using Chain-of-Thought reasoning (Wei et al., 2022). correct answer increases the score of each associated skill by +1, while an incorrect answer results in 1 penalty. At the end of this process, each model has profile Pi {P1, . . . , Pn} represented as dictionary. For example, models profile may be: {Algebra: 10, Biology: 3, Chemistry: -6, ...}. 3.2.2 Aggregator Selection An aggregator is model that consolidates outputs into single high-quality response. Our pilot experiments, along with findings from Wang et al. (2024a) and Li et al. (2024b), indicate that the aggregator model plays crucial role in the final performance, and selecting the most effective model for aggregation is non-trivial challenge. We find that reasoning capability and aggregation ability are often orthogonal. That is, strong reasoning model is not necessarily strong aggregator and vice versa; qualitatively, we show this later in Table 5. We find that adaptively selecting an aggregator on the instance level based on model profiles is less effective, motivating us to choose the aggregator based on models ability to consolidate different answers. To identify the best aggregator per task, we construct synthetic task using the same validation set. From the profile creation process, we obtain outputs from all models, some correct and some incorrect. For each question, we sample one correct reasoning chain and two incorrect ones, structuring the input as follows: {question}, {correct CoT}, {incorrect CoT}, {incorrect CoT}. We shuffle the order of the correct and incorrect CoTs and instruct each model to act as an aggregator (using the prompt shown in Appendix G), synthesizing final output with predicted answer. We 5 then benchmark each models aggregation ability, measuring how well it can generate correct answer based on this input, and select the best-performing aggregator for each task. 3.3 Inference 3.3.1 Skill-based Recruiting At inference time (see Fig. 2 (b)), we follow the same keyword annotation procedure as in Section 3.2.1 to generate relevant keywords for the test sample. To align test samples keywords with those in the model profiles, we employ Sentence-BERT (Reimers & Gurevych, 2020) to match keywords via the cosine similarity between their embeddings. Each test-time keyword is then mapped to its closest counterpart in the model profile. Next, the expert recruitment is performed by selecting the top models whose profiles best match the required skills of the test sample. This is determined by two factors: (1) local suitability score and (2) global competency. For each model Mi, its local suitability score for test sample q, S(Mi, q) is computed as the sum of its skill scores over the set of keywords needed for (denoted as Kq). It can be expressed as follows: S(Mi, q) = kjKq (i) kj (i) kj represents the score of model Mi for the j-th skill in the test sample q. This results in an where model ranking distribution Dq for each test sample q: Dq = {S(M1, q), S(M2, q), ..., S(Mn, q)} Intuitively, suppose M1 has scores of +3, +5, and 2 for algebra, calculus, and geometry, respectively, which are needed for given sample; its total score for this sample would be 3 + 5 2 = 6. Having this calculation for all models yields ranking distribution of model strengths for the given sample, e.g., {M1: 6, M2: 3, ..., Mn: -10}, which is the ranking of how suitable model is for sample. To account for each models overall strength in task, i.e., global competency, we compute each models total score across all keywords in their profile, and normalize it by the total sum of all agents scores. We denote this global strength as γi, representing models overall task performance relative to others. Finally, the expert selection is performed by sampling from the product of the local suitability score, S(Mi, q) (from model rank distribution Dq) and the global competency γi. That is, the relevance score of model Mi for test sample is: (i) = γiS(Mi, q) We apply softmax function with the temperature set to 0.5 to this distribution {w sample experts for each problem, i.e., (i) }n i=1, and then (i) Categorical(w (1) , (2) , ..., (n) ), = {1, 2, ..., k} To enhance efficiency and reduce computational overhead, we filter out low-frequency experts those who appear in fewer than 5% of test cases. For example, given test set with 100 samples, where 3 experts are recruited per sample (totaling 300 selections), any expert appearing fewer than 300 5% = 6 times is discarded and resampled from the remaining higher-frequency experts. 3.3.2 Final Answer Generation After expert recruitment, each sample will be passed to the experts, i.e., the input for each expert is the test problem, x0 = q. These experts generate their reasoning paths to the problem in the form of Chain-of-Thought (Wei et al., 2022): (i) 0 = E(i)(x0) {1, 2, ..., k} Then, the task-specific aggregator is introduced to synthesize the outputs into high-quality final answer (Wang et al., 2024a). That is, the final answer is produced by: = A((cid:13) (i) 0 ), (cid:13) where (cid:13) (cid:13) denotes the concatenation operation. In Appendix H, we provide detailed discussion on SYMBOLIC-MOE in the context of sparse MoE frameworks and how it shares its design principles. i=1y 6 Figure 3: Different approaches to achieving adaptiveness in SYMBOLIC-MOE, which uses different models for each instance. In naive setup (I), GPUs must be hosted simultaneously, allowing immediate access to outputs from each model. Another naive setup (II) requires only single GPU but involves constant loading and offloading of models to obtain outputs from the corresponding model. Our scalable batch inference process (III) strikes balance between (I) and (II). When models are assigned to problems, we group samples by model and sequentially load the corresponding LLM onto single GPU to generate outputs efficiently. Moreover, this approach still allows us to parallelize across GPUs if they are available. 3.3.3 Scalable Batched Inference In our experiments, we mostly consider 7B8B parameter LLMs, which have substantial memory footprint. Due to the adaptive nature of the recruitment process, the set of participating LLMs may change dynamically for different problems. For instance, one sample may require Qwen, Llama, and Mistral, while another may need Gemma, Exaone, and Phi. naive implementation of this approach can lead to high latency, particularly when the required models change frequently. In such cases, the computational cost includes not only inference but also data transfers across GPUs. To mitigate these computational challenges, SYMBOLIC-MOE introduces novel batching strategy to maximize throughput. Specifically, for given set of instances, we precompute (using inferred skills) which LLMs will be called for each instance. We then group instances based on their required experts, as illustrated in Fig. 3 (III) and Algorithm 1 in Appendix F. In other words, each active expert receives all relevant instances at once, ensuring that each expert is loaded only once per batch. This enables efficient batched inference on single GPU while supporting pool of 16 LLMs. Moreover, this approach is flexible, as more GPUs can further accelerate inference through parallelization."
        },
        {
            "title": "4 Results and Analysis",
            "content": "4.1 Experimental Setup Model Pool and Datasets. Our experiments consider 16 LLMs ranging from 3.5B to 12B parameters, with most models falling in the 78B range. These include general-purpose instruction-tuned models, domain-specific fine-tuned variants on math and biology, and models distilled from DeepSeek R1s trajectories (DeepSeek-AI et al., 2025a). full list of models is provided in Table 7 in the Appendix. We measure performance on diverse range of datasets, chosen to require expertise in number of domains. First, we consider MMLU-Pro (Wang et al., 2024c), which is harder version of MMLU (Hendrycks et al., 2021), containing variety of questions across 14 college-level subjects, from Philosophy and History to Economics and Chemistry. Given its large test set of 12,000 samples and the computational cost of evaluating proprietary models, we employ stratified sampling to create subset of 2,100 samples, ensuring each category contains 150 samples. We also evaluate on AIME 2024, which is challenging mathematics competition dataset containing math Olympiad problems. For more science-specific reasoning, we test on GPQA (Rein et al., 2023), which contains questions across variety of science fields written by experts, explicitly designed to be difficult to answer even by skilled humans with web access. Finally, we also include MedMCQA (Pal et al., 2022), which covers questions sourced from medical entrance exams across 21 medical subjects. For each dataset, 7 Category Method Model MMLU-Pro AIME GPQA MedMCQA Avg. Close-Source Single Model Open-Source 70B Model Open-Source 7B Model Advanced Single Model Single-Model Multi-Agent Multi-Model Multi-Agent Zero-Shot CoT Zero-Shot CoT Zero-Shot CoT Zero-Shot CoT Zero-Shot CoT Zero-Shot CoT Zero-Shot CoT GPT4o-mini Gemini 1.5 Pro DeepSeekV3 Qwen2.5 72B Llama3.3 70B QwenR1 Task-Best Self-Refine (SR) Self-Consistency (SC) Task-Best Task-Best x5 Debate Self-MoA MoA ReConcile SYMBOLIC-MOE Task-Best x3 Task-Best x3 Top-3 Top-3 Adaptive 63.95 76.38 76. 71.54 0.88 69.26 0.47 52.57 0.45 54.89 0.53 53.74 0.20 56.71 0.14 56.21 0.55 55.43 0.72 61.78 0.25 56.46 0.10 63.71 0.43 10.00 36.67 26. 42.93 61.62 60.10 25.55 3.85 32.22 3.85 51.02 0.27 51.44 0.62 55.93 5.16 55.93 5.16 44.95 1.49 48.43 3.10 53.33 3.34 67.78 1. 50.84 3.65 53.54 0.36 56.67 6.67 55.56 5.09 50.51 0.51 52.86 1.46 41.11 5.09 50.00 7.20 68.88 5.08 52.86 3.37 47.98 2.32 57.78 2.09 68.18 72.68 74. 69.02 0.32 59.78 0.74 38.72 0.44 55.44 0.50 49.57 0.59 56.85 0.11 51.63 0.80 53.27 0.60 59.29 0.32 60.74 0.43 59.35 0.14 46.27 61.84 59. 54.28 53.18 48.04 53.62 51.87 58.72 53.76 54.28 53.76 53.80 62.43 Table 1: Comparison of SYMBOLIC-MOE with single-model and multi-model baselines. SYMBOLICMOE outperforms all multi-agent baselines and achieves performance comparable to strong proprietary models like GPT4o-mini, as well as 70B models, while primarily operating with 7-8B models. Notably, no single baseline consistently secures the second-best performance, even when the strongest models for each task are known. In contrast, our method demonstrates robustness, yielding superior results through adaptive expert selection. We bold the best results and underline the second-best (excluding methods using bigger or proprietary models, shown in gray). we sample around 350 samples as the validation set to create the agent and aggregator selection profiles.2 Full dataset statistics are provided in Table 6 in the Appendix. Baselines. We compare against four categories of baselines. Zero-shot single-model methods: This category includes proprietary models such as GPT-4omini (OpenAI, 2024), Gemini 1.5 Pro (Team et al., 2024), and DeepSeek-V3 (DeepSeek-AI et al., 2025b); high-capacity open-source models like Qwen2.5 72B (Qwen et al., 2025) and Llama 3.3 70B (AI@Meta, 2024); and strong distilled 7B models such as QwenR1 (DeepSeek-AI et al., 2025a). For reference, we also report the best task-specific model from our pool for each task, denoted as Task-Best. Advanced single-model baselines with inference-time compute: We evaluate methods that enhance inference-time reasoning, specifically Self-Refine (SR) (Madaan et al., 2023b) and SelfConsistency (SC) (Wang et al., 2023b). To ensure fair comparison, we set SCs sample size to 5, aligning with the number of large language model (LLM) calls in SYMBOLIC-MOE, which engages three experts and one aggregator model.3 Additionally, for these baselines, we use the best-performing LLM for each task, inferred on the same dev set used for our agent profile creation. Single-model multi-agent baselines: To isolate the impact of SYMBOLIC-MOEs recruitment strategy, we compare against methods where multiple instances of the same model collaborate. Specifically, we consider Multi-Agent Debate (Debate) (Du et al., 2023) and Self-Mixture-ofAgents (Self-MoA) (Li et al., 2025), both of which rely on iterative, multi-round discussions using single model. These baselines employ three agents, each using the same task-best model, and conduct two rounds of discussion, resulting in total of 6 LLM calls per sample. Multi-model multi-agent baselines: We also evaluate approaches leveraging diverse models in multi-agent setup. This includes Mixture-of-Agents (MoA) (Wang et al., 2024a) and ReConcile (Chen et al., 2024c), both of which incorporate fixed set of models in multi-round interactions. To ensure fair comparison with our approach, particularly in the use of the validation set, we select the top three performing models from the validation set and conduct multi-round interactions. In MoA, agents participate in two rounds of discussion, while agents in ReConcile engage in three rounds, leading to 6 and 9 LLM calls per sample, respectively. 2For AIME, we sample validation questions from prior years problems (2012-2023). 3We use an odd number of SC calls to avoid ties. 8 For single-model baselines, we use the strongest task-specific LLM, while for multi-model baselines, we select the top three models per task. These selections, like those in SYMBOLIC-MOE, are determined based on validation performance. Table 8 in the Appendix details the top-1 and top-3 models for each task. Implementation Details. We conduct our experiments for SYMBOLIC-MOE and other singlemodel baselines on single A6000 GPU with 48 GB of memory, while MoA and ReConcile are executed on 8 A6000 GPUs for parallelization. For the 70B models, we use the original version without quantization and perform inference on 4 A6000 GPUs. All open-source models utilize vLLM (Kwon et al., 2023) for inference. The temperature is set to 0.7 for all methods. The maximum output token length is fixed at 4096 for all models, except for QwenR1 and LlamaR1, which have limit of 32768 since they are trained with longer trajectories and tend to generate longer outputs. All results, except those from proprietary models (due to budget constraints), are averaged over three random seeds. Further details on the model pool, distribution of the expert recruited, and all the prompts we use can be found in Table 7 and Appendix G. 4.2 Main Results We present the main results in Table 1, and summarize the key findings below. SYMBOLIC-MOE consistently outperforms all baselines. Across all domains, SYMBOLIC-MOE shows superior performance compared to all baselines, beating single-model baselines using the best overall model (e.g., SR, SC), multi-agent debate with single model (Debate and Self-MoA), as well as multi-model multi-agent baselines like MoA and ReConcile. SYMBOLIC-MOE outperforms the most competitive multi-agent baseline, Self-MoA, by 8.15% (absolute) on average, with consistent improvements across domains (e.g., 8.28% gain on MMLU-Pro, 13.45% on AIME, 4.92% on GPQA, 6.08% on MedMCQA). These gains are also seen when comparing to multi-model baselines like MoA and ReConcile that use the top three strongest models per domain. SYMBOLIC-MOE also substantially outperforms test-time scaling methods, such as Self-Refine (Madaan et al., 2023b) and Self-Consistency (Wang et al., 2023a). Surprisingly, when using the task-best model, SC beats multi-agent debate baselines (e.g., Self-MoA, MoA), though it still underperforms SYMBOLIC-MOE by an average of 3.71%. This indicates that scaling test-time compute with the task-best model is simple but effective way to improve performance, and adaptively selecting the most suitable experts leads to further improvements. SYMBOLIC-MOE generalize well across tasks. No single baseline in Table 1 is universally effective across all tasks. For instance, while MoA performs well on MMLU-Pro, it struggles on AIME; ReConcile excels in MedMCQA but fails to generalize to GPQA. Knowing which method works best for task is therefore nontrivial, requiring running every method on validation sets to choose from the many settings available. In contrast, SYMBOLIC-MOE consistently delivers strong performance across all domains. SYMBOLIC-MOE especially excels on AIME and GPQA, where SC is surprisingly strong baseline, and where other strong methods like MoA and Self-MoA fall far behind SYMBOLIC-MOE. Moreover, we see that, while SC with the top model is the most competitive setting on AIME and GPQA, it lags behind other baselines on MMLU-Pro and MedMCQA, where multi-agent baselines perform better. This discrepancy may stem from the broader subject diversity in MMLU-Pro and MedMCQA, where agent collaboration facilitates better consensus, whereas AIME is more math-focused, and favors individual model performance the task-best model, QwenR1 7B, delivers strong solo performance already. In light of that, we include QwenR1 7B in Table 1, which is powerful model distilled from DeepSeek-R1s trajectories (DeepSeek-AI et al., 2025a). While QwenR1 demonstrates exceptional math and code reasoning capabilities (55.93% on AIME), leading to strong Self-Consistency performance on AIME (67.78%), it struggles to generalize to other domains such as MedMCQA. This further underscores the need for robust and flexible framework like SYMBOLIC-MOE to achieve broad generalization across diverse tasks. SYMBOLIC-MOE matches strong proprietary models and larger 70B models. In Table 1, we also find that SYMBOLIC-MOE achieves similar average performance to models that have substantially more parameters. For example, SYMBOLIC-MOE outperforms Llama3.3 70B on AIME and GPQA and roughly matches it on MedMCQA, despite requiring only four 7-8B models 9 (three for the experts and one for the aggregator). Similarly, SYMBOLIC-MOE outperforms or matches number of strong proprietary models on average for instance, it matches Gemini 1.5 Pro and outperforms GPT4o-mini, driven in part by sizable gains on AIME and GPQA. These results underscore that, by drawing from large pool of experts, SYMBOLIC-MOE enables smaller set of models to achieve performance comparable to models with significantly more parameters, especially when considering heterogeneous set of tasks like the ones we explore. SYMBOLIC-MOE is more efficient. We compare SYMBOLIC-MOEs efficiency to naive implementation of sequential inference and to that of MoA and in Table 2. On GPQA, we measure the average run-time for the 198 examples in the test set. Unsurprisingly, the sequential inference baseline shows the highest latency, since the model is constantly changing, requiring loading and offloading it for each instance. We also find that SYMBOLIC-MOE operates in 44% less time on single GPU than MoA, which requires discussion, while achieving better accuracy. Also, SYMBOLIC-MOE running on single GPU shows similar time with MoA on 4 GPUs, and furthermore, when we run SYMBOLIC-MOE on 4 GPUs, it results in almost 2 speedup over MoA. Method # GPUs Run Time (s) Sequential Inference MoA MoA SYMBOLIC-MOE SYMBOLIC-MOE 1 4 1 4 196.92 45.98 21.66 25.76 10.85 Table 2: Efficiency comparison of MoA and SYMBOLIC-MOE. We report time per sample, based on the total time needed to perform inference for the whole GPQA test set (divided by the number of questions in the test set). SYMBOLIC-MOE achieves efficiency in two key ways: (1) it employs batched inference to minimize model loading time, and (2) it eliminates round-wise discussions, which introduce significant overhead at each round. In Table 3 we find that with reliable aggregator, skipping inter-agent discussions yields performance comparable to actually conducting them. Engaging agents in one-round discussion requires generating initial outputs, concatenating them, and then generating updated outputs, significantly increasing overhead. Like multi-agent discussion baselines, SYMBOLIC-MOE can also operate in discussion-based manner. Instead of aggregating initial responses, models first engage in round discussion before submitting final answers to an aggregator. Table 3 evaluates this approach, comparing the adaptive aggregator (suboptimal) and task-best aggregator (suboptimal), as well as the task-specific aggregator (optimal) on MMLU-Pro and GPQA. Given the optimal aggregator, we see that discussion yields marginal gains on MMLU-Pro (63.83 vs. 63.71) and modest drop on GPQA (57.72 vs. 57.78). We find that while round-wise discussion does improve performance incrementally, but the final outcome is ultimately determined by the strength of the aggregator. As result, SYMBOLIC-MOE has less run time than the baselines while maintaining or even surpassing their performance, as demonstrated in Table 2. 4.3 Analysis Utility of the Model Profile. SYMBOLIC-MOE profiles models based on their skills and leverages this information to recruit experts effectively. To underscore the importance of this step, we compare several alternative selection strategies in Table 4, evaluating accuracy on GPQA. Specifically, we assess performance when selecting the top-k agents overall and when choosing agents at random. In the top-k approach, experts are fixed as the best-performing models for the task, whereas in the random-k strategy, the selected experts vary across instances. Our results demonstrate that skill-based selection is essential. Notably, although 10 Discuss Aggr. MMLU-Pro GPQA Adaptive Adaptive Task-best Task-best Task-specific Task-specific 59.07 57.12 57.81 56.67 63.83 63.71 57.01 58.01 57.78 57. 57.72 57.78 Table 3: Comparison of SYMBOLICMOE with and without discussion across varying aggregators. Discussion stabilizes performance with suboptimal aggregators (first four rows) but has little effect with an optimal one (last two rows). Recruiting Strategy Acc. Top-3 Experts Top-5 Experts 3 Random Experts 5 Random Experts 52.86 47. 42.61 44.92 Model Profile (Ours) 57.78 Table 4: Comparison of different recruiting strategies on GPQA. selecting the top experts for task may seem intuitive, it consistently underperforms compared to SYMBOLIC-MOEs adaptive instance-level expert selection. Interestingly, top-5 selection performs worse than top-3 selection, suggesting that broader selection increases the likelihood of including weaker models, leading to performance degradation. Additionally, the random selection strategy consistently harms performance, showing 12.86% 15.61% drop compared to SYMBOLIC-MOE, likely also due to the inclusion of weaker experts. Overall, skill-based instance-level selection consistently outperforms more coarse-grained approaches, with SYMBOLIC-MOE surpassing MoA by 4.92% as also shown in Table 1. These findings emphasize the value of leveraging performance-based statistics and individual skill assessments for expert selection. 52.29 57.12 63. 48.92 58.01 57.78 Aggregator MMLU-Pro GPQA Random Adaptive Task-specific Role and Selection of the Aggregator. Unlike most of our discussion-based multi-agent baselines, SYMBOLICMOE collects single CoT and answer from each expert and combines them via an aggregator. Besides the efficiency gain as shown in Table 2, we investigate the role the aggregator plays in our framework. While experts are selected per instance, the aggregator is chosen based at the task, as we find that reasoning ability does not necessarily translate to effective aggregation. Table 5 examines the impact of aggregator selection in SYMBOLIC-MOE, comparing three strategies: (1) randomly chosen aggregator from the model pool, (2) an instance-specific top-1 aggregator based on model profiling, and (3) task-specific aggregator determined by task-level performance. Evaluated on MMLU-Pro and GPQA, the results indicate that random aggregator substantially degrades performance, indicating that the aggregator plays crucial role. While the instance-specific top-1 aggregator improves outcomes on both datasets, the task-specific aggregator outperforms it on MMLU-Pro and performs comparably on GPQA. We further find that the similar performance of instance-specific and task-specific aggregation on GPQA is due to high degree of overlap in selected aggregators. Overall, these findings suggest that model being good reasoner does not imply it will be good aggregator, supporting our decision to use task-based aggregation. Table 5: Ablations on different aggregators in our full setting."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "A key feature highlighted in Table 1 is the consistency of SYMBOLIC-MOEs performance. While baseline methods occasionally do well in isolated settings (e.g. MoA on MMLU-Pro, ReConcile on MedMCQA) it is important to highlight that no baseline does well consistently across settings. This means that without SYMBOLIC-MOE getting strong overall result would require evaluating all the baseline methods and choosing the best settings manually. In contrast to the baselines, SYMBOLICMOE consistently achieves high performance without human intervention. By automatically selecting discussion agents, SYMBOLIC-MOE provides consistent recipe that generalizes across domains. Modularity. Another key advantage of SYMBOLIC-MOE is its modularity. Unlike typical Mixtureof-Experts (MoEs) frameworks, which need to be trained end-to-end from scratch in centralized manner, SYMBOLIC-MOE uses the symbolic output channel of existing models to combine experts. This gradient-free approach enables seamless integration of pre-trained models without updates, allowing them to be trained independently and distributedly. Such delegation enhances domain specialization for each model. Moreover, while standard MoEs have fixed size determined before training, SYMBOLIC-MOE can adaptively grow and evolve as models are updated. Given the rapid advancements in LLMs, cost-effective and efficient updates are essential state-of-the-art models are often replaced within months. SYMBOLIC-MOEs modular and gradient-free design simplifies these updates, requiring only few calls to obtain new models agent profile. Connections to Inference-Time Scaling. Like other LLM-discussion frameworks, SYMBOLICMOE can be seen as form of multi-model inference-time scaling. Past work (Wang et al., 2023a; Snell et al., 2024; Shao et al., 2024) has highlighted the benefits of adding inference-time compute, ranging from simply sampling multiple responses (as done in our SC baselines) to more complex strategies such as refinement (Madaan et al., 2023a; Chen et al., 2024b). SYMBOLIC-MOE surpasses several such baselines  (Table 1)  by adaptively selecting the optimal set of expert models while avoiding the costly discussion process, reducing overhead while improving performance. Moreover, 11 our novel batching method enables SYMBOLIC-MOE to run efficiently on single GPU while remaining flexible for acceleration with multiple GPUs. Conclusion. We introduced SYMBOLIC-MOE, scalable MoE framework that combines models through their symbolic output (i.e., via natural language discussion). SYMBOLIC-MOE infers which skills are needed for given problem and recruits agents based on those skills to engage in discussion about given input, guiding the discussion to reach better consensus. On four diverse reasoning datasets, SYMBOLIC-MOE outperforms standard inference-time scaling methods as well as other debate frameworks and other mixture-of-agents methods, leading to consistently strong performance across domains without human intervention. SYMBOLIC-MOEs average performance across heterogeneous tasks is in fact stronger than that of advanced proprietary models such as GPT4o-mini. Moreover, unlike past work that requires multiple models to be loaded on separate GPUs running in parallel, SYMBOLIC-MOE introduces novel batching strategy that allows us to run on single GPU in roughly the same amount of time, obtaining the best of both worlds in terms of performance and efficiency."
        },
        {
            "title": "Limitations",
            "content": "Like other multi-agent discussion methods (Du et al., 2023; Chen et al., 2024c), SYMBOLIC-MOE involves running multiple models, which increases inference cost. This cost can be reduced via distillation: Chen et al. (2024a) distill multi-agent discussions between fixed set of agents into single model, showing improvements over distilling from single models. This approach could easily be adapted to distill from variable set of agents, allowing the student model to benefit from the routing and skill selection process. We leave distilling from SYMBOLIC-MOE to future work. SYMBOLIC-MOE also relies on skills inferred from small validation set to set the agent profiles. In our experiments, we ensure fair comparisons to the baselines by choosing models for the baselines according to the same validation set, giving the baselines equal access to the data. Skill inference relies on the Keyword LLM being sufficiently trained on given domain to infer relevant skills empirically, we find that this is the case across variety of domains. Overall, SYMBOLIC-MOE will continue to improve with better skill inference modules, which can easily be swapped in."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, Microsoft Accelerate Foundation Models Research (AFMR) grant program, NSFAI Engage Institute DRL-2112635, National Institutes of Health (NIH) under other transactions 1OT2OD038045-01, and Cisco and Capital One Faculty Awards. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of the NIH or other sponsors."
        },
        {
            "title": "References",
            "content": "Alhabib Abbas and Yiannis Andreopoulos. Biased mixtures of experts: Enabling computer vision inference under data transfer limitations. IEEE Transactions on Image Processing, 29:76567667, 2020. AI@Meta. Llama 3.3 model card. 2024. URL https://github.com/meta-llama/ llama-models/blob/main/models/llama3_3/MODEL_CARD.md. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. Justin Chen, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magdi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models. In Forty-first International Conference on Machine Learning, 2024a. Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. arXiv preprint Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning. arXiv:2409.12147, 2024b. Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms, 2024c. URL https://arxiv.org/ abs/2309.13007. Ke Chen, Lei Xu, and Huisheng Chi."
        },
        {
            "title": "Improved learning algorithms for mixture of experts in",
            "content": "multiclass classification. Neural networks, 12(9):12291252, 1999. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. URL https://arxiv.org/ abs/2501.17161. Herbert Clark. Using language. Cambridge university press, 1996. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025a. URL https://arxiv.org/abs/2501. 12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v3 technical report. 2025b. URL https://arxiv.org/abs/2412.19437. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023. David Eigen, MarcAurelio Ranzato, and Ilya Sutskever. Learning factored representations in deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. URL http://jmlr.org/papers/v23/21-0998.html. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning, pp. 1042110430. PMLR, 2023. Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, and Ion Stoica. Melange: Cost efficient large language model serving by exploiting gpu heterogeneity. arXiv preprint arXiv:2404.14527, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In International conference on machine learning, pp. 30403049. PMLR, 2019. 13 Hao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei, Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo, et al. Towards more effective and economic sparsely-activated model. arXiv preprint arXiv:2110.07431, 2021. Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, and Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pp. 35773599. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-emnlp.304. URL https://doi.org/10. 18653/v1/2021.findings-emnlp.304. Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. FineIn International tuning can distort pretrained features and underperform out-of-distribution. Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=UYneFzXSJWh. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:// openreview.net/forum?id=qrwe7XHTmYb. Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. Llm inference serving: Survey of recent advances and opportunities. arXiv preprint arXiv:2407.12391, 2024a. Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, and Jiayi Shen. Smoa: Improving multi-agent large language models with sparse mixture-of-agents. arXiv preprint arXiv:2411.03284, 2024b. Wenzhe Li, Yong Lin, Mengzhou Xia, and Chi Jin. Rethinking mixture-of-agents: Is mixing different large language models beneficial? arXiv preprint arXiv:2502.00674, 2025. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. Elita Lobo, Chirag Agarwal, and Himabindu Lakkaraju. On the impact of fine-tuning on chain-ofthought reasoning. arXiv preprint arXiv:2411.15382, 2024. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in neural information processing systems, 30, 2017. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023. MAA. American invitational mathematics examination - aime. in american invitational mathematics examination, 2 2024. URL https://maa.org/math-competitions/ american-invitational-mathematics-examination-aime. 14 Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. In NeurIPS, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html. Iterative refinement with self-feedback. Self-refine: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023b. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022. Brando Miranda, Alycia Lee, Sudharsan Sundar, and Sanmi Koyejo. Beyond scale: the diversity coefficient as data quality metric demonstrates LLMs are pre-trained on formally diverse data, 2024. URL https://openreview.net/forum?id=506Sxc0Adp. OpenAI. Gpt-4o mini: advancing cost-efficient intelligence, 2024. URL https://openai.com/ index/gpt-4o-mini-advancing-cost-efficient-intelligence/. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pp. 248260. PMLR, 2022. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, et al. Qwen2.5 technical report. 2025. URL https: //arxiv.org/abs/2412.15115. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2020. URL https:// arxiv.org/abs/2004.09813. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 85838595, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 48237d9f2dea8c74c2a72126cf63d933-Abstract.html. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017a. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017b. URL https:// openreview.net/forum?id=B1ckMDqlg. 15 Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language model capabilities. arXiv preprint arXiv:2406.04692, 2024a. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. Rethinking the bounds In Lun-Wei Ku, Andre Martins, of LLM reasoning: Are multi-agent discussions the key? and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 61066131, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.331. URL https://aclanthology.org/2024.acl-long.331/. Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, and Joseph Gonzalez. Deep mixture of experts via shallow embedding. In Uncertainty in artificial intelligence, pp. 552562. PMLR, 2020. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR. OpenReview.net, 2023a. URL https://openreview.net/pdf?id= 1PL1NIMMrw. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023b. URL https://openreview.net/forum?id=1PL1NIMMrw. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, November 2024c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. arXiv preprint arXiv:2305.11595, 2023. Enwei Xu, Wei Wang, and Qingxia Wang. The effectiveness of collaborative problem solving in promoting students critical thinking: meta-analysis based on empirical literature. Humanities and Social Sciences Communications, 10(1):16, 2023. ISSN 2662-9992. doi: 10.1057/ s41599-023-01508-1. URL https://doi.org/10.1057/s41599-023-01508-1. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. URL https: //arxiv.org/abs/2409.12122. Brandon Yang, Gabriel Bender, Quoc Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019. 16 W. Quin Yow and Tony Zhao Ming Lim. Sharing the same languages helps us work better together. Palgrave Communications, 5(1):154, 2019. ISSN 2055-1045. doi: 10.1057/s41599-019-0365-z. URL https://doi.org/10.1057/s41599-019-0365-z. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Seniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE Transactions on Neural Networks and Learning Systems, 23(8):11771193, 2012. doi: 10.1109/TNNLS.2012.2200299. Sukwon Yun, Inyoung Choi, Jie Peng, Yangfan Wu, Jingxuan Bao, Qiyiwen Zhang, Jiayi Xin, Qi Long, and Tianlong Chen. Flex-moe: Modeling arbitrary modality combination via the flexible mixture-of-experts. arXiv preprint arXiv:2410.08245, 2024. Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Conditional computation of transformer models for efficient inference. arXiv preprint arXiv:2110.01786, 2021. Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=B72HXs80q4."
        },
        {
            "title": "A Dataset Statistics and Licenses",
            "content": "We provide the sample sizes and licenses of the datasets used in this work in Table 6. All the datasets are in English, and all datasets are used in fashion consistent with their intended use. Validation Size Test Size License MMLU-Pro (Wang et al., 2024c) AIME (MAA, 2024) GPQA (Rein et al., 2023) MedMCQA (Pal et al., 2022) 350 354 249 2,100 30 198 4,183 Apache License CC0 MIT License MIT License Table 6: The statistics and licenses of the datasets we use in this work."
        },
        {
            "title": "B Model Pool",
            "content": "Model Name Size Huggingface Link BioLlama DeepSeekMath Exaone Gemma2 GLM4 Granite InternLM3 Llama3.1 LlamaR1 Mathstral Mistral Phi3.5-mini Qwen2.5 Qwen2.5-Coder Qwen2.5-Math QwenR1 8B ContactDoctor/Bio-Medical-Llama-3-8B deepseek-ai/deepseek-math-7b-instruct 7B 7.8B LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct google/gemma-2-9b-it 9B THUDM/glm-4-9b 9B ibm-granite/granite-3.1-8b-instruct 8B internlm/internlm3-8b-instruct 8B meta-llama/Llama-3.1-8B-Instruct 8B deepseek-ai/DeepSeek-R1-Distill-Llama-8B 8B 7B mistralai/Mathstral-7B-v0.1 12B mistralai/Mistral-Nemo-Instruct-2407 3.5B microsoft/Phi-3.5-mini-instruct 7B 7B 7B 7B Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-Coder-7B-Instruct Qwen/Qwen2.5-Math-7B-Instruct deepseek-ai/DeepSeek-R1-Distill-Qwen-7B Table 7: The models constituting the model pool."
        },
        {
            "title": "C Performance on the Validation Set",
            "content": "Table 8 shows the performance of each model on the validation set. We highlight the top-1 and top-3 models in bold font and yellow background, respectively. This information is also used for the baselines we compare against in Table 1."
        },
        {
            "title": "D Performance of Each Model as an Aggregator",
            "content": "Table 9 shows the performance of each model when acting as an aggregator. Note that the bestperforming model in Table 8 can be different from the best aggregator model in Table 9, motivating us to choose the aggregator based on this synthetic task described in Section 3.2.2."
        },
        {
            "title": "E Distribution of Experts",
            "content": "We present the distribution of recruited experts across different datasets in Fig. 4. As noted in Section 3.2.1, we trim experts with occurrences below 5% to reduce model loading time. This is 18 Model MMLU-Pro AIME GPQA MedMCQA BioLlama DeepSeekMath Exaone Gemma GLM Granite InternLM Llama LlamaR1 Mathstral Mistral Phi Qwen QwenCode QwenMath QwenR1 37.71 32.57 52.29 53.71 50.29 43.43 43.14 46.00 54.29 34.57 45.14 46.57 54.00 46.29 31.71 53. 0.85 3.32 25.99 7.73 7.37 5.92 7.91 6.78 51.98 3.11 1.41 1.41 13.56 9.89 11.13 57.06 27.31 28.11 32.13 36.95 30.92 34.14 36.14 33.73 56.22 36.55 33.73 47.79 37.35 30.52 28.51 51.41 42.86 35.71 56.35 64.29 58.33 56.15 55.56 66.87 53.37 52.38 46.43 65.87 67.06 50.79 36.90 37.90 Table 8: Comparison of model performance on the validation set. The best model on each task is bolded, and the top 3 models on each task are highlighted in yellow. Model MMLU-Pro AIME GPQA MedMCQA BioLlama DeepSeekMath Exaone Gemma GLM Granite InternLM Llama LlamaR1 Mathstral Mistral Phi Qwen QwenCode QwenMath QwenR1 37.31 32.57 57.43 49.71 52.57 48.86 55.14 51.14 59.71 41.71 48.00 27.71 56.86 51.14 31.71 58.00 21.47 5.37 47.92 3.11 26.27 36.44 16.95 11.86 53.67 26.27 18.93 9.04 38.14 29.66 5.93 57.63 30.12 21.69 35.34 31.73 35.34 38.96 42.57 40.56 46.18 35.74 33.33 26.10 39.36 38.96 16.06 48.59 42.46 35.71 56.35 53.37 51.39 56.15 51.59 50.60 49.01 46.43 46.43 25.40 53.37 50.79 36.90 45.44 Table 9: Performance of each model when used as an aggregator, on the validation set. The best model on each task is bolded, and is selected as the task-specific aggregator. 19 Figure 4: Distribution of the recruited experts across datasets. Top row: the distribution before trimming. Bottom row: the distribution after trimming and re-sampling. visualized in Fig. 4, where the top row shows the distribution before trimming, and the bottom row shows the distribution after trimming. The distribution varies significantly across datasets on more diverse datasets such as MMLU-Pro, the recruited experts are also more varied. In contrast, for AIME and GPQA, which focus more on math and science, the recruited experts are dominated by few models."
        },
        {
            "title": "F Algorithm",
            "content": "We provide the algorithm for our batched inference strategy in Appendix F. (k) RECRUITEXPERTS(q, M) expert sample map[e] expert sample map[e] {q} Algorithm 1 BatchedInference Require: Test samples Q, Model pool Ensure: Inference results for all samples 1: expert sample map 2: for do (2) (1) , , ..., 3: for Eq do 4: 5: 6: 7: end for 8: 9: results 10: for (e, qe) expert sample map do 11: 12: end for 13: return results end for results results e.GENERATE(qe) Expert-to-samples mapping Select experts per sample (3.3.1) Results collection Batch inference per expert"
        },
        {
            "title": "G Prompts",
            "content": "Prompt for the Keyword LLM to Generate Keywords Question: {question} What are the core knowledge, subjects or skills needed to solve this problem? List 2-5 keywords separated in comma. Example keywords: psychology, virology, behavioral theory, microbiology, diplomacy, political science, property law, finance, business. Give ONLY the keywords, no other words or explanation. Follow this format: Keywords: <keyword1>, <keyword2>... Prompt for Zero-shot Chain-of-Thought Generation (Multiple Choice) Question: {question} Provide your step-by-step reasoning first, and then print The answer is (X) where is the answer choice (one capital letter), at the end of your response. Prompt for Zero-shot Chain-of-Thought Generation (Math) Question: {question} Provide your step-by-step reasoning first, and then print The answer is boxed{X}, where is the final answer, at the end of your response. Prompt for the Aggregator (Wang et al., 2024a) You have been provided with set of responses from various open-source models to the latest user query. Your task is to synthesize these responses into single, high-quality response. It is crucial to critically evaluate the information provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the given answers but should offer refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability. Responses from models: {model 1 response} {model 2 response} {model 3 response} Question: {question} Provide your step-by-step reasoning first, and then print The answer is (X) where is the answer choice (one capital letter), at the end of your response. SYMBOLIC-MOE as Sparse Mixture-of-Expert In the Sparse Mixture-of-Experts (SMoE) framework (Shazeer et al., 2017a), trainable router dynamically selects subset of experts for each input. Formally, given an input x, the output of an SMoE layer, is computed as: = i=1 R(x)i fi(x), R(x) = softmax(Top-K(g(x)), k) 21 (1) where fi(x) represents the response of the i-th expert, and R(x) is trainable router that assigns selection probabilities to each expert based on g(x), typically small feedforward network (Shazeer et al., 2017b; Riquelme et al., 2021). The Top-K operation retains only the top experts, setting the probabilities of others to zero after the softmax operation. However, directly applying SMoE in our framework presents key challenges. Unlike SMoE, our method operates in symbolic, text-based space and is designed for test-time inference, meaning that we do not rely on trainable router to learn expert selection, nor do the experts in our method refer to model parameters. Instead, we introduce skill-based routing mechanism to select relevant experts based on predefined competencies rather than learned gating functions. Formally, our aggregation process can be expressed as: = A((cid:13) (cid:13) i=1y(i)) y(i) = E(i)(x) {1, 2, ..., k} E(i) Categorical(w(1), w(2), ..., w(n)) (2) where is the aggregator model determined via validation set, and (cid:13) (cid:13) denotes the concatenation of experts responses, i.e., y(). Here, y(j) represents the output of expert js forward response given an input x, defined as E(j)(x). Each expert E(i), is selected from our proposed skill-based routing strategy (Section 3.3.1). In short, we construct model profiles using validation set to evaluate each models specialization across different skills. This allows us to estimate probability distribution w(j) over models based on both their suitability for the required skills and their global competence relative to other experts. This skill-based routing framework retains the core benefits of SMoE while removing the reliance on trainable gating mechanism. Specifically, the aggregator model in SYMBOLIC-MOE plays role analogous to the weighted sum () operation in SMoE, synthesizing outputs from selected experts. Likewise, the recruited agent E(i) corresponds to the Top-k operation in SMoE, ensuring that only the most relevant and specialized experts contribute to the final output. We inherit the key conceptual benefits of SMoE dynamic expert selection and response aggregation while also introducing additional advantages. SYMBOLIC-MOE is gradient-free, eliminating the need for retraining, and is entirely automatic, leveraging large pool of pre-trained models to deliver better performance."
        }
    ],
    "affiliations": []
}