{
    "paper_title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
    "authors": [
        "Ruiheng Zhang",
        "Jingfeng Yao",
        "Huangxuan Zhao",
        "Hao Yan",
        "Xiao He",
        "Lei Chen",
        "Zhou Wei",
        "Yong Luo",
        "Zengmao Wang",
        "Lefei Zhang",
        "Dacheng Tao",
        "Bo Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 1 ] . [ 1 2 2 5 1 1 . 1 0 6 2 : r UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation Ruiheng Zhang1,, Jingfeng Yao2,, Huangxuan Zhao1,,, Hao Yan1, Xiao He1, Lei Chen2, Zhou Wei1, Yong Luo1, Zengmao Wang1, Lefei Zhang1, Dacheng Tao3, Bo Du1, 1Wuhan University 2Huazhong University of Science and Technology 3Nanyang Technological University"
        },
        {
            "title": "Abstract",
            "content": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and diffusion branch for highfidelity generation. Crucially, cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with rigorous data cleaning pipeline and multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves 46.1% improvement in understanding performance (Micro-F1) and 24.2% gain in generation quality (FD-RadDino), using only quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX. 1. Introduction Figure 1. The quantitative and qualitative results of UniX. Quantitative results show UniXs superiority over existing unified and single-task medical foundation models in understanding and generation. Qualitatively, UniX enables multi-disease X-ray interpretation and high-fidelity medical image generation. In recent years, visionlanguage pretrainingbased medical foundation models have shown remarkable success in both understanding [1, 4, 6, 19, 24, 29, 30, 42] and generation [2, 3, 9, 26, 38, 39] tasks. As research progresses, medical image understanding and generation are increasingly seen as interconnected tasks, where semantic reasoning and visual synthesis can mutually reinforce each other. This insight has spurred the development of unified medical foundation models [15, 17, 22, 46] that aim to integrate both capabilities within single framework. However, unified modeling of these two capabilities is inherently challenging, given their fundamentally different objectives of semantic abstraction versus pixel-level reconstruction. Existing efforts, such as LLM-CXR [17], often employ parameter sharing and joint multi-task heads for integrated learning. This approach, unfortunately, can introduce task competition and feature interference, which degrades performance in both understanding and generation. HealthGPT [22] mitigates this issue through task-specific H-LoRA modules, offering structured compromise but not fundamental solution. Moreover, most current unified medical foundation models still rely on discretized generation paradigms, whose outputs are constrained by vocabulary granularity and fail to recover fine structural details in medical images. straightforward alternative is to attach diffusion model to pre-trained visionlanguage model [8, 11]. While this improves generative quality to some extent, it fails to fully exploit understanding features to guide generation, thereby underutilizing the potential of unified architecture. Through systematic analysis, we identify two intrinsic limitations in existing unified medical foundation models. First, understanding and generation possess conflicting objectives. Understanding requires semantic abstraction, whereas generation demands pixel-level reconstruction. Jointly learning these opposing goals in shared feature space causes interference. Second, paradigm mismatch exists between discrete autoregression and continuous imaging. Discrete methods inherently struggle to capture the fine-grained structural details of medical images. Consequently, prior works often resort to superficial stacking or cascading to combine these tasks. This strategy achieves unification only in form and fails to exploit deep architectural synergy between the two capabilities. To address these challenges, we propose UniX. This framework fundamentally resolves the tension between semantic processing and visual synthesis. We adopt decoupled dual-branch architecture to eliminate the understanding-generation conflict. An autoregressive branch focuses on semantic abstraction, while separate branch handles pixel-level reconstruction. To bridge the paradigm mismatch, the generation branch leverages diffusion models. This design captures the continuous nature of medical images and avoids the granularity loss inherent in discrete tokenization. Finally, we introduce cross-modal self-attention mechanism to ensure architectural synergy. Unlike superficial stacking, this module dynamically injects understanding features into the diffusion process. This effectively links semantic reasoning with high-fidelity generation. To further improve data quality and training efficiency, we implement rigorous data cleaning pipeline and adopt In the first stage, we stagewise optimization strategy. freeze the generation branch and train only the understanding branch to acquire medical image interpretation capabilities. In the second stage, we freeze the understanding branch and pre-train the generation branch to learn basic image generation. In the third stage, we continue to freeze the understanding branch and fine-tune the generation branch for high-resolution image generation. Thanks to our architectural design and optimization strategy, UniX achieves dual-task modeling with significantly fewer parameters, maintaining strong vision-language understanding while attaining high-quality generation performance."
        },
        {
            "title": "The main contributions of this paper are summarized as",
            "content": "follows: We propose UniX, next-generation unified medical foundation model that structurally decouples yet coordinates understanding and generation. To our knowledge, it is among the first efforts to integrate autoregressive and diffusion paradigms in the medical imaging field. We introduce cross-modal self-attention mechanism to bridge the understanding and generation branches. This module seamlessly integrates the understanding features as contextual conditions, providing dynamic, contentaware guidance throughout the generation process. Experiments on chest X-ray report generation and image synthesis tasks show that UniX uses only quarter of the parameters of LLM-CXR, yet improves understanding performance (Micro-F1) by 46.1% and generation performance (FD-Raddino) by 24.2%, achieving performance comparable to single-task medical foundation models. 2. Related Work 2.1. Single-task Medical Foundation Model Medical foundation models for single tasks primarily center on two major objectives. The first category focuses on image understanding. These models handle tasks such as disease diagnosis, knowledge-based question answering, and report generation. Early studies [6, 24, 42] mainly targeted disease classification. They employed CNNs [16] or Transformers [32] to extract imaging features, followed by task-specific heads for classification, segmentation, or prediction. Recently, with the rise of multimodal large language models [12, 18, 33, 40], medical foundation models [1, 4, 19, 29, 30] have evolved toward more clinically meaningful applications, such as medical question answering and report generation. These models typically combine visual encoder with large language model, processing multimodal tokens in an autoregressive way. The second category centers on image generation. These models address tasks like synthesis, super-resolution, and inpainting. With recent advances in generative AI [7, 23, 27, Figure 2. Model Architecture. UniX comprises two decoupled yet synergistic branches: an autoregressive understanding branch for semantic encoding, and diffusion-based generation branch for visual synthesis. To enable effective collaboration between them, we introduce cross-modal self-attention mechanism that allows semantic features to dynamically guide the generation process. Data Processing and Training Pipeline. To fully exploit the potential of this architecture, we design rigorous data cleaning pipeline and three-stage training strategy. This strategy progressively freezes the branches during different stages, ensuring efficient knowledge transfer and stable training. 37, 41, 43, 47], diffusion-based approaches have become the dominant paradigm for high-fidelity medical image generation. Single-task generative models can produce realistic, instruction-aligned images, helping to expand datasets and mitigate long-tail issues. Some studies [2, 9] further demonstrate that synthetic data can enhance the performance of visual understanding models, revealing potential synergy between generation and understanding. 2.2. Unified Medical Foundation Model Unified models [5, 11, 20, 3436] aim to handle both understanding and generation within single architecture. They represent promising next step for medical foundation models. Existing unified medical foundation models [15, 17, 46] typically adopt shared Transformer backbone with multi-task heads. However, this parameter-sharing strategy faces inherent conflicts. Understanding tasks require compressing and abstracting information, while generative tasks demand preserving and reconstructing details. These opposing objectives cause feature interference and limit overall performance. To address this, HealthGPT [22] introduces H-LoRA modules to separate task-specific parameters. This improves performance but still lags behind specialized singletask models. In addition, most unified medical foundation models rely on discrete generation methods based on visual bag-of-words [10, 31]. Since these approaches compress continuous pixel data into fixed codebook, they inevitably discard high-frequency details and subtle texture variations. Consequently, they struggle to capture continuous, fine-grained pathological patterns. As result, image fidelity remains limited. In contrast, UniX introduces dual-branch architecture that integrates autoregressive and diffusion paradigms, echoing insights from BAGEL [7] on the value of bottleneck-free multimodal interaction. This design resolves the objective conflict through architectural decoupling. The understanding branch focuses on semantic comprehension, while the diffusion branch specializes in highfidelity image generation. The two branches interact via cross-modal self-attention, allowing understanding features to guide the generation process dynamically. Through this synergy, UniX achieves strong performance comparable to single-task models while maintaining high parameter efficiency. 3. Method In this section, we introduce UniX, next-generation medical foundation model designed to achieve decoupled yet synergistic learning between Chest X-ray understanding and generation. As shown in Figure 2, our model contains two core components: an autoregressive understanding branch and diffusion-based generation branch. The understanding branch, built on vision-language model, handles semantic abstraction and report reasoning. The generation branch is built directly upon the inherited LLM backbone from the understanding branch, and it specializes in synthesizing high-fidelity Chest X-ray images. cross-modal selfattention module connects the two, allowing dynamic feature exchange and semantic conditioning during generation. 3.1. Understanding via Autoregression The understanding branch formulates multimodal comprehension as an autoregressive sequence modeling problem. This formulation aligns naturally with medical report generation, where the model must reason over both visual and textual contexts in causal manner. Concretely, we define multimodal token sequence = [V, Tin, Tout], where , Tin, and Tout denote the visual tokens, input textual tokens, and output textual tokens respectively. Let be the starting index of Tout in S, and be the ending index of S. The cross-entropy loss is computed over the autoregressive predictions for all tokens in Tout: LCE = n1 (cid:88) i=m log (Si+1Si; ωu), (1) where is the index of the last token in the sequence S, and ωu denotes parameters of the understanding branch. This design allows the model to jointly capture visual semantics and linguistic reasoning within unified space. 3.2. Generation via Latent Diffusion The generation branch adopts latent diffusion framework that reconstructs medical images from high-level semantics extracted by the understanding branch. Instead of operating in pixel space, diffusion is performed in VAE-encoded latent space, which greatly improves efficiency and stability. Given latent variable xt sampled from the noisy distribution pt(x) at time t, the model learns to estimate the target velocity field ut(x) by minimizing the mean-square error: LM SE = Et,pt(x) (cid:2)vt (x; ωg) ut (x)2(cid:3) , (2) where ωg represents the parameters of the generation branch. The semantic embeddings from the understanding branch act as conditioning inputs, enabling disease-specific synthesis and improved lesion localization. 3.3. Cross-Modal Self-Attention To enable semantically informed visual generation, we introduce cross-modal self-attention mechanism [7] that facilitates bidirectional information flow between the understanding and generation branches. Unlike conventional cross-attention, which conditions one modality on static context, our formulation performs joint self-attention over unified multimodal token sequence. This design allows semantic representations from the understanding branch to directly modulate the generative trajectory, while also permitting generative states to feed back into the semantic space. Let the unified sequence be = [Tin, ], where Tin denotes the textual tokens produced by the understanding branch and denotes the noise-conditioned latent embeddings from the generation branch. For each token Si, we compute modality-specific projections for queries, keys, and values as: {Qi, Ki, Vi} = δu(i) {q,k,v}Si + δg(i) {q,k,v}Si, (3) where the modality selectors δu(i) and δg(i) are defined as: δu(i) = (cid:40) 1, Si Tin, 0, Si N, δg(i) = 1 δu(i). (4) This formulation yields two distinct parameter spaces for understanding and generation tokens, while maintaining shared attention operation across the unified sequence. The resulting attention map is computed in standard form: Attn(S) = softmax (cid:19) (cid:18) QK V, (5) but all cross-modal interactions are learned implicitly through the joint attention scores rather than through explicit conditioning. This mechanism synchronizes an autoregressive branch for understanding and diffusion branch for high-fidelity generation, ultimately improving the fidelity and clinical consistency of the generated images. 3.4. Three-Stage Training Pipeline As shown in Figure 2, we adopt three-stage training strategy to progressively align the understanding and generation branches. Stage 1: Medical Understanding Supervised FineTuning. In this stage, the generation branch is frozen. We fine-tune the visual encoder, visual connector, and language model backbone in the understanding branch using paired medical images and reports. This step helps Hyper Parameters Stage One Stage Two Stage Three Learning rate LR scheduler Resolution Use REPA REPA loss weight Batch Size Weight decay Gradient norm clip Optimizer Warm-up steps Training steps 1e-4 Constant 384 256 0.0 1.0 80 3840 1e-4 Constant 512 False 256 0.0 1.0 2e-4 Constant 256 True 0.5 256 0.0 1.0 AdamW(0.9, 0.95, 1e15) 2K 75K 0 5K Table 1. Hyper-parameter settings for three training stages. Note that we have introduced weights for the multiple loss functions to make them more accessible. Specifically, the REPA loss weight refers to the weight ratio between the MSE loss and the REPA loss. the model learn the semantic correspondence between images and text. As result, the understanding branch gains strong abilities in medical image interpretation and report generation. It also serves as high-level semantic feature provider for the generation branch in later stages. Stage 2: Medical Generation Pretraining. Here, we freeze the understanding branch and pre-train the generation branch on textlow-resolution image pairs. To accelerate convergence, we apply Representation Alignment [44], aligning the eighth-layer hidden states of the generation branchs language model with RadDino image features using similarity objective. This design enables the generation branch to better utilize high-level semantics from the understanding branch for low-resolution medical image synthesis. Stage 3: Medical Generation Fine-Tuning. We maintain the same freezing strategy as in Stage 2 and fine-tune the generation branch using texthigh-resolution image pairs. During this stage, we extend the positional encoding of the generation branch and remove feature-level supervision. After fine-tuning, the generation branch can synthesize high-resolution medical images with improved reportimage alignment, clearer lesion depiction, and higher visual fidelity. 4. Experiments This section, we describe how UniX exploits its decoupled autoregressivediffusion dual-branch design through data processing, model configuration, and three-stage training pipeline. use frontal-view radiographs and refine the paired reports with the DeepSeek large language model. The full cleaning pipeline is provided in the supplementary material. Following the official split, we obtain 163,344 imagereport pairs for training and 2,365 for testing. For the generation branch, we follow the processing and split protocol of ChexGenBench [9], resulting in 237,387 training and 4,352 test pairs. Images are resized to 384384 for understanding fine-tuning, 256256 for generation pretraining, and 512512 for generation fine-tuning. Aspect ratios are preserved by padding when needed. Model Details. Both branches are partially initialized from Janus-Pro [5]. For the understanding branch, we adopt siglip-large-patch16-384 [45] as the visual encoder. It produces 1024-dimensional embeddings, which are mapped to the 2048-dimensional LLM space through two-layer MLP. The language backbone contains 24 transformer layers and incorporates QK normalization and QKV bias. For the generation branch, we use 16 downsampled, 16-channel VAE for encoding and decoding. Two single-layer MLPs provide bidirectional projection between the 16-dimensional latent space and the 2048-dimensional language space. The generation backbone follows the same initialization strategy as the understanding branch. Training Details. All models are trained with fullparameter fine-tuning on eight NVIDIA L20 GPUs. The complete hyperparameter configuration is listed in Table 1. 4.2. Decoupled Architecture for Task Separation We conduct extensive comparisons among understandingonly, generation-only, and unified medical foundation models. Qualitative examples of UniXs performance in both understanding and generation are shown in Figures 3 and 4. As observed, UniX produces precise reports and highfidelity images. We next present detailed results for understanding and generation tasks. For understanding tasks, we primarily evaluate the medical reliability of generated reports using the CheXbert F1 score [28] between generated and ground-truth reports. Adincluding BLEU [25], Radditional evaluation metrics, graph [13] and ROUGE-L [21], are provided in the supplementary material. For generation tasks, we measure generation quality with FD-RadDino and KD-RadDino, assess imagetext consistency with the Alignment Score, and evaluate accuracy and diversity using the four PRDC metrics. 4.2.1. Understanding 4.1. Implementation Details Data Details. We conduct experiments on the MIMICCXR dataset [14]. For the understanding branch, we The training dynamics of the understanding branch are detailed in the Supplementary Material. As training progresses, the cross-entropy loss decreases steadily, and the models report generation ability improves rapidly at first Model Und. Params CheXbert uncertain as negative CheXbert uncertain as positive Micro F1-14 Micro F1-5 Macro F1-14 Macro F1-5 Micro F1-14 Micro F1-5 Macro F1-14 Macro F1-5 Signal-task Medical Foundation Model GPT-4V Med-PaLM LLaVA-Rad LLaVA-Med FlamingoCXR PromptMRG RGRG LLM-CXR HealthGPT UniX 84B 7B 7B 3B <1B <1B 12B 3.8B 1.5B 35.5 53.6 57.3 27.2 15.3 38.9 24.2 53.6 25.8 57.9 57.4 22.0 6.0 47.2 25.1 56.6 20.4 39.8 39.5 15.5 7.8 23. 14.6 33.2 19.6 51.6 47.7 16.6 3.5 40.8 35.6 57.3 27.3 51.9 15.0 37.4 Unified Medical Foundation Model 18.5 47.3 36.0 25.5 52. 33.3 60.2 24.4 58.0 6.9 49.0 28.2 57.9 25.3 44.0 18.7 8.4 24.4 21.1 17.5 35.5 29.6 53.3 20.5 4.1 42.7 22.6 49. Table 2. Comparison of various medical foundation models on X-ray understanding tasks. The data reveals that UniX achieves substantial improvement in understanding over the unified medical foundation model. Notably, it delivers performance comparable to larger, single-task medical foundation model, despite having fewer parameters. Figure 3. Demonstration of Data Processing and Report Generation Efficacy. The application of large language models enables the purification of raw data by eliminating extraneous information. This process ensures that the model prioritizes and extracts pertinent information related to disease diagnosis. before gradually plateauing. Although continued finetuning can further reduce the loss, we observed that key performance metrics begin to decline, indicating that the model tends to overfit specific patterns rather than learning general principles. Table 2 summarizes the understanding performance of various medical foundation models. UniX achieves better performance than unified models while using substantially fewer parameters. Compared with single-task models, it outperforms models of similar scale and approaches the performance of much larger ones. We exclude medical agent systems here since they typically build upon existing medical foundation models and rely on multi-model collaboration. comparison between UniX and such agents is provided in the supplementary material. 4.2.2. Generation We observed that the mean squared error for the generation branch decreases rapidly early on and then slows in both stages. During medical generation pre-training, key metrics show limited improvement between 25K and 50K steps but increase sharply from 50K to 75K steps. The corresponding loss curves and detailed analysis are included in the Supplementary Material. Table 3 reports the generation results. Compared with LLM-CXR, UniX delivers clear improvements in both image quality and imagetext alignment. It also consistently outperforms single-task models, reflecting the advantage of its decoupled yet collaborative architecture. Notably, UniX Model Gen. Params Resolution FD-RadDino KD-RadDino Alignment Score Precision Recall Density Coverage Flux.1-Dev Lumina 2.0 SD V3.5 Medium SD V2-1 RadEdit Sana Pixart Sigma LLM-CXR UniX UniX 2.6B 2.5B 2.5B 0.86B 0.86B 0.6B 0.6B 12B 1.5B 1.5B 1024 1024 1024 512 512 512 512 256 256 512 122.400 101.198 91.302 186.530 69.695 54.225 60.154 71.243 65.208 54. Signal-task Medical Foundation Model 0.144 0.110 0.103 0.413 0.033 0.016 0.023 0.061 0.051 0.024 0.036 0.121 0.044 0.197 0.677 0.695 0.697 0.420 0.574 0.632 0.530 0.397 0.674 0.666 Unified Medical Foundation Model 0.319 0.251 0.635 0.782 0.675 0.736 0.008 0.014 0.205 0.049 0.544 0.614 0.522 0.041 0.243 0.479 0.125 0.256 0.401 0.180 0.150 0.520 0.506 0.671 0.366 0. 0.326 0.170 0.244 0.038 0.285 0.548 0.506 0.459 0.419 0.550 Table 3. Comparison of various medical foundation models on X-ray generation tasks. Under standardized benchmark, UniX matches the output quality of single-task medical models. Furthermore, it demonstrates exceptional performance in both accuracy and diversity. HealthGPT was not included in this test due to the lack of publicly available text-to-image generation code. indicates that all these generative models from the natural image domain were fine-tuned on the same X-ray dataset. Figure 4. Qualitative Examples from UniX. (A)-(C) illustrate the models precise control over the attributes of generated findings, including their severity and location. In (D), the model successfully synthesizes complex radiographic scene containing multiple findings that are consistent with full clinical report, highlighting its ability to process and integrate extensive contextual information. performs on par with the strong baseline Sana. Even though Sana is also fine-tuned on the target dataset, UniX matches its performance and achieves comparable, and in some metrics slightly superior. This demonstrates that our unified approach maintains top-tier generation quality without compromising semantic consistency. We further evaluate pathology-specific generation quality in Table 4. UniX achieves consistently higher fidelity across wide range of lesion types, capturing subtle pathological cues and preserving clinically relevant details. In these fine-grained tasks, UniX remains highly competitive with Sana. This is particularly noteworthy given that UniX balances unified multi-task objective whereas Sana focuses on specialized generation task. Consequently, the Model Gen. Params Resolution FD-RadDino RadEdit Pixart Sigma Sana 0.86B 0.6B 0.6B LLM-CXR UniX UniX 12B 1.5B 1.5B 512 512 512 256 256 512 At Cd Cn Ec Fc Fr LL LO NF PE PO PN PT SD Signal-task Medical Foundation Model 63.38 62.79 136.59 76.94 155.97 197.58 184.11 61.90 67.88 60.60 215.92 114.66 151.34 53.10 59.27 60.39 133.96 73.93 155.53 179.44 174.63 56.83 48.74 59.05 210.90 108.42 150.55 51.61 51.03 54.68 127.46 67.84 147.00 172.32 163.14 49.23 44.60 49.80 199.45 88.52 141.99 46.51 Unified Medical Foundation Model 71.57 71.37 136.65 83.18 148.28 168.50 163.22 66.93 64.62 67.83 200.84 108.04 147.52 67.54 63.34 63.39 129.32 73.88 150.25 177.68 165.88 58.31 60.58 58.55 201.53 105.96 141.63 57.61 52.19 51.70 122.84 64.36 142.23 176.35 156.81 49.15 45.71 48.06 191.65 99.31 135.48 47.04 Table 4. Generation Performance per Pathology. Within the unified medical foundation model, UniX dominates the comparison, achieving top performance in 13 out of the 14 categories. indicates that all these generative models from the natural image domain were fine-tuned on the same X-ray dataset. Configs Metrics 5.2. Impact of Joint Dual-Branch Optimization Train Branch Data Ratio Micro-F1 Macro-F1 FD KD Gen Und & Gen Und & Gen Und & Gen Und & Gen 0 : 1 1 : 1 1 : 2 1 : 4 0 : 53.2 43.7 42.9 44.9 13.9 36.0 29.5 28.3 28.4 6.3 62.114 0.041 65.747 0.047 76.125 0.064 76.108 0.064 74.815 0.054 Table 5. Impact of Joint Dual-Branch Optimization. Freezing the understanding branch yields fast generative gains without harming comprehension. Unfreezing it without understanding data severely degrades comprehension and offers no generative benefit. Mixing both data types mitigates this degradation but slows generative learning. results demonstrate strong fine-grained visual synthesis capability and robust performance under diverse diagnostic conditions. 5. Ablations 5.1. Impact of Data Cleaning on Understanding Figure 3 illustrates the critical role of data cleaning with DeepSeek in mitigating hallucinations. Raw hospital reports often contain significant noise, such as underscores, technical metadata, and conversational fillers, which complicates the alignment between visual features and textual descriptions. By employing targeted prompts to strip away these non-diagnostic elements, we construct cleaner and more semantic-dense target for the model. This preprocessing step is crucial because it forces the model to attend strictly to clinically relevant patterns during training, resulting in generated reports that are factually grounded and free from structural hallucinations. To study how joint fine-tuning affects understanding and generation, we design multiple medical image generation experiments, each fine-tuned for 2K generation steps. We evaluate three strategies: 1) Unfreeze only the generation branch. 2) Unfreeze both branches and train with mixture of understanding and generation data, where we vary the mixing ratio. 3) Unfreeze both branches but train without any understanding data. The results in Table 5 lead to several observations. First, the fine-tuned understanding branch should not be involved in subsequent training of the generation branch. Fully freezing the understanding branch yields rapid gains in generation performance without harming understanding accuracy. In contrast, unfreezing the branch without providing understanding data is detrimental; it severely degrades understanding performance and offers no benefit to generation. Adjusting the ratio of understanding and generation data can partially mitigate the drop in understanding performance, as the semantic supervision stabilizes the updated parameters. However, this setup forces the model to balance two competing objectives within the same updates, which slows the acquisition of strong generative capability. 6. Conclusion We present UniX, next-generation unified medical foundation model that achieves architectural decoupling and coordination for Chest X-Ray understanding and generation. Existing unified medical foundation models neither resolve the intrinsic conflict nor fully exploit the strengths of different modeling paradigms. To address these limitations, we design dual-branch architecture that combines autoregressive understanding with diffusion-based generation. This structure decouples the two tasks and prevents mutual interference. We introduce cross-modal self-attention mechanism that aligns both branches and enables understanding features to guide generation dynamically. With these designs, UniX delivers stronger understanding and generation performance than prior unified medical foundation models while using fewer parameters, and it reaches competitiveness with dedicated single-task medical foundation models. We hope that UniX offers new perspective for advancing medical foundation models."
        },
        {
            "title": "References",
            "content": "[1] Yaowei Bai, Ruiheng Zhang, Yu Lei, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, et al. From bench to bedside: deepseekpowered ai system for automated chest radiograph interpretation in clinical practice. arXiv preprint arXiv:2507.19493, 2025. 1, 2 [2] Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier Van Der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. visionlanguage foundation model for the generation of realistic chest x-ray images. Nature Biomedical Engineering, 9(4):494506, 2025. 1, 3 [3] Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. Roentgen: vision-language foundation model for chest x-ray generation. arXiv preprint arXiv:2211.12737, 2022. 1 [4] Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, et al. Towards clinically accessible radiology foundation model: openaccess and lightweight, with automated evaluation. arXiv preprint arXiv:2403.08002, 2024. 1, 2 [5] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 3, 5 [6] Ziwei Cui, Jingfeng Yao, Lunbin Zeng, Juan Yang, Wenyu Liu, and Xinggang Wang. Lkcell: Efficient cell nuclei instance segmentation with large convolution kernels. arXiv preprint arXiv:2407.18054, 2024. 1, [7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 3, 4 [8] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. 2 [9] Raman Dutt, Pedro Sanchez, Yongchen Yao, Steven McDonagh, Sotirios Tsaftaris, and Timothy Hospedales. Chexgenbench: unified benchmark for fidelity, privacy and utility of synthetic chest radiographs. arXiv preprint arXiv:2505.10496, 2025. 1, 3, 5 [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 3 [11] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 2, 3 [12] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 [13] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew Lungren, Andrew Ng, et al. Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463, 2021. 5 [14] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, deidentified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. 5 [15] Tackeun Kim, Jihang Kim, Leonard Sunwoo, and Edward Choi. Unixgen: unified vision-language model for multiview chest x-ray generation and report generation. arXiv preprint arXiv:2302.12172, 2023. 1, [16] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 2002. 2 [17] Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye. Llm-cxr: instruction-finetuned llm for cxr image understanding and generation. arXiv preprint arXiv:2305.11490, 2023. 1, 2, 3 [18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [19] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. 1, 2 [20] Hao Li, Changyao Tian, Jie Shao, Xizhou Zhu, Zhaokai Wang, Jinguo Zhu, Wenhan Dou, Xiaogang Wang, Hongsheng Li, Lewei Lu, et al. Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2976729779, 2025. 3 [21] Chin-Yew Lin. Rouge: package for automatic evaluation In Text summarization branches out, pages of summaries. 7481, 2004. 5 [22] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large visionlanguage model for unifying comprehension and generation arXiv preprint via heterogeneous knowledge adaptation. arXiv:2502.09838, 2025. 1, 2, 3 [23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2 [24] DongAo Ma, Jiaxuan Pang, Michael Gotway, and Jianming Liang. fully open ai foundation model applied to chest radiography. Nature, pages 111, 2025. 1, 2 [25] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. 5 [26] Fernando Perez-Garcıa, Sam Bond-Taylor, Pedro Sanchez, Boris van Breugel, Daniel Castro, Harshita Sharma, Valentina Salvatelli, Maria TA Wetscherek, Hannah Richardstress-testing son, Matthew Lungren, et al. Radedit: biomedical vision models via diffusion image editing. In European Conference on Computer Vision, pages 358376. Springer, 2024. [27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [28] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Ng, and Matthew Lungren. Chexbert: combining automatic labelers and expert annotations for accuarXiv preprint rate radiology report labeling using bert. arXiv:2004.09167, 2020. 5 [29] Ryutaro Tanno, David GT Barrett, Andrew Sellergren, Sumedh Ghaisas, Sumanth Dathathri, Abigail See, Johannes Welbl, Charles Lau, Tao Tu, Shekoofeh Azizi, et al. Collaboration between clinicians and visionlanguage models in radiology report generation. Nature Medicine, 31(2):599608, 2025. 1, 2 [30] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. Nejm Ai, 1(3):AIoa2300138, 2024. 1, 2 [31] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 3 [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [34] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable and unified multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. 3 [35] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [36] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 [37] Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, et al. Pixel-perfect depth with semantics-prompted diffusion transformers. arXiv preprint arXiv:2510.07316, 2025. 3 [38] Ziyang Xu, Huangxuan Zhao, Ziwei Cui, Wenyu Liu, Chuansheng Zheng, and Xinggang Wang. Most-dsa: interactions for direct Modeling motion and structural arXiv preprint multi-frame interpolation in dsa images. arXiv:2407.07078, 2024. 1 [39] Ziyang Xu, Huangxuan Zhao, Wenyu Liu, and Xinggang Wang. Garamost: Parallel multi-granularity motion and structural modeling for efficient multi-frame interpolation in dsa images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2853028538, 2025. [40] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. 2 [41] Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. 3 [42] Jingfeng Yao, Xinggang Wang, Yuehao Song, Huangxuan Zhao, Jun Ma, Yajie Chen, Wenyu Liu, and Bo Wang. Evax: foundation model for general chest x-ray analysis with self-supervised learning. arXiv preprint arXiv:2405.05237, 2024. 1, 2 [43] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1570315712, 2025. 3 [44] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. 5 [45] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. 5 [46] Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, and Si Yong Yeo. Medunifier: Unifying vision-and-language pre-training on medical data with vision generation task usIn Proceedings of the ing discrete visual representations. Computer Vision and Pattern Recognition Conference, pages 2974429755, 2025. 1, 3 [47] Ya Zou, Jingfeng Yao, Siyuan Yu, Shuai Zhang, Wenyu Liu, and Xinggang Wang. Turbo-vaed: Fast and stable transfer of video-vaes to mobile devices. arXiv preprint arXiv:2508.09136, 2025."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Nanyang Technological University",
        "Wuhan University"
    ]
}