{
    "paper_title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?",
    "authors": [
        "Junhao Cheng",
        "Yuying Ge",
        "Teng Wang",
        "Yixiao Ge",
        "Jing Liao",
        "Ying Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in CoT reasoning and RL post-training have been reported to enhance video reasoning capabilities of MLLMs. This progress naturally raises a question: can these models perform complex video reasoning in a manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues. Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching a conclusion. To address this issue, we present Video-Holmes, a benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 7 3 1 2 . 5 0 5 2 : r Video-Holmes: Can MLLM Think like Holmes for Complex Video Reasoning? Junhao Cheng1,2, Yuying Ge1,(cid:66), Teng Wang1,(cid:66), Yixiao Ge1, Jing Liao 2, Ying Shan1 1ARC Lab, Tencent PCG 2City University of Hong Kong https://video-holmes.github.io/Page.github.io/ Figure 1: An example of Video-Holmes. Models are required to actively locate and connect multiple relevant visual clues scattered across different video segments to render the final answer."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Chain-of-Thought (CoT) reasoning and reinforcement learning (RL) post-training have been reported to enhance video reasoning capabilities of multimodal large language models (MLLMs). This progress naturally raises question: can these models perform complex video reasoning in manner comparable to human experts? However, existing video benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues (e.g., What is the woman wearing?). Such benchmarks do not fully capture the intricacies of realworld reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching conclusion. Empirical results show that even models with advanced thinking abilities achieve only marginal gains (e.g., from 68.3% to 69.4%) on these benchmarks, which raises doubts about the extent to which these tasks require genuine reasoning. To address this issue, we present Video-Holmes, benchmark inspired by the reasoning process of Sherlock Holmes, designed to evaluate the complex video reasoning capabilities of MLLMs. VideoHolmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks. Each task is constructed by first identifying key events and causal relationships within films, and then designing questions that require models to actively locate and connect multiple relevant visual Preprint. Under review. clues scattered across different video segments. We conduct detailed analysis of model reasoning processes, examining the factors that lead to both correct and incorrect answers. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We aim that Video-Holmes can serve as Holmes-test for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field. The benchmark is released in https://github.com/TencentARC/Video-Holmes."
        },
        {
            "title": "Introduction",
            "content": "The development of CoT reasoning [1] and RL post-training strategies [2] have contributed to significant improvements in the reasoning abilities of LLMs [3, 4, 5]. By generating human-like reasoning steps, these models have shown strong performance in addressing complex reasoning tasks. Furthermore, these advancements have been successfully adapted to MLLMs for video understanding and reasoning [6, 7, 8, 9]. This progress naturally raises question: can these models perform complex video reasoning in manner comparable to human experts? However, existing evaluation benchmarks for video reasoning [10, 11, 12, 13, 14, 15, 16, 17] are limited by their predominant focus on assessing the visual perception and grounding capabilities of models, where questions that can answered based on explicit prompts or isolated visual cues (e.g., What is the woman wearing?). Such benchmarks do not fully capture the intricacies of real-world reasoning, where humans must actively search for, integrate, and analyze multiple clues before reaching conclusion. Empirical results show that even models with advanced thinking abilities [9] achieve only marginal gains (e.g., from 68.3% to 69.4%) on these benchmarks [18], which raises doubts about the extent to which these tasks require genuine reasoning. To address this issue, we present Video-Holmes, benchmark inspired by the reasoning process of Sherlock Holmes. It is designed to assess the complex video reasoning abilities of MLLMs and exam the factors contributing their correct and incorrect answers. As demonstrated in Table 1, Video-Holmes differs from existing benchmarks in several key aspects: (1) We utilize suspense short films as the video sources with detailed manual annotations. These videos are characterized by rich elements of suspense, reasoning, and supernatural themes, making them particularly challenging for models to comprehend. (2) The questions in Video-Holmes require models to actively locate and connect multiple relevant visual clues scattered across different video segments to infer the final answer. As illustrated in Figure 1, models first need to identify the abnormal scene involving the man and then progressively integrate the extracted visual clues to deduce the cause of the mans death, just like the reasoning process of Sherlock Holmes. (3) We provide detailed analysis of models reasoning processes, examining the factors that lead to both correct and incorrect answers. Our comprehensive evaluation of state-of-the-art (SOTA) MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. For example, the best-performing model, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models scoring below 40%. We make the following contributions in this work: We present Video-Holmes, benchmark for complex video reasoning. Video-Holmes comprises 270 manually annotated suspense short films, along with 1,837 challenging questions, which require models to actively locate and link multiple relevant visual clues, offering the research community high-quality and challenging video reasoning benchmark. We conduct extensive experiments on Video-Holmes to evaluate existing SOTA MLLMs. We analyze the reasoning processes of these models and observe that while they perform well in visual perception, they face significant challenges in integrating clues and frequently overlook critical clues. These observations provide valuable insights for future research. 2 Table 1: Comparison between Video-Holmes and existing video reasoning benchmarks across several key aspects: the video source domain (Domain), annotation methodology (Anno.), the number of reasoning QA pairs (RQA Pairs), necessity for models to actively seek out clues (Active Seeking), necessity for models to link multiple clues (Chain-of-Clues), whether provide reasoning process analysis (RPA), and whether provide audio information (Aud.). Benchmarks Domain Anno. RQA Pairs Active Seeking Chain-of-Clues RPA Aud. VSI-Bench [10] MVBench [15] TempCompass [16] Video-MMMU [14] MMVU [12] Video-MME [18] VCR-Bench [13] Indoor scenes Open Open Academic Science Open Open A&M A&M A&M Video-Holmes (Ours) Suspense short films A&M 5,130 4,000 7,540 900 3,000 1,944 1, 1,"
        },
        {
            "title": "2 Related Works",
            "content": "MLLMs for Video Understanding and Reasoning. The advancements in MLLMs for image understanding and reasoning [19, 20, 21, 22] have enabled their extension to video-based tasks. Methods such as VideoChat [23], Video-ChatGPT [24], CogVLM2Video [25], InternVL [26, 27], LLaVAVideo [28], and Qwen-VL [29, 30] treat videos as sequences of images, allowing large language models (LLMs) to perform video understanding and reasoning. CoT reasoning and reinforcement post-training strategies have been shown to enhance the reasoning capabilities of LLMs [4, 3], and recent work extends these techniques to MLLMs for multimodal reasoning tasks involving images [31, 32, 33, 34, 35, 36] or videos [7, 6, 37, 8, 38]. For instance, Video-R1 [6] and VideoChatR1 [7] employ supervised fine-tuning (SFT) followed by reinforcement learning post-training using Group Relative Policy Optimization (GRPO) [2] based on Qwen-VL-2.5 [39]. These methods achieve higher performance compared to Vanilla Qwen-VL-2.5 across diverse reasoning tasks. Video Reasoning Benchmarks. Early video understanding benchmarks primarily assess model capabilities within specific scenarios. For instance, MSRVTT-QA [40], ActivityNet-QA [41], and NExT-QA [42] focus on fundamental tasks such as action recognition and video question answering. Recently, benchmarks like MMBench [43], TempCompass [16], and MVBench [15] evaluate reasoning over short video clips, while LongVideoBench [44] and Video-MME [14] extend evaluations to longer video sequences. However, these tasks are generally straightforward and do not require complex reasoning. With the success of chain-of-thought (CoT) reasoning, there is increasing interest in advancing video reasoning in more challenging scenarios. Benchmarks such as MMVU [12] and VideoMMMU [11] evaluate reasoning in academic and scientific domains, while VSI-Bench [10] focuses on indoor environments. The recent VCR-Bench [13] introduces benchmark specifically designed to assess CoT reasoning in video tasks. Despite these developments, such benchmarks primarily evaluate visual perception and grounding abilities, with questions that can be answered based on explicit prompts or isolated visual cues and do not fully capture the intricacies of real-world reasoning. In contrast, Video-Holmes require models to actively locate and connect multiple relevant visual clues, engaging them in more complex and demanding reasoning scenario."
        },
        {
            "title": "3 Video-Holmes",
            "content": "As shown in Figure 2, the construction of Video-Holmes involves three steps: video collection and annotation, task definition, and question-answer-explanation generation. Video Collection and Annotation. Suspense short films serve as an ideal source for evaluating the complex video reasoning capabilities of MLLMs, as they are characterized by compact narratives enriched with hints, plot twists, and supernatural elements. We utilize the keyword suspense short films to search videos from YouTube with durations between 1 and 5 minutes. We incorporate nine subkeywords1 in this process to ensure diversity. From the initial pool of over 2,500 videos with audio information retrieved through our search, we manually curated subset of 270 high-quality, 1Details are provided in Appendix C. 3 Figure 2: Construction and evaluation pipeline of Video-Holmes. We select 270 high-quality suspense short films for human annotation. Next, we design 7 challenging tasks and employ DeepSeek to generate questions. Finally, we evaluate SOTA MLLMs and use DeepSeek to analyze their responses. reasoning-reach short films with rigorous annotation process. Each film is annotated following structured template that considers the following aspects: Segmented Plot Descriptions: Annotators are asked to divide the video into segments based on the progression of the storyline, and provide detailed descriptions for each segment. Key Character Relationships: Annotators are asked to present the relationships between key characters in the video, along with evidence that supports the identification. Reasoning Shots: Annotators are asked to identify reasoning shots in the video, providing timestamps, visual clues, and the inferred conclusions associated with these shots. Supernatural Elements: Annotators are asked to specify any supernatural elements present in the videos and the implications they introduce, whether positive or negative. Core Theme: Annotators are asked to summarize the core themes of the videos. These diverse short films with intricate reasoning chains, along with high-quality and well-formatted manual annotations ensures the reliability and quality of Video-Holmes. Task Definition. To comprehensively evaluate the differences in MLLMs capabilities for complex video reasoning from multiple perspectives, we define seven distinct reasoning tasks for VideoHolmes. As illustrated in Figure 3, different from existing benchmarks that primarily designed around clue-given questions, Video-Holmes focus on tasks that require models to actively locate and connect multiple relevant visual clues scattered across different video segments: Social Reasoning (SR): Inferring social relationships between characters. This includes identifying identity associations across time (e.g., the same man in youth and old age). Physical Anomaly Reasoning (PAR): Identifying scenes in the video that deviate from real-world norms and reasoning about their underlying rules or implicit meanings. Multimodal Hint Reasoning (MHR): Decoding cues or fact from multimodal hints, such as semantic implications of camera movements or gradual changes in object positions. Intention & Motive Chaining (IMC): Observing characters actions or environmental cues to disentangle surface behaviors from underlying behavioral intentions. Temporal Causal Inference (TCI): Inferring causal mechanisms between events across time and space using cinematic language and multimodal clues. Timeline Analysis (TA): Integrating and reconstructing the narrative storyline of the film. Core Theme Inference (CTI): Extracting the core theme or deeper meaning of the video by analyzing its plot, dialogues, and symbolic elements. Question-Answer Generation. We utilize DeepSeek-R1 [3] with advanced reasoning capabilities to automatically generate questions based on formatted manual annotations and predefined question Figure 3: Comparison of question types between Video-Holmes and existing benchmarks. Existing benchmarks primarily involve clue-given questions, where models depend on explicitly provided clues to derive answers. In contrast, Video-Holmes adopts an active seeking paradigm, requiring models to actively locate and connect multiple relevant visual clues scattered across different video segments. (Key frames are marked with black boxes and magnified.) types. Each question is generated by strictly adhering to the provided information, with manual sampling inspection to ensure quality and relevance. Additionally, the model is required to provide correct answer explanations for each question, which are used to compare and analyze the models reasoning process. Please refer to Appendix for details. After data verification and annotation, we have ultimately constructed dataset comprising 270 videos and 1,837 question-answer pairs. The key statistics of Video-Holmes are presented in Appendix C."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Evaluation Models. We conduct an evaluation of several mainstream MLLMs, including the opensource models: InternVL2.5 (8B) [45], InternVL3 (8B) [27], Qwen2.5-VL (7B, 32B) [39], and Qwen2.5-Omni (7B) [30]. Additionally, we assess open-source models that incorporate RL post5 Table 2: Results of various models on Video-Holmes, where SR stands for Social Reasoning; IMC stands for Intention & Motive Chaining; TCI stands for Temporal Causal Inference; TA Timeline Analysis; MHR stands for Multimodal Hint Reasoning; PAR stands for Physical Anomaly Reasoning; CTI stands for Core Theme Inference. Blue represents the vanilla model, while Green represents its corresponding thinking version with RL post-training. Model Frames SR IMC TCI TA MHR PAR CTI Overall InternVL2.5-8B InternVL3-8B Qwen2.5-Omni-7B Qwen2.5-VL-32B Qwen2.5-VL-7B SEED-Bench-R1 VideoChat-R1 Video-R1 GPT-4o OpenAI o4-mini Claud 3.5 Sonnet Claud 3.7 Sonnet Gemini-1.5-Pro Gemini-2.5-Pro Gemini-2.0-Flash Gemini-2.0-Flash-Thinking Open Source Models 21.5 32.2 28.0 37.9 40.7 29.5 13.9 19.9 27.1 31.5 44.2 43.2 17.6 34.8 38.4 25.6 35.1 42.8 24.5 38.8 42.1 28.9 41.7 48. Closed Source Models 49.6 50.0 31.2 36.3 43.5 48.6 48.2 45.9 48.2 52.1 49.3 46.6 33.7 41.8 46.9 43.4 38.8 20.5 30.8 33.7 34.4 46.9 23.1 43.1 07.7 35.1 7.5 51.0 30.0 40.5 39.5 34.5 30.0 34.0 41.0 39.5 26.0 53.0 20.5 51.0 32 32 32 32 32 32 32 32 32 32 20 20 - - - - 25.7 24.6 14.8 36.4 27.1 29.2 29.5 31.0 44.0 30.1 39.8 40.7 39.2 40.1 30.1 37.9 23.8 38.9 14.9 31.4 18.6 29.9 27.8 33.5 39.2 30.9 36.6 39.7 46.4 44.3 26.8 43.6 22.6 24.1 13.7 32.2 25.2 32.6 29.3 35.9 37.0 27.4 33.7 38.1 38.9 37.4 33.7 39. 23.8 32.3 16.4 38.4 27.8 33.5 33.0 36.5 42.0 29.9 39.3 41.0 41.2 45.0 30.6 43.1 Table 3: Thinking model performances on Video-Holmes and other benchmarks. Model MMVU Video-MME Video-MMMU MVBench Video-Holmes Gemini-2.0-Flash Gemini-2.0-Flash-Thinking 65.2 64.5 (-0.7) 68.3 69.4 (+0.9) 54.3 56.4 (+1.9) 55.3 57.4 (+2.1) 30.6 43.1 (+12.5) training based on Qwen2.5-VL (7B): SEED-Bench-R1 [8], Video-R1 [6], and VideoChat-R1 [7]. We also include several advanced closed-source models in our evaluation: Gemini-2.0-Flash [46], Gemini-2.0-Flash-Thinking [9], Gemini-1.5-Pro [47], Gemini-2.5-Pro [48], GPT-4o [49], OpenAI o4-mini [50], Claud 3.5 Sonnet [51], and Claud 3.7 Sonnet [51]. Implementation Details. For models with native video input support, such as Qwen-VL and Gemini, videos were processed directly without additional pre-processing. For models lacking native video input capabilities (e.g., GPT-4o), frames were uniformly extracted from the video along with corresponding timestamp annotations, and multi-image input was utilized for evaluation. To ensure fair comparison, all models were deployed following their official guidelines and using the officially released checkpoints. During inference, models were required to first generate reasoning process before providing the final answer. Specifically, the models were instructed to produce step-by-step solution to the given question. For further details regarding model implementation and evaluation prompts, please refer to Appendix and B. 4.2 Main Results Table 2 presents the performance of each models on Video-Holmes benchmark. Most models achieve an accuracy below 40%, with the best-performing model, Gemini-2.5-Pro, reaching an overall accuracy of 45%. The widely-used open-source models, Qwen2.5-VL (7B) achieves an overall accuracy of 27.8%, far worse than its performance on other video reasoning benchmarks. This performance gap suggests that the Video-Holmes benchmark introduces unique challenges that are particularly demanding for current MLLMs in video reasoning tasks. Models trained with thinking strategies exhibit notable improvements over their vanilla version. For instance, Gemini-2.0-Flash-Thinking demonstrates 12.5% performance gain compared to Gemini-2.0-Flash. This observation indicates that the Video-Holmes benchmark imposes substantial Table 4: Reasoning process analysis results. Where VPE represents visual perception error, VOE represents visual omission error, RE represents reasoning error, TRAW represents think right answer wrong, TWAR represents think wrong answer right, and TRAR represents think right answer right. Model VPE VOE RE TRAW TRAR TWAR Answer Wrong Answer Right Count Ratio Count Ratio Count Ratio Count Ratio Count Ratio Count Ratio InternVL2.5-8B InternVL3-8B Qwen2.5-Omni-7B Qwen2.5-VL-32B Qwen2.5-VL-7B SEED-Bench-R1 VideoChat-R1 Video-R GPT-4o o4-mini Claud 3.5 Sonnet Claud 3.7 Sonnet Gemini-1.5-Pro Gemini-2.5-Pro Gemini-2.0-Flash Gemini-2.0-Flash-Thinking 0.06 0.03 0.02 0.04 0.02 0.03 0.03 0.04 0.02 0.03 0.02 0.01 0.07 0.03 0.03 0.03 Open Source Models 0.28 0.22 0.36 0.39 0.27 0.30 0.26 0.36 740 810 643 610 544 688 844 0.63 0.73 0.59 0.56 0.70 0.63 0.71 0.59 Closed Source Models 0.24 0.29 0.31 0.44 0.24 0.22 0.20 0.22 745 840 695 584 692 718 893 718 0.73 0.68 0.67 0.54 0.67 0.75 0.76 0.75 331 248 398 430 209 325 306 250 354 318 470 250 210 242 210 66 32 26 44 15 32 33 46 17 36 18 9 75 31 33 31 32 22 29 6 5 54 12 15 13 11 11 10 18 4 13 4 0.03 0.02 0.03 0.01 0.01 0.05 0.01 0. 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.01 332 419 236 363 457 543 494 504 720 513 687 612 637 725 542 774 0.79 0.76 0.82 0.78 0.87 0.80 0.85 0.83 0.94 0.94 0.92 0.86 0.85 0.88 0.89 0.94 89 135 53 103 67 133 88 43 34 59 103 114 98 65 49 0.21 0.24 0.18 0.22 0.13 0.20 0.15 0.17 0.06 0.06 0.08 0.14 0.15 0.12 0.11 0.06 reasoning challenges and effectively distinguishes models reasoning abilities. In contrast, other benchmarks do not reflect this pattern, as illustrated in Table 3. Model performance across seven reasoning tasks in Video-Holmes remains relatively even, with most models achieving accuracy below 40% for each task. This highlights that each task in Video-Holmes poses substantial challenges, requiring advanced reasoning capabilities from existing methods. 4.3 Analytical Study Reasoning Process Analysis. We analyze the factors contributing to the models answers by comparing its reasoning process with human-annotated descriptions and answer explanations. Specifically, we categorize the main causes of incorrect answers into the following four types: Visual Perception Error (VPE): The model extracts incorrect visual information for analysis, leading to an incorrect answer. Visual Omission Error (VOE): The model omits critical visual information (i.e., key objects or events), resulting in an incorrect answer. Reasoning Error (RE): The model makes errors during the reasoning process, such as misinterpreting or incorrectly associating multiple visual clues. Think Right Answer Wrong (TRAW): The models reasoning is largely aligned with the ground-truth explanation, but it selects an incorrect option when providing the final answer. For correctly answered questions, we define the following two categories: Think Wrong Answer Right (TWAR): The models reasoning process deviates significantly from the ground-truth explanation, yet it arrives at the correct answer. Think Right Answer Right (TRAR): The models reasoning process is largely aligned with the ground-truth explanation and produces answers consistent with its reasoning. We provide the human annotations, questions, the models reasoning outputs (if validated), and the type definitions as inputs to DeepSeek-R1 [3], prompting2 it to perform the analysis. The results in Table 4 and Figure 4 show that both open-source and closed-source models generally demonstrate the ability to accurately extract visual information and provide answers consistent with their reasoning processes. Approximately 35% of errors are attributed to the omission of critical visual 2Detailed in Appendix 7 Figure 4: Example of model reasoning processes on Video-Holmes. VideoChat-R1 misinterprets visual information, incorrectly perceiving tattooed man, Qwen2.5-VL overlooks critical visual clues (the baby), Video-R1 fails to establish logical connections between the visual clues, and Intern-VL38B guesses the right option with wrong reasoning process. information, while larger proportion (around 60%) stems from challenges in logical comprehension of multiple visual clues (Reasoning Errors). For correctly answered questions, the proportion of responses based on valid reasoning (TRAR) exceeds 80% across most models. This highlights the difficulty of the Video-Holmes benchmark, where models struggle to infer correct answers through inconsistent reasoning. Number of Input Frames. We analyze the performance of several models using different numbers of input frames. The results in Table 5 (a) indicate that increasing the number of input frames generally improves model performance, but does not lead to substantial gains. This observation indicates that in most cases, the visual information provided is sufficient, and the key challenge lies in the models ability to integrate and interpret visual clues effectively. 8 Table 5: Analyze experiment results on Video-Holmes. HA stands for human annotation; FLC stands for frame-level caption; VLC stands for video-level caption. (a) Number of input frames (b) Audio input Model Frames Acc Model Audio SR Overall Qwen2.5-VL-7B Qwen2.5-VL-7B Video-R1 Video-R1 GPT-4o GPT-4o 64 80 64 80 40 30.2 (+2.4) 33.0 (+5.2) 37.4 (+1.2) 38.5 (+2.0) 43.3 (+1.3) 44.6 (+2.6) Qwen2.5-Omni-7B Qwen2.5-Omni-7B Gemini-2.5-Pro Gemini-2.5-Pro Gemini-1.5-Pro Gemini-1.5-Pro 27.1 38.4 46.6 54.8 52.1 59.6 16.4 24.4 45.0 51.3 41.2 45.7 (c) Reasoning or not Model Frames Reasoning Acc Model (d) Text-only input Input Qwen2.5-VL-7B Qwen2.5-VL-7B Video-R1 Video-R1 Gemini-2.0-Flash Gemini-2.0-Flash 32 32 32 - - 27.8 29.4 36.5 28.2 30.6 28.5 DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 OpenAI o3 OpenAI o3 OpenAI FLC VLC HA FLC VLC HA Acc 31.2 64.6 92.0 25.4 61.3 89.7 Audio Input. We evaluate the performance of several models with audio input as an additional modality. Table 5 (b) demonstrates that integrating audio input enhances model performance, especially in social reasoning tasks where conversational cues offer essential insights into interpersonal dynamics. These results underscore the importance of audio information in multimodal reasoning. Reasoning or Not. We conduct experiments where models directly generate answers without using CoT prompts. The results in Table 5 (c) demonstrate that for stronger closed-source models, CoT prompting leads to higher accuracy compared to directly answering. In contrast, weaker open-source models exhibit the opposite trend. This suggests that the effectiveness of reasoning is contingent on the models overall capabilityonly models with sufficiently strong reasoning abilities can fully benefit from CoT prompts. Conversely, weaker models may amplify errors during CoT reasoning. Text-only Input. We conduct experiments using three types of text-only inputs for advanced reasoning models [3, 5]: (1) human-annotated movie plots and key clues (excluding reasoning conclusions), (2) frame-level captions generated by Qwen2.5-VL (sampled at one frame per second), and (3) video-level captions generated by Gemini-2.5-pro. Table 5 (d) shows that models achieve around 90% accuracy with human annotations, while performance drops significantly with frame-level and video-level captions. This can be attributed to human annotations, which provide logical connections between critical visual clues in the video. In contrast, video-level and frame-level captions may overlook key visual details or offer incorrect logical interpretations, leading to reasoning errors. These findings underscore the challenge posed by Video-Holmes in requiring models to locate and and capture logical relationships within multiple visual clues."
        },
        {
            "title": "5 Conclusion and Discussion",
            "content": "In this work, we propose Video-Holmes, benchmark designed to evaluate the complex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837 questions derived from 270 manually annotated suspense short films, which spans seven carefully designed tasks that require models to actively locate and connect multiple relevant visual clues scattered across different video segments. We conduct detailed analysis of model reasoning processes, examining the factors that lead to both correct and incorrect answers. Our comprehensive evaluation of state-of-the-art MLLMs reveals that, while these models generally excel at visual perception, they encounter substantial difficulties with integrating information and often miss critical clues. We aim that Video-Holmes can serve as Holmes-test for multimodal reasoning, motivating models to reason more like humans and emphasizing the ongoing challenges in this field."
        },
        {
            "title": "A Model Implementation Details",
            "content": "the utilize official checkpoints QwenVL-2.5-7B, QwenVL: We different QwenVL models: Qwen/Qwen2.5-VL-7B-Instruct for QwenVL-2.5-7B, Qwen/Qwen2.5-VL-7B-Instruct for QwenVL-2.5-32B, Qwen/Qwen2.5-VL-32B-Instruct Video-R1/Video-R1-7B for Video-R1, and OpenGVLab/VideoChat-R1-7B for VideoChatR1. The decoding configuration follows the settings provided in the official QwenVL-2.5 demo, with top-p set to 0.001 and temperature set to 0.01. During inference, we increase the frame resolution to 256 28 28 pixels to enhance visual fidelity. for for InternVL: We InternVL models: OpenGVLab/InternVL3-8B for InternVL3-8B and OpenGVLab/InternVL2-5-8B for InternVL2.58B. The input image size is resized to 448 448 according to the official configuration. checkpoints various official utilize the for API Models: We access the Gemini, GPT, and Claude model series via the official APIs provided by Google, OpenAI, and Anthropic. Specifically, for the GPT series, we use the official functions to retrieve image URLs and set the \"detail\" parameter to \"low\" as recommended."
        },
        {
            "title": "B Prompt Template",
            "content": "Question Generation Prompt You are professional logician and detective story enthusiast. will provide you with the plot information of short detective film. Your task is to create some reasoning multiple-choice questions for viewers who have watched this short detective film to test whether they truly understood it. Requirements: will provide the following information: (1) Segmental plot description: chronological plot description (2) Key character relationships and cause positioning: characters in the film and the information that can be used to infer their relationships. (3) Reasoning shots: key hint shots in the plot, plot twists, and climaxes. (4) Supernatural elements: supernatural elements and rules appearing in the video. (5) Main idea of the video: the possible main idea the video wants to express. the relationships between key You need to create at least one multiple-choice question from each of the following seven types of reasoning (according to the specific requirements): Type 1: Social Reasoning (SR) Task description: infer the implicit social relationship network between characters through clothing style matching, group interaction topology analysis, including identity association across time spans (such as the same character in youth and old age). Number of questions: Create one question for each combination based on the given key character relationships. If the [segmental plot description] provided includes the same character across time spans, create question based on this. Example 1: What is the relationship between the man at the beginning of the video and the skeleton at the end? (Answer: A. The same person) Type 2: Intention and Motive Chaining (IMC) Task description: infer the underlying psychological state and predict non-explicit behavioral intentions by observing characters actions, expressions, or environmental clues. The question format must strictly follow the example: [Character] at [time] performed [very brief, non-key detail action description] with what intention/psychological state? 10 Question Generation Prompt (Continue) Number of questions: 1-2 questions Example 1: What is the most likely psychological state of the man in black smoking at 00:03 in the video? (Answer: C. Anxiety about preparing for crime) Type 3: Temporal Causal Inference (TCI) Task description: infer causal mechanisms between non-continuous temporal events through camera language and multimodal clues. Number of questions: 1-2 questions Example 1: Why did the man in the shirt die? (Answer: F. Overuse of superpowers) Type 4: Timeline Analysis (TA) Task description: reorganize video clips in chronological order. You need to provide 5 key events and ask to restore the timeline. Number of questions: 1 question Example 1: 1. The man in black wakes up 2. The man in black is attacked 3. The man in black goes out 4. The man in black is sleeping 5. The man in black meets the woman Type 5: Multimodal Hint Reasoning (MHR) Task description: analyze the visual and auditory hints set by the director: camera movement semantics, object position changes, sound and picture metaphors. Ask about specific camera transitions, the appearance, movement, or change of objects. Number of questions: Create one question for each given hint shot based on the given [reasoning conclusion]. Example 1: The correct interpretation of the dancing scene between the man and woman in the video is (Answer: A. The mans own fantasy) Type 6: Physical Anomaly Reasoning (PAR) Task description: identify scenes in the video that do not conform to reality logic (such as magic, sci-fi elements). The first question asks about the rules of supernatural elements, and the second question asks about their implied meaning. Number of questions: (2n questions) Create questions based on the given supernatural elements. If none are provided, skip this type. Type 7: Core Theme Inference (CTI) Task description: infer the core theme or deep meaning through the plot, dialogue, or symbols in the video. (The given main idea of the video is subjective conclusion and may not be entirely correct) The questions you propose need to be understood and carefully reasoned to answer. Each question needs to provide 6 options, ensuring the correct option is objectively unique (distractor options cannot express similar meaning to the correct option, they must be clearly distinguished), and the distractor options should overlap with the correct option as much as possible (e.g., A. Because the man forgot to bring his phone B. Because the man forgot to bring his tablet C. Because the man forgot to bring his wallet). You need to provide the question type, question, answer, and explanation. Key point: The question stem cannot reveal character psychology, factual information, hint clues, and hint conclusions, viewers must find them themselves. For example: Bad question: \"What does the skeleton at the end of the video imply?\" - Good question: \"What does the skeleton at the end of the video imply?\"; Bad question: \"What is the psychological state of the man when he imagines the future in the lighting change scene?\" - Good question: \"What is the psychological state of the man in the lighting change scene?\" Answer format example: For each question, the 1st line: [Type]: CTI The 2nd line: [Question]: Your question; The 3rd-8th lines: A-F options; The 9th line: [Answer]: F; The 10th line: [Explanation]: Your explanation. 11 Model Evaluation Prompt (Thinking) Based on the given video, reason and answer the single-choice question. Provide your reasoning between the <think> and </think> tags, and then give your final answer between the <answer> and </answer> tags. The question is: question. The options are: options. Your answer: Model Evaluation Prompt (Without Thinking) Based on the given video, answer the single-choice question. Give your final answer choise between the <answer> and </answer> tags. The question is: question. The options are: options. Your answer: Reasoning Process Analysis Prompt (Thinking Wrong) You are an expert in logic. Your task is to analyze the cause of errors in multimodal large models responses to video reasoning tasks. The requirements are as follows: will provide you with plot description of short reasoning film, question, options, the correct answer, and an explanation of the correct answer.Then will provide you with the [thought process] of multimodal large model and its incorrect answer. Your task is to compare the multimodal large models thought process with the correct explanation and the film plot. Determine the most critical cause of the error from the following: (1) VPE (Visual Perception Error): The model extracted incorrect visual information for analysis, leading to the wrong answer. (For example, the video shows man robbing with knife, but the model perceives it as man robbing with gun.) (2) VOE (Visual Omission Error): The model failed to extract key visual information (such as key objects and events), leading to the wrong answer. (3) RE (Reasoning Error): The model made an error in the reasoning process, misinterpreting the implications of the visual information and incorrectly judging the relationships between multiple pieces of visual information. (4) TRAW (Think Right Answer Wrong): The models thought process is generally consistent with the answer explanation, but it chose the wrong option when answering. Your response format: Please provide the error abbreviation within <Type></Type>. Then give your brief judgment reason within <Reason></Reason>. Reasoning Process Analysis Prompt (Thinking Right) You are an expert in logic. Your task is to analyze the thought process of multimodal large model in responding to video reasoning tasks. The requirements are as follows: will provide you with [plot description] of short reasoning film, question, options, the correct answer, and an explanation of the correct answer. Then will provide you with the [models thought process] and its correct answer. Your task is to compare the multimodal large models thought process with the correct explanation and the film plot. Determine which type the response belongs to: (1) TWAR (Think Wrong Answer Right): The models thought process shows significant deviation from the answer explanation, using incorrect information to arrive at the correct conclusion. (2) TRAR (Think Right Answer Right): The models thought process is generally consistent with the answer explanation (allowing for minor deviations). Your response format: Please provide the response type abbreviation within <Type></Type>. Then give your brief judgment reason within <Reason></Reason>. 12 Key Statistics of Video-Holmes The key statistics of Video-Holmes are shown in Table 6. To ensure diversity, we include nine subkeywords (Anim, Comic, Detective, Future, Horror, Social, Supernatural, Thriller) when searching for suspense short films. The distribution of the nine specifically designed tasks is relatively balanced, with higher proportion of MHR tasks because single video often contains more than one reasoning shot annotated by humans. PAR tasks are absent in videos without supernatural phenomena, as such questions are not applicable. Examples of Video-Holmes Figures 5 to 12 illustrate an example of VideoHolmes. Specifically, Figure 5 presents the human annotation results, while Figures 6 to 12 display the questions and explanations generated by DeepSeek, along with the models answers and reasoning process analysis."
        },
        {
            "title": "E Broader Impact",
            "content": "Table 6: Key Statistics of Video-Holmes. Statistic Total Suspense Short Films - Anime - Comic - Detective - Future - Horror - Social - Supernatural - Thriller Total Questions - SR - IMC - TCI - TA - MHR - PAR - CTI Number 270 23 (8.5%) 32 (11.9%) 31 (11.5%) 7 (2.6%) 88 (32.6%) 66 (24.4%) 36 (13.3%) 23 (8.5%) 1,837 292 (15.6%) 276 (15.0%) 273 (14.9%) 200 (10.9%) 332 (18.1%) 194 (10.4%) 270 (14.7%) Average Question Word Count Average Explanation Word Count 17.4 31.3 The development and release of the Video-Holmes benchmark have the potential to impact the field of complex video reasoning by providing rigorous and comprehensive evaluation benchmark. However, it is important to acknowledge the potential ethical considerations associated with the use of this benchmark. The video content used in Video-Holmes, derived from suspense short films, may contain elements of horror or thriller genres, which could be distressing or inappropriate for certain audiences. Researchers and developers utilizing this benchmark should be mindful of the nature of the content and ensure that it is used responsibly, with appropriate content warnings and considerations for the intended audience. 13 Figure 5: Example of human annotation. 14 Figure 6: Example of question, model answers and reasoning process analysis. 15 Figure 7: Example of question, model answers and reasoning process analysis. 16 Figure 8: Example of question, model answers and reasoning process analysis. 17 Figure 9: Example of question, model answers and reasoning process analysis. 18 Figure 10: Example of question, model answers and reasoning process analysis. 19 Figure 11: Example of question, model answers and reasoning process analysis. 20 Figure 12: Example of question, model answers and reasoning process analysis."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [2] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3 [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3, 4, 7, 9 [4] OpenAI. Introducing openai o1. 2024. 2, 3 [5] OpenAI. Openai o3. 2025. 2, 9 [6] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 2, 3, [7] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 2, 3, 6 [8] Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, and Xihui Liu. Exploring the effect of reinforcement learning on video understanding: Insights from seed-bench-r1. arXiv preprint arXiv:2503.24376, 2025. 2, 3, 6 [9] Google. Gemini-2.0-flash-thinking, 2024. 2, 6 [10] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 2, 3 [11] Xuehai He, Weixi Feng, Kaizhi Zheng, Yujie Lu, Wanrong Zhu, Jiachen Li, Yue Fan, Jianfeng Wang, Linjie Li, Zhengyuan Yang, et al. Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos. arXiv preprint arXiv:2406.08407, 2024. 2, 3 [12] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, Chuhan Li, Junyang Song, et al. Mmvu: Measuring expert-level multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. 2, [13] Yukun Qi, Yiming Zhao, Yu Zeng, Xikun Bao, Wenxuan Huang, Lin Chen, Zehui Chen, Jie Zhao, Zhongang Qi, and Feng Zhao. Vcr-bench: comprehensive evaluation framework for video chain-of-thought reasoning. arXiv preprint arXiv:2504.07956, 2025. 2, 3 [14] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 2, 3 [15] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. 2, 3 [16] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, arXiv preprint and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv:2403.00476, 2024. 2, 3 [17] Zixu Cheng, Jian Hu, Ziquan Liu, Chenyang Si, Wei Li, and Shaogang Gong. V-star: Benchmarking video-llms on video spatio-temporal reasoning. arXiv preprint arXiv:2503.11495, 2025. 22 [18] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 3 [19] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 3 [20] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. 3 [21] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. 3 [22] Qingxing Cao, Junhao Cheng, Xiaodan Liang, and Liang Lin. Visdiahalbench: visual dialogue benchmark for diagnosing hallucination in large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1216112176, 2024. 3 [23] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 3 [24] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 3 [25] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. 3 [26] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [27] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 3, 5 [28] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 3 [29] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [30] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 3, 5 [31] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [32] Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Improved visual-spatial reasoning via r1-zero-like training. arXiv preprint Zhijie Deng. arXiv:2504.00883, 2025. 3 [33] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. 3 23 [34] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 3 [35] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 3 [36] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 3 [37] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641, 2025. 3 [38] Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025. 3 [39] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, [40] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In ACM Multimedia, 2017. 3 [41] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 91279134, 2019. 3 [42] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97779786, 2021. 3 [43] Cheng Xu, Xiaofeng Hou, Jiacheng Liu, Chao Li, Tianhao Huang, Xiaozhi Zhu, Mo Niu, Lingyu Sun, Peng Tang, Tongqiao Xu, et al. Mmbench: Benchmarking end-to-end multi-modal dnns and understanding their hardware-software implications. In 2023 IEEE International Symposium on Workload Characterization (IISWC), pages 154166. IEEE, 2023. 3 [44] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for longcontext interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 3 [45] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [46] Sundar Pichai, Hassabis, and Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era, 2024. 6 [47] Google. Gemini-2.0-pro, 2025. 6 [48] Google. Gemini-2.5-pro, 2025. 6 [49] OpenAI. Hello gpt-4o, 2024. 6 [50] OpenAI. o4-mini, 2025. [51] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "City University of Hong Kong"
    ]
}