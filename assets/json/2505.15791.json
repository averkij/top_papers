{
    "paper_title": "VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL",
    "authors": [
        "Fengyuan Dai",
        "Zifeng Zhuang",
        "Yufei Huang",
        "Siteng Huang",
        "Bangyan Liao",
        "Donglin Wang",
        "Fajie Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers a promising solution,current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): a novel approach that first learns a value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 1 9 7 5 1 . 5 0 5 2 : r VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL Fengyuan Dai*1,2 Zifeng Zhuang*2 Yufei Huang2 Siteng Huang1 Bangyan Liao2 Donglin Wang2 Fajie Yuan2 1Zhejiang University, 2Westlake University"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as powerful generative tools across various domains, yet tailoring pre-trained models to exhibit specific desirable properties remains challenging. While reinforcement learning (RL) offers promising solution, current methods struggle to simultaneously achieve stable, efficient fine-tuning and support non-differentiable rewards. Furthermore, their reliance on sparse rewards provides inadequate supervision during intermediate steps, often resulting in suboptimal generation quality. To address these limitations, dense and differentiable signals are required throughout the diffusion process. Hence, we propose VAlue-based Reinforced Diffusion (VARD): novel approach that first learns value function predicting expection of rewards from intermediate states, and subsequently uses this value function with KL regularization to provide dense supervision throughout the generation process. Our method maintains proximity to the pretrained model while enabling effective and stable training via backpropagation. Experimental results demonstrate that our approach facilitates better trajectory guidance, improves training efficiency and extends the applicability of RL to diffusion models optimized for complex, non-differentiable reward functions."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [Song et al., 2020a,b, Sohl-Dickstein et al., 2015] are generative models that synthesize data through iterative refinement of random noise via learned denoising steps. This framework has demonstrated broad applicability across multiple modalities, including image generation [Rombach et al., 2022], text synthesis [Gong et al., 2022], and protein design [Ahern et al., 2025, Wang et al., 2024]. However, pre-trained diffusion models trained on large-scale datasets may lack specific desired properties. These limitations motivate optimizing target properties scored by reward models, paradigm naturally suited to reinforcement learning (RL) frameworks. While recent studies [Miao et al., 2024, Uehara et al., 2024c, Zhang et al., 2024a, Lee et al., 2023, 2024] have made significant advancements in integrating RL with diffusion model training, existing approaches confront training strategy dilemma: non-differentiable reward vs. stable and efficient training. Specifically, policy gradient approaches [Black et al., 2023, Fan et al., 2023] presuppose nondifferentiable reward functions, inherently suffering from sample inefficiency and training instability due to high-variance gradient estimations [Liu et al., 2025, Deng et al., 2024, Jia et al., 2025]. Conversely, while reward backpropagation methods [Clark et al., 2023] typically achieve more stable and efficient optimization, they are fundamentally constrained to differentiable reward scenarios, thus limiting their use in non-differentiable reward optimization tasks. Equal contribution; Corresponding author. Preprint. Under review. Figure 1: Illustration of the proposed VARD. Different colors represent distinct diffusion trajectories, indexed by the superscripts on states (xi t) and final rewards (Ri). At intermediate steps, VARDs learned PRM calculates the value as the expected final reward, estimated by averaging Ri of relevant trajectories considered to pass through the current state representation. For instance, as depicted for the intermediate state representation x1 , thereby aggregating information from trajectories 0, 2, and 3. This predicted value serves as dense, global supervisory signal, utilized alongside KL regularization for backpropagation throughout the diffusion process. t1 , the value is computed as R3+R2+R 3 Furthermore, previous methods intuitively formulate these fine-tuning processes as RL problems with sparse rewards, i.e. allocating meaningful rewards exclusively to the final denoising step while leaving intermediate states entirely unrewarded. This formulation overemphasizes the final output, resulting in inadequate supervision over the diffusion process itself. Lacking intermediate guidance, the model is compelled to generalize these coarse-grained reward signals across the entire diffusion process. Such generalization readily leads to error accumulation and hinders convergence speed, ultimately yielding suboptimal performance. Inspired by the field of large language models, where process reward models (PRMs) predict intermediate rewards [Lightman et al., 2023, Luo et al., 2024], we explore similar concept for diffusion models: PRM designed to generate dense, differentiable feedback [Zhang et al., 2024b,c] across the entire diffusion trajectory, thereby overcoming the two aforementioned issues. We observe that obtaining such PRM may be particularly suitable for diffusion models because, unlike left-toright natural language generation where semantics can shift significantly mid-sequence, diffusion trajectories generally maintain greater consistency throughout the process. This observation motivates us to train PRM, analogous to value function from an RL perspective, to predict the expectation of the final reward given the current state within the diffusion trajectory. Motivated by the analysis above, we propose our method, VAlue-based Reinforced Diffusion (VARD), as shown in Figure 1. For each intermediate state xt, the learned PRM offers global signal by predicting the average final reward expected from all complete diffusion trajectories that traverse through xt. Crucially, this prediction also serves as look-ahead information, guiding the diffusion process on how to proceed towards high-reward final states. Using this dense supervision, VARD effectively and stably optimizes the diffusion model via backpropagation, complemented by KL regularization term that prevents significant divergence from the pre-trained models distribution. Experimental validation of VARD on generative models for both protein structure and text-to-image synthesis reveals several key advantages. Firstly, by providing dense supervision across the entire diffusion trajectory, VARD achieves faster convergence and improved sample efficiency. Furthermore, its trajectory-wise reward alignment, stabilized by KL regularizationin contrast to approaches that only consider final-state rewardsresults in enhanced data quality. Finally, VARD also extends to non-differentiable rewards, as its integrated PRM decouples training from needing an externally differentiable reward function. In summary, our contributions are as follows: 1. We address the challenges of sparse reward supervision and the dilemma between stable optimization and non-differentiable reward functions by using learned PRM. This PRM provides dense, differentiable reward signals throughout the entire diffusion process. 2 2. We introduce VARD, novel framework for fine-tuning diffusion models that integrates value as trajectory-wise supervision with KL regularization, thereby enhancing reward alignment while ensuring proximity to the pre-trained model. 3. Extensive experiments demonstrate VARDs significant advantages: enhanced sample quality with improved efficiency, effective mitigation of reward hacking, and broad applicability, especially for scenarios with non-differentiable reward functions."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 Diffusion Models We use the denoising diffusion probabilistic models (DDPMs) [Ho et al., 2020] and flow matching [Lipman et al., 2022] for data generation. Here we take text-to-image generation for example; for specifics on protein backbone generation, please refer to Appendix A.1. Let q0 be the image data distribution x0 q0 (x0) , x0 Rn. DDPM aims to approximates q0 with parameterized model (cid:105) pθ (x0) = (cid:82) (cid:104) dx1:T . The reverse process, also called denoising, is t=1 pθ (xt1 xt) Markov chain with the following initial distribution and dynamics: pT (xT ) (cid:81)T (xT ) = (0, I), pθ (xt1 xt) = (µθ (xt, t) , Σt) . (1) The forward or diffusion process of DDPMs utilizes an approximate posterior (x1:T x0), which can be modeled as Markov chain that incrementally adds Gaussian noise to the data according to predifined variance schedule β1, . . . , βT : (x1:T x0) = (cid:89) t=1 (xt xt1) , (xt xt1) = (cid:16)(cid:112)1 βtxt1, βtI (cid:17) . (2) Let αt = 1 βt, αt = (cid:81)t xt βt µθ (xt, t) = 1 αt (cid:16) 1 αt s=1 αs, and βt = 1 αt1 1 αt ϵθ (xt, t) (cid:17) to represent the mean in Equation (1). βt. DDPMs adopt the parameterization Training DDPM is performed by optimizing variational bound on the negative log-likelihood Eq [ log pθ (x0)], which is equivalent to optimizing Eq (cid:34) (cid:88) t=1 KL (q (xt1 xt, x0) pθ (xt1 xt)) . (3) (cid:35) t=1 (0, 1)T is chosen so that αT 0 is made to make is Note that the variance sequence (βt)T (xT x0) (0, I). The covariance matrix Σt in Equation 1 is often set to σ2 either βt or βt, which is not trainable. Furthermore, diffusion models also excel at conditional data generation such as text-to-image generation with classifier-free guidance. Given context prompt (c), the image distribution becomes q0 (x0, c). During training, the forward noising process is used regardless of input c. Both unconditional ϵθ (xt, t) and conditional ϵθ (xt, t, c) are learned together. At test time, given text prompt c, the model generates conditional data according to pθ (x0c). I, where σ2 2.2 Markov Decision Processes and Offline Reinforcement Learning Reinforcement learning (RL) [Sutton et al., 1998] is sequential decision making problem formulated by Markov Decision Processes (MDP). MDP is usually defined by the tuple = {S, A, ρ0, P, R} in which is the state space, is the action space, ρ0 is the initial distribution, is the transition dynamics and is the reward function. The policy π interacts with the environment to generate one trajectory τ = (s0, a0, s1, a1, , sT , aT ) and its distribution is defined as Pπ (τ ). Then the objective of RL is to derive one policy that can maximize the expectation under the trajectory (cid:105) distribution: π = arg maxπ Eτ Pπ(τ ) t=0 (st, at) (cid:104)(cid:80)T . Offline RL [Levine et al., 2020, Fujimoto and Gu, 2021, Zhuang et al., 2023] forbids the interaction with the environment M. And only the offline dataset = {st, at, st+1, rt} generated by the 3 behavior policy πβ is provided. Beyond return maximization, offline RL imposes an additional constraint that the learned policy must not deviate significantly from the behavior policy πβ: π = arg max π EstD,atπ(st) (cid:34) (cid:88) t= (cid:35) (st, at) DKL (ππβ) . (4)"
        },
        {
            "title": "3 Method",
            "content": "In this section, we first formulate the problem of reinforcement learning fine-tuning of diffusion models and analyze its essence to reveal the inherent challenges during RL fine-tuning. Furthermore, we propose minimalist approach for RL fine-tuning of diffusion called Value-based Reinforced Diffusion (VARD) and highlight the benefits of introduction of value function. Finally, we present the details of the implementation and the overall algorithm. 3.1 Reinforcement Learning Fine-tuning of Diffusion Models The generation process of conditional diffusion models, also the denoising process, can be modeled as Markov Decision Process (MDP) with the following specifications: = {S, A, ρ0, P, R} State space: = {stst = [xT t, c]} ; Action space: = {atat = xT t1} ; Initial distribution: ρ0 = [N (0, I) , (c)] ; Transition dynamics: (st+1st, at) = (cid:2)δc, δxT (cid:3) ; Reward function: (st, at) = ([xT t, c] , xT t1) . Here δc, δxT t1 are the Dirac distributions with non-zero density only at and xT t1. Under this definition of MDP, the policy is πθ (atst) = pθ (xT t1xT t, c). The goal of RL Fine-tuning is to maximize the expected cumulative rewards while close to the pretrained diffusion model: (θ) = Ep(c)Epθ(xT t,c) (cid:34) (cid:88) t=0 ([xT t, c] , xT t1) + DKL (pθpθ0) , (5) (cid:35) where pθ0 represents the pretrained diffusion model. To minimize this objective, two issues related to the reward function need to be considered: 1) Non-differentiable Rewards: Sometimes, reward functions are non-differentiable, which precludes the direct optimization. One classical approach to address this issue is Policy Gradient (PG) [Sutton et al., 1999], which transforms return maximization into return-weighted maximum likelihood estimation. While this approach is general, it introduces relative complexity and cannot directly provide reward signals to the target diffusion model for optimization. In contrast, when the reward function is differentiable, return maximization can be achieved through direct optimization-analogous to Deterministic Policy Gradient (DPG) [Silver et al., 2014] methods in RL. 2) Sparse Rewards: Strictly speaking, during the generation process, only the final step produces the actual samples, while all intermediate steps consist of mixture of noise and the target samples. This means that the reward can only be obtained at the very last step: Sparse Reward function: (st, at) = ([xT t, c] , xT t1) = 1 (t = 1) (x0, c) . (6) From reinforcement learning (RL) perspective, this presents sparse reward problem, which is notoriously difficult to solve due to credit assignment difficulty [Seo et al., 2019] and insufficient learning signal [Hare, 2019]. Additionally, drawing from the experience of RL fine-tuning for large language models (LLMs), the existing reward function resembles classic Output Reward Model (ORM). straightforward solution to overcome the sparse reward challenge involves introducing Process Reward Model (PRM)[Luo et al., 2024]. Algorithm 1 Value Pretraining 1: Require: Pretrained diffusion model pθ0 , randomly initialized value function Vϕ , x0 2: for (c) do for (cid:0)x0 (cid:1) pθ0 do 1, , x0 3: 0 Calculate the sparse reward (cid:0)x0 4: Update value function Vϕ using Equation (8) 5: end for 6: 7: end for 0, c(cid:1) Algorithm 2 Diffusion Fine-Tuning 1: Require: Pretrained value function Vϕ0 , pretrained and finetuned diffusion model pθ0 , pθ 2: for (c) do for [0, ] do 3: (cid:0)x, x0(cid:1) 4: Update pθ using Equation (9), and finetune Vϕ0 5: end for 6: 7: end for (pθ, pθ0 ) (xT t+1, c) 3.2 Our Minimalist Approach To address this sparse reward challenge, common approach in RL involves computing value functions to redistribute reward signals across intermediate steps that lack explicit feedback. This aligns with the Process Reward Model (PRM) paradigm in RL Fine-tuning for LLM, which emphasizes not just the final output but also the entire generation process. In RL Fine-tuning for diffusion models, we define this value function or PRM as follows: Vϕ (xT t, c) := Et[0,T ] (cid:34) (cid:88) t=t ([xT t, c] , xT t1) xT . (7) (cid:35) This value function represents the expected reward of images x0 generated by pθ given the current sample xT t. For xT t, this value function takes into account the rewards of all possible images that may be generated subsequently, such as Figure 1, effectively avoiding the interference of extreme rewards. Guided by the value function, the diffusion model can directly navigate to highvalue regions (also high reward) during the generation process, efficiently avoids unnecessary and potentially unproductive exploration. An unbiased value function estimation method based on Monte Carlo approaches operates as follows: (ϕ) = Ep(c)Et[0,T ] [r (x0, c) Vϕ (xT t, c)]2 . This value estimation technique leverages the neural networks inherent generalization capability to distribute the final steps reward signal across intermediate steps, effectively circumventing interference from artificial priors. This value function not only provides additional guidance for the generation process but is also differentiable, enabling direct optimization of pθ using Deterministic Policy Gradient (DPG). Given the definition of value (7), the problem definition (5) can be formulated as the following VARD objective: (8) (θ) = Ep(c)E (x,x0)T t(pθ,pθ0 )(xT t+1,c) (cid:104) Vϕ (xT t, c) + η (cid:0)xT x0 (cid:1)2(cid:105) , (9) where (cid:0)x, x0(cid:1) (pθ, pθ0 ) (xT t+1, c) is an abbreviation of xT pθ (xT t+1, c) , x0 pθ0 (xT t+1, c). Meanwhile, more straightforward implementation of KL divergence has been adopted. The KL divergence term, weighted by η, serves to prevent model collapse during RL optimization, which can be achieved by minimizing the gap between the output of the RL-optimized diffusion model pθ and the output of pre-trained pθ0 at each timestep. This can also be supported by: Lemma 1 Minimizing the gap between xT and x0 is equivalent to minimizing the KL. θEp(c)E (x,x0)T t(pθ,pθ0 )(xT t+1,c) (cid:104)(cid:0)xT x0 (cid:1)2(cid:105) = 2σ2 θDKL (pθpθ0 ) . (10) 3. Implementation Details The training process of VARD (Value-based Reinforced Diffusion) consists of two stages: pretraining of the value function Algorithm 1 and fine-tuning of the diffusion model Algorithm 2: Initially, the entire trajectory is sampled according to the pretrained diffusion model pθ0, and the corresponding value function Vϕ0 of pθ0 is trained using Equation (8). Once the value function has converged, the fine-tuning stage of the diffusion model commences. 5 Figure 2: Qualitative analysis of generated protein structure from FoldFlow-2 base and VARD version. Alpha-helices, beta-sheets and coils are colored yellow, green and gray, respectively. The VARD version produces structures with significantly more beta-sheet regions, whereas the FoldFlow2 base model generates almost exclusively alpha-helix regions. During this stage, the diffusion model generates new sample xT at each step, and the value function provides feedback signals to optimize the diffusion model following Equation (9). Notably, after the diffusion model updates, the prior value function can no longer accurately predict values for samples from the updated model. Thus, following such an update, the value function should also be fine-tuned accordingly, with minimal additional computational overhead."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we aim to comprehensively evaluate the effectiveness of VARD. First, we verify VARDs capability in handling non-differentiable rewards, specifically on protein structure design and image compressibility control (Section 4.1). Then, we evaluate VARDs performance on differentiable reward designed for human preference alignment, including Aesthetic Score, PickScore, and ImageReward (Section 4.2). In Section 4.3, we delve into VARDs generalization ability, focusing specifically on its performance with unseen prompts, across different reward models, and in maintaining fidelity to the prior distribution. Additional experiments are presented in Appendix A.5. 4.1 Non-differentiable rewards Protein secondary structure diversity. Current AI-generated protein models [Watson et al., 2023] demonstrate systematic deviations in secondary structure distributions relative to native counterparts, with pronounced overrepresentation of helices [Lin et al., 2024, Bose et al., 2023, Huguet et al., 2024]. Motivated by this issue, we aim to finetune the pretrained SE(3) diffusion framework [Yim et al., 2023] to recalibrate conformational sampling. Following FoldFlow2 [Huguet et al., 2024], we build reward model defined as: (cid:88) = ( pdwd)(1 + dD (cid:88) dD pdlogpd), (11) where = (a, b, c) represents secondary structure elements (alpha-helices, beta-sheets and coils). Here, we set wa = 1, wb = 5, wc = 0.5. We fine-tune the FoldFlow2-base model using our proposed VARD approach, resulting in FoldFlow-2-VARD. Figure 3: Secondary structure distribution comparison between FoldFlow-2-base and FoldFlow-2VARD generated proteins. Proteins are sampled at 50-residue increments between lengths 100-300, with 64 samples per length. Figure 3 reveals distinct differences in secondary structure distribution: FoldFlow-2-base demonstrates constrained beta-sheet formation, while our VARD-enhanced counterpart exhibits expanded structural diversity. This conclusion is qualitatively validated in Figure 2, where comparative analyses demonstrate the FoldFlow-2 model benefits from VARD fine-tuning and produces more beta-sheet. Compressibility and Incompressibility. Following DDPO [Black et al., 2023], we generate images with controlled JPEG compressibility. We formulate the reward scheme such that compressibility optimization minimizes the predicted size while incompressibility encourages larger file sizes. The 6 Figure 4: Qualitative analysis of compressibility vs. incompressibility rewards. Generated images correspond to checkpoints saved at 20-step intervals during training. Figure 5: Comparative visualization. Compared to ReFL and DRaFT-1 aligned with specified reward preferences, VARD achieves demonstrably superior image quality and enhanced realism. experiments are conducted on the 45 animal prompts. As shown in Figure 4, when optimizing for compressibility, generated images exhibit progressive blurring and reduced diversity due to attenuated high-frequency regions. Conversely, maximizing compression difficulty yields images with enhanced textural complexity and photorealistic details, demonstrating VARDs competence in non-differentiable reward scenarios, where it achieves frequency-specific preference generation. 4.2 Differentiable rewards Experimental setup. We examine three reward functions grounded in human preference learning: Aesthetic Score [Schuhmann, 2022], PickScore [Kirstain et al., 2023], and ImageReward [Xu et al., 2023]. Following DDPO [Black et al., 2023], we evaluate Aesthetic Score using their standardized 7 Figure 6: Reward v.s. sampled images. Both VARD w/o KL and VARD attain higher reward values than baselines with substantially fewer training samples. set of 45 animal prompts. For PickScore and ImageReward, we directly adopt their native training prompts (Pick-a-Pic and ImageRewardDB respectively) to preserve data distribution alignment. Baselines. We compare with two recent RL fine-tuning methods using direct reward backpropagation: ReFL [Xu et al., 2023] and DRaFT-1 [Clark et al., 2023]. ReFL randomly samples the last 10 diffusion timesteps for gradients, while DRaFT applies stop-gradient to early timesteps and propagates through the final step. Notably, while DRaFT-1s original implementation integrates LoRA [Hu et al., 2022] as an intrinsic component of their parameter-efficient framework, we intentionally adopt full model fine-tuning to isolate the effects of gradient propagation strategies. Results. Figure 6 traces the relationship between training sample size and reward improvement, where VARD w/o KL and VARD exhibit steeper growth trajectories than baselines. The convergence curves confirm their superior sample efficiency across three reward functions. Complementary qualitative evidence is provided in Figure 5. While all methods demonstrate improved sampled image quality, VARD achieves an optimal balance between reward maximization and distributional integrity through parameter-space anchoring near the pretrained model. This strategic optimization results in significantly enhanced resistance to reward hacking compared to baselines and VARD w/o KL. 4.3 Learning beyond the reward As noted in Gao et al. [2023], Liu et al. [2025], since reward functions serve as imperfect scorers, excessive focus on maximizing reward values through overoptimization can paradoxically degrade models true performance. As shown in Figure 7, previous reward backpropagation approaches frequently overlook this critical limitation, often leading to model collapse into homogeneous and low-fidelity image that does not align with human expectations (i.e., reward hacking). Figure 7: example of reward hacking. Directly backpropagation without KL catastrophically loses ability to generate promptaligned images, degenerating into texture-like noise patterns. All of the images is generated from checkpoints optimized with 500 steps. We investigate whether our method can effectively learn beyond the reward, ensuring that the model not only maximizes the reward function, but also captures generalizable and meaningful patterns that align with human expectations. Specifically, we employ three key evaluation settings: unseen-prompt generalization, cross-reward validation, and prior consistency-reward balance. Unseen-prompt generalization. Following the DRaFT [Clark et al., 2023], we first fine-tuned the model using the HPDv2 [Wu et al., 2023] training set with HPSv2 serving as the reward model. Subsequently, we evaluated the fine-tuned models performance across all four test sets from HPDv2, using the HPSv2 score as the evaluation metric. The evaluation results are presented in Table 1. Performance metrics for the baseline models are sourced from the DRaFT and PRDP [Deng et al., 2024] paper. DDPO fails to effectively optimize the reward function, whereas PRDP achieves only moderate reward values. Both methods underperform 8 Figure 8: Evaluated by HPSv2. VARD consistently attains the highest HPSv2 scores while simultaneously preserving modest reward. Figure 9: Prior maintenance. The image generated by VARD remains the closest to the base model as the reward increases. standard reward backpropagation approaches. VARD w/o KL demonstrates superior unseen-prompt generalization by outperforming ReFL and DRaFT across all four test sets. Furthermore, VARD achieves the best performance, indicating that incorporation of KL regularization enhances reward learning for unseen prompts. Cross-reward validation. Next, we assess the performance of the learned model using other reward models as benchmarks. We used HPSv2 as the evaluation metric, as the Aesthetic Score, ImageReward, and PickScore are all grounded in human preference learning. We save model checkpoints every 50 steps and generate images for evaluation. The results of this comprehensive evaluation are illustrated in Figure 8. Methods lacking KL regularization are prone to overfitting to high-reward signals, often resulting in significantly lower HPSv2 scores. This phenomenon is particularly pronounced with Aesthetic Score, as it does not account for textual alignment and the datasets we trained on, i.e. the animals, are relatively small, making it susceptible to model forgetting. In contrast, VARD consistently achieves the highest HPSv2 scores while maintaining balanced reward level, demonstrating superior robustness and generalization compared to previous approaches. This highlights the effectiveness of our approach in mitigating overfitting and preserving alignment with human preferences. Prior consistency-reward balance. We probe into the prior-preserving capabilities of the fine-tuned model. Specifically, we calculate the per-prompt FID score by comparing images generated from the pre-trained model with those from the fine-tuned model, and then average the FID scores across all prompts. higher FID score indicates significant deviation from the pre-trained model. As illustrated in Figure 9, models without KL tend to produce low-fidelity images as the reward increases. Table 1: Cross prompt generalization. The highest scores are emphasized in boldface, with secondary best results indicated by single underlining. Model Animation Concept Art Painting Photo Averaged Stable Diffusion (SD 1.4) DDPO PRDP ReFL DRaFT-1 DRaFT-LV VARD w/o KL VARD 0. 0.2673 0.3223 0.3446 0.3494 0.3551 0.3603 0.3625 0.2661 0.2558 0.3175 0.3442 0.3488 0.3552 0.2666 0. 0.2695 0.2570 0.3172 0.3447 0.3485 0.3560 0.2093 0.3159 0.3374 0.3400 0.3480 0.2474 0.3182 0.3427 0.3467 0.3536 0.3588 0.3597 0.3589 0. 0.3523 0.3537 0.3575 0.3589 9 In contrast, VARD achieves substantial reward enhancement while maintaining low FID score, thereby effectively balancing reward optimization with prior preservation."
        },
        {
            "title": "5 Related Work",
            "content": "RLHF and reward modeling in LLMs. Reinforcement learning from human feedback (RLHF) [Hou et al., 2024, Nika et al., 2024] has become cornerstone in aligning LLMs with human preferences and improved reasoning, the reward model being key component. For fine-grained supervision, especially in complex reasoning where the generation process matters, process reward models (PRMs) evaluate intermediate steps to guide the models internal pathways [Lightman et al., 2023, Luo et al., 2024, Wang et al., 2023]. This contrasts with outcome reward models (ORMs) [Lyu et al., 2025], which assess the final generated output. Due to their relative simplicity in training, ORMs are widely adopted and become mainstream technique for improving reasoning ability [Guo et al., 2025]. RL funetuning for diffusion models. Training diffusion models with RL is an increasingly prominent approach in image generation [Domingo-Enrich et al., 2024] and AI for science field [Uehara et al., 2025, 2024a,b, Li et al., 2024, Uehara et al., 2024c]. Some methods utilize differentiable rewards, allowing for direct gradient backpropagation through parts of the diffusion sampling process for endto-end optimization [Xing et al., 2025, Zhao et al., 2024, Eyring et al., 2024]. Another thrust adapts from RLHF in LLMs, such as RL algorithms like PPO [Schulman et al., 2017] or DPO [Rafailov et al., 2023], often to effectively incorporate human feedback or navigate non-differentiable objectives [Lee et al., 2024, Zhang and Xu, 2023, Xu et al., 2024, Wallace et al., 2024]."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce VAlue-based Reinforced Diffusion (VARD), novel approach for efficiently optimizing diffusion models with reward preference. By training value function (also PRM) to provide dense supervision signals throughout the diffusion trajectory, VARD effectively overcomes the limitations of sparse rewards and the dilemma between differentiable rewards and stable training. Our approach is validated on non-differentiable and differentiable rewards for human preference learning, as well as by exploring its generalization beyond simple reward maximization."
        },
        {
            "title": "7 Limitation",
            "content": "The primary limitation of VARD stems from its reliance on the learned value function, whose accuracy critically influences the entire fine-tuning process. This presents practical challenges. First, achieving sufficient accuracy for the value function sometimes necessitate pre-training phase before it can effectively guide the diffusion models training. Second, concurrently training the network of the value function alongside the diffusion model, while enabling online adaptation of the value estimates, inevitably introduces additional GPU memory requirements. Nevertheless, we suggest these issues can be substantially mitigated by selecting appropriate value function architectures, which we will elaborate on later. Looking ahead, future work will involve investigating and validating VARDs effectiveness when applied to more advanced and large-scale diffusion model architectures."
        },
        {
            "title": "8 Social impact",
            "content": "Our research introduces VARD, novel framework for aligning diffusion models with specific objectives using RL. The current study focuses primarily on algorithmic development and evaluation within controlled experimental settings, and does not involve direct real-world deployment or application studies. As result, there are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Woody Ahern, Jason Yim, Doug Tischer, Saman Salike, Seth Woodbury, Donghyo Kim, Indrek Kalvet, Yakov Kipnis, Brian Coventry, Han Altae-Tran, et al. Atom level enzyme active site scaffolding using rfdiffusion2. bioRxiv, pages 202504, 2025. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow matching for protein backbone generation. arXiv preprint arXiv:2310.02391, 2023. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. ICLR, 2023. Fei Deng, Qifei Wang, Wei Wei, Tingbo Hou, and Matthias Grundmann. Prdp: Proximal reward difference prediction for large-scale reward finetuning of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 74237433, June 2024. Carles Domingo-Enrich, Michal Drozdzal, Brian Karrer, and Ricky TQ Chen. Adjoint matching: Fine-tuning flow and diffusion generative models with memoryless stochastic optimal control. arXiv preprint arXiv:2409.08861, 2024. Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. Advances in Neural Information Processing Systems, 37:125487125519, 2024. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Naomi Fox, Steven Brenner, and John-Marc Chandonia. Scope: Structural classification of proteinsextended, integrating scop and astral data and classification of new structures. Nucleic acids research, 42(D1):D304D309, 2014. Scott Fujimoto and Shixiang Shane Gu. minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:2013220145, 2021. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Joshua Hare. Dealing with sparse rewards in reinforcement learning. arXiv preprint arXiv:1910.09281, 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao Dong. Does rlhf scale? exploring the impacts from data, model, and method. arXiv preprint arXiv:2412.06000, 2024. 11 Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. Learning inverse folding from millions of predicted structures. In International conference on machine learning, pages 89468970. PMLR, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Guillaume Huguet, James Vuckovic, Kilian Fatras, Eric Thibodeau-Laufer, Pablo Lemos, Riashat Islam, Cheng-Hao Liu, Jarrid Rector-Brooks, Tara Akhound-Sadegh, Michael Bronstein, et al. Sequence-augmented se (3)-flow matching for conditional protein backbone generation. arXiv preprint arXiv:2405.20313, 2024. Zhiwei Jia, Yuesong Nan, Huixi Zhao, and Gengdai Liu. Reward fine-tuning two-step diffusion models via learning differentiable latent-space surrogate reward, 2025. URL https://arxiv. org/abs/2411.15247. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et al. Parrot: Pareto-optimal multi-reward reinforcement learning framework for text-to-image generation. In European Conference on Computer Vision, pages 462478. Springer, 2024. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. Xiner Li, Yulai Zhao, Chenyu Wang, Gabriele Scalia, Gokcen Eraslan, Surag Nair, Tommaso Biancalani, Shuiwang Ji, Aviv Regev, Sergey Levine, et al. Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding. arXiv preprint arXiv:2408.08252, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversitypreserving diffusion alignment via gradient-informed GFlownets. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= Aye5wL6TCn. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, et al. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781, 2025. Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1084410853, 2024. Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios Tzannetos, Goran Radanovic, and Adish Singla. Reward model learning vs. direct policy optimization: comparative analysis of learning from human preferences. arXiv preprint arXiv:2403.01857, 2024. Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Chrisoph Schuhmann. Laion aesthetics, Aug 2022. URL https://laion.ai/blog/ laion-aesthetics/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Minah Seo, Luiz Felipe Vecchietti, Sangkeum Lee, and Dongsoo Har. Rewards prediction-based credit assignment for reinforcement learning with sparse binary rewards. IEEE Access, 7:118776118791, 2019. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning, pages 387395. Pmlr, 2014. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Richard Sutton, Andrew Barto, et al. Introduction to reinforcement learning. 1998. Richard Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194, 2024a. 13 Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Sergey Levine, and Tommaso Biancalani. Feedback efficient online fine-tuning of diffusion models. arXiv preprint arXiv:2402.16359, 2024b. Masatoshi Uehara, Yulai Zhao, Ehsan Hajiramezanali, Gabriele Scalia, Gokcen Eraslan, Avantika Lal, Sergey Levine, and Tommaso Biancalani. Bridging model-based optimization and generative modeling via conservative fine-tuning of diffusion models. Advances in Neural Information Processing Systems, 37:127511127535, 2024c. Masatoshi Uehara, Xingyu Su, Yulai Zhao, Xiner Li, Aviv Regev, Shuiwang Ji, Sergey Levine, and Tommaso Biancalani. Reward-guided iterative refinement in diffusion models at test-time with applications to protein and dna design. arXiv preprint arXiv:2502.14944, 2025. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567, 2024. Joseph Watson, David Juergens, Nathaniel Bennett, Brian Trippe, Jason Yim, Helen Eisenach, Woody Ahern, Andrew Borst, Robert Ragotte, Lukas Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. Xiaoying Xing, Avinab Saha, Junfeng He, Susan Hao, Paul Vicol, Moonkyung Ryu, Gang Li, Sahil Singla, Sarah Young, Yinxiao Li, et al. Focus-n-fix: Region-aware fine-tuning for text-to-image generation. arXiv preprint arXiv:2501.06481, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. Jason Yim, Brian Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. arXiv preprint arXiv:2302.02277, 2023. Hengtong Zhang and Tingyang Xu. Towards controllable diffusion models via reward-guided exploration. arXiv preprint arXiv:2304.07132, 2023. Yinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk. Large-scale reinforcement learning for diffusion models. In European Conference on Computer Vision, pages 117. Springer, 2024a. Ziyi Zhang, Li Shen, Sen Zhang, Deheng Ye, Yong Luo, Miaojing Shi, Bo Du, and Dacheng Tao. Aligning few-step diffusion models with dense reward difference learning. arXiv preprint arXiv:2411.11727, 2024b. Ziyi Zhang, Sen Zhang, Yibing Zhan, Yong Luo, Yonggang Wen, and Dacheng Tao. Confronting reward overoptimization for diffusion models: perspective of inductive and primacy biases. arXiv preprint arXiv:2402.08552, 2024c. 14 Yulai Zhao, Masatoshi Uehara, Gabriele Scalia, Sunyuan Kung, Tommaso Biancalani, Sergey Levine, and Ehsan Hajiramezanali. Adding conditional control to diffusion models with reinforcement learning. arXiv preprint arXiv:2406.12120, 2024. Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang Guo. Behavior proximal policy optimization. arXiv preprint arXiv:2302.11312, 2023. 15 Appendix / supplemental material A.1 Preliminaries on protein backbone generation A.1.1 Protein backbone parameterization The parameterization of protein backbone for generative modeling commonly follows principles similar to those used in AlphaFold2 [Jumper et al., 2021]. protein is sequence of amino acid residues, and the backbone of each residue is primarily defined by four heavy atoms: the amide nitrogen Nn, the alpha-carbon Cα,n, the carbonyl carbon Cn, and the oxygen On. To describe the coordinates of each heavy atom, each residue is associated with local coordinate frame, represented as rigid transformation Tn SE(3) , where SE(3) is the Special Euclidean group of three-dimensional rotations and translations. For protein with residues, the complete set of backbone frames is collectively denoted as = [T1, ..., TN ] SE(3)N . Once these frames are established, the global Cartesian coordinates of the Nn, Cα,n, and Cn atoms for each residue are determined by applying the respective transformation Tn to set of idealized, canonical local coordinates for these atoms. The position of the oxygen atom On can be finalized using an additional torsion angle that orients it correctly relative to the Cn - Cα,n segment. A.1.2 Flow matching on SE(3) natural approach to define flows on SE(3) for tasks like protein backbone generation is to first decompose SE(3) into its rotational component SO(3) (Special Orthogonal group) and its translational component R3. This decomposition then allows for the construction of independent flows on SO(3) and R3. Given that flow matching in Euclidean spaces R3 is well-studied area [Liu et al., 2022, Lipman et al., 2022], we will primarily focus our discussion here on the formulation of flow matching on the SO(3) manifold. Let ρ0 and ρ1 be two probability density functions on SO(3), representing source distribution (e.g., an easy-to-sample prior) and target data distribution (e.g., observed protein backbone frame orientations), respectively. flow is defined by time-dependent diffeomorphism ψt : SO(3) SO(3) for [0, 1]. This flow ψt is the solution to an ordinary differential equation (ODE): dt ψt(R) = ut(ψt(R)), (12) with the initial condition ψ0(R) = (if flowing from at = 0), where SO(3) and ut : [0, 1] SO(3) SO(3) is time-dependent smooth vector field on SO(3), where SO(3) means the tangent space at SO(3). The flow ψt generates the probability path ρt if it pushes forward the source distribution, e.g., ρt = [ψt]#ρ0. Conditional flow matching (CFM) aims to regress parameterized vector field vθ(t, Rt) to target conditional vector field ut(RtR0, R1) that transports points between R0 ρ0 and R1 ρ1. common strategy is to construct this conditional flow along the geodesic path. For SO(3), the geodesic interpolant Rt from R0 to R1 is Rt = expR0 (R1)). The corresponding target conditional vector field ut(RtR0, R1) is its time derivative. If we define the flow as transforming samples from ρ0 (e.g., noise at = 0) to ρ1 (data at = 1), common expression for the target vector field, representing the velocity at Rt pointing towards R1, is given by [Bose et al., 2023]: (t logR0 ut(RtR0, R1) = (R1) logRt , (13) where logRt components, the flow matching objective for SO(3) can be formulated as: (R1) is an element of the Lie algebra so(3) in the tangent space at Rt. Given these LSO(3)(θ) = EtU (0,1),q(R0,R1),Rtρt(RtR0,R1) vθ(t, Rt) ut(RtR0, R1) SO(3) . (14) Here, is sampled uniformly from U(0, 1), q(R0, R1) is coupling of samples from the source and target distributions, and ρt(RtR0, R1) is the conditional probability of Rt given R0 and R1. For the coupling q(R0, R1), samples R0 ρ0 and R1 ρ1 can be drawn independently from their respective distributions. Alternatively, to achieve straighter interpolation paths and improved training stability, many approaches approximate the optimal transport (OT) coupling π(R0, R1) using minibatch techniques [Pooladian et al., 2023]. 16 A.2 Proof of Lemma 1 Given two diffusion models with Gaussian conditional distributions: pθ(xT txT t+1, c) = (µθ, σ2I), txT t+1, c) = (µθ0 , σ2I). pθ0 (x0 For Gaussian distributions, the gradient of KL divergence simplifies to: θDKL(pθpθ0) = θ 1 2σ2 µθ µθ02. The expectation over sample pairs xT pθ and x0 xT tpθ,x0 tpθ0 (cid:2)(xT x0 pθ0 is: t)2(cid:3) = Eϵθ,ϵθ0 (0,σ2I) (cid:2)(µθ µθ0 + ϵθ ϵθ0)2(cid:3) = µθ µθ02 + 2(µθ µθ0)E [ϵθ ϵθ0 ] + (cid:2)ϵθ2(cid:3) 2E [ϵθϵθ0 ] + (cid:2)ϵθ0 2(cid:3) = µθ µθ02 + 2σ2d, where ϵθ, ϵθ0 (0, σ2I) are independent noise terms and is the state dimension. Substituting the KL expression into the MSE yields the lemma statement: t)2(cid:3) = 2σ2θDKL(pθpθ0 ). θE (cid:2)(xT x0 A.3 Value function architectures (15) (16) (17) (18) (19) The learned value function serves as crucial component in VARD, providing the supervision signal for fine-tuning. We now specify the architectures used to implement this value function in our different experimental settings. Non-differentiable rewards. For experiments on protein secondary structure diversity, we employ the GVP-Transformer encoder [Hsu et al., 2022] to generate translation-invariant and rotation-equivariant representations. This encoder uses an embedding dimension of 512 and 4 layers, followed by 2-layer MLP head that predicts helix and strand percentages. The complete architecture comprises approximately 42M parameters. For experiments on compressibility and incompressibility, we train standard ResNet-18 [He et al., 2016] (11M parameters) with regression head. This model is trained to predict the compressed file size for the data state at each diffusion timestep, serving directly as our differentiable value function. Differentiable rewards. The differentiable rewards utilized in our experiments consist of four models designed for scoring human preferences, namely: Aesthetic Scorer, PickScore, ImageReward, and HPSv2. In our preliminary experiments, we initially employed value function architecture based on the DPOK approach [Fan et al., 2023]; this is typically simple MLP whose primary role is to stabilize the training process. However, we found this architecture to be suboptimal for accurately predicting values from intermediate diffusion states, largely due to the MLPs limited capacity to model the complex and nuanced distribution underlying human preference scores. Consequently, we adopted more effective strategy by directly adapting these differentiable reward models themselves to function as the value function, using Low-Rank Adaptation (LoRA) [Hu et al., 2022]. Specifically, we integrated LoRA with an alpha of 32 and dropout rate of 0.1 into both the self-attention mechanisms and the subsequent feed-forward network layers of the visual transformer encoders within these reward models. This LoRA-based implementation offers distinct advantages. Firstly, its architecture, being derived from the reward models themselves, is inherently more suitable for modeling complex human preferences compared to simple MLP. Secondly, despite the large parameter from the original reward models, LoRA ensures training efficiency for the value function by updating only small fraction of these parameters, drastically reducing the GPU memory during training. Finally, leveraging the pre-trained weights of these reward models provides strong initialization that significantly enhances the value functions learning effectiveness; we found that the value function learns rapidly and performs well with only remarkably small number of sampled trajectories. 17 Table 2: Hyperparameters for non-differentiable rewards. Hyperparameters Protein secondary structure diversity Image compressibility and incompressibility Value Function (PRM) Pretraining"
        },
        {
            "title": "Learning rate\nGlobal batch size\nTrain steps",
            "content": "Diffusion Model Fine-tuning (with VARD) KL regularization weight η Base model learning rate Value function learning rate Gradient accumulate steps Train steps Global batch size 3e-4 128 2k 0.1 5e-5 1e-6 50 30 16 1e-4 32 3k 1 1e-6 1e-5 2 200 Table 3: Hyperparameters for differentiable rewards. Hyperparameters Aesthetic Score PickScore ImageReward Value Function (PRM) Pretraining Learning rate Batch size Train steps 1e-5 32 8 Diffusion Model Fine-tuning (with VARD) KL regularization weight η Base model learning rate Value function learning rate Gradient accumulate steps Train steps Global batch size 100 1e-6 5e-6 2 1k 32 A. Implementation details 1e-5 32 8 0.5 5e-6 5e-6 2 1k 32 1e-5 32 8 20 1e-5 5e-6 2 1k 32 For both protein structure and image generation, we employ 50-step diffusion sampling process, utilizing the official FoldFlow implementation1 for proteins and DDPM sampler [Ho et al., 2020] for images. Notably, our proposed method also supports DDIM [Song et al., 2020a] as an alternative sampler. In the context of protein generation, we observe that the proteins secondary structure is largely determined during the early diffusion stages, while subsequent timesteps primarily involve minor adjustments to atomic coordinates. Consequently, for protein-related objectives, the FoldFlow2 network is trained across all 50 diffusion timesteps. Conversely, for image generation, we find that objectives such as human preferences (e.g., aesthetic scores) and compressibility are more strongly influenced by fine-grained details that are finalized in the later stages of the diffusion process. Therefore, to achieve favorable trade-off between computational efficiency and the quality of preference alignment, the U-Net is fine-tuned only on the final 10 diffusion steps for these imagespecific tasks. All experiments are conducted on 8 H800 GPUs, taking approximately 30 minutes for training, and utilize the AdamW optimizer [Loshchilov and Hutter, 2017], configured with β1 = 0.9, β2 = 0.999, ϵ = 1e 8, and weight decay of 0.01. Additionally, we apply gradient clipping with maximum norm of 1.0. The subsequent paragraph outlines the detailed configurations for each experiment. Non-differentiable rewards. For the prediction of protein secondary structure, our value function is 1https://github.com/DreamFold/FoldFlow 18 Table 4: Impact of hyperparameter wb on the performance of FoldFlow-2-ReFT relative to FoldFlow-2-base. While the wb = 2 setting leads to FoldFlow-2-ReFT underperforming its baseline, our adjusted value of wb = 5 significantly improves its reward. trained on the SCOPe dataset Fox et al. [2014] to bypass data generation via FoldFlow-2 and thereby expedite the training process. To predict the compressed size of the image, we train the value function with images generated by Stable Diffusion 1.4. We provide the details of hyperparameters in Table 2. Besides, for experiments with protein generation, careful readers may find that in the original FoldFlow-2 paper [Bose et al., 2023], wb is set to be 2 instead of 5. However, we find that with the setting of wa = 1, wb = 2 and wc = 0.5, the reward of proteins that generate from the checkpoint of FoldFlow-2-ReFT is even lower than the FoldFlow-2-base, where we refer to the FoldFlow-2-ReFT as FoldFlow-2-base after the reinforced fine-tuning introduced in the original FoldFlow-2 paper, as shown in Table 4. We hypothesize that this is typo in the original paper. So we increase the wb to 5, so that the reward of FoldFlow-2-ReFT can be higher that FoldFlow-2. FoldFlow-2-base FoldFlow-2-ReFT FoldFlow-2-base FoldFlow-2-ReFT 0.83 0.76 0.85 1."
        },
        {
            "title": "Model",
            "content": "2 2 5 5 wb Differentiable rewards. Hyperparameters are presented in Table 3. Unseen prompt generalization. As referenced in the main text, Table 1 includes metrics from the DRaFT [Clark et al., 2023] and PRDP [Deng et al., 2024] papers. To provide full context for these comparisons, we detail relevant aspects of their experimental settings here. Notably, DDPO and PRDP are trained using 512,000 sampled images. In contrast, other methods such as ReFL, DRaFT-1, and DRaFT-LVwhich are reward backpropagation approachesare trained on 160,000 sampled images. Our method (VARD) also follows this 160,000-sample regimen to maintain parity, particularly with these reward backpropagation techniques. Cross-reward validation and prior consistency-reward balance. For Aesthetic Score, we use prompts from the animal category and generate images from 10 random seeds. For PickScore and ImageReward, we sample 100 prompts from the Pick-a-Pic and ImageRewardDB datasets, respectively, and generate images on 5 seeds. A.5 Additional experimental results A.5.1 Effect of KL regularization. We systematically adjust the η parameter controlling pretrained model regularization strength, using values [0, 20, 50, 100] for Aesthetic Score, [0, 0.1, 0.5, 1] for PickScore, and [0, 1, 5, 20] for ImageReward. Figure 10 confirm that higher η values: (1) better preserve the pretrained models priors during training, and (2) proportionally slow reward metric improvement, which demonstrates the inherent trade-off between prior preservation and reward optimization speed. A.5.2 Uncurated samples Here, we present uncurated samples from models fine-tuned with VARD for various reward functions. These include results for protein secondary structure diversity (Figure 11), compressibility and incompressibility (Figure 12), Aesthetic Score (Figure 13), PickScore and HPSv2 (Figure 16), and ImageReward (Figure 15). 19 Figure 10: Training trend of FID (left) and reward (right) under KL regularization. Color intensity correlates with η magnitude (darker corresponds to higher values). Higher η configurations maintain lower FID scores (better prior preservation) while exhibiting slower reward growth, quantitatively demonstrating the preservation-optimization tradeoff. Figure 11: Uncurated protein structures generated by FoldFlow-2-VARD. The showcased samples include proteins with various secondary structures, such as those containing beta-sheets. This illustrates that FoldFlow-2-VARD maintains broad generative capacity, retaining its ability to produce entirely alpha-helical proteins alongside more complex structures. 20 Figure 12: Uncurated samples of fine-tuning on compressibility and incompressibility as rewards. 21 Figure 13: Uncurated samples of fine-tuning on Aesthetic Score as the reward. Figure 14: Further comparison of the base model and VARD fine-tuned version with PickScore as reward. 22 Figure 15: Further comparison of the base model and VARD fine-tuned version with ImageReward as reward. Figure 16: Further comparison of the base model and VARD fine-tuned version with HPSv2 as reward."
        }
    ],
    "affiliations": [
        "Westlake University",
        "Zhejiang University"
    ]
}