{
    "paper_title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining",
    "authors": [
        "Pratyush Maini",
        "Vineeth Dorna",
        "Parth Doshi",
        "Aldo Carranza",
        "Fan Pan",
        "Jack Urbanek",
        "Paul Burstein",
        "Alex Fang",
        "Alvin Deng",
        "Amro Abbas",
        "Brett Larsen",
        "Cody Blakeney",
        "Charvi Bannur",
        "Christina Baek",
        "Darren Teh",
        "David Schwab",
        "Haakon Mongstad",
        "Haoli Yin",
        "Josh Wills",
        "Kaleigh Mentzer",
        "Luke Merrick",
        "Ricardo Monti",
        "Rishabh Adiga",
        "Siddharth Joshi",
        "Spandan Das",
        "Zhengping Wang",
        "Bogdan Gaza",
        "Ari Morcos",
        "Matthew Leavitt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb."
        },
        {
            "title": "Start",
            "content": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining"
        },
        {
            "title": "BeyondWeb",
            "content": "Lessons from Scaling Synthetic Data for Trillion-scale Pretraining DatologyAI Team"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting data wall. In response, the use of synthetic data for pretraining has emerged as promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CCs high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across suite of 14 benchmark evaluations. It delivers up to 7.7 faster training than open web data and 2.7 faster than Nemotron-Synth. Remarkably, 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that theres no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb. Figure 1: Left: BeyondWeb establishes new Pareto frontier for synthetic pretraining data. Notably, our 3B model outperforms all but one 8B model trained on baseline datasets with the same token budget. Average Accuracy (%) is the mean across 14 benchmarks. 1B model trained for 1T tokens; 3B and 8B models for 180B tokens. Right: For 8B model, we achieve up to 7.7 and 2.7 speedup (in time to reach baseline accuracy) over RedPajama and Nemotron-Synth respectively. 1See Contributions and Acknowledgements ( 7) for full author list. 1 5 2 0 2 4 ] . [ 1 5 7 9 0 1 . 8 0 5 2 : r BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining"
        },
        {
            "title": "Introduction",
            "content": "Until 2024, breakthroughs in language modeling followed predictable recipe: train ever-larger models on exponentially larger internet-scraped datasets. However, as the scale of data collection ballooned into the trillions of tokens, the field began to encounter data wall, beyond which data of high information-density becomes prohibitively scarce. The returns on collecting more internet data rapidly diminished, pushing researchers toward exploring alternative paradigms. Synthetic data has emerged as powerful complement to scarce, high-quality web text. Initial evidence from Tiny Stories (Eldan and Li, 2023) showed that targeted prompting of large language models can generate data suitable to train small language models from scratch. Follow-up work, most notably Microsofts Phi family (Li et al., 2023) and the open-source Cosmopedia corpus (Ben Allal et al., 2024), demonstrated that sub-2B models jointly trained on synthetic and raw data can outperform much larger baselines. We refer to this practice of using large models to generate training data de novo as the generator-driven approach. Generator-driven approaches, while powerful, are ultimately limited by the cost and idiosyncrasies of the large generator models they rely on, such as GPT-4 (Maini, 2023). To address these limitations, the Web Rephrase Augmented Pre-training (WRAP) paradigm (Maini et al., 2024) and its refinement in Nemotron-CC (Su et al., 2024) developed what we refer to as the source rephrasing approach. In these works, rather than prompting large LLM to create knowledge de novo, small LLMs are used to rephrase existing web data into higher-quality, task-aligned formats (e.g., Q&A pairs, instructional passages). This approach achieves superior coverage and diversity at lower compute costs, establishing synthetic rephrasing as practical solution for pretraining-scale data generation. The success of synthetic data methods has led research organizations to pour substantial compute resources into creating ever larger corpora of rephrased synthetic data (KimiTeam et al., 2025; Hui et al., 2024; xAI, 2025). Most recently, synthetic data was specifically highlighted as key innovation in GPT-5s development (OpenAI, 2025). While we know that synthetic pretraining data can work, we still lack comprehensive scientific understanding of the factors that determine when and how synthetic pretraining data works. Our work systematically addresses these questions and highlights the challenges of scalably generating high-quality pretraining data by conducting experiments across model scales up to 8B parameters. Specifically, we address the following: How Does Synthetic Data Provide its Benefits? First, we assess whether the model quality improvements imparted by the generator-driven paradigm can be explained as distillation of teacher models knowledge into higher per-token information density data. We find that simple summarization prompts in the source-rephrasing paradigm can match the performance of generator-driven approaches such as Cosmopedia (4.2). Next, we ask whether synthetic data can actually breach the data wall. We show that in dataconstrained setting, naive approaches such as simple continuations (of existing web data) provide limited accuracy improvements over just repeating data. However, thoughtfully-created data that fills in the distributional gaps of the web data has much larger benefits. What and How to Rephrase? First, data quality matters: rephrasing high-quality data provides benefits over using lowerquality sources, but high-quality input data alone is not enough to yield the highest quality synthetic data (4.4). Second, distributional style matching is important. Web data contains only 2.7% conversational content, despite chat being major inference use case. Style matching improves performance in 2 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining downstream tasks, but the benefits saturate quickly with the proportion of conversational data, indicating that it is not sufficient solution. (4.5). Third, diversity in generation strategies is critical to leverage continued benefits from synthetic data when scaling to large training budgets of trillions of tokens (4.6). Who Should Rephrase? Synthetic data benefits are largely consistent across different rephraser families, and rephraser quality does not predict synthetic data quality, indicating that rephrasing is generic capability and rephraser model efficacy is difficult to predict.(4.7). We observe diminishing returns when increasing rephraser size beyond 3B parameters, with 8B LLMs achieving only marginal gains over 3B. This validates that (given the right recipe) effective synthetic data generation doesnt necessarily require massive computational resources (4.8). Motivated by these insights, we introduce BeyondWeb, synthetic pretraining data generation paradigm that leverages targeted document rephrasing to yield diverse, relevant, and informationdense synthetic data. On 8B models trained for 180B tokens, we outperform RedPajama (Weber et al., 2024) and the high-quality synthetic subset of Nemotron-CC (Nemotron-Synth) by +7.1pp and +2.6pp, respectively, while achieving training speedups of 7.7x and 2.7x (see Figure 1). Futhermore, 3B model trained on BeyondWeb surpasses the performance of strong 8B baselines such as Cosmopedia. This establishes new pareto frontier for the accuracy-efficiency trade-off in LLM pretraining. Our results also indicate promising scaling trend: the benefits of our approach show consistent gains across model sizes, achieving (+3.1pp, +2.0pp, +2.6pp) over Nemotron-Synth at 1B, 3B, 8B parameters respectively. We note that BeyondWeb is one part of our full curation platform, which was used to curate the 7T token pretraining dataset for ArceeAIs AFM4.5B model (Atkins, 2025)and combining BeyondWeb with our full curation platform obtains even better results. The strength of AFM4.5B demonstrates the efficacy of BeyondWeb not just in the controlled experimental settings presented here, but also as part of an end-to-end curation pipeline for generating foundation-scale pretraining datasets for production models. Overall, our work demonstrates that generating high quality synthetic data is not trivial task, and that there is no singular silver bullet solution for obtaining the highest quality synthetic data. Naive approaches to synthetic data generation can provide little benefit or even some detriment, while also incurring substantial computational cost for generation. In contrast, the right synthetic generation approach can yield exceptional model quality improvements, as demonstrated by our BeyondWeb results. Getting synthetic data right requires careful consideration of numerous factors, including data selection strategies, generation methodologies, diversity preservation, and quality control, making it challenging endeavor that demands rigorous science and practical expertise."
        },
        {
            "title": "2 A Tale of Two Approaches for Synthetic Pretraining Data",
            "content": "2.1 The Generator-Driven Paradigm: Creating Knowledge from Models de Novo The generator-driven approach to synthetic data uses large-scale models to generate training data from scratch, encapsulating the knowledge embedded within these models. By generating data from pretrained model, researchers have been able to substantially improve the performance of the next generation of models. Notably, the seminal work on Tiny Stories (Eldan and Li, 2023) demonstrated that high-quality, carefully prompted synthetic data, such as simplified narratives crafted by GPT-4, could effectively train small language models from scratch. This result defied the 3 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining prevailing trend of scaling models and web data, sparking paradigm shift and paving the way for increased use of synthetic data for pretraining small, performant models. Soon after, the Phi model family (Li et al., 2023) advanced the generator-driven approach by training small (<2B parameter) models jointly on synthetic data and raw web data, and were able to outperform models up to 10 larger using fraction of the training compute. Cosmopedia (Ben Allal et al., 2024) furthered this direction by introducing large-scale, open-source synthetic dataset, generated by leveraging open-source LLMs and prompting it with diverse set of curated seed topics. However, this paradigm, which relies on using powerful existing model as knowledge bank to generate synthetic data, is constrained by the high computational cost of accessing state-of-the-art generators like GPT-4, making it inaccessible to many researchers. Furthermore, these approaches do not scale well and are susceptible to model collapse (Shumailov et al., 2024; Briesch et al., 2024; Guo et al., 2024), as they inherit the knowledge and biases of the generator, potentially resulting in limited diversity, coverage gaps, and hallucinated content (Maini, 2023). 2.2 The Source Rephrasing Paradigm: Enhancing Existing Knowledge Motivated by the shortcomings of the generator-driven approach, Maini et al. (2024) recently proposed an alternative synthetic generation methodology: Web Rephrase Augmented Pre-training (WRAP). WRAP leverages existing web documents and employs smaller models to rephrase this content into higher-quality, structured and/or targeted formats. Conditioning on existing data significantly reduces dependence on expensive large models while enriching the training corpus with natural, diverse, and topical language. Maini et al. (2024) were able to speed up pretraining by more than 3 through the use of strategic rephrasing. Of note, Allen-Zhu and Li (2023) concurrently worked on rephrasing articles during pretraining on toy task of author biographies, and showed such rewriting enhanced model performance. We refer to this collection of methodologies as the source rephrasing paradigm. The source rephrasing approach has gained considerable traction in industry. Projects such as Nvidias Nemotron-CC (Su et al., 2024), StabilityAIs multilingual augmentation efforts (Pieler et al., 2024), and Microsofts Phi-4 (Abdin et al., 2024) have all built on this paradigm, emphasizing the value of augmenting real-world internet data. This highlights critical ideological shift toward data-driven rather than model-driven synthetic data generation. This approach effectively combines the advantages of broad, naturally occurring knowledge from the internet with controlled style and format enhancements. In 2025, rephrasing has become the dominant paradigm with state-of-the-art LLMs like Kimi K2 (KimiTeam et al., 2025), Qwen-2.5 (Hui et al., 2024), Grok (xAI, 2025), and GPT-5 (OpenAI, 2025) reporting substantial use of and/or meaningful gains from source rephrasing. 2.3 Synthetic Data Beyond Pretraining The utility of synthetic data extends beyond pretraining and into other phases of model development, including fine-tuning and evaluation. Synthetic data can play pivotal role in addressing specific task-oriented training objectives, aligning model behavior, reducing toxicity, and improving generalization on targeted downstream tasks. Techniques such as instruction-tuning, Chain-ofThought prompting, and data backtracing emphasize synthetic datas potential beyond pretraining by directly influencing downstream capabilities, such as reasoning (Lu et al., 2024), alignment (Li et al., 2024b; Wang et al., 2024), and reducing model hallucinations (Jones et al., 2024). However, the relative lack of synthetic data research for pretraining compared to post-training and the potential magnitude of impact from successful pretraining improvements motivated us to focus exclusively on synthetic data for pretraining in this work. BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining"
        },
        {
            "title": "Introducing BeyondWeb",
            "content": "We introduce BeyondWeb, synthetic data approach that leverages grounding and diversity to substantially improve language model pretraining efficiency. It combines the broad coverage of web-scale corpora with strategically-generated content that fills critical gaps, particularly in underrepresented styles, formats, and topics. BeyondWeb employs diverse generation strategies, including format transformation (e.g. converting web content into questionanswer pairs), style modification (e.g. enhancing pedagogical tone), and content restructuring to improve information density and accessibility. This enables the creation of models better aligned with real-world language use and downstream task demands. Datasets. We compare BeyondWeb against state-of-the-art public synthetic pretraining datasets and methodologies, as well as non-synthetic open web dataset, RedPajama (RPJ) (Weber et al., 2024). We chose RPJ because it is well-established baseline that has minimal curation applied to it, allowing us to robustly assess the effects synthetic data. For comparisons with other synthetic datasets, we adopt mixed curation strategy: 60% of the training tokens are sourced randomly from RPJ, while 40% come from synthetic data. We evaluate against three synthetic baselines: Cosmopedia (Ben Allal et al., 2024) contains over 39 million textbooks, blog posts, and stories generated by Mixtral-8x7B-Instruct-v0.1, using web-derived prompts covering predefined set of topics. We use the latest version, Cosmopedia-v2, which contains approximately 27 billion tokens. When more tokens are needed for our experiments, we repeat the dataset to compensate. While repetition is not ideal because it potentially reduces the effective per-token quality of the dataset, we consider this general limitation of the generator-driven paradigm that stems from the cost and difficulty of prompting and generation. WRAP (Maini et al., 2024) rephrases web content into various formats, with question-answering being the most performant style. For our comparison, we use the RedPajama dataset as the source corpus for rephrasing, and the Llama-3.1-8b-Instruct LLM for rephrasing. Nemotron-Synth (Su et al., 2024) is the high-quality subset of synthetic data in Nemotron-CC. The data was generated by applying diverse rephrasing prompts to high-quality, classifier-selected inputs. We chose the subset of Nemotron-CC synthetic data derived from high-quality input data, which we refer to as Nemotron-Synth, instead of the full Nemotron-CC synthetic data because Nemotron-Synth is an extremely competitive baseline and sufficiently large for our long-horizon experiments. Nemotron-Synth contains 1.5 trillion tokens, so we randomly sample subset while maintaining the original proportions across different data subsets. We apply our BeyondWeb approach to high-quality subset of DCLM (Li et al., 2024a) selected using the methods described in DatologyAI et al. (2024). Training setup. We train three sizes of LLMs: 1B parameter model trained on 1 trillion tokens, 3B model trained on 180 billion tokens, and an 8B model trained on 180 billion tokens. We use LLAMA-3.2 architecture (Grattafiori et al., 2024) for the 1B and 3B models, and LLAMA-3.1 architecture for the 8B model. Additional training details can be found in Appendix A.1. We did not extensively tune hyperparameters other than an early search on the baseline RPJ data, as our goal was to improve model performance with data interventions alone. Evaluation setup. We evaluated the models on 14 benchmark tasks (enumerated in Appendix A.2) using both 0-shot and 5-shot prompting and report average accuracy across all settings and tasks. Multiple-choice questions are assessed using relative scoring method, as described in 5 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Hugging Faces analysis of the OpenLLM leaderboard and referred to as cloze form (CF) in Gu et al. (2025). This approach compares the probabilities assigned by the model, restricted to the set of valid answer choices. Performance improvements across scales. BeyondWeb demonstrates consistent improvements across all evaluated model sizes, as shown in Figure 1. At 1B parameters, we achieve 57.4% average accuracy (+6.7pp over RPJ), establishing strong foundation. The performance gains become more pronounced as scale increases: at 3B parameters, we reach 60.8% accuracy (+7.3pp over RPJ), and at 8B parameters, accuracy rises to 63.7% (+7.1pp over RPJ). BeyondWeb also consistently outperforms Nemotron-Synth, our strongest synthetic pretraining data baseline, at all scales (+3.1pp, +2.0pp, +2.6pp at 1B, 3B, and 8B, respectively). These results demonstrate that our synthetic data approach remains effective across scales and continues to offer substantial benefits in larger models. Train Models 7.7x faster. BeyondWeb enables substantial computational savings during pretraining, as shown in Figure 1. For the 8B model, BeyondWeb matches or exceeds RedPajamas 180B-token performance in just 23.2B tokens (7.7 speedup) and Nemotron-Synths 180B-token performance in only 66.2B tokens (2.7 speedup). Faster convergence directly reduces training cost, energy consumption, and iteration time, affording more experiments within fixed compute budget. This efficiency is valuable not only for large industry labs aiming to optimize costs but also for relatively smaller organizations constrained by infrastructure, thereby helping to democratize access to high-performance LLMs. Establishing new pareto frontier for synthetic data. Our results establish new Pareto frontier for the speed-accuracy trade-off in language model pretraining, as visualized in Figure 1. Remarkably, our 3B model (60.8% accuracy) outperforms all but the strongest of the 8B baseline models trained for the same number of tokens (180B), demonstrating that high-quality synthetic data can achieve superior results with dramatically fewer parameters. This highlights the scalability and headroom unlocked by high-quality synthetic data, enabling stronger models at lower computational cost and challenging the conventional reliance on ever-larger architectures. Scale Dataset 1B (1TT) 3B (180BT) 8B (180BT) RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb ARC(C) ARC(E) BoolQ COPA CSQA Hella. MMLU OBQA PIQA RACE-H RACE-M SIQA SciQ Wino. Avg. 50.7 52.5 52.2 54.3 57.4 43.8 44.2 43.9 47.1 49.3 53.9 53.3 53.1 54.1 56. 82.8 88.2 84.4 86.9 93.0 72.0 74.5 70.5 71.5 76.5 31.5 32.7 32.8 34.6 35.5 43.2 44.3 45.4 44.3 47.4 43.1 47.0 43.4 52.6 53.7 55.9 59.6 62.8 64.2 70. 35.4 37.0 34.5 39.3 41.7 71.3 71.8 73.3 74.1 75.0 35.2 35.2 37.0 36.5 39.1 60.0 65.7 59.7 62.8 68.3 29.4 30.9 34.0 35.9 41.4 51.8 51.1 55.6 56.2 56. 33.0 36.0 37.5 41.2 46.7 35.0 39.9 41.5 46.1 51.8 61.3 64.4 66.1 71.4 75.3 65.9 68.4 70.7 75.8 79.3 61.5 67.2 63.1 66.3 70.8 66.1 70.5 64.3 67.9 75. 70.0 73.0 74.5 80.5 82.0 75.5 79.0 79.5 83.5 83.0 47.9 46.5 48.9 50.2 51.7 51.4 50.4 50.6 52.4 54.3 58.6 58.6 62.3 63.9 63.5 65.7 64.3 68.2 69.1 69. 33.8 35.0 35.5 37.7 37.7 36.0 37.8 37.8 40.0 40.6 36.1 36.5 39.1 41.4 43.0 39.5 38.3 41.6 41.8 45.6 74.6 73.2 76.0 76.8 77.0 76.3 75.5 77.4 77.6 78. 37.7 40.1 38.4 42.7 44.6 39.6 41.5 41.2 44.1 47.0 46.7 51.7 49.3 55.0 57.9 47.7 54.5 52.1 57.5 60.5 44.7 46.2 46.2 46.5 49.4 47.0 47.0 47.3 47.4 50. 87.4 90.1 88.0 92.2 94.0 89.2 92.7 89.5 92.5 95.6 55.1 55.2 55.5 57.7 57.5 57.7 57.7 58.4 58.9 60.5 53.5 55.3 55.8 58.8 60.8 56.6 58.4 58.6 61.1 63. Table 1: Performance across all tasks averaged over 0-shot and 5-shot for different model scales and data curations. The best value at each scale is highlighted in bold, the second-best is underlined, and results where BeyondWeb outperforms all other methods are additionally shaded in blue . Consistent improvements across tasks. As presented in Table 1, BeyondWeb achieves the highest average accuracy at every model scale and also outperforms baselines on 13, 12, and 13 out of 14 tasks, in 1B, 3B, and 8B models, respectively. This demonstrates that the performance gains are not 6 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining confined to few standout benchmarks but are broadly distributed, reflecting the generalizability enabled by high-quality, diverse synthetic data. We now examine the factors that contribute to the strength of synthetic data approaches such as BeyondWeb and the obstacles to generating high-quality synthetic pretraining data."
        },
        {
            "title": "4 Systematically Evaluating Synthetic Data",
            "content": "Despite the ready availability of web data for pretraining, training models solely on internetsourced text presents several challenges. First, data redundancy is major issue. Web data is finite, and modern pretraining budgets are beginning to exceed the scale of high-quality web data. Unfortunately, repeating data leads to diminishing returns and overfitting (Muennighoff et al., 2023; Goyal et al., 2024), and thus is not substitute for new data. The second challenge is style mismatch, which describes the stylistic differences between the data models are trained on and the data they encounter at test time. The top three most common styles of web data are personal blogs, product pages, and news articles (Wettig et al., 2025), whereas deployed models predominantly interact in conversational or instructional formats (Maini et al., 2024). The third challenge is that knowledge bottlenecks emerge in underrepresented domains, making models blind to certain topics. Finally, training efficiency can be significantly improved if data is structured to maximize per-token knowledge utility rather than merely increasing token count. Finding Section What Does Synthetic Data Really Do? Are Generator-Driven Approaches Approximated by Summarization? Simple summarization approaches that increase per-token information density achieve similar performance as Cosmopedia. But carefully-crafted source rephrasing approaches can substantially outperform both. Surpassing the Data Wall: Synthetic data generation must be thoughtful in order to breach the data wall. Naive strategies can not break the data wall. How to Rephrase: Methods & Techniques Quality Matters: Synthetically rephrasing high-quality web data offers larger performance gains than using low-quality web data. Style Matters: Distributional style matching is critical. Simply upsampling conversational content from the web increases accuracy on eval tasks. Diversity Matters: As we scale synthetic data to 1T tokens, the importance of diversity in synthetic data generation is critical to avoid diminishing returns. Who Should Rephrase: Generator Properties Model Family Robustness: Synthetic data benefits are largely consistent across generator model families. Rephrasing is generator-agnostic capability. Generator Size Saturation: While larger generators help, improvements plateau beyond 3B parameters with diminishing returns beyond this scale. 4.2 4.3 4. 4.5 4.6 4.7 4.8 Table 2: Summary of key findings organized by theme. Each finding represents core insight from our systematic evaluation of synthetic data generation strategies. These challenges highlight the need to move beyond web data by generating synthetic data. However, not all synthetically generated data are equally effective. Through extensive experimentation 7 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining and systematic ablations, we examine how synthetic data can help overcome data limitations, better align training distributions with downstream use-cases, and enhance both knowledge efficiency and diversity. Our systematic evaluation reveals several key findings, structured around three central themes: the capabilities of synthetic data, effective techniques for rephrasing, and the properties of generator models. Table 2 summarizes these insights along with section references. 4.1 Experiment Setup As baseline for the following sections, we use subset of the RedPajama dataset (Weber et al., 2024). To construct high-quality splits from RedPajama, we follow the method from DatologyAI et al. (2024) (HQ Web). Unless otherwise specified, all ablations train Llama-3.2-1B model on total budget of 20 billion tokens, with 50:50 split between real (HQ Web) and synthetic data. Unless the experimental ablation demands otherwise, all synthetic data generation approaches rephrase the same 10 billion tokens of HQ Web data that are already part of the final 20 billion data mixture. This means that the total source knowledge from the internet is fixed at 10 billion tokens. For fair baseline that controls for the amount of new knowledge, RPJ-HQ baseline also sees the same 10 billion tokens, but twice during its training. For other curated datasets, such as Cosmopedia (Ben Allal et al., 2024), we randomly sample 10 billion tokens from their corpus. All evaluations report the average performance across 0-shot and 5-shot settings on benchmark suite of 14 tasks enumerated in Appendix A.2. 4.2 Research Question (RQ) 1: Are Generator-Driven Approaches Approximated by Summarization? One hypothesis that may explain the benefits of synthetic data is that it effectively increases the pertoken information density of training data. Rather than merely generating more tokens, the efficacy derives from restructuring and distilling existing knowledge into more compact and informative representations. This hypothesis particularly prevails in the generator-driven paradigm, in which large LLM is seeded with various topics of interest, and asked to create textbook-style material based on its parametric knowledge. Intuitively, when model writes book on specific topic, say laws of motion, it distills all the information it has read about it on the internet into much more condensed representation. To test this hypothesis, we compare generator-driven approach (Cosmopedia), with simple rephrasing approach that merely attempts to increase the information density of the internet by summarizing it into more compact, high-quality content. Summarization Prompt Summarize the following text. Directly start with the summary. Do not say anything else. Experiment Design. We consider two approaches: (1) Cosmopedia, which prompts large model to explain or teach concepts, reorganizing knowledge into compact, pedagogically optimized outputs. Cosmopedia uses an 8x7B model to generate high-quality data conditioned on seed topics. (2) Summarization, which compresses existing text using prompt that aims to preserve essential information while reducing length. This approach leverages an 8B-parameter model with simple prompt to summarize source content. Both approaches aim to condense learning-relevant content into fewer tokens, enabling more efficient training under fixed budgets. Following the setup in Section 4.3, we allocate 10 billion tokens for synthetic data and 10 billion tokens for the original dataset during training. BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 2: Knowledge transfer effectiveness across different synthetic data approaches. The yellow line represents Cosmopedia (46.8%), which uses generator-driven, sophisticated educational content generation technique and 8x7B model; the cyan line denotes the Summary (46.8%) approach, which uses an 8B model and simple summarization prompt; the gray line denotes the RPJ-HQ (no synthetic data) baseline. These results demonstrate that even naive summarization achieves substantial improvements similar to those of Cosmopedia, suggesting distillation works through increased information density rather than complex knowledge transfer. Results and Observations. Our analysis in Figure 2 reveals that even simple distillation strategies can achieve substantial performance improvements. 1. Simple summaries match generator-driven methods: The summarization approach (46.7%) nearly matches the performance of the more sophisticated and compute-intensive Cosmopedia approach (47.1%). This suggests that much of the benefit of synthetic data arises from basic information condensation. Notably, summarization, which uses single 8B-parameter model along with information on the internet, nearly matches the performance of Cosmopedia, which relies on much larger setup of 87B-parameter model and manually curated seed topics to guide generation. Importantly, this shows that in the case of Cosmopedia, much of the benefit of the generator-driven paradigm can be achieved using the source-rephrasing paradigm and simple summarization prompt. 2. Increasing token information density improves model performance: The fact that naive summary prompts achieve +1.2pp improvement over the baseline (45.5%) indicates that increasing per-token information density is one mechanism (among potentially many) by which synthetic data improves pre-training. 3. Synthetic data is not just knowledge distillation: While summarization shows similar benefits as Cosmopedia, BeyondWeb outperforms summarization-based approaches with 50.4% accuracy (+3.7pp). This highlights that substantial gains are possible beyond basic distillation and underscores the importance of careful synthetic data generation. 9 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Takeaway: Synthetic Data Is Not Just Knowledge Distillation We find that simple summarization-based rephrasing approach is able to nearly match the performance of the much more expensive generator-driven approach of Cosmopedia. This highlights that increasing per-token information density is one mechanism (among potentially many) by which synthetic data improves pre-training. However, BeyondWeb outperforms summarization-based approaches by clear margin, showing that substantial gains are possible beyond basic distillation, and underscoring the importance of intentional synthetic data approaches. 4.3 RQ2: Can Synthetic Data Break the Data Wall? Can synthetic data effectively compensate for limited high-quality real-world data? This section explores whether there exists fundamental limit to model performance that cannot be overcome through synthetic data. Our results reveal surprising finding: the data wall is surpassable depending on the type of synthetic data. Figure 3: Illustration of data splitting and corpus construction strategies to enable controlled setup. The figure shows how our 20 billion token dataset is divided and utilized across different experimental conditions. The top row displays three data segments: Original 1st Half (10B tokens of natural web content), Original 2nd Half (10B tokens of natural web content), and Continuation (10B tokens of synthetic content generated by extending documents from the first half). The example text snippets demonstrate how continuation generates stylistically consistent but novel content. The arrows below indicate corpus composition: Corpus 1 (Upper Bound) uses both original halves for full natural data coverage; Corpus 2 (2x Repeat) uses only the first half repeated twice; and Corpus 3 (Synthetic Extension) combines the second half with synthetic continuations. This experimental design isolates the effects of repetition versus synthetic augmentation when facing data constraints. Continuation Prompt Continue the following text in the same style as the original. Experiment Design. We designed three controlled datasets (see Figure 3) to systematically compare different training approaches operating in data-constrained setting of 20 billion tokens. 10 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining 1. Full Data (Upper Bound). This represents our performance ceiling, utilizing the complete available dataset of 20 billion tokens without any repetition or synthetic augmentation. This corpus serves as the oracle baseline, representing the maximum achievable performance given our data constraint. The figure shows this as complete, unbroken dataset. 2. 2x Repeat (Lower Bound). This corpus simulates data scarcity by artificially constraining our dataset to only 10 billion unique tokens, then repeating this exact content twice to reach our full 20 billion token training budget. We split each paragraph at the sentence boundary closest to its midpoint, and only retain the second half of the paragraph. This approach represents naive solution of repeating data to meet training token requirements, and serves as our lower bound. 3. Continuation (Naive Synthetic). Building on the same 10 billion token dataset as the previous strategy, this approach generates the remaining 10 billion tokens through simple model-driven continuation. An LLM (Llama-3.1-8B in this case) receives partial documents from the second half of the original dataset and is prompted to generate natural continuations. This represents the most straightforward approach to synthetic data generation, where models extend the available data. The synthetic content attempts to maintain stylistic consistency with the source material. We specifically chose the second half of each paragraph in the original data to avoid the confounding effect of the generator model already having seen the full sample during its own pretraining. If the generator model had seen the sample, then it may be able to cheat by generating the true second half when prompted with the first half, and hence approximate the full data upperbound. This would allow us to test whether naive continuation of part of the data can approximate the whole. Figure 4: Performance comparisons across different data augmentation strategies during training. The dark blue line represents BeyondWeb (50.4%) which significantly surpasses all other approaches. The light blue line shows Continuation (46.2%), the cyan line depicts Full Data Upper Bound (46.2%), and the gray line represents 2x Repeat Lower Bound (45.5%). The striking visual separation emphasizes BeyondWebs +4.2pp improvement over the Full Data upper bound. This reflects how intentionality is critical to breaking the data wall with synthetic data, and not just any synthetic data will yield benefits. Results and Observations. Figure 4 offers visual comparison of average accuracy across models pretrained on different datasets using various augmentation strategies. 11 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining 1. Repetition leads to performance degradation: The 2x Repeat strategy (gray line) achieves only 45.5% accuracy, representing 0.7pp drop from the full data baseline (46.2%). This demonstrates that naive data duplication leads to diminishing returns as seen in prior works (Muennighoff et al., 2023; Goyal et al., 2024). 2. Naive synthetic generation provides only modest improvements: The Continuation strategy (46.2%) offers small +0.7pp gain over 2x Repeat (45.5%). This matches the Full Data upper bound, suggesting that simple model-generated extensions can potentially compensate for limited quantities of web data. We note that an important confounder in this experiment is that the continuation model has seen lot more than the 20B tokens available for full data experiment. This means that it may be using its parametric knowledge to add new knowledge. 3. Strategic synthetic data breaches the data wall: BeyondWeb achieves 50.4% accuracy, surpassing the Full Data upper bound (46.2%) by +4.2pp. This finding demonstrates that carefully crafted synthetic data can exceed the performance ceiling of natural data. The data wall is not unsurpassable; it can be broken through strategic synthetic data generation. Takeaway: Synthetic Data Generation Must be Thoughtful to Improve over Real Data Simple continuation provides limited improvement over repetition, and may not be able to breach the data wall. Synthetic data must be well designed, as shown by BeyondWeb, which can significantly exceed the performance of training solely on natural data. 4.4 RQ3: How Important is the Quality of the Seed Data for Rephrasing? An important design decision in synthetic data generation involves the quality of both the source material and the generated synthetic content. This raises fundamental question: if one has limited high-quality data, is it better to use high-quality (but repeated) input data to seed synthetic rephrasing, or leverage lower-quality sources and turn them into high quality? Experiment Design. We operate under constraint of 10B tokens of high-quality data, along with an abundant supply of low-quality data. Following the method from DatologyAI et al. (2024), we sample small subset of high-quality (HQ Web) data from RedPajama. We also use random sample of the RPJ dataset as our low-quality subset (LQ Web). We then rephrase the LQ Web data to yield LQ Synth and the HQ Web data to yield HQ Synth, and compare training scenarios that use different quality combinations of synthetic and/or web data. We note that we rephrase the same HQ web data that we also use in the data mix. Results and Observations. Figure 5 shows two important insights about input data for rephrasing: 1. High-Quality Seed Data Dominates: The high performance of the combination of HQ Synth + HQ Web (49.2%) demonstrates that starting with high-quality source material for synthetic rephrasing is crucial. Notably, the BeyondWeb performance at 50.4% shows that there remain other avenues for performance improvement beyond high-quality source data. 2. Quality Trumps Novelty: The LQ Synth + HQ Web combination (48.6%) underperforms the HQ Synth + HQ Web data (49.2%), indicating that the quality of seed data for synthetic generation matters more than ensuring complete novelty in the knowledge base. However, it still provides +3.0pp improvement over the baseline. BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 5: Performance comparison across different quality combinations in training data. HQ refers to high-quality web data, LQ refers to low-quality web data. The dark blue line shows BeyondWeb (50.4%), dark cyan shows HQ Synth + HQ Web (49.2%), where the synthetic data are rephrased versions of the HQ web samples, and the light cyan line shows LQ Synth + HQ Web (48.6%). The gray baseline corresponds to LQ Web + HQ Web (45.6%). These results indicate that improving the quality of input data for rephrasing improves the rephrased data, even when there is overlap with the original input data. But improved input data quality alone is inadequate for producing the highest quality synthetic data. Takeaway: Quality Synthesis Can Enable Knowledge Amplification Given limited high-quality data, it is more beneficial to use it as seed data for synthetic rephrasing rather than low-quality data, despite the possible drawback of repeating knowledge from the high-quality sources. Nevertheless, high-quality input data alone is insufficient for generating the highest quality synthetic data. 4.5 RQ4: Is Distributional Style Matching Important? While vast amounts of text data are available online, the natural distribution of web content is different from the types of text typically encountered at inference. For instance, the internet is dominated by personal blogs, news articles, and product pages (Wettig et al., 2025). However, evaluation use cases typically involve chat, or in-context learning abilities. These can together be stylistically seen as ping-pong rally between questions and answers, or users and assistants. We investigate the impact of this implicit distribution shift between train and test, and if such stylistic alignment can be one mechanism by which synthetic rephrasing can improve model performance. Estimating the amount of conversational web data. We first estimate the fraction of conversational data in the general web corpus. To do so, we sampled 10k random examples from the RedPajama dataset, and queried the gpt4o model to label them as conversational or not. Here conversational refers to various forms of texts which have ping-pong effect (see Appendix A.3 for prompt). Our analysis indicated that conversational dialogue comprises less than 2.7% of internet 13 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining text. Yet, the dominant use case for modern language models lies in chat-based applications, such as virtual assistants, customer service bots, and interactive tools. This discrepancy reveals gap between the training distribution and real-world model usage. The formats and interaction styles most relevant to deployment, such as multi-turn dialogue, question answering, and instructional content, are significantly underrepresented in naturally occurring data. Synthetic rephrasing provides mechanism to bridge this gap by generating examples in the specific forms and contexts that matter for downstream applications. Estimating the impact of conversational web data. To systematically evaluate the impact of style alignment, we conduct controlled experiments that vary the proportion of conversational content in our training data while maintaining constant token budgets and data quality. We leverage the style classification filters from Organize the Web (Wettig et al., 2025) to identify naturally occurring conversational content within the RedPajama dataset. From their 20 identified web content styles, we focus on four that exhibit conversational characteristics: Audio Transcript, Customer Support, FAQ, and Q&A Forum. Manual inspection confirmed these categories contain the back-and-forth dialogue patterns characteristic of conversational interaction. As validation of our previous investigation, this naturally occurring conversational content comprises 3.67% of the RedPajama dataset, roughly aligning with our GPT-based estimate of 2.7% mentioned earlier. We construct three training datasets with varying conversational ratios: 10%, 20%, and 50% conversational content, with the remainder consisting of randomly sampled RedPajama data. Each dataset maintains our standard 20 billion token training budget, enabling direct performance comparisons across different style distributions. Figure 6: Effect of conversational data ratio on final accuracy. The first point corresponds to the RPJ baseline (randomly sampled) which contains 3.67% conversational data. For each of the other points, we upsampled conversational data in RPJ to the desired ratio and replaced the remaining portion with randomly sampled RPJ data to achieve the 20B token training budget. Increasing the percentage of conversational data beyond the 3.67% baseline can improve performance by up to 0.9pp at 50% conversational data, but the gains saturate beyond 20%, indicating that style-matching is important, but not sufficient for maximizing synthetic data quality. Results and Observations. Since our focus is on multi-turn, back-and-forth conversations, for this evaluation we measure only the 5-shot performance of the trained models. We find that increasing the proportion of conversational content in pretraining improves model performance 14 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining (Figure 6). The first point represents the RPJ baseline, which contains 3.67% conversational data from random sampling. For higher ratios, we upsample conversational data within RPJ to the target percentage and replace the remaining portion with randomly sampled RPJ data. Performance increases from 43.2% at the baseline to 44.1% at 50% conversational content, with 10% and 20% conversational data yielding 43.5% and 44.0%, respectively. This trend confirms that increased exposure to conversational patterns during pretraining modestly enhances downstream task capabilities, though gains exhibit diminishing returns. Takeaway: Distributional Style Matching is Useful but not Sufficient Conversational content is small fraction of web data (3.67%), yet chat and in-context learning are the primary inference use cases of LLMs. Upsampling natural conversational data improves downstream task performance, though these improvements are modest and show diminishing returns with the proportion of conversational data. These results indicate that style-matching can improve synthetic data quality, but is not sufficient for maximizing it. 4.6 RQ5: How Important is Diversity at Scale? In the previous section, we found that aligning styles between train and test time has positive but diminishing gains. This finding highlights critical limitation: while style alignment helps initially, models eventually saturate on uniform synthetic data patterns. This leads us to examine the importance of diversity in synthetic data generation. We investigate fundamental question: How does synthetic data diversity affect model performance when scaling to trillions of tokens? Specifically, do diverse generation strategies maintain benefits throughout extended training, or do all approaches eventually plateau? Our experiments reveal that different synthetic data generation approaches yield dramatically different scaling outcomes, with approaches that emphasize diversity, exemplified by BeyondWeb, significantly outperforming fixed synthetic data generation techniques. Experiment Design. To understand the scaling effects of different synthetic data strategies, we analyze training dynamics across model scales through extended training runs. Among the strategies we investigate, Cosmopedia generates textbook related to web sample under the generatordriven paradigm, QA Wrap rephrases existing web documents into conversational questionanswers. Nemotron-Synth builds on top of WRAP by diversifying the prompts beyond just conversational question-answering, into various styles like MCQ, Yes/No questions, open-ended questions, reading comprehension tasks, logical problem solving tasks, and so on. BeyondWeb further prioritizes diversity in synthetic data generation, and prioritizes learning patterns that aid learning and information retention. Results and Observations. The training dynamics reveal fundamental differences between generation strategies as we scale to hundreds of billions of tokens of data, particularly evident across different training regimes. Diversity in synthetic data generation manifests in two critical ways: 1. Diversity provides an initial performance bump: Multi-strategy approaches like BeyondWeb provide substantial early gains as models quickly benefit from immediate exposure to diverse formats and styles. We hypothesize this aids learning by making patterns that align with models final usage, core to its early representations. 2. Diversity leads to sustained improvements: Diverse generation strategies continue providing learning benefits throughout extended training, even in overtrained regimes. At the 8B scale, 15 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 7: Training dynamics across model scales showing performance improvements relative to RedPajama baseline. Left: 1B Scale, Center: 3B Scale, Right: 8B Scale. BeyondWeb (blue) maintains consistent positive slopes across all scales, while QA approaches (cyan) show early gains followed by plateauing effects, particularly evident in larger models. Notably, at 1B scale, models are trained to saturation at approximately 50 beyond Chinchilla optimal compute. Where other baselines begin to overfit and degrade, BeyondWeb continues to grow in its improvements over RedPajama performance. The sustainable improvement curves demonstrate that diverse generation strategies provide more robust long-term benefits than single-strategy approaches across training regimes. Note that in order to smooth intermediate fluctuations, each data point on the plot is the average accuracy of multiple previous intermediate checkpoints (e.g. the 100B token checkpoint is the average of all checkpoints until 100B, the 200B checkpoint is the average of all checkpoints between 100B and 200B, etc). the marginal improvements for BeyondWeb over RPJ (y-axis) continue to increase, whereas the benefits for strategies such as Cosmopedia (which generates fixed style of textbook content) tend to saturate (Figure 7). Remarkably, at 1B scale where models are trained approximately 50 beyond Chinchilla optimal compute, BeyondWeb maintains positive performance trends while other baselines begin to overfit and show degraded performance, demonstrating the robustness of diverse synthetic data against overfitting in extreme training regimes. Takeaway: Diversity Enables Sustained Learning While standard synthetic data generation methods provide initial benefits as models adapt to consistent formats, their lack of stylistic diversity leads to diminishing returns. Multi-faceted approaches continue providing valuable learning signals throughout training, whereas single-strategy methods saturate. 4.7 RQ6: How Important is the Synthetic Rephraser Model Family? key difference between the source rephrasing and the generator-driven approach is that the synthetic data generator is not required to be the source of the knowledge. Instead it only needs to transform the data to make it more useful for pretraining. While existing work like Cosmopedia (Ben Allal et al., 2024) and Phi-1.5 (Li et al., 2023) requires large generator models, Mixtral-8x7BInstruct-v0.1 (Jiang et al., 2024) and GPT-4, respectively, we question whether the specific generator family used for rephrasing significantly impacts the quality of the rephrased data. 16 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Experiment Design. We test the importance of generator family by using 4 different generator models: (i) OLMo-2-7B2, (ii) Phi-4-14B3, (iii) Mistral-7B-v0.34, and (iv) Llama-3.1-8B5. We use the same rephrasing prompt for each model and evaluate the performance of model trained on the synthetic data generated by each of these generator models. We generated 10B tokens of synthetic data using each of them, combined it with 10B tokens of web data, and trained model on each of the resulting 20B token datasets. Results and Observations. Synthetic data rephrased using different model families consistently produces high-quality training data, with the resulting datasets achieving gains ranging from +3.4pp to +4.4pp over the RPJ baseline (45.5%). All model families were within 1pp of each other, with OLMo-2-7B achieving the highest performance (49.9%) and Mistral-7B-v0.3 the lowest (48.9%). We compare the general benchmark performance of each rephraser model (OLMo-2-7B: 59.6%, Llama-3.1-8B: 61.2%, Mistral-7B-v0.3: 66.0%, and Phi-4: 66.6%) with the quality of the synthetic data they produce, measured by the performance of models trained on their rephrased outputs. While the rephrasers span 7-point range in general accuracy, the quality of the resulting synthetic datasets is strikingly similar, differing by less than 1 percentage point across generators. Notably, OLMo-27B, despite having the lowest benchmark accuracy, produces the highest-quality synthetic data in our evaluation (49.9%). This lack of positive correlation indicates that models general language modeling ability does not determine synthetic data quality, highlighting that even comparatively weaker models can be highly effective rephrasers. These findings suggest that the primary driver of performance is not the generators general language modeling capabilities, but rather the limited and simpler capability to perform effective rephrasing and data transformation, which appears to be consistent across different model families. This generator-agnostic behavior means that organizations can deploy synthetic data pipelines using available model families without worrying about generator-specific optimizations, thereby reducing overall resource usage and enabling the viability of open-source and permissible models. Takeaway: Generator Knowledge is Less Important Synthetic data benefits are largely robust across generator families, with consistent improvements across Phi-4-14B, Mistral-7B-v0.3, and Llama-3.1-8B, and further improved synthetic data quality from OLMo-2-7B. Additionally, generator model quality is not predictive of synthetic data quality. These results indicate that selecting good rephrasing model may be straightforward as most work well, but selecting the best rephrasing model is not. 4.8 RQ7: Does Rephraser Size Matter? Having observed near-invariance in the performance of synthetic data generated by different model families, we now turn to the question of whether the size of the rephraser model affects synthetic data quality. Rephrasing is much more constrained task than open-ended generation, so its possible that even small models may be able to generate high-quality synthetic data via rephrasing. This is once again in contrast to the generator-driven paradigm where the generator 2https://huggingface.co/allenai/OLMo-2-1124-7B-Instruct 3https://huggingface.co/microsoft/phi-4 4https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 5https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 17 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 8: Synthetic data benefits are largely consistent across generator model families, and generator model quality does not predict synthetic data quality. Left: This plot shows the quality of models trained on synthetic data generated by different models: the pink line represents data generated by OLMo-2-7B (49.9%), the yellow line shows data generated by Phi-4-14B (49.5%), the orange line indicates data generated by Mistral-7B-v0.3 (48.9%), and the blue line depicts data generated by Llama-3.1-8B (49.2%). Models trained on each of these synthetic datasets achieve substantial improvements over the gray baseline representing RPJ (45.5%), with three out of the four models generating synthetic data of nearly identical quality, and OLMo-2-7B generating better data. Right: We plot the benchmark performance (on the x-axis) of the rephraser models used for synthetic data generation and contrast with the performance of the LLM trained on their generated data (marked as synthetic data quality on the y-axis). We observe that the benchmark performance of language model does not positively correlate with its rephrasing capability. model is required to be the source of the knowledge, and the quality of the synthetic data is heavily dependent on the size of the generator model (Ben Allal et al., 2024). Experiment Design. To investigate this, we used Llama-3 models of varying sizes, 1B, 3B, and 8B, to generate 10B tokens of synthetic data. We combined the 10B synthetic tokens with 10B tokens of web data, and trained model on each of the resulting 20B token datasets. Results and Observations. As depicted in Figure 9, larger generators consistently produce better synthetic data, yielding substantial downstream performance gains (+3.7pp for 8B, +3.3pp for 3B) over RPJ baseline (45.5%). The 1B generator also shows significant benefit (+1.8pp), demonstrating that small generators can be effective. Larger LLMs provide only small additive gains. While the 8B model (49.2%) slightly outperforms the 3B model (48.8%), the small gain (+0.4pp) indicates diminishing returns beyond 3B. This near-saturation makes small models cost-efficient and practical choice for many applications, offering strong balance between quality, speed, and compute overhead. Larger generators likely offer advantages in knowledge depth, reasoning, and content diversity, which smaller models cannot replicate. However, this 3B capability threshold makes high-quality synthetic data accessible to researchers with limited computational resources. 18 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 9: Effect of generator model size on synthetic data quality. The plot demonstrates the impact of generator size: the light cyan line shows Llama-3.2-1B generator performance (47.3%), the cyan line represents Llama-3.2-3B generator results (48.8%), and the dark blue line indicates Llama-3.1-8B generator performance (49.2%). The improvement from 1B to 3B (+1.5pp) is substantial, while the gain from 3B to 8B (+0.4pp) shows diminishing returns. The gray baseline at 45.5% represents RPJ performance, emphasizing the consistent improvements achieved by all generator sizes. Takeaway: Small Models Can Produce High Quality Synthetic Data The quality of synthetic data increases when increasing generator size from 1B to 3B, then starts to saturate at 8B. The simplicity of rephrasing makes generator size less critical, enabling highly scalable synthetic pretraining data generation even with small models."
        },
        {
            "title": "5 Future Directions",
            "content": "Our work with synthetic data opens several research directions that warrant further investigation: Scaling Laws and Inherent Repetition. crucial area for future research lies in understanding the scaling laws specific to synthetic data. Unlike real data, where repetition is explicit and measurable, synthetic data presents more nuanced form of inherent repetition stemming from the generating models architecture and the prompting methods used. This intrinsic repetition, governed by the models parameters and rephrasing strategies, requires new theoretical frameworks and metrics for quantification. The development of such metrics would enable us to better assess the true value and diversity of synthetic data. This understanding is particularly important as we scale up synthetic data generation, as it could reveal fundamental limits or opportunities that differ from those observed in traditional scaling laws for real data. Democratizing Synthetic Data Generation. An important observation from our work is that the task of rephrasing, at its core, doesnt necessarily require extensive world knowledge. This suggests the possibility of using much smaller models for synthetic data generation, which could significantly reduce computational costs. Future work should explore the minimal model size 19 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining required for effective rephrasing while maintaining quality, potentially making synthetic data generation more accessible to researchers with limited computational resources. Alignment with Human Values. Since the synthetic data generation process offers greater control, it creates promising opportunities for developing pretraining methodologies that explicitly promote alignment with human values (Korbak et al., 2023). This aspect of synthetic data training could provide powerful tool for addressing alignment challenges in language model development, offering an alternative to post-hoc alignment techniques (Maini et al., 2025). Applicability Across Data Domains While the present work, like most documented applications of source rephrasing, uses web data as input, the approach is not inherently limited to web data or even the text modality. It can be applied to domain-specific or proprietary data without requiring the rephrasing model to possess domain expertise, making it broadly suitable across diverse settings. Future work should explore applying such techniques to new domains and modalities, using synthetic data generation to help overcome the inherent data wall. These research directions highlight the potential of synthetic data to not only improve model performance but also to address fundamental challenges in machine learning, from accessibility and scalability to alignment and safety. Future work in these areas could significantly impact how we approach language model training and deployment."
        },
        {
            "title": "6 Conclusion",
            "content": "Pretraining now routinely encounters data wall: the supply of high-quality, information-dense web text can not keep up with modern training budgets. Synthetic data has emerged as natural and effective response, yet the scientific understanding of when and how it helps has remained limited. We address this by studying synthetic data for pretraining at scale and introduce BeyondWeb, rephrasing-driven approach grounded in real web documents and diversified across styles and formats. Our work identifies three general principles for effective synthetic data generation: prioritize quality over novelty by rephrasing high-quality sources rather than adding low-quality content; align the training data style with the deployment use-case; and maintain diversity in generation to sustain improvement over long training horizons. We show that these benefits are robust across generator families and saturate beyond moderate rephraser sizes, indicating that effective synthetic data generation does not require particular model architecture or very large generators. Taken together, these results demonstrate that high quality synthetic pretraining data is challenging to generate, even more challenging to ensure the efficacy of for larger models and training budgets, and risks being extremely costly to generate. Furthermore, there is no silver bullet for synthetic data, strong outcomes require jointly optimizing many variables. However, all of these challenges and risks are surmountable with thoughtful, scientifically rigorous source rephrasing, as evidenced by BeyondWeb substantially outperforming other public synthetic pretraining data offerings. 20 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining"
        },
        {
            "title": "7 Contributions and Acknowledgements",
            "content": "Core Research Pratyush Maini and Vineeth Dorna. steering the ship, wrangling the data, and keeping the Celsius industry thriving. Core Infra Leadership Contributors Aldo Carranza, Fan Pan, Jack Urbanek, Parth Doshi, and Paul Burstein. the all-star cast who turned Mazra into code, numbers, and wins. Bogdan Gaza, Ari Morcos, and Matthew Leavitt. the wise captains that made sure we didnt steer into bad baseline. Alex Fang, Alvin Deng, Amro Abbas, Brett Larsen, Cody Blakeney, Charvi Bannur, Christina Baek, Darren Teh, David Schwab, Haoli Yin, Josh Wills, Kaleigh Mentzer, Luke Merrick, Ricardo Monti, Rishabh Adiga, Siddharth Joshi, Spandan Das, and Zhengping Wang. the ping-pong maestros who kept latency low and rallies long. Acknowledgements Gem De Leon and Kristin Reinke for fueling us with Beyond Burgers and unshakable good vibes. Jacqueline Liu and Tiffanie Pham for assembling the all-star cast that made this work possible. Liz Gatapia for the beautiful logo design. Kylie Clement, Jeremy Custenborder and Elise Clark for their sharp-eyed feedback on the draft. the backstage crew who kept us fed, staffed, and focused."
        },
        {
            "title": "References",
            "content": "M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett, M. Javaheripi, P. Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Z. Allen-Zhu and Y. Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023. L. Atkins. Announcing the official launch of afm-4.5b. July 2025. URL https://www.arcee.ai/ blog/announcing-the-official-launch-of-afm-4-5b. L. Ben Allal, A. Lozhkov, G. Penedo, T. Wolf, and L. von Werra. Cosmopedia. 2024. URL https://huggingface.co/datasets/HuggingFaceTB/cosmopedia. Y. Bisk, R. Zellers, R. Le Bras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of AAAI, 2020. M. Briesch, D. Sobania, and F. Rothlauf. Large language models suffer from their own output: An analysis of the self-consuming training loop. 2024. URL https://arxiv.org/abs/2311.16822. C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of NAACL, 2019. P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 21 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining DatologyAI, A. Carranza, A. Deng, P. Maini, M. Razzak, J. Urbanek, A. Abbas, P. Burstein, N. Cao, P. Goyal, J. McGrath, F. Pan, J. Wills, H. Yin, V. Kada, V. Shah, V. Veerendranath, B. Gaza, A. Morcos, and M. Leavitt. DatologyAI Technical Deep-Dive: Image-Text Data Curation at the Billion-Sample Scale. Nov. 2024. URL https://blog.datologyai.com/ technical-deep-dive-curating-our-way-to-a-state-of-the-art-text-dataset/. R. Eldan and Y. Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. S. Goyal, P. Maini, Z. C. Lipton, A. Raghunathan, and J. Z. Kolter. Scaling laws for data filteringdata curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2270222711, 2024. A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Y. Gu, O. Tafjord, B. Kuehl, D. Haddad, J. Dodge, and H. Hajishirzi. Olmes: standard for language model evaluations. 2025. URL https://arxiv.org/abs/2406.08446. Y. Guo, G. Shang, M. Vazirgiannis, and C. Clavel. The curious decline of linguistic diversity: Training language models on synthetic text. 2024. URL https://arxiv.org/abs/2311.09807. N. Habib, C. Fourrier, H. Kydlıˇcek, T. Wolf, and L. Tunstall. Lighteval: lightweight framework for llm evaluation. 2023. URL https://github.com/huggingface/lighteval. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2021. B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed. Mixtral of experts. 2024. URL https://arxiv.org/abs/2401.04088. E. Jones, H. Palangi, C. S. Ribeiro, V. Chandrasekaran, S. Mukherjee, A. Mitra, A. H. Awadallah, and E. Kamar. Teaching language models to hallucinate less with synthetic tasks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=xpw7V0P136. KimiTeam, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen, Z. Chen, J. Cui, H. Ding, M. Dong, A. Du, C. Du, D. Du, Y. Du, Y. Fan, Y. Feng, K. Fu, B. Gao, H. Gao, P. Gao, T. Gao, X. Gu, L. Guan, H. Guo, J. Guo, H. Hu, X. Hao, T. He, W. He, W. He, C. Hong, Y. Hu, Z. Hu, W. Huang, Z. Huang, Z. Huang, T. Jiang, Z. Jiang, X. Jin, Y. Kang, G. Lai, C. Li, F. Li, H. Li, M. Li, W. Li, Y. Li, Y. Li, Z. Li, Z. Li, H. Lin, X. Lin, Z. Lin, C. Liu, C. Liu, H. Liu, J. Liu, J. Liu, L. Liu, S. Liu, T. Y. Liu, T. Liu, W. Liu, Y. Liu, Y. Liu, Y. Liu, Y. Liu, Z. Liu, E. Lu, L. Lu, S. Ma, X. Ma, Y. Ma, S. Mao, J. Mei, X. Men, Y. Miao, S. Pan, Y. Peng, R. Qin, B. Qu, Z. Shang, L. Shi, S. Shi, F. Song, J. Su, Z. Su, X. Sun, F. Sung, H. Tang, J. Tao, Q. Teng, C. Wang, D. Wang, F. Wang, H. Wang, J. Wang, J. Wang, J. Wang, S. Wang, S. Wang, Y. Wang, Y. Wang, Y. Wang, Y. Wang, Y. Wang, Z. Wang, Z. Wang, Z. Wang, C. Wei, Q. Wei, W. Wu, X. Wu, Y. Wu, C. Xiao, X. Xie, W. Xiong, B. Xu, J. Xu, J. Xu, L. H. Xu, L. Xu, S. Xu, W. Xu, X. Xu, Y. Xu, Z. Xu, J. Yan, Y. Yan, X. Yang, Y. Yang, Z. Yang, Z. Yang, Z. Yang, H. Yao, X. Yao, W. Ye, Z. Ye, B. Yin, L. Yu, E. Yuan, 22 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining H. Yuan, M. Yuan, H. Zhan, D. Zhang, H. Zhang, W. Zhang, X. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Y. Zhang, Z. Zhang, H. Zhao, Y. Zhao, H. Zheng, S. Zheng, J. Zhou, X. Zhou, Z. Zhou, Z. Zhu, W. Zhuang, and X. Zu. Kimi k2: Open agentic intelligence. 2025. URL https://arxiv.org/abs/2507.20534. T. Korbak, K. Shi, A. Chen, R. V. Bhalerao, C. Buckley, J. Phang, S. R. Bowman, and E. Perez. Pretraining language models with human preferences. In International Conference on Machine Learning, pages 1750617533. PMLR, 2023. G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017. J. Li, A. Fang, G. Smyrnis, M. Ivgi, M. Jordan, S. Gadre, H. Bansal, E. Guha, S. Keh, K. Arora, S. Garg, R. Xin, N. Muennighoff, R. Heckel, J. Mercat, M. Chen, S. Gururangan, M. Wortsman, A. Albalak, Y. Bitton, M. Nezhurina, A. Abbas, C.-Y. Hsieh, D. Ghosh, J. Gardner, M. Kilian, H. Zhang, R. Shao, S. Pratt, S. Sanyal, G. Ilharco, G. Daras, K. Marathe, A. Gokaslan, J. Zhang, K. Chandu, T. Nguyen, I. Vasiljevic, S. Kakade, S. Song, S. Sanghavi, F. Faghri, S. Oh, L. Zettlemoyer, K. Lo, A. El-Nouby, H. Pouransari, A. Toshev, S. Wang, D. Groeneveld, L. Soldaini, P. W. Koh, J. Jitsev, T. Kollar, A. G. Dimakis, Y. Carmon, A. Dave, L. Schmidt, and V. Shankar. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024a. X. Li, P. Yu, C. Zhou, T. Schick, O. Levy, L. Zettlemoyer, J. Weston, and M. Lewis. Self-alignment with instruction backtranslation. 2024b. URL https://arxiv.org/abs/2308.06259. Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T. Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. Z. Lu, A. Zhou, H. Ren, K. Wang, W. Shi, J. Pan, M. Zhan, and H. Li. MathGenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of LLMs. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27322747, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long. 151. URL https://aclanthology.org/2024.acl-long.151/. P. Maini. Phi-1.5 model: case of comparing apples to oranges? 2023. URL https://pratyushmaini. github.io/phi-1 5/. P. Maini, S. Seto, H. Bai, D. Grangier, Y. Zhang, and N. Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. arXiv preprint arXiv:2401.16380, 2024. P. Maini, S. Goyal, D. Sam, A. Robey, Y. Savani, Y. Jiang, A. Zou, Z. C. Lipton, and J. Z. Kolter. Safety pretraining: Toward the next generation of safe ai. arXiv preprint arXiv:2504.16980, 2025. T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of EMNLP, 2018. N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao, A. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and C. Raffel. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264, 2023. OpenAI. Introducting gpt-5. 2025. URL https://www.youtube.com/live/0Uu VJeVVfo?feature= shared&t=1982. G. Penedo, H. Kydlıˇcek, L. B. allal, A. Lozhkov, M. Mitchell, C. Raffel, L. V. Werra, and T. Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. 2024. URL https: //arxiv.org/abs/2406.17557. 23 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining M. Pieler, M. Bellagente, H. Teufel, D. Phung, N. Cooper, J. Tow, P. Rocha, R. Adithyan, Z. Alyafeai, N. Pinnaparaju, et al. Rephrasing natural text data with different languages and quality levels for large language model pre-training. arXiv preprint arXiv:2410.20796, 2024. M. Roemmele, C. A. Bejan, and A. S. Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, 2011. K. Sakaguchi, R. Le Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale. In Proceedings of AAAI, 2020. M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi. Social iqa: Commonsense reasoning about social interactions. In Proceedings of EMNLP, 2019. I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. The curse of recursion: Training on generated data makes models forget. 2024. URL https://arxiv.org/abs/2305.17493. D. Su, K. Kong, Y. Lin, J. Jennings, B. Norick, M. Kliegl, M. Patwary, M. Shoeybi, and B. Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. A. Talmor, J. Herzig, N. Lourie, and J. Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. In Proceedings of NAACL, pages 142148, 2019. Z. Wang, C.-L. Li, V. Perot, L. Le, J. Miao, Z. Zhang, C.-Y. Lee, and T. Pfister. CodecLM: Aligning language models with tailored synthetic data. In K. Duh, H. Gomez, and S. Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 37123729, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-naacl.235. URL https://aclanthology.org/2024.findings-naacl.235/. M. Weber, D. Y. Fu, Q. Anthony, Y. Oren, S. Adams, A. Alexandrov, X. Lyu, H. Nguyen, X. Yao, V. Adams, B. Athiwaratkun, R. Chalamala, K. Chen, M. Ryabinin, T. Dao, P. Liang, C. Re, I. Rish, and C. Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. J. Welbl, N. F. Liu, and M. Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, 2017. A. Wettig, K. Lo, S. Min, H. Hajishirzi, D. Chen, and L. Soldaini. Organize the web: Constructing domains enhances pre-training data curation. arXiv preprint arXiv:2502.10341, 2025. xAI. We will use grok 3.5 (maybe we should call it 4), which has advanced reasoning, to rewrite the entire corpus of human knowledge, adding missing information and deleting errors. June 2025. URL https://x.com/elonmusk/status/1936333964693885089. R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of ACL, 2019. 24 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Training Details For all trainings, we use AdamW with β1=0.9, β2=0.95, learning rate of 5e4, and weight decay of 1e7. linear warmup schedule was applied with 4K steps for the 1B model and 16K steps for the 3B and 8B models. Training was performed using fully sharded data parallelism (FSDP), with batch size of 512, and context length of 2048. A.2 Evaluation Tasks We evaluate using lighteval (Habib et al., 2023) on suite of 14 evaluation datasets that includes the 8 used by FineWeb (Penedo et al., 2024) and additional datasets that are well-established and demonstrate above-chance performance and monotonic improvement during training. ARC-Challenge (Clark et al., 2018): dataset of 2,590 genuine grade-school level, multiplechoice science questions, designed to promote research in advanced question-answering. It challenges AI capabilities by requiring reasoning and comprehension beyond standard retrieval methods. ARC-Easy (Clark et al., 2018): Similar to ARC-Challenge, but consists of 7,787 easier questions. BoolQ (Clark et al., 2019): question answering dataset for yes/no questions containing 15,942 examples. These questions are naturally occurring and generated in unprompted and unconstrained settings. CommonsenseQA (Talmor et al., 2019): multiple-choice question answering dataset containing 12,102 questions requiring different types of commonsense knowledge. COPA (Roemmele et al., 2011): Designed to assess commonsense causal reasoning through 1000 questions where each question consists of premise and two alternatives. HellaSwag (Zellers et al., 2019): benchmark dataset for commonsense natural language inference that challenges state-of-the-art models with context and endings that are easy for humans but difficult for machines. MMLU (Hendrycks et al., 2021): benchmark evaluating capabilities across 57 subjects, including mathematics, history, and law. OpenbookQA (Mihaylov et al., 2018): Contains 5,957 multiple-choice questions designed to probe understanding of core science facts and their applications. PIQA (Bisk et al., 2020): Evaluates physical commonsense reasoning through everyday scenarios, with about 20,000 question-answer pairs. RACE-High (Lai et al., 2017): Contains 69,395 questions from 19,527 passages from English examinations for Chinese high school students. RACE-Middle (Lai et al., 2017): Consists of 28,293 questions from 8,718 passages targeted at middle school students. SciQ (Welbl et al., 2017): Contains 13,679 crowdsourced science exam questions covering topics such as physics, chemistry, and biology. SIQA (Sap et al., 2019): Contains over 38,000 multiple-choice questions about everyday social interactions. 25 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining WinoGrande (Sakaguchi et al., 2020): Contains 44,000 problems designed to evaluate commonsense reasoning in AI systems. A.3 Quantifying Conversational Content in Web-Scale Corpora To understand the proportion of conversational text in the web data, we conducted an experiment using classification task on sample of data from the C4 dataset. The goal was to assess what proportion of naturally occurring web data aligns with chat-based use cases. For this, we provided the GPT-4 model with classification task to label text as either conversational or non-conversational based on the following criteria for classification. Classification Task: Conversational vs. Non-Conversational Your task is to classify text as either conversational (1) or non-conversational (0). Conversational text (1) shows: Back-and-forth exchanges between participants Questions followed by relevant answers Non-conversational text (0) typically: Presents information in one-way format Lacks interaction between participants Contains formal or encyclopedic writing Focuses on describing or explaining without dialogue Rate each text as exactly 0 or 1. Return only the number. Examples: Text: The mitochondria is the powerhouse of the cell. It produces energy through cellular respiration. This process involves multiple steps including glycolysis and the Krebs cycle. Output: 0 Text: A: Hey, Im having trouble with my code. The function keeps returning null. B: Can you share the error message youre getting? A: Heres the stack trace: [error details] B: Ah, see the issue. You need to initialize the variable first. A: That fixed it, thanks! Output: 1 Text: Q: Whats the capital of France? A: The capital of France is Paris. Q: Whats its population? A: Paris has population of about 2.2 million people. Output: Text: To install Python, first download the installer from python.org. Run the executable and follow the installation wizard. Make sure to check the Add Python to PATH option during installation. Output: 0 Text: JavaScript was created by Brendan Eich in 1995 while he was working at Netscape Communications Corporation. The language was originally designed for client-side web development. Output: 0 26 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Text: User1: Did anyone solve the memory leak issue? User2: Yes, it was related to the event listeners User1: How did you fix it? User2: We added cleanup function in the useEffect hook User1: Got it, Ill try that Output: 1 Text: The Renaissance was period in European history marking the transition from the Middle Ages to modernity and covering the 15th and 16th centuries. Output: Text: - Can we push back the deadline? - need to check with the team first. When would you need it by? - Would next Friday work? - Yes, that should be fine. Ill update the project timeline. Output: 1 Classify the following text: The experiment was performed on random sample of 10,000 text examples from the C4 dataset, with final result indicating that approximately 2.7% of the sampled text was classified as conversational. This proportion is reflective of the prevalence of conversational data within large-scale corpus drawn from the web."
        },
        {
            "title": "B Additional Results",
            "content": "Figure 10 shows that BeyondWeb consistently leads throughout training, achieving the highest final accuracy at all scales. As shown in Figure 11, the 3B BeyondWeb model even surpasses most 8B baselines under the same token budget. Table 3 and Table 4 confirm broad 0-shot and 5-shot improvements across 14 tasks, with BeyondWeb outperforming other methods on most of benchmarks at each scale. Scale Dataset 1B (1TT) 3B (180BT) 8B (180BT) RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb ARC(C) ARC(E) BoolQ COPA CSQA Hella. MMLU OBQA PIQA RACE-H RACE-M SIQA SciQ Wino. Avg. 49.2 51.1 51.0 52.5 55.7 54.2 53.7 53.5 53.8 56.6 75.0 84.3 78.0 80.7 90.3 73.0 74.0 73.0 70.0 78. 51.9 51.3 56.1 56.4 56.4 31.0 32.1 31.9 33.8 34.9 34.0 34.6 36.8 36.2 39.4 42.4 43.5 44.4 43.2 44.5 50.8 55.3 56.8 58.8 66.4 35.4 37.0 35.4 39.9 41. 71.3 71.6 72.9 73.6 74.6 60.8 64.3 63.0 63.2 67.0 27.6 31.0 30.7 33.4 39.0 43.0 46.2 44.2 53.1 52.9 38.2 36.6 36.9 38.7 38.3 31.2 34.6 34.3 38.0 44. 33.3 37.9 38.1 43.3 48.9 56.4 59.9 60.0 68.5 71.5 61.9 64.2 65.3 72.3 76.5 63.0 65.6 62.6 65.0 67.4 64.2 68.8 64.6 65.4 73.0 72.0 70.0 73.0 80.0 80. 74.0 77.0 80.0 82.0 80.0 40.0 39.6 40.1 41.3 42.0 42.1 41.8 41.9 43.8 45.1 59.0 58.7 63.0 64.2 63.7 65.9 64.6 68.6 69.1 69.2 33.5 33.9 34.9 37.2 36. 35.3 36.8 37.0 38.9 39.6 35.8 36.8 38.6 39.6 41.6 38.6 36.4 40.2 41.4 43.6 74.4 72.6 75.8 76.6 76.9 76.2 74.7 77.4 77.3 78.8 37.7 39.8 38.6 43.2 44. 39.5 41.1 41.3 44.4 47.1 46.7 50.6 48.5 56.1 58.1 47.9 52.9 52.4 57.5 60.0 44.2 44.2 44.7 43.8 45.9 44.7 44.0 45.0 44.8 46.2 82.1 86.7 82.6 88.7 91. 83.9 90.0 84.3 89.1 94.4 55.7 55.1 56.8 58.2 58.8 59.0 58.5 58.6 59.5 61.9 52.3 53.4 53.8 57.2 58.8 54.7 56.3 56.8 59.2 61.7 Table 3: Performance across all tasks (0-shot) for different model scales and data curations. The best value at each scale is highlighted in bold, the second-best is underlined, and results where BeyondWeb outperforms all other methods are additionally shaded in blue . 27 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 10: Learning curves across model scales demonstrate our consistent performance advantages. The plots show training dynamics for 1B-1TT, 3B-180BT, and 8B-180BT configurations. Average Accuracy (%) is computed as the mean score across 14 standard benchmarks, averaged over both 0-shot and 5-shot settings. BeyondWeb (blue) maintains superior performance throughout training across all scales, achieving 57.4%, 60.8%, and 63.7% accuracy respectively. Scale Dataset 1B (1TT) 3B (180BT) 8B (180BT) RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb RPJ QA WRAP Cosmopedia Nemotron-Synth BeyondWeb ARC(C) ARC(E) BoolQ COPA CSQA Hella. MMLU OBQA PIQA RACE-H RACE-M SIQA SciQ Wino. Avg. 52.1 54.0 53.4 56.1 59.1 49.4 51.8 50.9 55.5 60. 53.5 52.8 52.6 54.4 55.3 90.5 92.2 90.8 93.1 95.6 71.0 75.0 68.0 73.0 75.0 51.7 50.9 55.1 55.9 55.9 32.1 33.3 33.7 35.3 36.1 44.1 45.1 46.3 45.3 50. 43.2 47.8 42.7 52.0 54.5 60.9 64.0 68.9 69.6 75.1 35.5 37.0 33.6 38.7 41.6 71.2 71.9 73.7 74.6 75.4 36.4 35.8 37.2 36.8 38.8 59.2 67.1 56.3 62.4 69. 31.1 30.8 37.4 38.4 43.9 34.8 37.4 40.8 44.4 49.3 36.8 41.9 44.9 48.9 54.7 66.1 69.0 72.2 74.2 79.2 69.9 72.7 76.1 79.3 82.1 60.0 68.9 63.6 67.6 74. 67.9 72.3 64.0 70.4 78.1 68.0 76.0 76.0 81.0 84.0 77.0 81.0 79.0 85.0 86.0 55.9 53.5 57.7 59.1 61.3 60.6 59.0 59.3 61.0 63.6 58.1 58.4 61.6 63.7 63. 65.5 64.0 67.7 69.0 69.0 34.2 36.1 36.2 38.2 38.6 36.7 38.9 38.5 41.1 41.5 36.4 36.2 39.6 43.2 44.4 40.4 40.2 43.0 42.2 47.6 74.8 73.8 76.2 77.1 77. 76.3 76.4 77.3 78.0 78.3 37.8 40.4 38.2 42.1 44.5 39.7 41.9 41.2 43.8 46.9 46.8 52.8 50.1 53.9 57.6 47.6 56.1 51.8 57.5 60.9 45.2 48.2 47.8 49.2 53. 49.3 50.0 49.6 50.1 54.4 92.7 93.6 93.4 95.8 96.1 94.6 95.4 94.8 96.0 96.8 54.5 55.4 54.1 57.1 56.2 56.5 56.8 58.2 58.2 59.0 54.7 57.1 57.7 60.5 62. 58.5 60.5 60.4 62.9 65.6 Table 4: Performance across all tasks (5-shot) for different model scales and data curations. The best value at each scale is highlighted in bold, the second-best is underlined, and results where BeyondWeb outperforms all other methods are additionally shaded in blue . 28 BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining Figure 11: Performance comparison showing BeyondWebs significant advantages over state-ofthe-art baselines across three model scales. Average Accuracy (%) is computed as the mean score across 14 standard benchmarks, averaged over both 0-shot and 5-shot settings. We highlight that the 1B BeyondWeb model outperforms all but one 3B baseline, and the 3B BeyondWeb model outperforms all but one 8B baseline, despite the 3B and 8B models being trained for the same token budget (180B tokens)."
        }
    ],
    "affiliations": [
        "DatologyAI"
    ]
}