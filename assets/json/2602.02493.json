{
    "paper_title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
    "authors": [
        "Zehong Ma",
        "Ruihan Xu",
        "Shiliang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 3 9 4 2 0 . 2 0 6 2 : r PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Zehong Ma 1 Ruihan Xu 1 Shiliang Zhang 1 Project Page: https://zehong-ma.github.io/PixelGen"
        },
        {
            "title": "Abstract",
            "content": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/ Zehong-Ma/PixelGen. Figure 1. This work shows that pixel diffusion with perceptual loss outperforms latent diffusion. (a) traditional two-stage latent diffusion denoises in the latent space, which is influenced by the artifacts of the VAE. (b) PixelGen introduces perceptual loss to encourage the diffusion model to focus on the perceptual manifold, enabling the pixel diffusion to learn meaningful manifold rather than the complex full image manifold. (c) PixelGen outperforms the latent diffusion models using only 80 training epochs on ImageNet without CFG. 1. Introduction Diffusion models (Ho et al., 2020; Song et al., 2020; Dhariwal & Nichol, 2021) have achieved remarkable success in high-fidelity image generation, offering exceptional quality and diversity. Research in this field generally follows two main directions: latent diffusion and pixel diffusion. Latent diffusion models (Rombach et al., 2022; Peebles & Xie, 2023; Ma et al., 2024; Labs, 2024) split generation into two stages. As illustrated in Figure 1(a), VAE first compresses images into latent space, and diffusion model then performs denoising in that space. The performance of 1State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University. latent diffusion methods is largely constrained by the VAEs, where the reconstruction quality limits the upper bound of the generation quality. Furthermore, the learned latent distribution can significantly affect the convergence speed of diffusion training (Yao & Wang, 2025; Leng et al., 2025). These works have demonstrated that VAEs introduce lowlevel artifacts and representational bottlenecks of the latent diffusion model. Pixel diffusion models avoid these limitations by modeling raw pixels directly. This end-to-end pipeline removes the need for latent representations and eliminates VAE-induced artifacts. However, it is difficult for the diffusion model to 1 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss no VAEs, and no auxiliary stages. We evaluate PixelGen on both class-to-image and text-toimage generation. PixelGen achieves leading FID score of 5.11 on ImageNet 256 without classifier-free guidance (CFG) using only 80 epochs. It outperforms the strong latent diffusion model REPA (Yu et al., 2024), which achieves an FID of 5.90 with 800 training epochs. Our pretrained text-to-image model also achieves an overall score of 0.79 on GenEval. These results show that pixel diffusion with perceptual loss has strong potential to outperform traditional two-stage latent diffusion method. In summary, our contributions are as follows: i) We propose PixelGen, simple end-to-end pixel diffusion framework with two complementary perceptual losses, simplifying the generative pipeline while improving performance. It requires no latent representations, no VAEs, and no auxiliary stages. ii) We demonstrate that pixel diffusion with perceptual losses can outperform the two-stage latent diffusion on ImageNet without CFG, highlighting pixel diffusion as simpler yet more powerful generative paradigm. 2. Related Work This work is closely related to latent diffusion, pixel diffusion, and perceptual supervision. This section briefly reviews recent works. Latent Diffusion. Latent diffusion trains diffusion models in compact latent space learned by VAE (Rombach et al., 2022). Compared to raw pixel space, the latent space significantly reduces spatial dimensionality, easing learning difficulty and computational cost (Rombach et al., 2022; Chen et al., 2024). Consequently, VAEs have become fundamental component in modern diffusion models (Peebles & Xie, 2023; Karras et al., 2024; Yue et al., 2024; Wang et al., 2024; Teng et al., 2024; Song et al., 2025; Gao et al., 2023b; Yao et al., 2024; Gao et al., 2023a). However, training VAEs often involves adversarial objectives, which complicate the overall pipeline (Wang et al., 2025a). Poorly trained VAEs can produce decoding artifacts (Zhou et al., 2024; Chen et al., 2025b) and introduce bottleneck, limiting the generalization quality of latent diffusion models. Early latent diffusion models mainly used U-Net-based architectures. The pioneering DiT (Peebles & Xie, 2023) introduced transformers into diffusion models, replacing the U-Net (Bao et al., 2023; Dhariwal & Nichol, 2021). SiT (Ma et al., 2024) further validated the DiT with linear flow diffusion. Subsequent works explore enhancing latent diffusion through representation alignment and joint optimization. REPA (Yu et al., 2024) and REG align intermediate features with pretrained DINOv2 (Oquab et al., 2023) model to learn better semantics. REG (Wu et al., 2025b) entangles latents with high-level class token from DINOv2 Figure 2. Illustration of different manifolds within the pixel space. The image manifold is large manifold containing both perceptually significant information and imperceptible signals. The perceptual manifold contains perceptually important signals, providing better target for pixel space diffusion. P-DINO and LPIPS are the two complementary perceptual supervision utilized in PixelGen. learn high-dimensional and complex velocity field in pixel space. Recently, JiT (Li & He, 2025) has improved pixel diffusion by predicting the clean image, i.e., x-prediction, rather than velocity. It simplifies the prediction target and significantly improves generation quality. Despite recent progress, clear performance gap remains between JiT and strong latent diffusion models. It is still too complex to predict the full image manifold in pixel space, which contains substantial perceptually insignificant components, e.g., sensor noise and imperceptible details (Rombach et al., 2022). As demonstrated in LDM (Rombach et al., 2022), learning such signals provides little perceptual benefit but increases optimization difficulty and training cost. To pursue more efficient optimization, pixel diffusion should prioritize meaningful perceptual manifold, rather than the complex full image manifold. Based on this insight, we propose PixelGen, new pixel diffusion framework enhanced by perceptual supervision as illustrated in Figure 1(b). Following JiT (Li & He, 2025), PixelGen adopts the x-prediction formulation, where the diffusion model predicts the image directly from noisy inputs. To encourage the model to focus on the important perceptual manifold rather than the full image manifold, we introduce two complementary perceptual losses on the predicted image. Concretely, we incorporate an LPIPS loss (Zhang et al., 2018) to better capture local textures and fine-grained details. Additionally, we propose DINO-based perceptual loss (P-DINO) to improve global semantics. As shown in Figure 2 and Figure 4, these two perceptual losses guide the diffusion model toward the perceptual manifold, yielding sharper textures and better global semantics compared to the baselines blurry images. PixelGen generates images directly in pixel space, requiring no latent representations, 2 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss for denoising. As representation learning method, REPA is compatible with our framework and is applied in both our baseline and PixelGen. REPA-E (Leng et al., 2025) attempts to jointly optimize the VAE and DiT in an end-to-end fashion. However, this approach may suffer from training collapse with diffusion loss, as the continually changing latent space leads to unstable denoising targets. In contrast, pixel diffusion denoises in fixed pixel space, ensuring consistent targets and stable training. Some other works try to improve the autoencoder to accelerate the diffusion training, such as VAVAE (Yao & Wang, 2025) and RAE (Zheng et al., 2025a). Recent DDT (Wang et al., 2025b) proposes the decoupled diffusion transformer and achieves leading performance. Pixel Diffusion. Pixel diffusion has progressed much more slowly than its latent counterparts due to the vast dimensionality of pixel space (Dhariwal & Nichol, 2021; Kingma & Gao, 2023; Teng et al., 2023; Chang et al., 2022; Hoogeboom et al., 2023). Early methods split the diffusion process into multiple resolution stages. Relay Diffusion (Teng et al., 2023) trains separate models for each scale, leading to higher cost and two-stage optimization. Pixelflow (Chen et al., 2025b) uses one model across all scales and needs complex denoising schedule that slows down inference. Alternative approaches explore different model architectures. FractalGen (Li et al., 2025) builds fractal generative models by recursively applying atomic modules, achieving selfsimilar pixel-level generation. TarFlow (Zhai et al., 2024) and FARMER (Zheng et al., 2025b) introduce transformerbased normalizing flows to directly model and generate pixels. Recent PixNerd (Wang et al., 2025a) employs DiT to predict neural field parameters for each patch, rendering pixel velocities akin to test-time training. EPG (Lei et al., 2025) newly introduces the self-supervised pretraining for pixel diffusion. DeCo (Ma et al., 2025), DiP (Chen et al., 2025c), and PixelDiT (Yu et al., 2025) propose to introduce an additional pixel decoder to learn the hard high-frequency signals. JiT (Li & He, 2025) predicts the clean image rather than velocity, encouraging the diffusion model to learn the low-dimensional image manifold. Perceptual Supervision. Perceptual supervision uses feature losses instead of pixel-wise losses. It computes the loss in the feature space of frozen pretrained model. It makes training focus on perceptually significant signals, not exact RGB matches. Such losses are common in autoencoders and GANs, since they can reduce blur and recover sharp details. LPIPS (Zhang et al., 2018) loss is popular choice and mainly improves local textures. Adversarial losses (Sauer et al., 2022) can further improve realism, but they are often unstable and even harder to optimize for pixel diffusion, so we do not adopt them. Recent self-supervised encoders can provide semantic features, such as the DINOv2 (Oquab et al., 2023). With pretrained DINOv2-base encoder, perceptual losses can also enforce global structure and object-level consistency. Following the insight that modeling every pixel equally is unnecessary and costly (Rombach et al., 2022), PixelGen combines local LPIPS and global DINO-based loss to guide pixel diffusion toward simpler perceptual manifold. 3. Methodology In this section, we introduce PixelGen, simple pixel diffusion framework with perceptual supervision. We first present an overview of PixelGen in Section 3.1. Then, we introduce two complementary perceptual losses: an LPIPS loss for local patterns in Section 3.2 and P-DINO loss for global semantics in Section 3.3. 3.1. Overview In pixel diffusion, the models output can be defined in any space: noise (ϵ), velocity (v), or image (x). We denote these outputs as ϵ, x, v-prediction, separately. Recently, JiT (Li & He, 2025) simplifies the prediction target by replacing the widely used v-prediction with the simple x-prediction, which substantially improves the generation performance of pixel diffusion. Despite the improvement of simplified x-prediction in JiT (Li & He, 2025), it remains challenging for pixel diffusion models to model the full image manifold. As demonstrated in LDM (Rombach et al., 2022), the raw image is still too high-dimensional and contains many perceptually insignificant components. The diffusion model has to learn sensor noise or imperceptible details, which distracts the model from learning perceptually significant components. Our key insight is that pixel diffusion should focus on the perceptual manifold rather than the entire image manifold. Fortunately, the x-prediction paradigm enables direct addition of perceptual supervision on pixel diffusion models, thus improving the generation of high-quality images. We briefly review the background of image prediction and flow matching in the following parts, and then provide an overview of the proposed perceptual supervision method. Image Prediction. Following JiT (Li & He, 2025), we adopt the image prediction formulation, i.e., x-prediction, to provide stable training target across various noise levels. Given noisy image xt at time [0, 1], the diffusion transformer netθ predicts the image xθ as: xθ = netθ(xt, t, c), (1) where denotes the conditional information, such as class labels or text embeddings. The noisy input xt at time is constructed via linear interpolation between the groundtruth image and Gaussian noise ϵ (0, I): xt = tx + (1 t)ϵ. (2) 3 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Velocity Conversion. To retain the sampling advantages of flow matching, we convert the predicted image xθ into velocity vθ following JiT (Li & He, 2025). The predicted velocity vθ is given by: vθ = xθ xt 1 , while the ground-truth velocity can be represented as: xt 1 = ϵ. = The resulting flow matching objective is: LFM = Et,x,ϵ vθ v2 = Et,x,ϵ (cid:13) (cid:13) (cid:13) (cid:13) xθ 1 (cid:13) 2 (cid:13) (cid:13) (cid:13) . (3) (4) (5) This formulation combines the stability of x-prediction with the favorable sampling advantages of flow matching. Perceptual Supervision. Although x-prediction simplifies the training objective, the image manifold still contains many perceptually irrelevant signals. To guide PixelGen toward perceptually meaningful manifold, we introduce two complementary perceptual losses: LPIPS loss for local textures and fine-grained details, and Perceptual DINO (P-DINO) loss for global semantics. Together with the widely used REPA loss (Yu et al., 2024), which encourages alignment of intermediate representations, the final training objective is: = LFM + λ1LLPIPS + λ2LP-DINO + LREPA, (6) where λ1 and λ2 balance the diffusion objective and perceptual supervision. This end-to-end training enables PixelGen to learn simple perceptual manifold without additional autoencoders or auxiliary stages. 3.2. LPIPS Loss High-quality image generation requires sharp details and realistic local textures, which are not well captured by pixelwise losses. To address this, we incorporate the Learned Perceptual Image Patch Similarity (LPIPS) loss (Zhang et al., 2018). LPIPS measures perceptual similarity by comparing multi-level feature activations extracted from frozen pre-trained VGG network fVGG. The LPIPS loss can be represented as: LLPIPS = (cid:88) (cid:13) (cid:13)wl (cid:0)f VGG(xθ) VGG(x)(cid:1)(cid:13) 2 2 , (cid:13) (7) where indexes the VGG layers, and wl denotes the learned per-channel weighting vector for layer l. For simplicity, spatial averaging over feature maps and channel-wise normalization are omitted in the above formulation. By minimizing LLPIPS, PixelGen learns to reconstruct perceptually important local patterns instead of exact pixel values, leading to sharper edges and more realistic textures. 4 Figure 3. Overview of PixelGen. The diffusion model directly predicts the image instead of velocity or noise ϵ to simplify the prediction target. flow-matching diffusion loss is retained to keep the advantages of flow matching via velocity conversion. Two complementary perceptual losses are introduced to encourage the diffusion model to focus on the perceptual manifold. 3.3. P-DINO Loss While LPIPS provides strong local perceptual supervision, local patterns alone are insufficient for high-fidelity generation. Therefore, we further introduce Perceptual DINO (P-DINO) loss to facilitate global semantics. Specifically, we extract patch-level features using frozen DINOv2-B (Oquab et al., 2023) encoder fDINO. Let DINO() denote the feature of patch p. We align the predicted image xθ and the ground-truth image using cosine similarity: LP-DINO = 1 (cid:88) pP (cid:0)1 cos(cid:0)f DINO(xθ), DINO(x)(cid:1)(cid:1) , (8) where denotes the set of all patches. The P-DINO loss provides global semantic guidance by aligning high-level representations, encouraging the generated image to be consistent with the overall scene layout and object semantics. Together with local LPIPS loss, it enables PixelGen to balance global semantics and local realism. 3.4. Empirical Analysis We conduct empirical analysis to study the effect of perceptual supervision in PixelGen. There are two important empirical observations. First, two perceptual losses complementarily improve generation quality. Second, perceptual losses should not be applied at high-noise time steps. Observation 1 Perceptual supervision improves pixel diffusion by enhancing local details and global semantics. Starting from baseline pixel diffusion model, we progressively introduce the LPIPS loss and the P-DINO loss. As shown in Figure 4, the baseline model produces blurry images with weak structural consistency. After adding the LPIPS loss, local textures become sharper, and fine details PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Table 1. Results of 200K training steps on ImageNet 256 without classifier-free guidance (CFG). All models use Euler sampler with 50 inference steps. The REPA loss is utilized for all models except for DiT-L/2 and PixelFlow-L/4. Latent diffusion models require an additional VAE with 86M parameters. Method Params FID IS Prec. Rec. e l P DiT-L/2 REPA-L/2 JiT-L/2 DDT-L/2 PixDDT-L/16 PixNerd-L/16 PixelFlow-L/4 DeCo-L/16 JiT (Baseline) PixelGen-L/16 36.5 458+86M 41.93 87.3 458+86M 16.14 458+86M 16.09 90.5 458+86M 10.00 112.9 36.2 43.0 24.7 48.4 64.1 131.7 459M 46.37 459M 37.49 459M 54.33 426M 31.35 459M 23.67 7.53 459M 0.52 0.65 0.63 0.67 - 0.46 0.43 0.51 0.55 0. 0.59 0.63 0.64 0.65 0.63 0.62 0.58 0.65 0.63 0.60 Recall (Kynkaanniemi et al., 2019). For text-to-image generation in Section 4.3, we report results on GenEval (Ghosh et al., 2023). 4.1. Comparison with Baselines Setup. We first compare PixelGen with latent diffusion and pixel diffusion baselines under the same settings. All models are trained on ImageNet at resolution of 256 256 for 200K training steps with DiT-L backbone (Peebles & Xie, 2023). Following prior works (Peebles & Xie, 2023; Wang et al., 2025a; Ma et al., 2025), we use global batch size of 256, AdamW optimizer with constant learning rate of 1 104, and employ log-normal timestep sampling. To ensure fair comparison, we apply REPA loss (Yu et al., 2024) to all models except for DiT-L/2 and PixelFlow-L/4 (Chen et al., 2025b). The patch size of DiT is set to 16. Its worth noting that we use JiT (Li & He, 2025) with REPA loss as our baseline. For inference, we use 50 Euler steps without classifier-free guidance (Ho & Salimans, 2022) (CFG). All experiments are conducted on node with 8H800 GPUs. Detailed Comparisons. Table 1 shows that PixelGen demonstrates superior performance compared to both latent and pixel-based diffusion models. First, compared to the JiT baseline (Li & He, 2025), PixelGen significantly reduces the FID from 23.67 to 7.53. This substantial improvement demonstrates that our perceptual supervision effectively guides the model to learn meaningful perceptual manifold, simplifying the learning of pixel diffusion. PixelGen also surpasses other recent pixel diffusion methods such as PixNerd (Wang et al., 2025a) and DeCo (Ma et al., 2025) under the same evaluation protocol. Second, PixelGen outperforms strong latent diffusion models under the same 200K-step budget. Specifically, PixelGen achieves an FID of 7.53, compared with 10.00 for DDTL/2 (Wang et al., 2025b) and 16.14 for REPA-L/2 (Yu et al., Figure 4. Effectiveness of perceptual supervision in PixelGen. LPIPS and P-DINO losses are progressively added to baseline pixel diffusion model. The LPIPS loss improves local texture fidelity, while P-DINO further enhances global semantics. are better preserved. This indicates that LPIPS effectively emphasizes perceptually important local patterns. When the P-DINO loss is further introduced, the generated images exhibit improved global structure and better semantics. These qualitative improvements are supported by quantitative results. The baseline model achieves an FID of 23.67 on ImageNet without classifier-free guidance. With LPIPS loss, the FID decreases to 10.00. After adding the P-DINO loss, it further drops to 7.46. This confirms that LPIPS and P-DINO provide complementary supervision. LPIPS focuses on local perceptual fidelity, while P-DINO enhances global semantics. Together, they guide the diffusion model toward perceptually meaningful manifold. Observation 2 Applying perceptual losses at high-noise time steps is risky, as it can reduce sample diversity. We observe that introducing perceptual losses at early diffusion time steps, where the noise level is high, degrades the recall metric. Based on this observation and the ablation in Table 6 (d), we disable perceptual losses during the first 30% of high-noise time steps and activate them only in the later 70% low-noise steps. We only apply them when the sample is closer to the clean image. This strategy improves recall and sample diversity while keeping FID and precision almost unchanged. 4. Experiments We conduct baseline comparisons in Section 4.1 and ablation studies in Section 4.4 on ImageNet 256 256 with 200k training steps. For class-to-image generation, we provide system-level comparison on ImageNet 256 256 in Section 4.2, and report FID (Heusel et al., 2017), Inception Score (IS) (Salimans et al., 2016), Precision, and 5 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Table 2. Class-to-image generation performance without CFG on ImageNet. PixelGen utilizes the Heun sampler (Heun et al., 1900) with 50 inference steps like JiT, while latent diffusion models adopt Euler sampler with 250 steps. Table 3. Class-to-image generation performance with CFG on ImageNet. Following JiT (Li & He, 2025), PixelGen utilizes the Heun sampler (Heun et al., 1900) with 50 inference steps, while latent diffusion models adopt Euler sampler with 250 steps. Method Epochs FID IS Prec. Rec. Method Epochs FID IS Prec. Rec. LDM-4 DiT-XL/2 SiT-XL/2 FlowDCN FasterDiT DDT-XL/2 DDT-XL/2 MDT REPA-XL/2 MaskDiT ADM PixelFlow-XL/4 PixNerd-XL/16 DeCo-XL/16 PixelGen-XL/16 t e 170 1400 1400 400 400 80 400 1300 800 400 320 320 320 80 10.56 103.5 121.5 9.62 131.7 8.61 122.5 8.36 131.3 7.91 135.2 6.62 154.7 6.27 143.0 6.23 157.8 5.90 177.9 5.69 - 10.94 12.23 103.3 88.9 15.61 88.2 14.88 5.11 159.2 0.71 0.67 0.68 0.69 0.67 0.69 0.69 0.71 0.70 0.74 0.69 0.63 0.59 0.60 0. 0.62 0.67 0.67 0.65 0.69 0.67 0.67 0.65 0.69 0.60 0.63 0.66 0.68 0.68 0.63 2024). This is critical milestone, as it demonstrates that an end-to-end pixel diffusion model can beat two-stage latent diffusion models under an identical training setting, without relying on pre-trained VAE or other complex techniques. 4.2. Class-to-Image Generation Setup. For class-to-image generation experiments on ImageNet, we train the PixelGen-XL with 676M parameters at 256256 resolution for 160 epochs. During inference, we use Heun (Heun et al., 1900) sampler with 50 inference steps following JiT. The guidance interval (Kynkaanniemi et al., 2024) is adopted when CFG is enabled. Results without CFG. Table 2 reports class-to-image generation performance without CFG. This setting is particularly challenging, as it directly reflects the models ability to capture the underlying image distribution. Under this setting, PixelGen-XL/16 achieves an FID of 5.11 using only 80 training epochs, outperforming the latent diffusion models that require substantially longer training, such as REPA-XL/2 (FID 5.90 with 800 epochs) and DDT-XL/2 (FID 6.27 with 400 epochs). Compared with other pixel diffusion models, PixelGen exhibits clear advantage. For example, DeCoXL/16 achieves FID of 14.88 with 320 training epochs, whereas PixelGen reduces FID by more than 60% using only one quarter of training cost. Besides, PixelGen also achieves competitive inception score, precision, and recall. These results show that by utilizing perceptual supervision, pixel diffusion can capture the true image distribution more precisely than latent diffusion models, which are constrained by the information loss and reconstruction artifacts introduced by VAEs. This comparison highlights that pixel diffusion e l P MaskDiT DiT-XL/2 SiT-XL/2 FlowDCN MDT REPA-XL/2 REPA-XL/2 ADM FARMER SimpleDiffusion RDM FractalMAR-H EPG-XL/16 PixelFlow-XL/4 DiP-XL/16 PixNerd-XL/16 DeCo-XL/16 JiT-H/16 PixelGen-XL/16 1600 1400 1400 400 1300 200 800 400 320 800 400 600 800 320 320 320 320 600 2.28 2.27 2.06 2.00 1.79 1.96 1.42 4.59 3.60 2.44 1.99 6.15 2.04 1.98 1.98 1.95 1.90 1.86 1.83 276.6 278.2 284.0 263.1 283.0 264.0 305.7 186.7 269.2 256.3 260.4 348.9 283.2 282.1 282.9 298.0 303.0 303.4 293.6 0.80 0.83 0.83 0.82 0.81 0.82 0.80 0.82 0.81 - 0.81 0.81 0.80 0.81 0.80 0.80 0.80 0.78 0. 0.61 0.57 0.59 0.58 0.61 0.60 0.64 0.52 0.51 - 0.58 0.46 0.61 0.60 0.62 0.60 0.61 0.62 0.63 models with perceptual losses can outperform latent diffusion models. Results with CFG. Table 3 presents the results using classifier-free guidance (CFG). With only 160 training epochs, PixelGen achieves an FID of 1.83 using 50 Heun inference steps. Although performance gap remains compared to the leading latent model REPA-XL/2 (Yu et al., 2024), PixelGen demonstrates remarkable training efficiency and strong potential. Notably, despite being trained for only 160 epochs, PixelGen outperforms recent pixel diffusion models such as DeCo-XL/16 (Ma et al., 2025) and JiT-H/16 (Li & He, 2025), which require 320 and 600 training epochs. As simple pixel diffusion framework, PixelGen demonstrates great potential in end-to-end image generation. We believe that the performance can be further improved by designing more effective pixel-based samplers, CFG methods, and other training strategies. 4.3. Text-to-Image Generation Setup. For text-to-image generation, we trained our model on approximately 36M pretraining images and 60k highquality instruction-tuning data from BLIP3o (Chen et al., 2025a). We adopt Qwen3-1.7B (Yang et al., 2025) as the text encoder. To improve the alignment of frozen text features (Fan et al., 2024), we jointly train several transformer layers on the frozen text features similar to Fluid (Fan et al., 2024). The total batch size is 1536 for 256 256 resolution 6 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Figure 5. Qualitative results of text-to-image generation of PixelGen. All images are 512512 resolution. Table 4. Text-to-image generation on GenEval (Ghosh et al., 2023) at 512512 resolution. Method Diffusion Params GenEval Sin.Obj. Two.Obj Counting Colors Pos Color.Attr. Overall PixArt-α (Chen et al., 2023) SD3 (Esser et al., 2024) FLUX.1-dev (Labs, 2024) DALL-E 3 (Betker et al., 2023) OmniGen2 (Wu et al., 2025a) PixelFlow-XL/4 (Chen et al., 2025b) PixNerd-XXL/16 (Wang et al., 2025a) PixelGen-XXL/16 0.6B 8B 12B - 4B 882M 1.2B 1.1B 0.98 0.98 0.99 0.96 1 - 0.97 0.99 0.50 0.84 0.81 0.87 0.95 - 0.86 0. 0.44 0.66 0.79 0.47 0.64 - 0.44 0.59 0.80 0.74 0.74 0.83 0.88 - 0.83 0.90 0.08 0.40 0.20 0.43 0.55 - 0.71 0.70 0.07 0.43 0.47 0.45 0.76 - 0.53 0.70 0.48 0.68 0.67 0.67 0.80 0.60 0.73 0.79 pretraining and 512 for 512 512 resolution pretraining. Following previous works (Wang et al., 2025a), we pretrain PixelGen-XXL on 256 256 resolution for 200K steps and pretrain on 512 512 resolution for 80K steps. We further fine-tune the pretrained PixelGen-XXL on BLIP3o-60k with 40k steps at the 512 512 resolution. We adopt the gradient clip to stabilize training. The entire training takes about 6 days on 8 H800 GPUs. We use the Adams-2nd solver with 25 steps as the default choice for sampling. The CFG scale is set to 4.0. Main Results. We evaluate the scalability and generalization of PixelGen on the challenging text-to-image task. Quantitative results on the GenEval are reported in Table 4. PixelGen-XXL achieves an overall score of 0.79 on GenEval, matching or surpassing recent large-scale diffusion models such as FLUX.1-dev (Labs, 2024), despite using fewer parameters. Notably, PixelGen outperforms recent pixel diffusion methods such as PixNerd by significant margin, highlighting the advantage of pixel diffusion with perceptual supervision. Overall, the text-to-image results confirm that PixelGen generalizes well beyond classto-image generation on ImageNet, providing simple and effective pixel diffusion framework for large-scale end-toend generation. Table 5. Effectiveness of each component. Method Baseline (JiT) + LPIPS Loss + P-DINO Loss + Noise-Gating FID 23.67 10.00 7.46 7. IS 64.13 113.16 137.95 131.70 Prec. 0.55 0.70 0.73 0.72 Rec. 0.63 0.59 0.58 0.60 4.4. Ablation Experiments We study the key components of PixelGen on ImageNet 256 256 under the same setup as Section 4.1. We first ablate each component in the framework, and then analyze the main hyperparameters within each component. Effectiveness of each component. Table 5 summarizes the contribution of each component. Starting from the JiT baseline, adding LPIPS loss yields large performance gain in FID and IS. This supports our claim that local perceptual supervision is crucial in pixel space. Adding P-DINO loss enhances the global structure and brings further improvements. However, adding perceptual constraints on all timesteps may reduce sample diversity, resulting in lower recall. To mitigate this, we adopt noise-gating strategy during training. It disables perceptual losses at early 30% timesteps with high noise. The noise-gating strategy recov7 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Figure 6. Qualitative results of class-to-image generation of PixelGen with CFG. All images are 256256 resolution. Table 6. Ablation experiments on perceptual losses and hyper-parameters. Text in blue background: the hyper-parameter that we adopt. (a) Weight λ1 of LPIPS Loss (b) Weight λ2 of P-DINO Loss Weight FID IS Prec. Rec. Weight FID IS Prec. Rec. 0.05 0.1 0.5 1.0 10.89 10.00 9.36 10.12 106.95 113.16 122.34 117. 0.68 0.70 0.71 0.71 0.61 0.59 0.58 0.57 0.005 0.01 0.02 0.04 8.11 7.46 6.84 6.62 128.86 137.95 149.23 157.78 0.72 0.73 0.74 0. 0.59 0.58 0.57 0.57 (c) Selected DINO Layer for P-DINO Loss. (d) Threshold of Noise-Gating Strategy Depth 6 9 12 6&9&12 FID 12.65 10.01 7.46 10.01 IS Prec. Rec. 95.92 111.51 137.95 111.50 0.68 0.71 0.73 0. 0.58 0.58 0.58 0.58 Threshold FID IS Prec. Rec. 0.0 0.1 0.3 0. 7.46 7.42 7.53 10.72 137.95 136.95 131.71 109.50 0.73 0.72 0.72 0.69 0.58 0.58 0.60 0.60 ers recall while keeping other metrics almost unchanged. Loss weight of LPIPS. We vary the weight λ1 of LPIPS loss in Table 6(a). small weight of 0.05 is too weak and leads to worse FID, while large weight of 0.5 or 1.0 may slightly hurt recall. We set λ1 = 0.1 as good trade-off. Loss weight of P-DINO. We ablate the weight λ2 of PDINO loss in Table 6(b). Increasing λ2 generally improves FID and IS, but large weight may reduce recall. We choose λ2 = 0.01 by balancing quality and diversity. Selected DINO layer. Table 6(c) compares different feature depths of DINOv2-B for P-DINO loss. Shallow layers are less effective, since they mainly encode low-level appearance. The last layer of 12 performs best, indicating that P-DINO loss benefits from high-level semantic features instead of low-level features. Using multiple layers introduces conflicting supervision and performs poorly. Threshold of Noise-Gating Strategy. Table 6(d) studies the threshold of noise-gating strategy. When the threshold is set to 0.0, perceptual losses are applied at all time steps, which gives strong quality but lower diversity. small threshold of 0.1 has limited effect, where the perceptual losses are disabled at the first 10% high-noise timesteps. Setting the threshold to 0.3 achieves better balance, where perceptual losses are only used at the last 70% timesteps with low noise. very large threshold of 0.6 removes too much supervision, substantially hurting FID and IS. 5. Conclusions and Future Works In this work, we introduce PixelGen, simple end-to-end pixel diffusion framework with two complementary perceptual losses, where LPIPS for local fidelity and P-DINO for global semantics. PixelGen encourages the model to focus on the perceptual manifold rather than the complex full image manifold. It substantially improves generation performance, allowing pixel diffusion to outperform latent diffusion models without CFG and achieve competitive performance with CFG. Future work includes developing more effective pixel-space samplers or CFG strategies, and incorporating richer perceptual objectives like adversarial losses. 8 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss"
        },
        {
            "title": "Acknowledgements",
            "content": "We greatly thank MiraclePlus for the support of GPUs."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Bao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu, J. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22669 22679, 2023. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W., Dhariwal, P., Chu, C., Jiao, Y., and Ramesh, A. Improving image generation with better captions. OpenAI Technical Report, 2023. URL https://cdn.openai.com/ papers/dall-e-3.pdf. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-α: Fast training of diffusion transformer for photorealistic textto-image synthesis, 2023. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. Chen, J., Xu, Z., Pan, X., Hu, Y., Qin, C., Goldstein, T., Huang, L., Zhou, T., Xie, S., Savarese, S., et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Fan, L., Li, T., Qin, S., Li, Y., Sun, C., Rubinstein, M., Sun, D., He, K., and Tian, Y. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2316423173, 2023a. Gao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2316423173, 2023b. Gao, Y., Gong, L., Guo, Q., Hou, X., Lai, Z., Li, F., Li, L., Lian, X., Liao, C., Liu, L., et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Gong, L., Hou, X., Li, F., Li, L., Lian, X., Liu, F., Liu, L., Liu, W., Lu, W., Shi, Y., et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. Heun, K. et al. Neue methoden zur approximativen integration der differentialgleichungen einer unabhangigen veranderlichen. Z. Math. Phys, 45:2338, 1900. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Chen, S., Ge, C., Zhang, S., Sun, P., and Luo, P. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025b. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Chen, Z., Zhu, J., Chen, X., Zhang, J., Hu, X., Zhao, H., Wang, C., Yang, J., and Tai, Y. Dip: Taming diffusion models in pixel space. arXiv preprint arXiv:2511.18822, 2025c. Hoogeboom, E., Heek, J., and Salimans, T. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. 9 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the training In Proceedings of the dynamics of diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36:65484 65516, 2023. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Kynkaanniemi, T., Aittala, M., Karras, T., Laine, S., Aila, T., and Lehtinen, J. Applying guidance in limited interval improves sample and distribution quality in diffusion models. arXiv preprint arXiv:2404.07724, 2024. Labs, B. F. Flux. black-forest-labs/flux, 2024. https://github.com/ Lei, J., Liu, K., Berner, J., Yu, H., Zheng, H., Wu, J., and Chu, X. There is no vae: End-to-end pixel-space generative modeling via self-supervised pre-training. arXiv preprint arXiv:2510.12586, 2025. Leng, X., Singh, J., Hou, Y., Xing, Z., Xie, S., and Zheng, L. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. Li, T. and He, K. Back to basics: Let denoising generative models denoise, 2025. URL https://arxiv.org/ abs/2511.13720. Li, T., Sun, Q., Fan, L., and He, K. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. Liao, C., Liu, L., Wang, X., Luo, Z., Zhang, X., Zhao, W., Wu, J., Li, L., Tian, Z., and Huang, W. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Ma, Z., Wei, L., Wang, S., Zhang, S., and Tian, Q. Deco: Frequency-decoupled pixel diffusion for end-to-end image generation. arXiv preprint arXiv:2511.19365, 2025. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., ElNouby, A., et al. Dinov2: Learning robust visual features 10 without supervision. arXiv preprint arXiv:2304.07193, 2023. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv:2010.02502, October 2020. URL https://arxiv.org/abs/2010.02502. Song, T., Feng, W., Wang, S., Li, X., Ge, T., Zheng, B., and Wang, L. Dmm: Building versatile image generation model via distillation-based model merging. arXiv preprint arXiv:2504.12364, 2025. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Teng, J., Zheng, W., Ding, M., Hong, W., Wangni, J., Yang, Z., and Tang, J. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. Teng, Y., Wu, Y., Shi, H., Ning, X., Dai, G., Wang, Y., Li, Z., and Liu, X. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023b. PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Wang, S., Li, Z., Song, T., Li, X., Ge, T., Zheng, B., and Wang, L. Exploring dcn-like architecture for fast image generation with arbitrary resolution. Advances in Neural Information Processing Systems, 37:8795987977, 2024. Zhai, S., Zhang, R., Nakkiran, P., Berthelot, D., Gu, J., Zheng, H., Chen, T., Bautista, M. A., Jaitly, N., and Susskind, J. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025a. Zheng, G., Zhao, Q., Yang, T., Xiao, F., Lin, Z., Wu, J., Deng, J., Zhang, Y., and Zhu, R. Farmer: Flow autoregressive transformer over pixels. arXiv preprint arXiv:2510.23588, 2025b. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. Wang, S., Gao, Z., Zhu, C., Huang, W., and Wang, L. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025a. Wang, S., Tian, Z., Huang, W., and Wang, L. Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025b. Wang, Z., Bai, L., Yue, X., Ouyang, W., and Zhang, Y. Native-resolution image synthesis. arXiv preprint arXiv:2506.03131, 2025c. Wu, C., Zheng, P., Yan, R., Xiao, S., Luo, X., Wang, Y., Li, W., Jiang, X., Liu, Y., Zhou, J., Liu, Z., Xia, Z., Li, C., Deng, H., Wang, J., Luo, K., Zhang, B., Lian, D., Wang, X., Wang, Z., Huang, T., and Liu, Z. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025a. Wu, G., Zhang, S., Shi, R., Gao, S., Chen, Z., Wang, L., Chen, Z., Gao, H., Tang, Y., Yang, J., et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025b. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yao, J. and Wang, X. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Yao, J., Wang, C., Liu, W., and Wang, X. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Yu, Y., Xiong, W., Nie, W., Sheng, Y., Liu, S., and Luo, J. Pixeldit: Pixel diffusion transformers for image generation. arXiv preprint arXiv:2511.20645, 2025. Yue, X., Wang, Z., Lu, Z., Sun, S., Wei, M., Ouyang, W., Bai, L., and Zhou, L. Diffusion models need visual priors for image generation. arXiv preprint arXiv:2410.08531, 2024. PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss A. More Implementary Details A.1. Baseline Comparisons In this subsection, we summarize the settings used for all baseline comparisons. In the baseline comparisons, all diffusion models are trained on ImageNet at 256256 resolution for 200k iterations using large DiT variant. Following previous works (Peebles & Xie, 2023; Wang et al., 2025a), we use global batch size of 256 and the AdamW optimizer with constant learning rate of 1e-4. Both baseline and PixelGen adopt SwiGLU (Touvron et al., 2023b;a), RoPE2d (Su et al., 2024), and RMSNorm, and are trained with lognorm sampling and REPA (Yu et al., 2024). The patch size of DiTs input is set to 16 for both baseline and our PixelGen. The only modification on the baseline is to add two complementary perceptual losses. For inference, we use 50 Euler steps without classifier-free guidance (Ho & Salimans, 2022) (CFG) for all models except PixelFlow (Chen et al., 2025b), which requires 100 steps. The timeshift is set to 1.0 for all experiments in Table 1. We also report results for the two-stage JiT-L/2 that requires VAE. For comprehensive comparison, we integrate DDT (Wang et al., 2025b) into the pixel diffusion to form PixDDT. A.2. Class-to-Image Generation This subsection describes the more implementation details for class-to-image generation. The batch size and learning rate follow the default settings previously described. We use global batch size of 256 and the AdamW optimizer with constant learning rate of 1e-4. The time sampler uses logit-normal distribution over t: logit(t)N (0.8, 0.82), which aligns with JiT (Li & He, 2025). We train the PixelGen-XL for 160 epochs, and use an autograd operation to balance gradients between flow-matching loss and perceptual losses after 80 epochs. We set the CFG scale to 2.25. The guidance interval (Kynkaanniemi et al., 2024) is set to (0.1, 0.9). For evaluation, we use Heun sampler with 50 inference steps following JiT (Li & He, 2025). The timeshift is set to 2.0 to match the time sampler. A.3. Text-to-Image Generation We adopt Qwen3-1.7B (Yang et al., 2025) as the text encoder. To improve the alignment of frozen text features (Fan et al., 2024), we jointly train several transformer layers on the frozen text features similar to Fluid (Fan et al., 2024). The total batch size is 1536 for 256 256 resolution pretraining and 512 for 512 512 resolution pretraining. Following PixNerd (Wang et al., 2025a), we pretrain PixelGen on 256 256 resolution for 200K steps and pretrain on 512 512 resolution for 80K steps. We further fine-tune the pretrained PixelGen on BLIP3o-60k with 40k steps at the 512 512 resolution following PixNerd. We adopt the gradient clip to stabilize training. The whole training only takes about 6 days on 8 H800 GPUs. We use the Adams-2nd solver with 25 steps as the default choice for sampling. The CFG scale is set to 4.0. We leave the native resolution (Wang et al., 2025c) or native aspect training (Gong et al., 2025; Gao et al., 2025; Liao et al., 2025) as future works. A.4. Experiment Configurations Table 7 summarizes the experiment configurations for PixelGen-L/16, PixelGen-XL/16, and PixelGen-XXL/16. In practice, we follow the training setups from previous works such as DiT (Peebles & Xie, 2023), SiT (Ma et al., 2024), and PixNerd (Wang et al., 2025a). B. Text-to-Image Prompts Below, we list the prompts used for text-to-image generation in Figure 5. These prompts cover mix of animals, people, and scenes to evaluate semantic understanding and visual detail generation. 12 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Table 7. Configurations of Experiments. PixelGen-L PixelGen-XL PixelGen-XXL architecture DiT depth hidden dim heads params bottleneck dim dropout image size patch size training optimizer batch size learning rate lr schedule weight decay ema decay time sampler noise scale clip of (1-t) sampling ODE solver ODE steps time steps timeshift CFG scale CFG interval 22 1024 16 459M 128 0.0 28 1152 16 676M - 0.1 256 16 16 1536 24 1.1B 256 0.0 512 AdamW, β1, β2 = 0.9, 0.999 256 1e-4 constant 0 0.9999 logit(t)N (µ, σ2), µ = -0.8, σ = 0.8 1.0 0.05 Euler 50 1.0 - Heun Adams-2nd 25 linear in [0, 1.0] 2.0 2.25 3.0 4.0 [0.1, 0.9] (if used) baby cat stands on two legs, wearing chothes. kungfu panda is wielding sword in realistic style. fox sleeping inside large transparent lightbulb. An extremely happy American Cocker Spaniel is smiling and looking up at the camera with his head tilted to one side. man sipping coffee on sunny balcony filled with potted plants, wearing linen clothes and sunglasses, basking in the morning light. beautiful girl with hair flowing like cascading waterfall. Dreamlike portrait with soft neon glow and painterly textures. raccoon wearing detectives hat, observing something with magnifying glass. Close-up of an aged man with weathered features and sharp blue eyes peering wisely from beneath tweed flat cap. C. Pseudocodes for PixelGen In Algorithm 1, we provide the pseudocodes for the training step of PixelGen. PixelGen follows the pipeline of JiT (Li & He, 2025) and addtionally introduce two complementary perceptual losses on the predicted image xθ. REPA (Yu et al., 2024) loss is used in both our Baseline and PixelGen. Algorithm 2 provides the pseudocodes for the sampling steps. 13 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Algorithm 1 Training step # netθ: DiT network # x: training batch # c: class label or textual prompt = sample t() ϵ = randn like(x) xt = * + (1 t) * x1 vt = (xt - x) / (1 - t) xθ = netθ(xt, t, c) vθ = (xθ - x) / (1 - t) lossFM = l2 loss(vθ - vt) lossLPIPS = LPIPS(xθ, x) lossP-DINO = P-DINO(xθ, x) loss = lossFM + λ1lossLPIPS + λ2lossP-DINO + lossREPA Algorithm 2 Sampling step # xt: current samplers at xθ = netθ(xt, t, c) vθ = (xθ - x) / (1 - t) xt next = xt + (t next - t) * vθ D. More Visualizations In this section, we provide more visualizations, including text-to-image generation in Figure 7, and class-to-image generation at 256256 resolution in Figure 8. Our PixelGen supports multiple languages with the Qwen3 text encoder after pretraining on the BLIP3o dataset (Chen et al., 2025a), such as Chinese and English. 14 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Figure 7. More Qualitative results of text-to-image generation at 512512 resolution. Our PixelGen supports multiple languages with the Qwen3 text encoder, such as Chinese and English. 15 PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss Figure 8. More qualitative results of class-to-image generation at 256256 resolution with CFG. The CFG scale is set to 3.0."
        }
    ],
    "affiliations": [
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}