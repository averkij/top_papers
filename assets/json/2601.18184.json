{
    "paper_title": "VIBEVOICE-ASR Technical Report",
    "authors": [
        "Zhiliang Peng",
        "Jianwei Yu",
        "Yaoyao Chang",
        "Zilong Wang",
        "Li Dong",
        "Yingbo Hao",
        "Yujie Tu",
        "Chenyu Yang",
        "Wenhui Wang",
        "Songchen Xu",
        "Yutao Sun",
        "Hangbo Bao",
        "Weijiang Xu",
        "Yi Zhu",
        "Zehua Wang",
        "Ting Song",
        "Yan Xia",
        "Zewen Chi",
        "Shaohan Huang",
        "Liang Wang",
        "Chuang Ding",
        "Shuai Wang",
        "Xie Chen",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 4 8 1 8 1 . 1 0 6 2 : r VIBEVOICE-ASR Technical Report Zhiliang Peng, Jianwei Yu, Yaoyao Chang, Zilong Wang, Li Dong Yingbo Hao, Yujie Tu, Chenyu Yang, Wenhui Wang, Songchen Xu, Yutao Sun Hangbo Bao, Weijiang Xu, Yi Zhu, Zehua Wang, Ting Song, Yan Xia, Zewen Chi Shaohan Huang, Liang Wang, Chuang Ding, Shuai Wang, Xie Chen, Furu Wei Microsoft Research https://aka.ms/GeneralAI This report presents VIBEVOICE-ASR, general-purpose speech understanding framework built upon VIBEVOICE [PYW+25], designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VIBEVOICE-ASR supports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into single end-to-end generation task. In addition, VIBEVOICE-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation. Code: github.com/microsoft/VibeVoice Hugging Face: microsoft/VibeVoice Demo: aka.ms/VibeVoice-ASR Figure 1: VIBEVOICE-ASR sets new state-of-the-art for long-form speech understanding, consistently outperforming strong closed-source multimodal models (Gemini-2.5/3-Pro) across five public benchmarks. The results demonstrate superior accuracy in both speaker attribution (DER) and time-aligned transcription (tcpWER), particularly in complex multi-speaker environments."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed paradigm shift in speech processing, driven by the integration of Large Language Models (LLMs) with acoustic encoders [CXZ+23]. While these large audio models Core contributors. Contact person: fuwei@microsoft.com. Figure 2: The architectural overview of VIBEVOICE-ASR. VIBEVOICE-ASR processes 60-minute long-form audio in single pass by ingesting continuous latents from dual-tokenizers alongside optional user-provided context. The output is generated stream of Rich Transcription, explicitly interleaving Speaker ID (Who), Timestamps (When), and Content (What) have achieved remarkable success in short-form speech recognition, transcribing and analyzing longform audiosuch as hour-long meetings, podcasts, and academic lecturesremains formidable challenge. The prevailing approach to long-form audio involves cascaded pipelines that segment continuous speech into short clips (typically < 30 seconds) for independent processing [HSW+24, BHHZ23, BYC+20]. While practical, this \"divide-and-conquer\" strategy suffers from two fundamental limitations: Context Fragmentation and Pipeline Complexity. First, independently processing segments severs global semantic dependencies, causing the model to lose track of cross-sentence context, which is fatal for disambiguating homophones or resolving coreferences in extended dialogue. Second, traditional systems treat Automatic Speech Recognition (ASR), Speaker Diarization, and Timestamping as separate tasks managed by disjoint models. Reconciling their outputs often requires complex heuristics, leading to error propagation where failure in segmentation or diarization corrupts the final transcript. To bridge this gap, we introduce VIBEVOICE-ASR, unified, general-purpose framework designed for high-fidelity long-form speech understanding. Built upon the VibeVoice architecture [PYW+25], our system fundamentally abandons the sliding-window paradigm in favor of single-pass approach. By leveraging an ultra-low frame rate tokenizer (7.5 Hz), VIBEVOICE-ASR compresses an hour of audio into sequence length that fits comfortably within the context window of modern LLMs. This allows the model to attend to the entire global context of 60-minute session simultaneously, ensuring semantic coherence and consistent speaker tracking without the need for external clustering algorithms. Concurrent with the development of VIBEVOICE-ASR, number of related research efforts have emerged [HSZ26, YCD+25, SXF+25, YLY+26]. Nevertheless, the majority of these works have not made their models publicly available. VIBEVOICE-ASR reformulates long-form transcription as an end-to-end generation task, as shown in Figure 2. Instead of outputting plain text, it generates structured Rich Transcription stream that explicitly interleaves speaker identities (Who\"), precise timestamps (When\"), and speech content (What\"). Furthermore, acknowledging the diverse needs of real-world applications, we introduce prompt-based context injection mechanism. This allows users to supply customized contextranging from hotword lists to background descriptionssignificantly enhancing the models ability to recognize domain-specific terminology and handle complex code-switching scenarios."
        },
        {
            "title": "2 Method",
            "content": "2.1 Overview Figure 2 presents the architectural overview of VIBEVOICE-ASR. We formulate long-form speech understanding as language modeling task. The model takes sequence of continuous audio embeddings, encoded from from the pre-trained Acoustic and Semantic encoders, as its primary input. To enable context-aware capabilities, optional text prompts (e.g., hotwords or background information) can be prepended to the audio sequence. These inputs are processed by decoder-only Large Language Model backbone (e.g., Qwen 2.5 [YYZ+24]) to autoregressively generate the target sequence. Distinct from conventional ASR models that output plain text, VIBEVOICE-ASR is designed to produce Rich Transcription. As illustrated in the output stream of Figure 2, the model generates structured sequence that explicitly interleaves speaker identity (Who), temporal boundaries (When), and speech content (What), enabling simultaneous recognition, diarization, and timestamping in single pass. 2.2 Speech Tokenizer In this work, we directly employ the pre-trained dual-tokenizers from VIBEVOICE [PYW+25], which integrates an Acoustic Tokenizer for spectral fidelity and Semantic Tokenizer for linguistic alignment. The Acoustic tokenizer, inspired by σ-VAE [SBW+24], applies hierarchical design with cumulative 3200 downsampling rate to the 24 kHz input, yielding an extremely compact representation of approximately 7.5 tokens per second. Meanwhile, the Semantic module extracts deterministic content features aligned with textual semantics. Note we only use tokenizer encoders here. This ultra-low frame rate is pivotal, as one-hour continuous audio session translates to: 3600 seconds 7.5 tokens/sec = 27, 000 tokens, (1) which fits comfortably within the single-pass context window of modern LLMs. 2.3 VIBEVOICE-ASR 2.3.1 Pre-training We use the data processing pipeline proposed in VIBEVOICE [PYW+25, YCB+24] to obtain the initial data corpus. The pre-training data distribution can be found in Figure 3. The pipeline consists of three stages: segmentation and transcription, diarization, and quality filtering. Long recordings are first segmented using Silero voice activity detection (VAD) into clips of up to 30 seconds, followed by transcription with Whisper-large-v3-turbo [RKX+23] to obtain punctuated text and word-level timestamps; segment boundaries are further refined by splitting at punctuation end timestamps (e.g., [.?!]) to better align with speaker turns. Speech diarization is then performed using the vblinkp model from the WeSpeaker toolkit [WLW+23], where speaker embeddings are extracted from overlapping frames (1.5 window, 0.75 hop), clustered with HDBSCAN [CMS13], and refined by merging clusters whose centroids have cosine similarity greater than 0.67, yielding final speaker turn annotations. Finally, to ensure annotation reliability, segments are re-transcribed using secondary ASR model [XJM+23], and recordings are discarded if more than 30% of segments have WER exceeding 20%, if speech accounts for less than 60% of the total duration. To ensure the effectiveness of the data processing pipeline, we conducted comparative study between our pipeline and two widely adopted audio processing pipelines, WhisperX [BHHZ23] and Emilia [HSW+24]. The evaluation is performed on three commonly used public multi-speaker meeting datasetsAMI [CAB+05], AliMeeting [YZF+22], and AISHELL-4 [FCL+21]and reports both diarization error rate (DER) and diarization invariant word error rate (WER). For fair comparison, we disable the data-filtering module in Emilia, as its default configuration removes substantial portion of the audio samples. As shown in Table 1, the proposed data pipeline consistently achieves lower DER and WER than both baseline systems across the majority of evaluated datasets. These results indicate that our pipeline provides more robust segmentation, diarization, and transcription performance under diverse acoustic conditions. 3 Model Table 1: DER and WER comparison across different data pipelines. AMI-IHM AISHELL4 DER WER DER WER DER WER DER WER AliMeeting AMI-SDM WhisperX Emilia Ours pipeline 14.55 16.58 16.93 29.69 49.40 18.99 18.27 35.44 15.46 24.12 47.85 23. 23.05 46.55 17.78 39.65 61.70 28.40 35.53 25.57 25.34 36.62 54.27 30.82 We employed curriculum learning strategy for the LLM input sequence length, progressively increasing from 8,192 to 65,536 tokens. 2.3.2 Supervised Fine-Tuning (SFT) Since the pre-training stage predominantly relies on pseudo-labeled data, the SFT phase is critical for aligning the model with precise instruction-following behaviors. We carefully curate high-quality dataset composition strategy, categorized into three distinct sources: High-Quality Speech and Music Benchmarks. To establish robust baseline for conversational speech recognition and speaker diarization, we utilize established datasets including the training splits of MLC-SLM [MGS+25] and Fisher [CMW04]. These provide high quality labels for multi-speaker interactions. Additionally, we incorporate the open-source synthesized music dataset Muse [JCX+26] as an independent subset. The inclusion of this music data allows the model to learn music-specific acoustic features, explicitly optimizing its performance and robustness when handling musical segments. Context-Aware Synthetic Data Pipeline. key capability of VIBEVOICE-ASR is utilizing user-provided Contextual Informationranging from specific entities to complete sentences and background descriptionsto guide recognition. To bridge the lack of such paired data in real-world scenarios, we constructed synthetic pipeline: Context-Driven Script Generation: We employ GPT-5 [SFP+25] to generate complex dialogue scripts containing specific entities, technical terms, and cross-lingual content (English, Chinese, and intra-sentential code-switching). Crucially, GPT-5 simultaneously generates the corresponding contextual reference text (e.g., keyword lists, related sentences, or background paragraphs) used to prompt the ASR model. Audio Synthesis: We leverage the VIBEVOICE engine to synthesize high-fidelity multispeaker audio. The synthesis predominantly targets Chinese, English, and complex EnglishChinese code-switching scenarios, fully exploiting VIBEVOICEs superior capabilities in modeling these specific linguistic distributions and transitions. Quality Filtering: We perform closed-loop verification where the synthesized speech is transcribed back; samples exceeding WER threshold are discarded to prevent noise injection. After that, we obtains about 6,000 hours synthesized audio. Long-Form Transcription Restoration. Existing high-quality datasets are predominantly short (<30 minutes), creating distribution shift for long-form applications. While we recall long-duration samples (>50 minutes) from our pre-training corpus, their original transcriptionsderived from our chunk-wise pipelinesalso suffer from context fragmentation. To address this, we employ GPT-5 as text refiner to rewrite and merge disjointed transcriptions into coherent, globally consistent long texts (\"Global Semantic Rectification\"). Furthermore, to handle the non-speech intervals inherent in long-duration recordings, we utilize GPT-Audio2 to automatically annotate these segments with general acoustic tags. Specifically, we label events such as [Unintelligible Speech], [Music], [Human Sounds], [Environmental Sounds], [Noise], and [Silence]. This explicit tagging strategy provides direct supervision for non-speech intervals, designed to prevent the model from hallucinating text during silence or background noise. 2https://platform.openai.com/docs/models/gpt-4o-audio-preview 4 To balance the VIBEVOICE-ASRs capabilities across standard recognition, music robustness, context awareness, and long-form coherence, we apply strategic data mixing ratio. Specifically, the sampling weights for Standard Benchmarks, Music Data, Synthetic Data, and Refined Long-Form Data are set to 0.5 : 0.1 : 0.1 : 0.3, respectively."
        },
        {
            "title": "3 Results",
            "content": "We follow the MeetEval3 evaluation protocol and report four complementary metrics that capture different aspects of multi-speaker transcription quality. Diarization Error Rate (DER) measures the accuracy of speaker attribution by accounting for speaker confusion, missed speech, and false alarm speech, and thus directly evaluates the models ability to answer who speaks when. Word Error Rate (WER) ignores speaker labels and timing information and computes the standard word-level error rate over the entire transcription, serving as measure of pure speech recognition accuracy (what) independent of diarization performance. Concatenated minimum-Permutation WER (cpWER) evaluates transcription accuracy under speaker permutation invariance by concatenating all utterances belonging to the same speaker and computing the minimum WER over all possible speaker permutations; this metric jointly reflects content recognition accuracy and speaker consistency, while being insensitive to local time alignment errors. Time-Constrained minimum-Permutation WER (tcpWER) further extends cpWER by enforcing temporal alignment constraints, such that words are only matched if they occur within predefined temporal collar, making tcpWER sensitive to both speaker attribution and word-level timing accuracy and thus jointly evaluating who, what, and when. We select Gemini-2.5-Pro and Gemini-3-Pro as comparison baselines, as they represent state-of-theart large-scale multimodal foundation models capable of jointly predicting timestamps, speaker labels, and transcription content. During our experiments, we observe that Gemini models exhibit substantial timestamp inaccuracies and occasional content hallucinations when processing long-form audio inputs. To ensure fair and stable comparison, we therefore segment the test audio into 240-second chunks before feeding them to the Gemini models. In contrast, VIBEVOICE-ASR processes the entire audio recording in single pass, without requiring chunk-wise inference. Table 2: Overall diarization and ASR results across datasets and languages. Dataset Language Gemini-3-Pro DER cpWER tcpWER WER DER cpWER tcpWER WER DER cpWER tcpWER WER VIBEVOICE-ASR Gemini-2.5-Pro AISHELL-4 Chinese 15.32 31.59 35.96 22.42 22. 27.43 54.17 22.75 6.77 24.99 25. 21.40 AMI-IHM AMI-SDM English English 23.54 23.79 29.57 34.78 38.35 41. 18.48 46.23 22.35 43.04 22.34 26.91 63.65 64.86 17.61 11.92 22.09 13.43 20.41 28.82 20.82 29. 18.81 24.65 AliMeeting Chinese 31.60 41.64 53. 27.43 38.75 32.84 65.61 26.75 10.92 29.33 29. 27.40 MLC-Challenge 20.67 English 7.66 French 18.19 German 12.55 Italian 20.40 Japanese 17.57 Korean Portuguese 20.86 5.35 Russian 9.10 Spanish 15.54 Thai Vietnamese 14.65 AVERAGE 16.29 16.23 23.06 30.36 16.88 30.41 19.23 30.03 14.26 13.82 20.84 16.71 20.37 26.72 24.60 39.43 25.20 37.36 29.81 40.20 16.59 17.49 30.28 27.28 28.90 9.76 30.88 17.17 40.82 17.76 42.14 12.87 23.45 16.58 59.68 10.18 39.28 20.15 39.17 10.74 22.76 9.09 25.54 14.84 22.09 12.33 32.24 13.05 32. 12.85 22.02 23.56 15.59 21.96 19.39 23.29 13.05 12.11 14.59 13.15 16.38 57.64 71.11 73.86 49.89 81.41 57.33 85.44 51.89 43.72 39.54 60.43 58.81 10.19 18.71 19.39 13.32 18.47 11.21 20.10 10.31 9.36 12.03 11.53 13.11 4.28 3.80 1.04 2.08 0.82 4.52 7.98 0.90 2.67 4.09 0.16 3.42 11.48 18.80 17.10 15.76 15.33 15.35 29.91 12.94 10.51 14.91 14.57 14.81 13.02 19.64 17.26 15.91 15.41 16.07 31.65 12.98 11.71 15.57 14.57 15. 7.99 15.21 16.30 13.91 14.69 9.65 21.54 12.40 8.04 13.61 14.43 12.07 As shown in Table 2, VIBEVOICE-ASR consistently outperforms Gemini-2.5-Pro and Gemini3-Pro in terms of DER and tcpWER across all evaluated datasets, demonstrating substantially stronger speaker modeling and more accurate alignment of speaker turns over time. On the cpWER metric, which more directly reflects the models ability to maintain speaker consistency, our model 3https://github.com/fgnt/meeteval 5 achieves the best performance on 11 out of 16 evaluation settings, significantly outperforming both Gemini variants and indicating more reliable speaker differentiation in multi-speaker conditions. Regarding WER, our model attains the lowest error rate on 8 out of 16 settings, while exhibiting only marginal degradation on the remaining datasets. Overall, these results indicate that VIBEVOICEASR achieves better balance between content recognition accuracy and robust speaker-aware transcription, with particularly strong advantages in speaker attribution, temporal consistency, and multilingual generalization."
        },
        {
            "title": "4 Conclusion and Limitations",
            "content": "In this report, we presented VIBEVOICE-ASR, unified single-pass framework that effectively solves context fragmentation in long-form speech understanding. Beyond technical contributions, we commit to comprehensive open-sourcing, releasing the model weights, fine-tuning pipelines, and high-performance inference code (e.g., vLLM [KLZ+23] support). By democratizing access to these tools, we aim to empower the research community to address the SFT gaps in low-resource languages and adapt the framework to diverse downstream applications, ultimately fostering more inclusive and advanced speech ecosystem. Despite these advancements, VIBEVOICE-ASR has several limitations that guide future research: Multilingual Forgetting in SFT: While our pre-training covered over 50 languages, the SFT phase predominantly focused on English, Chinese, and code-switching data. Consequently, the model may experience performance degradation on low-resource languages absent from the instruction tuning stage. We hope our open-source fine-tuning code will encourage the community to bridge this gap. Overlapping Speech: The current architecture generates serialized output stream and does not explicitly handle overlapping speech (the \"cocktail party problem\"). In scenarios where multiple speakers talk simultaneously, the model tends to transcribe the dominant speaker, potentially missing secondary information. Future iterations will explore separation-aware modeling to address this challenge."
        },
        {
            "title": "Acknowledge",
            "content": "We thank Ruibin Yuan, Tao Zhang and Zhengwei Huang for their in-depth discussions during the research and development of VIBEVOICE-ASR."
        },
        {
            "title": "References",
            "content": "[BHHZ23] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. Whisperx: Time-accurate speech transcription of long-form audio. arXiv preprint arXiv:2303.00747, 2023. [BYC+20] Hervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin Lavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and Marie-Philippe In ICASSP Gill. Pyannote. audio: neural building blocks for speaker diarization. 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 71247128. IEEE, 2020. [CAB+05] Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al. The ami meeting corpus: pre-announcement. In International workshop on machine learning for multimodal interaction, pages 2839. Springer, 2005. [CMS13] Ricardo JGB Campello, Davoud Moulavi, and Jörg Sander. Density-based clustering In Pacific-Asia conference on knowledge based on hierarchical density estimates. discovery and data mining, pages 160172. Springer, 2013. [CMW04] Christopher Cieri, David Miller, and Kevin Walker. The fisher corpus: resource for the next generations of speech-to-text. In LREC, volume 4, pages 6971, 2004. 6 [CXZ+23] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. [FCL+21] Yihui Fu, Luyao Cheng, Shubo Lv, Yukai Jv, Yuxiang Kong, Zhuo Chen, Yanxin Hu, Lei Xie, Jian Wu, Hui Bu, et al. Aishell-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario. arXiv preprint arXiv:2104.03603, 2021. [HSW+24] Haorui He, Zengqiang Shang, Chaoren Wang, Xuyuan Li, Yicheng Gu, Hua Hua, Liwei Liu, Chen Yang, Jiaqi Li, Peiyang Shi, et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 885890. IEEE, 2024. [HSZ26] Mingyue Huo, Yiwen Shao, and Yuheng Zhang. Tagspeech: End-to-end multispeaker asr and diarization with fine-grained temporal grounding. arXiv preprint arXiv:2601.06896, 2026. [JCX+26] Changhao Jiang, Jiahao Chen, Zhenghao Xiang, Zhixiong Yang, Hanchen Wang, Jiabao Zhuang, Xinmeng Che, Jiajun Sun, Hui Li, Yifei Cao, et al. Muse: Towards reproducible long-form song generation with fine-grained style control. arXiv preprint arXiv:2601.03973, 2026. [KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [MGS+25] Bingshen Mu, Pengcheng Guo, Zhaokai Sun, Shuai Wang, Hexin Liu, Mingchen Shao, Lei Xie, Eng Siong Chng, Longshuai Xiao, Qiangze Feng, et al. Summary on the multilingual conversational speech language model challenge: Datasets, tasks, baselines, and methods. arXiv preprint arXiv:2509.13785, 2025. [PYW+25] Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, et al. Vibevoice technical report. arXiv preprint arXiv:2508.19205, 2025. [RKX+23] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [SBW+24] Yutao Sun, Hangbo Bao, Wenhui Wang, Zhiliang Peng, Li Dong, Shaohan Huang, Jianyong Wang, and Furu Wei. Multimodal latent language modeling with next-token diffusion. arXiv preprint arXiv:2412.08635, 2024. [SFP+25] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. [SXF+25] Mohan Shi, Xiong Xiao, Ruchao Fan, Shaoshi Ling, and Jinyu Li. Train short, infer long: Speech-llm enables zero-shot streamable joint asr and diarization on long audio. arXiv preprint arXiv:2511.16046, 2025. [WLW+23] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang Chen, Binbin Zhang, Xu Xiang, Yanlei Deng, and Yanmin Qian. Wespeaker: research and production oriented speaker embedding learning toolkit. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [XJM+23] Hainan Xu, Fei Jia, Somshubra Majumdar, He Huang, Shinji Watanabe, and Boris Ginsburg. Efficient sequence transduction by jointly predicting tokens and durations. In International Conference on Machine Learning, pages 3846238484. PMLR, 2023. [YCB+24] Jianwei Yu, Hangting Chen, Yanyao Bian, Xiang Li, Yi Luo, Jinchuan Tian, Mengyang Liu, Jiayi Jiang, and Shuai Wang. Autoprep: An automatic preprocessing framework for in-the-wild speech data. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 11361140. IEEE, 2024. [YCD+25] Han Yin, Yafeng Chen, Chong Deng, Luyao Cheng, Hui Wang, Chao-Hong Tan, Qian Chen, Wen Wang, and Xiangang Li. Speakerlm: End-to-end versatile speaker diarization and recognition with multimodal large language models. arXiv preprint arXiv:2508.06372, 2025. [YLY+26] Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang, Zhaoye Fei, Hanfu Chen, Jingqi Chen, Ke Chen, Qinyuan Cheng, Liwei Fan, et al. Moss transcribe diarize: Accurate transcription with speaker diarization. arXiv preprint arXiv:2601.01554, 2026. [YYZ+24] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [YZF+22] Fan Yu, Shiliang Zhang, Yihui Fu, Lei Xie, Siqi Zheng, Zhihao Du, Weilong Huang, Pengcheng Guo, Zhijie Yan, Bin Ma, et al. M2met: The icassp 2022 multi-channel multi-party meeting transcription challenge. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61676171. IEEE, 2022."
        },
        {
            "title": "A Language Distribution of Training Data",
            "content": "Figure 3: Language distribution in the training data."
        }
    ],
    "affiliations": [
        "Microsoft Research"
    ]
}