{
    "paper_title": "compar:IA: The French Government's LLM arena to collect French-language human prompts and preference data",
    "authors": [
        "Lucie Termignon",
        "Simonas Zilinskas",
        "Hadrien Pélissier",
        "Aurélien Barrot",
        "Nicolas Chesnais",
        "Elie Gavoty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from a predominantly French-speaking general audience. The platform uses a blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across a diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French. We release three complementary datasets -- conversations, votes, and reactions -- under open licenses, and present initial analyses, including a French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of human-AI interaction."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 9 6 6 6 0 . 2 0 6 2 : r compar:IA: The French Governments LLM arena to collect French-language human prompts and preference data Lucie Termignon Simonas Zilinskas Hadrien Pélissier Aurélien Barrot Nicolas Chesnais Elie Gavoty contact@comparia.beta.gouv.fr Abstract Large Language Models (LLMs) often show reduced performance, cultural alignment, and safety robustness in non-English languages, partly because English dominates both pre-training data and human preference alignment datasets. Training methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) require human preference data, which remains scarce and largely non-public for many languages beyond English. To address this gap, we introduce compar:IA, an open-source digital public service developed inside the French government and designed to collect large-scale human preference data from predominantly French-speaking general audience. The platform uses blind pairwise comparison interface to capture unconstrained, real-world prompts and user judgments across diverse set of language models, while maintaining low participation friction and privacy-preserving automated filtering. As of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and 250,000 preference votes, with approximately 89% of the data in French (platform analytics; snapshot 2026-02-07). We release three complementary datasetsconversations, votes, and reactions under open licenses, and present initial analyses, including French-language model leaderboard and user interaction patterns. Beyond the French context, compar:IA is evolving toward an international digital public good, offering reusable infrastructure for multilingual model training, evaluation, and the study of humanAI interaction."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are overwhelmingly trained on English-language data. Publicly available training disclosures for major open models show that French typically represents very small share of pre-training data, with similarly low proportions during post-training stages such as instruction tuning and preference alignment. For example, the Llama 2 technical report indicates that French accounts for only 0.16% of the training corpus (Touvron et al. 2023). This imbalance causes degraded fluency, mismatched register, culturally inappropriate responses, and weaker safety guarantees in non-English languages (Conneau et al. 2020; Hershcovich et al. 1 2022; Bigoulaeva et al. 2021; W. Wang et al. 2024). Preference data captures human judgments crucial for training techniques like RLHF and DPO (Christiano et al. 2017; Ouyang et al. 2022; Rafailov et al. 2023), but large-scale preference datasets remain rare outside English, especially as open resources. While proprietary systems likely collect large volumes of multilingual interaction data, these datasets are generally inaccessible to smaller or emerging industrial, academic and public-sector actors, reducing model diversity and competition. Blind pairwise comparison has emerged as an effective method for collecting scalable human preference data while reducing brand and expectation biases (Chiang et al. 2024). LLM arenas, most notably the LMSYS Chatbot Arena, now known as Arena1, have demonstrated that crowdsourced preferences can meaningfully contribute to model training and evaluation. However, participation and data generation remain heavily concentrated in English, which constrains evaluation and training efforts for other languages. Public release policies also differ across arenas, and only subset of collected interactions is typically released as open data. To address these gaps, the French government launched compar:IA (https://comparia.beta. gouv.fr/), public LLM arena designed to collect human preference data from predominantly French-speaking audience. compar:IA adapts blind pairwise comparison for general public by removing barriers to entry and providing clear explanations, while remaining compatible with established preference-based evaluation practices. Since its public launch in October 2024 and as of 2026-02-07, compar:IA has collected over 600,000 free-form prompts and more than 250,000 preference votes (platform analytics; snapshot 2026-0207). All published data are released continuously under Etalab 2.0 licenses on Hugging Face and data.gouv.fr, following privacy filtering pipeline that excludes conversations containing detected personal data. Based on publicly released resources, compar:IA appears to be among the largest openly available collections of French-language human prompt and preference data for conversational AI. Additionally, since November 2025, the compar:IA interface has been adapted to other languages, paving the way for the creation of datasets in other less resourced languages. This paper documents compar:IA as both an awareness-raising platform and data collection infrastructure. We describe the platforms design and user experience, the structure and publication of the resulting datasets, and early indicators of adoption and impact. We also discuss limitations related to user representativeness and task coverage, and outline future directions, including multilingual expansion. More broadly, this work contributes empirical evidence to ongoing discussions on multilingual data, evaluation, alignment, and the role of public institutions in building open AI infrastructure."
        },
        {
            "title": "2.1 User interaction flow",
            "content": "The interaction flow consists of small number of clearly separated steps: 1https://arena.ai/ 2 1. Users begin by entering prompt. Prompts are free-form and unconstrained, reflecting real-world usage rather than predefined tasks. To reduce the blank-page effect, the interface also provides optional prompt suggestions (for now only in the French version of the platform) that users can reuse or adapt, although they are used for less than 6% of queries (platform analytics; snapshot 2026-02-07). 2. Two models then generate responses to the prompt. The responses are displayed side by side without any identifying information. Users can read both answers and continue the conversation if they wish, which allows the collection of multi-turn interactions. 3. Feedback can be provided at two levels. Users may react to individual messages, capturing fine-grained judgments, or vote on the overall conversation by selecting the response they prefer. These two mechanisms produce distinct but complementary signals. 4. After feedback is submitted, the platform reveals the identities of the models and displays associated metadata. This includes model descriptions and additional contextual information intended to support user understanding. Users are also shown an estimation of the environmental impact of the inference, based on the quantity of tokens generated, the architecture and size of the model. For proprietary models, the architecture and size are estimated based on publicly available information. The energy consumption is estimated thanks to library called Ecologits (Rincé and Banse 2025). (a) Enter prompt. (b) Blind side-by-side responses. (c) Message reactions. (d) Conversation vote. (e) Model reveal and metadata. Figure 1: compar:IA user interaction flow."
        },
        {
            "title": "2.2 Core design principles",
            "content": "compar:IA maximizes participation by minimizing friction. No account creation is required; users can interact immediately after checking data reuse consent checkbox. This limits metadata 3 collection but lowers the barrier to entry, especially for non-expert audiences. The platform has been widely used in schools and universities. Optional account creation may be introduced in future iterations, but the baseline experience will remain usable without authentication. Accessibility is also central. The interface avoids technical terminology, provides contextual explanations, and uses visual cues to guide users. The goal is meaningful participation from users without prior knowledge of LLM architectures or evaluation protocols."
        },
        {
            "title": "2.3 Value for end users",
            "content": "compar:IA gives users free access to diverse language models, including systems less visible in mainstream consumer products. As of 2026-02-07, 104 models (29 proprietary, the rest open-weight or open-source) are available. This diversity challenges the perception that only few dominant models define the state of the art. By comparing responses side by side, users observe linguistic, stylistic, and cultural differences between models. Pairwise comparison makes these differences more salient than isolated interactions, highlighting that model behavior is neither uniform nor neutral. The platform also introduces users to the environmental dimension of AI usage. By associating responses with energy estimates, compar:IA encourages reflection on responsible use without requiring technical expertise."
        },
        {
            "title": "2.4 Backend architecture and infrastructure",
            "content": "The initial version of compar:IA derived from the LMSYS Chatbot Arena codebase and relied on Gradio. Early deployments used inference credits from multiple partners (Scaleway, OVH, Google, Microsoft, Hugging Face). This setup enabled rapid prototyping but was not designed for long-term stability. As the platform matured, the backend was refactored for sustained public use and higher traffic. The current system uses FastAPI backend with SvelteKit frontend, enabling flexible development, improved performance, and finer-grained control over data flows. In parallel, compar:IA transitioned to self-financed inference model. Model inference now uses OpenRouter, Hugging Face Inference Providers, or other per-token APIs, allowing integration of broad model set while maintaining predictable costs and independence from ad hoc credit allocations."
        },
        {
            "title": "3.1 Overview of collected data",
            "content": "compar:IA was opened to the public in October 2024 and has been continuously collecting conversation data since that date. 4 As of 2026-02-07, the platform has collected over 600,000 free-form prompts, along with more than 250,000 conversation-level preference votes and message-level reactions (snapshot 2026-02-07). The language distribution is predominantly French (89.14%). While the platform is technically multilingual and occasionally receives prompts in other languages, French accounts for the large majority of collected data. Top languages: Language Count Percentage French (fr) English (en) Spanish (es) Danish (da) German (de) Italian (it) Chinese (zh) Arabic (ar) Latin (la) Portuguese (pt) 357,842 34,335 2,571 1,240 1,067 728 563 384 259 234 89.14% 8.55% 0.64% 0.31% 0.27% 0.18% 0.14% 0.10% 0.06% 0.06% Table 1: Top languages in compar:IA prompts (as of 2026-02-07). Topic wise, technical/educational prompts dominate the distribution (Natural Science + Education = 31.95%), but other topics are covered as well. This might be due to an overrepresentation of compar:IAs usage in the educational sector. Top categories (15 most frequent):"
        },
        {
            "title": "Count Percentage",
            "content": "Natural Science & Formal Science & Technology Education Business & Economics & Finance Society & Social Issues & Human Rights Entertainment & Travel & Hobby Politics & Government Culture & Cultural geography Arts Personal Development & Human Resources & Career Other Law & Justice Health & Wellness & Medicine Environment Daily Life & Home & Lifestyle Food & Drink & Cooking 145,357 117,452 83,111 57,065 55,244 52,915 44,712 38,937 36,889 30,005 28,624 26,303 26,093 23,307 20,658 Table 2: Top prompt categories (as of 2026-02-07). 17.67% 14.28% 10.10% 6.94% 6.72% 6.43% 5.43% 4.73% 4.48% 3.65% 3.48% 3.20% 3.17% 2.83% 2.51% 5 Figure 2: Thematic map of conversational AI uses on compar:IA. startup called Bunka.ai has reused subset of the dataset in order to do some mapping of all the different topics discussed on the platform. Using unsupervised topic modeling and large-scale LLM-based classification, the study examined multiple interaction dimensions, including topics, tasks, language complexity, emotional engagement, and humanAI interaction modes. The results show that French users primarily engage with conversational AI for learning, advice-seeking, content creation, and information retrieval, and that interactions are predominantly augmentative rather than fully automative. blog post describing the study and methods is available online."
        },
        {
            "title": "3.2 Published datasets",
            "content": "Data collected through compar:IA is published as three distinct datasets, each corresponding to different level of interaction: comparia-conversations, containing prompts and model-generated responses organized as multi-turn conversations3 2Hugging Face blog post: French people using AI (compar:IA Bunka.ai study). https://huggingface.co/ blog/comparIA/french-people-using-ai 3Hugging Face dataset: ministere-culture/comparia-conversations. https://huggingface.co/datasets/ ministere-culture/comparia-conversations 6 comparia-votes, containing conversation-level pairwise preference annotations4 comparia-reactions, containing message-level feedback signals5 The datasets are hosted on Hugging Face and mirrored on data.gouv.fr (https://www.data.gouv. fr/datasets/compar-ia) to ensure long-term accessibility. All datasets are released under the Etalab 2.0 open license, standard for French government data. However, responses from proprietary models and some open-weight models are restricted to analysis and evaluation, not training or fine-tuning."
        },
        {
            "title": "3.3 Privacy and data filtering pipeline",
            "content": "compar:IA does not require accounts and does not collect explicit personal data such as names, email addresses, or identifiers (other than short-term session IDs). This simplifies participation but shifts data protection to post-collection filtering. Before publication, all data pass through an LLM-based personal data detection pipeline. Each conversation is evaluated to determine whether it likely contains personal or sensitive information. Should conversation be wrongly identified as not containing PII or other sensitive data, publicly available form6 allows reporting these incidents so that the data points can be removed manually by compar:IA maintainers. The filtering strategy is deliberately conservative. When personal data is detected, the entire conversation and its associated votes and reactions are excluded, resulting in about 5% of conversations being filtered out. No attempt is made to mask or anonymize specific spans of text. The raw dataset is uploaded to Hugging Face but gated for research purposes only. This reduces data volume but limits re-identification risk and simplifies compliance with GDPR and national regulation. The trade-off favors privacy over maximal dataset size. Alternative approaches involving span-level anonymization are under investigation, but the associated risksincomplete masking, semantic distortion, or residual identifiabilityremain significant. Until these can be reliably mitigated, full-conversation exclusion remains the default policy."
        },
        {
            "title": "3.4 Comparison with existing datasets",
            "content": "Methodologically, compar:IA resembles LMarena preference datasets, notably LMSYS Chat-1M7 and newer Arena datasets.8 However, the linguistic composition differs substantially. 4Hugging Face dataset: ministere-culture/comparia-votes. https://huggingface.co/datasets/ ministere-culture/comparia-votes 5Hugging Face dataset: ministere-culture/comparia-reactions. https://huggingface.co/datasets/ ministere-culture/comparia-reactions 6https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ 7https://huggingface.co/datasets/lmsys/lmsys-chat-1m 8https://huggingface.co/lmarena-ai/datasets While French represents only small fraction of conversations in existing open preference datasets (reported at 1.5% in LMSYS Chat-1M (Chiang et al. 2024)), compar:IA provides several hundred thousand French-language prompts and interactions. Other corpora in less resourced languages will be published shortly, as the service expands to these countries, starting with Denmark. The compar:IA datasets are also enriched with electricity consumption estimates (kwh), based on the Ecologits calculation method."
        },
        {
            "title": "4.1 Platform usage metrics",
            "content": "Since its public launch in October 2024, compar:IA has attracted large and steady audience. As of 2026-02-07, the platform has recorded more than 300,000 unique visitors (platform analytics; snapshot 2026-02-07). Usage is not restricted to specific campaigns or events and reflects continuous organic traffic. Participation is entirely voluntary. As of January 2026 users do not receive compensation, rankings, or badges, and are not required to create accounts. The absence of explicit incentives reinforces the interpretation of collected data as reflective of spontaneous public engagement rather than task-driven annotation behavior."
        },
        {
            "title": "4.2 Dataset impact indicators",
            "content": "Given the difficulty of observing downstream reuse, dataset size serves as first-order proxy for impact. The number of prompts, conversations, and preference votes indicates both platform adoption and potential utility of the published datasets. Access metrics on hosting platforms provide partial visibility into reuse. 778 unique users requested access across the three datasets on Hugging Face. On data.gouv.fr the datasets are not gated. To gather more data about dataset reuse, the compar:IA team sent survey in October 2025 to people that requested access to the dataset. 25 people responded. They self declared as 68% of academics/researchers and 20% representing private companies. They used compar:IA datasets mostly to do model training (32%), research (24%), or evaluation (20%). For example for studying human preferences, language use, prompts and discourse. This suggests active interest, but systematic reporting of reuse remains limited, particularly among industrial actors, making impact measurement challenging."
        },
        {
            "title": "4.3 Mediation, outreach, and growth strategies",
            "content": "To sustain participation and broaden its audience, compar:IA has been integrated into range of educational formats. These include workshops, public talks, and educational programs focused on digital literacy and generative AI. In these contexts, the platform is used both as demonstration tool and as an interactive exercise. For example, PIX, national online platform open to everyone to assess, develop, and certify digital skills, has integrated compar:IA into its AI curriculum. In 2026, more than 1.5 million students are expected to use compar:IA through this program. dedicated mediation format, Les Duels de lIA, was developed to support using compar:IA for education. This format structures interactions around collective discussion about the model answers and specifications, encouraging critical reflection. More than 1400 potential facilitators have submitted their registration to receive material for the workshop. Several hundred people have already submitted the post-event feedback forms."
        },
        {
            "title": "4.4 First-party dataset reuses",
            "content": "Beyond data collection, compar:IA produces secondary outputs derived from preference data."
        },
        {
            "title": "4.4.1 Model leaderboard",
            "content": "compar:IA released first model leaderboard based on aggregated pairwise preferences.910 The leaderboard was developed in collaboration with PEReN and was made public in November 2025. Rankings are updated on weekly basis to account for newly collected data. The ranking methodology relies on preference modeling techniques commonly used in pairwise comparison settings, specifically BradleyTerry models (Bradley and Terry 1952). The leaderboard is constructed from conversation-level votes and message-level reactions and is designed to reflect relative user preferences rather than task-specific performance. This output has known limitations. Preference data is influenced by prompt distribution, user population, and self-selection effects. Leaderboard positions should be interpreted as indicative rather than definitive. The primary role is exploratory and educational, not formal benchmark."
        },
        {
            "title": "4.4.2 Thematic analysis of prompt usage",
            "content": "A collaborative analysis with Bunka.ai was conducted on subset of over 175,000 compar:IA conversations to characterize prompt usage patterns. Using unsupervised topic modeling and classification, the study identified four dominant interaction types: learning, advice seeking, content generation, and information retrieval, across domains including technology, education, work, health, and culture. 9https://comparia.beta.gouv.fr/ranking 10Hugging Face blog post: Publication du premier classement (compar:IA leaderboard). https://huggingface. co/blog/comparIA/publication-du-premier-classement 9 Figure 3: compar:IA model leaderboard (as of 2026-02-07). The analysis revealed systematic associations between domains and interaction types. Healthrelated prompts are predominantly advice-oriented, scientific topics are mainly learning-focused, and creative domains emphasize content generation. Across most domains, interactions are primarily augmentative rather than fully automative, suggesting that users treat conversational AI as an assistive system rather than replacement for human effort. The study highlights the value of large-scale, unconstrained conversational data for usage analysis and its limitations, including the absence of multimodal signals and potential prompt selection bias. The full analysis is available online."
        },
        {
            "title": "5.1 Research and model development",
            "content": "The datasets are primarily intended for research and model development workflows relying on human prompt and preference data. 11Hugging Face blog french-people-using-ai post: French people using AI. https://huggingface.co/blog/comparIA/ 10 One direct application is preference-based training, including reinforcement learning from human feedback and direct preference optimization (Christiano et al. 2017; Ouyang et al. 2022; Rafailov et al. 2023). The prompt and conversation data can also be used as basis for synthetic data generation. For example, prompts may serve as seeds for controlled generation of additional training data."
        },
        {
            "title": "5.2 Studying model usage",
            "content": "Prompt collections can be analyzed to study real-world usage distributions, topic prevalence, and interaction styles in French conversational AI, supporting sociological research. compar:IA data can contribute to multilingual evaluation benchmarks. By sampling prompts and preferences, researchers can construct test sets grounded in actual user behavior rather than expert-designed tasks, particularly relevant for underrepresented languages."
        },
        {
            "title": "6.1 Project genesis and development",
            "content": "compar:IA originated as an intrapreneurial initiative (commonly referred to as State startup) within the French public administration, jointly supported by the Ministry of Culture and the Interministerial Directorate for Digital Affairs (DINUM), with the initial problem to be solved being how to facilitate access to data in French for training language models while respecting copyright.12 Early development followed an agile, lean startup approach. Rather than defining fixed product upfront, the project began with workshops involving public servants, researchers, practitioners, and civil society actors to identify user needs, test prototypes, and surface usability constraints. The platform evolved iteratively. Initial prototypes validated the core interaction model; subsequent iterations improved interface design, model integration, and data pipelines. Over time, compar:IA transitioned from an experimental prototype to stable national digital public service."
        },
        {
            "title": "6.2 Institutional positioning and objectives",
            "content": "compar:IA is jointly operated by the Ministry of Culture and DINUM as non-commercial digital public service. The platform pursues two complementary objectives: raising public awareness of LLMs (their diversity, biases, and environmental impacts), and collecting human prompt and preference data in French for open release to academic, industrial, and public-sector actors. Since November 2025, compar:IA is recognised as digital public good by the Digital Public Goods 12https://beta.gouv.fr/approche/ 11 Alliance.13 The platform is free, open-source, and publishes datasets under open licenses. It is non-commercial and does not monetize user activity; success is evaluated through participation, awareness, dataset quality, and downstream reuse. This approach aligns with recent calls to build an open ecosystem for human feedback on AI systems, drawing from peer-production, open-source, and citizen-science practices (Don-Yehiya et al. 2025). As of writing, in January 2026, compar:IA is in the process of expanding the platform to other European languages. At the moment, the effort is based on bilateral partnerships, but in the medium-term the aim is to develop digital common with shared governance."
        },
        {
            "title": "7.1 User representativeness",
            "content": "compar:IA does not collect socio-demographic information. This follows from avoiding account creation and minimizing data collection to lower participation barriers and reduce privacy risks. As result, the user population cannot be characterized by age, profession, education level, or geographic distribution beyond coarse language-level inference. This limits preference analysis across user groups and prevents weighting schemes that would correct for population imbalances. The absence of such metadata constrains bias analysis. While model-level differences can be observed, whether these are driven by specific user subpopulations or usage contexts cannot be determined."
        },
        {
            "title": "7.2 Professional and task-specific coverage",
            "content": "The dataset likely underrepresents professional and sensitive use cases. Because all prompts may be published openly, users are often reluctant to submit work-related, confidential, or regulated queries. This affects both prompt diversity and applicability to professional tasks. Domains such as law, healthcare, internal administration, or corporate decision-making are sparsely represented compared to general informational or creative prompts. This also impacts the model leaderboard. Preference rankings from general-public usage may not reflect professional performance and should not be interpreted as task-specific evaluations."
        },
        {
            "title": "7.3 Evaluation bias and self-selection",
            "content": "Participation is voluntary and self-selected. The user population skews toward individuals with existing interest in AI or digital tools. Outreach through educational and institutional networks reinforces this profile. This introduces evaluation bias. Preferences may not generalize to the broader population of conversational AI users, particularly those with lower digital literacy or different usage patterns. 13DPGA registry entry: https://www.digitalpublicgoods.net/r/comparia"
        },
        {
            "title": "7.4 Arena-style platform limitations",
            "content": "Arena-style evaluation platforms constitute distinct usage context that differs from everyday chatbot interaction. Users on such platforms may adopt an evaluative or experimental mindset, submitting shorter, more simplified prompts designed to test models rather than to accomplish real tasks. This can lead to shorter conversations and different prompt distributions compared to naturalistic usage. Additionally, the communication and framing around compar:IA may overly encourage culturespecific or French-language-oriented questions, which could overrepresent certain prompt types relative to how French speakers actually use conversational AI in daily life. More fundamentally, pairwise evaluation itself has structural limits. Comparing two responses highlights relative differences but may obscure absolute quality or fail to capture dimensions that are not easily contrasted, such as factual completeness, long-term usefulness, or safety considerations. Certain model behaviors may therefore be under-emphasized or ignored by the evaluation setup, depending on the prompt and the comparison context. Recent work has begun examining methodological aspects of arena-style evaluation more broadly (Singh et al. 2025). While compar:IA contributes to transparency by releasing all collected data openly, it shares some inherent characteristics of pairwise evaluation platforms that warrant continued methodological attention."
        },
        {
            "title": "7.5 Model configuration and infrastructure constraints",
            "content": "Several technical factors affect the comparability of models on the platform. First, system prompt policies have evolved over time. Open-weight models served through the platform initially operated without system prompts, while proprietary models typically include default system instructions. This asymmetry may have disadvantaged open models in certain contexts. The issue became particularly apparent when integrating specific models (e.g., the Chocolatine model), which required custom system prompt to perform comparably. The platform has since adopted more consistent system prompt policies, but historical data may reflect these inconsistencies. Second, closed-source model behavior is not fully transparent. When querying proprietary models through APIs, we cannot verify whether the response comes solely from the language model or involves additional preprocessing, post-processing, routing logic, or tool use. We have attempted to avoid serving models known to perform tool calling or internet access, but complete certainty is not possible for closed-source systems. Third, model quantization varies across inference providers. While the platform targets full-precision or high-quality quantized model variants, some inference providers (notably through OpenRouter) may serve lower-precision quantized versions (e.g., 8-bit or lower) without explicit disclosure. We have attempted to avoid heavily quantized variants, but provider-level decisions are not always controllable or visible to the platform operators. These infrastructure-level factors introduce potential confounds in model comparisons and should be considered when interpreting leaderboard rankings or preference distributions."
        },
        {
            "title": "7.6 Other potential biases",
            "content": "Beyond the limitations discussed above, several additional sources of bias may affect the collected preference data. First, answer style can bias preferences. Models that produce longer, more confident, with more emojis, or more conversational responses may be favored over more concise or cautious ones, even when the underlying information quality is similar. This stylistic bias is inherent to open-ended, unconstrained prompts and is difficult to disentangle from genuine user preference. While preference data is essential, it is also known to be prone to subtle and systematic biases. Common effects include prefix bias, where early parts of completion disproportionately influence preference judgments (Kumar et al. 2025), which can then propagate to downstream models (Bharadwaj et al. 2025). Other well-documented phenomena include sycophancy (Sharma et al. 2024), as well as verbosity and length-related biases (Singhal et al. 2023; Bu et al. 2025) and formatting biases (Zhang et al. 2025). Many of these effects reflect underlying human judgment tendencies rather than annotation errors and are therefore difficult to eliminate at collection time. By releasing large-scale preference data openly, compar:IA aims to enable the research community to study these biases post hoc and to develop causal analyses, filtering strategies, or post-processing methods that help isolate and mitigate such effects in downstream training and evaluation. Second, model latency can influence user judgments. Faster responses may be perceived as more fluent or reliable, while slower responses may negatively impact user preference independently of content quality. Although responses are displayed simultaneously once generated, differences in generation time may still affect user attention and engagement. Third, user voting behavior is not fully observable. Some users may not read both responses carefully before voting, or may rely on superficial cues such as response length, tone, or formatting. While this behavior reflects real-world usage patterns, it introduces noise into the preference signal and reduces the reliability of individual votes. These biases do not invalidate the collected data but should be considered when interpreting preference signals and downstream analyses."
        },
        {
            "title": "8.1 Internationalisation and multilingual expansion",
            "content": "A primary direction is extending compar:IA to additional languages. While currently Frenchdominant, the underlying architecture is language-agnostic and could be reused for any language. Current expansion efforts focus on European languages that are underrepresented in existing evaluation datasets. For each new language, the objective is to reach dataset sizes sufficient for 14 meaningful reuse, with target on the order of tens of thousands of prompts and preference votes per language. Beyond Europe, the open-source infrastructure could support deployment in other linguistic contexts where preference data remains scarce. Multilingual deployment also enables cross-linguistic analysis, studying how model behavior and user preferences vary by linguistic and cultural context using consistent protocols."
        },
        {
            "title": "8.2 Enhanced data annotation and metadata",
            "content": "Future iterations may introduce optional metadata collection, including broad user categories or self-declared familiarity with AI systems. Such metadata would be opt-in and privacy-preserving, enabling contextualized preference signals and basic stratified analyses rather than fine-grained profiling."
        },
        {
            "title": "8.3 Specialized arenas",
            "content": "Another direction involves specialized arenas for professional or sectoral use cases. Unlike the general-public platform, these could operate with controlled cohorts, explicit consent, and stricter access conditions. Specialized arenas would enable higher-quality, task-specific preference data in domains like public administration, education, or regulated professions, allowing more reliable interpretation of preference rankings within defined contexts."
        },
        {
            "title": "9 Conclusion",
            "content": "This paper presented compar:IA, public LLM arena designed to collect large-scale human preference data in French through blind pairwise comparison. We described the platforms institutional context, design principles, and user interaction model, as well as the structure, publication, and early impact of the resulting datasets. With more than 600,000 prompts and over 250,000 preference votes released under open licenses, compar:IA constitutes significant contribution to the currently limited pool of open, non-English preference data for conversational AI. Beyond data volume, compar:IA demonstrates that large-scale human evaluation can be carried out with general public when interfaces are accessible and participation friction is minimized. The platform shows that preference-based evaluation is not limited to expert annotators or proprietary systems, and that real-world prompts can be collected responsibly at scale within public-sector context. The project highlights the role public institutions can play in AI evaluation infrastructure. By operating outside commercial incentives and prioritizing openness, compar:IA supports collective learning across academic, industrial, and public-sector actors. Its governance illustrates how public administrations can balance participation, transparency, and privacy in AI-related services. Finally, compar:IA offers replicable model for language-specific, human-centered evaluation. Its 15 focus on linguistic diversity and cultural context complements existing English-centric benchmarks. As multilingual extensions develop, compar:IA may serve as foundation for open evaluation infrastructures that better reflect the linguistic and cultural plurality of AI users."
        },
        {
            "title": "Acknowledgments",
            "content": "This work benefited from the support, advice, and resources of many individuals and organisations. Institutional and operational support. We gratefully acknowledge the French Ministry of Culture, in particular Mathilde Bras, Ned Baldessin, Léo Wellhoff, Romain Delassus, and Guillaume Combe, as well as DINUM, notably Pierre Pezziardi and Elsa Le Duigou, for sustained support and guidance throughout the project. Scientific, technical, and strategic advice. We thank colleagues, partners, and organisations for feedback, expertise, discussions, and exchanges throughout the project, including Inria (Djamé Seddah, Benoît Sagot), CNRS and IDRIS (Patrick Paroubek, Maziyar Panahi), GENCI and the Jean Zay infrastructure, the Bibliothèque nationale de France (BnF), INA, Linagora (Michel Marie Maudet, Julie Hunter), ALEIA (Antoine Couret), Mistral AI (Guillaume Lample), Illuin (Manuel Faysse, Gautier Viaud), ReciTAL (Gilles Moyse), OPSCI (Emile Hazard, Clément Bénesse), Le Voice Lab (Karel Bourgois), Teklia (Christopher Kermorvant), PLEIAS (Anastasia Stasenko, Pierre-Carl Langlais), ELDA / ELRA (Khalid Choukri), the Cité européenne des scénaristes (Pauline Rocafull), Dawex, Ouest-France, Google (Robert Dadashi, Léonard Hussenot), La Javaness, the Pantagruel project, Hugging Face (Clément Delangue, Clémentine Fourrier, Daniel van Strien, Nathan Habib, Quentin Lhoest), Meta (Thomas Mesnard), Liquid AI (Maxime Labonne), Cohere (Julia Kreutzer, Shivalika Singh), AI21 (Johanna Kramer), Hugues de Mazancourt, make.org, PIX (Marie Bancal, Benjamin Marteau), Datactivist (Samuel Goëta, Loup Cellard, Laurane Coudriet), Latitudes (Margaux Levisalles, Mélanie Brisard, Pauline Mélédo), Galances Conseil (Jérôme Lucereau, Nicolas Blanchon), CLEMI, Réseau Canopé, CAIRE, UNESCO, Mednum (Ondine Vernier), DRANE PACA, DRANE Strasbourg, the Conseil de lIA et du numérique (Jean Cattan, Joséphine Corcoral, Cécile Ravaux, Magali Jacquemet, Gabriel Ertle), the Campus du Numérique Public (Aude Chouleur, Sophie Louet, Marie Charbonnel), Médialab Sciences Po (Sylvain Parasie, Valentin Goujon), and CREIA (Clément Fantoli). Funding and public support. This project was funded by DINUM and the French Ministry of Culture. Data reuse and downstream applications. We acknowledge PEReN and Bunka.ai for reusing and building upon the datasets produced as part of this work. We also thank researchers, independent developers, and AI labs who have reused the datasets and shared feedback with us, even when such reuse has not been publicly disclosed. Compute and inference resources. We thank Google, Microsoft, Hugging Face, Scaleway, and OVH for providing inference credits and computational resources in the initial stages of the project. Pre-print review. We thank David Salinas (Group Lead, OpenEuroLLM) and Kenneth Enevoldsen (Aarhus University, Denmark) for their valuable reviews of this pre-print."
        },
        {
            "title": "References",
            "content": "Bharadwaj, Anirudh et al. (2025). Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models. In: arXiv preprint arXiv:2506.05339. doi: 10.48550/arXiv.2506. 05339. url: https://arxiv.org/abs/2506.05339. Bigoulaeva, Irina, Viktor Hangya, and Alexander Fraser (2021). Cross-Lingual Transfer Learning for Hate Speech Detection. In: Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pp. 1525. url: https://aclanthology.org/2021.ltedi1.3/. Bradley, Ralph Allan and Milton E. Terry (1952). Rank analysis of incomplete block designs: I. The method of paired comparisons. In: Biometrika 39.34, pp. 324345. doi: 10.1093/biomet/39.34.324. Bu, Yuyan et al. (2025). Beyond Excess and Deficiency: Adaptive Length Bias Mitigation in Reward Models for RLHF. In: Findings of the Association for Computational Linguistics: NAACL 2025, pp. 30913098. doi: 10.18653/v1/2025.findingsnaacl.169. url: https: //aclanthology.org/2025.findings-naacl.169/. Chiang, Wei-Lin et al. (2024). Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. In: Proceedings of the 41st International Conference on Machine Learning. arXiv:2403.04132, pp. 83598388. url: https://proceedings.mlr.press/v235/chiang24b. html. Christiano, Paul F. et al. (2017). Deep Reinforcement Learning from Human Preferences. In: Advances in Neural Information Processing Systems 30 (NeurIPS 2017), pp. 42994307. url: https: / / proceedings . neurips . cc / paper / 2017 / hash / d5e2c0adad503c91f91df240d0cd4e49 - Abstract.html. Conneau, Alexis et al. (2020). Unsupervised Cross-lingual Representation Learning at Scale. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 84408451. doi: 10.18653/v1/2020.acl-main.747. url: https://aclanthology.org/ 2020.acl-main.747/. Don-Yehiya, Shir, Ben Burtenshaw, Ramón Fernandez Astudillo, et al. (2025). The future of open human feedback. In: Nature Machine Intelligence 7, pp. 825835. doi: 10.1038/s42256-02501038-2. Hershcovich, Daniel et al. (2022). Challenges and Strategies in Cross-Cultural NLP. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 69977013. doi: 10.18653/v1/2022.acl-long.482. url: https://aclanthology. org/2022.acl-long.482/. Kumar, Ashwin et al. (2025). Detecting Prefix Bias in LLM-based Reward Models. In: Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency (FAccT 25), pp. 31963206. doi: 10.1145/3715275.3732204. Ouyang, Long et al. (2022). Training Language Models to Follow Instructions with Human Feedback. In: Advances in Neural Information Processing Systems. doi: 10.48550/arXiv.2203. 02155. url: https://arxiv.org/abs/2203.02155. Rafailov, Rafael et al. (2023). Direct Preference Optimization: Your Language Model is Secretly Reward Model. In: Advances in Neural Information Processing Systems 36 (NeurIPS 2023), pp. 5372853741. url: https://proceedings.neurips.cc/paper_files/paper/2023/hash/ a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html. Rincé, Samuel and Adrien Banse (2025). EcoLogits: Evaluating the Environmental Impacts of Generative AI. In: Journal of Open Source Software 10, p. 7471. doi: 10.21105/joss.07471. 17 Sharma, Mrinank et al. (2024). Towards Understanding Sycophancy in Language Models. In: International Conference on Learning Representations (ICLR). arXiv:2310.13548. url: https : / / proceedings . iclr . cc / paper _ files / paper / 2024 / hash / 0105f7972202c1d4fb817da9f21a9663-Abstract-Conference.html. Singh, Shivalika et al. (2025). The Leaderboard Illusion. In: arXiv preprint arXiv:2504.20879. doi: 10.48550/arXiv.2504.20879. url: https://arxiv.org/abs/2504.20879. Singhal, Prasann et al. (2023). Long Way to Go: Investigating Length Correlations in RLHF. In: arXiv preprint arXiv:2310.03716. doi: 10.48550/arXiv.2310.03716. url: https://arxiv. org/abs/2310.03716. Touvron, Hugo, Louis Martin, Kevin Stone, et al. (2023). Llama 2: Open Foundation and FineTuned Chat Models. In: arXiv preprint arXiv:2307.09288. doi: 10.48550/arXiv.2307.09288. url: https://arxiv.org/abs/2307.09288. Wang, Wenxuan et al. (2024). All Languages Matter: On the Multilingual Safety of LLMs. In: Findings of the Association for Computational Linguistics: ACL 2024, pp. 58655877. doi: 10.18653/v1/2024.findings-acl.349. url: https://aclanthology.org/2024.findingsacl.349/. Zhang, Xuanchang et al. (2025). From Lists to Emojis: How Format Bias Affects Model Alignment. In: Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2694026961. doi: 10.18653/v1/2025.acllong.1308. url: https://aclanthology.org/2025.acl-long.1308/."
        }
    ],
    "affiliations": [
        "French government"
    ]
}