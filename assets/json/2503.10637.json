{
    "paper_title": "Distilling Diversity and Control in Diffusion Models",
    "authors": [
        "Rohit Gandikota",
        "David Bau"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info"
        },
        {
            "title": "Start",
            "content": "Rohit Gandikota"
        },
        {
            "title": "Northeastern University",
            "content": "5 2 0 2 3 1 ] . [ 1 7 3 6 0 1 . 3 0 5 2 : r Figure 1. Diversity Distillation: (a) base diffusion model is very slow and has good diversity (b) distilled model is fast but sacrifices diversity (c) we show how the diversity of the base model can be distilled into the fast model by substituting the first timestamp. Control Distillation: (d) Control methods like Concept sliders can be transferred from base model to distilled models, effectively distilling control"
        },
        {
            "title": "Abstract",
            "content": "Distilled diffusion models suffer from critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at distillation.baulab.info 1. Introduction Distilled diffusion models generate images in far fewer timesteps but lack the sample diversity of their original base model counterparts. In this paper we ask: How can we distill both diversity and control capabilities from base diffusion models to their efficient distilled variants? Diffusion models demonstrate unprecedented quality [4, 13, 17, 24, 25], but their computational demands, requiring dozens or hundreds of sequential denoising steps, present significant deployment challenges. Diffusion distillation techniques [17, 18, 21, 29, 34, 35] address this by modifying base model weights to reduce required inference steps. However, this efficiency comes at critical cost: mode colCorrespondence to gandikota.ro@northeastern.edu 1 lapse, where different initial noise seeds produce visually similar outputs, creating fundamental trade-off between computational efficiency and generation diversity. Our analysis reveals surprising property: distilled diffusion models maintain consistent concept representations with their base counterparts, independent of the distillation procedure. We empirically verify this through concept transfer experiments, where control mechanisms like Concept Sliders [8, 10], LoRA adaptations [14, 16, 26] that are trained on base models can be seamlessly applied to distilled variants and vice versa without retraining. This preservation of representational structure despite the model weights modification suggests that the fundamental capabilities of base models remain intact in their distilled versions, enabling form of CONTROL DISTILLATION from base to efficient models. This raises an intriguing question: if representations are preserved, why does diversity collapse during distillation? To answer this question, we introduce novel analysis and debugging technique called DT-Visualization (Diffusion Target) that reveals what diffusion model thinks the final image will be at any intermediate timestep. Through DT-Visualization, we conduct detailed analysis of latent representations across timesteps and discover that the initial diffusion steps disproportionately determine structural composition and diversity, while subsequent steps primarily refine details. This critical insight connects our findings: while distilled models preserve concept representations, they fail to maintain the diversity-generating behavior of early timesteps, affecting both sample-level variation and distribution-level coverage. This observation motivates simple hybrid inference approach that achieves DIVERSITY DISTILLATION by strategically employing the base model for only the first critical timestep before transitioning to the distilled model for efficient completion of the generation process. By leveraging the representational compatibility between models, this approach aims to directly address the mode collapse in distilled models during the diversity-critical early denoising steps. Our experimental results reveal counterintuitive finding: this hybrid approach not only restores the diversity lost during distillation but exceeds the diversity of the original base model while maintaining nearly the computational efficiency of distilled inference. These results demonstrate that the traditional trade-off between computational efficiency and generation diversity can be mitigated through timestep-specific model selection. This work has implications for both theoretical understanding of diffusion model distillation and practical applications in model deployment. 2. Related Works Diffusion Distillation: While diffusion models [13, 30, 31] excel at high-quality image synthesis, their requirement for 20-100 sampling steps creates significant computational bottlenecks. Diffusion distillation techniques address this limitation by finetuning base models that maintain quality with fewer steps. Progressive distillation [27] established the foundation by iteratively training student models to match teacher outputs with half the sampling steps. Recent approaches have further improved efficiency through distinct methodologies: Adversarial Diffusion Distillation [29], implemented in SDXL-Turbo, integrates score distillation with adversarial training to enable high-fidelity generation in just 1-4 steps, effectively combining diffusion guidance with GAN-like discriminators. Distribution Matching Distillation [34], featured in SDXLDMD2, takes different approach by focusing on matching output distributions rather than specific trajectories, eliminating regression loss and implementing two time-scale update rule that significantly improves training stability. For balancing quality and mode coverage, Progressive Adversarial Diffusion Distillation [18] in SDXL-Lightning employs staged training with specialized latent-space discriminators, offering flexibility through checkpoints optimized for 1-8 step inference. Latent Consistency Models [21], applied in SDXL-LCM, ensure consistency in latent representations across noise levels for distillation, reducing steps to 4-8 while preserving generation quality. Despite these advances in efficiency, the relationship between model distillation and sample diversity has remained largely unexplored. Concept Representation: Research in concept representation for diffusion models has evolved from basic personalization to sophisticated control mechanisms [2, 3, 8, 22, 33, 36]. Textual Inversion [6] captures the semantics of concept with learnable embeddings in text space without modifying model weights, allowing personalization with just few images. DreamBooth [26] advanced this approach by fine-tuning models with unique identifiers and specialized prior preservation loss. Custom Diffusion [16] streamlined this process by optimizing only crossattention layers, reducing storage requirements to just 3% of model weights while enabling multi-concept customization simultaneously. For precise attribute manipulation, Concept Sliders [8] introduced low-rank adaptors that create interpretable controls over specific visual attributes like age or weather conditions. This technique was expanded in SliderSpace [10], which decomposes visual capabilities into multiple controllable dimensions from single prompt, enhancing creative exploration. Complementary to these control mechanisms, hierarchical concept trees [1, 32] were developed to enable intuitive exploration of related visual concepts. Recent work has also addressed ethical concerns 2 Figure 2. Customization adapters (custom diffusion [16] and dreambooth [26]) and concept control adapters (concept sliders [8]) trained on SDXL-base model can be transferred to all the distilled modeled without any additional finetuning. This demonstrates that concept representations are preserved through the diffusion distillation process through targeted concept removal techniques by editing selective weights [7, 9, 20], redirecting concept representations [15, 23]. Since distillation modifies the UNet model of diffusion, in this work, we mainly focus on custom concept and control representations that are captured in UNet modules. Our work uniquely explores whether such control mechanisms can be distilled from base to efficient models without additional training. 3. Control Distillation Diffusion distillation reduces computational requirements by modifying model weights to generate images in fewer timesteps, but introduces well-known limitation: mode collapse. While the diversity reduction is established, the state of internal representations during distillation remains unexplored. We investigate whether distilled models, despite producing less diverse outputs, still preserve the same concept representations as their base counterparts. To answer these questions, we investigate whether control mechanisms trained on base models can be directly applied to distilled models. Our investigation focuses on three distinct approaches for modifying diffusion models: Concept Sliders [8, 10], which are low-rank adaptors enabling fine-grained control over specific visual attributes such as age, weather, and eye size; Custom Diffusion [16], which optimizes cross-attention layers for efficient multiconcept customization; and DreamBooth [26], which enables subject-driven generation through unique identifiers and prior preservation loss. For each mechanism, we perform two types of transfer In Base Distilled Transfer, we train the experiments. control mechanism on the base model and apply it to the distilled model. Conversely, in Distilled Base Transfer, we train the control mechanism on distilled model and apply it to the base model. 3.1. Experimental Setup We experiment with multiple distilled model variants: SDXL-Turbo [29], SDXL-Lightning [18], SDXLLCM [21], and SDXL-DMD2 [34] each representing different distillation techniques. We train Concept Sliders, Custom Diffusion and DreamBooth using LoRA [14] optimization according to their official implementation. 3.2. Results To quantify transfer effectiveness, we evaluate control mechanisms both qualitatively and quantitatively. Figure 2 shows qualitative examples of how concept representations 3 Method Concept Sliders [8] Customization [16, 26] Concept Age Smile Muscular Lego Watercolor style Crayon style BaseBase BaseDMD BaseLCM BaseTurbo BaseLightning 20.4 19.7 34.6 32.2 34.3 32.7 17.8 21.4 26.7 26.8 31.4 27. 27.1 19.5 33.8 26.0 29.6 24.7 19.0 33.5 39.0 30.3 27.5 29.5 24.8 14.0 33.2 29.7 39.2 32.5 Table 1. We show the percentage change in CLIP score from the original image and the LoRA edited image. Higher values indicate stronger attribute change or style transfer. Control effectiveness is largely preserved when transferring from base to different distilled models, with only minor variations across distillation techniques. can be transferred seamlessly from base model to distilled models. For example, the comical big eyes slider trained on SDXL controls Turbos generations, despite latter being distilled model requiring 1-4 steps compared to SDXLs 20-100. Table 1 presents quantitative results showing CLIP scores [11] for various attributes. The transfer effectiveness remains consistently high across all tested combinations, confirming our hypothesis that concept representations are preserved during distillation. We show more experiments for the DistillationBase evidence in Appendix. This representational compatibility raises an intriguing question: if concept representations are preserved during distillation, why do distilled models exhibit reduced diversity?. To analyze this question, we introduce visualization technique to better understand diffusion generation in the next section. 4. DT: Diffusion Target Visualization To better analyze the information at each denoising step, we introduce DT-Visualization (Diffusion Target Visualization), technique that allows us to interpret what the model thinks the final image will be at any intermediate timestep, without actually completing the full denoising process. Let x0 be an initial image and xT be pure Gaussian noise. The forward diffusion process gradually adds noise to the image. The generative process aims to reverse this diffusion, starting from xT and progressively denoising to reconstruct x0. At timestep t, the model takes xt as input and predicts noise ϵθ(xt, t) to compute the next step xt1: xt xt1 = 1 αtϵθ(xt, t) αt (1) We can estimate the final image x0,t by taking the same direction ϵθ(xt, t) for all remaining diffusion steps. This can be achieved by recursively applying the denoising from Equation 1 with the same noise prediction. We label this approach DT-Visualization: 4 xt x0,t = 1 αtϵθ(xt, t) αt (2) where αt is the cumulative noise schedule parameter. This enables us to visualize the final image x0,t that the diffusion model is implicitly planning at each timestep without actually running denoising forward passes through all remaining timesteps. By computing x0,t at various timesteps throughout the denoising process and comparing the results, we can assess the contribution of each step to the final output. 4.1. DT for Investigating Generation Artifacts DT-Visualization serves as debugging tool for investigating artifacts and inconsistencies in diffusion model outputs. Figure 3 demonstrates this capability when analyzing generation prompted with Image of dog and cat sitting on sofa. While the final image appears to contain only dog, DT-Visualization at intermediate timestep = 10 reveals that the model initially conceptualized cat face (red box), but later retracted this decision by the final generation step. This insight exposes how diffusion models can change their mind during the denoising process, sometimes discarding semantic elements present in the prompt. By comparing visualizations across different timesteps, we can pinpoint exactly when and how these decisions occur, providing valuable insights for model developers to address inconsistencies between prompts and generations. This visualization technique helps explain why models sometimes produce outputs that lack requested elements despite properly understanding the prompts semantics. 4.2. DT for Investigating Mode Collapse Building on DT-Visualizations effectiveness for uncovering generation artifacts, we extend this technique to examine the mechanisms underlying mode collapse in distilled diffusion models. By applying this visualization approach to both base and distilled model generations, we can directly observe differences in how these models develop image structure throughout the denoising process. Through both qualitative examples and quantitative analysis, we demonFigure 3. DT-Visualization reveals generation inconsistencies. When prompted with Image of dog and cat sitting on sofa, the SDXL model produces an image with only dog. However, DTVisualization at = 10 shows the model initially conceptualizing cat face (red box) before abandoning this element in the final generation. This demonstrates how diffusion models can discard semantic elements during the denoising process. strate how DT-Visualization provides critical insights into the diversity reduction phenomenon. Figure 4 presents qualitative DT-Visualization results for the prompt picture of dog. Standard diffusion denoising visualizations (left) show minimal differences between model variants, but our DTVisualization technique (right) suggests potential explanation for mode collapse: distilled models appear to commit to final image structure almost immediately after the first denoising step, while base models progressively develop structural elements across multiple steps. This observation suggests that early timesteps might play disproportionate role in determining output diversity. If distilled models make critical structural decisions in single timestep, this could explain their tendency to produce similar outputs across different random seeds. Base models, with their gradual structural refinement, might maintain greater output diversity precisely because they distribute these decisions, as shown in Figure 6. Figure 5 quantifies this phenomenon by plotting the DreamSim similarity [5] distance between intermediate DT-visualizations and final images across COCO-10k [19] prompts. The data indicates that distilled models establish significant structural composition within single timestep, whereas base models require approximately 30% of their total inference steps to achieve comparable structural definition. These observations raise an intriguing question: if concept representations are preserved during distillation but diversity is reduced, could the first timestep be the critical factor? In the next section, we conduct causal experiments to determine whether the first timestep is indeed responsible for mode collapse, and explore how this insight might lead to solutions that preserve both efficiency and diversity. 5. Diversity Distillation Our DT-Visualization analysis established notable correlation between early timesteps and structural diversity in diffusion outputs. This motivates our investigation into whether initial denoising steps causally determine the diversity characteristics of generated samples. To empirically test this hypothesis, we propose hybrid inference approach that selectively combines base and distilled models during generation, enabling systematic examination of the mechanisms underlying mode collapse. We implement this approach in Algorithm 1, which uses the base model for the critical first timestep(s) to establish diverse structural compositions, then transitions to the distilled model for efficient refinement of details. This method leverages complementary strengths of both models while addressing their respective weaknesses, effectively distilling diversity from the base model into the distilled models generation process without requiring additional training or model modifications. Algorithm 1 Hybrid Inference for Diversity Distillation Require: Base model fbase, distilled model fdistil, total timesteps , transition point Ensure: Generated image x0 1: Initialize xT (0, I) 2: for = T, 1, . . . , 1 do 3: 4: 5: 6: 7: 8: end for 9: return x0 end if else if > then Critical timesteps for diversity xt1 fbase(xt, t, prompt) Efficient refinement timesteps xt1 fdistil(xt, t, prompt) 5.1. Experimental Results We evaluate our method on two distinct dimensions of diversity: distributional diversity and sample diversity. Distributional diversity measures how well the generated distribution matches the real training data distribution. It evaluates whether the model can generate outputs across the full spectrum of the training data when given various prompts, assessed primarily through FID [12] (lower is better). Sample diversity measures the variation among outputs generated from the same prompt with different random seeds, quantified by average pairwise DreamSim distance [5] (higher is better). Distributional Diversity. Table 2 presents comparison of our diversity distillation approach against base and distilled models. We measure the FID between the generated samples from baselines against the real COCO-30k Figure 4. Comparison of standard diffusion visualization vs. our Diffusion Target (DT) visualization. Left: Standard visualization of intermediate latents shows subtle differences between base and distilled models. Right: Our DT visualization technique reveals dramatic differences in how models predict the final output. Distilled models commit to final image structure in the first timestep, while base models gradually refine structure across multiple steps, explaining the observed mode collapse in distilled models. Method Steps FID() IS() CLIP() Time (s)() Base Distilled Hybrid (Ours) 50 4 4 12.74 15.52 10.79 24.74 27.20 26.13 31.83 31.69 32.12 9.22 0.64 0.64 Table 2. Comparing the distributional diversity using FID shows that our diversity distillation approach achieves diversity comparable to or better than the base model (SDXL-Base [24]) while maintaining nearly the computational efficiency of the distilled model (SDXL-DMD [34]). dataset as proxy for training dataset. We find that our approach has better FID (lower) and CLIP (higher) scores on COCO-30k dataset than both the distilled and base models while being as fast as distilled model. Sample Diversity. To specifically measure the diversity of samples for given prompt, we utilize sample diversity metric [10] based on DreamSim distance. For each baseline variant, we generate 100 images for the same prompt and 6 Figure 5. Measuring the dreamsim distance between intermediate DT-visualization and final image reveals that distilled models establish structural image composition within the initial diffusion step, whereas base models require approximately 30% of steps to achieve comparable structural definition. Figure 6. Visual comparison of generation diversity. Each row shows three different generations (different random seeds) for the same prompt using: (left) base model, (middle) distilled model, and (right) our diversity distillation approach. Note how the distilled model produces visually similar outputs across seeds, while our approach restores diversity comparable to the base model while maintaining similar inference speed as distilled model. Distilled Hybrid (Ours) 5.2. Hyperparameter Analysis"
        },
        {
            "title": "Prompt\nSunset beach\nCute puppy\nFuturistic city\nPerson\nVan Gogh art\nAverage",
            "content": "Base 0.396 0.233 0.237 0.484 0.337 0.337 0.271 0.199 0.198 0.347 0.305 0.264 0.373 0.265 0.283 0.461 0.366 0.350 Table 3. Sample diversity measured by average pairwise DreamSim distance (higher is more diverse). Our hybrid approach not only restores diversity lost during distillation but exceeds the diversity of the base model. calculate the average pairwise DreamSim distance between samples. Table 3 shows that our approach restores the lost sample diversity in the distilled models. Figure 6 provides visual comparison of generation diversity across methods. The distilled model clearly exhibits less structural diversity across random seeds compared to the base model, while our hybrid approach successfully distills this diversity while maintaining faster inference speeds. These results demonstrate that the traditional trade-off between computational efficiency and generation diversity can be effectively mitigated through our proposed diversity distillation approach. By strategically combining the strengths of base and distilled models, we achieve diversity distillation without requiring additional training or model modifications. 7 We conduct an analysis of different hyperparameters and variations of our approach to understand their impact on diversity and quality. Figure 7 presents our findings across multiple dimensions. First, as shown in Figure 7(a), the guidance scale from the base model significantly impacts diversity, with optimal performance occurring around zero guidance. This suggests that minimal guidance from the base model preserves the natural diversity of outputs. Figure 7(b) demonstrates the effect of varying k, the number of distilled model steps replaced by base model inference. Notably, even using the base model for just the first timestep (k=1) provides substantial diversity gains with minimal computational overhead. This confirms our hypothesis that the earliest timesteps are particularly critical for establishing output diversity in diffusion models. The computational efficiency of our approach is analyzed in Figure 7(c), which compares the total computational cost when replacing the first timestep of the distilled model with varying numbers of base model steps. Our results indicate that 1:1 replacement ratio achieves the optimal balance between diversity enhancement and computational efficiency. More extensive use of the base model provides diminishing returns while significantly increasing inference time. Figure 7. (a) Impact of guidance scale from the base model on diversity shows optimal performance around 0 guidance. (b) Effect of the number of distilled model steps (k) being replaced by base model inference. Running distilled model from first timestep (k = 1) provides diversity gains with minimal computational overhead. (c) Comparing the total timesteps of base model when replacing the first timestep of distilled model shows that replacing 1-1 timesteps of distilled with base is most ideal. Method Steps FID() IS() CLIP() Time (s)() Hybrid (Ours) Skip First Timestep 4 10.79 10.12 26.13 24.69 32.12 31.71 0.64 0.53 Table 4. Skipping first timestep demonstrates superior FID scores and faster inference time but underperforms on generative quality as indicated by CLIP [11] and Inception [28] scores. Resource-Efficient Alternative: For scenarios where loading both models simultaneously is not feasible [17], we explore an alternative approach: skipping the first step altogether in distilled model inference. This approach, compared in Figure 7 and Table 4 alongside our diversity distillation method and baselines, shows that simply skipping the first timestep in distilled inference provides significant boost in diversity. This suggests that the first timestep in distilled models constrains diversity, and removing its influence allows for more varied outputs. While this approach is more resource-efficient than loading both models, our hybrid method still achieves superior results in terms of quality, as shown by CLIP and IS scores. We provide qualitative examples in Appendix. 6. Limitations While our approach significantly improves diversity without substantial computational overhead, several limitations remain. First, our method requires maintaining both base and distilled models in memory, increasing resource requirements compared to distillation-only approaches. Future distillation works could explore our insights to design diversity preserving model into single distilled model. Second, our analysis focused primarily on image diversity metrics, but further investigation is needed to understand the impact on semantic diversitythe range of concepts and compositions the model can generate. Developing more sophisticated diversity metrics that capture both visual and semantic variation would provide deeper insights into the distillation process. Finally, our approach treats all prompts uniformly, but different content types may benefit from different base/distilled step allocations. Adaptive inference strategies that dynamically adjust the transition point based on prompt characteristics could further optimize the quality-efficiency trade-off. 7. Conclusion This work addresses fundamental limitation of distilled diffusion models: the trade-off between computational efficiency and sample diversity. Our contributions are threefold: (1) We demonstrate that distilled models preserve the concept representations of base models, enabling seamless transfer of control mechanisms like Concept Sliders and LoRAs without retraining; (2) We introduce DT-Visualization, revealing that initial timesteps disproportionately determine structural composition in the generation process; and (3) Based on these insights, we present diversity distillation, hybrid inference approach that strategically employs the base model for only the first critical timestep before switching to the efficient distilled model. Our experimental results challenge the conventional diversity-efficiency trade-off. Diversity distillation not only restores but exceeds the diversity of the original base model while maintaining the computational efficiency of distilled inference (0.64s vs. 9.22s per image). By eliminating this traditional trade-off without additional training or model modifications, our approach bridges the gap between high-quality, diverse generations and fast inference, opening new possibilities for real-time creative applications."
        },
        {
            "title": "Acknowledgment",
            "content": "RG and DB are supported by Open Philanthropy and NSF grant #2403304."
        },
        {
            "title": "Code",
            "content": "available code. Our methods as are Source code, and data sets for results can be found at sliderspace.baulab.info and at our GitHub repo github.com/rohitgandikota/sliderspace open-source reproducing our"
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers, pages 112, 2023. 2 [2] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2256022570, 2023. 2 [3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 2 [4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 1 [5] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 5 [6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 Joanna Materzynska, [7] Rohit Gandikota, Jaden FiottoKaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 24262436, 2023. 3 [8] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. In European Conference on Computer Vision, pages 172188. Springer, 2024. 2, 3, 4 [9] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 51115120, 2024. 3 9 [10] Rohit Gandikota, Zongze Wu, Richard Zhang, David Bau, Eli Shechtman, and Nick Kolkin. Sliderspace: Decomposing the visual capabilities of diffusion models. arXiv preprint arXiv:2502.01639, 2025. 2, 3, 6 [11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 4, 8 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2, 3 [15] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2269122702, 2023. 3 [16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. 2, 3, 4 [17] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 8 [18] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 1, 2, 3 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 5 [20] Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6430 6440, 2024. 3 [21] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 2, 3 [22] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, pages 42964304, 2024. [23] Minh Pham, Kelly Marshall, Chinmay Hegde, and Niv Cohen. Robust concept erasure using task vectors. arXiv preprint arXiv:2404.03631, 2024. 3 [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 6 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1 [26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 3, 4 [27] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 8 [29] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. 1, 2, 3 [30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. 2 [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [32] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. ACM Transactions on Graphics (TOG), 42(6): 113, 2023. 2 [33] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [34] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. 1, 2, 3, 6 [35] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 1 [36] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Control Distillation: Reverse Transfer D. Skip Step Approach In the main paper, we introduced resource-efficient alternative to our hybrid approach: skipping the first timestep altogether in distilled model inference. We provide additional qualitative comparisons between this approach and our hybrid method in Figure D.1. The skip-first-step approach provides reasonable compromise when resource constraints prevent loading both models simultaneously. However, our quantitative analysis in the main paper and these qualitative examples demonstrate that the hybrid approach consistently achieves superior results in terms of both diversity and quality. In the main paper, we demonstrated that control mechanisms trained on base models can be seamlessly transferred to distilled models. Here, we present additional results for the reverse direction: transferring control mechanisms trained on distilled models to base models. This bidirectional transfer capability further validates our hypothesis that concept representations are preserved during the distillation process. We note that while most control mechanisms transferred effectively, we encountered difficulties training LoRA adaptations on LCM due to its specialized architecture and training procedure. These challenges highlight potential avenues for future research in developing more universally transferable control mechanisms. B. Mode Collapse and Diversity The main paper introduced our finding that distilled diffusion models suffer from reduced sample diversity (mode collapse) compared to their base counterparts. We provide additional qualitative examples in Figure B.1-B.4 that visually demonstrate this phenomenon across various prompts and model variants. These examples highlight the significant diversity loss in distilled models. While the distilled models produce high-quality images, they often converge to similar structural compositions regardless of random seed initialization. Our diversity distillation approach effectively addresses this limitation, restoring the variety of outputs comparable to the base model while maintaining computational efficiency. C. Extended DT-Visualization Analysis The main paper introduced our DT-Visualization technique for analyzing how diffusion models develop structural information during the denoising process. We present additional visualizations in Figures C.1, C.2 that further illuminate the differences between base and distilled models. These visualizations reinforce our key finding: distilled models compress the diversity-generating behavior distributed across early timesteps in base models into single initial step, explaining the observed mode collapse. This insight directly informed our hybrid inference approach, which strategically leverages the diversity-generating capabilities of base models in critical early steps. 1 Figure A.1. Reverse Control Transfer: Control mechanisms (Custom Diffusion [16] and Concept Sliders [8]) trained on distilled models can be effectively transferred to base models without retraining. This bidirectional transferability confirms that concept representations are preserved during diffusion distillation. Note: LCM LoRA transfers were excluded due to training difficulties with the LCM architecture. 2 Figure B.1. Comparison of generation diversity across different models for the prompt image of toy. Each image shows different seeds for the same model. Note the structural similarity in distilled model outputs compared to the greater variation in base model and our hybrid approach. 3 Figure B.2. Comparison of generation diversity for image of flower Distilled models (middle column) produce structurally similar outputs across different seeds, while our approach (right column) restores diversity comparable to the base model (left column) while maintaining the speed advantage of distilled models. 4 Figure B.3. Additional diversity comparison for city streetDistilled models (middle column) produce structurally similar outputs across different seeds, while our approach (right column) restores diversity comparable to the base model (left column) while maintaining the speed advantage of distilled models. 5 Figure B.4. Diversity comparison for abstract prompt: picture of monster Distilled models (middle column) produce structurally similar outputs across different seeds, while our approach (right column) restores diversity comparable to the base model (left column) while maintaining the speed advantage of distilled models. 6 Figure C.1. Extended DT-Visualization comparison between SDXL-Base and SDXL-DMD for the prompt. The visualization reveals that DMD commits to final structural composition within the first timestep, while Base gradually develops structure across multiple steps. This pattern is consistent across different content types and prompts. 7 Figure C.2. Extended DT-Visualization comparison between SDXL-Base and SDXL-DMD for the prompt. The visualization reveals that DMD commits to final structural composition within the first timestep, while Base gradually develops structure across multiple steps. This pattern is consistent across different content types and prompts 8 Figure D.1. Qualitative comparison between (left) our hybrid approach, (right) skip-first-step approach. The skip-first-step approach improves diversity over the standard distilled model but exhibits reduced quality compared to our hybrid method, particularly in fine details and coherence."
        }
    ],
    "affiliations": []
}