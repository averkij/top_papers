{
    "paper_title": "World Models for Policy Refinement in StarCraft II",
    "authors": [
        "Yixin Zhang",
        "Ziyi Wang",
        "Yiming Rong",
        "Haoxi Wang",
        "Jinling Jiang",
        "Shuang Xu",
        "Haoran Wu",
        "Shiyu Zhou",
        "Bo Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is a challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating a learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2's hybrid dynamics, we introduce a structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop a multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWM's substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, a world-model-augmented decision system that integrates StarWM into a Generate--Simulate--Refine decision loop for foresight-driven policy refinement. Online evaluation against SC2's built-in AI demonstrates consistent improvements, yielding win-rate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment."
        },
        {
            "title": "Start",
            "content": "Yixin Zhang1,2 , Ziyi Wang1,2 , Yiming Rong1,2 , Haoxi Wang1,2 , Jinling Jiang1,2 , Shuang Xu1 , Haoran Wu1 , Shiyu Zhou1 , Bo Xu1,2 1The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences 2School of Artificial Intelligence, University of Chinese Academy of Sciences {zhangyixin2024, wuhaoran2018, shiyu.zhou, xubo}@ia.ac.cn 6 2 0 2 6 1 ] . [ 1 7 5 8 4 1 . 2 0 6 2 : r Abstract Large Language Models (LLMs) have recently shown strong reasoning and generalization capabilities, motivating their use as decision-making policies in complex environments. StarCraft II (SC2), with its massive state-action space and partial observability, is challenging testbed. However, existing LLM-based SC2 agents primarily focus on improving the policy itself and overlook integrating learnable, action-conditioned transition model into the decision loop. To bridge this gap, we propose StarWM, the first world model for SC2 that predicts future observations under partial observability. To facilitate learning SC2s hybrid dynamics, we introduce structured textual representation that factorizes observations into five semantic modules, and construct SC2-Dynamics-50k, the first instructiontuning dataset for SC2 dynamics prediction. We further develop multi-dimensional offline evaluation framework for predicted structured observations. Offline results show StarWMs substantial gains over zero-shot baselines, including nearly 60% improvements in resource prediction accuracy and self-side macro-situation consistency. Finally, we propose StarWM-Agent, worldmodel-augmented decision system that integrates StarWM into GenerateSimulateRefine decision loop for foresight-driven policy refinement. Online evaluation against SC2s built-in AI demonstrates consistent improvements, yielding winrate gains of 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macro-management stability and tactical risk assessment."
        },
        {
            "title": "1 Introduction\nIn recent years, Large Language Models (LLMs) have demon-\nstrated remarkable reasoning and generalization capabil-\nities, extending their utility from general language tasks",
            "content": "These authors contributed equally. Corresponding authors. 1Code is available at: https://github.com/yxzzhang/StarWM. to complex decision-making scenarios Yao et al. [2023]; Schick et al. [2023]; Wang et al. [2023a]. StarCraft II (SC2), with its enormous state-action space and imperfect information setting, serves as an ideal environment for testing the complex decision-making capabilities of LLMs. Recent work has explored LLM-based SC2 policies from multiple angles, including observation summarization and memory for long-context management Ma et al. [2023]; Qi et al. [2025b], augmenting inputs with external knowledge and multi-modal features Li et al. [2024], and hierarchical designs that separate strategic planning from tactical execution Shen et al. [2025]. Despite these advances, most methods primarily focus on improving the policy itself, while leaving key component unexplored: integrating learnable action-conditioned transition model into the decision loop for foresight-driven policy refinement. Cognitive science research indicates that when handling complex tasks, humans often rely on internal causal world models to perform short-term simulation, adjusting actions to avoid penalties and maximize rewards Lake et al. [2017]. In machine learning, world models have been extensively studied across multiple domains, such as model-based RL Janner et al. [2019]; Schrittwieser et al. [2019]; Hafner et al. [2023] and autonomous driving Wang et al. [2023b]; Russell et al. [2025]. However, to the best of our knowledge, there is still no systematic study of world models for SC2. Earlier attempts such as StarCraft Defogger Synnaeve et al. [2018] focus on state extrapolation (P (st+k ot)) rather than action-conditioned dynamics modeling (P (st+k ot, at)), and thus cannot serve as forward simulator for action-conditioned lookahead and policy refinement. We attribute the lack of SC2 world models to two major challenges. First, dynamics learning is intrinsically hard in SC2: the environment exhibits strongly coupled hybrid dynamics, involving resource flows, task progression, microlevel unit kinematics, and combat evolution governed by damage mechanics, all under partial observability. Second, decision integration is non-trivial: even with learned world model, it remains unclear how to seamlessly integrate predicted futures into an LLMs text-based decision process without resorting to expensive search algorithms. To address these challenges, we propose StarWM, the first world model for SC2: learnable action-conditioned dynamics model that predicts short-horizon future observaFigure 1: Case study comparing our world-model-augmented decision system (StarWM-Agent) with policy that does not use world model. Given the current observation, the LLM policy initially proposes Build Supply Depot. 5-second rollout by the world model predicts that minerals will drop to 50 and the supply depot will be 23% complete, while unused supply remains 18. Based on this prediction, the system revises the action to Train SCV, avoiding premature infrastructure expenditure that would lead to mineral shortage. This example illustrates that incorporating world model can improve macro-management decision-making. tions under partial observability. Concretely, we introduce structured textual observation representation that factorizes SC2 observations into five semantic modules. Based on this representation, we construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further develop multi-dimensional offline evaluation framework that measures world model quality across economy, development, micro-entities, and macrosituation. Offline evaluation results show that StarWM achieves substantial improvements over zero-shot baselines, including nearly 60% gains in resource prediction accuracy and self-side macro-situation consistency, demonstrating StarWMs capability to capture key deterministic dynamics and combat attrition mechanisms of SC2. Finally, we propose StarWM-Agent, world-model-augmented decision system that integrates StarWM into Generate SimulateRefine decision loop for model-based foresightdriven policy refinement. Online tests against SC2s builtin AI demonstrate consistent gains, improving win rates by 30%, 15%, and 30% against Hard (LV5), Harder (LV6), and VeryHard (LV7), respectively, alongside improved macromanagement stability and tactical risk assessment. In summary, our main contributions are threefold: World Model for SC2 Dynamics: We propose StarWM, the first action-conditioned world model for SC2. By introducing structured textual observation representation to factorize hybrid dynamics, StarWM successfully captures key deterministic dynamics and combat attrition mechanisms. Dataset and Evaluation Framework: We construct SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction, and develop multidimensional offline evaluation framework to systematically assess predictive quality across economy, development, micro-entities, and macro-situation. World-Model-Augmented Decision System: We present StarWM-Agent, world-model-augmented decision system with GenerateSimulateRefine decision loop that leverages StarWMs action-conditioned predictions for inference-time policy refinement and yields consistent online improvements."
        },
        {
            "title": "2.1 World Models in Decision Making\nResearch on world models in decision making focuses on\nconstructing internal representations of the environment\nto support policy learning through imagination or on-\nline planning.\nIn model-based RL, pioneering works such\nas DreamerV3 [Hafner et al., 2023] and MuZero [Schrit-\ntwieser et al., 2019] construct latent dynamics models, en-\nabling imagination-based policy optimization and online\nMonte Carlo Tree Search (MCTS), respectively, achieving\nsignificant progress in long-horizon sparse reward tasks\nlike Minecraft and board games. With the development\nof generative models, DriveWM [Wang et al., 2023b] and\nGAIA-2 [Russell et al., 2025] in autonomous driving gener-\nate high-fidelity video streams for online trajectory plan-\nning or offline long-tail data synthesis.\nIn semantically\nrich textual decision-making environments, methods such as\nRAP [Hao et al., 2023], Reflexion [Shinn et al., 2023] and Web-\nDreamer [Gu et al., 2024] explore using LLMs as world mod-\nels to simulate the consequences of candidate actions and\nestimate their values.",
            "content": "However, to the best of our knowledge, prior works have not studied world models for SC2, complex real-time strategy environment characterized by partial observability and coupled hybrid dynamics. We propose the first LLM-based world model for SC2 and explore its effectiveness in decision making."
        },
        {
            "title": "2.2 LLMs in StarCraft II\nThe success of AlphaStar Vinyals et al. [2019] demonstrated\nthe potential of end-to-end neural networks in SC2, but it re-\nlies on massive human data and long-term league-based self-\nplay training, with high computational costs and unverified\nout-of-distribution generalization. Recent work explores\nleveraging the pre-training knowledge of LLMs to build gen-\neralized and interpretable agents with low training resource\nconsumption. Existing studies explore enhancing LLM de-\ncision performance from various dimensions: TextStarCraft\nII Ma et al. [2023] proposes Chain of Summarization (CoS) to\ncompress observation histories; LLM-PySC2 Li et al. [2024]\nintroduces external Wiki knowledge and multi-modal obser-",
            "content": "vations. StarEvolve Shen et al. [2025] employs hierarchical framework to decouple strategic planning and tactical execution. Comm-CoT Qi et al. [2025b] achieves task decomposition via multi-agent collaboration. MASMP Qi et al. [2025a] introduces natural language state machines and strategic memory to constrain action generation. However, most existing methods focus on enhancing the LLM-based policy itself and have not systematically explored introducing learnable dynamics model into the decision loop. We propose world model for SC2 to perform shorthorizon lookahead given observations and candidate actions, enabling policy refinement for more reliable decision making."
        },
        {
            "title": "3 Method\n3.1 Problem Modeling\na Partially Observable Markov\nWe model SC2 as\nDecision Process\nrepresented as a tuple\n⟨S, A, T , R, Ω, O, γ⟩, where S, A, Ω denote the state,\naction, and observation spaces, T (s′|s, a) and O(o|s′, a)\nrepresent\nthe transition and observation probabilities,\nR(s, a) is the reward function, and γ ∈ [0, 1] is the discount\nfactor. Under this framework, it is crucial to distinguish\nbetween the Environment Simulator and the World Model.",
            "content": "(POMDP), Environment Simulator vs. World Model The Environment Simulator (God View), such as the SC2 Engine, operates on the assumption of perfect information with access to the global state st S. Its transition function : Aall calculates the next global state based on joint actions of all players. While it acts as the executor of objective physical rules, it is inherently inaccessible to any player constrained by the Fog of War. The World Model (Player View) studied in this paper is built on single players restricted perspective, constrained by the Fog of War and imperfect information. It can only access local observations ot and faces epistemic uncertainty regarding global states and opponent intentions. We define the world model as probabilistic, action-conditioned dynamics model that learns the distribution over future observations under partial observability, rather than full-state transitions. Formulation of Task We formulate the world models prediction task as follows: given the players current observation ot and sequence of intended actions at:t+τ , the world model Mϕ aims to predict the future observation ˆot+τ after τ steps. Formally: ˆot+τ PMϕ(ot+τ ot, at:t+τ ) . (1)"
        },
        {
            "title": "Dynamics Factorization",
            "content": "The SC2 engine exposes highly heterogeneous information, including scalars (e.g., minerals, gas, supply), discrete categorical variables (e.g., unit types, upgrades), and continuous spatial coordinates (e.g., positions). We adopt text as unified representation, as it provides semantically compatible interface that naturally maps heterogeneous information into LLM-compatible text space. Our key insight is that SC2s state evolution is inherently multi-task dynamics prediction problem: different parts of the state obey different dynamics. For instance, resource changes are governed by additive accumulation and consumption; construction, production and upgrade progress follow deterministic temporal progression; and unit movement follows spatial kinematics, while combat outcomes are driven by damage mechanics and interaction rules. Thus, we propose textual observation representation, explicitly factorizing observation into five distinct semantic modules. This design decomposes the observation-level dynamics into set of sub-dynamics {f1, f2, ..., fn}. The observation ot is structured as: 1. Info: Describes economy and status (Minerals, Gas, Collection Rate, Supply, Alerts, Upgrades). This module isolates numerical flow from spatial complexity. 2. Queue: Records ongoing tasks (construction, production, upgrades) and their progress. This module focuses on deterministic temporal progression. 3. My Units: Includes self units IDs, positions, health percentage (HP), energy and status. This module focuses on kinematics and damage interactions. 4. My Structures: Describes self static assets. This module separates structures from units to focus on state switching. 5. Visible Hostiles: Includes visible enemy units, structures, and snapshot enemy structures under the fog of war. This module isolates uncertainty handling for partial observability. This structure encourages the world model to invoke different sub-dynamics for different tasks, reducing the learning burden and accelerating convergence. Based on this representation, we construct SC2-Dynamics50k, trajectory-based dataset for dynamics prediction, and train the StarWM via supervised fine-tuning to learn actionconditioned observation dynamics. This prediction task involves two key challenges:"
        },
        {
            "title": "3.3 Multi-Dimensional Offline Evaluation",
            "content": "Intrinsic Evolution: Predicting the deterministic impact of the actions on economy, supply and development (e.g., resource consumption, supply usage and task progress). Extrinsic Interaction: Implicitly deducing possible opponent actions and interaction results (e.g., combat outcomes)."
        },
        {
            "title": "Framework",
            "content": "Existing metrics like BLEU or ROUGE fail to reflect numerical magnitude, spatial consistency, and logic. Thus, they are not suitable for evaluating structured textual observation representations. To overcome this, we propose multidimensional structured evaluation framework assessing four dimensions: Economy & Status. This dimension evaluates the models ability to predict economy and status. We adopt the Symmetric Mean Absolute Percentage Error (SMAPE) to ensure numerical stability: SMAPE ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 yt ˆyt (yt + ˆyt)/2 + ϵ , (2) where yt and ˆyt are the ground-truth and predicted values at time step t, is the total number of evaluation steps, and ϵ ensures numerical stability. For sparse events like Alerts and Upgrades, we calculate F1 Score only on active frames to avoid inflated scores caused by trivial empty frames. Development. This dimension evaluates the accuracy of predicting ongoing tasks and their progress, focusing on construction, production, and research queues. We calculate Queue F1 score to measure task prediction accuracy. For correctly predicted tasks, we compute the Progress MAE to assess the models ability to capture temporal progression. Micro-Entity. This dimension evaluates unit existence and attribute accuracy. We adopt hybrid matching strategy to construct mapping between predicted units pj and ground-truth units gi. pair is counted as true positive (TP) if either (i) ID-anchored: id(pj) = id(gi), or (ii) spatialanchored: type(pj) = type(gi) and pos(pj) pos(gi)2 δ. Unmatched predictions are false positives (FP), and unmatched ground-truth units are false negatives (FN), from which we compute Precision, Recall, and F1. For matched pairs, we report attribute MAE on HP and energy. Macro-Situation. The macro situation can be characterized by the spatial distribution of entities from both players. To measure the difference between predicted and groundtruth spatial distributions, inspired by optimal transport Villani [2008], we introduce the Augmented Wasserstein Distance (AWD). It calculates the minimum cost to transform the predicted distribution to the ground-truth distribution while penalizing unmatched entities. Given the ground- , truth set = {gi}M j=1 i=1 we formulate linear sum assignment problem with an augmented cost matrix: and the predicted set = {pj}N = (cid:20)Dmatch Dmiss Dhalluc 0 (cid:21) R(M +N )(N +M ), (3) where Dmatch RM measures pairwise Euclidean distance between predicted and ground-truth entities with Cij = if type(gi) = type(pj) to forbid cross-type matching. Dmiss RM and Dhalluc RN are diagonal matrices whose entries are set to penalty λ to penalize unmatched units. We solve the assignment problem using the Hungarian algorithm Kuhn [1955] to obtain the minimum total cost Ltotal, and define the final metric: AWD = Ltotal + . (4) Lower AWD indicates higher consistency with the groundtruth macro situation. Figure 2: Framework of our StarWM-Agent, which follows GenerateSimulateRefine loop: the policy first generates an initial action proposal from the current observation, the world model predicts the short-horizon future observation, and the policy then refines the action conditioned on the predicted future."
        },
        {
            "title": "3.4 World-Model-Augmented Decision System",
            "content": "for Online Testing To study the effect of incorporating world model into the decision loop, we propose StarWM-Agent, worldmodel-augmented decision system that follows Generate SimulateRefine pipeline for foresight-driven policy refinement. As illustrated in Figure 2, the policy first generates an initial action proposal, after which the world model predicts the resulting future observation. The predicted observation is then fed back to the policy to refine its decision. Algorithm 1 also details the inference procedure. By incorporating world model, StarWM-Agent obtains two dimensions of cognitive enhancement. At the macromanagement level, StarWM extends the agents effective time horizon by forecasting resource flow, supply, and task progress, enabling preemptive planning for upcoming bottlenecks (e.g., resource shortages or supply caps). At the microtactical level, StarWM serves as lightweight combat-andfeasibility simulator, assisting in assessing tactical risks (e.g., unfavorable engagements) before execution."
        },
        {
            "title": "4 Experiment\n4.1 Setup\nThe SC2-Dynamics-50k Dataset\nWe build SC2-Dynamics-50k, the first instruction-tuning\ndataset for SC2 dynamics prediction. We focus on Terran vs.\nTerran (TvT) games on the Flat64 map, as this setting pro-\nvides sufficient complexity for evaluating our core method-\nology. Extending to all races and maps is mainly an engi-\nneering scaling issue, and is beyond the primary focus of this\npaper.",
            "content": "Algorithm 1 Inference Procedure of StarWM-Agent Input: Current observation ot, Policy Model πθ, World Model Mϕ, Prediction horizon τ Output: Refined Action aref ined 1: // Phase 1: Initial Proposal 2: ainit πθ(ot) 3: // Phase 2: Forward Simulation 4: ˆot+τ Mϕ(ot, ainit) // Predict future observation via // Generate an initial action proposal dynamics 5: // Phase 3: Context Construction 6: ct Concatenate(ot, ainit, ˆot+τ ) context with predicted future // Augment policy 7: // Phase 4: Refinement 8: aref ined πθ(ct) predicted future 9: return aref ined // Refine action conditioned on Data Collection. We collect trajectories by running rule-based bot against SC2s built-in AI at Harder (LV6) and VeryHard (LV7), with 50 trajectories per level (100 trajectories in total). Data Processing. We then split trajectories into train/validation/test with ratio of 8:1:1. We set the prediction horizon to 5 seconds (i.e., τ = 5) with sliding window step of 1 second. Using the structured textual observation representation, we parse replays and convert them into instructiontuning pairs of the form: (ot, at:t+τ ) ot+τ . This yields 50,407 training samples, 6,774 validation samples, and 6,579 test samples. Appendix E.1 provides detailed sample of SC2-Dynamics-50k. (5) Implementation Details World Model Training. We use Qwen3-8B as the backbone of StarWM and perform supervised fine-tuning (SFT) with LLaMA-Factory Zheng et al. [2024]. We adopt LoRA Hu et al. [2021] with rank 8, learning rate of 5e-5, and train for 10 epochs on 8H100 GPUs. For the macro-situation Offline Evaluation Settings. metric (AWD), we set the penalty λ = 90.5, which corresponds to the diagonal distance of the Flat64 map. For micro-entity matching, we set the spatial threshold δ = 10.0. We compare StarWM against three baselines: Static Bias (copying the input observation as the prediction), Qwen3-8B (zero-shot), and Qwen3-32B (zero-shot). Online Testing Settings. We adopt SC2Arena Shen et al. [2025] as the online testing framework, with an LLM serving as the policy model. We treat the zero-shot Qwen3-8B and Qwen3-32B policies in SC2Arena as baselines. To implement our StarWM-Agent, we extend SC2Arena with additional modules for world model prediction and action refinement, while reusing its original components for initial action generation (including instructions) and interaction with the underlying game engine. We conduct online matches against the SC2s built-in AI at Hard (LV5), Harder (LV6), and VeryHard (LV7), which represent the highest non-cheating difficulty levels. Due to resource constraints, all online experiments are conducted under the /no_think setting. Metrics for Online Testing. We report multiple online metrics, including win rate, supply block rate, resource conversion rate, kill-loss ratio, and valid action rate, to comprehensively characterize decision quality. Detailed metric definitions and formulas are provided in Appendix B."
        },
        {
            "title": "4.2 Offline Evaluation Results\nIn this section, we evaluate our StarWM using the offline\nevaluation framework defined in Section 3.3.",
            "content": "Quantitative Analysis As shown in Table 1, StarWM yields the strongest performance across most evaluation metrics, indicating effective learning of action-conditioned dynamics. Economy: In resource forecasting, StarWM achieves SMAPE errors of 0.19 / 0.09 for minerals and gas, respectively, significantly outperforming the zero-shot 32B baseline (0.48 / 0.26), with reductions of 60% and 65%. Development: StarWM attains Queue F1 score of 0.92, while the progress prediction error (Progress MAE) drops to only 0.43%, whereas all baselines exhibit errors exceeding 24%. These results suggest that StarWM captures both task progression and macromanagement logic, demonstrating that the deterministic mechanisms of the SC2 engine can be internalized through learning on trajectory data. Micro-level attributes: Compared to zero-shot 32Bs 5.11% / 8.47%, StarWM reduces the HP MAE for self/enemy units to 4.15% / 7.90%, and also significantly outperforms the Static Bias baseline. This indicates that StarWM effectively models combat attrition dynamics, enabling the simulation of health degradation under combat interactions. Macro-Situation: For the self-side entity distribution (including units and structures), compared to 8.37 for Static Bias and 9.79 for the zero-shot 32B baseline, StarWM reduces the AWD error to 3.46, an improvement of nearly 60%, indicating more accurate actionconditioned kinematics prediction. Limitations of zero-shot LLMs. Notably, zero-shot LLMs (both 8B and 32B) fail to consistently outperform the simple Static Bias across most metrics. This suggests that generic pre-trained language models lack prior knowledge of SC2s specific physical laws, and therefore cannot directly function as accurate forward dynamics models without taskspecific adaptation. For Enemy Prediction under Partial Observability. enemy-side macro-situation, we observe that both StarWM and zero-shot LLMs perform slightly worse than Static Bias (e.g., AWD 18.09 vs. 16.13), which indicates fundamental limitation of single-frame prediction under partial observability. Under the Fog of War, enemy actions are highly unobservable. Static Bias strategy (assuming enemies do not move) can achieve better AWD, since enemy displacement within Method Static Bias Qwen3-8B Qwen3-32B StarWM Economy (Minerals / Gas SMAPE) 0.49 / 0.25 0.49 / 0.27 0.48 / 0.26 0.19 / 0.09 Development (Queue F1 / Progress MAE%) 0.80 / 24.81 0.73 / 27.07 0.70 / 24.27 0.92 / 0.43 Micro-Entity (Self / Enemy HP MAE%) 4.81 / 9.15 5.62 / 9.01 5.11 / 8.47 4.15 / 7. Macro-Situation (Self / Enemy AWD) 8.37 / 16.13 10.87 / 17.76 9.79 / 17.78 3.46 / 18.09 Table 1: Offline evaluation results. MAE for Progress and HP is computed as absolute difference in percentage. Our StarWM significantly outperforms baselines in predicting economy values, development progress, unit health, and self-side macro-situation. Detailed results are provided in the Appendix C. Figure 3: Evolution of Macro-Situation Metric (AWD) over game time. Left: Self-side entities. Right: Enemy-side entities. The green area indicates where StarWM outperforms the zero-shot Qwen332B baseline. short horizon is often limited. In contrast, our world model tries to predict plausible enemy behaviors (e.g., scouting or regrouping) conditioned on current observation. But without temporal history or explicit opponent-intent modeling, such probabilistic predictions are highly dependent on the training data distribution and inherently under-determined. This indicates accurate opponent forecasting requires temporal memory or explicit opponent modeling, which we leave for future work. Qualitative Analysis Time series analysis (Figure 3) further demonstrates the stability of StarWM. In the early game stages, the prediction error for self-side macro-situation (blue solid line) remains extremely low. Even during mid-game battles with high operational intensity, StarWM remains robust compared to the zero-shot 32B baseline. Figure 4 provides more intuitive case Case Study. study. The spatial distribution of self units predicted by StarWM (hollow circles) closely matches the ground truth (filled circles), preserving the formation structure of the army, whereas zero-shot LLM predictions appear scattered and lack coherent spatial organization. Notably, Figure 5 presents an interesting phenomenon. When self units move into unobservable areas, StarWM predicts potential enemy units within those regions (red hollow circles). Although this causes false positive in offline evaluation, it reflects the learned statistical regularity that enemy defenders are likely to exist when approaching hostile territory. In online settings, such conservative hallucinations may provide anticipatory signals of potential threats, enabling more cautious and risk-aware decision making. Figure 4: Offline case study. Left: Qwen3-8B. Middle: Qwen3-32B. Right: StarWM. Circles and squares denote units and structures, respectively. Filled markers indicate ground truth, while hollow markers represent predictions. StarWM exhibits stronger spatial consistency with the ground truth, reflecting more accurate actionconditioned movement prediction. Figure 5: Offline case study on scouting. When self units enter unobservable regions, StarWM predicts potential enemy presence (red hollow circles) within those areas, illustrating data-driven statistical predictive pattern."
        },
        {
            "title": "4.3 Online Testing Results\nIn this section, we evaluate the online decision-making per-\nformance of our StarWM-Agent.",
            "content": "Overall Performance Table 2 shows that integrating StarWM into the decision loop consistently improves overall performance across all difficulty levels. Compared to the baselines, StarWMAgent (8B) improves win rates by 10% / 5% / 20% against LV5 / LV6 / LV7, respectively, while StarWM-Agent (32B) achieves larger gains of 30% / 15% / 30%. Notably, although StarWM is trained only on LV6 and LV7 trajectories, the system still achieves substantial improvements against the unseen LV5 opponent, suggesting the world model captures opponent-agnostic, action-conditioned dynamics that generalize across opponents. Macro-management: From Reactive to Preemptive. Both StarWM-Agent (8B) and StarWM-Agent (32B) achieve Setting Win Rate(%) Supply Block Rate(%) Resource Conversion Rate(%) Kill-Loss Ratio(%) Valid Action Rate(%) LV5 (Hard) Qwen3-8B Qwen3-32B StarWM-Agent (8B) StarWM-Agent (32B) LV6 (Harder) Qwen3-8B Qwen3-32B StarWM-Agent (8B) StarWM-Agent (32B) LV7 (VeryHard) Qwen3-8B Qwen3-32B StarWM-Agent (8B) StarWM-Agent (32B) 0.0% 20.0% 10.0% 50.0% 5.0% 25.0% 10.0% 40.0% 0.0% 20.0% 20.0% 50.0% 63.58 13.54 25.45 17.47 5.42 4.97 6.09 4.61 58.31 21.15 21.41 21.29 10.57 9.28 5.93 5.20 58.74 19.99 16.39 21.25 5.51 5.21 5.39 4.90 29.09 19.14 52.32 23.03 84.20 14.21 81.11 7.56 36.12 21.36 58.81 21.34 78.77 8.82 78.67 8.66 32.78 21.61 55.74 20.72 82.89 8.96 76.27 16. 15.22% 62.42% 50.44% 89.87% 6.25% 27.42% 28.12% 41.30% 12.30% 29.26% 20.00% 50.51% 17.89 12.25 41.31 20.81 86.29 12.88 85.57 11.01 28.00 21.64 55.49 25.58 82.37 10.23 84.00 11.22 25.29 20.86 61.64 26.31 82.13 11.91 81.99 22. Table 2: Online evaluation against SC2s built-in AI at different difficulty levels. StarWM-Agent (8B/32B) denotes our world-modelaugmented decision system using zero-shot Qwen3-8B/32B as the policy model. Each setting is evaluated over 20 matches. Note that LV5 is an out-of-distribution (OOD) opponent, as StarWM was trained only on LV6 and LV7 trajectories. substantial reductions in Supply Block Rate (SBR) by approximately 53% and 15%, respectively. These gains stem from two complementary effects of the world model: Extending Temporal Horizon: Through predictive lookahead, the world model enables the policy to anticipate upcoming supply bottlenecks and prioritize supply-related build commands in advance, resulting in more preemptive macro management. Implicit Action Verification: Improvements in SBR are also tied to higher Valid Action Rate (VAR), with increases of 60% for StarWM-Agent (8B) and 31% for StarWM-Agent (32B). The world model acts as lowcost simulation sandbox: if an action fails to produce the expected future state, the resulting discrepancy triggers the policy to revise the action. This filters most invalid commands and improves the reliability of critical macro actions. Economic Efficiency. The macro-management improvements further lead to large gains in Resource Conversion Rate, with increases of 49% for StarWM-Agent (8B) and 23% for StarWM-Agent (32B). Reduced supply blocking ensures more continuous production, while the world models ability to predict task completion times allows the policy to issue production commands in advance. As result, collected resources are converted into units and technologies more consistently and efficiently. This high-conversion pattern indicates that the world model encourages more economically efficient decision-making. Tactical Guidance. At the micro-tactical level, improvements in Kill-Loss Ratio (KLR), with gains of around 21% for both StarWM-Agent (8B) and StarWM-Agent (32B), highlight the role of the world model as lightweight combatand-feasibility simulator. When simulations indicate unfavorable exchanges, the predicted losses discourage combat commitments; conversely, favorable forecasts support engagement decisions. This selective engagement mechSetting Generate (Qwen3-8B) + Refine + Zero-shot WM Simulate + StarWM Simulate Win 0.0 5.0 10.0 20.0 SBR 58.74 9.19 9.98 5.51 RCR KLR VAR ARR 32.78 76.44 72.80 82.89 12.30 13.97 14.80 20.00 25.29 78.04 78.44 82. 34.64 26.66 32.33 Table 3: Ablation Study against LV7 (VeryHard) using zero-shot Qwen3-8B as the policy model. All metrics are reported as percentages (%), where ARR denotes Action Revision Rate; other metrics are consistent with Table 2. anism reduces low-value attrition and contributes to improved combat efficiency. Ablation Study To clarify the source of performance gains, we conduct an ablation study against LV7 (VeryHard) using zero-shot Qwen3-8B as the policy model  (Table 3)  . Specifically, we compare four configurations with progressively added components: (1) Generate, where the policy directly outputs actions without refinement; (2) Generate + Refine, which enables self-reflection without external simulation; (3) Generate + Zero-shot WM Simulate + Refine, where zero-shot Qwen3-8B is used as world model for forward simulation with the same prompt as StarWM; and (4) Generate + StarWM Simulate + Refine (denoted as StarWM-Agent), which incorporates the trajectory-trained world model. Compared to the policy-only baseline (Generate), introducing self-reflection (Generate + Refine) leads to clear improvements in macro-management metrics: Supply Block Rate decreases from 58.74% to 9.19%, Resource Conversion Rate increases from 32.78% to 76.44%, with modest gain in win rate (0% to 5%). This indicates that additional inferencetime computation can improve decision quality to certain extent. Introducing zero-shot world model for forward simulation (Generate + Zero-shot WM Simulate + Refine) further improves win rate (5% to 10%) and Kill-Loss Ratio (13.97% icy refinement under partial observability. To enable dynamics learning in this hybrid and large-scale environment, we introduce structured textual observation representation that factorizes SC2 dynamics into semantic modules and build SC2-Dynamics-50k, the first instruction-tuning dataset for SC2 dynamics prediction. We further propose multi-dimensional offline evaluation framework to assess economy, development, micro-entities, and macro-situation, showing that StarWM captures key deterministic dynamics and combat attrition mechanisms. Finally, we propose StarWM-Agent, integrating the world model into Generate SimulateRefine decision loop for foresight-driven policy refinement, which yields consistent online gains against built-in AI across LV5 to LV7, alongside improved macromanagement stability and tactical risk assessment. References Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Is your llm secretly world model of the inYu Su. ternet? model-based planning for web agents. ArXiv, abs/2411.06559, 2024. Danijar Hafner, J. Pašukonis, Jimmy Ba, and Timothy P. Lillicrap. Mastering diverse domains through world models. ArXiv, abs/2301.04104, 2023. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, 2023. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685, 2021. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optiIn H. Wallach, H. Larochelle, A. Beygelzimer, mization. F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Harold W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics (NRL), 52, 1955. Brenden Lake, Tomer Ullman, Joshua Tenenbaum, and Samuel Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. Zongyuan Li, Yanan Ni, Runnan Qi, Lumin Jiang, Chang Lu, Xiaojie Xu, Xiangbei Liu, Pengfei Li, Yunzheng Guo, Zhe Ma, Xian Guo, Kuihua Huang, and Xuebo Zhang. Llm-pysc2: Starcraft ii learning environment for large language models. ArXiv, abs/2411.05348, 2024. Weiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin, Haifeng Zhang, and Jun Wang. Large language models play starcraft ii: Benchmarks and chain of summarization approach. ArXiv, abs/2312.11865, 2023. Figure 6: Distribution of action revision types for StarWM-Agent (32B) during online evaluation. Left: Added actions. Right: Removed actions. to 14.80%). Although this zero-shot model serves as simulator with limited predictive accuracy, it provides external predictive signals beyond internal self-reflection, encouraging more cautious and forward-looking decision-making. Incorporating the trajectory-trained StarWM (Generate + StarWM Simulate + Refine) further improves performance across all key metrics, with win rate increasing to 20%. These results suggest that the performance gains of StarWM-Agent do not stem merely from additional inference-time computation or generic LLM-based foresight, but from accurate action-conditioned simulation. more reliable world model enables more precise predictive simulation, which in turn supports stronger decision-making. Mechanism Analysis Analysis of Action Revisions We analyze action revisions under the main StarWM-Agent settings  (Table 2)  . Overall, the Action Revision Rate for StarWM-Agent (8B) and StarWM-Agent (32B) is 32.74% and 19.45%, respectively, aggregated across the three opponents. This indicates that simulation-based rollouts frequently lead to action revisions, especially for the smaller policy model. Figure 6 further breaks down the revision behavior of StarWM-Agent (32B) by action type. Among added actions, Build Supply Depot accounts for the largest proportion (44.9%), suggesting that world model simulations effectively promote stronger macro management. Figure 1 presents an illustrative online case Case Study. where our StarWM-Agent plays against the VeryHard (LV7) built-in AI. At this moment, minerals are scarce, while the unused supply remains sufficient at 18. The policy initially proposes to build Supply Depot. However, the world model simulation shows that building Supply Depot would further reduce minerals while unused supply would remain sufficient. Based on this simulated outcome, the policy refines its decision to train an SCV instead. This example again provides concrete evidence that incorporating an action-conditioned world model into the decision loop enables foresight-driven refinement of suboptimal actions."
        },
        {
            "title": "5 Conclusion\nWe present StarWM, the first action-conditioned world\nmodel for StarCraft II, and demonstrate its value for pol-",
            "content": "Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575:350 354, 2019. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv: Arxiv-2305.16291, 2023. Yu-Quan Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1474914759, 2023. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. Runnan Qi, Yanan Ni, Lumin Jiang, Zongyuan Li, Kuihua Huang, and Xian Guo. Memory-augmented state machine prompting: novel llm agent framework for real-time strategy games. ArXiv, abs/2510.18395, 2025. Runnan Qi, Yuming Quan, Yanan Ni, Zongyuan Li, Xiaojie Xu, Kuihua Huang, and Xian Guo. Comm-cot: Standardized chain-of-thought communication framework for efficient llm based multi-agent decision-making in real-time strategy games. In 2025 IEEE 2nd International Conference on Electronics, Communications and Intelligent Science (ECIS), pages 18, 2025. Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. ArXiv, abs/2503.20523, 2025. Timo Schick, Jane Dwivedi-Yu, Roberto Dessí, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: language models can teach themselves to use tools. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 6853968551, 2023. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, L. Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588:604 609, 2019. Pengbo Shen, Yaqing Wang, Ni Mu, Yao Luan, Runpeng Xie, Senhao Yang, Lexiang Wang, Hao Hu, Shuang Xu, Yiqin Yang, and Bo Xu. Sc2arena and starevolve: Benchmark and self-improvement framework for llms in complex decision-making tasks. ArXiv, abs/2508.10428, 2025. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 86348652, 2023. Gabriel Synnaeve, Zeming Lin, Jonas Gehring, Daniel Gant, Vegard Mella, Vasil Khalidov, Nicolas Carion, and Nicolas Usunier. Forward modeling for partial observation strategy games - starcraft defogger. In Neural Information Processing Systems, 2018. Cédric Villani. Optimal transport: Old and new. 2008. Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Joseph Dudzik, Junyoung Chung, David Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander Sasha Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom Le Paine, Caglar Gulcehre, Ziyun Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy P. Lillicrap, Koray Introduction to StarCraft II StarCraft II (SC2) is real-time strategy (RTS) game developed by Blizzard Entertainment and is widely Game Overview. regarded as benchmark for complex sequential decision-making. In the standard competitive 1v1 setting, the game can be modeled as two-player zero-sum partially observable stochastic game. Each player selects one of three asymmetric racesTerran, Protoss, or Zergwith race-specific units, technologies, and strategic styles. SC2 integrates both macro-management and micro-tactical control. At the macro level, players collect minComplexity. erals and vespene gas, maintain supply capacity, expand bases, and advance through technology trees to unlock stronger units. At the micro-tactical level, players control unit movement, positioning, and engagement decisions during battles, requiring coordination among heterogeneous units under time pressure. The state and action spaces are high-dimensional and combinatorial, making long-horizon planning and execution particularly challenging. SC2 dynamics are hybrid and heterogeneous. Resource quantities change according to colDynamics Characteristics. lection rates and action costs. Task progress follows deterministic build-time rules. Unit positions evolve under kinematic constraints, while unit health is governed by interaction-based combat dynamics. Meanwhile, due to the Fog of War, opponent states and actions are highly unobservable and must be inferred from limited information. This mixture of deterministic rules, interaction-driven effects, and partial observability makes accurate modeling of SC2 dynamics particularly challenging. Detailed Metrics for Online Testing We define the following metrics to characterize decision quality in online testing from multiple aspects. B.1 Primary Metric Win Rate measures the proportion of games won, reflecting the overall performance of the agent: Win Rate = Number of Games Won Total Number of Games Played 100%. (6) B.2 Macro-Management Metrics Supply Block Rate measures the fraction of in-game time during which the agent is supply-blocked, reflecting its ability to balance production and supply expansion. Lower rates indicate better macro management. Supply Block Rate = Time Supply Blocked Total Game Time 100%. (7) Resource Conversion Rate measures the proportion of collected resources that are effectively spent, reflecting the agents ability to translate economic growth into actions. Higher rates indicate more efficient resource utilization. Resource Conversion Rate = Total Resources Spent Total Resources Collected 100%. B.3 Combat Metric KillLoss Ratio evaluates combat efficiency based on the economic value of army units: KillLoss Ratio = Killed Enemy Army Value Lost Army Value 100%, (8) (9) where the numerator and denominator represent the total resource value of enemy units killed and own units lost, respectively. Higher scores indicate more favorable resource exchanges in combat. B.4 System Stability Metric Valid Action Rate measures the proportion of valid actions issued by the agent, reflecting the robustness of its action generation under diverse situations: An action is considered invalid if it cannot be executed by the game engine at the corresponding timestep. Valid Action Rate = Number of Valid Actions Total Number of Issued Actions 100%. B.5 Mechanism Analysis Metric Action Revision Rate measures how often the agent modifies its initial action proposal during decision refinement: Action Revision Rate = 1 (cid:88) t=1 I(cid:0)afinal = ainit (cid:1) 100%, (10) (11) where ainit metric reflects the extent to which world model predictions influence final decisions. denote the initial and final actions at step t, respectively. is the total number of decision steps. This and afinal Detailed Quantitative Results of Offline Evaluation Table 4 shows detailed quantitative results of offline evaluation, aligning with the conclusions presented in Section 4.2. Table 4: Offline quantitative evaluation results across multiple dimensions. Metric Classification Metrics Macro-Situation Consistency Self Unit AWD Economy & Status Development Micro Entity Attributes Self Struct AWD Enemy Unit AWD Enemy Struct AWD Snapshot Enemy Struct AWD Minerals SMAPE Minerals Rate SMAPE Gas SMAPE Gas Rate SMAPE Supply Used SMAPE Supply Cap SMAPE Alerts F1 Upgrades F1 Workers num SMAPE Queue F1 Progress(%) MAE Self Unit F1 Self Unit Prec. Self Unit Recall Self Unit HP(%) MAE Self Unit Energy(%) MAE Self Struct F1 Self Struct Prec. Self Struct Recall Self Struct HP(%) MAE Self Struct Energy(%) MAE Enemy Unit F1 Enemy Unit Prec. Enemy Unit Recall Enemy Unit HP(%) MAE Enemy Unit Energy(%) MAE Enemy Struct F1 Enemy Struct Prec. Enemy Struct Recall Enemy Struct HP(%) MAE Snap Enemy Struct F1 Snap Enemy Struct Prec. Snap Enemy Struct Recall Our StarWM Qwen3-32B (Zero-shot) Qwen3-8B (Zero-shot) Static Bias 5.96 0.96 30.94 15.17 8.16 0.19 0.09 0.09 0.12 0.02 0.01 0 1 0.02 0.92 0.43 0. 0.94 0.96 4.15 1.47 0.99 0.99 0.99 0.65 0.07 0.65 0.68 0.68 7.9 2.54 0.83 0.84 0.84 2.91 0.91 0.92 0.91 13.44 6.15 27.75 15.22 10.36 0.48 0.73 0.26 0.58 0.21 0.2 0.02 0.98 0.07 0.7 24. 0.87 0.87 0.91 5.11 4.81 0.93 0.92 0.96 0.63 3.42 0.69 0.73 0.72 8.47 2.59 0.83 0.84 0.84 4.4 0.88 0.9 0.89 14.41 7.32 28.34 15.73 9.2 0.49 0.37 0.27 0.34 0.04 0.03 0.02 0.98 0. 0.73 27.07 0.85 0.86 0.86 5.62 3.89 0.92 0.91 0.94 0.66 4.71 0.69 0.72 0.72 9.01 2.61 0.83 0.84 0.83 4.51 0.9 0.91 0.9 13.89 2.84 27.23 14.09 7.07 0. 0.12 0.25 0.12 0.04 0.02 0.02 0.98 0.03 0.8 24.81 0.85 0.86 0.85 4.81 3.49 0.97 0.98 0.96 0.61 3.33 0.7 0.74 0.73 9.15 2.58 0.84 0.85 0.85 4.85 0.92 0.93 0.93 Prompt Templates for World Model and Decision System D.1 World Model Prompt We provide the prompt template used for StarWM and zero-shot world models. This prompt is used both in offline training and inference, as well as online simulation within the GenerateSimulateRefine decision loop. World Model Prompt ### Role You are the StarCraft II World Model, dynamics simulator for Player {player_id}. ### Task Predict the Future Observation for Player {player_id} exactly {delta} seconds later based on the Current Observation, the Action Sequence and the Game Mechanics. ### Input 1. Current Observation: The partial state visible to Player {player_id} at time t. 2. Action Sequence: All actions executed by Player {player_id} between and t+{delta}s. ### Output Future Observation: The Predicted partial state visible to Player {player_id} at time t+{delta}s. ### Data Explanation #### Observation Schema: - **[Info]**: Player-specific status, race, enemy race, economy (minerals, gas, gather rate), supply (used, cap), alerts and upgrades. - **[Queue]**: Ongoing and queued production, construction, and research tasks. - **[My Units]**: Self units (workers & army). - **[My Structures]**: Self buildings, including key structures for production and technology. - **[Visible Hostiles]**: Enemy units and structures currently visible within the Fog of War. #### Action Schema: The Action follows the format: Subject - Command - Target. - **Subject**: UnitName [ID] (e.g., Marine [101]). - **Command**: The specific operation (e.g., Move, Attack, etc.). - **Target**: Coordinate (x, y) or TargetName [ID] or absent for some commands. ### Input Data #### Current Observation: {start_obs} #### Action Sequence({delta}s): {action_section} ### Rules for Reasoning && Game Mechanics Perform the state transition based on the following logic: 1. **Time:** Increment the \"Time\" field by {delta} seconds. 2. **Resources:** - Minerals/Gas Change (Income_Rate_per_min / 60 * {delta}s) - (Cost of new Actions). - Note: Income Rate is the (+value) found in the [Info] section. - Update Income Rate roughly proportional to the change in active mining workers. 3. **Supply:** - Used += Supply Cost of the newly issued Action. (Units in Queue consume Supply, not upon completion.) - Cap += Newly Completed Supply Structure (eg. Supply Depot, Command Center). 4. **Queue & Production:** - Progress += ({delta}s / Total Build Time at game_speed=1.4). Note: Game Speed is 1.4. - If Progress >= 100%: Remove from [Queue], add to [My Units](if unit) or [My Structures](if structure), and update [Info] (Supply Cap, Upgrades) accordingly. - New Actions: Add newly issued training/building Action to [Queue]. 5. **Movement:** For moving units: New Pos Start Pos + Direction_Vector * (min(Remaining Distance, Speed * {delta}s)). 6. **Status:** Status Change depends on the Command and the Target. 7. **Combat:** - If units are \"Attacking\" or under attack, reduce HP based on estimated DPS and duration. - Remove units if HP <= 0. 8. **Visibility:** - Unit Hiding: Units inside structures (e.g., SCVs in Refineries, units in Bunkers/Medivacs) do NOT appear in [My Units] list temporarily, though they still consume Supply. - Fog of War: Update [Visible Hostiles] based on current visual radius of self units and structures. Remove enemy units that move into the fog or become cloaked without detection. 9. **Consistency:** Units/Structures not mentioned in the Action Sequence remain unchanged (ID, Position, HP , Status and so on). ### Output Format Requirements 1. **Raw Text Only**: Output the observation text directly. Do NOT use JSON, Markdown code blocks, or conversational fillers (e.g., \"Here is the prediction\"). 2. **Format**: The output must start strictly with the [Info] tag and follow the exact structure of the input observation. D.2 Online Refinement Prompt with World Model Predictions We provide the online refinement prompt used in the GenerateSimulateRefine decision loop of StarWM-Agent. The same refinement prompt is also used in the online zero-shot world model ablation, where StarWM is replaced by zero-shot Qwen3-8B for future prediction. Online Refinement Prompt with World Model Predictions Here is simulation report showing the **PREDICTED GAME STATE** 5 seconds **IF** your original proposed action is executed. <simulation_report> {report} </simulation_report> ### TACTICAL ANALYSIS REQUIRED **CRITICAL INSTRUCTION**: 1. **LOOK BACK**: You MUST compare the **<simulation_report>** above with the **### Current Game State** provided in the **first user message**. 2. **HYPOTHETICAL STATE**: This report is **prediction**. The action you proposed previously has **NOT** been executed yet. - **KEEP** your original commands by default. - **ADD** new commands if you find idle units/structures or extra resources in the prediction. - **REMOVE/CHANGE** original commands ONLY if they lead to bad trades. Analyze the predicted state and output the OPTIMIZED JSON: 1. **Check Resources & Supply**: - **Supply Check**: Look at \"Supply unused\" in the prediction. Is it low (< 3)? - If YES, you are about to be Supply Blocked. **ADD** command to Build SupplyDepot (Use valid SCV ). - **Gas Strategy**: - Look at \"Vespene\". Is it very low (e.g., 0 or < 50) while \"Minerals\" are high? **ADD** Build Refinery to start gathering gas. - Is \"Vespene\" high (e.g., > 150)? Its time to develop technology. **ADD** commands to Build Factory/ Starport, Build TechLab, Upgrade (EngineeringBay), or Train Marauder/SiegeTank. - **Mineral Surplus & Expansion**: - Look at \"Minerals\". Are they high (e.g., > 400)? Its time to expand. **ADD** Building Orders (e.g., Build Command Center (Expand), Build Barracks (Increase Production)). 2. **Check Structure States**: - **Queue Limit Check**: Look at the \"Production list\" of each structure. - Does any list contain 4 or 5 items? (e.g., Marine (25%), Marine (Waiting), Marine (Waiting)...). - If YES, the queue is **FULL**. **Do NOT** add more training commands to this structure. Instead, if resources allow, **ADD** commands to Build Barracks/Factory to increase production capacity. - **Fill the Queue**: Look at \"Production list\". Is it empty or does it only have 1 item? - If YES, the structure needs more orders. **ADD** commands to fill the queue (e.g., Train SCV for CC , Train Marine for Barracks). - **Idle Structure Check**: - Look at \"Own structures\". Are there any marked as **\"State: idle\"**? If YES, immediately **ADD** commands to train units (e.g., Train SCV for CC, Train Marine for Barracks). - Are tech structures (e.g., TechLab, EngineeringBay) marked as **\"State: idle\"**? If YES, **ADD** commands to upgrade technology. 3. **Check Unit Health & Survival**: - **Idle Army Check**: Look at \"Own units\". Are there any Army units marked as **\"State: idle\"** while the **Visible enemy units** or **Visible enemy structures** list is NOT empty? - If YES, do not just wait. Add an **Attack** command to pressure the enemy. - **Survival & Bad Trade Check**: Compare the **Own units** and **Own structures** list in the prediction with the **### Current Game State**. Are any units missing (This means they **DIED**) without Killing or Heavily Damaging enemy units? - If YES, this is **Bad Trade**. **CHANGE** your command to **Retreat** or **Reposition** to save your units. - **Damage Check**: Look at the \"Health\" of your units. Is any units health low or dropped significantly ? - If YES, the unit is under attack. Add command to **Kite** (move back slightly) or **Heal/Repair** ( Medivac/SCV) to save it. - **Vision & Scouting Check**: is the **Visible enemy structures** list [Empty]? - If YES, your vision is limited. **ADD** Attack or Move commands to **search for enemy bases**. ### NEW ACTION RULES - **VALIDATE ACTORS**: When adding new commands, refer to **Unit abilities**, **Structure abilities**, ** Ability description** in the **### Current Game State** provided in the **first user message**. Ensure the selected units actually have the ability to perform the action. - **CORRECT UNIT SELECTION**: When adding TERRANBUILD commands, you MUST find valid **SCV** from the **### Current Game State** (marked as State: idle or State: collecting resources automatically). **Output Rules**: Create an OPTIMIZED JSON list that combines your beneficial original actions with any NEW necessary and valid actions derived from the analysis above. D.3 Online Refinement Prompt for Self-Reflection Ablation We provide the refinement prompt used in the online self-reflection ablation study. In this setting, no external future prediction is introduced, and the policy refines its initially generated action solely based on its own internal reasoning. Online Refinement Prompt for Self-Reflection Ablation Here is reflection guidance for your original proposed actions. ### TACTICAL REFLECTION REQUIRED **CRITICAL INSTRUCTION**: 1. **LOOK BACK**: You MUST review the **### Current Game State** provided in the **first user message**. 2. **REFLECTION**: The actions you proposed previously have **NOT** been executed yet. You have chance to refine it. - **KEEP** your original commands by default. - **ADD** new commands if you find idle units/structures or extra resources in the current state. - **REMOVE/CHANGE** original commands ONLY if they are not good enough. Analyze the current state and output the OPTIMIZED JSON: 1. **Check Resources & Supply**: - **Supply Check**: Look at \"Supply unused\". Is it low (< 3)? - If YES, you are about to be Supply Blocked. **ADD** command to Build SupplyDepot (Use valid SCV ). - **Gas Strategy**: - Look at \"Vespene\". Is it very low (e.g., 0 or < 50) while \"Minerals\" are high? **ADD** Build Refinery to start gathering gas. - Is \"Vespene\" high (e.g., > 150)? Its time to develop technology. **ADD** commands to Build Factory/ Starport, Build TechLab, Upgrade (EngineeringBay), or Train Marauder/SiegeTank. - **Mineral Surplus & Expansion**: - Look at \"Minerals\". Are they high (e.g., > 400)? Its time to expand. **ADD** Building Orders (e.g., Build Command Center (Expand), Build Barracks (Increase Production)). 2. **Check Structure States**: - **Queue Limit Check**: Look at the \"Production list\" of each structure. - Does any list contain 4 or 5 items? - If YES, the queue is **FULL**. **Do NOT** add more training commands to this structure. Instead, if resources allow, **ADD** commands to Build Barracks/Factory to increase production capacity. - **Fill the Queue**: Look at \"Production list\". Is it empty or does it only have 1 item? - If YES, the structure needs more orders. **ADD** commands to fill the queue (e.g., Train SCV for CC , Train Marine for Barracks). - **Idle Structure Check**: - Look at \"Own structures\". Are there any marked as **\"State: idle\"**? If YES, immediately **ADD** commands to train units (e.g., Train SCV for CC, Train Marine for Barracks). - Are tech structures (e.g., TechLab, EngineeringBay) marked as **\"State: idle\"**? If YES, **ADD** commands to upgrade technology. 3. **Check Unit Health & Status**: - **Idle Army Check**: Look at \"Own units\". Are there any Army units marked as **\"State: idle\"** while the **Visible enemy units** or **Visible enemy structures** list is NOT empty? - If YES, do not just wait. Add an **Attack** command to pressure the enemy. - **Damage Check**: Look at the \"Health\" of your units. Is any units health low? - If YES, add command to **Kite** (move back slightly) or **Heal/Repair** (Medivac/SCV) to save it. - **Vision & Scouting Check**: Is the **Visible enemy structures** list [Empty]? - If YES, your vision is limited. **ADD** Attack or Move commands to **search for enemy bases**. ### NEW ACTION RULES - **VALIDATE ACTORS**: When adding new commands, refer to **Unit abilities**, **Structure abilities**, ** Ability description** in the **### Current Game State** provided in the **first user message**. Ensure the selected units actually have the ability to perform the action. - **CORRECT UNIT SELECTION**: When adding TERRANBUILD commands, you MUST find valid **SCV** from the **### Current Game State** (marked as State: idle or State: collecting resources automatically). **Output Rules**: Create an OPTIMIZED JSON list that combines your beneficial original actions with any NEW necessary and valid actions derived from the reflection above. Examples of SC2-Dynamics-50k and StarWM-Agent Online Decision Making E.1 Example from SC2-Dynamics-50k We show representative sample from the SC2-Dynamics-50k dataset, including the structured textual observation, the action sequence, and the target future observation. Sample from SC2-Dynamics-50k Input: { \"role\": \"user\", \"content\": \"### Role You are the StarCraft II World Model, dynamics simulator for Player 1. ### Task Predict the Future Observation for Player 1 exactly 5 seconds later based on the Current Observation, the Action Sequence and the Game Mechanics. ### Input 1. Current Observation: The partial state visible to Player 1 at time t. 2. Action Sequence: All actions executed by Player 1 between and t+5s. ### Output Future Observation: The Predicted partial state visible to Player 1 at time t+5s. ### Data Explanation #### Observation Schema: - **[Info]**: Player-specific status, race, enemy race, economy (minerals, gas, gather rate), supply (used, cap), alerts and upgrades. - **[Queue]**: Ongoing and queued production, construction, and research tasks. - **[My Units]**: Self units (workers & army). - **[My Structures]**: Self buildings, including key structures for production and technology. - **[Visible Hostiles]**: Enemy units and structures currently visible within the Fog of War. #### Action Schema: The Action follows the format: Subject - Command - Target. - **Subject**: UnitName [ID] (e.g., Marine [101]). - **Command**: The specific operation (e.g., Move, Attack, etc.). - **Target**: Coordinate (x, y) or TargetName [ID] or absent for some commands. ### Input Data #### Current Observation: [Info] Time: 05:44 Race: Terran Enemy Race: Terran Map: Flat64 Minerals: 280 (+2575/min) Gas: 124 (+223/min) Supply: 64/93 (Army: 16, Workers: 48) Alerts: None Upgrades: Punishergrenades [Queue] - Orbitalcommand [1] at (63,61): Commandcentertrain_scv (34%) - Barracks [65] at (53,59): Barrackstrain_marine (80%) - Orbitalcommand [83] at (43,64): Commandcentertrain_scv (93%) - Barracks [86] at (55,55): Barrackstrain_marine (25%) - Barracks [90] at (58,57): Barrackstrain_marine (53%), Barrackstrain_marine (35%) - Commandcenter [226] at (24,61): Upgradetoorbital_orbitalcommand (0%) - Constructing: Barracksreactor [334] at (65,54) (59%) - Constructing: Refinery [335] at (28,68) (94%) - Constructing: Missileturret [352] at (18,68) (65%) [My Units] > Workers: (38Mining :[5,6,7,8,9,10,11,13,35,61,71,77,84,115,139,144,153,155,159,160,179,183,196,217,222,225,237,240,254,256,271, 292,307,311,320,337,351,360], 2Mule:[305,313]) - Scv [2] at (53,72) (HP:100%, St:Idle) - Scv [3] at (23,64) (HP:100%, St:Moving) - Scv [82] at (45,59) (HP:100%, St:Building) - Scv [154] at (21,65) (HP:100%, St:Moving) - Scv [264] at (28,69) (HP:100%, St:Building) - Scv [326] at (18,67) (HP:100%, St:Building) > Army: - Marine [135] at (37,57) (HP:100%, St:Idle) - Marine [143] at (37,56) (HP:100%, St:Idle) - Marine [147] at (36,57) (HP:100%, St:Attacking) - Marine [310] at (40,27) (HP:100%, St:Attacking) - Marine [324] at (41,19) (HP:100%, St:Attacking) - Marauder [332] at (44,38) (HP:73%, St:Idle) - Marauder [347] at (33,39) (HP:100%, St:Attacking) - Marine [356] at (61,46) (HP:100%, St:Attacking) - Marine [358] at (46,53) (HP:100%, St:Attacking) - Marine [359] at (41,49) (HP:100%, St:Attacking) [My Structures] - Orbitalcommand [1] at (63,61) (HP:100%, Eng:14%, St:Training) - Supplydepotlowered [36] at (50,60) (HP:100%, St:Idle) - Barracks [65] at (53,59) (HP:100%, St:Training, AddOn:Techlab [148]) - Orbitalcommand [83] at (43,64) (HP:100%, Eng:17%, St:Training) - Barracks [86] at (55,55) (HP:100%, St:Training, AddOn:Techlab [150]) - Barracks [90] at (58,57) (HP:100%, St:Training, AddOn:Reactor [158]) - Refinery [114] at (70,58) (HP:100%, St:Idle) - Refinery [122] at (59,68) (HP:100%, St:Idle) - Bunker [140] at (44,54) (HP:100%, St:Idle) - Engineeringbay [146] at (57,52) (HP:100%, St:Idle) - Barrackstechlab [148] at (56,59) (HP:100%, St:Idle) - Barrackstechlab [150] at (58,55) (HP:100%, St:Idle) - Supplydepotlowered [156] at (53,57) (HP:100%, St:Idle) - Barracksreactor [158] at (61,57) (HP:100%, St:Idle) - Supplydepotlowered [164] at (60,53) (HP:100%, St:Idle) - Factory [190] at (43,57) (HP:100%, St:Idle, AddOn:Techlab [252]) - Commandcenter [226] at (24,61) (HP:100%, St:Upgrading) - Supplydepotlowered [251] at (55,53) (HP:100%, St:Idle) - Factorytechlab [252] at (46,57) (HP:100%, St:Idle) - Barracks [257] at (62,54) (HP:100%, St:Building) - Supplydepotlowered [265] at (42,55) (HP:100%, St:Idle) - Missileturret [299] at (43,73) (HP:100%, St:Idle) - Missileturret [314] at (70,68) (HP:100%, St:Idle) - Supplydepot [333] at (46,59) (HP:100%, St:Idle) [Visible Hostiles] > Enemy Units: - Scv [88] at (29,30) (HP:100%) - Banshee [230] at (28,30) (HP:87%, Eng:3%) - Banshee [239] at (30,29) (HP:87%, Eng:15%) > Enemy Structures: - Starporttechlab [162] at (33,25) (HP:92%) - Barrackstechlab [365] at (33,22) (HP:100%) > Snapshot Enemy Structures: - Factory at (23,28) - Supplydepotlowered at (21,28) - Orbitalcommand at (24,22) - Refinery at (17,25) - Supplydepotlowered at (28,28) - Barracks at (30,22) - Refinery at (28,15) - Barracks at (30,28) - Starport at (30,25) #### Action Sequence(5s): - +0.1s: Marauder [347] - Attack - (31, 34) - +0.2s: Scv [196] - Move - (45, 66) - +0.2s: Scv [196] - Smartcmd - Orbitalcommand [83] - +0.3s: Supplydepot [333] - Morph_supplydepot_lower - +0.3s: Marauder [347] - Attack - (30, 34) - +0.4s: Scv [240] - Move - (68, 64) - +0.4s: Scv [240] - Smartcmd - Mineralfield [19] - +0.4s: Scv [217] - Move - (65, 63) - +0.4s: Scv [217] - Smartcmd - Orbitalcommand [1] - +0.4s: Scv [144] - Move - (66, 62) - +0.4s: Scv [144] - Smartcmd - Orbitalcommand [1] - +0.4s: Scv [7] - Move - (64, 64) - +0.4s: Scv [7] - Smartcmd - Orbitalcommand [1] - +0.4s: Mule [313] - Move - (39, 68) - +0.4s: Mule [313] - Smartcmd - Mineralfield750 [32] - +0.4s: Marauder [347] - Attack - (30, 33) - +0.6s: Factory [190] - Factorytrain_siegetank - +0.6s: Scv [337] - Move - (19, 64) - +0.6s: Scv [337] - Smartcmd - Mineralfield750 [46] - +0.6s: Marauder [347] - Attack - (30, 33) - +0.8s: Scv [237] - Move - (41, 66) - +0.8s: Scv [237] - Smartcmd - Orbitalcommand [83] - +0.8s: Marauder [347] - Attack - (30, 33) - +0.9s: Orbitalcommand [83] - Commandcentertrain_scv - +1.0s: Scv [367] - Harvest_gather - Mineralfield [33] - +1.0s: Marauder [347] - Attack - (30, 32) - +1.0s: Marine [135] - Attack - (36, 57) - +1.0s: Marine [143] - Attack - (36, 57) - +1.0s: Marine [147] - Attack - (36, 57) - +1.2s: Marauder [347] - Move - (30, 33) - +1.2s: Scv [13] - Move - (65, 63) - +1.2s: Scv [13] - Smartcmd - Orbitalcommand [1] - +1.2s: Scv [3] - Move - (22, 67) - +1.2s: Scv [3] - Smartcmd - Mineralfield750 [42] - +1.3s: Scv [196] - Move - (48, 67) - +1.3s: Scv [196] - Smartcmd - Mineralfield [25] - +1.3s: Marauder [347] - Move - (30, 33) - +1.5s: Marauder [347] - Move - (30, 33) - +1.6s: Scv [7] - Harvest_gather - Refinery [335] - +1.6s: Scv [61] - Harvest_gather - Refinery [335] - +1.6s: Scv [264] - Harvest_gather - Mineralfield750 [41] - +1.6s: Scv [217] - Move - (66, 66) - +1.6s: Scv [217] - Smartcmd - Mineralfield750 [21] - +1.7s: Scv [144] - Move - (69, 63) - +1.7s: Scv [144] - Smartcmd - Mineralfield750 [15] - +1.7s: Scv [5] - Move - (69, 62) - +1.7s: Scv [5] - Smartcmd - Mineralfield750 [16] - +1.7s: Marauder [347] - Move - (32, 35) - +1.9s: Marauder [347] - Move - (32, 35) - +2.1s: Scv [237] - Move - (38, 67) - +2.1s: Scv [237] - Smartcmd - Mineralfield [33] - +2.1s: Marauder [347] - Move - (32, 35) - +2.1s: Marine [135] - Attack - (36, 57) - +2.1s: Marine [143] - Attack - (36, 57) - +2.1s: Marine [147] - Attack - (36, 57) - +2.2s: Marauder [347] - Attack - (26, 33) - +2.3s: Scv [9] - Move - (45, 66) - +2.3s: Scv [9] - Smartcmd - Orbitalcommand [83] - +2.3s: Scv [6] - Move - (21, 61) - +2.3s: Scv [6] - Smartcmd - Commandcenter [226] - +2.4s: Scv [13] - Move - (68, 64) - +2.4s: Scv [13] - Smartcmd - Mineralfield [19] - +2.4s: Marauder [347] - Move - (30, 32) - +2.4s: Marine [356] - Attack - (67, 18) - +2.6s: Marauder [347] - Move - (32, 35) - +2.7s: Scv [351] - Move - (21, 61) - +2.7s: Scv [351] - Smartcmd - Commandcenter [226] - +2.8s: Marauder [347] - Move - (33, 35) - +2.9s: Mule [305] - Move - (42, 67) - +2.9s: Mule [305] - Smartcmd - Orbitalcommand [83] - +2.9s: Marauder [332] - Attack - (20, 22) - +2.9s: Marauder [347] - Move - (33, 35) - +3.0s: Marauder [332] - Move - (42, 37) - +3.1s: Scv [311] - Move - (65, 63) - +3.1s: Scv [311] - Smartcmd - Orbitalcommand [1] - +3.1s: Marauder [347] - Move - (33, 35) - +3.1s: Marine [135] - Attack - (36, 57) - +3.1s: Marine [143] - Attack - (36, 57) - +3.1s: Marine [147] - Attack - (36, 57) - +3.2s: Scv [367] - Move - (38, 67) - +3.2s: Scv [367] - Smartcmd - Mineralfield [33] - +3.2s: Marauder [332] - Attack - (20, 22) - +3.3s: Marauder [347] - Move - (33, 35) - +3.3s: Scv [2] - Move - (50, 70) - +3.3s: Scv [2] - Move - (53, 72) - +3.3s: Marine [359] - Attack - (17, 41) - +3.4s: Scv [10] - Move - (64, 64) - +3.4s: Scv [10] - Smartcmd - Orbitalcommand [1] - +3.4s: Marauder [332] - Move - (41, 37) - +3.5s: Scv [9] - Move - (48, 67) - +3.5s: Scv [9] - Smartcmd - Mineralfield [25] - +3.5s: Scv [254] - Move - (24, 64) - +3.5s: Scv [254] - Smartcmd - Commandcenter [226] - +3.5s: Scv [6] - Move - (18, 62) - +3.5s: Scv [6] - Smartcmd - Mineralfield750 [50] - +3.5s: Scv [351] - Move - (19, 61) - +3.5s: Scv [351] - Smartcmd - Mineralfield [51] - +3.6s: Scv [160] - Move - (66, 62) - +3.6s: Scv [160] - Smartcmd - Orbitalcommand [1] - +3.6s: Scv [154] - Move - (22, 63) - +3.6s: Scv [154] - Smartcmd - Commandcenter [226] - +3.6s: Marauder [332] - Attack - (20, 22) - +3.7s: Barracks [65] - Barrackstrain_marine - +3.7s: Marauder [347] - Move - (34, 35) - +3.8s: Mule [305] - Move - (41, 69) - +3.8s: Mule [305] - Smartcmd - Mineralfield [30] - +3.8s: Marauder [332] - Move - (40, 36) - +3.8s: Marauder [347] - Move - (34, 35) - +3.9s: Marauder [332] - Attack - (20, 22) - +4.0s: Scv [240] - Move - (65, 63) - +4.0s: Scv [240] - Smartcmd - Orbitalcommand [1] - +4.0s: Marauder [347] - Move - (35, 35) - +4.2s: Marauder [347] - Move - (35, 35) - +4.2s: Marine [135] - Attack - (36, 57) - +4.2s: Marine [143] - Attack - (36, 57) - +4.2s: Marine [147] - Attack - (36, 57) - +4.2s: Marine [368] - Attack - (17, 41) - +4.3s: Scv [254] - Move - (24, 66) - +4.3s: Scv [254] - Smartcmd - Mineralfield [39] - +4.4s: Scv [311] - Move - (66, 66) - +4.4s: Scv [311] - Smartcmd - Mineralfield750 [21] - +4.4s: Scv [337] - Move - (22, 63) - +4.4s: Scv [337] - Smartcmd - Commandcenter [226] - +4.4s: Marauder [347] - Move - (35, 35) - +4.6s: Marauder [332] - Attack - (36, 35) - +4.6s: Marine [358] - Move - (32, 45) - +4.6s: Marine [324] - Attack - (37, 21) - +4.7s: Scv [154] - Move - (21, 66) - +4.7s: Scv [154] - Smartcmd - Mineralfield [44] - +4.7s: Marauder [347] - Move - (36, 36) - +4.7s: Marauder [332] - Attack - (36, 35) - +4.7s: Marine [358] - Move - (32, 45) - +4.7s: Marine [324] - Move - (36, 24) - +4.8s: Scv [160] - Move - (69, 63) - +4.8s: Scv [160] - Smartcmd - Mineralfield750 [15] - +4.9s: Scv [10] - Move - (65, 67) - +4.9s: Scv [10] - Smartcmd - Mineralfield [17] - +4.9s: Marauder [347] - Move - (36, 36) - +4.9s: Marauder [332] - Attack - (36, 35) - +4.9s: Marine [358] - Move - (32, 44) - +4.9s: Marine [324] - Move - (36, 24) ### Rules for Reasoning && Game Mechanics Perform the state transition based on the following logic: 1. **Time:** Increment the \"Time\" field by 5 seconds. 2. **Resources:** - Minerals/Gas Change (Income_Rate_per_min / 60 * 5s) - (Cost of new Actions). - Note: Income Rate is the (+value) found in the [Info] section. - Update Income Rate roughly proportional to the change in active mining workers. 3. **Supply:** - Used += Supply Cost of the newly issued Action. (Units in Queue consume Supply, not upon completion.) - Cap += Newly Completed Supply Structure (eg. Supply Depot, Command Center). 4. **Queue & Production:** - Progress += (5s / Total Build Time at game_speed=1.4). Note: Game Speed is 1.4. - If Progress >= 100%: Remove from [Queue], add to [My Units](if unit) or [My Structures](if structure), and update [Info] (Supply Cap, Upgrades) accordingly. - New Actions: Add newly issued training/building Action to [Queue]. 5. **Movement:** For moving units: New Pos Start Pos + Direction_Vector * (min(Remaining Distance, Speed * 5s)). 6. **Status:** Status Change depends on the Command and the Target. 7. **Combat:** - If units are \"Attacking\" or under attack, reduce HP based on estimated DPS and duration. - Remove units if HP <= 0. 8. **Visibility:** - Unit Hiding: Units inside structures (e.g., SCVs in Refineries, units in Bunkers/Medivacs) do NOT appear in [My Units] list temporarily, though they still consume Supply. - Fog of War: Update [Visible Hostiles] based on current visual radius of self units and structures. Remove enemy units that move into the fog or become cloaked without detection. 9. **Consistency:** Units/Structures not mentioned in the Action Sequence remain unchanged (ID, Position, HP , Status and so on). ### Output Format Requirements 1. **Raw Text Only**: Output the observation text directly. Do NOT use JSON, Markdown code blocks, or conversational fillers (e.g., \"Here is the prediction\"). 2. **Format**: The output must start strictly with the [Info] tag and follow the exact structure of the input observation. /no_think\" }, Output: { \"role\": \"assistant\", \"content\": \"<think> </think>[Info] Time: 05:49 Race: Terran Enemy Race: Terran Map: Flat64 Minerals: 230 (+2547/min) Gas: 23 (+268/min) Supply: 68/93 (Army: 19, Workers: 49) Alerts: None Upgrades: Punishergrenades [Queue] - Orbitalcommand [1] at (63,61): Commandcentertrain_scv (75%) - Barracks [65] at (53,59): Barrackstrain_marine (7%) - Orbitalcommand [83] at (43,64): Commandcentertrain_scv (34%) - Barracks [86] at (55,55): Barrackstrain_marine (53%) - Barracks [90] at (58,57): Barrackstrain_marine (81%), Barrackstrain_marine (62%) - Factory [190] at (43,57): Factorytrain_siegetank (14%) - Commandcenter [226] at (24,61): Upgradetoorbital_orbitalcommand (0%) - Constructing: Barracksreactor [334] at (65,54) (73%) - Constructing: Missileturret [352] at (18,68) (93%) [My Units] > Workers: (40Mining :[3,5,6,7,8,9,11,13,35,61,71,77,82,84,115,139,144,153,155,159,179,183,196,217,222,225,237,240,254,256,264, 271,292,307,311,320,337,351,360,367], 2Mule:[305,313]) - Scv [2] at (51,71) (HP:100%, St:Moving) - Scv [10] at (65,66) (HP:100%, St:Moving) - Scv [154] at (21,66) (HP:100%, St:Moving) - Scv [160] at (68,63) (HP:100%, St:Moving) - Scv [326] at (17,67) (HP:100%, St:Building) > Army: - Marine [135] at (37,57) (HP:100%, St:Idle) - Marine [143] at (37,56) (HP:100%, St:Idle) - Marine [147] at (36,57) (HP:100%, St:Idle) - Marine [324] at (38,23) (HP:100%, St:Moving) - Marauder [332] at (38,35) (HP:73%, St:Attacking) - Marauder [347] at (36,36) (HP:21%, St:Moving) - Marine [356] at (67,44) (HP:100%, St:Attacking) - Marine [358] at (32,46) (HP:100%, St:Moving) - Marine [359] at (26,44) (HP:100%, St:Attacking) - Marine [368] at (54,61) (HP:100%, St:Attacking) [My Structures] - Orbitalcommand [1] at (63,61) (HP:100%, Eng:16%, St:Training) - Supplydepotlowered [36] at (50,60) (HP:100%, St:Idle) - Barracks [65] at (53,59) (HP:100%, St:Training, AddOn:Techlab [148]) - Orbitalcommand [83] at (43,64) (HP:100%, Eng:19%, St:Training) - Barracks [86] at (55,55) (HP:100%, St:Training, AddOn:Techlab [150]) - Barracks [90] at (58,57) (HP:100%, St:Training, AddOn:Reactor [158]) - Refinery [114] at (70,58) (HP:100%, St:Idle) - Refinery [122] at (59,68) (HP:100%, St:Idle) - Bunker [140] at (44,54) (HP:100%, St:Idle) - Engineeringbay [146] at (57,52) (HP:100%, St:Idle) - Barrackstechlab [148] at (56,59) (HP:100%, St:Idle) - Barrackstechlab [150] at (58,55) (HP:100%, St:Idle) - Supplydepotlowered [156] at (53,57) (HP:100%, St:Idle) - Barracksreactor [158] at (61,57) (HP:100%, St:Idle) - Supplydepotlowered [164] at (60,53) (HP:100%, St:Idle) - Factory [190] at (43,57) (HP:100%, St:Training, AddOn:Techlab [252]) - Commandcenter [226] at (24,61) (HP:100%, St:Upgrading) - Supplydepotlowered [251] at (55,53) (HP:100%, St:Idle) - Factorytechlab [252] at (46,57) (HP:100%, St:Idle) - Barracks [257] at (62,54) (HP:100%, St:Building) - Supplydepotlowered [265] at (42,55) (HP:100%, St:Idle) - Missileturret [299] at (43,73) (HP:100%, St:Idle) - Missileturret [314] at (70,68) (HP:100%, St:Idle) - Supplydepotlowered [333] at (46,59) (HP:100%, St:Idle) - Refinery [335] at (28,68) (HP:100%, St:Idle) [Visible Hostiles] > Enemy Units: - Scv [181] at (34,21) (HP:88%) - Banshee [230] at (32,29) (HP:83%, Eng:5%) - Banshee [239] at (34,27) (HP:87%, Eng:1%) - Vikingassault [281] at (28,36) (HP:57%) - Marine [366] at (30,34) (HP:11%) > Enemy Structures: - Barracks [141] at (30,22) (HP:100%) - Starport [142] at (30,25) (HP:100%) - Starporttechlab [162] at (33,25) (HP:91%) - Barracks [306] at (30,28) (HP:100%) - Barrackstechlab [365] at (33,22) (HP:92%) > Snapshot Enemy Structures: - Supplydepotlowered at (21,28) - Orbitalcommand at (24,22) - Supplydepotlowered at (28,28) - Factory at (23,28) - Refinery at (17,25) - Refinery at (28,15) \" } E.2 Example of StarWM-Agent Online Decision Making We present an example of the online decision-making context of StarWM-Agent, including the policys initial action proposal, the world models simulated future observation, and the refined action produced by the policy. This example demonstrates how the GenerateSimulateRefine loop operates in practice. Example of StarWM-Agent Online Decision Making [{ \"role\": \"user\", \"content\": \"As top-tier StarCraft II strategist, your task is to give one or more commands based on the current game state. Only give commands which can be executed immediately, instead of waiting for certain events. ### Aim Our final aim: destroy all enemies as soon as possible. Our strategy: - Resource collection: produce workers and gather minerals and gas - Development: build attacking units and structures - Attacking: concentrate forces to search and destroy enemies proactively ### Current Game State # Round state Time: 01:02 Race: Terran Minerals: 145 Vespene: 12 Supply army: 0 Supply workers: 13 Supply unused: 18 Map size: 64x64 # Own units [577, 145, 281, 713, 857, 457, 1, 721, 569, 425, 865]SCV State: collecting resources automatically [289]SCV Position: (25, 53) Health: 45/45 (100%) State: constructing [(24, 53)] # Unit abilities SCV[577, 145, 281, 713, 857, 457, 1, 721, 569, 425, 289, 865]: TERRANBUILD_COMMANDCENTER, TERRANBUILD_SUPPLYDEPOT, TERRANBUILD_REFINERY, TERRANBUILD_BARRACKS # Own structures [137]CommandCenter Position: (24, 61) Health: 1500/1500 (100%) State: idle Harvesters: 9/16 [169]SupplyDepot Position: (24, 57) Health: 400/400 (100%) State: idle [321]Barracks(building 29%) Position: (24, 53) [601]Refinery Position: (17, 58) Health: 500/500 (100%) State: idle Harvesters: 3/3 (no more harvesters accepted) [745]SupplyDepot Position: (29, 61) Health: 400/400 (100%) State: idle # Structure abilities CommandCenter[137]: COMMANDCENTERTRAIN_SCV # Visible enemy units [Empty] # Visible enemy structures [Empty] # Action history {\"action\": \"COMMANDCENTERTRAIN_SCV\", \"units\": [137]} {\"action\": \"TERRANBUILD_SUPPLYDEPOT\", \"units\": [281], \"target_position\": [24, 59]} {\"action\": \"TERRANBUILD_SUPPLYDEPOT\", \"units\": [865], \"target_position\": [27, 61]} {\"action\": \"TERRANBUILD_REFINERY\", \"units\": [577], \"target_unit\": 513} {\"action\": \"TERRANBUILD_BARRACKS\", \"units\": [289], \"target_position\": [24, 55]} # Map information Closest mineral fields: [665](25, 67), [633](18, 61), [729](24, 68), [801](20, 67), [497](17, 62), [297](18, 65), [193](21, 68), [313](17, 64) Closest vespene geysers: [513](17, 58), [329](28, 68), [369](36, 65), [945](20, 48) # Ability description COMMANDCENTERTRAIN_SCV(target: None): Train SCV, Terran worker. Cost: 50 minerals. TERRANBUILD_BARRACKS(target: Point): Build Barracks to produce infantry units (Marine, Reaper, Marauder, Ghost). Cost: 150 minerals. TERRANBUILD_COMMANDCENTER(target: Point): Build Command Center, the Terran base. Cost: 400 minerals. TERRANBUILD_REFINERY(target: Unit): Build Refinery on Vespene Geyser. Cost: 75 minerals. TERRANBUILD_SUPPLYDEPOT(target: Point): Build Supply Depot to increase 8 supplies and unlock Barracks. Cost: 100 minerals. ### Rules Rule checklist: 1. Produce as many units with the strongest attack power as possible. 2. The total cost of all commands should not exceed the current resources (minerals and gas). 3. Commands should not build redundant structures(e.g. 2 Refinery while one is not fully utilized). 4. Commands should not use abilities that are not supported currently. 5. Commands should not build structure that is not needed now (e.g. build Missile Turret but there is no enemy air unit). 6. The unit production list capacity of structures is 5. If the list is full, do not add more units to it. 7. Commands should not send SCV or MULE to gather resources because the system will do it automatically. 8. Commands should not train too many SCVs or MULEs, whose number should not exceed the capacity of CommandCenter and Refinery. 9. Commands can construct new one Supply Depot only when the remaining unused supply is less than 7. 10. Do not give any action that is irrelevant to the task. 11. Each of units can only be used in the whole response once at most. 12. If unit is already performing an action as given task, you should ignore it, instead of giving repeated action for it. 13. If one task cannot be finished, just ignore it. 14. If resource is not enough, just complete the most important part of the task. Give an action JSON in the following format wrapped with triple backticks: [ { \"action\": \"<action_name>\", \"units\": [<unit_id>, <unit_id>, ...], # units you want to command \"target_unit\" (optional): <unit_id>, # some existing unit \"target_position\" (optional): [x, y] }, // more actions ... ] Example: [ \"action\": \"ATTACK_ATTACK\", \"units\": [1, 2, 3], \"target_unit\": \"action\": \"MOVE_MOVE\", \"units\": [4, 5], \"target_position\": [50, 60] \"action\": \"COMMANDCENTERTRAIN_SCV\", \"units\": [6] { }, { }, { } ] \" }, { \"role\": \"assistant\", \"content\": \"<think> </think> [ \"action\": \"TERRANBUILD_SUPPLYDEPOT\", \"units\": [713], \"target_position\": [30, 61] { } ] \" }, { \"role\": \"user\", \"content\": \" Here is simulation report showing the **PREDICTED GAME STATE** 5 seconds **IF** your original proposed action is executed. <simulation_report> # Round state Time: 01:07 Race: Terran Enemy Race: Terran Minerals: 50 Vespene: 24 Supply army: 0 Supply workers: 13 Supply unused: 18 Map size: 64x64 # Own units [865, 857, 425, 713, 569, 1, 457]Scv State: collecting resources automatically [721]Scv Position: (21, 67) Health: 100% State: idle [281]Scv Position: (20, 61) Health: 100% State: idle [145]Scv Position: (24, 65) Health: 100% State: idle [289]Scv Position: (25, 54) Health: 100% State: constructing [577]Scv Position: (21, 60) Health: 100% State: idle # Own structures [137]Commandcenter Position: (24, 61) Health: 100% State: idle [169]Supplydepot Position: (24, 57) Health: 100% State: idle [601]Refinery Position: (17, 58) Health: 100% State: idle [745]Supplydepot Position: (29, 61) Health: 100% State: idle [321]Barracks Position: (24, 53) State: under construction (40%) [NEW]Supplydepot Position: (30, 61) State: under construction (23%) # Visible enemy units [Empty] # Visible enemy structures [Empty] </simulation_report> ### TACTICAL ANALYSIS REQUIRED **CRITICAL INSTRUCTION**: 1. **LOOK BACK**: You MUST compare the **<simulation_report>** above with the **### Current Game State** provided in the **first user message**. 2. **HYPOTHETICAL STATE**: This report is **prediction**. The action you proposed previously has **NOT** been executed yet. - **KEEP** your original commands by default. - **ADD** new commands if you find idle units/structures or extra resources in the prediction. - **REMOVE/CHANGE** original commands ONLY if they lead to bad trades. Analyze the predicted state and output the OPTIMIZED JSON: 1. **Check Resources & Supply**: - **Supply Check**: Look at \"Supply unused\" in the prediction. Is it low (< 3)? - If YES, you are about to be Supply Blocked. **ADD** command to Build SupplyDepot (Use valid SCV ). - **Gas Strategy**: - Look at \"Vespene\". Is it very low (e.g., 0 or < 50) while \"Minerals\" are high? **ADD** Build Refinery to start gathering gas. - Is \"Vespene\" high (e.g., > 150)? Its time to develop technology. **ADD** commands to Build Factory/ Starport, Build TechLab, Upgrade (EngineeringBay), or Train Marauder/SiegeTank. - **Mineral Surplus & Expansion**: - Look at \"Minerals\". Are they high (e.g., > 400)? Its time to expand. **ADD** Building Orders (e.g., Build Command Center (Expand), Build Barracks (Increase Production)). 2. **Check Structure States**: - **Queue Limit Check**: Look at the \"Production list\" of each structure. - Does any list contain 4 or 5 items? (e.g., Marine (25%), Marine (Waiting), Marine (Waiting)...). - If YES, the queue is **FULL**. **Do NOT** add more training commands to this structure. Instead, if resources allow, **ADD** commands to Build Barracks/Factory to increase production capacity. - **Fill the Queue**: Look at \"Production list\". Is it empty or does it only have 1 item? - If YES, the structure needs more orders. **ADD** commands to fill the queue (e.g., Train SCV for CC , Train Marine for Barracks). - **Idle Structure Check**: - Look at \"Own structures\". Are there any marked as **\"State: idle\"**? If YES, immediately **ADD** commands to train units (e.g., Train SCV for CC, Train Marine for Barracks). - Are tech structures (e.g., TechLab, EngineeringBay) marked as **\"State: idle\"**? If YES, **ADD** commands to upgrade technology. 3. **Check Unit Health & Survival**: - **Idle Army Check**: Look at \"Own units\". Are there any Army units marked as **\"State: idle\"** while the **Visible enemy units** or **Visible enemy structures** list is NOT empty? - If YES, do not just wait. Add an **Attack** command to pressure the enemy. - **Survival & Bad Trade Check**: Compare the **Own units** and **Own structures** list in the prediction with the **### Current Game State**. Are any units missing (This means they **DIED**) without Killing or Heavily Damaging enemy units? - If YES, this is **Bad Trade**. **CHANGE** your command to **Retreat** or **Reposition** to save your units. - **Damage Check**: Look at the \"Health\" of your units. Is any units health low or dropped significantly ? - If YES, the unit is under attack. Add command to **Kite** (move back slightly) or **Heal/Repair** ( Medivac/SCV) to save it. - **Vision & Scouting Check**: is the **Visible enemy structures** list [Empty]? - If YES, your vision is limited. **ADD** Attack or Move commands to **search for enemy bases**. ### NEW ACTION RULES - **VALIDATE ACTORS**: When adding new commands, refer to **Unit abilities**, **Structure abilities**, ** Ability description** in the **### Current Game State** provided in the **first user message**. Ensure the selected units actually have the ability to perform the action. - **CORRECT UNIT SELECTION**: When adding TERRANBUILD commands, you MUST find valid **SCV** from the **### Current Game State** (marked as State: idle or State: collecting resources automatically). **Output Rules**: Create an OPTIMIZED JSON list that combines your beneficial original actions with any NEW necessary and valid actions derived from the analysis above. /no_think\" }, { \"role\": \"assistant\", \"content\": \"<think> </think> [ \"action\": \"COMMANDCENTERTRAIN_SCV\", \"units\": [137] { } ] \" } ]"
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "The Key Laboratory of Cognition and Decision Intelligence for Complex Systems, Institute of Automation, Chinese Academy of Sciences"
    ]
}