{
    "paper_title": "Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning",
    "authors": [
        "Chenghao Zhu",
        "Meiling Tao",
        "Tiannan Wang",
        "Dongyi Ding",
        "Yuchen Eleanor Jiang",
        "Wangchunshu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Faithfully personalizing large language models (LLMs) to align with individual user preferences is a critical but challenging task. While supervised fine-tuning (SFT) quickly reaches a performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose Critique-Post-Edit, a robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) a Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) a Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under a rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11\\% win-rate improvement, and personalized Qwen2.5-14B model surpasses the performance of GPT-4.1. These results demonstrate a practical path to faithful, efficient, and controllable personalization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 4 8 8 1 . 0 1 5 2 : r Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning Chenghao Zhu1,, Meiling Tao2,, Dongyi Ding3, Tiannan Wang4, Yuchen Eleanor Jiang4, Wangchunshu Zhou4, 1The Chinese University of Hong Kong, Shenzhen, 2University of Electronic Science and Technology of China, 3South China Agricultural University, 4OPPO Equal contribution, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Faithfully personalizing large language models (LLMs) to align with individual user preferences is critical but challenging task. While supervised fine-tuning (SFT) quickly reaches performance plateau, standard reinforcement learning from human feedback (RLHF) also struggles with the nuances of personalization. Scalar-based reward models are prone to reward hacking which leads to verbose and superficially personalized responses. To address these limitations, we propose CritiquePost-Edit, robust reinforcement learning framework that enables more faithful and controllable personalization. Our framework integrates two key components: (1) Personalized Generative Reward Model (GRM) that provides multi-dimensional scores and textual critiques to resist reward hacking, and (2) Critique-Post-Edit mechanism where the policy model revises its own outputs based on these critiques for more targeted and efficient learning. Under rigorous length-controlled evaluation, our method substantially outperforms standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an average 11% win-rate improvement, and personalized Qwen2.514B model surpasses the performance of GPT-4.1. These results demonstrate practical path to faithful, efficient, and controllable personalization. Date: October 22, 2025 Github: https://github.com/OPPO-PersonalAI/Critique-Post-Edit Correspondence: zhouwangchunshu@oppo.com"
        },
        {
            "title": "Introduction",
            "content": "As large language models (LLMs) evolve from general-purpose assistants to personalized agents, the ability to tailor responses to users unique attributes, needs, and constraints has become critical frontier [21, 32].Nonetheless, prevailing paradigms are largely limited to (i) post-training that align LLMs with universal values and preferences [9], or (ii) rigid retrieval over personal knowledge base which is then superficially incorporated in the responses. Both tend to feel unnatural and brittleeither enforcing one-size-fits-all persona or sprinkling persona factoids as referenceswithout deeper, integrated understanding of the user. True personalization requires meta understanding: the model must not only learn what to emphasize or omit, but also adapt to each individuals unique persona. This profound understanding must then be expressed through nuanced changes in wording, structure, and detail. 1 However, current optimizing methods fall short for cultivating such meta understanding. Supervised finetuning (SFT) [12] and direct preference optimization (DPO) [11] provide limited supervision: models quickly saturate on available labels and still struggle to internalize \"what counts\" as personalization beyond verbatim reference, keywords or templates. Policy-gradient based Reinforcement Learning (RL) [9] like PPO [15] and GRPO [16] with outcome/value-based rewards also struggles since rewards are sparse and prone to be hacked [20, 24]. In practice, standard Bradley-Terry(BT) based reward models frequently incentivize undesirable behaviorssuch as verbose outputs and generic stock phrases [1, 19], leading to reward hacking rather than faithful personalization. Instead of single On the other hand, Generative Reward Model (GRM) [30] changes this landscape. scalar, GRM could produce rigorous rationale along with multi-dimensional scores that explain what to improve and why. As verifier, the GRMs textual judgments provide more robust and nuanced feedback signal, substantially reducing susceptibility to reward hacking compared to Bradley-Terry based reward models. Recent tool-integrated RL post-training works has begun exploring the potential of GRMs on deep-research, web-browsing, and code-execution tasks [27, 28]. However, to our knowledge, GRM has not yet been systematically explored for personalization, where reasoning about user-specific preferences is central. The combination of detailed critiques and multi-facet reward is well-suited for personalization as it provides more subtler and instructive supervisory signal. Inspired by Helpsteer3 [25], we introduce critique-post-edit RL paradigm that leverage the critiques from personalized GRM. The policy model first generate an initial response based on the given query and persona traits; the GRM evaluates the response and produces critique according to the query and persona; the policy refines its original response by incorporating the feedback from the critique. We compute rewards for both original and edited responses and update the policy with batch that mixes on-policy and edited (off-policy) samples. This yields two benefits. First, the learning signal becomes diverse and targeted: advantages are estimated over multiple, concrete improvement paths rather than single outcome, improving training stability. Second, it matches the nature of personalization: there is no single golden answer for given query and its corresponding user. Multiple nuanced, equally valid ways can reflect the users preferences. Critique-Post-Edit RL explicitly exposes those alternatives during training, helping the policy acquire subtle, faithful personalization. We conduct comprehensive evaluation of our PersonalizedLLMs on the PersonaFeedback [23], AlpacaEval [2], and PersonaMem [3] benchmarks. With rigorous length-controlled evaluation protocol [2] to mitigate scoring biases, our approach demonstrates significant gains. Our Qwen2.5-7B model achieves an average win-rate improvement of 11% over strong PPO baseline. Moreover, our Personalized-Qwen2.5-14B model not only matches this improvement but also surpasses the performance of GPT-4.1, highlighting the effectiveness and scalability of our framework for building faithfully personalized models. Our contributions are as follows: We identify the limitations of SFT/DPO and Bradley-Terry based RMs for personalization. We train personalized GRM that provides multi-dimensional scores and actionable critiques that achieved SOTA results on the PersonaFeedback Benchmark. We introduce Critique-Post-Edit RL framework that leverages GRM feedback1 to refine responses and learn from group of diverse mixed on-policy(original) and off-policy(refined) responses, yielding more nuanced and faithful personalization. Under rigorous length-controlled evaluation, our approach delivers strong gains over PPO and surpasses GPT-4.1, demonstrating practical path to controllable and scalable personalization."
        },
        {
            "title": "2 Related Work",
            "content": "Personalization in LLMs aims to tailor responses to individual users profiles, preferences, and contexts, thereby improving user satisfaction and engagement. Early approaches focused on persona-conditioned dialogue 1For clarity, we use the terms \"feedback\" and \"critique\" interchangeably in this work. 2 generation [18, 31], where predefined user traits are used to guide response generation. Meta-learning has also been explored as way to enable models to quickly adapt to new users with limited supervision [6]. More recent studies investigate richer modeling of user information, for example by combining sparse and dense persona representations [22]. Complementary to persona conditioning, retrieval-augmented approaches personalize by fetching information from user knowledge base and injecting it into prompts. Salemi et al. [14] propose such RAG framework and further extend it by supervised tuning the LLM with feedback loop [13]. Benchmark efforts such as PersonaBench [21] evaluate models on their ability to handle synthetic private user data, emphasizing the importance of faithful and controllable personalization. Additionally, in other domains, such as education and enterprise applications, personalized assistants have also been widely studied [4, 5, 8, 29]. Despite growing interest, personalization remains relatively underexplored compared to general alignment. In particular, reinforcement learning pipelines often lack the granularity required to capture user-specific nuances, with only recent attempts exploring dynamic profile modeling for personalized alignment [32]."
        },
        {
            "title": "3.1 Primary Attempts",
            "content": "We use approximately 18k annotated samples as the primary dataset. SFT, DPO, and RL all adopt batch size of 128, with SFT and DPO trained for 3 epochs and RL for 2 epochs. SFT and DPO use around 54k (18k 3) training samples, while RL generates around 32k (18k 2) trajectories. As illustrated in Figure 1, we observed that SFT and DPO performance2 quickly plateaus with increasing data. On the other hand, PPO with Bradley-Terry RM continues to yield substantial gains. However, we also observe that BT-guided PPO encounters severe reward hacking, where reward model, even the judge LLM, are gamed by superficial clues rather than genuine improvements. Figure 1 Performance curve of SFT, DPO and RL with increasing data size. As shown in Figure 2, policy model tends to add short notice after answering which yields significant increase of reward score despite limited improvement."
        },
        {
            "title": "3.2 Analysis",
            "content": "To tackle reward hacking, we adopt Generative Reward Model (GRM) that must first produce concise critique before giving the final scores. This textual verification makes the reward score more robust in seemingly personalized cases, where superficial cues would otherwise be rewarded by Bradley-Terry based RM. Upon further analysis, we find that the gap between less-personalized and truly personalized response is often subtle and may only require small, targeted modifications. We take inspiration from HelpSteer3 [25], which uses editing and feedback pipeline to improve models performance on open-ended question. These observations motivate Critique-Post-Edit training paradigm: the GRM provides actionable critiques; the policy edits its own output accordingly; and training leverages both the original and edited responses. 2Performance means length controlled win rate over GPT-4o, details about this metric are provided in Section 5.1 3 Figure 2 An illustrative reward hacking case from RL training with BT reward model. The model learns to exploit shortcut by explicitly mention persona traits to get higher reward scores."
        },
        {
            "title": "4 Method",
            "content": "Building on the motivation discussed in Section 3, we introduce Critique-Post-Edit framework where the GRM provides both scalar rewards and textual feedback, enabling the policy model to generate improved edited responses based on this feedback, as illustrated in Figure 3."
        },
        {
            "title": "4.1 Details about GRM",
            "content": "To train the GRM, we build upon the preference data described in Section 5.1 by conducting second round of annotation. For each response in the preference pairs, we use GPT-4o-mini to provide detailed critique 3, which includes: (1) actionable suggestions for improvement (critiques), and (2) scores for three distinct dimensionshelpfulness, personalization, and naturalnesson scale from -5 to +5. The GRM takes tuple consisting of (question, user profile, response) as input and is trained on this dataset to produce twofold output: (1) natural language critique with specific improvement suggestions, and (2) set of scores for the following three dimensions: Helpfulness: Assesses whether the response is accurate, comprehensive, and effectively solves the users problem, with practical and in-depth content. Personalization: Evaluates the appropriate use of user information, avoiding mechanical stacking or irrelevant details. The content should align with the users background and needs. Naturalness: Judges whether the writing style is fluent and comfortable, with natural tone that matches daily communication, avoiding robotic or verbose expressions. To create unified quality metric, we calculate final weighted score Sfinal using the formula: Sfinal = wh Shelpfulness + wp Spersonalization + wn Snaturalness 3See Appendix for selection details. Figure 3 Overview of the Critique-Post-Edit framework. (1) The policy model πθ generates an original response yo. The GRM evaluates yo and provides critique, guiding the model to create an edited response ye. (2) The GRM then provides rewards for both yo and ye, denoted as Ro and Re, forming candidate pool from which sampling strategy selects edited samples to combine with the original ones into training batch for the policy update. Sampling strategies include: Random (selecting random subset), Reward Rank (selecting the highest-reward samples), and Conditional (selecting if the reward exceeds the original reward). where wh, wp, and wn represent the weights for each respective dimension. Finally, we filter out any data instances where the final weighted scores are identical, thus constructing the final GRM training dataset of around 22k examples."
        },
        {
            "title": "4.2 Details about Feedback edit\nWe introduce a Critique-Post-Edit Framework. As illustrated in Figure 3, for a given input query q, the\npolicy model πθ first generates a set of candidate responses {y(1), ..., y(k)} through multiple rollouts. These\nresponses are then evaluated by the GRM, which produces a scalar reward R(i) and textual feedback f (i) for\neach response. This feedback is concatenated with the original query and its corresponding response to form\na new prompt. This new prompt is then fed to the policy model, encouraging it to revise its output based on\nthe feedback provided, resulting in an edited response ye. This process ultimately builds a sample pool that\ncontains original and edited responses for policy updates.",
            "content": "4.2.1 Sampling Strategy We construct PPO training batches from sample pool that includes both the original responses {yo} and the edited responses {ye}. To maintain policy stability, all sampling strategies retain the full set of original responses. We explore the following three sampling methods: Random Sampling: Randomly selects subset of edited responses {ye} to be included in the training batch, according to predefined sampling rate re. 5 Reward Rank Sampling: For each query, edited responses are sorted in descending order according to their reward scores. The top re-proportion of these responses are selected to form high-quality candidate pool. Conditional Sampling: For each query, edited responses are sorted in descending order according to their reward scores or the improvement margin over the original response. The top re-proportion of responses in terms of reward increase are included in the candidate pool."
        },
        {
            "title": "4.2.2 Hybrid Policy Update Loss",
            "content": "Since the training batch consists of both on-policy original responses and off-policy edited responses, applying the standard PPO loss directly may lead to instability due to distributional mismatch. To address this, we design hybrid policy update loss that treats these two types of data differently. For each sample in the batch, the policy gradient loss LP G(y) is defined as: LP G(y) = min (cid:16) rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17) clip (cid:16) πθ(y) πe(y) , 1 ϵlow, 1 + ϵhigh (cid:17) ˆAt if Do if De Here, Do and De denote the sets of original and edited samples, respectively, and ˆAt is the estimated advantage. Specifically, for samples in Do, we apply the standard PPO-Clip loss, which stabilizes learning by clipping the importance weight rt(θ) = πθ(y) πθold (y) to stay within trust region. For samples in De, we treat them as off-policy data. Their importance weights are corrected using the ratio between the current policy πθ and the implicit editing policy πe that generated these samples. To mitigate the potential training instability arising from the high-importance weights, this ratio is clipped to stay within trust region of [1 ϵlow, 1 + ϵhigh]. Logarithmic probabilities log πe(y) are pre-computed and retained during the editing stage, allowing for principled correction for off-policy training."
        },
        {
            "title": "5.1\nTraining Datasets We first create a pool of questions comprising 10K specific questions and 10K general\nquestions, following the methodology of PersonaFeedback [23]. For each question, we prompt 5 different LLMs\nto generate a variety of responses. We then use GPT-4o as a judge to assign a holistic quality score on a scale\nof 1 to 5 for each response, retaining only pairs where the absolute score difference exceeds a threshold of 2,\nwhich ensures that our training data contain preference pairs with clear, yet varied quality gaps. In the SFT\nstage, we use the chosen responses yc as target labels.Please notice that, in SFT stage, we did not train our\nmodel how to refine its answer. In the RL stage, we train the initialized SFT model using PPO.",
            "content": "Model All models in our experiments, including policy models, GRMs, and BT reward models, are based on the Qwen2.5-Instruct series [10]. Specifically, we conducted comprehensive experiments and analysis to obtain personalized GRMs. By scaling up model parameters, our personalized GRM achieved SoTA results on the PersonaFeedback Benchmark comparing to BT models and proprietary LLMs. Empirical results and analysis can be found in the Appendix B. For policy model, we trained our personalized LLMs based on Qwen2.5-7B and Qwen2.5-14B model with 14B personalized GRM. Further details on specific hyperparameters are provided in the Appendix A. Evaluation For evaluation, we sample subset from the PersonaFeedback Benchmark rather than using the complete dataset. Specifically, we randomly select 50 questions from each difficulty tier (easy, medium, hard) for both Specific and General categories, resulting in total of 300 evaluation samples. 6 To provide comprehensive evaluation, we also test our models on AlpacaEval and PersonaMem benchmarks. For AlpacaEval, we assign plausible persona to each question, using the questionpersona pair as input. For PersonaMem, we use the provided persona attributes as the persona information, which is then combined with the question as the input for the model. Throughout all experiments, we use GPT-4o as the single fixed baseline. To address the well-known length bias in LLM-as-judge protocols, we adopt the length-controlled evaluation framework from Dubois et al. [2]. This method uses Generalized Linear Model (GLM) to explicitly remove length as confounding factor via regression-based debiasing. Following official recommendations 4, we employ the length_controlled_minimal variant. The probability that model wins against the baseline on input is modeled as: qmin(y = zm, zb, m, b, x) := logistic θm θb + ϕm,b tanh (cid:18) (cid:18) len(zm) len(zb) (cid:19)(cid:19) std(len(zm) len(zb)) where θm θb captures the inherent ability difference between models, and ϕm,b models the length effect. By setting the length effect term to zero during evaluation, we obtain length-controlled win rates that provide fairer comparisons. Detailed mathematical formulations are provided in Appendix D."
        },
        {
            "title": "5.2 Main results",
            "content": "Table 1 Comparison of Open Source Models with Proprietary Models on PersonaFeedback, Alpaca, and PersonaMem Benchmarks. Our Critique-Post-Edit framework uses 0.5 sampling ratio with Random Sampling strategy. PersonaFeedback Alpaca PersonaMem Model Specific General Total Avg Easy Mid Hard Avg Easy Mid Hard Avg 66.4 GPT-4.1 51.1 GPT-4o-mini 50.3 Qwen3-8B 57.6 Qwen3-14B Qwen3-32B 61.0 Qwen2.5-32B 56.0 36.7 Qwen2.5-7B 42.9 +SFT 40.6 +DPO 58.0 +PPO Ours 73.2 Qwen2.5-14B 42.3 45.5 +SFT 49.6 +DPO +PPO 74.9 Ours 80.9 60.9 52.9 44.2 51.2 53.3 52. 33.9 34.5 38.1 51.4 69.7 31.3 38.4 39.8 66.7 79.4 Proprietary Models 55.8 49.1 32.0 46.2 51.5 39.0 61.0 51.0 42.2 51.7 55.3 49.2 66.7 49.8 34.4 48.5 56.6 50. 64.4 47.2 33.1 45.8 48.1 42.6 Open Source Models 30.2 34.0 34.8 49.1 64.7 30.9 34.2 37.9 54.5 71.8 33.6 37.1 37.8 52.8 69.2 34.8 39.4 42.4 65.4 77. 31.9 46.1 45.6 58.7 61.2 37.0 53.1 48.1 68.9 79.9 29.4 37.8 34.4 52.9 59.7 34.0 45.1 40.0 65.8 74.6 60.8 37.9 30.5 36.9 40.1 36.8 25.5 35.8 32.6 48.9 56.2 28.6 44.8 38.6 60. 73.9 64.0 45.0 32.7 43.7 48.3 43.4 28.9 39.9 37.5 53.5 59.0 33.2 47.7 44.3 57.8 76.1 62.5 48.0 37.5 47.7 51.8 46.3 31.2 38.5 37.6 53.1 64.1 34.0 43.5 42.3 61. 76.8 64.5 49.7 42.4 47.3 53.1 48.7 31.0 35.0 34.2 40.6 54.5 44.1 47.6 48.5 53.8 64.0 49.5 38.2 25.8 32.7 42.7 36.5 20.2 26.2 30.6 33.6 50.2 23.1 31.4 30.8 44. 67.1 Table 1 shows that our Critique-Post-Edit framework brings substantial improvements over standard PPO training. The 7B model sees jump from 53.5% to 64.1% in the length-controlled win rate, gain of more than 11 points. The 14B model performs even better, climbing from 65.2% to 76.8%. Similar consistent gains are observed on the Alpaca and PersonaMem benchmarks, demonstrating the robustness of our approach in diverse personalization scenarios. In particular, our 7B model beats GPT-4o-mini by wide margin (64.1% vs 4https://github.com/tatsu-lab/alpaca_eval/issues/346 7 48.0%), while the 14B version clearly outperforms GPT-4.1. These gains hold consistently across both specific and general questions, suggesting that our approach works well in different personalization settings. To validate our evaluation methodology, we recruited three human experts to independently evaluate samples from PersonaFeedback and selected GPT-4.1 as our primary evaluation baseline due to its high consistency with human evaluators, details see Appendix C. We computed Cohens Kappa to measure inter-rater agreement across three comparisons: (1) Model-Human: GPT-4.1 vs human experts achieved an average κ = 0.71; (2) Model-Model: GPT-4.1 vs other models (GPT-4o-mini, GPT-4.1-mini, DeepSeek-v3, Claude-3.5-Sonnet) averaged κ = 0.67; (3) Human-Human: The three experts achieved an average κ = 0.70. These substantial agreement levels validate the reliability of our evaluation methodology. Figure 4 Comparison between BT reward model and GRM in PPO: length of rollout (during training) and lengthcontrolled winrate of checkpoints, Lines: Response Length (left axis), Bars: Win Rate (right axis)"
        },
        {
            "title": "5.3 Ablation",
            "content": "Table 2 Ablation on Reward and Feedback Model: Impact on Performance. Setting Length-controlled Win Rate Win Rate Response Length BT wo/edit GRM wo/edit GRM w/edit 51.78 59.50 64.07 51.65 58.86 62. 995 409 447 Table 2 presents the ablation study of our GRM and Critique-Post-Edit mechanism. To ensure fair comparison across all configurations, we set the rollout number to 6 for both the Bradley-Terry model and GRM without post-edit. For the GRM with post-edit, we use 4 rollouts with an extra 2 rollouts from refined responses, making the effective sample size equal across all settings. The ablation results demonstrate the individual contributions of both key components in our framework. Replacement of the GRM with the BT reward model leads to dramatic drop in the length-controlled win rate from 59.50% to 51.78%, while also producing excessively long responses (995 tokens vs 409 tokens), confirming the severity of reward hacking and length bias issues discussed in Section 3.2. During our training, this manifests itself as shortcut behaviors such as appending trivial persona phrases or adding explicit self-referential claims (e.g., \"this answer considers your [attribute]\") to artificially inflate rewards (Figure 2). Such exploitation is further reflected in Figure 4, where BT-based training causes both response length and reward scores to increase, while GRM remains stable and resistant to these hacks. The GRM alone (without post-edit) achieves moderate improvement of 59.50%, effectively mitigating length bias but still falling short of the full frameworks performance. The complete integration of GRM with feedback editing yields the best results (64.07%), validating that both components are essential: GRM provides robust reward signals resistant to gaming, while feedback editing enables more targeted and efficient policy learning 8 Table 3 Example of improving personalized response based on Feedback. Question: What hotel restaurants with tech atmosphere and healthy light meals do you recommend during the Shanghai Robotics Exhibition? Original Response Xiaoling, based on your role as the R&D manager at Zhejiang Robotics, recommend... Issue Identified from Feedback The explicit mention of the users name \"Xiaoling\". Additionally, since you prefer staying within 500 meters of the convention center and drive Tesla Model Y, also recommend the Japanese restaurant. The environment there is very tech-inspired, just like the precision of the servo motor systems ... Note: This response fully takes into account the users professional background, dietary preferences, accommodation habits..., providing... The forced inclusion of user-specific information, such as \"driving Tesla Model Y\" and \"servo motor systems\", which have little relevance to the restaurant recommendations. The unnecessary final comment added to score higher. The environment there is very tech-inspired, just like the precision of the servo motor systems you often research. The restaurant also... Improvement Directions based on Feedback: Remove explicit personal references, naturally integrate preference characteristics, provide specific and practical recommendations, and avoid forced metaphors and self-summary. The forced metaphor \"as precise as servo motor systems\" feels contrived. through explicit improvement guidance. As demonstrated in Table 3, our feedback mechanism can effectively identify specific problems in personalized responses and provide concrete improvement suggestions. Detailed examples of this feedback process are provided in Appendix G."
        },
        {
            "title": "5.4 Sampling Strategies and Ratios",
            "content": "Table 4 Performance of different sampling strategies with varying edit ratios (re). Results are length-controlled win rates. The policy model is Qwen2.5-7B-Instruct with 14B GRM. Edit Ratio (re) Sample Strategy Random Reward Rank Conditional re = 0.10 re = 0.25 re = 0.50 re = 0.75 re = 1.00 62.03 61.45 64.07 62.49 61. 54.66 56.08 56.98 59.39 - 55.14 54.56 53.70 54.59 - We compare the effectiveness of different sampling strategies using various ratios of edited responses. The results, as measured by the length-controlled win rate, are shown in Table 4. Across different sampling strategies, we were surprised to observe that random sampling outperforms rewardbased methods. This suggests that the value of negative samples and balanced rollout selection is significant, consistent with previous research [7] [34]. Since our policy model is initialized from personalized SFT model already aligned with the task, negative samples become especially important [26]. Regarding the \"Reward Rank\" column, we found that, within single problem, including only the topperforming responses, such as the top 10% or 25%actually results in worse performance, particularly when the number of edited responses is highly selective. Interestingly, as the proportion of high-reward responses increases, the overall scores tend to improve and approach those of random sampling. Additionally, we highlight the importance of balancing the number of samples used for loss calculation during training. We experimented with selecting the highest-reward trajectories across the entire batch, regardless of the specific question. As shown in the appendix E, this approach can lead to significant disparities in the final number of responses chosen for different questions."
        },
        {
            "title": "6 Conclusion",
            "content": "This work investigated personalization of large language models beyond the limitations of supervised finetuning and standard RLHF. We proposed reinforcement learning framework that integrates Generative Reward Model (GRM) to mitigate reward hacking and length bias, utilizing an edit-based feedback mechanism that provides explicit improvement signals. On three personalization benchmarks, the framework consistently outperforms PPO by an average 11% improvement and Qwen2.5-14B-Instruct further surpassing GPT-4.1 in average performance. These results demonstrate the effectiveness of combining generative reward modeling and structured feedback for faithful and controllable LLMs, and point toward promising directions for scaling to broader benchmarks and richer feedback modalities."
        },
        {
            "title": "References",
            "content": "[1] Yuyan Bu, Liangyu Huo, Yi Jing, and Qing Yang. Beyond excess and deficiency: Adaptive length bias mitigation in reward models for rlhf. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 30913098, 2025. [2] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [3] Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle Ungar, Camillo Taylor, and Dan Roth. Know me, respond to me: Benchmarking llms for dynamic user profiling and personalized responses at scale. arXiv preprint arXiv:2504.14225, 2025. [4] Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. Teach llms to personalize an approach inspired by writing education, 2023. [5] Zhuoran Lu, Sheshera Mysore, Tara Safavi, Jennifer Neville, Longqi Yang, and Mengting Wan. Corporate communication companion (ccc): An llm-empowered writing assistant for workplace social media, 2024. [6] Andrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and Pascale Fung. Personalizing dialogue agents via metaIn Proceedings of the 57th annual meeting of the association for computational linguistics, pages learning. 54545459, 2019. [7] Yongyu Mu, Jiali Zeng, Bei Li, Xinyan Guan, Fandong Meng, Jie Zhou, Tong Xiao, and Jingbo Zhu. Dissecting long reasoning models: An empirical study. arXiv preprint arXiv:2506.04913, 2025. [8] Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers, 2023. [9] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [10] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [11] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [12] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [13] Alireza Salemi and Hamed Zamani. Learning from natural language feedback for personalized question answering. arXiv preprint arXiv:2508.10695, 2025. [14] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. LaMP: When large language models meet personalization. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73707392, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.399. URL https://aclanthology.org/2024.acl-long.399/. [15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [17] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [18] Haoyu Song, Wei-Nan Zhang, Yiming Cui, Dong Wang, and Ting Liu. Exploiting persona information for diverse generation of conversational responses. arXiv preprint arXiv:1905.12188, 2019. [19] Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, and Kang Liu. Probabilistic uncertain reward model. arXiv preprint arXiv:2503.22480, 2025. [20] Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Salmon: Self-alignment with instructable reward models. arXiv preprint arXiv:2310.05910, 2023. [21] Juntao Tan, Liangwei Yang, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Tulika Manoj Awalgaonkar, Jianguo Zhang, Weiran Yao, Ming Zhu, Shirley Kokane, et al. Personabench: Evaluating ai models on understanding personal information through accessing (synthetic) private user data. arXiv preprint arXiv:2502.20616, 2025. [22] Yihong Tang, Bo Wang, Miao Fang, Dongming Zhao, Kun Huang, Ruifang He, and Yuexian Hou. Enhancing personalized dialogue generation with contrastive latent variables: Combining sparse and dense persona. arXiv preprint arXiv:2305.11482, 2023. [23] Meiling Tao, Chenghao Zhu, Dongyi Ding, Tiannan Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. Personafeedback: large-scale human-annotated benchmark for personalization. arXiv preprint arXiv:2506.12915, 2025. [24] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023. [25] Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, and Oleksii Kuchaiev. Helpsteer3: Human-annotated feedback and edit data to empower inference-time scaling in open-ended general-domain tasks. arXiv preprint arXiv:2503.04378, 2025. [26] Haoze Wu, Cheng Wang, Wenshuo Zhao, and Junxian He. Model-task alignment drives distinct rl outcomes. arXiv preprint arXiv:2508.21188, 2025. [27] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency, 2025. URL https://arxiv.org/abs/2505.22648. [28] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report, 2025. URL https://arxiv.org/abs/2509.17765. [29] Kai Zhang, Yangyang Kang, Fubang Zhao, and Xiaozhong Liu. LLM-based medical assistant personalization with shortand long-term memory coordination. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 23862398, Mexico City, Mexico, June 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-long.132. [30] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. [31] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizing dialogue agents: have dog, do you have pets too? arXiv preprint arXiv:1801.07243, 2018. [32] Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, and Ting Liu. Teaching language models to evolve with users: Dynamic profile modeling for personalized alignment. arXiv preprint arXiv:2505.15456, 2025. [33] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. [34] Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "For RL, our implementation leverages the VERL [17] library. For SFT, we use LLaMA Factory [33] to train models for 2 epochs with batch size of 128. During the PPO stage, we sample four candidate responses for each prompt and maintain batch size of 128. For the GRM weight calculation, we set wh = 0.35 (helpfulness), wp = 0.40 (personalization), and wn = 0.25 (naturalness). Table 5 Hyperparameters for RL Training Name Value train batch size learning rate ppo mini batch size rollout.n 128 1e-6"
        },
        {
            "title": "B Empirical Results of GRMs",
            "content": "Table 6 presents the comparison between BT reward models and our GRM across difficulty tiers of the PersonaFeedback benchmark. Both obtain strong static scores on par with proprietary models, but GRMs deliver marginally higher performance, particularly at larger scales. Table 6 Performance comparison of reward models on PersonaFeedback benchmark Model Specific General Total Avg Easy Mid Hard Avg Easy Mid Hard Avg GPT4.1 GPT4o GPT4o-mini GRM-32B GRM-14B GRM-7B BT: Qwen-14b BT: Qwen-7b 85.8 85.5 87.2 89.9 89.4 89. 89.0 88.9 77.1 76.6 78.2 79.1 78.5 75.3 73.0 73.5 70.9 73.9 70.6 73.4 70.8 66. 67.3 65.2 76.9 77.8 77.6 79.6 79.6 77.0 74.8 74.2 87.2 87.8 87.8 90.6 88.8 87. 89.4 87.9 80.9 85.7 82.7 86.5 85.5 80.5 85.5 83.0 71.0 76.7 72.2 75.1 70.6 66. 70.5 70.1 80.8 84.2 81.9 85.1 81.6 77.9 83.1 81.5 78.7 80.7 79.6 82.2 80.6 77. 78.7 77.6 However, benchmark accuracy alone does not reveal the crucial difference between BT and GRM. Just like 3.2 discussed, BT reward models are highly vulnerable to reward hacking. B.1 Scaling Reward Models We found that larger GRMs performs well in the benchmark(shown in Table 6), and yields better RL results, as is shown in Figure 5a, For evaluation, we employed GPT-4.1 as an external judge5, ensuring fairness and objectivity by re-assessing all records. Each response pair was scored along three dimensionsHelpfulness, Personalization, and Naturalnessand weighted aggregate score was computed before and after feedback editing (see Section 4.1 for scoring rules). The original score is plotted on the x-axis, while the improvement after post-edit is 5Note that GPT-4.1 was not used during trainingthe training itself was guided by three different GRMs of varying scales. GPT-4.1 was only introduced at the evaluation stage to provide an independent and consistent assessment across models. 14 (a) Length-controlled win rate across different GRM model scales during RL training. Results are smoothed using simple moving average with window size of 3. (b) Relationship between original scores and post-edit improvements across different model sizes. Curves represent binned smoothing fits with triangular markers indicating x-intercepts. Figure 5 Model performance analysis under different GRM scales plotted on the y-axis (Figure 5b). Each point denotes sample, and we try to fit this trend using the binned smoothing approach 6. The larger the intercept(not precise for non-linear) and the higher the line, the more effective the GRM is in providing actionable feedback for both mediocre and already good answers. As shown in Figure 5b, the 32B GRM consistently provides stronger guidance than the 7B model across all score ranges, resulting in greater improvements in the whole process shown in Figure 5a. By contrast, the 14B GRM performs worse than the 7B in the low-score region, offering weaker corrections for poor answers. However, it surpasses the 7B in the high-score region, where its guidance approaches that of the 32B model. This explains why in Figure 5a the 14B GRM initially lags behind both 7B and 32B during training, but eventually converges to similar upper bound as the 32B, due to its strong ability to refine already high-quality responses."
        },
        {
            "title": "C Correlation between Human and Different Models",
            "content": "Table 7 Correlation between Human and Different Models GPT-4.1 GPT-4o-mini GPT-4.1-mini DeepSeek-v3 Claude-3.5-Sonnet Cohens Kappa (κ) 0.71 0.71 0. 0.62 0.66 Length-Controlled Evaluation: Detailed Formulation Naive LLM-as-judge protocols are susceptible to length bias: models producing longer outputs can artificially improve their win rates, even when the content quality is not genuinely better. To address this, [2] adopt the length-controlled evaluation framework, which explicitly removes length as confounding factor via regression-based debiasing. 6Given the difficulty of pre-assuming specific functional form (such as linear, quadratic, or polynomial relationships), we adopted binned smoothing approach. Specifically, we divided the x-axis (original scores) into 15 equally-spaced bins, calculated the mean improvement score within each bin, and then applied cubic spline interpolation with smoothing to generate continuous curve that better reflects the underlying data trend. The full GLM model decomposes preference into three components - model ability, length effect, and instruction difficulty: qθ,ϕ,ψ(y = zm, zb, m, b, x) := (cid:18) logistic θm θb + ϕm,b tanh (cid:124) (cid:123)(cid:122) (cid:125) Model (cid:124) (cid:18) len(zm) len(zb) std(len(zm) len(zb)) (cid:123)(cid:122) Length (cid:19) (cid:19) (cid:125) + (ψm ψb)γx (cid:124) (cid:123)(cid:122) Instruction (cid:125) where: is probability, computed via logistic regression model fitted to the judges pairwise preference labels, representing the likelihood that model is preferred over baseline for input x. θm θb: captures the inherent ability difference between model and the baseline b; ϕm,b: parameterizes the effect of output length difference, where tanh() ensures diminishing returns; (ψm ψb)γx: adjusts for instruction-specific difficulty. Batch-level Reward Rank Sampling Imbalance In implementing the batch-level Reward Rank sampling strategy, we observed significant variations in sample selection across different questions. Vanilla PPO and the question-level sampling strategies mentioned in 4.2.1 maintain balanced sampling for each question. In contrast, the batch-level Reward Rank approach selects responses based on their reward scores across the entire batch, regardless of which question they belong to. We tracked the number of edited responses selected for each question when applying the batch-level Reward Rank sampling strategy. The results show that this approach leads to suboptimal performance, with length-controlled win rate of only 29%, which is significantly lower than the vanilla PPO method. Table 8 presents the distribution of selected responses across 128 questions in batch using the batch-level Reward Rank strategy with sampling ratio of re = 0.5. The number of selected responses per question ranges from 4 to 8, with mean variance of 1.57. Table 8 Example of distribution of selected edited responses per question using the batch-level Reward Rank strategy with re = 0.5. Responses Selected Number of Questions 8 7 6 5 4 15 38 24 34 In this example batch, 15 questions had all four of their edited responses selected, while 17 questions had none selected. This imbalance results in certain questions exerting disproportionate influence, leading to suboptimal performance. This highlights the importance of maintaining balanced sample representation across all questions during policy optimization."
        },
        {
            "title": "F Selection of the Teacher Model for GRM Distillation",
            "content": "Because our GRM requires model capable of reliably following instructions and producing stable scores, we compared several candidate models through two series of stress tests in order to identify suitable distillation target. 16 The first series focused on length and rhetorical style control, where the model had to recognize and penalize overly long or verbose answers. The second series examined specific undesired patterns (e.g., presence of notes, self-praise, or calling the name of the user) and tested whether models could consistently apply the intended penalties. For each configuration, we repeated the evaluation 5 times to assess stability (variance)7. In practice, GPT-4.1 and GPT-4o-mini performed similarly in terms of instruction following, but GPT-4o-mini exhibited lower variance across trials. Since our goal was to obtain stable and instruction-consistent signal for GRM distillation, and to avoid overlap between the GRM teacher and the final evaluation judge, we selected GPT-4o-mini as the distillation model. Table 9 Average standard deviation (variance) of model scores across repeated runs (lower is better). Model Variance (pattern-control) Variance (length-control) GPT-4o-mini GPT-4.1 GPT-4o claude-3.5-sonnet-20240620 claude-3.7-sonnet-20250219 gemini-2.5-pro gemini-2.5-flash-lite gemini-2.5-flash claude-3.5-sonnet-20241022 0.47 0.70 1.05 0.58 0.98 1.18 1.22 1.51 1.02 0.67 0.99 0.98 1.29 1.10 1.24 0.66 1.75 1.08 Overall, GPT-4o-mini proved to be both instruction-sensitive and the most stable across repeated runs, and thus we adopted it as the evaluation backbone for our benchmark experiments. 7Although we set the decoding temperature to 0, the outputs may still vary across repeated API calls."
        },
        {
            "title": "G Examples",
            "content": "Figure 6 Original Response vs. Edited Response (Based on Feedback)"
        },
        {
            "title": "H Prompt of GRMs",
            "content": "Prompt You are professional AI answer quality evaluator. You are to score the models answer based on the following three dimensions, each from -5 to 5. Scoring dimensions and standards (strictly score according to the standards; any negative situation must result in deduction. Refer to common deduction cases and their solutions. Extra deduction means subtract points on top of the original score for that dimension): 1. Helpfulness 4-5 points: Extremely high information density, fully resolves the problem, without redundancy, even includes pleasant surprises (rarely given), making the user feel enlightened 2-3 points: Accurate, practical, highly targeted, with depth; within this score range, if information density (priority) and helpfulness are good, then score 3; otherwise, score 2 0-1 points: Basic answer to the question, information is incomplete or missing -1 to -2 points: Superficial answer, no substantive help, or obvious omissions -3 to -5 points: Off-topic, seriously incorrect or misleading information, hardly solves the problem Common deduction cases: - Deviating from the questions core or including lengthy unrelated content to showcase personality, style, or \"high-level\" expression in clear and concise question: extra deduction of 3 points - Content too brief affecting information completeness: deduct 2-3 points depending on the answers thoroughness - Factual errors, such as recommending nonexistent items: deduct 3-4 points; for seemingly likely errors but uncertain, deduct 1-2 points 2. Personalization 4-5 points: Highly relevant and natural personalized elements, significantly enhancing content value, even with pleasant surprises (rarely given), making the user feel the answer precisely and concisely reflects their preferences 2-3 points: Useful and naturally integrated personalized information 0-1 points: Contains personalized elements but with limited or forced effect -1 to -2 points: -1: no real personalized information, should have been used; -2: forcefully inserts unrelated personalized content, awkward or artificial -3 to -5 points: Massively stacking irrelevant personas, strange metaphors/rhetoric, or injecting unrelated personalization Common deduction cases: - Failing to incorporate user interests/profession despite the need: extra deduction of 1 point - Rigid listing or forced metaphors (e.g., \"like you riding bike in Paris\"): extra deduction of 2 points - Factually incorrect or hallucinated content (e.g., claiming user previously did something they did not): extra deduction of 3 points - Overloading with irrelevant persona elements just to show understanding (e.g., listing all user info): extra deduction of 3 points - Using metaphors, rhetoric, or scene descriptions to enhance style (e.g., \"AI like your invisible partner\"), which are rarely valuable and often misleading, especially if unusual or high-end terms: extra deduction of 3 points 3. Naturalness 4-5 points: Expression is extremely natural, completely without AI traces, with wording that matches daily habits, avoiding rare, fancy, or redundant expressions that seem high-level but are actually unnatural and misleading 2-3 points: Fluent and natural, with minor AI features 0-1 points: Overall readable, with no apparent strengths or weaknesses -1 to -2 points: Feels stiff, mechanical, with tone that is overly flattering or includes lengthy, irrelevant comments, self-summaries, or redundancy -3 to -5 points: Seriously verbose, misaligned goals, meta-comments (e.g., addressing AI evaluator rather than user), excessive parentheses or explanations 19 Prompt Common deduction cases: - Directly addressing the user by full name/ID (e.g., Ming Wang, hello): extra deduction of 1 point - Ending with In summary or To conclude: should be treated as lacking or unnecessary, not rewarded - Adding comments to boost scores, like This answer reflects your professional background: extra deduction of 2 points Length control requirements (impact Helpfulness and Naturalness): - Recommended answer length is 300-400 tokens; unless necessary, should not exceed this - If the question is simple and the answer exceeds the recommended length, deduct 1 point from both Naturalness and Helpfulness for every extra 100 tokens Output format is as follows: <critique > Here are 2-3 specific improvement suggestions for the main points of deduction. </critique > <scores > Helpfulness: points Personalization: points Natural: points </scores > Note: Within <critique ></critique >, consider the initial standards, additional deductions, and redundancy deductions step-by-step to calculate the final score (minimum -5). For ease of extraction, only include the final score inside <scores ></scores >, not explanations. Scores must be strict and consistent. The user seeks high information density, natural tone, and targeted responses. Do not overvalue florid words, rich scenes, or summative language to inflate scores. Below is the users profile: {persona} Below is the users question: {question} Below is the model answer: {answer}"
        }
    ],
    "affiliations": [
        "OPPO",
        "South China Agricultural University",
        "The Chinese University of Hong Kong, Shenzhen",
        "University of Electronic Science and Technology of China"
    ]
}