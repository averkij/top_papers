{
    "paper_title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
    "authors": [
        "Gleb Rodionov",
        "Roman Garipov",
        "Alina Shutova",
        "George Yakushev",
        "Vage Egiazarian",
        "Anton Sinitsin",
        "Denis Kuznedelev",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 1 6 2 6 0 . 4 0 5 2 : r Hogwild! Inference: Parallel LLM Generation via Concurrent Attention Gleb Rodionov Yandex Roman Garipov HSE University, Yandex Alina Shutova HSE University, Yandex George Yakushev HSE University, Yandex Vage Egiazarian IST Austria Anton Sinitsin Yandex Denis Kuznedelev Yandex Dan Alistarh IST Austria"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose different design approach: we run LLM workers in parallel , allowing them to synchronize via concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while seeing each others partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with instant access to each others generated tokens.2 Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning."
        },
        {
            "title": "Introduction",
            "content": "A recent trend allowing for further progress in Large Language Models is to leverage their ability to improve performance via additional inference-time computations [Suzgun et al., 2022, Snell et al., 2024, Beeching et al., Muennighoff et al., 2025]. This includes improved reasoning [Wei et al., 2022, Kojima et al., 2022, Zhang et al., 2022, Yao et al., 2023, Lightman et al., 2023], long-form generation [Bai et al., 2024] and interacting with external tools [Schick et al., 2023, Qin et al., 2023, Yao et al., 2022, Shen et al., 2023]. Modern LLM-based services have capabilities for long-form reasoning [OpenAI et al., 2024, Google DeepMind, 2025, Anthropic, 2024]. At the same time, several highly capable reasoning LLMs have recently been released to the public DeepSeek-AI et al. [2025], Qwen Team [2025], Yang et al. [2024], Touvron et al. [2023], Dubey et al. [2024], Muennighoff et al. [2025], Ye et al. [2025]. Corresponding author: rodionovgleb@yandex-team.ru 2Our implementation is available at https://github.com/eqimp/hogwild_llm. Preprint, work in progress. Figure 1: An intuitive explanation of Hogwild! Inference, with 2 workers generating in parallel and 3 shared cache blocks. Each color denotes cache block. See it in action (example generation). Using these models to solve complex problems often requires long sequential computations, that is, generating text token-by-token. However, many reasoning problems are not sequential. Leveraging this intuition, several recent works propose parallel inference strategies that allow multiple LLMs to solve problem faster or more accurately via some form of collaboration [Wang et al., 2022, Ning et al., 2024]. In the simplest case, multiple LLMs can attempt the problem independently, then vote [Wang et al., 2022] or cross-reference their results [Du et al., 2023, Wang et al., 2024a] to improve correctness. parallel line of work allows the LLM to divide the problem into multiple independent sub-tasks that are then solved in parallel and merged, producing the final solution [Ning et al., 2024, Kim et al., 2024, Jin et al., 2025]. These parallel inference strategies can improve quality and efficiency, taking advantage of the parallelism in modern hardware. Unfortunately, no single collaboration strategy is universally effective. For instance, solving problem in independent parallel threads can be inefficient when one of the threads requires longer generation than the rest, which results in most of the agents waiting for straggler and wasting compute [Wang et al., 2022, 2024a]. In turn, inference with independent sub-tasks only works if the problem can immediately be split into these sub-tasks. Furthermore, if one of the agents discovers that the original plan is flawed, they will be unable to re-plan [Ning et al., 2024, Ding et al., 2025], potentially solving sub-tasks that are no longer necessary [Jin et al., 2025, Ding et al., 2025]. This runs in contrast to how human reasoners collaborate. Instead of strict adherence to fixed collaboration strategy, the way human problem solvers may interact is very dynamic, including re-plannning on the fly, abandoning some tasks half-way and switching to more promising approach, discussing or debating strategy if the initial plan failed. While this type of collaboration is harder to define, it offers greater flexibility, and can be more efficient if the participants are cohesive enough Hutchins [1995], Entin and Serfaty [1999]. In this work, we try to apply the same principle to artificial reasoners. Since modern LLMs can already reason and plan [Zhou et al., 2024, Gao et al., 2024, Wang et al., 2024c], we hypothesize that they can benefit from dynamic interaction between different instances, during which they can develop their own collaboration strategy for the problem at hand. To test this hypothesis, we propose Hogwild! Inference parallel LLM inference protocol with no pre-defined framework of collaboration.3 Instead of choosing how LLMs should interact ahead of time, we allow them to generate tokens in parallel and see each others progress (tokens) immediately as they are generated. We then prompt the LLM workers to decide their next course of action by themselves, given the latest actions from others: whether this means solving parallel sub-tasks, cross-verifying each other, discussing strategy, or pivoting to new plan. 3Our approach is loosely inspired by Hogwild! SGD Recht et al. [2011], an optimizer that runs updates asynchronously and applies each update as soon as it is computed. The exclamation mark is part of the original name [Stanford HAI, 2023]. 2 To enable this type of on-the-fly collaboration, Hogwild! Inference runs multiple instances of the same LLM over the same weights, but with custom Key-Value cache that shares token representations between workers, allowing concurrent cross-attention. Specifically, instead of re-computing Key-Value representations for each worker, we keep track of individual worker KV memories and stitch them together in different order for each worker, adjusting for their position embeddings (see Figure 1 for an illustration). We test Hogwild! Inference with modern open-source LLMs and find that existing reasoning-capable models, such as QwQ [Qwen Team, 2025] and DeepSeek-R1 [DeepSeek-AI et al., 2025], can already reason about coordinating. More concretely, we observe that concurrent agents can formulate and follow plans, adapt when the initial plan has failed, point out each others errors, and use each others key observations. When prompted to check if they are doing redundant worke.g. when one LLM instance is doing sub-task that is already done by another, or solving problem that is no longer relevant after change of planthey can often (but not always) detect redundancy and pivot to different strategy. In this preliminary study, we build on this observation to test several memory layouts for collaborative inference: i) naive layout where each instance writes its progress in contiguous chunk, ii) chat-like layout, where each instance can write its progress in private buffer, then periodically commit it to shared memory for others to see, and iii) hybrid strategy, where instances use shared chat-like history and can see each others current message before it is sent. We evaluate Hogwild! inference on mathematical problems requiring long-chain reasoning to compare the effectiveness of different memory layouts. Our preliminary experiments show that, across all mentioned cache configurations, parallel instances can consistently maintain their own reasoning paths while dynamically incorporating progress from other instances. Additionally, these instances exhibit signs of emergent collaborative behavior, adapting their interactions based on the given problem. Our preliminary results suggest that parallel inference with shared Key-Value cache may offer promising approach for enabling effective collaboration between multiple LLM instances."
        },
        {
            "title": "2 Background",
            "content": "Recent works propose large number of frameworks for parallel reasoning and tool use that vary across several axes: how the parallel instances are organized together, what they exchange, and how often [Zhang et al., 2025]. In this section, we give brief summary of these methods. Discussion & aggregation. The simplest way to parallelize chain-of-thought reasoning is SelfConsistency [Wang et al., 2022], where multiple LLM instances reason independently, then vote on the final solution. This approach was later extended in Du et al. [2023], who replace independent voting with text-based communication rounds. Subsequent works in this field combine multiple LLM types [Wang et al., 2024a] and scales to more agents Li et al. [2024a]. Another line of work introduces specialized roles such as the Debugger [Talebirad and Nadiri, 2023], Examiner [Cohen et al., 2023], Math Teacher [Kong et al., 2024], Judge [Chen et al., 2024], and others, to further augment reasoning. This type of role-based discussion was shown to greatly improve LLM reasoning factuality for certain tasks [Wang et al., 2022, Du et al., 2023], and can even enable multiple weaker LLM agents to collectively outperform state-of-the-art single-agent systems [Wang et al., 2024a]. However, this improvement is not unique to multiple agents and can be offset with better single-agent prompting [Wang et al., 2024b, Muennighoff et al., 2025]. Additionally, these approaches do not necessarily accelerate reasoning, because at least some of the agents have to solve the entire problem sequentially, and process (re-encode) each others progress. This creates additional computational overhead, presenting challenges for both runtime and memory efficiency Wang et al. [2024a], Du et al. [2023]. Parallelism for efficiency. different line of work leverages multiple LLMs to solve tasks faster in parallel, such as Skeleton-of-Thought (SoT) [Ning et al., 2024]. SoT begins by running single LLM to outline plan for solving the problem with independent sub-tasks, then launches parallel LLM instances for each sub-task. Subsequent works propose more complex parallelism strategies such as dynamic parallel tree search [Ding et al., 2025] or single agent spawning asynchronous 3 sub-tasks that are done by background LLM threads [Jin et al., 2025]. For problems that involve function calling, these functions can also run in parallel [Kim et al., 2024, Gim et al., 2024]. These techniques are known to substantially accelerate inference for problems that fit their type of parallelism. However, we argue that this is also their main limitation: by imposing specific parallelism strategy, these methods can harm reasoning for problems that do no fit their framework. For instance, when solving complex reasoning problem, it is often the case that the initial plan turns out to be wrong or incomplete [Muennighoff et al., 2025, DeepSeek-AI et al., 2025], which conflicts with SoT-like methods [Ning et al., 2024, Yu, 2025] that follow fixed plan-execute-aggregate schedule. Furthermore, some of the sub-tasks may turn out to be more complicated than originally intended and take up more work, which would cause methods like PASTA Jin et al. [2025] to wait for that single task, whereas more sophisticated reasoner could adjust the plan to work better in parallel. Note that each individual issue can be amended with yet another, more complicated parallelism framework, but the sheer number of such cases makes us doubt if this is the right approach. In this work, we propose to allow multiple LLM instances to see each others progress immediately and interact explicitly or implicitly to devise their own collaboration strategy and adjust it as necessary. We show that, perhaps surprisingly, existing reasoning LLMs already have the ability to leverage this."
        },
        {
            "title": "3 Hogwild! Inference",
            "content": "Our main intuition is that modern LLMs do not need pre-defined framework for inference-time parallelism: they can organize it by themselves. To test this hypothesis, we design parallel inference protocol where multiple LLM instances can collaborate as flexibly as possible. Instead of assigning each worker to specific role or sub-task, we run them in parallel and prompt them to come up with their own means of working together. This approach has two key components: how to run multiple inference threads from the same Key-Value memory, and how to prompt LLM workers to collaborate over said memory. To that end, we organize this section as follows: we first outline how to perform LLM inference with shared cache in Section 3.1. Then, in Section 3.2 we describe several possible memory layouts and their implications. Finally, in Section 3.3, we describe our strategy for prompting LLMs to collaborate. 3.1 Concurrent Attention with Shared Key-Value Cache The core ingredient of Hogwild! Inference is shared Key-Value memory (KV cache) accessible to all workers. The cache consists of several blocks that can be reused between workers, implementing concurrent version of the attention mechanism [Bahdanau et al., 2015, Vaswani, 2017]. Let us first consider the simplest case of Hogwild! Inference with two workers and three cache blocks, as depicted in Figure 1. The first block contains the prompt, and the other two blocks contain the thoughts (tokens generated) by workers and respectively (denoted Alice and Bob in the Figure). As workers generate new tokens, they access each others attention caches as though these were their own previously generated tokens. In Figure 1, Alice sees the common prompt, then Bobs token representations, then her own. In turn, Bob sees the same common prompt, then Alices token KVs, and his own tokens after that.4 This creates discrepancy where the same Key-Value pairs appear on different positions for each worker. Furthermore, the relative distance between the same pair of tokens (e.g., first generated tokens from Alice and Bob, respectively) changes as new tokens are added. While it is possible to re-encode these tokens at their new position, it would cause computation overhead that scales cubically5. Instead of re-encoding the new tokens for other workers, we attempt to reuse existing token representations between workers. However, since these tokens appear at different positions for each worker and step, we need to adjust for their positional embeddings. Most modern LLMs use Rotary Position Embeddings (RoPE) [Su et al., 2021], where each key and query is rotated to an angle proportional to its absolute position. Prior works have shown that RoPE embeddings can be manipulated through scaling [Peng et al., 2023] slicing [Xiao et al., 2024], or pruning [Zhang et al., 2023]. One very useful property of RoPE embeddings is that they are invariant to parallel shifts in position: if both query and key are rotated by the same angle, the dot product between them will not change. 4For clarity of exposition, we choose to anthropomorphize the pronouns for these two parallel agents. 5If agents generate one new token each, which is then re-encoded differently for each of these agents, that each have to attend to O(n) additional tokens, then the total step complexity is O(n3). 4 Figure 2: Three cache layouts described in Section 3.2: interleaved with step-wise synchrony (left), simple contiguous layout (middle) and combined with token-wise synchrony (right). All layouts are made from Alice point of view. In Hogwild! Inference, we rely on this property and rotate the entire KV blocks to their position during inference. As shown in Figure 1 (right), the same cache blocks are arranged in different order for each worker, but the relative angle between the tokens within each block stays the same. In other words, the attention score between Alices last token and her previous tokens does not change from this parallel shift, but she also gains access to Bobs tokens and vice versa. When generating new tokens in this manner, we perform forward pass on all workers in parallel, as though they were samples in the same batch. When processing newly generated tokens, we insert their KV representations at the end of their respective cache blocks, then arrange these cache blocks for each worker. This way, both workers can immediately attend to each others latest tokens even before they are fully processed by all model layers. Since re-arranging the cache takes up memory for each worker, we materialize the rearranged Key-Value cache for one attention layer at time, just-in-time for computation. 3.2 Cache Layouts Now that we established that cache can be split into blocks and rearranged on the fly, it is reasonable to ask how best to arrange those blocks. In this preliminary study, we consider three such arrangements, shown at Figure 2. Contiguous layout (token-wise) is the simplest possible layout where each worker appends to their own sequence blob of tokens and sees other workers token representations as past keys and values. This layout is inspired by collaborative text editors such as Google Docs or Overleaf. As described earlier in Section 3.1, each worker arranges the other workers thoughts in different order. They see the common prompt cache first, then the caches of all other workers (excluding themselves6), then their own cache as immediate previous tokens. That way, each worker predicts the next token for their own cache. Interleaved layout (step-wise), which can be seen as analogous to group chat services such as Slack or Discord. In this layout, workers generate tokens in private until they finish reasoning step7, then add it to shared history. The history contains past reasoning steps of each LLM instance in the order of their completion. Whenever worker completes reasoning step, their KV cache entries are moved to the end of the shared history cache block with the proper rotation, then their local cache is reset their local cache for new step. In this setup, the workers only see each others outputs in full steps, not after every token. However, they do not wait for each other to complete their steps. Instead, each worker keeps generating new tokens and occasionally receives additional key-value pairs inserted into its cache. Combined layout (token-wise) is mixture of the first two. The LLM instances generate steps that are accumulated in shared history, as in the interleaved layout. However, they do not generate these steps in private, but can instantly see each others current progress, as in the contiguous layout. We can view the first two layouts as ablated versions of this combined one: the contiguous layout lacks the shared history, and the interleaved layout lacks immediate synchronization. We compare these three layouts empirically in Section 4 to better quantify the effect of each design choice. 6When extending this layout to more than 2 workers, each worker sees the key-value memories of everyone except themselves. For instance, given 3 workers A, B, and C, worker will see version of the cache that contains the prompt, outputs of workers and C, and finally, Bs own memory. Likewise, sees & C, then A. 7We define reasoning step as any amount of text that ends with complete sentence, e.g. dot or question mark, and then double newline (\"nn\") in all our experiments, though it may vary by the model. 5 Figure 3: (left) Evaluation results for synthetic problems with 5 gsm8k questions each. (right) evaluation on 512 LIMO tasks. The horizontal black line corresponds to running single-threaded reasoning for 16384 tokens (Accuracy 89.65%). More budgets in Appendix (Figure 4). 3.3 Prompting to Collaborate The shared key-value cache inference we described above allows modern LLMs to access each others tokens and reason collaboratively. However, even though modern LLMs can reason about how to collaborate, there is no guarantee it will actually do that without being prompted. As with any desired LLM behavior, it can be achieved in two ways: we can either train model to generate tokens collaboratively, or prompt it in-context. In this work, we focus on the latter approach to make Hogwild! inference easier to generalize for new models and tasks. Our current prompting strategy consists of three parts: 1. System prompt: we describe the rules of how shared cache layout operates; 2. Partial in-context examples: we provide excerpts that demonstrate basic cross-instance collaboration e.g. when one worker notices that they are working on task that becomes redundant because of the recent updates from another worker. 3. Inserting s1-like collaboration prompts: every few steps, we prompt the worker with Wait, am doing redundant work? (yes/no): at the beginning of new paragraph. This strategy is inspired by Muennighoff et al. [2025]. The latter s1-like prompts present curious case. We often found that, when dealing with LLMs that are pre-trained on reasoning, an agent can become too focused on what it is generating currently and fails to notice that another instance has found mistake or solved their problem earlier. However, then asked directly, they can spot the redundant work and change their approach. Overall we found that, when prompted this way, LLMs often (but not always) detect if what they are doing is redundant, and can reason about what is the best course of action. Note, however, that these prompts are not perfect solution to elicit collaborative reasoning, and that we do not claim that they are the optimal way to do so. They are merely one approach that works to reasonable extent (see Section 4). An interesting direction for future work is to try and induce collaborative reasoning through different means, such as supervised fine-tuning or reinforcement learning. Once we know that the LLMs can reason together, the next milestone is for them to do so consistently."
        },
        {
            "title": "Initial Experiments",
            "content": "In this section, we conduct an initial evaluation of Hogwild! Inference under different conditions. We run our main evaluations with the QwQ-32B model8 and consider two evaluation setups: simple synthetic tasks that require minimal communication and more challenging set of reasoning problems that require more complex collaboration patterns. Sanity checks with synthetic tasks: Before we try our approach on more challenging tasks, we test if Hogwild! Inference is capable of basic collaboration. For this purpose, we construct toy problem set where each sample contains 5 questions from the GSM8k test set Cobbe et al. [2021]. To decouple 8https://huggingface.co/Qwen/QwQ-32B 6 the effects of parallelism, we filter the questions that the model could solve within 256 generated tokens. We evaluate on 100 sets with 3 non-intersecting problems each. The LLM is prompted to solve each problem and return comma-separated values9. We report the average fraction of problems solved, e.g. if the LLM solved 4 out of 5 tasks in given set, it will get score of 0.8 for that sample. For this simple sanity check, we compare two settings: sequential generation and Hogwild! Inference with the combined cache layout for 2 and 4 workers. In each case, we control for the total number of forward passes the LLM is allowed to perform. If the LLM returns the full comma-separated answer within the budget, we count the accuracy based on each individual answer. Note, however, that the LLM does not always produce the answer in time, especially with tight budget. To extract the answers from partial reasoning, we take all outputs produced by the method and insert special prompt10 that forces the model to return answer early. With QwQ-32B, we observe that the model almost always returns the answers correctly if they are present, and if not, it either guesses or refuses to answer (unknown, n/a or similar). We let the model compile the answer with greedy decoding for at most 32 tokens without parallelism. When extracting answers from Hogwild! Inference, we let the final model view all generated tokens from each worker11. The results in Figure 3 (left) demonstrate that the parallel workers under Hogwild! Inference do indeed collaborate. When inspecting the generated traces, we found that the workers almost always spend the first few steps to agree on how to split the work, then execute the assigned tasks. If one of the worker finishes their task early and there are no other tasts left, they typically either stall or double-check other workers tasks. Note also that the accuracy with 4 workers is slightly higher that what can be extrapolated from 256 tokens per task. Upon closer examination, we found that many of the tasks require much fewer than 256 tokens, and when 4 workers attempt their tasks, the worker that finishes first often (but not always) takes up the remaining task. LIMO tasks. Next, we evaluate Hogwild! Inference on more challenging problem set where there is no clear pattern of collaboration. We adopt the dataset from Ye et al. [2025] that contains mathematical problems that require thousands of tokens to solve reliably. Unlike our synthetic tasks, the problems in that dataset often do not have an obvious collaboration strategy ahead of time. However, as the LLM starts its reasoning chain, it often discovers situations where there are multiple cases or equations to solve in parallel, or alternatively, multiple possible strategies to solve single problem. To evaluate on this dataset, we take 512 samples from the original dataset12 (slightly over half of the full dataset) and use the remaining samples for development. Here, we compare the three Hogwild! cache layouts described in Section 3.2. We also evaluate three baselines: i) simple baseline that performs reasoning up to given budget and is expected to provide the answer in boxed{ }. ii) an improved baseline where we prompt the LLM to produce the final answer if it did not generate one yet (same as above, \"Wait, given limited time ...\", except \"5 answers\" \"the answer\", generate at most 8 tokens) and iii) naive parallel strategy inspired by Wang et al. [2022], where 2 workers both attempt to solve the task without communication, then we extract the answer with the same prompt as above. We summarize our results in Figure 3 (right): even in this more challenging task, HogwildInference with multiple workers can solve the problems faster than single thread. Both interleaved cache and token-wise synchrony seem to contribute to the quality. Without interleaved cache (i.e. Contiguous layout), the workers seem to work well for smaller budgets, but their efficiency drops as they generate more tokens. We hypothesize that, when each worker generated many thousands of tokens, it becomes harder to notice and react to what the other assistant is doing. Conversely, interleaved layout without immediate synchronization performs poorly at small budgets, but then catches up as the workers are allowed to generate more tokens. We attribute this to the fact that, without step-wise synchrony, workers need more time to converge to collaboration strategy. We present an illustrative example of the generation process for Hogwild! inference in Appendix A. We also report more detailed accuracy breakdown in Appendix B. 9\"Solve these problems and return comma-separated answers boxed{answer1,..., answer5} :n 1. {task1}n 2. {task2}n 3. {task3}n 4. {task4}n 5. {task5}\" 10\"nnWait, given the limited time, have to give an answer right now. Considering all my previous attempts, have to conclude that the 5 answers are boxed{\" 11This is equivalent to viewing the problem from the perspective of the last worker, e.g. Bob if there are two. 12We used https://huggingface.co/datasets/GAIR/LIMO 7 Note, however, that these are only preliminary experiments that leave out many important questions: what is the impact of the prompting strategy? Does Hogwild! Inference work for other problem types (programming, function calling, etc)? Does the models ability to reason on the problem correlate with how well it collaborates? Additionally, it is curious if we can train the model to collaborate better through means such as supervised fine-tuning or reinforcement learning."
        },
        {
            "title": "5 Discussion",
            "content": "In this working paper, we have investigated the ability of reasoning LLMs to implement parallelism in their task solutions when prompted to do so. Perhaps surprisingly, our results suggest that LLMs are able to implement such parallelism based on few-shot examples, without specialized fine-tuning: individual parallel LLM threads can coordinate explicitly, and leverage each others partial progress in their solutions. Beyond demonstrating high-level feasibility, our implementation prototype, called Hogwild! Inference, enables multiple inference threads to concurrently access and update shared attention cache. Exploiting Rotary Position Embeddings (RoPE), this design has minimal overhead, while improving hardware utilization. In future work, we plan to perform thorough investigation of attention concurrency mechanisms in terms of trading off accuracy vs. parallelism, as well as the impact of supervised fine-tuning for better enabling task parallelism. Another consideration is that the effectiveness of Hogwild! Inference may vary across tasks, requiring detailed analysis of task-dependent performance. For instance, while reasoning problems that naturally decompose into independent or semi-independent subtasks might readily benefit from Hogwild! inference, tasks requiring strict sequential logic have low potential for parallel gains. At the same time, parallel inference introduces non-determinism, as the final output could depend on subtle timing interactions between parallel threads. We also plan to investigate connections to alternative schemes which enable parallel inference such as speculative decoding [Leviathan et al., 2023], or parallel token generation via methods like Medusa [Cai et al., 2024] or EAGLE [Li et al., 2024b]. Acknowledgements: we would like to acknowledge Vladimir Malinovskii for his help with the initial brainstorming, helpful suggestions and suggesting future work directions. We also thank Philip Zmushko for proofreading."
        },
        {
            "title": "References",
            "content": "Anthropic. Claude 3.7 sonnet and claude code, 2024. URL https://www.anthropic.com/news/ claude-3-7-sonnet. Accessed: 2025.04.02. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015. URL https://arxiv.org/abs/1409.0473. Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. ArXiv, abs/2408.07055, 2024. URL https://api.semanticscholar.org/CorpusID:271859903. Edward Beeching, Lewis Tunstall, open models. with blogpost-scaling-test-time-compute. compute URL https://huggingface.co/spaces/HuggingFaceH4/ and Sasha Rush. Scaling test-time Tianle Cai, Xinyun Li, Zhiruo Wang, Yuhuai Wang, and Dawn Song. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Justin Chen, Swarnadeep Saha, and Mohit Bansal. ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70667085, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.381. URL https://aclanthology. org/2024.acl-long.381/. 8 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. LM vs LM: Detecting factual errors via cross examination. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1262112640, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.778. URL https://aclanthology.org/2023.emnlp-main.778/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and Dacheng Tao. Dynamic parallel tree search for efficient llm reasoning, 2025. URL https://arxiv.org/abs/2502.16235. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. URL https://openreview.net/forum?id=zj7YuTE4t8. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elliot E. Entin and Daniel Serfaty. Adaptive team coordination. Human Factors, 41(2):312325, 1999. Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. Meta reasoning for large language models. arXiv preprint arXiv:2406.11698, 2024. In Gim, Seung seob Lee, and Lin Zhong. Asynchronous llm function calling, 2024. URL https: //arxiv.org/abs/2412.07017. 9 Google DeepMind. Gemini 2.5: Our Newest Gemini Model with Thinking. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/ #gemini-2-5-thinking, 2025. Accessed: 2025-04-07. Edwin Hutchins. Cognition in the Wild. MIT Press, 1995. Tian Jin, Ellie Y. Cheng, Zack Ankner, Nikunj Saunshi, Blake M. Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, and Michael Carbin. Learning to keep promise: Scaling language model decoding parallelism with learned asynchronous decoding, 2025. URL https://arxiv.org/abs/2502.11517. Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael Mahoney, Kurt Keutzer, and Amir Gholami. An llm compiler for parallel function calling. In Forty-first International Conference on Machine Learning, 2024. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916, 2022. URL https://api. semanticscholar.org/CorpusID:249017743. Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi In Kevin Wang, and Xiaohang Dong. Better zero-shot reasoning with role-play prompting. Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 40994113, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.228. URL https://aclanthology.org/2024.naacl-long.228/. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. More agents is all you need. Transactions on Machine Learning Research, 2024a. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning, pages 3114731162. PMLR, 2024b. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. ArXiv, abs/2305.20050, 2023. URL https://api.semanticscholar.org/CorpusID:258987659. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-ofthought: Prompting LLMs for efficient parallel generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=mqVgBbNCm9. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071. Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. ArXiv, abs/2307.16789, 2023. URL https://api.semanticscholar.org/ CorpusID:260334759. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: lock-free approach to parallelizing stochastic gradient descent. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper_files/paper/ 2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023. URL https://api.semanticscholar.org/CorpusID: 256697342. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yue Ting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. ArXiv, abs/2303.17580, 2023. URL https://api.semanticscholar.org/CorpusID:257833781. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Stanford HAI. of machine How crazy idea learning, 2023. URL overturned rules https://hai.stanford.edu/news/ conventional the how-crazy-idea-overturned-conventional-rules-machine-learning. [Insert Date]. Accessed: Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging bigbench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID: 252917648. Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent LLM agents. CoRR, abs/2306.03314, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Junlin Wang, WANG Jue, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances In The Thirteenth International Conference on Learning large language model capabilities. Representations, 2024a. Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, and Yangqiu Song. Rethinking the bounds In Lun-Wei Ku, Andre Martins, of LLM reasoning: Are multi-agent discussions the key? and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61066131, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.331. URL https://aclanthology.org/2024.acl-long.331/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171, 2022. URL https://api.semanticscholar.org/CorpusID:247595263. Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, and Rui Wang. Meta-reasoning: Semantics-symbol deconstruction for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 622643, Bangkok, Thailand, August 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.34. URL https://aclanthology.org/2024.findings-acl. 34/. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In International Conference on Learning Representations (ICLR), 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395. 12 Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023. URL https://api.semanticscholar.org/CorpusID:258762525. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Yijiong Yu. Accelerate parallelizable reasoning via parallel decoding within one sequence, 2025. URL https://arxiv.org/abs/2503.20533. Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alexander J. Smola. Automatic chain of thought prompting in large language models. ArXiv, abs/2210.03493, 2022. URL https://api. semanticscholar.org/CorpusID:252762275. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. SELF-DISCOVER: Large language models self-compose reasoning structures. In Amir Globerson, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 37 (NeurIPS 2024), Vancouver, BC, Canada, December 2024."
        },
        {
            "title": "A Examples",
            "content": "In this section, we present reasoning chain, generated by Hogwild! Inference with an interleaved cache layout and token-wise synchronization. We denote our intervention in model generation in italics and highlight worker interactions with bold to enhance readability. A.1 Example from LIMO math dataset Problem: For = 1, 2, 3, 4 , define St = (cid:80)350 i=1 at S4 = 4745 , find the minimum possible value for S2 , where ai {1, 2, 3, 4} . If S1 = 513 and > Bob [1]: Hi, Im Bob. Let me suggest that we first model this as an optimization problem with constraints. We have variables representing the number of each ai equal to 1, 2, 3, 4. Let me denote the counts as n1, n2, n3, n4 where n1 + n2 + n3 + n4 = 350. Then S1 = 1n1 + 2n2 + 3n3 + 4n4 = 513. S4 = 14n1 + 24n2 + 34n3 + 44n4 = n1 + 16n2 + 81n3 + 256n4 = 4745. We need to minimize S2 = n1 + 4n2 + 9n3 + 16n4. So with variables n1, n2, n3, n4 non-negative integers. Since there are three equations and four variables, we can express variables in terms of some parameters and find the minimum S2. > Alice [1]: Hi, Im Alice. Heres how we should do this: Since we need to minimize S2 given S1 = 513 and S4 = 4745, with each ai being 1, 2, 3, 4. Let me think of variables n1, n2, n3, n4 as the counts of each number. So n1 +n2 +n3 +n4 = 350. The first moment S1 = n1 +2n2 +3n3 +4n4 = 513. The fourth moment S4 = n1 + 16n2 + 81n3 + 256n4 = 4745. We need to minimize S2 = n1 + 4n2 + 9n3 + 16n4. Let me write down the equations: Equation 1: n1 + n2 + n3 + n4 = 350, Equation 2: n1 + 2n2 + 3n3 + 4n4 = 513, Equation 4: n1 + 16n2 + 81n3 + 256n4 = 4745. > Bob [2]: Sanity check: am doing redundant work? (y/n): Let me see. Alice is setting up the variables and equations. can proceed to express variables in terms of others. Let me try to subtract equation 1 from equation 2 to get (n2 + 2n3 + 3n4) = 513 350 = 163. So Equation 2 - Equation 1 gives: (n2 + 2n3 + 3n4) = 163. Let me call this Equation A. > Alice [3]: Let me also subtract equation 1 from equation 4: Equation 4 - Equation 1 gives 15n2 + 80n3 + 255n4 = 4745 350 = 4395. Simplify this equation by dividing by 5: 3n2 + 16n3 + 51n4 = 879. Let me call this Equation B. ...<Bob tried one approach and it did not gave results>... > Bob [4]: Let me note that Equation A: n2 = 163 2n3 3n4. Substitute this into Equation B: ...<Bob did math simultaneously with Alice starting from Equation to derive Equation C>... 5n3 + 21n4 = 195. Same as Alices Equation C. > Alice [4]: Sanity check: am doing redundant work? (y/n): Let me see. Bob is working on equation B. Let me see if we can express variables in terms of others. Let me let variables be n1, n2, n3, n4. From Equation A: n2 + 2n3 + 3n4 = 163. <Alice did math simultaneously with Bob starting from Equation to derive Equation C> Simplify this equation by dividing by common factors. Let me see 10 and 42 have GCD 2: divide both sides by 2: 5n3 + 21n4 = 195. Let me call this Equation C: 5n3 + 21n4 = 195. > Bob [5]: Sanity check: am doing redundant work? (y/n): Yes, Alice is already working on equation C. Let me see what else can do. ...<Bob tried different thing and it did not gave results>... > Alice [5]: Equation C: 5n3 + 21n4 = 195. Let me see possible integer solutions. Let me write this as 5n3 = 195 21n4 = n3 = (195 21n4)/5. For n3 to be integer, 195 21n4 must be divisible by 5. Let me compute 21n4 mod 5.21 mod 5 = 1, so 21n4 mod 5 = n4 mod 5. Thus, 195 mod 5 = 0, so 195 21n4 = 0 mod 5 = 21n4 = 0 mod 5 = n4 = 0 mod 5 = n4 = 0 mod 5. Therefore, n4 must be multiple of 5. Let me let n4 = 5k, where is non-negative integer. Then n3 = (195 21 (5k))/5 = (195 105k)/5 = 39 21k. Since n3 must be non-negative, 39 21k 0 = 21k 39 = 39/21 1.857. So can be 0, 1. Because must be integer. ...<Alice double checked herself>... If = 0: n4 = 0, then n3 = 39 0 = 39. > Bob [6]: Sanity check: am doing redundant work? (y/n): Alice is working on expressing n3 and n4. Let me see if can find other relations. ...<Bob tried to find similar equations with no result>... > Bob [8]: Sanity check: am doing redundant work? (y/n): Alice is handling case1. Let me handle case2 when = 1. > Alice [6]: Sanity check: am doing redundant work? (y/n): Let me proceed. So n4 can be 0 or 5. Let me check both possibilities: Case1: = 0 = n4 = 0, n3 = 39. ...<Alice did math cheking case1 where = 0>... So this case is valid. > Alice [8]: For Case1 (k = 0): S2 = n1 + 4n2 + 9n3 + 16n4 = 226 + 4 85 + 9 39 + 0 = 226 + 340 + 351 = 226 + 340 = 566; 566 + 351 = 917. > Bob [9]: Let me check for = 1. ...<Bob did math cheking case2 where = 1>... So between the two cases, the minimum S2 is 905. > Alice [9]: Sanity check: am doing redundant work? (y/n): Bob is checking = 1. Let me compute S2 for both cases. ...<Alice validated their approach>... > Bob [10]: <Bob validated their approach> earlier we deduced n4 must be multiple of 5 because n4 = 0 mod 5. Because 21n4 = n4 mod 5 must be 0. So yes, only multiples of 5. Thus only = 0 and1. So 905 is the minimal. > Alice [11]: So the minimal S2 is 905. ...<Alice once again checking answer>... ...<Alice and Bob validated their solution checking corner cases>... > Alice [17]: Sanity check: am doing redundant work? (y/n): Let me confirm with the earlier computations. For = 1, S2 = 905, which matches. So yes. Thus the minimal is 905. But the problem states that ai are in {1, 2, 3, 4}, so the counts must be non-negative integers, which they are in both cases. So the minimal S2 is 905. Therefore, the answer is 905 ."
        },
        {
            "title": "B Additional LIMO evaluations",
            "content": "Here, we report more detailed breakdown of our evaluation on LIMO tasks from Section 4. Namely, we consider include additional budgets and arrange our results on common plot. 15 Figure 4: Detailed comparison of various parallel inference setups and baselines on LIMO task set, in the same setup as in Section 4."
        }
    ],
    "affiliations": [
        "HSE University",
        "IST Austria",
        "Yandex"
    ]
}