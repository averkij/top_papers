{
    "paper_title": "Stronger Normalization-Free Transformers",
    "authors": [
        "Mingzhi Chen",
        "Taiming Lu",
        "Jiachen Zhu",
        "Mingjie Sun",
        "Zhuang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\\mathrm{Derf}(x) = \\mathrm{erf}(Î±x + s)$, where $\\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures."
        },
        {
            "title": "Start",
            "content": "Stronger Normalization-Free Transformers Mingzhi Chen1 Taiming Lu1 Jiachen Zhu2 Mingjie Sun3 Zhuang Liu1 1Princeton University 2NYU 3Carnegie Mellon University 5 2 0 D 1 1 ] . [ 1 8 3 9 0 1 . 2 1 5 2 : r a) We search point-wise functions of different shapes as norm layer replacement. LayerNorm: ğ‘¥ğœ‡ ğœ2+ğœ– DyT: tanh(ğ›¼ğ‘¥) Derf: erf(ğ›¼ğ‘¥ + ğ‘ ) method ViT acc () DiT FID () DNA acc () LN DyT Derf 82.3% 82.5% 82.8% 45.91 45.66 43. 86.9% 86.9% 87.3% b) Formulation of LayerNorm (LN), DyT, and Derf (ours). c) Performance across domains. Figure 1 We introduce Dynamic erf (Derf), point-wise function, that outperforms normalization layers and other point-wise functions. (a) We identify the feasible function shape for replacing the normalization layer and propose large set of point-wise functions within this space. Evaluating all candidates, we identify and introduce Derf as the strongest choice. (b) LayerNorm, DyT (Zhu et al., 2025), and Derf operate in fundamentally different ways: with channels ğ¶ and tokens ğ‘‡ , LayerNorm normalizes each channel across the token axis, whereas DyT and Derf apply independent scalar mappings to each element. (c) Across ImagenNet-1K classification and generation, and DNA sequence modeling, Derf consistently outperforms LayerNorm and DyT. Derf demonstrates that point-wise function can not only replace normalization but also surpass it."
        },
        {
            "title": "Abstract",
            "content": "Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) (Zhu et al., 2025) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct large-scale search for more effective function design. Through this exploration, we introduce Derf(ğ‘¥) = erf(ğ›¼ğ‘¥ + ğ‘ ), where erf(ğ‘¥) is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across wide range of domains, including visual recognition and generation, speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf practical choice for normalization-free Transformer architectures. Our code is available at this link."
        },
        {
            "title": "1 Introduction",
            "content": "Normalization layers have become critical component in modern deep neural networks. Since the invention of Batch Normalization (Ioffe and Szegedy, 2015), more and more variants have been developed to adapt normalization to various architectures and model modalities (Ba et al., 2016; Salimans and Kingma, 2016; Ulyanov et al., 2016; Wu and He, 2018; Zhang and Sennrich, 2019). By regulating the distribution of intermediate activations, normalization layers have long demonstrated their strong capability in stabilizing training and accelerating model convergence (Santurkar et al., 2018; Bjorck et al., 2018). Due to the inherent formulation of normalization layers, they heavily rely on activation statistics during training. This introduces additional memory access and synchronization overhead (Zhang and Sennrich, 2019; Chen et al., 2020; Yang et al., 2022). Moreover, some normalization methods are highly sensitive to batch size, and inappropriate batch settings can lead to unstable training (Wu and He, 2018; Lian and Liu, 2019; Singh and Krishnan, 2020). These issues motivate recent efforts to develop normalization-free methods. Among these attempts, Dynamic Tanh (Zhu et al., 2025), an S-shaped point-wise function, has emerged as simple yet effective drop-in replacement for normalization layers. This work has established the foundation for point-wise functions that match the performance of normalization layers, yet functions that can surpass them remain unexplored. In this work, we aim to discover point-wise functions that outperform normalization layers to push toward stronger Transformer architectures (Vaswani et al., 2017; Dosovitskiy, 2021). We first systematically study how the intrinsic properties of point-wise functions affect the training dynamics and final performance. Specifically, we focus on four fundamental and representative function properties: zero-centeredness, boundedness, center sensitivity, and monotonicity. Each property is independently examined through controlled experiments on diverse set of point-wise functions to assess its impact on the training result. This analysis isolates subset of point-wise functions as effective normalization replacements and yields concrete design principle for normalization-free Transformers. Guided by these principles, we identify set of promising point-wise functions that have the potential to surpass the performance of normalization layers. Within this set, we empirically search for the optimal designs, among which Dynamic erf (Derf) emerges as simple yet the most performant function (Figure 1a). Derf augments erf(ğ‘¥) with learnable parameters, where the error function erf(ğ‘¥) is an S-shaped, rescaled cumulative distribution of standard Gaussian around zero. We evaluate Derf spanning multiple modalities (vision, language, speech, and DNA sequences); covering various tasks (classification, generation, and sequence modeling), under different training paradigms (supervised and self-supervised). Across all these settings, Derf consistently surpasses LayerNorm, RMSNorm, and Dynamic Tanh (Figure 1b). To pinpoint the source of these gains, we measure the training loss in evaluation mode after optimization. Derf exhibits higher training loss than normalization-based models, indicating that its superior performance stems from stronger generalization rather than enhanced fitting capacity. Overall, our work demonstrates that well-designed point-wise functions can outperform normalization layers."
        },
        {
            "title": "2 Background",
            "content": "Normalization layers. Normalization layers have become pivotal components of modern neural networks. Among the various normalization techniques, Batch Normalization (BN) (Ioffe and Szegedy, 2015), Layer Normalization (LN) (Ba et al., 2016), and Root Mean Square Normalization (RMSNorm) (Zhang and Sennrich, 2019) are the three most widely used in deep learning models. ğ‘¦ = ğ›¾ * ğ‘¥ ğœ‡ ğœ2 + ğœ– + ğ›½ (1) All normalization methods adhere to unified paradigm, formalized in Equation 1, where activations within each group are centered and scaled by their mean ğœ‡ and standard deviation ğœ (with ğœ– for numerical stability) to maintain consistent scale and stable gradient flow. The main distinction among different normalization methods lies in how the activations are grouped when computing ğœ‡ and ğœ. For example, LN computes the statistics along the channel dimension for each token independently. Given token representation ğ‘¥ Rğ¶, the mean and variance are computed as Equation 2, where ğ¶ denotes the number of hidden features (channels). 2 Figure 2 Key properties of point-wise function. The four properties: zero-centeredness, boundedness, center sensitivity, and monotonicity collectively characterize functional behavior on activations and influence training dynamics. Blue curves represent functions that satisfy each property, while red curves violate them. Due to its per-token normalization, LN is particularly well-suited for Transformer architectures, where activations across tokens exhibit diverse statistics. ğœ‡ = 1 ğ¶ ğ¶ ğ‘˜=1 ğ‘¥ğ‘˜, ğœ2 = 1 ğ¶ ğ¶ ğ‘˜=1 (ğ‘¥ğ‘˜ ğœ‡)2, (2) Point-wise functions. The strong reliance of normalization layers on activation statistics has motivated further exploration of statistics-free methods (He and Hofmann, 2024; Heimersheim, 2024; Jha and Reagen, 2024; Zhu et al., 2025). Among these approaches, point-wise functions (Zhu et al., 2025) have emerged as simple yet effective alternatives to traditional normalization methods. Unlike normalization, point-wise function applies the same parametric mapping ğ‘“ (ğ‘¥; ğœƒ) to each activation independently. The parameters ğœƒ are fixed or learned, rather than being computed from batch-, token-, or channel-level statistics. recent study (Zhu et al., 2025) introduces the Dynamic Tanh (DyT) function (Equation 3), where ğ›¼ is learnable parameter. This design is motivated by the observation that Layer Normalization often produces an S-shaped input-output mapping in practice. The saturating nature of the tanh function squashes extreme activations, thereby fulfilling role analogous to the re-centering and re-scaling effects of normalization layers. DyT(ğ‘¥) = ğ›¾ * tanh(ğ›¼ğ‘¥) + ğ›½ (3) While DyT has shown similar empirical performance to normalization layers across various Transformer architectures, comprehensive analysis of the design space for these statistics-free operators remains missing. In this work, we target at the optimal form of the point-wise function as normalization replacement. We identify the function properties crucial for convergence and performance, and then we introduce Derf, point-wise function consistently surpassing normalization layers rather than merely matching their performance."
        },
        {
            "title": "3 Function Property Analysis",
            "content": "Training Transformers without normalization requires understanding the factors that make point-wise function stable and effective as replacement. In this section, we examine four essential properties: zerocenteredness, boundedness, center sensitivity, and monotonicity (see Figure 2). These properties collectively characterize the fundamental shape of point-wise functions and their behavior on activations. By isolating the impact of each property, we explore its influence on optimization and final performance. To investigate these properties, we replace each normalization layer with point-wise function of the form: ğ‘¦ = ğ›¾ ğ‘“ (ğ›¼ğ‘¥) + ğ›½, (4) where ğ‘“ () denotes the chosen base function with learnable ğ›¼ rescaling the input. ğ›¾ and ğ›½ are affine parameters, similar to those in normalization layers. We begin with three base functions: tanh(ğ‘¥), erf(ğ‘¥), and arctan(ğ‘¥). In subsequent experiments, we modify these functions with controlled transformations to examine the impact of each property. All experiments are conducted with ViT-Base (Dosovitskiy, 2021), and top-1 accuracy on ImageNet-1K (Deng et al., 2009) is reported. In Appendix A, we provide more detailed training results."
        },
        {
            "title": "3.1 Zero-centeredness",
            "content": "Zero-centeredness means that the functions outputs are balanced around zero, with positive and negative values of similar magnitude and symmetry. Because normalization layers inherently recenter activations to the origin for stabilizing gradients, maintaining this property could reduce internal covariate shifts and promote smoother gradient flow during training. Setup. Under the ViT setup, we manipulate the centering of the functions. For each base function, we consider two types of shifts: horizontal and vertical, defined in Equation 5. In this form, ğœ†horiz and ğœ†vert respectively denote the magnitudes of horizontal and vertical shifts. For both types of shifts, we vary ğœ† over { 1 2 , 1, 2} to examine how increasing deviation from zero-centeredness affects the functions behavior. All other training settings remain unchanged. ğ‘“horiz(ğ‘¥) = ğ‘“ (ğ‘¥ + ğœ†horiz), ğ‘“vert(ğ‘¥) = ğ‘“ (ğ‘¥) + ğœ†vert, (5) Results. As shown in Table 1, the results are consistent across different base functions: for horizontal shifts, performance remains largely comparable to the zero-centered base function when ğœ†horiz 0.5. However, as ğœ†horiz increases, performance gradually degrades, and training diverges when ğœ†horiz 2. Similarly, vertical shifts consistently lead to decline in performance as ğœ†vert grows with training failure once ğœ†vert 2. These results show that zero-centeredness is requirement for stable convergence and effective training. function shift type erf(ğ‘¥) tanh(ğ‘¥) arctan(ğ‘¥) horizontal vertical horizontal vertical horizontal vertical -2 -1 -0.5 -0.1 ğœ† = 0 +0.1 +0. +1 +2 82.0% 81.8% 82.1% 81.5% 81.9% 81.4% 82.5% 82.3% 82.5% 81.9% 82.3% 81.9% 82.6% 82.4% 82.6% 82.4% 82.3% 82.2% 82.6% 82.6% 82.5% 82.5% 82.7% 82.5% 82.3% 82.5% 82.6% 82.4% 81.9% 82.3% 82.3% 82.4% 82.2% 82.3% 82.3% 82.0% 82.1% 81.6% 82.2% 81.4% 82.0% 81.2% Table 1 Results of zero-centeredness on ViT-Base. Horizontal shift corresponds to modifying the input as ğ‘“ (ğ›¼ğ‘¥ ğœ†), while vertical shift adds or subtracts constant to the output as ğ‘“ (ğ›¼ğ‘¥) ğœ†. indicates training failure."
        },
        {
            "title": "3.2 Boundedness",
            "content": "Boundedness refers to the property of function whose output is constrained within finite range. Formally, function ğ‘“ () is bounded if there exist constants ğ‘, ğ‘ such that ğ‘ ğ‘“ (ğ‘¥) ğ‘ for all ğ‘¥ in its domain. This ensures that activations remain finite and do not accumulate variance across layers. Unbounded functions, in contrast, may induce signal explosion and gradient instability. Setup. Under the same ViT setup, we study the role of boundedness with two methods. Firstly, we select three inherently unbounded S-shaped functions (e.g., arcsinh(ğ‘¥)) and compare them with their clamped versions shown in Equation 6, where ğ‘“ğ‘¢(ğ‘¥) denotes the unbounded point-wise function, and ğœ† is chosen value specifying the clipping range. ğ‘¦ = clip(ğ‘“ğ‘¢(ğ‘¥), ğœ†ğ‘¢, ğœ†ğ‘¢), (6) Secondly, we gradually transition bounded functions (e.g., erf(ğ‘¥)) toward unbounded linear form, defined in Equation 7, where ğ‘“ğ‘ denotes bounded point-wise function, and ğœ† controls how quickly the function becomes unbounded. We vary ğœ†ğ‘¢ over {0.5, 0.8, 1.0, 2.0, 3.0, 5.0} in the first method and ğœ†ğ‘ over {0.01, 0.1, 0.5} for the second. The original unmodified function is also included as baseline. ğ‘¦ = (1 ğœ†)ğ‘“ğ‘(ğ‘¥) + ğœ†ğ‘ğ‘¥, ğœ†ğ‘ (0, 1). (7) Results. For the first method, among the three unbounded functions in Table 2, only arcsinh(ğ‘¥) and logsign(ğ‘¥) converge effectively, while linear(ğ‘¥) does not. For the convergent functions, their clipped versions consistently 4 ğœ†ğ‘¢ 0.5 0.8 1.0 2.0 3.0 5. arcsinh(ğ‘¥) logsign(ğ‘¥) linear(ğ‘¥) 82.2% 82.3% 82.3% 82.4% 82.4% 82.4% 82.3% 82.2% 82.4% 82.4% 82.4% 82.4% 82.3% 82.3% 82.1% 82.2% 82.2% 82.1% 82.1% 82.0% Figure 3 Visualization of several unbounded point-wise functions on the positive half-axis, illustrating their different growth rates. ğ‘ğ‘Ÿğ‘ğ‘ ğ‘–ğ‘›â„(ğ‘¥) refers to its standard analytical form. The remaining functions are defined 2 3 , logsign(ğ‘¥) = sign(ğ‘¥) ln(ğ‘¥ + 1), as linear(ğ‘¥) = ğ‘¥, power23(ğ‘¥) = ğ‘¥ 1+ğ‘¥ , and logquad(ğ‘¥) = sign(ğ‘¥) ln(ğ‘¥2+1). Among them, smoothsign(ğ‘¥) = ğ‘¥ logquad(ğ‘¥) shows the fastest growth that still ensures stable convergence. Table 2 Results of clamping for boundedness on ViT-Base. Clipped version of unbounded functions consistently achieves better performance than unbounded baselines. denotes the original unmodified function. indicates training failure. outperform the unbounded baselines across all tested ğœ† values. These results indicate that incorporating boundedness can improve optimization and result in better performance. For the second, as shown in Table 3, the results are consistent with clipping the intrinsic unbounded functions: the unbounded variant yields slightly lower accuracy than the bounded baseline. ğœ†ğ‘ 0.01 0.1 0.5 erf(ğ‘¥) tanh(ğ‘¥) arctan(ğ‘¥) isru(ğ‘¥) 82.6% 82.4% 82.3% 82.5% 82.4% 82.3% 82.4% 82.1% 82.1% 82.3% 82.2% 82.1% Table 3 Results of removing boundedness on ViT-Base. Performance decreases as the function is less bounded. denotes the original function without modification and donotes training failure. Limitation of growth rate. From Table 2 and Table 3, we observe that there is an upper limit on their acceptable growth rate. Large growth rates often lead to training failure. To determine this limit, we evaluate family of inherently unbounded functions with varying growth rates, as illustrated in Figure 3. Among them, logquad(ğ‘¥) exhibits the fastest growth that still allows training convergence (see Table 4). Functions with faster growth, such as linear(ğ‘¥) and power23(ğ‘¥), tend to cause optimization divergence in the early stages of training. This failure occurs because rapidly growing functions fail to suppress variance effectively, leading to large gradient norms at the start of optimization. logsign(ğ‘¥) arcsinh(ğ‘¥) logquad(ğ‘¥) power23(ğ‘¥) linear(ğ‘¥) 82.2% 82.2% 82.1% Table 4 Results of unbounded functions with different growth rates on ViT-Base. Point-wise functions have growth rate upper bound, with logquad(ğ‘¥) being the fastest function that still converges. indicates training failure."
        },
        {
            "title": "3.3 Center Sensitivity",
            "content": "We use center sensitivity to characterize how quickly point-wise function becomes responsive to input variations around zero. Without center sensitivity, function is locally flat around the origin, returning zero or near-zero over finite interval. The region around zero is particularly important, as most activations tend to concentrate near the origin during training. Consequently, the responsiveness of function in this area directly influences how effectively small signals can propagate through the network. 5 Setup. Since center sensitivity is difficult to isolate independently, we approximate it using controllable near-zero inactive region. Under the same ViT setup, we modify each base function to incorporate symmetric flat region around the origin with sensitivity scale ğœ† > 0 to control the extent of this region. Specifically, for inputs in the range ğ‘¥ [ğœ†, ğœ†], we enforce ğ‘“ (ğ‘¥) = 0 and smoothly shift the positive and negative parts outward for ğ‘¥ > ğœ† to ensure continuity at the boundaries. smaller ğœ† results in narrower flat region and higher sensitivity near zero, while larger ğœ† leads to lower sensitivity. We vary ğœ† over {0.1, 0.5, 1.0, 2.0, 3.0} across three base functions. Results. As shown in Table 5, the best performance is achieved at ğœ† = 0. As ğœ† increases, the performance consistently degrades. This trend is not very clear when ğœ† 0.5, but once ğœ† exceeds 1.0, the degradation becomes much more obvious. Finally, when ğœ† 3.0, the training process diverges at an early stage. function ğœ† = 0.1 0.5 1.0 2.0 3.0 erf(ğ‘¥) tanh(ğ‘¥) arctan(ğ‘¥) 82.6% 82.5% 82.5% 82.1% 81.3% 82.5% 82.5% 82.4% 82.1% 81.1% 82.3% 82.3% 82.1% 81.8% 80.9% Table 5 Results of center sensitivity (ğœ†) on ViT-Base. indicates training failure. The best performance is achieved when no flat region is given, showing the importance of center sensitivity."
        },
        {
            "title": "3.4 Monotonicity",
            "content": "Monotonicity ensures functions output consistently increases (or decreases) as the input increases, preserving the relative order of inputs throughout the transformation. Non-monotonic functions may disrupt the relative ordering of activations. Furthermore, since non-monotonic function necessarily has regions where its derivative changes sign, it may also produce flipped gradient signals during training. Setup. Each base function selected can serve as the monotonically increasing case, while its negated counterpart is defined as ğ‘“neg(ğ‘¥) = ğ‘“ (ğ‘¥), representing the monotonically decreasing variant. As nonmonotonic comparisons, we include hump-shaped functions and oscillatory functions (see Figure 4) to examine how violations of monotonicity influence the training performance. To control potential confounding factors, we rescale each function so that its output range matches that of the monotonic functions. After rescaling, all functions are aligned in terms of zero-centeredness, boundedness, and center sensitivity. function ğ‘“ (ğ‘¥) ğ‘“neg(ğ‘¥) function ğ‘“ (ğ‘¥) erf(ğ‘¥) tanh(ğ‘¥) 82.6% 82.5% arctan(ğ‘¥) 82.3% 82.5% 82.5% 82.2% sin(ğ‘¥) dampx(ğ‘¥) 81.6% 80.7% dampexp(ğ‘¥) 81.2% (a) Monotonic (b) Non-monotonic Figure 4 Visualization of point-wise functions with different monotonicity behaviors. erf(ğ‘¥) and sin(ğ‘¥) refer to their standard form. The remaining functions are defined as negerf(ğ‘¥) = erf(ğ‘¥), dampx(ğ‘¥) = 2ğ‘¥ 1+ğ‘¥2 , dampexp(ğ‘¥) = 2.72ğ‘¥ ğ‘’ğ‘¥ Table 6 Results of monotonicity on ViT-Base. Monotonic functions consistently achieve better performance than their negated versions and other non-monotonic functions, whether humpshaped or oscillatory. This identifies monotonicity as key property for effective learning. Results. As shown in Table 6, both increasing and decreasing monotonic functions train stably and achieve high accuracy. In contrast, non-monotonic functions, whether hump-shaped or oscillatory, consistently perform worse than monotonic functions and lead to clear drop in final accuracy. These results highlight monotonicity as key property for point-wise functions to ensure effective learning. 6 function alias top-1 acc FID ViT-Base DiT-B/4 DiT-L/ 0 ğ‘‘ğ‘¡ 2ğœ‹1/2 ğ‘¥ ğ‘’ğ‘¡2 (ğ‘’ğ‘¥ ğ‘’ğ‘¥)(ğ‘’ğ‘¥ + ğ‘’ğ‘¥)1 sin(clip(ğ‘¥, ğœ‹ 2 , ğœ‹ ğ‘¥2 + 1), 1, 1) 2 )) clip (ln(ğ‘¥ + ğ‘¥(ğ‘¥2 + 1)1/2 sign(ğ‘¥) ((1 ğ‘’ ğ‘¥)) clip(ğ‘¥, 1, 1) sign(ğ‘¥) ((ğ‘’ğ‘¥ 1)) clip (sign(ğ‘¥) ln(ğ‘¥ + 1), 1, 1) ğ‘¥( ğ‘¥2 + 1 + 1) arctan(ğ‘¥) ğ‘¥(1 + ğ‘¥)1 clip (sign(ğ‘¥) ln(ğ‘¥2 + 1), 1, 1) clip (sign(ğ‘¥) ğ‘¥2/3, 1, 1) sign(ğ‘¥) ln(ğ‘¥ + 1) (ln(ğ‘¥ + 1) + 1)1 ğ‘¥3(ğ‘¥3 + 1)1 LayerNorm erf(ğ‘¥) tanh(ğ‘¥) satursin(ğ‘¥) arcsinhclip(ğ‘¥) isru(ğ‘¥) exproot(ğ‘¥) linearclip(ğ‘¥) expsign(ğ‘¥) logsignclip(ğ‘¥) relsign(ğ‘¥) arctan(ğ‘¥) smoothsign(ğ‘¥) logquadclip(ğ‘¥) power23clip(ğ‘¥) saturlog(ğ‘¥) cubsign(ğ‘¥) 82.3% 82.8% 82.6% 82.6% 82.5% 82.3% 82.4% 82.3% 82.2% 82.4% 82.3% 82.4% 82.4% 82.2% 82.1% 81.8% 81.4% 64. 63.23 63.71 63.90 64.72 65.72 65. 66.08 64.85 65.59 68.42 67.07 68. 65.92 66.11 68.23 70.22 45.91 43. 45.48 44.83 45.48 45.93 46.91 45. 45.82 46.34 48.33 46.62 47.29 47. 46.47 47.44 49.16 Table 7 Top-1 accuracy on ViT-Base and image generation quality (FID) on DiT-B/4 and DiT-L/4. Different functions show noticeable differences in performance. Among all the point-wise functions and LayerNorm, erf(ğ‘¥) shows the best performance in both top-1 accuracy and FID. Visualization of each function is included in Appendix B."
        },
        {
            "title": "4 Function Search",
            "content": "From the previous section, we observe that functions that are near zero-centered, bounded, center-sensitive (responsive to input variations around zero), and monotonic (increasing or decreasing) tend to yield better optimization performance. Building upon these insights, we start to construct our function set from widely used scalar functions and cumulative distribution functions (CDFs), including polynomial, rational, exponential, logarithmic, and trigonometric forms. We then generate variants via simple transformations such as translation, scaling, mirroring, rotation, and clipping. Functions that satisfy our four function properties after these transformations are retained as the candidate subset used in the search. For example, we transform the unbounded function arcsinh(ğ‘¥) by clipping it to the range [1, 1], limiting it to finite range and conforming to all four principles. In Appendix B, we provide further details about how we obtain these candidate functions. Within this set, we evaluate their performance, and Derf emerges as the most effective function. Setup. We conduct an empirical search on two representative vision architectures: Vision Transformer (ViT-Base) (Dosovitskiy, 2021) and Diffusion Transformer (DiT-B/4 and DiT-L/4) (Peebles and Xie, 2023). Models are trained on ImageNet-1K (Deng et al., 2009) under their default training settings. For ViT, model performance is measured using top-1 accuracy on the ImageNet-1K validation set. For DiT, we follow the standard ImageNet reference batch evaluation and report the FrÃ©chet Inception Distance (FID) as the metric. Formulation. We quantitatively evaluate set of functions under the constraint of our function properties, as illustrated in Figure 5. Each point-wise function is instantiated in unified form in Equation 8, where ğ‘“ () denotes candidate point-wise function, with learnable parameter ğ‘  and ğ›¼ recentering and rescaling the input. The parameters ğ›¾ and ğ›½ follow the same role as in standard normalization layers. We introduce learnable shift parameter ğ‘ , as it improves the final performance to varying degrees across different functions. Detailed ablation results on the effect of ğ‘  are provided in Section 7.1. ğ‘¦ = ğ›¾ * ğ‘“ (ğ›¼ğ‘¥ + ğ‘ ) + ğ›½, (8) Figure 5 Visualization of candidate point-wise functions on the positive half-axis. All functions are self-symmetric with respect to the origin. Quantitative evaluation. As shown in Table 7, even though these S-shaped functions appear highly similar in form, their empirical training results show noticeable differences in final performance. Among all the point-wise functions, erf(ğ‘¥) with the introduced transformations stands out as the best-performing function, consistently surpassing all other candidates and the baseline normalization layers."
        },
        {
            "title": "5 Dynamic erf (Derf)",
            "content": "From the search, we identify erf(ğ‘¥) as the most performant point-wise function. The error function erf() is closely related to the cumulative distribution function (CDF) of standard Gaussian distribution. Specifically, erf(ğ‘¥) can be defined by Equation 9. In our setup, erf(ğ‘¥) is in the form augmented with learnable parameters, which we introduce as Derf, Dynamic erf. Given an input tensor ğ‘¥, Derf layer is defined in Equation 10, where both the shift ğ‘  and the scale ğ›¼ are learnable scalars. ğ›¾ and ğ›½ are learnable per-channel vectors. To integrate Derf into transformer-based architecture, we replace each normalization layer with corresponding Derf layer. In particular, the pre-attention, the pre-FFN, and the final normalization layers are all substituted in one-to-one manner, ensuring consistent incorporation of Derf across the entire model. erf(ğ‘¥) = 2 ğœ‹ ğ‘¥ 0 ğ‘’ğ‘¡ ğ‘‘ğ‘¡ Derf(ğ‘¥) = ğ›¾ erf(ğ›¼ğ‘¥ + ğ‘ ) + ğ›½ (9) (10) Parameter initialization. We initialize ğ›¾ to an all-one vector and ğ›½ to an all-zero vector following the same strategy as in standard normalization layers. For the additional scalar parameters introduced by Derf, the scaling parameter ğ›¼ is initialized to 0.5, while the shift parameter ğ‘  is initialized to 0. Unless otherwise specified, these initialization settings are adopted throughout all experiments."
        },
        {
            "title": "6 Experiments",
            "content": "We evaluate the effectiveness of Derf across various transformer-based and few other modern architectures. For each model, we replace the original normalization layers with DyT and Derf, following the standard training and evaluation protocols, as detailed in Appendix C. Across all tested architectures, Derf consistently achieves stronger performance over the baseline normalization methods and DyT. Besides each models default normalization, we also report results with other common normalization methods in Appendix D. Vision Transformers. We train ViT-Base and ViT-Large models (Dosovitskiy, 2021) on ImageNet-1K (Deng et al., 2009) using LayerNorm (LN), DyT, and Derf for comparison. Table 8 reports the top-1 classification accuracy. Compared to LN and DyT, Derf achieves clearly higher top-1 accuracy. 8 model LN DyT Derf Î”LN Î”DyT ViT-B ViT-L 82.3% 83.1% 82.5% 83.6% 82.8% 83.8% 0.5% 0.7% 0.3% 0.2% Table 8 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than both LN and DyT on different model sizes, demonstrating its effectiveness in vision transformer architectures. Diffusion Transformers. We train three Diffusion Transformer (DiT) (Peebles and Xie, 2023) models on ImageNet-1K (Deng et al., 2009). Consistent with the original DiT setup, the affine parameters in the normalization layers are retained for class conditioning across LN, DyT, and Derf. After training, we evaluate the FID scores using the standard ImageNet reference batch to measure image generation quality, as reported in Table 9. Derf achieves clear improvement in FID compared to both LayerNorm and DyT. model LN DyT Derf DiT-B/4 DiT-L/4 DiT-XL/2 64.93 45.91 19.94 63.94 45.66 20. 63.23 43.94 18.92 Î”LN 1.70 1.97 1.02 Î”DyT 0.71 1.72 1.91 Table 9 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf achieves lower FID scores than both LN and DyT across all DiT model sizes. Speech models. We train two wav2vec 2.0 Transformer models (Baevski et al., 2020) on the LibriSpeech dataset (Panayotov et al., 2015) for speech representation learning. We report the final validation loss in Table 10. Compared to LayerNorm and DyT, Derf yields lower validation loss on different model sizes. model LN DyT Derf Î”LN Î”DyT wav2vec 2.0 Base wav2vec 2.0 Large 1.95 1.92 1.95 1.91 1.93 1.90 0.02 0. 0.02 0.01 Table 10 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than both LN and DyT across two wav2vec 2.0 models, indicating its better representation quality. DNA models. For the long-range DNA sequence modeling task, we pretrain the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024) using the human reference genome from (GRCh38, 2013). Model evaluation is conducted on the GenomicBenchmarks dataset (GreÅ¡ovÃ¡ et al., 2023). We report the averaged accuracy over all subtasks. As shown in Table 11, Derf surpasses both normalization layers and DyT in performance, demonstrating its robustness in genomic sequence modeling. model Norm DyT Derf Î”Norm Î”DyT Hyena Caduceus 85.2% 86.9% 85.2% 86.9% 85.7% 87.3% 0.5% 0.4% 0.5% 0.4% Table 11 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Each model is evaluated with its default normalization layer (LN for Heyna, RMSNorm for Caduceus). Derf consistently achieves higher accuracy than both normalization layers and DyT, indicating its effectiveness in DNA model. 9 Language models. We pretrain GPT-2 (124M) model on the OpenWebText dataset and report the validation loss in Table 12. For DyT and Derf, we additionally finetune the initialization of the learnable parameter ğ›¼. We observe that Derf achieves comparable performance to LN, while clearly outperforming DyT. model GPT-2 LN 2.94 DyT 2. Derf 2.94 Î”LN 0.00 Î”DyT 0. Table 12 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performance of LN while achieving lower validation loss than DyT."
        },
        {
            "title": "6.1 Stronger Generalization or Better Fitting?",
            "content": "Given Derfs superior performance, we aim to determine whether the gains arise from improved fitting capacity or stronger generalization. To this end, we compare the training loss of models respectively trained with normalization layers, DyT, and Derf. Since lower training loss indicates stronger fitting ability, this comparison helps us assess whether Derf improves optimization or enhances generalization. Setup. We compute training losses across diverse architectures and scales. To measure fitting capacity fairly, we do not use the loss during optimization, which is confounded by stochastic regularization (e.g., stochastic depth (Huang et al., 2016a)) and train-time augmentations. Instead, after training, we switch to evaluation mode, disable stochastic depth (when present), adopt the test-time preprocessing pipeline, and compute the loss on the training set. This yields fair estimate of each models fitting capacity. In Appendix E, we provide the detailed procedure for computing the evaluation-mode training loss for each model. Results. Across all architectures and scales, both Derf and DyT result in higher training loss than normalizationbased models, with Derf generally yielding slightly lower training loss than DyT, as shown in Table 13. This consistent pattern indicates that neither Derf nor DyT improves fitting capacity over normalization layers. model ViT-B ViT-L DiT-B DiT-L DiT-XL wav2vec 2.0 wav2vec 2.0 Hyena Caduceus GPT-2 Norm 0.2623 0.2034 0.1531 0.1501 0.1432 1.8509 1.8241 1.1297 0.8917 2.9478 Derf 0.2681 0.2066 0.1533 0.1510 0.1436 1.8821 1.8563 1.1526 0.9129 2.9702 DyT 0.2714 0.2083 0.1535 0.1518 0.1440 1.8946 1.8641 1.1631 0.9203 2.9822 Table 13 Evaluation-mode training loss of normalization layers (Norm), Derf, and DyT after optimization. Bolded indicates the lowest loss, and underlined means the second-lowest loss. Across all model architectures, the training loss follows the relation: Norm < Derf < DyT. Both DyT and Derf exhibit higher training loss than normalization layers, while Derf achieves slightly lower loss than DyT. Discussion. Despite the reduced fitting capacity, Derf delivers consistent performance gains across all evaluated tasks. We hypothesize that these gains arise primarily from both better generalization than normalization layers and stronger fitting capacity than DyT. Firstly, point-wise functions promote stronger generalization. Although Derf yields higher training loss, it achieves superior downstream performance, indicating that its benefits stem not from improved fitting but from enhanced generalization. This difference likely originates from the contrasting operational principles between normalization layers and point-wise functions. Normalization layers adapt their transformation 10 based on training statistics, allowing them to dynamically fit activation distributions throughout training. In contrast, point-wise functions are controlled by only small set of learnable scalar parameters (e.g., ğ›¼ for DyT and ğ›¼, ğ‘  for Derf) that do not adapt to activation statistics after training. They apply the same transformation regardless of activation distribution. This limited adaptability constrains overfitting and effectively serves as an implicit regularizer, leading to improved generalization. Secondly, Derf exhibits stronger fitting power than DyT. It achieves lower training loss while retaining the implicit regularization of point-wise functions, combining higher fitting capacity with strong generalization to outperform both DyT and normalization-based models."
        },
        {
            "title": "7 Analysis",
            "content": "In this section, we begin with two ablation studies examining the influence of the learnable shift parameter ğ‘  on the training results, followed by an analysis of an approximation of Derf."
        },
        {
            "title": "7.1 Effect of s",
            "content": "Removing ğ‘ . We investigate the effect of the learnable scalar parameter ğ‘  by removing it from the point-wise function. As shown in Table 14, introducing this learnable shift consistently improves the overall training performance, and the degree of improvement varies across different functions. The stronger results of erf(ğ‘¥) over tanh(ğ‘¥) indicate that Derf surpasses DyT not only because of the shift ğ‘ . top-1 acc FID function without ğ‘  with ğ‘  without ğ‘  with ğ‘  erf(ğ‘¥) tanh(ğ‘¥) satursin(ğ‘¥) isru(ğ‘¥) arctan(ğ‘¥) arcsinhclip(ğ‘¥) 82.6% 82.5% 82.4% 82.2% 82.3% 82.4% 82.8% 82.6% 82.6% 82.3% 82.4% 82.5% 63.39 63.94 65.28 66.14 67.41 65. 63.23 63.71 63.90 65.72 67.07 64.72 Table 14 Ablation study of ğ‘ . Top-1 accuracy on ViT-Base and FID score on DiT-B/4, comparing models with and without ğ‘ . ğ‘  improves the overall training performance, while its effect varies across different point-wise functions. Scalar vs. vector ğ‘ . We further examine whether using per-channel vector parameter instead of scalar ğ‘  leads to any performance improvement. As shown in Table 15, across all three point-wise functions, the choice between scalar and per-channel vector shows no significant impact on the final performance. Therefore, we adopt the scalar form of ğ‘  for efficiency and simplicity during training. function vector scalar function ViT-B ViT-L DiT-B DiT-L erf(ğ‘¥) arctan(ğ‘¥) arcsinhclip(ğ‘¥) 82.8% 82.5% 82.5% 82.8% 82.4% 82.5% tanh(ğ‘¥) tanh(ğœ€ğ‘¥) erf(ğ‘¥) 82.6% 82.7% 82.8% 83.6% 83.7% 83.8% 63.71 63.88 63. 45.48 45.13 43.94 Table 15 Top-1 accuracy of scalar vs. vector ğ‘  on ViT-Base. Using either scalar or per-channel vector for the parameter ğ‘  yields nearly identical performance. Table 16 Top-1 accuracy of tanh(ğœ€ğ‘¥) on ViT and DiT. tanh(ğœ€ğ‘¥) yields comparable or slightly improved performance over tanh(ğ‘¥) but still remains below erf(ğ‘¥)."
        },
        {
            "title": "7.2 Approximating Derf",
            "content": "Given the superior performance of erf(ğ‘¥) over tanh(ğ‘¥), we approximate erf(ğ‘¥) by scaling tanh(ğ‘¥) and examine whether this modification can lead to performance improvement. We introduce fixed coefficient ğœ€ and use tanh(ğœ€ğ‘¥), where ğœ€ is obtained by minimizing the following objective: + min ğœ€ tanh(ğœ€ğ‘¥) erf(ğ‘¥) ğ‘‘ğ‘¥. (11) The optimal value is found to be ğœ€ 1.205. As shown in Table 16, tanh(ğœ€ğ‘¥) achieves comparable or slightly improved performance over the original tanh(ğ‘¥), while still performing worse than erf(ğ‘¥). This indicates that simply scaling tanh(ğ‘¥) is insufficient to match the behavior or performance of erf(ğ‘¥)."
        },
        {
            "title": "8 Related Work",
            "content": "Normalization layers. Since the introduction of Batch Normalization (BN) (Ioffe and Szegedy, 2015), various normalization methods have been proposed to better stabilize training. To address BNs limitations with small batches, several alternatives (Salimans and Kingma, 2016; Wu and He, 2018; Yan et al., 2020; Shen et al., 2020; Singh and Krishnan, 2020) have been explored. In parallel, LayerNorm (Ba et al., 2016; Nguyen and Salazar, 2019; Xu et al., 2019; Xiong et al., 2020) and RMSNorm (Zhang and Sennrich, 2019) were designed for RNN (Hochreiter and Schmidhuber, 1997) and Transformer architectures (Vaswani et al., 2017). Task-specific variants (Ulyanov et al., 2016; Wu and He, 2018; Shen et al., 2020) further adapt normalization to applications such as object detection and style transfer. Mechanisms of normalization. series of studies has investigated how normalization layers contribute to model convergence. From an optimization perspective, normalization stabilizes gradient flow (Balduzzi et al., 2017; Daneshmand et al., 2020; Lubana et al., 2021), reduces sensitivity to initialization (Zhang et al., 2019; De and Smith, 2020; Shao et al., 2020), and implicitly tunes learning rates (Arora et al., 2019; Tanaka and Kunin, 2021). It has also been shown to smooth the loss landscape (Santurkar et al., 2018; Bjorck et al., 2018; Karakida et al., 2019) and reduce sharpness (Lyu et al., 2022; Dai et al., 2023; Mueller et al., 2023), promoting more stable optimization dynamics. Understanding these underlying functionalities provides valuable guidance for designing normalization-free training methods. Normalization-free methods. Building on this understanding of normalization, recent work explores how to achieve stable convergence without normalization. One line of work operates at the parameter and optimization level, using tailored initialization schemes (Bachlechner et al., 2021; De and Smith, 2020; Zhang et al., 2019), self-normalizing activations (Klambauer et al., 2017), weight normalization (Salimans and Kingma, 2016; Brock et al., 2021a), or adaptive gradient clipping (Brock et al., 2021b) to maintain stable gradient propagation. Another line of work modifies the architecture through structural simplifications (He and Hofmann, 2024) and Softmax-only formulations (Jha and Reagen, 2024). More recently, point-wise functions such as Dynamic Tanh (Zhu et al., 2025) have been proposed, with theoretical analyses revealing their similarity to normalization operations (Stollenwerk, 2025). Unlike previous methods that aim to match the performance of normalization layers, Derf consistently delivers stronger performance across diverse models."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we demonstrate that well-designed point-wise functions do not merely match the performance of normalization layers, but can surpass them. By revisiting the design space of point-wise functions, we identify zero-centeredness, boundedness, center sensitivity, and monotonicity as four key properties that enable strong performance in Transformer-based models. Among the functions satisfying these properties, Derf stands out as the most effective design: it consistently outperforms normalization-based methods and another notable point-wise function, DyT, across wide range of modalities and tasks. Its simplicity and strong empirical performance make Derf compelling replacement for normalization layers in many Transformer architectures."
        },
        {
            "title": "Acknowledgments",
            "content": "We gratefully acknowledge the use of the Neuronic GPU computing cluster maintained by the Department of Computer Science at Princeton University. This work was substantially performed using Princeton Research Computing resources, consortium led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Research Computing at Princeton University. This work is also supported by the computational resources generously provided by Googles TPU Research Cloud program."
        },
        {
            "title": "References",
            "content": "Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. ICLR, 2019. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In UAI, 2021. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. In NeurIPS, 2020. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In ICML, 2017. Nils Bjorck, Carla Gomes, Bart Selman, and Kilian Weinberger. Understanding batch normalization. In NeurIPS, 2018. Andrew Brock, Soham De, and Samuel Smith. Characterizing signal propagation to close the performance gap in unnormalized resnets. ICLR, 2021a. Andrew Brock, Soham De, Samuel Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In ICML, 2021b. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. Zhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Ling Liang, Yufei Ding, and Yuan Xie. Effective and efficient batch normalization using few uncorrelated data for statistics estimation. IEEE Transactions on Neural Networks and Learning Systems, 2020. Yan Dai, Kwangjun Ahn, and Suvrit Sra. The crucial role of normalization in sharpness-aware minimization. In NeurIPS, 2023. Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids ranks collapse for randomly initialised deep networks. In NeurIPS, 2020. Soham De and Sam Smith. Batch normalization biases residual blocks towards the identity function in deep networks. In NeurIPS, 2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 13 Ensembl GRCh38. p13 (genome reference consortium human build 38), insdc assembly, 2013. KatarÃ­na GreÅ¡ovÃ¡, Vlastimil Martinek, David ÄŒechÃ¡k, Petr Å imeÄek, and Panagiotis Alexiou. Genomic benchmarks: collection of datasets for genomic sequence classification. BMC Genomic Data, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Bobby He and Thomas Hofmann. Simplifying transformer blocks. ICLR, 2024. Stefan Heimersheim. You can remove gpt2s layernorm by fine-tuning. arXiv preprint arXiv:2409.13710, 2024. Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computation, 1997. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth, 2016a. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep networks with stochastic depth. In ECCV, 2016b. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. Nandan Kumar Jha and Brandon Reagen. Aero: Softmax-only llms for efficient private inference. arXiv preprint arXiv:2410.13060, 2024. Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviating pathological sharpness in wide neural networks. In NeurIPS, 2019. GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. In NeurIPS, 2017. Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via composition optimization. In AISTATS, 2019. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, 2022. Ekdeep Lubana, Robert Dick, and Hidenori Tanaka. Beyond batchnorm: Towards unified understanding of normalization in deep learning. In NeurIPS, 2021. Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normalization layers: Sharpness reduction. In NeurIPS, 2022. Maximilian Mueller, Tiffany Vlaar, David Rolnick, and Matthias Hein. Normalization layers are all that sharpness-aware minimization needs. In NeurIPS, 2023. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. In NeurIPS, 2023. Toan Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. IWSLT, 2019. 14 Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In ICASSP, 2015. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. Tim Salimans and Durk Kingma. Weight normalization: simple reparameterization to accelerate training of deep neural networks. In NeurIPS, 2016. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? In NeurIPS, 2018. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling. In ICML, 2024. Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj. Is normalization indispensable for training deep neural network? In NeurIPS, 2020. Sheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. Powernorm: Rethinking batch normalization in transformers. In ICML, 2020. Saurabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. In CVPR, 2020. Felix Stollenwerk. The mathematical relationship between layer normalization and dynamic activation functions. arXiv preprint arXiv:2503.21708, 2025. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016. Hidenori Tanaka and Daniel Kunin. Noethers learning dynamics: Role of symmetry breaking in neural networks. In NeurIPS, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, 2020. Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. In NeurIPS, 2019. Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun. Towards stabilizing batch statistics in backward propagation of batch normalization. ICLR, 2020. 15 An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qiming Yang, Kai Zhang, Chaoxiang Lan, Zhi Yang, Zheyang Li, Wenming Tan, Jun Xiao, and Shiliang Pu. Unified normalization for accelerating and stabilizing transformers. In ACM MM, 2022. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. 2018. Hongyi Zhang, Yann Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. ICLR, 2019. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In AAAI, 2020. Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization. In CVPR, 2025."
        },
        {
            "title": "A Property Analysis Details",
            "content": "In this section, we provided detailed explanation and visualization on how different function properties affect model training. A.1 Zero-centeredness We plot the training curves for ğœ†horiz and ğœ†vert with values {0, 0.1, 1} in Figure 6. The trends are consistent with those observed in top-1 accuracy on ImageNet-1K. For horizontal shifts, the training loss with ğœ†horiz = 0.1 nearly overlaps with that of ğœ†horiz = 0, and even reaches slightly lower loss. In contrast, vertical shifts exhibit monotonic pattern: increasing ğœ†vert consistently raises the training loss, suggesting reduced fitting capacity under larger vertical shift. (a) Horizontal shift (b) Vertical shift Figure 6 Training loss curve for horizontal and vertical shifts on the base point-wise function erf(ğ‘¥). The trends are consistent with the patterns observed in top-1 accuracy on ImageNet-1K. A.2 Center Sensitivity We visualize the training losses obtained as ğœ† varies over {0, 0.1, 0.5, 1.0, 2.0} on the base point-wise function erf(ğ‘¥). As shown in Figure 7, training loss shows clear monotonic trend: larger ğœ† consistently leads to higher loss, indicating that the width of the flat zone directly limits the models fitting capacity. 16 Figure 7 Training loss curve for different center sensitivity (controlled by ğœ†). larger ğœ† leads to higher training loss and poorer fitting ability. Figure 8 Training loss curve for different monotonicity. Monotonic functions consistently achieve lower training loss than non-monotonic functions. A.3 Monotonicity We plot the training losses of four functions with distinct monotonicity patterns: the monotonically increasing erf(ğ‘¥), the monotonically decreasing negerf(ğ‘¥), the hump-shaped dampx(ğ‘¥), and the oscillatory sin(ğ‘¥). As shown in Figure 8, both increasing and decreasing monotonic functions achieve clearly lower training loss, indicating stronger fitting capacity. In contrast, the non-monotonic functions exhibit higher training loss. This behavior aligns closely with the top-1 accuracy trends observed on ImageNet-1K."
        },
        {
            "title": "B Function Search Details",
            "content": "In function search, wide variety of common functional forms are systematically explored under the constraint of our function properties. The candidates range from polynomial and rational functions to the trigonometric and hyperbolic families, as well as various cumulative distribution functions. Beyond these common functional forms, we also experiment with their variants through translation, scaling, concatenation, and clipping. We categorize all candidate functions (see Table 7) into four groups: natural functions, transformed basic functions, clipped unbounded functions, and canonical ratio functions, and present detailed descriptions and visualizations of how each group is constructed. Natural functions. This category consists of three functions: erf(ğ‘¥), tanh(ğ‘¥), and arctan(ğ‘¥). As shown in Figure 9, these functions naturally satisfy all the function properties, including zero-centeredness, boundedness, center sensitivity, and monotonicity. Among them, only arctan(ğ‘¥) is rescaled so that all three functions have their ranges unified to [1, 1]. Transformed basic functions. This category consists of six functions: satursin(ğ‘¥), expsign(ğ‘¥), exproot(ğ‘¥), relsign(ğ‘¥), isru(ğ‘¥), and cubsign(ğ‘¥). These functions are constructed by starting from simple and commonly used primitives, such as power functions and polynomial forms. Through transformations including translation, scaling, and rotation, we reshape their original structures so that they satisfy all four function properties while preserving the qualitative behavior of the underlying base functions, as shown in Figure 10. Clipped unbounded functions. This category consists of five functions: logsign(ğ‘¥), logquad(ğ‘¥), arcsinh(ğ‘¥), power23(ğ‘¥), and linear(ğ‘¥). These functions inherently satisfy zero-centeredness and center sensitivity. For logsignclip(ğ‘¥), logquad(ğ‘¥), and power23clip(ğ‘¥), either due to domain asymmetry or because the original form is not monotonic, we construct the negative branch by mirroring the positive side around the origin to ensure monotonicity, as shown in Figure 11. To additionally enforce boundedness, we clip their outputs to the interval [1, 1], which leads to improved performance in practice. Canonical ratio functions. This category consists of two functions: saturlog(ğ‘¥) and smoothsign(ğ‘¥). Both functions are constructed using the canonical ratio form ğ‘“ (ğ‘¥) ğ‘“ (ğ‘¥)+1 , which naturally enforces boundedness and monotonicity. By selecting ğ‘“ (ğ‘¥) to be an odd, zero-centered base function, the resulting ratio form automatically satisfies zero-centeredness and center sensitivity as well. As shown in Figure 12, this construction yields smooth saturating behaviors that remain stable across wide input range. 17 Figure 9 Visualization of natural functions. Figure 11 Visualization of clipped unbounded functions. Figure 10 Visualization of transformed basic functions. Figure 12 Visualization of canonical ratio functions."
        },
        {
            "title": "C Experimental Settings",
            "content": "Vision Transformers. For all supervised classification experiments on ImageNet-1K, we adopt the training configurations summarized in Table 17. ViT-B and ViT-L share the same hyperparameters, except that ViT-L employs modified AdamW momentum setting with (ğ›½1=0.9, ğ›½2=0.95) and higher stochastic depth rate of 0.5. config optimizer base learning rate weight decay optimizer momentum effective batch size learning rate schedule warmup epochs training epochs augmentation label smoothing (Szegedy et al., 2016) mixup (Zhang et al., 2018) cutmix (Yun et al., 2019) random erase (Zhong et al., 2020) drop path (Huang et al., 2016b) exp. moving average (EMA) value AdamW 4e-3 0.05 ğ›½1=0.9, ğ›½2=0.999 (ğµ), 0.95 (ğ¿) 4096 cosine decay 20 300 rand-m9-mstd0.5-inc1 0.1 0.8 1.0 0.25 0.15 (B), 0.5 (L) 0.9999 Table 17 Training Configurations of ViT. Diffusion Transformers. We use the official implementation (Peebles and Xie, 2023) to train all DiT model sizes as shown in Table 18. We observe that the default learning rate is suboptimal for the models in this work. For both the search function experiments and the final evaluation of Derf, we go through three learning rates, 1 104, 2 104, and 4 104, for all models, whether they use LayerNorm or point-wise function, and report the best result. We also observe that the zero initialization negatively affects the performance of Derf models and other point-wise function models. Therefore, we retain the zero initialization for LN models but remove it for the other models. config optimizer base learning rate weight decay optimizer momentum effective batch size learning rate schedule training epochs exp. moving average (EMA) value AdamW {1e-4, 2e-4, 4e-4} 0 ğ›½1=0.9, ğ›½2=0.999 256 constant 80 0.9999 Table 18 Training Configurations of DiT. Speech models. For both wav2vec 2.0 models, we retain the first GroupNorm layer and the LayerNorm located after the convolutional feature extractor, since both primarily serve as data normalization to handle the unnormalized input data. We use the official implementation (Baevski et al., 2020) for both the Base and Large models, keeping all hyperparameters identical to the original setup, as shown in Table 19. The only change we make is running all modelswhether normalization-based or point-wise-function-basedin fp32 precision instead of the default bf16. We report the final validation loss. DNA models. For both the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024), we directly follow their official implementations without modifying hyperparameters, as shown in Table 20. In particular, Hyena uses LayerNorm and Caduceus uses RMSNorm. For our evaluation, we replace each models original normalization layer with Derf and report the average accuracy across all tasks. Language models. For the GPT-2 (124M) model, we follow the hyperparameters as shown in Table 21. For Derf and DyT, we configure the ğ›¼ initialization separately for the point-wise function layer following config optimizer learning rate weight decay optimizer momentum max tokens learning rate schedule warmup updates max updates dropout (input to encoder) dropout (target features) dropout (transformer) layer dropout feature grad mult latent temp max sample size value Adam 5e-4 (B), 3e-4 (L) 0.01 ğ›½1=0.9, ğ›½2=0.98 1400000 (B), 1200000 (L) polynomial decay 32000 (B), 20000 (L) 400000 (B), 250000 (L) 0.1 0.1 0.0 (B), 0.1 (L) 0.05 (B), 0.2 (L) 0.1 [2,0.5,0.999995] (B), [2.0,0.1,0.999995] (L) 250000 (B), 320000 (L) Table 19 Training Configurations of wav2vec 2.0. config optimizer learning rate sequence length effective batch size training steps RC augmentation MLM probability bidirectional value AdamW 6e-4 (H), 8e-3 (C) 1024 (H), 131072 (C) 1024 (H), 8 (C) 10000 (H), 50000 (C) true (H), false (C) 0.0 (H), 0.15 (C) false (H), true (C) Table 20 Training Configurations of HyenaDNA and Caduceus. denotes HyenaDNA, denotes Caduceus. the attention layer and for the other point-wise function layers. We try multiple combinations of these initialization settings and report the best validation loss. config optimizer base learning rate weight decay optimizer momentum gradient clipping block size gradient accumulation steps effective batch size learning rate schedule warmup iterations training iterations dropout mixed precision value AdamW 6e-4 0.1 ğ›½1=0.9, ğ›½2=0.95 1.0 1024 40 491,520 cosine decay 2,000 300,000 0.0 bf16 Table 21 Training Configurations of GPT-2 (124M)."
        },
        {
            "title": "D Additional Results",
            "content": "Beyond evaluating each model with its default normalization layer (typically LN), we additionally test RMSNorm and GroupNorm (GN) to enable more complete comparison. RMSNorm is widely used in modern large language models, including T5 (Raffel et al., 2020), LLaMA (Touvron et al., 2023a,b; Dubey et al., 2024), Qwen (Bai et al., 2023; Yang et al., 2024), and DeepSeek (Liu et al., 2024; Guo et al., 2025), while GN is employed in several vision architectures, including ConvNeXt (Liu et al., 2022), DETR (Carion et al., 2020), and Swin Transformer (Liu et al., 2021). All evaluations follow the same experimental settings described in the previous section. These additional results show that Derf not only surpasses the default choices used in each model, but also outperforms the 20 other normalization alternatives we evaluate. Vision Transformers. For both ViT-Base and ViT-Large (Dosovitskiy, 2021), the default normalization layer is LayerNorm. To complement the results, we also evaluate RMSNorm (Zhang and Sennrich, 2019) and GN (Wu and He, 2018) as additional replacements in Table 22. Compared to all other methods, Derf achieves clearly higher top-1 accuracy, demonstrating its effectiveness in vision transformer architectures. model LN DyT Derf RMSNorm GN ViT-B ViT-L 82.3% 83.1% 82.5% 83.6% 82.8% 83.8% 82.4% 83.0% 82.5% 83.1% Table 22 Supervised classification accuracy on ImageNet-1K. Derf achieves higher top-1 accuracy than all other methods on different model sizes. Diffusion Transformers. For DiT models (Peebles and Xie, 2023), we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and Derf. As shown in Table 23, Derf achieves clear improvement in FID compared to all other methods. model DiT-B/4 DiT-L/4 DiT-XL/2 LN 64.93 45.91 19.94 DyT 63.94 45.66 20.83 Derf 63.23 43.94 18.92 RMSNorm 65.08 45.02 20.76 Table 23 Image generation quality (FID) on ImageNet. Lower FID indicates better image generation quality. Derf achieves lower FID scores than all other methods across different DiT models. Speech models. For two wav2vec 2.0 Transformer models (Baevski et al., 2020), we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) as an alternative normalization layer and compare its performance with LN, DyT, and Derf in Table 24. Compared to other methods, Derf yields lower validation loss on different model sizes model wav2vec 2.0 Base wav2vec 2.0 Large LN 1.95 1.92 DyT 1.95 1.91 Derf 1.93 1.90 RMSNorm 1.95 1.93 Table 24 Speech pretraining validation loss on the LibriSpeech dataset. Derf achieves lower validation loss than all other methods across two wav2vec 2.0 models. DNA models. For the HyenaDNA model (Nguyen et al., 2023) and the Caduceus model (Schiff et al., 2024), we additionally evaluate both LayerNorm and RMSNorm for each architecture, regardless of their default choices, and compare their performance with DyT and Derf in Table 25. model LN DyT Derf RMSNorm Hyena Caduceus 85.2% 87.0% 85.2% 86.9% 85.7% 87.3% 85.2% 86.9% Table 25 DNA classification accuracy on the GenomicBenchmarks dataset, averaged over each subtask. Derf consistently outperforms other methods across two different DNA models. Language models. For the GPT-2 (124M) model, we additionally evaluate RMSNorm (Zhang and Sennrich, 2019) for more complete comparison of normalization choices. As shown in Table 26, Derf achieves comparable performance to both LN and RMSNorm, while clearly outperforming DyT. 21 model GPT-2 LN 2. DyT 2.97 Derf 2.94 RMSNorm 2. Table 26 GPT-2 validation loss on the OpenWebText dataset. Derf matches the performances of both LayerNorm and RMSNorm while achieving lower validation loss than DyT."
        },
        {
            "title": "E Loss Calculation Details",
            "content": "Vision Transformers. For ViT models, we measure fitting capacity under deterministic evaluation setup. We switch the model to evaluation mode, disable drop-path, mixup, cutmix, label smoothing, and all data augmentations, and apply only the standard test-time preprocessing (center crop and normalize). The cross-entropy loss is then computed on the training set and averaged over all samples. Diffusion Transformers. For DiT models, we evaluate fitting capacity by switching the model to evaluation mode. We apply the standard test-time preprocessing (center crop, random horizontal flip, and normalize). Since DiT does not employ drop-path, no stochastic regularization needs to be disabled. We then compute the diffusion MSE loss over the first 100 training batches and report the average. Other models. For all other models, wav2vec 2.0, HyenaDNA, Caduceus, and GPT2, we simply apply the same procedure: use the standard test-time preprocessing, disable drop-path or dropout when present, and compute the training loss over the full training set, reporting the average."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "NYU",
        "Princeton University"
    ]
}