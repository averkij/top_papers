{
    "paper_title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model",
    "authors": [
        "Kevin Cannons",
        "Saeed Ranjbar Alvar",
        "Mohammad Asiful Hossain",
        "Ahmad Rezaei",
        "Mohsen Gholami",
        "Alireza Heidarikhazaei",
        "Zhou Weimin",
        "Yong Zhang",
        "Mohammad Akbari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \\href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \\href{https://github.com/vbdi/tad_bench}{Github}, respectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 7 2 5 0 . 2 1 5 2 : r From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Models Kevin Cannons1 Saeed Ranjbar Alvar1 Mohammad Asiful Hossain1 Ahmad Rezaei1 Mohsen Gholami1 Alireza Heidarikhazaei1 Zhou Weimin2 Yong Zhang1 Mohammad Akbari1 1 Huawei Technologies Canada Co., Ltd., 2 Huawei Cloud"
        },
        {
            "title": "Dataset",
            "content": "Figure 2. Overview of the proposed TAD benchmark. Left: An example of labels and question types. Each segment covers approximately five seconds of video from the NuScenes dataset. The labels consist of segment-level action annotations for the ego and other vehicles visible from the front camera. Two question types pertain to actions within these video segments, while the remaining five types relate to actions that require information from throughout the entire video scene. Right: The performance of open-source VLMs, the proposed training-free solutions (Scene-CoT and TCogMap), closed-source VLM, and human performance across the 7 benchmark tasks in TAD."
        },
        {
            "title": "Abstract",
            "content": "a for even challenge, significant Temporal understanding in autonomous driving (AD) remains recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 questionanswer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists *Equal contribution and corresponding authors {kevin.cannons, saeed.ranjbar.alvar1}@huawei.com, Equal contribution 1 of 9 closedand open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at Hugging Face and GitHub, respectively. 1. Introduction Temporal understanding of video content is critical requirement for creating autonomous agents, including cars and robots. VLMs have emerged as powerful tools that can reason across both vision and language, making them promising components in the design of autonomous agents. Accordingly, means for evaluating VLMs on video understanding tasks has emerged as an important research direction [6, 18, 28, 29, 31, 33]. Benchmarks for video understanding have generally focused on several specific types of video content, such as sports [4], cooking [58], and fulllength movies [43]. AD footage exhibits number of unique challenges compared to general video data, including: 1) Temporal Scale. Large variation in the temporal scale of activities, which necessitates methods that can temporally localize events with varying durations. 2) Ego-Centric View. The camera viewpoint makes understanding ego vehicle actions challenging, since the ego vehicle is not observable in the frames. 3) Fine-grained actions. Some actions performed by vehicles are subtle and may require additional scene cues for robust detection (e.g., gradual lane change compared to driving straight). Currently, there is no AD-oriented benchmark for assessing the performance of VLMs on the unique aspects that make temporal understanding difficult. In light of this gap, this paper proposes the Temporal Understanding in Autonomous Driving (TAD) benchmark, which evaluates various temporal understanding tasks. The benchmark is built on top of the popular NuScenes dataset [5] and is comprised of 5,861 QA pairs across 150 videos. The benchmark provides mix of tasks, most of which focus on scene-level understanding (i.e., video-level, terminology which is adopted from NuScenes), but two of which consider segment-level action recognition (i.e., short video segments). Inclusion of both scene and segment questions is novel characteristic of TAD compared to other benchmarks. Segment questions assess the VLMs ability to understand fine-grained vehicle actions (e.g., lefthand turns vs. changing lanes to the left); whereas, scene questions require an understanding of the temporal dynamics between atomic vehicle actions, that altogether form longer video. In addition to the benchmark questions, 4,481 segment-level vehicle action annotations were also created. An example of video scene with its associated segment labels and questions are shown in Fig. 2. The paper presents an extensive evaluation on 9 unique VLMs, considering total of 30 system configurations, including open-source generalist, AD specialist, and closedsource models. The results in Fig. 2 show that there is significant gap between the performance of VLMs and humans. To address this gap, two novel solutions, called Scene-CoT and TCogMap, are proposed. Scene-CoT is general method that leverages CoT to answer AD questions by breaking down the motions in scene. In contrast, TCogMap constructs an ego-centric temporal cognitive map using the ego vehicles trajectory and leverages it to enrich the input context for VLMs during inference. Thus, the major contributions of this work are as follows: Creation of TAD, the first QA benchmark for AD, specifically designed to evaluate temporal understanding. TAD systematically probes the complex temporal dynamics inherent in real-world driving scenarios. Creation of new dataset of fine-grained, humanannotated action labels for ego and non-ego vehicles across 150 NuScenes videos. comprehensive evaluation of SoTA open-source, closed-source and specialist VLMs on TAD, establishing baseline for AD temporal understanding in this domain. Introduction of Scene-CoT and TCogMap, two novel, training-free methods that can be seamlessly integrated with any VLM to significantly enhance the temporal understanding capabilities. These solutions are shown to improve average performance on TAD by up to 17.72%. 2. Related Work 2.1. Temporal Understanding in General Videos. The temporal understanding of general-purpose videos has been explored through various tasks and benchmarks. Key tasks include Temporal Localization specifying when an event occurs [9, 22, 47, 52] and Dense Video Captioning, which localizes and describes every event [13, 26, 27, 49, 55, 59, 60]. Corresponding benchmarks have been developed using diverse, general-domain videos. Some, like MVBench [29] and Video-MME [18], assess combination of spatial and temporal abilities. In contrast, others such as TempCompass [33] and TemporalBench [6] focus exclusively on fine-grained temporal reasoning (e.g., motion, magnitude, and event ordering) within an array of realworld videos spanning daily life, sports, and documentaries. 2.2. Benchmarks and Methods for VLM-based AD VLMs have emerged as promising direction for enhancing AD systems, offering improved explainability and leveraging multiple modalities [10, 14, 34, 36, 37, 39, 42, 44, 45, 48, 54]. Their applications range from conversational driving assistants [34] and visual question answering (VQA) systems [36, 44] to integration within the driving pipeline itself [10, 39, 42, 45, 48, 54]. With respect to benchmarks, several have been released in recent years to assess different aspects of AD-related tasks. LingoQA [36], DriveLM [44], and VLADBench [30] offer broad, extensive QA datasets that span the full stack of perception, prediction, planning. Other research has focused on narrow aspects of AD video understanding. BDDX [25] provides descriptions of the action performed by 2 Table 1. Overview of AD benchmarks for spatiotemporal understanding. #QAs is the number of questions. I/V Indicates the benchmark modality (image or video). Seg. Qs and Scene Qs denote segmentand scene-level questions, respectively. Benchmark #QAs I/V Seg. Qs Scene Qs DriveLM [44] DriveBench [53] DriveLLM-o1 [24] LingoQA [36] VLADBench [30] STSBench [17] STIBench [31] TAD (Ours) 15k 20k 4.6k 1k 12k 971 2k 5.8k the ego vehicle and the reason for that action. NuScenesQA [40] and NuPrompt [51] are perception-oriented benchmarks. In DRAMA [35] and Rank2Tell [41], critical scene objects are annotated along with suggestions for the ego vehicles action. Ego3D-Bench [20] considers models 3D spatial understanding capabilities in AD. SURDS [21] focuses on evaluating VLMs fine-grained spatial understanding in driving scenes, while CODA-LM [7] offers benchmark for evaluating understanding of corner cases. Tab. 1 summarizes the most closely-related QA benchmarks for AD with spatiotemporal understanding. TAD is the second-largest video-based benchmark focused on temporal understanding and the only one covering both segmentand scene-level questions. 3. TAD Benchmark 3.1. Benchmark Annotations The TAD benchmark is built on top of the NuScenes [5] validation split, which contains 150 videos, each roughly 20 seconds in length, with accompanying sensor data and 3D bounding-box annotations. TAD focuses on the following fine-grained vehicle classes defined in NuScenes: car, bus, bicycle, construction vehicle, motorcycle, trailer, and truck. The remaining object annotations were ignored for TAD. NuScenes does not provide action labels for scene objects, yet action understanding is critical aspect of understanding the temporal dynamics of scene. Thus, the first annotation task to create TAD was to label the vehicle actions in NuScenes, which required pre-processing step that divides each video into segments, as shown in Fig. 3. For vehicle action understanding, employing short segment-level approach is appropriate, since an action typically cannot be discerned from single frame and in long video, multiple vehicle actions may occur. For TAD, fivesecond segments were selected, since it was empirically observed that most vehicle atomic actions are completed within five seconds. Thus, each NuScenes video was divided into ten, uniformly distributed, overlapping segments. To streamline annotation and focus on the most relevant vehicles, each segment was filtered to include only vehicles within 50 of the ego car. This threshold was empirically set to retain the most relevant vehicles, while optimizing human annotation effort. Further, vehicles with negligible trajectory displacement within segment were automatically labeled as stopped and excluded from manual labeling. Video segments with the corresponding birds eye view (BEV) visualization, including the trajectories of ego and nearby vehicles, were provided to annotators. Annotators were then asked to label vehicles action corresponding to the dominant behavior during the video segment. Accordingly, only one action label was assigned to vehicle per segment. The following set of eight action categories were used for TAD: 1) traveling straight at constant speed, 2) stopping, 3) stopped, 4) starting, 5) turning left, 6) turning right, 7) changing lanes to the left, and 8) changing lanes to the right. In addition to the TAD benchmark, the vehicleaction annotations are released as standalone resource and can be used to evaluate vehicle action recognition. The final step of the annotation pipeline generates JSON-formatted source file that stores the object and action labels per segment. These files constituted the raw materials that were used to construct the TAD question-answer set. 3.2. QA Creation As shown in Fig. 3, QA generation followed templatedriven approach. Seven QA templates were used and each applied to the NuScenes videos. Logical checks were also included to verify that each question was unambiguous, e.g., ensuring that the objects referenced in each question are uniquely identifiable in the imagery. final humanverification stage was included to further improve the quality of the QA pairs and task definitions. After running the QA generation pipeline, 5,861 QAs were created for TAD, which are distributed across seven tasks, and will be described next. Several tasks, Temporal Ordering, Temporal Action Localization, Relative Temporal Action Localization, and Temporal Object Localization, are designed to challenge VLMs on when specific actions or events occur. Temporal Ordering and Relative Temporal Action Localization require the VLMs to understand the temporal ordering of events, although the exact timing of the events is not needed. The Action Duration task does not focus on when events happen, but how long they last. The final two tasks, Exact Answer Action Recognition and Multiple Choice Action Recognition, consider the VLMs ability to understand fine-grained actions in temporally cropped video segments. Sample questions from each task are shown in Fig. 2. Detailed descriptions of each question type are provided in the supplementary materials. 3 Figure 3. TAD annotation and question generation pipeline. Boxes with green dash: Steps with human verification and quality assessment. Table 2. Question counts per task in the TAD benchmark. 4. Proposed Methods Category Ego Non-Ego Total Exact Answer Action Recognition Multiple Choice Action Recognition Action Duration Temporal Ordering Temporal Action Localization Relative Temporal Action Localization Temporal Object Localization Total Dataset 1500 1030 124 62 338 92 0 3146 1104 756 0 18 432 0 405 2715 2604 1786 124 80 770 92 405 3.3. TAD Benchmark Statistics This section provides insight into the benchmark statistics. The question distribution is shown in Tab. 2. In addition to the Total column, which denotes the total number of questions per type, breakdown is also provided showing the number of ego vs. non-ego questions. All question types are well-represented, with minimum of 80 per category. Tab. 2 also shows that there are two question categories, Action Duration and Relative Temporal Action Localization that only consider the ego vehicle. Conversely, Temporal Object Localization only includes questions about non-ego car objects, since this question considers when those objects are visible to the ego. Tab. 2 also shows that the overall split between ego and non-ego vehicle questions is almost even, with 53.7% and 46.3% of the questions, respectively. As such, there is no significant benchmark bias favouring ego or non-ego objects. Further stats, including the instance count per action category as well as the breakdown of the vehicle types and counts are in the supplementary material. 4 4.1. Scene-CoT: CoT for Temporal Understanding The proposed Scene-CoT framework introduces CoT [50] based approach for structured scene reasoning in dynamic driving environments, consisting of three modules: 1) Video partitioning, 2) CoT reasoning, and 3) Large Language Model (LLM) QA. The system modules will be described next, while figure showing the Scene-CoT architecture is provided in the supplementary materials. Video Partitioning. Scene-CoT first partitions the input video into smaller segments. Let denote the original video and = {s1, s2, . . . , sl} are the set of segments. The segment length and overlap are configurable to meet design and resource constraints. In the current implementation, segments are uniformly distributed across V, each lasting 5 seconds with approximately 50% overlap between consecutive segments. This choice is suitable for driving scenarios, as five-second window with overlap provides sufficient temporal context to capture common maneuvers (e.g., lane change and turn) while maintaining responsiveness to rapidly changing scenes. Exploration of more sophisticated segmentation strategies is left for future work. To reduce pixel-wise redundancy from frame to frame and the number of visual tokens input to the VLM, only subset of frames were used for forming the CoT descriptions. In the current approach, four frames were uniformly sampled across each segment, i.e., {f 1 }, where denotes frame and corresponds to segment number. CoT Reasoning. For each segment, sj, the VLM is used to generate reasoning trace that should be sufficiently descriptive to answer the questions in TAD. To that end, four-step CoT reasoning procedure is designed as follows: 1) Scene description. The VLM provides high-level dej , 2 , 3 , 4 scription of the scene. 2) Ego vehicle motion. The VLM focuses only on the ego car motion and estimates its action. 3) Motion of nearby vehicles. The VLM isolates other vehicles close to the ego and describes their motion. 4) Summary formatting. The VLM generates final JSON-style summary containing fields for motions of all vehicles and nearby-vehicle identifiers. The final output of the reasoning stage for segment sj is denoted as cj, which is the concatenation of the scene description (Step 1) with the summary (Step 4). LLM-Based QA. The LLM takes as input the benchmark question together with the temporally ordered CoT reasoning outputs for segments = (c1, c2, c3, . . . , cl). Since the first step of Scene-CoT partitions the video into short, temporally ordered segments, the resulting CoT outputs support questions that require temporal reasoning and ordering. Formally, the final answer aScene-CoT is aScene-CoT = L(C, q), (1) where denotes the LLM. Additional details regarding Scene-CoT, are provided in the supplementary materials. 4.2. TCogMap: Temporal Understanding via Ego-"
        },
        {
            "title": "Centric Temporal Cognitive Maps",
            "content": "The second proposed method, TCogMap, is solution tailored to temporal understanding in the AD domain, as it leverages vehicle trajectory analysis module. Fig. 4 shows the three steps in TCogMap, as follows: Video Partitioning. TCogMap uses the same video partitioning module as Scene-CoT, resulting in set of video segments, = {s1, s2, s3 . . . sl}. Ego Vehicle Temporal Cognitive Map. Recent works have shown that generating so-called cognitive map representation assists VLM in its ability to perform questionanswering [20, 57]. Building on this insight, the proposed approach extends cognitive maps to the video domain for improved temporal understanding in outdoor and dynamic AD scenes. full extension of cognitive maps to the temporal domain that includes information about all objects is appealing, but may result in too much extraneous information confounding the VLM. Rather, TCogMap only considers the most important object in driving video: the ego vehicle. TCogMap analyzes the sequence of ego car poses (translation, rotation, and timestamp, which are provided with NuScenes) in order to classify its motion. The motion classification procedure, detailed in Algorithm 1, begins by processing the raw sequence of ego-poses. For each segment, the frame-wise global velocities and 2D speeds are computed via finite differences between consecutive poses (L58 in Algorithm 1). preliminary check classifies the segment as Stopped if the instantaneous speeds fall below Algorithm 1 Ego-Vehicle Motion Classification 1: Input: Ego-poses = {p0, . . . , pN 1}, each pi each with translation xi, quaternion list qi, and timestamp ti. 2: Input: Thresholds: Vstat, Vstopping, Ψturn, Vy,lc, Vx,lc. 3: Output: Motion label (e.g., Turn left). 4: procedure CLASSIFYMOTION(P ) for 1 to 1 do 5: 6: 7: 8: 9: vi (xi xi1)/(ti ti1) ωi vi[0 : 2] /(N 1) > 0.5 then end for if (cid:16)(cid:80)N (cid:17) i=1 [ωi < Vstat] return Stopped 10: 11: 12: 13: 14: 15: end if Aggregate motion features ψ yaw(qN 1) yaw(q0) ωstart ω1, ωend ωN 1 {vlocal,i} TransformToLocalFrame({vi}, {qi}) vx,local, vy,local mean({vlocal,i}) Hierarchical classification if ψ > Ψturn then return Turn left if ψ > 0, else Turn right else if vy,local > Vy,lc & vx,local > Vx,lc then if vy,local > 0 then return Change lane left return Change lane right else if ωstart < Vstopping & ωend > 1.5 Vstopping then else if ωstart > 1.5 Vstopping & ωend < Vstopping then return Starting return Stopping return Straight else 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: end if 30: 31: end procedure end if else predefined stationary threshold, Vstat, (L9-11). For segments with significant motion, set of dynamic features are extracted that serve as summary of the trajectory. These features include: the total change in yaw between the initial and final poses, ψ (L12); the initial and final speeds, ωstart and ωend (L13); and the average lateral and forward velocities, vy,local and vx,local (L15). To obtain these values, each global velocity vector, vi, is transformed into the ego vehicles local coordinate frame using its corresponding rotation matrix, Ri (L14): vi, vlocal,i = (2) where vlocal,i = [vx,local,i, vy,local,i, vz,local,i] contains the instantaneous forward, lateral, and vertical velocity components. The average values used for classification are then computed over the frames in the segment (L15): vx,local = (cid:80)N 1 i=1 vx,local,i 1 , vy,local = 5 (cid:80)N 1 i=1 vy,local,i 1 . (3) Figure 4. Overview for the proposed TCogMap method. These features are then fed into hierarchical decisionmaking process that uses set of empirically determined thresholds to assign single motion label, which is described in detail on L16-26 of Algorithm 1. This structured, rule-based approach effectively translates complex kinematic data into concise and semantically meaningful summary, which can be interpreted more easily by VLM. VLM-based QA. The final step in TCogMap is to answer the question. To do so, the VLM, ϕ, is provided with all frames from the input video, = {Fk}K k=1, the question itself, q, and the structured summary of the egovehicles motion, = {mi}l i=1. This summary consists of the motion categories derived for each of the video segments. Crucially, to provide clear temporal context for the model, each motion summary mi is formatted as string that specifies the corresponding frames in the segment (e.g., Motion summary for Frame1 to Frame7: The ego-vehicle is stopped). So, the final answer, aTCogMap, is obtained as: aTCogMap = ϕ(F, M, q). (4) 5. TAD Benchmarking 5.1. Experimental Setup For most TAD benchmarking results, four variants of each model are presented: baseline, baseline + textual ego pose, Scene-CoT (baseline + CoT outputs), and TCogMap (baseline + Temporal Cognitive Map). For the baseline, the VLM receives only the scene frames (approximately 40) and question. The two segment tasks, Exact Answer Action Recognition and Multiple Choice Action Recognition, are exceptions, and instead utilize approximately ten frames from the relevant segment as input to the VLMs. The baseline + textual ego pose variant is included to offer fair comparison with TCogMap. Most of the questions in TAD are multiple choice and are evaluated using accuracy. Performance for questions that output frame list format is measured via temporal mean intersection over union (mIoU), which is standard metric for temporal localization [22]. For questions expecting output text, binary exact match metric is used. For Scene-CoT, Qwen2.5-14B-Instruct-1M [56] was used as the LLM for experiments, unless specified otherwise. For the TCogMap method, the thresholds were empirically set to Vstat = 0.2 m/s, Vstopping = 1.0 m/s, Ψturn = 10.0 degrees, Vy,lc = 0.4 m/s, Vx,lc = 1.0 m/s. 5.2. Chance/Human Performance Levels Human and chance-level performance were also computed on TAD. Chance performance was computed based on random selection for the multiple-choice questions; whereas, random subset of frames was used for temporal localization tasks. Human-level performance was computed using randomly-sampled 10% subset for each question type. The first two rows in Tab. 3 show human-level and chance performance, respectively. Generally, human performance is far greater than chance-level across the tasks. However, human-level performance is lower for Temporal Action Localization, likely due to the similarity of some of the motion categories (e.g., stopped vs. stopping) and minor misalignments between the ground truth and human results (ground truth is in segments; humans responded with frames.). 5.3. Benchmarking Generalist VLMs Experiments were performed on the benchmark using both closedand open-source generalist VLMs that were not specifically trained for AD. For closed-source models, GPT5-mini [38] and Gemini-2.5-Flash [11] were tested. With respect to open-source models, Qwen2.5-VL [3] and InternVL3 [61] were selected since the InternVL and QwenVL model families consistently provide top-tier performance. Further details regarding the parameters specific to 6 Table 3. Main results for openand closed-source generalist VLMs as well as AD-specific VLMs. Bold and underline indicate best and second-best results when comparing the four system variants for each open-source VLM. C/Gen : closed-source generalist models, O/Gen : open-source generalist models, O/Drv : open-source driving specialist models. EA: exact answer, MC: multiple choice. Model Type EA Act. Recog. MC Act. Recog. Action Dur. Temp. Ord. Temp. Act. Local. Rel. Temp. Local. Temp. Obj. Local. Method Human level Chance level GPT-5-mini [38] Gemini-2.5-Flash [11] C/Gen C/Gen O/Gen O/Gen Qwen2.5-VL-7B [3] Qwen2.5-VL-7B+Ego Pose + Scene-CoT + TCogMap Qwen2.5-VL-32B [3] O/Gen Qwen2.5-VL-32B+Ego Pose O/Gen + Scene-CoT + TCogMap InternVL3-8B [61] InternVL3-8B+Ego Pose + Scene-CoT + TCogMap InternVL3-14B [61] InternVL3-14B+Ego Pose + Scene-CoT + TCogMap InternVL3-38B [61] InternVL3-38B+Ego Pose + Scene-CoT + TCogMap RoboTron [23] RoboTron+Ego Pose + Scene-CoT + TCogMap Cosmos-Reason [2] Cosmos-Reason+Ego Pose + Scene-CoT + TCogMap O/Gen O/Gen O/Gen O/Gen O/Gen O/Gen O/Drv O/Drv O/Drv O/Drv 81.68 12.17 54.57 53.07 47.77 46.54 57.49 62.79 60.56 64.13 60.45 71.85 57.26 53.57 62.52 61.37 56.87 60.41 62.06 66.09 50.69 55.57 63.67 67. 43.63 43.01 58.68 48.20 50.46 47.08 54.38 58.26 76.67 24.24 52.86 53.86 51.18 49.50 50.73 65.79 53.25 59.24 54.09 68.59 59.85 57.28 54.31 66.69 53.70 59.80 53.42 65.29 58.34 61.81 55.77 68. 58.17 59.69 52.80 62.77 49.38 53.86 47.87 64.61 64.29 25.81 45.97 45.16 34.68 35.48 52.42 50.81 47.58 53.23 46.77 58.87 53.23 54.84 51.61 57.26 52.42 52.42 51.61 58.06 45.97 52.42 53.23 60. 29.84 33.06 52.42 37.90 39.52 37.90 55.65 50.00 87.50 30.00 70.00 58.75 50.00 48.75 55.00 60.00 56.25 60.00 55.00 65.00 50.00 47.50 56.25 66.25 56.25 55.00 56.25 71.25 55.00 56.25 57.50 72. 41.25 43.75 57.50 52.50 52.50 52.50 60.00 67.50 50.21 41.70 26.79 38.35 35.28 35.12 30.77 46.26 44.96 44.45 31.56 41.42 35.42 37.58 37.31 47.08 43.53 46.78 36.61 50.24 45.41 38.33 38.27 44. 31.00 28.01 30.40 40.15 16.39 37.42 28.79 36.39 100 47.83 73.91 58.70 46.74 57.61 48.91 69.57 60.87 52.17 44.57 78.26 52.17 46.74 51.09 67.39 59.78 52.17 51.09 75.00 71.74 68.48 55.43 80. 65.22 61.96 54.35 79.35 64.13 45.65 55.43 67.39 62.69 58.85 40.18 56.81 45.60 37.27 45.65 57.95 62.91 60.10 48.99 62.91 45.64 62.96 50.34 64.10 43.90 62.83 51.64 59.00 68.27 66.00 53.77 66. 28.92 36.10 34.79 40.58 16.06 46.70 43.72 23.97 Avg 74.72 34.37 52.04 52.10 44.46 44.44 48.71 62.18 55.20 56.19 48.78 63. 50.51 51.50 51.92 61.45 52.35 55.63 51.81 63.56 56.49 56.98 53.95 65.66 42.58 43.65 48.71 51.64 41.21 45.87 49.41 52.59 each model are provided in the supplementary materials. Open-Source Models. As shown in Tab. 3, for the baseline models the average performance increases with the model size. For example, for Qwen2.5-VL, the average performance increases by more than 10% from the 7B model to the 32B model. Scene-CoT, the proposed CoT-based method, provides modest performance gains over the baselines for the small generalist models (e.g., Qwen2.5-VL-7B with Scene-CoT yields 4.25% improvement). For larger models, Scene-CoT does not appear to offer accuracy gains. It is hypothesized that this result arises from larger VLMs stronger inherent capabilities, due to their increased parameter count, which allows them to generate rich internal representations that are better suited for temporal QA than an explicit CoT-based description. TCogMap consistently outperforms the baseline + ego pose variants in average performance by as much as 17.74% (for Qwen2.5VL-7B) and large models still show substantial gains (e.g., an 8.68% improvement with InternVL338B). Even though both the baseline + ego pose and TCogMap receive identical input, the VLM is unable to digest the raw pose information with the baseline variant, exemplifying the importance of the temporal cognitive map. Closed-Source Models. Generalist closed-source models are also included to demonstrate their performance on AD temporal understanding. The average performance of 7 GPT-5-mini and Gemini-2.5-Flash are comparable to the medium-sized InternVL3-14B. Interestingly, GPT-5-mini shows strong results on two of the easier, Video Temporal Ordering and Relative Temporal Localization, as measured by the high Human Level scores. Gemini-2.5-Flash performs worse on these same two tasks, but achieves higher accuracies for Temporal Action Localization and Temporal Object Localization compared to GPT-5-mini. 5.4. Benchmarking on AD-Specific Models Two models trained on AD data, RoboTron [23] (8B) and Cosmos-Reason [2] (7B), were selected as the specialist models due to their SoTA results. As given in Tab. 3, accuracies for the baseline variants of these two models are on par with Qwen2.5-VL-7B. However, Cosmos-Reason is significantly outperformed by RoboTron on the Temporal Action Localization and Temporal Object Localization tasks, likely due to its input design. Cosmos-Reason receives entire videos rather than frame sequences, and thus lacks explicit frame-index awareness required for such temporally grounded questions. These results indicate that in-domain training data is not panacea for improving temporal understanding. Beyond the influence of training data, mechanisms that explicitly capture temporal structure and causal dependencies across frames are required to enhance understanding. To that end, incorporating the proposed Scene-CoT and TCogMap as training-free extensions on top of the AD-specific models results in average accuracy gains of up to 11.38% on TAD. 5.5. Ablation Studies Several ablation studies were completed, which utilized InternVL3-8B as the base model, unless noted otherwise. Additional ablations (e.g., effect of LLM selection and inference time) are provided in the supplementary materials. Ego vs. Non-ego Performance Gains: As mentioned in Sec. 3, the questions related to the ego and non-ego vehicles are relatively balanced; however, it is crucial to investigate whether the performance varies between these two question categories. Tab. 4 shows the average accuracies for ego and non-ego vehicle questions, where it can be seen that baseline VLMs achieve higher accuracy for ego-related questions. Ego-related cues are implicitly encoded in every frame through camera motion, whereas non-ego agents only appear intermittently at lower resolution, giving the model longer, more stable temporal signal for ego questions. Furthermore, while Scene-CoT drops accuracy slightly for ego questions (0.52%) compared to the baseline, it improved performance on non-ego questions more than 5%. Finally, results for TCogMap show boosts of 13.02% and 4.57% on ego and non-ego questions, respectively. These results are encouraging as they show that the additional context of the ego cars motion provides the VLM with Table 4. Average task accuracies on ego and non-ego questions. Method Ego Questions Non-Ego Question InternVL3-8B + Scene-CoT + TCogMap 55.15 54.63 68.17 43.24 48.62 47.81 hints about other objects actions and generally improves the models temporal understanding of the scene. Further analysis with other models and detailed tasks accuracies for ego vs. non-ego are given in the supplementary materials. Figure 5. Blind test results for InternvL3-8B. Blind VLM Test: This ablation examines two questions regarding the TAD benchmark: 1) How much model can infer from the question alone. 2) Whether temporal cognitive maps are sufficient for temporal reasoning in isolation. As shown in Fig. 5, four configurations were evaluated: 1) Blind: Only the question is provided to the VLM. 2) Image Only: Scene frames and question are given as input. 3) TCogMap Only: Only the temporal cognitive map and question are provided, excluding frames. 4) Image + TCogMap: All inputs (frames, TCogMap, and question) are provided together. Task accuracies in the Blind setting are the lowest among all configurations, often approaching chance-level. Comparing Blind with Image Only confirms that the benchmark questions cannot be answered Interestingly, using the TCogMap without visual input. Only setting yields higher accuracies in five tasks compared to Image Only. This result demonstrates that the proposed TCogMap effectively captures temporal dynamics even without direct visual input. Finally, combining both frames and TCogMap achieves best performance across most tasks, indicating that while TCogMap substantially enhances temporal reasoning, visual content remains essential for complete temporal understanding. 6. Conclusion and Future Work In this work, TAD, benchmark for temporal understanding in AD, is proposed. Evaluation of generalist and AD8 specific VLMs illuminates that there is still significant room for VLM temporal understanding improvement to reach human level accuracies. To address this gap, two novel, training-free solutions are proposed: Scene-CoT, generalpurpose, CoT-based reasoning approach and TCogMap, solution that leverages an ego-centric temporal cognitive map. Future work includes the extension of temporal cognitive maps to encompass other non-ego scene objects, as well as creating dataset to support the fine-tuning of VLMs for improved temporal understanding. From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Benchmark and Codes The benchmark questions and raw labels for the segments of the validation split of the NuScenes dataset are shared in the uploaded supplementary materials. The inference code, scripts, Conda environment and evaluation code are also provided. To run the scripts, please refer to the README.md file in the uploaded benchmark and codes folder. 8. TAD Benchmark: Details and Additional"
        },
        {
            "title": "Statistics",
            "content": "In the main body of the paper, fundamental statistics regarding the benchmark, such as the number of questions per task, are provided. Further analyses on the statistics of the TAD benchmark are provided in this section. 8.1. Object Statistics in TAD breakdown of the vehicle types considered in the TAD benchmark, as well as their counts, is shown in Tab. 5. The benchmark considers all vehicle categories relatively evenly, the lone exception being Emergency Vehicle, which is understandably quite rare in the original data itself. The remaining vehicle categories are well-represented, with the standard car category appearing in the most questions, at 749. It should be noted that some questions refer to NonEgo vehicles, which corresponds to any vehicle type other than the ego. 8.2. Vehicle Action Statistics in TAD To determine the distribution of vehicle actions in the benchmark, an analysis was performed on the Exact Answer Action Recognition question type, the results of which are shown in Fig. 6. The data is presented in terms of ego, non-ego, as well as the combined total. The two most dominant action categories are stopped and straight, constant speed. Notably, the ego vehicle is most commonly found Table 5. Statistics of the vehicle types in the TAD benchmark Object Type Count Object Type Count Bicycle Bus Car Construction Vehicle Emergency Vehicle 153 286 749 123 13 Motorcycle Non-Ego Trailer Truck Total 257 432 108 594 2715 Figure 6. Distribution of the actions in the Exact Answer Action Recognition question type. Shorthand notations: Lane (L): change lane to left, Lane (R): change lane to right, Turn (L): Turn left, Turn (R): Turn right, and Straight: Straight, constant speed to be traveling straight, constant speed; whereas, non-ego vehicles are most often stopped. This result is quite intuitive, as the NuScenes data generally shows the ego vehicle driving throughout cities with significant number of stationary vehicles parked in lots or along streets. Thus, these statistics reflect the underlying NuScenes data distribution. Note that in the current work, action labels were also assigned to the video segments for the first time. total of 4,481 vehicle actions were labeled during the human annotation procedure. 8.3. Vehicle Action Definition Clarification To avoid confusion, an additional note should be made regarding the definition of the actions used in TAD. In particular, the actions are defined with respect to the vehicle that is performing the action. As an example, imagine scenario where truck is traveling in the opposite direction to the ego vehicle and subsequently, the truck turns right onto side street. In TAD, the trucks action would be labeled as Turn Right, since the driver of the truck has literally turned the wheel of his/her vehicle to make right hand turn. The labels in TAD are not based on how the ego vehicle observes vehicles moving from its own perspective (i.e., in this example, within the ego vehicles front camera field of view, the truck would be seen as traveling further to the left during its turn). Defining vehicle actions in this manner presents an additional challenge for VLMs, but models that can differentiate between such actions exhibit greater untokens obtained from long sequences of frames within the models maximum context length, min pixels=256 28 28 and max pixels=512 28 28 are used. max new token=1024 is used for this model. Robotron: The parameters and prompts provided in their official examples are used 1. Cosmo-reason: The parameters provided in their official code 2 are used. Since the model receives video as input, the sampled frames in the NuScenes dataset are converted to videos with FPS=2. The stored videos are then used for inference. 9.1. Task Details and Examples In the main body of the paper, Fig. 1 provides an overview of the TAD benchmark, including listing of the seven different question types, as well as the frames and labels for several video segments. In this section, additional details and examples from TAD are given. Tab. 6 provides more detailed description for each of the seven question types, along with an explicit example (including both the question and answer) from the benchmark. In the description, some of the motivations that inspired the inclusion of question types are also listed. An additional pictorial example showing the format of questions in TAD is provided in Fig. 8. This figure is meant for visualization purposes only to understand the format of the benchmark questions. The figure highlights the different question types and the breakdown of the video into segments. The seven question types are shown in grey bubbles. Dark blue text corresponds to the questions, while red text shows the available answer options (note, not all of the options are correct) and the correct answers are bolded. Three video segments are displayed (Segment 1, Segment 2, and Segment 10), represented by three frames from the respective segment. Cyan text and arrows are used to highlight questions related to the actions of the bus; whereas orange text and arrows draw attention to the bicycle. The frame numbers displayed on the image in red (top-left corIn practice, ner) are also included only for visualization. the red numbers are not superimposed on the images that are used as input to the VLMs. 10. Scene-CoT: Additional Details In the main body of the paper, the critical details of the proposed Scene-CoT method were provided. Here, more details including the overall system diagram, shown in Fig. 9, are provided. Of note, the VLM CoT reasoning block shows the information flow during the series of CoT VLM calls. As discussed in the main paper, general scene descriptions 1https : / / github . com / zhijian11 / RoboTron - Drive / blob/main/scripts/inference_demo/demo_video.py 2https : / / github . com / nvidia - cosmos / cosmos - reason1/blob/main/scripts/inference_sample.py Figure 7. Word cloud for the questions in the TAD benchmark (excluding the instructions) derstanding of the environment, road layout, and 3D scene. 8.4. Word Cloud of Questions in the TAD Benchmark The word cloud in Fig. 7 summarizes the distribution of question terms in the benchmark after removing instructional tokens related to the answer format. The most prominent words are vehicle, motion, ego, video, segment, and action, which highlight the benchmarks emphasis on reasoning about temporal dynamics. The dominance of these words reflects the centrality of motion cues, ego-vehicle context, and temporal segmentation in the tasks, underscoring the benchmarks focus on temporal understanding in autonomous driving. 9. Parameters for the Baselines In this section, the specific parameters and settings related to the baselines in the main body are introduced. GPT-5-mini: The model is called using its default API configuration, with reasoning enabled by default and set to medium. max new token is set to 2048. Gemini-2.5-Flash: The model is called using its default API configuration, with thinking enabled by default and the thinking budget set to 1024 tokens. Gemini2.5-Flash supports video inputs. Hence, the sampled frames in the NuScenes dataset are stored as videos with FPS=2. Then, the stored videos are used for inference. max new token is set to 2048. is used to get the InternVL3 family of models. InternVL3 family: The LMDeploy inference frameresults To keep tokens produced by long seframes within the models maximum length, max dynamic patch=1 is used. work [12] for the number of visual quences of context max new token=1024 is used for this model. the inference Qwen-2.5-VL family: The LMDeploy inference framework [12] is used to get the inference results for Qwen2.5-VL family of models. To keep the number of visual 2 Task Description Example Table 6. TAD Question Types and Examples Exact Answer Action Recognition Multiple Choice Action Recognition Action Duration Temporal Ordering Temporal Action Localization Relative Temporal Action Localization Temporal Object Localization Evaluates if model can perform vehicle action recognition, on short, temporally cropped video segment. This segment-level question type isolates the VLMs ability to perform fine-grained action recognition without concerning longer term temporal understanding in full-length video. The model is expected to produce the exact textual answer from list of action options. Evaluates if model can understand the action of vehicle in short, temporally cropped video segment. This segment-level question type isolates the VLMs ability to perform fine-grained action recognition without concerning longer term temporal understanding in full-length video. Assesses if model can understand the durations of various events that transpire in video. The question asks which event lasts the longest/shortest in video and is presented with several options in multiple choice setting. The actions for this question type are all related to the ego vehicle. Determines if model can properly identify the temporal sequence of actions performed by specific vehicle in the scene. Various action sequences are presented as multiple choice options. Evaluates if model can identify the frames where specific action is being performed by vehicle. The expected output is list of the frames that correspond to the specified actions. Assesses if model can identify which of two vehicle actions happened earlier/later in the video. Whereas temporal localization exactly localizes single action, this task requires localization of two actions and determining their relative temporal ordering. The action answers are presented as multiple choice options. Evaluates if model can identify the frame ranges where specific object is present. This task disentangles the challenges of action recognition and temporal localization (i.e., to determine if the VLM can identify the frames where non-action event occurs in video). The expected output is list of the frames that correspond to the question criteria. Question: What best describes the motion of the bus visible in this video segment? Respond with exactly one full phrase from the following list: Starting, Stopping, Turn left, Turn right, Change lane to the left, Change lane to the right, Straight, constant speed, Stopped Answer: Turn left Question: What is the motion of the ego vehicle in this video segment? Respond with exactly one letter corresponding to the correct option. A. Turn left. B. Straight, constant speed. C. Stopped. D. Starting. Answer: Question: Which action did the ego vehicle spend the most time doing? Respond with exactly one letter corresponding to the correct option. A. Straight, constant speed. B. Starting. C. Stopping. D. Stopped. Answer: Question: Which of the following represents the correct temporal order of motions for the ego vehicle? Respond with exactly one letter corresponding to the correct option. A. Starting; Straight, constant speed; Stopping. B. Stopping; Starting; Straight, constant speed; C. Starting, Stopping; Straight, constant speed; D. Stopping; Straight, constant speed; Starting. Answer: Question: In which frames is the ego vehicle doing action stopped? Respond with an explicit list of all frame numbers, enumerating each value individually, without summarizing as range. Answer: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Question: Which ego vehicle event happened earlier in the video? Respond with exactly one letter corresponding to the correct option. A. Stopping. B. Starting. Answer: Question: In which frames are there any trucks visible to the ego vehicle? Respond with an explicit list of all frame numbers, enumerating each value individually, without summarizing as range. Answer: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17] are first generated using uniformly sampled sub-set of frames in each segment. Using these scene descriptions as additional context, the VLM is prompted to describe the ego vehicle motion and subsequently the nearby vehicles 3 Figure 8. Pictorial visualization example of the question types and format in the TAD benchmark. Figure 9. Overview of the proposed Scene-CoT method. motion. The VLM also receives the additional context of the ego vehicle motion descriptions, as well as the scene descriptions, when it is prompted to describe the motion of the nearby vehicles. The final step in the CoT generates JSON-formatted output summary using the ego and nearby motion descriptions as contextual information. Finally, to perform question-answering, the LLM is provided with three inputs: 1) The original question. 2) The general Scene Description. 3) The JSON-style motion summary. concrete example from TAD showing the series of CoT prompts, along with the VLMs outputs is shown in Fig. 10. 11. Additional Ablation Studies In the main body of the paper, ablations were presented that showed accuracy breakdowns for ego and non-ego questions, as well as Blind VLM test. In this section, results for four additional ablation experiments are presented. The ablation experiments are as follows: 1) Ego vs. Non-Ego Results. Additional details and results further analyzing model accuracies on ego and non-ego questions. 2) Captioning Style. Ablations on the prompting style used in Scene-CoT, for creating the segment descriptions. 3) QA LLM Selection. An ablation experiment considering the effect of the 4 Figure 10. Exact prompts and sample output with Scene-CoT for question in TAD. 5 specific LLM used in Scene-CoT. 4) Inference Time. An analysis of inference time for the methods. 11.1. Ego. vs Non-Ego Performance Gains: Additional Results and Analysis As mentioned in the main body of the paper, the questions related to the ego and non-ego vehicles are relatively balanced; however, it is still important to consider whether the performance varies between these two question categories. In the main body of the paper, Tab. 4 shows summarized version of the results for the ego vs. non-ego vehicle questions by presenting the average accuracies across all question types for the InternVL3-8B model. Here, these results are significantly expanded upon along two dimensions: 1) Additional models. The ego vs. non-ego breakdown is provided for six additional models (InternVL314B, InternVL3-38B, Qwen2.5-VL-7B, Qwen2.5-VL-32B, RoboTron, and Cosmos-Reason). This set of models forms the complete set of open-source generalist and specialist models that were considered in Tab 3 in the main body. 2) Task-Wise Accuracies. The ego vs. non-ego question accuracies are provided on per-type basis, permitting more fine-grained results analysis. The detailed ego vs. non-ego vehicle accuracies are presented in Tab. 7 and Tab. 8. The format of these two tables matches that of the main results table in the body of the paper, but each table only evaluates sub-set of the questions (i.e., ego questions in Tab. 7 and non-ego question in Tab. 8). Note that N/A in column indicates that the corresponding task does not include either ego-related or non-ego-related questions. comparison of the results in Tab. 7 and Tab. 8 shows that the impact of Scene-CoT varies notably across models. While some models, such as Qwen2.5-VL-7B, exhibit substantially larger gains on non-ego vehicle queries (+8.11% vs. +3.38% for ego), others display the opposite pattern. For instance, RoboTron benefits most on ego-centric questions (+10.81%), suggesting that certain models are more capable of exploiting Scene-CoTs structured reasoning for inferring ego-motion cues. With TCogMap, Tab. 7 shows significant accuracy improvements on ego-related questions (typically around 15%) and interestingly, more modest increases are still generally observed in Tab. 8 for non-ego questions. These results indicate that incorporating the ego vehicles motion offers useful contextual cues about the behavior of surrounding objects, leading to more reliable temporal reasoning by the VLM. 11.2. Additional Experiments on CoT Descriptions for Scene-CoT In the main body of the paper, details of the four-step CoT reasoning process were described, providing motivation for dividing the problem into sub-tasks that focus on general scene descriptions, ego vehicle motion, motion of nearby vehicles and summarizing/formatting. However, one may question the following: Why was this four step process selected? and How sensitive are the benchmark results to the specific CoT prompts that are used? These questions are answered with this ablation study. Table 9 compares different styles of descriptions generated during CoT reasoning for Scene-CoT. Three configurations were considered. With the Scene Description configuration, only the first step from the four-step CoT reasoning process was used to describe the video segments. In other words, for each video segment, only high-level scene description was used for the LLM-based QA. This scenario can essentially be thought of as CoT-free approach, where general video description or caption is produced. In the second configuration, CoT Output Summary, only the final step of CoT reasoning was used for the segment descriptions during QA. The output from Step 4 is formatted summary derived from the first three steps, so it is expected to be relatively complete. Finally, the Scene Description + CoT Output Summary configuration concatenates the output of Step 1 and Step 4 to form the segment descriptions. This is the configuration used for the results reported in the main body of the paper. The average accuracies for these three segment description methods are shown in Table 9. The descriptions that consist of scene descriptions alone are relatively lower at 46.26%. General scene descriptions are likely too impoverished to properly answer the breadth of questions considered in TAD. General scene descriptions tend to focus on the overall environment along with few prominent scene objects. However, the question set of TAD focuses on many aspects of the scene that may not appear in general description: vehicle actions, less prominent vehicles in the background, and ego vehicle actions. Using the final output from the CoT procedure shows significant improvement, which is derived from richer summary description that includes details of both the ego vehicles and other scene vehicles motions. However, best results are achieved when both the scene description and final CoT output are concatenated. This trend indicates that explicit scene-level context complements the structured reasoning provided by CoT, yielding more comprehensive representations for downstream question answering. 11.3. Inference Time Analysis While inference time was not primary focus of our evaluation, we further analyzed the inference time for completeness. To measure the inference time, roughly 200 questions across three different question types (Ego Action Duration, Relative Temporal Localization, and Temporal Ordering) were used, the results for which are shown in Tab. 10 Four configurations were considered: the two baselines 6 Table 7. Results of ego-related questions for openand closed-source generalist VLMs as well as AD-specific VLMs. Bold and underline indicate best and second-best results when comparing the 3 system variants for each open-source VLM. O/Gen : open-source generalist models, O/Drv : open-source driving specialist models. EA: exact answer, MC: multiple choice. Method Model Type EA Act. Recog. MC Act. Recog. Action Dur. Temp. Ord. Temp. Act. Local. Rel. Temp. Local. Temp. Obj. Local. Qwen2.5-VL-7B O/Gen + Scene-CoT + TCogMap Qwen2.5-VL-32B O/Gen + Scene-CoT + TCogMap InternVL3-8B + Scene-CoT + TCogMap InternVL3-14B + Scene-CoT + TCogMap InternVL3-38B + Scene-CoT + TCogMap RoboTron + Scene-CoT + TCogMap Cosmos-Reason + Scene-CoT + TCogMap O/Gen O/Gen O/Gen O/Drv O/Drv 63.53 65.13 79.67 63.47 67.93 80.53 71.60 67.60 79.13 68.87 68.60 80.53 55.67 70.00 80. 58.27 71.07 65.73 66.73 65.87 80.13 58.93 56.60 79.32 55.63 60.58 78.35 62.33 58.93 76.50 60.87 59.51 77.67 65.73 62.23 78.06 71.26 64.17 77.77 61.55 57.96 79. 34.68 52.42 50.81 47.58 46.77 58.87 53.23 51.61 57.26 52.42 51.61 58.06 45.97 53.23 60.48 4.84 52.42 37.90 39.52 55.65 50.00 56.45 54.84 62.90 58.06 54.84 74.19 51.61 56.45 72.58 59.68 54.84 80.65 53.23 56.45 74. 43.55 58.06 58.06 54.84 59.68 72.58 35.00 37.69 48.87 46.09 36.31 46.23 39.98 42.08 56.15 43.52 42.80 55.83 46.15 44.48 51.26 29.78 37.70 46.46 16.66 34.78 40. 46.74 48.91 69.57 60.87 44.57 78.26 52.17 51.09 67.39 59.78 51.09 75.00 71.74 55.43 80.43 65.22 54.35 79.35 64.13 55.43 67.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A Avg 49.22 52.60 65.19 55.81 51.83 69.83 55.15 54.63 68.17 57.52 54.74 71.29 56.42 56.97 70.83 45.49 56.30 60. 50.57 54.90 64.99 Table 8. Results of non-ego questions for openand closed-source generalist VLMs as well as AD-specific VLMs. Bold and underline indicate best and second-best results when comparing the 3 system variants for each open-source VLM. O/Gen : open-source generalist models, O/Drv : open-source driving specialist models. EA: exact answer, MC: multiple choice. Method Model Type EA Act. Recog. MC Act. Recog. Action Dur. Temp. Ord. Temp. Act. Local. Rel. Temp. Local. Temp. Obj. Local. O/Gen Qwen2.5-VL-7B + Scene-CoT + TCogMap Qwen2.5-VL-32B O/Gen + Scene-CoT + TCogMap InternVL3-8B + Scene-CoT + TCogMap InternVL3-14B + Scene-CoT + TCogMap InternVL3-38B + Scene-CoT + TCogMap RoboTron + Scene-CoT + TCogMap Cosmos-Reason + Scene-CoT + TCogMap O/Gen O/Gen O/Gen O/Drv O/Drv 26.36 47.10 39.86 56.61 50.27 60.05 37.77 55.62 37.23 40.58 53.17 46.47 43.93 55.07 48.64 23.73 41.85 24. 28.35 38.77 28.53 40.61 42.72 47.35 50.00 45.24 55.29 56.48 48.02 53.31 43.92 45.11 48.41 48.28 46.96 54.89 40.34 37.30 42.33 32.80 34.13 44.05 27.78 55.56 50.00 50.00 55.56 33. 44.44 55.56 44.44 44.44 61.11 38.89 61.11 61.11 66.67 33.33 55.56 33.33 44.44 61.11 50.00 35.50 25.35 44.23 44.08 27.84 37.65 31.86 33.57 39.98 43.53 31.76 45.87 44.84 33.42 39.23 31.95 24.68 35. 16.18 24.10 33.46 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A NA N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 45.60 45.65 57.97 62.91 48.99 62. 45.64 50.34 64.10 43.90 51.64 59.00 68.27 53.77 66.43 28.92 34.79 40.58 16.06 43.72 23.97 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 7 Avg 35.17 43.28 47.88 52.72 45.58 49.85 43.24 48.62 47.81 43.27 48.56 47.73 53.29 50.07 55.17 31.65 38.84 35. 27.57 40.37 36.00 Table 9. Effect of using different sections of the CoT outputs to form the segment descriptions. Description Style Average Accuracy Scene Description CoT Output Summary Scene Description + CoT Output Summary 46.26 51.13 51. Table 10. Average inference time for the baselines and proposed solutions."
        },
        {
            "title": "Method",
            "content": "Inference time (s) InternVL3-8B InternVL3-8B + Ego pose + Scene-CoT + TCogMap 2.20 2.72 47.10 2.21 (with and without additional ego pose context) and the two proposed solutions (Scene-CoT and TCogMap). As the table shows, the baselines and TCogMap have quite comparable inference times. This result is due to the fact that the image tokens for all three methods are identical but the textual tokens are slightly different, depending on the specific contextual information that is added by the method. TCogMap has the additional overhead of computing the ego vehicle temporal cognitive map and classifying the ego vehicle actions, but these processing steps have negligible effect on latency. For Scene-CoT, on the other hand, much higher inference time is observed. This large inference time is primarily due to the increased number of VLM inference calls, since CoT descriptions are provided for each segment of the video. Specifically, for the experiments in this paper, the videos are divided into ten segments, each of which is described using four CoT processing steps, yielding total of 40 VLM inference calls. This design results in an inference time of 42 seconds per question to generate the segment descriptions. In comparison, the actual LLM question-answering inference time is rather minor, at under 5 seconds per question. Although this run-time of Scene-CoT can be viewed as disadvantage, there is significant ongoing research on VLM inference optimization, including quantization (e.g., [15, 16, 19, 46]) and pruning [1, 8, 32], that can provide significant speed improvements with limited accuracy losses. Moreover, the experiments in this paper were performed in single inference setting, but batch inference could be used to parallelize the description generation across the segments, resulting in significant inference speed improvements (i.e., up to 10, in the case of the experiments presented in this paper). Thus, there is significant scope to reduce the runtime of Scene-CoT. 11.4. Effect of LLM on Scene-CoT In the main body of the paper, the results for SceneCoT were generated using Qwen2.5-14B-Instruct-1M as the LLM for question answering. This LLM was selected through an empirical evaluation process. This procedure, as well as the effects of utilizing different LLMs, are described next. In Tab. 11, the effect of changing the question answering LLM for Scene-CoT is shown. clear accuracy improvement of 2.21% was observed when moving from the 7B Qwen2.5-7B-Instruct to the comparable 14B model, Qwen2.5-14B-Instruct. This observation motivated the selection of 14B LLM that can offer greater reasoning power. Among the 14B models evaluated, Qwen2.514B-Instruct-1M achieves the highest average accuracy of 51.92%. The comparison between the standard and 1M versions of the 14B model indicates that extended context capacity and stronger instruction tuning improve reasoning over temporally grounded inputs. Additionally, Qwen314B with its thinking mode enabled achieves comparable accuracy to Qwen2.5-14B-Instruct-1M; however, Qwen2.514B-Instruct-1M was ultimately selected due to its higher throughput. Overall, the trends indicate that there is 3-4% improvement when moving to the larger 14B LLMs and that amongst the 14B models, the accuracies are fairly comparable. Table 11. Effect of using different LLMs for question answering in Scene-CoT. Average accuracy is computed over all seven tasks."
        },
        {
            "title": "Average Accuracy",
            "content": "Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-14B-Instruct-1M Qwen3-14B (thinking enabled) 47.87 50.08 51.92 51.05 12. Data Annotation Interface Fig. 11 illustrates the annotation interface used to collect vehicle-action labels for TAD. Since labeling the actions of specific vehicles, including the ego vehicle, requires level of granularity not supported by existing annotation tools, custom interface is implemented. Each video segment is displayed alongside its corresponding birds-eyeview (BEV) visualization, which shows the ego trajectory and the motion of nearby vehicles. Annotators view both modalities jointly to assess the dominant behavior of each target vehicle within the segment. drop-down menu is provided for selecting the appropriate motion label for the target object, and the final annotation is saved through the interface. This setup ensures consistent access to both ap8 scene that appears in Fig. 15. Note that for the baseline and TCogMap approaches, question answering is performed via VLM, whereas for Scene-CoT, the final step of answering the question is completed by an LLM. As Fig. 16 shows, the prompt for the baseline method is straightforward, consisting merely of the frame labels and corresponding image token tags, as well as the question text itself. TCogMap extends the baseline prompt by adding motion summary, derived from the temporal cognitive map, for each segment of the video. As shown quantitatively in the main body of the paper, this additional context significantly improves the average accuracy on TAD for all of the VLMs that were considered. For Scene-CoT, since the prompt is being used with an LLM, the reference to frames and image tokens are removed. Instead, the prompt consists of the question itself along with textual descriptions for each video segment. These descriptions were formed using the proposed CoT procedure and include scene description as well as JSON-style summary. As shown in the main body of the paper, Scene-CoT, with its CoT reasoning process and LLM-based question answering, generally improves accuracies on the TAD benchmark for smaller models. pearance and trajectory cues, facilitating reliable segmentlevel action labeling. 13. Qualitative Examples Figures 1215 present qualitative examples corresponding to Tasks Multiple Choice Action Recognition, Exact Answer Action Recognition, Action Duration, and Relative Temporal Action Localization, respectively, complementing the quantitative results in the main table of the paper. Each example includes representative frames from the relevant segment or full scene, the question posed, the groundtruth answer, and the model (i.e, InternVL3-8B) outputs under different inference settings. The baseline corresponds to the model receiving only the frames and the question. The +Scene-CoT setting applies the proposed Scene-CoT reasoning procedure, while +TCogMap shows the outputs produced using the proposed TCogMap. Across tasks, the qualitative results show that applying Scene-CoT or TCogMap improves the models predictions, often correcting cases where the baseline fails, underscoring the value of both methods for strengthening temporal and scene-level reasoning. In more detail, in Fig. 12, the ego vehicle goes straight toward barrier, which then lifts to allow the ego vehicle to pass. The baseline selects an incorrect answer, whereas both Scene-CoT and TCogMap produce the correct output. In Fig. 13, the query concerns the motion of the car visible in the frames. Because the scene occurs at night and the rear lights of the vehicle are illuminated red, both the baseline and Scene-CoT incorrectly predict that the car is Stopped. In contrast, TCogMap leverages the ego trajectory and scene dynamics to correctly identify the action of the vehicle ahead as Straight, constant speed. In Fig. 14, the video shows the ego vehicle first approaching parked bus. The ego vehicle performs long, gradual lane change to the right to avoid the bus, followed by quick lane change back to the left. The baseline and Scene-CoT both predict incorrect answers, whereas TCogMap is able to differentiate between the left and right lane changes to provide the correct answer of Change lane to the left. In Fig. 15, the video first shows the ego vehicle driving straight down city road. Thereafter, the ego vehicle performs series of short, consecutive atomic actions: lane change to the left, followed by left hand turn around corner, and finally lane change to the right. The baseline incorrectly selects Change lane to the right as its answer. In contrast, SceneCoT and TCogMap are both able to correctly assess that the ego vehicle drives straight at the beginning of the video, while the lane change to the right occurs near the end, thus allowing them both to provide the correct answer of B. sample of the exact prompts used during the question answering step for the baseline and two proposed approaches is shown in Fig. 16, depicting the same video 9 Figure 11. Data annotation pipeline. Figure 12. Qualitative example for Multiple Choice Action Recognition task using the baseline (InternVL3-8B) and the proposed methods. Green text indicates correct answer, red denotes an incorrect answer. 10 Figure 13. Qualitative example for Exact Answer Action Recognition task using the baseline (InternVL3-8B) and proposed methods. Green text indicates correct answer, red denotes an incorrect answer. Figure 14. Qualitative example for Action Duration task using the baseline (InternVL3-8B) and proposed methods. Green text indicates correct answer, red denotes an incorrect answer. 11 Figure 15. Qualitative example for Relative Temporal Action Localization task using the baseline (InternVL3-8B) and proposed methods. Green text indicates correct answer, red denotes an incorrect answer. 12 Figure 16. Prompts used for question answering. Note that for the baseline and TCogMap, these prompts are provided to the VLM as input, whereas for Scene-CoT, this prompt is given to the LLM."
        },
        {
            "title": "References",
            "content": "[1] Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversity-based visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 93929401, 2025. 8 [2] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From arXiv physical common sense to embodied reasoning. preprint arXiv:2503.15558, 2025. 7, 8 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 7 [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 2 [5] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 2, 3 [6] Mu Cai, Reuben Tan, Jianrui Zhang, Bocheng Zou, Kai Zhang, Feng Yao, Fangrui Zhu, Jing Gu, Yiwu Zhong, Yuzhang Shang, et al. Temporalbench: Benchmarking finegrained temporal understanding for multimodal video models. arXiv preprint arXiv:2410.10818, 2024. [7] Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, et al. Automated evaluation of large vision-language models on self-driving corner cases. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 78177826. IEEE, 2025. 3 [8] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 8 [9] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability. 2024. 2 [10] Yuan Chen, Zi-han Ding, Ziqin Wang, Yan Wang, Lijun Zhang, and Si Liu. Asynchronous large language model enhanced planner for autonomous driving. In European Conference on Computer Vision, pages 2238. Springer, 2024. 2 [11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 7 [12] LMDeploy Contributors. Lmdeploy: toolkit for compressing, deploying, and serving llm. https://github. com/InternLM/lmdeploy, 2023. [13] Chaorui Deng, Shizhe Chen, Da Chen, Yuan He, and Qi Wu. Sketch, ground, and refine: Top-down dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 234243, 2021. 2 [14] Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. Holistic autonomous driving understanding by birds-eye-view injected multi-modal large In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 13668 13677, 2024. 2 [15] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118, 2024. 8 [16] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Gptq: Accurate post-training quantization arXiv preprint Alistarh. for generative pre-trained transformers. arXiv:2210.17323, 2022. 8 [17] Christian Fruhwirth-Reisinger, Duˇsan Malic, Wei Lin, David Schinagl, Samuel Schulter, and Horst Possegger. Stsbench: spatio-temporal scenario benchmark for multi-modal large language models in autonomous driving. 2025. [18] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 2 [19] Mohsen Gholami, Mohammad Akbari, Kevin Cannons, and Yong Zhang. Casp: Compression of large multimodal models based on attention sparsity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 93729381, 2025. 8 [20] Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Sitong Mao, Shunbo Zhou, Yong Zhang, and Mohammad Akbari. Spatial reasoning with vision-language models in ego-centric multi-view scenes. arXiv preprint arXiv:2509.06266, 2025. 3, 5 [21] Xianda Guo, Ruijun Zhang, Yiqun Duan, Yuhang He, Dujun Nie, Wenke Huang, Chenming Zhang, Shuai Liu, Hao Zhao, and Long Chen. Surds: Benchmarking spatial understanding and reasoning in driving scenarios with vision language models. 2024. 3 [22] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. In European Conference on Computer Vision, pages 202218. Springer, 2024. 2, 6 [23] Zhijian Huang, Chengjian Feng, Feng Yan, Baihui Xiao, Zequn Jie, Yujie Zhong, Xiaodan Liang, and Lin Ma. Robotrondrive: All-in-one large multimodal model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 80118021, 2025. 7, 14 [24] Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal, et al. Drivelmm-o1: step-by-step reasoning dataset and large multimodal model for driving scenario understanding. arXiv preprint arXiv:2503.10621, 2025. 3 [25] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving vehicles. In Proceedings of the European conference on computer vision (ECCV), pages 563578, 2018. 2 [26] Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim. Do you remember? dense video captioning with cross-modal memory retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1389413904, 2024. 2 [27] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 2 [28] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [29] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2 [30] Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, Yueyi Zhang, Zhiwei Xiong, and Xinhai Zhao. Fine-grained evaluation of large vision-language models in autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9431 9442, 2025. 2, 3 [31] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding? 2025. 2, 3 [32] Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. Boosting multimodal large language models with visual toIn Proceedings of kens withdrawal for rapid inference. the AAAI Conference on Artificial Intelligence, pages 5334 5342, 2025. 8 [33] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? 2024. 2 [34] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. In European Conference on Computer Vision, pages 403420. Springer, 2024. 2 [35] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. Drama: Joint risk localization and captioning in driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 10431052, 2023. [36] Ana-Maria Marcu, Long Chen, Jan Hunermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question answering for autonomous drivIn European Conference on Computer Vision, pages ing. 252269. Springer, 2024. 2, 3 [37] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, and Li Zhang. Reason2drive: Towards interpretable and chain-based reasoning for auIn European Conference on Computer tonomous driving. Vision, pages 292308. Springer, 2024. 2 [38] OpenAI. Gpt-5. https://openai.com/gpt5/, 2025. 6, 7 [39] Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup Mallik, Alessandro Allievi, Senem Velipasalar, and Liu Ren. Vlp: Vision language planning for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1476014769, 2024. [40] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 45424550, 2024. 3 [41] Enna Sachdeva, Nakul Agarwal, Suhas Chundi, Sean Roelofs, Jiachen Li, Mykel Kochenderfer, Chiho Choi, and Behzad Dariush. Rank2tell: multimodal driving dataset for joint importance ranking and reasoning. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 75137522, 2024. 3 [42] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. 2 [43] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2616026169, 2025. 2 [44] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European conference on computer vision, pages 256274. Springer, 2024. 2, 3 [45] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. 2024. 2 [46] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. 8 [47] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. 2024. 2 15 [59] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, 2018. 2 [60] Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1824318252, 2024. 2 [61] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 6, [48] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. Omnidrive: holistic vision-language dataset for autonomous driving with counterfactual reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2244222452, 2025. 2 [49] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end dense video captioning In Proceedings of the IEEE/CVF with parallel decoding. international conference on computer vision, pages 6847 6857, 2021. 2 [50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 4 [51] Dongming Wu, Wencheng Han, Yingfei Liu, Tiancai Wang, Cheng-zhong Xu, Xiangyu Zhang, and Jianbing Shen. LanIn Proceedings of guage prompt for autonomous driving. the AAAI Conference on Artificial Intelligence, pages 8359 8367, 2025. 3 [52] Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, and Xu Yang. Number it: Temporal grounding videos like flipping manga. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1375413765, 2025. 2 [53] Shaoyuan Xie, Lingdong Kong, Yuhao Dong, Chonghao Sima, Wenwei Zhang, Qi Alfred Chen, Ziwei Liu, and Liang Pan. Are vlms ready for autonomous driving? an empirical study from the reliability, data and metric perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 65856597, 2025. 3 [54] Zhenhua Xu, Yan Bai, Yujia Zhang, Zhuoling Li, Fei Xia, Kwan-Yee Wong, Jianqiang Wang, and Hengshuang Zhao. Drivegpt4-v2: Harnessing large language model capabilities for enhanced closed-loop autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1726117270, 2025. [55] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1071410726, 2023. 2 [56] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. 6 [57] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. 5 [58] Luowei Zhou, Nathan Louis, and Jason Corso. Weaklysupervised video object grounding from text by loss weighting and object interaction. arXiv preprint arXiv:1805.02834, 2018."
        }
    ],
    "affiliations": [
        "Huawei Cloud",
        "Huawei Technologies Canada Co., Ltd."
    ]
}