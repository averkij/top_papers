{
    "paper_title": "Robust and Calibrated Detection of Authentic Multimedia Content",
    "authors": [
        "Sarim Hashmi",
        "Abdelrahman Elsayed",
        "Mohammed Talha Alam",
        "Samuele Poppi",
        "Nils Lukas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques."
        },
        {
            "title": "Start",
            "content": "Sarim Hashmi1 Abdelrahman Elsayed1 Mohammed Talha Alam1 Samuele Poppi1 Nils Lukas1 1Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) {sarim.hashmi, abdelrahman.elsayed, mohammed.alam, samuele.poppi, nils.lukas}@mbzuai.ac.ae 5 2 0 2 7 1 ] . [ 1 2 8 1 5 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose resynthesis framework to determine if sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, lowrecall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages stateof-the-art inversion techniques. 1. Introduction Recent advances in large-scale generative models have led to notable improvements in the photorealism of synthesized media across modalities. Text-to-image and diffusion families (e.g., DALL-E variants [27] and latent diffusion models [29]) now produce images that are frequently hard to distinguish from real images; similar progress has rapidly followed in audio [8] and video generation [25] These advances enable valuable creative and assistive applications, but they also erode core assumption of digital forensics: namely, that post-hoc analysis can reliably separate authentic from synthetic content. The rise of highly capable, finetuned generators, and the ease with which adversaries can adapt them, therefore poses two linked challenges for content authenticity: (i) resynthesis indistinguishability (generators can reproduce real content with extremely high fidelity) and (ii) robustness (small, imperceptible manipulations can defeat many detectors). As result, simply asking is this image fake? is no longer reliable approach. two into Existing defenses categories: fall watermarking-based methods and post-hoc detection. watermarking adds authenticity signals during generation but requires model changes and is vulnerable to removal and distribution-level attacks. Post-hoc detectors instead look for residual artifacts left by generators; while effective against earlier GANs [14], these methods suffer from brittle generalization to new image generation methods [2, 6, 24, 37, 40, 42], high false positive/negative rates on in-the-wild data [44], and poor robustness to adversarial perturbations and common image transformations [5]. For many applications, especially those requiring high trust, it is important to bound the false positive rate (FPR); however, doing so with traditional detectors is often infeasible unless we operate in high-precision, low-recall regime. We therefore propose an alternative approach: moving from binary real vs. fake detection to calibrated notion of authentic vs. plausibly deniable. An image is considered authentic only if its origin can be confidently verified as real; otherwise either because it is synthetic or because it can be faithfully resynthesized by existing generators it falls into the plausibly deniable region. Many images cannot be definitively labeled as fake or authentic. Instead of forcing binary decisions, we aim to provide calibrated, interpretable risk scores. We introduce an authenticity index computed from inversion and resynthesis statistics that quantify how modern generator can reproduce an input. An images authenticity is labeled plausibly deniable if generator inputs can be found that reproduce its content with high perceptual similarity, meaning its authenticity cannot be established post-hoc regardless of the true origin. Our index aggregates complementary perceptual metrics (pixel-level, structural, and semantic similarity) to produce more robust calibrated separation between easily-inverted and hard-to-invert content. We adopt reconstruction-free inversion [31] and we explicitly evaluate robustness under adversarial perturbations tailored to the inversion pipeline. Empirically, we show that this inversion-based authenticity index (i) generalizes better than several recent detectors under realistic distribution shifts, (ii) is substantially more robust to targeted adversarial perturbations that break conventional detectors, and (iii) is modality-agnostic, with an extension demonstrated on videos. As key part of our evaluation, we validate the method on large-scale social media corpus ( 3,000 images scraped from public Reddit communities), demonstrating the real-world prevalence of plausibly deniable content and highlighting the practical risks of post-hoc authenticity verification. In this work, we aim to bridge the gaps in understanding and improving the detection of synthetic content. Our key contributions are as follows: We formalize plausible deniability as an operational notion of authenticity, and introduce an authenticity index that operationalizes this paradigm through modern inversion and resynthesis. We propose adversarial objectives tailored for inversion pipelines and evaluate robustness under realistic threat model. We evaluate at scale, including social-media study and an video modality extension, and show superior generalization and robustness compared to recent baselines. Together, these contributions offer principled, practically useful alternative to brittle binary detectors: instead of chasing an absolute fake/real label that may not exist, we provide calibrated assessment of whether an images authenticity can be established post-hoc. 2. Related Work Detection of AI-generated images has been approached through passive forensic detection and latent inversion techniques. In this section, we review key prior work in each area, highlighting their strengths, limitations, and robustness issues. 2.1. Post-hoc Detection Methods Post-hoc detection methods analyze subtle forensic artifacts without altering image content. Early works utilized spectral anomalies and sensor noise fingerprints, such as PhotoResponse Non-Uniformity (PRNU) [12, 19]. Recent approaches primarily employ deep learning models trained on large datasets, notably FaceForensics++ [30] and the DeepFake Detection Challenge (DFDC) [10]. Models based on architectures like XceptionNet and vision transformers have significantly advanced detection capabilities [30]. However, these methods often suffer from high false-positive rates and poor generalization to unseen generative architectures [7]. Furthermore, passive detectors are vulnerable to adversarial attacks and image transformations commonly employed on social media, challenging their real-world applicability [12]. Emerging zero-shot methods, such as ZED aim to generalize better by training only on real images, thus providing promising direction for handling unknown generative models [7]. Despite these improvements, the evolving nature of generative models and adversarial strategies continuously challenges passive detection efficacy. 2.2. Inversion-Based Detection Methods Inversion-based detection approaches reconstruct suspect images into generative models latent space to assess authenticity. This concept leverages the notion that synthetic images typically reconstruct with lower errors compared to real images [49]. Methods like LatentTracer formalize inversion-based detection by leveraging gradient-based optimization to distinguish images generated by specific diffusion models from real images or those generated by other models [43]. Similarly, GAN inversion techniques have been employed to detect GAN-generated images based on latent-space reconstruction fidelity [49, 50]. Despite promising results, inversion-based methods require direct model access, limiting their scalability across diverse or unknown generative models. Additionally, overlapping distributions between real and synthetic images, as well as varying scene complexity, can introduce significant false positives or negatives, complicating their practical implementation [43, 49]. Collectively, these diverse methodologies illustrate substantial progress but also underscore persistent challenges in reliably identifying AI-generated imagery. Our work aims to bridge these gaps, proposing an integrated framework that addresses the inherent limitations of existing methods to enhance detection robustness in real-world scenarios. 3. Methodology 3.1. Preliminaries In this section, we review the core technical ingredients that underpin our Authenticity Index (A-index), by introducing foundational concepts like diffusion generative modeling, latent-space inversion , and the reconstruction-free inversion paradigm. Diffusion Generative Modeling. Diffusion models [16] construct forward stochastic noising process and learned reverse denoising process that together transform simple Gaussian noise into complex images. Concretely, given data sample x0 pdata, the forward process defines xt = (cid:112)1 βt xt1 + (cid:112)βt ϵt1, ϵt1 (0, I), 2 Figure 1. Conceptual illustration of the Authenticity Index. (A) Traditional post-hoc detectors separate real from fake using feature cues but struggle as generators improve. (B) Our method instead asks: can generator resynthesize the query image x? The similarity s(x, x) between and its inversion is calibrated into the Authenticity Index, where high similarity implies plausible deniability and low similarity indicates likely authentic. This allows us to reliably identify authentic content the generator is unlikely to have generated. for = 1, . . . , , where {βt} is prescribed variance schedule [16]. The generative reverse process is parameterized by neural network ϵθ(xt, t) and models: pθ(xt1 xt) = (cid:0)xt1; µθ(xt, t), Σθ(xt, t)(cid:1), with µθ and Σθ chosen so that sampling from pθ iteratively from xT (0, I) recovers x0 in distribution. These models have achieved photorealism on par with GANs while offering exact likelihoods and stable training dynamics [29]. Importantly for our work, the latent trajectory {xt} admits inversion, enabling techniques that map given image back to an initial noise za property we will leverage in the next subsection on reconstruction-free inversion. Reconstruction-Free Inversion for Authenticity Assessment. Among inversion methods, we focus on reconstruction-free inversion [31] (RF-Inversion), which quantifies generators ability to reproduce an images defining features without performing full pixel-level reconstruction. Let Ψ denote feature extractor (e.g., Fourier magnitude spectra or wavelet coefficients). Given an image x, RF-Inversion applies an approximate inverter (cid:101)G1 (which may be shallow encoder). Formally, we define the feature discrepancy as Ψ = (cid:13) (cid:13)Ψ(x) Ψ(cid:0) (cid:101)G1(x)(cid:1)(cid:13) (cid:13). small Ψ indicates that the generator can replicate the images core features, suggesting reduced authenticity. By focusing on feature distances rather than pixel reconstruction error, RF-Inversion avoids costly gradient-based optimization in pixel space and dramatically reduces computation [39]. This makes it well suited for large-scale screening of images to assess whether they are likely produced by given generative model. We employ different techniques to invert images based on the level of interpretability and knowledge of the models. 3 Similarity Metrics for Authenticity Assessment. To quantify the similarity between an image and its inverted counterpart, we rely on standard perceptual and signalbased metrics, like PSNR [13], SSIM [41], LPIPS [47], and CLIP similarity [26] , The corresponding formulas are provided in the supplementary material. Figure 2. Distribution of A-index(x, x) for fake image and real image. 3.2. From Binary Detection to Calibrated Authenticity Traditional deepfake detection approaches treat authenticity as binary decision: an image is either real or fake ( Figure 1). However, as discussed in Section 1, this framing is increasingly unreliable: some real images invert poorly and appear synthetic, while some generated images invert easily and appear real. We propose calibrated notion of authenticity that reflects this ambiguity. Instead of forcing every image into binary label, we distinguish between (1) Authentic: images that remain consistently stable under inversion, producing reconstructions above calibrated threshold, (2) Plausibly deniable: all other cases, including generated images and real images whose inversion yields reconstructions that are too easily reproduced by the generator. Figure 3. Architecture of the Authenticity Index (A-Index). Given an input image x, reconstruction-free inverter G1 produces an inverted reconstruction x. We then compute complementary similarities between and x: pixel fidelity(PSNR), structural fidelity(SSIM), perceptual distance(1LPIPS), and semantic consistency (CLIP cosine). calibrated weighted combiner (learned α1, α2, α3, α4) produces scalar s(x, x), yielding the A-Index in [0, 1]. safety threshold τ certifies content as Authentic when A-Index τ , and otherwise labels it as Plausibly Deniable. We further analyze robustness by applying ℓ-bounded perturbations δ (PGD-style) through G1 to maximize or minimize the A-Index. This framing aligns with the operational limits of posthoc forensics: if generator can reproduce an input with high perceptual fidelity, the contents authenticity cannot be guaranteed, regardless of its true origin. Our methodology therefore aims not to declare an absolute binary verdict, but to provide calibrated score that separates authentic from plausibly deniable content. 3.3. Authenticity Index Having introduced RF-Inversion and similarity metrics, we now define the Authenticity index, composite score designed to quantify how well an image aligns with its inverted counterpart. The index is built as weighted combination of complementary image quality measures, ensuring that both low-level fidelity and high-level semantic consistency are captured. Formally, given an image and its inverted reconstruction (cid:101)x, we compute four similarity measures: PSNR, SSIM, inverted LPIPS (i.e., 1LPIPS), and CLIP similarity. These are aggregated into linear combination s(x, x) = α1 PSNR(x, x) + α2 SSIM(x, x) + α3 (1 LPIPS(x, x)) + α4 CLIP(x, x). (1) where α1, . . . , α4 are scalar weights. To normalize and bound the score, we apply sigmoid transformation with scale parameter σ: A-index(x, (cid:101)x) = exp(cid:0)σ (s(x, (cid:101)x)(cid:1) 1 + exp(cid:0)σ (s(x, (cid:101)x)(cid:1) . The weights α1, . . . , α4 are selected through an optimization procedure that minimizes the overlap between score distributions of real vs. inverted real images and fake vs. inverted fake images. To do that, we adopt differential evolution, global optimization algorithm that allows both positive and negative weights to capture different relationships between similarity metrics and inversion quality( Figure 2). This authenticity index operationalizes the plausibly deniable paradigm: it provides calibrated measure of how well an image aligns with its inversion, serving as the primary metric in our experiments to distinguish content that can be asserted as authentic from content that remains plausibly deniable. Figure 3 shows the entire pipeline of our A-index(x, (cid:101)x) calibration. 3.4. From Authenticity Index to Detection While the authenticity index provides calibrated score of inversion consistency, using it for detection requires reasoning over the distributions of scores assigned to real and generated images. Concretely, given dataset with both classes, we compute the authenticity index for each image, yielding two empirical distributions: one for real images and one for generated images. We observe three key properties of these distributions: (1) Variability, where both real and generated images span range of scores: some generated images achieve moderately high values, while real images vary depending on how well they align with the inversion process, (2) Importantly, real images that are difficult to invert tend to achieve the highest scores, extending the tail of the real distribution beyond that of the fake distribution.(3) This separation enables the definition of threshold τsafety such that any image with authenticity index A-index(x, (cid:101)x) τsafety can be confidently classified as real with 1% False Positive Rate (FPR). Images below τsafety are not necessarily fake, but fall into the plausibly deniable region. In this way, the authenticity index underpins practical detection method: instead of forcing every input into binary real/fake label, we identify calibrated boundary that guarantees the authenticity of content above threshold while acknowledging uncertainty below it. This procedure makes explicit the transition from metric of inversion fidelity to an operational detection rule. 3.5. Robustness to Adversarial Attacks Beyond serving as static metric, the authenticity index also enables robustness analysis: we can probe its stability under adversarial perturbations of the input image. This motivates the following study of adversarial robustness. Motivation. Conventional adversarial attacks such as FGSM [15] and PGD [20] are designed for classification pipelines and rely on perturbing logits with simple loss functions. However, our setting involves generative inversion pipeline, where direct application of these methods is meaningless. Threat Model. We therefore define task-specific threat model: given an image I, the adversary seeks to add small perturbation δ (bounded by ℓ norm) such that the inversion process (cid:101)G1(I + δ) alters the Authenticity Index. Importantly, this shift in authenticity enables new paradigm of analysis: instead of the binary distinction real vs. fake, we identify the regime of real vs. uncertain. Real images that remain stable above threshold are deemed authentic, while those driven below it become indistinguishable from generated content. Adversarial Objective for Inversion. Formally, we define the adversarial optimization problem as max δϵ A-index(cid:0)I, (cid:101)G1(I + δ)(cid:1), where AI denotes our authenticity index that aggregates PSNR, SSIM, inverted LPIPS, and CLIP similarity into calibrated score, and ϵ = 8/255 constrains the perturbation magnitude to remain imperceptible to the human eye. Depending on the optimization direction, one can also minimize the objective, thereby reducing the authenticity score of real images. The adversarial perturbation δ is optimized via gradientbased methods over differentiable approximation of the inversion pipeline, while the ℓ constraint is enforced after each step. Unlike classification-based attacks, the gradient flows through the entire generative inversion process rather than through classification logits. This formulation provides principled way to evaluate the robustness of the authenticity index under adversarial perturbations. In the experimental section, we instantiate this framework to quantify robustness margins across real and generated datasets. To account for adversarial vulnerability, we additionally define new threshold, τsecurity, such that any image with authenticity index A-index(x, (cid:101)x) τsecurity can be confidently classified as real with 1% False Positive Rate (FPR) in an adversarial attack setting. Table 1. Results before/after PGD (ϵ = 8 255 ) on 2,000 images showing correctly classified fake/real images, accuracies, and attack success rates. Before attack After attack Attack success (%) Model UFD [23] FreqNet[37] NPR [35] FatFormer [18] D3 [46] C2PClip [36] Correct fake Correct real Acc. (%) Correct fake Correct real Acc. (%) on fake 1 53 86 1 736 51 974 995 653 987 942 949 48.75 52.40 36.95 49.40 83.90 50.00 0 0 0 0 24 0 0 0 0 0 11 2 0.00 100.0 0.00 100.0 0.00 100.0 0.00 100.0 1.75 96.7 0.10 100. on real 100.0 100.0 100.0 100.0 98.8 99.8 4. Experiments 4.1. Setting We evaluate our method using five open-weight generative models: Stable Diffusion 2.1 [28], Stable Diffusion 3 (medium) [11, 34], Stable Diffusion 3.5 (medium) [33], FLUX.1 [dev] [1], and FLUX.1 [dev] with Realism LoRA adapter [45]. For evaluation, we employ curated subset of the De-Factify 4 dataset [38], which contains 1,000 real images and 1,000 corresponding synthetic images, with 200 images generated from each of five generation methods present in the dataset (Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3, DALLE 3, and Midjourney 6) [22]. All experiments are conducted using Rectified-Flow Inversion (RF-Inversion) [32] executed on 4 NVIDIA A6000 GPUs (50 GB each), which enables large-scale inversion across models and samples. The inversion process hyperparameters are provided in the supplementary material. We also evaluate six baselines alongside our method in zeroshot setting. The six baselines are UFD [24], FreqNet [37], NPR [35], FatFormer [18], D3 [46], and C2PClip [36].For the social-media study, we further collect 3,000 images from Reddit using diverse queries to evaluate the prevalence and risk patterns of plausibly deniable content in the wild. More details are provided in the supplementary material. 4.2. Results on Generalizability and Robustness to"
        },
        {
            "title": "Adversarial Attacks",
            "content": "Current Deepfake Detectors do not Generalize. Most existing deepfake detectors rely on binary classification to distinguish between real and fake images. However, as shown in Figure 4, this approach often leads to strong bias toward classifying out-of-domain inputs as real. In particular, the C2-CLIP detector correctly classifies only 51 fake images, while misclassifying 949 as real, despite correctly identifying most authentic inputs. This imbalance highlights strong bias toward high recall on real images at the cost of failing to flag synthetic content. similar failure pattern is evident under adversarial conditions. As shown in Figure 5 and Table 1, all evaluated detectors collapse completely under imperceptible PGD [21] perturbations, with several models including UFD [24], FreqNet [37], and FatFormer [18] dropping to 0% accuracy post-attack. Even D3 [46], the strongest baseline, falls from 83.90% to just 1.75% accuracy. These results underscore general and critical weakness: traditional detectors exhibit strong confirmation bias toward authenticity and offer no graceful degradation under adversarial manipulation or distributional shift. Refer to supplementary for additional results. To address these limitations, our framework adopts an opposing philosophy: instead of forcing binary decision, we prioritize high precision and low recall for authentication. We confidently certify only those images whose authenticity can be established with certainty, while abstaining from judgment on uncertain cases (Plausible Deniablity). This design reflects the practical reality that false authenticity claim is far more damaging than conservative abstention. Adversarial Robustness. We evaluate whether adversaries can push fake images above the threshold τ using imperceptible perturbations. Unlike binary classifiers that require only crossing decision boundary, our thresholdbased framework defines two calibrated cutoffs: Safety Threshold τsafety that guarantees low false positives on authentic data, and Security Threshold τsecurity that limits false acceptances of adversarially perturbed fakes. Successful attacks must therefore not only invert the input convincingly, but also elevate the authenticity index past strict, datadriven thresholds making evasion substantially more difficult in practice ,Through this evaluation, we determined that for Stable Diffusion 3 (medium) the threshold τsafety is 0.0365, while the corresponding adversarially calibrated threshold τsecurity is 0.038. Protocol. To assess generalization to unseen generators, we evaluate six baselines alongside our method in zero-shot Figure 4. Like most traditional methods, C2-CLIP fails to generalize, as real and fake prediction densities heavily overlap, indicating poor separability. setting where test images are generated by models not seen during training or calibration. On top of that, we employ PGD attacks [21] with ℓ budget ϵ = 8/255 (maximum 8 intensity levels per pixel) to assess the models in adversarial attacks setting. We report before-attack accuracy, postattack accuracy, attack success rate, precision and recall. Results. Existing detectors exhibit complete and catastrophic collapse under adversarial perturbations. Table 1 highlights this brittleness under PGD attacks with ϵ = 8/255. UFD [23] degrades from 48.75% accuracy to 0.00%, misclassifying every image after attack and achieving 100.0% attack success rate (ASR) on both fake and real samples. FreqNet [37] suffers the same outcome, dropping from 52.40% to 0.00% accuracy with 100.0% ASR. The trend continues across all baselines: NPR [35], FatFormer [18], and C2PClip [36] are fully compromised post-attack, with 100.0% ASR on fakes and either 100.0% (NPR, FatFormer) or 99.8% (C2PClip) ASR on reals. Even D3 [46], the strongest baseline, falls sharply from 83.90% to just 1.75% accuracy under attack. This brittleness stems from architectural limitations: binary classifiers must produce confident predictions on all inputs. When adversarial perturbations shift samples across Figure 5. Robustness evaluation of the traditional D3 detector under PGD attack. Before the attack, the model confidently distinguishes between real and fake samples. However, after the adversarial perturbation, the distribution inverts completely, resulting in near-total misclassification , the detector is effectively and perfectly fooled. 6 max δε A-index(cid:0)x, (cid:101)G1(x + δ)(cid:1), ε = 8/255. This objective is implemented with projected gradient ascent on δ: at each iteration the attacker computes or estimates gradients of differentiable approximation of (cid:101)G1 and of A-index with respect to the inverter input, takes an ascent step on δ, projects δ back to the admissible set {δ : δ ε}, and clips x+δ to the valid pixel range. To characterize this attack we ran experiments with = 100; the density of candidates versus A-index is plotted in Figure 7. From the sampled pool we selected the top-scoring candidate whose A-index was 0.0148. After performing the constrained adversarial refinement described above the reported score increased only to 0.0154, indicating small shift and suggesting that for this prompt and this budget the medium-resource attacker achieves only modest improvement and it is also below the safety threshold and security threshold. 4.4. Social Media Study and the Effect of Adapters The motivation for this study is to understand how vulnerable real-world imagery is to resynthesis as generative models become increasingly powerful. If common internet images can be inverted with high fidelity, then their authenticity becomes difficult to establish, since an adversary can plausibly claim they were generated. To examine this risk in practice, we collected approximately 3,000 images from Reddit across diverse categories using range of keywords, and we captioned these images with Salesforce BLIP-2 [17].We are then inverting them using Stable Diffusion 3 (medium). Separately, we calibrate safety threshold for each generative method Stable Diffusion 2.1, SD3 (medium), SD3.5 (medium), Flux Dev, and Flux Dev with Realism LoRA using distributions of real-inverted and fake-inverted images. This calibration yields method specific safety thresholds: 0.015 for SD2.1, 0.0368 for SD3 (medium), 0.0365 for SD3.5 (medium), 0.035 for Flux Dev, Figure 7. Distribution of index(x, x) for N=100. 7 Figure 6. A-index density comparison across four settings for distribution comparison under adversarial perturbations. the decision boundary, they induce complete failure with no mechanism for graceful degradation. In contrast, our method maintains significant robustness. As shown in Figure 6, even under perturbation, the A-index distributions for real and fake images remain well-separated. Adversarial examples increase overlap modestly but do not collapse the distribution. Our abstention-based framework therefore preserves utility where binary classifiers fail completely, offering calibrated degradation path grounded in perceptual similarity rather than forced decisions. To address these limitations, our framework adopts an opposing philosophy: instead of forcing binary decision, we prioritize high precision and low recall for authentication. We confidently certify only those images whose authenticity can be established with certainty, while abstaining from judgment on uncertain cases (Plausible Deniablity). This design reflects the practical reality that false authenticity claim is far more damaging than conservative abstention. 4.3. Medium Resource Attacker We first note the limiting case: if an attacker had effectively infinite resources and could synthesize and invert millions of candidates, then they could select the candidate with maximal authenticity score and adversarially refine it to drive the defenders metric arbitrarily high, in which case post-hoc verification would be defeated. In practice, attackers are resource-limited; we therefore analyze medium-resource protocol in which the attacker is given single prompt, samples random seeds, and synthesizes candidates xi = Gθ(zi; prompt) for = 1, . . . , . For each candidate the attacker obtains the inversion (cid:101)xi = (cid:101)G1(xi) and evaluates the authenticity index A-index(xi, (cid:101)xi). The attacker then selects = xi with = arg maxi=1,...,N AI(xi, (cid:101)xi) and searches for small perturbation δ that increases the score when the perturbed image is passed into the inversion routine. Formally, the adversarial problem is written as and 0.038 for Flux Dev with Realism LoRA. We then apply each of these thresholds to the same set of SD3 inverted internet images, and count how many images fall above the threshold for each method. As shown in Figure 8, SD2.1 marks 1,116 images as authentic, while newer models yield significantly fewer (5579), reflecting their stronger ability to resynthesize real-world content. These results show clear progression: newer and larger models can more easily reproduce existing internet imagery. In simple terms, as generative models improve, it becomes increasingly difficult to determine whether an image was captured from reality or synthesized pointing toward an eventual erosion of post-hoc distinguishability between real and generated content. Figure 8. Number of real images (out of 3,000) that exceed the safety threshold, with thresholds calibrated to 1% FPR for each generative model. Newer models yield only 5579 images, indicating that modern generators are more capable of closely resynthesizing real-world content, thereby reducing the number of confidently authenticatable samples. 4.5. Extension to Video To evaluate whether our image-based authentication framework extends to the video domain, we conduct experiments on subset of 100 videos ( 50 real and 50 fake ) from the Deepfake-Eval-2024 benchmark [3]. Prior work shows that existing detectors suffer approximately 50% AUC degradation on this benchmark compared to older datasets, highlighting severe generalization failures under real-world distribution shift. Our method was designed for single images and requires no modification for video. For each video, we sample 8 frames uniformly (one frame per 30-frame interval),then use blip 2 [17] to caption then invert and regenerate each frame independently, and compute per-frame authenticity indices A-index(fi, fi) using the same pipeline as our image experiments. The video-level authenticity score is defined as: A-indexvideo = 1 8 8 (cid:88) i=1 A-index(fi, fi). (2) Table 2. Failure of video-specific deepfake detectors on 50 videos from Deepfake-Eval-2024. All models exhibit poor precision, rendering them unreliable for real-world authentication despite exploiting temporal information. Model AUC Precision Recall F1 GenConViT [9] FTCN [48] Styleflow [4] 0.6154 0.4832 0.5087 0.59 0.50 0.53 0.49 0.64 0. 0.53 0.40 0.47 Results and Comparison. Table 2 exposes the precision collapse of purpose-built video deepfake detectors when evaluated on real-world content. Despite being specifically designed to exploit temporal cues and trained on videospecific features, all three baselines fail to achieve reliable authentication capability.GenConViT achieves the highest AUC (0.6154) but maintains only 59% precision. FTCN performs near-random (AUC 0.4832) and despite achieving 64% recall through aggressive classification, collapses to 50% precision, effectively no better than coin flip. Styleflow shows marginally better performance but still fails with 53% precision. These results mirror the failures observed with image-based binary classifiers, while our method, as shown in Figure 9, exhibits similar pattern: real videos are harder to invert than fake ones. Figure 9. Authenticity Index distribution across real and fake videos 5. Discussion As generated images grow more realistic and adversarial, binary detectors fail to generalize and collapse under imperceptible adversarial perturbations. We introduce calibrated method using an Authenticity Index (A-index) to assess whether content is authentic or plausibly deniable via inversion behavior. By fusing similarity metrics into single score, our approach enables strict, attack-resistant thresholds. Tested on images, videos, and internet scrapped images, it yields high precision, low recall, and abstains in uncertain cases. Stronger generators, especially LoRAbased, further shrink verifiable content. Our method generalizes to video and supports shift toward risk-aware, abstention-based detection."
        },
        {
            "title": "References",
            "content": "[1] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 5 [2] George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1075910769, 2024. 1 [3] Nuria Alina Chandra, Ryan Murtfeldt, Lin Qiu, Arnab Karmakar, Hannah Lee, Emmanuel Tanumihardja, Kevin Farhat, Ben Caffee, Sejin Paik, Changyeon Lee, Jongwook Choi, Aerin Kim, and Oren Etzioni. Deepfake-eval-2024: multi-modal in-the-wild benchmark of deepfakes circulated in 2024, 2025. 8 [4] Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, and Jongwon Choi. Exploiting style latent flows for In Proceedings of generalizing deepfake video detection. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11331143, 2024. 8 [5] Federico Cocchi, Lorenzo Baraldi, Samuele Poppi, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Unveiling the impact of image transformations on deepfake detection: An experimental analysis. In International Conference on Image Analysis and Processing, 2023. [6] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models, 2022. 1 [7] Davide Cozzolino, Giovanni Poggi, Matthias Nießner, and Luisa Verdoliva. Zero-shot detection of ai-generated images. In ECCV. Springer, 2024. 2 [8] Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. Audiocraft: pytorch library for deep learning research on audio generation, 2023. 1 [9] Deressa Wodajo Deressa, Hannes Mareen, Peter Lambert, Solomon Atnafu, Zahid Akhtar, and Glenn Van Wallendael. Genconvit: Deepfake video detection using generative convolutional vision transformer. Applied Sciences, 15(12), 2025. 8 [10] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) dataset. arXiv preprint arXiv:2006.07397, 2020. 2 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. [12] Joel Frank, Thorsten Eisenhofer, Lea Schonherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Leveraging frequency analysis for deep fake image recognition. In International conference on machine learning. PMLR, 2020. 2 [13] Rafael C. Gonzalez and Richard E. Woods. Digital Image Processing. Prentice Hall, 2nd edition, 2002. 3 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 1 [15] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. arXiv Explaining and harnessing adversarial examples. preprint arXiv:1412.6572, 2014. 5 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 2020. 2, [17] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023. 7, 8 [18] Huan Liu, Zichang Tan, Chuangchuang Tan, Yunchao Wei, Yao Zhao, and Jingdong Wang. Forgery-aware adaptive transformer for generalizable synthetic image detection, 2023. 5, 6, 12 [19] Jan Lukas, Jessica Fridrich, and Miroslav Goljan. Digital camera identification from sensor pattern noise. IEEE Transactions on Information Forensics and Security, 2006. 2 [20] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 5 [21] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018. 6, 12 [22] Shrikant Malviya, Neelanjan Bhowmik, and Stamos Katsigiannis. Skdu at de-factify 4.0: Vision transformer with data augmentation for ai-generated image detection. arXiv preprint arXiv:2503.18812, 2025. 5 [23] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In CVPR, 2023. 5, 6, 12 [24] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models, 2024. 1, 5, 6, [25] OpenAI. Sora: Openais video generation model, 2024. 1 [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 3, 11 [27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 1 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021. 5 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. 9 [44] Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomaˇs Foltynek, Jean Guerrero-Dib, Olumide Popoola, Petr ˇSigut, and Lorna Waddington. Testing of detection tools for ai-generated text. International Journal for Educational Integrity, 2023. 1 [45] XLabs-AI. flux-realismlora (lora adapter for flux.1 [dev]). https : / / huggingface . co / XLabs - AI / flux - RealismLora, 2024. Accessed: 2025-11-05. [46] Yongqi Yang, Zhihao Qian, Ye Zhu, Olga Russakovsky, and Yu Wu. D3: Scaling up deepfake detection by learning from discrepancy. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2385023859, 2025. 5, 6, 12 [47] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 586595, 2018. 3, 11 [48] Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, and Fang Wen. Exploring temporal coherence for more genIn 2021 IEEE/CVF Ineral video face forgery detection. ternational Conference on Computer Vision (ICCV), pages 1502415034, 2021. 8 [49] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. InIn ECCV. domain gan inversion for real image editing. Springer, 2020. 2 [50] Jiren et al. Zhu. Hidden: Hiding data with deep networks. In Proceedings of the European Conference on Computer Vision (ECCV), 2018. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 3 [30] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nießner. Faceforensics++: Learning to detect manipulated facial images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 111, 2019. 2 [31] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations, 2024. 2, 3, 11 [32] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. In International Conference on Learning Representations (ICLR), 2025. 5 [33] Stability AI. Introducing stable diffusion 3.5. https: //stability.ai/news/introducingstablediffusion-3-5, 2024. Accessed: 2025-11-05. [34] Stability AI. Stable diffusion 3 medium. https : //stability.ai/news/stable-diffusion-3medium, 2024. Accessed: 2025-11-05. 5 [35] Chuangchuang Tan, Huan Liu, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Rethinking the up-sampling operations in cnn-based generative network In Proceedings of for generalizable deepfake detection. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 5, 6, 12 [36] Chuangchuang Tan, Renshuai Tao, Huan Liu, Guanghua Gu, Baoyuan Wu, Yao Zhao, and Yunchao Wei. C2p-clip: Injecting category common prompt in clip to enhance generalization in deepfake detection, 2024. 5, 6, 12 [37] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. Frequency-aware deepfake detection: Improving generalizability through frequency space learning, 2024. 1, 5, 6, [38] Defactify Team. De-factify 4: AI-generated image detection and source identification. https://defactify.com/, 2025. Accessed: 2025-11-05. 5 [39] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 3 [40] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A. Efros. Cnn-generated images are surprisingly easy to spot... for now, 2020. 1 [41] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: From error visibility IEEE Transactions on Image Proto structural similarity. cessing, 13(4):600612, 2004. 3, 11 [42] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection, 2023. 1 [43] Zhenting Wang, Vikash Sehwag, Chen Chen, Lingjuan Lyu, Dimitris Metaxas, and Shiqing Ma. How to trace latent generative model generated images without artificial watermark? arXiv preprint arXiv:2405.13360, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Evaluation metric A.2. Authenticity Index Calibration A.1. Similarity Metrics Formulation In this section, we provide the mathematical formulations for all similarity metrics used in computing the Authenticity Index. Peak Signal-to-Noise Ratio (PSNR). PSNR measures the pixel-level fidelity between the original image and its reconstruction x: PSNR(x, x) = 10 log (cid:18) MAX2 (cid:19) MSE(x, x) (3) (cid:80)N where MSE(x, x) = 1 i=1(xi xi)2, MAXI is the maxN imum possible pixel value (255 for 8-bit images), and is the total number of pixels. PSNR is measured in decibels (dB), with higher values indicating better reconstruction quality. Structural Similarity Index (SSIM). SSIM [41] evaluates structural information preservation between images: SSIM(I, ˆI) = (2µI µ ˆI + C1)(2σI ˆI + C2) + µ2 + C1)(σ2 ˆI + σ2 ˆI + C2) (µ , (4) where µ, σ2, and σI ˆI are the local means, variances, and cross-covariance of and ˆI, and C1, C2 are stabilizing constants. Learned Perceptual Image Patch Similarity (LPIPS). LPIPS [47] computes perceptual distance using deep features extracted from pretrained network: LPIPS(x, x) = (cid:88) ℓ 1 HℓWℓ (cid:88) h,w wℓ(ϕℓ(x)h,wϕℓ(x)h,w)2 2 (5) where ϕℓ represents features from layer ℓ of pretrained network, wℓ are learned weights, and Hℓ, Wℓ are spatial dimensions at layer ℓ. Since LPIPS measures distance, we use (1LPIPS) in our formulation to align with other similarity metrics. CLIP Similarity. CLIP [26] measures semantic consistency in the joint vision-language embedding space: CLIP(x, x) = E(x) E(x) E(x) E(x) (6) where E() is the CLIP vision encoder, and the similarity is computed as the cosine similarity between the normalized embeddings. Weight Optimization for the composite similarity score. Given our Similarity score: s(x, x) = α1 PSNR(x, x) + α2 SSIM(x, x) + α3 (1 LPIPS(x, x)) + α4 CLIP(x, x). and our Authenticity index: A-index(x, x) = exp(σ s(x, x)) 1 + exp(σ s(x, x)) (7) (8) we aim to optimize the weights α1, α2, α3, α4 via using Differential Evolution to maximize separation between real and fake image distributions. The objective function minimizes the overlap between distributions: minimize overlap(Dreal, Dfake) = (cid:90) min(preal(s), pfake(s)) ds (9) Differential Evolution Parameters. Population size: 20 Mutation factor (F ): 0.6 Crossover probability (CR): 0.7 Maximum iterations: 300 Bounds: αi [10.0, 10.0] for = 1, 2, 3, 4 Convergence tolerance: 1 1010 Optimized Weights. α1 (PSNR): -0.0181 α2 (SSIM): 1.380 α3 (1-LPIPS): -4.058 α4 (CLIP): 8.066 σ (sigmoid scale): 0.9 B. Inversion Hyperparameters We employ Rectified Flow Inversion [31] for all experiments. The key hyperparameters are described below. General Inversion Parameters. Parameter Value Description Number of inference steps Guidance scale Scheduler DDIM η base Eta trend Eta start step Eta end step Gamma Seed Data type 28 3.5 FlowMatchEulerDiscrete 0.95 constant 0 9 0.5 42 float16 Denoising timesteps Classifier-free guidance strength Sampling scheduler used during inversion Base eta for interpolated denoise Trend of eta values (e.g., constant, linear) Step where eta schedule starts Step where eta schedule ends Gamma parameter for rectified inversion Random seed for reproducibility Precision for model computations Table A. General inversion hyperparameters used across all experiments. C. Zero-Shot Detection Graphs for All BaseE. Examples of Hard-to-Invert and Easy-toline Methods"
        },
        {
            "title": "Invert Images",
            "content": "We report the prediction distributions of all baseline detectors under zero-shot setting.These detectors include UFD [23], FreqNet [37], NPR [35], FatFormer [18], D3 [46], and C2P-CLIP [36]. The evaluation was conducted on test samples generated by models unseen during training, namely Stable Diffusion 2.1, Stable Diffusion XL, Stable Diffusion 3 (medium), Stable Diffusion 3.5 (medium), DALLE 3, and Midjourney 6. Our observations indicate that most methods exhibit substantial prediction score overlap between real and fake classes. As shown in Figure A, models like UFD, FreqNet, and NPR tend to overfit to familiar artifacts and thus default to high-confidence predictions of real even when presented with synthetic samples from unseen generators. Although D3 performs slightly better, it still suffers from notable misclassification rates. These plots underline the generalization weakness of current binary detectors and reinforce the motivation behind adopting calibrated authenticity framework. D. PGD Attack Graphs for All Baseline Methods To further assess robustness, we visualize how the prediction distributions of five baseline detectors change under adversarial perturbation. Specifically, we perform projected gradient descent (PGD) attacks [21] with ℓ-norm bounded perturbations (ϵ = 8/255), targeting both real and fake inputs. The objective is to push predictions across the decision boundary using imperceptible noise. The results show complete collapse in performance across all methods. Models like UFD [24], FreqNet [37], and NPR [35] fail entirely, with prediction distributions collapsing into indistinguishability between real and fake classes. FatFormer [18] and C2P-CLIP [36] also misclassify nearly all samples post-attack. These visualizations in Figure highlight the fragility of binary detectors and the lack of graceful degradation under adversarial pressure (a phenomenon that is particularly pronounced in the cases of NPR and FatFormer). In contrast, our proposed method maintains calibrated separation and abstains when confidence is insufficient, as demonstrated in the main paper. To illustrate the imperceptibility of the attack, Figure shows real image before and after PGD perturbation, along with the normalized perturbation itself. Despite the negligible visual difference, the prediction shifts significantly post-attack, causing traditional detectors to fail. This section presents qualitative examples of images that fall on opposite ends of the Authenticity Index spectrum. We distinguish between hard-to-invert images, which yield low similarity under inversion and thus achieve high A-index scores, and easy-to-invert images, which reconstruct with high perceptual fidelity and result in low A-index values. Images that are hard to invert typically contain finegrained texture, complex scenes, occlusions, or motion blur factors that challenge the inversion pipeline. In contrast, easy-to-invert images often include centered objects with minimal background clutter, strong lighting, or symmetry, all of which tend to align well with the priors of modern generative models. Figure displays four such images along with their reconstructions using our inversion method. The top two rows correspond to easy-to-invert samples, where the inverted outputs closely match the originals in both low-level and semantic structure. The bottom two rows show hard-toinvert cases, where the reconstructions diverge significantly in content or structure, despite the input being real. These qualitative results support our core claim that not all real images can be reliably certified post-hoc, particularly when inversion fidelity is low. F. Scraped Internet Images: Hard-to-Invert and Easy-to-Invert Examples To assess the real-world implications of our method, we analyze corpus of approximately 3,000 images scraped from Reddit across diverse topics. Each image is inverted using Stable Diffusion 3 (medium) and scored using the A-index. Based on the calibrated safety threshold for this model, we categorize images as either likely authentic (above threshold) or plausibly deniable (below threshold). Figure shows two representative cases: one image that is easy to invert, and one that is hard to invert. The easy-toinvert image reconstructs with high perceptual and semantic fidelity, resulting in low A-index score. In contrast, the hard-to-invert image exhibits substantial degradation in the inverted output, despite being real. These examples further demonstrate that some authentic content cannot be reliably verified post-hoc, especially when generative models closely match the visual features. To understand how this manifests across broader internet content, we include visual summary in Figure F, which displays eight scraped images from different Reddit communities. Despite all being real, only handful exceed the safety threshold, indicating how widespread plausible deniability has become in online imagery. 12 (a) UFD (b) FreqNet (c) NPR (d) FatFormer Figure A. Zero-shot prediction score distributions for each baseline method. Each subplot shows the histogram of classifier outputs for real and fake images. Vertical dashed lines indicate classification thresholds. most methods struggle to generalize to unseen generators, misclassifying significant number of fake images as real. (e) D3 G. Medium Resource Attacker Full Details To operationalize our medium-resource attacker scenario , we simulate threat model where the adversary is granted access to single prompt and fixed computational budget. The attacker samples = 100 images by drawing random seeds and generating samples xi = Gθ(zi; prompt) for = 1, . . . , 100, where Gθ is the target generative model (Stable Diffusion 3 medium in our case). Each generated image is inverted using our reconstruction-free inversion pipeline (cid:101)G1 and scored using the Authenticity Index: A-index(xi, (cid:101)xi) = A-index(xi, (cid:101)G1(xi)). The attacker then selects the highest-scoring candidate and optimizes small perturbation δ, constrained by δ 8/255, to maximize the A-index after inversion of the perturbed image. The prompt used in this experiment is: hungry man standing outside real pizza shop at night, mouth slightly open, drooling, pointing toward the glowing neon pizza sign. from the shop window reveals fresh cheesy pizzas inside with steam, the scene looks realistic and cinematic with natural lighting and lifelike details. Warm light To illustrate the range of outputs available under the attackers sampling budget, we visualize six randomly sampled images generated from this prompt using different seeds in Figure D. These images are unmodified generations prior to inversion or adversarial refinement. While visually diverse, the top-scoring image among the 100 candidates achieved an A-index of 0.0148. After applying PGD to perturb x, the A-index(x, (cid:101)G1(x + δ)) increased modestly to 0.0154. This slight improvement remains below both the 13 (a) UFD (b) FreqNet (c) NPR (d) FatFormer Figure B. Prediction score distributions before and after PGD attack for each baseline method (excluding D3). Each plot shows how adversarial perturbation erodes separability between real and fake inputs, often collapsing confidence entirely. (e) C2P-CLIP safety threshold (τsafety = 0.0365) and the adversarial security threshold (τsecurity = 0.038), indicating the limited effectiveness of medium-resource attacker. H. Limitations Despite its strengths, our approach has several limitations that highlight directions for future research. First, the authenticity index relies on access to highquality inversion pipelines and perceptual similarity models such as CLIP and LPIPS. While this enables robust and calibrated score, it assumes white-box or partially open generative models. In scenarios where the generator is fully black-box or proprietary, the inversion step may not be feasible or reliable. second, the safety and security thresholds are specific to each generative model and require calibration using real and synthetic data. This introduces practical overhead, especially in dynamic environments where new models are rapidly emerging. Automating or generalizing this calibration remains an open challenge. Finally, while our extension to the video domain is promising, it treats each frame independently and does not exploit temporal consistency or motion cues. Incorporating these temporal dynamics could improve performance in challenging video-based deepfake scenarios. (a) Original Image (b) Adversarially Perturbed Image Figure C. Visual comparison between the original image and its PGD-perturbed version. The perturbation is imperceptible to the human eye, yet it significantly alters detector predictions. Figure D. Six image samples generated from the same prompt (Section G), with different random seeds. All were generated under the medium-resource attackers sampling budget. These highlight the diversity in candidate outputs that the attacker can choose from. 15 (a) Original (Easy to invert) (b) Inverted (Easy to invert) (c) Original (Easy to invert) (d) Inverted (Easy to invert) (e) Original (Hard to invert) (f) Inverted (Hard to invert) (g) Original (Hard to invert) (h) Inverted (Hard to invert) Figure E. Examples of images and their reconstructions using our inversion method. Top two rows: easy-to-invert images with high reconstruction fidelity. Bottom two rows: hard-to-invert images with noticeable semantic and structural degradation. Figure F. Collage of 8 real images scraped from the internet. Despite their authenticity, most fall below the A-index safety threshold, indicating plausible deniability due to high-fidelity inversion. 16 (a) Original (Easy to invert) (b) Inverted (Easy to invert) (c) Original (Easy to invert) (d) Inverted (Easy to invert) (e) Original (Hard to invert) (f) Inverted (Hard to invert) (g) Original (Hard to invert) (h) Inverted (Hard to invert) Figure G. Comparison of inversion performance on scraped internet images. The top row displays two examples that produce accurate, high-quality reconstructions. The bottom row shows cases where the inversion process struggles, often due to fine text, low contrast, or complex object structure, resulting in substantially degraded reconstructions."
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)"
    ]
}