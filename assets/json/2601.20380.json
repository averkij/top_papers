{
    "paper_title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
    "authors": [
        "Le Zhang",
        "Yixiong Xiao",
        "Xinjiang Lu",
        "Jingjia Cao",
        "Yusai Zhao",
        "Jingbo Zhou",
        "Lang An",
        "Zikan Feng",
        "Wanxiang Sha",
        "Yu Shi",
        "Congxi Xiao",
        "Jian Xiong",
        "Yankai Zhang",
        "Hua Wu",
        "Haifeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 2 ] . [ 1 0 8 3 0 2 . 1 0 6 2 : r OmegaUse: Building General-Purpose GUI Agent for Autonomous Task Execution Le Zhang, Yixiong Xiao, Xinjiang Lu, Jingjia Cao, Yusai Zhao, Jingbo Zhou, Lang An, Zikan Feng, Wanxiang Sha, Yu Shi, Congxi Xiao, Jian Xiong, Yankai Zhang, Hua Wu, Haifeng Wang Baidu Frontier Research Department *Equal contribution; Contact authors:{zhoujingbo, wu hua, wanghaifeng}@baidu.com Abstract Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing humancomputer interaction and improving human productivity. In this report, we present OmegaUse, general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computeruse and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce carefully engineered dataconstruction pipeline and decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OSNav, benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav."
        },
        {
            "title": "1 Introduction",
            "content": "GUI agents have recently emerged as transformative frontier for multimodal interaction, enabling artificial intelligence to navigate digital environments ranging from mobile applications to desktop software in manner analogous to human users [1, 2, 3]. By perceiving screen states through screenshots and executing atomic actions such as clicking, typing, and scrolling, these agents aim to bridge the gap between high-level user intent and complex operational sequences [1, 4], as illustrated in Figure 1. Depending on the target platform, they are often referred to as computer-use, phone-use, or browser-use agents. Figure 1: An overview of OmegaUses core capabilities in realistic GUI scenarios. Despite significant progress, current GUI agents still face critical bottlenecks in performance, training-data quality, and the lack of comprehensive evaluation across diverse digital ecosystems. To address these challenges, we present OmegaUse, general-purpose GUI agent model1 designed for autonomous task execution. We name the agent OmegaUse to reflect its unified support for both computer-use and phone-use scenarios across diverse platforms. OmegaUse is built on Mixtureof-Experts (MoE) backbone. Compared with compact dense models (e.g., 7B or 72B) [2, 5, 6], this design preserves the reasoning capacity of large-parameter models while activating only subset of parameters, enabling superior performance with substantially reduced computational overhead. We acknowledge that data quality is primary determinant of GUI agent model performance, as noisy training signals can substantially degrade both spatial perception and decision-making. In grounding tasks, labels automatically derived from HTML or Accessibility (A11y) trees often suffer from rendering offsets, leading to misaligned bounding boxes and ambiguous textual descriptions [4, 5]. Moreover, existing navigation datasets frequently contain inconsistencies, such as incorrect execution trajectories and excessive redundant actions, that provide weak or incoherent supervision for long-horizon planning [5, 7]. To address these issues, we develop high-quality training-corpus construction pipeline. For grounding, we apply stringent filtering procedure to improve label precision. For navigation, we propose novel hierarchical synthesis framework that integrates three complementary data sources: (1) rigorously curated open-source datasets, (2) automatically synthesized trajectories generated by combining bottom-up autonomous exploration with top-down taxonomy-guided generation, and (3) high-fidelity cross-terminal expert demonstrations. To effectively leverage the curated data, we propose decoupled two-stage training paradigm. We first apply supervised fine-tuning (SFT) to establish foundational interaction syntax and basic task logic [1, 2]. We then employ Group Relative Policy Optimization (GRPO) to refine spatial grounding and sequential planning [8, 9, 5]. With specialized reward design, such as an Inside-of-BoundingBox reward for grounding and stepwise coordinate-based rewards for navigation, OmegaUse is encouraged to focus on precise interaction regions rather than ambiguous boundary pixels [5, 10, 11]. Beyond architecture and training strategies, we observe that existing benchmarks may not fully capture an agents proficiency across diverse digital environments, such as Chinese mobile applications or multi-step desktop workflows. To complement current evaluation resources, we introduce OSNav2, specialized offline benchmark comprising two sub-benchmarks across different operating 1While the term GUI agent typically refers to the full system that interacts with digital environment (e.g., including external tools), our work focuses on end-to-end model training. We treat GUI agentic capability as high-level policy learned via dedicated model-based approach. 2https://huggingface.co/datasets/baidu-frontier-research/OS-Nav 2 systems: ChiM-Nav, focusing on Chinese Android mobile systems, and Ubu-Nav, targeting routine desktop interactions on Ubuntu. Both datasets provide expert-verified reasoning trajectories, enabling more comprehensive evaluation of agent generalization and planning consistency. Through extensive empirical evaluations, we demonstrate that OmegaUse consistently outperforms or remains competitive with SOTA GUI agents across multiple platforms. On standard grounding benchmarks, OmegaUse achieves record 96.3% on ScreenSpot-V2. In navigation tasks, it reaches leading 79.1% step success rate on AndroidControl and demonstrates robust interactive capabilities on AndroidWorld. Furthermore, on our proposed benchmarks, OmegaUse delivers superior performance with 74.24% step success rate on ChiM-Nav and 55.9% average success rate on Ubu-Nav. These results underscore the effectiveness of OmegaUse agent. Our main contributions are summarized as follows: We introduce OmegaUse, general-purpose GUI agent built on parameter-efficient MoE architecture for autonomous task execution. OmegaUse is trained using decoupled two-stage paradigm, and we present holistic framework for building GUI agents that jointly addresses data construction and model training. We establish high-quality data foundation for GUI agents. In particular, we propose hierarchical navigation data construction pipeline featuring novel automated synthesis framework that combines bottom-up autonomous exploration with top-down taxonomy-guided generation. This approach substantially reduces reliance on manual annotations while ensuring data diversity, coverage, and fidelity across platforms. To bridge the evaluation gap in specific digital environments, we release OS-Nav, specialized offline benchmark suite comprising ChiM-Nav for Chinese Android mobile ecosystems and Ubu-Nav for routine Ubuntu desktop system. OS-Nav enables rigorous assessment of crossplatform generalization and planning consistency. Extensive empirical evaluations demonstrate that OmegaUse achieves highly competitive performance across wide range of GUI benchmarks, including state-of-the-art results on several tasks. Notably, OmegaUse attains record 96.3% accuracy on ScreenSpot-V2 and leading 79.1% step success rate on AndroidControl."
        },
        {
            "title": "2 Related Work",
            "content": "Recent years have witnessed rapid progress in GUI agents, which are models that perceive GUI states (e.g., screenshots and/or structured UI representations) and execute actions (e.g., clicking, typing, and scrolling) to accomplish user goals. In this section, we review prior work along two main axes: (1) UI grounding and GUI perception; and (2) GUI agent architectures, including modular pipelines and native (end-to-end) agent models. 2.1 UI Grounding and GUI Perception Accurate UI grounding, which aligns natural language references with specific GUI elements on the screen, is widely recognized as core bottleneck for GUI agents. representative line of work focuses on grounding-centric models that localize UI elements directly from screenshots and instructions, while establishing standardized evaluations for cross-platform generalization. Early approaches typically relied on supervised learning over annotated screenshots, predicting click points or bounding boxes conditioned on natural language instructions [12, 13, 14]. Representative efforts such as SeeClick [4] and subsequent grounding-oriented models [15, 13, 16, 17] demonstrated the feasibility of instruction-conditioned UI localization, but also revealed strong sensitivity to screen resolution, layout diversity, and domain shift. To better characterize these challenges, several benchmarks have been proposed. ScreenSpot [4] introduced cross-platform grounding evaluation across mobile, web, and desktop interfaces, while ScreenSpot-V2 [3] improves upon ScreenSpot by revising and correcting its original annotations. Follow-up datasets such as ScreenSpot-Pro [18] further emphasize small targets and professional workflows. Subsequent strong baselines, often reused across later agent studies, include OS-Atlas [3], Aguvis [19], and UGround [20]. Together, these works demonstrate that GUI grounding performance remains highly sensitive to resolution, layout diversity, and distribution shift. More recent work explores reinforcement-learning-style post-training for UI grounding, in which rewards directly reflect spatial correctness to improve generalization and reduce dependence on dense annotations. Examples include UI-R1 [21], GUI-R1 [22], InfiGUI-R1 [23], and coordinate-free grounding approaches such as GUI-Actor [24]. Related variants investigate reward modeling and policy optimization strategies tailored to GUI grounding, including GUI-G2 [11] and InfiGUI-G1 [23]. 2.2 GUI Agent Architectures: Modular Pipelines vs. Native Agents Early and many contemporary GUI agents adopt modular architectures, decomposing the overall problem into separate components for perception, planning, memory, and execution. Agent-S [25], Agent-S3 [26] and Cradle [27] exemplify framework-centric designs that leverage strong foundation model for planning and reflection, while relying on explicit modules such as prompted planners, memory buffers, verifiers, and tool wrappers to improve controllability and interpretability. MobileAgent [28, 29, 30] follows similar decomposition for mobile environments, using vision-based perception to reduce reliance on platform metadata. OS-Symphony [31] and GTA1 [32] both advance computer-using agent frameworks by improving robustness and generalization for GUI-based tasks through careful system design and enhanced inference-time scaling. common pattern in these systems is to incorporate strong grounding models and optionally UI parsers, such as OmniParser [33], to obtain structured UI representations for downstream planning. However, modular pipelines are prone to error accumulation across components and often require extensive hand engineering to support diverse applications and long-horizon tasks. In contrast, recent work has increasingly shifted toward native or end-to-end GUI agents, which unify perception, reasoning, and action within single model. AutoWebGLM [34] and UI-TARS [2] frame this shift as analogous to end-to-end tool-using agents, arguing that unified policies can more effectively leverage large-scale data and reinforcement-learning signals. AutoGLM [35] introduces an intermediate interface to decouple planning from grounding and proposes progressive, selfevolving online curriculum reinforcement-learning framework for web and mobile GUI control. UI-TARS-2 [36] further emphasizes multi-turn reinforcement learning as key driver of performance gains, enabling agents to optimize long-horizon behavior and recover from intermediate errors. AgentCPM-GUI [37] targets efficient on-device mobile GUI interaction by introducing compact action space and three-stage training pipeline. Step-GUI [38] proposes self-evolving training pipeline and couples it with hierarchical GUI-MCP protocol to enable standardized, privacypreserving execution across heterogeneous devices. OpenCUA [39] provides open foundations for computer-use agents, including datasets, evaluation protocols, and strong baselines. Mano [40] investigates training strategies and system designs for general computer use, including iterative improvement and evaluation-oriented components that bridge framework-based approaches and endto-end policy learning. UI-Venus [5] further highlights the central role of data quality and trajectory curation in driving performance gains. MAI-UI [41] explicitly emphasizes deployment considerations, including agentuser interactive operation and MCP-augmented tool use. Although these methods enable implicit planning and memory to emerge from multi-step trajectory training, they also introduce challenges in training stability and environment scalability."
        },
        {
            "title": "3 Methodology",
            "content": "Our training paradigm uses decoupled design with two specialized models: (i) grounding model for high-precision visual perception and (ii) navigation model for sequential decision-making. Figure 2 illustrates the overall framework architecture. This separation enables targeted optimization and reduces interference between low-level spatial grounding and high-level reasoning. 3.1 OmegaUse-G: Foundation of Visual Perception The grounding model is designed to map textual queries to precise spatial coordinates on the UI. We first describe the data construction process for the grounding model and then present the corresponding training strategy. 4 Figure 2: The overall architecture of the OmegaUse framework. The pipeline proceeds through four distinct layers: (1) hybrid data processing stage integrating automated LLM-assisted annotation and human-in-the-loop refinement; (2) SFT of an MoE foundation model; (3) decoupled RL using GRPO with tailored rewards for grounding and navigation tasks; and (4) final deployment of the optimized agents across diverse application environments. 3.1.1 Grounding Data Pipeline We aggregated diverse GUI grounding corpus by consolidating six publicly available datasets: Aguvis [19], UI RefExp [42], Widget Captioning [43], SeeClick [4], Uground [20], and OSAtlas [3]. As summarized in Table 1, these sources provide comprehensive coverage of mobile, web, and desktop interfaces. The combined raw pool contains approximately 1.66 million instances. Table 1: Statistics of the GUI grounding datasets used in our study. The raw pool of 1.66M instances was distilled into 111k high-quality training set. Dataset Platform Raw Samples Aguvis UI RefExp Widget Captioning SeeClick Uground OS-Atlas Mobile Mobile Mobile Web Web Desktop Total Raw Pool Final Sampled Set Mixed - 110k 16k 40k 250k 750k 490k 1.66M 111k Despite the large scale of existing open-source datasets, we observe that nearly 40% of raw instances contain substantial noise, including misaligned bounding boxes and ambiguous textual prompts. These issues are particularly prevalent in datasets whose labels are automatically extracted from HTML or accessibility trees, where rendering offsets frequently introduce spatial inaccuracies. Prior studies [5] have shown that data quality critically affects grounding performance, and fully automated filtering methods often struggle to reliably identify high-quality examples. To address these bottlenecks, we employed manual inspection and correction pipeline. We first eliminate redundant or overly simplistic samples, followed by downsampling to retain 300K instances. Subsequently, we manually realign shifted bounding boxes and rephrase ambiguous or meaningless instructions to enforce precise one-to-one correspondence between visual elements 5 and their textual descriptions. Besides, samples containing blurred images or inherently ambiguous instructions are strictly filtered out. This rigorous refinement process yields curated dataset of 111K high-quality samples, ensuring that the model is trained with reliable supervision signals. 3.1.2 Two-Stage Grounding Training To optimize the spatial reasoning and localization precision of our model, we adopt hierarchical training paradigm using the manually refined grounding dataset. We partition the dataset into transition from foundational coordinate formatting to high-precision reinforcement refinement. (1) Policy Initialization (SFT): In the first stage, we perform SFT to establish the fundamental capability of the model to interpret instructions and output spatial coordinates in the standard [xmin, ymin, xmax, ymax] format. This phase ensures that the model masters basic task logic and syntax across mobile, and PC platforms before entering the reinforcement stage. (2) Reinforcement Learning for Spatial Precision: Building upon the SFT baseline, we employ reinforcement fine-tuning using the GRPO framework. GRPO enhances training stability by estimating baselines through relative rewards within groups, significantly reducing the computational overhead typically associated with separate critic model. Specifically, for each training prompt q, GRPO samples group of rollouts {o1, o2, ..., oG} from the old policy πθold. The advantage ˆAi for each rollout is computed by normalizing the rewards within the group: The policy is then optimized by maximizing the following objective function: ˆAi = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) JGRP O(πθ) = qQ,{oi}G i=1πθold 1 (cid:88) i=1 1 oi oi (cid:88) t=1 Lclip(θ) βDKL(πθπref ) (1) (2) where Lclip(θ) represents the surrogate objective with clipping mechanism to prevent excessive policy updates, and the KL divergence term with coefficient β constrains the policy from diverging from the reference model πref . For grounding task, we select classic dual-component reward function to calibrate the models spatial perception [5]: 1). Format Reward (Rf mt): binary reward that validates whether the predicted string conforms to the predefined syntax, ensuring the model outputs executable and parsable responses. 2). Inside-of-Bounding-Box Reward (Rpos): This reward targets localization accuracy by incentivizing the model to predict center point (x, y) that falls strictly within the ground-truth interactive region [xmin, ymin, xmax, ymax]. The reward is formulated as follows: Rpos = (cid:26)1 0 if x1 x2 and y1 y2 otherwise (3) 3). Total Reward Balancing: To synchronize structural correctness with action precision, the final action-wise reward is computed as weighted combination: = Rf mt w1 + Rpos w2 (4) By carefully balancing the weights w1 and w2, we prevent potential reward conflicts where the model might sacrifice format for precision or vice-versa, ultimately leading to more robust and coherent grounding policy. 3.2 OmegaUse: Advanced Planning and Navigation In this section, we detail the design and training of OmegaUses navigation model, thereby operationalizing our high-quality data construction and decoupled training paradigm. We first present hierarchical navigation data pipeline that integrates three complementary sources: (1) rigorously curated open-source datasets, (2) automatically synthesized trajectories via bottom-up autonomous exploration and top-down taxonomy-guided generation, and (3) high-fidelity cross-terminal expert demonstrations. We then describe two-stage optimization strategy, consisting of SFT to establish foundational interaction syntax and task logic, followed by GRPO with specialized reward designs to refine spatial grounding and sequential decision-making. Table 2: Unified Action Space Of OmegaUse Across Different Platforms. Platform Action Schema Functional Definition Click(box=(x, y)) Drag(start, end) Scroll(start, end, dir) Type(content=) Shared Wait() Finished(content=) Hotkey(key=[, ...]) LeftDouble(box=(x, y)) RightSingle(box=(x, y)) Desktop Web Hover(box=(x, y)) BrowserStop() Mobile LongPress(box=(x, y)) PressBack() PressHome() PressEnter() Performs single-tap or left-click at the given coordinates. Executes drag-and-drop sequence from start point (x1, y1) to end point (x2, y2). Scrolls from (x1, y1) to (x2, y2) in the given direction. Injects the specified text string into the active input focus. Suspends execution to allow for UI state synchronization. Terminates the task and returns the final result. Simulates hardware keyboard combinations. Executes double-click at (x, y). Executes right-click at (x, y). Moves the mouse cursor to specific point. Interrupts the current page loading process. Long presses at (x, y). Navigates to the previous screen. Returns the device to the primary home screen. Presses the enter key. 3.2.1 Unified Action Space To ensure consistent navigation across diverse platforms, we propose unified action space that standardizes interaction primitives across mobile, desktop, and web platforms. This design organizes agent operations hierarchically, with core set of shared actions for universal GUI interaction and platform-specific extensions tailored to the unique affordances of each terminal. As detailed in Table 2, the shared primitives establish cross-platform baseline (e.g., click, drag, and type), while specialized actions address terminal-unique requirementssuch as desktop hotkeys or mobile system gestures. By harmonizing these disparate operational schemas into single cohesive space, the model achieves robust cross-terminal generalization. When synchronized with our hierarchical task taxonomy, this architecture enables the agent to execute complex trajectories with unified logical reasoning regardless of the underlying digital ecosystem. 3.2.2 Hierarchical Navigation Data Pipeline To bridge the gap between low-level visual perception and high-level logical planning, we construct large-scale, multi-platform navigation dataset using hierarchical three-pronged approach: (1) rigorous curation of open-source data, (2) automated trajectory synthesis in virtual sandboxes, and (3) high-fidelity expert demonstrations across multiple terminals. (1) Open-source Data Curation and Auditing: We leverage the AGUVIS [19] stage-2 collection to construct our foundational interaction dataset, which aggregates diverse array of GUI execution trajectories from both mobile and web terminals, such as AITW [44] and Mind2Web [45]. However, these open-source datasets frequently suffer from significant noise, including misaligned coordinates and fragmented action chains, which can adversely impact model performance if utilized directly. To mitigate these issues [5], we implement two-stage quality control pipeline: Initially, we apply rule-based filtering to eliminate obvious noise and uninformative samples. This involves: (i) enforcing minimum trajectory length threshold (e.g., > 3 steps) to ensure the presence of sufficient learning signals; and (ii) detecting and discarding trajectories characterized by redundant or repetitive action patterns, which typically indicate agent stalling or unproductive exploration. Subsequently, we employ MLLMs as high-level trajectory auditor to perform task-completion verification. For each candidate trajectory, the auditor is provided with the specific user goal and the complete execution trace, which includes step-wise action descriptions paired with their corre7 Figure 3: Overview of the Exploration-driven (Bottom-up) data construction pipeline. (a) Triples Collection: Gathering raw interaction primitives < pre state, action, post state > through autonomous application exploration. (b) State Transition Graph Construction: Organizing interaction traces into structured graph with MLLM-based semantic clustering to merge redundant UI states. (c) Trajectory Extraction: Sampling diverse execution paths while enriching them with natural language task goals and step-wise action interpretations. sponding UI screenshots. By jointly analyzing the linguistic intent of the actions and the visual state transitions, the model judges whether the sequence of operations successfully fulfills the original task. Trajectories identified as incomplete or logically inconsistent are strictly filtered out. (2) Automated Trajectory Synthesis: To expand the diversity and robustness of our navigation dataset, we implement an automated synthesis framework within simulation environments. We utilize two complementary strategies to balance dataset coverage and task complexity: an Explorationdriven (Bottom-up) approach for autonomous UI discovery, and Taxonomy-guided (Top-down) approach for generating sophisticated tasks based on expert knowledge. Exploration-driven Synthesis (Bottom-up): To overcome the critical challenges of designing realistic task goals and obtaining diverse execution paths, we implement systematic bottom-up data construction pipeline as illustrated in Figure 3. This approach automates high-quality data generation through four-stage process: interaction exploration, state aggregation, trajectory extraction, and semantic enrichment. Taking the mobile environment as an example, we employ Depth-First Search (DFS) strategy to explore individual applications within simulator [46]. By interacting with UI elements parsed from the Accessibility Tree, the agent collects raw interaction samples in the form of triples: < pre state, action, post state >. Each state is captured as screenshot, while actions describe specific user behaviors such as clicking or text input. unique identification and hashing mechanism based on UI structures and action encoding is utilized to avoid redundant exploration of historical states. To organize these exploration traces into structured framework, we construct state transition graph, where each node represents unique UI state and each directed edge denotes specific action leading from one state to another. Recognizing the potential for structural redundancy in raw graphs, we introduce MLLMs-based state clustering and compression mechanism. MLLMs are utilized to perform semantic understanding of screenshots, enabling the system to judge whether multiple nodes belong to the same functional page, such as Settings pages with minor visual variations. These redundant nodes are merged into virtual nodes to reduce the graph scale and significantly improve subsequent computational efficiency. Based on the refined state transition graph, we perform trajectory extraction by enumerating multiple reachable paths from the initial state. To ensure the logical coherence of the generated data, we implement cycle-avoidance strategy that maintains visit set for each path, skipping branches that would lead to unproductive UI loops, such as Settings Back Settings. Following trajectory extraction, we utilize MLLMs for semantic enrichment at two levels: action interpretation and task goal generation. Each triple is translated into natural language description, such as Click the Settings button in the top right corner, while the entire action sequence is abstracted into coherent task objective, like Modify notification permissions in the settings menu. This mapping from execution trajectories to high-level linguistic goals provides the core supervision signal required to train the agent for robust instruction-to-action generation. Taxonomy-guided Generation (Top-down): We propose taxonomy-guided generation framework and apply it across desktop and mobile environments to ensure comprehensive coverage of diverse real-world interaction behaviors. For each kind of environment, we design specialized hierarchical task taxonomy grounded in its unique ecosystem and typical usage patterns. Guided by these taxonomies, task descriptions are generated and subsequently executed by high-capability expert model within our unified simulation environments. The agent performs self-assessment of execution correctness based on real-time environmental feedback, and trajectories with successful outcomes are recorded as candidate samples. As representative instance, Table 3 shows hierarchical taxonomy developed for typical daily GUI usage patterns in desktop environment. To ensure annotation reliability, we build human-in-the-loop verification platform in which human annotators verify cases that the model has labeled as successful. This design reduces noise, prevents the accumulation of errors from automatic labeling, and improves the overall robustness of the dataset. In addition, we collect two types of failure cases: (i) those that LLM itself judges as unsuccessful and (ii) those it incorrectly judges as successful. These model-generated failure cases are then handed over to human annotators for careful relabeling, resulting in curated diagnostic failure subset of challenging GUI tasks that even state-of-the-art closed-source models such as LLM fail to solve. (3) Cross-Terminal Expert Demonstrations: To establish high-quality data for the navigation model, we also implement an expert demonstration pipeline for desktop and mobile environments. Based on the hierarchical taxonomies established for each terminal, we utilize LLMs to synthesize task instructions through divergent reasoning. To ensure task depth and challenge, we enforce strict complexity constraint, requiring each instruction to involve at least five operational steps. Before the annotation phase, human experts manually vet these prompts for logical validity and environmental feasibility, refining or discarding any substandard entries to ensure high-quality instruction pool. The verified instructions are subsequently distributed to our proprietary annotation platform, where professional annotators perform step-by-step executions within simulation environments until task completion. This process ensures that the resulting trajectories capture precise state transitions and aligned action sequences. To guarantee maximum data reliability, we implement two-tier quality assurance protocol where each completed trajectory must undergo independent audit by two inspectors. Only samples passing both goal alignment and logical consistency checks are finally retained, resulting in high-fidelity expert demonstration dataset that serves as robust foundation for model training and benchmarking. 3.2.3 Two-Stage Navigation Training To develop robust GUI navigation agent capable of complex multi-step planning, we implement two-stage optimization paradigm. This strategy leverages massive initial corpus for general behavioral alignment followed by high-precision reinforcement learning on expert-verified data. (1) Policy Initialization (SFT): The navigation model is first trained using SFT on diverse dataset of approximately 260K instances. This corpus consists of aggregated open-source navigation traces and automatically synthesized trajectories. This stage focuses on teaching the model the fundamental mapping between linguistic goals and cross-platform action sequences, establishing stable starting policy that adheres to the unified action space. At each step t, the agent receives the multimodal input Xt = {I, Vt, Ht}, where is the task instruction, Vt is the current screenshot, and Ht represents the historical reasoning traces. The agents response is structured as triplet Yt = (Ot, Tt, At): the observation Ot semantically describes the UI state; the thought Tt performs goal-oriented reasoning based on I; and the action At provides the executable code snippet conforming to our unified action space. This pipeline ensures each action is grounded in explicit perception and logical planning. (2) Reinforcement Learning for Decision Robustness: Based on the SFT baseline, we also employ reinforcement learning using the GRPO framework. To provide the fine-grained feedback, we design multi-dimensional reward function that assesses both structural integrity and operational logic: 9 Table 3: Hierarchical task taxonomy for desktop GUI Navigation. This taxonomy guides the topdown generation process to ensure diverse coverage of real-world user scenarios. Core Functionalities and Sub-scenarios Domain Desktop Office Browser & Web Document Editing, Spreadsheet Processing, Presentation Creation, PDF Workflows, Collaboration & Sharing. Tab Management, Privacy & Security, Browser Extensions, Account Sync, Developer Tools. Communication Instant Messaging, Meetings & Remote Collaboration, Email, Calendar Integration. File Management Search & Indexing, Compression, Archive Management, Storage Sync, External Media Operations. System Operations Display & Device Settings, Network Connectivity, Power & Updates, Software Management, Notifications & Focus. Media & Ent. Image Editing, Media Playback, Content Library Management. DevOps & Tech Development Environments, Version Control, System Technical Operations, Deployment. Productivity Tools Screen Capture, Notes & Tasks, Calculator, Time Management, Desktop Enhancements. Security & Privacy Account Access Security, System Protection, Encryption, Privacy Shielding. 1). Format Reward (Rf mt): This reward validates whether the output strictly conforms to the required template, ensuring reasoning and actions are correctly enclosed within structured tags. 2). Action-wise Reward (Ract): This component evaluates the execution logic and is further decomposed into: Type Accuracy (Rtype): binary reward for matching the correct action primitive (e.g., Click vs. Scroll). Coordinate Precision (Rcoord): For spatial actions, we apply stepwise reward based on the distance between the predicted and ground-truth coordinates: Rcoord = 1.0 0.5 0 if x, < θ1, if θ1 x, < θ2, otherwise. (5) where = xpred xgt and = ypred ygt denote the absolute differences between the predicted coordinates and the ground truth along the and axes, respectively. The parameters θ1 and θ2 serve as predefined distance thresholds that determine the precision of the agents spatial grounding during coordinate-based actions such as Click or LeftDouble. For the Drag action, the reward Rdrag is calculated based on the coordinate deviations of both the start and end points: Rdrag = 1.0 0.5 0 if max(x1, y1, x2, y2) α1 if α1 < max(x1, y1, x2, y2) α2 otherwise (6) where xi = xi,pred xi,gt and yi = yi,pred yi,gt represent the absolute errors for the start (i = 1) and end (i = 2) coordinates. For the Scroll action, the reward Rscroll incorporates both spatial precision and directional accuracy: if max(x1, y1, x2, y2) β1, and dirpred = dirgt if β1 < max(x1, y1, x2, y2) β2 ,and dirpred = dirgt otherwise 1.0 0.5 0 Rscroll = (7) where dirpred and dirgt denote the predicted and ground-truth scroll directions. This formulation ensures the agents scrolling behavior is both spatially grounded and semantically correct. Content Fidelity (Rcontent): For typing tasks, the reward is determined by the token-level F1-score of the predicted string S1 relative to the ground-truth target S2 Rcontent = (cid:26)1.0 0 if F1-score 0.5, otherwise. (8) For the Hotkey action, the reward Rhotkey is defined by binary matching criterion, requiring the predicted key combination to be identical to the ground truth: Rhotkey = (cid:26)1.0 0 if Kpred = Kgt, otherwise. (9) where Kpred and Kgt represent the predicted and ground-truth hotkey parameter sets (e.g., [ctrl, c]). Given that hotkey operations are sensitive to exact key combinations, this strict matching ensures the agent executes the precise system-level command intended. 3). Total Reward Balancing: The final reward for each step is weighted sum that balances structural consistency with action accuracy: where w3 and w4 are hyper-parameters tuned to prevent the model from sacrificing action precision for format compliance or vice-versa. = Rf mt w3 + Ract w4 (10)"
        },
        {
            "title": "4 Offline Benchmarks for Real-World GUI Navigation",
            "content": "To facilitate evaluation of agent performance in realistic digital environments, we introduce OS-Nav, specialized offline benchmark comprising two sub-benchmarks across different operating systems: ChiM-Nav, focusing on Chinese Android mobile systems, and Ubu-Nav, targeting routine desktop interactions on Ubuntu. The benchmark is open-sourced, and can be publicly accessed 3. To ensure the reliability of state transitions and the transparency of agent logic, both benchmarks were developed using rigorous human-AI collaborative pipeline. We curated expert-labeled execution traces to ensure all tasks reflect authentic user behavior. For every step, we utilized MLLMs to synthesize intermediate CoT descriptions, providing semantic bridge between linguistic goals and raw actions. Every trajectory, including the AI-generated reasoning, underwent final refinement by human experts to ensure the gold labels are logically sound and environment-feasible. 4.1 ChiM-Nav: Chinese Mobile Navigation Benchmark The ChiM-Nav benchmark assesses an agents ability to navigate popular applications within the Chinese mobile ecosystem. This suite comprises 142 trajectories across 69 distinct applications, totaling 991 operational steps. With an average trajectory length of 6.98 steps, the benchmark emphasizes daily usage scenarios and evaluates the agents robustness against the unique UI layouts and multi-step workflows characteristic of Chinese digital platforms. 4.2 Ubu-Nav: General Desktop Navigation Benchmark The Ubu-Nav benchmark consists of 101 trajectories with total of 641 steps, targeting agent performance in Ubuntu environments. Trajectories in this benchmark range from 2 to 11 steps, with an average length of 6.35 steps per task. It covers extensive routine desktop operations and typical system interactions, focusing on the multi-step reasoning required for common PC tasks."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate OmegaUse on set of grounding and navigation benchmarks across mobile and desktop platforms. Our experiments validate the contributions of our high-quality data construction pipeline, the decoupled training strategy, and cross-terminal generalization on OS-Nav. 3https://huggingface.co/datasets/baidu-frontier-research/OS-Nav 11 Table 4: Performance comparison on ScreenSpot-V2 dataset. The Avg. column represents the overall success rate across all categories. Models Closed-source Models GPT-4o [47] UI-TARS-1.5 [48] Seed1.5-VL [49] GUI-specific Models (SFT) SeeClick-9.6B [4] ShowUI-2B [17] UGround-7B [20] OS-Atlas-7B [3] Aguvis-7B [19] UI-TARS-7B [2] UI-TARS-72B [2] JEDI-7B [50] GUI-Actor-7B [24] OpenCUA-7B [39] OpenCUA-32B [39] GUI-specific Models (RL) UI-R1-E-3B [21] SE-GUI-7B [51] LPO [52] GUI-G2-7B [11] Phi-Ground-7B-16C-DPO [53] GTA1-7B [32] GTA1-72B [32] UI-Venus-Ground-7B [5] UI-Venus-Ground-72B [5] OmegaUse-G Mobile Desktop Web Text Icon/Widget Text Icon/Widget Text Icon/Widget 26.6 - - 78.4 92.1 75.1 95.2 89.3 96.9 94.8 96.9 97.6 - - 98.2 - 97.9 - 96.5 99.0 99.3 99.0 99.7 99.3 24.2 - - 50.7 75.4 84.5 75.8 68.7 89.1 86.3 87.2 88.2 - - 83.9 - 82.9 - 62.0 88.6 92.4 90.0 93.8 94. 24.2 - - 70.1 78.9 85.1 90.7 80.6 95.4 91.2 95.9 96.9 - - 94.8 - 95.9 - 90.2 94.9 97.4 97.0 95.9 99.0 19.3 - - 29.3 78.9 61.4 63.6 67.9 85.0 87.9 87.9 85.7 - - 75.0 - 86.4 - 76.4 89.3 89.3 90.7 90.0 96.4 12.8 - - 55.2 84.2 84.6 90.6 89.3 93.6 91.5 94.4 93.2 - - 93.2 - 95.6 - 93.6 92.3 95.3 96.2 96.2 97. 11.8 - - 32.5 61.1 71.9 77.3 70.0 85.2 87.7 84.2 86.7 - - 83.7 - 84.2 - 75.9 86.7 91.4 88.7 92.6 94.0 Avg 20.1 94.2 95. 55.1 77.3 76.3 84.1 80.5 91.6 90.3 91.7 92.1 92.3 93.4 89.5 90.3 90.5 93.3 83.8 92.4 94.8 94.1 95.3 96.3 5.1 Experimental Setup 5.1.1 Model Configurations We employed 30B-A3B VL model as the backbone of OmegaUse. In the SFT phase, we fine-tuned the model for one epoch using learning rate of 1e5, global batch size of 32, and temperature of 1.0. For the subsequent Grounding and Navigation RL phase, we trained for one epoch with learning rate of 5e5, global batch size of 64, and temperature of 1.0. Specifically for RL, we utilized 8 sampled responses per instruction and set the KL penalty coefficient β to 0.04. Across both phases, we maintained an MoE auxiliary loss coefficient of 1e6 and maximum image token limit of 16, 384. 5.2 Evaluation of GUI Grounding We evaluate the grounding performance of our OmegaUse model across two major benchmarks: ScreenSpot-V2 and ScreenSpot-Pro. These benchmarks test the models ability to associate natural language instructions with diverse UI elements across mobile, web, and desktop platforms. ScreenSpot-V2. As fundamental GUI grounding benchmark, ScreenSpot-V2 measures the agents localization reliability across mobile, web, and desktop interfaces. As shown in Table 4, OmegaUse achieves an exceptional state-of-the-art average score of 96.3%, establishing new performance It consistently outperforms leading baselines, including UI-Venusceiling for this benchmark. Ground-72B (95.3%) and Seed1.5-VL (95.2%). detailed breakdown reveals that OmegaUse-G maintains near-perfect accuracy on text-based elements, particularly in the mobile and desktop segments, where it scores 99.3% and 99.0%, respectively. Furthermore, its performance on icon and widget localization remains remarkably high, reaching 96.4% on desktop and 94.0% on web platforms, demonstrating robust cross-platform generalization and precise spatial perception. ScreenSpot-Pro. Compared to standard GUI grounding benchmarks, ScreenSpot-Pro presents more rigorous evaluation by featuring high-resolution interfaces from professional software, often 12 Table 5: Performance comparison of different agent models on ScreenSpot-Pro. The Avg. column represents the overall success rate across all categories. Creative Scientific Office CAD Dev OS Model Avg. Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Closed-source Models GPT-4o [47] Claude Computer Use [54] UI-TARS-1.5 [48] Seed1.5-VL [49] GUI-specific Models (SFT) SeeClick-9.6B [4] FOCUS-2B [55] CogAgent-18B [1] Aria-UI [56] OS-Atlas-7B [3] ShowUI-2B [17] UGround-7B [20] UGround-V1-7B [20] UI-TARS-7B [2] UI-TARS-72B [2] JEDi-7B [50] GUI-Actor-7B [24] OpenCUA-7B [39] OpenCUA-32B [39] GUI-specific Models (RL) UI-R1-E-3B [21] UI-R1-7B [21] InfiGUI-R1-3B [23] GUI-G1-3B [10] SE-GUI-7B [51] Phi-Ground-7B-16C-DPO [53] GUI-G2-7B [11] UI-TARS-1.5-7B [48] GTA1-7B[32] GTA1-72B [32] UI-Venus-Ground-7B [5] UI-Venus-Ground-72B [5] 2.0 14.5 - - 2.5 7.6 7.1 7.6 12.2 2.5 14.2 15.8 20.8 18.8 38.0 - - - 37.1 23.9 33.0 39.6 51.3 26.9 55.8 - 53.3 56.9 60.4 66.5 0.0 3.7 - - 0.0 3.1 3.1 1.6 4.7 0.0 1.6 1.2 9.4 12.5 14.1 - - - 12.5 6.3 14.1 9.4 42.2 17.2 12.5 - 17.2 28.1 21.9 29.7 1.3 22.0 - - 0.6 22.8 14.9 16.2 33.1 16.9 26.6 51.9 58.4 62.9 42.9 - - - 46.1 49.4 51.3 50.7 68.2 70.8 68.8 - 66.9 79.9 74.7 84.4 0.0 3.9 - - 0.0 1.7 0.7 0.0 1.4 1.4 2.1 2.8 12.4 17.2 11.0 - - - 6.9 4.8 12.4 10.3 19.3 16.7 17.2 - 20.7 33.1 24.1 33.1 1.0 25.9 - - 1.0 23.7 9.6 23.7 28.8 9.1 27.3 47.5 50.0 57.1 50.0 - - - 41.9 38.9 44.9 36.6 57.6 56.6 57.1 - 62.6 73.2 63.1 73.2 0.0 3.4 - - 0.0 1.7 0.0 2.1 2.8 0.0 2.8 9.7 9.1 15.4 11.9 - - - 4.2 8.4 7.0 11.9 9.1 13.3 15.4 - 18.2 20.3 14.7 30.8 2.1 33.9 - - 3.5 25.0 22.2 27.1 37.5 13.2 31.9 57.6 63.9 64.6 72.9 - - - 56.9 55.6 58.3 61.8 75.0 58.0 77.1 - 76.4 81.9 76.4 84.7 0.0 15.8 - - 0.0 7.1 1.8 6.4 7.3 7.3 2.7 14.5 31.8 20.9 25.5 - - - 21.8 11.8 20.0 30.0 28.2 29.1 24.5 - 31.8 38.2 31.8 42.7 1.1 30.1 - - 1.1 23.2 13.0 20.3 33.9 15.3 31.6 60.5 63.3 63.3 75.1 - - - 65.0 58.7 65.5 67.2 78.5 76.4 74.0 - 82.5 85.3 75.7 83.1 0.0 16.3 - - 0.0 7.7 0.0 1.9 5.7 7.5 11.3 13.2 20.8 26.4 47.2 - - - 26.4 26.4 28.3 32.1 43.4 44.0 32.7 - 50.9 49.1 41.5 60.4 0.0 11.0 - - 2.8 17.8 5.6 4.7 27.1 10.3 17.8 38.3 30.8 42.1 33.6 - - - 32.7 42.1 43.9 23.5 49.5 55.1 57.9 - 48.6 73.8 49.5 75.7 0.0 4.5 - - 0.0 2.5 0.0 0.0 4.5 2.2 0.0 7.9 16.9 15.7 16.9 - - - 10.1 16.9 12.4 10.6 25.8 25.8 21.3 - 25.9 39.1 22.5 36.0 0.8 17.1 61.6 60.9 1.1 13.3 7.7 11.3 18.9 7.7 16.5 31.1 35.7 38.1 39.5 44.6 50.0 55. 33.5 - 35.7 37.1 47.3 43.2 47.5 49.6 50.1 58.4 50.8 61.9 OmegaUse-G 48.73 23.44 78.57 31. 66.67 22.38 75.69 34.55 81.36 47. 74.77 43.82 55.47 characterized by intricate and microscopic visual elements. In this challenging setting, as detailed in Table 5, OmegaUse-G achieves competitive average score of 55.47%. While ultra-large-scale models such as UI-Venus-Ground-72B (61.9%) and GTA1-72B (58.4%) maintain lead in overall performance, OmegaUse demonstrates specialized strengths in specific domains. Notably, it achieves the highest accuracy in the OS-Icon category (43.82%), outperforming all baseline models. Furthermore, it attains runner-up performance in several key metrics, including 74.77% in OS-Text, 31.72% in Dev-Icon, and 66.67% in Creative-Text. These results indicate that despite smaller parameter scale compared to 72B-class models, OmegaUse-G exhibits robust precision in professional and system-level GUI environments, particularly in capturing fine-grained icon details and complex text layouts within creative and developer tools. 5.3 Evaluation of GUI Navigation Navigation performance is evaluated on both widely used standard benchmarks and our specialized offline benchmark OS-Nav. 5.3.1 Standard Benchmark We evaluate the multi-step decision-making and planning capabilities of OmegaUse across two widely-adopted benchmarks: AndroidControl [7] for offline trajectory planning and AndroidWorld [57] for online interaction. These evaluations assess the models ability to translate high-level user goals into coherent, executable action sequences. Offline Benchmark. We further assess the agents fundamental planning and task decomposition capabilities using the AndroidControl dataset, which provides high-level instructions that require significant summarization and reasoning. According to the results in Table 6, OmegaUse achieves SOTA performance, securing the first place in both evaluated metrics. 13 Table 6: Performance comparison on the AndroidControl offline UI navigation dataset. Model Type Acc. (%) Step SR (%) Open-source Models SeeClick [4] OS-Atlas-7B [3] Aguvis-7B [19] Aguvis-72B [19] OS-Genesis-7B [58] UI-TARS-7B [2] UI-TARS-72B [2] GUI-R1-7B [22] NaviMaster-7B [59] UI-AGILE-7B [60] AgentCPM-GUI [37] UI-Venus-Navi-7B [5] UI-Venus-Navi-72B [5] OmegaUse 82.9 85.2 66.2 83.7 85.2 71.6 72.9 80.1 77.7 86.5 85. 87.6 59.1 71.2 61.5 66.4 44.5 72.5 74.7 51.7 54.0 60.6 69.2 76.1 77.2 79.1 Table 7: Performance comparison on AndroidWorld for end-to-end models. Success Rate Models Planner A11y Tree Screenshot Closed-source Models GPT-4o [47] ScaleTrack [61] SeedVL-1.5 [49] UI-TARS-1.5 [48] Open-source Models GUI-Critic-R1-7B [62] Qwen2.5-VL-72B [6] UGround [20] Aria-UI [56] UI-TARS-72B [2] GLM-4.5v [63] UI-Venus-Navi-7B [5] UI-Venus-Navi-72B [5] OmegaUse 30.6 44.0 62.1 64.2 27.6 35.0 44.0 44.8 46.6 57. 49.1 65.9 55.7 Specifically, OmegaUse reaches Type Accuracy of 87.6% and Step Success Rate (SR) of 79.1%. These scores surpass previous leading models such as UI-Venus-Navi-72B (85.9% Type Acc. / 77.2% Step SR) and UI-TARS-72B (85.2% Type Acc. / 74.7% Step SR). The superior performance on high-level instructions indicates that OmegaUse possesses more robust internal world model for GUI environments. Online Benchmark. To evaluate real-time interactive capabilities, we employ the AndroidWorld benchmark, which requires agents to navigate dynamic mobile environments. As shown in Table 7, OmegaUse achieves success rate of 55.7%. Notably, OmegaUse operates as streamlined end-toend agent, relying solely on screenshots without the assistance of external planners or Accessibility (A11y) trees. Despite using fewer input modalities, OmegaUse demonstrates competitive performance against several larger-scale open-source models. It outperforms UI-TARS-72B (46.6%) and Aria-UI (44.8%), while remaining comparable to the high-parameter GLM-4.5v (57.0%). While performance gap remains compared to state-of-the-art models such as UI-Venus-Navi-72B (65.9%), it is worth noting that UI-Venus-Navi-72B is dense model with much larger parameter size, whereas OmegaUse is MoE-based model with smaller overall parameter size. Table 8: Performance comparison on the ChiM-Nav offline navigation dataset. Model Type Acc. (%) Step SR (%) Open-source Models UI-TARS-SFT [2] UI-TARS-1.5 [48] GUI-R1-7B [22] OS-Atlas-7B [3] UI-AGILE-7B [60] AgentCPM-GUI [37] Holo2-30b-A3B [64] Qwen3-VL-30b-A3B [65] UI-Venus-72B [5] Qwen3-VL-32B [65] OmegaUse 53.28 64.12 63.74 59.63 70.2 75.02 73.76 78.2 81.23 80.83 87.78 36.97 37.24 34.74 38.26 45.96 51.62 60.69 65.19 67.51 66.39 74.24 Table 9: Performance comparison on the Ubu-Nav offline navigation dataset. Coord actions include Click, Drag, Scroll, LeftDouble, and RightSingle; Non-coord actions include Type, Hotkey, PressEnter, and Finish. Model Coord Actions (%) Non-coord Actions (%) Average (%) Open-source Models UI-TARS-7B-SFT [2] UI-TARS-1.5-7B [48] OS-Atlas-Pro-7B [3] Holo2-30B-A3B [64] Qwen3-VL-30B-A3B [65] UI-Venus-Navi-72B [5] OmegaUse 32.8 32.2 34.2 52.5 54.3 45.1 57.1 4.6 17.4 16.0 34.3 7.6 40. 48.6 28.9 30.2 31.7 50.0 47.7 44.4 55.9 5.3.2 Specialized Offline Benchmarks To further evaluate the agents generalization across diverse platforms and complex real-world workflows, we conduct experiments on our specialized OS-Nav offline benchmarks: ChiM-Nav for the Chinese mobile ecosystem and Ubu-Nav for Ubuntu desktop environments. ChiM-Nav (Mobile). This benchmark specifically targets the unique UI layouts and multi-step workflows found in popular applications within the Chinese mobile ecosystem. As shown in Table 8, OmegaUse achieves Type Accuracy of 87.78% and Step Success Rate (SR) of 74.24%, outperforming all existing open-source baselines. Notably, it surpasses the high-parameter UI-Venus-72b, which scores 81.23% Type Acc. and 67.51% Step SR. The significant lead in Step SR (a gain of approximately 6.7%) suggests that OmegaUse is more capable of maintaining reasoning consistency in this scene. Ubu-Nav (Desktop). The Ubu-Nav benchmark evaluates the agents proficiency in handling routine Ubuntu desktop operations across varied system interfaces. According to Table 9, OmegaUse reaches an average performance of 55.9%, establishing clear lead over the best-performing baseline, Holo2-30B-A3B (50.0%). breakdown of action types reveals that OmegaUse excels in both coordinate-based actions (Click, Drag, etc.) and non-coordinate actions (Type, Hotkey, etc.). Specifically, it achieves 48.6% in non-coordinate tasks, substantial improvement over UI-Venus-Navi-72B (40.0%) and Holo2-30B-A3B (34.3%). These results demonstrate that OmegaUse effectively bridges the gap between spatial perception and semantic command execution, even in complex desktop environments requiring multi-window coordination."
        },
        {
            "title": "6 Conclusion",
            "content": "In this report, we presented OmegaUse, high-performance autonomous GUI agent model capable of navigating complex tasks across mobile and desktop, supporting phone-use and computer-use scenarios. By adopting Mixture-of-Experts (MoE) backbone, we demonstrate that OmegaUse can maintain superior reasoning depth while significantly optimizing computational efficiency compared to dense models. To build reliable data foundation, we introduced carefully engineered dataconstruction pipeline that combines rigorously curated open-source datasets with an automated synthesis framework integrating bottom-up autonomous exploration and top-down taxonomy-guided generation, thereby producing high-fidelity training trajectories. To effectively leverage this curated data, we proposed decoupled two-stage training paradigm, combining SFT with GRPO, successfully calibrates the models spatial grounding and sequential planning through specialized reward mechanisms. Empirical results across multiple platforms validate the robustness of our approach. OmegaUse establishes new performance records on major benchmarks, notably achieving SOTA score of 96.3% on ScreenSpot-V2 and leading 79.1% Step success rate on AndroidControl. Furthermore, we introduce OS-Nav, an offline benchmark for real-world GUI navigation, to enable systematic evaluation of GUI agents in an offline setting. In particular, ChiM-Nav, Chinese GUI offline benchmark, provides the community with comprehensive evaluation suite to help bridge the assessment gap within the Chinese digital ecosystem. Additionally, Ubu-Nav is the first offline benchmark designed to evaluate computer-use agents on Ubuntu desktop workflows. Moving forward, we aim to extend OmegaUses capabilities to even more intricate, real-world workflows and explore more advanced safety constraints and self-correction mechanisms to ensure reliable and trustworthy autonomous GUI interaction."
        },
        {
            "title": "References",
            "content": "[1] Wenyi Hong, Weihan Wang, Qingsong Lv, et al. Cogagent: visual language model for In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern gui agents. Recognition (CVPR), 2024. [2] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [3] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [4] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93139332, 2024. [5] Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, et al. Ui-venus technical report: Building high-performance ui agents with rft. arXiv preprint arXiv:2508.10833, 2025. [6] Shuai Bai, Keqin Chen, Xuejing Liu, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. [8] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Zhihong Shao, Peiyi Wang, Qihao Zhu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [10] Yuqi Zhou, Sunhao Dai, Shuai Wang, et al. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. 16 [11] Fei Tang, Zhangxuan Gu, Zhengxi Lu, et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. [12] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. SetarXiv preprint of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv:2310.11441, 2023. [13] Yijun Qian, Yujie Lu, Alexander Hauptmann, and Oriana Riva. Visual grounding for user interfaces. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 97107, 2024. [14] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [15] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1.5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. [16] Anthony Nguyen. Improved gui grounding via iterative narrowing. arXiv preprint arXiv:2411.13591, 2024. [17] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1949819508, 2025. [18] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 8778 8786, 2025. [19] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [20] Rui Qian, Xin Yin, Chuanhang Deng, Zhiyuan Peng, Jian Xiong, Wei Zhai, and Dejing Dou. Uground: Towards unified visual grounding with unrolled transformers. arXiv preprint arXiv:2510.03853, 2025. [21] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [22] Run Luo, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [23] Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. [24] Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. [25] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. 17 [26] Gonzalo Gonzalez-Pumariega, Vincent Tu, Chih-Lun Lee, Jiachen Yang, Ang Li, and Xin Eric Wang. The unreasonable effectiveness of scaling agents for computer use. arXiv preprint arXiv:2510.02250, 2025. [27] Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. [28] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. [29] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. Advances in Neural Information Processing Systems, 37:26862710, 2024. [30] Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. [31] Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, et al. Os-symphony: holistic framework for robust and generalist computer-using agent. arXiv preprint arXiv:2601.07779, 2026. [32] Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. [33] Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1564115653, 2024. [34] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language modelbased web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 52955306, 2024. [35] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai Sun, Jiaqi Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. [36] Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. Ui-tars-2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025. [37] Zhong Zhang, Yaxi Lu, Yikun Fu, et al. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning. arXiv preprint arXiv:2506.01391, 2025. [38] Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, et al. Step-gui technical report. arXiv preprint arXiv:2512.15431, 2025. [39] Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025. [40] Tianyu Fu, Anyang Su, Chenxu Zhao, Hanning Wang, Minghui Wu, Zhe Yu, Fei Hu, Mingjia Shi, Wei Dong, Jiayao Wang, et al. Mano technical report. arXiv preprint arXiv:2509.17336, 2025. 18 [41] Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, et al. Mai-ui technical report: Real-world centric foundation gui agents. arXiv preprint arXiv:2512.22047, 2025. [42] Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, et al. Uibert: Learning generic multimodal representations for ui understanding. arXiv preprint arXiv:2107.13731, 2021. [43] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. arXiv preprint arXiv:2010.04295, 2020. [44] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. [45] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems, volume 36, pages 2809128114, 2023. [46] Yuanchun Li, Ziyue Yang, Yao Guo, and Xiangqun Chen. Droidbot: lightweight ui-guided test input generator for android. In 2017 IEEE/ACM 39th international conference on software engineering companion (ICSE-C), pages 2326. IEEE, 2017. [47] Raisa Islam and Owana Marzia Moushi. Gpt-4o: The cutting-edge advancement in multimodal llm. In Intelligent Computing-Proceedings of the Computing Conference, pages 4760. Springer, 2025. [48] ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025. [49] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [50] Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. [51] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. [52] Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, et al. Lpo: Towards accurate gui agent interaction via location preference optimization. arXiv preprint arXiv:2506.09373, 2025. [53] Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, et al. Phi-ground tech report: Advancing perception in gui grounding. arXiv preprint arXiv:2507.23779, 2025. [54] Anthropic. Developing computer use. https://www.anthropic.com/news/ developing-computer-use, 2024. Accessed: 2025-01-16. [55] Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing gui grounding via fast and slow systems. arXiv preprint arXiv:2503.06470, 2025. [56] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Ariaui: Visual grounding for gui instructions. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2241822433, 2025. 19 [57] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [58] Qiushi Sun, Kanzhi Cheng, Zichen Ding, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2025. [59] Zhihao Luo, Wentao Yan, Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, and Xin Tan. Navimaster: Learning unified policy for gui and embodied navigation tasks. arXiv preprint arXiv:2508.02046, 2025. [60] Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inference-time grounding. arXiv preprint arXiv:2507.22025, 2025. [61] Jing Huang, Zhixiong Zeng, Wenkang Han, Yufeng Zhong, Liming Zheng, Shuai Fu, Jingyuan Chen, and Lin Ma. Scaletrack: Scaling and back-tracking automated gui agents. arXiv preprint arXiv:2505.00416, 2025. [62] Yuyang Wanyan, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Jiabo Ye, Yutong Kou, Ming Yan, Fei Huang, Xiaoshan Yang, et al. Look before you leap: gui-critic-r1 model for pre-operative error diagnosis in gui automation. arXiv preprint arXiv:2506.04614, 2025. [63] GLM-V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, et al. Glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025. [64] Company. Holo2 - open foundation models for navigation and computer use agents, 2025. [65] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025."
        }
    ],
    "affiliations": [
        "Baidu Frontier Research Department"
    ]
}