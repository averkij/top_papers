{
    "paper_title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "authors": [
        "Jintao Tong",
        "Shilin Yan",
        "Hongwei Xue",
        "Xiaojun Tang",
        "Kunyu Shi",
        "Guannan Zhang",
        "Ruixuan Li",
        "Yixiong Zou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 0 4 0 6 0 . 2 0 6 2 : r SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs Jintao Tong1,2 Shilin Yan2 Hongwei Xue2 Xiaojun Tang2 Kunyu Shi2 Guannan Zhang2 Ruixuan Li1 Yixiong Zou1 1Huazhong University of Science and Technology 2Accio Team, Alibaba Group Project Leader Corresponding Author Abstract Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject fixed number of continuous hidden states as visual thoughts into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved visiontext reasoning. To enable this capability, we adopt hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods. Project Page: https://accio-lab.github.io/SwimBird Github Repo: https://github.com/Accio-Lab/SwimBird HuggingFace: https://huggingface.co/datasets/Accio-Lab/SwimBird-SFT-92K"
        },
        {
            "title": "Introduction",
            "content": "Building on the success of Chain-of-Thought (CoT) [32, 9] reasoning in LLMs, recent multimodal research has adopted step-by-step reasoning to decompose complex vision-and-language problems into intermediate steps that are easier to solve. With textual CoT, Multimodal Large Language Models (MLLMs) [44, 7, 16, 24] have significantly improved on tasks requiring symbolic manipulation, numerical calculation, and logical analysis. However, this success does not fully transfer to vision-dense tasks where the bottleneck lies in dense perception and spatial reasoning rather than logical structure [4]. Typical examples include maze solving, fine-grained visual search, and other problems where accurate intermediate visual states are essential. On such tasks, purely textual CoT [19] can be an ill-posed interface: the model is forced to describe intermediate visual evidence in language even when language is not faithful carrier, causing brittle reasoning and error accumulation [37]. To address this, recent works introduce latent visual reasoning [11, 22] that supervises models to generate semantically grounded continuous Figure 1: SwimBird enables query-adaptive multimodal reasoning by dynamically switching among text-only, vision-only, and interleaved visiontext modes. As illustrated, it avoids redundant latent steps on text-centric queries (Case 1), relies on latent visual thoughts for vision-dense spatial problems (Case 2), and interleaves visual grounding with textual deduction when both are needed (Case 3), mitigating modality mismatch and improving robustness. hidden states as visual thoughts, enabling intermediate visual representations to be maintained and updated across steps, which substantially strengthens performance on vision-dense benchmarks. Despite these advances, existing multimodal CoT designs largely rely on rigid, pre-defined reasoning pattern. Concretely, prior methods [25, 39, 36] typically fall into three fixed paradigms: text-only CoT, vision-only CoT, or interleaved visiontext CoT. As shown in Fig. 1, such fixed patterns create mismatch between the reasoning modality and the actual needs of the question: forcing visual thoughts for text-centric queries can interfere with discrete symbolic reasoning, while restricting strongly visual queries to text-only reasoning removes an appropriate latent workspace. Even interleaved reasoning remains fixed schedule that may generate redundant modality steps [23]. We argue that the core limitation is the assumption that single, static reasoning template can generalize across heterogeneous multimodal queries. Different questions demand different internal computation formats. Some require only discrete symbolic steps, some require only latent visual transitions, and some require tight alternation between visual grounding and textual deduction. more capable MLLM should therefore be able to choose when to think in language, when to think in vision, conditioned on the input and the evolving reasoning state. Motivated by this, we propose SwimBird, reasoning-switchable MLLM for query-adaptive multimodal reasoning. SwimBird is built on two key ideas derived from the limitations above. First, we adopt hybrid autoregressive formulation that supports both (i) standard next-token prediction for textual thoughts and (ii) next-embedding prediction for continuous visual thoughts. This unified generation interface provides the foundation for switchable reasoning. Second, we attribute the rigidity of prior patterns partly to training data bias. We therefore design systematic curation strategy that filters and categorizes multimodal CoT samples into reasoning modes based on their visual dependency and reasoning characteristics. Through this strategy, we construct SwimBird-SFT-92K, diverse supervised fine-tuning dataset covering text-only, vision-only, and interleaved visiontext patterns. With these designs, SwimBird can dynamically switch among three reasoning modes. 2 Importantly, SwimBird also removes the fixed-budget constraint in visual reasoning. Instead of generating constant-length sequence of visual thought tokens, it dynamically determines the number of visual thought tokens during vision-only or interleaved reasoning, allocating more latent computation to vision-dense queries while avoiding redundant visual thoughts for text-centric problems. As result, single model can robustly handle diverse query types, whereas fixed-pattern baselines typically excel only on subset and may underperform when the required thinking modality or visual-thought budget deviates from their pre-defined design. Our contributions are summarized as follows: We identify two key bottlenecks of prior multimodal CoT frameworks, namely fixed reasoningmode templates and fixed visual-thought lengths, and show how they lead to modality mismatch that harms either vision-dense performance or text-based logical reasoning. We introduce SwimBird, hybrid autoregressive MLLM that can dynamically switch among textonly, vision-only, and interleaved reasoning modes, combining next-token prediction for textual thoughts with next-embedding prediction for visual thoughts. We further introduce adaptive visual-thought allocation, enabling SwimBird to dynamically determine the number of continuous visual-thought tokens based on query complexity. We design systematic reasoning-mode curation strategy for multimodal CoT samples and construct SwimBird-SFT-92K, dataset covering three reasoning patterns that enables query-adaptive mode selection. Extensive experiments across diverse benchmarks demonstrate that SwimBird achieves state-of-theart performance on both text-centric reasoning and challenging vision-dense tasks, outperforming prior fixed-pattern multimodal reasoning methods."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Textual CoT in MLLMs The integration of vision and language has evolved from discriminative tasks toward generative reasoning frameworks. Early MLLMs focus primarily on visual question answering through direct answer generation [13, 15, 27, 14, 35]. With the success of step-by-step reasoning in LLMs, recent MLLMs incorporate explicit reasoning chains to handle complex multimodal problems [1, 29, 34]. These models generate intermediate textual explanations before producing final answers, demonstrating improved performance on mathematical word problems, scientific diagram understanding, and multi-hop visual reasoning [38, 31, 17]. Despite their effectiveness on logic-heavy benchmarks, these text-based reasoning approaches struggle when the core challenge lies in visual perception rather than logical decomposition [20]. Tasks requiring spatial transformation tracking, visual state prediction, or fine-grained visual comparison expose the fundamental limitation that the model is forced to describe intermediate visual evidence in language, even when language is not faithful or efficient carrier for the required information, leading to brittle reasoning and error accumulation. 2.2 Latent Visual Reasoning Recognizing the constraints of language-only reasoning, researchers have explored alternative computational substrates for visual thinking [18, 28]. Recent methods propose latent visual reasoning by training models to produce continuous embeddings supervised by visual reconstruction objectives. For instance, Mirage [36] employs hidden states trained to approximate annotated helper images, while LVR [11] focuses on reconstructing cropped image regions. SkiLa [22] proposes unified reasoning that alternates between generating latent visual tokens and discrete textual tokens. However, existing latent reasoning methods uniformly apply the same reasoning structure across all inputs: models trained with visual thoughts always generate them, even for purely textual queries. Furthermore, these methods use fixed-length latent tokens regardless of whether problem requires minimal or extensive visual deliberation. SwimBird addresses both limitations through dynamic mode selection and adaptive visual token budgets, enabling truly query-adaptive multimodal reasoning. 3 Figure 2: SwimBird adopts hybrid autoregressive formulation that performs next-token prediction for textual thoughts and switches to next-embedding prediction for visual thoughts. During inference, SwimBird performs query-adaptive multimodal reasoning by dynamically selecting among three modes conditioned on the input: text-only, vision-only, and interleaved vision-text reasoning."
        },
        {
            "title": "3 Method",
            "content": "SwimBird adopts hybrid autoregressive formulation that supports both discrete textual tokens and continuous latent visual tokens. As shown in Fig. 2 (left), it performs standard next-token prediction for textual thoughts, optimized with shifted cross-entropy loss, and performs nextembedding prediction for visual thoughts, optimized with MSE loss to reconstruct the embeddings of intermediate thinking images. During inference (Fig. 2 right), SwimBird performs query-adaptive reasoning by generating either (i) text-only traces, (ii) vision-only traces with variable-length latent span, or (iii) interleaved visiontext traces, conditioned on the input. 3.1 Hybrid Autoregressive Modeling Textual thought as next-token prediction. For textual reasoning spans, SwimBird behaves like standard language model. Given token sequence {w1, . . . , wT }, the model outputs logits parameterizing pθ(wt w<t, x), (1) where denotes the observed image (and prior context). We train these spans with the standard cross-entropy loss: Ltext = (cid:88) t=1 log pθ(wt w<t, x). (2) This objective preserves the discrete symbolic manipulation and logical consistency of the language backbone, which is essential for text-centric reasoning tasks. Visual thought as next-embedding prediction. For vision-only reasoning or visual segments inside interleaved reasoning, SwimBird generates sequence of continuous latent tokens (visual thoughts) {z1, . . . , zK}, each represented as hidden-state embedding rather than discrete word. Concretely, we treat each visual-thought step as predicting the next embedding in an autoregressive manner: ˆzk = fθ(z<k, wT , x), and supervise it with shifted mean squared error (MSE) loss against target embeddings zk: (cid:88) Lvis = ˆzk zk2 2 . (3) (4) k= Here, the target embeddings are computed by encoding the intermediate thinking images with the same vision encoder (and projection) used by SwimBird, thus grounding latent visual thoughts in semantically meaningful visual states. Unified training objective. training instance may contain pure textual CoT, pure visual CoT, or interleaved segments. We optimize unified objective that sums modality-specific losses over the activated segments: = λtextLtext + λvisLvis, (5) 4 Data Source All Mode Text Only Vision Only Interleave Problem Domain Zebra-CoT ThinkMorph MathCanvas OpenMMReasoner Total 26.3K 7.1K 8.9K 50K 92.3K 0 0 0 50K 50K 5.9K 1.2K 1.7K 0 8.8K 20.4K Visual Search, Jigsaw, Maze, Geometry, Chess... 5.9K Visual Search, Spatial Navigation, Jigsaw, Chart 7.2K Geometry, Algebra, Calculus, Statistics General VQA, Math VQA, Text QA 33.5K Table 1: Detailed statistics of SwimBird-SFT-92K. where λtext and λvis are balancing coefficients. In practice, each sample only contributes to the losses of the modes it contains, enabling the model to learn all three reasoning patterns without forcing unnecessary supervision. Mode switching with special delimiters To enable controllable and learnable switching among reasoning modes, we introduce explicit delimiters in the target sequences. Specifically, we mark visual-thought spans using special tokens such as <latent_start> and <latent_end>. During training, these delimiters define where the model should produce continuous latent embeddings instead of textual tokens. During inference, SwimBird generates these delimiters autoregressively, which makes mode selection query-adaptive: the model can decide whether to enter latent visualthinking phase, remain in text-only reasoning, or alternate between the two (Fig. 2 right). 3.2 Dynamic Latent Token Budget Prior latent visual reasoning methods typically adopt fixed number of latent tokens (or fixed pooling strategy) for all inputs. This design has two drawbacks: (1) it can lead to insufficient capacity for vision-dense, high-resolution images, while wasting computation on vision-easy, lowresolution images; (2) pooling intermediate process images into fixed token length during training may discard spatial details, making it harder for the model to learn semantically meaningful latent embeddings. Figure 3: Resolution-aware, dynamic latent tokens budget. As shown in Figure 3, SwimBird addresses these issues with resolution-aware, dynamic latent token budget. Benefiting from the naive-resolution property of the Qwen ViT, we assign different maximum pixel budgets to the question image and the intermediate thinking images during training, which directly controls the maximum number of visual tokens produced by the vision encoder for each type of image. Concretely, we allow the vision encoder to output variable number of visual tokens according to image resolution, bounded by an independent range [Nmin, Nmax] (implemented via pixel/patch budget control). This avoids aggressive pooling that discards fine-grained evidence, while preventing excessively long visual sequences from dominating computation. Consequently, SwimBird can preserve detailed visual information when needed (e.g., tiny targets or dense diagrams) and remain efficient on simpler cases. With this resolution-aware training setup, SwimBird further learns to allocate latent computation dynamically at inference time. In vision-only and interleaved modes, the number of latent tokens is not pre-defined: the model keeps generating latent embeddings until it decides to stop by emitting </latent>. This variable-length latent span naturally matches the amount of visual thinking to the perceived difficulty of the query. 3.3 Switchable Reasoning SFT Dataset Construction To enable switchable reasoning modes, we curate diverse SFT dataset covering three reasoning patterns: (1) text-only CoT, (2) vision-only CoT where intermediate images are sufficient, and (3) interleaved vision-text CoT requiring both modalities. Our curation pipeline consists of three stages: 5 Model V* Bench HR-Bench 4K HR-Bench 8K MME RealWorld Avg. Textual Reasoning Models GPT-4o [8] GPT-5-mini Qwen2.5-VL-32B-Instruct Qwen2.5-VL-7B-Instruct Qwen3-VL-8B-Instruct * Qwen3-VL-8B-Thinking InternVL3-8B [46] LLaVA-OneVison [12] Vision-R1 [7] 66.0 63.9 80.6 75.3 83.8 77.5 81.2 75.4 80. 59.0 66.3 69.3 65.5 76.5 72.4 70.0 63.0 64.8 55.5 60.9 63.6 62.1 71.3 68.1 69.3 59.8 57.0 Monet [28] LVR [11] SkiLa [22] SEAL [33] Pixel Reasoner [26] DeepEyes [45] Thyme [42] DeepEyesV2 [6] SwimBird Latent Visual Reasoning Models 83.3 81.7 84.3 74.8 84.3 83.3 82.2 81.8 85.5 71.0 69.6 72.0 68.0 66.1 66.5 Multimodal Agentic Models - 72.6 73.2 77.0 77.9 79.0 - 66.1 69.5 72.0 73.8 74.9 62.8 - 59.1 56.8 61.9 - - 57.4 - - - - - 64.4 64.1 64.8 64.9 65.3 60.9 - 68.2 64.9 73.4 - - 63.9 - - - - - 71.9 72.5 74.0 74.6 76. Table 2: Performance on fine-grained visual understanding benchmarks. Here, * denotes the results are reproduced by ourselves. Stage 1: Candidate collection and easy-instance filtering. We collect raw image-text interleaved CoT data from ThinkMorph [5], Zebra-CoT [10], and MathCanvas-Instruct [21]. These datasets provide multimodal reasoning chains with intermediate visual thinking steps. where each sample contains intermediate thinking images. To focus on cases where intermediate visual reasoning is useful, we remove instances that are already solvable from the original input: Qwen3VL-8B is evaluated on the question and the original image, and correctly answered samples are filtered out. Stage 2: Reasoning-mode labeling via pass@8. For each remaining sample, we compute two pass@8 scores with Qwen3VL-8B: passbase using only the question and problem image, and passhint additionally providing the intermediate thinking images as visual hints. We judge each sampled answer using Qwen3-235B-Instruct given the question, prediction, and ground truth. We keep samples with passhint passbase, indicating that intermediate thinking images provide non-negative gains. Among them, we label samples with passhint 0.75 as vision-only, since the model can solve the problem with high probability using the intermediate thinking images without an explicit textual CoT. The remaining kept samples, where passhint passbase but passhint < 0.75, are labeled as interleaved visiontext, since the images help but are insufficient for consistently correct solutions and textual reasoning is still needed. This procedure yields 42K high-quality SFT samples covering the vision-only and interleaved modes. Stage 3: Add text-only CoT data. To complete the three-mode training set, we sample 50K text-only CoT instances from OpenMMReasoner [40], which provides pass@8-filtered textual CoT traces. Combining them with the 42K samples from Stage 2 yields SwimBird-SFT-92K, covering text-only, vision-only, and interleaved visiontext patterns. Detailed statistics are reported in Table 1."
        },
        {
            "title": "4 Experiments",
            "content": "Training Details We adopt Qwen3-VL 8B [1] as the base model and conduct supervised fine-tuning on our curated SwimBird-SFT-92K. Training is performed on A100-80G GPUs with global batch size of 128. The vision encoder and multimodal projector are kept frozen, and only the LLM parameters are updated. cosine learning rate scheduler is applied with an initial learning rate of 1e-5. 6 Models General VQA Multimodal Reasoning MMStar RealWorldQA WeMath DynaMath MathVerse_MINI Qwen2.5-VL-32B-Instruct Qwen2.5-VL-7B-Instruct Qwen3-VL-8B-Instruct * LLaVA-OneVision [12] DeepEyes [45] DeepEyesV2 [6] SkiLa [22] SwimBird 70.3 60.3 64.7 61.9 - - 64.8 71.2 - 67.4 71.8 69.9 - - 69.3 73. - 34.6 38.8 20.9 38.9 38.1 - 49.5 - 53.3 65.3 - 55.0 57.2 - 67.2 48.5 45.6 61.3 19.3 47.3 52.7 - 65. Table 3: Performance on general vqa and multimodal reasoning tasks. Here, * denotes the results are reproduced by ourselves. Baselines and Benchmarks To comprehensively assess the effectiveness of SwimBird, we compare it against three categories of baselines: (1) textual reasoning models, including advanced closed-source systems (e.g., GPT-4o and GPT-5-mini) and state-of-the-art open-source models (e.g., Qwen2.5/3-VL, LLaVA-OneVision); (2) latent visual reasoning models (e.g., Monet, LVR, SkiLa); and (3) multimodal agentic models that rely on explicit tool/workflow designs (e.g., Pixel Reasoner, DeepEyes, Thyme). We evaluate on two groups of benchmarks: (i) fine-grained/high-resolution visual understanding (V* Bench [33], HR-Bench 4K/8K [30], MME-RealWorld [43]; Table 2), and (ii) general VQA and multimodal reasoning (MMStar [2], RealWorldQA [3], WeMath [17], DynaMath [47], MathVerse_MINI [41]; Table 3). Results marked with * are reproduced by ourselves. 4.1 Main Results Fine-grained Visual Understanding Table 2 demonstrates that SwimBird achieves state-of-the-art performance on fine-grained and high-resolution perception. SwimBird obtains 85.5 on V* Bench, 79.0 on HR-Bench 4K, and 74.9 on HR-Bench 8K, outperforming strong textual reasoning baselines such as Qwen3-VL-8B-Instruct (83.8/76.5/71.3). Notably, Qwen3-VL-Thinking performs worse than Qwen3-VL-Instruct on visual perception, further supporting our claim that mismatched reasoning mode can harm performance. Furthermore, SwimBird also outperforms current state-of-the-art multimodal agentic models such as Thyme (82.2/77.0/72.0) and DeepEyesV2 (81.8/77.9/73.8), which enhance perception via explicit cropping tools, highlighting that SwimBird can achieve stronger fine-grained perception without relying on complex tool pipelines. We attribute these gains to SwimBirds query-adaptive reasoning mode switching and adaptive latent-token allocation. Finegrained visual tasks often require precise spatial evidence that is difficult to faithfully compress into text; meanwhile, forcing latent visual thoughts on text-centric steps can be redundant. By switching to vision-only reasoning when dense perception is needed (and allocating more latent computation for high-resolution inputs), SwimBird better preserves visual details and reduces modality mismatch, leading to consistently higher accuracy. General VQA and Multimodal Reasoning Beyond perception, SwimBird also shows strong improvements on general VQA and reasoning-heavy benchmarks. As shown in Table 3, SwimBird reaches 71.2 on MMStar and 73.1 on RealWorldQA, exceeding Qwen3-VL-8B-Instruct* (64.7/71.8) and even outperforming Qwen2.5-VL-32B-Instruct on MMStar. More importantly, SwimBird delivers clear gains on multimodal reasoning: 49.5 on WeMath, 67.2 on DynaMath, and 65.8 on MathVerse_MINI, outperforming strong open-source methods and agentic models. These results suggest that SwimBirds latent visual thoughts do not come at the cost of symbolic reasoning. Instead, SwimBird stays in text-only reasoning when the task is primarily linguistic or mathematical, and invokes vision-only or interleaved latent thinking only when additional visual evidence is beneficial. Learned from the multi-pattern supervision in SwimBird-SFT-92K, this query-adaptive selection avoids redundant visual thoughts that could interfere with textual logic, while still leveraging latent visual computation for vision-dependent subproblems. Latent Tokens HRBench4K HRBench8K RealWorldQA MSE Weight HRBench4K HRBench8K RealWorldQA 16 32 64 128 76.4 79.0 77.8 76.0 71.4 74.9 73.4 71.8 73.1 73.1 72.7 72. 0.1 0.2 0.5 1.0 79.0 79.0 77.8 79.4 71.8 74.9 75.9 73.8 72.8 73.1 72.0 71.9 Table 4: Impact of maximum latent tokens budget. Table 5: Impact of MSE loss weight coefficients. Figure 4: Distribution of reasoning mode across different benchmarks for SwimBird. 4.2 Ablation Studies Impact of the Maximum Latent Token Budget. We study how the maximum latent token budget Nmax influences performance under our dynamic range setting [Nmin, Nmax]. We fix Nmin = 2 to ensure small images can be encoded without losing effective resolution, and vary Nmax {16, 32, 64, 128}. As shown in Table 4, increasing Nmax from 16 to 32 yields clear gains on vision-dense benchmarks (HRBench4K: 76.4 vs. 79.0; HRBench8K: 71.4 vs. 74.9), indicating that moderate upper bound provides sufficient capacity for high-resolution perception. However, further expanding Nmax to 64 or 128 does not help and even degrades performance (e.g., HRBench8K: 74.9 vs. 73.4 vs. 71.8), while RealWorldQA slightly drops (73.1 vs. 72.7). This suggests that an overly large latent budget may introduce redundant visual computation and interfere with overall reasoning. Overall, Nmax = 32 offers the best trade-off and is used as the default setting. Impact of the MSE Loss Weight Coefficient. We ablate the weight of the visual-thought reconstruction loss by varying λvis while keeping other settings fixed. As shown in Table 5, moderate MSE weight yields the most balanced performance. Specifically, setting λvis = 0.2 achieves strong results across all benchmarks. When λvis is too small (0.1), the supervision on latent visual thoughts becomes weak, leading to notable drop on the most vision-dense benchmark (HRBench8K: 71.8). In contrast, increasing λvis to 0.5 improves HRBench8K (75.9) but degrades RealWorldQA (72.0), suggesting that overly emphasizing MSE training may bias the model toward visual reconstruction at the expense of general multimodal reasoning. With λvis = 1.0, HRBench4K slightly increases (79.4) but performance drops on HRBench8K and RealWorldQA, indicating instability under overly strong visual-loss weighting. Overall, we use λvis = 0.2 as the default, which best balances visual reasoning and text-centric reasoning. 8 Figure 5: Analysis of Different Reasoning-Mode Case. 4.3 Analysis of Switchable Reasoning Mode Analysis of Reasoning-Mode Distribution We analyze the distribution of SwimBirds reasoning modes across benchmarks  (Fig. 4)  to verify its query-adaptive behavior. Overall, the selected mode matches each benchmarks dominant difficulty. On text-logic-dominant multimodal reasoning datasets (DynaMath and MathVerse_MINI), SwimBird almost always uses text-only reasoning, with vision-only and interleaved traces rarely triggered, suggesting it avoids redundant latent visual thoughts when symbolic manipulation and linguistic deduction are sufficient. On vision-dense perception benchmarks (V* Bench and HR-Bench 4K/8K), SwimBird frequently activates visiononly and especially interleaved visiontext reasoning, reflecting the need to alternate between visual grounding (e.g., tiny targets in high-resolution images) and explicit textual deduction. The proportion of vision-only reasoning increases from HR-Bench 4K to 8K, consistent with higher perceptual load at higher resolutions. WeMath exhibits more balanced mixture of all three modes, where some problems are text-centric while others require substantial visual grounding. These results confirm that SwimBird does not follow fixed template, but instead selects reasoning modes in an instance-dependent manner to mitigate modality mismatch. Analysis of Different Reasoning-Mode Cases Fig. 5 provides qualitative examples of SwimBirds mode selection. For vision-only reasoning (top-left), the cube-net folding problem mainly requires spatial perception and mental rotation; SwimBird directly enters latent visual-thought span and outputs the answer without unnecessary textual CoT, while allocating an appropriate latent length (e.g., =18). For text-only reasoning (top-right), the arithmetic equation is purely symbolic; SwimBird solves it with textual deduction, avoiding redundant visual thoughts that could interfere with logical steps. For interleaved visiontext reasoning (bottom), reading phone number from small region in natural image requires both precise visual localization and explicit option comparison; SwimBird first uses latent visual thoughts to focus on the relevant region, then switches back to text for verification and decision making, again with dynamically allocated latent length (e.g., =24). Together, these cases show that SwimBird mitigates modality mismatch by choosing when to think in vision versus language and by adaptively allocating visual-thought computation to match perceptual difficulty."
        },
        {
            "title": "5 Prompt",
            "content": "To guide SwimBirds query-adaptive reasoning mode selection, we design system prompt that explicitly instructs the model on how to switch between textual and visual thinking modes. As shown in Figure 6, the prompt defines three reasoning patterns: text-only, vision-only, and interleaved, using structured tags (<reason> for textual thoughts and <latent> for visual thoughts), and allows the model to dynamically choose the most appropriate mode or combination based on the input query. System Message You are multimodal reasoning assistant capable of thinking in textual and visual modes. Use the following tags to switch your thinking mode: 1. Textual Mode: <reason>Your textual reasoning process</reason> For logical analysis, planning, and verbal thought. 2. Visual Mode: <latent_start>Your visual reasoning process<latent_end> For mental visualization, imagination and simulation. Output Rules: Depending on the problem, you can use: textual reasoning only, visual reasoning only, or mix of both (alternating multiple times as needed). After all thinking is complete, place the final answer inside <answer>Your Final Answer</answer>. Figure 6: System prompt used for SwimBird."
        },
        {
            "title": "6 Conclusion",
            "content": "We present SwimBird, reasoning-switchable MLLM that addresses the fixed reasoning pattern in prior multimodal CoT frameworks. SwimBird adopts hybrid autoregressive paradigm and can adaptively switch among text-only, vision-only, and interleaved visiontext reasoning, while dynamically allocating the latent visual token budget. We also construct SwimBird-SFT-92K with systematic curation and mode-labeling strategy to enable effective multi-mode training. Extensive experiments show that SwimBird achieves SoTA performance on both text-centric reasoning and challenging vision-dense tasks."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [3] Nigel Dsouza. Comparative analysis of leading generative ai conversational systems: Chatgpt, grok ai, gemini, and meta ai. Authorea Preprints, 2025. [4] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024. [5] Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492, 2025. [6] Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. [7] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [8] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [9] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [10] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. [11] Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025. [12] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [14] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [16] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [17] Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Chong Sun, Xiaoshuai Song, Jiapeng Wang, Zhuoma Gongque, Shanglin Lei, Yifan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2002320070, 2025. [18] Yiming Qin, Bomin Wei, Jiaxin Ge, Konstantinos Kallidromitis, Stephanie Fu, Trevor Darrell, and XuDong Wang. Chain-of-visual-thought: Teaching vlms to see and think better with continuous visual tokens. arXiv preprint arXiv:2511.19418, 2025. [19] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [20] Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. Zoomeye: Enhancing multimodal llms with human-like zooming capabilities through tree-based image exploration. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 66136629, 2025. [21] Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, et al. Mathcanvas: Intrinsic visual chain-of-thought for multimodal mathematical reasoning. arXiv preprint arXiv:2510.14958, 2025. [22] Jintao Tong, Jiaqi Gu, Yujing Lou, Lubin Fan, Yixiong Zou, Yue Wu, Jieping Ye, and Ruixuan Li. Sketch-in-latents: Eliciting unified reasoning in mllms. arXiv preprint arXiv:2512.16584, 2025. [23] Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, and Ruixuan Li. Flowcut: Rethinking redundancy via information flow for efficient vision-language models. arXiv preprint arXiv:2505.19536, 2025. [24] Jintao Tong, Shiwei Li, Zijian Zhuang, Jinghan Hu, and Yixiong Zou. Emosync: Multi-stage reasoning with multimodal large language models for fine-grained emotion recognition. In Proceedings of the 3rd International Workshop on Multimodal and Responsible Affective Computing, pages 9599, 2025. [25] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [26] Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 11 [27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [28] Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395, 2025. [29] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [30] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 79077915, 2025. [31] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [33] Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13084 13094, 2024. [34] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20872098, 2025. [35] Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, and Ray Zhang. Crosslmm: Decoupling long video sequences from lmms via dual cross-attention mechanisms. arXiv preprint arXiv:2505.17020, 2025. [36] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. [37] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [38] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [39] Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, et al. Latent sketchpad: Sketching visual thoughts to elicit multimodal reasoning in mllms. arXiv preprint arXiv:2510.24514, 2025. [40] Kaichen Zhang, Keming Wu, Zuhao Yang, Bo Li, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reasoning with an open and general recipe. arXiv preprint arXiv:2511.16334, 2025. [41] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [42] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. [43] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. 12 [44] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [45] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [46] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [47] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        }
    ],
    "affiliations": [
        "Accio Team, Alibaba Group",
        "Huazhong University of Science and Technology"
    ]
}