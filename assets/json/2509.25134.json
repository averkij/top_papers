{
    "paper_title": "LayerD: Decomposing Raster Graphic Designs into Layers",
    "authors": [
        "Tomoyuki Suzuki",
        "Kang-Jun Liu",
        "Naoto Inoue",
        "Kota Yamaguchi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designers craft and edit graphic designs in a layer representation, but layer-based editing becomes impossible once composited into a raster image. In this work, we propose LayerD, a method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose a simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop a quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing."
        },
        {
            "title": "Start",
            "content": "LayerD: Decomposing Raster Graphic Designs into Layers Tomoyuki Suzuki1 Kang-Jun Liu2 Naoto Inoue1 Kota Yamaguchi1 1CyberAgent 2Tohoku University 5 2 0 2 9 ] . [ 1 4 3 1 5 2 . 9 0 5 2 : r Figure 1. LayerD effectively decomposes raster graphic design images into layers, where the input design contains various elements such as typographic entities, embellishments, vector shapes, or even image materials. Once decomposed, one can apply image editing operations such as color conversion or translation at the layer level, or further apply other post-processing such as OCR to vectorize each raster layer."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Designers craft and edit graphic designs in layer representation, but layer-based editing becomes impossible once composited into raster image. In this work, we propose LayerD, method to decompose raster graphic designs into layers for re-editable creative workflow. LayerD addresses the decomposition task by iteratively extracting unoccluded foreground layers. We propose simple yet effective refinement approach taking advantage of the assumption that layers often exhibit uniform appearance in graphic designs. As decomposition is ill-posed and the ground-truth layer structure may not be reliable, we develop quality metric that addresses the difficulty. In experiments, we show that LayerD successfully achieves high-quality decomposition and outperforms baselines. We also demonstrate the use of LayerD with state-of-the-art image generators and layer-based editing. Code and models are publicly available 1. 1https://cyberagentailab.github.io/LayerD/ In the creative workflow, designers create and edit graphic designs at the layer level, which is basic unit of visual objects, such as text or images, and is commonly seen in design authoring tools like Adobe Photoshop or PowerPoint. Once the workflow is complete, authoring tools composite these layers into final image and deliver it to display device or print media, such as social media posts, flyers, and posters. Composite raster images do not retain layer information, making it difficult for designers to edit or retouch raster graphic design. Precise decomposition of raster artwork into layers, i.e., the inverse problem of composition, addresses this situation and enables workflow that uses existing raster artwork assets to create new artwork. In this work, we investigate graphic layer decomposition, aiming to automatically decompose raster graphic design into composable sequence of raster layers. Since designers create graphic designs in layered format, we can view this task as restoring the original layered representation. Layer decomposition involves several computer vision tasks, such as object localization, segmentation, order estimation, and image inpainting. Unlike natural images, graphic design is mixture of various elements, including typography, embellishments, vector art, illustrations, and even natural image materials  (Fig. 1)  . Naively applying image decomposition approaches [52, 61, 63] tuned for the natural image domain results in unintended decomposition (e.g., objects in photo material are decomposed) or undesirable artifacts (e.g., background lighting affects solidcolor vector-art), which are prohibitive for creative work. Graphic layer decomposition is also inherently ill-posed; there are multiple possible solutions, and layer can be arbitrarily divided into multiple layers. This can be problematic, particularly when ensuring consistent evaluation. We propose method for fully automatic graphic layer decomposition, LayerD, which we formulate as iterative top-layer matting and background completion. We define top-layer by objects appearing on the front without occlusion in the raster image, and in graphic designs, they typically contain typography at the beginning, followed by embellishment behind texts or photo materials in later iterations. We learn top-layer matting model from highquality graphic design dataset to ensure that the layer granularity aligns with humans, and together with an off-the-shelf inpainting model and simple-yet-effective heuristic refinement to remove artifacts, we build complete layer decomposition pipeline for graphic designs. There have been few similar attempts at fully automatic image decomposition into layer representations [6, 52], where they build modular decomposition pipeline consisting of components for each subproblem, such as object detection [28, 59], segmentation [39], ordering [23, 38], and inpainting [42]. While the stacked pipeline approach can take advantage of pre-trained models at each stage, component stacking cannot avoid error accumulation throughout the pipeline; e.g., segmentation can fail when object detection contains overlapping bounding boxes or there is large hall in the region. LayerD unifies detection, segmentation, and layer ordering by an iterative matting model to reduce the error accumulation while improving the efficiency. In addition, we introduce refinement approach for both foreground and background layers utilizing the domain prior that graphic design often consists of texture-less flat regions, which improves the final decomposition quality. As layer decomposition can have multiple solutions and even humans are not consistent on the granularity of layers, we propose qualitative metrics for evaluation based on edit distance and visual quality between layer sequences aligned by dynamic time warping (DTW) [35], which account for the inconsistency of the ground truth layers. We compare LayerD with several baselines and demonstrate that our method achieves the highest quality. We summarize our contributions as follows: We propose LayerD, fully automatic framework for layer decomposition from raster graphic designs. LayerD unifies the subtasks inherent in layer decomposition into iterative top-layer extraction and leverages domain priors to improve the final decomposition quality. We propose consistent evaluation protocol for layer decomposition based on the edit distance and appearance quality between aligned layer sequences, which accounts for the ambiguity in the ground truth layer structure. We empirically show that LayerD achieves the highest quality compared to baselines and decomposed layers can be used for downstream graphic design editing. 2. Related Work 2.1. Image Layer Decomposition Image layer decomposition is task to decompose an image into sequence of layers, which are composable with specific compositing function (e.g., alpha compositing) to reproduce or approximate the original image [37]. Color segmentation represents an image with semi-transparent color layers, targeting digital paintings [50] and natural images [1, 2, 51]. Koyama et al. [20] propose to handle nonlinear color blending functions, followed by the efficient deep learning-based extension [13]. There have been many studies on decomposing natural scenes at the object level [16, 31, 34, 52, 61, 63]. For instance, PCNet [61] decomposes scene image into object layers by estimating the order of objects and the RGB of occluded parts. While PCNet assumes the object modal mask is given, Zhang et al. [63] create layered data including occluded parts in indoor scenes and decompose the image by training instance segmentation, depth estimation, and background completion. Text2Layer [62] extracts salient objects from natural images using matting and generates training data for layered image generation. Recently, MULAN [52] decomposes natural images, including outdoor scenes where obtaining ground truth data is difficult, by combining the latest off-the-shelf open vocabulary object detection models [59], zero-shot segmentation [19], depth estimation [38], and instance ordering [23] with heuristics. While the above studies mainly focus on object decomposition, Yang et al. [57] decompose physical object effects (e.g., shadows or reflections) as well. Compared to the natural image decomposition, graphic design decomposition has to deal with different granularities of objects; e.g., corporate logo in graphic design consists of an illustration and text, and whether they should be decomposed into parts depends on the context. Considering the nature of the task, we propose simple and effective method and new quantitative evaluation protocol for inconsistent ground-truth. concurrent work [6] tackles the same task as ours with stacked pipeline approach 2 Figure 2. LayerD decopmoses raster graphic designs into layers by iteratively extracting the top-layer and completing the background. Our training target is the top-layer matting model. Figs. 3 and 4 illustrate details of the top-layer extraction and background completion. using VLM trained on closed data. LayerDs pipeline is overwhelmingly simple and leverages domain knowledge to refine the final quality. We compare LayerD with VLMbased pipeline in our experiment. 2.2. Image Vectorization Related to layer decomposition, image vectorization converts an image or part into set of parameters of specific drawing function, rather than layer images. Our layer decomposition approach can be useful for vectorization as pre-processing step to extract part-based raster images. Du et al. [9] and Favreau et al. [10] obtain sequence of linear gradient layers that approximate the original image by optimization using alpha blending. Several works attempt to generate SVG-based representation from raster images [5, 33, 40, 41, 43, 45], where they typically assume vector art, cleanly masked images, or clean segmented images as input. few specifically focus on typographic representation in graphic design, where they estimate text rendering parameters [6, 44]. 2.3. Image Matting and Foreground Extraction Image matting is task to estimate alpha mattes of objects in an image, and together with other tasks such as background inpainting, forms the layer decomposition task. Matting approach often assumes user-specified trimap [8, 47, 55, 58], and few trimap-free methods have been reported recently [25, 64]. LayerD mainly uses network architectures used in matting [64] to extract unoccluded top layers. While matting estimates alpha mattes, foreground color estimation involves determining the color of the foreground that is mixed with the background. There are energy-based methods [3, 7, 24] and their efficient versions [11, 12], as well as deep learning-based methods [32] that estimate the foreground color given the alpha. Hou et al. and Li et al. simultaneously estimate the alpha map and foreground color given an image and trimap [14, 26]. The foreground color is deterministic when the background color and foreground alpha are given. In our setup, we obtain the foreground matte from our trained model and the background color from high-quality background inpainting [48], and then calculate the foreground color. 3. Problem Formulation Graphic layer decomposition is the task of decomposing raster graphic design image [0, 1]HW 3 into sequence of layers = (lk [0, 1]HW 4)K k=0. Here, and represent the height and width of the image, respectively. is an RGB image, and lk is an RGBA image, with 3 and 4 channels, respectively. represents the blending order of the layer, i.e., the z-index. lk>0 is the foreground layer, and l0 is the background layer."
        },
        {
            "title": "The layer sequence Y is composited by the following",
            "content": "recursive process from = 1 to = (x = xK): = B(lk, xC xC lA k1), + xC = lC k1 (1 lA ). (1) (2) Here, the superscript represents the alpha channel, and represents one of the RGB channels. B() is the alpha blending function, is element-wise multiplication, and xk is the k-th blended image. In this study, we solve the inverse problem of the above, i.e., layer decomposition that estimates the layer sequence from the raster image x. The granularity of the layer depends on the dataset, and in this study, we treat the humanmade graphic designs in the dataset as ground truth. 4. Approach LayerD solves the decomposition task by iterative extraction of top-layers, which are not occluded by any other layers, and background completion  (Fig. 2)  . Our formulation integrates the subtasks of layer decomposition, which prior 3 methods [6, 52] separately address, into single task, leading to simplified implementation and performance gain by the simple training goal. Additionally, we refine the final decomposition quality by leveraging the domain prior that graphic designs often contain simple, texture-less elements or backgrounds. 4.1. Iterative Decomposition We obtain layer predictions ˆY = (ˆlm [0, 1]HW 3)M m=0 from an input image by iterative processes from front (m = ) to back (m = 1) as follows: ˆlA = Fθ( ˆxm) ˆxm1 = Gϕ( ˆxm, ˆlA m) ˆlC = B1( ˆxC m1, ˆxC = ( ˆxC ˆxC m, ˆlA m) m1 (1 ˆlA m)) ˆlA (3) (4) (5) (6) where ˆxM = x, ˆl0 = ˆx0, and is an element-wise division. The superscripts and denote the alpha channel and one of the RGB channels, respectively. Fθ() is model that takes an image as input and outputs an alpha map, which is the same as the trimap-free matting task, as long as the matting target is the top-layers. The output alpha contains all top layers; they are decomposed in one iteration. Gϕ() is background completion model that takes an image and target mask obtained from the top-layers alpha map as input and outputs an image with the target area completed. The background completion model should not insert new objects. We tried several inpainting approaches, including generative model-based completion [21], and found that generative approaches often insert unnecessary objects. We use LaMa [48] for Gϕ, which satisfies our inpainting requirement. B1() is process that estimates the RGB values from the completed background and the alpha map of top-layers. Since we know the alpha map and the completed background, we can calculate the RGB values of toplayers by simple arithmetic as the inverse process of alpha blending B(). Note that existing methods [6, 52] replace the alpha of the original image with the predicted soft or hard segmentation mask. For pixels with an alpha of 1, our method estimates the same RGB values as these naive methods. However, for other transparent pixels, our method estimates the RGB values while considering blending with the background. This primarily improves the quality of layer boundaries where soft blending is applied. We terminate the iteration (i.e., = ) when there are no pixels above certain threshold in the matting result ˆlA m. 4.2. Training In LayerD, we utilize two learnable models: top-layer matting model Fθ and background inpainting model Gϕ(). Since image inpainting is general task and reasonably 4 performant models are available in the public [48], we use an off-the-shelf pretrained model without fine-tuning. For training the top-layer matting model, we prepare pairs of an input RGB image and target alpha map of top-layers lA from Crello [56] for supervised learning. We first check the occlusion of each layer based on the layer information and integrate the alpha maps of non-occluded layers into single alpha map. This clear target definition eliminates ambiguity in training. Similarly to LayerDs pipeline, we create multiple pairs per design sample by recursively performing the same process on the remaining background. We follow the prior study [64] and define the loss function as below: L(ˆlA, lA) = λBCELBCE(ˆlA, lA) + λIoULIoU(ˆlA, lA) + λSSIMLSSIM(ˆlA, lA), (7) where LBCE(), LIoU(), and LSSIM() are binary crossentropy, Intersection-over-Union (IoU) loss, and structural similarity index (SSIM) loss, respectively, and λBCE, λIoU, and λSSIM are weights for each loss term. We train the matting model using all loss functions at the early steps and then use only the SSIM loss to improve the boundary quality. During inference, the model takes the background completion result ( ˆxm) as input instead of the clean intermediate composite result, except for the first iteration. This gap between training and inference can degrade the decomposition quality. To make the matting model robust to the inpainting artifacts, we include training examples where the top-layer regions are completed by the background completion model. Since the completion in areas spanning the back layers can alter their shape, leading to inconsistencies with the ground truth, we do not complete such areas when making training data. 4.3. Palette-based Refinement Graphic designs often contain flat elements or backgrounds with few textures, such as decorations, texts, and vector shapes. Based on this observation, we introduce simple refinement approach that greatly improves the resulting appearance at the end of each iteration. Background refinement  (Fig. 3)  We first divide the alpha map into connected regions and process each area. We calculate the color gradients of the surrounding area of the completion target area. If the area with zero color gradients is dominant, we assume that the completion target region is flat-paint area with few textures. We extract the dominant colors, i.e., the palette, of the region based on percentiles and assign the completed RGB values to the nearest palette color in the Lab color space. The background completion model can make rough predictions for such flat backgrounds even with noticeable artifacts, allowing our simple Figure 3. Background completion with palette-based refinement. We first complete the area of the predicted alpha map, then refine the target connected region based on the color palette of the surrounding area. We select the target area based on the color gradient of the surrounding area (shown in red). refinement to work effectively. Our refinement eliminates such artifacts. Foreground refinement  (Fig. 4)  Similar to background refinement, we first divide the alpha map into connected regions. Next, we calculate the color gradients within each region and classify regions where the area with zero color gradients dominates as flat regions. We extract the region that matches the palette color from the input image (x) or intermediate completed background ( ˆxm) and select the region if the overlap with the predicted alpha exceeds threshold. Then, we integrate the selected regions as new mask. Since the obtained mask is binary, we calculate the alpha value around the mask using the palette color and the background color to derive the final alpha value. We often observe that this refinement improves the quality of the boundary parts and thin decoration layers (e.g., lines and frames), which the plain matting model frequently fails. 5. Decomposition Metrics There are two problems in evaluating the quality of predicted layers ˆY against the ground truth . First, the number of layers in the ground truth and the predicted layers can differ, making it non-trivial to compare directly. To address this, we apply order-aware layer alignment using DTW [35]. Second, the quality of the predicted layers can be evaluated from two perspectives: visual quality and granularity. If we do not consider these aspects separately, we may underestimate the quality of the predicted layers due to differences in granularity, even if they are practically useful. We measure granularity by the number of edits required to align the two; we allow merging adjacent layers in the zindex and report both the number of edits and the visual Figure 4. Palette-based foreground refinement. First, we estimate the RGB values of the top-layer using the input image, the toplayers alpha, and the background by the unblend function. Next, we extract the color palette of the connected components of the top-layer, extract the region that matches the color from the original image, integrate the connected color region with large overlap with the predicted alpha map, and use it as new alpha map. Note that the missing blue edge is refined in this figure. quality after the editing operations. Layer alignment As pre-processing, we first group the ground truth and predicted layers based on visibility. Specifically, we extract layers whose visible regions (i.e., alpha values greater than zero) are not occluded by any other higher layers in z-index, blend them into single layer, and repeat the same operation with the remaining layers. This operation never affects the appearance of the composite image and forms what we refer to as top-layer. Next, we find alignment between the two layer sequences with different lengths using DTW, which considers the sequence order even if the lengths differ. We obtain set of pairs = {(ks, qs)s = 0, 1, . . . , S}, where ks and qs represent the layer indices, and is the number of pairs. Note that resolved pairs satisfy the monotonicity condition, i.e., ks and qs are increasing sequences; in other words, layers cannot be shuffled during alignment (see Supp. Sec. F.1). We define the distance metric for the layer pair as the sum of the negative value of the alphas soft IoU and L1 distance of the RGB channels weighted by the ground-truth alpha, as introduced in [49]. Finally, we compute the quality metric between the two layer sequences as follows: E( ˆY , ) ="
        },
        {
            "title": "1\nS",
            "content": "S (cid:88) s=0 e(ˆlks , lqs), (8) where e() is an arbitrary function that measures the similarity or distance between layers. We use the weighted L1 distance of the RGB channels and the soft IoU of the alpha channel as e(), similar to DTWs distance metric. Layer merge Due to the ill-posed nature of layer decomposition, decomposition results sometimes do not align well with the ground truth. In this work, we relax the alignment constraints by allowing edits. The idea is inspired by minimum edit distance [53], which is commonly used for string alignment. We define specific edit operation set for layers, and report both the maximum number of allowed edits and the distance metric used in DTW after edits. This gives straightforward insight into how many layer-level edits are required for good alignment. For simplicity, we define single edit operation; Merge, which merges two consecutive layers in z-index, when the edit yields the highest positive distance improvement. We apply edits iteratively until no further improvements are possible or the number of layers is reduced to 2. The ground truth is also mergeable. Visual examples of the edit process can be found in Supp. Figs. and H. 6. Experiments 6.1. Datasets We use the Crello dataset [56], which is collection of graphic design templates, for both training and evaluation. We obtain pairs of input images and their ground-truth layer sequences from the layer structure in design templates. We follow the data split of v5.1.0 to obtain 19,478 / 1,852 / 1,971 samples for train, validation, and test split, respectively. We resize all images to maintain their aspect ratio, with the shorter side set to 512. In this work, we exclude transparent layers from the evaluation since neither our method nor the baselines primarily focuses on accurate transparency estimation. For all methods, we conduct training and validation on the train and validation split, respectively, and report the results on the test split. For training of LayerD as described in Sec. 4.2, we generate input / groundtruth pairs, and finally obtain 48,725 and 4,674 pairs for the train and validation, respectively. As typography is one of the unique domain properties, we prepare the full dataset and the variant that excludes all text layers for evaluation. 6.2. Baselines Although there are few methods comparable to LayerD, none of them have publicly available code or models. We design the following baseline and implement an existing approach with minor modifications to fit our setting. Additionally, we evaluate all methods using Hi-SAM [60], state-ofthe-art text segmentation, for initial layer extraction. YOLO baseline We design naive baseline that combines state-of-the-art object detection and pretrained segmentation models. First, since most graphic designs contain text on the top, we segment text using Hi-SAM [60] and complete the background using LaMa [48]. Next, we detect bounding boxes of layers from the remaining background. To this end, we extract bounding boxes from the layer structure of Crello and fine-tune YOLO [18] using them. We determine the z-index of layers based on heuristics in graphic design, assuming that the smaller box is in front when the boxes overlap. Then, we obtain the segmentation masks of the topmost boxes using pretrained SAM2 [39] and perform background completion. We repeat this process, except for text segmentation, until the number of detections becomes zero. We obtain the final layers by replacing the alpha of the input or completion image with the predicted segmentation mask and blacking out the color of pixels with an alpha close to zero. VLM baseline We also consider baseline that follows the approach of Accordion [6]. Accordion generates layered graphic design by combining raster-based image generation [22] and layer decomposition, where VLM first takes an image as input and generates decomposition plan with JSON-like description of bounding boxes and zindices of layers, and then applies segmentation and background completion in front-to-back order to obtain the layer sequence. Since the model and training data are not publicly available, we reproduce this with minor modifications in our experiment. We use PaliGemma2 [46] as the backbone VLM and fine-tune on the Crello train set, HiSAM for text detection, and SAM2 for other elements. For background completion, we use LaMa [48] to ensure fairness with other methods. 6.3. Implementation Details We use BiRefNet [64] with Swin-L [30] pre-trained on natural image object segmentation for the top-layers matting model, and train it on Crello for 60 epochs with batch size of 12. We set the maximum number of iterations for decomposition to 3 for LayerD and the YOLO baseline. The maximum number of colors for palette-based refinement is set to 10 for the foreground and 2 for the background. For evaluation, we change the maximum number of allowed edits from 0 to 5, and report the visual quality for each case. 6.4. Quantitative Evaluation Baseline comparison We compare LayerD with baselines with and without text layers in Fig. 5b and Fig. 5a, respectively. In all metrics, LayerD generates layer sequences close to the ground truth with fewer edits. Our simplified pipeline and training objective are effective in layer decomposition. Moreover, in the results for all layers (Fig. 5b), LayerD alone shows higher performance than LayerD + Hi-SAM, which replaces the first iteration with Hi-SAM. 6 (a) Without text layers (b) All layers Figure 5. Baseline comparisons. We show visual quality metrics (RGB L1, Alpha IoU) as the maximum number of allowed edits increases. The left two are the results when we exclude text layers from the dataset, and the right two are the results when all layers are included. w/o text training indicates the case where text layers are not included during training. Figure 6. Ablation results of foreground color estimation and refinement. LayerD, which is specifically trained for graphic design, is more effective than Hi-SAM, which is trained for text segmentation without being limited to graphic design. More interestingly, in the case of decomposition without text layers, as shown in Fig. 5a, LayerD trained with text layers exhibits slightly better performance than LayerD trained without text, even though the decomposition targets contain no text. We suspect that text is essentially variant of vector shapes, and training with text layers improves the decomposition performance of these elements. Our method outperforms the BiRefNet without additional training in Fig. 5b, indicating the importance of training on top-layer matting. Refinement ablation Fig. 6 shows the ablation results of foreground color estimation and refinement. First, the color estimation by the inverse blending (Eq. (6)) reduces the RGB L1 compared to the Naive, which simply replaces the alpha with the predicted mask. Background refinement significantly improves the RGB L1, resulting in better subsequent layer decomposition, as indicated by the improvement in Alpha IoU. The foreground refinement slightly improves the quality of the alpha map. Although the quanFigure 7. Comparison of decomposition results by LayerD and baselines. The leftmost images are the input image (LayerD), object detections (VLM), and text-removed input (YOLO). Red rectangles indicate text, and blue rectangles indicate other elements. titative improvement is slight, we observe that foreground refinement improves the quality of boundary regions. 6.5. Qualitative Evaluation Baseline comparison Fig. 7 shows the qualitative comparison of LayerD with VLM and YOLO baseline. The VLM baseline, which relies on bounding boxes, struggles 7 Figure 8. Example with or without foreground refinement. Without refinement, layers tend to have collapsed boundary. Figure 10. Decomposition results of LayerD on raster images generated by FLUX.1 [dev] [22]. Decomposing generated images Fig. 10 shows the decomposition results of LayerD on graphic design generated by FLUX.1 [dev] [22], which is one of the stateof-the-art text-to-image generator, using prompts from DESIGNERINTENTION-v2 benchmark [17]. Note that quantitative evaluation is not possible in this setup as ground truth layers are not available. The results suggest that LayerD can generalize and successfully decompose generated images, enabling an editable workflow for raster image generators. Image editing We show the image editing examples using the decomposed layers by LayerD in Fig. 1 and Supp. Sec. A. Here, we only perform simple operations such as color conversion, translation, and resizing at the layer level. Although the layers obtained by LayerD are grouped by top-layers, we can easily divide them into connected components for more granular editing. Our decomposition enables flexible and intuitive layer-based editing. Note also that there is no significant artifact in the editing results. 7. Conclusion We present LayerD for decomposing raster graphic designs, where we propose the iterative extraction of unoccluded layers and background completion as well as refinement tailored for graphic materials. We propose an evaluation protocol for the ill-posed decomposition task, where we introduce the notion of layer edit to quantify the difference from the unreliable ground truth. The experiment showed that LayerD led to solid improvement over baselines. Our approach aims at decomposition, but it might be interesting to further consider vectorization [5, 33, 40, 45], which would expand the possible creative workflow. In the other direction, our method could help learn layered design generation model [15, 17] as preprocessing component, or be combined with recent automatic layered design editing [27, 36], or aniinteresting applications. for mation generation [29] Figure 9. Example with or without background refinement. Our refinement prevents the background from being divided into segments or filled with unexpected colors. with proper decomposition when detection fails or when the detected boxes overlap. The bounding box of layer with large hole like donut includes the elements of the entire image, making it difficult for the subsequent segmentation (Fig. 7 top sample). The YOLO baseline suffers from false negatives in detection, resulting in incomplete decomposition. In contrast, LayerD directly extracts layers without relying on bounding boxes, resulting in clean decomposition results in all cases. Refinement effect Fig. 8 shows an example of foreground refinement. The foreground refinement recovers the large missing gold decoration with large hole, which is typical failure case of the top-layers matting model. In Fig. 9, we show an example of background refinement. Background refinement successfully completes the missing part of the background. Since layer decomposition is recursive, failures in the previous iteration have negative impact on subsequent iterations. Our refinement contributes not only to the quality of the target layer itself but also to the quality of the subsequent layers, i.e., the overall quality of the layer decomposition. 6.6. Applications In this section, we demonstrate that LayerD enables few applications out of the box."
        },
        {
            "title": "References",
            "content": "[1] Naofumi Akimoto, Huachun Zhu, Yanghua Jin, and Yoshimitsu Aoki. Fast soft color segmentation. In CVPR, 2020. 2 [2] Yagiz Aksoy, Tunc Ozan Aydin, Aljoˇsa Smolic, and Marc Pollefeys. Unmixing-based soft color segmentation for image manipulation. ACM TOG, 36(2), 2017. 2 [3] Yagiz Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Designing effective inter-pixel information flow for natural image matting. In CVPR, 2017. 3 [4] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In CVPR, pages 93659374, 2019. 11, 12 [5] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. In NeurIPS, 2020. 3, 8 [6] Jingye Chen, Zhaowen Wang, Nanxuan Zhao, Li Zhang, Difan Liu, Jimei Yang, and Qifeng Chen. Rethinking layered graphic design generation with top-down approach. arXiv preprint arXiv:2507.05601, 2025. 2, 3, 4, 6 [7] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. Knn matting. IEEE TPAMI, 35(9), 2013. [8] Yung-Yu Chuang, Brian Curless, David Salesin, and Richard Szeliski. bayesian approach to digital matting. In CVPR, 2001. 3 [9] Zheng-Jun Du, Liang-Fu Kang, Jianchao Tan, Yotam Gingold, and Kun Xu. Image vectorization and editing via linear gradient layer decomposition. ACM TOG, 42(4), 2023. 3 [10] Jean-Dominique Favreau, Florent Lafarge, and Adrien Bousseau. Photo2clipart: Image abstraction and vectorization using layered linear gradients. ACM TOG, 36(6), 2017. 3 [11] Marco Forte. Approximate fast foreground colour estimation. In ICIP, 2021. 3 [12] Thomas Germer, Tobias Uelwer, Stefan Conrad, and Stefan Harmeling. Fast multi-level foreground estimation. In ICPR, 2021. 3 [13] Daichi Horita, Kiyoharu Aizawa, Ryohei Suzuki, Taizan Yonetsuji, and Huachun Zhu. Fast nonlinear image unblending. In WACV, 2022. [14] Qiqi Hou and Feng Liu. Context-aware image matting for simultaneous foreground and alpha estimation. In ICCV, 2019. 3 [15] Naoto Inoue, Kento Masui, Wataru Shimoda, and Kota Yamaguchi. OpenCOLE: Towards Reproducible Automatic Graphic Design Generation. In CVPRW, 2024. 8 [16] Phillip Isola and Ce Liu. Scene collaging: Analysis and synthesis of natural images with semantic layers. In ICCV, 2013. 2 [17] Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, et al. COLE: hierarchical generation framework for graphic design. arXiv preprint arXiv:2311.16974, 2023. 8 [18] Rahima Khanam and Muhammad Hussain. YOLOv11: An arXiv overview of the key architectural enhancements. preprint arXiv:2410.17725, 2024. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2 [20] Yuki Koyama and Masataka Goto. Decomposing images into layers with advanced color blending. CGF, 37(7), 2018. 2 [21] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-Fill-dev, 2024. Last accessed 7 March, 2025. 4, 11 FLUX.1 fill [dev]. [22] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-dev, 2024. Last accessed 7 March, 2025. 6, 8, 11 [23] Hyunmin Lee and Jaesik Park. Instance-wise occlusion and FLUX.1 [dev]. depth orders in natural scenes. In CVPR, 2022. 2 [24] Anat Levin, Dani Lischinski, and Yair Weiss. closed-form solution to natural image matting. IEEE TPAMI, 30(2), 2007. 3 [25] Jiachen Li, Jitesh Jain, and Humphrey Shi. Matting anything. arXiv: 2306.05399, 2023. 3 [26] Xiaodi Li, Zongxin Yang, Ruijie Quan, and Yi Yang. Drip: Unleashing diffusion priors for joint foreground and alpha prediction in image matting. In NeurIPS, 2024. 3 [27] Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li, and Jiang Bian. From elements to design: layered aparXiv proach for automatic graphic design composition. preprint arXiv:2412.19712, 2024. 8 [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2 [29] Vivian Liu, Rubaiat Habib Kazi, Li-Yi Wei, Matthew Fisher, Timothy Langlois, Seth Walker, and Lydia Chilton. Logomotion: Visually-grounded code synthesis for creating and editing animation. In CHI, 2025. [30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 6, 11 [31] Zhengzhe Liu, Qing Liu, Chirui Chang, Jianming Zhang, Daniil Pakhomov, Haitian Zheng, Zhe Lin, Daniel CohenOr, and Chi-Wing Fu. Object-level scene deocclusion. In ACM SIGGRAPH Conference Papers, 2024. 2 [32] Sebastian Lutz and Aljosa Smolic. Foreground color prediction through inverse compositing. In WACV, 2021. 3 [33] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layerwise image vectorization. In CVPR, 2022. 3, 8 [34] Tom Monnier, Elliot Vincent, Jean Ponce, and Mathieu Aubry. Unsupervised layered image decomposition into object prototypes. In ICCV, 2021. 2 [35] Meinard Muller. Information Retrieval for Music and Motion. Springer Verlag, 2007. 2, [36] Sohan Patnaik, Rishabh Jain, Balaji Krishnamurthy, and Mausoom Sarkar. Aesthetiq: Enhancing graphic layout design via aesthetic-aware preference alignment of multimodal large language models. In CVPR, 2025. 8 [37] Thomas Porter and Tom Duff. Compositing digital images. In Proceedings of the 11th annual conference on Computer graphics and interactive techniques, 1984. 2 9 [54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: versatile backbone for dense prediction without convolutions. In ICCV, 2021. 11 [55] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In CVPR, 2017. [56] Kota Yamaguchi. CanvasVAE: Learning to generate vector graphic documents. In ICCV, 2021. 4, 6, 11, 12, 13, 14, 15 [57] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects. arXiv preprint arXiv:2411.17864, 2024. 2 [58] Jingfeng Yao, Xinggang Wang, Shusheng Yang, and Baoyuan Wang. Vitmatte: Boosting image matting with pretrained plain vision transformers. Information Fusion, 103, 2024. 3 [59] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-vocabulary object detection pre-training via wordregion alignment. In CVPR, 2023. 2 [60] Maoyuan Ye, Jing Zhang, Juhua Liu, Chenyu Liu, Baocai Yin, Cong Liu, Bo Du, and Dacheng Tao. Hi-sam: Marrying segment anything model for hierarchical text segmentation. arXiv preprint arXiv:2401.17904, 2024. 6 [61] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Self-supervised scene deLin, and Chen Change Loy. occlusion. In CVPR, 2020. 2 [62] Xinyang Zhang, Wentian Zhao, Xin Lu, and Jeff Chien. Text2layer: Layered image generation using latent diffusion model. arXiv preprint arXiv:2307.09781, 2023. [63] Chuanxia Zheng, Duy-Son Dao, Guoxian Song, Tat-Jen Cham, and Jianfei Cai. Visiting the invisible: Layer-by-layer completed scene decomposition. IJCV, 129, 2021. 2 [64] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 3, 2024. 3, 4, 6, 17 [38] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 44(3), 2020. 2 [39] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. SAM 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 2, 6 [40] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. Im2vec: Synthesizing vector graphics without vector supervision. In CVPR, 2021. 3, 8 [41] Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images. arXiv preprint arXiv:2312.11556, 2023. 3 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [43] I-Chao Shen and Bing-Yu Chen. Clipgen: deep generative model for clipart vectorization and synthesis. IEEE TVCG, 28(12), 2021. 3 [44] Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, and In ICCV, Kota Yamaguchi. De-rendering stylized texts. 2021. 3 [45] Yiren Song, Xuning Shao, Kang Chen, Weidong Zhang, Zhongliang Jing, and Minzhe Li. Clipvg: Text-guided image manipulation using differentiable vector graphics. In AAAI, 2023. 3, 8 [46] Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. PaliGemma 2: family of versatile VLMs for transfer. arXiv preprint arXiv:2412.03555, 2024. 6 [47] Jian Sun, Jiaya Jia, Chi-Keung Tang, and Heung-Yeung In ACM SIGGRAPH Conference Shum. Poisson matting. Papers, 2004. 3 [48] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. 3, 4, 6, [49] Tomoyuki Suzuki, Kotaro Kikuchi, and Kota Yamaguchi. Fast sprite decomposition from animated graphics. In ECCV, 2024. 5 [50] Jianchao Tan, Jyh-Ming Lien, and Yotam Gingold. Decomposing images into layers via rgb-space geometry. ACM TOG, 36(1), 2016. 2 [51] Jianchao Tan, Jose Echevarria, and Yotam Gingold. Efficient palette-based decomposition and recoloring of images via rgbxy-space geometry. ACM TOG, 37(6), 2018. 2 [52] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In CVPR, 2024. 2, 4 [53] Robert Wagner and Michael Fischer. The string-to-string correction problem. JACM, 21(1), 1974. 6 LayerD: Decomposing Raster Graphic Designs into Layers"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Editing Examples We show editing examples in Fig. A. Here, we use LayerD to decompose the input image into layers, divide each layer into connected components, and group text components using CRAFT [4] to facilitate editing. We import the layers into PowerPoint2 and perform various edits, from simple layout manipulation to applying built-in image effects, at the layer level. As the examples show, once the images are decomposed, users can intuitively edit them with precise control over each graphic element. B. Additional Results We present additional examples of decomposed graphic design images using our method in Figs. and C. These examples are selected from the Crello [56] test set and demonstrate the effectiveness of our method across diverse design styles. C. Failure Cases In Figs. and E, we show typical failure cases of our method. The first set of failure cases (Fig. D) involves objects that are too small, such as detailed text descriptions, which are challenging to decompose due to their limited spatial extent. We believe that these can be mitigated by increasing the resolution of the input images. The second set of failure cases (Fig. E) is due to the ambiguity of the layer granularity. For these samples, it is difficult even for humans to decompose them into the same layers consistently. Although our evaluation metrics account for such ambiguity, we may need to improve training objectives or the postrefinement process to address these cases. D. User Study We conduct user study in which 21 cloudworkers experienced in layer-based image editing rate the practical utility of 50 decomposition resultsrandomly ordered and anonymizedfrom LayerD and our two baselines on the same images using five-point scale. Tab. summarizes the results of the user study. LayerD achieves the highest average score, and significant majority of the users (71.4%) rate LayerD the highest average score. This result further emphasizes the practical superiority of our method. 2https://www.microsoft.com/powerpoint Table A. Results of the user study. We report the average score, the number of users who rate each method as the best on average across all samples (#Pref. users), and the number of samples for which each method is rated the best on average across all users (#Win samples)."
        },
        {
            "title": "Score",
            "content": "#Pref. users #Win samples"
        },
        {
            "title": "LayerD\nYOLO base\nVLM base",
            "content": "3.74 3.52 3.31 15 (71.4%) 2 (9.5%) 4 (19.0%) 27 (54.0%) 15 (30.0%) 8 (16.0%) E. Influence of Matting and Inpainting Model"
        },
        {
            "title": "Choices",
            "content": "We vary the matting backbones (Swin-L/T [30], PVTM/S [54]) and replace the inpainting model with FLUX.1 Fill [dev] [21] and evaluate their influence. The larger matting models improve performance while using FLUX.1 Fill [dev] shows significant degradation. Generative inpainting often introduces unwanted objects, which interfere with subsequent decomposition steps. This highlights the need for graphic design-specific inpainting as well as refinement. (a) Results with different matting backbones, SwinTransformer [30] and PVT [54] variants. The inpainting model is fixed to LaMa [48]. (b) Results with different inpainting models, LaMa [48] and FLUX [22]. Figure F. Evaluation results of LayerD with different matting (a) and inpainting model (b) choices. 11 Figure A. Editing examples on Crello [56] test set. The leftmost images are the original images, and the remaining images are edited ones based on the decomposed layers. We use LayerD to decompose the original images into layers, divide them into connected components, and group text components using CRAFT [4]. Then, we perform various layer-level edits, from simple layout changes to applying built-in image effects, on PowerPoint. 12 Figure B. Additional qualitative results of our method on Crello [56] test set. The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front. 13 Figure C. Additional qualitative results of our method on Crello [56] test set. The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front. 14 Figure D. Failure samples for too small objects on Crello [56] test set. The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front. Figure E. Failure samples due to the ambiguity of the layer granularity on Crello [56] test set. The leftmost column shows the input image, and the remaining columns show the decomposed layers from back to front. 15 either the maximum number of edits is reached or the number of layers is reduced to two, as shown in Algorithms and C. To efficiently approximate the optimal edit, we adopt greedy search strategy: at iteration i, we focus on changes in distances between consecutive layersspecifically, layers i, + 1, and + 2 (if present)rather than evaluating all layers globally. The optimal edit is then selected from among all candidates at each iteration, ensuring balance between computational efficiency and alignment accuracy. Although Algorithms and describe only the merging of predicted layers for simplicity, we apply the same merging procedure to both the predicted and ground truth layers to address both underand over-decomposition. See Figs. and for visualization of the alignment and merging process."
        },
        {
            "title": "Algorithm B MergeEdit",
            "content": "# Inputs: # - ls: decomposition results of length (bottom to top) # - gts: ground truth of length (bottom to top) # - emax: maximum number of edits # - dist: distance function bounded in [0, 1] # # Outputs: # - pairs: list of (l_idx, gt_idx) # - D: distance # - e: number of edits = 0 while < emax and len(ls) > 2: pairs, _ = dtw(ls, gts) merged_ids, gains = find_gains(ls, gts, pairs, dist) if len(gains) > 0: best_id = merged_ids[argmin(gains)] merged = merge(ls[best_id], ls[best_id+1]) ls[best_id] = merged ls.pop(best_id+1) else: break += 1 return dtw(ls, gts), def merge(x, y): # Merge func by OpenCV return Image.alpha_composite(x, y) F. Detail of Decomposition Metrics F.1. Dynamic Time Warping k=0 and ground truth = (lq)Q We implement the Dynamic Time Warping (DTW) as shown in Algorithm A. Given decomposition results ˆY = (ˆlk)K q=0, the output pairs must include (0, 0) and (K, Q) as the start point and end point with step size of 1, and every layer must be included in at least one pair. An average distance is then computed over all pairs as the final output. Algorithm Dynamic Time Warping (DTW) # Inputs: # - ls: decomposition results of length (from bottom to top) # - gts: ground truth of length (from bottom to top) # - dist: distance func bounded in [0, 1] # # Outputs: # - pairs: list of (l_idx, gt_idx) # - D: distance # Step 1: Compute Cost Matrix = np.zeros((len(ls), len(gts))) for in range(len(ls)): for in range(len(gts)): C[i,j] = dist(ls[i], ls[j]) # Step 2: Compute Accumulated Cost Matrix = np.zeros((len(ls), len(gts))) for in range(1, len(ls)): D[i, 0] = D[i-1,0] + C[i,0] for in range(1, len(gts)): D[0, j] = D[0,j-1] + C[0,j] for in range(1, len(ls)): for in range(1, len(gts)): D[i,j] = C[i, j] + min(D[i-1,j], D[i,j-1], D[i -1,j-1]) # Step 3: Backtrace to Find Optimal Alignment i, = len(ls)-1, len(gts)-1 pairs = [(i,j)] while True: if i==0 and j==0: break elif i==0: pairs.append((i,j-1)) j-=1 elif j==0: pairs.append((i-1,j)) -= 1 pairs.append((i-1,j-1)) -= 1 -= 1 elif D[i-1,j-1]<=D[i-1,j] and D[i-1,j-1]<=D[i,j-1]: elif D[i-1,j]<=D[i-1,j-1] and D[i-1,j]<=D[i,j-1]: else: pairs.append((i-1,j)) -= pairs.append((i,j-1)) -= 1 = sum([Dist(ls[i], gts[j]) for i,j in pairs])/len( pairs) return pairs, F.2. Edits algorithm We employ an iterative refinement process with DTW to quantify the number of edits required to align the decomposition results with the given ground truth. At each iteration, we apply the edit (Merge) that yields the highest gain until"
        },
        {
            "title": "Algorithm C FindGains",
            "content": "G. Loss functions We use binary cross-entropy loss LBCE, IoU loss LIoU, and SSIM loss LSSIM in our training as BiRefNet [64]. Definitions of each loss function are as follows. LBCE(ˆlA, lA) = (cid:88) lA i,j log ˆlA i,j 1 Ω i,jΩ (1 lA i,j) log(1 ˆlA i,j), (9) LIoU(ˆlA, lA) = 1 (cid:80) m,nΩ (cid:80) i,jΩ lA i,j ˆlA i,j m,n + ˆlA lA m,n lA m,n , (10) ˆlA m,n LSSIM(ˆlA, lA) = 1 1 (cid:88) pP µˆlA (2µlA + µ2 ˆlA (µ2 lA + C1)(2σlA ˆlA + σ2 + C1)(σ2 ˆlA lA + C2) , (11) + C2) , as well as the local mean µlA where Ω denotes the set of spatial indices, and represents the set of overlapping patches. The local mean µˆlA and variance σ2 and variˆlA ance σ2 of ground truth, are computed within correspondlA ing patches indexed by P. The covariance σlA quantifies structural similarity between the prediction and ground truth patches. C1 and C2 are constants and the setting details follow [64], except that both the predicted and groundtruth alpha maps ˆlA and lA, are not binarized due to shading and smooth transitions commonly used in graphic design. ˆlA p # Inputs: # - ls: decomposition results of length (bottom to top) # - gts: ground truth of length (bottom to top) # - pairs: list of (l_idx, gt_idx) obtained from DTW # - dist: distance function bounded in [0, 1] # # Outputs: # - merged_ids: list of indices where merging occurs # - gains: list of corresponding distance reductions merged_ids, gains = [], [] for in range(len(ls)-1): # Step 1: Compute merged layer candidates subls = [merge(ls[i], ls[i+1])] + ([ls[i+2]] if i+2 < len(ls) else []) # Step 2: Gather corresponding ground truth layers subgts = [ [gts[p[1]] for in pairs if p[0] == i], [gts[p[1]] for in pairs if p[0] == i+1] ] # Step 3: Compute current distance sum curD = sum([dist(ls[i], subgt) for subgt in subgts sum([dist(ls[i+1], subgt) for subgt in subgts [0]]) + [1]]) # Step 4: Compute distance sum after merging Ds = [] for in range(len(subls)): for in range(len(subgts)): Ds.append(sum([dist(subls[j], subgt) for subgt in subgts[k]])) Ds = [d + Ds[0] for in Ds[1:]] minD = min(Ds) # Step 5: Check if merging reduces distance if minD < curD: merged_ids.append(i) gains.append(minD - curDs) return merged_ids, gains def merge(x, y): # Merge func by OpenCV return Image.alpha_composite(x, y) Figure G. Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between LayerDs prediction and the ground truth; their thickness represents the matching score (the inverse of the distance), i.e., the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right. Although the decomposition result appears useful for editing the input image, its quality is underestimated due to mismatch in granularity with the ground truth. Layer merging resolves this mismatch, enabling more faithful evaluation of the decomposition quality. 18 Figure H. Visual example of the DTW-based layer alignment and editing process. Red lines connect matched layers between LayerDs prediction and the ground truth; their thickness represents the matching score (the inverse of the distance), i.e., the thicker the line, the higher the score. Green boxes indicate the layers that are merged during the editing process. All layers are sorted from back to front, with the backmost layer on the left and the frontmost on the right. LayerD overdecomposes the white background, but in practical scenarios, it is easy to merge these into single layer. Our evaluation treats such cases as requiring single edit operation, reflecting the actual editing workload for users."
        }
    ],
    "affiliations": [
        "CyberAgent",
        "Tohoku University"
    ]
}