{
    "paper_title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models",
    "authors": [
        "Bofei Gao",
        "Feifan Song",
        "Zhe Yang",
        "Zefan Cai",
        "Yibo Miao",
        "Qingxiu Dong",
        "Lei Li",
        "Chenghao Ma",
        "Liang Chen",
        "Runxin Xu",
        "Zhengyang Tang",
        "Benyou Wang",
        "Daoguang Zan",
        "Shanghaoran Quan",
        "Ge Zhang",
        "Lei Sha",
        "Yichang Zhang",
        "Xuancheng Ren",
        "Tianyu Liu",
        "Baobao Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 5 8 9 7 0 . 0 1 4 2 : r Omni-MATH OMNI-MATH: UNIVERSAL OLYMPIAD LEVEL MATHEMATIC BENCHMARK FOR LARGE LANGUAGE MODELS Bofei Gao1*, Feifan Song1, Zhe Yang1, Zefan Cai2, Yibo Miao4, Qingxiu Dong1, Lei Li9, Chenghao Ma5, Liang Chen1, Runxin Xu1, Zhengyang Tang6, Benyou Wang6, Daoguang Zan7, Shanghaoran Quan3, Ge Zhang8, Lei Sha10,Yichang Zhang3, Xuancheng Ren3, Tianyu Liu3, Baobao Chang1 1Peking University 4Shanghai Jiao Tong University 6The Chinese University of Hong Kong, Shenzhen 7Institute of Software, Chinese Academy of Sciences 8University of Waterloo 9The University of Hong Kong 10Zhongguancun Laboratory 5Engineering Research Center of Information Networks 2University of Wisconsin - Madison 3Alibaba Group"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose comprehensive and challenging benchmark specifically designed to assess LLMs mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning. Github Repo Project Page Dataset OmniJudge [GitHub Page] [Project Page & Leaderboard] [Huggingface Dataset] [Huggingface Model]"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (OpenAI, 2023; Abhimanyu Dubey & Abhishek Kadian, 2024; Yang et al., 2024a) have shown remarkable capabilities in producing human-like text (Abhimanyu Dubey & Abhishek Kadian, 2024), code generation (Hui et al., 2024; Rozi`ere et al., 2024), and dialogue (Chiang et al., 2024). Among these capabilities, mathematical ability serves as fundamental measure of the problem-solving and complex reasoning skills of LLMs, motivating various endeavors towards improving the mathematical reasoning of LLMs (Yang et al., 2024b; Azerbayev et al., 2024; Wang et al., 2024). Project Lead. Corresponding Author. 1 Omni-MATH Figure 1: Comparisons among different models on GSM8K, MATH, and Omni-MATH, where the models are ranked based on their performance on MATH, and those marked with are closedsource models. As observed, the iterative advancements of these models show that existing benchmarks are nearing saturation. Our proposed Omni-MATH introduces challenging benchmark to further advance mathematical intelligence in large language models. To evaluate the mathematical capabilities of LLMs, researchers have introduced various mathematical benchmarks (Zhang et al., 2024), including the well-recognized GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). However, as LLMs have evolved rapidly, these benchmarks have increasingly lost their challenge (OpenAI, 2024), resulting in diminished capacity to differentiate between model capabilities, as illustrated in Figure 1. Although some efforts have been made to establish more challenging benchmarks, most prior work has not focused specifically on evaluating mathematical capabilities (He et al., 2024). In addition, the test results are closely coupled with other LLM capabilities, such as multimodal fusion, rendering the findings to be vague for understanding the mathematical reasoning improvements (Huang et al., 2024). To bridge this gap, we introduce Omni-MATH, universal Olympiad-level benchmark specifically designed for mathematical reasoning. Our dataset exclusively focuses on text-only mathematics and encompasses an extensive collection of 4,428 competition-level problems. We curate our benchmark based on the actual selection processes of mathematical Olympiads, ensuring that our dataset includes problems from both introductory competitions and professional international contests. To further explore LLMs performance in multiple mathematical areas, we have established hierarchical classification of mathematical domains, including 33 sub-domains and more than 10 distinct difficulty levels (see Figure 2), allowing for nuanced analysis of model performance across various mathematical disciplines and levels of complexity. For better evaluation, we provide GPT4o-based model evaluation and Omni-Judge, an open-source verifier. Our Omni-Judge provides achieves over 91% consistency with GPT-4o and 86% consistency with human judgments, providing reliable feedback. Whats more, the open-source verifier provides simple and effective way to evaluate the Olympiad-level mathematical problems. We conduct comprehensive evaluation of the currently strongest models on Omni-MATH. As shown in 3, it presents significant challenge to the existing models. Even the most capable models OpenAI o1-mini and o1-preview, renowned for their reasoning ability, only achieve an accuracy of 60.54% and 52.55%, while the highest score among open-source models is just 36.2%. In addition, the detailed categorization of mathematical disciplines and comprehensive evaluation provide new insights into current LLMs. For instance, LLMs show marginally greater aptitude for solving algebra, while struggling significantly with discrete mathematics. The commonly used approach for test-time scaling, Best-of-N, has proven ineffective for Olympiad-level mathematics problems. This underscores the urgent need for further research into test-time scaling techniques. To summarize, our contribution includes: 1) We introduce Omni-Math, universal Olympiad-level mathematical benchmark with over 33 subdomains and diverse difficulty levels, posing new challenges to the problem solving and complex reasoning capability of LLMs. 2) We introduce Omni-Judge, the first mathematical verifier designed for highly challenging prob2 Omni-MATH lems. Despite its compact size of only 7B, Omni-Judge achieves over 90% consistency with GPT-4o, providing an efficient solution for mathematical verification. 3) We conducted comprehensive evaluation of 15 LLMs and found that even the most advanced models, OpenAI o1, struggle with highly challenging problems, with 60.54% accuracy. Additionally, LLMs frequently fail to solve discrete mathematics problems and often make logical missteps, highlighting significant challenges in mathematical reasoning and problem-solving for LLMs. Figure 2: An overall illustration of Omni-MATH. The left-top part presents the diverse and wellstructured data sources of Omni-MATH. The left-bottom part represents the hierarchical mathematical domains of Omni-Math. The right part presents concrete data example of Omni-Math. Table 1: Comparison of various mathematical reasoning benchmarks. # Data denotes the total number of data in the corresponding benchmark, # T.M denotes the number of textual English mathematical reasoning data, # T.O.M indicates the number of textual English Olympiad-level mathematical reasoning data, # Domain refers to the number of domains in Olympiad-level mathematical data, # Diff represents the number of difficulty levels, Leak Det. signifies whether data leakage detection is performed and Evaluator describes the evaluation methods employed. Name # Data # T.M. # T.O.M # Domain # Diff. Leak Det. Evaluation GSM8K MATH JEEBench CHAMP Olympiad Bench MATH Odyssey Olympic Arena Omni-MATH 1319 5000 515 270 8476 387 11163 1319 5000 515 270 675 387 2036 4428 0 0 0 270 675 148 2036 4428 - 6 5 5 3 12 - - 5 - - - 4 - 33+ 10+ Rule Rule Rule Rule Rule GPT-4 Rule & GPT-4 OmniJudge (Ours) & GPT-4o"
        },
        {
            "title": "2 OMNI-MATH BENCHMARK",
            "content": "In this section, we explore the construction process of Omni-MATH. This involves data generation, manual annotation(E.1), and the classification of domains (2.3) and difficulty levels (2.2). Furthermore, we will introduce the evaluation (2.4) of the Omni-MATH, which includes GPT-4o-based evaluation and the open-source evaluator Omni-Judge. 3 Omni-MATH Figure 3: The overall data collection and annotation process of Omni-MATH. 2.1 DATA COLLECTION AND ANNOTATION First, we conducted comprehensive review of mathematics competitions worldwide and classified them into five levels based on difficulty, scale, and prestige, referencing the website AoPS Wiki: Competition Ratings and other forums containing the discussions of mathematical contests. The details are presented in Table 5. It is important to note that this classification is somewhat preliminary; it serves only to determine which competitions we include in our benchmark, rather than providing definitive judgments on the difficulty level of each problem. After obtaining these target contests, we start our data collection and annotation process, as shown in Figure 3. Data Collection For some target competitions like IMO and IMC, we can directly crawl the publicly available problems and solutions from the official website. Then we utilized MathPix to convert the PDF documents of the problems and solutions into LaTeX format. Given that the provided solutions are guaranteed to be correct, we prioritized crawling these solutions for data collection. For sections lacking open-source solutions, we extracted data from the famous AoPS website, which includes information from both the AoPS Wiki1 and the AoPS forum2. The AoPS Wiki features recognized solutions, whereas solutions in the AoPS forum are often user-uploaded and may not be correct. We crawl the problem with multiple user-uploaded solutions following methods detailed in Appendix D. rough consistency check is conducted by extracting final answers and checking whether they are identical to each other, leading to the exclusion of any inconsistent cases. Additionally, cases from which fewer than three answers could be scraped were also discarded. For each data source, we applied rules for initial filtering after obtaining the data, ensuring that both the questions and responses generally adhered to the specified input and output formats. Data Annotation After the automated construction of all the data, we engaged team of professional annotators, comprised of graduate and doctoral students to verify the solutions and the answers of the dataset manually. For the data from Contest Page and AoPS Wiki sections, which included official solutions, the verification process is relatively simple. We focus on ensuring consistency with the respective data sources. Conversely, we place particular emphasis on the problems sourced from the AoPS Forum. Following an initial screening, we narrow the dataset down to 1,100 problems. Table 2: The number of data in the construction #Data denotes the number of data, process. #R.Filter denotes the number of data after rulebased filtering, and #H.Filter denotes the number of data after human annotation. Data Source #Data #R.Filter #H.Filter 3750 Contest Page AoPS Wiki 8575 AoPS Forum 17502 3596 298 1100 3242 298 1https://artofproblemsolving.com/wiki/index.php 2https://artofproblemsolving.com/community/c13 contests 4 Omni-MATH At this stage, we engage team of four annotators to assess whether the most frequent responses from the original forum align with the final outputs produced. Each annotator reviews 550 problems, ensuring that each problem is analyzed by two different personnel for greater reliability. We employ cross-validation to enhance the robustness of our findings, yielding an accuracy rate of 92.7%. After excluding inconsistent cases, we conducted manual sampling of 150 entries, resulting in an improved accuracy rate of 97.3%. The overall data generation process is shown in Table 2, and additional annotation specifics can be found in Appendix C. After the data collection and data annotation section, we obtain the overall data for Omni-MATH."
        },
        {
            "title": "2.2 DIFFICULTY CLASSIFICATION",
            "content": "Through practical research, we find out that the difficulty of Olympic-level problems varies significantly. To enable comprehensive assessment of Olympiad-level mathematical reasoning across various levels, We categorize the overall data by different difficulty levels. Relying solely on factors such as the difficulty and popularity of competitions to determine problem difficulty is insufficient. To achieve more objective assessment of the difficulty of each problem in our dataset, we consulted the AoPS: Rating 3 page, which provides difficulty ratings for problems from various competitions. Specifically, the difficulty is quantified on scale from 0 to 10, including increments such as 0.5 and 0.25. For competitions listed on this page, we directly assign the corresponding difficulty score on the problem. For those competitions not included, we utilize the existing competitions, problems, and their associated difficulties on this page as basis for in-context learning and prompt GPT-4o to assign specific difficulty ratings. The details of this prompting procedure can be found in the Appendix E.4. Following categorization, the overall distribution of difficulty levels is presented in Figure 4. Figure 4: Difficulty distribution across contests. After conducting instance-level difficulty classification, we organized the difficulty distribution by competition. Our analysis revealed that the actual difficulty distribution rankings shown in Figure 4 closely align with the tiered classifications we previously surveyed in Figure 5. This consistency further validates the reliability of our domain classification. 2.3 DOMAIN CLASSIFICATION Unlike existing benchmarks, we argue that Olympic-level mathematics competitions encompass wide range of complex topics. The data from different domains present greater challenge for model generalization, and it is quite likely that single training set will yield inconsistent improvements across various fields. Therefore, it is essential to categorize the existing dataset into distinct mathematical domains to better investigate the models performance across different math areas. Specifically, we draw upon relevant competition guidebooks (Engel, 1998; Zeitz, 2017) to organize the mathematical fields into hierarchical tree structure. This tree-based structure not only enhances 3https://artofproblemsolving.com/wiki/index.php/AoPS Wiki:Competition ratings 5 Omni-MATH the rationality of domain classification by aligning with the table of contents of reference materials but also allows us to compute accuracy metrics at varying levels of granularity. For the domain categorization, we utilized GPT-4o to classify the problems. The prompt used for classification can be found in Appendix E.5, along with the detailed classification results."
        },
        {
            "title": "2.4 EVALUATION",
            "content": "Figure 5: Examples of the problem, reference answer, and model-generated answer from different datasets, where the model-generated answers are all correct. On the right side, model-generated answers have issues in reformatting or requiring additional reasoning to correctly evaluate. We discover that conducting answer-level assessments for Olympiad-level problems is complex task due to the diverse formats of the final answers produced by models (Figure 5). This diversity makes it challenging to evaluate model outputs using fixed rules, as seen in other benchmarks, and we accordingly employ model-based evaluation for the assessment of Omni-MATH. Specifically, we leverage GPT-4o to verify whether the content generated by the tested model aligns with the standard answer. In our prompt, we provide problem, reference answer, model-generated solution, querying GPT-4o to determine whether the model solution is consistent with the reference answer. Detailed prompts and model outputs can be found in the Appendix H. Additionally, in Section 4.2, we assessed the evaluative capability of GPT-4o to ensure the accuracy of our results. To promote public accessibility for Omni-MATH, we additionally developed an open-source evaluation model, named Omni-Judge, to assess the consistency between model solutions and reference answers at low cost. Specifically, we first constructed dataset for training (17618), validation (2200), and test (2200) based on partial results from GPT-4o evaluation, which have no overlaps of questions with each other. Then we treat this task as similar flow of instruction following, where the extraction of the model answer, judgment, and detailed justification are all required to be generated by Omni-Judge. We accordingly test multiple instruct models while appending case in the context to boost the performances. It is finally evaluated on the consistency between its judges and those from GPT-4o, which surprisingly exceeds 90%, demonstrating the strong feasibility of Omni-Judge. Detailed analysis can be found in Sec 4.2 and Appendix I."
        },
        {
            "title": "3 OLYMPIAD-LEVEL MATH EVALUATION ON EXISTING LLMS",
            "content": "3.1 EXPERIMENTAL SETUP We evaluate 15 models recognized for their strong mathematical reasoning capabilities. All prompts for these models are formatted according to the guidelines provided on their respective repositories. The detailed model information and prompt specifications can be found in Appendix F. To ensure more accurate assessment, we employ GPT-4o to evaluate whether the model outputs are consistent with the correct answers. We then analyze the accuracy rates across different domains and levels of difficulty. For clarity and simplicity in the presentation, we report the accuracy rates of the first-level subdomains of the domain tree. For difficulty, we categorize it into four tiers according to the difficulty tag: T1: 1-3, T2: 3-5, T3: 5-7, and T4: 7-10. Detailed accuracy statistics for each distinct difficulty level are provided in Appendix G. 6 Omni-MATH"
        },
        {
            "title": "3.2 EXPERIMENTAL RESULT",
            "content": "Table 3: Main Result. The results with bold text represent the best performance score. Qwen2.5MATH-72b-Instruct and OpenAI o1-mini separately perform best in the vanilla model setting and test-time scaled model setting. Model Acc Alg. P.Cal Cal Geo. D.M. Num. App. #T1 #T2 #T3 #T4 InternLM2-MATH-mixtral8*22B DeepSeekMATH-7b-RL Mathstral-7B-v0.1 DeepSeek-Coder-V2-Lite-Instruct MetaLlama-3.1-70B-instruct DeepSeek-Coder-V2 Claude-3.5-SONNET NuminaMATH-72B-COT Qwen2-MATH-7b-Instruct GPT-4o Qwen2.5-MATH-7b-Instruct Qwen2-MATH-72b-Instruct Qwen2.5-MATH-72b-Instruct Qwen2.5-MATH-7b-Instruct RM@8 Qwen2.5-MATH-7b-Instruct RM@256 Qwen2.5-MATH-72b-Instruct RM@8 Qwen2.5-MATH-72b-Instruct RM@256 OpenAI o1-preview OpenAI o1-mini 14.24 16.12 19.13 19.73 24.16 25.78 26.23 28.45 29.36 30.49 33.22 33.68 36.20 35.70 35.79 36.34 35.95 52.55 60.54 18.19 21.28 23.99 24.55 29.15 30.24 30.30 34.74 36.08 36.12 39.39 40.27 43. 42.12 42.54 43.89 43.47 57.70 67.82 Vanilla Models 12.50 20.45 25.00 23.86 27.59 35.23 29.55 27.27 35.23 39.77 37.50 37.50 42.53 10.16 12.50 13.28 13.28 18.75 15.62 19.53 21.88 24.22 21.88 31.25 27.34 39.84 8.70 9.87 12.19 13.06 14.76 17.99 17.70 20.41 18.68 21.57 26.89 22.53 26. Test-time Scaled Models 36.78 49.43 48.28 47.13 57.47 68.18 33.59 39.06 34.38 35.94 53.91 60.94 31.89 25.79 26.18 25. 43.11 51.50 8.03 7.71 10.04 8.92 11.74 12.71 15.74 16.95 14.41 15.74 16.93 17.50 18.28 18.96 19.75 18.28 19.41 31.26 37.68 10.09 9.98 14.58 15.88 17.03 20.90 19.51 23.47 27.04 25.75 28.62 30.01 34.28 29.59 31.66 33.30 32. 49.67 61.74 12.36 13.58 16.30 16.81 24.66 23.58 26.70 25.06 25.93 29.38 30.37 32.96 33.37 30.88 33.13 34.12 34.12 53.42 60.52 42.78 49.07 53.07 55.93 62.66 65.38 66.23 65.63 63.52 68.38 66.23 70.10 70.96 67.95 68.24 71.24 68. 80.11 82.23 8.01 9.11 10.93 13.15 16.82 18.84 18.91 23.70 24.30 25.01 29.20 29.06 31.37 31.46 30.48 32.04 31.46 50.83 63.10 10.35 11.49 15.29 12.86 16.95 18.06 18.27 20.33 21.52 21.83 24.68 24.71 27.75 27.41 27.81 26.94 27. 42.25 49.11 6.74 7.80 11.86 9.55 13.71 14.61 17.41 21.08 18.54 15.81 20.34 17.98 22.29 24.0 23.71 23.43 26.28 37.71 42.69 Olympic-level mathematics remains far from solved Our findings indicate that Omni-MATH currently presents significant challenges to all models. The strongest model evaluated is OpenAI o1-mini, which utilizes test-time enhancement to achieve an accuracy rate of just 60.54%. OpenAI o1-preview attains an accuracy of 52.55%, while the SOTA vanilla model acquires 36.2%, leaving substantial gap from the top two models. It should be noted that open-source models, like Qwen2.5MATH, have surpassed GPT-4o in the field of Olympiad mathematical reasoning. Domain-Specific Analysis Models demonstrate stronger proficiency in domains such as algebra, calculus, and number theory while showing significant weaknesses in discrete mathematics. We hypothesize that this phenomenon stems from the prevalence of the former subjects in mathematical datasets; nearly all datasets reference algebra and calculus. Conversely, datasets related to discrete mathematics are scarcer, rendering this domain particularly challenging for models. Limitations in Best-of-N Scaling The efficient test-time scaling techniques warrant further development. We have employed the common approach in the current mathematical reasoning studies, known as Best-of-N (Wang et al., 2024; Gao et al., 2024; Yang et al., 2024b), utilizing the Qwen2.5-Math-RM-72B. However, we found that scaling the inference time did not yield consistent improvements in performance. We propose two potential reasons: (1) The reward model demonstrates limited ability to supervise tasks related to Olympic-level mathematics. The reward model can successfully improve the Qwen2.5-MATH-7b-Instruct from total accuracy of 33.22 to 35.79, but fails with Qwen2.5-MATH-72b-Instruct. The reason might be the limitations of the reward models discriminative ability, which is comparable to that of Qwen2.5-MATH-72b-Instruct, making it challenging to further improve performance. (2) The policy model struggles to search the correct solutions. Moreover, with the limitations on inference tokens for OpenAI o1-preview and OpenAI o1-mini set to maximum of 4096, which is considerably lower than the RM@256 costs, we still observe performance that greatly surpasses that of vanilla models. Thus, the development of more efficient test-time enhancement approaches remains an open area for exploration. 7 Omni-MATH"
        },
        {
            "title": "4.1 DATA LEAKAGE ANALYSIS",
            "content": "To eliminate the potential impact of data contamination on the conclusions of the experiments above, it is essential to conduct data leakage detection. Following Xu et al. (2024), we employed n-gram accuracy to identify any data leakage present in the existing models. Specifically, for each sample in the dataset, we concatenated the problem and its corresponding solution, then randomly selected positions for 5-gram extraction. Whether the given sample has been contaminated corresponds to whether the 5 grams predicted by the model are identical to the ground truth 5 grams. We report the results in Table 4. Table 4: Results of data contamination detection. (a) Contaminated; (b) Contaminated & Correct. Model (a) (b) Prop. Avg.D. Prop. Avg. D. DeepSeekMATH-7b-RL DeepSeekCoder-V2-Lite Qwen2-MATH-7b-instruct Mathstral-7B-v0.1 Qwen2.5-MATH-7b-instruct Qwen2.5-MATH-72b-instruct Qwen2-MATH-7b-instruct NuminaMATH-72B-COT MetaLlama-3.1-70B-instruct GPT-4o 0.23% 0.27% 0.41% 0.29% 0.63% 0.70% 0.47% 0.56% 0.09% 0.04% 4.02 4.25 5.3 4.02 4.61 5.39 5.27 2.46 5.25 4.5 0.02% 0.07% 0.09% 0.07% 0.16% 0.27% 0.16% 0.34% 0.07% 0.04% 1.0 3.2 4.12 2.16 4.2 4.54 4.5 1.9 4.8 4. It can be seen that most models exhibit certain data leakage, as Omni-MATH is based on data sourced from the internet, while Qwen2.5-MATH-72b-instruct has the highest degree of data leakage with 5 grams accurately predicted in 31 samples. We also collect the samples that are contaminated and then correctly answered by the tested models, finding that the numbers of the Contaminated and Contaminated & Correct samples are both extremely low. It indicates that data leakage has little impact on the conclusions before and Omni-MATH still poses significant challenges for them. Moreover, we focus on the distribution of problem difficulty in these contaminated samples, where for most tested models, the average difficulty of Contaminated & Correct samples is consistently lower than that of all contaminated samples, except GPT-4o and MetaLlama-3.1-70B-instruct that almost have no data leakage. It seems that difficulty level continues to exert notable influence on the performance of models even in scenarios of data leakage. 4.2 RELIABILITY OF JUDGMENT MODELS There are several studies utilizing model-based evaluation like GPT-4 to evaluate the correctness of answers (Fang et al., 2024; Huang et al., 2024), but whether such judgment models can consistently provide reliable feedback on Olympic-level problems remains inconclusive. In this part, we conduct meta-evaluation experiment to assess the reliability of model-generated judgments. In detail, we first build subset containing 100 samples and engage several graduate and doctoral students to annotate each sample as golden judgments, i.e. whether each solution is aligned with the standard answer for the given problem. Next, the predictions from judgment models are compared with the gold-standard annotations. We were surprised to find that GPT-4o acquires 98% accuracy with human annotations, demonstrating the strong reliability of both the evaluation tool and the results in Omni-MATH. Additionally, Omni-Judge also shows considerable effectiveness with an accuracy of 86%, making it cost-effective tool for quickly evaluating and iterating models. Since GPT-4o shows great consistency with human annotations, we treat it as reliable proxy of human judgment and further analyze the performance of Omni-Judge in Appendix I. 4.3 EMPIRICAL ANALYSIS ON DATA SELECTION In this section, we aim to provide practical conclusions of the data selection in pursuit of better Olympic-level mathematical reasoner. Here we develop novel approach to estimating the impact of domain/difficulty distributions on the final performance, that is, for each model, we manipulate demonstrations in the input and observe variation in performance through in-context learning. To balance the in-context ability of the model and its mathematical reasoning ability, we choose GPT-4o as the base model for this experiment. Due to cost constraints, we randomly sample 500 8 Omni-MATH Figure 6: Results of data selection experiments. (a) The impact of different demonstration domains on model performance. (b) The effect of the distance within the domain tree on model performance. (c) The influence of the difficulty level of in-context examples on model performance. data from Omni-MATH as the evaluation set for this section. The following are the details and conclusions drawn from our experiments. Domain To investigate the mutual influence of data from different domains, we retrieve data from different domains as in-context demonstrations. To balance reasoning cost and the impact of cases, we select four same-domain examples for in-context learning. We present the mutual influence among 5 main domains in Figure 6 (a). For the sake of presentation, we normalize the column according to the diagonal data, i.e., putting in-domain data as the baseline. From the experimental results, we observe that the mutual influence among these domains is asymmetric. For example, Discrete Mathematics has worse influence on Number Theory, while Number Theory has positive effect on the prediction of Discrete Mathematics. Additionally, Applied Mathematics and Algebra show significant positive effect on Discrete Math, with an accuracy of 124% of the in-domain data. Also, Number Theory greatly supports Geometry, with accuracy reaching 150% of in-domain accuracy. Additionally, we find that Algebra interferes with applied mathematics. Common Length The similarity between the in-context examples and the target domain is an important factor that may influence the models performance. To explore this, we conduct experiments to examine the impact of the fine-grained domain similarity between demonstrations and the target domain on the models effectiveness. The results are shown in Figure 6 (b). We define domain similarity as the length of common path on the domain tree. longer common path indicates greater domain similarity, and we find that as the domain similarity increases, the models performance consistency also increases. Compared to random selection, the most significant improvement is brought by the same first-level domain data, suggesting that the model has the ability to generalize the in-domain data at broader level. Difficulty We also discussed the impact of the difficulty of in-context examples on model performance. Consistent with the settings in Sections 3.1 and 4.4, we divided the difficulty into 4 intervals. We find that as the difficulty of examples in the prompt increases, the models performance gradually improves, except for slight decline on T4. Overall, we can conclude that more difficult data would help improve the model performance. 4.4 DIFFICULTY CONSISTENCY ANALYSIS Recent studies (Yang et al., 2024c) have shown that models may exhibit the capability to solve difficult problems, yet fail to perform well on easier ones. Specifically, the accuracy of solving challenging problems is not consistently higher than that of simpler ones in all cases. In this section, we examine the consistency of model performance within the problem difficulty in Omni-MATH. Our difficulty tag is obtained differently from the pairwise data construction employed in Yang et al. (2024c), so it is unsuitable to directly utilize the consistency score from their study. However, we can measure the trend intensity of model performance as the difficulty increases for consistency. Specifically, we use the four difficulty intervals in Table 3 for the overall assessment. The model performance is defined by the variables x1, x2, x3, x4, where 0 < xi < 100. Then we define the trend intensity by simplifying the implementation of ARIMA Modeling (Box & Pierce, 1970) 9 Omni-MATH and adapting it to our situation. The implementation of with variables is shown below: = (cid:40) n1 (cid:88) i=1 (xi xi+1) min( max(x) , xi xi+1) if xi+1 > xi if xi+1 <= xi (1) This equation measures the predicted values and their relative comparison at each position based on the first difference. We assume that decreases as increases. For counter-trend situations, we apply penalty coefficient that is double that of positive trends. We constrain the largest impact of each position to the max(x) = 25. For sequences of four variables with penalty coefficient of = 2, the value range of is [200, 75]. Intuitively, the more pronounced the downward trend of x, the larger the value of A; conversely, the smaller the value of A. The experimental result is shown in Figure 7. Figure 7: Performance and Consistency Comparison of Different Models. Consistency is measured by Equation 1. The difficulty consistency represents shows that our difficulty level is generally aligned with the performance of models. From the experimental results, we observe that the consistency of all models is positive, indicating that as the difficulty increases, the overall accuracy of all models declines. This finding also validates the rationality of our difficulty classification. Additionally, we note that as model capabilities improve, their consistency also increases, but exceptions also exist. This conclusion is similar to that reached in Yang et al. (2024c), even though our methods of measurement are entirely different."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 MATHEMATICS BENCHMARKS Measuring the mathematical reasoning abilities of language models has long been focal point in both academia and industry. GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) are the key benchmarks in the mathematical reasoning domain. GSM8K primarily emphasizes models capacity to solve practical application-based math problems, while MATH presents greater challenge, encompassing university-level and introductory competition-level mathematics questions. As models become more powerful, several works have proposed more challenging problems, such as OCWCourses (Lewkowycz et al., 2022), SAT (Azerbayev et al., 2024), JeeBench (Arora et al., 2023), and MATHOdyssey (Fang et al., 2024). However, as models rapidly improve, the challenge posed by these benchmarks is gradually diminishing, making it increasingly difficult to differentiate their mathematical reasoning abilities. Another line of research (Zheng et al., 2022; Azerbayev et al., 2023) employs formal languages to measure the models theorem proving capabilities. 5.2 OLYMPIAD-LEVEL BENCHMARKS To further explore the boundaries of large language model capabilities, many studies have attempted to yield Olympiad-level data as benchmarks. Among these, AlphaGeometry (Trinh et al., 2024) offers Olympiad-level geometry problems. CHAMP (Mao et al., 2024) provides high school competition-level math problems, distinctly annotated with concepts and hints relevant and helpful for each problem. OlympiadBench (He et al., 2024) and OlympicArena (Huang et al., 2024) 10 Omni-MATH are two comprehensive benchmarks that encompass Olympiad problems from various fields. However, within these benchmarks, the proportion of text-only mathematical reasoning data remains scarce. Furthermore, none of the aforementioned benchmarks have further differentiated and analyzed the problems from Olympiad mathematics competitions. Our proposed Omni-MATH focuses exclusively on text-only Olympiad-level mathematical reasoning to truly explore the boundaries of current LLMs mathematical capabilities. We further categorize the problems from the Olympiad mathematics competitions into various difficulty levels, providing clear and reasonable difficulty classification setting for better diagnosing model performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce Omni-MATH, comprehensive and highly challenging benchmark for Olympiad-level mathematical reasoning. Our experiments reveal that even the most advanced model in mathematical reasoning, OpenAI o1-mini, achieves maximum accuracy of only 60.5%, while the open-source model currently achieves only 36.2%. These results demonstrate that the highest difficulty level in mathematics presents significant challenges for large models. Additionally, we conducted an in-depth analysis of model performance based on this dataset, aiming to provide valuable contributions to the mathematics community."
        },
        {
            "title": "REFERENCES",
            "content": "Abhinav Pandey Abhimanyu Dubey, Abhinav Jauhri and et al. Abhishek Kadian. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Mistral AI. Mathstral. https://mistral.ai/news/mathstral/, 2024. Accessed: 20247-16. Anthropic. Claude 3.5. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2024-6-21. Daman Arora, Himanshu Gaurav Singh, and Mausam. Have llms advanced enough? challenging problem solving benchmark for large language models, 2023. URL https://arxiv.org/ abs/2305.15074. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W. Ayers, Dragomir Radev, and Jeremy Avigad. Proofnet: Autoformalizing and formally proving undergraduate-level mathematics, 2023. URL https://arxiv.org/abs/2302.12433. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics, 2024. URL https://arxiv.org/abs/2310.10631. George EP Box and David Pierce. Distribution of residual autocorrelations in autoregressiveintegrated moving average time series models. Journal of the American statistical Association, 65(332):15091526, 1970. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/abs/2403.04132. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, 11 Omni-MATH Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024. URL https://arxiv.org/abs/2406.11931. Arthur Engel. Problem-Solving Strategies. Springer New York, NY, 1998. doi: https://doi.org/10. 1007/b97682. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data, 2024. URL https://arxiv.org/abs/2406.18321. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Junyang Lin, Chang Zhou, Tianyu Liu, et al. The reason behind good or bad: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai, 2024. URL https://arxiv.org/abs/2406.12753. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/ abs/2409.12186. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface. co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024. Yujun Mao, Yoon Kim, and Yilun Zhou. Champ: competition-level dataset for fine-grained analyses of llms mathematical reasoning capabilities, 2024. URL https://arxiv.org/ abs/2401.06961. OpenAI. Gpt-4 technical report. 2023. URL https://api.semanticscholar.org/ CorpusID:257532815. OpenAI. Learning to reason with llms, 2024. https://openai.com/index/ learning-to-reason-with-llms/. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. 12 Omni-MATH Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024. URL https://arxiv.org/abs/2312.08935. Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui Hsieh, Inderjit Dhillon, and Sanjiv Kumar. Preserving in-context learning ability in large language model fine-tuning. 2022. Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824, 2024. URL https://arxiv.org/ abs/2404.18824. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024b. URL https://arxiv.org/abs/2409.12122. Zhe Yang, Yichang Zhang, Tianyu Liu, Jian Yang, Junyang Lin, Chang Zhou, and Zhifang Sui. Can large language models always solve easy problems if they can solve harder ones?, 2024c. URL https://arxiv.org/abs/2406.12809. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. Internlm-math: Open math large language models toward verifiable reasoning, 2024. URL https://arxiv.org/abs/2402.06332. Paul Zeitz. The art and craft of problem solving. John Wiley & Sons, 2017. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?, 2024. URL https://arxiv.org/abs/ 2403.14624. Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: cross-system benchmark for formal olympiad-level mathematics, 2022. URL https://arxiv.org/abs/2109.00110. 13 Omni-MATH ANALYSIS OF PROCESS-LEVEL ASSESSMENT In this section, we utilize GPT-4o for process-level assessment. Specifically, we provide GPT-4o of the question, the corresponding standard answer, and the model-generated answer in the prompt. We split both the reference answer and model-generated answer into steps and let GPT-4o evaluate the correctness of each step of the model-generated answer. If step is identified as incorrect, we further query an explanation for the error. Following the Gao et al. (2024), we categorize the mathematical reasoning steps as follows: Unrelated: This indicates that the step is irrelevant and does not contribute towards deducing the final answer. Accumulation: This denotes that the step is incorrect due to mistake in the preceding step, leading to subsequent errors. Calculation: This categorization is reserved for errors arising from incorrect calculations, which is one of the most common errors in mathematical reasoning. Logic: This applies to steps that are logically flawed in the context of solving the given problem. Other: This category encompasses steps that are erroneous for reasons not covered by the aforementioned categories. Then we compiled the proportions of step-level error types and the model performances, as illustrated in Figure 8. (a) Figure 8: Process-level result across differnec (b) Logical errors represent significant issue for the models. As shown in Figure 8 (a), among the best existing inference models, most erroneous steps can be attributed to logical errors, followed by accumulation errors and calculation errors. This highlights the challenges posed by our proposed assessment set for current models. OpenAI O1-mini demonstrates significantly fewer errors across all categories. Figure 8 (b) illustrates the error distribution across different categories for various models. Given that the total number of errors varies by category, we applied normalization. Our findings indicate that the OpenAI O1-mini model outperforms other models across all error categories. Notably, GPT-4o exhibits higher incidence of calculation and logical errors compared to other math-specialized models, underscoring that specialized training in mathematics is essential for Olympiad-Level mathematical reasoning. Additionally, Qwen2.5-MATH-72b-Instruct shows tendency to produce unrelated content compared with other models. 14 Omni-MATH"
        },
        {
            "title": "B DETAILED DATA LEAKAGE INFORMATION",
            "content": "We examine whether the given instance has been contaminated by testing whether the models predicted 5 grams are identical to the ground truth 5 grams. The detailed experimental results are illustrated in the Figure 9. (a) Number of instances of different models affected by contamination and the correct instances under contaminated conditions. (b) Average difficulty of contaminated and correct instances. Figure 9: Data Leakage and Correct Instances for Different Models."
        },
        {
            "title": "C ANNOTATION DETAILS",
            "content": "C.1 ANNOTATION GUIDELINE Figure 10: The illustration of the UI page of the annotation task. The detailed annotation tasks and principles are listed here. Specifically, the annotators were assigned the following three tasks: Determine whether the current problem contains multiple questions or is proof-based. If it is multi-part question or proof question, the evaluation of this data becomes more complex, making it unsuitable for the evaluation in our benchmark; therefore, we decided to discard this part of the data. Verify the correctness of the answers present in the existing dataset. This step is essential for assessing answer quality; different verification methods are required depending on the data source. For entries from AoPS Wiki and problems with provided solutions, it suffices Omni-MATH to check for consistency with the answers in the data source. For data from the AoPS Forum, it is also necessary to refer back to the results provided in the forum. If the original answer is found to be incorrect, provide corrected answer based on the data source. C.2 ANNOTATION DETAILS For the 1100 questions collected from the AoPS Forum, we employed 4 annotators to verify the consistency between the final responses and the original forum replies. Each annotator labeled 550 questions, resulting in two annotations per question. We then conducted cross-validation and sampled cases for inspection. The accuracy of the cross-validation was 92.7%(1020/1100), while the post-sampling accuracy was 97.3%(146/150). After removing discrepancies, we obtained total of 888 questions from this set. For the Contest Page questions converted from PDF to LaTeX, the quality was exceptionally high. We employed rough filtering rules to eliminate some errors that occurred during the conversion process. Subsequently, manual check was performed to ensure the consistency between answers and explanations. This part resulted in total of 3295 questions. Regarding the questions from the AoPS Wiki, comprehensive explanations were readily available. We only needed to filter out the video-based explanations and exclude questions that did not fit within our collection scope, such as AMC and AIME questions. This section yielded total of 298 questions. After the labeling process, the initial dataset comprised 4481 questions. Subsequently, we hired 2 annotators to inspect the entire dataset, following set of principles. After final filtration, 53 questions were removed from the Contest Page source, resulting in final dataset of 4428 questions."
        },
        {
            "title": "D DATA CRAWLING DETAILS",
            "content": "To enhance the possibility of obtaining correct answers in AoPS Wiki, we specifically targeted posts that contained hidden tags indicating solution, as well as those that included boxed{} or . Following LI et al. (2024), we assume that these posts have higher probability of containing accurate answers. Then employed GPT-4 to reformat the user-uploaded solutions into the standardized format of solution + boxed{Final Answer} to improve the answer format quality. 16 Omni-MATH DETAILED DATA INFORMATION OF OMNI-MATH E.1 DETAILED DATA SOURCE OF OMNI-MATH Table 5: The hierarchical data source of the Omni-MATH. Contests Number of Problems Highly Influential International Competitions 75 190 43 101 68 IMO IMO Shortlist IMO Longlist Putnam IMC Notable International/National Challenging Competitions USAMO IZhO China TST CMO USA TST 133 14 106 38 45 Intermediate International/National Competitions 85 39 26 35 23 33 55 37 22 21 269 APMO USAJMO Balkan MO Balkan MO Shortlist JBMO JBMO Shortlits ToT BalticWay Alibaba global Contest Middle European Mathematical Olympiad Other Olympiads National Championships or Competitions HMMT 2 HMMT 11 Yau Contest Introductory Competitions Pascal Cayley Fermat 1385 896 249 201 232 E.2 DIFFICULTY The detailed difficulty distribution of the Omni-MATH is shown in Figure 11. The difficulty distribution roughly follows normal distribution E.3 DOMAIN CLASSIFICATION TREE The overall domain tree with the problem number is shown in Figure 12. Note that the total number is larger than the total data of 4428 because the question might belong to multiple domains. E.4 DIFFICULTY CLASSIFICATION PROMPTS For data not included in Aops:Ratings, we need to assign difficulty score to the problem. We collect all the problems with its solution and the data source. Then we use GPT-4o to assign difficulty level of the problem, the prompt we use is shown in Figure 13. The relevant page information makes the prompt too long to show on this page. Instead, we upload the total prompt as the additional materials. 17 Omni-MATH Figure 11: The difficulty distribution of Omni-MATH. E.5 DOMAIN CLASSIFICATION PROMPTS We use GPT-4o to assign domain tag to each problem, the prompt we use is shown in Figure 14. The domain tree makes the prompt too long to show on this page. Instead, we upload the total prompt as the additional materials. 18 Omni-MATH Figure 12: The total domain tree of Omni-MATH."
        },
        {
            "title": "F DETAILED EXPERIMENTAL SETUP",
            "content": "F.1 MODELS Here are the baseline models we evaluated: InternLM2-MATH-mixtral8*22B (Ying et al., 2024); DeepSeekMATH-7b-RL (Shao et al., 2024); Mathstral-7B-v0.1 (AI, 2024); DeepSeek-Coder-V2-Lite-Instruct (DeepSeek-AI et al., 2024); MetaLlama-3.1-70B-instruct (Abhimanyu Dubey & Abhishek Kadian, 2024); DeepSeek-Coder-V2 (DeepSeek-AI et al., 2024); Claude-3.5-SONNET (Anthropic, 2024); NuminaMATH-72B-COT (LI et al., 2024); Qwen2-MATH-7b-Instruct (Yang et al., 2024a); Qwen2.5-MATH-7b-Instruct (Yang et al., 2024a); Qwen2-MATH-72b-Instruct (Yang et al., 2024a); Qwen2.5-MATH-72b-Instruct (Yang et al., 2024a); GPT-4o (OpenAI, 2023); OpenAI o1-preview (OpenAI, 2024); OpenAI o1mini (OpenAI, 2024). F.2 MODEL INFERENCE SETUP For all the models under evaluation, the prompts follow the instructions provided on their respective release pages, such as DeepseekMATH-7b-RL and Qwen2.5-MATH-7b. Omni-MATH Figure 13: The detailed difficulty classification prompt. In chat models where there are no specific instructions on how to phrase the mathematical reasoning prompts, we directly input the model with the problem along with system prompt: You are an experienced educator in the field of MATHEMATICS. To mitigate randomness in the responses, we set the parameters as follows: temperature = 0, top = 1, and maximum of 2048 tokens. For the O1-preview and O1-mini, due to constraints of inference costs, we configured the maximum completion tokens to 4096. Additionally, for inference with non-API models, we employed the vllm framework. 20 Omni-MATH Figure 14: The detailed domain classification prompt."
        },
        {
            "title": "G METRICS WITH DISTINCT DIFFICULTY LEVEL",
            "content": "In this section, we present the fine-grained performance of the model across varying difficulty levels in Table 6. Due to the limited number of instances at certain decimal levels, such as 4.75 and 2.25, we categorize all difficulty levels into integer buckets. For example, score of 4.75 falls under the #D5 category, while 2.25 falls under #D3. The results of our experiments are depicted in the figure below. We observed consistent decline in the models performance as difficulty increased, except for the #D10 category, which contains very few instances (only 16), leading to significant fluctuations in results. Additionally, we note that the accuracy for nearly all model responses in the #D1 category is exceptionally high, indicating that this difficulty level does not effectively differentiate model performance. However, as the difficulty increases, the differences among the model performances begin to emerge. Overall, we find that the difficulty of the GSM8K dataset mainly corresponds to the difficulty of #D1, while the MATH dataset aligns with the difficulty of #D2. This further underscores that our proposed benchmark significantly exceeds the existing evaluation datasets. 21 Omni-MATH Table 6: Main Result with different distinct difficulty levels. Model Acc #D1 #D2 #D3 #D #D5 #D6 #D7 #D8 #D9 #D DeepSeekMATH-7b-RL MetaLlama-3.1-70B-instruct DeepSeek-Coder-V2 Claude-3.5-SONNET NuminaMATH-72B-COT GPT-4o Qwen2.5-MATH-7b-Instruct Qwen2.5-MATH-72b-Instruct Qwen2.5-MATH-7b-Instruct RM@8 Qwen2.5-MATH-7b-Instruct RM@256 Qwen2.5-MATH-72b-Instruct RM@8 Qwen2.5-MATH-72b-Instruct RM@256 OpenAI o1-preview OpenAI o1-mini Vanilla Models 82.53 84.12 88.09 88.88 84.12 88.09 86.50 87.30 49.06 65.86 69.06 69.06 68.26 70.93 66.66 73. 27.77 42.92 43.93 46.46 45.45 51.01 52.52 55.55 11.83 20.05 25.23 26.23 26.02 32.53 36.12 38.89 Test-time Scaled Models 86.50 85.71 84.92 84.92 88.09 88.09 69.06 73.06 73.06 70. 79.2 81.55 54.04 58.58 59.09 54.04 76.77 79.79 37.14 37.81 40.51 39.83 58.55 72.70 16.12 24.16 25.78 26.23 28.45 30.49 33.22 36. 35.70 35.79 36.34 35.95 52.55 60.54 7.37 14.99 15.28 14.59 19.58 20.73 25.34 27.19 28.20 26.41 27.50 26.80 46.78 57.98 8.51 16.38 14.39 15.47 17.64 18.29 23.06 26. 26.67 27.76 24.80 25.89 43.21 51.08 12.07 16.01 20.94 19.20 23.16 23.12 25.00 28.33 27.92 28.33 27.31 29.15 41.07 47.03 12.61 16.00 16.56 17.48 19.93 21.77 23.00 24. 24.92 24.30 26.15 24.92 39.38 44.78 8.58 13.75 15.95 19.01 23.92 16.04 20.37 22.5 23.12 20.62 24.37 27.5 40.00 45.39 0.00 12.5 0.0 6.25 12.5 12.5 18.75 25. 25.0 18.75 12.5 18.75 12.5 12."
        },
        {
            "title": "H DETAILED EVALUATION INFORMATION",
            "content": "H.1 GPT-4O EVALUATION PROMPTS Figure 15 shows the detailed prompt we use in GPT-4o based evaluation. However, the few-shot prompt is too long to show in the paper. Instead, we will provide the total prompts in our additional materials. 22 Omni-MATH Figure 15: The evaluation prompt of GPT-4o. Omni-MATH H.2 CASE STUDY OF EVALUATION ISSUE Figure 16: The case study of GPT-4o-based evalution. In Figure 16, we selected data from two Human Annotated Datasets that represent two scenarios where rule-based evaluation is impractical. However, GPT-4o demonstrates the capability to assess correctly in both situations. The first scenario is reasoning. It requires nuanced understanding of the problem statement, which implies that is positive integer greater than 0. The correct answer is = 1; however, the model generates the answer as 2. Considering the defined domain for and the constraints of the answer, we can infer that these two expressions are equivalent. Thus, certain level of reasoning ability is necessary to arrive at the correct conclusion. The second scenario involves complex formatting. In this case, the order of the answers does not align with the sequence of the tuples generated by the model in LaTeX, leading to inconsistencies. This makes it challenging to employ rule-based evaluations effectively. 24 Omni-MATH Table 7: Omni-Judge results. We implement Omni-Judge based on three base models: LLaMA2-7b-Chat, LLaMA-3-8b-Instruct, and LLaMA-3.1-8b-Instruct, and evaluate their performance according to the rate of successfully parsing outputs and the rate of consistency between the judge of Omni-Judge and GPT-4o for each problem. Model MetaLlama-3.1-70B-instruct DeepSeek-Coder-V2 Qwen2.5-MATH-7b-Instruct OpenAI o1-preview OpenAI o1-mini Mathstral-7B-v0.1 NuminaMATH-72B-COT Qwen2.5-MATH-72b-Instruct Total Training with Homology Data LLaMA-2-7b-Chat LLaMA-3-8b-Instruct LLaMA-3.1-8b-Instruct Success Consistency Success Consistency Success Consistency - 98.81 97.56 98.45 99.33 99.78 99.33 100.00 98.66 98.99 73.63 38.36 35.92 55.03 63.11 31.49 31.11 37.28 45.50 99.76 100.00 99.78 100.00 100.00 99.78 100.00 99.78 99.89 77.67 93.35 89.80 90.83 89.56 95.12 88.89 91.96 89. 99.76 100.00 100.00 99.78 100.00 100.00 100.00 100.00 99.94 82.19 94.01 90.69 91.28 91.78 95.79 90.44 93.30 91.26 Figure 17: Results of predicted accuracy on different models, conducted by Omni-Judge and GPT4o, respectively. ANALYSIS OF OMNI-JUDGE Omni-Judge is proposed as an economic but effective way to provide judgment for complicated mathematical problems accompanied by reference answers and tested solutions. In this part, we test Omni-Judge on the quality of generated judgment in two aspects: whether in precise format and whether to generate correct judgment, represented by the rates of being successfully parsed (Success) and consistency rates with golden judgments (Consistency), as in Table 7. Note that judgments from GPT-4o are seen as the golden ones here. Furthermore, as discussed in Section 2.4, we experiment with various LLaMA-series models, and with upgraded versions, the comprehensive capabilities of models are generally acknowledged to improve. We find the performance of Omni-Judge also shows enhancements along this trend. In detail, we collect test data from several models tested in Table 3, 5 of which also provide data for training Omni-Judge (denoted as Training with Homology Data) and the rest 3 ones do not, while they demonstrate similar conclusions. In terms of format accuracy, the Success rates are approximately 100% across different settings, while the Consistency rates are not. Omni-Judge model trained with LLaMA-3.1-8b-Instruct shows the best Consistency with GPT-4o in total, followed by the version of LLaMA-3-8b-Instruct. On the other hand, Omni-Judge exhibits the lowest scores based on LLaMA-2-7b-Chat. The high Success suggests that models tend to learn formats first, aligning with findings of format learning in Wang et al. (2022). However, to provide better judgments, the base models are required to effectively capture the optimization directions during fine-tuning or possess knowledge in this domain. Therefore, the highest Consistency of Omni-Judge on LLaMA-3.1-8b-Instruct is reasonable, and we consequently use it as the default version of Omni-Judge. Another significant value of mathematical benchmarks is the provided ranking, which reflects the relative capability of each subject among all tested models. Therefore, it is meaningful to compare the ranking of model capabilities from Omni-Judge and GPT-4o, which suggests whether Omni25 Omni-MATH Judge can be practical judgment model offering reliable feedback. As in Figure 17, Omni-Judge provides outcomes similar to GPT-4o for each tested model, as well as an identical ranking of model capabilities according to the predicted accuracy, which proves its usability."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Engineering Research Center of Information Networks",
        "Institute of Software, Chinese Academy of Sciences",
        "Peking University",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong, Shenzhen",
        "The University of Hong Kong",
        "University of Waterloo",
        "University of Wisconsin - Madison",
        "Zhongguancun Laboratory"
    ]
}