{
    "paper_title": "RotaTouille: Rotation Equivariant Deep Learning for Contours",
    "authors": [
        "Odin Hoff Gardaa",
        "Nello Blaser"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contours or closed planar curves are common in many domains. For example, they appear as object boundaries in computer vision, isolines in meteorology, and the orbits of rotating machinery. In many cases when learning from contour data, planar rotations of the input will result in correspondingly rotated outputs. It is therefore desirable that deep learning models be rotationally equivariant. In addition, contours are typically represented as an ordered sequence of edge points, where the choice of starting point is arbitrary. It is therefore also desirable for deep learning methods to be equivariant under cyclic shifts. We present RotaTouille, a deep learning framework for learning from contour data that achieves both rotation and cyclic shift equivariance through complex-valued circular convolution. We further introduce and characterize equivariant non-linearities, coarsening layers, and global pooling layers to obtain invariant representations for downstream tasks. Finally, we demonstrate the effectiveness of RotaTouille through experiments in shape classification, reconstruction, and contour regression."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 5 3 6 1 . 8 0 5 2 : r RotaTouille: Rotation Equivariant Deep Learning for Contours Odin Hoff Gardaa University of Bergen odin.garda@uib.no Nello Blaser University of Bergen nello.blaser@uib.no"
        },
        {
            "title": "Abstract",
            "content": "Contours or closed planar curves are common in many domains. For example, they appear as object boundaries in computer vision, isolines in meteorology, and the orbits of rotating machinery. In many cases when learning from contour data, planar rotations of the input will result in correspondingly rotated outputs. It is therefore desirable that deep learning models be rotationally equivariant. In addition, contours are typically represented as an ordered sequence of edge points, where the choice of starting point is arbitrary. It is therefore also desirable for deep learning methods to be equivariant under cyclic shifts. We present RotaTouille, deep learning framework for learning from contour data that achieves both rotation and cyclic shift equivariance through complexvalued circular convolution. We further introduce and characterize equivariant non-linearities, coarsening layers, and global pooling layers to obtain invariant representations for downstream tasks. Finally, we demonstrate the effectiveness of RotaTouille through experiments in shape classification, reconstruction, and contour regression."
        },
        {
            "title": "Introduction",
            "content": "Equivariance and invariance are the central concepts in geometric deep learning. Designing architectures that respect certain symmetries, often defined in terms of group actions, allows us to incorporate prior geometric knowledge about the data into the learning process. This can lead to models that generalize better, require less data, and are more efficient by reducing the effective hypothesis space. Equivariance is especially useful when the task requires the model to be sensitive to transformations of the input, such as translation, rotation, or permutation, while still producing consistent and meaningful output. Invariance, on the other hand, is desirable when the output should remain unchanged under transformations. Convolutional neural networks (CNNs) illustrate both concepts in image analysis. Convolutions are translation-equivariant, so shifting an input image shifts the feature maps accordingly, which is useful for segmentation, while applying global pooling after the convolutional layers achieves invariance, allowing classification to be insensitive to the objects position. Graph neural networks (GNNs) offer another example: they are often designed to be invariant or equivariant under permutations of node orderings (graph isomorphisms). comprehensive overview of methods and concepts in geometric deep learning can be found in [9] and [17]. In this paper, we focus on what we will refer to as contours. Contours are complex-valued signals on finite cyclic groups, i.e., functions Zn Ck, where Zn denotes the cyclic group of order n, and Ck is the k-dimensional complex vector space consisting of k-tuples of complex numbers. Contours can serve as sparse representations of object boundaries, but they are not limited to simple closed shapes. Contours can also accommodate more general signals, including self-crossing curves and multichannel contours. For example, contours occur naturally as cell shapes in cell morphology [7, 10, 29], and as orbit plots in the vibrational analysis of rotating machinery [11, 21, 22]. Our method is based on complex-valued neural networks, which are neural networks using complexvalued weights that are often employed in applications such as radar imaging [16, 33], MRI fingerprinting and reconstruction [14, 40], and other areas where data are naturally represented in the complex domain. See [27] and [4] for detailed overview of complex-valued neural networks and Preprint. Preliminary work. Rotation Equivariant Deep Learning for Contours their applications. Complex-valued CNNs have also been applied to images and shown to demonstrate self-regularizing effects [18]. 1.1 Contributions The group Gn = Zn S1 acts on contour : Zn Ck by combining two operations: the cyclic group Zn acts by cyclically shifting the starting point of the contour, and S1 acts by rotating the image of about the origin in each copy of the complex plane. We propose rotationand cyclic shiftequivariant (as well as invariant) layers for deep learning on contours, leveraging complex-valued convolutions over the cyclic group (circular convolution). Convolution is well known for its cyclic shift-equivariant property, and by working in the complex domain, we automatically get equivariance to planar rotations as well. Our framework RotaTouille consists of the following main types of layers: Convolution layers. Linear equivariant layers based on complex circular convolutions. Activation functions. Equivariant non-linear functions applied element-wise allowing the network to learn more complex functions. Coarsening layers. Equivariant local pooling layers downsampling the signal by coarsening the domain. Invariant layers. Invariant global pooling layers producing real-valued embeddings for downstream tasks. This layer taxonomy aligns well with the Geometric Deep Learning Blueprint proposed in [9, p. 29]. Throughout the paper, we show that the proposed layers are indeed equivariant and provide classification of all possible equivariant non-linear activation functions in Section 2.4. We evaluate RotaTouille in different tasks, including shape classification, shape reconstruction, and node-level curvature regression. Our implementation, including all code required to reproduce the experiments, is publicly available.1 1.2 Related Work Shape analysis is central topic in computer vision and machine learning, with early approaches often based on hand-crafted descriptors computed from contours. Examples include Curvature Scale Space (CSS) representations [1, 30] and generalized CSS (GCSS) [6], which were later used in the DeepGCSS neural network classifier [31]. The use of neural networks for contour data appeared already in [19], where the authors used fully connected neural networks to classify contours. Other classical works combined contour fragments with skeleton-based features to improve shape recognition [3537]. The shape context (SC) descriptors [5], based on log-polar histograms, have also been shown to be effective in capturing the local geometric structure in rotation-invariant way. More recently, deep learning methods have become more popular for contour analysis. ContourCNN [15] models planar contours using real-valued circular convolutions on point sequences, with custom pooling strategy to discard shape-redundant points. similar convolutional approach was proposed in [28]. Two-dimensional CNNs have also been applied to contour images [3, 11, 22] and transformer-based architectures have been adapted for shape data using SC descriptors in [23]. None of these deep learning-based methods is intrinsically equivariant or invariant to rotations and they instead rely on data augmentation or manual feature extraction of invariant features. Several works have explored learning rotation equivariant or invariant representations more directly, especially in settings of images or point clouds. The O2-VAE model [10], for instance, applies steerable CNNs [13] to cell images, encoding them into latent space that is invariant to planar rotations. Although effective in capturing textureand intensity-based features, such approaches operate on pixel grids rather than directly on contour data. Group-equivariant convolutional neural networks (G-CNNs) [12] likewise provide general framework for building equivariant networks with respect to discrete symmetry groups, although these have been applied mostly in image-based contexts. Closer in spirit to our approach are quaternion neural networks for 3D point clouds [38], where rotation-equivariant layers are constructed using the action of unit quaternions on R3. Topological methods have also been applied to contour data. Persistent homology has been used to extract stable shape features for morphological classification and comparison [7, 29]. 1https://github.com/odinhg/rotation-equivariant-contour-learning 2 Rotation Equivariant Deep Learning for Contours"
        },
        {
            "title": "2 Methodology",
            "content": "We begin by defining contours and the action of the group Zn S1 on them, along with the concepts of equivariant and invariant maps. Subsequently, we introduce the various equivariant and invariant layers that form RotaTouille. 2.1 Contours For an integer > 0, we let Zn = {0, 1, . . . , 1} denote the cyclic group of order under addition modulo n. contour is function : Zn Ck where > 0. The set of all contours = map(Zn, Ck) is vector space over with addition and scalar multiplication defined pointwise. For any function : Ck where is set, we use the shorthand notation fj = πj where πj : Ck is the projection onto the j-th coordinate. 2.2 Group Actions and Equivariant Maps The circle group S1 consists of all unit complex numbers and the group product is given as complex multiplication. We consider the product group Gn = Zn S1 and let it act on contours as follows: for (l, w) Gn and contour we define (l, w) to be the contour mapping (cid:55) wx(q l). Here, acts by scalar multiplication in Ck and is computed modulo n. function : where is divisor of is called equivariant if ((l, w) x) = (l, w) (x) for all and all (l, w) Gn. That is, the function commutes with our group action on contours. function : where is set is called invariant if ((l, w) x) = (x) for all and all (l, w) Gn. The concepts of invariance and equivariance are more general than described here and can be defined for any group action, including group representations (also known as linear actions). For broader exposition, see [9, Chapter 3.1] or [41, Chapter 2.1]. 2.3 Circular Convolution as Linear Equivariant Layer 1 satisfies (wx) = wT (x) and (x + y) = (x) + (y) for all S1 and If map : 1 n, then is automatically C-linear. This is consequence of the fact that every complex x, 1 number can be written as (nonunique) finite sum of unit complex numbers. Now, suppose also commutes with cyclic shifts, so is any additive map that is equivariant with respect to our group action: ((l, w) x) = (l, w) (x). Then it is well known that is necessarily the circular convolution operator. For completeness, we include proof of this in Proposition A.1. is defined by letting (y x)(q) = (cid:80)n1 The circular convolution : 1 j=0 y(j)x(q j) and Zn. In convolutional neural networks, we tend to work with kernels smaller for x, 1 than the signal. If 1 for some n, we extend with zeros to 1 before convolving with x. That is, we let y(q) = 0 whenever m. We also want our equivariant layers to handle multichannel signals, both as inputs and as hidden representations. By convention, we integrate information across the input channels by summing. Fixing ϕ m, we define the circular convolution operator Convϕ : 1 1 as Convϕ(x) = (cid:88) j=1 ϕj xj for n. We refer to ϕ as filter (or kernel) that is typically learned during training, and as the kernel size. Note that we do not use an additive bias term as this would break rotational equivariance. In practice, we often have multiple filters in each convolutional layer. Let Φ = (ϕ1, . . . , ϕk ) be collection of filters, often referred to as filter bank. The convolutional layer denoted ConvΦ : is defined coordinate-wise by letting ConvΦ(x)j = Convϕj (x) for all = 1, . . . , k. We show that the convolutional layer satisfies the equivariance property. Proposition 2.1. The convolutional layer ConvΦ : (l, w) Gn and n, we have (l, w) ConvΦ(x) = ConvΦ((l, w) x). is equivariant, that is, for all k Rotation Equivariant Deep Learning for Contours Proof. For (l, w) Gn, Zn and ϕ Φ, we have ((l, w) Convϕ(x))(q) = (cid:88) (ϕj xj)(q l) = j= (cid:88) j=1 w(ϕj xj)(q l) = (cid:88) j= (ϕj ((l, w) xj))(q) = Convϕ((l, w) x)(q) for any n. Since ConvΦ and the group action are defined coordinate-wise, we are done. 2.4 Non-linear Activation Functions To learn more complex, non-linear functions on the space of contours, we need to introduce nonlinear activation functions between the linear layers. We also want these activation functions to be equivariant. Any function : can be extended to function by point-wise application in each coordinate, and this function is equivariant if and only if a(wz) = wa(z) for all and S1. Such functions can readily be classified: Proposition 2.2. function : satisfies the equivariance condition a(wz) = wa(z) for all and S1 if and only if there exists function : [0, ) such that a(z) = g(z)z for all C. Proof. If a(z) = g(z)z, then a(wz) = g(wz)wz = wg(z)z = wa(z) since = 1. Now, assume that satisfies the equivariance condition. We have a(0) = wa(0) for all S1, so a(0) must be zero. If = 0, write = rw with > 0 and S1, and observe that a(z) = a(rw) = wa(r) = zr1a(r). Define the function by letting g(0) = 0 and g(r) = r1a(r) whenever = 0. Functions on this form already appear in the existing literature on complex-valued neural networks and we list some of them in Table 1. Table 1: Examples of equivariant activation functions for contours. Activation Function Siglog [40] ModReLU2 [2, 42] Amplitude-Phase [20] Choice of g(r) (r + 1)1 ReLU(r + b)r1 tanh(r)r 2.5 Coarsening Functions coarsening (or local pooling) function is an equivariant function : with < n. In this section, we suppose that = mp for some integer > 1 and think of as the coarsening factor. We fix two positive integers ns and nd called stride and dilation, respectively, and let (cid:76) : Cp be any function such that (cid:76)(wz) = (cid:76)(z) for all S1 and Cp. For j=0 zj instead of (cid:76)(z0, . . . , zp1). Examples include magnitude-based convenience, we write (cid:76)p1 argmax (cid:76)p1 j=0 zj for Cp. For contour 1 j=0 zj = arg maxj=0,...,p1 zj, and the mean (cid:76)p1 we define the coarsening function : 1 j=0 zj = p1 (cid:80)p1 1 by letting P(x)(q) = p1 (cid:77) j=0 x(qns + ndj) for all Zm and extend this to function : coordinate-wise. We refer to the case with ns = 1 and nd = as coset pooling. That is, we pool over the cosets + = {q, + m, . . . , + (p 1)m} for Zm and the function is equivariant as we have 2The ModReLU activation function has learnable bias parameter R. 4 Rotation Equivariant Deep Learning for Contours P((l, w) x)(q) = p1 (cid:77) j=0 wx((q + mj) l) = p1 (cid:77) j=0 x((q l) + mj) = ((l, w) P(x))(q). For ns = and nd = 1, we get strided pooling, which is the most common type of pooling operation in CNNs. However, strided pooling is only approximately equivariant in the sense that cyclically shifting the input signal by lp, cyclically shifts the output signal by l: P((lp, w) x)(q) = p1 (cid:77) j=0 wx((qp + j) lp) = p1 (cid:77) j=0 x((q l)p + j) = ((l, w) P(x))(q). In other words, strided pooling is equivariant with respect to the subgroup Zn S1 of Gn, but is true to the assumption that points close in the domain Zn tend to be mapped to close points in Ck. Coset pooling, on the other hand, is truly equivariant, but does not aggregate locally. We found strided pooling to work better in practice despite the approximate equivariance. 2.6 Invariant Feature Extraction global pooling function is an invariant map : where is some set. We will only consider = Rk. The idea then is that aggregates channel-wise information into real-valued contour embedding that can be used in downstream tasks. In the implementation, we use combination of the mean and maximum of absolute values similar to what is done in [35]. Specifically, we define the global pooling function : Rk by setting the i-th component of A(x) to be αn1 n1 (cid:88) j=0 xi(j) + (1 α) max j=0,...,n1 xi(j) for and where α [0, 1] is learned parameter. In practice, using only the mean results in faster and more stable convergence, whereas the magnitude-based maximum improved the overall validation scores."
        },
        {
            "title": "3 Experiments",
            "content": "We perform various experiments to assess the feasibility of RotaTouille. This section outlines the details of preprocessing and implementation, introduces the datasets, and presents the results. 3.1 Preprocessing and Implementation Details Data preprocessing. For the datasets that are based on images, we convert the images to contours by following three-step process: 1. Binarization. For grayscale images, we convert the images to binary images by applying thresholding, for example, by using Otsus method [32]. 2. Contour extraction. We use the OpenCV library [8] to extract contours from the binary images, selecting the one with the largest area in the case of multiple contours. 3. Equidistant resampling. We resample the contour to fixed number of points, equidistantly with respect to the Euclidean distance. See Fig. 1 for an illustration of the image-to-contour conversion process. Before passing the contours to the model, they are centered at their mean and rescaled by dividing by the standard deviation of the magnitudes, as this showed to improve convergence of the complexvalued models. The image datasets are standardized using statistics computed on the training dataset. Rotation Equivariant Deep Learning for Contours Figure 1: Image-to-contour conversion process. The input image is binarized, contours are extracted, and then resampled to fixed length with equidistant points. Multi-scale invariant features. The choice of optimal kernel size in one-dimensional CNNs is not obvious. One approach is to build an ensemble of models having different kernel sizes to learn features at multiple scales. This is viable option but requires larger number of parameters increasing the computational cost. In the classification experiments, we found that extracting invariant features at different depths in the network resulted in better performance without increasing the number of learnable parameters. See Fig. 2 for more details. Input Conv ModReLU Coarsening Global Pool Invariant features Figure 2: Multi-scale invariant feature extraction. After each convolutional block, we apply global pooling operation to obtain an invariant feature vector at different scales. The equivariant features are passed to the next convolutional block, and the final feature vector is concatenation of the invariant feature vectors from different depths. Equivariant features Contour re-centering. Before every convolutional layer and global pooling layer, we re-center the contour channel-wise as this showed improvements in training stability and performance of our method. This re-centering is an equivariant operation. Training, model selection and evaluation. We use the Adam optimizer in all experiments and use 10% of the training data for validation to choose the best model for evaluation on the separate test data. For the classification tasks, the test data is randomly rotated to test the robustness of the candidate models. In this case, we also apply random rotation to augment the training data for the non-invariant baseline models to make the comparison fair. Each experiment is repeated ten times with different seeds, and we report mean test scores together with standard deviations. For more details on hyperparameter values for the different model and dataset combinations, see Table 5 in the appendix. 3.2 Datasets Now, we describe the five datasets used in our experiments. We have three datasets for shape classification, one for shape reconstruction, and one for node-level regression. Fashion MNIST. The Fashion MNIST dataset [44] consists of 60, 000 training and 10, 000 test examples. The original dataset contains grayscale images of clothing items, each of size 28 28 pixels. We convert these images to contours by applying the image to contour conversion process described earlier. For the baseline CNN, we create two versions of the dataset: one with filled contours and one with unfilled contours. The filled contours are binary images where the contour is filled in, while the unfilled contours are binary images where only the contour is drawn. Hence, we discard all texture information in the images and only use the shape of the clothing items. We refer to this dataset as FashionMNIST. ModelNet. We create multi-channel dataset based on the ModelNet dataset introduced in [43] which contains 3D CAD models from various object categories. To create this dataset, we select the classes bottle, bowl, cone, and cup, chosen for their tendency to form single connected component in cross-section. For each 3D model, we uniformly sample point cloud and divide it Rotation Equivariant Deep Learning for Contours into four disjoint volumes along the second axis. From each volume, we generate 64 64 binary image by projecting the points onto an orthogonal plane. We then extract contours from the four image channels. This results in dataset of multi-channel inputs where each sample consists of stack of four contour slices, or binary images. The final ModelNet dataset comprises 644 training examples and 160 test examples. Rotated MNIST. The rotated MNIST dataset was introduced in [25] and is modified version of the MNIST dataset [26] where each image is randomly rotated by an angle uniformly sampled from [0, 2π). The dataset consists of 12, 000 training and 50, 000 test examples, each of size 28 28 pixels. The images are grayscale and contain handwritten digits from 0 to 9. We convert these images to contours using the image to contour conversion process described above. In addition to the contours, we also include simple texture feature based on the radial histogram (RH) of the image. The RH is rotation invariant feature that describes the distribution of pixel intensities in the image. We compute the RH by dividing the image into 14 radial bins and counting the number of pixels in each bin. We refer to this dataset as RotatedMNIST. Cell Shapes. We use subsample of the Profiling Cell Shape and Texture (PCST) benchmark dataset introduced [10] with 1000 shapes from each of the 9 categories corresponding to different combinations of eccentricity and boundary randomness. From this subsample, we extract contours and refer to this dataset as PCST. Curvature contours. We construct synthetic dataset for curvature regression. Given continuous contour γ : [0, 2π) where γ(t) = x(t) + iy(t) with and twice differentiable, the curvature κ is defined in terms of the first and second derivatives as κ = xy yx (x 2 + 2)3/2 . The curvature measures the local deviation of the curve from straight line. The Curvature dataset is generated as follows: First, we sample number of modes uniformly from {2, 3, 4, 5}. Then, for k, ay each mode = 1, . . . , m, we sample four coefficients ax independently from the uniform distribution on [1, 1], and define the contour γ = + iy where k, by k, bx x(t) = (cid:88) k=1 (ax cos(kt) + bx sin(kt)) and y(t) = (cid:88) k=1 (ay cos(kt) + by sin(kt)) . This construction ensures periodicity and smoothness while introducing controlled geometric variability through the random coefficients and number of modes. Lastly, we sample 100 points equidistant in arc length, and use the curvature in these points as the ground truth. See Fig. 3 for examples of generated curves. We discard contours with maximum curvature greater than 1000 to avoid extreme values. We generate 2000 contours for training and 1000 contours for test data. Figure 3: Five example contours from the Curvature dataset with log-curvature values colored. 3.3 Results Shape classification. We evaluate RotaTouille on the classification datasets FashionMNIST, ModelNet, and RotatedMNIST. The cross-entropy loss function is used for training and performance is measured using test accuracy (or test error). As first proof of concept, we compare RotaTouille with the baseline models on FashionMNIST. The baseline CNN model uses contour images, and we test both filled and unfilled contours. We also include baseline graph neural network based on the graph convolutional network (GCN) introduced in [24] where the graph is 7 Rotation Equivariant Deep Learning for Contours fixed cycle graph with the Cartesian coordinates of the contour points as its input node features. As an additional baseline, we implement the ContourCNN method using the optimal configuration reported in [15]. To evaluate RotaTouille on multichannel data, we also compare it with these baseline models on ModelNet. The accuracies for these two classification tasks are reported in Table 2, where RotaTouille slightly outperforms the baseline models on both datasets. The GCN model performs poorly on FashionMNIST, but is comparable to RotaTouille on the ModelNet dataset. Table 2: Comparison of accuracies on the FashionMNIST and ModelNet contour datasets between RotaTouille and the baseline models. Model Accuracy FashionMNIST ModelNet CNN (filled contours) CNN (unfilled contours) GCN ContourCNN 0.849 0.002 0.852 0.001 0.626 0.003 0.771 0.004 0.898 0.032 0.905 0.012 0.923 0.027 0.849 0."
        },
        {
            "title": "RotaTouille",
            "content": "0.867 0.002 0.934 0.016 The RotatedMNIST dataset was originally designed to evaluate model robustness to rotations and has been widely used as benchmark in prior work. We evaluate RotaTouille on this dataset to enable comparison with these existing approaches. As shown in Table 3, using only contours yields test error of 5.70%, which is not particularly competitive. However, augmenting our method with the simple radial histogram (RH) feature reduces the error to 3.72%, outperforming some of the other methods, including conventional image-based CNN used in [12]. While this does not match the accuracy of the most competitive methods, it demonstrates that contour-based representation, when combined with basic invariant feature, can achieve competitive performance on challenging rotation benchmark. Table 3: Comparison of test errors (in percent) on the RotatedMNIST dataset between existing methods and RotaTouille, both with and without the invariant radial histogram (RH) feature. Method SVM (RBF kernel)[25] TIRBM [39] RC-RBM+Gradients IHOF [34] CNN [12] P4CNN [12] H-Net [42] GCN ContourCNN Test error (%) 10.38 0.27 4.2 3.98 5.03 0.0020 2.28 0.0004 1.69 48.44 0.79 21.79 1.12 RotaTouille (contours only) RotaTouille (contours + RH feature) 5.70 0.13 3.72 0.11 Shape reconstruction. To demonstrate the flexibility of RotaTouille, we include an unsupervised learning task. Here, we train an autoencoder for contour reconstruction on the PCST dataset, and compare it to similar architecture on binary images. We use the mean square error (MSE) as the reconstruction loss function. Using 10% of the dataset, we evaluate performance by visual inspection and compute the intersection over union (IoU) between the original shape and its reconstruction. For the contour-based model, we first convert the contours to binary images to compute the IoU scores. See Fig. 4 for some visual examples of the reconstructions. Both models are able to reconstruct the overall shapes, but seem to struggle with reconstructing high frequency details. Based on the visual appearance of the reconstructions, the contour-based model is able to capture sharp corners better than the image-based model. This is particularly clear in example 2 and 3 of Fig. 4. Another advantage of the contour-based model is that it is guaranteed to produce valid contours, while the image-based model can produce invalid shapes with holes or multiple components. Moreover, it is not restricted to fixed pixel grid. In terms of IoU, both models scores 0.97 on the validation data. 8 Rotation Equivariant Deep Learning for Contours 0. 2. 4. 6. 8. 1. 3. 5. 7. 9. Figure 4: Ten randomly selected examples (two on each row) of original and reconstructed cell shapes from PCST validation data. From left to right in each example we have: the original image, the reconstruction from the image-based autoencoder, the original contour, and the reconstruction from the contour-based autoencoder. Curvature regression. We also consider node-level regression task in which the goal is to predict the curvature at each point of discrete contour from the Curvature dataset. For this task, we use the mean absolute error (MAE) as both the loss function and the evaluation metric. As baselines, we implement one-dimensional real-valued CNN operating on the Cartesian coordinates of the contour points, finite difference method that approximates curvature using central differences, and circle fitting method that estimates curvature by fitting circle to every three consecutive contour points and computing the curvature from the fitted radius. As shown in Table 4, RotaTouille achieves the lowest MAE, outperforming both the traditional approaches and the CNN baseline. Table 4: Comparison of mean absolute errors (MAE) on the curvature regression task. Model MAE Finite difference Circle fitting CNN (real-valued) RotaTouille 2.3271 0.4411 0.4647 0.0058 0.3968 0."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we introduced framework for deep learning on contours that is equivariant to rotations by defining an action of the group Zn S1 on contours and constructing corresponding complex-valued convolutional layers. We also developed non-linear activation functions and pooling operations that preserve equivariance, as well as global pooling layer to produce invariant features. While the performance gains are modest, RotaTouille provides easy-to-implement framework that explicitly encodes rotational symmetry on contour data. This makes it promising option for practical applications in shape analysis and other fields where rotation equivariance and/or invariance is essential."
        },
        {
            "title": "References",
            "content": "[1] Sadegh Abbasi, Farzin Mokhtarian, and Josef Kittler. Curvature scale space image in shape similarity retrieval. In: Multimedia systems 7 (1999), pp. 467476. 2 [2] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In: International conference on machine learning. PMLR. 2016, pp. 11201128. 4 [3] Habibollah Agh Atabay. Binary shape classification using convolutional neural networks. In: IIOAB 7.5 (2016), pp. 332336. 9 Rotation Equivariant Deep Learning for Contours [4] Joshua Bassey, Lijun Qian, and Xianfang Li. survey of complex-valued neural networks. In: arXiv preprint arXiv:2101.12249 (2021). 1 [5] Serge Belongie, Jitendra Malik, and Jan Puzicha. Shape context: new descriptor for shape matching and object recognition. In: Advances in neural information processing systems 13 (2000). 2 [6] Ameni Benkhlifa and Faouzi Ghorbel. novel 2D contour description generalized curvature scale space. In: Representations, Analysis and Recognition of Shape and Motion from Imaging Data: 6th International Workshop, RFMI 2016, Sidi Bou Said Village, Tunisia, October 27-29, 2016, Revised Selected Papers 6. Springer. 2017, pp. 129140. [7] Yossi Bokor Bleile, Patrice Koehl, and Florian Rehfeldt. Persistence diagrams as morphological signatures of cells: method to measure and compare cells within population. In: arXiv preprint arXiv:2310.20644 (2023). 1, 2 [8] G. Bradski. The OpenCV Library. In: Dr. Dobbs Journal of Software Tools (2000). 5 [9] Michael Bronstein et al. Geometric deep learning: Grids, groups, graphs, geodesics, and [10] gauges. In: arXiv preprint arXiv:2104.13478 (2021). 13 James Burgess et al. Orientation-invariant autoencoders learn robust representations for shape profiling of cells and organelles. In: Nature Communications 15.1 (2024), p. 1022. 1, 2, 7 [11] Caponetto et al. Deep learning algorithm for predictive maintenance of rotating machines through the analysis of the orbits shape of the rotor shaft. In: International Conference on Smart Innovation, Ergonomics and Applied Human Factors. Springer. 2019, pp. 245250. 1, 2 [12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In: International conference on machine learning. PMLR. 2016, pp. 29902999. 2, [13] Taco Cohen and Max Welling. Steerable CNNs. In: arXiv preprint arXiv:1612.08498 (2016). 2 [14] Elizabeth Cole et al. Analysis of deep complex-valued convolutional neural networks for MRI reconstruction and phase-focused applications. In: Magnetic resonance in medicine 86.2 (2021), pp. 10931109. 1 [15] Ahmad Droby and Jihad El-Sana. ContourCNN: convolutional neural network for contour data classification. In: 2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME). IEEE. 2021, pp. 17. 2, 8 Jingkun Gao et al. Enhanced radar imaging using complex-valued convolutional neural network. In: IEEE Geoscience and Remote Sensing Letters 16.1 (2018), pp. 3539. 1 Jan Gerken et al. Geometric deep learning and equivariant neural networks. In: Artificial Intelligence Review 56.12 (2023), pp. 1460514662. 1 [17] [16] [18] Nitzan Guberman. On complex valued convolutional neural networks. In: arXiv preprint arXiv:1602.09046 (2016). 2 [19] Lalit Gupta, Mohammad Sayeh, and Ravi Tammana. neural network approach to robust shape classification. In: Pattern Recognition 23.6 (1990), pp. 563568. 2 [20] Akira Hirose. Complex-Valued Neural Networks: Distinctive Features. In: Complex-Valued Neural Networks. Berlin, Heidelberg: Springer Berlin Heidelberg, 2012, pp. 1756. ISBN: 978-3-642-27632-3. DOI: 10.1007/978-3-642-27632-3_3. 4 [21] Leonardo Jablon et al. Diagnosis of rotating machine unbalance using machine learning algorithms on vibration orbital features. In: Journal of Vibration and Control 27.3-4 (2021), pp. 468476. [22] Haedong Jeong et al. Rotating machinery diagnostics using deep learning on orbit plot images. In: Procedia Manufacturing 5 (2016), pp. 11071118. 1, 2 [23] Qi Jia et al. rotation robust shape transformer for cartoon character recognition. In: The Visual Computer (Oct. 2023). ISSN: 1432-2315. DOI: 10.1007/s00371-023-03123-2. URL: https://doi.org/10.1007/s00371-023-03123-2. 2 [24] TN Kipf. Semi-Supervised Classification with Graph Convolutional Networks. In: arXiv preprint arXiv:1609.02907 (2016). 7 [25] Hugo Larochelle et al. An empirical evaluation of deep architectures on problems with many factors of variation. In: Proceedings of the 24th international conference on Machine learning. 2007, pp. 473480. 7, [26] Yann LeCun et al. Gradient-based learning applied to document recognition. In: Proceedings of the IEEE 86.11 (2002), pp. 22782324. 7 10 Rotation Equivariant Deep Learning for Contours [27] ChiYan Lee, Hideyuki Hasegawa, and Shangce Gao. Complex-Valued Neural Networks: Comprehensive Survey. In: IEEE/CAA Journal of Automatica Sinica 9.8 (2022), pp. 1406 1426. DOI: 10.1109/JAS.2022.105743. 1 [28] Makrem Mhedhbi, Slim Mhiri, and Faouzi Ghorbel. new deep convolutional neural network for 2D contour classification. In: (2022). 2 [29] Zoë Migicovsky et al. Rootstock effects on scion phenotypes in Chambourcin experimental vineyard. In: Horticulture Research 6.1 (May 2019), p. 64. ISSN: 2052-7276. DOI: 10.1038/ s41438-019-0146-2. 1, 2 [30] Farzin Mokhtarian, Sadegh Abbasi, and Josef Kittler. Efficient and robust retrieval by shape content through curvature scale space. In: Image databases and multi-media search. World Scientific, 1997, pp. 5158. 2 [31] Mallek Mziou-Sallami et al. DeepGCSS: robust and explainable contour classifier providing generalized curvature scale space features. In: Neural Computing and Applications 35.24 (Aug. 2023), pp. 1768917700. ISSN: 1433-3058. DOI: 10.1007/s00521-023-08639-1. URL: https://doi.org/10.1007/s00521-023-08639-1. 2 [32] Nobuyuki Otsu et al. threshold selection method from gray-level histograms. In: Automatica 11.285-296 (1975), pp. 2327. [33] Theresa Scarnati and Benjamin Lewis. Complex-valued neural networks for synthetic aperture radar image classification. In: 2021 IEEE Radar Conference (RadarConf21). IEEE. 2021, pp. 16. 1 [34] Uwe Schmidt and Stefan Roth. Learning rotation-aware features: From invariant priors to equivariant descriptors. In: 2012 IEEE conference on computer vision and pattern recognition. IEEE. 2012, pp. 20502057. 8 [35] Wei Shen et al. Bag of shape features with learned pooling function for shape recognition. In: Pattern Recognition Letters 106 (2018), pp. 3340. 2, 5 [36] Wei Shen et al. Shape recognition by bag of skeleton-associated contour parts. In: Pattern Recognition Letters 83 (2016), pp. 321329. [37] Wei Shen et al. Shape recognition by combining contour and skeleton into mid-level representation. In: Pattern Recognition: 6th Chinese Conference, CCPR 2014, Changsha, China, November 17-19, 2014. Proceedings, Part 6. Springer. 2014, pp. 391400. 2 [38] Wen Shen et al. 3D-rotation-equivariant quaternion neural networks. In: Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16. Springer. 2020, pp. 531547. 2 [39] Kihyuk Sohn and Honglak Lee. Learning invariant representations with local transformations. In: arXiv preprint arXiv:1206.6418 (2012). 8 [40] Patrick Virtue, Yu Stella, and Michael Lustig. Better than real: Complex-valued neural nets for MRI fingerprinting. In: 2017 IEEE international conference on image processing (ICIP). IEEE. 2017, pp. 39533957. 1, 4 [41] Maurice Weiler et al. Equivariant and coordinate independent convolutional networks. World Scientific Singapore, 2023, p. 110. [42] Daniel Worrall et al. Harmonic networks: Deep translation and rotation equivariance. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017, pp. 50285037. 4, 8 [43] Zhirong Wu et al. 3D shapenets: deep representation for volumetric shapes. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, pp. 1912 1920. 6 [44] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: Novel Image Dataset for Benchmarking Machine Learning Algorithms. Aug. 28, 2017. arXiv: cs.LG/1708.07747 [cs.LG]. 6 11 Rotation Equivariant Deep Learning for Contours"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Proofs Proposition A.1. If : 1 convolution operator. 1 is C-linear and commutes with cyclic shifts, then is circular Proof. Let δj : [n] be the Kronecker delta function defined by δj(q) = (cid:26)1 0 if = j, and otherwise. can be written uniquely as = (cid:80)n1 Every 1 for the cyclic shift operator defined by S(x)(q) = x(q 1), then we can express the cyclic shift equivariance as = . On basis elements, we have S(δj) = δj+1 so δj = Sj(δ0) for all Zn. Thus, we can rewrite = (cid:80)n1 j=0 x(j)δj. If we write : 1 j=0 x(j)Sj(δ0) and since is linear and commutes with S, we have that 1 (x) = x(j)Sj(δ0) = x(j)T (Sj(δ0)) = x(j)Sj(T (δ0)) n1 (cid:88) n1 (cid:88) n1 (cid:88) j= j=0 j=0 and hence (x)(q) = (cid:80)n1 convolution of and y. j=0 x(j)y(q j) where = (δ0) 1 n. This is precisely the circular A.2 Training Details Table 5: Hyperparameters for the training of the models. All models were trained using the Adam optimizer with the specified learning rate. Hyperparameter values were chosen based on performance on validation data. The number of (real) parameters is an estimate of the model complexity. Random rotations were applied to improve generalization for the non-equivariant and non-invariant models."
        },
        {
            "title": "Dataset",
            "content": "LR Batch Size Epochs Parameters Data Aug. FashionMNIST 0.01 CNN (2D) FashionMNIST 0.001 GCN FashionMNIST 0.001 ContourCNN FashionMNIST 0.0005 RotaTouille 0.01 CNN (2D) ModelNet 0.001 GCN ModelNet 0.001 ContourCNN ModelNet 0.0005 RotaTouille ModelNet RotatedMNIST 0.001 GCN RotatedMNIST 0.001 ContourCNN RotatedMNIST 0.0005 RotaTouille RotaTouille + RH RotatedMNIST 0.0005 0.001 CNN (2D) PCST 0.001 RotaTouille PCST 0.001 CNN (1D) Curvature 0.001 RotaTouille Curvature 128 128 128 128 16 16 16 16 128 128 128 128 32 32 32 32 200 200 200 200 200 200 200 200 200 200 200 200 200 200 100 100 294 666 74 826 42 874 65 089 1 143 012 75 594 42 964 64 747 74 826 42 874 65 089 66 881 5 315 4 378 34 033"
        },
        {
            "title": "Yes\nYes\nYes\nNo\nYes\nYes\nYes\nNo\nYes\nYes\nNo\nNo\nNo\nNo\nNo\nNo",
            "content": ""
        }
    ],
    "affiliations": [
        "University of Bergen"
    ]
}