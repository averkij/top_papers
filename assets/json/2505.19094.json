{
    "paper_title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards",
    "authors": [
        "Chuming Shen",
        "Wei Wei",
        "Xiaoye Qu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI ($\\textbf{S}patially$ $\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with $\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to $15.7\\%$ improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 4 9 0 9 1 . 5 0 5 2 : r SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards Chuming Shen1, Wei Wei1, Xiaoye Qu1, Yu Cheng2 1 School of Computer Science & Technology, Huazhong University of Science and Technology 2 The Chinese University of Hong Kong {scm, weiw, xiaoye}@hust.edu.cn chengyu@cse.cuhk.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text domain through stable reinforcement learning (RL). Recently, in the multimodal domain, works have begun to directly apply RL to generate R1-like free-form reasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks share an intrinsically different nature from textual tasks, which heavily rely on the understanding of the input image to solve the problem. Therefore, such free-form reasoning faces two critical limitations in the VQA task: (1) Extended reasoning chains diffuse visual focus away from task-critical regions, degrading answer accuracy. (2) Unverifiable intermediate steps amplify policy-gradient variance and computational costs overhead. To address these issues, in this paper, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), which decomposes VQA into three verifiable stages, including global image captioning, region localization, and answer prediction, each supplying explicit reward signals. Furthermore, we also introduce VQA-Verify, 12k dataset annotated with answer-aligned captions and bounding-boxes to facilitate training. Experiments demonstrate consistent performance improvements across seven VQA benchmarks, achieving up to 15.7% improvement in accuracy in accuracy compared to the R1-like baseline. Our analysis of the attention map confirms enhanced focus on critical regions, which brings improvements in accuracy. Our code is available at https://github.com/justairr/SATORI-R1."
        },
        {
            "title": "Introduction",
            "content": "The recent surge of interest in models such as DeepSeek-R1 [14, 48] has sparked renewed attention toward using reinforcement learning (RL) as direct training strategy for reasoning models [63, 33, 73, 51, 69]. Inspired by these advances, in the realm of multimodal tasks, particularly Visual Question Answering (VQA), prevailing approaches [22, 33, 44] draw inspiration from the RL paradigm widely adopted in language models. This approach encourages models to first generate verbose reasoning pattern, akin to free-form exploration, before producing final answer [14, 48, 56, 60, 52]. While such reflection can benefit multi-hop or abstract reasoning tasks by promoting systematic knowledge retrieval and decomposition, it reveals fundamental limitations when applied to standard VQA tasks. Specifically, there are two key limitations in applying R1-like reasoning patterns to standard VQA tasks: (1) Attention Dilution in Critical Regions: Free-form reasoning patterns tend to diffuse the models focus away from task-relevant visual areas. As result, the visual attention maps exhibit lower density over regions that are essential for answering the question. By comparing attention distributions across several reinforcement-learning training paradigms, we observe that when the Corresponding authors. Preprint. Under review. model generates long reasoning chains, it reallocates attention to irrelevant elements in the scene. This dispersion significantly degrades VQA performance. (2) Convergence Impediment [15, 77]: Unstructured reasoning paths not only multiply token consumption but also, in the absence of quantifiable intermediate supervisory verifiable signals, induce high variance in the policy-gradient estimates, thereby slowing convergence. Under standard RL configurations, each training example must undergo multiple rollouts to accurately evaluate an extended reasoning trajectory, which further inflates computational overhead. Consequently, scalable training becomes impractical in environments with limited resources. To bridge these gaps, we introduce SATORI (Spatially Anchored Task Optimization with ReInforcement Learning), new visual reinforcement learning paradigm that incorporates verifiable reasoning patterns. Previous works [47, 4] have demonstrated that pre-outputting an overall image caption and the critical region yields significant improvements in VQA tasks, and both of them are easy to verify. This motivates us to have the model first generate an image caption to capture the entire scene and establish global understanding, then produce key bounding-boxes to shift visual attention to regions relevant to the question, and finally formulate an answer based on this focused information. We employ image caption and bounding-box as verifiable reasoning patterns, followed by reinforcement learning to directly train the model. To facilitate the training of our above proposed paradigm, we introduce VQA-Verify, the first multimodal VQA dataset with both bounding-box and caption annotations. It comprises 12k samples from 17 benchmarks, structured hierarchically into 3 categories, Perception, Reasoning, and Multilingual, and 11 fine-grained task classes. Each entry includes an image, question, answer, bounding-box highlighting the answer cue, and concise caption describing the image. SATORI offers two key advantages: (1) Sharper Attention Alignment: Through analysis of the visual attention maps generated by MLLMs, our method focuses more on critical regions compared to typical R1-like reasoning patterns, thereby improving accuracy in visual question answering (VQA). (2) Verifiable Intermediate Rewards: During the reinforcement learning process, our newly designed reward provides smooth approximation of the VQA accuracy reward, reducing policygradient variance by 27%. We conduct evaluations across seven VQA benchmarks, demonstrating that SATORI achieves state-of-the-art performance among models with 3B parameters, improving MMBench [29] accuracy by an absolute 15.7% over the base model and surpassing comparable methods on mathematical reasoning tasks by 4.6 to 9.0 points. To summarize, our key contributions are: We analyze MLLM attention maps and show that free-form reasoning dilutes focus on answer-relevant regions, motivating more targeted intermediate steps. We propose three-step visual reasoning pattern and RL paradigm SATORI. By turning caption and localization into verifiable reward signals, our RL paradigm lowers policygradient variance by 27% and speeds up convergence. We release VQA-Verify, the first augmented dataset of 12k VQA samples with answerrelevant bounding-boxes and scene captions to enable explicit supervision for our rewards. Our method outperforms traditional R1-like free-form reasoning on four comprehensive benchmarks, achieving up to 15.7% improvement in accuracy."
        },
        {
            "title": "2 Background",
            "content": "2.1 MLLM Architecture and Visual Attention Multimodal Large Language Models (MLLMs) unify visual and textual reasoning through hybrid architecture. Given an input image RHW C, vision encoder (e.g., ViT [9]) partitions it into patches, linearly projected into visual tokens {vi}NI p2 . These tokens reside in the same latent space as text tokens {tj}NT j=1 from language models like Qwen [2] or Llama [12]. i=1 with NI = HW The fused sequence [v1, . . . , vNI ; t1, . . . , tNT ] is processed by transformer decoder layers using masked multi-head self-attention (MHSA). For each layer, the attention operation computes: Attention(Q, K, V) = softmax (cid:19) (cid:18) QK V, (1) 2 where Q, K, are projections of the input sequence. During auto-regressive answer generation, the query QA for each answer token attends to both visual and textual contexts through: (cid:20) = softmax (cid:19)(cid:21) (cid:18) QAK L,K RLKNAN + , (2) where and denote the number of layers and attention heads, respectively. To analyze visual grounding, we isolate attention weights over visual tokens by reshaping into spatial dimensions (h, w), then aggregate multi-head/layer responses: (cid:101)A = Normalize Rhw + . Al,k"
        },
        {
            "title": "1\nLK",
            "content": "(cid:88) l,k (3) 2.2 Group Relative Policy Optimization (GRPO) Group Relative Policy Optimization (GRPO) [48] is reinforcement learning algorithm that optimizes sequence-generating models without an explicit critic network. For each input q, the current policy πθold samples group of candidate outputs {o1, . . . , oG}. Each output oi receives reward ri = R(q, oi), and GRPO directly incorporates clipping and KL-regularization into its objective: JGRP O(θ) = E[q (Q), {oi}G (cid:88) oi (cid:88) (cid:110) (cid:104) 1 1 oi i= t=1 i=1 πθold (Oq)] min hi,t ˆAi,t, clip (hi,t, 1 ε, 1 + ε) ˆAi,t (cid:105) βDKL [πθπref ] (cid:111) , (4) where hi,t = πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) and ˆAi,t is the (possibly standardized) advantage at step t. This formulation blends the clipped importance-sampling term with KL-penalty to keep the updated policy close to the reference πref ."
        },
        {
            "title": "3 SATORI",
            "content": "In this section, we first analyze the visual attention maps of the MLLM, demonstrating that different reasoning patterns influence the models focus on critical regions and that spatial reasoning patterns enhance attention to key areas (Section 3.1). Next, we examine the impact of introducing verifiable reasoning patterns on gradient variance during the RL process (Section 3.2). Finally, we propose visual reinforcement learning paradigm that incorporates verifiable reasoning patterns (Section 3.3). 3.1 Spatial Reasoning Patterns Enhance Attention to Critical Regions Previous studies [75, 65] demonstrate that attention patterns in MLLM implicitly reflect visual regions critical to task, where localized attention spikes often correlate with image areas relevant to answer. Some studies have attempted to enhance these regions to improve model performance, indicating that attention to critical regions has significant impact on VQA performance [65]. However, we find that inference patterns based on free-form reasoning tend to weaken the models focus on critical regions. This motivates us to explore new forms of reasoning that can guide the model to more accurately attend to image regions relevant to the question, thereby improving VQA performance. To quantify the differences in attention distributions under three distinct reasoning paradigms, we randomly sampled 2,000 images from the OpenImages [23] dataset and applied the following inference patterns without any fine-tuning: Conventional VQA, free-form reasoning, and CaptionBBox-Answer. These three represent the inference patterns of the original model, the reasoningenhanced model, and our proposed method, respectively. As illustrated in Figure 1, we ensured fair comparison by swapping only the output pipeline and employing one-shot exemplar to steer the model toward each required format. For each generated answer token, we extract the visual attention weights from all layers and heads, and aggregate them into an grid to obtain the normalized spatial attention distribution A. More Figure 1: Left: The average accuracy and RAD of three different reasoning patterns. Middle: An example of different reasoning patterns for the same question. Right: visual example illustrates the differences in attention to key image regions across various inference modes. Using the same model Qwen2.5-VL-Instruct-3B with only the output patterns altered, we observe that compared to other inference approaches, our method enables the model to focus more effectively on the critical regions. The critical regions in the ground truth are marked with red bounding-boxes. details could be found at Appendix B. Experimental results in Figure 1 show that the attention under the Free-Form setting is more dispersed, whereas the Caption-BBox-Answer setting clearly focuses on regions relevant to the question. Our analysis reveals critical dependency on thinking paths: different reasoning strategies yield distinct attention distributions. As visualized in Figure 1, R1-like free-form reasoning (middle) produces scattered attention patterns across decoder layers, with less attention mass concentrated on critical regions. This phenomenon may be attributed to the fact that regular VQA tasks typically do not require complex chains of reasoning, in contrast to the success of free-form reasoning in more complex mathematical problems. This \"overthinking\" phenomenon allows the model to hallucinate irrelevant visual features, ultimately diverting focus from semantically salient areas. In contrast, spatial reasoning patterns (bottom) demonstrate layer-consistent attention alignment with human annotations. This misalignment motivates our quantification framework measuring Region Attention Density (RAD): RAD = (cid:80) (i,j)G (cid:101)Ai,j (cid:80)w j=1 (cid:101)Ai,j i=1 (cid:80)h (5) where is the set of ground truth boundingboxes. RAD measures the models attention to critical regions by calculating the concentration of the attention map within G. In Figure 2, free-form reasoning patterns exhibit degraded RAD performance due to dispersed attention, whereas our structured caption-bboxanswer paradigm maintains higher RAD values, with average scores of 0.2621 and 0.2664, respectively. The results also indicate positive correlation between RAD and accuracy. More details can be found in Appendix B. Figure 2: RAD and accuracy distributions for three different reasoning types. The light-shaded region represents the 95% confidence interval. Compared to free-form reasoning, spatially anchored reasoning patterns are also more verifiable and thus better suited as reward signals. 4 3.2 Gradient Variance Reduction via Verifiable Reasoning Patterns Figure 3: The overview of our proposed method. SATORI guides the model to capture the global information, then analyzes critical regions and finally produces an answer, providing verifiable rewards for step-by-step supervision. Previous studies [72, 48] have demonstrated that combining verifiable reasoning paths with reinforcement learning (RL) yields strong performance on tasks such as mathematical problem solving and logical inference. This success is largely attributed to the availability of well-structured, deterministic reasoning paradigms that allow for step-by-step supervision. In contrast, open-ended visual reasoning tasks such as VQA present significantly higher uncertainty: the reward signals are sparse, the answers are short, and intermediate reasoning steps are typically not explicitly supervised. These characteristics introduce substantial variance in the estimation of the policy gradient, which poses major challenge to effective learning [15, 77]. In particular, token-level policy gradient methods like GRPO rely on sampled trajectories to estimate gradients, where each trajectory receives global reward that is distributed uniformly across all tokens. Without verifiable intermediate signals, this global reward is highly variable and often lacks sufficient granularity to guide learning effectively. Motivated by this issue, we aim to reduce the variance of policy gradients by introducing reasoning patterns that are more stable and verifiable. Instead of relying solely on free-form text reasoning, which is difficult to evaluate and highly stochastic, we design our method to incorporate intermediate reasoning steps that can be evaluated through deterministic criteria. This strategy provides foundation for smoother gradient estimation, which we analyze in detail in Appendix D. 3.3 Spatially Anchored Task Optimization with Reinforcement Learning As stated in Section 3.1, we propose structured and verifiable reasoning pattern that aligns with the intrinsic requirements of VQA. Specifically, we replace the free-form reasoning with caption focusing on the overall image and bounding-box highlighting the key region. This structured supervision bridges the semantic gap between free-form reasoning and visual grounding requirements. SATORI (Spatially Anchored Task Optimization with ReInforcement Learning) guides the model to capture both the overall image context and the critical regions before answering the question, providing verifiable rewards for step-by-step supervision. Figure 3 clearly presents the information flow and reward allocation from Caption to BBox to Answer. VQA-Verify enables direct computation of two verifiable reward signals during RL training: Rcaption = 1 2 (BLEU-4smooth + ROUGE-LF1) , Rbbox = Union IoU(P, G) (6) where is the set of predicted boxes and is the set of ground-truth bound boxes. The caption reward Rcaption combines smoothed BLEU-4 (handling zero n-gram matches via additive smoothing) with ROUGE-L F1 (measuring the longest common subsequence overlap). Since there may be multiple bounding-boxes in the ground-truth, we define the Union IoU to compute the intersection over union of the combined bounding-boxes in the image. The detailed calculation process can be found in Algorithm 1. Similar to the R1-like reasoning training method, we also sample two types of reward signals during training: the accuracy reward Racc and the format reward Rformat. The accuracy reward Racc measures whether the generated answer matches the ground-truth, while the format 5 reward Rformat ensures that the output follows the expected format of <caption-BBox-answer>. Both reward signals take binary values of 0 or 1. Algorithm 1 Union IoU Reward Computation Require: Predicted boxes: = {Bp }Ng Ground-truth boxes: = {Bg Ensure: IoU score: Rbbox [0, 1] 2, yi 1, xi 2] 2, yj 2] j=1 where Bg = [xi 1, yj i=1 where Bp 1, yi 1, xj = [xj }Np (cid:40) Convert boxes to geometric polygons: 1, xi 1, xj Ppoly = (cid:83)Np Gpoly = (cid:83)Ng i=1 Rect(xi j=1 Rect(xj Rect(a, b, c, d): Axis-aligned rectangle defined by coordinates (a, b) and (c, d). 2, yi 2) 2, yj 2) 1, yi 1, yj Compute union regions: (cid:26)Up = Union(Ppoly) Ug = Union(Gpoly) Calculate intersection and union areas: (cid:26)A = Area(Up Ug) = Area(Up Ug) Compute final IoU with numerical stability: Rbbox = A+ϵ (ϵ = 106) Subsequently, we guide the model within the prompt to first reason about the image caption and the key region bounding-box, and optimize the model solely using the GRPO paradigm. Compared to SFT, which simply imitates annotated data, this training approach leverages policy gradients to encourage the model to explore better generation strategies. By employing independent reward functions for each subtask, the model receives explicit feedback to optimize final accuracy. Similar to the setup in GRPO, we also adopt the basic format reward and accuracy reward."
        },
        {
            "title": "4 VQA-Verify Dataset",
            "content": "To address the scarcity of explicit visual supervision in VQA training, we introduce VQA-Verify, an small size augmented dataset providing verifiable grounding signals for 12,000 samples across standard VQA benchmarks. Different from standard VQA datasets, VQA-Verify provides annotations for each sample in the form of (Image, Question, Caption, BBox, Answer). To the best of our knowledge, VQA-Verify is the first multimodal dataset that annotates bounding-box and image caption for the answers. Inspired by previous works [29, 5], VQA-Verify integrates 17 benchmark datasets through hierarchical framework designed to address diverse visualtextual understanding capabilities. At the highest level, the dataset spans three primary categories: Perception, Reasoning, and Multilingual tasks. Perception. This category focuses on foundational visual-textual recognition through two subcategories. Document Text Recognition is supported by SROIE [16], which specializes in scanned receipt OCR and key information extraction. Scene Text Recognition encompasses six datasets: TotalText [7] for multi-oriented and curved text, ICDAR 2013 [20] and ICDAR 2015 [21] as standard benchmarks for horizontal and scene text detection, CTW1500 [30] for curved text analysis, and COCO-Text [58] for text detection in complex scenes. Object Recognition and Detection integrates CUB-200 [59] for fine-grained bird classification and OpenImages [23] for large-scale object detection with bounding-boxes and visual relationships. Figure 4: Overview of VQA-Verify. VQAVerify is divided into 3 categories, 11 subtasks, and 17 benchmarks in total. Reasoning. This category targets advanced cognitive tasks through six subdomains. Scene Text-based VQA leverages TextVQA [50] for question answering requiring textual reasoning in images, while Text Description Generation uses TextCaps [49] to challenge models in generating context-aware captions that fuse text and visual elements. Document Understanding combines DocVQA [38] and DUDE [57] to evaluate multi-page document comprehension and layout-aware reasoning. Infographic Understanding employs InfographicVQA [39] to test joint analysis of graphical layouts, data visualizations, and textual content. General VQA integrates GQA [17] for balanced question-answering with scene graph support and Visual7W [82] for object-grounded multimodal QA. Lastly, Spatial and Relational Reasoning incorporates VSR [26] to assess spatial relation verification between objects. Multilingual. This category includes LSVT [54] for Chinese text detection in street-view scenarios and MLT [40] for script-agnostic text detection across diverse languages. This hierarchical integration provides unified framework for evaluating both fundamental perception skills and sophisticated reasoning abilities across monolingual and cross-lingual contexts, while maintaining alignment with real-world visual-textual challenges through its constituent datasets. The bounding-boxes in the dataset are derived from the works of Shao et al. [47] and Wang et al. [62]. We employed GPT-4o, one of the state-of-the-art models, for image captioning. Following this process, we conducted manual quality review and refinement, and further integrated the VQA-Verify dataset. more detailed description of the datasets is provided in Appendix C."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Our model is based on the Qwen2.5-VL-Instruct-3B backbone. We perform direct RL training using the framework introduced in [81] without any cold start. The training approach adopts the well-known GRPO-zero [48] method, with the reward function aligned with that of Section 3. For training data, we use the lightweight VQA-Verify dataset proposed earlier in Section 4. The model uses configuration of 256 28 28 as the min pixel setting and 512 28 28 as the max pixel setting. The evaluation is conducted under the same settings. We set GRPO-3B as the baseline, which uses the same dataset but does not incorporate BBox or Caption rewards. Its reasoning format follows the original R1-like free-form reasoning, namely, \"think first, then answer.\" More implementation details could be found in Appendix E. For evaluation, we primarily rely on several comprehensive benchmarks: MMBench [29], MMStar [5], MME [11] and OCRBench [31]. We also compare our method with the current state-of-the-art reasoning models on three mathematical datasets: MathVista [34], Math-V [61], and MathVerse [78]. Results are presented in Table 1, Table 2 and Figure 5. 5.2 Main Results As shown in Table 1, SATORI achieves substantial improvements across most sub-tasks. The overall average accuracy increases from 60.8% (original Qwen2.5-VL-Instruct-3B) to 76.5%, marking gain of 15.7 percentage points. Compared to the free-form reasoning approach, which achieves 64.6%, SATORI improves by 11.9 percentage points. closer look at individual metrics reveals that in the Fine-grained Perception (FP-S) sub-task Object Localization, SATORI boosts performance from 40.0% to 60.0%. Even in more challenging tasks such as Logical Reasoning (LR) and Relational Reasoning (RR), SATORI maintains leadfor instance, improving accuracy on Social Relation from 56.0% to 62.0%, and on Nature Relation from 56.8% to 58.1%. These results strongly demonstrate the effectiveness of our reasoning patterns in enhancing visionlanguage alignment and fine-grained understanding. In Table 2, we can see that our SATORI-3B consistently narrows the gap between smaller opensource reasoners and much larger closed-source systems. On MathVista, SATORI-3B attains 60.9%, improving by over 9 points compared to the next best 23B model (R1-VL-2B). Likewise, on the more challenging Math-V and MathVerse datasets, SATORI-3B outperforms R1-VL-2B by 4.6 and 6.0 points, respectively. On the MMStar benchmark, SATORI-3B achieves 55.9%, matching the performance of several larger 811B reasoning models. In Figure 5, SATORI consistently outperforms the original Qwen2.5-VL-Instruct-3B model as well as its free-form reasoning variant on the MME datasets Reasoning, Code Reasoning, and Commonsense Reasoning tasks, as well as on OCRBench, demonstrating the superiority of our approach. 7 Table 1: Comparison of SATORI with other MLLMs and methods in MMBench [29]. SATORI outperforms other open-source models, surpasses alternative reasoning-based MLLM approaches, and achieves competitive performance across most benchmarks. Specifically, LR denotes Logical Reasoning, AR denotes Attribute Reasoning, RR denotes Relation Reasoning, PPR denotes Physical Property Reasoning, SITU represents Structuralized Image-Text Understanding, FP-C represents Fine-grained Perception (Cross Instance), FP-S represents Fine-grained Perception (Single Instance), and CP refers to Coarse Perception. Results marked with are sourced from [10]. FP-S t o y b C t o n c i i o e FP-C i m t r o n e u t h i e i S O CP AR LR t e y a g e S m t a p g n s n c g o R t I t e r F P S RR t R i P t R t n a l o Model/Method Gemini-1.5 Pro 85.5 69.5 84.4 77.8 66.7 65.3 93.3 74.4 49.2 84.7 79.3 75.6 85.6 94.7 64.6 61.3 65.1 83.7 Avg. GPT-4V 73.5 36.2 61.2 93.3 44.0 46.7 78.7 56.7 37.9 81.9 76.1 94.4 88.9 96.1 53.2 65.3 56.0 68.5 33.3 64.8 65.4 69.2 74.6 Gemini-2.0 Flash 86.3 66.7 72.1 74.4 65.3 78.7 70.8 64.4 54.0 74.3 65.2 78.9 80.0 78.9 60.8 64.0 63.3 79.3 50.7 75.8 70.4 Llava-Next-8B 80.3 64.8 74.8 83.3 44.0 66.7 79.8 78.9 48.4 85.4 79.3 97.8 88.9 97.4 64.6 66.7 37.6 66.3 50.7 84.6 72.1 Llava-CoT-11B 81.2 62.9 89.1 92.2 62.7 86.5 74.4 46.8 83.3 75.0 88.9 90.0 98.7 67.1 66.7 49.5 79.3 56.0 82.4 74. 52 48 I Qwen2.5-VL-Ins-3B Original 78.2 40.0 66.7 70.0 22.0 29.4 56.7 81.7 21.4 89.6 59.7 76.7 83.3 96.1 41.5 42.0 45.9 66.1 40.0 88.7 60.8 GRPO-3B 76.9 52.9 94.9 75.0 38.0 27.5 86.7 81.7 39.3 56.2 83.9 43.3 86.7 98.0 34.0 42.0 56.8 56.5 56.0 85.5 64.6 SATORI-3B 83.3 60.0 97.0 91.7 46.0 66.7 90.0 88.3 38.1 93.8 82.3 91.7 86.7 98.0 52.8 54.0 58.1 80.6 62.0 91.9 76.5 Table 2: Comparison with other reasoning models on three mathematical datasets (MathVista [34], Math-V [61], MathVerse [78]) and MMStar [5]. The results indicate that our method also maintains competitive performance on mathematical problems. Method Closed-Source Model GPT-4o [18] Claude-3.5 Sonnet [1] 8-11B Reasoning Model MultiMath-7B [42] LLaVA-CoT-11B [68] LLaVA-Reasoner-8B [79] Insight-V-8B [8] 2-3B Reasoning Model Mulberry-2B [70] R1-VL-2B [76] Aquila-VL-2B [13] SATORI-3B MathVista Math-V MathVerse MMStar 63.8 67.7 50.0 54.8 50.6 49.8 51.7 52.1 59.0 60.9 30.3 38.0 - - - - - 17.1 18.4 21.7 39.4 - 26.9 - - - - 26.2 26.2 32.2 63.8 62.2 - 57.6 54.0 57. 51.3 49.8 54.9 55.9 5.3 Analysis Figure 5: Performance of different methods across various reasoning and OCR benchmarks. Specifically, MMER denotes the Reasoning subtask of MME [11], MMECode denotes Code Reasoning subtask and MMECR denotes Commonsense Reasoning subtask. Ablation Study on Reasoning Patterns. Table 3: Ablation results on reasoning patterns. Method To validate the effectiveness of each component in SATORI, we compare the models performance under different reward configurations: using BBox without Caption, using neither, and supervised fine-tuning. All experiments were conducted on the same VQA-Verify dataset introduced in Section 4, with hyperparameter settings consistent with those described in Appendix E. The only differences lie in the training strategies and the choice of reward signals. The results in Table 3 demonstrate that our method achieves the best performance when both BBox and Caption reward signals are present. Qwen2.5-VL-Ins-3B +BBox+SFT +BBox+Caption+SFT +Free Form Reasoning+RL +BBox+RL MMBench MMStar 60.8 58.9 65.2 64.6 71.0 48.0 49.7 50.5 50.4 54.1 SATORI 55.9 76.5 8 Are Captions Really Needed? In our method, employing image captions as the reward signal introduces additional reasoning overhead. To address concerns about the issues above, we conducted an ablation in which the model is trained with the BBoxAnswerCaption sequence but, at inference time, stops generation immediately after the answer token. We refer to this variant as SATORIReverse. Table 4 summarizes the comparison between the original SATORI (CaptionBBoxAnswer), SATORI-Reverse, and baseline using only the bounding-box reward (BBox-only RL). Table 4: Comparison of SATORI, SATORI-Reverse, and BBox-only RL paradigms. Method MMBench MMStar MMER Avg. Len. SATORI (CapBBoxAns) SATORI-Reverse (BBoxAnsCap) BBox-only RL 76.5 75.9 71.0 55.9 55.7 54.1 622.9 549.6 526.1 686.24 79.22 59. Although SATORI-Reverse incurs the same training overhead, since captions are still generated and scored during RL, the inference-time token consumption drops by roughly 88.46% compared to SATORI (79.22 vs. 686.24 length on average). More importantly, the accuracy of SATORI-Reverse decreases by less than 1% compared to SATORI on both MMBench and MMStar, confirming that delaying caption generation to the final step does not hurt model focus or performance. These results validate that the caption-based reward is not only helpful for grounding visual attention and reducing gradient variance but also flexible in deployment: by reversing the output order, one can retain nearly identical performance while substantially reducing inference costs. Variance Reduction Analysis. As shown in Figure 6a, SATORI exhibits substantially lower gradient variance compared to free-form reasoning baselines. The average variance during training drops from 0.025 to 0.018, representing 27% reduction. This suggests that verifiable intermediate rewards act as variance-reducing control signals, enabling more stable and efficient policy learning. Moreover, the gradient-norm curves in Figure 6b show that SATORI converges in fewer steps, confirming that variance reduction translates directly into faster training. (a) (b) Figure 6: a) Comparison of SATORI with the classic free-form reasoning approach reveals the variance in GRPO performance. b) Changes in gradient norm over training epochs. The results show that our method converges significantly faster than free-form reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present SATORI, staged optimization framework for visual reasoning tasks that integrates hierarchical intermediate supervision via three steps: caption generation, object localization, and final answer prediction. This design enables more verifiable and spatially grounded reasoning, and improves both learning efficiency and model robustness. To support this method, we construct VQA-Verify, dataset with 12k annotated examples, enabling reward decomposition for reinforcement learning without extra models or manual reward engineering. Extensive experiments across multiple benchmarks demonstrate consistent performance gains, highlighting the effectiveness of intermediate supervision signals in guiding multi-modal models. Overall, this work sheds light on how spatial anchoring and intermediate supervision can be effectively combined with reinforcement learning to enhance multi-modal reasoning."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [4] Soravit Changpinyo, Doron Kukliansy, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut. All you may need for vqa are image captions. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 19471963, 2022. [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [6] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [7] Chee-Kheng Chng, Chee Seng Chan, and Cheng-Lin Liu. Total-text: toward orientation robustness in scene text detection. International Journal on Document Analysis and Recognition (IJDAR), 23(1):3152, 2020. [8] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [10] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv. org/abs/2306.13394. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, and etc. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data, 2025. URL https://arxiv. org/abs/2410.18558. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane DwivediYu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. [16] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. [17] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [19] Leslie Pack Kaelbling, Michael Littman, and Andrew Moore. Reinforcement learning: survey. Journal of artificial intelligence research, 4:237285, 1996. [20] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In 2013 12th international conference on document analysis and recognition, pages 14841493. IEEE, 2013. [21] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, In 2015 13th international Shijian Lu, et al. conference on document analysis and recognition (ICDAR), pages 11561160. IEEE, 2015. Icdar 2015 competition on robust reading. [22] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 10, 2023. [23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. [24] Xiang Lan, Feng Wu, Kai He, Qinghao Zhao, Shenda Hong, and Mengling Feng. Gem: Empowering mllm for grounded ecg understanding with time series and images. arXiv preprint arXiv:2503.06073, 2025. [25] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023. [26] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. [28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [30] Yuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recognition, 90: 337345, 2019. 11 [31] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [32] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. [33] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [35] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. [36] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093, 2023. [37] Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):7791, 1952. [38] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [39] Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [40] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowdhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Icdar2019 robust reading challenge on multi-lingual scene text detection and recognitionrrc-mlt-2019. In 2019 International conference on document analysis and recognition (ICDAR), pages 15821587. IEEE, 2019. [41] OpenAI. Introducing openai o1, 2024. URL https://openai.com/o1/. [42] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. [43] Xiaoye Qu, Jiashuo Sun, Wei Wei, and Yu Cheng. Look, compare, decide: Alleviating hallucination in large vision-language models via multi-view multi-path reasoning. arXiv preprint arXiv:2408.17150, 2024. [44] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. [45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [47] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [49] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. [50] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [51] Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024. [52] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. [53] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. video-salmonn: Speech-enhanced audio-visual large language models. arXiv preprint arXiv:2406.15704, 2024. [54] Yipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, and Jingtuo Liu. Chinese street view text: Large-scale chinese text reading with partially supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90869095, 2019. [55] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [56] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [57] Jordy Van Landeghem, Rubèn Tito, Łukasz Borchmann, Michał Pietruszka, Pawel Joziak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, et al. Document understanding dataset and evaluation (dude). In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1952819540, 2023. [58] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. [59] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. [60] Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou. Drt-o1: Optimized deep reasoning translation via long chain-of-thought. arXiv e-prints, pages arXiv2412, 2024. [61] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [62] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the general value of evidence, and bilingual scene-text visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012610135, 2020. [63] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey, 2025. URL https://arxiv.org/abs/2503.12605. 13 [64] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279292, 1992. [65] Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Guannan Jiang, Xiaoshuai Sun, and Rongrong Ji. Controlmllm: Training-free visual prompt learning for multimodal large language models. Advances in Neural Information Processing Systems, 37: 4520645234, 2024. [66] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [67] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [68] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [69] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. [70] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [71] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [72] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. [73] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [74] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816, 2024. [75] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025. [76] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [77] Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models, 2025. URL https://arxiv. org/abs/2504.09696. [78] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [79] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [80] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 14 [81] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, et al. Swift: scalable lightweight infrastructure for fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2973329735, 2025. [82] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49955004, 2016."
        },
        {
            "title": "A Related Works",
            "content": "A.1 Enhancing Reasoning in MLLMs Multimodal Large Language Models (MLLMs) have rapidly advanced the state of visionlanguage understanding by integrating text, image, and other sensory inputs. Early efforts primarily tackled imagetext tasks, demonstrating the ability to generate descriptive captions and answer visual queries based on single-image prompts [28, 27, 43]. Subsequent research extended these models to video understanding [53, 6] and to more diverse modalities such as audio and point clouds [36, 66]. Domainspecific adaptations have further pushed the envelope: examples include specialized medical image interpretation [80, 25, 24] and structured document analysis [71, 32]. Building on the success of chain-of-thought prompting in pure-language settings [41], recent approaches seek to endow MLLMs with stronger inference capabilities via supervised fine-tuning on high-quality reasoning traces. Methods such as LLaVA-CoT generate structured intermediate reasoning stepssummary, description, analysis, conclusionusing powerful teacher model and then train the target MLLM on these examples [67]. Other works incorporate search techniques: Mulberry employs collective Monte Carlo Tree Search across multiple model instances to discover effective reasoning pathways, which are subsequently distilled into single model [70]. A.2 Reinforcement Learning for Structured Reasoning Reinforcement Learning (RL) provides principled approach for sequential decision-making, where agents optimize long-term return through trial-and-error interactions [19, 64]. In the context of large language models, RL with human feedback (RLHF) has been instrumental in aligning generation quality to human preferences, using algorithms like PPO [46] and DPO [45] [3]. More recently, RL has been adopted to improve reasoning: ReST-MCTS* introduces learned process reward model to evaluate intermediate reasoning steps [74], while other studies demonstrate that simple outcome-level rewardsassigning positive credit only to sequences that reach correct answersare sufficient to guide policy optimization [35, 14, 55]."
        },
        {
            "title": "B Details of Visual Attention Map Comparison",
            "content": "B.1 Implementation To ensure fair comparison, we employed the original Qwen2.5-VL-Instruct-3B model to analyze visual attention maps under three different reasoning patterns, without any additional fine-tuning. The experiments were conducted on 2,000 randomly selected samples from the OpenImages [23] dataset. Since the instruction-following capability of the 3B model is relatively limited, simply prompting it with reasoning pattern may not guarantee adherence. Therefore, we adopted one-shot example setting for comparative experiments. The prompts used in the experiments are as follows: Prompt for Visual Attention Map Comparison \"ConventionalVQA\": # Output Example Question: What is the capital city of France? Answer: Paris ------------------ \"Free_Form_Reasoning\": Output the thinking process in <think> and final answer in <answer> </answer> tags. # Output Example Question: What is the capital city of France? <think>France is country in Europe, and its capital city is Paris. </think> <answer>Paris</answer> ------------------ \"Caption_BBox_Answer\": First, provide an image caption describing the overall scene inside <caption> </caption>. Then, output the list of bounding-boxes in the format of [[x1,y1,x2,y2], ...] inside <bbox> </bbox>. Finally, give the final answer in <answer> </answer>. # Output Example Question: What is shown in the image? <caption>A group of people playing soccer on green field.</caption> <bbox>[[50,60,120,180], [200,80,260,190], [300,90,360,200]]</bbox> <answer>People are playing soccer.</answer> B.2 Visual Attention Map Computation in MLLMs Similar to the work of Zhang et al., when the model generates the n-th answer token during autoregressive decoding, we first extract the attention tensor across all layers and heads: = (cid:2)softmax(cid:0) QAK (cid:1)(cid:3) LK RLKNAN + , (7) where is the number of layers, is the number of heads per layer, NA is the number of answertoken queries, and is the total length of text and visual tokens. We then locate the start and end indices of the visual tokens in the input sequence, denoted vs_pos and ve_pos, and crop each layerhead attention to the visual embedding segment: (8) where HW is the flattened spatial length of the visual patches. This segment is reshaped into two-dimensional grid: n, vs_pos:ve_pos RNAHW = A(ℓ,k) A(ℓ,k) + , (9) with and denoting the number of patch rows and columns. To obtain unified attention distribution, we average over all answer queries and attention heads: reshape A(ℓ,k) Rhw + , A(ℓ,k) (cid:98)A = 1 NA NA(cid:88) (cid:88) n=1 k=1 A(ℓ,k) Rhw + , and normalize across the spatial dimensions to yield the final importance distribution: (cid:101)A = Normalizeh,w (cid:0) (cid:98)A(cid:1). (10) (11)"
        },
        {
            "title": "C Details of the Dataset",
            "content": "C.1 Caption Annotation As stated previously in Section 4, the bounding-boxes used in our dataset are based on the annotations provided by Shao et al. [47] and Wang et al. [62]. For image captioning, we utilized GPT-4o, cuttingedge model in the field. After generating captions, we carried out manual review and refinement to 16 ensure quality, and subsequently incorporated the VQA-Verify dataset into our work. The following prompt was used to generate caption annotations for the dataset."
        },
        {
            "title": "Prompt for Caption Annotation",
            "content": "<image> Describe this image in general. Examples of VQA-Verify are illustrated in Figure 7. Figure 7: Examples of VQA-Verify. C.2 Statics of VQA-Verify Table 5 summarizes the overall size and annotation density of VQA-Verify, as well as the composition and average grounding signal per source dataset. The first row reports the total number of training samples, the average number of bounding-boxes per sample, and the average caption length in words. Subsequent rows break down these metrics for each of the integrated benchmark datasets, illustrating variation in visual complexity (boxes) and descriptive detail (caption length). The estvqa [62] dataset integrates data from Text-Total [7], ICDAR2013 [20], ICDAR2015 [21], CTW1500 [30], COCOText [58], LSVT [54], and MLT [40]. It is worth noting that the bounding-box annotations provided by Wang et al. are defined by four points rather than the standard rectangular format. To ensure consistency, we converted them to the common [x1, y1, x2, y2] representation by computing the enclosing rectangle. C.3 Dataset Verification To ensure the quality of the automatically generated captions and bounding-boxes in VQA-Verify, we performed manual verification on sampled subset. Below we describe the verification procedure and summarize the results. We randomly sampled 1,500 instances (12.5% of the 12,000 total samples) and and conducted manual review of each. During this review, captions were evaluated to ensure they accurately reflected 17 Source Train cub-200 [59] docvqa [38] dude [57] estvqa [62] gqa [17] infographicsvqa [39] openimages [23] sroie [16] textcap [49] textvqa [50] v7w [82] vsr [26] # Samples Avg BBoxes / Sample Avg Caption Words 12,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1,000 1.34 1.00 2.17 1.00 1.00 1.00 2.31 1.00 1.00 1.84 1.72 1.00 1.00 112. 76.03 145.53 150.27 98.53 86.53 200.11 81.27 146.31 96.93 95.77 87.08 80.97 Table 5: Overall and per-source statistics for VQA-Verify. key image content, contained between 10 and 20 words, and were free from spelling or semantic errors; bounding-boxes were verified to tightly enclose the region relevant to the answer, with an Intersection-over-Union (IoU) of at least 0.8 relative to the original automatic annotation; and answers were checked for consistency with both the question and the image. Item Failures Failure Rate Common Issues Caption Quality Check bounding-box Accuracy Check Answer Consistency Check 27 95 9 1.8% Overly verbose, missing details 6.3% Box misalignment, incomplete area Mismatch with image/question 0.6% Table 6: Summary of manual verification results."
        },
        {
            "title": "D Supplementary Variance Analysis",
            "content": "To rigorously characterize the sources of policy gradient variance in GRPO, we leverage the Law of Total Variance to decouple the variance into intra-trajectory and inter-trajectory components. Let JGRPO denote the policy gradient estimator: θJGRPO Eq,oπθold (cid:34) 1 (cid:88) i= 1 oi oi (cid:88) t=1 hi,t θ log πθ(oi,t q, oi,<t) Ai,t (12) (cid:35) In this expression, we omit the KL-divergence penalty and clipping terms from PPO since they are deterministic functions of the gradient and do not contribute to sampling noise. The total variance of the estimator can be decomposed as: Var(JGRPO) = Eτ (cid:124) (cid:2)Var(JGRPO τ )(cid:3) (cid:123)(cid:122) (cid:125) Intra-Trajectory Variance + Varτ (cid:124) (cid:0)E[JGRPO τ ](cid:1) (cid:125) (cid:123)(cid:122) Inter-Trajectory Variance (13) Here, τ denotes sampled trajectory. The inter-trajectory term reflects variance due to differences in total rewards R(τ ) across trajectories. In GRPO, the advantage at each token is normalized by the group statistics: ri = ri mean(r) std(r) , = {ri}G i=1. Thus the trajectory-conditional expected gradient scales as 18 E(cid:2)JGRPO τ (cid:3) 1 o (cid:88) t=1 ht gt ri, gi,t = θ log πθ(oi,t q, oi,<t). (14) Since trajectories are sampled independently but token generation within each trajectory is autoregressive, we have: Varτ (cid:0)E[JGRPO τ ](cid:1) Var(cid:0)R(τ )(cid:1), R(τ ) = (cid:88) k=1 βk Rk. (15) This shows that reducing the variance of the total reward R(τ ) directly lowers the inter-trajectory variance and thus stabilizes gradient estimates. Our observation in experiments shows that even though the verifiable reasoning patterns reward and the accuracy reward are often positively correlated, the overall variance of the total reward still decreases. This phenomenon can be explained by the diversification effect [37]: when the total reward is constructed as weighted combination of multiple sub-rewards with weights {βk}, the overall variance can be reduced even if the components are positively correlated. According to the variance formula for weighted sum: Var(cid:0)R(τ )(cid:1) = β2 Var(Ri) + 2 (cid:88) (cid:88) i<j βiβjCov(Ri, Rj), the total variance depends not only on the variances of the individual components but also on their pairwise covariances. Even when Cov(Ri, Rj) > 0, the squared weights β2 < 1 for all dilute the contribution of each term, and as long as the correlation coefficients ρij < 1, the combined variance can be strictly smaller than the variance of single reward term. In the context of GRPO, the verifiable reasoning reward emphasizes consistency in intermediate reasoning steps (e.g., caption or bounding-box grounding), while the accuracy reward focuses on the final answer correctness. Although the two rewards are positively correlated, they capture complementary aspects of the task. By allocating appropriate weights, we retain useful signal from both while suppressing the noise associated with each individual component. This diversification not only reduces inter-trajectory variance but also leads to more stable gradient estimates and improved convergence behavior during training."
        },
        {
            "title": "E Implementation Details",
            "content": "Our experiments are conducted using Qwen2.5-VL-Instruct-3B. The models MAX_PIXELS is set to 512 28 28, and MIN_PIXELS to 256 28 28. Training is performed on four NVIDIA H100 Tensor Core GPUs. For the dataset, we utilize VQA-Verify (see Section 4), which enables the incorporation of intermediate caption and bounding-box reward signals during training. In the reward configuration, we assign equal weight to all reward signals. That is, all values of β are set to 1/k. We do not perform cold-start; instead, we train directly using GRPO. During training, we set max_length to 2048, and the GRPO group size to 16, corresponding to per-device batch size of 4. The sampling parameters are configured as follows: temperature = 1.0, top_k = 50, top_p = 0.9, and repetition_penalty = 1.0. We use learning rate of 1 106 and perform full fine-tuning for one epoch. The clip range is set to 0.2, and the KL divergence coefficient to 0.05. Throughout training, only the linear layers are updated, while the visual encoder remains frozen. During training, we randomly selected 1% of the training set as the validation set."
        },
        {
            "title": "F More Experiments",
            "content": "F.1 Detailed Results on MMStar We present detailed comparative results on the MMStar dataset in Figure 8. The results show that our method consistently outperforms the 3B-GRPO baselinewhich uses the same dataset and settings 19 but lacks verifiable signalsacross all categories. Notably, on more complex reasoning and math tasks, SATORI surpasses it by more than 5%. Figure 8: Comparison of model average performance on each category. SATORI outperforms the 3B-GRPO baseline, demonstrating the effectiveness of using verifiable reasoning patterns as rewards."
        },
        {
            "title": "G Discussion",
            "content": "G.1 Limitations Dependence on Base Model Instruction-Following and Grounding. While SATORI demonstrates significant benefits in zero-shot visual reasoning by leveraging nocold-start GRPO training scheme atop powerful base MLLMs, several limitations and avenues for future exploration remain. Our approach capitalizes on the strong instruction-following and visual grounding abilities of models such as Qwen2.5-VL. The ability to output bounding-boxes as intermediate rewards is direct consequence of this pre-training on grounding tasks. However, for weaker base models that lack such capabilities, purely zerocold-start strategy may struggle. In these cases, an initial supervised fine-tuning (SFT) phase with task-specific data would likely be necessary to bootstrap both instruction adherence and structured reasoning. Similarly, models not pre-trained on visual grounding would benefit from phase of visual-instruction tuningexposing them to paired image, instruction, and bounding-box annotationsbefore applying our reinforcement framework. Simplified Reasoning Paths in Standard VQA. Our experiments focus exclusively on standard VQA benchmarks, whose questions often admit relatively straightforward, single-step or shallow multi-step reasoning. Consequently, SATORIs three-stage pipelinecaption, bbox, answersuffices to guide the model effectively. Yet, real-world applications frequently demand deeper, branching reasoning, as exemplified by complex multi-modal mathematical problems. In such tasks, single bounding-box per question does not adequately localize the multiple sub-regions pertinent to each reasoning step. G.2 Future Works Towards Fine-Grained, Step-by-Step Verification. Our future work will explore more fine-grained verification framework in which, at each reasoning step, the model attends to and is rewarded on distinct image region. By leveraging dynamic visual attention maps rather than single bounding-box, we can decompose complex, multi-step problemsparticularly in mathematicsinto sequence of visually grounded subtasks. Adaptive Stage Decomposition and Model-Learned Structuring. Another promising direction is to move beyond fixed three-stage pipeline toward models that learn their own optimal decomposition of tasks. By introducing learnable stage controller, SATORI could adapt the number and nature of intermediate steps to each questions complexity. Meta-learning or conditional computation 20 techniques may enable the model to decide, at inference time, how many reasoning sub-tasks are required and what form each should take (e.g., object detection, relation extraction, sub-captioning)."
        }
    ],
    "affiliations": [
        "School of Computer Science & Technology, Huazhong University of Science and Technology",
        "The Chinese University of Hong Kong"
    ]
}