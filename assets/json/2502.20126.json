{
    "paper_title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "authors": [
        "Sotiris Anagnostidis",
        "Gregor Bachmann",
        "Yeongmin Kim",
        "Jonas Kohler",
        "Markos Georgopoulos",
        "Artsiom Sanakoyeu",
        "Yuming Du",
        "Albert Pumarola",
        "Ali Thabet",
        "Edgar Schönfeld"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their remarkable performance, modern Diffusion Transformers are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into \\emph{flexible} ones -- dubbed FlexiDiT -- allowing them to process inputs at varying compute budgets. We demonstrate how a single \\emph{flexible} model can generate images without any drop in quality, while reducing the required FLOPs by more than $40$\\% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to $75$\\% less compute without compromising performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 6 2 1 0 2 . 2 0 5 2 : r FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute Sotiris Anagnostidis,, Markos Georgopoulos Gregor Bachmann, Artsiom Sanakoyeu Yeongmin Kim, Yuming Du Jonas Kohler Albert Pumarola Ali Thabet Edgar Schonfeld Figure 1. We flexify DiTs and adjust the compute per diffusion step, generating high-quality samples with significantly less compute."
        },
        {
            "title": "Abstract",
            "content": "how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to 75% less compute without compromising performance. Despite their remarkable performance, modern Diffusion Transformers (DiTs) are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates fixed compute budget per denoising iteration and propose dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones dubbed FlexiDiT allowing them to process inputs at varying compute budgets. We demonstrate how single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than 40% compared to their static counterparts, for both class-conditioned and textconditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show Work done during an internship at Meta GenAI. Meta GenAI. ETH Zurich. KAIST. Correspondence to: sanagnos@ethz.ch 1. Introduction Diffusion models [87] have recently become the core building block of major improvements in image generation [10, 13, 24, 90, 107]. These models gradually denoise random sample xt, drawn typically from pnoise = (0, I), by iteratively calling neural network trained to reverse pre-defined noise corruption process pθ(xt1xt). This process enables the generation of samples from the desired distribution pdata.The remarkable performance of these models is closely tied to the amount of computational resources invested in them, as evidenced by established scaling laws [52]. The Transformer architecture [95] has proven to be highly scalable across various modalities, leading to its adoption in diffusion models, in the form of the recent In DiTs, the denoising proDiffusion Transformer [78]. Figure 2. Diffusion can be viewed as spectral autoregression [25]. Left: Diffusion and its effect on the spatial frequency of images. Right: To investigate the role of different frequency components in image generation, we apply low or high pass filter to single diffusion step update (while keeping all other updates unchanged). With all other sources of randomness fixed, we compare the generated samples with and without filtering using LPIPS [110], L2 distance of the pixels, SSIM [99] and DreamSim [32]. Notably, the influence of low and high pass filters varies depending on whether they are applied early or late in the denoising process. cess pθ(xt1xt) is parametrized using Transformer blocks instead of traditional convolutional layers. As popularized in Vision Transformers [26], (latent) images of dimension are divided into patches of size p, which serve as input tokens that are transformed via series of attention and feed-forward layers. The use of DiTs offers two key advantages: (i) unified architecture and input processing framework that facilitates multimodal applications and generalization to other domains, such as audio and video; and (ii) exceptional scalability due to their signal propagation characteristics and efficient training on modern hardware. The computational complexity of DiT with hidden dimension and depth is O(LN d2 +LN 2d), where corresponds to the total number of tokens1. Given steps of the diffusion process, function calls to the same monolithic DiT model are repeated for all steps, and the total amount of compute is thus uniformly allocated. However, image generation via diffusion exhibits distinct, non-uniform, and temporally with respect to the noise process varying characteristics. Consistent with intuition, prior work has observed that high-level image features tend to emerge at early generative diffusion steps (i.e., large ts). In contrast, later steps refine and progressively generate high-frequency and detailed visual features [14]. We illustrate further differences during the denoising generation in Fig. 2. Concisely, different denoising steps have profoundly different influence on high and low-level features of the resulting images. In theory, different denoising methods and models can be employed for each step t, i.e. one can use separate parameters θt to learn pθt(xt1xt). While this notion has been previously explored in the literature, prior works propose computationally intensive solutions, such as training 1For high-resolution image generation even when diffusion is performed in latent space the second term O(LN 2d) that corresponds to the attention operations, can quickly become the main bottleneck. separate expert denoisers for different ts [5] or using model cascades [91]. These approaches face two significant limitations: (i) they require multiple models to be managed during inference, increasing memory demands. This can quickly become major issue, especially as current trends favor the use of larger and larger models. Moreover, (ii) using separate models restricts the opportunity for knowledge sharing across steps. Although one can treat denoising steps independently, the denoising process itself retains certain shared characteristics across steps. This inherent smoothness implies that separate models would need to learn these shared properties individually. In this work, we address these challenges and instead propose to perform different denoising steps with varying levels of compute using single model. In summary, our contributions are the following: We present simple framework that flexifies DiT models, allowing them to convert samples into different sequences of tokens by adjusting the patch size. Processing samples as different sequences enables us to control the compute budget being used for each denoising step. By leveraging specific image characteristics at different denoising steps, we demonstrate that strategically allocating less compute to certain steps can yield significant computational savings (over 40%) without compromising the quality of generated samples for both classconditioned and text-conditioned image generation. We also show how denoising based on larger patch size can serve as more effective guidance signal. We illustrate the versatility of our framework by extending it to other modalities. For video generation, we achieve substantial computational savings (up to 75%) with no considerable drop in performance or conceptual differences in the generated samples. 2. Background Figure 3. Tokenizing images into patches. For simplicity, we limit the discussion here to images, and detail later changes due to different modalities. DiTs use Transformer encoder to process image patches as tokens. Hereafter, we refer as tokenization to the process of converting (latent) image into series of tokens and as de-tokenization to the opposite process, of transforming series of tokens back into an image. Given an image of size and chosen patch size2 p, an input image is cropped into non-overlapping patches of dimensions Rppcin, where cin denotes the number of channels of the input image. The total number of tokens is then equal to = (h/p) (w/p). This (flattened) sequence of patches is then projected using linear layer Membed with weights Rppcind and potential biases Rd. This tokenization process is equivalent to performing 2D convolution, where the kernel size and stride are both equal to p. The embedded tokens are then processed using Transformer encoder layers. The output tokens of dimension RN are projected back to the image space with linear de-embedding layer Mde-embed with weights Rdcoutpp and potential biases Rcoutpp. Here cout denotes the number of output channels, typically cout = 2cin if the prediction includes the variance, else cout = cin. DiTs adhere to the scaling properties of Transformers, as demonstrated in various other applications [4, 37, 41, 52, 109]. DiTs are an increasingly popular alternative to convolutional networks for denoising corrupted images during generation, i.e. modeling pθ(xt1xt). Diffusion defines two Markov chains, the forward and the backward process. During the forward process, Gaussian noise3 is added to samples of the real distribution x0 q(x0) [40]: q(x1:T x0) = (cid:89) t=1 q(xtxt1), where q(xtxt1) = (xt1; (cid:112)1 βtxt1, βtI). (1) In the backward process, samples are drawn from Gaussian noise distribution p(xT ) = pnoise = (0, I) and then gradually denoised, using Tweedies Formula [28]: 2In reality, different patch sizes can be considered along the height and the width dimensions. We will however refrain from doing that. 3Although we focus on Gaussian noise here, other corruptions apart from Gaussian noise have also been analysed [6, 20, 75]. pθ(xT :0) = p(xT ) 1 (cid:89) t=T pθ(xt1xt), where pθ(xt1xt) = (xt; µθ(xt, t), Σθ(xt, t)). (2) Typically, single model is used to model the prediction pθ(xt1xt) for every t. 3. Flexible Diffusion Transformers We flexify DiTs, via small architectural changes, that allow them to process images as different length sequences, by adjusting the patch size used in tokenization. Flexible tokenization of images has been utilized before for singlestep inference applications [9] and to accelerate training [3]. We instead propose to use different patch sizes at different steps in the denoising process of the same image. This is based on the following intuition: Early steps focus on low-frequency details, which can be performed with bigger patch sizes at the same quality. Changing the patch size directly affects the total number of tokens (recall that = (h/p) (w/p)) and thus the overall compute required for function evaluation. Hence, by leveraging bigger patch sizes only for early steps, we can reduce the overall generation time while maintaining both low and high-frequency details in the produced images. We obtain flexible model coined FlexiDiT by modifying and fine-tuning pre-trained DiT, which enables it to process and understand images with new patch sizes. In our experiments we focus on efficiency, so newly added patch sizes are always larger than the one of the pre-trained model, leading to fewer tokens and thus smaller computational footprint. The single FlexiDiT model can be instantiated in different modes depending on the selected patch size. We will refer to instances of the model that use the original and smaller patch size ppowerful as powerful, compared to using weak model with larger patch size pweak. To simplify the discussion, we largely ignore how additional conditioning may be applied for now, and instead refer to specific implementation details in the experiments section. DiTs as any Transformer can process sequences of any arbitrary length . Fundamentally, we just need to modify (i) how tokenization and de-tokenization are performed to ensure that the input and output representation space stay unchanged and (ii) ensure that the model can interpret these different length sequences. In the following, we discuss and outline two different ways that FlexiDiTs can be derived, based on whether the forward pass for the pre-trained model (we refer to this as the target model) is preserved exactly or not. In both cases, we emphasize that any additional fine-tuning and new parameters are minimal. Figure 4. Left: We flexify DiTs by allowing them to process images with more patch sizes, by changing the lightweight embedding and de-embedding layers. We showcase this for class-conditioned ImageNet model. Right: We plot the difference in predictions between weak and powerful model. For the first denoising steps, differences are small, and thus using the weak model there allows accelerated generation without performance degradation. In all our experiments, training is stable, and no special tricks are necessary, while the total compute for fine-tuning is less than 5% of the original pre-training compute. 3.1. Shared Parameters for all Sequences When the original training data is available, it becomes possible to fine-tune the pre-trained model while preserving its performance, along with its existing abilities, biases, and potential safety features. We demonstrate that this approach enables processing images with varying patch sizes by introducing only minimal additional trainable parameters, as shown in Fig. 4 (left). Below, we detail the specific components of the architecture that require adaptation. Tokenization: We introduce new embedding layer embed, for some underlying patch size with new weights flex embed Rppcind and biases bflex embed Rd. Then, wflex when instantiating FlexiDiT with patch size pcurrent, we project these weights with fixed projection matrix Qembed Rpcurrentpcurrentpp to the desired shape, and perform the 2D convolution with kernel size and stride equal to pcurrent pcurrent. We use as Qembed the pseudo-inverse of the bi-linear interpolation projection, as this leads to better norm preservation of the output [9]. We initialize the weights based on the pre-trained parameters and the pre-trained patch size as wflex 4 and bflex embed = bembed, preserving the functional form of the model for the pre-trained patch size. When adding positional encodings, we identify for each patch its pixel coordinates in the original image [15, 103]. We additionally introduce patch size embedding for each of the patch sizes used by the model, which we add to every token in the sequence, and patch size dependent layer-normalization layers. These layers help with signal propagation [36, 77, 104] by preserving the norms of the activations and let the model recover embed = Qembed wembed 4Here denotes the pseudo-inverse. All projection matrices multiply each channel separately. its expressivity [49, 100]. and biases bflex de-embed with weights wflex de-embed Rcoutpp De-tokenization: We similarly adapt the de-embedding For an underlying patch size p, we layer Mde-embed. define new layer flex de-embed Rdcoutpp . Depending on the current patch size pcurrent in the neural network evaluation, we project with fixed projection matrix Qde-embed Rpppcurrentpcurrent to wflex de-embedQde-embed and bflex de-embedQde-embed. Again, we use as Qde-embed the pseudoinverse of the bi-linear interpolation, now with flipped dimensions. We initialize the new parameters as wflex de-embed = . In wde-embedQde-embed total, less than 0.005 % of auxiliary parameters are introduced to attain FlexiDiT for the models tested in this case. de-embed = bde-embedQde-embed and bflex 3.2. Different LoRAs for each Sequence In practice, however, pre-training often requires extensive computational resources and may span multiple stages using various datasets, which may not always be accessiIn such cases, ble, even for models with open weights. fine-tuning can have unintended effects, potentially diminishing model capabilities [46] or compromising safety guarantees [80]. For such situations, where it is essential to preserve the original forward pass of pre-trained model, we demonstrate method that enables fine-tuning across different patch sizes with minimal additional compute and small supplementary dataset. Fig. 5 illustrates our approach. We perform similar changes to the model, but instead of training the DiT block parameters, we freeze the original weights and add trainable LoRAs [42]. These LoRAs are specialized for each new patch size and activated only when our FlexiDiT is instantiated with that. There are no LoRAs for the patch size of the pre-trained model. For the tokenization and de-tokenization layers, we simply add new layers Membed, Mde-embed for each new patch size. As before, Figure 5. We preserve the functional form of the target model for the pre-trained patch size and add new trainable parameters (LoRAs) for each additional patch size we want to fine-tune the model to operate with. We showcase this for text-to-image/video model that uses cross-attention for text conditioning. We find that freezing cross-attention layers without any additional LoRAs works the best. During inference, we can either keep the LoRAs unmerged (Inference with LoRAs) leading to slight FLOPs increase that depends on the LoRAs dimensions, or create different copies of the model for each patch size, by merging the LoRAs (Inference without LoRAs). The latter leads to additional memory requirements. FLOPs and parameter numbers on the right correspond to our flexible T2I Emu model. we use patch size embedding, but only for the new patch sizes. Note that functional preservation is imposed for the pre-trained patch size, i.e. inference using only the powerful model generates exactly the same samples. We finetune by using the predictions of the original model (powerful model) as labels to distill knowledge [38] for the new patch sizes, i.e. we train to minimize: Et,xtϵθ(xt1xt; ppowerful) ϵθ(xt1xt; pweak)2. We find that this leads to improved performance, faster convergence [18] and better alignment between predictions of different patch sizes, which can be important given potential discrepancies in the data used during pre-training and our fine-tuning. Here we use ϵθ to denote the models prediction parametrizing the distribution pθ. Note that ϵθ(xt1xt; ppowerful) has no trainable parameters. During inference, we have two options: (i) keep the new LoRA parameters unmerged, which incurs minor additional computational cost, or (ii) merge these weights into copy of the original model parameters, with some additional memory cost. If we keep the LoRA parameters of dimension dlora unmerged, the computational complexity of the corresponding linear layer with input and output (dinput, doutput) will change from dinputdoutput to (dinputdoutput + dinputdlora + dloradoutput). If we merge LoRAs, there is no computational overhead, but the new merged parameters need to be kept in memory. Depending on the model requirements and the available resources, one can choose between the two options. We note that compute overhead by keeping LoRAs unmerged is minimal, see also Fig. 5 (right). For all models tested, additional parameters in this case are less than 5% of the original model parameters. Further details and ablations can be found in App. C. 3.3. Inference Scheduler At every denoising step, we need to decide how to instantiate our FlexiDiT, i.e. which patch size to use. Following our intuition, we find that for early steps of the denoising process, weak and powerful models produce similar predictions, and thus using the weak model preserves quality while reducing computational complexity, as also seen in Fig. 4 (right). We therefore propose an inference scheduler that, starting from random noise pnoise, first denoises using weak model for the first Tweak steps and switches to the powerful model for the last Tpowerful = Tweak steps. By adjusting the steps performed by the weak model Tweak, we can adjust the amount of compute that we are saving. In practice, unless otherwise mentioned, we fine-tune models to process images with one additional patch size. We choose this patch size, corresponding to the weak model, as 2 larger than the one corresponding to the powerful model. Then, the sequence length corresponding to tokenizing with the new patch size is 4 smaller, and thus compute required for the powerful model is > 4 compared to the weak model. 3.4. Generation Guidance For conditional generation, classifier-free guidance (CFG) is typically employed [29, 39] to enhance sample quality. This entails performing two neural function evaluations (NFEs) (or one NFE with twice the batch size) to compute predictions with and without the conditioning ϵθ(xt1xt, c) and ϵθ(xt1xt, ). Sampling can c, i.e. Figure 6. Left: As the weak model is used more extensively during generation, compute benefits increase, but at the cost of some performance degradation. Middle: The optimal CFG scale varies depending on the extent to which the weak model is used. Each line corresponds to an inference scheduler that applies the weak model for different proportion of denoising steps. Right: Benefits from our inference scheduler are orthogonal to performing smaller overall number of diffusion steps. We plot FID for different overall number of steps and different number of weak steps Tweak, using in every case the DDPM scheduler. then take place as ϵθ(xt1xt, ) + scfg(ϵθ(xt1xt, c) ϵθ(xt1xt, )), where scfg is the guidance scale. Recent work [53] has shown that using smaller or less welltrained version of the model rather than an unconditional model can lead to better guidance signal [1, 83]. We adapt these findings in our setting, leading to better generation quality without the need to train or deploy additional models. For each denoising step, given patch size used for the conditional pcond and patch size used for guidance puncond, we compute: ϵθ(xt1xt, ; puncond) + scfg(ϵθ(xt1xt, c; pcond) ϵθ(xt1xt, ; puncond)), ϵθ(xt1xt, c; puncond) + scfg(ϵθ(xt1xt, c; pcond) ϵθ(xt1xt, c; puncond)), if pcond = puncond if pcond < puncond In this setup, we use the powerful model for the conditional prediction and leverage the weak models output as guidance. Unlike traditional approaches, our method applies guidance based on the conditional prediction from the weak model. Our guidance scheme requires performing inference using both the weak (for the unconditional) and the powerful (for the conditional) model, for some denoising steps. We show in Fig. 12 (appendix) how this can be efficiently implemented, making use of packing [23]. Depending on the guidance signal used, optimum generation quality can vary with respect to the guidance scale scfg. This will become more apparent in the following experiments. 4. Experiments For clarity, we present efficiency gains with respect to FLOPs and point to Section 4.4 for detailed analysis of the relationship between FLOPs and latency. 4.1. Class-Conditioned Image Generation able, we fine-tune pre-trained models using the same parameters for all sequences, without the use of LoRAs, as described in Sec. 3.1. During training, we randomly noise images according to Eq. (1), and learn to denoise using one of the available patch sizes. We primarily report FID and point to App. for more experiments and different metrics. In practice, we use pre-trained model with patch size of 2 (powerful), that we fine-tune to also process images with patch size of 4 (weak). Since we are fine-tuning the powerful model, we can also teach it how to correct specific mistakes made by the weak model, accumulated in the backward process during the first Tweak steps. We provide more details in App. B.1 on how to reduce this exposure bias [57, 76]. Unless otherwise mentioned, reported metrics are computed by generating images at resolution 256 256, using 250 steps of the DDPM scheduler [40, 78]. Compute gains. We generate images with our FlexiDiT and the proposed inference scheduler, varying the amount of compute by adjusting the number of initial denoising steps Tweak performed with the weak model. For each level of compute, we report the FID of the generated In general, performimages in Fig. 6 (left and middle). ing few steps with the weak model (60-100% of baseline compute) leads to no drop in performance. Saving even more compute is possible, albeit at the cost of minor drop in the quality of the generated images. When using only the powerful mode of our FlexiDiT, we get the same performance FID-50k = 2.25 as the pre-trained DiTXL/2 model FID-50k = 2.27. Thus, fine-tuning for more patch sizes does not reduce the capacity of the model with respect to the pre-trained one. We also show that other inference schedulers, such as starting with the powerful model and switching to the weak model, lead to worse results (appendix Fig. 19), validating our intuition. We fine-tune models (DiT-XL/2) on ImageNet, using the same setup as in [78]. Since training data is publicly availRelation between and Tweak. Naturally, the question arises whether doing fewer overall diffusion steps leads to Model (compute %) FID CLIP VQAScore T2I Transf. (100 %) T2I Transf. ( 86 %) T2I Transf. ( 72 %) T2I Transf. ( 58 %) Emu (100 %) Emu ( 84 %) Emu ( 69 %) Emu ( 53 %) 14.75 14.72 14.77 14.71 26.00 25.96 26.00 26.10 25.60 25.62 25.63 25.58 26.05 26.06 26.08 26.07 63.29 63.30 63.40 63.26 70.17 70.19 70.37 70. Steps (Powerful/Weak) Ours Baseline 50 (40,10) 50 (30,20) 50 (20,30) 50 (10,40) 42 (42,0) 34 (34,0) 26 (26,0) 18 (18,0) Win 33.5 35.5 35.5 43. Votes (in %) Tie 42.5 41.5 44.0 41.5 Lose 24.0 23.0 20.5 15.5 We asked 8 annotators to express preferences between pairs of images for given prompt. We generate images for 200 prompts and collect 3 different votes per each pair of images, collectively 2400 votes. Figure 7. Left: We plot FID vs CLIP score for images generated with different CFG scales using the T2I Transf. model (we refer to the appendix for results regarding our Emu model). The red line represents images generated with varying CFG scales using only the (powerful) target model. By employing our dynamic scheduler, we can match image quality in terms of both FID and text alignment while significantly reducing compute requirements. Middle: Our flexible models can match the baseline (for fixed pre-defined guidance scale scfg) across benchmarks, with significantly less compute. Right: Human study results show votes indicating win, tie, or loss for our method compared to baseline, which corresponds to running the pre-trained model (only the powerful model) for fewer steps. Comparisons are between Emu inference modes that require approximately equal FLOPs and time. the same efficiency improvements compared to performing more steps with the weak model. After applying the DDPM scheduler for different number of overall diffusion steps , we plot in Fig. 6 (right) the efficiency gains across them. Results indicate that gains from performing weak steps are orthogonal to performing fewer overall diffusion steps. In other words, more steps are required to achieve better image fidelity, but performing some of the initial steps with our weak model is sufficient to achieve the targeted fidelity. In App. B.3, we provide further insights on how the predictions of the weak model closely align with the ones of the powerful model. This similarity supports parameter sharing, which not only reduces resource requirements during inference but also enables faster convergence during finetuning. 4.2. Text-conditioned Image Generation The universality of Transformers and the holistic view of different input sources as sequences of tokens implies that the generalization of the architecture to more modalities and variety of different inputs is straightforward. This is also the case for DiTs, which have been already extended to accommodate text conditioning [7, 15], video generation [72] and speech synthesis [67]. We showcase how our framework can be applied out of the box to state-of-the-art text-toimage (T2I) model architectures. T2I DiTs have the same architecture as class-conditioned DiTs, with the exception that conditioning is imposed via cross-attention. We fine-tune T2I models, by introducing new parameters in the form of LoRAs. Specifically, we use DiT following PIXART [15] we refer to this as T2I Transf. generating 256 256 images and 1.7B DiT based on EMU [19] we refer to this as Emu generating 1024 1024 images. Implementation details are provided in App. B.5, in short, we fine-tune both low and high-resolution target models with pre-trained patch size of 2, to also support patch size of 4. For T2I Transf., we follow the inference scheduler protocol in [15], and use the DDPM solver for 100 steps. We point to the App. for results with different solvers and number of steps (namely 20 steps of DPM, 25 steps of SAsolver, and varying number of steps of the DDPM solver) showing that performance benefits are orthogonal to these choices. As before, we vary the percentage of denoising steps with the weak model for either the conditional or the guidance predictions. CLIP-vs-FID. We report FID and CLIP score alignment for captions from the MS COCO dataset with varying CFG scale values in Fig. 7 (left). As before, we notice that lower compute versions of our model require higher CFG scale to generate images that match similar FID vs CLIP score values of the baseline (full compute) model. We show that for fixed CFG scale of the baseline model we chose scfg = 4.5 for the T2I Transf. model as proposed in [15] and scfg = 6.0 for our Emu model we can match the attained performance for less than 60% of the original compute. Detailed scores are given in Fig. 7 (middle). Additional benchmarks. We perform additional evaluations on the final generated images. We follow [59] and present text alignment for visual question-answering (VQA). We generate images based on the captions of DrawBench, TIFA160, Pick-a-Pic, Winoground and report average VQA scores (we point to App. for detailed results). We compare the baseline model (same CFG scales as before), against dynamic inference with our flexible models. Consistent with our previous findings, results indicate that performing few denoising steps with our weak model generally generates high-quality images. We can again save up to 40% of compute without performance drop. For highresolution images generated from our Emu model, we also embark on human study, as shown in Fig. 7 (right). There we validate that human annotators prefer images from our dynamic scheduler instead of using similar compute, but Figure 8. Left: Samples from our flexible Video DiT model using 25.2% of compute compared to the pre-trained baseline. Right: We perform varying number of steps of the denoising process with weak model, using either our spatial or our temporal weak model. Both weak models lead to significant compute savings with little to no degradation in performance. this time uniformly allocated. 4.3. Text-Conditioned Video Generation Finally, we also explore other modalities, such as video generation. Text-to-video (T2V) generation typically follows the same setup as T2I [72, 79]. Given video of dimensions Rf hwcin , where is the temporal component corresponding to the number of frames, and chosen patch size (pf, ph, pw) for each of the dimensions, the input (latent) video is cropped into non-overlapping spatialtemporal patches of dimensions Rpfphpwcin . Patches are then embedded using now 3D convolutional layer into tokens, subsequently transformed by the DiT blocks where every token attends to every other token in the sequence, and finally projected back to patches with linear layer. We perform the same changes as for our T2I models in Sec. 3.2, adding LoRAs for new patch sizes. We initialize tokenization and de-tokenization layers as before, and when the temporal patch size pf is increased, we duplicate weights along the temporal dimension. We fine-tune 4.9B video model as the one in [79] referred to as Video DiT and train with distillation as before. We generate videos of resolution (f w) = (256, 384, 704) using 250 steps of the DDPM scheduler as in [79]. Diffusion is performed in latent space that down-samples each dimension , h, by 8. More dimensions of the latent space now offer more possibilities in terms of determining the characteristics of our weak model. For our experiments, we fine-tune pretrained model with patch size (pf, ph, pw) = (1, 2, 2) to also support patch size (2, 2, 2), we denote this weak model as temporal, and (1, 4, 4), denoted as spatial. We then use either one of these modes as weak model during inference. Both modes could also be used iteratively for the generation of single sample, which we leave for future work. By adjusting the number of steps performed with our weak model, we can again control the overall compute required per generated sample. We use VBench [45] to evaluate the quality of the generated videos and report results in Fig. 8. In this case, we can save up to 75% of compute without significant drop in performance. Recent training-free methods [51, 62, 113], while appealing due to their lack of training requirements, achieve significantly lower compute savings before performance begins to degrade noticeably. This demonstrates that allowing the model to learn optimized compute allocation is more effective than relying on predefined rules for inference efficiency. Additional comparisons to previous work are provided in the appendix. 4.4. FLOPs vs Latency High-resolution image/video generation is predominantly compute-bound. To verify, we propagate sequences of different lengths through fixed size DiT and measure performance FLOPs and latency on NVIDIA H100 SXM5 with batch size of 2, simulating inference with CFG5. In Fig. 9 we show that the weak Emu and Video DiT models are also compute-bound, for the setup (generated image/video resolution) that we presented in the paper. Indeed, for T2V, FLOPs utilization is higher for our weak models, due to inefficiencies of the self-attention operation for large sequence lengths when using the powerful model. This effect can be expected to be even more predominant for multi-GPU inference. Consequently, latency benefits are even higher than FLOPs benefits presented so far. 5. Conclusion We have demonstrated how regular DiTs can be converted into flexible ones, that can process samples with different patch sizes after minimal fine-tuning. Adjusting the compute for some denoising steps in the diffusion process readily allows accelerating inference without compromising. Notably, the efficiency benefits of our approach are independent of the chosen solver or the number of denoising steps. We have displayed how our approach is generic and can be straightforwardly applied to class-conditioned image 5In all cases, we compile using torch.compile with fullgraph=True and mode = reduce-overhead. and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. Advances in Neural Information Processing Systems, 36, 2024. 3 [7] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 7 [9] Lucas Beyer, Pavel [8] Lucas Beyer, Xiaohua Zhai, Amelie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: good teacher is patient and consistent. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1092510934, 2022. 14 Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, and Filip Pavetic. Flexivit: One model for all patch sizes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1449614506, 2023. 3, 4, 14, [10] BlackForestlabs AI. https : / / blackforestlabs.ai/#get-flux, 2024. [Online; accessed 17-Oct-2024]. 1 Flux. [11] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 15 [12] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. ToarXiv preprint ken merging: Your vit but arXiv:2210.09461, 2022. 14 faster. [13] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. [14] Angela Castillo, Jonas Kohler, Juan Perez, Juan Pablo Perez, Albert Pumarola, Bernard Ghanem, Pablo Arbelaez, and Ali Thabet. Adaptive guidance: Training-free accelarXiv preprint eration of conditional diffusion models. arXiv:2312.12487, 2023. 2, 21 [15] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 4, 7, 22, 26 [16] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 14 [17] Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing diffusion models by asynchronous denoising. arXiv preprint arXiv:2406.06911, 2024. 17 Figure 9. GPU utilization for one denoising step, when propagating sequences with different overall number of tokens, corresponding to different patch sizes (pf, ph, pw). Our T2I model has no temporal dimension, but we overload notation and set pf = 1. For simplicity, we use DiT of similar configuration (width and depth) for both the T2I and T2V reported in this plot numbers, but results generalize across model shapes. generation, low and high-resolution text-conditioned image generation, and text-conditioned video generation. Looking ahead, we anticipate further applications of our flexible DiT framework across various modalities, such as audio and 3D modeling. As computational resources become increasingly in demand, developing efficient and adaptable models like ours will be crucial for enabling generative capabilities that are of high-quality, but also more scalable."
        },
        {
            "title": "References",
            "content": "[1] Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, and Seungryong Kim. Self-rectifying diffusion sampling with perturbed-attention guidance, 2024. 6 [2] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Advances in Neural Information Processing Systems, 36:6520265223, 2023. 14 [3] Sotiris Anagnostidis, Gregor Bachmann, Imanol Schlag, and Thomas Hofmann. Navigating scaling laws: Compute optimality in adaptive model training. In Forty-first International Conference on Machine Learning, 2024. 3, 26 [4] Gregor Bachmann, Sotiris Anagnostidis, and Thomas Hofmann. Scaling mlps: tale of inductive bias. Advances in Neural Information Processing Systems, 36, 2024. 3 [5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2, 14 [6] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, [18] Jang Hyun Cho and Bharath Hariharan. On the efficacy of In Proceedings of the IEEE/CVF knowledge distillation. international conference on computer vision, pages 4794 4802, 2019. 5 [19] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 7 [20] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros Dimakis, and Peyman Milanfar. Soft difarXiv fusion: Score matching for general corruptions. preprint arXiv:2209.05442, 2022. 3 [21] Giannis Daras, Yuval Dagan, Alex Dimakis, and Constantinos Daskalakis. Consistent diffusion models: Mitigating sampling drift by learning to be consistent. Advances in Neural Information Processing Systems, 36, 2024. 15 [22] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. 27 [23] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36, 2024. 6 [24] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 22 [25] Sander Dieleman. Diffusion is spectral autoregression, 2024. 2 [26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, [27] Stephane dAscoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving vision transformers with soft convolutional inductive biases. In International conference on machine learning, pages 22862296. PMLR, 2021. 17 [28] Bradley Efron. Tweedies formula and selection bias. Journal of the American Statistical Association, 106(496): 16021614, 2011. 3 [29] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 5 [30] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph FeichtenIn Proceedings of hofer. Multiscale vision transformers. the IEEE/CVF international conference on computer vision, pages 68246835, 2021. 14 [31] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models, 2023. 14 [32] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 2 [33] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alexander Smola. kernel twosample test. The Journal of Machine Learning Research, 13(1):723773, 2012. 15 [34] Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose Alvarez, Jan Kautz, and Pavlo Molchanov. Fastervit: Fast vision transformers with hierarchical attention. arXiv preprint arXiv:2306.06189, 2023. 14 [35] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. In European Conference on Computer Vision, pages 3755. Springer, 2025. 15 [36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 10261034, 2015. [37] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws arXiv preprint for autoregressive generative modeling. arXiv:2010.14701, 2020. 3 [38] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 5 [39] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 6, 15 [41] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. 3, 25 [42] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. 4 [43] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023. [44] Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, and Tieniu Tan. Vision transformer with super token sampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2269022699, 2023. 14 [45] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 8, 27 [46] Adam Ibrahim, Benjamin Therien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothee Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models, 2024. 4 [47] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh. Transformer fusion with optimal transport. arXiv preprint arXiv:2310.05719, 2023. 14 [48] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and Sidak Pal Singh. Transformer fusion with optimal transport. In The Twelfth International Conference on Learning Representations, 2024. 14 [49] Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 4 [50] Bowen Jing, Gabriele Corso, Renato Berlinghieri, and Tommi Jaakkola. Subspace diffusion generative models. In European Conference on Computer Vision, pages 274289. Springer, 2022. 14 [51] Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Michael Ryoo, and Tian Xie. Adaptive caching for faster video generation with diffusion transformers. arXiv preprint arXiv:2411.02397, 2024. 8, 15 [52] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1, [53] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 6 [54] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Pagoda: Progressive growing of one-step generator from low-resolution diffusion teacher. arXiv preprint arXiv:2405.14822, 2024. 14 [55] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 24 [56] Jonas Kohler, Albert Pumarola, Edgar Schonfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with backward distillation. arXiv preprint arXiv:2405.05224, 2024. 15 [57] Mingxiao Li, Tingyu Qu, Ruicong Yao, Wei Sun, and Marie-Francine Moens. Alleviating exposure bias in diffusion models through sampling with shifted time steps. arXiv preprint arXiv:2305.15583, 2023. 6, 15 [58] Yangming Li and Mihaela van der Schaar. On error propagation of diffusion models. In The Twelfth International Conference on Learning Representations, 2023. [59] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with imageto-text generation. arXiv preprint arXiv:2404.01291, 2024. 7 [60] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with imageto-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2025. 23 [61] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 15 [62] Haozhe Liu, Wentian Zhang, Jinheng Xie, Francesco Faccio, Mengmeng Xu, Tao Xiang, Mike Zheng Shou, JuanManuel Perez-Rua, and Jurgen Schmidhuber. Faster diffusion via temporal attention decomposition. arXiv e-prints, pages arXiv2404, 2024. 8, 15 [63] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. 15 [64] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token prunarXiv preprint ing by fully exploiting self-attention. arXiv:2209.13802, 2022. 14 [65] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [66] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 14 [67] Zhijun Liu, Shuai Wang, Sho Inoue, Qibing Bai, and Haizhou Li. Autoregressive diffusion transformer for textarXiv preprint arXiv:2406.05551, to-speech synthesis. 2024. 7 [68] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35: 57755787, 2022. 22 [69] Chenyang Lu, Daan de Geus, and Gijs Dubbelman. Content-aware token sharing for efficient semantic segmenIn Proceedings of the tation with vision transformers. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2363123640, 2023. 14 [70] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024. 20 [71] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1576215772, 2024. 14, [72] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 7, 8 [73] Zhiyuan Ma, Yuzhu Zhang, Guoli Jia, Liangliang Zhao, Yichao Ma, Mingjie Ma, Gaofeng Liu, Kaiyan Zhang, Jianjun Li, and Bowen Zhou. Efficient diffusion models: comprehensive survey from principles to practices. arXiv preprint arXiv:2410.11795, 2024. 15 [74] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 15 [75] Eliya Nachmani, Robin San Roman, and Lior Wolf. arXiv preprint Denoising diffusion gamma models. arXiv:2110.05948, 2021. 3 [76] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. arXiv preprint arXiv:2308.15321, 2023. 6, [77] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:2719827211, 2022. 4 [78] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 1, 6, 17, 26 [79] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 8, 15 [80] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to!, 2023. 4 [81] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transAdformers see like convolutional neural networks? information processing systems, 34: vances in neural 1211612128, 2021. [82] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34: 1393713949, 2021. 14 [83] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann M. Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models, 2024. 6 [84] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 24 [85] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. 16 [86] Shuwei Shi, Wenbo Li, Yuechen Zhang, Jingwen He, Biao Gong, and Yinqiang Zheng. Resmaster: Mastering highresolution image generation via structural and fine-grained guidance. arXiv preprint arXiv:2406.16476, 2024. 14 [87] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. 1 [88] Yang Song and Prafulla Dhariwal. niques for training consistency models. arXiv:2310.14189, 2023. 16 Improved techarXiv preprint [89] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya arXiv preprint Consistency models. Sutskever. arXiv:2303.01469, 2023. 15, 16, 18 [90] Stability AI. Stablediffusion3. https://stability. ai/news/stable-diffusion-3, 2024. [Online; accessed 17-Oct-2024]. 1 [91] Stability AI. Introducing stable cascade, 2024. 2 [92] Vadim Sushko, Edgar Schonfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna Khoreva. You only need adversarial supervision for semantic image synthesis. arXiv preprint arXiv:2012.04781, 2020. 17 [93] Gerald Tesauro et al. Temporal difference learning and tdgammon. Communications of the ACM, 38(3):5868, 1995. 15 [94] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for In Proceedings of the visio-linguistic compositionality. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. [95] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1 [96] Dimitri Von Rutte, Sotiris Anagnostidis, Gregor Bachmann, and Thomas Hofmann. language models guide through In International Conference on Machine latent space. Learning, pages 4965549687. PMLR, 2024. 17 [97] Jing Wang, Ao Ma, Jiasong Feng, Dawei Leng, Yuhui Yin, and Xiaodan Liang. Qihoo-t2x: An efficiency-focused diffusion transformer via proxy tokens for text-to-any-task. arXiv preprint arXiv:2409.04005, 2024. 14 [98] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, pages 13981402. Ieee, 2003. 17 [99] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE transactions on image proto structural similarity. cessing, 13(4):600612, 2004. 2 [112] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and Yang You. Dynamic diffusion transformer. arXiv preprint arXiv:2410.03456, 2024. 14 [113] Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024. 8, 14, 15, 17 [114] Mingjian Zhu, Yehui Tang, and Kai Han. Vision transformer pruning. arXiv preprint arXiv:2104.08500, 2021. [100] Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62116220, 2024. 4, 14, 17 [101] Xinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, and Xinghao Chen. Ppt: Token pruning and pooling for efficient arXiv preprint arXiv:2310.01812, vision transformers. 2023. 14 [102] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar, and Ross Girshick. Early convolutions help transformers see better. Advances in neural information processing systems, 34:3039230400, 2021. 14, 17 [103] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Unlocking transferability of large diffusion models via simIn Proceedings of the ple parameter-efficient fine-tuning. IEEE/CVF International Conference on Computer Vision, pages 42304239, 2023. 4 [104] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transIn International Conference on Maformer architecture. chine Learning, pages 1052410533. PMLR, 2020. 4 [105] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian, Ziming Liu, and Tommi Jaakkola. Restart sampling for improving generative processes. Advances in Neural Information Processing Systems, 36:7680676838, 2023. 27 [106] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 22 [107] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [108] Zhihang Yuan, Pu Lu, Hanling Zhang, Xuefei Ning, Linfeng Zhang, Tianchen Zhao, Shengen Yan, Guohao Dai, and Yu Wang. Ditfastattn: Attention compression for diffusion transformer models. arXiv preprint arXiv:2406.08552, 2024. 14, 17 [109] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1210412113, 2022. [110] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2, 17 [111] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. 14 A. Detailed Related Work Efficiency in Vision Transformers [26] falls primarily into two broad categories: reducing the computation per token or the number of tokens overall. Reducing the amount of computation per token. Reducing the amount of computation per token can be achieved by reducing the model size when training via distillation [8] or pruning the network after training [47, 114]. These methods again though typically work with static inference protocol. As in [112], we compare in Fig. 10 our class-conditioned ImageNet FlexiDiT model, against popular based pruning techniques, based on Diff pruning [31], Taylor, magnitude or random pruning [48]. As one can see, our dynamic scheduler outperforms these baselines. We note that our method can also be Figure 10. We compare our dynamic scheduler with more baselines. applied in conjunction with pruning techniques to achieve even higher efficiency gains, offering potential orthogonal benefits. In concurrent work, [112] propose to adjust, in dynamic way, the model during inference in different steps. In our work, we do not train separate models for each target FLOPs ratio like they do, meaning that we can train single model and decide how many FLOPs we want to invest during inference. This makes our approach more versatile. Additionally, our training is very stable, and no specific training tricks are required to converge successfully. That is how we were able to extend experiments to high-resolution image and video generation, achieving significantly better speed-ups than the ones they reported. We also outperform their results when the compute budget is very small. Nonetheless, this work could inspire adaptive per-sample schedulers, that could open new future directions. Given the large computational requirements of the attention operation, many methods nowadays focus on that to reduce the overhead imposed. These methods commonly use some form of hierarchical attention [34, 66], skip (usually the first) attention layers altogether [102], or reduce the number of the attended keys [16, 30, 108], by commonly aggregating keys in spatial neighborhood or applying some form of windowed attention. Reducing number of tokens. Our method primarily falls within the second category of reducing the overall number of tokens. Previous work here, typically relied on filtering [64, 82, 101], merging [12, 44, 69] or dropping [2]. Although merging works well for applications that eventually lead to some pooling operations (like classification tasks or for the task of creating an image-specific embedding), it works significantly less well for applications that require dense (token-level) predictions, where some un-merging operation has to be defined. In other concurrent work, [97] reduces the number of representative tokens to calculate the attention over. Our approach resembles most [9], where vision Transformers are trained to handle inputs with varying patch resolution. By applying less compute for some steps, we can reduce computational complexity significantly, without drop in performance. Image generation. In the context of image generation, diffusion has been largely established as the method for attaining state-of-the-art results. There have been previous works that try to take advantage of potential correlations between successive denoising step predictions [86], by either caching intermediate results in the activations [71, 100], or in the attention [113]. Caching has the advantage of training-free method. Nonetheless, potential benefits are lower. Similar to our work, [5] use different experts for different denoising steps. Instead of using different experts that require separate training and separate deployment, we show how single model can be easily formed into flexible one that can be instantiated in different modes, with each of its modes corresponding essentially to different expert. Similar in-spirit approaches have been proposed that rely on the smaller compute requirements for lower resolution image generation [54, 111]. [50] also adapt the computation per step, by projecting into smaller subspaces. We instead, keep the dimension of the latent space and the characteristics of it the same across diffusion steps. Orthogonal gains to our approach are also possible through methods such as guidance distillation [56, 74] and consistency models [89]. Our approach is also largely agnostic to the diffusion process and can be applied out of the box for flow matching methods [61]. We point the interested reader to [73] for survey for further efficiency in diffusion models. Finally, compared to other established techniques [35, 63] we do not fundamentally change the architecture, which allows us to apply our framework effortlessly for numerous pre-trained models across different modalities. Video generation. Our approach can be easily extended for video generation, and in principle for the generation of any modality where some inductive bias (spatial, temporal, etc) is employed in the diffusion (latent) space. In video generation, typically, latent video tokens are processed in parallel [11, 65, 79]. Training-free methods [51, 62, 113] have been proposed in this case to accelerate video generation. Benefits with training-free methods are nonetheless minimal before performance degradation kicks in (see Table 1 in [51], where one can typically save less than 30%). An interesting direction for future work involves adapting the inference scheduler, i.e. what patch size we are using for each denoising step, based on the requirements of each sample. It is natural to assume that when generating more static videos, increasing the temporal patch size, and thus decreasing the amount of compute along the temporal dimension, will result in smaller drops in performance. The same holds for the spatial patch sizes when generating images or videos that require less high-frequency details. B. Additional Experiments and Details We provide additional experiments, complementary to the main text. B.1. Exposure Bias and Accumulation of Error Figure 11. Left: We use maximum mean discrepancy to estimate the distribution mismatch between pθ(xtxT :t+1) and q(xtx0). Right: The proposed bootstrapped loss. During training, we perform few denoising steps with weak model followed by few denoising steps with powerful model (reminiscent of the scheduler during inference) and apply distribution matching loss on the resulting samples. Inference with diffusion suffers from exposure bias, due to the discrepancy of the input distribution during training and inference [21, 57, 58, 76]. Briefly, models are trained to denoise images sampled from the q(xtx0) distribution [40]. Inference on the other hand is characterized by repeated model evaluations pθ(xtxt+1) and any distribution mismatch between pθ(xtxT :t+1) and q(xtx0) accumulates, as also shown in Fig. 11 (left). The error at each iteration depends on the model, with perfect model resulting in 0 error and thus no error accumulated. In our case, the accumulation of error is exacerbated by the characteristics of our model, where weak models could lead to higher, but also specific in nature, kinds of errors. Training with the standard denoising objective, where real samples are randomly noised for some t, does not make the more powerful model aware of the nature of the mistakes made by the weak model, rendering it unable to potentially correct them. We propose to mitigate this issue by introducing bootstrapped distribution matching loss [93], as illustrated in Fig. 11 (right). The loss is applied in patch size-dependent manner, according to the desired inference protocol (from weak to powerful model calls during inference). Given natural images x0, x0 q(x0), we sample two time points t1 > t2 and corrupt the images with noise xtarget t2 q(xt2 x0). We then apply chain of denoising steps ϵθ(xpred t1 t1 q(xt1x0), xpred ; p) for (t1, t2] and patch size p. Ultimately, we wish for the distributions of xtarget to match, for which we employ the maximum mean discrepancy (MMD) [33]. To let the powerful model learn and potentially correct mistakes of the weak model and to simulate how our inference patch size scheduler works, we perform the first of these denoising steps with the weak model, followed by i=1 where p1 < p2 < < pk and number of denoising steps with the powerful model. Given set of patch sizes {pi}k and xpred t1 t1xpred j=i+1 sj, t1 + (cid:80)k denoising steps to perform with each s1, s2, . . . , sk, where (cid:80)k j=1 sj = t2 t1, we denoise with given patch size pi for all ts in (t1 + (cid:80)k j=i sj]. An illustration of this process can also be seen in Fig. 11 (right). When sampling time step t1, we bias our sampling similar to [85]. The proposed distribution matching loss, inspired by the notion of consistency [88, 89], provides principled way to correct the errors accumulated during inference. We note that we are optimizing simple distribution matching loss, instead of over-optimizing according to desired downstream metrics (namely FID), thus not violating Goodharts law6. Different distribution matching losses (including discriminator-based losses) can also be used. Note that we only correct this exposure bias for the class-conditioned image generation experiments, as this is the only case where we fine-tune the powerful pre-trained model. B.2. Inference with Packing Figure 12. Different approaches can be employed to perform forward passes with CFG when the conditional (C) and unconditional (UC) predictions use different patch sizes. Here, each row corresponds to sequence of tokens propagated through the DiT, and each bracket corresponds to batch of sequences for single NFE. Generally Approach 2 leads to the smallest amount of FLOPs, but for batch size 1, inference can be memory bound for low-resolution image generation. Approach 4 mostly leads to the smallest latency, as long as the number of generated images is larger than 4, i.e. the ratio of the sequence lengths between the powerful and the weak model. On the right, we plot FLOPs and Latency from the four different approaches of performing inference, for different number of generated images. Batch size plays role here (class-conditioned image generation experiments) as generated images are of lower resolution, namely 256 256, and thus sequence lengths through the Transformer are smaller. Normalized FLOPs are determined based on Approach 2 and normalized latency based on Approach 3. We use torch.compile with fullgraph=True and mode = reduce-overhead. We provide more details in Fig. 12, on how to perform inference with CFG when the conditional and unconditional predictions employ different patch size. We show this for our class-conditioned model, but results easily generalize for all our FlexiDiT models. Performing CFG entails NFEs with double the batch size (or 2 distinct NFEs), for the conditional and unconditional input, respectively. Performing the conditional and unconditional calls with different patch sizes leads to propagating sequences of different lengths through the DiT. Depending on how these sequences are packed together, and for lack of hardware-specific implementation of masked attention, more FLOPs can be traded for better latency. Our weak model additionally leads to memory benefits, which can be traded for bigger batch size when serving the model. Notice that current state-of-the-art image generation models in practice require much longer sequences compared to the 256 256 images generated here (see also Section 4.4) and so generation is compute-bound even when generating with batch size equal to 1. 6Goodharts law states that: When measure becomes target, it ceases to be good measure. B.3. What does the Model Learn? Transformers are composed of series of channel mixing components feed-forward layers with shared weight applied to all tokens in the sequence and token mixing components attention applied to tokens in the sequence. By coercing the model to learn the denoising objective when applied to images processed with different patch sizes, we are enforcing inductive bias in its weights and helping it better understand global structures in the image [27, 81, 102]. We test this hypothesis and evaluate what the model is learning in the following ways in Fig. 13. (left) We visualize using t-SNE, centered kernel alignment (CKA) between feature maps across layers when performing NFEs with different patch sizes. Activations across layers exhibit similar transformations [96], except the early layers, where features are lower level, i.e. more patch specific. (right) We visualize the JensenShannon divergence (JSD) between attention maps (interpolated to the same image space) when performing NFEs with different patch sizes. We compare using our FlexiDiT model with different patch sizes (Flexible) versus using two static models trained with different patch sizes (namely DiT-XL/2 and trained from scratch DiT-XL/4). Our flexible model showcases lower JSD, demonstrating better knowledge transfer between the different patch sizes. We believe that this transfer of knowledge is crucial to (1) confirm that parameter sharing across patch sizes is valid and (2) ensure that fine-tuning can be fast and sample efficient. Figure 13. Interpretability of the model activations and attention scores, when propagating samples tokenized with different patch sizes. B.4. Class-Conditioned Image Generation For class-conditioned experiments on the main text we focused on the DiT-XL/2 [78] and FID as Additional metrics. metric. Here, we report more metrics apart from FID, namely Inception Score (IS), sFID, and Precision/Recall. Results are presented in Fig. 14 for our flexible DiT-XL/2 model. We remind that for class-conditioned models, we fine-tune models using our distribution matching loss. As result, the powerful model that we get after fine-tuning is different to the pretrained checkpoints we start from. To verify that our weak model does not lead to less diverse samples, we embark on small experimental study to guarantee the diversity of generated images. We follow [92] and generate images from the same label map. We then calculate pairwise similarity/distance between these images and average across all similarities/distances and all label maps. We use MS-SSIM [98], LPIPS [110] and plot results in Fig. 15. Results indicate very similar values in terms of the diversity of the generated images. We also provide some sample images demonstrating diversity from the baseline model in Fig. 16 and our tuned model in Fig. 17. Note that the diversity of the generated images is in general high and not affected much by using the weak model for more of the initial denoising steps. Caching distance. In the main text, we have shown how weak and powerful models generate more similar predictions during the early steps of the denoising process. Previous papers to accelerate diffusion have largely relied on caching [17, 71, 100, 108, 113] previous activations, by taking advantage of the similarity in activations between successive steps. For completeness, we also plot the caching distance between activations of the same layer between successive generation steps in Fig. 18. In this paper, we do not employ caching but focus on an orthogonal approach. We advocate that all steps are important for high-quality image generation, as demonstrated by our experiments on reducing the overall number of generation steps. Figure 14. More metrics for our FlexiDiT based on the DiT-XL/2 model for class-conditioned generation on ImageNet. We plot (a) FID (b) sFID, (c) inception score, (d) precision, and (e) recall when generating 50, 000 samples with 250 steps of the DDPM schedule for various values of the CFG scales. Red lines correspond to the values that lead to the optimum FID scores for each compute level. Figure 15. Average distance/similarity of images generated from the same label map. Both metrics take values between 0 and 1. Instead of completely skipping steps, we simply invest less compute for them, and let the model decide how to allocate this compute. Additional schedulers. Based on the results on activation distance between successive denoising steps  (Fig. 18)  , one could argue that first denoising steps are also important and thus better inference scheduler would deploy the powerful model for these as well. In practice, we found no benefit from deploying scheduler that works like that. Notice though how activation distance is high for the first denoising steps only for some of the layers. We additionally experimented with dynamic schedulers that choose the patch size of each denoising step based on the activation distance of different layers between successive denoising steps. We did not find additional potential benefits. In this paper, we are training single model that can denoise images with any patch size for any denoising step. Given fixed desired inference scheduler i.e. if we know exactly which ts to run with the powerful and the weak model , one can train model specifically based on that, leading to undoubtedly better quality images for the same compute. Similar techniques are regularly applied in consistency models [89]. Finally, we compare our scheduler performing the first Tweak denoising steps with weak model versus the opposite scheduler, i.e. performing the last Tweak denoising steps with weak model. Results in Fig. 19 indicate that, as expected, using the weak model in the last diffusion steps is suboptimal, leading to loss in fine-grained details. We also provide qualitative examples of how these different schedulers affect image quality in Fig. 20. Figure 16. Sample images generated with the baseline DiT-XL/2 for the ImageNet category Brambling. Figure 17. Sample images generated with our flexible DiT-XL model when performing inference using only the powerful model, for the ImageNet category Brambling. More results on CFG. In the main text, we presented results on performing inference with different CFG scales and different invocations to our weak model for the unconditional and conditional part. The 4 generated curves in Fig. 6 (middle) correspond to performing our scheduler as 250/250, 130/130, 70/70, and 30/0 where x/y means using the powerful model for the last denoising steps for the conditional and denoising steps for the unconditional part. When performing CFG, we Figure 18. We plot average distance (L2-norm) between activation of different layers during successive steps of the denoising process of the DiT-XL/2 model. Different layers exhibit different characteristics. Similar observations have been made in [70]. Figure 19. We compare our scheduler versus different scheduler that uses the weak model for the last denoising steps when generating class-conditioned images. Points correspond to the minimum concerning CFG scale FID values. use the update rule as presented in the main text (cid:40) ϵθ(xt1xt, ; puncond) + scfg1(ϵθ(xt1xt, c; pcond) ϵθ(xt1xt, ; puncond)), ϵθ(xt1xt, c; puncond) + scfg2(ϵθ(xt1xt, c; pcond) ϵθ(xt1xt, c; puncond)), if pcond = puncond if pcond < puncond . This guidance scheme seeks to reduce errors made by the powerful model, enhancing potential differences in predictions of the corresponding weak model, when the two models disagree, indicating the general direction towards higher-quality samples. In practice, different values of scfg1 and scfg2 lead to the best results. We find that the rule (1scfg1)/(1scfg2) = 2.5, Figure 20. We compare our scheduler versus different scheduler that uses the weak model for the last denoising steps when generating class-conditioned images. Using the weak model for the last denoising steps leads to images with lower image fidelity. works consistently across experiments. Although we fix the value of the CFG scale during inference, different combinations are likely to lead to higher quality images as demonstrated by previous work [14], which we leave for future exploration. We point out that our scheduler is very stable in terms of performance attained for similar compute. For instance, performing inference with 70/70 scheduler or 90/50 scheduler, which both require the same overall compute, produces FID results of 2.64 and 2.65 respectively. Finally, we present detailed experiments on the effect of the CFG scale for different levels of compute and more metrics in Fig. 21. Figure 21. Effect of CFG scale on the generated images from our class-conditioned FlexiDiT model. We plot (a) FID, (b) sFID, (c) inception score, (d) precision, and (e) recall when generation 50, 000 samples with 250 steps of the DDPM scheduler. B.5. Text-to-Image Experiments Generally, T2I generation is performed for fixed target CFG scale. For our experiments we choose scfg = 4.5 for the T2I Transf. model, as this is the value used in [15] and scfg = 6.0 for the Emu model, as for these values we observed the best quality images. In general, we can match with our dynamic inference other target values of the CFG scale. One simply needs to adjust the used CFG scale for the dynamic inference accordingly. We follow the evaluation protocol of PIXART-α [15] and perform inference using the same solvers as they do, namely iDDPM [24] for 100 steps, DPM solver [68] for 20 steps, and SA solver [106] for 25 steps. In the main text Fig. 7 (left) we presented results for the iDDPM solver. We present results for all the schedulers with the settings used in PIXART-α in Fig. 22, 23 and 24. For all the schedulers, there are settings where we reach the Pareto front of FID vs CLIP score of the baseline model with lot less required compute. Figure 22. FID vs CLIP score using iDDPM for 100 steps for the T2I Transf. model. Figure 23. FID vs CLIP score using the DPM-solver for 20 steps for the T2I Transf. model. Figure 24. FID vs CLIP score using the SA-solver for 25 steps for the T2I Transf. model. To better characterize the effect of reducing compute, i.e. heavier use of the weak model, we also present more detailed results for the DDPM scheduler in Fig. 25. Less compute-heavy inference schedulers, often produce images with smaller possible maximum CLIP scores (for large CFG guidance scales scfg). In practice, as large CFG scale values lead to larger values of FID, these are less preferred. In every case, our weak models can max the FID vs CLIP score tradeoff of the base model for the default configuration used, i.e. scfg = 4.5. Figure 25. FID vs CLIP score using DDPM for 100 steps for the T2I Transf. model, for different levels of compute. On the right, we present results separately for each compute level. the baseline We also provide results on using smaller overall number of steps with the DDPM solver in Fig. 27. set To generate {1.0, 1.125, 1.25, 1.375, 1.5, 1.625, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5}. The same values are used when sampling with our flexible models. Finally, we also provide FID vs CLIP for the Emu model in Fig. 26. In this case, we take CFG scales scfg from the set {1.0, 1.5, 2.0, 2.5, 3.0, 4.5, 6.0, 7.5, 8.0, 9.0}. We use captions from the training set of MS COCO to generate images. Neither of the models was trained on images from this dataset. sample 10, 000 samples using CFG scale curves, we from the scfg Figure 26. FID vs CLIP score using DDIM for 50 steps for the Emu model. VQA results. VQA scores are calculated by querying visual-question-answering model to produce an alignment score by computing the probability of Yes answer to simple Does this figure show {text}?. question. To calculate this score, we use the clip-flant5-xxl model from huggingface7 as suggested in [60]. We provide more detailed results on the VQA benchmark in Tables 1 and 2. More specifically, we provide per dataset VQA scores, along with the CFG scale scfg used to generate the images for each case . As we can see, using the weak model requires bigger CFG scale to reach the same level of optimality (calculated from the FID vs CLIP score tradeoff). We also note that using the weak model often leads to images with better text alignment. We hypothesize that fewer tokens (as result of larger patch sizes) help with spatial consistency at the beginning of the denoising process. To calculate VQA scores, we take the first 200 prompts from each dataset and 7https://huggingface.co/zhiqiulin/clip-flant5-xxl Figure 27. We plot FID vs CLIP score when generating images with different CFG scales. (left) Overall Pareto front when generating images with 100 denoising steps. (right) Pareto front generating images with different number of steps zoomed in the typical tradeoff generation values. use the train split from the DrawBench [84], train split from the Pick-a-Pic [55], test split from the Winoground [94] and tifa v1.0 text inputs8 from the TIFA160 [43] dataset. CFG scale scfg DrawBench Pick-a-Pic Winoground TIFA160 Average T2I Transf. (100 %) T2I Transf. (92.9 %) T2I Transf. (85.8 %) T2I Transf. (78.8 %) T2I Transf. (71.7 %) T2I Transf. (64.6 %) T2I Transf. (57.7 %) T2I Transf. (50.5 %) T2I Transf. (43.4 %) 4.5 4.5 4.5 4.7 4.7 4.9 5.0 5.0 5.0 58.93 58.37 57.58 58.62 58.44 60.16 59.04 56.77 56.87 50.56 51.53 51.67 51.97 51.56 52.34 50.80 51.72 51. 62.01 62.09 62.41 62.04 63.03 62.98 63.27 61.87 61.30 81.65 82.16 81.53 80.60 80.57 80.06 79.90 79.38 78.33 63.29 63.54 63.30 63.31 63.40 63.89 63.26 62.44 62.11 Table 1. Detailed VQA evaluations for the benchmarks tested with the T2I Transf. model. As in the class-conditioned experiments, using more of the weak model during denoising requires higher CFG scale scfg to reach optimum performance. CFG-scale scfg DrawBench Pick-a-Pic Winoground TIFA160 Average Emu (100 %) Emu (84.3 %) Emu (68.6 %) Emu (52.9 %) 6.0 6.0 6.25 6.5 69.44 68.00 69.53 69.79 58.70 58.93 60.62 58.14 65.75 67.33 66.00 66.23 86.77 86.51 85.33 86. 70.17 70.19 70.37 70.09 Table 2. Detailed VQA evaluations for the benchmarks tested with the Emu model. As in the class-conditioned experiments, using more of the weak model during denoising requires higher CFG scale scfg to reach optimum performance. Alignment between powerful and weak model. It is common practice nowadays to train images (and especially videos) in different stages, where large (potentially lower quality) dataset is used for the first stage, followed by shorter finetuning stage, characterized by higher quality and aesthetically more pleasing images. Although we are directly distilling the weak model from the predictions of the powerful model, the data used throughout training are still important. In practice, 8https://github.com/Yushi-Hu/tifa/blob/main/tifa_v1.0/tifa_v1.0_text_inputs.json our fine-tuning is sample efficient, and we find that even few thousand images (< 5000) are enough to succeed. We thus suggest fine-tuning on the last (potentially smaller) but higher-quality dataset. When generating images based on shorter prompts with Emu, we use prompt re-writer, prompting small LLM to expand on the information provided. We consider this prompt re-writer as part of the model. C. Implementation Details We provide additional details on the experiments in the main text. C.1. Figure Details Prompts used for Fig. 1. We provide in Table 3 the exact prompts used to generate the images. Prompts for Fig. 1 The image shows frog wearing golden crown with intricate designs, sitting on wooden log in serene environment reminiscent of Japanese anime setting. The frogs crown is adorned with small gems and its eyes are large and expressive. The log is covered in moss and surrounded by lush greenery, with few cherry blossoms visible in the background. The frogs skin is vibrant shade of green with blue stripes, and it has regal demeanor, as if it is monarch of the forest. The overall atmosphere is peaceful and whimsical. The image shows serene waterfall cascading down rocky slope in lush tropical forest, reminiscent of Claude Monets impressionist style. Sunlight filters through the dense foliage above, casting dappled shadows on the misty veil surrounding the falls. The water plunges into crystal-clear pool, surrounded by large rocks and vibrant greenery. The atmosphere is tranquil, with warm color palette and soft brushstrokes evoking sense of serenity. The forest floor is covered in thick layer of leaves, and the sound of the waterfall echoes through the air. Table 3. Details on the prompts used to generate the images in the paper. In Fig. 2 (b), we fix randomness of the denoising process in terms of the initial sampled image p(xT ), and Details on Fig. 2. from the denoising process in Eq. (2). Then we generate images with 250 steps using the DiT-XL/2 official public checkpoint. During denoising, we modify only 1 of the 250 denoising steps, bypassing the model predictions from that step through high/low pass filter. We then compare the resulting generated images in terms of LPIPS, L2 distance, SSIM, and DreamSim. In general, modifying one of the first denoising steps, leads to larger final image differences, due to the accumulation of differences. We still note distinctive pattern: high-pass filter, i.e. removing low-pass components, leads to larger image differences during the first denoising steps. The opposite holds for the last denoising steps. We can thus argue, that low-pass components, i.e. coarser image details are more important compared to high-frequency details, for the first denoising steps. To calculate spatial frequencies, we keep the corresponding values of the FFT of an image. Details on Fig. 9. In Fig. 9, we plot GPU utilization when propagating different sequence lengths. All experiments are conducted in bfloat16 using PyTorch 2.5 and CUDA 12.4 and with our Emu DiT that has 24 layers and hidden dimension of 2048. We also plot peak FLOPs and memory bandwidth for the GPU tested, an NVIDIA H100 SXM5 in this case. When we report FLOPs, we count additions and multiplications separately, as it is commonly done [41]. We count FLOPs as the theoretical required operations for forward pass, and not the actual GPU operations, which might be higher number due to the potential recalculation of partial results. As bytes for the x-axis, we only consider the model parameters (2 bytes per model parameter). Note that for our choice of weak models, the GPU is fully utilized (it reaches the maximum compute intensity that can be achieved for this application). In reality, compute intensity drops when larger sequence lengths are used, mainly due to the larger memory footprint of intermediate activations. Thus, latency benefits are indeed even larger than FLOPs benefits reported in the paper. Compiling and fusing operations is crucial to ensure that the GPU is not bottlenecked by waiting instructions from the CPU. When we are performing inference with the weak model without first merging the LoRAs, inference time is proportional to the additional FLOPs required. For the attention operation, we use the memory efficient attention operation from the xformers library. Other efficient implementations of attention do not lead to significant differences. Our T2I model operates in 128 128 latent space, which means that using patch size of (1, 2, 2) results in 4096 tokens, compared to 1024 tokens for the (1, 4, 4) patch size. Our T2V model operates in 32 88 48 latent space, which means that using patch size of (1, 2, 2), (2, 2, 2), (1, 4, 4) leads to 33792, 16896 and 8448 tokens respectively. We obtain similar results using different-sized models, like our Video DiT model. When we report FLOPs in the paper, we report numbers for the denoising process, as it is commonly done. We thus ignore latency induced by decoders that map samples from latent space, or potential prompt-rewrite modules. The added latency by the decoder, is in our settings negligible. C.2. Flexifying Diffusion Transformers Although for the class-conditioned experiments, we use single embedding and de-embedding layer that we always project to the required patch size, we note that this projection can be done once to pre-calculate embedding and de-embeddding parameters during inference. These projected embeddings can then be used out of the box, for the tradeoff of some minuscule additional memory. The choice of as the underlying patch size is not too important for our experiments. In practice, we use value of = 4. As mentioned in the main text, we add positional embeddings according to the coordinates of each patch in the original image. schematic of this can be found on the right. Apart from the architecture modifications listed in the main text, we experimented with adding patch size specific temperatures in the self-attention computation: softmax (cid:32) QK τp (cid:33) V, where Q, K, are the queries, keys and values respectively and τp is patch size specific temperature initialized as 1. We do not include this in the end, as it occasionally leads to instabilities during fine-tuning, even under different parametrizations. Class-conditioned implementation details. We largely use the same hyperparameters as [78] to fine-tune. When finetuning to match distributions, we train to minimize the MMD loss, as introduced in Section B.1. During bootstrapping, we denoise images with DDPM scheduler, operating on = 250 steps, the same as the target inference scheduler. As we found that MMD distance is higher for diffusion steps closer to x0 q(x0) see also Fig. 11 , we bias the sampling to reflect that during training as well. Parameter training data learning rate weight decay EMA update frequency EMA update rate Value ImageNet 104 0.0 every step 0.9999 Table 4. Class-conditioned implementation details. In our experiments, we focused on fine-tuning pre-trained models, as we were interested in efficiency. We note that training with different patch sizes has been used in the past to also accelerate pre-training [3, 9]. We believe that flexible patch sizes can also be used in this application to accelerate pre-training. T2I Transf. implementation details. For the T2I Transf. model, we follow exactly the recipe of [15], and fine-tune 256 256 pre-trained variant9. For fine-tuning, we use the same image dataset, namely the SAM dataset10 with captions generated from vision-language model. The model has overall the same parameters as the DiT-XL/2 model, with the addition of cross-attention blocks. When adding new embedding and de-embedding layers, we initialize them as we did for wembed and de-embedding layers are initialized the class-conditioned experiments. Embedding layers are initialized to Qembed . Here wembed, wde-embed are the pre-trained model parameters and Qembed, Qde-embed are the same to wde-embedQde-embed patch size dependent fixed projection matrices that better preserve the norm of the output activations at initialization. As aforementioned, we add patch size embedding that is added to all tokens in the sequences after the tokenization step. This embedding is equal to 0 for the pre-trained patch size, to ensure functional preservation. We fine-tune the T2I Transf. model on small subset of the SAM dataset used to originally train the target model. We add LoRAs on the self-attention and feed-forward layers, with LoRA dimension of 32. We use higher learning rate, due to the different learning objectives distilling powerful models predictions into the ones of weal model. 9Our starting pre-trained model exactly matches the public checkpoint https://huggingface.co/PixArt-alpha/PixArt-XL-2-SAM256x256. 10https://segment-anything.com/."
        },
        {
            "title": "Value",
            "content": "training data optimizer learning rate weight decay gradient clipping batch size EMA update frequency EMA update rate LoRA rank SAM with captions from VLM model AdamW 8 104 102 0.02 512 every step 0.9999 32 Table 5. Image text-conditioned implementation details. Emu implementation details. Our Emu model is fundamentally identical to the T2I Transf. model. Small variations are due to different ways to calculate text embeddings, which lead to different number and size of the cross-attention tokens, and slight architectural modifications primarily the use of QK-normalization [22] and the use of learnable positional embeddings. We train using high-quality aesthetic dataset. To calculate metrics based on the 1024 1024 images generated with this model, we follow the evaluation protocol of [105] and resize images to 512 512. For both our Emu and our Video DiT model, we use LoRA rank of 64. T2V implementation details. As aforementioned, our Video DiT model has pre-trained patch size of (pf, ph, pw) = (1, 2, 2). Compared to our T2I experiments, we only change the 2D convolutional layers used for tokenization with 3D convolution layer. When increasing the temporal patch size pf, we duplicate parameters along that dimension. When interpolating positional embeddings, we also do that along the temporal dimension. No additional changes are made. For evaluation, we use the prompts from https://github.com/facebookresearch/MovieGenBench/blob/main/ benchmark/MovieGenVideoBench.txt to generate videos of length equal to 256 frames. We evaluate according to VBench [45] and report the average over Subject Consistency, Background Consistency, Temporal Flickering, Motion Smoothness, Dynamic Degree, Aesthetic Quality, Imaging Quality, Temporal Style and Overall Consistency. C.3. Human Evaluation Details We prompt humans, asking them to: Compare the two side-by-side images. Focus on visual appeal and flawlessness, considering factors like aesthetic appeal, clarity, composition, color harmony, lighting, and overall quality, then select left if the left image is better, right if the right image is better, or tie if both are equally appealing or you have no preference.. In total, we collected votes for the 4 different settings presented in the paper and aggregated them across 200 prompts. For each setting and each prompt, we ask 3 people for votes. In cases where there are 3 votes for each of left, right, and tie, we ask fourth labeler to break the tie. D. Generated Samples We provide more examples of generated samples. D.1. Text-Conditioned Image Generation We showcase more examples with varying amounts of compute in Fig. 28, 29 and 30. We further show more examples of how performance and diversity in image generation are preserved in Fig. 31 and 32. Finally, we show examples of the effect of CFG scale scfg and reducing the overall number of FLOPs used to generate an image using our method, in Fig. 33. For all the images seen in the paper with our Emu model, we use 50 steps of the DDIM scheduler. D.2. Text-Conditioned Video Generation We showcase more examples of video generation with our flexible model in Fig. 34 and 35. D.3. Class-Conditioned ImageNet Generation We provide more comprehensive comparison for generated images from the same original sample from pnoise, using varying amounts of compute from our flexible model in Fig. 36. Note that for class-conditioned generation we do not use Figure 28. More samples generated by our Emu model for varying amounts of compute. LoRAs, and images generated from the original baseline model may not be exactly the same. They do however have the same characteristics (FID score) as seen in Sec 4.1. We also show samples of our flexible DiT-XL/2 model when using only 64% of the compute of the original model in Fig. 37, 38, 39, 40, 41, 42 and 43. All images are generated using 250 DDPM steps and CFG-scale equal to 4.0. Figure 29. More samples generated by our Emu model for varying amounts of compute. Figure 30. More samples generated by our Emu model for varying amounts of compute. Figure 31. Samples for the prompt: playful kitten just waking up.. We showcase the image generated by the baseline and our flexible scheduler using only 53% of FLOPs. Figure 32. Samples for the prompt: baby hippo swimming in the river.. We showcase the image generated by the baseline and our flexible scheduler using only 53% of FLOPs. Figure 33. Effect of CFG and total compute for the prompt: The image shows gigantic juicy burger placed on white plate on wooden table. The burger is composed of large beef patty, crispy bacon, melted cheese, lettuce, tomato, onion, pickles, and slice of red tomato, all sandwiched between soft bun. The burger is so large that it occupies most of the plate, with some toppings falling out of the sides. The bun is slightly toasted, and the cheese is melted to perfection, giving off savory aroma. The burger is garnished with side of crispy fries and refreshing glass of cola.. Figure 34. More samples generated by our Video DiT model, using 25.2 % compute compared to the pre-trained baseline. Figure 35. More samples generated by our Video DiT model, using 25.2 % compute compared to the pre-trained baseline. Figure 36. Sample for the ImageNet dataset comparing the baseline model and varying inference schedulers of our model, using different levels of compute. Figure 37. Samples for the ImageNet class tree frog, tree-frog from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 38. Samples for the ImageNet class prairie chicken, prairie grouse, prairie fowl from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 39. Samples for the ImageNet class hummingbird from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 40. Samples for the ImageNet class cairn, cairn terrier from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 41. Samples for the ImageNet class French bulldog from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 42. Samples for the ImageNet class Granny Smith from our FlexiDiT model that uses only 46% of the compute compared to the baseline model. Figure 43. Samples for the ImageNet class cock from our FlexiDiT model that uses only 46% of the compute compared to the baseline model."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "KAIST",
        "Meta GenAI"
    ]
}